Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212?216,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language-Independent Parsing with Empty Elements
Shu Cai and David Chiang
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
{shucai,chiang}@isi.edu
Yoav Goldberg
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
yoavg@cs.bgu.ac.il
Abstract
We  present  a  simple, language-independent
method for integrating recovery of empty ele-
ments into syntactic parsing. This method out-
performs  the  best  published  method  we  are
aware of on English and a recently published
method on Chinese.
1 Introduction
Empty elements in the syntactic analysis of a sen-
tence are markers that show where a word or phrase
might otherwise be expected to appear, but does not.
They play an important role in understanding the
grammatical relations in the sentence. For example,
in the tree of Figure 2a, the first empty element (*)
marks where John would be if believed were in the
active voice (someone believed. . .), and the second
empty element (*T*) marks where the manwould be
ifwhowere not fronted (John was believed to admire
who?).
Empty elements exist in many languages and serve
different purposes. In languages such as Chinese and
Korean, where subjects and objects can be dropped
to avoid duplication, empty elements are particularly
important, as they indicate the position of dropped
arguments. Figure 1 gives an example of a Chinese
parse tree with empty elements. The first empty el-
ement (*pro*) marks the subject of the whole sen-
tence, a pronoun inferable from context. The second
empty element (*PRO*) marks the subject of the de-
pendent VP (sh?sh? f?l? ti?ow?n).
The Penn Treebanks (Marcus et  al., 1993; Xue
et al, 2005) contain detailed annotations of empty
elements. Yet  most  parsing  work  based  on  these
resources has ignored empty elements, with some
.IP
. .VP
. .VP
. .IP
. .VP
. .NP
. .NN
.??
ti?ow?n
clause
.
NN
.??
f?l?
law
.
VV
.??
sh?sh?
implement
.
NP
.-NONE-
.*PRO*
.
VV
.??
zh?ngzh?
suspend
.
ADVP
.AD
.??
z?nsh?
for now
.
NP
.-NONE-
.*pro*
Figure 1: Chinese parse tree with empty elements marked.
The meaning of the sentence is, ?Implementation of the
law is temporarily suspended.?
notable exceptions. Johnson (2002) studied empty-
element  recovery in English, followed by several
others (Dienes and Dubey, 2003; Campbell, 2004;
Gabbard et al, 2006); the best results we are aware of
are due to Schmid (2006). Recently, empty-element
recovery for Chinese has begun to receive attention:
Yang and Xue (2010) treat it as classification prob-
lem, while Chung and Gildea (2010) pursue several
approaches for both Korean and Chinese, and ex-
plore applications to machine translation.
Our intuition motivating this work is that empty
elements are an integral part of syntactic structure,
and should be constructed jointly with it, not added
in afterwards. Moreover, we expect empty-element
recovery to improve as the parsing quality improves.
Our method makes use of a strong syntactic model,
the PCFGs with latent annotation of Petrov et al
(2006), which  we  extend  to  predict  empty  cate-
212
gories  by the  use  of lattice  parsing. The method
is language-independent and performs very well on
both languages we tested it on: for English, it out-
performs the best published method we are aware of
(Schmid, 2006), and for Chinese, it outperforms the
method of Yang and Xue (2010).
1
2 Method
Our method is fairly simple. We take a state-of-the-
art parsing model, the Berkeley parser (Petrov et al,
2006), train it on data with explicit empty elements,
and test it on word lattices that can nondeterminis-
tically insert empty elements anywhere. The idea is
that the state-splitting of the parsing model will en-
able it to learn where to expect empty elements to be
inserted into the test sentences.
Tree transformations Prior to training, we alter
the annotation of empty elements so that the termi-
nal label is a consistent symbol (?), the preterminal
label is the type of the empty element, and -NONE-
is deleted (see Figure 2b). This simplifies the lat-
tices because there is only one empty symbol, and
helps the parsing model to learn dependencies be-
tween nonterminal labels and empty-category types
because there is no intervening -NONE-.
Then, following Schmid (2006), if a constituent
contains an empty element that is linked to another
node with label X, then we append /X to its label.
If there is more than one empty element, we pro-
cess them bottom-up (see Figure 2b). This helps the
parser learn to expect where to find empty elements.
In our experiments, we did this only for elements of
type *T*. Finally, we train the Berkeley parser on the
preprocessed training data.
Lattice parsing Unlike the training data, the test
data does not mark any empty elements. We allow
the parser to produce empty elements by means of
lattice-parsing (Chappelier et al, 1999), a general-
ization of CKY parsing allowing it to parse a word-
lattice instead of a predetermined list of terminals.
Lattice parsing adds a layer of flexibility to exist-
ing parsing technology, and allows parsing in sit-
uations where the yield of  the tree  is  not  known
in advance. Lattice parsing originated in the speech
1
Unfortunately, not  enough  information  was  available  to
carry out comparison with the method of Chung and Gildea
(2010).
processing community  (Hall, 2005; Chappelier  et
al., 1999), and  was  recently  applied  to  the  task
of joint clitic-segmentation and syntactic-parsing in
Hebrew  (Goldberg  and  Tsarfaty, 2008; Goldberg
and Elhadad, 2011) and Arabic (Green and Man-
ning, 2010). Here, we use lattice parsing for empty-
element recovery.
We use a modified version of the Berkeley parser
which allows handling lattices as input.
2
The modifi-
cation is fairly straightforward: Each lattice arc cor-
respond to a lexical item. Lexical items are now in-
dexed by their start and end states rather than by
their sentence position, and the initialization proce-
dure of the CKY chart is changed to allow lexical
items of spans greater than 1. We then make the nec-
essary adjustments to the parsing algorithm to sup-
port this change: trying rules involving preterminals
even when the span is greater than 1, and not relying
on span size for identifying lexical items.
At test time, we first construct a lattice for each
test sentence that allows 0, 1, or 2 empty symbols
(?) between each pair of words or at the start/end of
the sentence. Then we feed these lattices through our
lattice parser to produce trees with empty elements.
Finally, we reverse the transformations that had been
applied to the training data.
3 Evaluation Measures
Evaluation metrics for empty-element recovery are
not well established, and previous studies use a vari-
ety of metrics. We review several of these here and
additionally propose a unified evaluation of parsing
and empty-element recovery.
3
If A and B are multisets, let A(x) be the number
of occurrences of x in A, let |A| = ?x A(x), and
let A ? B be the multiset such that (A ? B)(x) =
min(A(x), B(x)). If T is the multiset of ?items? in the
trees being tested andG is the multiset of ?items? in
the gold-standard trees, then
precision =
|G ? T |
|T | recall =
|G ? T |
|G|
F1 =
2
1
precision
+
1
recall
2
The modified parser is available at http://www.cs.bgu.
ac.il/~yoavg/software/blatt/
3
We provide a scoring script which supports all of these eval-
uation metrics. The code is available at http://www.isi.edu/
~chiang/software/eevalb.py .
213
.SBARQ
. .SQ
. .VP
. .S
. .VP
. .VP
. .NP
.-NONE-
.*T*
.
VB
.admire
.
TO
.to
.
NP
.-NONE-
.*
.
VBN
.believed
.
.NP
.NNP
.John
.
VBZ
.is
.
WHNP
.WP
.who
.SBARQ
. .SQ/WHNP
. .VP/WHNP/NP
. .S/WHNP/NP
. .VP/WHNP
. .VP/WHNP
. .NP/WHNP
.*T*
.?
.
VB
.admire
.
TO
.to
.
NP
.*
.?
.
VBN
.believed
.
.NP
.NNP
.John
.
VBZ
.is
.
WHNP
.WP
.who
(a) (b)
Figure 2: English parse tree with empty elements marked. (a) As annotated in the Penn Treebank. (b) With empty
elements reconfigured and slash categories added.
where ?items? are defined differently for each met-
ric, as  follows. Define  a nonterminal node, for
present purposes, to be a node which is neither a ter-
minal nor preterminal node.
The  standard  PARSEVAL metric  (Black  et  al.,
1991) counts labeled nonempty brackets: items are
(X, i, j) for each nonempty nonterminal node, where
X is its label and i, j are the start and end positions
of its span.
Yang  and  Xue  (2010)  simply  count unlabeled
empty elements: items are (i, i) for each empty ele-
ment, where i is its position. If multiple empty ele-
ments occur at the same position, they only count the
last one.
The metric originally proposed by Johnson (2002)
counts labeled empty brackets: items are (X/t, i, i) for
each empty nonterminal node, where X is its label
and t is the type of the empty element it dominates,
but also (t, i, i) for each empty element not domi-
nated by an empty nonterminal node.
4
The following
structure has an empty nonterminal dominating two
empty elements:
.SBAR
. .S
.-NONE-
.*T*
.
-NONE-
.0
Johnson  counts  this  as (SBAR, i, i), (S/*T*, i, i);
Schmid  (2006)  counts  it  as  a  single
4
This happens in the Penn Treebank for types *U* and 0, but
never in the Penn Chinese Treebank except by mistake.
(SBAR-S/*T*, i, i).5 We  tried  to  follow  Schmid
in a generic way: we collapse any vertical chain of
empty nonterminals into a single nonterminal.
In order to avoid problems associated with cases
like this, we suggest a pair of simpler metrics. The
first is to count labeled empty elements, i.e., items
are (t, i, i) for each empty element, and the second,
similar in spirit to SParseval (Roark et al, 2006), is
to count all labeled brackets, i.e., items are (X, i, j)
for  each nonterminal  node (whether  nonempty or
empty). These two metrics, together with part-of-
speech accuracy, cover all possible nodes in the tree.
4 Experiments and Results
English As is standard, we trained the parser on
sections 02?21 of  the Penn Treebank Wall  Street
Journal corpus, used section 00 for development, and
section 23 for testing. We ran 6 cycles of training;
then, because we were unable to complete the 7th
split-merge cycle with the default setting of merg-
ing 50% of splits, we tried increasing merges to 75%
and ran 7 cycles of training. Table 1 presents our
results. We chose the parser settings that gave the
best labeled empty elements F1 on the dev set, and
used these settings for the test set. We outperform the
state of the art at recovering empty elements, as well
as achieving state of the art accuracy at recovering
phrase structure.
5
This difference is not small; scores using Schmid?s metric
are lower by roughly 1%. There are other minor differences in
Schmid?s metric which we do not detail here.
214
Labeled Labeled All Labeled
Empty Brackets Empty Elements Brackets
Section System P R F1 P R F1 P R F1
00 Schmid (2006) 88.3 82.9 85.5 89.4 83.8 86.5 87.1 85.6 86.3
split 5? merge 50% 91.0 79.8 85.0 93.1 81.8 87.1 90.4 88.7 89.5
split 6? merge 50% 91.9 81.1 86.1 93.6 82.4 87.6 90.4 89.1 89.7
split 6? merge 75% 92.7 80.7 86.3 94.6 82.0 87.9 90.3 88.5 89.3
split 7? merge 75% 91.0 80.4 85.4 93.2 82.1 87.3 90.5 88.9 89.7
23 Schmid (2006) 86.1 81.7 83.8 87.9 83.0 85.4 86.8 85.9 86.4
split 6? merge 75% 90.1 79.5 84.5 92.3 80.9 86.2 90.1 88.5 89.3
Table 1: Results on Penn (English) Treebank, Wall Street Journal, sentences with 100 words or fewer.
Unlabeled Labeled All Labeled
Empty Elements Empty Elements Brackets
Task System P R F1 P R F1 P R F1
Dev split 5? merge 50% 82.5 58.0 68.1 72.6 51.8 60.5 84.6 80.7 82.6
split 6? merge 50% 76.4 60.5 67.5 68.2 55.1 60.9 83.2 81.3 82.2
split 7? merge 50% 74.9 58.7 65.8 65.9 52.5 58.5 82.7 81.1 81.9
Test Yang and Xue (2010) 80.3 57.9 63.2
split 6? merge 50% 74.0 61.3 67.0 66.0 54.5 58.6 82.7 80.8 81.7
Table 2: Results on Penn (Chinese) Treebank.
Chinese We  also  experimented  on  a  subset  of
the  Penn  Chinese  Treebank  6.0. For  comparabil-
ity  with  previous  work  (Yang  and  Xue, 2010),
we trained the parser on sections 0081?0900, used
sections 0041?0080 for development, and sections
0001?0040 and 0901?0931 for testing. The results
are shown in Table 2.We selected the 6th split-merge
cycle based on the labeled empty elements F1 mea-
sure. The unlabeled empty elements column shows
that our system outperforms the baseline system of
Yang and Xue (2010). We also analyzed the empty-
element recall by type (Table 3). Our system outper-
formed that of Yang and Xue (2010) especially on
*pro*, used for dropped arguments, and *T*, used
for relative clauses and topicalization.
5 Discussion and Future Work
The  empty-element  recovery  method  we  have
presented  is  simple, highly  effective, and  fully
integrated with  state  of  the  art  parsing. We hope
to  exploit  cross-lingual  information  about  empty
elements  in  machine  translation. Chung  and
Gildea (2010)  have  shown that  such  information
indeed helps translation, and we plan to extend this
work  by  handling  more  empty  categories  (rather
Total Correct Recall
Type Gold YX Ours YX Ours
*pro* 290 125 159 43.1 54.8
*PRO* 299 196 199 65.6 66.6
*T* 578 338 388 58.5 67.1
*RNR* 32 20 15 62.5 46.9
*OP* 134 20 65 14.9 48.5
* 19 5 3 26.3 15.8
Table 3: Recall on different types of empty categories.
YX = (Yang and Xue, 2010), Ours = split 6?.
than just *pro* and *PRO*), and to incorporate them
into a syntax-based translation model instead of a
phrase-based model.
We also plan to extend our work here to recover
coindexation information (links between a moved el-
ement and the trace which marks the position it was
moved from). As a step towards shallow semantic
analysis, this may further benefit other natural lan-
guage processing tasks such as machine translation
and summary generation.
Acknowledgements
We would like to thank Slav Petrov for his help in
running the Berkeley parser, and Yaqin Yang, Bert
215
Xue, Tagyoung Chung, and Dan Gildea for their an-
swering our  many questions. We would also like
to  thank  our  colleagues  in  the  Natural  Language
Group  at  ISI for  meaningful  discussions  and  the
anonymous reviewers for their thoughtful sugges-
tions. This work was supported in part by DARPA
under contracts HR0011-06-C-0022 (subcontract to
BBN Technologies) and DOI-NBC N10AP20031,
and by NSF under contract IIS-0908532.
References
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proc. DARPA Speech and Natu-
ral Language Workshop.
Richard Campbell. 2004. Using linguistic principles to
recover empty categories. In Proc. ACL.
J.-C. Chappelier, M. Rajman, R. Aragu?es, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Proc. Traitement Automatique du Langage Naturel
(TALN).
Tagyoung Chung and Daniel  Gildea. 2010. Effects
of empty categories on machine translation. In Proc.
EMNLP.
Pe?ter Dienes and Amit Dubey. 2003. Antecedent recov-
ery: Experiments with a trace tagger. In Proc. EMNLP.
Ryan Gabbard, Seth Kulick, and Mitchell Marcus. 2006.
Fully parsing the Penn Treebank. In Proc. NAACL
HLT.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew segmentation and parsing using a PCFG-LA lat-
tice parser. In Proc. of ACL.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
ative model for joint morphological segmentation and
syntactic parsing. In Proc. of ACL.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis. In
Proc of COLING-2010.
Keith B. Hall. 2005. Best-first word-lattice parsing:
techniques for integrated syntactic language modeling.
Ph.D. thesis, Brown University, Providence, RI, USA.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm  for  recovering  empty  nodes  and  their  an-
tecedents. In Proc. ACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Slav  Petrov, Leon Barrett, Romain  Thibaux, and  Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING-ACL.
Brian  Roark, Mary  Harper, Eugene  Charniak, Bonnie
Dorr, Mark Johnson, Jeremy G. Kahn, Yang Liu, Mari
Ostendorf, John Hale, Anna Krasnyanskaya, Matthew
Lease, Izhak Shafran, Matthew Snover, Robin Stewart,
and Lisa Yung. 2006. SParseval: Evaluation metrics
for parsing speech. In Proc. LREC.
Helmut Schmid. 2006. Trace prediction and recovery
with unlexicalized PCFGs and slash features. In Proc.
COLING-ACL.
Nianwen  Xue, Fei  Xia, Fu-dong  Chiou, and  Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
recovering empty categories in the Chinese Treebank.
In Proc. COLING.
216
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 748?752,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Smatch: an Evaluation Metric for Semantic Feature Structures
Shu Cai
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
shucai@isi.edu
Kevin Knight
USC Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
knight@isi.edu
Abstract
The evaluation of whole-sentence seman-
tic structures plays an important role in
semantic parsing and large-scale seman-
tic structure annotation. However, there is
no widely-used metric to evaluate whole-
sentence semantic structures. In this pa-
per, we present smatch, a metric that cal-
culates the degree of overlap between two
semantic feature structures. We give an
efficient algorithm to compute the metric
and show the results of an inter-annotator
agreement study.
1 Introduction
The goal of semantic parsing is to generate all se-
mantic relationships in a text. Its output is of-
ten represented by whole-sentence semantic struc-
tures. Evaluating such structures is necessary for
semantic parsing tasks, as well as semantic anno-
tation tasks which create linguistic resources for
semantic parsing.
However, there is no widely-used evalua-
tion method for whole-sentence semantic struc-
tures. Current whole-sentence semantic parsing
is mainly evaluated in two ways: 1. task cor-
rectness (Tang and Mooney, 2001), which eval-
uates on an NLP task that uses the parsing re-
sults; 2. whole-sentence accuracy (Zettlemoyer
and Collins, 2005), which counts the number of
sentences parsed completely correctly.
Nevertheless, it is worthwhile to explore evalua-
tion methods that use scores which range from 0 to
1 (?partial credit?) to measure whole-sentence se-
mantic structures. By using such methods, we are
able to differentiate between two similar whole-
sentence semantic structures regardless of specific
tasks or domains. In this work, we provide an eval-
uation metric that uses the degree of overlap be-
tween two whole-sentence semantic structures as
the partial credit.
In this paper, we observe that the difficulty
of computing the degree of overlap between two
whole-sentence semantic feature structures comes
from determining an optimal variable alignment
between them, and further prove that finding such
alignment is NP-complete. We investigate how to
compute this metric and provide several practical
and replicable computing methods by using Inte-
ger Linear Programming (ILP) and hill-climbing
method. We show that our metric can be used
for measuring the annotator agreement in large-
scale linguistic annotation, and evaluating seman-
tic parsing.
2 Semantic Overlap
We work on a semantic feature structure represen-
tation in a standard neo-Davidsonian (Davidson,
1969; Parsons, 1990) framework. For example,
semantics of the sentence ?the boy wants to go? is
represented by the following directed graph:
In this graph, there are three concepts: want-
01, boy, and go-01. Both want-01 and go-01 are
frames from PropBank framesets (Kingsbury and
Palmer, 2002). The frame want-01 has two argu-
ments connected with ARG0 and ARG1, and go-
01 has an argument (which is also the same boy
instance) connected with ARG0.
748
Following (Langkilde and Knight, 1998) and
(Langkilde-Geary, 2002), we refer to this semantic
representation as AMR (Abstract Meaning Repre-
sentation).
Semantic relationships encoded in the AMR
graph can also be viewed as a conjunction of logi-
cal propositions, or triples:
instance(a, want-01) ?
instance(b, boy) ?
instance(c, go-01) ?
ARG0(a, b) ?
ARG1(a, c) ?
ARG0(c, b)
Each AMR triple takes one of these forms:
relation(variable, concept) (the first three triples
above), or relation(variable1, variable2) (the last
three triples above).
Suppose we take a second AMR (for ?the boy
wants the football?) and its associated proposi-
tional triples:
instance(x, want-01) ?
instance(y, boy) ?
instance(z, football) ?
ARG0(x, y) ?
ARG1(x, z)
Our evaluation metric measures precision, re-
call, and f-score of the triples in the second AMR
against the triples in the first AMR, i.e., the
amount of propositional overlap.
The difficulty is that variable names are not
shared between the two AMRs, so there are mul-
tiple ways to compute the propositional overlap
based on different variable mappings. We there-
fore define the smatch score (for semantic match)
as the maximum f-score obtainable via a one-to-
one matching of variables between the two AMRs.
In the example above, there are six ways to
match up variables between the two AMRs:
M P R F
x=a, y=b, z=c: 4 4/5 4/6 0.73
x=a, y=c, z=b: 1 1/5 1/6 0.18
x=b, y=a, z=c: 0 0/5 0/6 0.00
x=b, y=c, z=a: 0 0/5 0/6 0.00
x=c, y=a, z=b: 0 0/5 0/6 0.00
x=c, y=b, z=a: 2 2/5 2/6 0.36
----------------------------------
smatch score: 0.73
Here, M is the number of propositional triples that
agree given a variable mapping, P is the precision
of the second AMR against the first, R is its re-
call, and F is its f-score. The smatch score is the
maximum of the f-scores.
However, for AMRs that contain large number
of variables, it is not efficient to get the f-score by
simply using the method above. Exhaustively enu-
merating all variable mappings requires comput-
ing the f-score for n!/(n?m)! variable mappings
(assuming one AMR has n variables and the other
has m variables, and m ? n). This algorithm is
too slow for all but the shortest AMR pairs.
3 Computing the Metric
This section describes how to compute the smatch
score. As input, we are given AMR1 (with m vari-
ables) and AMR2 (with n variables). Without loss
of generality, m ? n.
Baseline. Our baseline first matches variables
that share concepts. For example, it would match
a in the first AMR example with x in the second
AMR example of Section 2, because both are in-
stances of want-01. If there are two or more vari-
ables to choose from, we pick the first available
one. The rest of the variables are mapped ran-
domly.
ILP method. We can get an optimal solution
using integer linear programming (ILP). We create
two types of variables:
? (Variable mapping) vij = 1 iff the ith vari-
able in AMR1 is mapped to the jth variable
in AMR2 (otherwise vij = 0)
? (Triple match) tkl = 1 iff AMR1 triple
k matches AMR2 triple l, otherwise tkl
= 0. A triple relation1(xy) matches
relation2(wz) iff relation1 = relation2, vxw
= 1, and vyz = 1 or y and z are the same con-
cept.
Our constraints ensure a one-to-one mapping of
variables, and they ensure that the chosen t values
are consistent with the chosen v values:
For all i,
?
j
vij ? 1
For all j,
?
i
vij ? 1
For all triple pairs r(xy)r(wz) (r for relation),
tr(xy)r(wz) ? vxw
749
tr(xy)r(wz) ? vyz
when y and z are variables.
Finally, we ask the ILP solver to maximize:
?
kl
tkl
which denotes the maximum number of matching
triples which lead to the smatch score.
Hill-climbing method. Finally, we develop a
portable heuristic algorithm that does not require
an ILP solver1. This method works in a greedy
style. We begin with m random one-to-one map-
pings between the m variables of AMR1 and the
n variables of AMR2. Each variable mapping is
a pair (i,map(i)) with 1 ? i ? m and 1 ?
map(i) ? n. We refer to the m mappings as a
variable mapping state.
We first generate a random initial variable map-
ping state, compute its triple match number, then
hill-climb via two types of small changes:
1. Move one of the m mappings to a currently-
unmapped variable from the n.
2. Swap two of the m mappings.
Any variable mapping state has m(n ? m) +
m(m ? 1) = m(n ? 1) neighbors during the
hill-climbing search. We greedily choose the best
neighbor, repeating until no neighbor improves the
number of triple matches.
We experiment with two modifications to the
greedy search: (1) executing multiple random
restarts to avoid local optima, and (2) using our
Baseline concept matching (?smart initialization?)
instead of random initialization.
NP-completeness. There is unlikely to be
an exact polynomial-time algorithm for comput-
ing smatch. We can reduce the 0-1 Maximum
Quadratic Assignment Problem (0-1-Max-QAP)
(Nagarajan and Sviridenko, 2009) and the sub-
graph isomorphism problem directly to the full
smatch problem on graphs.2
We note that other widely-used metrics, such as
TER (Snover et al, 2006), are also NP-complete.
Fortunately, the next section shows that the smatch
methods above are efficient and effective.
1The tool can be downloaded at
http://amr.isi.edu/evaluation.html.
2Thanks to David Chiang for observing the subgraph iso-
morphism reduction.
4 Using Smatch
We report an AMR inter-annotator agreement
study using smatch.
1. Our study has 4 annotators (A, B, C, D), who
then converge on a consensus annotation E.
We thus have 10 pairs of annotations: A-B,
A-C, . . . , D-E.
2. The study is carried out 5 times. Each
time annotators build AMRs for 4 sentences
from the Wall Street Journal corpus. Sen-
tence lengths range from 12 to 54 words, and
AMRs range from 6 to 29 variables.
3. We use 7 smatch calculation methods in our
experiments:
? Base: Baseline matching method
? ILP: Integer Linear Programming
? R: Hill-climbing with random initializa-
tion
? 10R: Hill-climbing with random initial-
ization plus 9 random restarts
? S: Hill-climbing with smart initializa-
tion
? S+4R: Hill-climbing with smart initial-
ization plus 4 random restarts
? S+9R: Hill-climbing with smart initial-
ization plus 9 random restarts
Table 1 shows smatch scores provided by the
methods. Columns labeled 1-5 indicate sen-
tence groups. Each individual smatch score is
a document-level score of 4 AMR pairs.3 ILP
scores are optimal, so lower scores (in bold) in-
dicate search errors.
Table 2 summarizes search accuracy as a per-
centage of smatch scores that equal that of ILP.
Results show that the restarts are essential for hill-
climbing, and that 9 restarts are sufficient to obtain
good quality. The table also shows total runtimes
over 200 AMR pairs (10 annotator pairs, 5 sen-
tence groups, 4 AMR pairs per group). Heuris-
tic search with smart initialization and 4 restarts
(S+4R) gives the best trade-off between accuracy
and speed, so this is the setting we use in practice.
Figure 1 shows smatch scores of each annotator
(A-D) against the consensus annotation (E). The
3For documents containing multiple AMRs, we use the
sum of matched triples over all AMR pairs to compute pre-
cision, recall, and f-score, much like corpus-level Bleu (Pap-
ineni et al, 2002).
750
B C D E1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5Base 0.68 0.74 0.84 0.71 0.83 0.69 0.70 0.80 0.69 0.78 0.77 0.72 0.75 0.68 0.63 0.79 0.86 0.92 0.85 0.89ILP 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92R 0.74 0.79 0.84 0.75 0.86 0.74 0.75 0.80 0.77 0.88 0.83 0.76 0.75 0.72 0.75 0.85 0.92 0.92 0.89 0.89A 10R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S 0.74 0.80 0.84 0.75 0.88 0.75 0.78 0.80 0.76 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S+4R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92S+9R 0.74 0.80 0.84 0.76 0.88 0.75 0.78 0.80 0.77 0.88 0.83 0.77 0.75 0.72 0.76 0.85 0.92 0.92 0.89 0.92
Base - - - - - 0.72 0.68 0.74 0.69 0.79 0.71 0.72 0.76 0.65 0.57 0.68 0.71 0.83 0.79 0.86ILP - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89R - - - - - 0.74 0.83 0.72 0.72 0.83 0.78 0.83 0.76 0.68 0.68 0.74 0.81 0.83 0.83 0.89B 10R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S - - - - - 0.73 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S+4R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89S+9R - - - - - 0.74 0.83 0.74 0.75 0.85 0.78 0.83 0.76 0.68 0.73 0.76 0.81 0.83 0.83 0.89
Base - - - - - - - - - - 0.68 0.68 0.74 0.69 0.65 0.64 0.64 0.87 0.79 0.83ILP - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89R - - - - - - - - - - 0.74 0.79 0.74 0.75 0.78 0.71 0.76 0.87 0.85 0.89C 10R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89S - - - - - - - - - - 0.74 0.79 0.74 0.77 0.81 0.74 0.76 0.87 0.85 0.89S+4R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89S+9R - - - - - - - - - - 0.74 0.79 0.74 0.78 0.81 0.74 0.76 0.87 0.85 0.89
Base - - - - - - - - - - - - - - - 0.68 0.69 0.81 0.74 0.64ILP - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79R - - - - - - - - - - - - - - - 0.77 0.73 0.81 0.78 0.79D 10R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S+4R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79S+9R - - - - - - - - - - - - - - - 0.77 0.78 0.81 0.78 0.79
Table 1: Inter-annotator smatch agreement for 5 groups of sentences, as computed with seven different
methods (Base, ILP, R, 10R, S, S+4R, S+9R). The number 1-5 indicate the sentence group number. Bold
scores are search errors.
Base ILP R 10R S S+4R S+9RAccuracy 20% 100% 66.5% 100% 92% 100% 100%Time (sec) 0.86 49.67 5.85 64.78 2.31 28.36 59.69
Table 2: Accuracy and running time (seconds) of
various computing methods of smatch over 200
AMR pairs.
plot demonstrates that, as time goes by, annotators
reach better agreement with the consensus.
We also note that smatch is used to measure
the accuracy of machine-generated AMRs. (Jones
et al, 2012) use it to evaluate automatic seman-
tic parsing in a narrow domain, while Ulf Her-
mjakob4 has developed a heuristic algorithm that
exploits and supplements Ontonotes annotations
(Pradhan et al, 2007) in order to automatically
create AMRs for Ontonotes sentences, with a
smatch score of 0.74 against human consensus
AMRs.
5 Related Work
Related work on directly measuring the seman-
tic representation includes the method in (Dri-
dan and Oepen, 2011), which evaluates semantic
parser output directly by comparing semantic sub-
structures, though they require an alignment be-
tween sentence spans and semantic sub-structures.
In contrast, our metric does not require the align-
4personal communication
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 1  2  3  4  5Sm
atc
h s
co
re
s o
f e
ac
h a
nn
ota
tor
 ag
ain
st 
co
ns
en
su
s
Time
Annotator A
Annotator B
Annotator C
Annotator D
Figure 1: Smatch scores of annotators (A-D)
against the consensus annotation (E) over time.
ment between an input sentence and its semantic
analysis. (Allen et al, 2008) propose a metric
which computes the maximum score by any align-
ment between LF graphs, but they do not address
how to determine the alignments.
6 Conclusion and Future Work
We present an evaluation metric for whole-
sentence semantic analysis, and show that it can
be computed efficiently. We use the metric to
measure semantic annotation agreement rates and
parsing accuracy. In the future, we plan to investi-
gate how to adapt smatch to other semantic repre-
sentations.
751
7 Acknowledgements
We would like to thank David Chiang, Hui Zhang,
other ISI colleagues and our anonymous review-
ers for their thoughtful comments. This work was
supported by NSF grant IIS-0908532.
References
J.F. Allen, M. Swift, and W. Beaumont. 2008. Deep
Semantic Analysis of Text. In Proceedings of the
2008 Conference on Semantics in Text Processing.
D. Davidson. 1969. The Individuation of Events. In
Nicholas Rescher (ed.) Essays in Honor of Carl G.
HempeL Dordrecht: D. Reidel.
R. Dridan and S. Oepen. 2011. Parser Evaluation us-
ing Elementary Dependency Matching. In Proceed-
ings of the 12th International Conference on Parsing
Technologies.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-Based Machine Trans-
lation with Hyperedge Replacement Grammars. In
Proceedings of COLING.
P. Kingsbury and M. Palmer. 2002. From Treebank to
Propbank. In Proceedings of LREC.
I. Langkilde and K. Knight. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of COLING-ACL.
I. Langkilde-Geary. 2002. An Empirical Verifica-
tion of Coverage and Correctness for a General-
Purpose Sentence Generator. In Proceedings of In-
ternational Natural Language Generation Confer-
ence (INLG?02).
V. Nagarajan and M. Sviridenko. 2009. On the Maxi-
mum Quadratic Assignment Problem. Mathematics
of Operations Research, 34.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics.
T. Parsons. 1990. Events in the Semantics of English.
The MIT Press.
S. S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A Unified Relational Semantic Representation. In
Proceedings of the International Conference on Se-
mantic Computing (ICSC ?07).
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings
of the 7th Conference of the Association for Machine
Translation in the Americas (AMTA-2006).
L. R. Tang and R. J. Mooney. 2001. Using Multiple
Clause Constructors in Inductive Logic Program-
ming for Semantic Parsing. In Proceedings of the
12th European Conference on Machine Learning.
L. S. Zettlemoyer and M. Collins. 2005. Learning to
Map Sentences to Logical Form: Structured Classi-
fication with Probabilistic Categorial Grammars. In
Proceedings of the 21st Conference in Uncertainty
in Artificial Intelligence.
752
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 178?186,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Abstract Meaning Representation for Sembanking
Laura Banarescu
SDL
lbanarescu@sdl.com
Claire Bonial
Linguistics Dept.
Univ. Colorado
claire.bonial@colorado.edu
Shu Cai
ISI
USC
shucai@isi.edu
Madalina Georgescu
SDL
mgeorgescu@sdl.com
Kira Griffitt
LDC
kiragrif@ldc.upenn.edu
Ulf Hermjakob
ISI
USC
ulf@isi.edu
Kevin Knight
ISI
USC
knight@isi.edu
Philipp Koehn
School of Informatics
Univ. Edinburgh
pkoehn@inf.ed.ac.uk
Martha Palmer
Linguistics Dept.
Univ. Colorado
martha.palmer@colorado.edu
Nathan Schneider
LTI
CMU
nschneid@cs.cmu.edu
Abstract
We describe Abstract Meaning Represen-
tation (AMR), a semantic representation
language in which we are writing down
the meanings of thousands of English sen-
tences. We hope that a sembank of simple,
whole-sentence semantic structures will
spur new work in statistical natural lan-
guage understanding and generation, like
the Penn Treebank encouraged work on
statistical parsing. This paper gives an
overview of AMR and tools associated
with it.
1 Introduction
Syntactic treebanks have had tremendous impact
on natural language processing. The Penn Tree-
bank is a classic example?a simple, readable file
of natural-language sentences paired with rooted,
labeled syntactic trees. Researchers have ex-
ploited manually-built treebanks to build statisti-
cal parsers that improve in accuracy every year.
This success is due in part to the fact that we have
a single, whole-sentence parsing task, rather than
separate tasks and evaluations for base noun iden-
tification, prepositional phrase attachment, trace
recovery, verb-argument dependencies, etc. Those
smaller tasks are naturally solved as a by-product
of whole-sentence parsing, and in fact, solved bet-
ter than when approached in isolation.
By contrast, semantic annotation today is balka-
nized. We have separate annotations for named en-
tities, co-reference, semantic relations, discourse
connectives, temporal entities, etc. Each annota-
tion has its own associated evaluation, and training
data is split across many resources. We lack a sim-
ple readable sembank of English sentences paired
with their whole-sentence, logical meanings. We
believe a sizable sembank will lead to new work in
statistical natural language understanding (NLU),
resulting in semantic parsers that are as ubiquitous
as syntactic ones, and support natural language
generation (NLG) by providing a logical seman-
tic input.
Of course, when it comes to whole-sentence se-
mantic representations, linguistic and philosophi-
cal work is extensive. We draw on this work to de-
sign an Abstract Meaning Representation (AMR)
appropriate for sembanking. Our basic principles
are:
? AMRs are rooted, labeled graphs that are
easy for people to read, and easy for pro-
grams to traverse.
? AMR aims to abstract away from syntac-
tic idiosyncrasies. We attempt to assign the
same AMR to sentences that have the same
basic meaning. For example, the sentences
?he described her as a genius?, ?his descrip-
tion of her: genius?, and ?she was a ge-
nius, according to his description? are all as-
signed the same AMR.
? AMR makes extensive use of PropBank
framesets (Kingsbury and Palmer, 2002;
Palmer et al, 2005). For example, we rep-
resent a phrase like ?bond investor? using
the frame ?invest-01?, even though no verbs
appear in the phrase.
? AMR is agnostic about how we might want
to derive meanings from strings, or vice-
versa. In translating sentences to AMR, we
do not dictate a particular sequence of rule
applications or provide alignments that re-
flect such rule sequences. This makes sem-
banking very fast, and it allows researchers
to explore their own ideas about how strings
178
are related to meanings.
? AMR is heavily biased towards English. It
is not an Interlingua.
AMR is described in a 50-page annotation guide-
line.1 In this paper, we give a high-level descrip-
tion of AMR, with examples, and we also provide
pointers to software tools for evaluation and sem-
banking.
2 AMR Format
We write down AMRs as rooted, directed, edge-
labeled, leaf-labeled graphs. This is a com-
pletely traditional format, equivalent to the sim-
plest forms of feature structures (Shieber et al,
1986), conjunctions of logical triples, directed
graphs, and PENMAN inputs (Matthiessen and
Bateman, 1991). Figure 1 shows some of these
views for the sentence ?The boy wants to go?. We
use the graph notation for computer processing,
and we adapt the PENMAN notation for human
reading and writing.
3 AMR Content
In neo-Davidsonian fashion (Davidson, 1969), we
introduce variables (or graph nodes) for entities,
events, properties, and states. Leaves are labeled
with concepts, so that ?(b / boy)? refers to an in-
stance (called b) of the concept boy. Relations link
entities, so that ?(d / die-01 :location (p / park))?
means there was a death (d) in the park (p). When
an entity plays multiple roles in a sentence, we
employ re-entrancy in graph notation (nodes with
multiple parents) or variable re-use in PENMAN
notation.
AMR concepts are either English words
(?boy?), PropBank framesets (?want-01?), or spe-
cial keywords. Keywords include special entity
types (?date-entity?, ?world-region?, etc.), quan-
tities (?monetary-quantity?, ?distance-quantity?,
etc.), and logical conjunctions (?and?, etc).
AMR uses approximately 100 relations:
? Frame arguments, following PropBank
conventions. :arg0, :arg1, :arg2, :arg3, :arg4,
:arg5.
? General semantic relations. :accompa-
nier, :age, :beneficiary, :cause, :compared-to,
:concession, :condition, :consist-of, :degree,
:destination, :direction, :domain, :duration,
1AMR guideline: amr.isi.edu/language.html
LOGIC format:
? w, b, g:
instance(w, want-01) ? instance(g, go-01) ?
instance(b, boy) ? arg0(w, b) ?
arg1(w, g) ? arg0(g, b)
AMR format (based on PENMAN):
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
GRAPH format:
Figure 1: Equivalent formats for representating
the meaning of ?The boy wants to go?.
:employed-by, :example, :extent, :frequency,
:instrument, :li, :location, :manner, :medium,
:mod, :mode, :name, :part, :path, :polarity,
:poss, :purpose, :source, :subevent, :subset,
:time, :topic, :value.
? Relations for quantities. :quant, :unit,
:scale.
? Relations for date-entities. :day, :month,
:year, :weekday, :time, :timezone, :quarter,
:dayperiod, :season, :year2, :decade, :cen-
tury, :calendar, :era.
? Relations for lists. :op1, :op2, :op3, :op4,
:op5, :op6, :op7, :op8, :op9, :op10.
AMR also includes the inverses of all these rela-
tions, e.g., :arg0-of, :location-of, and :quant-of. In
addition, every relation has an associated reifica-
tion, which is what we use when we want to mod-
ify the relation itself. For example, the reification
of :location is the concept ?be-located-at-91?.
Our set of concepts and relations is designed to
allow us represent all sentences, taking all words
into account, in a reasonably consistent manner. In
the rest of this section, we give examples of how
AMR represents various kinds of words, phrases,
and sentences. For full documentation, the reader
is referred to the AMR guidelines.
179
Frame arguments. We make heavy use of
PropBank framesets to abstract away from English
syntax. For example, the frameset ?describe-01?
has three pre-defined slots (:arg0 is the describer,
:arg1 is the thing described, and :arg2 is what it is
being described as).
(d / describe-01
:arg0 (m / man)
:arg1 (m2 / mission)
:arg2 (d / disaster))
The man described the mission as a disaster.
The man?s description of the mission:
disaster.
As the man described it, the mission was a
disaster.
Here, we do not annotate words like ?as? or ?it?,
considering them to be syntactic sugar.
General semantic relations. AMR also in-
cludes many non-core relations, such as :benefi-
ciary, :time, and :destination.
(s / hum-02
:arg0 (s2 / soldier)
:beneficiary (g / girl)
:time (w / walk-01
:arg0 g
:destination (t / town)))
The soldier hummed to the girl as she
walked to town.
Co-reference. AMR abstracts away from co-
reference gadgets like pronouns, zero-pronouns,
reflexives, control structures, etc. Instead we re-
use AMR variables, as with ?g? above. AMR
annotates sentences independent of context, so if
a pronoun has no antecedent in the sentence, its
nominative form is used, e.g., ?(h / he)?.
Inverse relations. We obtain rooted structures
by using inverse relations like :arg0-of and :quant-
of.
(s / sing-01
:arg0 (b / boy
:source (c / college)))
The boy from the college sang.
(b / boy
:arg0-of (s / sing-01)
:source (c / college))
the college boy who sang ...
(i / increase-01
:arg1 (n / number
:quant-of (p / panda)))
The number of pandas increased.
The top-level root of an AMR represents the fo-
cus of the sentence or phrase. Once we have se-
lected the root concept for an entire AMR, there
are no more focus considerations?everything else
is driven strictly by semantic relations.
Modals and negation. AMR represents nega-
tion logically with :polarity, and it expresses
modals with concepts.
(g / go-01
:arg0 (b / boy)
:polarity -)
The boy did not go.
(p / possible
:domain (g / go-01
:arg0 (b / boy))
:polarity -))
The boy cannot go.
It?s not possible for the boy to go.
(p / possible
:domain (g / go-01
:arg0 (b / boy)
:polarity -))
It?s possible for the boy not to go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy))
:polarity -)
The boy doesn?t have to go.
The boy isn?t obligated to go.
The boy need not go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy)
:polarity -))
The boy must not go.
It?s obligatory that the boy not go.
(t / think-01
:arg0 (b / boy)
:arg1 (w / win-01
:arg0 (t / team)
:polarity -))
The boy doesn?t think the team will win.
The boy thinks the team won?t win.
Questions. AMR uses the concept ?amr-
unknown?, in place, to indicate wh-questions.
(f / find-01
:arg0 (g / girl)
:arg1 (a / amr-unknown))
What did the girl find?
(f / find-01
:arg0 (g / girl)
:arg1 (b / boy)
:location (a / amr-unknown))
Where did the girl find the boy?
180
(f / find-01
:arg0 (g / girl)
:arg1 (t / toy
:poss (a / amr-unknown)))
Whose toy did the girl find?
Yes-no questions, imperatives, and embedded wh-
clauses are treated separately with the AMR rela-
tion :mode.
Verbs. Nearly every English verb and verb-
particle construction we have encountered has a
corresponding PropBank frameset.
(l / look-05
:arg0 (b / boy)
:arg1 (a / answer))
The boy looked up the answer.
The boy looked the answer up.
AMR abstracts away from light-verb construc-
tions.
(a / adjust-01
:arg0 (g / girl)
:arg1 (m / machine))
The girl adjusted the machine.
The girl made adjustments to the machine.
Nouns.We use PropBank verb framesets to rep-
resent many nouns as well.
(d / destroy-01
:arg0 (b / boy)
:arg1 (r / room))
the destruction of the room by the boy ...
the boy?s destruction of the room ...
The boy destroyed the room.
We never say ?destruction-01? in AMR. Some
nominalizations refer to a whole event, while oth-
ers refer to a role player in an event.
(s / see-01
:arg0 (j / judge)
:arg1 (e / explode-01))
The judge saw the explosion.
(r / read-01
:arg0 (j / judge)
:arg1 (t / thing
:arg1-of (p / propose-01))
The judge read the proposal.
(t / thing
:arg1-of (o / opine-01
:arg0 (g / girl)))
the girl?s opinion
the opinion of the girl
what the girl opined
Many ?-er? nouns invoke PropBank framesets.
This enables us to make use of slots defined for
those framesets.
(p / person
:arg0-of (i / invest-01))
investor
(p / person
:arg0-of (i / invest-01
:arg1 (b / bond)))
bond investor
(p / person
:arg0-of (i / invest-01
:manner (s / small)))
small investor
(w / work-01
:arg0 (b / boy)
:manner (h / hard))
the boy is a hard worker
the boy works hard
However, a treasurer is not someone who trea-
sures, and a president is not (just) someone who
presides.
Adjectives. Various adjectives invoke Prop-
Bank framesets.
(s / spy
:arg0-of (a / attract-01))
the attractive spy
(s / spy
:arg0-of (a / attract-01
:arg1 (w / woman)))
the spy who is attractive to women
?-ed? adjectives frequently invoke verb framesets.
For example, ?acquainted with magic? maps to
?acquaint-01?. However, we are not restricted to
framesets that can be reached through morpholog-
ical simplification.
(f / fear-01
:arg0 (s / soldier)
:arg1 (b / battle-01))
The soldier was afraid of battle.
The soldier feared battle.
The soldier had a fear of battle.
For other adjectives, we have defined new frame-
sets.
(r / responsible-41
:arg1 (b / boy)
:arg2 (w / work))
The boy is responsible for the work.
The boy has responsibility for the work.
While ?the boy responsibles the work? is not good
English, it is perfectly good Chinese. Similarly,
we handle tough-constructions logically.
181
(t / tough
:domain (p / please-01
:arg1 (g / girl)))
Girls are tough to please.
It is tough to please girls.
Pleasing girls is tough.
?please-01? and ?girl? are adjacent in the AMR,
even if they are not adjacent in English. ?-able?
adjectives often invoke the AMR concept ?possi-
ble?, but not always (e.g., a ?taxable fund? is actu-
ally a ?taxed fund?).
(s / sandwich
:arg1-of (e / eat-01
:domain-of (p / possible)))
an edible sandwich
(f / fund
:arg1-of (t / tax-01))
a taxable fund
Pertainym adjectives are normalized to root form.
(b / bomb
:mod (a / atom))
atom bomb
atomic bomb
Prepositions. Most prepositions simply sig-
nal semantic frame elements, and are themselves
dropped from AMR.
(d / default-01
:arg1 (n / nation)
:time (d2 / date-entity
:month 6))
The nation defaulted in June.
Time and location prepositions are kept if they
carry additional information.
(d / default-01
:arg1 (n / nation)
:time (a / after
:op1 (w / war-01))
The nation defaulted after the war.
Occasionally, neither PropBank nor AMR has an
appropriate relation, in which case we hold our
nose and use a :prep-X relation.
(s / sue-01
:arg1 (m / man)
:prep-in (c / case))
The man was sued in the case.
Named entities. Any concept in AMR can be
modified with a :name relation. However, AMR
includes standardized forms for approximately 80
named-entity types, including person, country,
sports-facility, etc.
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown"))
Mollie Brown
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown")
:arg0-of (s / slay-01
:arg1 (o / orc)))
the orc-slaying Mollie Brown
Mollie Brown, who slew orcs
AMR does not normalize multiple ways of re-
ferring to the same concept (e.g., ?US? versus
?United States?). It also avoids analyzing seman-
tic relations inside a named entity?e.g., an orga-
nization named ?Stop Malaria Now? does not in-
voke the ?stop-01? frameset. AMR gives a clean,
uniform treatment to titles, appositives, and other
constructions.
(c / city
:name (n / name
:op1 "Zintan"))
Zintan
the city of Zintan
(p / president
:name (n / name
:op1 "Obama"))
President Obama
Obama, the president ...
(g / group
:name (n / name
:op1 "Elsevier"
:op2 "N.V.")
:mod (c / country
:name (n2 / name
:op1 "Netherlands"))
:arg0-of (p / publish-01))
Elsevier N.V., the Dutch publishing group...
Dutch publishing group Elsevier N.V. ...
Copula. Copulas use the :domain relation.
(w / white
:domain (m / marble))
The marble is white.
(l / lawyer
:domain (w / woman))
The woman is a lawyer.
(a / appropriate
:domain (c / comment)
:polarity -))
The comment is not appropriate.
182
The comment is inappropriate.
Reification. Sometimes we want to use an
AMR relation as a first-class concept?to be able
to modify it, for example. Every AMR relation has
a corresponding reification for this purpose.
(m / marble
:location (j / jar))
the marble in the jar ...
(b / be-located-at-91
:arg1 (m / marble)
:arg2 (j / jar)
:polarity -)
:time (y / yesterday))
The marble was not in the jar yesterday.
If we do not use the reification, we run into trou-
ble.
(m / marble
:location (j / jar
:polarity -)
:time (y / yesterday))
yesterday?s marble in the non-jar ...
Some reifications are standard PropBank frame-
sets (e.g., ?cause-01? for :cause, or ?age-01? for
:age).
This ends the summary of AMR content. For
lack of space, we omit descriptions of compara-
tives, superlatives, conjunction, possession, deter-
miners, date entities, numbers, approximate num-
bers, discourse connectives, and other phenomena
covered in the full AMR guidelines.
4 Limitations of AMR
AMR does not represent inflectional morphology
for tense and number, and it omits articles. This
speeds up the annotation process, and we do not
have a nice semantic target representation for these
phenomena. A lightweight syntactic-style repre-
sentation could be layered in, via an automatic
post-process.
AMR has no universal quantifier. Words like
?all? modify their head concepts. AMR does not
distinguish between real events and hypothetical,
future, or imagined ones. For example, in ?the boy
wants to go?, the instances of ?want-01? and ?go-
01? have the same status, even though the ?go-01?
may or may not happen.
We represent ?history teacher? nicely as ?(p /
person :arg0-of (t / teach-01 :arg1 (h / history)))?.
However, ?history professor? becomes ?(p / pro-
fessor :mod (h / history))?, because ?profess-01?
is not an appropriate frame. It would be reason-
able in such cases to use a NomBank (Meyers et
al., 2004) noun frame with appropriate slots.
5 Creating AMRs
We have developed a power editor for AMR, ac-
cessible by web interface.2 The AMR Editor al-
lows rapid, incremental AMR construction via text
commands and graphical buttons. It includes on-
line documentation of relations, quantities, reifi-
cations, etc., with full examples. Users log in,
and the editor records AMR activity. The ed-
itor also provides significant guidance aimed at
increasing annotator consistency. For example,
users are warned about incorrect relations, discon-
nected AMRs, words that have PropBank frames,
etc. Users can also search existing sembanks for
phrases to see how they were handled in the past.
The editor also allows side-by-side comparison of
AMRs from different users, for training purposes.
In order to assess inter-annotator agreement
(IAA), as well as automatic AMR parsing accu-
racy, we developed the smatch metric (Cai and
Knight, 2013) and associated script.3 Smatch re-
ports the semantic overlap between two AMRs by
viewing each AMR as a conjunction of logical
triples (see Figure 1). Smatch computes precision,
recall, and F-score of one AMR?s triples against
the other?s. To match up variables from two in-
put AMRs, smatch needs to execute a brief search,
looking for the variable mapping that yields the
highest F-score.
Smatch makes no reference to English strings
or word indices, as we do not enforce any par-
ticular string-to-meaning derivation. Instead, we
compare semantic representations directly, in the
same way that the MT metric Bleu (Papineni et
al., 2002) compares target strings without making
reference to the source.
For an initial IAA study, and prior to adjust-
ing the AMR Editor to encourage consistency, 4
expert AMR annotators annotated 100 newswire
sentences and 80 web text sentences. They then
created consensus AMRs through discussion. The
average annotator vs. consensus IAA (smatch) was
0.83 for newswire and 0.79 for web text. When
newly trained annotators doubly annotated 382
web text sentences, their annotator vs. annotator
IAA was 0.71.
2AMR Editor: amr.isi.edu/editor.html
3Smatch: amr.isi.edu/evaluation.html
183
6 Current AMR Bank
We currently have a manually-constructed AMR
bank of several thousand sentences, a subset of
which can be freely downloaded,4 the rest being
distributed via the LDC catalog.
In initially developing AMR, the authors built
consensus AMRs for:
? 225 short sentences for tutorial purposes
? 142 sentences of newswire (*)
? 100 sentences of web data (*)
Trained annotators at LDC then produced AMRs
for:
? 1546 sentences from the novel ?The Little
Prince?
? 1328 sentences of web data
? 1110 sentences of web data (*)
? 926 sentences from Xinhua news (*)
? 214 sentences from CCTV broadcast con-
versation (*)
Collections marked with a star (*) are also in
the OntoNotes corpus (Pradhan et al, 2007;
Weischedel et al, 2011).
Using the AMR Editor, annotators are able to
translate a full sentence into AMR in 7-10 minutes
and postedit an AMR in 1-3 minutes.
7 Related Work
Researchers working on whole-sentence semantic
parsing today typically use small, domain-specific
sembanks like GeoQuery (Wong and Mooney,
2006). The need for larger, broad-coverage sem-
banks has sparked several projects, including the
Groningen Meaning Bank (GMB) (Basile et al,
2012a), UCCA (Abend and Rappoport, 2013),
the Semantic Treebank (ST) (Butler and Yoshi-
moto, 2012), the Prague Dependency Treebank
(Bo?hmova? et al, 2003), and UNL (Uchida et al,
1999; Uchida et al, 1996; Martins, 2012).
Concepts. Most systems use English words
as concepts. AMR uses PropBank frames (e.g.,
?describe-01?), and UNL uses English WordNet
synsets (e.g., ?200752493?).
Relations. GMB uses VerbNet roles (Schuler,
2005), and AMR uses frame-specific PropBank
relations. UNL has a dedicated set of over 30 fre-
quently used relations.
Formalism. GMB meanings are written in
DRT (Kamp et al, 2011), exploiting full first-
4amr.isi.edu/download.html
order logic. GMB and ST both include universal
quantification.
Granularity. GMB and UCCA annotate short
texts, so that the same entity can participate in
events described in different sentences; other sys-
tems annotate individual sentences.
Entities. AMR uses 80 entity types, while
GMB uses 7.
Manual versus automatic. AMR, UNL, and
UCCA annotation is fully manual. GMB and ST
produce meaning representations automatically,
and these can be corrected by experts or crowds
(Venhuizen et al, 2013).
Derivations. AMR and UNL remain agnostic
about the relation between strings and their mean-
ings, considering this a topic of open research.
ST and GMB annotate words and phrases directly,
recording derivations as (for example) Montague-
style compositional semantic rules operating on
CCG parses.
Top-down verus bottom-up. AMR annota-
tors find it fast to construct meanings from the
top down, starting with the main idea of the sen-
tence (though the AMR Editor allows bottom-up
construction). GMB and UCCA annotators work
bottom-up.
Editors, guidelines, genres. These projects
have graphical sembanking tools (e.g., Basile et al
(2012b)), annotation guidelines,5 and sembanks
that cover a wide range of genres, from news to
fiction. UNL and AMR have both annotated many
of the same sentences, providing the potential for
direct comparison.
8 Future Work
Sembanking. Our main goal is to continue
sembanking. We would like to employ a large
sembank to create shared tasks for natural lan-
guage understanding and generation. These
tasks may additionally drive interest in theoreti-
cal frameworks for probabilistically mapping be-
tween graphs and strings (Quernheim and Knight,
2012b; Quernheim and Knight, 2012a; Chiang et
al., 2013).
Applications. Just as syntactic parsing has
found many unanticipated applications, we expect
sembanks and statistical semantic processors to be
used for many purposes. To get started, we are
exploring the use of statistical NLU and NLG in
5UNL guidelines: www.undl.org/unlsys/unl/unl2005
184
a semantics-based machine translation (MT) sys-
tem. In this system, we annotate bilingual Chi-
nese/English data with AMR, then train compo-
nents to map Chinese to AMR, and AMR to En-
glish. A prototype is described by Jones et al
(2012).
Disjunctive AMR. AMR aims to canonicalize
multiple ways of saying the same thing. We plan
to test how well we are doing by building AMRs
on top of large, manually-constructed paraphrase
networks from the HyTER project (Dreyer and
Marcu, 2012). Rather than build individual AMRs
for different paths through a network, we will con-
struct highly-packed disjunctive AMRs. With this
application in mind, we have developed a guide-
line6 for disjunctive AMR. Here is an example:
(o / *OR*
:op1 (t / talk-01)
:op2 (m / meet-03)
:OR (o2 / *OR*
:mod (o3 / official)
:arg1-of (s / sanction-01
:arg0 (s2 / state))))
official talks
state-sanctioned talks
meetings sanctioned by the state
AMR extensions. Finally, we would like
to deepen the AMR language to include more
relations (to replace :mod and :prep-X, for
example), entity normalization (perhaps wik-
ification), quantification, and temporal rela-
tions. Ultimately, we would like to also in-
clude a comprehensive set of more abstract
frames like ?Earthquake-01? (:magnitude, :epi-
center, :casualties), ?CriminalLawsuit-01? (:de-
fendant, :crime, :jurisdiction), and ?Pregnancy-
01? (:father, :mother, :due-date). Projects like
FrameNet (Baker et al, 1998) and CYC (Lenat,
1995) have long pursued such a set.
References
O. Abend and A. Rappoport. 2013. UCCA: A
semantics-based grammatical annotation scheme. In
Proc. IWCS.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berke-
ley FrameNet project. In Proc. COLING.
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012a.
Developing a large semantically annotated corpus.
In Proc. LREC.
6Disjunctive AMR guideline: amr.isi.edu/damr.1.0.pdf
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012b.
A platform for collaborative semantic annotation. In
Proc. EACL demonstrations.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The Prague dependency treebank. In Tree-
banks. Springer.
A. Butler and K. Yoshimoto. 2012. Banking meaning
representations from treebanks. Linguistic Issues in
Language Technology, 7.
S. Cai and K. Knight. 2013. Smatch: An accu-
racy metric for abstract meaning representations. In
Proc. ACL.
D. Chiang, J. Andreas, D. Bauer, K. M. Hermann,
B. Jones, and K. Knight. 2013. Parsing graphs with
hyperedge replacement grammars. In Proc. ACL.
D. Davidson. 1969. The individuation of events.
In N. Rescher, editor, Essays in Honor of Carl G.
Hempel. D. Reidel, Dordrecht.
M. Dreyer and D. Marcu. 2012. Hyter: Meaning-
equivalent semantics for translation evaluation. In
Proc. NAACL.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-based machine trans-
lation with hyperedge replacement grammars. In
Proc. COLING.
H. Kamp, J. Van Genabith, and U. Reyle. 2011. Dis-
course representation theory. In Handbook of philo-
sophical logic, pages 125?394. Springer.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. LREC.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11).
R. Martins. 2012. Le Petit Prince in UNL. In Proc.
LREC.
C. M. I. M. Matthiessen and J. A. Bateman. 1991.
Text Generation and Systemic-Functional Linguis-
tics. Pinter, London.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 workshop: Frontiers in corpus anno-
tation.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, Philadelphia, PA.
185
S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In-
ternational Journal of Semantic Computing (IJSC),
1(4).
D. Quernheim and K. Knight. 2012a. DAGGER: A
toolkit for automata on directed acyclic graphs. In
Proc. FSMNLP.
D. Quernheim and K. Knight. 2012b. Towards prob-
abilistic acceptors and transducers for feature struc-
tures. In Proc. SSST Workshop.
K. Schuler. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
S. Shieber, F. C. N. Pereira, L. Karttunen, and M. Kay.
1986. Compilation of papers on unification-based
grammar formalisms. Technical Report CSLI-86-
48, Center for the Study of Language and Informa-
tion, Stanford, California.
H. Uchida, M. Zhu, and T. Della Senta. 1996. UNL:
Universal Networking Language?an electronic lan-
guage for communication, understanding and col-
laboration. Technical report, IAS/UNU Tokyo.
H. Uchida, M. Zhu, and T. Della Senta. 1999. A
gift for a millennium. Technical report, IAS/UNU
Tokyo.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. In Proc.
IWCS.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R. Belvin, S. Pradhan, L. Ramshaw, and N. Xue.
2011. OntoNotes: A large training corpus for en-
hanced processing. In J. Olive, C. Christianson, and
J. McCary, editors, Handbook of Natural Language
Processing and Machine Translation. Springer.
Y. W. Wong and R. J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation.
In Proc. HLT-NAACL.
186
