Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 549?555,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Reranking with Linguistic and Semantic Features
for Arabic Optical Character Recognition
Nadi Tomeh, Nizar Habash, Ryan Roth, Noura Farra
Center for Computational Learning Systems, Columbia University
{nadi,habash,ryanr,noura}@ccls.columbia.edu
Pradeep Dasigi Mona Diab
Safaba Translation Solutions The George Washington University
pradeep@safaba.com mtdiab@gwu.edu
Abstract
Optical Character Recognition (OCR) sys-
tems for Arabic rely on information con-
tained in the scanned images to recognize
sequences of characters and on language
models to emphasize fluency. In this paper
we incorporate linguistically and seman-
tically motivated features to an existing
OCR system. To do so we follow an n-best
list reranking approach that exploits recent
advances in learning to rank techniques.
We achieve 10.1% and 11.4% reduction in
recognition word error rate (WER) relative
to a standard baseline system on typewrit-
ten and handwritten Arabic respectively.
1 Introduction
Optical Character Recognition (OCR) is the task
of converting scanned images of handwritten,
typewritten or printed text into machine-encoded
text. Arabic OCR is a challenging problem due
to Arabic?s connected letter forms, consonantal
diacritics and rich morphology (Habash, 2010).
Therefore only a few OCR systems have been de-
veloped (Ma?rgner and Abed, 2009). The BBN
Byblos OCR system (Natajan et al, 2002; Prasad
et al, 2008; Saleem et al, 2009), which we use
in this paper, relies on a hidden Markov model
(HMM) to recover the sequence of characters from
the image, and uses an n-gram language model
(LM) to emphasize the fluency of the output. For
an input image, the OCR decoder generates an n-
best list of hypotheses each of which is associated
with HMM and LM scores.
In addition to fluency as evaluated by LMs,
other information potentially helps in discrimi-
nating good from bad hypotheses. For example,
Habash and Roth (2011) use a variety of linguistic
(morphological and syntactic) and non-linguistic
features to automatically identify errors in OCR
hypotheses. Another example presented by De-
vlin et al (2012) shows that using a statistical ma-
chine translation system to assess the difficulty of
translating an Arabic OCR hypothesis into English
gives valuable feedback on OCR quality. There-
fore, combining additional information with the
LMs could reduce recognition errors. However,
direct integration of such information in the de-
coder is difficult.
A straightforward alternative which we advo-
cate in this paper is to use the available informa-
tion to rerank the hypotheses in the n-best lists.
The new top ranked hypothesis is considered as
the new output of the system. We propose com-
bining LMs with linguistically and semantically
motivated features using learning to rank meth-
ods. Discriminative reranking allows each hypoth-
esis to be represented as an arbitrary set of features
without the need to explicitly model their interac-
tions. Therefore, the system benefits from global
and potentially complex features which are not
available to the baseline OCR decoder. This ap-
proach has successfully been applied in numerous
Natural Language Processing (NLP) tasks includ-
ing syntactic parsing (Collins and Koo, 2005), se-
mantic parsing (Ge and Mooney, 2006), machine
translation (Shen et al, 2004), spoken language
understanding (Dinarelli et al, 2012), etc. Fur-
thermore, we propose to combine several ranking
methods into an ensemble which learns from their
predictions to further reduce recognition errors.
We describe our features and reranking ap-
proach in ?2, and we present our experiments and
results in ?3.
2 Discriminative Reranking for OCR
Each hypothesis in an n-best list {hi}ni=1 is repre-
sented by a d-dimensional feature vector xi ? Rd.
Each xi is associated with a loss li to generate a
labeled n-best list H = {(xi, li)}ni=1. The loss is
computed as the Word Error Rate (WER) of the
549
hypotheses compared to a reference transcription.
For supervised training we use a set of n-best lists
H = {H(k)}Mk=1.
2.1 Learning to rank approaches
Major approaches to learning to rank can be di-
vided into pointwise score regression, pairwise
preference satisfaction, and listwise structured
learning. See Liu (2009) for a survey. In this
paper, we explore all of the following learning to
rank approaches.
Pointwise In the pointwise approach, the rank-
ing problem is formulated as a regression, or ordi-
nal classification, for which any existing method
can be applied. Each hypothesis constitutes a
learning instance. In this category we use a regres-
sion method called Multiple Additive Regression
Trees (MART) (Friedman, 2000) as implemented
in RankLib.1 The major problem with pointwise
approaches is that the structure of the list of hy-
potheses is ignored.
Pairwise The pairwise approach takes pairs of
hypotheses as instances in learning, and formal-
izes the ranking problem as a pairwise classifica-
tion or pairwise regression. We use several meth-
ods from this category.
RankSVM (Joachims, 2002) is a method based
on Support Vector Machines (SVMs) for which
we use only linear kernels to keep complexity low.
Exact optimization of the RankSVM objective can
be computationally expensive as the number of
hypothesis pairs can be very large. Approximate
stochastic training strategies reduces complexity
and produce comparable performance. There-
fore, in addition to RankSVM, we use stochas-
tic sub-gradient descent (SGDSVM), Pegasos (Pe-
gasosSVM) and Passive-Aggressive Perceptron
(PAPSVM) as implemented in Sculley (2009).2
RankBoost (Freund et al, 2003) is a pairwise
boosting approach implemented in RankLib. It
uses a linear combination of weak rankers, each of
which is a binary function associated with a single
feature. This function is 1 when the feature value
exceeds some threshold and 0 otherwise.
RankMIRA is a ranking method presented in (Le
Roux et al, 2012).3 It uses a weighted linear
combination of features which assigns the highest
1http://people.cs.umass.edu/?vdang/
ranklib.html
2http://code.google.com/p/sofia-ml
3https://github.com/jihelhere/
adMIRAble
score to the hypotheses with the lowest loss. Dur-
ing training, the weights are updated according to
the Margin-Infused Relaxed Algorithm (MIRA),
whenever the highest scoring hypothesis differs
from the hypothesis with the lowest error rate.
In pairwise approaches, the group structure of
the n-best list is still ignored. Additionally, the
number of training pairs generated from an n-best
list depends on its size, which could result in train-
ing a model biased toward larger hypothesis lists
(Cao et al, 2006).
Listwise The listwise approach takes n-best lists
as instances in both learning and prediction. The
group structure is considered explicitly and rank-
ing evaluation measures can be directly optimized.
The listwise methods we use are implemented in
RankLib.
AdaRank (Xu and Li, 2007) is a boosting ap-
proach, similar to RankBoost, except that it opti-
mizes an arbitrary ranking metric, for which we
use Mean Average Precision (MAP).
Coordinate Ascent (CA) uses a listwise linear
model whose weights are learned by a coordinate
ascent method to optimize a ranking metric (Met-
zler and Bruce Croft, 2007). As with AdaRank we
use MAP.
ListNet (Cao et al, 2007) uses a neural network
model whose parameters are learned by gradient
descent method to optimize a listwise loss based
on a probabilistic model of permutations.
2.2 Ensemble reranking
In addition to the above mentioned approaches,
we couple simple feature selection and reranking
models combination via a straightforward ensem-
ble learning method similar to stacked general-
ization (Wolpert, 1992) and Combiner (Chan and
Stolfo, 1993). Our goal is to generate an overall
meta-ranker that outperforms all base-rankers by
learning from their predictions how they correlate
with each other.
To obtain the base-rankers, we train each of the
ranking models of ?2.1 using all the features of
?2.3 and also using each feature family added to
the baseline features separately. Then, we use the
best model for each ranking approach to make pre-
dictions on a held-out data set of n-best lists. We
can think of each base-ranker as computing one
feature for each hypothesis. Hence, the scores
generated by all the rankers for a given hypothe-
sis constitute its feature vector.
The held-out n-best lists and the predictions of
550
the base-rankers represent the training data for the
meta-ranker. We choose RankSVM4 as the meta-
ranker since it performed well as a base-ranker.
2.3 Features
Our features fall into five families.
Base features include the HMM and LM scores
produced by the OCR system. These features are
used by the baseline system5 as well as by the var-
ious reranking methods.
Simple features (?simple?) include the baseline
rank of the hypothesis and a 0-to-1 range normal-
ized version of it. We also use a hypothesis confi-
dence feature which corresponds to the average of
the confidence of individual words in the hypoth-
esis; ?confidence? for a given word is computed
as the fraction of hypotheses in the n-best list
that contain the word (Habash and Roth, 2011).
The more consensus words a hypothesis contains,
the higher its assigned confidence. We also use
the average word length and the number of con-
tent words (normalized by the hypothesis length).
We define ?content words? as non-punctuation and
non-digit words. Additionally, we use a set of bi-
nary features indicating if the hypothesis contains
a sequence of duplicated characters, a date-like se-
quence and an occurrence of a specific character
class (punctuation, alphabetic and digit).
Word LM features (?LM-word?) include the
log probabilities of the hypothesis obtained us-
ing n-gram LMs with n ? {1, . . . , 5}. Separate
LMs are trained on the Arabic Gigaword 3 corpus
(Graff, 2007), and on the reference transcriptions
of the training data (see ?3.1). The LM models
are built using the SRI Language Modeling Toolkit
(Stolcke, 2002).
Linguistic LM features (?LM-MADA?) are
similar to the word LM features except that they
are computed using the part-of-speech and the
lemma of the words instead of the actual words.6
Semantic coherence feature (?SemCoh?) is
motivated by the fact that semantic information
can be very useful in modeling the fluency of
phrases, and can augment the information pro-
vided by n-gram LMs. In modeling contextual
4RankSVM has also been shown to be a good choice for
the meta-learner in general stacking ensemble learning (Tang
et al, 2010).
5The baseline ranking is simply based on the sum of the
logs of the HMM and LM scores.
6The part-of-speech and the lemmas are obtained using
MADA 3.0, a tool for Arabic morphological analysis and
disambiguation (Habash and Rambow, 2005; Habash et al,
2009).
lexical semantic information, simple bag-of-words
models usually have a lot of noise; while more
sophisticated models considering positional infor-
mation have sparsity issues. To strike a balance
between these two extremes, we introduce a novel
model of semantic coherence that is based on a
measure of semantic relatedness between pairs of
words. We model semantic relatedness between
two words using the Information Content (IC) of
the pair in a method similar to the one used by Lin
(1997) and Lin (1998).
IC(w1,d, w2) = log
f(w1, d, w2)f(?,d, ?)
f(w1, d, ?)f(?,d, w2)
Here, d can generally represent some form of re-
lation between w1 and w2. Whereas Lin (1997)
and Lin (1998) used dependency relation between
words, we use distance. Given a sentence, the dis-
tance between w1 and w2 is one plus the number
of words that are seen after w1 and before w2 in
that sentence. Hence, f(w1, d, w2) is the number
of times w1 occurs before w2 at a distance d in
all the sentences in a corpus. ? is a placeholder
for any word, i.e., f(?, d, ?) is the frequency of all
word pairs occurring at distance d. The distances
are directional and not absolute values. A simi-
lar measure of relatedness was also used by Kolb
(2009).
We estimate the frequencies from the Arabic
Gigaword. We set the window size to 3 and cal-
culate IC values of all pairs of words occurring at
distance within the window size. Since the dis-
tances are directional, it has to be noted that given
a word, its relations with three words before it and
three words after it are modeled. During testing,
for each phrase in our test set, we measure se-
mantic relatedness of pairs of words using the IC
values estimated from the Arabic Gigaword, and
normalize their sum by the number of pairs in the
phrase to obtain a measure of Semantic Coherence
(SC) of the phrase. That is,
SC(p) = 1m ?
?
1?d?W
1?i+d<n
IC(wi,d, wi+d)
where p is the phrase being evaluated, n is the
number of words in it, d is the distance between
words, W is the window size (set to 3), and m is
the number of all possible wi, wi+d pairs in the
phrase given these conditions.
551
print hand
|H?| n |h| |H?| n |h|
Hb 1,560 62 9 2,295 225 8
Hm 1,000 76 9 1,000 225 9
Ht 1,000 64 9 1,000 227 9
Table 1: Data sets statistics. |H?| refers to the
number of n-best lists, n is the average size of the
lists, and |h| is the average length of a hypothesis.
print hand
Baseline 13.8% 35%
Oracle 9.8% 20.9%
Best result 12.4% 30.9%
Table 2: WER for baseline, oracle and best
reranked hypotheses.
3 Experiments
3.1 Data and baselines
We used two data sets derived from high-
resolution image scans of typewritten and hand-
written Arabic text along with ground truth tran-
scriptions.7 The BBN Byblos system was then
used to process these scanned images into se-
quences of segments (sentence fragments) and
generate a ranked n-best list of hypotheses for
each segment (Natajan et al, 2002; Prasad et al,
2008; Saleem et al, 2009). We divided each of the
typewritten data set (?print?) and handwritten data
set (?hand?) into three disjoint parts: a training set
for the base-rankersHb, a training set for the meta-
ranker Hm and a test set Ht. Table 1 presents
some statistics about these data sets. Our base-
line is based on the sum of the logs of the HMM
and LM scores. Table 2 presents the WER for our
baseline hypothesis, the best hypothesis in the list
(our oracle) and our best reranking results which
we describe in details in ?3.2.
For LM training we used 220M words from
Arabic Gigaword 3, and 2.4M words from each
?print? and ?hand? ground truth annotations.
Effect of n-best training size onWER The size
of the training n-best lists is crucial to the learning
of the ranking model. In particular, it determines
the number of training instances per list. To deter-
mine the optimal n to use for the rest of this pa-
per, we conducted the following experiment aims
to understand the effect of the size of n-best lists
7The Anfal data set discussed here was collected by the
Linguistic Data Consortium.
30.5 
31 
31.5 
32 
32.5 
33 
33.5 
34 
12 
12.5 
13 
13.5 
14 
14.5 
15 
5 15 25 35 45 55 
Size of each training n-best list 
WER print 
hand 
Figure 1: Effect of the size of training n-best lists
on WER. The horizontal axis represents the max-
imum size of the n-best lists and the vertical axis
represents WER, left is ?print? and right is ?hand?.
on the reranking performance for one of our best
reranking models, namely RankSVM. We trained
each model with different sizes of n-best, varying
from n = 5 to n = 60 for ?print? data, and be-
tween n = 5 and n = 150 for ?hand? data. The
top n hypotheses according to the baseline are se-
lected for each n. Figure 1 plots WER as a func-
tion of the size of the training list n for both ?print?
and ?hand? data.
The lowest WER scores are achieved for n =
10 and n = 15 for both ?print? and ?hand? data.
We note that a small number of hypotheses per list
is sufficient for RankSVM to obtain a good per-
formance, but also increasing n further seems to
increase the error rate. For the rest of this paper
we use the top 10-best hypotheses per segment.
3.2 Reranking results
The reranking results for ?print? and ?hand? are
presented in Table 3. The results are presented
as the difference in WER from the baseline WER.
See the caption in Table 3 for more information.
For ?print?, the pairwise approaches clearly out-
perform the listwise approaches and achieve the
lowest WER of 12.4% (10.1% WER reduction rel-
ative to the baseline) with 7 different combinations
of rankers and feature families. While both ap-
proaches do not minimize WER directly, the pair-
wise methods have the advantage of using objec-
tives that are simpler to optimize, and they are
trained on much larger number of examples which
may explain their superiority. RankBoost, how-
ever, is less competitive with a performance closer
to that of listwise approaches. All the methods
improved over the baseline with any feature fam-
ily, except for the pointwise approach which did
552
Pointwise Listwise Pairwise
Features MA
RT
Ad
aR
ank
Lis
tNe
t
CA Ra
nkB
oos
t
Ra
nkS
VM
SG
DS
VM
Ra
nkM
IRA
Peg
a.S
VM
PA
PS
VM
Pr
int
Base 1.1 -0.4 -1.0 -1.0 -1.0 -1.1 -1.2 -1.2 -1.3 -1.3
+simple -0.1 0.0 -0.1 -0.2 0.0 -0.1 0.1 0.0 0.1 0.0
+LM-word -1.0 -0.2 0.1 -0.1 -0.1 -0.3 -0.2 -0.1 0.0 -0.1
+LM-MADA 0.0 -0.3 0.1 -0.2 -0.1 0.0 -0.1 -0.2 -0.1 -0.1
+SemCoh 0.0 -0.4 0.0 -0.2 -0.1 -0.1 0.0 -0.1 0.0 0.1
+All 0.6 0.1 0.0 0.1 0.0 0.1 0.2 0.2 0.2 0.0
Ha
nd
Base 4.2 -3.1 -3.2 -3.4 -2.9 -3.2 -3.5 -3.8 -3.6 -3.8
+simple 0.3 -0.1 0.1 0.2 0.1 -0.1 0.2 -0.2 0.1 0.2
+LM-word 0.4 -0.1 0.1 0.8 -0.2 -0.7 -0.2 -0.1 0.0 0.1
+LM-MADA 0.0 -0.5 0.1 0.0 0.1 -0.4 -0.1 0.3 -0.2 0.1
+SemCoh 0.0 -0.1 0.0 -0.4 0.0 -0.2 -0.3 -0.2 -0.2 0.0
+All 0.2 0.4 0.0 0.4 0.2 0.4 0.2 0.1 0.2 0.0
Table 3: Reranking results for the ?print? and ?hand? data sets; the ?print? baseline WER is 13.9% and the ?hand? baseline
WER is 35.0%. The ?Base? numbers represent the difference in WER between the corresponding ranker using ?Base? features
only and the baseline, which uses the same ?Base? features. The ?+features? numbers represent additional gain (relative to
?Base?) obtained by adding the corresponding feature family. The ?+All? numbers represent the gain of using all features,
relative to the best single-family system. The actual WER of a ranker can be obtained by summing the baseline WER and the
corresponding ?Base? and ?+features? scores. Bolded values are the best performers overall.
worse than the baseline. When combined with
the ?Base? features, ?LM-words? lead to improve-
ments with 8 out of 10 rankers, and proved to be
the most helpful among feature families. ?LM-
MADA? follows with improvements with 7 out of
10 rankers. The lowest WER is achieved using
one of these two LM-based families. Combining
all feature families did not help and in many cases
resulted in a higher WER than the best family.
Similar improvements are observed for ?hand?.
The lowest achieved WER is 31% (11.4% WER
reduction relative to the baseline). Here also,
the pointwise method increased the WER by 12%
relative to the baseline (as opposed to 7% for
?print?). Again, the listwise approaches are over-
all less effective than their pairwise counterparts,
except for RankBoost which resulted in the small-
est WER reduction among all rankers. The two
best rankers correspond to RankMIRA with the
?simple? and the ?SemCoh? features. The ?Sem-
Coh? feature resulted in improvements for 6 out of
the 10 rankers, and thus was the best single feature
on average for the ?hand? data set. As observed
with ?print? data, combining all the features does
not lead to the best performance.
In an additional experiment, we selected the
best model for each ranking method and combined
them to build an ensemble as described in ?2.2.
For ?hand?, the ensemble slightly outperformed
all the individual rankers and achieved the lowest
WER of 30.9%. However, for the ?print? data, the
ensemble failed to improve over the base-rankers
and resulted in a WER of 12.4%.
The best overall results are presented in Table 2.
Our best results reduce the distance to the oracle
top line by 35% for ?print? and 29% for ?hand?.
4 Conclusion
We presented a set of experiments on incorporat-
ing features into an existing OCR system via n-
best list reranking. We compared several learn-
ing to rank techniques and combined them us-
ing an ensemble technique. We obtained 10.1%
and 11.4% reduction in WER relative to the base-
line for ?print? and ?hand? data respectively. Our
best systems used pairwise reranking which out-
performed the other methods, and used the MADA
based features for ?print? and our novel semantic
coherence feature for ?hand?.
Acknowledgment
We would like to thank Rohit Prasad and Matin
Kamali for providing the data and helpful dis-
cussions. This work was funded under DARPA
project number HR0011-08-C-0004. Any opin-
ions, findings and conclusions or recommenda-
tions expressed in this paper are those of the au-
thors and do not necessarily reflect the views of
DARPA. The last two authors, Dasigi and Diab,
worked on this project while at Columbia Univer-
sity.
553
References
Yunbo Cao, Jun Xu, Tie-Yan Liu, Hang Li, Yalou
Huang, and Hsiao-Wuen Hon. 2006. Adapting
ranking SVM to document retrieval. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?06, pages 186?193.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24th international conference on Machine learning,
ICML ?07, pages 129?136.
Philip K. Chan and Salvatore J. Stolfo. 1993. Exper-
iments on multistrategy learning by meta-learning.
In Proceedings of the second international confer-
ence on Information and knowledge management,
CIKM ?93, pages 314?323.
Michael Collins and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Comput.
Linguist., 31(1):25?70, March.
Jacob Devlin, Matin Kamali, Krishna Subramanian,
Rohit Prasad, and Prem Natarajan. 2012. Statisti-
cal Machine Translation as a Language Model for
Handwriting Recognition. In ICFHR, pages 291?
296.
Marco Dinarelli, Alessandro Moschitti, and Giuseppe
Riccardi. 2012. Discriminative Reranking for
Spoken Language Understanding. IEEE Transac-
tions on Audio, Speech & Language Processing,
20(2):526?539.
Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm
for combining preferences. J. Mach. Learn. Res.,
4:933?969, December.
Jerome H. Friedman. 2000. Greedy Function Approx-
imation: A Gradient Boosting Machine. Annals of
Statistics, 29:1189?1232.
Ruifang Ge and Raymond J. Mooney. 2006. Discrimi-
native Reranking for Semantic Parsing. In ACL.
David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan, June.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
Arabic handwriting recognition. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Thorsten Joachims. 2002. Optimizing search en-
gines using clickthrough data. In Proceedings of
the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ?02,
pages 133?142.
Peter Kolb. 2009. Experiments on the difference be-
tween semantic similarity and relatedness. In Pro-
ceedings of the 17th Nordic Conference of Computa-
tional Linguistics, NEALT Proceedings Series Vol.
4.
Joseph Le Roux, Benoit Favre, Alexis Nasr, and
Seyed Abolghasem Mirroshandel. 2012. Gener-
ative Constituent Parsing and Discriminative De-
pendency Reranking: Experiments on English and
French. In SP-SEM-MRL 2012.
Dekang Lin. 1997. Using syntactic dependency as
local context to resolve word sense ambiguity. In
Proceedings of the eighth conference on European
chapter of the Association for Computational Lin-
guistics, EACL ?97, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics and 17th International Conference on Com-
putational Linguistics - Volume 2, ACL ?98.
Tie-Yan Liu. 2009. Learning to Rank for Informa-
tion Retrieval. Now Publishers Inc., Hanover, MA,
USA.
Volker Ma?rgner and Haikal El Abed. 2009. Ara-
bic Word and Text Recognition - Current Develop-
ments. In Khalid Choukri and Bente Maegaard, ed-
itors, Proceedings of the Second International Con-
ference on Arabic Language Resources and Tools,
Cairo, Egypt, April. The MEDAR Consortium.
Donald Metzler and W. Bruce Croft. 2007. Linear
feature-based models for information retrieval. Inf.
Retr., 10(3):257?274, June.
Premkumar Natajan, Zhidong Lu, Richard Schwartz,
Issam Bazzi, and John Makhoul. 2002. Hid-
den Markov models. chapter Multilingual machine
printed OCR, pages 43?63. World Scientific Pub-
lishing Co., Inc., River Edge, NJ, USA.
554
Rohit Prasad, Shirin Saleem, Matin Kamali, Ralf Meer-
meier, and Premkumar Natarajan. 2008. Improve-
ments in hidden Markov model based Arabic OCR.
In Proceedings of International Conference on Pat-
tern Recognition (ICPR), pages 1?4.
Shirin Saleem, Huaigu Cao, Krishna Subramanian,
Matin Kamali, Rohit Prasad, and Prem Natarajan.
2009. Improvements in BBN?s HMM-Based Of-
fline Arabic Handwriting Recognition System. In
Proceedings of the 2009 10th International Confer-
ence on Document Analysis and Recognition, IC-
DAR ?09, pages 773?777.
D. Sculley. 2009. Large scale learning to rank. In
NIPS 2009 Workshop on Advances in Ranking.
Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004. Discriminative Reranking for Machine Trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 177?184, Boston, Massachusetts, USA,
May.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Buzhou Tang, Qingcai Chen, Xuan Wang, and Xiao-
long Wang. 2010. Reranking for stacking ensem-
ble learning. In Proceedings of the 17th interna-
tional conference on Neural information processing:
theory and algorithms - Volume Part I, ICONIP?10,
pages 575?584.
David H. Wolpert. 1992. Original Contribution:
Stacked generalization. Neural Netw., 5(2):241?
259, February.
Jun Xu and Hang Li. 2007. AdaRank: a boosting al-
gorithm for information retrieval. In Proceedings of
the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?07, pages 391?398.
555
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161?167,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Generalized Character-Level Spelling Error Correction
Noura Farra, Nadi Tomeh?, Alla Rozovskaya, Nizar Habash
Center for Computational Learning Systems, Columbia University
{noura,alla,habash}@ccls.columbia.edu
?LIPN, Universit? Paris 13, Sorbonne Paris Cit?
nadi.tomeh@lipn.univ-paris13.fr
Abstract
We present a generalized discrimina-
tive model for spelling error correction
which targets character-level transforma-
tions. While operating at the charac-
ter level, the model makes use of word-
level and contextual information. In con-
trast to previous work, the proposed ap-
proach learns to correct a variety of er-
ror types without guidance of manually-
selected constraints or language-specific
features. We apply the model to cor-
rect errors in Egyptian Arabic dialect text,
achieving 65% reduction in word error
rate over the input baseline, and improv-
ing over the earlier state-of-the-art system.
1 Introduction
Spelling error correction is a longstanding Natural
Language Processing (NLP) problem, and it has
recently become especially relevant because of the
many potential applications to the large amount
of informal and unedited text generated online,
including web forums, tweets, blogs, and email.
Misspellings in such text can lead to increased
sparsity and errors, posing a challenge for many
NLP applications such as text summarization, sen-
timent analysis and machine translation.
In this work, we present GSEC, a Generalized
character-level Spelling Error Correction model,
which uses supervised learning to map input char-
acters into output characters in context. The ap-
proach has the following characteristics:
Character-level Corrections are learned at the
character-level
1
using a supervised sequence la-
beling approach.
Generalized The input space consists of all
characters, and a single classifier is used to learn
1
We use the term ?character? strictly in the alphabetic
sense, not the logographic sense (as in the Chinese script).
common error patterns over all the training data,
without guidance of specific rules.
Context-sensitive The model looks beyond the
context of the current word, when making a deci-
sion at the character-level.
Discriminative The model provides the free-
dom of adding a number of different features,
which may or may not be language-specific.
Language-Independent In this work, we in-
tegrate only language-independent features, and
therefore do not consider morphological or lin-
guistic features. However, we apply the model
to correct errors in Egyptian Arabic dialect text,
following a conventional orthography standard,
CODA (Habash et al, 2012).
Using the described approach, we demonstrate
a word-error-rate (WER) reduction of 65% over a
do-nothing input baseline, and we improve over
a state-of-the-art system (Eskander et al, 2013)
which relies heavily on language-specific and
manually-selected constraints. We present a de-
tailed analysis of mistakes and demonstrate that
the proposed model indeed learns to correct a
wider variety of errors.
2 Related Work
Most earlier work on automatic error correction
addressed spelling errors in English and built mod-
els of correct usage on native English data (Ku-
kich, 1992; Golding and Roth, 1999; Carlson
and Fette, 2007; Banko and Brill, 2001). Ara-
bic spelling correction has also received consider-
able interest (Ben Othmane Zribi and Ben Ahmed,
2003; Haddad and Yaseen, 2007; Hassan et al,
2008; Shaalan et al, 2010; Alkanhal et al, 2012;
Eskander et al, 2013; Zaghouani et al, 2014).
Supervised spelling correction approaches
trained on paired examples of errors and their cor-
rections have recently been applied for non-native
English correction (van Delden et al, 2004; Li et
al., 2012; Gamon, 2010; Dahlmeier and Ng, 2012;
161
Rozovskaya and Roth, 2011). Discriminative
models have been proposed at the word-level for
error correction (Duan et al, 2012) and for error
detection (Habash and Roth, 2011).
In addition, there has been growing work on lex-
ical normalization of social media data, a some-
what related problem to that considered in this pa-
per (Han and Baldwin, 2011; Han et al, 2013;
Subramaniam et al, 2009; Ling et al, 2013).
The work of Eskander et al (2013) is the
most relevant to the present study: it presents
a character-edit classification model (CEC) using
the same dataset we use in this paper.
2
Eskan-
der et al (2013) analyzed the data to identify the
seven most common types of errors. They devel-
oped seven classifiers and applied them to the data
in succession. This makes the approach tailored to
the specific data set in use and limited to a specific
set of errors. In this work, a single model is con-
sidered for all types of errors. The model consid-
ers every character in the input text for a possible
spelling error, as opposed to looking only at cer-
tain input characters and contexts in which they
appear. Moreover, in contrast to Eskander et al
(2013), it looks beyond the boundary of the cur-
rent word.
3 The GSEC Approach
3.1 Modeling Spelling Correction at the
Character Level
We recast the problem of spelling correction into
a sequence labeling problem, where for each input
character, we predict an action label describing
how to transform it to obtain the correct charac-
ter. The proposed model therefore transforms a
given input sentence e = e
1
, . . . , e
n
of n char-
acters that possibly include errors, to a corrected
sentence c of m characters, where corrected char-
acters are produced by one of the following four
actions applied to each input character e
i
:
? ok: e
i
is passed without transformation.
? substitute ? with(c): e
i
is substituted with
a character c where c could be any character
encountered in the training data.
? delete: e
i
is deleted.
? insert(c): A character c is inserted before
e
i
. To address errors occurring at the end
2
Eskander et al (2013) also considered a slower, more
expensive, and more language-specific method using a mor-
phological tagger (Habash et al, 2013) that outperformed the
CEC model; however, we do not compare to it in this paper.
Input Action Label
k substitute-with(c)
o ok
r insert(r)
e ok
c ok
t ok
d delete
Table 1: Character-level spelling error correction process
on the input word korectd, with the reference word correct
Train Dev Test
Sentences 10.3K 1.67K 1.73K
Characters 675K 106K 103K
Words 134K 21.1K 20.6K
Table 2: ARZ Egyptian dialect corpus statistics
of the sentence, we assume the presence of a
dummy sentence-final stop character.
We use a multi-class SVM classifier to predict the
action labels for each input character e
i
? e. A
decoding process is then applied to transform the
input characters accordingly to produce the cor-
rected sentence. Note that we consider the space
character as a character like any other, which gives
us the ability to correct word merge errors with
space character insertion actions and word split er-
rors with space character deletion actions. Table 1
shows an example of the spelling correction pro-
cess.
In this paper, we only model single-edit actions
and ignore cases where a character requires mul-
tiple edits (henceforth, complex actions), such as
multiple insertions or a combination of insertions
and substitutions. This choice was motivated by
the need to reduce the number of output labels, as
many infrequent labels are generated by complex
actions. An error analysis of the training data, de-
scribed in detail in section 3.2, showed that com-
plex errors are relatively infrequent (4% of data).
We plan to address these errors in future work.
Finally, in order to generate the training data
in the described form, we require a parallel cor-
pus of erroneous and corrected reference text (de-
scribed below), which we align at the character
level. We use the alignment tool Sclite (Fiscus,
1998), which is part of the SCTK Toolkit.
3.2 Description of Data
We apply our model to correcting Egyptian Ara-
bic dialect text. Since there is no standard dialect
orthography adopted by native speakers of Ara-
bic dialects, it is common to encounter multiple
162
Action % Errors Example Error? Reference
Substitute 80.9
E
Alif A @ forms ( @/

@/ @

/

@A?/
?
A/
?
A) 33.3 AHdhm? ?Hdhm ??Yg@? ??Yg

@
E
Ya ?


/? forms ( y/?) 26.7 ?ly? ?l?
?


?
??
??
?
E
h/~ ?/

? , h/w ?/? forms 14.9 kfrh? kfr~ ?Q
	
???

?Q
	
??
E
h/H ?/h forms 2.2 ht?mlhA? Ht?mlhA A????

J?? A????

Jk
Other substitutions 3.8 AltAny~? Al?Any~

?J


	
K A

J? @?

?J


	
K A

J? @ ; dA? dh @X? ?X
Insert 10.5
EP
Insert {A} 3.0 ktbw? ktbwA ?J
.

J?? @?J
.

J?
EP
Insert {space} 2.9 mAtz?l?? mA tz?l?

???
	
Q

KA??

???
	
Q

K A?
Other insertion actions 4.4 Aly? Ally ?


?@?
?


?
? @
Delete 4.7
E
Del{A} 2.4 whmA? whm A???? ???
Other deletion actions 2.3 wfyh? wfy ?J


	
??? ?


	
??
Complex 4.0 mykwn?? mA ykwn?

?
	
??J


??

?
	
??K


A?
Table 3: Character-level distribution of correction labels. We model all types of transformations except complex actions, and
rare Insert labels with counts below a tuned threshold. The Delete label is a single label that comprises all deletion actions.
Labels modeled by Eskander et al (2013) are marked with
E
, and
EP
for cases modeled partially, for example, the Insert{A}
would only be applied at certain positions such as the end of the word.
spellings of the same word. The CODA orthogra-
phy was proposed by Habash et al (2012) in an
attempt to standardize dialectal writing, and we
use it as a reference of correct text for spelling
correction following the previous work by Eskan-
der et al (2013). We use the same corpus (la-
beled "ARZ") and experimental setup splits used
by them. The ARZ corpus was developed by
the Linguistic Data Consortium (Maamouri et al,
2012a-e). See Table 2 for corpus statistics.
Error Distribution Table 3 presents the distri-
bution of correction action labels that correspond
to spelling errors in the training data together with
examples of these errors.
3
We group the ac-
tions into: Substitute, Insert, Delete, and Complex,
and also list common transformations within each
group. We further distinguish between the phe-
nomena modeled by our system and by Eskander
et al (2013). At least 10% of all generated action
labels are not handled by Eskander et al (2013).
3.3 Features
Each input character is represented by a feature
vector. We include a set of basic features inspired
by Eskander et al (2013) in their CEC system and
additional features for further improvement.
Basic features We use a set of nine basic fea-
tures: the given character, the preceding and fol-
lowing two characters, and the first two and last
3
Arabic transliteration is presented in the Habash-Soudi-
Buckwalter scheme (Habash et al, 2007). For more informa-
tion on Arabic orthography in NLP, see (Habash, 2010).
two characters in the word. These are the same
features used by CEC, except that CEC does
not include characters beyond the word boundary,
while we consider space characters as well as char-
acters from the previous and next words.
Ngram features We extract sequences of char-
acters corresponding to the current character and
the following and previous two, three, or four
characters. We refer to these sequences as bi-
grams, trigrams, or 4-grams, respectively. These
are an extension of the basic features and allow
the model to look beyond the context of the cur-
rent word.
3.4 Maximum Likelihood Estimate (MLE)
We implemented another approach for error cor-
rection based on a word-level maximum likeli-
hood model. The MLE method uses a unigram
model which replaces each input word with its
most likely correct word based on counts from the
training data. The intuition behind MLE is that it
can easily correct frequent errors; however, it is
quite dependent on the training data.
4 Experiments
4.1 Model Evaluation
Setup The training data was extracted to gener-
ate the form described in Section 3.1, using the
Sclite tool (Fiscus, 1998) to align the input and
reference sentences. A speech effect handling step
was applied as a preprocessing step to all models.
163
This step removes redundant repetitions of charac-
ters in sequence, e.g.,
Q



J





J






J? ktyyyyyr ?veeeeery?.
The same speech effect handling was applied by
Eskander et al (2013).
For classification, we used the SVM implemen-
tation in YamCha (Kudo and Matsumoto, 2001),
and trained with different variations of the fea-
tures described above. Default parameters were
selected for training (c=1, quadratic kernel, and
context window of +/- 2).
In all results listed below, the baseline corre-
sponds to the do-nothing baseline of the input text.
Metrics Three evaluation metrics are used. The
word-error-rate WER metric is computed by sum-
ming the total number of word-level substitution
errors, insertion errors, and deletion errors in the
output, and dividing by the number of words in the
reference. The correct-rate Corr metric is com-
puted by dividing the number of correct output
words by the total number of words in the refer-
ence. These two metrics are produced by Sclite
(Fiscus, 1998), using automatic alignment. Fi-
nally, the accuracy Acc metric, used by Eskander
et al (2013), is a simple string matching metric
which enforces a word alignment that pairs words
in the reference to those of the output. It is cal-
culated by dividing the number of correct output
words by the number of words in the input. This
metric assumes no split errors in the data (a word
incorrectly split into two words), which is the case
in the data we are working with.
Character-level Model Evaluation The per-
formance of the generalized spelling correction
model (GSEC) on the dev data is presented in the
first half of Table 4. The results of the Eskan-
der et al (2013) CEC system are also presented
for the purpose of comparison. We can see that
using a single classifier, the generalized model is
able to outperform CEC, which relies on a cascade
of classifiers (p = 0.03 for the basic model and
p < 0.0001 for the best model, GSEC+4grams).
4
Model Combination Evaluation Here we
present results on combining GSEC with the
MLE component (GSEC+MLE). We combine the
two models in cascade: the MLE component is
applied to the output of GSEC. To train the MLE
model, we use the word pairs obtained from the
original training data, rather than from the output
of GSEC. We found that this configuration allows
4
Significance results are obtained using McNemar?s test.
Approach Corr%/WER Acc%
Baseline 75.9/24.2 76.8
CEC 88.7/11.4 90.0
GSEC 89.7/10.4* 90.3*
GSEC+2grams 90.6/9.5* 91.2*
GSEC+4grams 91.0/9.2* 91.6*
MLE 89.7/10.4 90.5
CEC + MLE 90.8/9.4 91.5
GSEC+MLE 91.0/9.2 91.3
GSEC+4grams+ MLE 91.7/8.3* 92.2*
Table 4: Model Evaluation. GSEC represents the gener-
alized character-level model. CEC represents the character-
level-edit classification model of Eskander et al (2013).
Rows marked with an asterisk (*) are statistically signifi-
cant compared to CEC (for the first half of the table) or
CEC+MLE (for the second half of the table), with p < 0.05.
us to include a larger sample of word pair errors
for learning, because our model corrects many
errors, leaving fewer example pairs to train an
MLE post-processor. The results are shown in the
second half of Table 4.
We first observe that MLE improves the per-
formance of both CEC and GSEC. In fact,
CEC+MLE and GSEC+MLE perform similarly
(p = 0.36, not statistically significant). When
adding features that go beyond the word bound-
ary, we achieve an improvement over MLE,
GSEC+MLE, and CEC+MLE, all of which are
mostly restricted within the boundary of the word.
The best GSEC model outperforms CEC+MLE
(p < 0.0001), achieving a WER of 8.3%, corre-
sponding to 65% reduction compared to the base-
line. It is worth noting that adding the MLE com-
ponent allows Eskander?s CEC to recover various
types of errors that were not modeled previously.
However, the contribution of MLE is limited to
words that are in the training data. On the other
hand, because GSEC is trained on character trans-
formations, it is likely to generalize better to words
unseen in the training data.
Results on Test Data Table 5 presents the re-
sults of our best model (GSEC+4grams), and best
model+MLE. The latter achieves a 92.1% Acc
score. The Acc score reported by Eskander et al
(2013) for CEC+MLE is 91.3% . The two results
are statistically significant (p < 0.0001) with re-
spect to CEC and CEC+MLE respectively.
Approach Corr%/WER Acc%
Baseline 74.5/25.5 75.5
GSEC+4grams 90.9/9.1 91.5
GSEC+4grams+ MLE 91.8/8.3 92.1
Table 5: Evaluation on test data.
164
4.2 Error Analysis
To gain a better understanding of the performance
of the models on different types of errors and their
interaction with the MLE component, we separate
the words in the dev data into: (1) words seen in
the training data, or in-vocabulary words (IV), and
(2) out-of-vocabulary (OOV) words not seen in
the training data. Because the MLE model maps
every input word to its most likely gold word seen
in the training data, we expect the MLE compo-
nent to recover a large portion of errors in the IV
category (but not all, since an input word can have
multiple correct readings depending on the con-
text). On the other hand, the recovery of errors in
OOV words indicates how well the character-level
model is doing independently of the MLE compo-
nent. Table 6 presents the performance, using the
Acc metric, on each of these types of words. Here
our best model (GSEC+4grams) is considered.
#Inp Words Baseline CEC+MLE GSEC+MLE
OOV 3,289 (17.2%) 70.7 76.5 80.5
IV 15,832 (82.8%) 78.6 94.6 94.6
Total 19,121 (100%) 77.2 91.5 92.2
Table 6: Accuracy of character-level models shown sepa-
rately on out-of-vocabulary and in-vocabulary words.
When considering words seen in the training
data, CEC and GSEC have the same performance.
However, when considering OOV words, GSEC
performs significantly better (p < 0.0001), veri-
fying our hypothesis that a generalized model re-
duces dependency on training data. The data is
heavily skewed towards IV words (83%), which
explains the generally high performance of MLE.
We performed a manual error analysis on a sam-
ple of 50 word errors from the IV set and found
that all of the errors came from gold annotation er-
rors and inconsistencies, either in the dev or train.
We then divided the character transformations in
the OOV words into four groups: (1) characters
that were unchanged by the gold (X-X transforma-
tions), (2) character transformations modeled by
CEC (X-Y CEC), (3) character transformations not
modeled by CEC, and which include all phenom-
ena that were only partially modeled by CEC (X-Y
not CEC), and (4) complex errors. The character-
level accuracy on each of these groups is shown in
Table 7.
Both CEC and GSEC do much better on the
second group of character transformations (that
is, X-Y CEC) than on the third group (X-Y not
CEC). This is not surprising because the former
Type #Chars Example CEC GSEC
X-X 16502 m-m, space-space 99.25 99.33
X-Y 609 ~-h, h-~,
?
A-A 80.62 83.09
(CEC) A-
?
A, y-?
X-Y 161 t-? , del{w} 31.68 43.48
(not CEC) n-ins{space}
Complex 32 n-ins{A}{m} 37.5 15.63
Table 7: Character-level accuracy on different transforma-
tion types for out-of-vocabulary words. For complex trans-
formations, the accuracy represents the complex category
recognition rate, and not the actual correction accuracy.
transformations correspond to phenomena that are
most common in the training data. For GSEC,
they are learned automatically, while for CEC they
are selected and modeled explicitly. Despite this
fact, GSEC generalizes better to OOV words. As
for the third group, both CEC and GSEC per-
form more poorly, but GSEC corrects more errors
(43.48% vs. 31.68% accuracy). Finally, CEC is
better at recognizing complex errors, which, al-
though are not modeled explicitly by CEC, can
sometimes be corrected as a result of applying
multiple classifiers in cascade. Dealing with com-
plex errors, though there are few of them in this
dataset, is an important direction for future work,
and for generalizing to other datasets, e.g., (Za-
ghouani et al, 2014).
5 Conclusions
We showed that a generalized character-level
spelling error correction model can improve
spelling error correction on Egyptian Arabic data.
This model learns common spelling error patterns
automatically, without guidance of manually se-
lected or language-specific constraints. We also
demonstrate that the model outperforms existing
methods, especially on out-of-vocabulary words.
In the future, we plan to extend the model to use
word-level language models to select between top
character predictions in the output. We also plan
to apply the model to different datasets and differ-
ent languages. Finally, we plan to experiment with
more features that can also be tailored to specific
languages by using morphological and linguistic
information, which was not explored in this paper.
Acknowledgments
This publication was made possible by grant
NPRP-4-1058-1-168 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
The statements made herein are solely the respon-
sibility of the authors.
165
References
Mohamed I. Alkanhal, Mohammed A. Al-Badrashiny,
Mansour M. Alghamdi, and Abdulaziz O. Al-
Qabbany. 2012. Automatic Stochastic Arabic
Spelling Correction With Emphasis on Space Inser-
tions and Deletions. IEEE Transactions on Audio,
Speech & Language Processing, 20:2111?2122.
Michele Banko and Eric Brill. 2001. Scaling to very
very large corpora for natural language disambigua-
tion. In Proceedings of 39th Annual Meeting of the
Association for Computational Linguistics, pages
26?33, Toulouse, France, July.
Chiraz Ben Othmane Zribi and Mohammed Ben
Ahmed. 2003. Efficient Automatic Correction
of Misspelled Arabic Words Based on Contextual
Information. In Proceedings of the Knowledge-
Based Intelligent Information and Engineering Sys-
tems Conference, Oxford, UK.
Andrew Carlson and Ian Fette. 2007. Memory-based
context-sensitive spelling correction at web scale. In
Proceedings of the IEEE International Conference
on Machine Learning and Applications (ICMLA).
Daniel Dahlmeier and Hwee Tou Ng. 2012. A beam-
search decoder for grammatical error correction. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
568?578.
Huizhong Duan, Yanen Li, ChengXiang Zhai, and
Dan Roth. 2012. A discriminative model for
query spelling correction with latent structural svm.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1511?1521, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ramy Eskander, Nizar Habash, Owen Rambow, and
Nadi Tomeh. 2013. Processing spontaneous orthog-
raphy. In The Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL
HLT ?13.
Jon Fiscus. 1998. Sclite scoring package ver-
sion 1.5. US National Institute of Standard
Technology (NIST), URL http://www. itl. nist.
gov/iaui/894.01/tools.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 163?171, Los
Angeles, California, June.
Andrew R. Golding and Dan Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning, 34(1-3):107?130.
Nizar Habash and Ryan M. Roth. 2011. Using deep
morphology to improve automatic error detection in
arabic handwriting recognition. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies - Volume 1, HLT ?11, pages 875?884, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Mona Diab, and Owen Rambow.
2012. Conventional orthography for dialectal Ara-
bic. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet U?gur
Do?gan, Bente Maegaard, Joseph Mariani, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-Words in Arabic: A Hybrid
Approach. International Journal of Computer Pro-
cessing Of Languages (IJCPOL).
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies-Volume 1, pages 368?378.
Association for Computational Linguistics.
Bo Han, Paul Cook, and Timothy Baldwin. 2013.
Lexical normalization for social media text. ACM
Transactions on Intelligent Systems and Technology
(TIST), 4(1):5.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction us-
ing Finite State Automata. In Proceedings of the In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2008).
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies, NAACL ?01, pages 1?
8, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Sur-
veys, 24(4).
166
Yanen Li, Huizhong Duan, and ChengXiang Zhai.
2012. A generalized hidden markov model with dis-
criminative training for query spelling correction. In
Proceedings of the 35th international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?12, pages 611?620, New
York, NY, USA. ACM.
Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2013. Paraphrasing 4 microblog normal-
ization. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 73?84, Seattle, Washington, USA, Octo-
ber. Association for Computational Linguistics.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012a.
Egyptian Arabic Treebank DF Part 1 V2.0. LDC
catalog number LDC2012E93.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012b.
Egyptian Arabic Treebank DF Part 2 V2.0. LDC
catalog number LDC2012E98.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012c.
Egyptian Arabic Treebank DF Part 3 V2.0. LDC
catalog number LDC2012E89.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012d.
Egyptian Arabic Treebank DF Part 4 V2.0. LDC
catalog number LDC2012E99.
Mohamed Maamouri, Ann Bies, Seth Kulick, Sondos
Krouna, Dalila Tabassi, and Michael Ciul. 2012e.
Egyptian Arabic Treebank DF Part 5 V2.0. LDC
catalog number LDC2012E107.
Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for esl correction tasks.
In Proc. of the Annual Meeting of the Association of
Computational Linguistics (ACL), Portland, Oregon,
6. Association for Computational Linguistics.
Khaled Shaalan, Rana Aref, and Aly Fahmy. 2010. An
approach for analyzing and correcting spelling er-
rors for non-native Arabic learners. Proceedings of
Informatics and Systems (INFOS).
L Venkata Subramaniam, Shourya Roy, Tanveer A
Faruquie, and Sumit Negi. 2009. A survey of types
of text noise and techniques to handle noisy text.
In Proceedings of The Third Workshop on Analytics
for Noisy Unstructured Text Data, pages 115?122.
ACM.
Sebastian van Delden, David B. Bracewell, and Fer-
nando Gomez. 2004. Supervised and unsupervised
automatic spelling correction algorithms. In Infor-
mation Reuse and Integration, 2004. Proceedings of
the 2004 IEEE International Conference on, pages
530?535.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale Arabic error annotation: Guidelines and
framework. In Proceedings of the 9th edition of the
Language Resources and Evaluation Conference.
167
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 160?164,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The Columbia System in the QALB-2014 Shared Task
on Arabic Error Correction
Alla Rozovskaya Nizar Habash
?
Ramy Eskander Noura Farra Wael Salloum
Center for Computational Learning Systems, Columbia University
?
New York University Abu Dhabi
{alla,ramy,noura,wael}@ccls.columbia.edu
?
nizar.habash@nyu.edu
Abstract
The QALB-2014 shared task focuses on
correcting errors in texts written in Mod-
ern Standard Arabic. In this paper, we
describe the Columbia University entry in
the shared task. Our system consists of
several components that rely on machine-
learning techniques and linguistic knowl-
edge. We submitted three versions of the
system: these share several core elements
but each version also includes additional
components. We describe our underlying
approach and the special aspects of the dif-
ferent versions of our submission. Our
system ranked first out of nine participat-
ing teams.
1 Introduction
The topic of text correction has seen a lot of in-
terest in the past several years, with a focus on
correcting grammatical errors made by learners of
English as a Second Language (ESL). The two
most recent CoNLL shared tasks were devoted to
grammatical error correction for non-native writ-
ers (Ng et al., 2013; Ng et al., 2014).
The QALB-2014 shared task (Mohit et al.,
2014) is the first competition that addresses the
problem of text correction in Modern Standard
Arabic (MSA) texts. The competition makes
use of the recently developed QALB corpus (Za-
ghouani et al., 2014). The shared task covers all
types of mistakes that occur in the data.
Our system consists of statistical models, lin-
guistic resources, and rule-based modules that ad-
dress different types of errors.
We briefly discuss the task in Section 2. Sec-
tion 3 gives an overview of the Columbia system
and describes the system components. In Sec-
tion 4, we evaluate the complete system on the de-
velopment data and show the results obtained on
test. Section 5 concludes.
2 Task Description
The QALB-2014 shared task addresses the prob-
lem of correcting errors in texts written in Modern
Standard Arabic (MSA). The task organizers re-
leased training, development, and test data. All
of the data comes from online commentaries writ-
ten to Aljazeera articles.
1
The training data con-
tains 1.2 million words; the development and the
test data contain about 50,000 words each. The
data was annotated and corrected by native Arabic
speakers. For more detail on the QALB corpus, we
refer the reader to Zaghouani et al. (2014). The re-
sults in the subsequent sections are reported on the
development set.
It should be noted that in the annotation process,
the annotators did not assign error categories but
only specified an appropriate correction. In spite
of this, it is possible, to isolate certain error types
automatically, by using the corrections in coordi-
nation with the input words. The first type con-
cerns punctuation errors. Errors involving punc-
tuation account for about 39% of all errors in the
data. In addition to punctuation mistakes, another
very common source of errors refers to subopti-
mal spelling for two groups of letters ? Alif (and
its Hamzated versions) and Ya (and its undotted or
Alif Maqsura versions). For more detail on this
and other Arabic phenomena, we refer the reader
to Habash (2010; Buckwalter (2007; El Kholy and
Habash (2012). Mistakes associated with Alif and
1
http://www.aljazeera.net/
160
Component System
CLMB-1 CLMB-2 CLMB-3
MADAMIRA
MLE
Na??ve Bayes
GSEC
MLE-unigram
Punctuation
Dialectal
Patterns
Table 1: The three versions of the Columbia sys-
tem and their components.
Ya spelling constitute almost 30% of all errors.
3 System Overview
The Columbia University system consists of sev-
eral components designed to address different
types of errors. We submitted three versions of the
system. We refer to these as CLMB-1, CLMB-2,
and CLMB-3. Table 1 lists all of the components
and indicates which components are included in
each version. The components are applied in the
order shown in the table. Below we describe each
component in more detail.
3.1 MADAMIRA Corrector
MADAMIRA (Pasha et al., 2014) is a tool
designed for morphological analysis and dis-
ambiguation of Modern Standard Arabic.
MADAMIRA performs morphological analysis
in context. This is a knowledge-rich resource
that requires a morphological analyzer and a
large corpus where every word is marked with
its morphological features. The task organizers
provided the shared task data pre-processed
with MADAMIRA, including all of the features
generated by the tool for every word. In addition
to the morphological analysis and contextual
morphological disambiguation, MADAMIRA
also performs Alif and Ya spelling correction
for the phenomena associated with these letters
discussed in Section 2. The corrected form was
included among the features and can be used
for correcting the input. We use the corrections
proposed by MADAMIRA and apply them to the
data. As we show in Section 4, while the form
proposed by MADAMIRA may not necessarily
be correct, MADAMIRA performs at a very high
precision. MADAMIRA corrector is used in the
CLMB-1 and CLMB-2 systems.
3.2 Maximum Likelihood Model
The Maximum Likelihood Estimator (MLE) is a
supervised component that is trained on the train-
ing data of the shared task. Given the annotated
training data, a map is defined that specifies for ev-
ery word n-gram in the source text the most likely
n-gram corresponding to it in the target text. The
MLE model considers source n-grams of lengths
between 1 to 3; the MLE-unigram model that is
part of the CLMB-3 version only considers n-
grams of length 1.
The MLE approach performs well on errors that
have been observed in the training data and can
be unambiguously corrected without using the sur-
rounding context, i.e. do not have many alternative
corrections. Consequently, MLE fails on words
that have many possible corrections, as well as
words not seen in training.
3.3 Na??ve Bayes for Unseen Words
The Na??ve Bayes component addresses errors for
words that were not seen in training. The system
uses the approach proposed in Rozovskaya and
Roth (2011) that proved to be successful for cor-
recting errors made by English as a Second Lan-
guage learners. The model operates at the word
level and targets word replacement errors that in-
volve single tokens. Candidate corrections are
generated using a character confusion table that is
based on the training data. The model is a Na??ve
Bayes classifier trained on the Arabic Gigaword
corpus (Parker et al., 2011) with word n-gram fea-
tures in the 4-word window around the word to be
corrected. The Na??ve Bayes component is used in
the CLMB-1 system.
3.4 The GSEC Model
The CLMB-3 system implements a Generalized
Character-Level Error Correction model (GSEC)
proposed in Farra et al. (2014). GSEC is a super-
vised model that operates at the character level.
Because of this, the source and the target side of
the training data need to be aligned at the charac-
ter level. We use the alignment tool Sclite (Fiscus,
1998). The alignment maps each source charac-
ter to itself, a different character, a pair of char-
acters, or an empty string. For the shared task,
punctuation corrections are ignored since punctu-
ation errors are handled by the punctuation correc-
tor described in the following section. It should
161
also be noted that the model was not trained to
insert missing characters. The model is a multi-
class SVM classifier (Kudo, 2005) that makes use
of character-level features using a window of four
characters that may occur within the word bound-
aries as well as in the surrounding context. Due
to a long training time, GSEC was trained on a
quarter of the training data. The system is post-
processed with a unigram word-level maximum-
likelihood model described in Section 3.2. For
more detail on the GSEC approach, we refer the
reader to Farra et al. (2014).
3.5 Punctuation Corrector
The shared task data contains a large number of
punctuation mistakes. Punctuation errors, such as
missing periods and commas, account for about
30% of all errors in the data. Most of these errors
involve incorrectly omitting a punctuation symbol.
Our punctuation corrector is a statistical model
that inserts periods and commas. The system is
a decision tree model trained on the shared task
training data using WEKA (Hall et al., 2009). For
punctuation insertion, every space that is not fol-
lowed or preceded by a punctuation mark is con-
sidered.
To generate features, we use a window of size
three around the target space. The features are de-
fined as follows:
? The part-of-speech of the previous word
? The existence of a conjunctive or connective
proclitic in the following word; that is a ?w?
or ?f? proclitic that is either a conjunction, a
sub-conjunction or a connective particle
The part-of-speech and proclitic information is
obtained by running MADAMIRA on the text.
We also ran experiments where the model is
trained with a complete list of features produced
by MADAMIRA; that is part-of-speech, gender,
number, person, aspect, voice, case, mood, state,
proclitics and enclitics. This was done for two pre-
ceding words and two following words. However,
this model did not perform as well as the one de-
scribed above, which we used in the final system.
Note that the punctuation model predicts pres-
ence or absence of a punctuation mark in a spe-
cific location and is applied to the source data
from which all punctuation marks have been re-
moved. However, when we apply our punctuation
model in the correction pipeline, we find that it
is always better to keep the already existing peri-
ods and commas in the input text instead of over-
writing them with the model prediction. In other
words, we only attempt to add missing punctua-
tion.
3.6 Dialectal Usage Corrector
Even though the shared task data is written in
MSA, MSA is not a native language for Arabic
speakers. Typically, an Arabic speaker has a native
proficiency in one of the many Arabic dialects and
learns to write and read MSA in a formal setting.
For this reason, even in MSA texts produced by
native Arabic speakers, one typically finds words
and linguistic features specific to the writer?s na-
tive dialect that are not found in the standard lan-
guage.
To address such errors, we use Elissa (Salloum
and Habash, 2012), which is Dialectal to Standard
Arabic Machine Translation System. Elissa uses
a rule-based approach that relies on the existence
of a dialectal morphological analyzer (Salloum
and Habash, 2011), a list of hand-written trans-
fer rules, and dialectal-to-standard Arabic lexi-
cons. Elissa uses different dialect identification
techniques to select dialectal words and phrases
(dialectal multi-word expressions) that need to be
handled. Then equivalent MSA paraphrases of the
selected words/phrases are generated and an MSA
lattice for each input sentence is constructed. The
paraphrases within the lattice are then ranked us-
ing language models and the n-best sentences are
extracted from lattice. We use 5-gram language
models trained using SRILM (Stolcke, 2002) on
about 200 million untokenized, Alif /Ya normal-
ized words extracted from Arabic GigaWord. This
component is employed in the CLMB-2 system.
3.7 Pattern-Based Corrector
We created a set of rules that account for very
common phenomena involving incorrectly split or
merged tokens. The MADAMIRA corrector de-
scribed above does not handle splits and merges;
however, some of the cases are handled in the
MLE method. Note that the MLE method is re-
strictive since it does not correct words not seen
in training, while the pattern-based corrector is
more general. The rules were created through
analysis of samples of the QALB Shared Task
162
training data. Some of the rules use regular ex-
pressions, while others make use of the rule-
based Standard Arabic Morphological Analyzer
(SAMA) (Maamouri et al., 2010), the same out-
of-context analyzer used inside of MADAMIRA.
Rules for splitting words
? All digits are separated from words.
? A space is added after all word medial Ta-
Marbuta characters.
? A space is added after the very common
?ElY? ?at/about/on? preposition if it is at-
tached to the following word.
? If a word has a morphological analysis that
includes ?lmA? (as negation particle, relative
pronoun or pseudo verb), ?hA? (a demonstra-
tive pronoun), or ?Ebd? and ?>bw? in proper
nouns, a space is inserted after those parts of
the analysis.
? If a word has no morphological analysis, but
starts with a set of commonly mis-attached
words, and the rest of the word has an anal-
ysis, the word is split after the mis-attached
word sequence.
Rules for merging words
? All lone occurrences of the conjunction w
?and? are attached to the following word.
? All sequences of the punctuation marks (., ?,
!) that occur between two and six times are
merged: e.g ! ! ! ? !!!.
4 Experimental Results
In Section 3, we described the individual sys-
tem components that address different types of
errors. In this section, we show how the sys-
tem improves when each component is added into
the system. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012), the official
scorer of the shared task.
Table 2 reports performance results of each ver-
sion of the Columbia system on the development
data. Table 3 shows the performance results for the
best-performing system, CLMB-1, as each system
component is added.
System P R F1
CLMB-1 72.22 62.79 67.18
CLMB-2 69.49 61.72 65.38
CLMB-3 69.71 59.42 64.15
Table 2: Performance of the Columbia systems
on the development data.
System P R F1
MADAMIRA 83.33 32.94 47.21
+ MLE 86.52 42.52 57.02
+ NB 85.80 43.27 57.53
+ Punc. 73.66 59.51 65.83
+ Patterns 72.22 62.79 67.18
Table 3: Performance of the CLMB-1 system on
the development data and the contribution of
its components.
System P R F1
CLMB-1 73.34 63.23 67.91
CLMB-2 70.86 62.21 66.25
CLMB-3 71.45 60.00 65.22
Table 4: Performance of the Columbia systems
on the test data.
Finally, Table 4 reports results obtained on the
test data. These results are comparable to the per-
formance observed on the development data. In
particular, CLMB-1 achieves the highest score.
5 Conclusion
We have described the Columbia University sys-
tem that participated in the first shared task
on grammatical error correction for Arabic and
ranked first out of nine participating teams. We
have presented three versions of the system; all of
these incorporate several components that target
different types of mistakes, which we presented
and evaluated in this paper.
Acknowledgments
This material is based on research funded by grant
NPRP-4-1058-1-168 from the Qatar National Re-
search Fund (a member of the Qatar Foundation).
The statements made herein are solely the respon-
sibility of the authors. Nizar Habash performed
most of his contribution to this paper while he was
at the Center for Computational Learning Systems
at Columbia University.
163
References
T. Buckwalter. 2007. Issues in Arabic Morphological
Analysis. In A. van den Bosch and A. Soudi, editors,
Arabic Computational Morphology: Knowledge-
based and Empirical Methods. Springer.
D. Dahlmeier and H. T. Ng. 2012. Better evaluation
for grammatical error correction. In Proceedings of
NAACL.
A. El Kholy and N. Habash. 2012. Orthographic and
morphological processing for English?Arabic sta-
tistical machine translation. Machine Translation,
26(1-2).
N. Farra, N. Tomeh, A. Rozovskaya, and N. Habash.
2014. Generalized character-level spelling error cor-
rection. In Proceedings of ACL.
J. Fiscus. 1998. Sclite scoring package ver-
sion 1.5. US National Institute of Standard
Technology (NIST), URL http://www. itl. nist.
gov/iaui/894.01/tools.
N. Y. Habash. 2010. Introduction to Arabic natural
language processing. Synthesis Lectures on Human
Language Technologies 3.1.
M. Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H. Witten. 2009. The WEKA data
mining software: an update. SIGKDD Explorations,
11(1):10?18.
T. Kudo. 2005. YamCha: Yet another multipurpose
chunk annotator. http://chasen.org/ taku/software/.
M. Maamouri, D. Graff, B. Bouziri, S. Krouna, A. Bies,
and S. Kulick. 2010. LDC Standard Arabic Mor-
phological Analyzer (SAMA) Version 3.1. Linguistic
Data Consortium.
B. Mohit, A. Rozovskaya, N. Habash, W. Zaghouani,
and O. Obeid. 2014. The first QALB shared task on
automatic text correction for Arabic. In Proceedings
of EMNLP Workshop on Arabic Natural Language
Processing.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The CoNLL-2013 shared task
on grammatical error correction. In Proceedings of
CoNLL: Shared Task.
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of CoNLL: Shared Task.
R. Parker, D. Graff, K. Chen, J. Kong, and K. Maeda.
2011. Arabic Gigaword Fifth Edition. Linguistic
Data Consortium.
A. Pasha, M. Al-Badrashiny, A. E. Kholy, R. Eskan-
der, M. Diab, N. Habash, M. Pooleery, O. Rambow,
and R. Roth. 2014. MADAMIRA: A fast, compre-
hensive tool for morphological analysis and disam-
biguation of arabic. In Proceedings of LREC.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks.
In Proceedings of ACL.
W. Salloum and N. Habash. 2011. Dialectal to stan-
dard arabic paraphrasing to improve arabic-english
statistical machine translation. In Proceedings of
the First Workshop on Algorithms and Resources for
Modelling of Dialects and Language Varieties.
W. Salloum and N. Habash. 2012. Elissa: A dialectal
to standard arabic machine translation system. In
Proceedings of COLING (Demos).
A. Stolcke. 2002. Srilm-an extensible language mod-
eling toolkit. In Proceedings of International Con-
ference on Spoken Language Processing.
W. Zaghouani, B. Mohit, N. Habash, O. Obeid,
N. Tomeh, A. Rozovskaya, N. Farra, S. Alkuhlani,
and K. Oflazer. 2014. Large scale arabic error anno-
tation: Guidelines and framework. In Proceedings
of the Ninth International Conference on Language
Resources and Evaluation (LREC?14).
164
