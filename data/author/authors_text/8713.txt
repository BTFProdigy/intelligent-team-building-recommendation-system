A Deve lopment  Env i ronment  for an MTT-Based  Sentence  
Generator  
Bernd Bohnet,  Andreas Langjahr and Leo Wanner 
Computer  Science Depar tment  
Univers i ty  of Stut tgar t  
Breitwiesenstr .  20-22 
-70565:Stut tgar t ,  Germany . . . . . . . . . .  " ........ 
{bohnet \ [ langjahr\[wanner}?informatik.uni-stuttgart .de 
1 I n t roduct ion  
With the rising standard of the state of the art in 
text generation and the increase of the number 
of practical generation applications, it becomes 
more and more important o provide means for 
the maintenance of the generator, i.e. its ex- 
tension, modification, and monitoring by gram- 
marians who are not familiar with its internals. 
However, only a few sentence and text gener- 
ators developed to date actually provide these 
means. One of these generators is KPML (Bate- 
man, 1997). I~PML comes with a Development 
Environment and there is no doubt about the 
contribution of this environment to the popular- 
ity of the systemic approach in generation. 
In the generation project at Stuttgart, the 
realization of a high quality development en- 
vironment (henceforth, DE) has been a central 
topic from the beginning. The De provides sup- 
port to the user with respect to writing, mod- 
ifying, testing, and debugging of (i) grammar 
rules. (ii) lexical information, and (iii) linguis- 
tic structures at different levels of abstraction. 
Furthermore, it automatically generalizes tile or- 
ganization of the lexica and the grammar. In 
what follows, we briefly describe oF,'s main fea- 
tures. The theoretical linguistic background of 
the DE is the Meaning-Text Theory (Mel'euk, 
1988: Polgu~re, 1998). However. its introduc- 
tion is beyond tile scope of this note: tile inter- 
ested reader is asked to consuh the above reg 
erences as well as further literature on the use 
Of MTT ill text generation---for illSlallCe, (Ior- 
danskaja cta l . ,  1992: I,avoie ?- Rainbow. 1997: 
(.'och. 1997). 
2 Globa l  V iew on the  DE 
In MTT, seven levels (or strata) of linguis- 
tic description are distinguished, of which 
five are relevant for generation: semantic 
(Sem), deep-syntactic (DSynt), surface-syntactic 
(SSynt), deep-morphologicM (DMorph) and 
surface-morphological (SMorph). In order to be 
able to generate starting from the data in a data 
base, we introduce an additional, the conceptual 
(Con) stratum. The input structure to DE is thus 
a conceptual structure (ConStr) derived from the 
data in the DB. The generation process consists 
of a series of structure mappings between adja- 
cent strata until the SMorph stratum is reached. 
At the SMorph stratum, the structure is a string 
of linearized word forms. 
The central module of the DE iS a compiler 
that maps a structure specified at one of tile five 
first of the above strata on a structure at the 
adjacent stratum. To support the user in the ex- 
amination of the internal information gathered 
during the processing of a structure, a debug- 
ger and an inspector are available. The user can 
interact with the compiler either via a graphic 
interface or via a text command interface. For 
the maintenance of the grammar, of the lexica 
and of the linguistic structures, the DE possesses 
separate ditors: a rule editor, a lexicon editor, 
and a structure editor. 
2.1 The  Ru le  Ed i tor  
The  Ru les .  Most of the grammatical rules 
in an MTT-based generator are two-level rules. 
.'\ two-level rule establishes a correspomlence 
260 
between minimal structures of two adjacent 
strata. Given that in generation five of 
MTT'S strata are used, four sets of two-level 
rules are available: (1) Sem=vDSynt-rules, (2) 
DSynt~SSynt-rules, (3) SSynt=vDMorph rules, 
and (4) DMorph~SMorph-rules. 
Formally, a two-level rule is defined by the 
optimize the organization of the grammar by a u - 
tomatic detection Of common parts in several 
rules and their extraction into abstract 'class' 
rules. The theoretical background and the proce- 
dure of rule generalization is described in detail 
in (Wanner & Bohnet, submitted) and will hence 
not be discussed in this note. 
quintuple (/2, Ctxt, Conds, 7~, Corr). ? specifies While editing a rule, the developer has the 
the lefthand side :of the r.,ule-~a,~minimal~so~rce~a stc nd~r.d,c.ommands:,:~t,,his/'her~ disposal. Rules 
substructure that is mapped by the rule onto its can be edited either in a text rule editor or via 
destination structure specified in 7~, the right- 
hand side of the rule. Ctxt specifies the wider 
context of the lefthand side in the input structure 
(note that by far not all rules contain context in- 
formation). Conds specifies the conditions that 
must be satisfied for the rule to be applicable to 
an input substructure matched by ?. Corr spec- 
ifies the correspondence between the individual 
nodes of the lefthand side and the righthand side 
structures. 
Consider a typical Sem=~,DSynt-rule, 
which maps the semantic relation '1' that 
holds between a property and an entity 
that possesses this property onto the deep- 
syntactic relation ATTR. The names begin- 
ning with a '7' are variables. The condition 
' Lex: : (Sem:: (?Xsem.sem). lex) .cat  = adj' 
requires that the lexicalization of the property 
is an adjective. '?Xsem ~ ?Xdsynt' and '?Ysem 
?:~ ?Ydsynt' mean that the semantic node ?Xsem 
is expressed at the deep-syntactic stratum by 
?Xdsynt, and ?Ysem by ?Ydsynt. 
property (Sem_DSynt) { 
leftside : 
?Xsem -i-+ ?Ysem 
condit ions : 
Sem: :?Xsem.sem,1:ype = property 
Lex: :(Sem::(?Xsem.sem).lex).cat = adj 
rightside: 
?Xds 
?Yds 
?Yds -ATTR-+?Xds 
correspondence : 
?Xsem ~ ?Xds 
?Ysem ~ ?Yds} 
The rule editor (l~t-~) has two main \['unctions: 
(i) to support the mai)~tenance (i.e. editing and 
examination) of grammatical rules, and (ii) to 
a graphic interface. Obviously incorrect rules 
can be detected during the syntax and the se- 
mantic rule checks. The syntax check exam- 
ines the correctness of the notation of the state- 
ments in a rule (i.e. of variables, relations, con- 
ditions, etc.)-- in the same way as a conventional 
compiler does. The semantic check examines 
the consistency of the conditions, relations, and 
attribute-feature pairs in a rule, the presence of 
an attribute's value in the set of values that are 
available to this attribute, etc. If, for instance 
in the above rule 'adj' is misspelled as 'adk' or 
erroneously a multiple correspondence between 
?gds and ?Xsem and ?Ysem is introduced, the 
rule editor draws the developer's attention to tile 
respective rror (see Figure 1). 
Ru le  Test ing.  Rule testing is usually a very 
time consuming procedure, this is so partly be- 
cause tile generator needs to be started as a 
whole again and again, partly because tlle re- 
suiting structure and the trace must be carefully 
inspected in order to find out whether tile rule 
in question fired and if it did not fire why it 
did not. The DE attempts to minimize this el'- 
fort. With "drag and drop' the developer can 
select one or several rules and apply them onto 
an input structure (which can be presented ei- 
ther graphically or in a textual format; see be- 
low). When a rule dropped onto the structure 
fires, the affected parts of the input structure are 
made visually prominent, and the resulting out- 
put (sub)structure appears in the corresponding 
window of the slructure editor. If a rule did not 
fire. the inspector indicates which conditions of 
tim rule in question were not satisfied. See also 
I)elow lhe description of the features of lhe in- 
spect or. 
261 
Figure 1: Error messages of the rule editor 
2.2 The  St ructure  Ed i tor  
The structure editor manages two types of win- 
dows: windows in which the input structures are 
presented and edited, and windows in which the 
resulting structures are presented. Both types of 
windows can be run in a text and in a graphic 
mode. The input structures can be edited in 
both modes, i.e., new nodes and new relations 
can be introduced, attribute-value pairs associ- 
ated with the nodes can be changed, etc. 
In the same way as rules, structures can be 
checked with respect to their syntax and se- 
mantics. Each structure can be exported into 
postscript files and thus conveniently be printed. 
2.3 The  Lex icon Ed i tor  
The main function of the lexicon editor is to sup- 
port the maintenance of the lexica. Several types 
of lexica are distinguished: conceptual lexica, se- 
mantic lexica, and lexico-syntactic lexica.. 
Besides tile standard editor functions, the 
lexicon editor provides the following options: (i) 
sorting of tile entries (either alphabetically or ac- 
cording to such criteria as 'category'); (ii) syntax 
check; (iii) finding information that. is common 
to several entries and extracting it into abstract 
entries (the result is a hierarchical organization 
of the resource). During the demonstration, each 
of these options will be shown ilt action. 
2.4 The  Inspector  
The inspector fulfils mainly three functions. 
First.  it presents in format ion  collected ( lur ing 
the application of the rules selected by the de- 
veloper Io :-ill inpul  st ructure.  This  i l l fornia-  
tion is especially useful for generation experts 
who are familiar with the internal processing. It 
concerns (i) the correspondences tablished be- 
tween nodes of the input structure and nodes of 
the resulting structure, (ii) the instantiation of 
the variables of those rules that are applied to- 
gether to the input structure in question, and 
(iii) the trace of all operations performed by the 
compiler during the application of the rules. 
Second, it indicates to which part of the input 
structure a specific rule is applicable and what 
its result at the destination side is. Third, it in- 
dicates which rules failed and why. The second 
and third kind of information is useful not only 
for generation experts, but also for grammarians 
with a pure linguistic background. 
Figure 2 shows a snapshot of the inspecLor 
editor interface. Sets of rules that can simulta: 
neously be applied together to an input struc- 
ture without causing conflicts are grouped dur- 
ing processing into so-called clusters. At the 
left side of the picture, we see two such clus- 
ters (Cluster 13 and Cluster 22). Tile instances 
of the rules of Cluster 13 are shown to the righl 
of the cluster pane. The cluster pane also con- 
tains sets of rules that failed (in the picture, the 
corresponding icon is not expanded). The left 
graph in Figure 2 is the input structure to which 
the rules are applied. For illustration, one of 
the rules, namely date,  has been selected for ap- 
plication: tile highlighted arcs and nodes of tile 
input structure are the part to which date  is ap- 
plicable. 'Pile result of its application is tile tree 
at the right. Beneath the graphical structures, 
we see tile correspondence between input nodes 
262 
SOUlCe s~'ucture:(  1 ) 
Eva luat ion  F al lecl  
C lu$1er  22  ~3 ? 
+ ..... , \ \ !  
IX . . . .  " + ,  .... \ \ 
? , . j  ~m .+ 2 
taq  tm, . :  a,,: ; ,t , i  
i i . i . i .~ i+_  
~Jt" l  
RI~? ~- ,~k- - ,~+- -Bg~t I l *  \ ' 
T 
A r r~ 
~4 
+ . . . . . . .  \ ] l i .  
N}~Y?P: ' - . .4 J  .~ ~ ,. -7" - -=- -  - -  
!p !e(? )  . . . . . . . .  I o? in  Ume ,~.~!e(?}  
? ~ :o9(.__~ . . . . . . . . . . . .  
ocal Jo ln(5)  . . . . . . .  : loc ln  .spaco(6) 
171 (g)  .371 (9) 
;~yerltB.~__) . . . . . . . . . . .  .VVOrI(I 1 ) _ __  . 
+.~(L0). ...... i.~7.(!_al ................. 
:CO(B)  ,CO(141 
. . . . . . . . . . .  ,,,, , ,  ,,,,, 
Figure 2: The inspector interface of the DE.  
and result nodes. The numbers in parentheses 
are for system use. 
2.5 The  Debugger  
In the rule editor, break points within individual 
rules can be set. When the compiler reaches a 
break point it stops and enters the debugger. In 
the debugger, the developer can execute the rules 
statement by statement. As in the inspector, the 
execution trace, the variable instantiation and 
node correspondences can be examined. During 
the demonstration, the function of the debugger 
will be shown in action. 
3 Cur rent  Work  
DE is written in ,Java 1.2 and has been tested on 
a SUN workstation and on a PC pentium with 
300 MHz and 128 MB of RAM. 
Currently, the described functions of the DE 
are consolidated and extended bv new features. 
The most important of these features aa'e the im- 
port and the export feature. The import feature 
allows for a transformation of grammatical rules 
and lexical information encoded in a different 
format into the format used bv our generator. 
Tests are being carried out with the import of 
RealPro (Lavoie ,~,: Rainbow. 1997) grammati- 
cal rules and lexical information (in particular 
ubcategorization a d diathesis information) en- 
coded in the DATR-formalism. The export fea- 
ture allows for a transformation f the rules and 
lexical information encoded in our format into 
external formats. 
B ib l iography  
Bateman, J.A. 1997. Enabling technology for mul- 
tilingual natural anguage generation: the KPblL 
development environment. Natural Language Engi- 
neering. 3.2:15-55. 
Coch, J. 1997. Quand l'ordinateur prend la plume :
la gdn4ration de textes. Document Numdrique. 1 :~. 
Iordanskaja, L.N., M. Kim, R. Kittredge, B. Lavoie 
~: A. Polgu6re. 1992. Generation of Extended Bilin- 
gual Statistical Reports. COLING-92. 1019 1022. 
Nantes. 
Lavoie, B. & O. Rainbow. 1997. A fast and portable 
realizer for text generation systems. Proceedings of
the Fifth Conference on Applied Natural Language 
Processing. Washington, DC. 
Mel'euk, I.A. 1988. Dependenc!l Syntaz:: Theory and 
Prac&ce. Albany: State University of New York 
Press. 
Polgu6re, A. 1998. La th~orie sens+textre. I)t- 
alangue,. 8-9:9-30. 
Wanner. L. ~ B. Bohnet. submitted. Inheritance in
the MTT-grammar. 
263 
 	
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 98?106,
Beijing, August 2010
Broad Coverage Multilingual Deep Sentence Generation with a
Stochastic Multi-Level Realizer
Bernd Bohnet1, Leo Wanner1,2, Simon Mille1, Alicia Burga1
1Department of Information and Communication Technologies
Pompeu Fabra University
2Institucio? Catalana de Recerca i Estudis Avanc?ats (ICREA)
{first-name.last-name}@upf.edu
Abstract
Most of the known stochastic sentence
generators use syntactically annotated
corpora, performing the projection to
the surface in one stage. However,
in full-fledged text generation, sentence
realization usually starts from semantic
(predicate-argument) structures. To be
able to deal with semantic structures,
stochastic generators require semantically
annotated, or, even better, multilevel an-
notated corpora. Only then can they
deal with such crucial generation issues as
sentence planning, linearization and mor-
phologization. Multilevel annotated cor-
pora are increasingly available for multi-
ple languages. We take advantage of them
and propose a multilingual deep stochastic
sentence realizer that mirrors the state-of-
the-art research in semantic parsing. The
realizer uses an SVM learning algorithm.
For each pair of adjacent levels of anno-
tation, a separate decoder is defined. So
far, we evaluated the realizer for Chinese,
English, German, and Spanish.
1 Introduction
Recent years saw a significant increase of inter-
est in corpus-based natural language generation
(NLG), and, in particular, in corpus-based (or
stochastic) sentence realization, i.e., that part of
NLG which deals with mapping of a formal (more
or less abstract) sentence plan onto a chain of in-
flected words; cf., among others, (Langkilde and
Knight, 1998; Oh and Rudnicky, 2000; Bangalore
and Rambow, 2000; Wan et al, 2009). The advan-
tage of stochastic sentence realization over tradi-
tional rule-based realization is mainly threefold:
(i) it is more robust, (ii) it usually has a signifi-
cantly larger coverage; (iii) it is per se language-
and domain-independent. Its disadvantage is that
it requires at least syntactically annotated corpora
of significant size (Bangalore et al, 2001). Given
the aspiration of NLG to start from numeric time
series or conceptual or semantic structures, syn-
tactic annotation even does not suffice: the cor-
pora must also be at least semantically annotated.
Up to date, deep stochastic sentence realization
was hampered by the lack of multiple-level an-
notated corpora. As a consequence, available
stochastic sentence generators either take syntac-
tic structures as input (and avoid thus the need for
multiple-level annotation) (Bangalore and Ram-
bow, 2000; Langkilde-Geary, 2002; Filippova
and Strube, 2008), or draw upon hybrid models
that imply a symbolic submodule which derives
the syntactic representation that is then used by
the stochastic submodule (Knight and Hatzivas-
siloglou, 1995; Langkilde and Knight, 1998).
The increasing availability of multilevel anno-
tated corpora, such as the corpora of the shared
task of the Conference on Computational Natu-
ral Language Learning (CoNLL), opens new per-
spectives with respect to deep stochastic sentence
generation?although the fact that these corpora
have not been annotated with the needs of genera-
tion in mind, may require additional adjustments,
as has been, in fact, in the case of our work.
98
In this paper, we present a Support Vector
Machine (SVM)-based multilingual dependency-
oriented stochastic deep sentence realizer that
uses multilingual corpora of the CoNLL ?09
shared task (Hajic?, 2009) for training. The sen-
tences of these corpora are annotated with shal-
low semantic structures, dependency trees, and
lemmata; for some of the languages involved,
they also contain morphological feature annota-
tions. The multilevel annotation allows us to take
into account all levels of representation needed
for linguistic generation and to model the pro-
jection between pairs of adjacent levels by sep-
arate decoders, which, in its turn, facilitates the
coverage of such critical generation tasks as sen-
tence planning, linearization, and morphologiza-
tion. The presented realizer is, in principle,
language-independent in that it is trainable on any
multilevel annotated corpus. In this paper, we dis-
cuss its performance for Chinese, English, Ger-
man, and Spanish.
The remainder of the paper is structured as fol-
lows. In Section 2, we discuss how the shallow se-
mantic annotation in the CoNLL ?09 shared task
corpora should be completed in order to be suit-
able for generation. Section 3 presents the train-
ing setup of our realizer. Section 4 shows the in-
dividual stages of sentence realization: from the
semantic structure to the syntactic structure, from
the syntactic structure to the linearized structure
and from the linearized structure to a chain of in-
flected word forms (if applicable for the language
in question). Section 5 outlines the experimental
set up for the evaluation of our realizer and dis-
cusses the results of this evaluation. In Section 6,
finally, some conclusions with respect to the char-
acteristics of our realizer and its place in the re-
search landscape are drawn.
The amount of the material which comes into
play makes it impossible to describe all stages
in adequate detail. However, we hope that the
overview provided in what follows still suffices to
fully assess our proposal.
2 Completing the Semantic Annotation
The semantic annotation of sentences in CoNLL
?09 shared task corpora follows the PropBank an-
notation guidelines (Palmer et al, 2005). Prob-
lematic from the viewpoint of generation is that
this annotation is not always a connected acyclic
graph. As a consequence, in these cases no valid
(connected) syntactic tree can be derived. The
most frequent cases of violation of the connectiv-
ity principle are not attached adjectival modifiers,
determiners, adverbs, and coordinations; some-
times, the verb is not connected with its argu-
ment(s). Therefore, prior to starting the training
procedure, the semantic annotation must be com-
pleted: non-connected adjectival modifiers must
be annotated as predicates with their syntactic
heads as arguments, determiners must be ?trans-
lated? into quantifiers, detached verbal arguments
must be connected with their head, etc.
Algorithm 1 displays the algorithm that com-
pletes the semantic annotations of the corpora.
Each sentence xi of the corpus I , with i =
1, . . . , |I|, is annotated with its dependency tree
yi and its shallow semantic graph si. The algo-
rithm traverses yi breath-first, and examines for
each node n in yi whether n?s corresponding node
in si is connected with the node corresponding to
the parent of n. If not, the algorithm connects both
by a directed labeled edge. The direction and the
label of the edge are selected consulting a look up
table in which default labels and the orientation
of the edges between different node categories are
specified.
Figure 1 shows the semantic representation of
a sample English sentence obtained after the ap-
plication of Algorithm 1. The solid edges are
the edges available in the original annotation; the
dashed edges have been introduced by the algo-
rithm. The edge labels ?A0? and ?A1? stand for
?first argument? and ?second argument? (of the
corresponding head), respectively, ?R-A0? for ?A0
realized as a relative clause?, and ?AM-MNR? for
?manner modifier?. As can be seen, 6 out of the
total of 14 edges in the complete representation
of this example have been added by Algorithm 1.
We still did not finish the formal evaluation of
the principal changes necessary to adapt the Prop-
Bank annotation for generation, nor the quality of
our completion algorithm. However, the need of
an annotation with generation in mind is obvious.
99
Algorithm 1: Complete semantic graph
//si is a semantic graph and yi a dependency tree
// si = ?Nsi , Lsi , Esi?, where Nsi is the set of nodes// Lsi the set of edge labels// Esi ? Ns ?Ns ? Ls is the set of edgesfor i? 1 to |I| // iteration over the training examples
let ry ? yi be the root node of the dependency tree
// initialization of the queue
nodeQueue ? children(ry)
while nodeQueue 6= ? do
ny ? removeFirst(nodeQueue)
// breath first: add nodes at the end of the queue
nodeQueue? nodeQueue ? children(ny)
nys ? sem(ny); pys ? sem(parent(ny))//get the semantic equivalents of ny and of its parent
if not exists path(nys , pys ) then
l? label(ny ,parent(ny))
ls ? look-up-sem-label(nys , pys , l)if look-up-sem-direction(nys , pys , ls) = ??? then// add the semantic edge
Es? Es ? (pys , nys , ls)else // direction of the edge ???
// add the semantic edge
Es? Es ? (nys , pys , ls)
3 Realizer Training Setup
Figure 2 shows the training setup of our realizer.
For each level of annotation, an SVM feature ex-
tractor and for each pair of adjacent levels of an-
notation, an SVM decoder is defined. The Sem-
Synt decoder constructs from a semantic graph
the corresponding dependency tree. The Synt-
Linearization decoder derives from a dependency
tree a chain of lemmata, i.e., determines the word
order within the sentence. The Linearization-
Morph decoder generates the inflected word form
for each lemma in the chain. Both the fea-
ture extractors and the decoders are language-
independent, which makes the realizer applicable
to any language for which multilevel-annotated
corpora are available.
To compute the score of the alternative realiza-
tions by each decoder, we apply MIRA (Margin
Infused Relaxed Algorithm) to the features pro-
vided by the feature extractors. MIRA is one
of the most successful large-margin training tech-
niques for structured data (Crammer et al, 2006).
It has been used, e.g., for dependency parsing,
semantic role labelling, chunking and tagging.
Since we have similar feature sets (of compara-
ble size) as those for which MIRA has proven to
work well, we assume that it will also perform
a an
A1
A1
A0
A1
A0
A1
A1
A1
A0
AM-MNR
A1
A2
A0
be
illustrate
but
Panama
that
substute
their
system
produce
gridlock
that
absurd
R-A0
Figure 1: Semantic representation of the sentence
But Panama illustrates that their substitute is a
system that produces an absurd gridlock. after
completion
well for sentence realization. Unfortunately, due
to the lack of space, we cannot present here the
instantiation of MIRA for all stages of our model.
For illustration, Algorithm 2 outlines it for mor-
phological realization.
The morphologic realization uses the minimal
string edit distance (Levenshtein, 1966) to map
lemmata to word forms. As input to the MIRA-
classifier, we use the lemmata of a sentence, its
dependency tree and the already ordered sentence.
The characters of the input strings are reversed
since most of the changes occur at the end of the
words and the string edit scripts work relatively
to the beginning of the string. For example, to
calculate the minimal string edit distance between
the lemma go and the form goes, both are first
reversed by the function compute-edit-dist and
then the minimal string edit script between og and
seog is computed. The resulting script is Ie0Is0.
It translates into the operations ?insert e at the po-
sition 0 of the input string? and ?insert s at the po-
sition 0?.
Before MIRA starts, we compute all mini-
mal edit distance scripts to be used as classes of
MIRA. Only scripts that occur more often than
twice are used. The number of the resulting edit
scripts is language-dependent; e.g., we get about
100
Semantic annotation
Syntactic annotation
Lineariz. annotation
Morphol. annotation
sem. feature extr.
synt. feature extr.
lineariz. feature extr.
morph. feature extr.
Sem-Synt DECODER
Synt-Lineariz. DECODER
Linear-Morph DECODER
SVM
Figure 2: Realizer training scenario setup
1500 scripts for English and 2500 for German.
The training algorithms typically perform 6 it-
erations (epochs) over the training examples. For
each training example, a minimal edit script is se-
lected. If this script is different from the gold
script, the features of the gold script are calcu-
lated and the weight vector of the SVM is adjusted
according to the difference between the predicted
vector and the gold feature vector. The classifi-
cation task consists then in finding the classifica-
tion script that maps the lemma to the correct word
form. For this purpose, the classifier scores each
of the minimal edit scripts according to the input,
choosing the one with the highest score.
4 Sentence Generation
Sentence generation that starts from a given se-
mantic structure as input consists in the applica-
tion of the previously trained SVM decoders in se-
quence in order to realize the following sequence
of mappings:
SemStr? SyntStr? LinearStr? Surface
4.1 Semantic Generation
Algorithm 3 shows the algorithm for semantic
generation, i.e., the derivation of a dependency
tree from a semantic structure. It is a beam search
that creates a maximum spanning tree. In the first
step, a seed tree consisting of one edge is built.
In each of the subsequent steps, this tree is ex-
tended by one node. For the decision, which node
Algorithm 2: Morphological realization
training with MIRA
// yi, li; yi is a dependency tree, li lemmatized sentence
script-list? {} //initialize the script-list
for i? 1 to |I| // iteration over the training examples
for l? 1 to |li| do//// iteration over the lemmata of li
lemmal? lower-case (li,l)
//ensure that all lemmata start with a lower case letter
script? compute-edit-dist-script(lemmal, form(li,l))
if script 6? script-list
script-list? script-list ? { script }
for k? 1 to E // E = number of traininig epochs
for i? 1 to |I| // iteration over the training examples
for l? 1 to |li| do
scriptp? predict-script(li,yi,l)
scriptg ? edit-dist-script(lemmal, form(li,l))
if scriptp 6= scriptg then
// update the weight vector v and the vector w, which
// averages over all collected weight vectors acc.
// to diff. of the predicted and gold feature vector
update w, v according to ?(?(scriptp), ?(scriptg))
//with ?(scriptp), ?(scriptg) as feature vectors of
//scriptp and scriptg , respectively
is to be attached next and to which node, we con-
sider the highest scoring options. This procedure
works well since nodes that are close in the se-
mantic structure are usually close in the syntactic
tree as well. Therefore subtrees that contain those
nodes are considered first.
Unlike the traditional n-gram based stochastic
realizers such as (Langkilde and Knight, 1998),
we use for the score calculation structured fea-
tures composed of the following elements: (i) the
lemmata, (ii) the distance between the starting
node s and the target node t, (iii) the direction
of the path (if the path has a direction), (iv) the
sorted bag of in-going edges labels without repi-
tition, (v) the path of edge labels between source
and target node.
The composed structured features are:
? label+dist(s, t)+dir
? label+dist(s, t)+lemmas+dir
? label+dist(s, t)+lemmat+dir
? label+dist(s, t)+lemmas+lemmat+dir
? label+dist(s, t)+bags+dir
? label+dist(s, t)+bagt+dir
? label+path(s, t)+dir
101
# word-pairs(w1,w2) # n-grams
1 labelw1+labelw2 13 PoS1+PoS2+PoS32 labelw1+lemma1 14 PoS1+PoS2+PoS3+dist3 labelw1+lemma2 15 lemma1+lemma2+lemma34 labelw2+lemma1 16 lemma1+lemma2+lemma3+dist5 labelw2+lemma2 17 lemma1+lemma3+head(w1,w2,w3)6 PoS1+PoS2 18 lemma1+lemma3+head(w1,w2,w3)+dist7 PoS1+PoS2+head(w1,w2) 19 label1+label2+label3+head(w1,w2,w3)8 labelw1+labelw2+PoS1+head(w1,w2) 20 label1+label2+label3+head(w1,w2,w3)+dist9 labelw1+labelw2+PoS2+head(w1,w2) 21 label1+label2+label3+lemma1+PoS2+head(w1,w2,w3)10 labelw1+labelw2+PoS1+PoS2+head(w1,w2) 22 label1+label2+label3+lemma1+PoS2+head(w1,w2,w3)+dist11 labelw1+labelw2+PoS1+#children2+head(w1,w2) 23 label1+label2+label3+lemma2+PoS1+head(w1,w2,w3)12 labelw1+labelw2+PoS2+#children1+head(w1,w2) 24 label1+label2+label3+lemma2+PoS1+head(w1,w2,w3)+dist
# global features for constituents
25 if |constituent| > 1 then label1st+labellast+labellast?1+PoSfirst+PoSlast+PoShead
26 if |constituent| > 2 then label1st+label2d+label3d+PoSlast+PoSlast?1+PoShead+contains-?
27 if |constituent| > 2 then label1st+label2d+label3d+PoSlast+PoSlast?1+lemmahead+contains-?
28 if |constituent| > 3 then PoS1st+PoS2d+PoS3d+PoS4th+PoSlast+labelhead+contains-?+pos-head
29 if |constituent| > 3 then PoSlast+PoSlast?1+PoSlast?2+PoSlast?3+PoSfirst+labelhead+contains-?+pos-head30 PoSfirst+PoSlast+lemmafirst+lemmalast+lemmahead+contains-?+pos-head
Table 1: Feature schemas used for linearization (labelw is the label of the in-going edge to a word w in
the dependency tree; lemmaw is the lemma of w, and PoSw is the part-of-speech tag of w; head(w1,w2,
. . . ) is a function which is 1 if w1 is the head, 2 if w2 is the head, etc. and else 0; dist is the position
within the constituent; contains-? is a boolean value which is true if the sentence contains a question
mark and false otherwise; pos-head is the position of the head in the constituent)
4.2 Dependency Tree Linearization
Since we use unordered dependency trees as syn-
tactic structures, our realizer has to find the opti-
mal linear order for the lexemes of each depen-
dency tree. Algorithm 4 shows our linearization
algorithm. To order the dependency tree, we use a
one classifier-approach for all languages?in con-
trast to, e.g., Filippova and Strube (2009), who use
a two-classifier approach for German.1
The algorithm is again a beam search. It starts
with an elementary list for each node of the depen-
dency tree. Each elementary list is first extended
by the children of the node in the list; then, the
lists are extended stepwise by the children of the
newly added nodes. If the number of lists during
this procedure exceeds the threshold of 1000, the
lists are sorted in accordance with their score, and
the first 1000 are kept. The remaining lists are
removed. Afterwards, the score of each list is ad-
justed according to a global score function which
takes into account complex features such as the
first word of a consitutent, last word, the head, and
the edge label to the head (cf. Table 1 for the list
of the features). Finally, the nodes of the depen-
1We decided to test at this stage of our work a uniform
technology for all languages, even if the idiosyncrasies of
some languages may be handled better by specific solutions.
dency tree are ordered with respect to the highest
ranked lists.
Only in a very rare case, the threshold of the
beam search is exceeded. Even with a rich feature
set, the procedure is very fast. The linearization
takes about 3 milliseconds in average per depen-
dency tree on a computer with a 2.8 Ghz CPU.
4.3 Morphological Realization
The morphological realization algorithm selects
the edit script in accordance with the highest score
for each lemma of a sentence obtained during
training (see Algorithm 2 above) and applies then
the scripts to obtain the word forms; cf. Algo-
rithm 5.
Table 2 lists the feature schemas used for mor-
phological realization.
5 Experiments
To evaluate the performance of our realizer, we
carried out experiments on deep generation of
Chinese, English, German and Spanish, starting
from CoNLL ?09 shared task corpora. The size of
the test sets is listed in Table 3.2
2As in (Langkilde-Geary, 2002) and (Ringger et al,
2004), we used Section 23 of the WSJ corpus as test set for
English.
102
Algorithm 3: Semantic generation
//si, y semantic graph and its dependency tree
for i? 1 to |I| // iteration over the training examples
// build an initial tree
for all n1 ? si do
trees? {} // initialize the constructed trees list
for all n2 ? si do
if n1 6= n2 then
for all l ? dependency-labels do
trees = trees ? {(synt(n1),synt(n2),l)}
trees? sort-trees-descending-to-score(trees)
trees? look-forward(1000,sublist(trees,20))
//assess at most 1000 edges of the 20 best trees
tree? get-best-tree-due-to-score(trees)
(s,t,l)? first-added-edge(tree)
// create the best tree
best-tree? (s,t,l)
// compute the nodes that still need to be attached
rest? nodes(si) - {s, t}
while rest 6= ? do
trees? look-forward(1000,best-tree,rest)
tree? get-best-tree-due-to-score(trees)
(s,t,l)? first-added-edge(tree)
best-tree? best-tree ? { (s,t,l) }
if (root(s,best-tree)) then rest? rest - {s}
else rest? rest - {t}
The performance of both the isolated stages and
the realizer as a whole has been assessed.
5.1 Evaluation Metrics
In order to measure the correctness of the se-
mantics to syntax mapping, we use the unlabeled
and labeled attachment score as it commonly used
in dependency parsing. The labeled attachment
score (LAS) is the proportion of tokens that are as-
signed both the correct head and the correct edge
label. The unlabeled attachment score (ULA) is
the proportion of correct tokens that are assigned
the correct head.
To assess the quality of linearization, we use
three different evaluation metrics. The first metric
is the per-phrase/per-clause accuracy (acc snt.),
which facilitates the automatic evaluation of re-
sults:
acc = correct constituentsall constituents
As second evaluation metric, we use a metric
related to the edit distance:
di = 1? mtotal number of words
(with m as the minimum number of deletions
combined with insertions to obtain the correct or-
der (Ringger et al, 2004)).
Algorithm 4: Dependency tree lineariza-
tion
//yi a dependency tree
for i? 1 to |I| // iteration over the training examples
// iterate over all nodes of the dependency tree yi
for n? 1 to |yi| do
subtreen? children(n) ? {n}
ordered-listsn? {} // initialize
for all m ? subtreen do
beam? {}
for all l ? ordered-lists do
beam? beam ? { append(clone(l),m)}
for all l ? ordered-lists do
score(l)? compute-score-for-word-list(l)
sort-lists-descending-to-score(beam,score)
if | beam | > beam-size then
beam? sublist(0,1000,beam)
ordered-listsn? beam
scoreg(l)? score(l) + compute-global-score(l)
sort-lists-descending-in-score(beam,scoreg)
Algorithm 5: Morphological realization
// yi a dependency tree, and li an ordered list of lemmata
for l? 1 to |li| do
scriptp? predict-script(li,yi,l)
forml? apply-edit-dist-script(lemmal, scriptp)
To be able to compare our results with (He et
al., 2009) and (Ringger et al, 2004), we use the
BLEU score as a third metric.
For the asessment of the quality of the word
form generation, we use the accuracy score. The
accuracy is the ratio between correctly generated
word forms and the entire set of generated word
forms.
For the evaluation of the sentence realizer as a
whole, we use the BLEU metric.
5.2 Experimental Results
Table 4 displays the results obtained for the iso-
lated stages of sentence realization and of the real-
ization as a whole, with reference to a baseline and
to some state-of-the-art works. The baseline is
the deep sentence realization over all stages start-
ing from the original semantic annotation in the
CoNLL ?09 shared task corpora.
Note, that our results are not fully comparable
with (He et al, 2009; Filippova and Strube, 2009)
and (Ringger et al, 2004), respectively, since the
data are different. Furthermore, Filippova and
Strube (2009) linearize only English sentences
103
# features
1 es+lemma
2 es+lemma+m.feats
3 es+lemma+m.feats+POS
4 es+lemma+m.feats+POS+position
5 es+lemma+(lemma+1)+m.feats
6 es+lemma+(lemma+1)+POS
7 es+lemma+(m.feats-1)+(POS-1)
8 es+lemma+(m.feats-1)+(POS-1)+position
9 es+m.feats+(m.feats-1)
10 es+m.feats+(m.feats+1)
11 es+lemma+(m.feats-1)
12 es+m.feats+(m.feats-1)+(m.feats-2)
13 es+m.feats+POS
14 es+m.feats+(m.feats+1)
15 es+m.feats+(m.feats+1)+lemma
16 es+m.feats
17 es+e0+e1+m.feats
18 es+e0+e1+e2+m.feats
19 es+e0+e1+e2+e3+m.feats
20 es+e0+e1+e2+e3+e4+m.feats
21 es+e0+m.feats
Table 2: Feature schemas used for morphological
realization
Chinese English German Spanish
2556 2400 2000 1725
Table 3: The number of sentences in the test sets
used in the experiments
that do not contain phrases that exceed 20,000 lin-
earization options?which means that they filter
out about 1% of the phrases.
For Spanish, to the best of our knowledge, no
linearization experiments have been carried out so
far. Therefore, we cannot contrast our results with
any reference work.
As far as morphologization is concerned, the
performance achieved by our realizer for English
is somewhat lower than in (Minnen et al, 2001)
(97.8% vs. 99.8% of accuracy). Note, however,
that Minnen et al describe a combined analyzer-
generator, in which the generator is directly de-
rived from the analyzer, which makes both ap-
proaches not directly comparable.
5.3 Discussion
The overall performance of our SVM-based deep
sentence generator ranges between 0.611 (for Ger-
man) and 0.688 (for Chinese) of the BLEU score.
HALogen?s (Langkilde-Geary, 2002) scores range
between 0.514 and 0.924, depending on the com-
pleteness of the input. The figures are not directly
comparable since HALogen takes as input syntac-
tic structures. However, it gives us an idea where
our generator is situated.
Traditional linearization approaches are rule-
based; cf., e.g., (Bro?ker, 1998; Gerdes and Ka-
hane, 2001; Duchier and Debusmann, 2001), and
(Bohnet, 2004). More recently, statistic language
models have been used to derive word order, cf.
(Ringger et al, 2004; Wan et al, 2009) and (Fil-
ippova and Strube, 2009). Because of its partially
free order, which is more difficult to handle than
fixed word order, German has often been worked
with in the context of linearization. Filippova and
Strube (2009) adapted their linearization model
originally developed for German to English. They
use two classifiers to determine the word order
in a sentence. The first classifier uses a trigram
LM to order words within constituents, and the
second (which is a maximum entropy classifier)
determines the order of constituents that depend
on a finite verb. For English, we achieve with
our SVM-based classifier a better performance.
As mentioned above, for German, Filippova and
Strube (2009)?s two classifier approach pays off
because it allows them to handle non-projective
structures for the Vorfeld within the field model.
It is certainly appropriate to optimize the perfor-
mance of the realizer for the languages covered in
a specific application. However, our goal has been
so far different: to offer an off-the-shelf language-
independent solution.
The linearization error analysis, first of all of
German and Spanish, reveals that the annotation
of coordinations in corpora of these languages as
?X ? and/or/. . .? Y? is a source of errors. The
?linear? annotation used in the PropBank (?X ?
and/or/. . .? Y?) appears to facilitate higher qual-
ity linearization. A preprocessing stage for au-
tomatic conversion of the annotation of coordi-
nations in the corpora would have certainly con-
tributed to a higher quality. We refrained from
doing this because we did not want to distort the
figures.
The morphologization error analysis indicates
a number of error sources that we will address
in the process of the improvement of the model.
Among those sources are: quotes at the beginning
of a sentence, acronyms, specific cases of start-
ing capital letters of proper nouns (for English and
Spanish), etc.
104
Chinese English German Spanish
Semantics-Syntax (ULA/LAS) 95.71/86.29 94.77/89.76 95.46/82.99 98.39/93.00
Syntax-Topology (di/acc) 0.88/64.74 0.91/74.96 0.82/50.5 0.83/52.77
Syntax-Topology (BLEU) 0.85 0.894 0.735 0.78
Topology-Morphology (accuracy=correct words/all words) ? 97.8 97.49 98.48
All stages (BLEU) 0.688 0.659 0.611 0.68
Baseline (BLEU) 0.12 0.18 0.11 0.14
Syntax-Topology (He et al, 2009) (di/acc) 0.89/? ? ? ?
Syntax-Topology (He et al, 2009) (BLEU) 0.887 ? ? ?
Syntax-Topology (Filippova and Strube, 2009) (di/acc) ? 0.88/67 0.87/61 ?
Syntax-Topology (Ringger et al, 2004) (BLEU) ? 0.836 ? ?
Table 4: Quality figures for the isolated stages of deep sentence realization and the complete process.
As far as the contrastive evaluation of the qual-
ity of our morphologization stage is concerned,
it is hampered by the fact that for the traditional
manually crafted morphological generators, it is
difficult to find thorough quantitative evaluations,
and stochastic morphological generators are rare.
As already repeatedly pointed out above, so far
we intentionally refrained from optimizing the in-
dividual realization stages for specific languages.
Therefore, there is still quite a lot of room for im-
provement of our realizer when one concentrates
on a selected set of languages.
6 Conclusions
We presented an SVM-based stochastic deep mul-
tilingual sentence generator that is inspired by the
state-of-the-art research in semantic parsing. It
uses similar techniques and relies on the same re-
sources. This shows that there is a potential for
stochastic sentence realization to catch up with
the level of progress recently achieved in parsing
technologies.
The generator exploits recently available
multilevel-annotated corpora for training. While
the availability of such corpora is a condition for
deep sentence realization that starts, as is usually
the case, from semantic (predicate-argument)
structures, we discovered that current annotation
schemata do not always favor generation such
that additional preprocessing is necessary. This
is not surprising since stochastic generation is a
very young field. An initiative of the generation
community would be appropriate to influence
future multilevel annotation campaigns or to feed
back the enriched annotations to the ?official?
resources.3
The most prominent features of our generator
are that it is per se multilingual, it achieves an ex-
tremely broad coverage, and it starts from abstract
semantic structures. The last feature allows us to
cover a number of critical generation issues: sen-
tence planning, linearization and morphological
generation. The separation of the semantic, syn-
tactic, linearization and morphological levels of
annotation and their modular processing by sep-
arate SVM decoders also facilitates a subsequent
integration of other generation tasks such as re-
ferring expression generation, ellipsis generation,
and aggregation. As a matter of fact, this gen-
erator instantiates the Reference Architecture for
Generation Systems (Mellish et al, 2006) for lin-
guistic generation.
A more practical advantage of the presented
deep stochastic sentence generator (as, in prin-
ciple, of all stochastic generators) is that, if
trained on a representative corpus, it is domain-
independent. As rightly pointed out by Belz
(2008), traditional wide coverage realizers such
as KPML (Bateman et al, 2005), FUF/SURGE
(Elhadad and Robin, 1996) and RealPro (Lavoie
and Rambow, 1997), which were also intended
as off-the-shelf plug-in realizers still tend to re-
quire a considerable amount of work for integra-
tion and fine-tuning of the grammatical and lexical
resources. Deep stochastic sentence realizers have
the potential to become real off-the-shelf modules.
Our realizer is freely available for download at
http://www.recerca.upf.edu/taln.
3We are currently working on a generation-oriented mul-
tilevel annotation of corpora for a number of languages. The
corpora will be made available to the community.
105
Acknowledgments
Many thanks to the three anonymous reviewers for
their very valuable comments and suggestions.
References
Bangalore, S. and O. Rambow. 2000. Exploiting a
Probabilistic Hierarchical Model for Generation. In
Proceedings of COLING ?00, pages 42?48.
Bangalore, S., J. Chen, and O. Rambow. 2001. Impact
of Quality and Quantity of Corpora on Stochastic
Generation. In Proceedings of the EMNLP Confer-
ence, pages 159?166.
Bateman, J.A., I. Kruijff-Korbayova?, and G.-J. Krui-
jff. 2005. Multilingual Resource Sharing Across
Both Related and Unrelated Languages: An Imple-
mented, Open-Source Framework for Practical Nat-
ural Language Generation. Research on Language
and Computation, 15:1?29.
Belz, A. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
Bohnet, B. 2004. A graph grammar approach to map
between dependency trees and topological models.
In Proceedings of the IJCNLP, pages 636?645.
Bro?ker, N. 1998. Separating Surface Order and Syn-
tactic Relations in a Dependency Grammar. In Pro-
ceedings of the COLING/ACL ?98.
Crammer, K., O. Dekel, S. Shalev-Shwartz, and
Y. Singer. 2006. Online Passive-Aggressive Al-
gorithms. Journal of Machine Learning Research,
7:551?585.
Duchier, D. and R. Debusmann. 2001. Topological de-
pendency trees: A constraint-based account of lin-
ear precedence. In Proceedings of the ACL.
Elhadad, M. and J. Robin. 1996. An overview of
SURGE: A reusable comprehensive syntactic real-
ization component. Technical Report TR 96-03,
Department of Mathematics and Computer Science,
Ben Gurion University.
Filippova, K. and M. Strube. 2008. Sentence fusion
via dependency graph compression. In Proceedings
of the EMNLP Conference.
Filippova, K. and M. Strube. 2009. Tree lineariza-
tion in English: Improving language model based
approaches. In Proceedings of the NAACL ?09 and
HLT, Short Papers, pages 225?228.
Gerdes, K. and S. Kahane. 2001. Word order in Ger-
man: A formal dependency grammar using a topo-
logical hierarchy. In Proceedings of the ACL.
Hajic?, J. et al 2009. The CoNLL-2009 Shared Task:
Syntactic and Semantic Dependencies in Multiple
Languages. In Proceedings of the CoNLL.
He, W., H. Wang, Y. Guo, and T. Liu. 2009. De-
pendency based chinese sentence realization. In
Proceedings of the ACL and of the IJCNLP of the
AFNLP, pages 809?816.
Knight, K. and V. Hatzivassiloglou. 1995. Two-level,
many paths generation. In Proceedings of the ACL.
Langkilde, I. and K. Knight. 1998. Generation that
exploits corpus-based statistical knowledge. In Pro-
ceedings of the COLING/ACL, pages 704?710.
Langkilde-Geary, I. 2002. An empirical verification
of coverage and correctness for a general-purpose
sentence generator. In Proceedings of the Second
INLG Conference, pages 17?28.
Lavoie, B. and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th Conference on ANLP.
Levenshtein, V.I. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics, 10:707?710.
Mellish, C., D. Scott, L. Cahill, D. Paiva, R. Evans, and
M. Reape. 2006. A reference architecture for natu-
ral language generation systems. Natural Language
Engineering, 12(1):1?34.
Minnen, G., J. Carroll, and D. Pearce. 2001. Ap-
plied morphological processing for English. Nat-
ural Language Engineering, 7(3):207?223.
Oh, A.H. and A.I. Rudnicky. 2000. Stochastic lan-
guage generation for spoken dialogue systems. In
Proceedings of the ANL/NAACL Workshop on Con-
versational Systems, pages 27?32.
Palmer, M., D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?105.
Ringger, E., M. Gamon, R.C. Moore, D. Rojas,
M. Smets, and S. Corston-Oliver. 2004. Linguis-
tically informed statistical models of constituent
structure for ordering in sentence realization. In
Proceedings of COLING, pages 673?679.
Wan, S., M. Dras, Dale R., and C. Paris. 2009. Im-
proving Grammaticality in Statistical Sentence Gen-
eration: Introducing a Dependency Spanning Tree
Algorithm with an Argument Satisfaction Model. In
Proceedings of the EACL ?09, pages 852?860.
106
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1402?1413, Dublin, Ireland, August 23-29 2014.
Deep-Syntactic Parsing
Miguel Ballesteros
1
, Bernd Bohnet
2
, Simon Mille
1
, Leo Wanner
1,3
1
Natural Language Processing Group, Pompeu Fabra University, Barcelona, Spain
2
School of Computer Science, University of Birmingham, United Kingdom
3
Catalan Institute for Research and Advanced Studies (ICREA)
1,3
{name.lastname}@upf.edu
2
bohnetb@cs.bham.ac.uk
Abstract
?Deep-syntactic? dependency structures that capture the argumentative, attributive and coordi-
native relations between full words of a sentence have a great potential for a number of NLP-
applications. The abstraction degree of these structures is in-between the output of a syntactic
dependency parser (connected trees defined over all words of a sentence and language-specific
grammatical functions) and the output of a semantic parser (forests of trees defined over indi-
vidual lexemes or phrasal chunks and abstract semantic role labels which capture the argument
structure of predicative elements, dropping all attributive and coordinative dependencies). We
propose a parser that delivers deep syntactic structures as output.
1 Introduction
Surface-syntactic structures (SSyntSs) as produced by data-driven syntactic dependency parsers are per
force idiosyncratic in that they contain governed prepositions, determiners, support verb constructions
and language-specific grammatical functions such as, e.g., SBJ, OBJ, PRD, PMOD, etc. (Johansson and
Nugues, 2007). For many NLP-applications, including machine translation, paraphrasing, text simpli-
fication, etc., such a high idiosyncrasy is obstructive because of the recurrent divergence between the
source and the target structures. Therefore, the use of more abstract ?syntactico-semantic? structures
seems more appropriate. Following Mel??cuk (1988), we call these structures deep-syntactic structures
(DSyntSs). DSyntSs are situated between SSyntSs and PropBank- (Palmer et al., 2005) or Semantic
Frame-like structures (Fillmore et al., 2002). Compared to SSyntSs, they have the advantage to ab-
stract from language-specific grammatical idiosyncrasies. Compared to PropBank and Semantic Frame
stuctures, they have the advantage to be connected and complete, i.e., capture all argumentative, attribu-
tive and coordinative dependencies between the meaningful lexical items of a sentence, while PropBank
and Semantic Frame structures are not always connected, may contain either individual lexical items or
phrasal chunks as nodes, and discard attributive and coordinative relations (be they within the chunks or
sentential). In other words, they constitute incomplete structures that drop not only idiosyncratic, func-
tional but also meaningful elements of a given sentence and often contain dependencies between chunks
rather than individual tokens. Therefore, we propose to put on the research agenda the task of deep-
syntactic parsing and show how a DSyntS is obtained from a SSynt dependency parse using data-driven
tree transduction in a pipeline with a syntactic parser.
1
In Section 2, we introduce SSyntSs and DSyntSs
and discuss the fundamentals of SSyntS?DSyntS transduction. Section 3 describes the experiments that
we carried out on Spanish material, and Section 4 discusses their outcome. Section 5 summarizes the
related work, before in Section 6 some conclusions and plans for future work are presented.
2 Fundamentals of SSyntS?DSyntS transduction
Before we set out to discuss the principles of the SSyntS?DSynt transduction, we must specify the
DSyntSs and SSyntSs as used in our experiments.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The term ?tree transduction? is used in this paper in the sense of Rounds (1970) and Thatcher (1970) to denote an extension
of finite state transduction (Aho, 1972) to trees.
1402
2.1 Defining SSyntS and DSyntS
SSyntSs and DSyntSs are directed, node- and edge-labeled dependency trees with standard feature-value
structures (Kasper and Rounds, 1986) as node labels and dependency relations as edge labels.
The features of the node labels in SSyntSs are lex
ssynt
, and ?syntactic grammemes? of the value of
lex
ssynt
, i.e., number, gender, case, definiteness, person for nouns and tense, aspect, mood and voice for
verbs. The value of lex
ssynt
can be any (either full or functional) lexical item; in graphical representations
of SSyntSs, usually only the value of lex
ssynt
is shown. The edge labels of a SSyntS are grammatical
functions ?subj?, ?dobj?, ?det?, ?modif?, etc. In other words, SSyntSs are syntactic structures of the kind
as encountered in the standard dependency treebanks; cf., e.g., dependency version of the Penn TreeBank
(Johansson and Nugues, 2007) for English, Prague Dependency Treebank for Czech (Haji?c et al., 2006),
Ancora for Spanish (Taul?e et al., 2008), Copenhagen Dependency Treebank for Danish (Buch-Kromann,
2003), etc. In formal terms that we need for the outline of the transduction below, a SSyntS is defined as
follows:
Definition 1 (SSyntS) An SSyntS of a language L is a quintuple T
SS
= ?N,A, ?
l
s
?n
, ?
r
s
?a
, ?
n?g
?
defined over all lexical items L of L, the set of syntactic grammemes G
synt
, and the set of grammatical
functions R
gr
, where
? the set N of nodes and the set A of directed arcs form a connected tree,
? ?
l
s
?n
assigns to each n ? N an l
s
? L,
? ?
r
s
?a
assigns to each a ? A an r ? R
gr
, and
? ?
n?g
assigns to each ?
l
s
?n
(n) a set of grammemes G
t
? G
synt
.
The features of the node labels in DSyntSs as worked with in this paper are lex
dsynt
and ?seman-
tic grammemes? of the value of lex
dsynt
, i.e., number and determination for nouns and tense, aspect,
mood and voice for verbs.
2
In contrast to lex
ssynt
in SSyntS, DSyntS?s lex
dsynt
can be any full, but
not a functional lexeme. In accordance with this restriction, in the case of look after a person, AFTER
will not appear in the corresponding DSyntS; it is a functional (or governed) preposition (so are TO or
BY, in Figure 1).
3
In contrast, AFTER in leave after the meeting is a full lexeme; it will remain in the
DSyntS because there it has its own meaning of ?succession in time?. The edge labels of a DSyntS are
language-independent ?deep-syntactic? relations I,. . . ,VI, ATTR, COORD, APPEND. ?I?,. . . ,?VI? are
argument relations, analogous to A0, A1, etc. in the PropBank annotation. ?ATTR? subsumes all (cir-
cumstantial) ARGM-x PropBank relations as well as the modifier relations not captured by the PropBank
and FrameNet annotations. ?COORD? is the coordinative relation as in: John-COORD?and-II?Mary,
publish-COORD?or-II?perish, and so on. APPEND subsumes all parentheticals, interjections, direct
addresses, etc., as, e.g., in Listen, John!: listen-APPEND?John. DSyntSs thus show a strong similarity
with PropBank structures, with four important differences: (i) their lexical labels are not disambiguated;
(ii) instead of circumstantial thematic roles of the kind ARGM-LOC, ARGM-DIR, etc. they use a unique
ATTR relation; (iii) they capture all existing dependencies between meaningful lexical nodes; and (iv)
they are connected.
4
A number of other annotations have resemblance with DSyntSs; cf. (Ivanova et al.,
2012) for an overview of deep dependency structures. Formally, a DSyntS is defined as follows:
Definition 2 (DSyntS) An DSyntS of a language L is a quintuple T
DS
= ?N,A, ?
l
s
?n
, ?
r
s
?a
, ?
n?g
?
defined over the full lexical items L
d
of L, the set of semantic grammemes G
sem
, and the set of deep-
syntactic relations R
dsynt
, where
? the set N of nodes and the set A of directed arcs form a connected tree,
? ?
l
s
?n
assigns to each n ? N an l
s
? L
d
,
? ?
r
s
?a
assigns to each a ? A an r ? R
dsynt
, and
? ?
n?g
assigns to each ?
l
s
?n
(n) a set of grammemes G
t
? G
sem
.
Consider in Figure 1 an example for an SSyntS and its corresponding DSyntS.
2
Most of the grammemes have a semantic and a surface interpretation; see (Mel??cuk, 2013).
3
Functional lexemes also include auxiliaries (e.g. HAVE, or BE when it is not a copula), and definite and indefinite deter-
miners (THE, A); see Figure 1).
4
Our DSyntSs are thus DSyntSs as used in the Meaning-Text Theory (Mel??cuk, 1988), only that our DSyntSs do not
disambiguate lexical items and do not use lexical functions (Mel??cuk, 1996).
1403
(a) almost 1.2 million jobs have been created by the state thanks to their endeavours
restr
quant
quant
subj
analyt perf
analyt pass
agent
adv
prepos
det
obl obj
prepos
det
(b) almost 1.2 million job create state thanks their endeavour
ATTR
ATTR
ATTR
II
I
ATTR
II
I
Figure 1: An SSyntS (a) and its corresponding DSyntS (b)
2.2 Fleshing out the SSyntS?DSyntS transduction
It is clear that the SSyntS and DSyntS of the same sentence are not isomorphic. The following corre-
spondences between the SSyntS S
ss
and DSyntS S
ds
of a sentence need to be taken into account during
SSyntS?DSyntS transduction:
(i) a node in S
ss
is a node in S
ds
;
(ii) a relation in S
ss
corresponds to a relation in S
ds
;
(iii) a fragment of the S
ss
tree corresponds to a single node in S
ds
;
(iv) a relation with a dependent node in S
ss
is a grammeme in S
ds
;
(v) a grammeme in S
ss
is a grammeme in S
ds
;
(vi) a node in S
ss
is conflated with another node in S
ds
; and
(vii) a node in S
ds
has no correspondence in S
ss
.
The grammeme correspondences (iv) and (v) and the ?pseudo? correspondences in (vi) and (vii)
5
are
few or idiosyncratic and are best handled in a rule-based post-processing stage. The main task of the
SSyntS?DSyntS transducer is thus to cope with the correspondences (i)?(iii). For this purpose, we can
view both SSyntS and DSyntS as vectors indexed in terms of two-dimensional matrices I = N ?N (N
being the set of nodes of a given tree 1, . . . ,m), with I(i, j) = ?(n
i
, n
j
), if n
i
, n
j
? N and (n
i
, n
j
) ? A
and I(i, j) = 0 otherwise (where ??(n
i
, n
j
)? is the function that assigns to an edge a relation label and
i, j = 1, . . . ,m; i 6= j are nodes of the tree). That is, for a given SSyntS, the matrix I(i, j) contains in
the cells (i, j), i, j = 1, . . . ,m, the names of the SSynt-relations between the nodes n
i
and n
j
, and ?0?
otherwise, while for a given DSyntS, the cells of its matrix I
D
contain DSyntS-relations.
Starting from the matrix I
S
of a given SSyntS, the task is therefore to obtain the matrix I
D
of the
corresponding DSyntS, that is, to identify correspondences between i/j, (i, j) and groups of (i, j) of
I
S
with i
?
/j
?
and (i
?
, j
?
) of I
D
; see (i)?(iii) above. In other words, the task consists in identifying and
removing all functional lexemes, and attach correctly the remaining nodes between them.
6
As a ?token chain?surface-syntactic tree? projection, this task can be viewed as a classification task.
However, while the former is isomorphic, we know that the SSyntS?DSyntS projection is not. In order
to approach the task to an isomorphic projection (and thus simplify its modelling), it is convenient to
interpret SSyntS and the targeted DSyntS as collections of hypernodes:
Definition 3 (Hypernode) Given a SSyntS S
s
with its index matrix I
S
(a DSyntS S
d
with its index matrix
I
D
), a node partition p (with |p |? 1) of I
S
(I
D
) is a hypernode h
s
i
(h
d
i
) iff p corresponds to a partition
p
?
(with |p
?
|? 1) of S
d
(S
s
).
In this way, the SSyntS?DSyntS correspondence boils down to a correspondence between individual
hypernodes and between individual arcs, and the transduction embraces the following three (classifica-
tion) subtasks: 1. Hypernode identification, 2. DSynt tree construction, and 3. DSynt arc labeling, which
are completed by a post-processing stage.
5
(vi) covers, e.g., reflexive verb particles such as se in Spanish, which are conflated in the DSyntS with the verb:
se?aux refl dir-conocer vs. CONOCERSE ?know each other?; (vii) covers, e.g., the zero subject in pro-drop languages (which
is absent in the SSyntS and present in the DSyntS).
6
What is particularly challenging is the identification of functional prepositions: based on the information found in the
corpus only, our system must decide if a given preposition is a full or a functional lexeme. That is, we do not resort to any
external lexical resources.
1404
1. Hypernode identification. The hypernode identification consists of a binary classification of the
nodes of a given SSyntS as nodes that form a hypernode of cardinality 1 (i.e., nodes that have a one-
to-one correspondence to a node in the DSyntS) vs. nodes that form part of a hypernode of cardinality
> 1. In practice, hypernodes of type one will be formed by: 1) noun nodes that do not govern determiner
or functional preposition nodes, 2) full verb nodes that are not governed by any auxiliary verb nodes
and that do not govern any functional preposition node, adjective nodes, adverbial nodes, and semantic
preposition nodes. Hypernodes of type two will be formed by: 1) noun nodes + determiner / func-
tional preposition nodes they govern, 2) verb nodes + auxiliary nodes they are governed by + functional
preposition nodes they govern.
2. DSynt tree reconstruction. The outcome of the hypernode identification stage is thus the set H
s
=
H
s
|p|=1
?H
s
|p|>1
of hypernodes of two types. With this set at hand, we can define an isomorphy function
? : H
s
? H
d
|p|=1
(with h
d
? H
d
|p|=1
consisting of n
d
? N
ds
, i.e., the set of nodes of the target DSyntS).
? is the identity function for h
s
? H
s
|p|=1
. For h
s
? H
s
|p|>1
, ? maps the functional nodes in h
s
onto
grammemes (attribute-value pairs) of the lexically meaningful node in h
d
and identifies the lexically
meaningful node as head. Some of the dependencies of the obtained nodes n
d
? N
ds
can be recovered
from the dependencies of their sources. Due to the projection of functional nodes to grammemes (which
can be also seen as node removal), some dependencies will be also missing and must be introduced.
Algorithm 1 recalculates the dependencies for the target DSyntS S
d
, starting from the index matrix I
S
of
SSyntS S
s
to obtain a connected tree.
Algorithm 1: DSyntS tree reconstruction
for ?n
i
? N
d
do
if ?n
j
: (n
j
, n
i
) ? S
s
? ?(n
j
) ? N
d
then
(n
j
, n
i
)? S
d
// the equivalent of the head node of n
i
is included in DSyntS
else if ?n
j
, n
a
: (n
j
, n
i
) ? S
s
? ?(n
j
) 6? N
d
?
?(n
a
) ? N
d
then
//n
a
is the first ancestor of n
j
that has an equivalent in DSyntS
//the equivalent of the head node of n
i
is not included in DSyntS, but the ancestor n
a
is
(n
a
, n
i
)? S
d
else
//the equivalent of the head node of n
i
is not included in DSyntS, but several ancestors of it are
n
b
:= BestHead(n
i
, S
s
, S
d
)
(n
b
, n
i
)? S
d
endfor
BestHead recursively ascends S
s
from a given node n
i
until it encounters one or several head nodes
n
d
? N
ds
. In case of several encountered head nodes, the one which governs the highest frequency
dependency is returned.
3. Label Classification. The tree reconstruction stage produces a ?hybrid? connected dependency tree
S
s?d
with DSynt nodes N
ds
, and arcs A
s
labelled by SSynt relation labels, i.e., an index matrix we
can denote as I
?
, whose cells (i, j) contain SSynt labels for all n
i
, n
j
? N
ds
: (n
i
, n
j
) ? A
s
and
?0? otherwise. The next and last stage of SSynt-to-DSyntS transduction is thus the projection of SSynt
relation labels of S
s?d
to their corresponding DSynt labels, or, in other words, the mapping of I
?
to I
D
of the target DSyntS.
4. Postprocessing. As mentioned in Section 2, there is a limited number of idiosyncratic correspon-
dences between elements of SSyntS and DSyntS (the correspondences (iv?vii) which can be straight-
forwardly handled by a rule-based postprocessor because (a) they are non-ambiguous, i.e., a ? b, c ?
d ? a = b ? c = d, and (b) they are few. Thus, only determiners and auxiliaries in SSyntS map onto a
grammeme in DSyntS, both SSyntS and DSyntS count with less than a dozen grammemes, etc.
3 Experiments
In order to validate the outlined SSyntS?DSyntS transduction and to assess its performance in combi-
nation with a surface dependency parser, i.e., starting from plain sentences, we carried out a number of
1405
experiments in which we implemented the transducer and integrated it into a pipeline shown in Figure 2.
JointPoS TaggerSSynt parser
SSynt?DSyntTransducerPlainSentences
DSyntTreebankSSyntTreebank
SSyntS DSynS
Figure 2: Setup of a deep-syntactic parser
For our experiments, we use the AnCora-UPF SSyntS and DSyntS treebanks of Spanish (Mille et
al., 2013) in CoNLL format, adjusted for our needs. In particular, we removed from the 79-tag SSyntS
treebank the semantically and information structure influenced relation tags to obtain an annotation gran-
ularity closer to the ones used for previous parsing experiments (55 relation tags, see (Mille et al., 2012)).
Our development set consisted of 219 sentences (3271 tokens in the DSyntS treebank and 4953 tokens
in the SSyntS treebank), the training set of 3036 sentences (57665 tokens in the DSyntS treebank and
86984 tokens in the SSyntS treebank), and the test set held-out for evaluation of 258 sentences (5641
tokens in the DSyntS treebank and 8955 tokens in the SSyntS treebank).
To obtain the SSyntS, we use Bohnet and Nivre (2012)?s transition-based parser, which combines
lemmatization, PoS tagging, and syntactic dependency parsing?tuned and trained on the respective sets
of the SSyntS treebank. Cf. Table 1 for the performance of the parser on the development set.
POS LEMMA LAS UAS
96.14 91.10 78.64 86.49
Table 1: Results of Bohnet and Nivre?s surface-syntactic parser on the development set
In what follows, we first present the realization of the SSyntS?DSyntS transducer and then the real-
ization of the baseline.
3.1 SSyntS?DSyntS transducer
As outlined in Section 2.2, the SSyntS?DSyntS transducer is composed of three submodules and a post-
processing stage:
1. Hypernode identification. For the hypernode identification, we trained a binary polynomial (degree
2) SVM from LIBSVM (Chang and Lin, 2001). The SVM allows both features related to the processed
node and higher-order features, which can be related to the head node of the processed node or to its
sibling nodes. After several feature selection trials, we chose the following features for each node n:
? lemma or stem of the label of n,
? label of the relation between n and its head,
? surface PoS of n?s label (the SSynt and DSyntS treebanks distinguish between surface and deep
PoS),
? label of the relation between n?s head to its own head,
? surface PoS of the label of n?s head node.
After an optimization round of the parameters available in the SVM implementation, the hypernode
identification achieved over the gold development set 99.78% precision and 99.02% recall (and thus
99.4% F1). That is, only very few hypernodes are not identified correctly. The main error source are
governed prepositions: the classifier has to learn when to assign a preposition an own hypernode (i.e.,
when it is lexically meaningful) and when it should be included into the hypernode of the governor (i.e.,
when it is functional). Our interpretation is that the features we use for this task are appropriate, but
that the training data set is too small. As a result, some prepositions are erroneously left out from or
introduced into the DSyntS.
1406
2. Tree reconstruction. The implementation of the tree reconstruction module shows an unlabelled
dependency attachment precision of 98.18% and an unlabelled dependency attachment recall of 97.43%
over the gold development set. Most of the errors produced by this module have their origin in the
previous module, i.e., hypernode identification. When a node has been incorrectly removed, the module
errs in the attachment because it cannot use the node in question as the destination or the origin of a
dependency, as it is the case in the gold-standard annotation:
Gold-standard: ser como e?ne
be like letter-n
II
II
Predicted: ser e?ne
II
When a node has erroneously not been removed, no dependencies between its governor and its depen-
dent can be established since DSyntS must remain a tree (which gives the same LAS and UAS errors as
when a node has been erroneously removed):
Gold-standard: y Michael Jackson
II
Predicted: y a Michael Jackson
and to Michael Jackson
II
II
3. Relation label classification. For relation label classification, we use a multiclass linear SVM. The
label classification depends on the concrete annotation schemata of the SSyntS and DSyntS treebanks
on which the parser is trained. Depending on the schemata, some DSynt relation labels may be easier to
derive from the original SSyntS relation labels than others. Table 2 lists all SSynt relation labels that have
a straightforward mapping to DSyntS relation labels in the used treebanks, i.e., neither their dependent
nor their governor are removed, and the SSyntS label always maps to the same DSynt label.
SSynt DSynt
abbrev ATTR
abs pred ATTR
adv ATTR
adv mod ATTR
agent I
appos ATTR
attr ATTR
aux phras ?
aux refl dir II
SSynt DSynt
aux refl indir III
bin junct ATTR
compl1 II
compl2 III
compl adnom ATTR
coord COORD
copul II
copul clitic II
copul quot II
SSynt DSynt
dobj clitic II
dobj quot II
elect ATTR
juxtapos APPEND
modal II
modif ATTR
num junct COORD
obj copred ATTR
prepos II
SSynt DSynt
prepos quot II
prolep APPEND
quant ATTR
quasi coord COORD
quasi subj I
relat ATTR
restr ATTR
sequent ATTR
subj I
subj copred ATTR
Table 2: Straightforward SSynt to DSyntS mappings
Table 3 shows SSyntS relation?DSyntS relation label correspondences that are not straightforward.
SSynt DepRel
A
Mapping to DSynt
analyt fut remove Gov and Dep; add tense=FUT
analyt pass remove Gov; invert I and II; add voice=PASS
analyt perf remove Gov; add tense=PAST
analyt progr remove Gov; add tem constituency=PROGR
aux refl lex remove Dep; add se at the end of Gov?s lemma
aux refl pass remove Dep; invert I and II; add voice=PASS
compar remove Dep if conjunction
compar /coord /sub conj remove Dep if governed preposition
det
IF Dep=el?un THEN remove Dep; add definiteness=DEF/INDEF
IF Dep=possessive THEN DepRel ATTR?I?II?III
IF Dep=other THEN DepRel ATTR
dobj remove Dep if governed preposition
iobj remove Dep if governed preposition; DepRel II?III?IV?V?VI
iobj clitic DepRel II?III?IV?V?VI
obl compl remove Dep if governed preposition; DepRel I?II?III?IV?V?VI
obl obj remove Dep if governed preposition; DepRel II?III?IV?V?VI
punc ?
punc init ?
Table 3: Complex SSynt to DSynt mappings
1407
The final set of features selected for label classification includes: (i) lemma of the dependent node, (ii)
dependency relation to the head of the dependent node, (iii) dependency relation label of the head node
to its own head, (iv) dependency relation to the head of the sibling nodes of the dependent node, if any.
After an optimization round of the parameter set of the SVM-model, relation labelling achieved
94.00% label precision and 93.28% label recall on the development set. The recall is calculated con-
sidering all the nodes that are included in the gold standard. The error sources for relation labelling
were mostly the dependencies that involved possessives and the various types of objects (see Table
3) due to their differing valency. For instance, the relation det in su?det?coche ?his/her car? and
su?det?llamada ?his/her phone call? have different correspondences in DSyntS: su?ATTR?coche
vs. su?I?llamada. That is, the DSyntS relation depends on the lexical properties of the governor.
7
Once again, more training data is needed in order to classify better those cases.
4. Postprocessing In the postprocessing stage for Spanish, the following rules capture non-ambiguous
correspondences between elements of the SSynt-index matrix I
S
= N
s
?N
s
and DSyntS index matrix
I
D
= N
d
?N
d
, with n
s
? N
s
and n
d
? N
d
, and n
s
and n
d
corresponding to each other (we do not list
here identity correspondences such as between the number grammemes of n
s
and n
d
):
? if n
s
is dependent of analyt pass or analyt refl pass relation, then the voice grammeme in n
d
is
PASS;
? if n
s
is dependent of analyt progr, then the voice grammeme in n
d
is PROGR;
? if n
s
is dependent of analyt refl lex, then add the particle -SE as suffix of node label (word) of d
d
;
? if any of the children of n
s
is labelled by one of the tokens UN ?a
masc
?, UNA ?a
fem
?, UNOS
?some
masc
? or UNAS ?some
fem
?, then the definiteness grammeme in n
d
INDEF, otherwise it is
DEF;
? if the n
s
label is a finite verb and n
s
does not govern a subject relation, then add to I
?
the relation
n
d
? I?n
?
d
, with n
?
d
being a newly introduced node.
3.2 Baseline
As point of reference for the evaluation of the performance of our SSyntS?DSyntS transducer, we use a
rule-based baseline that carries out the most direct transformations extracted from Tables 2 and 3. The
baseline detects hypernodes by directly removing all the nodes that we are sure need to be removed, i.e.
punctuation and auxiliaries. The nodes that are only potentially to be removed, i.e., all dependents of
DepRels that have a possibly governed preposition or conjunction in Table 3, are left in the DSyntS. The
new relation labels in the DSyntS are obtained by selecting the label that is most likely to substitute the
SSyntS relation label according to classical grammar studies. The rules of the rule-based baseline look
as follows:
1 if (deprel==abbrev) then deep deprel=ATTR
2 if (deprel==obl obj) then deep deprel=II
. . .
n if (deprel==punc) then remove(current node)
4 Results and Discussion
Let us look in this section at the performance figures of the SSyntS parser, the SSyntS?DSyntS trans-
ducer, and the sentence?DSyntS pipeline obtained in the experiments.
4.1 SSyntS?DSyntS transducer results
In Table 4, the performance of the subtasks of the SSyntS?DSyntS transducer is contrasted to the per-
formance of the baselines; the evaluation of the postprocessing subtask is not included because the one-
to-one projection of SSyntS elements to DSyntS guarantees an accuracy of 100% of the operations
performed. The transducer has been applied to the gold standard test set, which is the held-out test set,
with gold standard PoS tags, lemmas and dependency trees. It outputs in total 5610 nodes; the rule-based
baseline outputs 8653 nodes. As mentioned in Section 3, our gold standard includes 5641 nodes.
7
Note that lexemes are not generalized: a verb and its corresponding noun (e.g., construct/construction) are considered
distinct lexemes.
1408
Hyper-Node Detection
Measure Rule-based Baseline Tree Transducer
p 64.31 (5565/8653) 99.79 (5598/5610)
r 98.65 (5565/5641) 99.24 (5598/5641)
F1 77.86 99.51
Attachment and Labelling
Measure Rule-based Baseline Tree Transducer
LAP 50.02 (4328/8653) 91.07 (5109/5610)
UAP 53.05 (4590/8653) 98.32 (5516/5610)
LA-P 57.66 (4989/8653) 92.37 (5182/5610)
LAR 76.72 (4328/5641) 90.57 (5109/5641)
UAR 81.37 (4590/5641) 97.78 (5516/5641)
LA-R 88.44 (4989/5641) 91.86 (5182/5641)
Table 4: Performance of the SSyntS?DSyntS transducer and of the rule-based baseline over the gold-
standard held-out test set (LAP: labelled attachment precision, UAP: unlabelled attachment precision, LA-P: label assign-
ment precision, LAR: labelled attachment recall, UAR: Unlabelled attachment recall and LA-R: Label assignment recall)
Our data-driven SSyntS?DSyntS transducer is much better than the baseline with respect to all eval-
uation measures.
8
The transducer relies on distributional patterns identified in the training data set, and
makes thus use of information that is not available for the rule-based baseline, which studies one node
at a time. However, the rule-based baseline results also show that transduction that would remove a few
nodes would provide results close to a 100% recall for the hypernode detection because a DSynt tree is a
subtree of the SSynt tree (if we ignore the nodes introduced by post-processing). This is also evidenced
by the labeled and attachment recall scores. The results of the transducer on the test and development
sets are quite comparable. The hypernode detection is even better on the test set. The label accuracy
suffers most from using unseen data during the development of the system. The attachment figures are
approximately equivalent on both sets.
4.2 Results of deep-syntactic parsing
Let us consider now the performance of the complete DSynt parsing pipeline (PoS-tagger+surface-
dependency parser? SSyntS?DSyntS transducer) on the held-out test set. Table 5 displays the figures
of the Bohnet and Nivre parser. The figures are in line with the performance of state-of-the-art parsers
for Spanish (Mille et al., 2012).
POS LEMMA LAS UAS
96.05 92.10 81.45 88.09
Table 5: Performance of Bohnet and Nivre?s joint PoS-tagger+dependency parser trained on Ancora-UPF
Table 6 shows the performance of the pipeline when we feed the output of the syntactic parser to the
rule-based baseline SSyntS?DSyntS module and the tree transducer. We observe a clear error propaga-
tion from the dependency parser (which provides 81.45% LAS) to the SSyntS?DSyntS transducer, which
loses in tree quality more than 18%.
Hyper-Node Detection
Measure Baseline Tree Transducer
p 63.87 (5528/8655) 97.07 (5391/5554)
r 98.00 (5528/5641) 95.57 (5391/5641)
F1 77.33 96.31
Labelling and Attachment
Measure Baseline Tree Transducer
LAP 38.75 (3354/8655) 68.31 (3794/5554)
UAP 44.69 (3868/8655) 77.31 (4294/5554)
LA-P 49.66 (4298/8655) 80.47 (4469/5554)
LAR 59.46 (3354/5641) 67.26 (3794/5641)
UAR 68.57 (3868/5641) 76.12 (4294/5641)
LA-R 76.19 (4298/5641) 79.22 (4469/5641)
Table 6: Performance of the deep-syntactic parsing pipeline
5 Related Work
To the best of our knowledge, data-driven deep-syntactic parsing as proposed in this paper is novel. As
semantic role labeling and frame-semantic analysis, it has the goal to obtain more semantically oriented
structures than those delivered by state-of-the-art syntactic parsing. Semantic role labeling received
considerable attention in the CoNLL shared tasks for syntactic dependency parsing in 2006 and 2007
8
We also ran MaltParser by training it on the DSynt-treebank to parse the SSynt-test set; however, the outcome was too
weak to be used as baseline.
1409
(Buchholz and Marsi, 2006; Nivre et al., 2007), the CoNLL shared task for joint parsing of syntactic and
semantic dependencies in 2008 (Surdeanu et al., 2008) and the shared task in 2009 (Haji?c et al., 2009).
The top ranked systems were pipelines that started with a syntactic analysis (as we do) and continued
with predicate identification, argument identification, argument labeling, and word sense disambigua-
tion; cf. (Johansson and Nugues, 2008; Che et al., 2009). At the end, a re-ranker that considers jointly
all arguments to select the best combination was applied. Some of the systems were based on integrated
syntactic and semantic dependency analysis; cf., e.g., (Gesmundo et al., 2009); see also (Llu??s et al.,
2013) for a more recent proposal along similar lines. However, all of them lack the ability to perform
structural changes?as, e.g., introduction of nodes or removal of nodes necessary to obtain a DSyntS.
Klime?s (2006)?s parser removes nodes (producing tectogrammatical structures as in the Prague Depen-
dency Treebank), but is based on rules instead of classifiers, as in our case. The same applies to earlier
works in the TAG-framework, as, e.g., in (Rambow and Joshi, 1997).
However, this is not to say that the idea of the surface?surface syntax?deep syntax pipeline is new.
It goes back at least to Curry (1961) and is implemented in a number of more recent works; see, e.g., (de
Groote, 2001; Klime?s, 2006; Bojar et al., 2008).
6 Conclusions and Future Work
We have presented a deep-syntactic parsing pipeline which consists of a state-of-the-art dependency
parser and a novel SSyntS?DSyntS transducer. The obtained DSyntSs can be used in different applica-
tions since they abstract from language-specific grammatical idiosyncrasies of the SSynt structures as
produced by state-of-the art dependency parsers, but still avoid the complexities of genuine semantic
analysis.
9
DSyntS-treebanks needed for data-driven applications can be bootstrapped by the pipeline.
If required, a SSyntS?DSyntS structure pair can be also mapped to a pure predicate-argument graph
such as the DELPH-IN structure (Oepen, 2002) or to an approximation thereof (as the Enju conversion
(Miyao, 2006), which keeps functional nodes), to an DRS (Kamp and Reyle, 1993), or to a PropBank
structure. On the other hand, DSyntS-treebanks can be used for automatic extraction of deep grammars.
As shown by Cahill et al. (2008), automatically obtained resources can be of an even better quality than
manually-crafted resources. In this context, especially research in the context of CCGs (Hockenmeier,
2003; Clark and Curran, 2007) and TAGs (Xia, 1999) should be also mentioned.
To validate our approach with languages other than Spanish, we carried out an experiment on a Chi-
nese SSyntS-DSyntS Treebank (training the DSynt-transducer on the outcome of the SSynt-parser). The
results over predicted input showed an accuracy of about 75%, i.e., an accuracy comparable to the accu-
racy achieved for Spanish. We are also investigating multilingual approaches, such as the one proposed
by McDonald et al. (2013).
In the future, we will carry out further in-depth feature engineering for the task of DSynt-parsing. It
proved to be crucial in semantic role labelling and dependency parsing (Che et al., 2009; Ballesteros and
Nivre, 2012); we expect it be essential for our task as well. Furthermore, we will join surface syntactic
and deep-syntactic parsing we kept so far separate; see, e.g., (Zhang and Clark, 2008; Llu??s et al., 2013;
Bohnet and Nivre, 2012) for analogous proposals. Further research is required here since although joint
models avoid error propagation from the first stage to the second, overall, pipelined models still proved
to be competitive; cf. the outcome of CoNLL shared tasks.
The deep-syntactic parser described in this paper is available for downloading at https://code.
google.com/p/deepsyntacticparsing/.
Acknowledgements
This work has been supported by the European Commission under the contract number FP7-ICT-610411.
Many thanks to the three anonymous COLING reviewers for their very helpful comments and sugges-
tions.
9
The motivation to work with DSyntS instead of SSyntS is thus similiar to the motivation of the authors of the Abstract
Meaning Representation (AMR) for Machine Translation (Banarescu et al., 2013), only that AMRs are considerably more
semantic than DSyntSs.
1410
References
Alfred V. Aho. 1972. The theory of parsing, translation and, compiling. Prentice Hall, Upper Saddle River, NJ.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOptimizer: A System for MaltParser Optimization. In Proceed-
ings of the Eighth International Conference on Language Resources and Evaluation (LREC 12).
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and
N. Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop & Interoperability with Discourse, pages 178?186, Sofia, Bulgaria.
Bernd Bohnet and Joakim Nivre. 2012. A transition-based system for joint part-of-speech tagging and labeled
non-projective dependency parsing. In EMNLP-CoNLL.
O. Bojar, S. Cinkov?a, and J. Pt?a?cek. 2008. Towards English-to-Czech MT via Tectogrammatical Layer. The
Prague Bulletin of Mathematical Linguistics, 90:57?68.
Mathias Buch-Kromann. 2003. The Danish dependency treebank and the dtag treebank tool. In 2nd Workshop on
Treebanks and Linguistic Theories (TLT), Sweden, pages 217?220.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceed-
ings of the 10th Conference on Computational Natural Language Learning (CoNLL), pages 149?164.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Stefan Riezler, Josef van Genabith, and Andy Way. 2008. Wide-
coverage deep statistical parsing using automatic dependency structure annotation. Computational Linguistics,
34(1):81?124.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: A Library for Support Vector Machines. Software available
at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Wanxiang Che, Zhenghua Li, Yongqiang Li, Yuhang Guo, Bing Qin, and Ting Liu. 2009. Multilingual
dependency-based syntactic and semantic parsing. In Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning (CoNLL 2009): Shared Task, pages 49?54, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33:493?552.
R. Curry. 1961. Some logical aspects of grammatical structure. In R. Jakobson, editor, Structure of Language and
Its Mathematical Aspects, pages 56?68. American Mathematical Society, Providence, RI.
Ph. de Groote. 2001. Towards abstract categorial grammar. In Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics (ACL).
Charles J. Fillmore, Collin F. Baker, and Hiroaki Sato. 2002. The FrameNet database and software tools. In
Proceedings of the Third International Conference on Language Resources and Evaluation, volume IV, Las
Palmas. LREC, LREC.
A. Gesmundo, J. Henderson, P. Merlo, and I.Titov. 2009. Latent variable model of synchronous syntactic-semantic
parsing for multiple languages. In CoNLL 2009 Shared Task., Conf. on Computational Natural Language
Learning, pages 37?42, Boulder, Colorado, USA.
Jan Haji?c, Jarmila Panevov?a, Eva Haji?cov?a, Petr Sgall, Petr Pajas, Jan
?
St?ep?anek, Ji?r?? Havelka, Marie Mikulov?a,
and Zden
?
k
?
Zabokrtsk?y. 2006. Prague Dependency Treebank 2.0. Linguistic Data Consortium, Philadelphia.
Jan Haji?c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart??, Llu??s M`arquez,
Adam Meyers, Joakim Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu, Nianwen Xue,
and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.
In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared
Task, pages 1?18.
J. Hockenmeier. 2003. Parsing with generative models of predicate-argument structure. In Proceedings of the 41st
Annual Meeting of the Association for Computational Linguistics (ACL), pages 359?366, Sapporo, Japan.
Angelina Ivanova, Stephan Oepen, Lilja ?vrelid, and Dan Flickinger. 2012. Who did what to whom? a contrastive
study of syntacto-semantic dependencies. In Proceedings of the Sixth Linguistic Annotation Workshop, pages
2?11, Jeju, Republic of Korea, July. Association for Computational Linguistics.
1411
R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for english. In J. Nivre, H.-J.
Kaalep, K. Muischnek, and M. Koit, editors, Proceedings of NODALIDA 2007, pages 105?112, Tartu, Estonia.
Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic?semantic analysis with PropBank and
NomBank. In CoNLL 2008: Proceedings of the Twelfth Conference on Natural Language Learning, pages
183?187, Manchester, United Kingdom.
H. Kamp and U. Reyle. 1993. From Discourse to Logic. Kluwer Academic Publishers, Dordrecht, NL.
R.T. Kasper and W.C. Rounds. 1986. A logical semantics for feature structures. In Proceedings of the 24th annual
meeting on Association for Computational Linguistics, pages 257?266.
V?aclav Klime?s. 2006. Analytical and Tectogrammatical Analysis of a Natural Language. Ph.D. thesis, UFAL,
MFF UK, Prague, Czech Republic.
Xavier Llu??s, Xavier Carreras, and Llu??s M`arquez. 2013. Joint arc-factored parsing of syntactic and semantic
dependencies. Transactions of the Association for Computational Linguistics, pages 219?230.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev,
Keith Hall, Slav Petrov, Hao Zhang, Oscar T?ackstr?om, Claudia Bedini, N?uria Bertomeu Castell?o, and Jungmee
Lee. 2013. Universal dependency annotation for multilingual parsing. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 92?97.
Igor Mel??cuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.
Igor Mel??cuk. 1996. Lexical functions: A tool for the description of lexical relations in the lexicon. In L. Wanner,
editor, Lexical functions in lexicography and natural language processing, pages 37?102. Benjamins Academic
Publishers, Amsterdam.
Igor Mel??cuk. 2013. Semantics: From meaning to text, Volume 2. Benjamins Academic Publishers, Amsterdam.
Simon Mille, Alicia Burga, Gabriela Ferraro, and Leo Wanner. 2012. How does the granularity of an annotation
scheme influence dependency parsing performance? In Conference on Computational Linguistics, COLING
2012.
Simon Mille, Alicia Burga, and Leo Wanner. 2013. AnCora-UPF: A Multi-Level Annotation of Spanish . In
Proceedings of the Second International Conference on Dependency Linguistics (DEPLING 2013).
Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-Oriented Grammar Development
and Feature Forest Model. Ph.D. thesis, University of Tokyo.
J. Nivre, J. Hall, S. K?ubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 Shared Task
on Dependency Parsing. In Proceedings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pages 915?932.
Stephan Oepen. 2002. Collaborative Language Engineering: A Case Study in Efficient Grammar-based Process-
ing. Stanford Univ Center for the Study.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank. Computational Linguistics,
31:71?106.
Owen Rambow and Aravind Joshi. 1997. A formal look at dependency grammar and phrase structure grammars,
with special consideration of word-order phenomena. In L. Wanner, editor, Recent Trends in Meaning-Text
Theory, pages 167?190. Benjamins Academic Publishers, Amsterdam.
W.C. Rounds. 1970. Mappings and grammars on trees. Mathematical Systems Theory, 4(3):257?287.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu??s M`arquez, and Joakim Nivre. 2008. The conll 2008
shared task on joint parsing of syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the
Twelfth Conference on Computational Natural Language Learning, pages 159?177.
M. Taul?e, M. Ant`onia Mart??, and Marta Recasens. 2008. Ancora: Multilevel annotated corpora for Catalan and
Spanish. In Proceedings of the Sixth International Language Resources and Evaluation (LREC?08), Marrakech,
Morocco, may. European Language Resources Association (ELRA).
J.W. Thatcher. 1970. Generalized sequential machine maps. Journal of Computer and System Sciences, 4(4):339?
367.
1412
F. Xia. 1999. Extracting tree adjoining grammars from bracketed corpora. In Proceedings of the 5th Natural
Language Processing Pacific Rim Symposium, pages 398?403, Beijing, China.
Yue Zhang and Stephen Clark. 2008. Joint word segmentation and POS tagging using a single perceptron. In Pro-
ceedings of ACL-08: HLT, pages 888?896, Columbus, Ohio, June. Association for Computational Linguistics.
1413
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 22?30,
Utica, May 2012. c?2012 Association for Computational Linguistics
Towards a Surface Realization-Oriented Corpus Annotation
Leo Wanner
ICREA and
Universitat Pompeu Fabra
Roc Boronat 138
Barcelona, 08018, Spain
leo.wanner@upf.edu
Simon Mille
Universitat Pompeu Fabra
Roc Boronat 138
Barcelona, 08018, Spain
simon.mille@upf.edu
Bernd Bohnet
Universita?t Stuttgart
IMS, Pfaffenwaldring 5b
Stuttgart, 70569, Germany
bohnet@ims.uni-
stuttgart.de
Abstract
Until recently, deep stochastic surface realiza-
tion has been hindered by the lack of seman-
tically annotated corpora. This is about to
change. Such corpora are increasingly avail-
able, e.g., in the context of CoNLL shared
tasks. However, recent experiments with
CoNLL 2009 corpora show that these popu-
lar resources, which serve well for other ap-
plications, may not do so for generation. The
attempts to adapt them for generation resulted
so far in a better performance of the realizers,
but not yet in a genuinely semantic generation-
oriented annotation schema. Our goal is to
initiate a debate on how a generation suit-
able annotation schema should be defined. We
define some general principles of a semantic
generation-oriented annotation and propose an
annotation schema that is based on these prin-
ciples. Experiments shows that making the
semantic corpora comply with the suggested
principles does not need to have a negative im-
pact on the quality of the stochastic generators
trained on them.
1 Introduction
With the increasing interest in data-driven surface
realization, the question on the adequate annota-
tion of corpora for generation also becomes increas-
ingly important. While in the early days of stochas-
tic generation, annotations produced for other ap-
plications were used (Knight and Hatzivassiloglou,
1995; Langkilde and Knight, 1998; Bangalore and
Rambow, 2000; Oh and Rudnicky, 2000; Langkilde-
Geary, 2002), the poor results obtained, e.g., by
(Bohnet et al, 2010) with the original CoNLL 2009
corpora, show that annotations that serve well for
other applications, may not do so for generation and
thus need at least to be adjusted.1 This has also
been acknowledged in the run-up to the surface re-
alization challenge 2011 (Belz et al, 2011), where
a considerable amount of work has been invested
into the conversion of the annotations of the CoNLL
2008 corpora (Surdeanu et al, 2008), i.e., PropBank
(Palmer et al, 2005), which served as the reference
treebank, into a more ?generation friendly? annota-
tion. However, all of the available annotations are to
a certain extent still syntactic. Even PropBank and
its generation-oriented variants contain a significant
number of syntactic features (Bohnet et al, 2011b).
Some previous approaches to data-driven genera-
tion avoid the problem related to the lack of seman-
tic resources in that they use hybrid models that im-
ply a symbolic submodule which derives the syntac-
tic representation that is then used by the stochas-
tic submodule (Knight and Hatzivassiloglou, 1995;
Langkilde and Knight, 1998). (Walker et al, 2002),
(Stent et al, 2004), (Wong and Mooney, 2007), and
(Mairesse et al, 2010) start from deeper structures:
Walker et al and Stent et al from deep-syntactic
structures (Mel?c?uk, 1988), and Wong and Mooney
and Mairesse et al from higher order predicate logic
structures. However, to the best of our knowledge,
1Trained on the original ConLL 2009 corpora, (Bohnet et al,
2010)?s SVM-based generator reached a BLEU score of 0.12 for
Chinese, 0.18 for English, 0.11 for German and 0.14 for Span-
ish. Joining the unconnected parts of the sentence annotations to
connected trees (as required by a stochastic realizer) improved
the performance to a BLEU score of 0.69 for Chinese, 0.66 for
English, 0.61 for German and 0.68 for Spanish.
22
none of them uses corpora annotated with the struc-
tures from which they start.
To deep stochastic generation, the use of hybrid
models is not an option and training a realizer on
syntactically-biased annotations is highly problem-
atic in the case of data-to-text NLG, which starts
from numeric time series or conceptual or seman-
tic structures: the syntactic features will be simply
not available in the input structures at the moment
of application.2. Therefore, it is crucial to define a
theoretically sound semantic annotation that is still
good in practical terms.
Our goal is thus to discuss some general prin-
ciples of a semantic generation-oriented annotation
schema and offer a first evaluation of its possible
impact on stochastic generation. Section2 details
what kind of information is available respectively
not available during data-to-text generation. Sec-
tion 3 states some general principles that constrain
an adequate semantic representation, while Section
4 formally defines their well-formedness. Section 5
reports then on the experiments made with the pro-
posed annotation, and Section6 offers some conclu-
sions.
2 What can we and what we cannot count
on?
In data-to-text or ontology-to-text generation, with
the standard content selection?discourse structur-
ing?surface generation pipeline in place, and no
hard-wired linguistic realization of the individual
chunks of the data or ontology structure, the input
to the surface realization module can only be an ab-
stract structure that does not contain any syntactic
(and even lexical) information. Conceptual graphs
in the sense of Sowa (Sowa, 2000) are structures of
this kind;3 see Figure 1 for illustration (?Cmpl? =
?Completion?, ?Rcpt? = ?Recipient?, ?Strt? = ?Start?,
?Attr? = Attribute, ?Chrc? = ?Characteristic?, and
?Amt? = ?Amount?). Content selection accounts for
the determination of the content units that are to be
communicated and Discourse Structuring for the de-
limitation of Elementary Discourse Units (EDUs)
2Even though in this article we are particularly interested in
data-to-text generation, we are convinced that clean semantic
and syntactic annotations also facilitate text-to-text generation.
3But note that this can be any other content structure.
and their organization and for the discursive rela-
tions between them (e.g., Bcas (Because) in the Fig-
ure).
In particular, such a structure cannot contain:
? non-meaningful nodes: governed prepositions
(BECAUSE of, CONCENTRATION of), auxil-
iaries (passive be), determiners (a, the);
? syntactic connectors (between A and B), rela-
tive pronouns, etc.
? syntactic structure information: A modifies B,
A is the subject of B, etc.
In other words, a deep stochastic generator has
to be able to produce all syntactic phenomena from
generic structures that guarantee a certain flexibil-
ity when it comes to their surface form (i.e., without
encoding directly this type of syntactic information).
For instance, a concentration of NO2 can be realized
as a NO2 concentration, between 23h00 and 00h00
as from 23h00 until 00h00, etc. This implies that
deep annotations as, for instance, have been derived
so far from PennTreeBank/PropBank, in which ei-
ther all syntactic nodes of the annotation are kept
(as in (Bohnet et al, 2010)) or only certain syntac-
tic nodes are removed (as THAT complementizers
and TO infinitives in the shared task 2011 on sur-
face realization (Belz et al, 2011)) still fall short
of a genuine semantic annotation. Both retain a
lot of syntactic information which is not accessible
in genuine data-to-text generation: nodes (relative
pronouns, governed prepositions and conjunctions,
determiners, auxiliaries, etc.) and edges (relative
clause edges, control edges, modifier vs. argumen-
tal edges, etc.).
This lets us raise the question how the annotation
policies should look like to serve generation well
and to what extent existing resources such as Prop-
Bank comply with them already. We believe that
the answer is critical for the future research agenda
in generation and will certainly play an outstanding
role in the shared tasks to come.
In the next section, we assess the minimal princi-
ples which the annotation suitable for (at least) data-
to-text generation must follow in order to lead to
a core semantic structure. This core structure still
ignores such important information as co-reference,
23
Figure 1: Sample conceptual structure as could be produced by text planning (Because of a concentration of NO2 of
13?g/m3, the NO2 threshold value was exceeded between 23h00 and 00h00)
scope, presupposition, etc.: this information is ob-
viously necessary, but it is not absolutely vital for
a sufficient restriction of the possible choices faced
during surface generation. Further efforts will be re-
quired to address its annotation in appropriate depth.
3 The principles of generation-suitable
semantic annotation
Before talking about generation-suitable annotation,
we must make some general assumptions concern-
ing NLG as such. These assumptions are necessary
(but might not always be sufficient) to cover deep
generation in all its subtleties: (i) data-to-text gener-
ation starts from an abstract conceptual or semantic
representation of the content that is to be rendered
into a well-formed sentence; (ii) data-to-text gener-
ation is a series of equivalence mappings from more
abstract to more concrete structures, with a chain of
inflected words as the final structure; (iii) the equiva-
lence between the source structure Ss and the target
structure St is explicit and self-contained, i.e., for
the mapping from Ss to St, only features contained
in Ss and St are used. The first assumption is in
the very nature of the generation task in general; the
second and the third are owed to requirements of sta-
tistical generation (although a number of rule-based
generators show these characteristics as well).
The three basic assumptions give rise to the fol-
lowing four principles.
1. Semanticity: The semantic annotation must cap-
ture the meaning and only the meaning of a given
sentence. Functional nodes (auxiliaries, determin-
ers, governed conjunctions and prepositions), node
duplicates and syntactically-motivated arcs should
not appear in the semantic structure: they re-
flect grammatical and lexical features, and thus al-
ready anticipate how the meaning will be worded.
For example, meet-AGENT?the (directors), meet-
LOCATION?in (Spain), meet-TIME?in (2002)
cited in (Buch-Kromann et al, 2011) as semantic
annotation of the phrase meeting between the direc-
tors in Spain in 2002 in the Copenhagen Depen-
dency Treebank does not meet this criterion: the,
and both ins are functional nodes. Node dupli-
cates such as the relative pronoun that in the Prop-
Bank annotation (But Panama illustrates that their
their substitute is a system) that?R-A0-produces
(an absurd gridlock) equally reflect syntactic fea-
tures, as do syntactically-motivated arc labels of the
kind ?R(elative)-A0?.
The PropBank annotation of the sentence cited
above also intermingles predicate-argument rela-
tions (?Ai?) with syntactico-functional relations
(?AM-MNR?): gridlock?AM-MNR?absurd. The
predicate-argument analysis of modifiers suggests
namely that they are predicative semantemes that
take as argument the node that governs them
in the syntactic structure; in the above struc-
ture: absurd?A1?gridlock. This applies also
to locatives, temporals and other ?circumstan-
tials?, which are most conveniently represented
as two-place semantemes: house?A1?location?
A2?Barcelona, party?A1?time?A2?yes-
terday, and so on. Although not realized at the sur-
face, location, time, etc. are crucial.
24
2. Informativity: A propositional semantic annota-
tion must be enriched by information structure fea-
tures that predetermine the overall syntactic struc-
ture (paratactic, hypotactic, parenthetical, . . . ), the
internal syntactic structure (subject/object, clefted or
not, any element fronted or not, etc.), determiner dis-
tribution, etc. in the sentence. Otherwise, it will be
always underspecified with respect to its syntactic
equivalence in that, as a rule, a single semantic struc-
ture will correspond to a number of syntactic struc-
tures. This is not to say that with the information
structure in place we will always achieve a 1:1 cor-
respondence between the semantic and syntactic an-
notations; further criteria may be needed?including
prosody, style, presupposedness, etc. However, in-
formation structure is crucial.
The most relevant information structure features
are those of Thematicity, Foregroundedness and
Givenness.4
Thematicity specifies what the utterance states
(marked as rheme) and about what it states it
(marked as theme).5 Theme/rheme determines, in
the majority of cases, the subject-object structure
and the topology of the sentence. For instance,6
[John]theme?A1?[see?A2?Maria]rheme may
be said to correspond to John?subject?see?
dir.obj?Maria and [John?A1?see]rheme?A2
?[Maria]theme to John ?obj?seepass?subject
?Maria. For the generation of relative sentence
structures such as John bought a car which was old
and ugly, we need to accommodate for a recursive
definition of thematicity: [John]theme?A1?[buy?
A2?[c1 : car]theme?A1?[old]rheme; c1?A1
?[ugly]rheme]rheme.7 With no recursive (or sec-
ondary in Mel?c?uk?s terms) thematicity, we would
4We use mainly the terminology and definitions (although in
some places significantly simplified) of (Mel?c?uk, 2001), who,
to the best of our knowledge, establishes the most detailed cor-
relation between information structure and syntactic features.
5Similar notions are topic/focus (Sgall et al, 1986) and
topic/comment (Gundel, 1988).
6As in PropBank, we use ?Ai? as argument labels of predica-
tive lexemes, but for us, ?A1? stands for the first argument, ?A2?
for the second argument, etc. That is, in contrast to PropBank,
we do not support the use of ?A0? to refer to a lexeme?s exter-
nal argument since the distinction between external and internal
arguments is syntactic.
7c1 is a ?handle? in the sense of Minimal Recursion Seman-
tics (Copestake et al, 1997).
get John bought an old and ugly car.8
It is quite easy to find some counter-examples
to the default theme/rheme?syntactic feature cor-
relation, in particular in the case of questions
and answers. For instance, the neutral answer
to the question What will John bake tomorrow?,
John will bake a cake, would be split as follows:
[John?A1?bake]theme ?A2?[cake]rheme. In
this case, the main verb at the surface, bake, is in-
cluded in the theme and not in the rheme. Consider
also the sentence In a cross-border transaction, the
buyer is in a different region of the globe from the
target, where the main theme is in a cross-border
transaction, i.e., not the subject of the sentence (with
the subject the buyer being the embedded theme of
the main rheme). In these cases, the correlation is
more complex, but it undoubtedly exists and needs
to be distilled during the training phase.
Foregroundedness captures the ?prominence?
of the individual elements of the utterance for
the speaker or hearer. An element is ?fore-
grounded? if it is prominent and ?backgrounded?
if it is of lesser prominence; elements that are
neither foregrounded nor backgrounded are ?neu-
tral?. A number of correlations can be iden-
tified: (i) a ?foregrounded? A1 argument of a
verb will trigger a clefting construction; e.g.,
[John]foregr;theme?A1?[see?A2?Maria]rheme
will lead to It was John who saw Maria; similarly,
[John?A1?bake]foregr;theme ?A2?[cake]rheme
will lead to What John will bake is a cake; (ii) a
?foregrounded? A2 argument of a verb will corre-
spond to a clefting construction or a dislocation: It
was Maria, whom John saw; (iii) a ?foregrounded?
A1 or A2 argument of a noun will result in an argu-
ment promotion, as, e.g., John?s arrival (instead of
arrival of John); (iv) a ?foregrounded? circumstan-
tial will be fronted: Under this tree he used to rest;
(v) marking a part of the semantic structure as ?back-
grounded? will lead to a parenthetical construction:
John (well known among the students and professors
alike) was invited as guest speaker. If no elements
8We believe that operator scopes (e.g., negations and quan-
tifiers) can, to a large extent, be encoded within the thematic
structure; see (Cook and Payne, 2006) for work in the LFG-
framework on German, which provides some evidence for this.
However, it must be stated that very little work has been done
on the subject until now.
25
are marked as foregrounded/backgrounded, the de-
fault syntactic structure and the default word order
are realized.
Givenness captures to what extent an information
element is present to the hearer. The elementary
givenness markers ?given? and ?new? correlate in
syntax with determiner distribution. Thus, the ?new?
marker of an object node will often correspond to
an indefinite or zero determiner of the correspond-
ing noun: A masked man was seen to enter the
bank (man is newly introduced into the discourse).
The ?given? marker will often correlate with a defi-
nite determiner: The masked man (whom a passer-
by noticed before) was seen to enter the bank. To
distinguish between demonstratives and definite de-
terminers, a gradation of givenness markers as sug-
gested by Gundel et al (Gundel et al, 1989) is nec-
essary: ?given1/2/3?.
As already for Thematicity, numerous examples
can be found where the giveness-syntactic feature
correlation deviates from the default correlation. For
instance, in I have heard a cat, the cat of my neigh-
bour, there would be only one single (given) node
cat in the semantic structure, which does not pre-
vent the first appearance of cat in the sentence to be
indefinite. In A warrant permits a holder that he ac-
quire one share of common stock for $17.50 a share,
warrant is given, even if it is marked by an indefinite
determiner. Again, this only shows the complexity
of the annotation of the information structure, but it
does not call into question the relevance of the infor-
mation structure to NLG.
As one of the few treebanks, the Prague Depen-
dency Treebank (PDT) (Hajic? et al, 2006) accounts
for aspects of the information structure in that it an-
notates Topic-Focus Articulation in terms of various
degrees of contextual boundness, which are corre-
lated with word order and intonation (Mikulova? et
al., 2006, p.152).
3. Connectivity: The semantic annotation must
ensure that the annotation of an utterance forms
a connected structure: without a connected struc-
ture, generation algorithms that imply a traver-
sal of the input structure will fail to generate a
grammatical sentence. For instance, the Prop-
Bank annotation of the sentence But Panama il-
lustrates that their substitute is a system that pro-
duces an absurd gridlock (here shown partially)
does not comply with this principle since it con-
sists of four unconnected meaning-bearing sub-
structures (the single node ?but? and the subtrees
governed by ?illustrate?, ?produce? and ?substi-
tute?): but | Panama?A0?illustrate?A1?that |
system?A0?produce?A1?gridlock?AM-
MNR?absurd | substitute?A0?their.
4 Outline of a Generation-Oriented
Annotation
The definitions below specify the syntactic well-
formedness of the semantic annotation. They do not
intend to and cannot substitute a detailed annotation
manual, which is indispensable to achieve a seman-
tically accurate annotation.
Definition 1: [Semantic Annotation of a sentence
S, SA]
SA of S in the text T in language L is a pair
?Ssem, Sinf ?, where Ssem is the semantic structure
of S (ensuring Semanticity and Connectivity), and
Sinf is the information structure of S (ensuring In-
formativity).
Let us define each of the two structures of the se-
mantic annotation in turn.
Definition 2: [Semantic Structure of a sentence S,
Ssem]
Ssem of S is a labeled acyclic directed connected
graph (V,E, ?, ?) defined over the vertex label al-
phabet L := LS ?MC ?MT ?Mt ?Ma (such that
LS ? (MC ?MT ?Mt ?Ma) = ?) and the edge
label alphabet Rsem ? {A1, A2, A3, A4, A5, A6},
with
? V as the set of vertices;
? E as the set of directed edges;
? ? as the function that assigns each v ? V an ele-
ment l ? L;
? ? as the function that assigns each e ? E an ele-
ment a ? Rsem;
? LS as the meaning bearing lexical units (LUs) of
S;
? MC ? {LOC, TMP, EXT, MNR, CAU, DIR,
SPEC, ELAB, ADDR} as the ?circumstantial meta
semantemes? (with the labels standing for ?locative?,
?temporal?, ?temporal/spatial extension?, ?manner?,
?cause?, ?direction?, ?specification?, ?elaboration?,
and ?addressee?);
? MT ? {TIME, TCST} as the ?temporal meta se-
mantemes? (with the labels standing for ?time? and
26
?time constituency?);
? Mt ? {past?, present?, future?} as the ?time
value semantemes?;
? Ma ? {imperfective?, durative?,
semelfactive?, iterative?, telic?, atelic?,
nil?} as the ?aspectual value semantemes?9
such that the following conditions hold:
(a) the edges in Ssem are in accordance with the va-
lency structure of the lexical units (LUs) in S: If
lp?Ai?lr ? Ssem (lp, lr ? LS , i ? {1, 2, 3, . . .}),
then the semantic valency of lp possesses at least i
slots and lr fulfils the semantic restrictions of the i-
th slot
(b) the edges in Ssem are exhaustive: If ?(nr) =
lr ? L instantiates in S the i-th semantic argument
of ?(np) = lp, then lp?Ai?lr ? Ssem
(c) Ssem does not contain any duplicated argument
edges: If ?(np)?Ai??(nr), ?(np) ?Aj? ?(nq) ?
Ssem (with np, nr, nq ? N ) then Ai 6= Aj and nr 6=
nq
(d) circumstantial LUs in S are represented in Ssem
by two-place meta-semantemes: If lr ? Lsem is
a locative/temporal/ manner/cause/direction/specifi-
cation/elaboration/addressee LU and in the syntac-
tic dependency structure of S, lr modifies lp, then
lr?A2-?-A1?lp ? Ssem (with ? ? LOC, TMP,
MNR, CAU, DIR, SPEC, ELAB, ADDR)
(e) verbal tense is captured by the two-place predi-
cate TIME: If lp ? Lsem is a verbal LU then lr?A2-
TIME-A1?lp ? Ssem, with lr ? Mt
(f) verbal aspect is captured by the two-place predi-
cate TCST: If lp ? Lsem is a verbal LU then lr?A2-
TCST-A1?lp ? Ssem, with lr ? Ma.
(a) implies that no functional node is target of an ar-
gument arc: this would contradict the semantic va-
lency conditions of any lexeme in S. (b) ensures that
no edge in Ssem is missing: if a given LU is an argu-
ment of another LU in the sentence, then there is an
edge from the governor LU to the argument LU. (c)
means that no predicate in Ssem possesses in S two
different instances of the same argument slot. The
circumstantial meta-semantemes in (d) either cap-
ture the semantic role of a circumstantial that would
otherwise get lost or introduce a predicate type for a
name. Most of the circumstantial meta-semantemes
9The aspectual feature names are mainly from (Comrie,
1976).
reflect PropBank?s modifier relations ?AM-X? (but in
semantic, not in syntactico-functional terms), such
that their names are taken from PropBank or are in-
spired by PropBank. LOC takes as A1 a name of a
location of its A2: Barcelona?A1-LOC-A2?live-
A1?John; TMP a temporal expression: yesterday
?A1-TMP-A2?arrive-A1?John; MNR a man-
ner attribute: player?A1-MNR-A2?solo; CAU
the cause: accept?A1-CAU-A2?reason in This
is the reason why they accepted it; DIR a spa-
tial direction: run around?A2-DIR-A1?circles in
I?m running around in circles; SPEC a ?context
specifier?: should?A2-SPEC-A1?thought in You
should leave now, just a thought; ELAB an appos-
itive attribute company?A1-ELAB-A2 ?bank in
This company, a bank, closed; and ADDR direct ad-
dress: come?A1-ADDR-A2?John in John, come
here!
Definition 3: [Information Structure of a sen-
tence S, Sinf ]
Let Ssem of S be defined as above. Sinf of S is
an undirected labeled hypergraph (V, I) with V as
the set of vertices of S and I the set of hyperedges,
with I := {themei (i = 1, 2, . . . ), rhemei (i = 1, 2,
. . . ), givenj (j = 1,. . . ,3), new, foregrounded, back-
grounded}. The following conditions apply:
(a) thematicity is recursive, i.e., a thematic hyper-
edge contains under specific conditions embedded
theme/rheme hyperedges: If ?nk ? themei such that
?(nk) = lp is a verbal lexeme and lp -A1?lr ?
Ssem, then ? themei+1, rhemei+1 ? themei
(b) theme and rheme hyperedges of the same re-
cursion level, given and new hyperedges, and fore-
grounded and backgrounded hyperedges are dis-
joint: themei ? rhemei = ? (i = 1, 2, . . . ), givenj
? new = ? (j = 1,. . . ,3), foregr. ? backgr. = ?
(c) any node in Ssem forms part of either theme or
rheme: ?np ? Ssem : np ? theme1 ? rheme1.
Consider in Figure 2 an example of SA with its
two structures.10 All syntactic nodes have been re-
moved, and all the remaining nodes are connected
in terms of a predicate?argument structure, with no
use of any syntactically motivated edge, so as to en-
sure that the structure complies with the Semantic-
ity and Connectivity principles. Figure 2 illustrates
the three main aspects of Informativity: (i) thematic-
10The meta-semanteme TCST is not shown in the figure.
27
ity, with the two theme/rheme oppositions; (ii) fore-
groundedness, with the backgrounded part of the
primary rheme; and (iii) givenness, with the attribute
givenness and the value 2 on the node program. The
information structure constrains the superficial real-
ization of the sentence in that the primary theme will
be the subject of the sentence, and the main node of
the primary rheme pointing to it will be the main
verb of the same sentence. The secondary theme
and rheme will be realized as an embedded sen-
tence in which you will be the subject, that is, forc-
ing the realization of a relative clause. However, it
does not constrain the appearance of a relative pro-
noun. For instance: we obtained technologies you
do not see anywhere else and we obtained technolo-
gies that you do not see anywhere else are possible
realizations of this structure. Leaving the relative
pronoun in the semantic structure would force one
realization to occur when it does not have to (both
outputs are equally correct and meaning-equivalent
to the other). Similarly, marking the Soviet space
program as backgrounded leaves some doors open
when it comes to surface realization: Cosmos, the
Soviet space program vs. Cosmos (the Soviet space
program) vs. the Soviet space program Cosmos (if
Cosmos is backgrounded too) are possible realiza-
tions of this substructure.
ELABORATION is an example of a meta-node
needed to connect the semantic structure: Cosmos
and program have a semantic relation, but neither is
actually in the semantic frame of the other?which
is why the introduction of an extra node cannot be
avoided. In this case, we could have a node NAME,
but ELABORATION is much more generic and can
actually be automatically introduced without any ad-
ditional information.
5 Experiments
Obviously, the removal of syntactic features from
a given standard annotation, with the goal to ob-
tain an increasingly more semantic annotation, can
only be accepted if the quality of (deep) stochas-
tic generation does not unacceptably decrease. To
assess this aspect, we converted automatically the
PropBank annotation of the WSJ journal as used in
the CoNLL shared task 2009 into an annotation that
complies with all of the principles sketched above
for deep statistical generation and trained (Bohnet
et al, 2010)?s generator on this new annotation.11
For our experiments, we used the usual training,
development and test data split of the WSJ cor-
pus (Langkilde-Geary, 2002; Ringger et al, 2004;
Bohnet et al, 2010); Table 1 provides an overview
of the used data.
set section # sentences
training 2 - 21 39218
development 24 1334
test 23 2400
Table 1: Data split of the used data in the WSJ Corpus
The resulting BLEU score of our experiment was
0.64, which is comparable with the accuracy re-
ported in (Bohnet et al, 2010) (namely, 0.659), who
used an annotation that still contained all functional
nodes (such that their generation task was consider-
ably more syntactic and thus more straightforward).
To assess furthermore whether the automatically
converted PropBank already offers some advantages
to other applications than generation, we used it in a
semantic role labeling (SRL) experiment with (Bjo?
rkelund et al, 2010)?s parser. The achieved overall
accuracy is 0.818, with all analysis stages (including
the predicate identification stage) being automatic,
which is a rather competitive figure. In the original
CoNLL SRL setting with Oracle reading, an accu-
racy of 0.856 is achieved.
Another telling comparison can be made between
the outcomes of the First Surface Realization Shared
Task (Belz et al, 2011), in which two different
input representations were given to the competing
teams: a shallow representation and a deep repre-
sentation. The shallow structures were unordered
syntactic dependency trees, with all the tokens of
the sentence, and the deep structures were predicate-
argument graphs with some nodes removed (see
Section 2). Although the performance of shallow
generators was higher than the performance of the
deep generators (the StuMaBa shallow generator
(Bohnet et al, 2011a) obtained a BLEU score of
0.89, as opposed to 0.79 of the StuMaBa deep gen-
11Obviously, our conversion can be viewed only preliminary.
It does not take into account all the subtleties that need to be
taken account?for instance, with respect to the information
structure; see also Section 6.
28
Figure 2: Illustration of the semantic annotation of the sentence Through the development of Cosmos, the Soviet space
program, we obtained technologies you do not see anywhere else.
erator), the difference is not as striking as one would
expect.12
6 Conclusions
Our experiments and the Surface Realization Shared
Task 2011 suggest that making the deep annotation
more semantic does not necessarily imply an unsur-
mountable problem for stochastic generation. We
can thus conclude that deriving automatically a deep
semantic annotation from PropBank allowed us to
obtain very promising results, both for NLG and
SRL. By sticking to universal predicate-argument
structures, as PropBank does, we maintain the po-
tential of the corpus to be mapped to other, more id-
iosyncratic, annotations. Still, automatic conversion
will always remain deficient. Thus, a flawless iden-
tification of semantic predication cannot be guaran-
teed. For instance, when an actancial arc points to a
preposition, it is not clear how to deduce whether
this preposition is semantic or lexical. Also, the
treatment of phraseological nodes is problematic,
as is the annotation of a comprehensive informa-
12Note that our results mentioned above cannot be directly
compared with the StuMaBa results during the Generation
Challenges 2011 because the realizers are different.
tion structure: the criteria for the automatic deriva-
tion of the information structure from the syntactic
structure and the topology of a sentence can only
be superficial and likely to be even less efficient in
longer and complex sentences. The annotation of
intersentential coreferences and the identification of
gapped elements are further major hurdles for an au-
tomatic derivation of a truly semantic resource. As
a consequence, we believe that new annotation poli-
cies are needed to obtain a high quality semantic re-
source. The best strategy is to start with a conver-
sion of an existing semantically annotated treebank
such as PropBank, revising and extending the result
of this conversion in a manual concerted action?
always following truly semantic annotation policies.
Acknowledgments
We would like to thank the reviewers for their valu-
able comments and suggestions and the Penn Tree-
bank/PropBank/NomBank team, without whom our
experiments would not be possible. Many thanks
also to Mike White for the useful discussions on
some of the topics discussed in the paper. Although
we might still not agree on all of the details, he
made us see the task of generation-oriented annota-
29
tion from another angle and revise some of our ini-
tial assumptions.
References
S. Bangalore and O. Rambow. 2000. Exploiting a Proba-
bilistic Hierarchical Model for Generation. In Proc. of
COLING ?00.
A. Belz, M. White, D. Espinosa, D. Hogan, and A. Stent.
2011. The First Surface Realization Shared Task:
Overview and Evaluation Results. In ENLG11.
A. Bjo?rkelund, B. Bohnet, L. Hafdell, and P. Nugues.
2010. A high-performance syntactic and semantic de-
pendency parser. In Proc. of COLING ?10: Demon-
stration Volume.
B. Bohnet, L. Wanner, S. Mille, and A. Burga. 2010.
Broad coverage multilingual deep sentence generation
with a stochastic multi-level realizer. In Proc. of COL-
ING ?10.
B. Bohnet, S. Mille, B. Favre, and L. Wanner. 2011a.
<STUMABA>: From Deep Representation to Sur-
face. In ENLG11.
B. Bohnet, S. Mille, and L. Wanner. 2011b. Statisti-
cal language generation from semantic structures. In
Proc. of International Conference on Dependency Lin-
guistics.
M. Buch-Kromann, M. Gylling-J?rgensen, L. Jelbech-
Knudsen, I. Korzen, and H. Mu?ller. 2011. The
inventory of linguistic relations used in the Copen-
hagen Dependency Treebanks. www.cbs.dk/ con-
tent/download/149771/1973272/file.
B. Comrie. 1976. Aspect. Cambridge University Press,
Cambridge.
P. Cook and J. Payne. 2006. Information Structure and
Scope in German. In LFG06.
A. Copestake, D. Flickinger, and I. Sag. 1997. Minimal
recursion semantics. Technical report, CSLI, Stanford
University, Stanford.
J. Gundel, N. Hedberg, and R. Zacharski. 1989. Give-
ness, Implicature and Demonstrative Expressions in
English Discourse. In CLS-25, Part II (Parasession
on Language in Context), pages 89?103. Chicago Lin-
guistics Society.
J.K. Gundel. 1988. ?Universals of Topic-Comment
Structure?. In M. Hammond, E. Moravc?ik, and
J. Wirth, editors, Studies in Syntactic Typology. John
Benjamins, Amsterdam & Philadelphia.
J. Hajic?, J. Panevova?, E. Hajic?ova?, P. Sgall, P. Pa-
jas, J. S?te?pa?nek, J. Havelka, M. Mikulova?, and
Z. Z?abokrtsky?. 2006. Prague Dependency Treebank
2.0.
K. Knight and V. Hatzivassiloglou. 1995. Two-level,
many paths generation. In Proc. of ACL ?95.
I. Langkilde and K. Knight. 1998. Generation that ex-
ploits corpus-based statistical knowledge. In Proc. of
COLING/ACL ?98.
I. Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sen-
tence generator. In Proc. of 2nd INLG Conference.
F. Mairesse, M Gas?ic?, F. Juric???c?, S Keizer, B. Thomson,
K. Yu, and S. Young. 2010. Phrase-based statistical
language generation using graphical models and active
learning. In Proc. of ACL ?10.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. SUNY Press, Albany.
I.A. Mel?c?uk. 2001. Communicative Organization
in Natural Language (The Semantic-Communicative
Structure of Sentences). Benjamins Academic Pub-
lishers, Amsterdam.
M. Mikulova? et al 2006. Annotation on
the tectogrammatical level in the Prague
Dependency Treebank: Reference book.
www.cbs.dk/content/download/149771/ 1973272/file.
A.H. Oh and A.I. Rudnicky. 2000. Stochastic language
generation for spoken dialogue systems. In Proc. of
ANL/NAACL Workshop on Conversational Systems.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?105.
E. Ringger, M. Gamon, R.C. Moore, D. Rojas, M. Smets,
and S. Corston-Oliver. 2004. Linguistically informed
statistical models of constituent structure for ordering
in sentence realization. In Proceedings of COLING,
pages 673?679.
P. Sgall, E. Hajic?ova?, and J. Panevova?. 1986. The Mean-
ing of the Sentence in its Semantic and Pragmatic As-
pects. Reidel Publishing Company, Dordrecht.
J. F. Sowa. 2000. Knowledge Representation: Logi-
cal, Philosophical, and Computational Foundations.
Brooks Cole Publishing Co., Pacific Grove, CA, USA.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sen-
tence planning for complex information presentation
in spoken dialog systems. In Proc. of ACL ?04.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and
semantic dependencies. In Proceedings of the 12th
CoNLL-2008.
M.A. Walker, O.C. Rambow, and M. Rogati. 2002.
Training a sentence planner for spoken dialogue using
boosting. Computer Speech and Language, 16:409?
433.
Y.W. Wong and R.J. Mooney. 2007. Generation by in-
verting a semantic parser that uses statistical machine
translation. In Proc. of the HLT Conference.
30
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 136?140,
Utica, May 2012. c?2012 Association for Computational Linguistics
The Surface Realisation Task: Recent Developments and Future Plans
Anja Belz
Computing, Engineering and Maths
University of Brighton
Brighton BN1 4GJ, UK
a.s.belz@brighton.ac.uk
Bernd Bohnet
Institute for Natural Language Processing
University of Stuttgart
70174 Stuttgart
bohnet@ims.uni-stuttgart.de
Simon Mille, Leo Wanner
Information and Communication Technologies
Pompeu Fabra University
08018 Barcelona
<firstname>.<lastname>@upf.edu
Michael White
Department of Linguistics
Ohio State University
Columbus, OH, 43210, US
mwhite@ling.osu.edu
Abstract
The Surface Realisation Shared Task was first
run in 2011. Two common-ground input rep-
resentations were developed and for the first
time several independently developed surface
realisers produced realisations from the same
shared inputs. However, the input representa-
tions had several shortcomings which we have
been aiming to address in the time since. This
paper reports on our work to date on improv-
ing the input representations and on our plans
for the next edition of the SR Task. We also
briefly summarise other related developments
in NLG shared tasks and outline how the dif-
ferent ideas may be usefully brought together
in the future.
1 Introduction
The Surface Realisation (SR) Task was introduced
as a new shared task at Generation Challenges 2011
(Belz et al, 2011). Our aim in developing the SR
Task was to make it possible, for the first time, to
directly compare different, independently developed
surface realisers by developing a ?common-ground?
representation that could be used by all participat-
ing systems as input. In fact, we created two dif-
ferent input representations, one shallow, one deep,
in order to enable more teams to participate. Corre-
spondingly, there were two tracks in SR?11: In the
Shallow Track, the task was to map from shallow
syntax-level input representations to realisations; in
the Deep Track, the task was to map from deep
semantics-level input representations to realisations.
By the time teams submitted their system outputs,
it had become clear that the inputs required by some
types of surface realisers were more easily derived
from the common-ground representation than the in-
puts required by other types. There were other re-
spects in which the representations were not ideal,
e.g. the deep representations retained too many syn-
tactic elements as stopgaps where no deeper infor-
mation had been available. It was clear that the in-
put representations had to be improved for the next
edition of the SR Task. In this paper, we report on
our work in this direction so far and relate it to some
new shared task proposals which have been devel-
oped in part as a response to the above difficulties.
We discuss how these developments might usefully
be integrated, and outline plans for SR?13, the next
edition of the SR Task.
2 SR?11
The SR?11 input representations were created by
post-processing the CoNLL 2008 Shared Task
data (Surdeanu et al, 2008), for the preparation of
which selected sections of the WSJ Treebank were
converted to syntactic dependencies with the Pen-
nconverter (Johansson and Nugues, 2007). The
resulting dependency bank was then merged with
Nombank (Meyers et al, 2004) and Propbank
(Palmer et al, 2005). Named entity information
from the BBN Entity Type corpus was also incorpo-
rated. The SR?11 shallow representation was based
on the Pennconverter dependencies, while the deep
representation was derived from the merged Nom-
bank, Propbank and syntactic dependencies in a pro-
136
cess similar to the graph completion algorithm out-
lined by Bohnet (2010).
Five teams submitted a total of six systems to
SR?11 which we evaluated automatically using a
range of intrinsic metrics. In addition, systems were
assessed by human judges in terms of Clarity, Read-
ability and Meaning Similarity.
The four top-performing systems were all statis-
tical dependency realisers that do not make use of
an explicit, pre-existing grammar. By design, statis-
tical dependency realisers are robust and relatively
easy to adapt to new kinds of dependency inputs
which made them well suited to the SR?11 Task. In
contrast, there were only two systems that employed
a grammar, either hand-crafted or treebank-derived,
and these did not produce competitive results. Both
teams reported substantial difficulties in converting
the common ground inputs into the ?native? inputs
required by their systems.
The SR?11 results report pointed towards two
kinds of possible improvements: (i) introducing (ad-
ditional) tasks where performance would not depend
to the same extent on the relation between common-
ground and native inputs, e.g. a text-to-text shared
task on sentential paraphrasing; and (ii) improving
the representations themselves. In the remainder of
this paper we report on developments in both these
directions.
3 Towards SR?13
As outlined above, the first SR Shared Task turned
up some interesting representational issues that re-
quired some in-depth investigation. In the end, it
was this fact that led to the decision to postpone
the 2nd SR Shared Task until 2013 in order to al-
low enough time to address these issues properly. In
this section, we describe our plans for SR?13 to the
extent to which they have progressed.
3.1 Task definition
As in the first SR task, the participating teams will
be provided with annotated corpora consisting of
common-ground input representations and their cor-
responding outputs. Two kinds of input will be of-
fered: deep representations and surface representa-
tions. The deep input representations will be se-
mantic graphs; the surface representations syntactic
trees. Both will be derived from the Penn Treebank.
The task will consist in the generation of a text start-
ing from either of the input representations.
3.2 Changes to the input representations
During the working group discussions which fol-
lowed SR?11, it became apparent that the CoNLL
syntactic dependency trees overlaid with Prop-
bank/Nombank relations had turned out to be inade-
quate in various respects for the purpose of deriving
a suitable semantic representation. For instance:
? Governed prepositions are not distinguished
from semantically loaded prepositions in the
CoNLL annotation. In SR?11, only strongly
governed prepositions such as give something
TO someone were removed, but in many cases
the meaning of a preposition which introduces
an argument (of a verb, a noun, an adjective
or an adverb) clearly depends on the predicate:
believe IN something, account FOR some-
thing, etc. In those cases, too, the preposition
should be removed from the semantic annota-
tion, since the relisers have to be able to intro-
duce non-semantic features un-aided. On the
contrary, semantically loaded governed prepo-
sitions such as live IN a flat/ON a roof/NEXT
TO the main street etc. should be retained in
the annotation. These prepositions all receive
argumental arcs in PropBank/NomBank, so it
is not easy to distinguish between them. One
possibility would be to target a restricted list of
prepositions which are void of meaning most of
the time, and remove those prepositions when
they introduce arguments.
? The annotation of relative pronouns did not
survive the conversion of the original Penn
Treebank to the CoNLL format unscathed: the
antecedent of the relative pronoun is sometimes
lost or the relative pronoun is not annotated,
predominantly because the predicate which the
relative pronoun is an argument of was not con-
sidered to be a predicate by annotators, as in
the degree TO WHICH companies are irritated.
However, in the original constituency annota-
tion, the traces allow for retrieving antecedents
and semantic governors, hence using this orig-
137
inal annotation could be useful in order to get a
clean annotation of such phenomena.
Agreement has been reached on a range of other is-
sues, although the feasibility of implementing the
corresponding changes might have to be further
evaluated:
? Coordinations should be annotated in the se-
mantic representation with the conjunction as
the head of all the conjuncts. This treatment
would allow e.g. an adequate representation of
sharing of dependents among the conjuncts.
? The inversion of ?modifier? arcs and the intro-
duction of meta-semantemes would avoid an-
ticipating syntactic decisions such as the direc-
tion of non-argumental syntactic edges, and al-
low for connecting unconnected parts of the se-
mantic structures.
? In order to keep the scope of various phenom-
ena intact after inverting non-argumental edges,
we should explicitly mark the scope of e.g.
negations, quantifiers, quotation marks etc. as
attribute values on the nodes.
? Control arcs should be removed from the se-
mantic representation since they do not provide
information relevant at that level.
? Named entities will be further specified adding
a reduced set of named entity types from the
BBN annotations.
Finally, we will perform automatic and manual qual-
ity checks in order to ensure that the proposed
changes are adequately introduced in the annotation.
3.3 Evaluation
We will once again follow the main data set divi-
sions of the CoNLL?08 data (training set = WSJ Sec-
tions 02?21; development set = Section 24; test set =
Section 23), with the proviso that we have removed
300 randomly selected sentences from the develop-
ment set for use in human evaluations. Of these, we
used 100 sentences in SR?11 and will use a different
100 in SR?13.
Evaluation criteria identified as important for
evaluation of surface realisation output in previous
work include Adequacy (preservation of meaning),
Fluency (grammaticality/idiomaticity), Clarity, Hu-
manlikeness and Task Effectiveness. We will aim to
evaluate system outputs submitted by SR?13 partic-
ipants in terms of most of these criteria, using both
automatic and human-assessed methods.
As in SR?11, the automatic evaluation metrics (as-
sessing Humanlikeness) will be BLEU, NIST, TER
and possibly METEOR. We will apply text normal-
isation to system outputs before scoring them with
the automatic metrics. For n-best ranked system
outputs, we will again compute a single score for all
outputs by computing their weighted sum of their
individual scores, where a weight is assigned to a
system output in inverse proportion to its rank. For
a subset of the test data we may obtain additional al-
ternative realisations via Mechanical Turk for use in
the automatic evaluations.
We are planning to expand the range of human-
assessed evaluation experiments (assessing Ade-
quacy, Fluency and Clarity) to the following meth-
ods:
1. Preference Judgement Experiment (C2, C3):
Collect preference judgements using an exist-
ing evaluation interface (Kow and Belz, 2012)
and directly recruited evaluators. We will
present sentences in the context of a chunk of
5 consecutive sentences to the evaluators, and
ask for separate judgements for Clarity, Flu-
ency and Meaning Similarity.
2. HTER (Snover et al, 2006): In this evaluation
method, human evaluators are asked to post-
edit the output of a system, and the edits are
then categorised and counted. Crucial to this
evaluation method is the construction of clear
instructions for evaluators and the categorisa-
tion of edits. We will categorise edits as relat-
ing to Meaning Similarity, Fluency and/or Clar-
ity; we will also consider further subcategorisa-
tions.
We will once again provide evaluation scripts to par-
ticipants so they can perform automatic evaluations
on the development data. These scores serve two
purposes. Firstly, development data scores must be
included in participants? reports. Secondly, partici-
138
pants may wish to use the evaluation scripts in de-
veloping and tuning their systems.
We will report per-system results separately for
the automatic metrics (4 sets of results), and for the
human-assessed measures (2 sets of results). For
each set of results, we will report single-best and
n-best results. For single-best results, we may fur-
thermore report results both with and without miss-
ing outputs. We will rank systems, and report sig-
nificance of pairwise differences using bootstrap re-
sampling where necessary (Koehn, 2004; Zhang and
Vogel, 2010). We will separately report correlation
between human and automatic metrics, and between
different automatic metrics.
3.4 Assessing different aspects of realisation
separately
In addition, we will consider measuring different as-
pects of the realisation performance of participating
systems (syntax, word order, morphology) since a
system can perform well on one and badly on an-
other. For instance, a system might perform well
on morphological realisation while it has poor re-
sults on linearisation. We would like to capture this
fact. This may involve asking participating teams to
submit intermediate representations or identifiers to
identify the reference words. This more fine-grained
approach should help us to obtain a more precise
picture of the state of affairs in the field and could
help to reveal the respective strengths of different
surface realisers more clearly.
4 Related Developments
4.1 Syntactic Paraphrase Ranking
The new shared task on syntactic paraphrase ranking
described elsewhere in this volume (White, 2012) is
intended to run as a follow-on to the main surface
realisation shared task. Taking advantage of the hu-
man judgements collected to evaluate the surface re-
alisations produced by competing systems, the task
is to automatically rank the realisations that differ
from the reference sentence in a way that agrees with
the human judgements as often as possible. The task
is designed to appeal to developers of surface real-
isation systems as well as machine translation eval-
uation metrics. For surface realisation systems, the
task sidesteps the thorny issue of converting inputs
to a common representation. Developers of reali-
sation systems that can generate and optionally rank
multiple outputs for a given input will be encouraged
to participate in the task, which will test the system?s
ability to produce acceptable paraphrases and/or to
rank competing realisations. For MT evaluation
metrics, the task provides a challenging framework
for advancing automatic evaluation, as many of the
paraphrases are expected to be of high quality, dif-
fering only in subtle syntactic choices.
4.2 Content Selection Challenge
The new shared task on content selection has been
put forward (Bouayad-Agha et al, 2012) to initi-
ate work on content selection from a common, stan-
dardised semantic-web format input, and thus pro-
vide the context for an objective assessment of dif-
ferent content selection strategies. The task con-
sists in selecting the contents communicated in ref-
erence biographies of celebrities from a large vol-
ume of RDF-triples. The selected triples will be
evaluated against a gold triple selection set using
standard quality assessment metrics.
The task can be considered complementary to the
surface realisation shared task in that it contributes
to the medium-term goal of setting up a task that
covers all stages of the generation pipeline. In fu-
ture challenges, it can be explored to what extent and
how the output content plans can be mapped onto
semantic representations that serve as input to the
surface realisers.
5 Plans
We are currently working on the new improved
common-ground input representation scheme and
converting the data to the new scheme.
The provisional schedule for SR?13 looks as
follows:
Announcement and call for expres-
sions of interest:
6 July 2012
Preliminary registration and release
of description of new representations:
27 July 2012
Release of data and documentation: 2 Nov 2012
System Submission Deadline: 10 May 2013
Evaluation Period: 10 May?
10 Jul 2013
Provisional dates for results session: 8?9 Aug 2013
139
6 Conclusion
For a large number of NLP applications (among
them, e.g., text generation proper, summarisation,
question answering, and dialogue), surface realisa-
tion (SR) is a key technology. Unfortunately, so
far in nearly all of these applications, idiosyncratic,
custom-made SR implementations prevail. How-
ever, a look over the fence at the language analy-
sis side shows that the broad use of standard de-
pendency treebanks and semantically annotated re-
sources such as PropBank and NomBank that were
created especially with parsing in mind led to stan-
dardised high-quality off-the-shelf parser implemen-
tations. It seems clear that in order to advance the
field of surface realisation, the generation commu-
nity also needs adequate resources on which large-
scale experiments can be run in search of the surface
realiser with the best performance, a surface realiser
which is commonly accepted, follows general trans-
parent principles and is thus usable as plug-in in the
majority of applications.
The SR Shared Task aims to contribute to this
goal. On the one hand, it will lead to the creation
of NLG-suitable resources in that it will convert
the PropBank into a more semantic and more com-
pletely annotated resource. On the other hand, it will
offer a forum for the presentation and evaluation of
various approaches to SR and thus help us to search
for the best solution to the SR task with the greatest
potential to become a widely accepted off-the-shelf
tool.
Acknowledgments
We gratefully acknowledge the contributions to dis-
cussions and development of ideas made by the
other members of the SR working group: Miguel
Ballesteros, Johan Bos, Aoife Cahill, Josef van Gen-
abith, Pablo Gerva?s, Deirdre Hogan and Amanda
Stent.
References
Anja Belz, Michael White, Dominic Espinosa, Deidre
Hogan, Eric Kow, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG?11), pages 217?226. Association for Compu-
tational Linguistics.
Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics, Beijing, China.
Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wanner,
and Chris Mellish. 2012. Content selection from
semantic web data. In Proceedings of the 7th In-
ternational Natural Language Generation Conference
(INLG?12).
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, Proceedings of NODALIDA
2007, pages 105?112, Tartu, Estonia.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Eric Kow and Anja Belz. 2012. LG-Eval: A toolkit
for creating online language evaluation experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?12).
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In NAACL/HLT Workshop Frontiers in Corpus
Annotation.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: A corpus annotated with
semantic roles. In Computational Linguistics Journal,
pages 71?105.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL?08), Manchester, UK.
Michael White. 2012. Shared task proposal: Syntac-
tic paraphrase ranking. In Proceedings of the 7th In-
ternational Natural Language Generation Conference
(INLG?12).
Ying Zhang and Stephan Vogel. 2010. Significance tests
of automatic machine translation evaluation metrics.
Machine Translation, 24:51?65.
140
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 146?149,
Utica, May 2012. c?2012 Association for Computational Linguistics
Content Selection From Semantic Web Data
Nadjet Bouayad-Agha1
Gerard Casamayor1
Leo Wanner1,2
1DTIC, University Pompeu Fabra
2Institucio? Catalana de Recerca i Estudis Avanc?ats
Barcelona, Spain
firstname.lastname@upf.edu
Chris Mellish
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
c.mellish@abdn.ac.uk
Abstract
So far, there has been little success in Natural
Language Generation in coming up with gen-
eral models of the content selection process.
Nonetheless, there has been some work on
content selection that employ Machine learn-
ing or heuristic search. On the other side, there
is a clear tendency in NLG towards the use of
resources encoded in standard Semantic Web
representation formats. For these reasons, we
believe that time has come to propose an initial
challenge on content selection from Semantic
Web data. In this paper, we briefly outline the
idea and plan for the execution of this task.
1 Motivation
So far, there has been little success in Natural Lan-
guage Generation in coming up with general mod-
els of the content selection process. Most of the
researchers in the field agree that this lack of suc-
cess is because the knowledge and context (commu-
nicative goals, user profile, discourse history, query,
etc) needed for this task depend on the application
domain. This often led in the past to template-
or graph-based combined content selection and dis-
course structuring approaches operating on idiosyn-
cratically encoded small sets of input data. Fur-
thermore, in many NLG-applications, target texts
and sometimes even empirical data are not avail-
able, which makes it difficult to employ empirical
approaches to knowledge elicitation. Nonetheless,
during the last decade, there has been a steady flow
of new work on content selection that employed Ma-
chine learning (Barzilay and Lapata, 2005; Duboue
and McKeown, 2003; Jordan and Walker, 2005;
Kelly et al, 2009), heuristic search (O?Donnell et
al., 2001; Demir et al, 2010; Mellish and Pan,
2008), or a combination thereof (Bouayad-Agha et
al., 2011). All of these strategies can deal with large
volumes of data.
On the other side, there is a clear tendency in NLG
towards the use of resources encoded in terms of
standard Semantic Web representation formats such
as OWL and RDF, e.g., (Wilcock and Jokinen, 2003;
Bontcheva and Wilks, 2004; Mellish and Pan, 2008;
Power and Third, 2010; Bouayad-Agha et al, 2011;
Dannells et al, 2012), to name but a few. However,
although most of these works make a good attempt
at realisation, the problem of content determination
from Semantic Web data is relatively untouched.
For these reasons, we believe that the time has
come to bring together researchers working on (or
interested in working on) content selection to par-
ticipate in a challenge for this task using standard
freely available web data as input. The availability
of open modular multi-domain multi-billion triple
data and of open ontological resources (Bizer et al,
2009) presented in a standard knowledge represen-
tation formalism make semantic web data a natural
choice for such a challenge.
As will be presented below, this initial challenge
presents a relatively simple content selection task
with no user model and a straightforward commu-
nicative goal so that people are encouraged to take
part and motivated to stay on for later challenges, in
which the task will be successively enhanced from
gained experience.
A content determination challenge would be a
chance to (i) directly compare the performance of
146
different types of content selection strategies; (ii)
contribute towards developing a standard ?off-the-
shelf? content selection module; and (iii) contribute
towards a standard interface between text planning
and linguistic generation.
To get the widest reception possible, the challenge
will be open to any approach, be it template-, rule-
or heuristic-based, or empirical. Furthermore, it will
be advertised in the Semantic Web Community to
get contributors from other horizons, see, e.g., (Dai
et al, 2010).
In what follows, we briefly outline the idea and
plan for the execution of the challenge. In Section 2,
we outline a description of the task. In Section 3,
the data and domain that will be used are presented.
Section 4 describes how this data is to be prepared
for the task, and Section 5 how it will be released to
the participants. In Section 6, we sketch the eval-
uation including the preparation of the evaluation
dataset. Section 7 gives a proposed schedule for
each of the tasks involved in organizing the chal-
lenge. Finally, in Section 8, we provide short bi-
ographies of the members of the organization team,
focusing on their experience in the proposed task.
2 Task Description
The core of the task to be addressed can be formu-
lated as follows:
Build a system which, given a set of RDF
triples containing facts about a celebrity
and a target text (for instance, a wikipedia-
style article about that person), selects
those triples that are reflected in the target
text.
The participants are also free to consider the se-
mantics defined by the data sources in their ap-
proach, rely on additional resources like ontologies
from other sources, or disregard the semantics com-
pletely.
The implemented system should output its results
in a predefined standard format that can be used for
automatic evaluation.
It could be that the RDF data does not contain ev-
erything that would ideally be included in such an
article, but that is ignored here. The task consists in
selecting content that is communicated in the target
text.
3 The data
The domain will be constituted by short biographies
of famous people. This is an interesting domain for
the challenge because Semantic Web data and corre-
sponding texts for this domain are available in large
quantities (e.g., DBPedia or Freebase for the data
and many other sources for biography texts, among
them Wikipedia).
The data will consist, for each famous person, of
a pair of RDF-triple set and associated text(s). For
each pair, the RDF data will include both informa-
tion communicated and excluded from the text. The
text may convey information not present in the RDF-
triples, but this will be kept to a minimum, always
subject to using naturally-occurring texts. All pairs
should contain enough RDF-triples and text to make
the pair interesting for the content selection task.
When choosing data for the challenge, we will
prefer semantic contents classified under consistent
ontologies over plain Linked Data with no explicit
semantics. The semantics of the RDF data (vocab-
ularies, ontologies) will be provided, preferably en-
coded in Semantic Web standards (e.g., in RDFS or
OWL).
4 Data Preparation
The task of data preparation consists in 1) data gath-
ering and preparation, which is to be carried out by
the organizers, and 2) working dataset selection and
annotation, which is to be carried out by both the
organizers and participants.
4.1 Data gathering and preparation
This preparatory stage consists in choosing the
repository sources, downloading the relevant on-
tologies (to the extent those will be provided), and
downloading and pairing the data and associated
texts (= the paired corpus).
4.2 Working Dataset selection and annotation
The participants will be asked to participate in a pre-
liminary task consisting in marking which triples are
included in the text given a subset of the paired cor-
pus (the size of the subset still has to be decided).
This task could be supported by some automatic
anchoring techniques such as used in (Duboue and
McKeown, 2003; Barzilay and Lapata, 2005). The
147
objectives of the task are threefold: (1) to provide all
participants with a common set of ?correct answers?
to be exploited in their approach, (2) to familiarize
the participants with the nature of the contents, their
semantics and the texts, and (3) to provide the task
with a ceiling for the evaluation, i.e. inter-annotator
agreement.
Annotation guidelines will be needed to ensure
that all participants follow the same procedure when
annotating texts. For this purpose, an early docu-
ment will be produced detailing the procedure to-
gether with examples and descriptions of relevant
problems such as ambiguities in the annotation. The
guidelines will be improved in multiple stages of an-
notation and revision with the goal of maximizing
inter-annotator agreement.
5 Data release
The participants in the challenge will be given ac-
cess to the set of all correct answers and a large
portion of the non-marked paired corpus, as well as
their semantics (i.e., ontologies and the like). The
remaining unseen, non-marked set will be kept for
evaluation.
6 Evaluation
The evaluation consists of 1) a preparatory stage for
selecting and annotating the evaluation dataset, and
2) an evaluation stage.
6.1 Evaluation dataset selection and annotation
Once all participants have submitted their exe-
cutable to solve the task, the evaluation set will be
processed. If timing is tight, however, this could be
done whilst the participants are still working on the
task or extra effort (for instance, from the organiz-
ers) could be brought in. A subset of the data is
randomly selected and annotated with the selected
triples by the participants. This two-stage approach
to triple selection annotation is proposed in order to
avoid any bias on the evaluation data.
6.2 Evaluation
Each executable is run against the test corpus and the
selected triples evaluated against the gold triple se-
lection set. Since this is formally a relatively simple
task of selecting a subset of a given set, we will use
for evaluation standard precision, recall and F mea-
sures. In addition, other appropriate metrics will be
explored?for instance, certain metrics for extrac-
tive summarisation (which is to some extent a simi-
lar task).
The organizers will explore whether it will be fea-
sible to select and annotate some test examples from
a different corpus and have the systems evaluated on
these as a separate task.
7 Schedule
Table 1 presents the different tasks, protagonists and
the schedule involved in the organization of the chal-
lenge. The challenge proper will take place between
November 2012 and May/June 2013.
8 Organizers
Nadjet Bouayad-Agha has been a lecturer and re-
searcher at DTIC, UPF, since 2002. She obtained
her PhD on Text Planning in 2001 from the Univer-
sity of Brighton and has been working ever since her
postgraduate studies at the University of Paris VII in
NLG, more specifically on Text Planning. In recent
years her focus has been on how to exploit semantic
web representations and technologies for Text Plan-
ning in general and content selection in particular.
Gerard Casamayor is a PhD student at DTIC,
UPF, working on text planning from general-
purpose semantic data. His main interests are ma-
chine learning and interactive, collaborative text
planning. As part of his thesis, he is developing a
text planning approach that can be trained directly
by domain experts, minimizing the need of encoding
or annotating prior knowledge about how to solve
the task.
Chris Mellish has been a professor at the Univer-
sity of Aberdeen since 2003, when he moved from a
similar position at the University of Edinburgh. He
has been doing research in NLG since 1984 and or-
ganised the second European NLG workshop. His
work on content selection includes the opportunis-
tic planning approach used by the ILEX system and
a rule-based approach to content selection from se-
mantic web data presented in ENLG 2011.
Leo Wanner has been ICREA Research Profes-
sor at DTIC, UPF, since 2005. Before, he was
148
What? Who? When?
Data gathering and preparation Organizers Summer 2012
Working dataset selection and annotation Organizers and Participants Sept/Oct 2012
Data Release Organizers November 2012
Evaluation dataset selection and annotation Organizers and Participants May 2013
Evaluation Organizers June 2013
Publication@INLG Organizers August 2013
Table 1: Content Selection Challenge Organization Schedule
affiliated as Assistant Professor with the Univer-
sity of Stuttgart. Wanner is involved in research
on multilingual text generation since the late 80ies.
Among his research foci are user-oriented con-
tent selection and the interface between language-
independent ontology-based and linguistic represen-
tations in text generation.
References
Regina Barzilay and Mirella Lapata. 2005. Collec-
tive Content Selection for Concept-to-Text Genera-
tion. Proceedings of the Joint Human Language Tech-
nology and Empirical Methods in Natural Language
Processing Conferences (HLT/EMNLP-2005) Van-
couver, Canada.
Christian Bizer, Tom Heath and Tim Berners-Lee 2009.
Linked Data - The Story So Far. International Journal
on Semantic Web and Information Systems 5(3) 1?22
Kalina Bontcheva and Yorick Wilks 2004. Automatic
Report Generation from Ontologies: the MIAKT ap-
proach Nineth International Conference on Appli-
cations of Natural Language to Information Systems
(NLDB?2004) 324?335.
Nadjet Bouayad-Agha, Gerard Casamayor and Leo Wan-
ner. 2011. Content selection from an ontology-based
knowledge base for the generation of football sum-
maries Proceedings of the 13th European Workshop
on Natural Language Generation (ENLG?2011) 72?
81 Nancy, France.
Yintang Dai, Shiyong Zhang, Jidong Chen, Tianyuan
Chen and Wei Zhang. 2010 Semantic Network Lan-
guage Generation based on a Semantic Networks Seri-
alization Grammar World Wide Web 13:307341
Dana Danne?lls, Mariana Damova, Ramona Enache and
Milen Chechev. 2012 Multilingual Online Genera-
tion from Semantic Web Ontologies Proceedings of
the 21st International Conference on World Wide Web
(WWW?12) 239?242
Seniz Demir, Sandra Carberry and Kathleen F. McCoy.
2010. A Discourse-Aware Graph-Based Content-
Selection Framework. Proceedings of the Interna-
tional Language Generation Conference. Sweden.
Pablo A. Duboue and Kathleen R. McKeown. 2003. Sta-
tistical Acquisition of Content Selection Rules for Nat-
ural Language Generation. Proceedings of the 2003
conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP). Sapporo, Japan.
Dimitrios Galanis and Ion Androutsopoulos. 2007. Gen-
erating Multilingual Personalized Descriptions from
OWL Ontologies on the Semantic Web: the Natu-
ralOWL System. Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation
(ENLG07)
Pamela W. Jordan and Marilyn A. Walker 2005 Learning
content selection rules for generating object descrip-
tions in dialogue Journal of Artificial Intelligence Re-
search 24, 157?194.
Colin Kelly, Ann Copestake, and Nikiforos Karamanis.
2009 Investigating content selection for language gen-
eration using machine learning. Proceedings of the
12th European Workshop on Natural Language Gen-
eration.. 130?137.
Chris Mellish and Jeff Z. Pan. 2008 Language Di-
rected Inference from Ontologies. Artificial Intelli-
gence. 172(10):1285-1315.
Mick ODonnell, Chris Mellish, Jon Oberlander, and Alis-
tair Knott. 2001. ILEX: an architecture for a dynamic
hypertext generation system. Natural Language Engi-
neering. 7(3):225?250.
Richard Power and Allan Third 2010 Expressing OWL
axioms by English sentences: dubious in theory, fea-
sible in practice Proceedings of the 23rd Interna-
tional Conference on Computational Linguistics (CI-
CLING?01). 1006?1013.
Graham Wilcok and Kristiina Jokinen 2003 Generat-
ing Responses and Explanations from RDF/XML and
DAML+OIL. IJCAI03 Workshop on Knowledge and
Reasoning in Practical Dialogue Systems. 58?63.
149
Proceedings of the 14th European Workshop on Natural Language Generation, pages 98?102,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Overview of the First Content Selection Challenge from Open Semantic
Web Data
Nadjet Bouayad-Agha1
Gerard Casamayor1
Leo Wanner1,2
1DTIC, University Pompeu Fabra
2Institucio? Catalana de Recerca i Estudis Avanc?ats
Barcelona, Spain
firstname.lastname@upf.edu
Chris Mellish
Computing Science
University of Aberdeen
Aberdeen AB24 3UE, UK
c.mellish@abdn.ac.uk
Abstract
In this overview paper we present the out-
come of the first content selection chal-
lenge from open semantic web data, fo-
cusing mainly on the preparatory stages
for defining the task and annotating the
data. The task to perform was described
in the challenge?s call as follows: given a
set of RDF triples containing facts about
a celebrity, select those triples that are re-
flected in the target text (i.e., a short bi-
ography about that celebrity). From the
initial nine expressions of interest, finally
two participants submitted their systems
for evaluation.
1 Introduction
In (Bouayad-Agha et al, 2012), we presented the
NLG challenge of content selection from seman-
tic web data. The task to perform was described
as follows: given a set of RDF triples contain-
ing facts about a celebrity, select those triples that
are reflected in the target text (i.e., a short biogra-
phy about that celebrity). The task first required
a data preparation stage that involved the follow-
ing two subtasks: 1) data gathering and prepara-
tion, that is, deciding which data and texts to use,
then downloading and pairing them, and 2) work-
ing dataset selection and annotation, that is, defin-
ing the criteria/guidelines for determining when a
triple is marked as selected in the target text, and
producing a corpus of triples annotated for selec-
tion.
There were initially nine interested participants
(including the two organizing parties). Five of
which participated in the (voluntary) triple anno-
tation rounds.1 In the end, only two participants
submitted their systems:
1We would like to thank Angelos Georgaras and Stasinos
Konstantopoulos from NCSR (Greece) for their participation
in the annotation rounds.
UA: Roman Kutlak, Chris Mellish and Kees van
Deemter. Department of Computing Science
, University of Aberdeen, Scotland (UK).
UIC: Hareen Venigalla and Barbara Di iEugenio.
Department of Computer Science, University
of Illinois at Chicago (USA).
Before the presentation of the baseline evalua-
tion of the submitted systems and the discussion
of the results (Section 4), we outline the two data
preparation subtasks (Sections 2 and 3). In Sec-
tion 5, we then sketch some conclusions with re-
gard to the achievements and future of the con-
tent selection task challenge. More details about
the data, annotation and resources described in this
overview, as well as links for downloading the data
and other materials (e.g., evaluation results, code,
etc.) are available on the challenge?s website.2
2 Data gathering and preparation
We chose Freebase as our triple datastore.3,4 We
obtained the triple set for each person in the Turtle
format (ttl) by grepping the official Freebase RDF
dump released on the 30th of December 2012 for
all triples whose subject is the person?s URI; cer-
tain meta-data and irrelevant triples (i.e., triples
with specific namespaces such as ?base? or ?com-
mon?) have been filtered out.
Each triple set is paired with the person?s sum-
mary biography typically available in Wikipedia,
which consists of the first paragraph(s) preceding
the page?s table of contents5
Our final corpus consists of 60000+ pairs, all of
which follow two restrictions that are supposed to
2http://www.taln.upf.edu/cschallenge2013/
3http://www.freebase.com
4For a comparison between Freebase and DBPedia, see
http://wiki.freebase.com/wiki/DBPedia.
5For example, the first four paragraphs in the follow-
ing page constitute the summary biography of that person:
http://en.wikipedia.org/wiki/George Clooney.
98
maximize the chances of having interesting pairs
with sufficient original and selected input triples
for the challenge. Firstly, the number of unique
predicates in the input ttl must be greater than
10. The number 10 is estimated based on the
fact that a person?s nationality, date and place of
birth, profession, type and gender are almost al-
ways available and selected, such that we need a
somewhat large set to select content from in or-
der to make the task minimally challenging. Sec-
ondly, the Wikipedia-extracted summary biogra-
phy must contain more than 5 anchors and at least
20% of the available anchors, where an anchor is
a URI in the text (i.e., external href attribute value
in the html) pointing to another Wikipedia article
which is directly related to that person. Given that
most Freebase topics have a corresponding DBPe-
dia entity with a Wikipedia article, anchors found
in the introductory text are an indicator of potential
relevant facts available in Freebase and are com-
municated in the text. In other words, the anchor
threshold restriction is useful to discard pairs with
very few triples to annotate. We found this crite-
rion more reliable than the absolute length of the
text which is not necessarily proportional with the
number of triples available for that person.
3 Working Dataset selection and
annotation
The manual annotation task consisted in emulat-
ing the content selection task of a Natural Lan-
guage Generation system, by marking in the triple
dataset associated with a person the triples predi-
cated in the summary biography of that person ac-
cording to a set of guidelines. We performed two
rounds of annotations. In the first round, partic-
ipants were asked to select content for the same
three celebrities. The objectives of this annota-
tion, in which five individuals belonging to four
distinct institutions participated, were 1) for par-
ticipants to get acquainted with the content selec-
tion task envisaged, the domain and guidelines,
2) to validate the guidelines, and 3) to formally
evaluate the complexity of the task by calculat-
ing inter-annotator agreement. For the latter we
used free-marginal multi-rater Kappa, as it seemed
suited for the annotation task (i.e. independent rat-
ings, discrete categories, multiple raters, annota-
tors are not restricted in how they distribute cat-
egories across cases) (Justus, 2005). We obtained
an average Kappa of 0.92 across the three pairs for
the 5 annotators and 2 categories (selected, not se-
lected), which indicates a high level of agreement
and therefore validates our annotation guidelines.
Our objective for the second round of annota-
tions was to obtain a dataset for participants to
work with. In the end, we gathered 344 pairs from
5 individuals of 5 distinct institutions. It should be
noted that although both rounds of annotations fol-
low the anchor restriction presented in Section 2,
the idea to set a minimum number of predicates
for the larger corpus of 60000+ pairs came forth
after analysing the results of the second round and
noting the data sparsity in some pairs. In what fol-
lows, we detail how the triples were presented to
human annotators and what were the annotation
criteria set forth in the guidelines.
3.1 Data presentation
A machine-readable triple consists of a subject
which is a Freebase machine id (mid), a predicate
and an object which can either be a Freebase mid
or a literal, as shown in the following two triples:
ns:m.0dvld
ns:people.person.spouse_s
ns:m.02kknf3 .
ns:m.0dvld
ns:people.person.date_of_birth
"1975-10-05"??xsd:datetime .
Triples were transformed into a human-readable
form. In particular, each mid in object position
(e.g., 02kknf3) was automatically mapped onto
an abbreviated description of the Freebase topic
it refers to. Thus, the triples above have been
mapped onto a tabular form consisting of (1) pred-
icate, (2) object description, (3) object id, and (4)
object types (for literals):
(1) /people/person/spouse_s
(2) "1998-11-22 - Jim Threapleton -
2001-12-13 - Marriage -
Freebase Data Team - Marriage"
(3) /m/02kknf3
(1) /people/person/date_of_birth
(2) value
(3) "1975-10-05"
(4) "datetime"
For each triple thus presented, annotators were
asked to mark 1) whether it was selected, 2) in
which sentence(s) of the text did it appear, and
3) which triples, if any, are its coreferents. Two
triples are coreferent if their overlap in meaning is
such that either of them can be selected to repre-
sent the content communicated by the same text
99
fragment and as such should not count as two sep-
arate triples in the evaluation. Thus, the same text
might say He is probably best known for his stint
with heavy metal band Godsmack and He has also
toured and recorded with a number of other bands
including Detroit based metal band Halloween
?The Heavy Metal Horror Show? . . . , thus refer-
ring in two different sentences to near-equivalent
triples /music/artist/genre ??Heavy
metal" and /music/artist/genre
??Hard rock".
3.2 Annotation criteria
Annotators were asked to first read the text care-
fully, trying to identify propositional units (i.e.,
potential triples) and then to associate each iden-
tified propositional unit with zero, one or more
(coreferent) triples according to the following
rules:
Rule 1. One cannot annotate facts that are not
predicated and cannot be inferred from predicates
in the text. In other words, all facts must be
grounded in the text. For example, in the sentence
He starred in Annie Hall, the following is pred-
icated: W.H.has profession actor and
W.H. acted in film Annie Hall. The
former fact can be inferred from the latter. How-
ever, the following is not predicated: (1) Person
has name W.H., (2) W.H. is Male, and (3)
W.H. is Person.
Rule 2. In general, one can annotate more
generic facts if they can be inferred from more
specific propositions in the text, but one cannot
annotate specific facts just because a more gen-
eral proposition is found in the text. In the exam-
ple He was a navigator, we can mark the triples
Person has profession Sailor as well
as Person has profession Navigator
(we would also mark them as coreferent). How-
ever, given the sentence He was a sailor, we can-
not mark the triple Person has profession
Navigator, unless we can infer it from the text
or world knowledge.
Rule 3. One can annotate specific facts from a
text where the predicate is too vague or general if
the facts can be inferred from the textual context,
from the available data, or using world knowledge.
This rule subsumes four sub-cases:
Rule 3.1. The predicate in the proposition is too
vague or general and can be associated with mul-
tiple, more specific triples. In this case, do not
select any triple. In the example Film A was a
great commercial success, we have several triples
associating the celebrity with Film A, as direc-
tor, actor, writer, producer and composer and none
of them with a predicate resembling ?commercial
success?. In this case there are no triples that can
be associated with the text.
Rule 3.2. The predicate in the proposition is
too vague or general, but according to the data
there is just one specific triple it can be associated
with. In this case, select that triple. In the ex-
ample Paris released Confessions of an Heiress,
the term released could be associated with au-
thored, wrote or published. However, there is only
one triple associating that subject with that object,
which matches one of the interpretations (i.e., au-
thoring) of the predicate. Therefore that triple can
be selected.
Rule 3.3. The predicate in the proposition is
too vague or general, but one or more specific
triples can be inferred using world knowl-
edge. In this case, select all. The sentence
He is also a jazz clarinetist who performs
regularly at small venues in Manhattan, can
be associated with the available triples W.H.
profession Clarinetist and W.H.
music/group member/instruments played
Clarinet, even though for this latter triple
the person being in a group is not mentioned
explicitly. However, this can be inferred from
basic world knowledge.
Rule 3.4. The predicate in the proposition is
too vague or general, but one or more specific
triples can be inferred using the textual context.
In this case, select all. In the example By the
mid-1960s Allen was writing and directing films
. . . Allen often stars in his own films . . . Some of
the best-known of his over 40 films are Annie Hall
(1977) . . . , the relations of the person with the
film Annie Hall are that of writer, director and
actor, as supported by the previous text. There-
fore we would annotate facts stating that the per-
son wrote, directed and starred in Annie Hall.
However, we wouldn?t annotate composer or pro-
ducer triples if they existed.
Rule 4. A proposition can be associated
with multiple facts with identical or over-
lapping meanings. In the example, Woody
Allen is a musician, we have the triples
W.H occupation musician and W.H
profession musician, which have near
100
identical meanings. Therefore, we mark both
triples and indicate that they co-refer. The
sentence Woody Allen won prize as best director
for film Manhattan, on the other hand, can be
associated with non-coreferring triples W.H won
prize and W.H. directed Manhattan.
Rule 5. If the text makes reference to a set of
facts but it does not enumerate them explicitly, and
there is no reason to believe it makes reference to
any of them in particular, then do not annotate in-
dividual facts. Thus, sentence Clint Eastwood has
seven children does not warrant marking each of
the seven children triples as selected, given that
they are not enumerated explicitly.
Rule 6. If the text makes a clear and unam-
biguous reference to a fact, do not annotate any
other facts, even though they can be inferred from
it. In other words, as explained in Rule 1, all an-
notated triples must be grounded in the text. In
the sentence For his work in the films Unforgiven
(1992) and Million Dollar Baby (2004), Eastwood
won Academy Awards for Best Director and Pro-
ducer of the Best Picture, we can infer from world
knowledge that the celebrity was nominated prior
to winning the award in those categories. How-
ever, the text makes a clear reference only to the
fact that he won the award and there is no reason
to believe that it is also predicating the fact that the
celebrity was nominated.
4 Baseline evaluation
Briefly speaking, the UA system uses a general
heuristic based on the cognitive notion of com-
munal common ground regarding each celebrity,
which is approximated by scoring each lexicalized
triple (or property) associated with a celebrity ac-
cording to the number of hits of the Google search
API. Only the top-ranked triples are selected (Kut-
lak et al 2013). The UIC system uses a small
set of rules for the conditional inclusion of pred-
icates that was derived offline from the statistical
analysis of the co-occurrence between predicates
that are about the same topic or that share some
shared arguments; only the best performing rules
tested against a subset of the development set are
included (Venigalla and Di Eugenio, 2013).
For the baseline evaluation, we used the devel-
opment set obtained in the second round annota-
tion (see Section 3). However, we only consider
pairs obtained during the second round annotation
that 1) follow both restrictions presented in Sec-
Baseline UIC UA
Precision 49 64 47
Recall 67 50 39
F1 51 51 42
Table 1: Baseline evaluation results (%)
tion 2, and 2) have no coreferring triples. This
last restriction was added to minimize errors be-
cause we observed that annotators were not al-
ways consistent in their annotation of triple coref-
erence.6 We therefore considered 188 annotations
from the 344 annotations of the development set.
Of these, we used 40 randomly selected annota-
tions for evaluating the systems and 144 for es-
timating a baseline that only considers the top 5
predicates (i.e., the predicates most often selected)
and the type-predicate.7.
The evaluation results of the three systems
(baseline, UIC and UA) are presented in Table 1.
The figures in the table were obtained by compar-
ing the triples selected and rejected by each system
against the manual annotation. The performance
of the baseline is quite high. The UA system based
on a general heuristic scores lower than the base-
line, whilst the UIC system has a better precision
than the baseline, albeit a lower recall. This might
be due, as the UA authors observe in their sum-
mary (Venigalla and Di Eugenio, 2013), to ?the
large number of predicates that are present only
in a few files . . . [which] makes it harder to de-
cide whether we have to include these predicates
or not.?
5 Conclusions
We have given an overview of the first content se-
lection challenge from open semantic web data,
focusing on the rather extensive and challenging
technological and methodological work involved
in defining the task and preparing the data. Unfor-
tunately, despite agile participation in these early
6Type-predicate triples were filtered out of the annotated
files in the development set whilst they were included in the
large corpus made available to the candidates. Therefore,
we added type-predicate triples in the development set a
posteriori for this evaluation. These type-predicate triples
might be coreferring with other triples, say ns:m.08rd51
ns:type.object.type ns:film.actor and
ns:m.08rd5 people/person/profession
"Actor" /m/02hrh1q . Nonetheless, this was not
taken into account in the evaluation.
7The top 5 predicates were (in descending order of fre-
quency): music track, film actor, profession, date of birth and
nationality
101
preparatory stages, the number of submitted sys-
tems was limited. Both of the presented systems
were data-intensive in that they usedeither a pool
of textual knowledge or the corpus of triple data
provided by the challenge in order to select the
most relevant data.
Unlike several previous challenges that involve
more traditional NLG tasks (e.g., surface realiza-
tion, referring expression generation), content se-
lection from large input semantic data is a rela-
tively new research endeavour in the NLG com-
munity that coincides with the rising interest in
statistical approaches to NLG and dates back, to
the best of our knowledge, to (Duboue and McK-
eown, 2003). Furthermore, although we had ini-
tially planned to produce a training set for the
task, the cost of manual annotation turned out
to be prohibitive and the resulting corpus was
only fit for development and baseline evaluation.
Despite these setbacks, we believe that open se-
mantic web data is a promising test-bed and ap-
plication field for NLG-oriented content selec-
tion (Bouayad-Agha et al, 2013) and trust that
this first challenge has prepared the ground for
follow up challenges with a larger participation.
We would also like to encourage researchers from
NLG and Semantic Web research fields to exploit
the framework and materials developed during the
course of this challenge to advance research in
content selection.
References
Nadjet Bouayad-Agha, Gerard Casamayor, and Leo
Wanner. 2013. Natural Language Generation in the
Context of the Semantic Web. Submitted to the Se-
mantic Web Journal.
Nadjet Bouayad-Agha, Gerard Casamayor, Chris Mel-
lish, and Leo Wanner. 2012. Content Selection from
Semantic Web Data. INLG ?12 Proceedings of the
Seventh International Natural Language Generation
Conference. Pages 146-149.
Pablo A. Duboue and Kathleen R. McKeown. 2003.
Statistical Acquisition of Content Selection Rules
for Natural Language Generation Proceedings of
the Conference on Empirical Methods for Natural
Language Processing (EMNLP). Pages 121?128.
Randolph, Justus J. 2005. Free-marginal multirater
kappa (multirater Kfree): An alternative to fleiss
fixed-marginal multirater kappa. Presented as the
Joensuu University Learning and Instruction Sym-
posium.
Roman Kutlak, Chris Mellish and Kees van Deemter
2013. Content Selection Challenge University of
Aberdeen entry Proceedings of the 14th European
Natural Language Generation (ENLG) Workshop.
Hareen Venigalla and Barbara Di Eugenio. 2013. UIC-
CSC: The Content Selection Challenge Entry from
the University of Illinois at Chicago Proceedings
of the 14th European Natural Language Generation
(ENLG) Workshop.
102
Proceedings of the 8th International Natural Language Generation Conference, pages 108?112,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Classifiers for data-driven deep sentence generation
Miguel Ballesteros1, Simon Mille1 and Leo Wanner2,1
1NLP Group, Department of Information and Communication Technologies
Pompeu Fabra University, Barcelona
2Catalan Institute for Research and Advanced Studies (ICREA)
<fname>.<lname>@upf.edu
Abstract
State-of-the-art statistical sentence gener-
ators deal with isomorphic structures only.
Therefore, given that semantic and syntac-
tic structures tend to differ in their topol-
ogy and number of nodes, i.e., are not iso-
morphic, statistical generation saw so far
itself confined to shallow, syntactic gener-
ation. In this paper, we present a series
of fine-grained classifiers that are essen-
tial for data-driven deep sentence genera-
tion in that they handle the problem of the
projection of non-isomorphic structures.
1 Introduction
Deep data-driven (or stochastic) sentence gener-
ation needs to be able to map abstract seman-
tic structures onto syntactic structures. This has
been a problem so far since both types of struc-
tures differ in their topology and number of nodes
(i.e., are non-isomorphic). For instance, a truly
semantic structure will not contain any functional
nodes,1 while a surface-syntactic structure or a
chain of tokens in a linearized tree will con-
tain all of them. Some state-of-the-art propos-
als use a rule-based module to handle the projec-
tion between non-isomorphic semantic and syn-
tactic structures/chains of tokens, e.g., (Varges and
Mellish, 2001; Belz, 2008; Bohnet et al., 2011),
and some adapt the semantic structures to be iso-
morphic with syntactic structures (Bohnet et al.,
2010). In this paper, we present two alternative
stochastic approaches to the projection between
non-isomorphic structures, both based on a cas-
cade of Support Vector Machine (SVM) classi-
fiers.2 The first approach addresses the projection
as a generic non-isomorphic graph transduction
1See, for instance, (Bouayad-Agha et al., 2012).
2Obviously, other machine learning techniques could also
be used.
problem in terms of four classifiers for 1. identi-
fication of the (non-isomorphic) correspondences
between fragments of the source and target struc-
ture, 2. generation of the nodes of the target struc-
ture, 3. generation of the dependencies between
corresponding fragments of the source and target
structure, and 4. generation of the internal depen-
dencies in all fragments of the target structure.
The second approach takes advantage of the lin-
guistic knowledge about the projection of the in-
dividual linguistic token types. It replaces each
of the above four classifiers by a set of classifiers,
with each single classifier dealing with only one
individual linguistic token type (verb, noun, ad-
verb, etc.) or with a configuration thereof. As will
be seen, the linguistic knowledge pays off: the sec-
ond approach achieves considerably better results.
Since our goal is to address the challenge of the
projection of non-isomorphic structures, we focus,
in what follows, on this task. That is, we do not
build a complete generation pipeline until the sur-
face. This could be done, for instance, by feed-
ing the output obtained from the projection of a
semantic onto a syntactic structure to the surface
realizer described in (Bohnet et al., 2010).
2 The Task
The difference in the linguistic abstraction of se-
mantic and syntactic structures leads to diver-
gences that impede the isomorphy between the
two and make the mapping between them a chal-
lenge for statistical generation. Let us, before we
come to the implementation, give some theoretical
details on these structures as we picture them and
on the possible approaches to the projection of a
semantic structure to a syntactic one.
2.1 The Notion of semantic and syntactic
structures
As semantic structure, we assume a shallow se-
mantic representation that is very similar to the
108
PropBank (Babko-Malaya, 2005) and deep anno-
tations as used in the Surface Realisation Shared
Task (Belz et al., 2011): the deep-syntactic layer
of the AnCora-UPF corpus (Mille et al., 2013).
Deep-syntactic structures (DSyntSs) do not
contain any punctuation and functional nodes, i.e.,
governed prepositions and conjunctions, auxil-
iaries and determiners.3
As syntactic structure (in the terminology
of Ancora-UPF: surface-syntactic structures,
SSyntSs), we assume dependency trees in which
the nodes are labeled by open or closed class
lexemes and the edges by grammatical function
relations of the type subject, oblique object,
adverbial, modifier, etc.; cf.4 See Figure 1 for a
contrastive illustration of DSyntS and SSyntS.
rumor want new song be successful
I
II
ATTR I II
a rumor wants that the new song will be successful
det subj dobj
conj
detmodif subj analyt fut copul
Figure 1: DSyntS (above) and SSyntS (below) of
an English Sentence.
Note, however, that the proposal outlined be-
low for the projection of non-isomorphic struc-
tures is trainable on any multi-layered treebanks
where different layers are not isomorphic.
2.2 Projection of DSyntSs onto SSyntSs
In order to project a DSyntS onto its correspond-
ing SSyntS in the course of sentence generation,
the following types of actions need to be per-
formed:
1. Project each node in the DSyntS onto its SSynS-
correspondence. This correspondence can be a
single node, as, e.g., successful? successful, or a
subtree (hypernode, known as syntagm in linguis-
tics), as, e.g., song ? the song ?DT NN? (where
?DT? is a determiner and ?NN? a noun) or be
? that will be ?IN VAUX VB? (where ?IN? is a
preposition, ?VAUX? an auxiliary and ?VB? a full
verb). In formal terms, we assume any SSyntS-
correspondence to be a hypernode with a cardinal-
ity ? 1.
2. Generate the correct lemma for the nodes in
3For more details on the SSyntS, see (Mille et al., 2013).
4DSyntSs and their corresponding SSyntSs are stored in
the 14-column CoNLL?08 format.
SSyntS that do not have a 1:1 correspondence in
the SSyntS (as ?DT?, ?IN? and ?VAUX? above).
3. Establish the dependencies within the individ-
ual SSyntS-hypernodes.
4. Establish the dependencies between the
SSyntS-hypernodes (more precisely, between the
nodes of different SSyntS-hypernodes) to obtain a
connected SSyntS-tree.
3 Classifiers
As mentioned in the Introduction, the realization
of the actions 1.? 4. can be approached either in
terms of 4 generic classifiers (Section 3.1) or in
terms of 4 sets of fine-grained (micro) classifiers
(Section 3.2) that map one representation onto an-
other. As also mentioned above, we realize both
approaches as Support Vector Machines (SVMs).
3.1 Generic classifier approach
Each of the generic classifiers deals with one of
the following tasks.
a. Hypernode Identification: Given a deep
syntactic node nd from the DSyntS, the system
must find the shape of the surface hypernode (=
syntagm) that corresponds to nd in the SSyntS.
The hypernode identification SVM uses the fol-
lowing features:
POS of nd, POS of nd?s head, voice,
temp. constituency, finiteness, tense, lemma of
nd, and nd?s dependencies.
In order to simplify the task, we define the shape
of a surface hypernode as a list of surface PoS-
tags. This list contains the PoS of each of the lem-
mas within the hypernode and a tag that signals the
original deep node; for instance:
[ VB(deep), VAUX, IN]
b. Lemma Generation. Once the hypernodes
of the SSyntS under construction have been pro-
duced, the functional nodes that have been newly
introduced in the hypernodes must be assigned a
lemma. The lemma generation SVM uses the fol-
lowing features of the deep nodes nd in the hyper-
nodes:
? finiteness, ? definiteness, ? PoS of nd, ? lemma
of nd, ? PoS of the head of nd
to select the most likely lemma.
c. Intra-hypernode Dependency Generation.
Given a hypernode and its lemmas provided by
the two previous stages, the dependencies (i.e., the
dependency attachments and dependency labels)
between the elements of the hypernode must be
109
determined (and thus also the governor of the hy-
pernode). For this task, the intra-hypernode de-
pendency generation SVM uses the following fea-
tures:
? lemmas included in the hypernode, ? PoS-tags
of the lemmas in the hypernode, ? voice of the
head h of the hypernode, ? deep dependency re-
lation to h.
[ VB(deep), VAUX, IN]
analyt fut prepos
Figure 2: Internal dependency within a hypernode.
d. Inter-hypernode Dependency Generation.
Once the individual hypernodes have been con-
verted into connected dependency subtrees, the
hypernodes must be connected between each
other, such that we obtain a complete SSyntS. The
inter-hypernode dependency generation SVM uses
the following features of a hypernode ss:
? the internal dependencies of ss, ? the head of
ss, ? the lemmas of ss, ? the PoS of the depen-
dent of the head of ss in DSyntS
to determine for each hypernode its governor.
[ VB(deep), VAUX, IN] [ NN(deep), DT]
subj
Figure 3: Surface dependencies between two hy-
pernodes.
3.2 Implementation of sets of micro
classifiers
In this alternative approach, a single classifier is
foreseen for each kind of input. Thus, for the
hypernode identification module, for each deep
PoS tag (which can be one of the following four:
?N? (noun), ?V? (verb), ?Adv? (adverb), ?A? (ad-
jective)), a separate multi-class classifier is de-
fined. For instance, in the case of ?N?, the N-
classifier will use the above features to assign
to the a DSynt-node with PoS ?N? the most ap-
propriate (most likely) hypernode?in this case,
[NN(deep), DT]. In a similar way, in the case of
the lemma generation module, for each surface
PoS tag, a separate classifier is defined. Thus,
the DT-classifier would pick for the hypernode
[NN(deep), DT] the most likely lemma for the DT-
node (optimally, a determiner).
For the intra-hypernode attachments module,
for each kind of hypernode, dynamically a sepa-
rate classifier is generated.5 In the case of the hy-
5This implies that the number of classifiers varies depend-
ing on the training set, in the intra-hypernode dependency
generation there are 108 SVMs.
pernode [ VB(deep), VAUX, IN], the correspond-
ing classifier will create a link between the prepo-
sition and the auxiliary, and between the auxiliary
and the verb, with respectively the preposition and
the auxiliary as heads because it is the best link
that it can find; cf. Figure 2 for illustration.
Finally, for the inter-hypernode attachments
module, for each hypernode with a distinct in-
ternal dependency pattern, a separate classifier is
dynamically derived (for our treebank, we ob-
tained 114 different SVM classifiers because it
also takes into account hypernodes with just one
token). For instance, the classifier for the hypern-
ode [ NN(deep), DT] is most likely to identify as
its governor VAUX in the hypernode [ VB(deep),
VAUX, IN]; cf. Figure 3.
4 Experiments and Results
In this section, we present the performance of the
two approaches to DSyntS?SSyntS projection on
the DSyntS- and SSynt-layers of the AnCora-UPF
treebank (Mille et al., 2013).6 Table 1 displays
the results for the generic classifier for all tasks
on the development and the test set, while Table
2 displays the results obtained through the sets of
micro classifiers.
Dev.set # %
Hypernode identification 3131/3441 90.99
Lemma generation 818/936 87.39
Intra-hypernode dep. generation 545/798 68.30
Inter-hypernode dep. generation 2588/3055 84.71
Test set # %
Hypernode identification 5166/5887 87.75
Lemma generation 1822/2084 87.43
Intra-hypernode dep. generation 1093/1699 64.33
Inter-hypernode dep. generation 4679/5385 86.89
Table 1: Results of the evaluation of the generic
classifiers for the non-isomorphic transduction.
The results show that for hypernode identifica-
tion and inter-hypernode dependency generation,
the results of both types of classifiers are compara-
ble, be it on the development set or on the test set.
However, thanks to the micro classifiers, with the
same features, the lemma generation model based
on micro classifiers improves by 4 points and the
intra-hypernode dependency generation by nearly
6Following a classical machine learning set-up, we di-
vided the treebank into: (i) a development set (219 sen-
tences, 3271 tokens in the DSyntS treebank and 4953 tokens
in the SSyntS treebank); (ii) a training set (3036 sentences,
57665 tokens in the DSyntS treebank and 86984 tokens in
the SSyntS treebank); and a (iii) a held-out test for evalua-
tion (258 sentences, 5641 tokens in the DSyntS treebank and
8955 tokens in the SSyntS treebank).
110
Dev.set # %
Hypernode identification 3133/3441 91.05
Lemma generation 851/936 90.92
Intra-hypernode dep. generation 767/798 96.12
Inter-hypernode dep. generation 2574/3055 84.26
Test set # %
Hypernode identification 5169/5886 87.82
Lemma generation 1913/2084 91.79
Intra-hypernode dep. generation 1630/1699 95.94
Inter-hypernode dep. generation 4648/5385 86.31
Table 2: Results of the evaluation of the micro
classifiers for the non-isomorphic transduction.
30 points. This means that the intra-hypernode de-
pendency generation task is too sparse to be real-
ized as a single classifier. The micro classifiers
are in this case binary, i.e., 2:1, or unary, i.e., 1:1
classifiers, which implies a tremendous reduction
of the search space (and thus higher accuracy). In
contrast, the single classifier is a multi-class clas-
sifier that must decide among more than 60 pos-
sible classes. Although most of these 60 classes
are diferentiated by features, the differentiation
is not perfect. In the case of lemma generation,
we observe a similar phenomenon. In this case,
the micro-classifiers are multi-class classifiers that
normally have to cope with 5 different classes
(lemmas in this case), while the unique classi-
fier has to cope with around 60 different classes
(or lemmas). Hypernode identification and inter-
hypernode dependency generation are completely
guided by the input; thus, it seems that they do not
err in the same way.
Although the micro classifier approach leads
to significantly better results, we believe that it
can still be improved. First, the introduction of
prepositions causes most errors in hypernode de-
tection and lemma generation: when a preposition
should be introduced or not and which preposi-
tion should be introduced depends exclusively on
the sub-categorization frame of the governor of
the deep node. A treebank of a limited size as
used in our experiments simply does not contain
subcategorization patterns of all predicative lexi-
cal items (especially of nouns)?which would be
crucial. Thus, in the test set evaluation, out of the
171 lemma errors 147 are prepositions and out of
the 717 errors on hypernode identification, more
than 500 are due to nouns and preposition. The in-
crease of the size of the treebank would therefore
be an advantage.
Second, in the case of inter-hypernode depen-
dency, errors are due to the labels of the dependen-
cies more than to the attachements, and are quite
distributed over the different types of configura-
tions. The generation of these dependencies suf-
fers from the fact that the SSyntS tag-set is very
fine-grained. For instance, there are 9 different
types of verbal objects in SSyntS,7 which capture
very specific syntactic properties of Spanish, such
as ?can the dependent can be replaced by a clitic
pronoun? Can the dependent be moved away from
its governor? Etc. This kind of information is not
of a high relevance for generation of well-formed
text. Using a more reduced (more coarse-grained)
SSyntS tag set would definitely improve the qual-
ity of the projection.
5 Related work
There is an increasing amount of work on sta-
tistical sentence generation; see, e.g., (Bangalore
and Rambow, 2000; Langkilde-Geary, 2002; Fil-
ippova and Strube, 2008). However, hardly any
addresses the problem of the projection between
non-isomorphic semantic and syntactic structures.
In general, structure prediction approaches use
a single classifier model (Smith, 2011). But
see, e.g., (Carreras et al., 2008), who use dif-
ferent models to predict each part of the triplet
for spinal model pruning, and (Bjo?rkelund et al.,
2010; Johansson and Nugues, 2008), who use
a set of classifiers for predicate identification in
the context of semantic role labelling. Amalgam
(Corston-Oliver et al., 2002), which maps a logi-
cal input onto sentences with intermediate syntac-
tic (phrase-based) representation, uses language-
specific decision trees in order to predict when to
introduce auxiliaries, determiners, cases, etc.
6 Conclusions
We presented two alternative classifier approaches
to deep generation that cope with the projection
of non-isomorphic semantic and syntactic struc-
tures and argued that the micro classifier approach
is more adequate. In spite of possible improve-
ments presented in Section 4, each set of micro
classifiers achieves results above 86% on the test
set. For intra-hypernode dependency generation,
it even reaches 95.94% .
Acknowledgments
This work has been partially funded by the Euro-
pean Commission under the contract number FP7-
ICT-610411.
7There are 47 SSynt dependencies in total, to compare to
the 7 dependencies in the DSyntS.
111
References
Olga Babko-Malaya, 2005. Propbank Annotation
Guidelines.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gener-
ation. In Proceedings of the 18th International Con-
ference on Computational Linguistics (COLING),
pages 42?48, Saarbru?cken, Germany.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first Surface Realisation Shared Task: Overview and
evaluation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation (ENLG), pages
217?226, Nancy, France.
Anja Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Journal of Natural Lan-
guage Engineering, 14(4):431?455.
A. Bjo?rkelund, B. Bohnet, L. Hafdell, and P. Nugues.
2010. A high-performance syntactic and semantic
dependency parser. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics : Demonstration Volume (COLING), pages 33?
36, Beijing, China.
Bernd Bohnet, Leo Wanner, Simon Mille, and Ali-
cia Burga. 2010. Broad coverage multilingual
deep sentence generation with a stochastic multi-
level realizer. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING), pages 98?106, Beijing, China.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. StuMaBa: From deep representation
to surface. In Proceedings of the Generation Chal-
lenges Session at the 13th European Workshop on
Natural Language Generation (ENLG), pages 232?
235, Nancy, France.
Nadjet Bouayad-Agha, Gerard Casamayor, Simon
Mille, and Leo Wanner. 2012. Perspective-oriented
generation of football match summaries: Old tasks,
new challenges. ACM Transactions on Speech and
Language Processing, 9(2):3:1?3:31.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the per-
ceptron for efficient, feature-rich parsing. In Pro-
ceedings of the 12th Conference on Computational
Natural Language Learning (CoNLL), pages 9?16,
Manchester, UK.
Simon Corston-Oliver, Michael Gamon, Eric Ringger,
and Robert Moore. 2002. An overview of Amal-
gam: A machine-learned generation module. In
Proceedings of the 2nd International Natural Lan-
guage Generation Conference (INLG), pages 33?40,
New-York, NY, USA.
Katja Filippova and Michael Strube. 2008. Sen-
tence fusion via dependency graph compression.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 177?185, Honolulu, Hawaii.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based Semantic Role Labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 69?78, Honolulu, Hawaii.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of
the 2nd International Natural Language Generation
Conference (INLG), pages 17?24, New-York, NY,
USA. Citeseer.
Simon Mille, Alicia Burga, and Leo Wanner. 2013.
AnCora-UPF: A multi-level annotation of Spanish.
In Proceedings of the 2nd International Conference
on Dependency Linguistics (DepLing), pages 217?
226, Prague, Czech Republic.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Synthesis Lectures on Human Language Technolo-
gies. Morgan and Claypool.
Sebastian Varges and Chris Mellish. 2001. Instance-
based Natural Language Generation. In Proceed-
ings of the 2nd Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL), pages 1?8, Pittsburgh, PA, USA.
112
