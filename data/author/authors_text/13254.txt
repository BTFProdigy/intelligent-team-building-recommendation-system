Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 627?635,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Extracting Glosses to Disambiguate Word Senses
Weisi Duan
Carnegie Mellon University
Language Technologies Institute
5000 Forbes Ave.
Gates Hillman Complex 5407
Pittsburgh, PA 15213
wduan@cs.cmu.edu
Alexander Yates
Temple University
Computer and Information Sciences
1805 N. Broad St.
Wachman Hall 303A
Philadelphia, PA 19122
yates@temple.edu
Abstract
Like most natural language disambiguation
tasks, word sense disambiguation (WSD) re-
quires world knowledge for accurate predic-
tions. Several proxies for this knowledge
have been investigated, including labeled cor-
pora, user-contributed knowledge, and ma-
chine readable dictionaries, but each of these
proxies requires significant manual effort to
create, and they do not cover all of the ambigu-
ous terms in a language. We investigate the
task of automatically extracting world knowl-
edge, in the form of glosses, from an unlabeled
corpus. We demonstrate how to use these
glosses to automatically label a training cor-
pus to build a statistical WSD system that uses
no manually-labeled data, with experimental
results approaching that of a supervised SVM-
based classifier.
1 Introduction
For many semantic natural language processing
tasks, systems require world knowledge to disam-
biguate language utterances. Word sense disam-
biguation (WSD) is no exception ? systems for
WSD require world knowledge to figure out which
aspects of a word?s context indicate one sense over
another. A fundamental problem for WSD is that the
required knowledge is open-ended. That is, for ev-
ery ambiguous term, new kinds of information about
the world become important, and the knowledge that
a system may have acquired for previously-studied
ambiguous terms may have little or no impact on the
next ambiguous term. Thus open-ended knowledge
acquisition is a fundamental obstacle to strong per-
formance for this disambiguation task.
Researchers have investigated a variety of tech-
niques that address this knowledge acquisition bot-
tleneck in different ways. Supervised WSD tech-
niques, for instance, can learn to associate features
in the context of a word with a particular sense of
that word. Knowledge-based techniques rely on
machine-readable dictionaries or lexical resources
like WordNet (Fellbaum, 1998) to provide the nec-
essary knowledge. And most recently, systems
have used resources like Wikipedia, which contain
user-contributed knowledge in the form of sense-
disambiguated links, to acquire world knowledge for
WSD. Yet each of these approaches is limited by the
amount of manual effort that is needed to build the
necessary resources, and as a result the techniques
are limited to a subset of English words for which
the manually-constructed resources are available.
In this work we investigate an alternative ap-
proach that attacks the problem of knowledge acqui-
sition head-on. We use information extraction (IE)
techniques to extract glosses, or short textual char-
acterizations of the meaning of one sense of a word.
In the ideal case, we would extract full logical forms
to define word senses, but here we instead focus on
a more feasible, but still very useful, sub-task: for a
given word sense, extract a collection of terms that
are highly correlated with that sense and no other
sense of the ambiguous word. Our system requires
as input only an unlabeled corpus of documents that
each contain the ambiguous term of interest.
In experiments, we demonstrate that our gloss
extraction system can often determine key aspects
of a word?s senses. In one experiment our sys-
tem was able to extract glosses with 60% precision
for 20 ambiguous biomedical terms, while discov-
ering 7 senses of those terms that never appeared
in a widely-used dictionary of biomedical terminol-
ogy. In addition, we demonstrate that our extracted
glosses are useful for real WSD problems: our sys-
627
tem outperforms a state-of-the-art unsupervised sys-
tem, and it comes close to the performance of a su-
pervised WSD system on a challenging dataset.
In the next section, we describe previous work. In
Section 3, we formally define the gloss extraction
task and refine it into a sub-task that is feasible for
an IE approach, and Section 5 presents our technique
for using extracted glosses in a WSD task. Section
6 discusses our experiments and results.
2 Previous Work
Many previous systems (Cui et al, 2007; Androut-
sopoulos and Galanis, 2005) have studied the re-
lated task of answering definitional questions on the
Web, such as ?What does cold mean??. Such sys-
tems are focused on information retrieval for human
consumption, and especially on recall of definitional
information (Velardi et al, 2008). They generally
do not consider the problem of how to merge the
large number of similar extracted definitions into a
single item (Fujii and Ishikawa, 2000), so that the
overall result contains one definition per sense of
the word. A separate approach (Pasca, 2005) relies
on the WordNet lexical database to supply the set of
senses, and extracts alternate glosses for the senses
that have already been defined. When glosses are to
be used by computational methods, as in a WSD sys-
tem in our case, it becomes critical that the system
extract one coherent gloss per sense. As far as we
are aware, no previous system has extracted glosses
for word sense disambiguation.
Gloss extraction is related to the task of ontol-
ogy extraction, in which systems extract hierarchies
of word classes (Snow et al, 2006; Popescu et al,
2004). Gloss extraction differs from ontology ex-
traction in that it extracts definitional information
characterizing senses of a single word, rather than
trying to place a word in a hierarchy of other words.
Most WSD systems have relied on hand-labeled
training examples (Leroy and Rindflesch, 2004;
Joshi et al, 2005; Mohammad and Pedersen, 2004)
or on dictionary glosses (Lesk, 1986; Stevenson
and Wilks, 2001) or the WordNet hierarchy (Boyd-
Graber et al, 2007) to help make disambiguation
choices. In recent coarse-grained evaluations, such
systems have achieved accuracies of close to 90%
(Pradhan et al, 2007; Agirre and Soroa, 2007; Schi-
jvenaars et al, 2005). However, by some estimates,
English contains over a million word types, and new
words and new senses are added to the language ev-
ery day. It is unreasonable to expect that any system
will have access to hand-labeled training examples
or useful dictionary glosses for each of them.
More recent techniques based on user-contributed
knowledge (Mihalcea, 2007; Chklovski and Mihal-
cea, 2002; Milne and Witten, 2008), such as that
found in Wikipedia, suffer from similar problems ?
Wikipedia contains many articles on well known en-
tities, categories, and events, but very few articles
that disambiguate verbs, adjectives, adverbs, and
certain kinds of nouns which are poorly represented
in an encyclopedia.
On the other hand, word usages in large corpora
like the Web reflect nearly all of the word senses
in use in English today, albeit without manually-
supplied labels. Unsupervised approaches to WSD
use clustering techniques to group instances of
words into clusters that correspond to different
senses (Pantel and Lin, 2002). While such systems
are more general than supervised and dictionary-
based approaches in that they can handle any word
type and word sense, they have lagged behind other
approaches in terms of accuracy thus far ? for ex-
ample, the best system in the recent word sense in-
duction task of Semeval 2007 (Agirre and Soroa,
2007) achieved an F1 score of 78.7, slightly below
the baseline (78.9) in which all instances of a word
are part of a single cluster. Part of the problem
is that the clustering techniques operate in a bag-
of-words-like representation. This is an extremely
high-dimensional space, and it is difficult in such
a space to determine which dimensions are noise
and which ones correlate with different senses. Our
gloss extraction technique helps to address this curse
of dimensionality by reducing the large vocabulary
of a corpus to a much smaller set of terms that are
highly relevant for WSD. Others (Kulkarni and Ped-
ersen, 2005) have used feature selection techniques
like mutual information to reduce dimensionality,
but so far these techniques have only been able to
find features that correlate with an ambiguous term.
With gloss extraction, we are able to find features
that correlate with individual senses of a term.
3 Overview: The Gloss Extraction Task
Given an input corpus C of documents where each
document contains at least one instance of a keyword
k, a Gloss Extraction system should produce a set of
glosses G = {gi}, where each gi is a logical expres-
sion defining the meaning of a particular sense si of
628
Glosses:
1. cold(a) ? isA(a, b) ? disease(b) ? symptom(a, c) ? possibly-includes(c, d) ? fever(d)
2. cold(a) ? isA(a, b) ? physical-entity(b) ? temperature(a, c) ? less-than(c, 25C)
Sense Indicators:
1. common cold, virus, symptom, fever
2. hot, ice cold, lukewarm, cold room, room temperature
Figure 1: Example glosses and sense indicators for two senses of the word cold.
k, to the exclusion of all other senses of k. Note that
the system must discover the appropriate number of
senses in addition to the gloss for each sense.
While extraction technology has made impressive
advancements, it is not yet at a stage where it can
produce full logical forms for sense glosses. As a
first step towards this goal, we introduce the task of
Sense Indicator Extraction, in which each gloss gi
consists of a set of features that, when present in the
context of an instance of k, strongly indicate that the
instance has sense si, and no other sense. Exam-
ples of both tasks are given in Figure 1. The Sense
Indicator Extraction task represents a nontrivial ex-
traction challenge, but it is much more feasible than
full Gloss Extraction. And the task preserves key
properties of Gloss Extraction: the results are quite
useful for word sense disambiguation. The results
are also readily interpreted upon inspection, making
it easy to monitor a system?s accuracy.
4 Extracting Word Sense Glosses
We present the GLOSSY system, an unsupervised in-
formation extraction system for Sense Indicator Ex-
traction. GLOSSY proceeds in two phases: a col-
location detection phase, in which the system de-
tects components of the glosses, and an arrangement
phase, in which the system decides how many dis-
tinct senses there are, and puts together the compo-
nents of the glosses.
4.1 Collocation Detection
The first major challenge to a Gloss Extraction sys-
tem is that the space of possible features is enor-
mous, and almost all of them are irrelevant to the
task at hand. Supervised techniques can use la-
beled examples to provide clues, but in an unsu-
pervised setting the curse of dimensionality can be
overwhelming. Indeed, unsupervised WSD tech-
niques suffer from exactly this problem.
GLOSSY?s answer to this problem is based on the
following observation: pairs of potential features
which rarely or never co-occur in the same docu-
ment in a large corpus are likely to represent fea-
tures for two distinct senses. The well-known obser-
vation that words rarely exhibit more than one sense
per discourse (Yarowsky, 1995) implies that features
closely associated with a particular sense have a low
probability of appearing in the same document as
features associated with another sense. Features that
are independent of any particular sense of the key-
word, on the other hand, have no such restriction,
and are just as likely to appear in the context of one
sense as any other. As a consequence, a low count
for the co-occurrence of two potential features over
a large corpus of documents for keyword k is a re-
liable indicator that the two features are part of the
glosses of two distinct senses of k.
GLOSSY?s collocation detector begins by index-
ing the corpus and counting the frequency of each
vocabulary word. Using the index, the collocation
detector determines all pairs of potential features
such that each feature appears at least T times, and
the pair of features never co-occurs in the same doc-
ument. We call the pairs that this step finds the ?non-
overlapping? features. Finally, we rank the feature
pairs according to the total number of documents
they appear in, and choose the most frequent N
pairs. This excludes non-overlapping pairs that have
not been seen often enough to provide reliable evi-
dence that they are features of different senses, and
it cuts down on processing time for the next phase of
the algorithm. The collocation detector outputs the
set of features F = {f |?f ?(f, f ?) or (f ?, f) is one
of the top N non-overlapping pairs}. The GLOSSY
system uses stems, words, and bigrams as potential
features. We use N = 100 and T = 50 in our ex-
periments. Figure 2 shows an example corpus and
the set of features that the collocation detector would
output.
629
Corpus of documents for term cold:
DOCUMENT 1: ?Symptoms of the common cold may include fever, headache, sore throat, and coughing.?
DOCUMENT 2: ?Hibernation is a common response to the cold winter weather of temperate climates.?
Non-overlapping feature pairs:
(symptoms,temperate) (headache, climate) (cold winter, common cold) (response, headache)
Detected collocations:
symptoms, temperate, headache, climate, cold winter, common cold, response
Arranged glosses:
cold 1: symptoms, common cold, headache
cold 2: temperate, climate, cold winter
Figure 2: Example operation of the GLOSSY extraction system. The collocation detector finds potential features
using its non-overlapping pair heuristic. The arranger selects a subset of the potential features (in this example, it
drops the feature response) and clusters them to produces glosses containing sense indicators.
4.2 Arranging Glosses
Given the corpus C for keyword k and the features F
that GLOSSY?s collocation detector has discovered,
the arrangement phase groups these features into co-
herent sense glosses. Figure 2 shows an example of
how the features found during collocation detection
may be arranged to form coherent glosses for two
senses of the word ?cold.?
GLOSSY?s Arranger component uses a combina-
tion of a small set of statistics to determine whether
a particular arrangement of the features into glosses
is warranted, based on the given corpus. Let A ? 2F
be an arrangement of the features into clusters rep-
resenting glosses. We require that clusters in A be
disjoint, but we do not require every feature in F to
be included in a cluster in A ? in other words, A
is a partition of a subset of F . We define a scoring
function S that is a linear interpolation of several
statistics of the arrangement A and the corpus C:
S(A|C,w) =
?
i
wifi(A, C) (1)
After experimenting with a number of options, we
settled on the following for our statistics fi:
NUMCLUSTERS: the number of clusters in A. We
use a negative weight for this statistic to favor fewer
senses and encourage clustering.
DOCSCOVERED: the total number of documents in
C in which at least one feature from A appears. We
use this statistic to encourage the Arranger to find an
arrangement that explains the sense of as many ex-
amples of the keyword as possible.
BADOVERLAPS: the number of pairs of features
that co-occur in at least one document in C, and that
belong to different clusters of A. A negative weight
for this statistic encourages overlapping feature pairs
to be placed in the same cluster.
BADNONOVERLAPS: the number of pairs of fea-
tures that never co-occur in C, and that belong to the
same cluster in A. A negative weight for this statis-
tic encourages non-overlapping feature pairs to be
placed in different clusters.
Given such an optimization function, the Ar-
ranger attempts to maximize its value by search-
ing for an optimal A. Note that this is a struc-
tured prediction task in which the choice for some
sub-component of A can greatly affect the choice of
other clusters and features. GLOSSY addresses this
optimization problem with a greedy hill-climbing
search with random restarts. Each round of hill-
climbing is initialized with a randomly chosen sub-
set of features, which are then all assigned to a sin-
gle cluster. Using a randomly chosen search opera-
tor from a pre-defined set, the search procedure at-
tempts to move to a new arrangement A?. It accepts
the move to A? if the optimization function gives a
higher value than at the previous state; otherwise, it
continues from the previous state. Our set of search
operators include a move that splits a cluster; a move
that joins two clusters; a move that swaps a feature
from one cluster to another; a move that removes a
feature from the arrangement altogether; and a move
630
that adds a feature from the pool of unused features.
We used 100 rounds of hill-climbing, and found that
each round converged in fewer than 1000 moves.
To estimate the weights wi for each of the four
features of the Arranger, we use a development cor-
pus consisting of 185 documents each containing the
same ambiguous term, and each labeled with sense
information. Because of the small number of pa-
rameters, we performed a grid search on the devel-
opment data for the optimal values of the weights.
5 A Bootstrapping WSD System
Yarowsky (1995) first recognized that it is possi-
ble to use a small number of features for different
senses to bootstrap an unsupervised word sense dis-
ambiguation system. In Yarowsky?s work, his sys-
tem requires an initial, manually-supplied colloca-
tion as a feature for each sense of a keyword. In con-
trast, we can use GLOSSY?s extracted glosses to sup-
ply starter features fully automatically, using only an
unlabeled corpus. Thus GLOSSY complements the
efforts of Yarowsky and other bootstrapping tech-
niques for WSD (Diab, 2004; Mihalcea, 2002).
Building on their efforts, we design a boot-
strapping WSD system using GLOSSY?s extracted
glosses as follows. Let A be the arranged features
representing glosses for a keyword. We first retrieve
all the documents from our unlabeled corpus which
contain features in A. We then label appearances
of the target word according to the cluster of the
features that appear in that document. If features
for more than one cluster appear in the same docu-
ment, we discard it. The result is an automatically
labeled corpus containing examples of all the ex-
tracted senses.
We use this automatically labeled ?bootstrap cor-
pus? to perform supervised WSD. This allows our
system a great deal of flexibility once the bootstrap
corpus is created: we can use any features of the
corpus, plus the labels, in our classifier. Importantly,
this means we do not need to rely on just the features
in the extracted glosses. We use a multi-class SVM
classifier with a linear kernel and default parameter
settings. We use LibSVM (Chang and Lin, 2001) for
all of our experiments. We use standard features for
supervised WSD (Liu et al, 2004): all stems, words,
bigrams, and trigrams within a context window of 20
words surrounding the ambiguous term.
6 Experiments
We ran two types of experiments, one to measure
the accuracy of our sense gloss extractor, and one to
measure the usefulness of the extracted knowledge
for word sense disambiguation.
6.1 Data
We use a dataset of biomedical literature abstracts
from Duan et al(2009). The data contains a set of
documents for 21 ambiguous terms. We reserved
one of these terms (?MCP?) for setting parameters,
and ran our algorithms on the remaining keywords.
The ambiguous terms vary from acronyms (7 terms),
which are common and important in biomedical lit-
erature, to ambiguous biomedical terminology (3
terms), to terms like ?culture? and ?mole? that have
some biomedical senses and some senses that are
part of the general lexicon (11 terms). There were on
average 271 labeled documents per term; the small-
est number of documents for a term is 125, and
the largest is 503. For every ambiguous term, we
added on average 9625 (minimum of 1544, maxi-
mum of 15711) unlabeled documents to our collec-
tion by searching for the term on PubMed Central
and downloading additional PubMed abstracts.
6.2 Extracting Glosses
We measured the performance of GLOSSY?s gloss
extraction by comparing the extracted glosses with
definitions contained in the Unified Medical Lan-
guage System (UMLS) Metathesaurus. First, for
each ambiguous term, we looked up the set of ex-
act matches for that term in the Metathesaurus, and
downloaded definitions for all of the different senses
listed under that term. Wherever possible, we used
the MeSH definition of a sense; when that was un-
available, we used the definition from the NCI The-
saurus; and when both were unavailable, we used the
definition from the resource listed first. 34 senses
(40%) had no available definitions at all, but in all
cases, the Metathesaurus lists a short (usually 1-3
word) gloss of the sense, which we used instead.
We manually aligned extracted glosses with
UMLS senses in a way that maximizes the number
of matched senses for every ambiguous term. We
consider an extracted gloss to match a UMLS sense
when the extracted gloss unambiguously refers to
a single sense of the ambiguous term, and that
sense matches the definition in UMLS. Typically,
this means that the extracted features in the gloss
631
overlap content words in the UMLS definition (e.g.,
the extracted feature ?symptoms? for the ?common
cold? sense of the term ?cold?). In some cases, how-
ever, there was no strict overlap in content words
between the extracted gloss and the UMLS defini-
tion, but the sense of the extracted gloss still unam-
biguously matched a unique UMLS sense: e.g., for
the term ?transport,? the extracted gloss ?intracel-
lular transport? was matched with the UMLS sense
of ?Molecular Transport,? which the NCI Thesaurus
defines as, ?Any subcellular or molecular process in-
volved in translocation of a biological entity, such
as a macromolecule, from one site or compartment
to another.? In the end, such matchings were deter-
mined by hand. Table 1 shows extracted glosses and
UMLS definitions for the term ?mole.?
For each ambiguous term, we measure the num-
ber of extracted glosses, the number of UMLS
senses, and the number of matches between the
two. We report on the precision (number of matches
/ number of extracted glosses), recall (number of
matches / number of UMLS senses), and F1 score
(harmonic mean of precision and recall). Table 2
shows the average of the precision and recall num-
bers over all terms. Since these terms have different
numbers of senses, we can compute this average in
two different ways: a Macro average, in which each
term has equal weight in the average; and a Micro
average, in which each term?s weight in the average
is proportional to the number of senses (extracted
senses for the precision, and UMLS senses for the
recall). We report on both.
A strict matching between GLOSSY?s glosses and
UMLS senses is potentially unfair to GLOSSY in
several ways: GLOSSY may discover valid senses
that happen not to appear in UMLS; UMLS senses
may overlap one another, and so multiple UMLS
senses may match a single GLOSSY gloss; and the
two sets of senses may differ in granularity. For the
sake of repeatable experiments, in this evaluation we
make no attempt to change existing UMLS senses.
However, to highlight one particular strength of
the Gloss Extraction paradigm, we do consider a
separate evaluation that allows for new senses that
GLOSSY discovers, but do not appear in UMLS.
For instance, ?biliopancreatic diversion? and ?bipo-
lar disorder? are both valid senses for the acronym
?BPD.? GLOSSY discovers both, but UMLS does
not contain entries for either, so in our original eval-
uation both senses would count against GLOSSY?s
precision. To correct for this, our second evalua-
tion adds senses to the list of UMLS senses when-
ever GLOSSY discovers valid entries missing from
the Metathesaurus. The last five columns of Table 2
show our results under these conditions.
Despite the difficulty of the task, GLOSSY is able
to find glosses with 53% precision and 47% re-
call (Macro average, no discovered senses) using
only unlabeled corpora as input, and it is extract-
ing roughly the right number of senses for each am-
biguous term. In addition, GLOSSY is able to iden-
tify 7 valid senses missing from UMLS for the 20
terms in our evaluation. Including these senses in
the evaluation increases GLOSSY?s F1 by 6.2 points
Micro (4.7 Macro). We are quite encouraged by
the results, especially because they hold promise for
WSD. Note that in order to improve upon a WSD
baseline which tags all instances of a word as the
same sense, GLOSSY only needs to be able to sep-
arate one sense from the rest. GLOSSY is finding
between 1.85 and 2.2 correct glosses per term, more
than enough to help with WSD.
6.3 WSD with Extracted Glosses
While extracting glosses is an important application
in its own right, we also aim to show that this ex-
tracted knowledge is useful for an established ap-
plication: namely, word sense disambiguation. Our
next experiment compares the performance of our
WSD system with an established unsupervised al-
gorithm, and with a supervised technique ? support
vector machines (SVMs).
Using the same dataset as above, we trained
GLOSSY on the ambiguous term ?MCP?, and tested
it on the remaining ones. For comparison, we also
report the state-of-the-art results of Duan et al?s
(2009) SENSATIONAL system, and the results of a
BASELINE system that lumps all documents into
a single cluster. SENSATIONAL is a fast cluster-
ing system based on minimum spanning trees and
a pruning mechanism that eliminates noisy points
from consideration during clustering. Since SEN-
SATIONAL uses both ?MCP? and ?white? to train a
small set of parameters, we leave ?white? out of our
comparison as well. We measure accuracy by align-
ing each system?s clusters with the gold standard
clusters in such a way as to maximize the number
of elements that belong to aligned clusters. We use
an implementation of the MaxFlow algorithm to de-
termine this alignment. We then compute accuracy
632
GLOSSY UMLS
1. choriocarcinoma,
invasive, complete,
hydatidiform mole,
hydatidiform
1. Hydatidiform Mole ? Trophoblastic hyperplasia associated with normal gestation,
or molar pregnancy. . . . Hydatidiform moles or molar pregnancy may be catego-
rized as complete or partial based on their gross morphology, histopathology, and
karyotype.
2. grams per mole 2. Mole, unit of measurement ? A unit of amount of substance, one of the seven
base units of the International System of Units. It is the amount of substance that
contains as many elementary units as there are atoms in 0.012 kg of carbon-12.
3. mole fractions -
- 3. Nevus ? A circumscribed stable malformation of the skin . . . .
- 4. Talpidae ? Any of numerous burrowing mammals found in temperate regions . . .
Table 1: GLOSSY?s extracted glosses and UMLS dictionary entries for the example term ?mole?.
Without Discovered Senses With Discovered Senses
GLOSSY UMLS UMLS
Senses Senses Matches P R F1 Senses Matches P R F1
Macro Avg 4.35 4.25 1.85 53.1 47.1 49.9 4.6 2.2 60.6 49.7 54.6
Micro Avg N/A N/A N/A 42.5 43.5 43.0 N/A N/A 50.6 47.8 49.2
Table 2: GLOSSY can automatically discover glosses that match definitions in an online dictionary. ?Without
Discovered Senses? counts only the senses that are listed in the UMLS Metathesaurus; ?With Discovered Senses?
enhances the Metathesaurus with 7 new senses that GLOSSY has automatically discovered.
as the percentage of elements that belong to aligned
clusters. This metric is very similar to the so-called
?supervised? evaluation of Agirre et al (2006).
The first four columns of Table 3 show our results.
Clearly, both SENSATIONAL and GLOSSY outper-
form the BASELINE significantly, and traditionally
this is a difficult baseline for unsupervised WSD sys-
tems to beat. SENSATIONAL outperforms GLOSSY
by approximately 6%. There appear to be two rea-
sons for this. In other experiments, SENSATIONAL
has been shown to be competitive with supervised
systems, but only when the corpus consists mostly
of two, fairly well-balanced senses, as is true for
this particular dataset, where the two most common
senses always covered at least 70% of the examples
for every ambiguous term.
A more serious problem for GLOSSY is that the
unlabeled corpus that it extracts glosses from may
not match well with the labeled test data. If the rela-
tive frequency of senses in the unlabeled documents
does not match the relative frequency of senses in
the labeled test set, GLOSSY may not extract the
right set of glosses. Manual inspection of the ex-
tracted glosses shows that this is indeed a problem:
for example, the labeled data contains two senses of
the word ?mole?: a discolored area of skin (78%),
and a burrowing mammal (22%); our unlabeled data
contains both of these senses, but the additional
sense of ?mole? as a unit of measurement is by far
predominant. GLOSSY manages to extract glosses
for ?skin? and ?unit of measurement,? but misses out
on ?mammal? as a result of the skew in the data.
Note that this problem, though serious for our ex-
periments, is largely artificial from the point of view
of applications. In a typical usage of a WSD system,
there is a supply of data that the system needs to dis-
ambiguate, and accuracy is measured on a labeled
sample of this data. Here, we started from a sample
of labeled data, constructed a larger corpus that does
not necessarily match it, and then ran our algorithm.
To correct for the artificial bias in our experiment,
we ran a second test in which we manually labeled a
random sample of 100 documents for each ambigu-
ous term from the larger unlabeled corpus. We used
a subset of 14 of the 21 keywords in the original
dataset. As before, we compared our system against
SENSATIONAL and the most-frequent-sense BASE-
LINE. We also compare against an SVM system us-
ing 3-fold cross-validation. We use a linear kernel
SVM, with the same set of features that are available
633
Duan et al(2009) Data Sampled Data
Num. BASE- SENSE- Num. BASE- SENSE-
Keyword senses LINE GLOSSY ATIONAL senses LINE ATIONAL GLOSSY SVM
ANA 2 63.1 87.9 100 13 75 79 74 75.8
BPD 3 39.8 71.6 52.9 7 33 48 85 66.7
BSA 2 50.1 77.9 94.7 5 97 53 89 87.9
CML 2 55.0 99.2 89.5 4 81 75 84 75.8
MAS 2 50.0 100 100 35 46 90 67 66.7
VCR 2 79.2 79.2 64.0 8 72 32 72 75.8
cold 3 37.1 73.3 66.8 3 87 81 44 90.9
culture 2 52.0 67.1 81.7 3 74 39 62 66.7
discharge 2 66.3 82.4 95.1 5 57 41 84 54.5
fat 2 50.6 50.1 53.2 2 97 60 97 97.0
mole 2 78.3 71.3 95.8 7 78 47 57 84.8
pressure 2 52.1 69.8 86.4 5 47 60 65 75.8
single 2 50.0 59.7 99.5 4 53 63 37 45.4
white - - - - 7 32 33 58 51.5
fluid 2 64.3 83.5 99.6 - - - - -
glucose 2 50.5 64.5 50.5 - - - - -
inflammation 3 35.5 52.8 50.4 - - - - -
inhibition 2 50.4 50.4 54.2 - - - - -
nutrition 3 38.8 53.8 54.9 - - - - -
transport 2 50.6 41.1 56.8 - - - - -
AVERAGE 2.16 53.4 70.3 76.1 7.71 66.3 57.2 69.6 72.5
Diff from BL - 0.0 +16.9 +22.7 - 0.0 -9.1 +3.3 +6.2
Table 3: GLOSSY?s extracted glosses can be used to create an unsupervised WSD system that achieves an accu-
racy within 3% of a supervised system. Our WSD system outperforms our BASELINE system, widely recognized
as a difficult baseline for unsupervised WSD, by 16.9% and 3.3% on two different datasets.
to the SVM in the GLOSSY system. We run our un-
supervised systems on all of the unlabeled data, and
then intersect the resulting clusters with the docu-
ment set that we randomly sampled.
The last four columns of Table 3 show our results.
The sampled data set appears to be a significantly
harder test, since even the supervised SVM achieves
only a 6% gain over the BASELINE. The SEN-
SATIONAL system does significantly worse on this
data, where there is a wider variation in the distri-
bution of senses. The GLOSSY system outperforms
both the SENSATIONAL system and the BASELINE.
7 Conclusion and Future Work
Gloss Extraction is an important, and difficult task of
extracting definitions of words from unlabeled text.
The GLOSSY system succeeds at a more feasible re-
finement of this task, the Sense Indicator Extrac-
tion task. GLOSSY?s extractions have proven use-
ful as seed definitions in an unsupervised WSD task.
There is a great deal of room for future work in ex-
panding the ability of Gloss Extraction systems to
extract sense glosses that more closely match the
meanings of a word. An important first step in this
direction is to extract relations, rather than ngrams,
that make up the definition a word?s senses.
Acknowledgments
Presentation of this work was supported by the Insti-
tute of Education Sciences, U.S. Department of Ed-
ucation, through Grant R305A080157 to Carnegie
Mellon University. The opinions expressed are those
of the authors and do not necessarily represent the
views of the Institute or the U.S. Department of Edu-
cation. The authors thank the anonymous reviewers
for their helpful suggestions and comments.
634
References
Eneko Agirre and Aitor Soroa. 2007. Semeval 2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval),
pages 7?12.
E. Agirre, O. Lopez de Lacalle, D. Martinez, and
A. Soroa. 2006. Evaluating and optimizing the param-
eters of an unsupervised graph-based WSD algorithm.
In Proceedings of the NAACL Textgraphs Workshop.
I. Androutsopoulos and D. Galanis. 2005. A practi-
cally unsupervised learning method to identify single-
snippet answers to definition questions on the web. In
Proceedings of HLT-EMNLP, pages 323?330.
Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In Empirical Methods in Natural Language Process-
ing.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a
library for support vector machines.
T. Chklovski and R. Mihalcea. 2002. Building a sense
tagged corpus with Open Mind Word Expert. In Pro-
ceedings of the Workshop on Word Sense Disambigua-
tion: Recent Successes and Future Directions.
H. Cui, M.K. Kan, and T.S. Chua. 2007. Soft pattern
matching models for definitional question answering.
ACM Trans. Information Systems, 25(2):1?30.
Mona Diab. 2004. Relieving the data acquisition bottle-
neck in word sense disambiguation. In Proceedings of
the ACL.
Weisi Duan, Min Song, and Alexander Yates. 2009. Fast
max-margin clustering for unsupervised word sense
disambiguation in biomedical texts. BMC Bioinfor-
matics, 10(S3)(S4).
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Bradford Books.
A. Fujii and T. Ishikawa. 2000. Utilizing the world wide
web as an encyclopedia: Extracting term descriptions
from semi-structured texts. In Proceedings of ACL,
pages 488?495.
M. Joshi, T. Pedersen, and R. Maclin. 2005. A compar-
ative study of support vector machines applied to the
supervised word sense disambiguation problem in the
medical domain. In Proceedings of the Second Indian
International Conference on Artificial Intelligence.
Anagha Kulkarni and Ted Pedersen. 2005. Name dis-
crimination and email clustering using unsupervised
clustering and labeling of similar contexts. In Pro-
ceedings of the Second Indian International Confer-
ence on Artificial Intelligence, pages 703?722.
Gondy Leroy and Thomas C. Rindflesch. 2004. Us-
ing symbolic knowledge in the umls to disambiguate
words in small datasets with a naive bayes classifier.
In MEDINFO.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference.
Hongfang Liu, Virginia Teller, and Carol Friedman.
2004. A multi-aspect comparison study of supervised
word sense disambiguation. Journal of the American
Medical Informatics Association, 11:320?331.
Rada Mihalcea. 2002. Bootstrapping large sense-tagged
corpora. In International Conference on Languages
Resources and Evaluations (LREC).
Rada Mihalcea. 2007. Using wikipedia for automatic
word sense disambiguation. In Proceedings of the
NAACL.
David Milne and Ian H. Witten. 2008. Learning to link
with wikipedia. In Proceedings of the 17th Conference
on Information and Knowledge Management (CIKM).
S. Mohammad and Ted Pedersen. 2004. Combining lex-
ical and syntactic features for supervised word sense
disambiguation. In Proceedings of CoNLL.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In Procs. of ACM Conference on Knowl-
edge Discovery and Data Mining (KDD-02).
Marius Pasca. 2005. Finding instance names and alterna-
tive glosses on the web: WordNet reloaded. In Com-
putational Linguistics and Intelligent Text Processing,
pages 280?292. Springer Berlin / Heidelberg.
Ana-Maria Popescu, Alexander Yates, and Oren Etzioni.
2004. Class extraction from the world wide web. In
AAAI-04 ATEM Workshop, pages 65?70.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: English
lexical sample, srl and all words. In Proceedings of
the Fourth International Workshop on Semantic Eval-
uations (SemEval).
B.J. Schijvenaars, B. Mons, M. Weeber, M.J. Schuemie,
E.M. van Mulligen, H.M. Wain, and J.A. Kors. 2005.
Thesaurus-based disambiguation of gene symbols.
BMC Bioinformatics, 6.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL.
M. Stevenson and Yorick Wilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?349.
Paola Velardi, Roberto Navigli, and Pierluigi D?Amadio.
2008. Mining the web to create specialized glossaries.
IEEE Intelligent Systems, 23(5):18?25.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the ACL.
635
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 105?110,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Generating Example Contexts to Illustrate a Target Word Sense 
 
 
Jack Mostow Weisi Duan 
Carnegie Mellon University Carnegie Mellon University 
RI-NSH 4103, 5000 Forbes Avenue Language Technologies Institute 
Pittsburgh, PA 15213-3890, USA Pittsburgh, PA 15213-3890, USA 
mostow@cs.cmu.edu wduan@cs.cmu.edu 
 
  
Abstract 
Learning a vocabulary word requires seeing it 
in multiple informative contexts.  We describe 
a system to generate such contexts for a given 
word sense.  Rather than attempt to do word 
sense disambiguation on example contexts al-
ready generated or selected from a corpus, we 
compile information about the word sense into 
the context generation process.  To evaluate the 
sense-appropriateness of the generated contexts 
compared to WordNet examples, three human 
judges chose which word sense(s) fit each ex-
ample, blind to its source and intended sense.  
On average, one judge rated the generated ex-
amples as sense-appropriate, compared to two 
judges for the WordNet examples.  Although 
the system?s precision was only half of Word-
Net?s, its recall was actually higher than 
WordNet?s, thanks to covering many senses for 
which WordNet lacks examples. 
 
1 Introduction 
Learning word meaning from example contexts is 
an important aspect of vocabulary learning. Con-
texts give clues to semantics but also convey many 
other lexical aspects, such as parts of speech, mor-
phology, and pragmatics, which help enrich a per-
son?s word knowledge base (Jenkins 1984; Nagy et 
al. 1985; Schatz 1986; Herman et al 1987; Nagy 
et al 1987; Schwanenflugel et al 1997; Kuhn and 
Stahl 1998; Fukkink et al 2001). Accordingly, one 
key issue in vocabulary instruction is how to find 
or create good example contexts to help children 
learn a particular sense of a word. Hand-vetting 
automatically generated contexts can be easier than 
hand-crafting them from scratch (Mitkov et al 
2006; Liu et al 2009). 
This paper describes what we believe is the first 
system to generate example contexts for a given 
target sense of a polysemous word.  Liu et al 
(2009) characterized good contexts for helping 
children learn vocabulary and generated them for a 
target part of speech, but not a given word sense.  
Pino and Eskenazi  (2009) addressed the polysemy 
issue, but in a system for selecting contexts rather 
than for generating them.  Generation can supply 
more contexts for a given purpose, e.g. teaching 
children, than WordNet or a fixed corpus contains. 
Section 2 describes a method to generate sense-
targeted contexts. Section 3 compares them to 
WordNet examples.  Section 4 concludes. 
2 Approach 
An obvious way to generate sense-targeted con-
texts is to generate contexts containing the target 
word, and use Word Sense Disambiguation (WSD) 
to select the ones that use the target word sense.  
However, without taking the target word sense into 
account, the generation process may not output any 
contexts that use it. Instead, we model word senses 
as topics and incorporate their sense indicators into 
the generation process ? words that imply a unique 
word sense when they co-occur with a target word.   
For example, retreat can mean ?a place of pri-
vacy; a place affording peace and quiet.?  Indica-
tors for this sense, in decreasing order of Pr(word | 
topic for target sense), include retreat, yoga, place, 
retreats, day, home, center, church, spiritual, life, 
city, time, lake, year, room, prayer, years, school, 
dog, park, beautiful, area, and stay.  Generated 
contexts include ?retreat in this bustling city?. 
Another sense of retreat (as defined in Word-
Net) is ?(military) a signal to begin a withdrawal 
105
from a dangerous position,? for which indicators 
include states, war, united, american, military, 
flag, president, world, bush, state, Israel, Iraq, in-
ternational, national, policy, forces, foreign, na-
tion, administration, power, security, iran, force, 
and Russia.  Generated contexts include ?military 
leaders believe that retreat?. 
We decompose our approach into two phases, 
summarized in Figure 1.  Section 2.1 describes the 
Sense Indicator Extraction phase, which obtains 
indicators for each WordNet synset of the target 
word.  Section 2.2 describes the Context Genera-
tion phase, which generates contexts that contain 
the target word and indicators for the target sense. 
 
Figure 1: overall work flow diagram 
2.1 Sense Indicator Extraction 
Kulkarni and Pedersen (2005) and Duan and Yates 
(2010) performed Sense Indicator Extraction, but 
the indicators they extracted are not sense targeted.  
Content words in the definition and examples for 
each sense are often good indicators for that sense, 
but we found that on their own they did poorly. 
One reason is that such indicators sometimes co-
occur with a different sense.  But the main reason 
is that there are so few of them that the word sense 
often appears without any of them.  Thus we need 
more (and if possible better) sense indicators. 
 To obtain sense-targeted indicators for a target 
word, we first assemble a corpus by issuing a 
Google query for each synset of the target word.  
The query lists the target word and all content 
words in the synset?s WordNet definition and ex-
amples, and specifies a limit of 200 hits.  The re-
sulting corpus contains a few hundred documents. 
To extract sense indicators from the corpus for a 
word, we adapt Latent Dirichlet Allocation (LDA) 
(Blei et al 2003).  LDA takes as input a corpus of 
documents and an integer k, and outputs k latent 
topics, each represented as a probability distribu-
tion over the corpus vocabulary.  For k, we use the 
number of word senses.  To bias LDA to learn top-
ics corresponding to the word senses, we use the 
content words in their WordNet definitions and 
examples as seed words. 
After learning these topics and filtering out stop 
words, we pick the 30 highest-probability words 
for each topic as indicators for the corresponding 
word sense, filtering out any words that also indi-
cate other senses. We create a corpus for each tar-
get word and run LDA on it. 
Having outlined the extraction process, we now 
explain in more detail how we learn the topics; the 
mathematically faint-hearted may skip to Section 
2.2.  Formally, given corpus   with  documents, 
let   be the number of topics, and let    and    be 
the parameters of the document and topic distribu-
tions respectively.  LDA assumes this generative 
process for each document    for a corpus  : 
1. Choose            where           
2. Choose            where           
3. For each word      in    where   
       ,   is the number of words in    
(a) Choose a topic                       
(b) Choose a topic                     
where        
In classical LDA, all   ?s are the same. We al-
low them to be different in order to use the seed 
words as high confidence indicators of target 
senses to bias the hyper-parameters of their docu-
ment distributions. 
For inference, we use Gibbs Sampling (Steyvers 
and Griffiths 2006) with transition probability  
                            
              
               
 
              
                  
 
Here        denotes the topic assignments to all 
other words in the corpus except     ;            
is the number of times word   is assigned to topic 
  in the whole corpus;          is the number of 
words assigned to topic   in the entire corpus; 
106
          is the count of tokens assigned to topic 
  in document   ; and    and       are the hyper-
parameters on     and      respectively in the two 
Dirichlet distributions. 
For each document    that contains seed words 
of some synset, we bias    toward the topic   for 
that synset by making      larger; specifically, we 
set each      to 10 times the average value of   .  
This bias causes more words     in    to be as-
signed to topic   because the words of    are likely 
to be relevant to  . These assignments then influ-
ence the topic distribution of   so as to make      
likelier to be assigned to   in any document     , 
and thus shift the document distribution in      
towards  . By this time we are back to the start of 
the loop where the document distribution of      
is biased to  .  Thus this procedure can discover 
more sense indicators for each sense. 
Our method is a variant of Labeled LDA (L-
LDA) (Ramage 2009), which allows only labels 
for each document as topics.  In contrast, our va-
riant allows all topics for each document, because 
it may use more than one sense of the target word.  
Allowing other senses provides additional flexibili-
ty to discover appropriate sense indicators. 
The LDA method we use to obtain sense indica-
tors fits naturally into the framework of bootstrap-
ping WSD (Yarowsky 1995; Mihalcea 2002; 
Martinez et al 2008; Duan and Yates 2010), in 
which seeds are given for each target word, and the 
goal is to disambiguate the target word by boot-
strapping good sense indicators that can identify 
the sense.  In contrast to WSD, our goal is to gen-
erate contexts for each sense of the target word.  
2.2 Context Generation 
To generate sense-targeted contexts, we extend the 
VEGEMATIC context generation system (Liu et 
al. 2009). VEGEMATIC generates contexts for a 
given target word using the Google N-gram cor-
pus.  Starting with a 5-gram that contains the target 
word, VEGEMATIC extends it by concatenating 
additional 5-grams that overlap by 4 words on the 
left or right. 
To satisfy various constraints on good contexts 
for learning the meaning of a word, VEGEMATIC 
uses various heuristic filters.  For example, to gen-
erate contexts likely to be informative about the 
word meaning, VEGEMATIC prefers 5-grams that 
contain words related to the target word, i.e., that 
occur more often in its presence.  However, this 
criterion is not specific to a particular target sense. 
To make VEGEMATIC sense-targeted, we 
modify this heuristic to prefer 5-grams that contain 
sense indicators.  We assign the generated contexts 
to the senses whose sense indicators they contain. 
We discard contexts that contain sense indicators 
for more than one sense. 
3 Experiments and Evaluation 
To evaluate our method, we picked 8 target words 
from a list of polysemous vocabulary words used 
in many domains and hence important for children 
to learn (Beck et al 2002).  Four of them are 
nouns:  advantage (with 3 synsets), content (7), 
force (10), and retreat (7). Four are verbs:  dash 
(6), decline (7), direct (13), and reduce (20).  Some 
of these words can have other parts of speech, but 
we exclude those senses, leaving 73 senses in total. 
We use their definitions from WordNet because 
it is a widely used, comprehensive sense inventory. 
Some alternative sense inventories might be un-
suitable. For instance, children?s dictionaries may 
lack WordNet?s rare senses or hypernym relations. 
We generated contexts for these 73 word senses 
as described in Section 2, typically 3 examples for 
each word sense.  To reduce the evaluation burden 
on our human judges, we chose just one context for 
each word sense, and for words with more than 10 
senses we chose a random sample of them.  To 
avoid unconscious bias, we chose random contexts 
rather than the best ones, which a human would 
likelier pick if vetting the generated contexts by 
hand.  For comparison, we also evaluated WordNet 
examples (23 in total) where available. 
We gave three native English-speaking college-
educated judges the examples to evaluate indepen-
dently, blind to their intended sense.  They filled in 
a table for each target word.  The left column listed 
the examples (both generated and WordNet) in 
random order, one per row.  The top row gave the 
WordNet definition of each synset, one per col-
umn.  Judges were told:  For each example, put a 
1 in the column for the sense that best fits how 
the example uses the target word.  If more than 
one sense fits, rank them 1, 2, etc.  Use the last 
two columns only to say that none of the senses 
fit, or you can't tell, and why.  (Only 10 such cas-
es arose.) 
We measured inter-rater reliability at two levels. 
107
At the fine-grained level, we measured how well 
the judges agreed on which one sense fit the exam-
ple best.  The value of Fleiss? Kappa (Shrout and 
Fleiss 1979) was 42%, considered moderate.  At 
the coarse-grained level, we measured how well 
judges agreed on which sense(s) fit at all.  Here 
Fleiss? Kappa was 48%, also considered moderate. 
We evaluated the examples on three criteria. 
Yield is the percentage of intended senses for 
which we generate at least one example ? whether 
it fits or not.  For the 73 synsets, this percentage is 
92%.  Moreover, we typically generate 3 examples 
for a word sense.  In comparison, only 34% of the 
synsets have even a single example in WordNet. 
 (Fine-grained) precision is the percentage of 
examples that the intended sense fits best accord-
ing to the judges. Human judges often disagree, so 
we prorate this percentage by the percentage of 
judges who chose the intended sense as the best fit.  
The result is algebraically equivalent to computing 
precision separately according to each judge, and 
then averaging the results.  Precision for generated 
examples was 36% for those 23 synsets and 27% 
for all 67 synsets with generated examples.  Al-
though we expected WordNet to be a gold stan-
dard, its precision for the 23 synsets having 
examples was 52% ? far less than 100%. 
This low precision suggests that the WordNet 
contexts to illustrate different senses were often 
not informative enough for the judges to distin-
guish them from all the other senses.  For example, 
the WordNet example reduce one?s standard of 
living is attached to the sense ?lessen and make 
more modest.?  However, this sense is hard to dis-
tinguish from ?lower in grade or rank or force 
somebody into an undignified situation.? In fact, 
two judges did not choose the first sense, and one 
of them chose the second sense as the best fit.   
Coarse-grained precision is similar, but based on 
how often the intended sense fits the example at 
all, whether or not it fits best.  Coarse-grained pre-
cision was 67% for the 23 WordNet examples, 
40% for the examples generated for those 23 syn-
sets, and 33% for all 67 generated examples. 
Coarse-grained precision is important because 
fine-grained semantic distinctions do not matter in 
illustrating a core sense of a word.  The problem of 
how to cluster fine-grained senses into coarse 
senses is hard, especially if consensus is required 
(Navigli et al 2007). Rather than attempt to identi-
fy a single definitive partition of a target word?s 
synsets into coarse senses, we implicitly define a 
coarse sense as the subset of synsets rated by a 
judge as fitting a given example.  Thus the cluster-
ing into coarse senses is not only judge-specific but 
example-specific:   different, possibly overlapping 
sets of synsets may fit different examples. 
Recall is the percentage of synsets that fit their 
generated examples. Algebraically it is the product 
of precision and yield.  Fine-grained recall was 
25% for the generated examples, compared to only 
18% for the WordNet examples. Coarse-grained 
recall was 30% for the generated examples, com-
pared to 23% for the WordNet examples. 
Figure 2 shows how yield, inter-rater agreement, 
and coarse and fine precision for the 8 target words 
vary with their number of synsets.  With so few 
words, this analysis is suggestive, not conclusive. 
We plot all four metrics on the same [0,1] scale to 
save space, but only the last two metrics have di-
rectly comparable values,  However, it is still mea-
ningful to compare how they vary.  Precision and 
inter-rater reliability generally appear to decrease 
with the number of senses.  As polysemy increases, 
the judges have more ways to disagree with each 
other and with our program.  Yield is mostly high, 
but might be lower for words with many senses, 
due to deficient document corpora for rare senses.   
  
Figure 2: Effects of increasing polysemy 
Table 1 compares the generated and WordNet 
examples on various measures.  It compares preci-
sion on the same 23 senses that have WordNet ex-
amples.  It compares recall on all 73 senses.  It 
compares Kappa on the 23 WordNet examples and 
the sample of generated examples the judges rated.   
Number of target word synsets 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 5 10 15 20
Yield
Fleiss' 
Kappa
Precision 
(Coarse)
Precision 
(Fine)
108
 
Generated WordNet 
Yield 92% 34% 
Senses with examples 67 23 
Avg. words in context 5.91 7.87 
Precision 
(same 23) 
    Fine 36% 52% 
Coarse 40% 67% 
Recall 
Fine  25% 18% 
Coarse 30% 23% 
Fleiss? 
Kappa 
Fine 0.43 0.39 
Coarse 0.48 0.49 
Table 1: Generated examples vs. WordNet 
Errors occur when 1) the corpus is missing a 
word sense; 2) LDA fails to find good sense indi-
cators; or 3) Context Generation fails to generate a 
sense-appropriate context. 
Our method succeeds when (1) the target sense 
occurs in the corpus, (2) LDA finds good indica-
tors for it, and (3) Context Generation uses them to 
construct a sense-appropriate context.  For exam-
ple, the first sense of advantage is ?the quality of 
having a superior or more favorable position,? for 
which we obtain the sense indicators support, work, 
time, order, life, knowledge, mind, media, human, 
market, experience, nature, make, social, informa-
tion, child, individual, cost, people, power, good, 
land, strategy, and company, and generate (among 
others) the context ?knowledge gave him an ad-
vantage?. 
Errors occur when any of these 3 steps fails.  
Step 1 fails for the sense ?reduce in scope while 
retaining essential elements? of reduce because it 
is so general that no good example exists in the 
corpus for it.  Step 2 fails for the sense of force in 
?the force of his eloquence easily persuaded them? 
because its sense indicators are men, made, great, 
page, man, time, general, day, found, side, called, 
and house.  None of these words are precise 
enough to convey the sense.  Step 3 fails for the 
sense of advantage as ?(tennis) first point scored 
after deuce,? with sense indicators point, game, 
player, tennis, set, score, points, ball, court, ser-
vice, serve, called, win, side, players, play, team, 
games, match, wins, won, net, deuce, line, oppo-
nent, and turn.  This list looks suitably tennis-
related.  However, the generated context ?the 
player has an advantage? fits the first sense of 
advantage; here the indicator player for the tennis 
sense is misleading. 
4 Contributions and Limitations 
This paper presents what we believe is the first 
system for generating sense-appropriate contexts to 
illustrate different word senses even if they have 
the same part of speech.  We define the problem of 
generating sense-targeted contexts for vocabulary 
learning, factor it into Sense Indicator Extraction 
and Context Generation, and compare the resulting 
contexts to WordNet in yield, precision, and recall 
according to human judges who decided, given 
definitions of all senses, which one(s) fit each con-
text, without knowing its source or intended sense.  
This test is much more stringent than just deciding 
whether a given word sense fits a given context. 
There are other possible baselines to compare 
against, such as Google snippets. However, Google 
snippets fare poorly on criteria for teaching child-
ren vocabulary (Liu et al under revision).  Another 
shortcoming of this alternative is the inefficiency 
of retrieving all contexts containing the target word 
and filtering out the unsuitable ones.  Instead, we 
compile constraints on suitability into a generator 
that constructs only contexts that satisfy them.  
Moreover, in contrast to retrieve-and-filter, our 
constructive method (concatenation of overlapping 
Google 5-grams) can generate novel contexts. 
There is ample room for future improvement. 
We specify word senses as WordNet synsets rather 
than as coarser-grain dictionary word senses more 
natural for educators.  Our methods for target word 
document corpus construction, Sense Indicator 
Extraction, and Context Generation are all fallible.  
On average, 1 of 3 human judges rated the result-
ing contexts as sense-appropriate, half as many as 
for WordNet examples.  However, thanks to high 
yield, their recall surpassed the percentage of syn-
sets with WordNet examples.  The ultimate crite-
rion for evaluating them will be their value in 
tutorial interventions to help students learn vocabu-
lary. 
Acknowledgments 
This work was supported by the Institute of Educa-
tion Sciences, U.S. Department of Education, 
through Grant R305A080157 to Carnegie Mellon 
University. The opinions expressed are those of the 
authors and do not necessarily represent the views 
of the Institute or the U.S. Department of Educa-
tion.  We thank the reviewers and our judges. 
109
References  
Isabel L.  Beck, Margaret G. Mckeown and Linda 
Kucan. 2002. Bringing Words to Life:  Robust 
Vocabulary Instruction. NY, Guilford. 
David Blei, Andrew Ng and Michael Jordan. 2003. 
Latent Dirichlet alocation. Journal of Machine 
Learning Research 3: 993?1022. 
Weisi Duan and Alexander Yates. 2010. Extracting 
Glosses to Disambiguate Word Senses. Human 
Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, Los 
Angeles. 
Ruben G. Fukkink, Henk Blok and Kees De Glopper. 
2001. Deriving word meaning from written context: 
A multicomponential skill. Language Learning 51(3): 
477-496. 
Patricia A. Herman, Richard C. Anderson, P. David 
Pearson and William E. Nagy. 1987. Incidental 
acquisition of word meaning from expositions with 
varied text features. Reading Research Quarterly 
22(3): 263-284. 
Joseph R. Jenkins, Marcy  Stein and Katherine Wysocki. 
1984. Learning vocabulary through reading. 
American Educational Research Journal 21: 767-787. 
Melanie R. Kuhn and Steven A. Stahl. 1998. Teaching 
children to learn word meaning from context: A 
synthesis and some questions. Journal of Literacy 
Research 30(1): 119-138. 
Anagha Kulkarni and Ted Pedersen. 2005. Name 
discrimination and email clustering using 
unsupervised clustering and labeling of similar 
contexts. Proceedings of the Second Indian 
International Conference on Artificial Intelligence, 
Pune, India. 
Liu Liu, Jack Mostow and Greg Aist. 2009. Automated 
Generation of Example Contexts for Helping 
Children Learn Vocabulary. Second ISCA Workshop 
on Speech and Language Technology in Education 
(SLaTE), Wroxall Abbey Estate, Warwickshire, 
England. 
Liu Liu, Jack Mostow and Gregory S. Aist. under 
revision. Generating Example Contexts to Help 
Children Learn Word Meaning. Journal of Natural 
Language Engineering. 
David Martinez, Oier Lopez de Lacalle and Eneko 
Agirre. 2008. On the use of automatically acquired 
examples for all-nouns word sense disambiguation. 
Journal of Artificial Intelligence Research 33: 79--
107. 
Rada Mihalcea. 2002. Bootstrapping large sense tagged 
corpora. Proceedings of the 3rd International 
Conference on Languages Resources and Evaluations 
LREC 2002, Las Palmas, Spain. 
R. Uslan Mitkov, Le An Ha and Nikiforos Karamanis. 
2006. A computer-aided environment for generating 
multiple choice test items. Natural Language 
Engineering 12(2): 177-194. 
William E. Nagy, Richard C. Anderson and Patricia A. 
Herman. 1987. Learning Word Meanings from 
Context during Normal Reading. American 
Educational Research Journal 24(2): 237-270. 
William E. Nagy, Patricia A. Herman and Richard C. 
Anderson. 1985. Learning words from context. 
Reading Research Quarterly 20(2): 233-253. 
Roberto Navigli, Kenneth C. Litkowski and Orin 
Hargraves. 2007. Semeval-2007 task 07: Coarse-
grained English all-words task. Proceedings of the 
4th International Workshop on Semantic Evaluations, 
Association for Computational Linguistics: 30-35. 
Juan Pino and Maxine Eskenazi. 2009. An Application 
of Latent Semantic Analysis to Word Sense 
Discrimination for Words with Related and Unrelated 
Meanings. The 4th Workshop on Innovative Use of 
NLP for Building Educational Applications, 
NAACL-HLT 2009 Workshops, Boulder, CO, USA. 
Daniel Ramage, David Hall, Ramesh Nallapati, and 
Christopher D. Manning. 2009. Labeled LDA: A 
supervised topic model for credit attribution in multi-
labeled corpora. Proceedings of the 2009 Conference 
on Empirical Methods in Natural Language 
Processing, Association for Computational 
Linguistics. 
Elinore K. Schatz and R. Scott Baldwin. 1986. Context 
clues are unreliable predictors of word meanings. 
Reading Research Quarterly 21: 439-453. 
Paula J. Schwanenflugel, Steven A. Stahl and Elisabeth 
L. Mcfalls. 1997. Partial Word Knowledge and 
Vocabulary Growth during Reading Comprehension. 
Journal of Literacy Research 29(4): 531-553. 
Patrick E. Shrout and Joseph L. Fleiss. 1979. Intraclass 
correlations: Uses in assessing rater reliability. 
Psychological Bulletin 86(2): 420-428. 
Mark Steyvers and Tom Griffiths. 2006. Probabilistic 
topic models. Latent Semantic Analysis: A Road to 
Meaning. T. Landauer, D. McNamara, S. Dennis and 
W. Kintsch. Hillsdale, NJ, Laurence Erlbaum. 
David Yarowsky. 1995. Unsupervised WSD rivaling 
supervised methods. Proceedings of the 33rd Annual 
Meeting of the Association for Computational 
Linguistics, Massachusetts Institute of Technology, 
Cambridge, MA. 
  
 
 
110
