Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 267?275,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Geo-mining: Discovery of Road and Transport Networks
Using Directional Patterns
Dmitry Davidov
ICNC
The Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
One of the most desired information types
when planning a trip to some place is
the knowledge of transport, roads and
geographical connectedness of prominent
sites in this place. While some transport
companies or repositories make some of
this information accessible, it is not easy
to find, and the majority of information
about uncommon places can only be found
in web free text such as blogs and fo-
rums. In this paper we present an algo-
rithmic framework which allows an auto-
mated acquisition of map-like information
from the web, based on surface patterns
like ?from X to Y?. Given a set of loca-
tions as initial seeds, we retrieve from the
web an extended set of locations and pro-
duce a map-like network which connects
these locations using transport type edges.
We evaluate our framework in several set-
tings, producing meaningful and precise
connection sets.
1 Introduction
Textual geographical information such as location
descriptions, directions, travel guides and trans-
port tables is extensively used by people. Dis-
covering such information automatically can as-
sist NLP applications such as question answering
(Santos and Cardoso, 2008), and can be useful
for a variety of other applications, including au-
tomatic map annotation.
Some textual geographical information can be
found in web sites of transport companies, tourist
sites and repositories such as Wikipedia. Such
sites usually utilize structured information such
as machine-readable meta-data, tables, schedule
forms or lists, which are relatively convenient
for processing. However, automatic utilization of
such information is limited. Even on these sites,
only a small fraction of the available geographi-
cal information is stored in a well-structured and
freely accessible form. With the growth of the
web, information can be frequently found in ?or-
dinary? web pages such as forums, travelogues or
news. In such sites, information is usually noisy,
unstructured and present in the form of free text.
This type of information can be addressed by
lexical patterns. Patterns were shown to be very
useful in all sorts of lexical acquisition tasks, giv-
ing high precision results at relatively low com-
putational costs (Pantel et al, 2004). Pattern-
driven search engine queries allow to access such
information and gather the required data very effi-
ciently (Davidov et al, 2007).
In this paper we present a framework that given
a few seed locations as a specification of a region,
discovers additional locations (including alternate
location names) and map-like travel paths through
this region labeled by transport type labels.
The type of output produced by our framework
here differs from that in previous pattern-based
studies. Unlike mainstream pattern-based web
mining, it does not target some specific two-slot
relationship and attempts to extract word tuples for
this relationship. Instead, it discovers geographi-
cal networks of transport or access connections.
Such networks are not unstructured sets of word
pairs, but a structured graph with labeled edges.
Our algorithm utilizes variations of the basic
pre-defined pattern ?[Transport] from Location1
to Location2? which allows location names and
connections to be captured starting from the given
seed location set. We acquire search engine snip-
pets and extract contexts where location names co-
appear. Next we construct a location graph and
merge transport edges to identify main transport
group types. Finally, we improve the obtained data
by reducing transitive connections and identifying
key locations.
267
The obtained location data can be used as a
draft for preparation of travel resources and on-
demand travel plans. It can also be used for ques-
tion answering systems and for automated enrich-
ment and verification of existing geographical re-
sources.
We evaluate our framework on three different
regions of different scale and type: Annapurna in
Nepal, the south Israel area and the Cardiff area
in England. In our evaluation we estimated pre-
cision and the amount of discovered locations and
transport edges, and examined the quality of the
obtained map as a whole by visually comparing
the overall connectedness of the graph to an actual
road or transport map.
2 Related Work
In this paper we utilize a pattern-based lexical
acquisition framework for the discovery of geo-
graphical information. Due to the importance of
lexical databases for many NLP tasks, substantial
research has been done on direct or indirect auto-
mated acquisition of concepts (sets of terms shar-
ing a significant aspect of their meaning) and con-
cept relationships in the form of graphs connect-
ing concepts or terms inside concepts into usually
hierarchical or bipartite networks. In the case of
geo-mining, concepts can include sets of alterna-
tive names for some place, or sets of all locations
of the same type (e.g., all countries). Geographical
relationships can include nearness of two locations
and entity-location relationships such as institute-
address, capital-country, tourist site-city etc.
The major differences between relationship ac-
quisition frameworks come from the types and an-
notation requirements of the supplied input and
the basic algorithmic approach used to process
this input. A first major algorithmic approach
is to represent word contexts as vectors in some
space and use distributional measures and auto-
matic clustering in that space. Curran (2002)
and Lin (1998) use syntactic features in the vec-
tor definition. Caraballo (1999) uses conjunction
and appositive annotations in the vector represen-
tation.While efforts have been made for improv-
ing the computational complexity of these meth-
ods (Gorman and Curran, 2006), they remain data
and computation intensive.
The second major algorithmic approach is to
use lexico-syntactic patterns, which have been
shown to produce more accurate results than fea-
ture vectors at a lower computational cost on large
corpora (Pantel et al, 2004). Most related work
deals with discovery of hypernymy (Hearst, 1992;
Pantel and Lin, 2002) and synonymy (Widdows
and Dorow, 2002; Davidov and Rappoport, 2006).
Some studies deal with the discovery of more spe-
cific relation sub-types, including inter-verb re-
lations (Chklovski and Pantel, 2004) and seman-
tic relations between nominals (Davidov and Rap-
poport, 2008). Extensive frameworks were pro-
posed for iterative discovery of pre-specified (e.g.,
(Riloff and Jones, 1999)) and unspecified (e.g.,
(Agichtein and Gravano, 2000)) relation types.
Some concepts and relationships examined by
seed-based discovery methods were of a geo-
graphical nature. For example, (Etzioni et al,
2004) discovered a set of countries and (Davidov
et al, 2007) discovered diverse country relation-
ships, including location relationships between a
country and its capital and a country and its rivers.
As noted in Section 1, the type of output that we
produce here is not an unstructured collection of
word pairs but a labeled network. As such, our
task here is much more complex.
Our study is related to geographical informa-
tion retrieval (GIR) systems. However, our prob-
lem is very far from classic GIR problem settings.
In GIR, the goal is to classify or retrieve possi-
bly multilingual documents in response to queries
in the form ?theme, spatial relationship, location?,
e.g., ?mountains near New York? (Purves et al,
2006). Our goal, in contrast, is not document re-
trieval, but the generation of a structured informa-
tion resource, a labeled location graph.
Spatial relationships used in natural language
tend to be qualitative and descriptive rather than
quantitative. The concept of Naive Geography,
which reflects the way people think and write
about geographical space, is described in (Egen-
hofer and Shariff, 1995). Later in (Egenhofer
and Shariff, 1998) they proposed a way to convert
coordinate-based relationships between spatial en-
tities to natural language using terms as ?crosses?,
?goes through? or ?runs into?. Such terms can be
potentially used in patterns to extract geographi-
cal information from text. In this paper we start
from a different pattern, ?from ... to?, which helps
in discovering transport or connectedness relation-
ships between places, e.g., ?bus from X to Y? and
?road from X to Y?.
The majority of geographical data mining
268
frameworks utilize structured data such as avail-
able gazetteers and Wikipedia metadata. Sev-
eral other studies utilize semi-structured data like
Wikipedia links (Overell and Ruger, 2007) or sub-
structures in web pages, including addresses and
phone numbers (Borges et al, 2007).
The recent Schockaert et al( 2008) framework
for extraction of topological relationships from the
web has some similarities to our study. In both
cases the algorithm produces map-like structures
using the web. However, there are significant dif-
ferences. They utilize relatively structured address
data on web pages and rely on the order of named
entities in address data for extracting containment
relationships. They also use co-appearances in
addresses (e.g., ?R1 / R2? and ?R1 & R2? as in
?Newport & Gabalfa, Cardiff?) to deduce location
boundaries. This allows them to get high precision
data for modern and heavily populated regions like
Cardiff, where the majority of offices have avail-
able well-formatted web pages.
However, in less populated regions (a major tar-
get for tourist information requests), this strategy
could be problematic since a major information
source about these places would be not local web
sites (in which local addresses are likely to be
found) but foreign visitor sites, web forums and
news. We rely on free text available in all types
of web pages, which allows us to capture unstruc-
tured information which contains a significant por-
tion of the web-available geographical knowledge.
Our goals are also different from Schockaert et
al.( 2008), since we focus on obtaining informa-
tion based on paths and transport between loca-
tions, while in their work the goal is to find a net-
work representing nearness of places rather than
their connectivity by means of transport or walk-
ing. Nevertheless, in one of our evaluation settings
we targeted the area of Cardiff as in (Schockaert
et al, 2008). This allowed us to make an indi-
rect comparison of a relevant part of our results
to previous work, achieving state-of-the-art per-
formance.
3 The Algorithm
As input to our algorithm we are given a seed of
a few location names specifiying some geograph-
ical region. In this section we describe the algo-
rithm which, given these names, extracts the la-
beled structure of connections between entities in
the desired region. We first use a predefined pat-
tern for recursive extraction of the first set of enti-
ties. Then we discover additional patterns from
co-appearing location pairs and use them to get
more terms. Next, we label and merge the ob-
tained location pairs. Finally, we construct and
refine the obtained graph.
3.1 Pattern-based discovery with web queries
In order to obtain the first set of location connec-
tions, we use derivatives of the basic pattern ?from
X to Y?. Using Yahoo! BOSS, we have utilized
the API?s ability to search for phrases with wild-
cards. Given a location name L we start the search
with patterns ?from * to L?, ?from * * to L?. These
are Yahoo! BOSS queries where enclosing words
in ?? means searching for an exact phrase and ?*?
means a wildcard for exactly one arbitrary word.
This pattern serves a few goals beyond the dis-
covery of connectedness. Thus putting ?*?s inside
the pattern rather than using ?from L to? allowed
us to avoid arbitrary truncation of multiword ex-
pressions as in ?from Moscow to St. Petersburg?
and reduced the probability of capturing unrelated
sentence parts like ?from Moscow to cover deficit?.
Location names are usually ambiguous and this
type of web queries can lead to a significant
amount of noise or location mix. There are two
types of ambiguity. First, as in ?from Billericay to
Stock....?, stock can be a location or a verb. We
filter most such cases by allowing only capital-
ized location names. Besides, such an ambiguity
is rarely captured by ?from * to L? patterns. The
second type is location ambiguity. Thus ?Moscow?
refers to at least 5 location names including farms
in Africa and Australia and a locality in Ireland. In
order to reduce mixing of locations we use the fol-
lowing simple disambiguation technique. Before
performing ?from...to? queries, we downloaded up
to 100 web pages pointed by each possible pair
from the given seed locations, generating from
a location pair L
1
, L
2
a conjunctive query ?L
1
* L
2
?. Then we extracted the most informative
terms using a simple probabilistic metric:
Rank(w) =
P (w|QueryCorpus)
P (w|GeneralCorpus)
,
comparing word distribution in the downloaded
QueryCorpus to a large general purpose offline
GeneralCorpus
1
. We thus obtained a set of
1
We used the DMOZ corpus (Gabrilovich and
Markovitch, 2005).
269
query-specific disambiguating words. Then we
added to all queries the same most frequent word
(DW) out of the ten words with highest ranks.
Thus for the seed set {Moscow, St. Petersburg},
an example of a query is <?from * to Moscow?
Russia>.
3.2 Iterative location retrieval
We retrieved all search engine snippets for each
of these initial queries
2
. If we succeeded to get
more than 50 snippets, we did not download the
complete documents. In case where only a hand-
ful of snippets were obtained, the algorithm down-
loaded up to 25 documents pointed by these snip-
pets in an attempt to get more pattern instances.
In the majority of tested cases, snippets provide
enough information for our task, and this informa-
tion was not significantly extended by download-
ing the whole documents.
Once we retrieve snippets we identify terms
appearing in the snippets in wildcard slots. For
example, if the query is <?from * to Moscow?
Russia> and we encounter a snippet ?...from
Vladivostok to Moscow...?, we add ?Vladivostok?
to our set of seeds. We then continue the search in
a breadth first search setting, stopping the search
on three conditions: (1) runaway detected ? the
total frequency of newly obtained terms through
some term?s patterns is greater than the total fre-
quency of previously discovered terms+seeds. In
this case we stop exploration through the prob-
lematic term and continue exploration through
other terms
3
; (2) we reached a predefined maxi-
mal depth D
4
; (3) no new terms discovered.
At the end of this stage we get the extended
set of terms using the set of snippets where these
terms co-appear in patterns.
3.3 Enhancement of initial pattern set
In order to get more data, we enhance the pattern
set both by discovery of new useful secondary pat-
terns and by narrowing existing patterns. After ob-
taining the new pattern set we repeat the extraction
stage described in Section 3.2.
2
Yahoo! Boss allows downloading up to a 1000 descrip-
tions, up to 50 in each request. Thus for each seed word, we
have performed a few dozen search requests.
3
Note that the ?problematic? term may be the central term
in the region we focus upon ? if this happen it means that the
seeds do not specify the region well.
4
Depth is a function of the richness of transport links in
the domain. For connected domains (Cardiff, Israel) we used
4, for less connected ones (Nepal) we used 10.
Adding secondary patterns. As in a number of
previous studies, we improve our results discover-
ing additional patterns from the obtained term set.
The algorithm selects a subset of up to 50 discov-
ered (t
1
, t
2
) term pairs appearing in ?from t
1
to t
2
?
patterns and performs the set of additional queries
of the form <?t
1
* t
2
? DW>.
We then extract from the obtained snippets the
patterns of the form ?Prefix t
1
Infix t
2
Postfix?,
where Prefix and Postfix should contain either a
punctuation symbol or 1-3 words. Prefix/Postfix
should also be bounded from left/right by a punc-
tuation or one of the 50 most frequent words in the
language (based on word counts in the offline gen-
eral corpus). Infix should contain 1-3 words with
the possible addition of punctuation symbols
5
.
We examine patterns and select useful ones ac-
cording to the following ranking scheme, based
on how well each pattern captures named entities.
For each discovered pattern we scan the obtained
snippets and offline general corpus for instances
where this pattern connects one of the original
or discovered location terms to some other term.
Let T be the set of all one to three word terms
in the language, T
d
? T the set of discovered
terms, T
c
? T the set of all capitalized terms and
Pat(t
1
, t
2
) indicates one or more co-appearances
of t
1
and t
2
in pattern Pat in the retrieved snippets
or offline general corpus. The rank R of pattern
Pat is defined by:
R(Pat) =
|{Pat|Pat(t
1
, t
2
), t
1
? T
c
, t
2
? T
d
}|
|{Pat|Pat(t
1
, t
2
), t
1
? T, t
2
? T
d
}|
In other words, we rank patterns according to the
percentage of capitalized words connected by this
pattern. We sort patterns by rank and select the
top 20% patterns. Once we have discovered a new
pattern set, we repeat the term extraction in Sec-
tion 3.2. We do this only once and not reiterate
this loop in order to avoid potential runaway prob-
lems. Obtained secondary patterns include dif-
ferent from/to templates ?to X from Y by bus?;
time/distance combinations ?X -N km bus- Y?, ?X
(bus, N min) Y? or patterns in different languages
with English location/transport names.
Narrowing existing patterns. When available
data volume is high, we would like to take advan-
tage of more data by utilizing more specific pattern
5
Search engines do not support punctuation in queries,
hence these symbols were omitted in web requests and con-
sidered only when processing the retrieved snippets.
270
sets. Since Yahoo! allows to obtain only the first
1K snippets, in case we get more than 10K hits,
we extend our queries by adding the most com-
mon term sequences appearing before or after the
pattern. Thus if for the query ?from * to Moscow?
we got more than 10K hits and among the snippets
we see ?... bus from X to Moscow...? we create an
extended pattern ?bus from * to Moscow? and use
the term extraction in Section 3.2 to get additional
terms. Unlike the extraction of secondary patterns,
this narrowing process can be repeated recursively
as long as a query brings more than 10K results.
3.4 Extraction of labeled connections
At the end of the discovery stage we get an ex-
tended set of patterns and a list of search engine
snippets discovered using these patterns. Each
snippet which captures terms t
1
, t
2
in either pri-
mary ?from t
1
to t
2
? or secondary patterns repre-
sents a potential connection between entities.
Using an observed property of the primary pat-
tern, we select as a label a term or set of terms ap-
pearing directly before ?from? and delimited with
some high frequency word or punctuation. For
example, labels for snippets based on ?from...to?
patterns and containing ?the road from...?, ?got a
bus from?, ?a TransSiberian train from...? would
be road, bus and TransSiberian train.
Once we acquire labels for the primary patterns,
we also attempt to find labels in snippets obtained
for secondary patterns discovered as described in
Section 3.3. We first locate some already labeled
pairs in secondary patterns? snippets where we can
see both label and the labeled term pair. Then,
based on the label?s position in this snippet, we
define a label slot position for this type of snip-
pet. Suppose that during the labeling of primary
pattern snippets we assigned the label ?bus? to the
pair (Novgorod, Moscow) and during the pattern
extension stage the algorithm discovered a pattern
P
new
= ?ride to t
2
from t
1
,? with a corresponding
snippet ?... getting bus ride to Moscow from Nov-
gorod...?. Then using the labeled pair our algo-
rithm defines the label slot in such a snippet type:
?getting [label] ride to t
2
from t
1
?. Once a label
slot is defined, all other pairs captured by P
new
can be successfully labeled.
3.5 Merging connection labels
Some labels may denote the same type of con-
nection. Also, large sets of connections can
share the same set of transport types. In this
case it is desired to assign a single label for
a shared set of transports. We do this by a
simple merging technique. Let C
1
, C
2
be sets
of pairs assigned to labels L
1
, L
2
. We merge
two labels if one of the following conditions holds:
(1)|C
1
? C
2
| > 0.75 ?min(|C
1
|, |C
2
|)
(2)|C
1
? C
2
| > 0.45 ?max(|C
1
|, |C
2
|)
Thus, either one group is nearly included in the
other or each group shares nearly half with the
other group. We apply this rule only once and do
not iterate recursively. At this stage we also dis-
miss weakly populated labels, keeping the 10 most
populated labels.
3.6 Processing of connection graph
Now once we have merged and assigned the la-
bels we create a pattern graph for each label and
attempt to clean the graph of noise and unneces-
sary edges. Our graph definition follows (Wid-
dows and Dorow, 2002). In our pattern graph for
label L, nodes represent terms and directed edges
represent co-appearance of two terms in some pat-
tern in snippet labeled by L. We do not add unla-
beled snippets to the graph. Now we use a set of
techniques to reduce noise and unnecessary edges.
3.6.1 Transitivity elimination
One of the main problems with the pattern-based
graph is transitivity of connections. Thus if loca-
tion A is connected to B and B to C, we frequently
acquire a ?shortcut? edge connecting A to C. Such
an edge diminishes our ability to create a clear and
meaningful spatial graph. In order to reduce such
edges we employ the following two strategies.
First, neighboring places frequently form fully
connected subgraphs. We would like to sim-
plify such cliques to reduce the amount of tran-
sitive connections. If three overlapping sets of
nodes {A
1
. . . A
n?2
},{A
2
. . . A
n?1
},{A
3
. . . A
n
}
form three different cliques, then we remove all
edges between A
1
and the nodes in the third clique
and between A
n
and the nodes in the first clique.
Second, in paths obtained by directional pat-
terns, it is common that if there is a path A
1
?
A
2
? ? ? ? A
n
where A
1
and A
n
are some
major ?key? locations
6
, then each of the nodes
A
2
. . . A
n?1
tend to be connected both to A
1
and
6
Such locations will be shown in double circles in the
evaluation.
271
to A
n
while intermediate nodes are usually con-
nected only to their close neighbors. We would
like to eliminate such transitive edges leaving only
the inter-neighbor connections.
We define as key nodes in a graph, nodes whose
degree is more than 1.5 times the average graph
degree. Then we eliminate the transitive con-
nections: if A
1
is a key node and A
1
is con-
nected to each of the nodes A
2
. . . A
n?1
, and
?i ? {2 . . . n ? 1}, A
i
is connected to A
i+1
,
then we remove the connection of A
1
to all nodes
A
3
. . . A
n?1
, leaving A
1
only connected to A
2
.
3.6.2 Clearing noise and merging names
Finally we remove potential noise which acciden-
tally connects remote graph parts. If some edge
discovered through a single pattern instance con-
nects distant (distance>3) parts of the graph we
remove it.
Additionally, we would like to merge common
name alternations and misspellings of places. We
merge two nodes A and B into one node if ei-
ther (1) A, B have exactly the same edges, and
their edge count is greater than 2; or (2) edges
of A are subset of B?s edges and the string edit
distance between A and B is less than a third of
min(StringLength(A), StringLength(B)).
4 Evaluation
Since our problem definition differs significantly
from available related work, it is not possible to
make direct comparisons. We selected three dif-
ferent cases (in Nepal, Israel, Wales) where the
obtained information can be reliably verified, and
applied our framework on these settings. As a de-
velopment set, we used the Russian rail network.
We have estimated the quality of our framework
using several measures and observations. First, we
calculated the precision and quantity of obtained
locations using map information. Then we manu-
ally estimated precision of the proposed edges and
their labels, comparing them with factual infor-
mation obtained from maps
7
, transport companies
and tourist sites. Finally we visually compared a
natural drawing of the obtained graph with a real
map. In addition, while our goals differ, the third
evaluation setting has deliberate significant simi-
larities to (Schockaert et al, 2008), which allows
us to make some comparisons.
7
We recognize that in case of some labels, e.g. ?walk?, the
precision measure is subjective. Nevertheless it provides a
good indication for the quality of our results.
4.1 The Annapurna trek area
One of the most famous sites in Nepal is the Anna-
purna trekking circuit. This is a 14-21 day walk-
ing path which passes many villages. Most of the
tourists going through this path spend weeks in
prior information mining and preparations. How-
ever, even when using the most recent maps and
guides, they discover that available geographical
knowledge is far from being complete and precise.
This trek is a good example of a case when formal
information is lacking while free-text shared expe-
rience in the web is abundant. Our goal was to test
whether the algorithm can discover such knowl-
edge automatically starting from few seed location
names (we used Pokhara, which is one of the cen-
tral cities in the area, and Khudi, a small village).
The quality of results for this task was very good.
While even crude recall estimation is very hard for
this type of task, we have discovered 100% of the
Annapurna trek settlements with population over
1K, all of the flight and bus connections, and about
80% of the walking connections.
On Figure 1 we can compare the real map and
the obtained map
8
. This discovered map includes
a partial map
9
for 4 labels ? flights, trek, bus and
jeep. You can see on the map different lines for
each label. The algorithm discovered 132 enti-
ties, all of them Annapurna-related locations. This
includes correctly recognized typos and alterna-
tive spellings, and the average was 1.2 names per
place. For example for Besisahar and Pokhara
the following spellings were recognized based
both on string distance and spatial collocation:
Besishahar, Bensisahar, BesiSahar, Besi Sahar,
Beshishahar, Beisahar, Phokra, Pohkala, Poka-
hara, Pokhara, Pokhar, Pokra, Pokhura, Pokhra.
We estimated correctness of edges comparing
to existing detailed maps. 95% of the edges were
correctly placed and labeled. Results were good
since this site is well covered and also not very
interconnected ? most of it is connected in a sin-
gle possible way. After the elimination process
described in the previous section, only 6% of the
nodes participate in 3-cliques. Thus, due to the
linearity of the original path, our method success-
8
Graph nodes were manually positioned such that edges
do not intersect. Recall that our goal is to build a network
graph, which is an abstract structure. The 2-D embedding of
the graph shown here is only for illustrative purposes and is
not part of our algorithm.
9
A few dozens of correctly discovered places were omit-
ted to make the picture readable.
272
Figure 1: Real path map of Annapurna circuit
(above) compared to automatically acquired graph
(below). The graph nodes were manually posi-
tioned such that edges do not cross each other.
Dozens of correctly discovered places were omit-
ted for readability. Double circles indicate key
nodes as explained in section 3.6.1
fully avoided the problem of mixing transitively
connected nodes into one large clique.
4.2 The Israeli south
The southern part of Israel (mostly the Negev
desert) is a sparsely populated region containing
a few main roads and a few dozen towns. There
is a limited number of tourists sites in the Negev
and hence little web information is supposed to be
available. Our goal was to see if the algorithm can
successfully detect at least major entities and to
discover their connectedness.
We discovered 56 names of different places, of
them 50 correctly belong to the region, where the
region is defined as south from the Ashquelon-
Jerusalem-Yericho line, the other 6 were Is-
raeli cities/locations outside the region (Tiberias,
Metulla, Ben Gurion, Tel Aviv, Ashdod, Haifa).
In addition we discovered 23 alternative names for
some of the 56 places. We also constructed the
corresponding connectedness graphs.
We tested the usefulness of this data attempting
to find the discovered terms in the NGA GEOnet
Names Server
10
which is considered one of the
most exhaustive geographical resources. We could
find in the database only 60% of the correctly dis-
covered English terms denoting towns, so 40% of
the terms were discovered by us and ignored by
this huge coverage database. We also tested the
quality of edges, and found that 80% of the dis-
covered edges were correctly placed and labeled.
Figure 2 shows a partial graph of the places ob-
tained for the ?road? label.
Figure 2: Partial graph for Israel south settings.
4.3 The Cardiff area
Cardiff is the capital, largest city and most pop-
ulous county in Wales. Our goal was to see
if we can discover basic means of transport and
corresponding locations connected to and inside
Cardiff. This exploration also allowed us to com-
pare some of our results to related studies. We ex-
ecuted our algorithm using as seeds Grangetown,
Cardiff and Barry. Table 1 shows the most utilized
merged labels obtained for most edge-populated
graphs together with graph size and estimated pre-
cision. In case of flights, treks and trains, precision
was estimated using exact data. In other cases we
estimated precision based on reading relevant web
pages. We can see that the majority of connectiv-
ity sets are meaningful and the precision obtained
for most of these sets is high. Figure 3 shows a
partial graph for ?walking?-type labels and Figure
10
http://earth-info.nga.mil/gns/html/
273
Nodes Edges(Prec) Label
88 120(81) walking,walk,cycling,short ride
taxis, Short bus ride,short walk
131 140(95) flights, airlines,# flights a day
12 16(100) foot path, trek, walking # miles
36 51(89) train, railway, rail travel,rail
32 98(65) bus, road, drive,direct bus
Table 1: The merged labels obtained for 5
most edge-populated graphs, including number of
nodes and edges for each label. The estimated pre-
cision according to each label definition is shown
in parentheses.
4 shows such a graph for train labels
11
. Compar-
ing the obtained map with real map data we notice
a definite correlation between actual and induced
relative connection of discovered places.
(Schockaert et al, 2008) used their frame-
work to discover neighborhoods of Cardiff. In
our case, the most appropriate relation which con-
nects neighborhood locations is walking/cycling.
Hence, comparing the results to previous work, we
have examined the results obtained for the ?walk-
ing? label in details. (Schockaert et al, 2008) re-
port discovery of 68 locations, of them 7 are al-
ternate entries, 4 can be considered vernacular or
colloquial, 10 are not considered to be neighbor-
hoods, and 5 are either close to, but not within,
Cardiff, or are areas within Cardiff that are not
recognized neighborhoods. In our set we have dis-
covered 88 neighborhood names, of them 18 are
alternate entries of correct neighborhoods, 4 can
be considered vernacular or colloquial, 3 are not
considered to be neighborhoods, and 15 are areas
outside the Cardiff area.
Considering alternate entries as hits, we got su-
perior precision of 66/88 = 0.75 in comparison to
49/68 = 0.72. It should be noted however that we
found many more alternative names possibly due
to our larger coverage. Also both our framework
and the goal were substantially different.
5 Discussion
In this paper we presented a framework which,
given a small set of seed terms describing a ge-
ographical region, discovers an underlying con-
nectivity and transport graph together with the ex-
traction of common and alternative location names
in this region. Our framework is based on the
11
Spatial position of displayed graph components is arbi-
trary, we only made sure that there are no intersecting edges.
Figure 3: Partial graph of the obtained Cardiff re-
gion for the walk/walking/cycling label.
Figure 4: Partial graph of the obtained Cardiff re-
gion for the railway/train label.
observation that ?from...to?-like patterns can en-
code connectedness in very precise manner. In our
framework, we have combined iterative pattern-
and web-based relationship acquisition with the
discovery of new patterns and refinement of the lo-
cation graph. In our evaluation we showed that our
framework is capable of extracting high quality
non-trivial information from free text given very
restricted input and not relying on any heavy pre-
processing techniques such as parsing or NER.
The success of the proposed framework opens
many challenging directions for its enhancement.
Thus we would like to incorporate in our net-
work patterns which allow traveling times and dis-
tances to be extracted, such as ?N miles from X
to Y?. While in this paper we focus on specific
type of geographical relationships, similar frame-
works can be useful for a wider class of spatial re-
lationships. Automated acquisition of spatial data
can significantly help many NLP tasks, e.g., ques-
tion answering. We would also like to incorpo-
rate some patterns based on (Egenhofer and Shar-
iff, 1998), such as ?crosses?, ?goes through? or
?runs into?, which may allow automated acquisi-
tion of complex spatial relationships. Finally, we
would like to incorporate in our framework mod-
274
ules which may allow recognition of structured
data, like those developed by (Schockaert et al,
2008).
References
Eugene Agichtein, Luis Gravano, 2000. Snowball:
Extracting Relations from Large Plain-text Collec-
tions. ACM DL ?00.
Karla Borges, Alberto Laender, Claudia Medeiros,
Clodoveu Davis, 2007. Discovering Geographic
Locations in Web Pages Using Urban Addresses.
Fourth Workshop on Geographic Information Re-
trieval.
Sharon Caraballo, 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. ACL
?99.
Timothy Chklovski, Patrick Pantel 2004. VerbOcean:
Mining the Web for Fine-grained Semantic Verb Re-
lations. EMNLP ?04.
James R. Curran, Marc Moens, 2002. Improvements
in Automatic Thesaurus Extraction. SIGLEX ?02.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel,
2007. Fully Unsupervised Discovery of Concept-
specific Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2008. Classification
of Semantic Relationships Between Nominals Using
Pattern Clusters. ACL ?08.
Max Egenhofer, Rashid Shariff, 1995. Naive Geogra-
phy. Proceedings of COSIT ?95.
Max Egenhofer, Rashid Shariff, 1998. Metric Details
for Natural-Language Spatial Relations. Journal of
the ACM TOIS, 4:295?321, 1998.
Oren Etzioni, Michael Cafarella, Doug Downey, Stan-
ley Kok, Ana-maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, Alexander Yates, 2004.
Web-scale Information Extraction in KnowItAll.
WWW ?04.
Evgeniy Gabrilovich, Shaul Markovitch, 2005. Fea-
ture Generation for Text Categorization Using World
Knowledge. IJCAI ?05.
James Gorman, James R. Curran, 2006. Scaling Dis-
tributional Similarity to Large Corpora. COLING-
ACL ?06.
Marti Hearst, 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING ?92.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING ?98.
Simon Overell, Stefan Ruger, 2007. Geographic Co-
occurrence as a Tool for GIR. Fourth ACM Work-
shop on Geographical information retrieval.
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
Ross Purves, Paul Clough, Christopher Jones, Avi
Arampatzis, Benedicte Bucher, Gaihua Fu, Hideo
Joho, Awase Syed, Subodh Vaid, Bisheng Yang,
2007. The Design and Implementation of SPIRIT: a
Spatially Aware Search Engine for Information Re-
trieval on the Internet. International Journal of Ge-
ographical Information Science, 21(7):717-745.
Ellen Riloff, Rosie Jones, 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. AAAI ?99.
Diana Santos, Nuno Cardoso, 2008. GikiP: Evalu-
ating Geographical Answers from Wikipedia. Fifth
Workshop on Geographic Information Retrieval.
Steven Schockaert, Philip Smart, Alia Abdelmoty,
Christopher Jone, 2008. Mining Topological Re-
lations from the Web. International Workshop on
Flexible Database and Information System Technol-
ogy, workshop at DEXA, Turin, pp. 652?656.
Dominic Widdows, Beate Dorow, 2002. A Graph
Model for Unsupervised Lexical Acquisition. COL-
ING ?02.
275
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 852?861,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Enhancement of Lexical Concepts Using Cross-lingual Web Mining
Dmitry Davidov
ICNC
The Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
Sets of lexical items sharing a significant
aspect of their meaning (concepts) are fun-
damental in linguistics and NLP. Manual
concept compilation is labor intensive, er-
ror prone and subjective. We present a
web-based concept extension algorithm.
Given a set of terms specifying a concept
in some language, we translate them to
a wide range of intermediate languages,
disambiguate the translations using web
counts, and discover additional concept
terms using symmetric patterns. We then
translate the discovered terms back into
the original language, score them, and ex-
tend the original concept by adding back-
translations having high scores. We eval-
uate our method in 3 source languages and
45 intermediate languages, using both hu-
man judgments and WordNet. In all cases,
our cross-lingual algorithm significantly
improves high quality concept extension.
1 Introduction
A concept (or lexical category) is a set of lex-
ical items sharing a significant aspect of their
meanings (e.g., types of food, tool names, etc).
Concepts are fundamental in linguistics and NLP,
in thesauri, dictionaries, and various applications
such as textual entailment and question answering.
Great efforts have been invested in manual
preparation of concept resources such as WordNet
(WN). However, manual preparation is labor in-
tensive, which means it is both costly and slow
to update. Applications needing data on some
very specific domain or on a recent news-related
event may find such resources lacking. In addition,
manual preparation is error-prone and susceptible
to subjective concept membership decisions, fre-
quently resulting in concepts whose terms do not
belong to the same level of granularity
1
. As a re-
sult, there is a need to find methods for automatic
improvement of concept coverage and quality.
The web is a huge up-to-date corpus covering
many domains, so using it for concept extension
has the potential to address the above problems.
The majority of web pages are written in a few
salient languages, hence most of the web-based in-
formation retrieval studies are done on these lan-
guages. However, due to the substantial growth of
the multilingual web
2
, languages in which concept
terms are expressed in the most precise manner
frequently do not match the language where in-
formation is needed. Moreover, representations of
the same concept in different languages may com-
plement each other.
In order to benefit from such cross-lingual in-
formation, concept acquisition systems should be
able to gather concept terms from many available
languages and convert them to the desired lan-
guage. In this paper we present such an algorithm.
Given a set of words specifying a concept in some
source language, we translate them to a range
of intermediate languages and disambiguate the
translations using web counts. Then we discover
additional concept terms using symmetric patterns
and translate the discovered terms back into the
original language. Finally we score the back-
translations using their intermediate languages?
properties, and extend the original concept by
adding back-translations having high scores. The
only language-specific resource required by the al-
gorithm are multilingual dictionaries, and its pro-
cessing times are very modest.
We performed thorough evaluation for 24 con-
cepts in 3 source languages (Hebrew, English and
Russian) and 45 intermediate languages. Concept
definitions were taken from existing WordNet sub-
trees, and the obtained new terms were manually
1
See Section 5.1.1.
2
http://www.internetworldstats.com/stats7.htm
852
scored by human judges. In all cases we have sig-
nificantly extended the original concept set with
high precision. We have also performed a fully
automatic evaluation with 150 concepts, showing
that the algorithm can re-discover WN concepts
with high precision and recall when given only
partial lists as input.
Section 2 discusses related work, Section 3 de-
tails the algorithm, Section 4 describes the evalua-
tion protocol and Section 5 presents our results.
2 Related work
One of the main goals of this paper is the extension
or automated creation of lexical databases such
as WN. Due to the importance of WN for NLP
tasks, substantial research was done on direct or
indirect automated extension of the English WN
(e.g., (Snow et al, 2006)) or WN in other lan-
guages (e.g., (Vintar and Fi?ser, 2008)). The major-
ity of this research was done on extending the tree
structure (finding new synsets (Snow et al, 2006)
or enriching WN with new relationships (Cuadros
and Rigau, 2008)) rather than improving the qual-
ity of existing concept/synset nodes. Other re-
lated studies develop concept acquisition frame-
works for on-demand tasks where concepts are de-
fined by user-provided seeds or patterns (Etzioni et
al., 2005; Davidov et al, 2007), or for fully unsu-
pervised database creation where concepts are dis-
covered from scratch (Banko et al, 2007; Davi-
dov and Rappoport, 2006).
Some papers directly target specific applica-
tions, and build lexical resources as a side effect.
Named Entity Recognition can be viewed as an in-
stance of the concept acquisition problem where
the desired concepts contain words that are names
of entities of a particular kind, as done in (Fre-
itag, 2004) using co-clustering and in (Etzioni et
al., 2005) using predefined pattern types.
The two main algorithmic approaches to the
problem are pattern-based concept discovery and
clustering of context feature vectors. The latter
approach represents word contexts as vectors in
some space and uses similarity measures and au-
tomatic clustering in that space (Deerwester et al,
1990). Pereira et al(1993), Curran and Moens
(2002) and Lin (1998) use syntactic features in the
vector definition. Pantel and Lin (2002) improves
on the latter by clustering by committee. Cara-
ballo (1999) uses conjunction and appositive an-
notations in the vector representation. While great
effort has been made for improving the computa-
tional complexity of these methods (Gorman and
Curran, 2006), they still remain data and compu-
tation intensive.
The second major algorithmic approach is to
use lexico-syntactic patterns. Patterns have been
shown to produce more accurate results than fea-
ture vectors, at a lower computational cost on large
corpora (Pantel et al, 2004). In concept acquisi-
tion, pattern-based methods were shown to out-
perform LSA by a large margin (Widdows and
Dorow, 2002). Since (Hearst, 1992), who used a
manually prepared set of initial lexical patterns in
order to acquire relationships, numerous pattern-
based methods have been proposed for the discov-
ery of concepts from seeds (Pantel et al, 2004;
Davidov et al, 2007; Pasca et al, 2006). Most of
these studies were done for English, while some
show the applicability of their methods to other
languages, including Greek, Czech, Slovene and
French.
Most of these papers attempt to discover con-
cepts from data available in some specific lan-
guage. Recently several studies have proposed to
utilize a second language or several specified lan-
guages in order to extract or extend concepts (Vin-
tar and Fi?ser, 2008; van der Plas and Tiedemann,
2006) or paraphrases (Bosma and Callison-Burch,
2007). However, these methods usually require
the availability of parallel corpora, which limits
their usefulness. Most of these methods utilize
distributional measures, hence they do not possess
the advantages of the pattern-based framework.
Unlike in the majority of recent studies, where
the framework is designed with specific languages
in mind, in our task, in order to take advantage
of information from diverse languages, the algo-
rithm should be able to deal well with a wide va-
riety of possible intermediate languages without
any manual adaptations. Relying solely on mul-
tilingual dictionaries and the web, our algorithm
should be able to discover language-specific pat-
terns and concept terms. While some of the pro-
posed frameworks could potentially be language-
independent, little research has been done to con-
firm this. There are a few obstacles that may
hinder applying common pattern-based methods
to other languages. Many studies utilize parsing
or POS tagging, which frequently depend on the
availability and quality of language-specific tools.
Some studies specify seed patterns in advance, and
853
it is not clear whether translated patterns can work
well on different languages. Also, the absence of
clear word segmentation in some languages (e.g.,
Chinese) can make many methods inapplicable.
A few recently proposed concept acquisition
methods require only a handful of seed words and
no pattern pre-specification (Davidov et al, 2007;
Pasca and Van Durme, 2008). While these studies
avoid some of the obstacles above, it still remains
open whether such methods are indeed language-
independent. In the translation to intermediate lan-
guages part of our framework, we adapt the algo-
rithms in (Davidov and Rappoport, 2006; Davi-
dov et al, 2007) to suit diverse languages (includ-
ing ones without explicit word segmentation). We
also develop a method for efficient automated dis-
ambiguation and translation of terms to and from
any available intermediate language.
Our study is related to cross-language infor-
mation retrieval (CLIR/CLEF) frameworks. Both
deal with information extracted from a set of lan-
guages. However, the majority of CLIR stud-
ies pursue different targets. One of the main
CLIR goals is the retrieval of documents based
on explicit queries, when the document lan-
guage is not the query language (Volk and Buite-
laar, 2002). These frameworks usually develop
language-specific tools and algorithms including
parsers and taggers in order to integrate multilin-
gual queries and documents (Jagarlamudi and Ku-
maran, 2007). Our goal is to develop a language-
independent method using cross-lingual informa-
tion, for the extension and improvement of con-
cepts rather than the retrieval of documents. Be-
sides, unlike in many CLIR frameworks, interme-
diate languages are not specified in advance and
the language of requested data is the same as the
language of request, while available information
may be found in many different intermediate lan-
guages.
3 The Algorithm
Our algorithm is comprised of the following
stages: (1) given a set of words in a source lan-
guage as a specification for some concept, we au-
tomatically translate them to a diverse set of inter-
mediate languages, using multilingual dictionar-
ies; (2) the translations are disambiguated using
web counts; (3) for each language, we retrieve a
set of web snippets where these translations co-
appear and apply a pattern-based concept exten-
sion algorithm for discovering additional terms;
(4) we translate the discovered terms back to the
source language, and disambiguate them; (5) we
score the back-translated terms using data on their
behavior in the intermediate languages, and merge
the sets obtained from different languages into a
single one, retaining terms whose score passes a
certain threshold. Stages 1-3 of the algorithm have
been described in (Davidov and Rappoport, 2009),
where the goal was to translate a concept given in
one language to other languages. The framework
presented here includes the new stages 4-5, and its
goal and evaluation methods are completely dif-
ferent.
3.1 Concept specification and translation
We start from a set of words denoting a concept in
a given source language. Thus we may use words
like (apple, banana, ...) as the definition of the
concept of fruit or (bear, wolf, fox, ...) as the def-
inition of wild animals. In order to reduce noise,
we limit the length (in words) of multiword ex-
pressions considered as terms. To calculate this
limit for a language, we randomly take 100 terms
from the appropriate dictionary and set a limit
as Lim
mwe
= round(avg(length(w))) where
length(w) is the number of words in term w. For
languages like Chinese without inherent word seg-
mentation, length(w) is the number of characters
in w. While for many languages Lim
mwe
= 1,
some languages like Vietnamese usually require
two or more words to express terms.
3.2 Disambiguation of translated terms
One of the problems in utilization of multilingual
information is ambiguity of translation. First, in
order to apply the concept acquisition algorithm,
at least some of the given concept terms must be
automatically translated to each intermediate lan-
guage. In order to avoid reliance on parallel cor-
pora, which do not exist or are extremely small for
most of our language pairs, we use bilingual dic-
tionaries. Such dictionaries usually provide many
translations, one or more for each sense, so this
translation is inherently fuzzy. Second, once we
acquire translated term lists for each intermedi-
ate language, we need to translate them back to
the source language and such back-translations are
also fuzzy. In both cases, we need to select the ap-
propriate translation for each term.
While our desire would be to work with as many
languages as possible, in practice, some or even
854
most of the concept terms may be absent from the
appropriate dictionary. Such concept terms are ig-
nored.
One way to deal with ambiguity is by applying
distributional methods, usually requiring a large
single-language corpus or, more frequently, paral-
lel corpora. However, such corpora are not readily
available for many languages and domains. Ex-
tracting such statistical information on-demand is
also computationally demanding, limiting its us-
ability. Hence, we take a simple but effective
query-based approach. This approach, while be-
ing powerful as we show in the evaluation, only
relies on a few web queries and does not rely on
any language-specific resources or data.
We use the conjecture that terms of the same
concept tend to co-appear more frequently than
ones belonging to different concepts
3
. Thus, we
select a translation of a term co-appearing most
frequently with some translation of a different
term of the same concept. We estimate how well
translations of different terms are connected to
each other. Let C = {C
i
} be the given seed
words for some concept. Let Tr(C
i
, n) be the
n-th available translation of word C
i
and Cnt(s)
denote the web count of string s obtained by a
search engine. We select a translation Tr(C
i
)
according to:
F (w
1
, w
2
) =
Cnt(?w
1
? w
2
?)? Cnt(?w
2
? w
1
?)
Cnt(w
1
)? Cnt(w
2
)
Tr(C
i
) =
argmax
s
i
(
max
s
j
j 6=i
(F (Tr(C
i
, s
i
), T r(C
j
, s
j
)))
)
We utilize the Y ahoo! ?x * y?,?x * * y? wild-
cards that allow to count only co-appearances
where x and y are separated by a single word or
word pair. As a result, we obtain a set of disam-
biguated term translations. This method is used
both in order to translate from the source lan-
guage to each intermediate language and to back-
translate the newly discovered concept terms from
the intermediate to the source language.
The number of queries in this stage depends on
the ambiguity of the concept terms? translations.
In order to decrease the amount of queries, if there
are more than three possible senses we sort them
by frequency
4
and take three senses with medium
frequency. This allows us to skip the most ambigu-
ous and rare senses without any significant effect
on performance. Also, if the number of combina-
3
Our results here support this conjecture.
4
Frequency is estimated by web count for a given word.
tions is still too high (>30), we randomly sample
at most 30 of the possible combinations.
3.3 Pattern-based extension of concept terms
in intermediate languages
We first mine the web for contexts containing
the translations. Then we extract from the re-
trieved snippets contexts where translated terms
co-appear, and detect patterns where they co-
appear symmetrically. Then we use the detected
patterns to discover additional concept terms. In
order to define word boundaries, for each language
we manually specify boundary characters such as
punctuation/space symbols. This data, along with
dictionaries, is the only language-specific data in
our framework.
Web mining for translation contexts. In order
to get language-specific data, we need to restrict
web mining each time to the processed interme-
diate language. This restriction is straightforward
if the alphabet or term translations are language-
specific or if the search API supports restriction to
this language
5
. In case where there are no such
natural restrictions, we attempt to detect and add
to our queries a few language-specific frequent
words. Using our dictionaries, we find 1?3 of the
15 most frequent words in a desired language that
are unique to that language, and we ?and? them
with the queries to ensure proper language selec-
tion. This works well for almost all languages (Es-
peranto being a notable exception).
For each pair A,B of disambiguated term trans-
lations, we construct and execute the following
two queries: {?A * B?, ?B * A?}
6
. When we
have 3 or more terms we also add {A B C D}-like
conjunction queries which include 3-5 words. For
languages with Lim
mwe
> 1, we also construct
queries with several ?*? wildcards between terms.
For each query we collect snippets containing text
fragments of web pages. Such snippets frequently
include the search terms. Since Y ahoo! Boss al-
lows retrieval of up to the 1000 first results (50 in
each query), we collect several thousands snippets.
For most of the intermediate languages, only a few
dozen queries (40 on the average) are required to
obtain sufficient data, and queries can be paral-
lelized. Thus the relevant data can be downloaded
5
Yahoo! allows restriction for 42 languages.
6
These are Yahoo! queries where enclosing words in ??
means searching for an exact phrase and ?*? means a wild-
card for exactly one arbitrary word.
855
in seconds. This makes our approach practical for
on-demand retrieval or concept verification tasks.
Meta-patterns. Following (Davidov et al,
2007), we seek symmetric patterns to retrieve
concept terms. We use two meta-pattern types.
First, a Two-Slot pattern type constructed as
follows:
[Prefix] C
1
[Infix] C
2
[Postfix]
C
i
are slots for concept terms. We allow up to
Lim
mwe
space-separated
7
words to be in a sin-
gle slot. Infix may contain punctuation, spaces,
and up to Lim
mwe
? 4 words. Prefix and Post-
fix are limited to contain punctuation characters
and/or Lim
mwe
words.
Terms of the same concept frequently co-appear
in lists. To utilize this, we introduce two additional
List pattern types
8
:
[Prefix] C
1
[Infix] (C
i
[Infix])+ (1)
[Infix] (C
i
[Infix])+ C
n
[Postfix] (2)
Following (Widdows and Dorow, 2002), we define
a pattern graph. Nodes correspond to terms and
patterns to edges. If term pair (w
1
, w
2
) appears
in pattern P , we add nodes N
w
1
, N
w
2
to the graph
and a directed edge E
P
(N
w
1
, N
w
2
) between them.
Symmetric patterns. We consider only sym-
metric patterns. We define a symmetric pat-
tern as a pattern where some concept terms
C
i
, C
j
appear both in left-to-right and right-to-
left order. For example, if we consider the
terms {apple, pineapple} we select a List pattern
?(one C
i
, )+ and C
n
.? if we find both ?one apple,
one pineapple, one guava and orange.? and ?one
watermelon, one pineapple and apple.?. If no such
patterns are found, we turn to a weaker definition,
considering as symmetric those patterns where the
same terms appear in the corpus in at least two dif-
ferent slots. Thus, we select a pattern ?for C
1
and
C
2
? if we see both ?for apple and guava,? and ?for
orange and apple,?.
Retrieving concept terms. We collect terms in
two stages. First, we obtain ?high-quality? core
terms and then we retrieve potentially more noisy
ones. At the first stage we collect all terms
9
that
7
As before, for languages without space-based word sep-
aration Lim
mwe
limits the number of characters instead.
8
(E)+ means one or more instances of E.
9
We do not consider as terms the 50 most frequent words.
are bidirectionally connected to at least two differ-
ent original translations, and call them core con-
cept terms C
core
. We also add the original ones as
core terms. Then we detect the rest of the terms
C
rest
that are connected to the core stronger than
to the remaining words, as follows:
G
in
(c)={w?C
core
|E(N
w
, N
c
) ? E(N
c
, N
w
)}
G
out
(c)={w/?C
core
|E(N
w
, N
c
) ? E(N
c
, N
w
)}
C
rest
={c||G
in
(c)|>|G
out
(c)|}
For the sake of simplicity, we do not attempt to
discover more patterns/instances iteratively by re-
querying the web. If we have enough data, we use
windowing to improve result quality. If we obtain
more than 400 snippets for some concept, we di-
vide the data into equal parts, each containing up
to 400 snippets. We apply our algorithm indepen-
dently to each part and select only the words that
appear in more than one part.
3.4 Back-translation and disambiguation
At the concept acquisition phase of our framework
we obtained sets of terms for each intermediate
language, each set representing a concept. In or-
der to be useful for the enhancement of the origi-
nal concept, these terms are now back-translated to
the source language. We disambiguate each back-
translated term using the process described in Sec-
tion 3.2. Having sets of back-translated terms for
each intermediate language, our goal is to combine
these into a single set.
3.5 Scoring and merging the back
translations
We do this merging using the following scoring
strategy, assigning for each proposed term t
?
in
concept C the score S(t
?
, C), and selecting terms
with S(t
?
, C) > H where H is a predefined
threshold.
Our scoring is based on the two following con-
siderations. First, we assume that terms extracted
from more languages tend to be less noisy and
language-dependent. Second, we would like to fa-
vor languages with less resources for a given con-
cept, since noise empirically appears to be less
prominent in such languages
10
.
For language L and concept C = {t
1
. . . t
k
}
we get a disambiguated set of translations
{Tr(t
1
, L) . . . T r(t
k
, L)}. We define relative lan-
10
Preliminary experimentation, as well as the evaluation
results presented in this paper, support both of these consid-
erations.
856
guage frequency by
LFreq(L,C) =
?
t
i
?C
(Freq(Tr(t
i
, L)))
?
L
?
,t
i
?C
(Freq(Tr(t
i
, L
?
))
where Freq(Tr(t
i
, L)) is a frequency of term?s t
i
translation to language L estimated by the num-
ber of web hits. Thus languages in which trans-
lated concept terms appear more times will get
higher relative frequency, potentially indicating a
greater concept translation ambiguity. Now, for
each new term t
?
discovered through LNum(t
?
)
different languages L
1
. . . L
LNum(t
?
)
we calculate
a term score
11
S(t
?
, C):
S(t
?
, C) = LNum(t
?
)?
(
1?
?
i
LFreq(L
i
, C)
)
For each discovered term t
?
, S(t
?
, C) ?
[0, LNum(t
?
)], while discovery of t
?
in less fre-
quent languages will cause the score to be closer to
LNum(t
?
). So terms appearing in a greater num-
ber of infrequent languages will get higher scores.
After the calculation of score for each proposed
term, we retain terms whose scores are above the
predefined threshold H . In our experiments we
have used H = 3, usually meaning that acquisi-
tion of a term through 3-4 uncommon intermedi-
ate languages should be enough to accept it. The
same score measure can also be used to filter out
?bad? terms in an already existing concept.
4 Experimental Setup
We describe here the languages, concepts and dic-
tionaries we used in our experiments.
4.1 Languages and concepts
One of the main goals in this research is to take
advantage of concept data in every possible lan-
guage. As intermediate languages, we used 45 lan-
guages including major west European languages
like French or German, Slavic languages like Rus-
sian, Semitic languages as Hebrew and Arabic,
and diverse Asian languages such as Chinese and
Persian. To configure parameters we have used a
set of 10 concepts in Russian as a development set.
These concepts were not used in evaluation.
We examined a wide variety of concepts and for
each of them we used all languages with available
translations. Table 1 shows the resulting top 10
most utilized languages in our experiments.
11
In this expression i runs only on languages with term t
?
hence the summation is not 1.
English Russian Hebrew
German(68%) English(70%) English(66%)
French(60%) German(62%) German(65%)
Italian(60%) French(62%) Italian(61%)
Portuguese(57%) Spanish(58%) French(59%)
Spanish(55%) Italian(56%) Spanish(57%)
Turkish(51%) Portuguese(54%) Portuguese(57%)
Russian(50%) Korean(50%) Korean(48%)
Korean(46%) Turkish(49%) Russian(43%)
Chinese(45%) Chinese(47%) Turkish(43%)
Czech(42%) Polish (44%) Czech(40%)
Table 1: The ten most utilized intermediate languages in
our experiments. In parentheses we show the percentage of
new terms that these languages helped discover.
We have used the English, Hebrew (Ordan and
Winter, 2008) and Russian (Gelfenbeynand et al,
2003) WordNets as sources for concepts and for
the automatic evaluation. Our concept set selec-
tion was based on English WN subtrees. To per-
form comparable experiments with Russian and
Hebrew, we have selected the same subtrees in
the Hebrew and Russian WN. Concept definitions
given to human judges for evaluation were based
on the corresponding WN glosses. For automated
evaluation we selected 150 synsets/subtrees con-
taining at least 10 single word terms (existing in
all three tested languages).
For manual evaluation we used a subset of 24
of these concepts. In this subset we tried to select
generic concepts manually, such that no domain
expert knowledge was required to check their cor-
rectness. Ten of these concepts were identical to
ones used in (Widdows and Dorow, 2002; Davi-
dov and Rappoport, 2006), which allowed us to
compare our results to recent work in case of En-
glish. Table 2 shows these 10 concepts along with
the sample terms. While the number of tested con-
cepts is not very large, it provides a good indica-
tion for the quality of our approach.
Concept Sample terms
Musical instruments guitar, flute, piano
Vehicles/transport train, bus, car
Academic subjects physics, chemistry, psychology
Body parts hand, leg, shoulder
Food egg, butter, bread
Clothes pants, skirt, jacket
Tools hammer, screwdriver, wrench
Places park, castle, garden
Crimes murder, theft, fraud
Diseases rubella, measles, jaundice
Table 2: Ten of the selected concepts with sample terms.
857
4.2 Multilingual dictionaries
We developed tools for automatic access to a num-
ber of dictionaries. We used Wikipedia cross-
language links as our main source (> 60%) for
offline translation. These links include translation
of Wikipedia terms into dozens of languages. The
main advantage of using Wikipedia is its wide cov-
erage of concepts and languages. However, one
problem it has is that it frequently encodes too
specific senses and misses common ones (bear is
translated as family Ursidae, missing its common
?wild animal? sense). To overcome these difficul-
ties, we also used Wiktionary and complemented
these offline resources with automated queries to
several (25) online dictionaries. We start with
Wikipedia definitions, then Wiktionary, and then,
if not found, we turn to online dictionaries.
5 Evaluation and Results
Potential applications of our framework include
both the extension of existing lexical databases
and the construction of new databases from a small
set of seeds for each concept. Consequently, in
our evaluation we aim to check both the ability
to extend nearly complete concepts and the abil-
ity to discover most of the concept given a few
seeds. Since in our current framework we extend
a small subset of concepts rather than the whole
database, we could not utilize application-based
evaluation strategies such as performance in WSD
tasks (Cuadros and Rigau, 2008).
5.1 Human judgment evaluation
In order to check how well we can extend existing
concepts, we count and verify the quality of new
concept terms discovered by the algorithm given
complete concepts from WN. Performing an auto-
matic evaluation of such new terms is a challeng-
ing task, since there are no exhaustive term lists
available. Thus, in order to check how well newly
added terms fit the concept definition, we have to
use human judges.
We provided four human subjects with 24 lists
of newly discovered terms, together with original
concept definitions (written as descriptive natural
language sentences) and asked them to rank (1-10,
10 being best) how well each of these terms fits
the given definition. We have instructed judges to
accept common misspellings and reject words that
are too general/narrow for the provided definition.
We mixed the discovered terms with equal
amounts of terms from three control sets: (1) terms
from the original WN concept; (2) randomly se-
lected WN terms; (3) terms obtained by apply-
ing the single-language concept acquisition algo-
rithm described in Section 3.3 in the source lan-
guage. Kappa inter-annotator agreement scores
were above 0.6 for all tests below.
5.1.1 WordNet concept extension
The middle column of Table 3 shows the judge
scores and average amount of added terms for
each source language. In this case the algorithm
was provided with complete term lists as con-
cept definitions, and was requested to extend these
lists. We can see that while the scores for original
WN terms are not perfect (7/10), single-language
and cross-lingual concept extension achieve nearly
the same scores. However, the latter discovers
many more new concept terms without reducing
quality. The difference becomes more substan-
tial for Hebrew, which is a resource-poor source
language, heavily affecting the performance of
single-language concept extension methods.
The low ranks for WN reflect the ambiguity of
definition of some of its classification subtrees.
Thus, for the ?body part? concept defined in Word-
Net as ?any part of an organism such as an or-
gan or extremity? (which is not supposed to re-
quire domain-specific knowledge to identify) low
scores were given (correctly) by judges to generic
terms such as tissue, system, apparatus and pro-
cess (process defined in WN as ?a natural pro-
longation or projection from a part of an organ-
ism?), positioned in WN as direct hyponyms of
body parts. Low scores were also given to very
specific terms like ?saddle? (posterior part of the
back of a domestic fowl) or very ambiguous terms
like ?small? (the slender part of the back).
5.1.2 Seed-based concept extension
The rightmost column of Table 3 shows similar in-
formation to the middle column, but when only
the three most frequent terms from the original
WN concept were given as concept definitions.
We can see that even given three words as seeds,
the cross-lingual framework allows to discover
many new terms. Surprisingly, terms extracted by
the cross-lingual framework achieve significantly
higher scores not only in comparison to the single-
language algorithm but also in comparison to ex-
isting WN terms. Thus while the ?native? WN
concept and single-language concept extension re-
858
sults get a score of 7/10, terms obtained by the
cross-lingual framework obtain an average score
of nearly 9/10.
This suggests that our cross-lingual framework
can lead to better (from a human judgment point
of view) assignment of terms to concepts, even in
comparison to manual annotation.
Input
all terms 3 terms
English
WordNet 7.2 7.2
Random 1.8 1.8
SingleLanguage 7.0(10) 7.8(18)
Crosslingual 6.9(19) 8.8(26)
Russian
WordNet 7.8 7.8
Random 1.9 1.9
SingleLanguage 7.4(10) 8.1(16)
Crosslingual 7.6(21) 9.0(29)
Hebrew
WordNet 7.0 7.0
Random 1.3 1.3
SingleLanguage 6.5(4) 7.5(6)
Crosslingual 6.8(18) 8.9(24)
Table 3: Human judgment scores for concept extension in
three languages (1 . . . 10, 10 is best). The WordNet, Random
and SingleLanguage rows provide corresponding baselines.
Average count of newly added terms are shown in parenthe-
ses. Average original WN concept size in this set was 36 for
English, 32 for Russian and 27 for Hebrew.
5.2 WordNet-based evaluation
While human judgment evaluation provides a
good indication for the quality of our framework,
it has severe limitations. Thus terms in many con-
cepts require domain expertise to be properly la-
beled. We have complemented human judgment
evaluation with automated WN-based evaluation
with a greater (150) number of concepts. For each
of the 150 concepts, we have applied our frame-
work on a subset of the available terms, and esti-
mated precision and recall of the resulting term list
in comparison to the original WN term list. The
evaluation protocol and metrics were very simi-
lar to (Davidov and Rappoport, 2006; Widdows
and Dorow, 2002) which allowed us to do indirect
comparison to previous work.
Table 4 shows precision and recall for this task
comparing single-language concept extension and
the cross-lingual framework. We can see that
in all cases, utilization of the latter greatly im-
proves recall. It also significantly outperforms
the single-language pattern-based method intro-
duced by (Davidov and Rappoport, 2006), which
achieves average precision of 79.3 on a similar set
in English (in comparison to 86.7 in this study).
We can also see a decrease in precision when the
algorithm is provided with 50% of the concept
terms as input and had to discover the remaining
50%. However, careful examination of the results
shows that this decrease is due to discovery of ad-
ditional correct terms not present in WordNet.
Input
50% terms 3 terms
P R F P R F
English
SingleLanguage 89.2 75.9 82.0 80.6 15.2 25.6
CrossLingual 86.5 91.1 88.7 86.7 60.2 71.1
Russian
SingleLanguage 91.3 69.0 78.6 82.1 18.3 29.9
CrossLingual 84.9 86.2 85.5 85.3 62.1 71.9
Hebrew
SingleLanguage 93.8 38.6 54.7 90.2 5.7 10.7
CrossLingual 86.5 82.4 84.4 93.9 55.6 69.8
Table 4: WordNet-based precision (P) and recall (R) for
concept extension.
5.3 Contribution of each language
Each of the 45 languages we used influences the
score of at least 5% of the discovered terms. How-
ever, it is not apparent if all languages are indeed
beneficial or if only a handful of languages can
be used. In order to check this point we have per-
formed partial automated tests as described in Sec-
tion 5.2, removing one language at a time. We also
tried to remove random subsets of 2-3 languages,
comparing them to removal of one of them. We
saw that in each case removal of more languages
caused a consistent (while sometimes minor) de-
crease both in precision and recall metrics. Thus,
each language contributes to the system.
6 Discussion
We proposed a framework which given a set of
terms defining a concept in some language, uti-
lizes multilingual information available on the
web in order to extend this list. This method
allows to take advantage of web data in many
languages, requiring only multilingual dictionar-
ies. Our method was able to discover a substan-
tially greater number of terms than state-of-the-art
single language pattern-based concept extension
methods, while retaining high precision.
We also showed that concepts obtained by this
method tend to be more coherent in compari-
son to corresponding concepts in WN, a man-
ually prepared resource. Due to its relative
language-independence and modest data require-
ments, this framework allows gathering required
859
concept information from the web even if it is scat-
tered among different and relatively uncommon or
resource-poor languages.
References
Mishele Banko, Michael J Cafarella , Stephen Soder-
land, Matt Broadhead, Oren Etzioni, 2007. Open
information extraction from the Web. IJCAI ?07.
Wauter Bosma, Chris Callison-Burch, 2007. Para-
phrase substitution for recognizing textual entail-
ment.. Evaluation of Multilingual and Multimodal
Information Retrieval, Lecture Notes in Computer
Science ?07.
Sharon Caraballo, 1999. Automatic construction of
a hypernym-labeled noun hierarchy from text. ACL
?99.
Montse Cuadros, German Rigau, 2008. KnowNet:
Building a large net of knowledge from the Web.
COLING ?08.
James R. Curran, Marc Moens, 2002. Improvements
in automatic thesaurus extraction SIGLEX 02?, 59?
66.
Dmitry Davidov, Ari Rappoport, 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel,
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2009. Translation
and extension of concepts across languages. EACL
?09.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. In-
dexing by latent semantic analysis. J. of the Ameri-
can Society for Info. Science, 41(6):391?407.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using curvature and Markov clustering in graphs for
lexical acquisition and word sense discrimination.
MEANING ?05.
Oren Etzioni, Michael Cafarella, Doug Downey,
S. Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel Weld, Alexander Yates, 2005.
Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91134.
Dayne Freitag, 2004. Trained named entity recogni-
tion using distributional clusters. EMNLP ?04.
Ilya Gelfenbeyn, Artem Goncharuk, Vladislav Lehelt,
Anton Lipatov, Victor Shilo, 2003. Automatic
translation of WordNet semantic network to Russian
language (in Russian) International Dialog 2003
Workshop.
J. Gorman, J.R. Curran, 2006. Scaling distributional
similarity to large corpora. COLING-ACL ?06.
Marti Hearst, 1992. Automatic acquisition of hy-
ponyms from large text corpora. COLING ?92.
Jagadeesh Jagarlamudi, A Kumaran, 2007. Cross-
lingual information retrieval system for Indian lan-
guages. Working Notes for the CLEF 2007 Work-
shop.
Dekang Lin, 1998. Automatic retrieval and clustering
of similar words. COLING ?98.
Noam Ordan, Shuly Wintner, 2007. Hebrew Word-
Net: a test case of aligning lexical databases across
languages. International Journal of Translation
19(1):39-58, 2007.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, Alpa Jain, 2006. Names and similarities on
the web: fact extraction in the fast lane. COLING-
ACL ?06.
Marius Pasca, Benjamin Van Durme, 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
ACL ?08.
Patrick Pantel, Dekang Lin, 2002. Discovering word
senses from text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards terascale knowledge acquisition.
COLING ?04.
John Paolillo, Daniel Pimienta, Daniel Prado, et al,
2005. Measuring linguistic diversity on the Internet.
UNESCO Institute for Statistics Montreal, Canada.
Adam Pease, Christiane Fellbaum, Piek Vossen, 2008.
Building the global WordNet grid. CIL18.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional clustering of English words. ACL ?93.
Ellen Riloff, Rosie Jones, 1999. Learning dictionaries
for information extraction by multi-level bootstrap-
ping. AAAI ?99.
Rion Snow, Daniel Jurafsky, Andrew Ng, 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. COLING-ACL ?06.
Lonneke van der Plas, Jorg Tiedemann, 2006. Find-
ing synonyms using automatic word alignment and
measures of distributional similarity. COLING-ACL
?06.
860
Martin Volk, Paul Buitelaar, 2002. A systematic eval-
uation of concept-based cross-language information
retrieval in the medical domain. In: Proc. of 3rd
Dutch-Belgian Information Retrieval Workshop.
?
Spela Vintar, Darja Fi?ser, 2008. Harvesting multi-
word expressions from parallel corpora. LREC ?08.
Dominic Widdows, Beate Dorow, 2002. A graph
model for unsupervised lexical acquisition. COL-
ING ?02.
861
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 175?183,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Translation and Extension of Concepts Across Languages
Dmitry Davidov
ICNC
The Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
We present a method which, given a few
words defining a concept in some lan-
guage, retrieves, disambiguates and ex-
tends corresponding terms that define a
similar concept in another specified lan-
guage. This can be very useful for
cross-lingual information retrieval and the
preparation of multi-lingual lexical re-
sources. We automatically obtain term
translations from multilingual dictionaries
and disambiguate them using web counts.
We then retrieve web snippets with co-
occurring translations, and discover ad-
ditional concept terms from these snip-
pets. Our term discovery is based on co-
appearance of similar words in symmetric
patterns. We evaluate our method on a set
of language pairs involving 45 languages,
including combinations of very dissimilar
ones such as Russian, Chinese, and He-
brew for various concepts. We assess the
quality of the retrieved sets using both hu-
man judgments and automatically compar-
ing the obtained categories to correspond-
ing English WordNet synsets.
1 Introduction
Numerous NLP tasks utilize lexical databases that
incorporate concepts (or word categories): sets
of terms that share a significant aspect of their
meanings (e.g., terms denoting types of food, tool
names, etc). These sets are useful by themselves
for improvement of thesauri and dictionaries, and
they are also utilized in various applications in-
cluding textual entailment and question answer-
ing. Manual development of lexical databases is
labor intensive, error prone, and susceptible to
arbitrary human decisions. While databases like
WordNet (WN) are invaluable for NLP, for some
applications any offline resource would not be ex-
tensive enough. Frequently, an application re-
quires data on some very specific topic or on very
recent news-related events. In these cases even
huge and ever-growing resources like Wikipedia
may provide insufficient coverage. Hence appli-
cations turn to Web-based on-demand queries to
obtain the desired data.
The majority of web pages are written in En-
glish and a few other salient languages, hence
most of the web-based information retrieval stud-
ies are done on these languages. However, due
to the substantial growth of the multilingual web1,
queries can be performed and the required infor-
mation can be found in less common languages,
while the query language frequently does not
match the language of available information.
Thus, if we are looking for information about
some lexical category where terms are given in
a relatively uncommon language such as Hebrew,
it is likely to find more detailed information and
more category instances in a salient language such
as English. To obtain such information, we need
to discover a word list that represents the desired
category in English. This list can be used, for in-
stance, in subsequent focused search in order to
obtain pages relevant for the given category. Thus
given a few Hebrew words as a description for
some category, it can be useful to obtain a simi-
lar (and probably more extended) set of English
words representing the same category.
In addition, when exploring some lexical cate-
gory in a common language such as English, it is
1http://www.internetworldstats.com/stats7.htm
175
frequently desired to consider available resources
from different countries. Such resources are likely
to be written in languages different from English.
In order to obtain such resources, as before, it
would be beneficial, given a concept definition in
English, to obtain word lists denoting the same
concept in different languages. In both cases a
concept as a set of words should be translated as a
whole from one language to another.
In this paper we present an algorithm that given
a concept defined as a set of words in some source
language discovers and extends a similar set in
some specified target language. Our approach
comprises three main stages. First, given a few
terms, we obtain sets of their translations to the tar-
get language from multilingual dictionaries, and
use web counts to select the appropriate word
senses. Next, we retrieve search engine snippets
with the translated terms and extract symmetric
patterns that connect these terms. Finally, we use
these patterns to extend the translated concept, by
obtaining more terms from the snippets.
We performed thorough evaluation for various
concepts involving 45 languages. The obtained
categories were manually verified with two human
judges and, when appropriate, automatically com-
pared to corresponding English WN synsets. In
all tested cases we discovered dozens of concept
terms with state-of-the-art precision.
Our major contribution is a novel framework for
concept translation across languages. This frame-
work utilizes web queries together with dictio-
naries for translation, disambiguation and exten-
sion of given terms. While our framework relies
on the existence of multilingual dictionaries, we
show that even with basic 1000 word dictionaries
we achieve good performance. Modest time and
data requirements allow the incorporation of our
method in practical applications.
In Section 2 we discuss related work, Section 3
details the algorithm, Section 4 describes the eval-
uation protocol and Section 5 presents our results.
2 Related work
Substantial efforts have been recently made to
manually construct and interconnect WN-like
databases for different languages (Pease et al,
2008; Charoenporn et al, 2007). Some stud-
ies (e.g., (Amasyali, 2005)) use semi-automated
methods based on language-specific heuristics and
dictionaries.
At the same time, much work has been done
on automatic lexical acquisition, and in particu-
lar, on the acquisition of concepts. The two main
algorithmic approaches are pattern-based discov-
ery, and clustering of context feature vectors. The
latter represents word contexts as vectors in some
space and use similarity measures and automatic
clustering in that space (Deerwester et al, 1990).
Pereira (1993), Curran (2002) and Lin (1998) use
syntactic features in the vector definition. (Pantel
and Lin, 2002) improves on the latter by cluster-
ing by committee. Caraballo (1999) uses conjunc-
tion and appositive annotations in the vector rep-
resentation. While a great effort has focused on
improving the computational complexity of these
methods (Gorman and Curran, 2006), they still re-
main data and computation intensive.
The current major algorithmic approach for
concept acquisition is to use lexico-syntactic pat-
terns. Patterns have been shown to produce more
accurate results than feature vectors, at a lower
computational cost on large corpora (Pantel et al,
2004). Since (Hearst, 1992), who used a manu-
ally prepared set of initial lexical patterns in order
to acquire relationships, numerous pattern-based
methods have been proposed for the discovery of
concepts from seeds (Pantel et al, 2004; Davidov
et al, 2007; Pasca et al, 2006). Most of these
studies were done for English, while some show
the applicability of their method to some other
languages including Russian, Greek, Czech and
French.
Many papers directly target specific applica-
tions, and build lexical resources as a side ef-
fect. Named Entity Recognition can be viewed
as an instance of the concept acquisition problem
where the desired categories contain words that
are names of entities of a particular kind, as done
in (Freitag, 2004) using co-clustering and in (Et-
zioni et al, 2005) using predefined pattern types.
Many Information Extraction papers discover re-
lationships between words using syntactic patterns
(Riloff and Jones, 1999).
Unlike in the majority of recent studies where
the acquisition framework is designed with spe-
cific languages in mind, in our task the algorithm
should be able to deal well with a wide variety
of target languages without any significant manual
adaptations. While some of the proposed frame-
works could potentially be language-independent,
little research has been done to confirm it yet.
176
There are a few obstacles that may hinder apply-
ing common pattern-based methods to other lan-
guages. Many studies utilize parsing or POS tag-
ging, which frequently depends on the availabil-
ity and quality of language-specific tools. Most
studies specify seed patterns in advance, and it is
not clear whether translated patterns can work well
on different languages. Also, the absence of clear
word segmentation in some languages (e.g., Chi-
nese) can make many methods inapplicable.
A few recently proposed concept acquisition
methods require only a handful of seed words
(Davidov et al, 2007; Pasca and Van Durme,
2008). While these studies avoid some of the ob-
stacles above, it still remains unconfirmed whether
such methods are indeed language-independent.
In the concept extension part of our algorithm we
adapt our concept acquisition framework (Davi-
dov and Rappoport, 2006; Davidov et al, 2007;
Davidov and Rappoport, 2008a; Davidov and
Rappoport, 2008b) to suit diverse languages, in-
cluding ones without explicit word segmentation.
In our evaluation we confirm the applicability of
the adapted methods to 45 languages.
Our study is related to cross-language infor-
mation retrieval (CLIR/CLEF) frameworks. Both
deal with information extracted from a set of lan-
guages. However, the majority of CLIR stud-
ies pursue different targets. One of the main
CLIR goals is the retrieval of documents based
on explicit queries, when the document lan-
guage is not the query language (Volk and Buite-
laar, 2002). These frameworks usually develop
language-specific tools and algorithms including
parsers, taggers and morphology analyzers in or-
der to integrate multilingual queries and docu-
ments (Jagarlamudi and Kumaran, 2007). Our
goal is to develop and evaluate a language-
independent method for the translation and exten-
sion of lexical categories. While our goals are dif-
ferent from CLIR, CLIR systems can greatly ben-
efit from our framework, since our translated cate-
gories can be directly utilized for subsequent doc-
ument retrieval.
Another field indirectly related to our research
is Machine Translation (MT). Many MT tasks re-
quire automated creation or improvement of dic-
tionaries (Koehn and Knight, 2001). However,
MT mainly deals with translation and disambigua-
tion of words at the sentence or document level,
while we translate whole concepts defined inde-
pendently of contexts. Our primary target is not
translation of given words, but the discovery and
extension of a concept in a target language when
the concept definition is given in some different
source language.
3 Cross-lingual Concept Translation
Framework
Our framework has three main stages: (1) given
a set of words in a source language as definition
for some concept, we automatically translate them
to the target language with multilingual dictionar-
ies, disambiguating translations using web counts;
(2) we retrieve from the web snippets where these
translations co-appear; (3) we apply a pattern-
based concept extension algorithm for discovering
additional terms from the retrieved data.
3.1 Concept words and sense selection
We start from a set of words denoting a category
in a source language. Thus we may use words
like (apple, banana, ...) as the definition of fruits
or (bear, wolf, fox, ...) as the definition of wild
animals2. Each of these words can be ambiguous.
Multilingual dictionaries usually provide many
translations, one or more for each sense. We need
to select the appropriate translation for each term.
In practice, some or even most of the category
terms may be absent in available dictionaries.
In these cases, we attempt to extract ?chain?
translations, i.e., if we cannot find Source?Target
translation, we can still find some indirect
Source?Intermediate1?Intermediate2?Target
paths. Such translations are generally much
more ambiguous, hence we allow up to two
intermediate languages in a chain. We collect all
possible translations at the chains having minimal
length, and skip category terms for whom this
process results in no translations.
Then we use the conjecture that terms of the
same concept tend to co-appear more frequently
than ones belonging to different concepts3. Thus,
2In order to reduce noise, we limit the length (in words)
of multiword expressions considered as terms. To calculate
this limit for a language we randomly take 100 terms from
the appropriate dictionary and set a limit as Limmwe =
round(avg(length(w))) where length(w) is the number of
words in term w. For languages like Chinese without inherent
word segmentation, length(w) is the number of characters in
w. While for many languages Limmwe = 1, some languages
like Vietnamese usually require two words or more to express
terms.
3Our results in this paper support this conjecture.
177
we select a translation of a term co-appearing
most frequently with some translation of a differ-
ent term of the same concept. We estimate how
well translations of different terms are connected
to each other. Let C = {Ci} be the given seed
words for some concept. Let Tr(Ci, n) be the
n-th available translation of word Ci and Cnt(s)
denote the web count of string s obtained by a
search engine. Then we select translation Tr(Ci)
according to:
F (w1, w2) =
Cnt(?w1 ? w2?)? Cnt(?w2 ? w1?)
Cnt(w1)? Cnt(w2)
Tr(Ci) =
argmax
si
(
max
sj
j 6=i
(F (Tr(Ci, si), T r(Cj , sj)))
)
We utilize the Y ahoo! ?x * y? wildcard that al-
lows to count only co-appearances where x and y
are separated by a single word. As a result, we ob-
tain a set of disambiguated term translations. The
number of queries in this stage depends on the am-
biguity of concept terms translation to the target
language. Unlike many existing disambiguation
methods based on statistics obtained from parallel
corpora, we take a rather simplistic query-based
approach. This approach is powerful (as shown
in our evaluation) and only relies on a few web
queries in a language independent manner.
3.2 Web mining for translation contexts
We need to restrict web mining to specific tar-
get languages. This restriction is straightforward
if the alphabet or term translations are language-
specific or if the search API supports restriction to
this language4. In case where there are no such
natural restrictions, we attempt to detect and add
to our queries a few language-specific frequent
words. Using our dictionaries, we find 1?3 of the
15 most frequent words in a desired language that
are unique to that language, and we ?and? them
with the queries to ensure selection of the proper
language. While some languages as Esperanto do
not satisfy any of these requirements, more than
60 languages do.
For each pair A, B of disambiguated term trans-
lations, we construct and execute the following 2
queries: {?A * B?, ?B * A?}5. When we have
3 or more terms we also add {A B C . . .}-like
conjunction queries which include 3?5 terms. For
languages with Limmwe > 1, we also construct
4Yahoo! allows restrictions for 42 languages.
5These are Yahoo! queries where enclosing words in ??
means searching for an exact phrase and ?*? means a wild-
card for exactly one arbitrary word.
queries with several ?*? wildcards between terms.
For each query we collect snippets containing text
fragments of web pages. Such snippets frequently
include the search terms. Since Y ahoo! allows re-
trieval of up to the 1000 first results (100 in each
query), we collect several thousands snippets. For
most of the target languages and categories, only a
few dozen queries (20 on the average) are required
to obtain sufficient data. Thus the relevant data
can be downloaded in seconds. This makes our
approach practical for on-demand retrieval tasks.
3.3 Pattern-based extension of concept terms
First we extract from the retrieved snippets con-
texts where translated terms co-appear, and de-
tect patterns where they co-appear symmetrically.
Then we use the detected patterns to discover ad-
ditional concept terms. In order to define word
boundaries, for each target language we manu-
ally specify boundary characters such as punctu-
ation/space symbols. This data, along with dic-
tionaries, is the only language-specific data in our
framework.
3.3.1 Meta-patterns
Following (Davidov et al, 2007) we seek symmet-
ric patterns to retrieve concept terms. We use two
meta-pattern types. First, a Two-Slot pattern type
constructed as follows:
[Prefix] C1 [Infix] C2 [Postfix]
Ci are slots for concept terms. We allow up to
Limmwe space-separated6 words to be in a sin-
gle slot. Infix may contain punctuation, spaces,
and up to Limmwe ? 4 words. Prefix and Post-
fix are limited to contain punctuation characters
and/or Limmwe words.
Terms of the same concept frequently co-appear
in lists. To utilize this, we introduce two additional
List pattern types7:
[Prefix] C1[Infix] (Ci[Infix])+ (1)
[Infix] (Ci[Infix])+ Cn [Postfix] (2)
As in (Widdows and Dorow, 2002; Davidov and
Rappoport, 2006), we define a pattern graph.
Nodes correspond to terms and patterns to edges.
If term pair (w1, w2) appears in pattern P , we add
nodes Nw1 , Nw2 to the graph and a directed edge
EP (Nw1 , Nw2) between them.
6As before, for languages without explicit space-based
word separation Limmwe limits the number of characters in-
stead.
7(X)+ means one or more instances of X .
178
3.3.2 Symmetric patterns
We consider only symmetric patterns. We define
a symmetric pattern as a pattern where some cate-
gory terms Ci, Cj appear both in left-to-right and
right-to-left order. For example, if we consider the
terms {apple, pineapple} we select a List pattern
?(one Ci, )+ and Cn.? if we find both ?one apple,
one pineapple, one guava and orange.? and ?one
watermelon, one pineapple and apple.?. If no such
patterns are found, we turn to a weaker definition,
considering as symmetric those patterns where the
same terms appear in the corpus in at least two dif-
ferent slots. Thus, we select a pattern ?for C1 and
C2? if we see both ?for apple and guava,? and ?for
orange and apple,?.
3.3.3 Retrieving concept terms
We collect terms in two stages. First, we obtain
?high-quality? core terms and then we retrieve po-
tentially more noisy ones. In the first stage we col-
lect all terms8 that are bidirectionally connected to
at least two different original translations, and call
them core concept terms Ccore. We also add the
original ones as core terms. Then we detect the
rest of the terms Crest that appear with more dif-
ferent Ccore terms than with ?out? (non-core) terms
as follows:
Gin(c)={w?Ccore|E(Nw, Nc) ? E(Nc, Nw)}
Gout(c)={w/?Ccore|E(Nw, Nc) ? E(Nc, Nw)}
Crest={c| |Gin(c)|>|Gout(c)| }
where E(Na, Nb) correspond to existence of a
graph edge denoting that translated terms a and b
co-appear in a pattern in this order. Our final term
set is the union of Ccore and Crest.
For the sake of simplicity, unlike in the ma-
jority of current research, we do not attempt to
discover more patterns/instances iteratively by re-
examining the data or re-querying the web. If we
have enough data, we use windowing to improve
result quality. If we obtain more than 400 snip-
pets for some concept, we randomly divide the
data into equal parts, each containing up to 400
snippets. We apply our algorithm independently
to each part and select only the words that appear
in more than one part.
4 Experimental Setup
We describe here the languages, concepts and dic-
tionaries we used in our experiments.
8We do not consider as terms the 50 most frequent words.
4.1 Languages and categories
One of the main goals in this research is to ver-
ify that the proposed basic method can be applied
to different languages unmodified. We examined
a wide variety of languages and concepts. Table
3 shows a list of 45 languages used in our experi-
ments, including west European languages, Slavic
languages, Semitic languages, and diverse Asian
languages.
Our concept set was based on English WN
synsets, while concept definitions for evaluation
were based on WN glosses. For automated evalua-
tion we selected as categories 150 synsets/subtrees
with at least 10 single-word terms in them. For
manual evaluation we used a subset of 24 of these
categories. In this subset we tried to select generic
categories, such that no domain expert knowledge
was required to check their correctness.
Ten of these categories were equal to ones used
in (Widdows and Dorow, 2002; Davidov and Rap-
poport, 2006), which allowed us to indirectly
compare to recent work. Table 1 shows these 10
concepts along with the sample terms. While the
number of tested categories is still modest, it pro-
vides a good indication for the quality of our ap-
proach.
Concept Sample terms
Musical instruments guitar, flute, piano
Vehicles/transport train, bus, car
Academic subjects physics, chemistry, psychology
Body parts hand, leg, shoulder
Food egg, butter, bread
Clothes pants, skirt, jacket
Tools hammer, screwdriver, wrench
Places park, castle, garden
Crimes murder, theft, fraud
Diseases rubella, measles, jaundice
Table 1: 10 of the selected categories with sample terms.
4.2 Multilingual dictionaries
We developed a set of tools for automatic access
to several dictionaries. We used Wikipedia cross-
language links as our main source (60%) for of-
fline translation. These links include translation
of Wikipedia terms into dozens of languages. The
main advantage of using Wikipedia is its wide cov-
erage of concepts and languages. However, one
problem in using it is that it frequently encodes too
specific senses and misses common ones. Thus
bear is translated as family Ursidae missing its
common ?wild animal? sense. To overcome these
179
difficulties, we also used Wiktionary and comple-
mented these offline resources with a few auto-
mated queries to several (20) online dictionaries.
We start with Wikipedia definitions, then if not
found, Wiktionary, and then we turn to online dic-
tionaries.
5 Evaluation and Results
While there are numerous concept acquisition
studies, no framework has been developed so far
to evaluate this type of cross-lingual concept dis-
covery, limiting our ability to perform a meaning-
ful comparison to previous work. Fair estimation
of translated concept quality is a challenging task.
For most languages there are no widely accepted
concept databases. Moreover, the contents of the
same concept may vary across languages. Fortu-
nately, when English is taken as a target language,
the English WN allows an automated evaluation of
concepts. We conducted evaluation in three differ-
ent settings, mostly relying on human judges and
utilizing the English WN where possible.
1. English as source language. We applied our
algorithm on a subset of 24 categories using
each of the 45 languages as a target language.
Evaluation is done by two judges9.
2. English as target language. All other lan-
guages served as source languages. In this
case human subjects manually provided in-
put terms for 150 concept definitions in each
of the target languages using 150 selected
English WN glosses. For each gloss they
were requested to provide at least 2 terms.
Then we ran the algorithm on these term
lists. Since the obtained results were English
words, we performed both manual evaluation
of the 24 categories and automated compari-
son to the original WN data.
3. Language pairs. We created 10 different non-
English language pairs for the 24 concepts.
Concept definitions were the same as in (2)
and manual evaluation followed the same
protocol as in (1).
The absence of exhaustive term lists makes recall
estimation problematic. In all cases we assess the
quality of the discovered lists in terms of precision
(P ) and length of retrieved lists (T ).
9For 19 of the languages, at least one judge was a native
speaker. For other languages at least one of the subjects was
fluent with this language.
5.1 Manual evaluation
Each discovered concept was evaluated by two
judges. All judges were fluent English speakers
and for each target language, at least one was a flu-
ent speaker of this language. They were given one-
line English descriptions of each category and the
full lists obtained by our algorithm for each of the
24 concepts. Table 2 shows the lists obtained by
our algorithm for the category described as Rela-
tives (e.g., grandmother) for several language pairs
including Hebrew?French and Chinese?Czech.
We mixed ?noise? words into each list of terms10.
These words were automatically and randomly ex-
tracted from the same text. Subjects were re-
quired to select all words fitting the provided de-
scription. They were unaware of algorithm details
and desired results. They were instructed to ac-
cept common abbreviations, alternative spellings
or misspellings like yel
?
ow?color and to accept a
term as belonging to a category if at least one
of its senses belongs to it, like orange?color and
orange?fruit. They were asked to reject terms re-
lated or associated but not belonging to the target
category, like tasty/?food, or that are too general,
like animal/?dogs.
The first 4 columns of Table 3 show averaged
results of manual evaluation for 24 categories. In
the first two columns English is used as a source
language and in the next pair of columns English is
used as the target. In addition we display in paren-
theses the amount of terms added during the ex-
tension stage. We can see that for all languages,
average precision (% of correct terms in concept)
is above 80, and frequently above 90, and the aver-
age number of extracted terms is above 30. Inter-
nal concept quality is in line with values observed
on similarly evaluated tasks for recent concept ac-
quisition studies in English. As a baseline, only
3% of the inserted 20-40% noise words were in-
correctly labeled by judges. Due to space limita-
tion we do not show the full per-concept behavior;
all medians for P and T were close to the average.
We can also observe that the majority (> 60%)
of target language terms were obtained during the
extension stage. Thus, even when considering
translation from a rich language such as English
(where given concepts frequently contain dozens
of terms), most of the discovered target language
terms are not discovered through translation but
10To reduce annotator bias, we used a different number of
noise words, adding 20?40% of the original number of words.
180
English?Portuguese:
afilhada,afilhado,amigo,avo?,avo?,bisavo?,bisavo?,
bisneta,bisneto,co?njuge,cunhada,cunhado,companheiro,
descendente,enteado,filha,filho,irma?,irma?o,irma?os,irma?s,
madrasta,madrinha,ma?e,marido,mulher,namorada,
namorado,neta,neto,noivo,padrasto,pai,papai,parente,
prima,primo,sogra,sogro,sobrinha,sobrinho,tia,tio,vizinho
Hebrew?French:
amant,ami,amie,amis,arrie`re-grand-me`re,
arrie`re-grand-pe`re,beau-fre`re,beau-parent,beau-pe`re,bebe,
belle-fille,belle-me`re,belle-soeur,be`be`,compagnon,
concubin,conjoint,cousin,cousine,demi-fre`re,demi-soeur,
e?pouse,e?poux,enfant,enfants,famille,femme,fille,fils,foyer,
fre`re,garcon,grand-me`re,grand-parent,grand-pe`re,
grands-parents,maman,mari,me`re,neveu,nie`ce,oncle,
papa,parent,pe`re,petit-enfant,petit-fils,soeur,tante
English?Spanish:
abuela,abuelo,amante,amiga,amigo,confidente,bisabuelo,
cun?ada,cun?ado,co?nyuge,esposa,esposo,esp??ritu,familia,
familiar,hermana,hermano,hija,hijo,hijos,madre,marido,
mujer,nieta,nieto,nin?o, novia,padre,papa?,primo,sobrina,
sobrino,suegra,suegro,t??a,t??o,tutor, viuda,viudo
Chinese?Czech:
babic?ka,bratr,bra?cha,chlapec,dcera,de?da,de?dec?ek,druh,
kamara?d,kamara?dka,mama,manz?el,manz?elka,matka,
muz?,otec,podnajemnik,pr???telkyne?, sestra,stars???,stry?c,
stry?c?ek, syn,se?gra,tcha?n,tchyne?,teta,vnuk,vnuc?ka,z?ena
Table 2: Sample of results for the Relatives concept. Note
that precision is not 100% (e.g. the Portuguese set includes
?friend? and ?neighbor?).
during the subsequent concept extension. In fact,
brief examination shows that less than half of
source language terms successfully pass transla-
tion and disambiguation stage. However, more
than 80% of terms which were skipped due to lack
of available translations were re-discovered in the
target language during the extension stage, along
with the discovery of new correct terms not exist-
ing in the given source definition.
The first two columns of Table 4 show similar
results for non-English language pairs. We can see
that these results are only slightly inferior to the
ones involving English.
5.2 WordNet based evaluation
We applied our algorithm on 150 concepts with
English used as the target language. Since we
want to consider common misspellings and mor-
phological combinations of correct terms as hits,
we used a basic speller and stemmer to resolve
typos and drop some English endings. The WN
columns in Table 3 display P and T values for
this evaluation. In most cases we obtain > 85%
precision. While these results (P=87,T=17) are
lower than in manual evaluation, the task is much
harder due to the large number (and hence sparse-
ness) of the utilized 150 WN categories and the
incomplete nature of WN data. For the 10 cat-
egories of Table 1 used in previous work, we
have obtained (P=92,T=41) which outperforms
the seed-based concept acquisition of (Widdows
and Dorow, 2002; Davidov and Rappoport, 2006)
(P=90,T=35) on the same concepts. However, it
should be noted that our task setting is substan-
tially different since we utilize more seeds and
they come from languages different from English.
5.3 Effect of dictionary size and source
category size
The first stage in our framework heavily relies on
the existence and quality of dictionaries, whose
coverage may be insufficient. In order to check
the effect of dictionary coverage on our task, we
re-evaluated 10 language pairs using reduced dic-
tionaries containing only the 1000 most frequent
words. The last columns in Table 4 show evalu-
ation results for such reduced dictionaries. Sur-
prisingly, while we see a difference in coverage
and precision, this difference is below 8%, thus
even basic 1000-word dictionaries may be useful
for some applications.
This may suggest that only a few correct trans-
lations are required for successful discovery of
the corresponding category. Hence, even a small
dictionary containing translations of the most fre-
quent terms could be enough. In order to test
this hypothesis, we re-evaluated the 10 language
pairs using full dictionaries while reducing the
initial concept definition to the 3 most frequent
words. The results of this experiment are shown at
columns 3?4 of Table 4. We can see that for most
language pairs, 3 seeds were sufficient to achieve
equally good results, and providing more exten-
sive concept definitions had little effect on perfor-
mance.
5.4 Variance analysis
We obtained high precision. However, we also ob-
served high variance in the number of terms be-
tween different language pairs for the same con-
cept. There are many possible reasons for this out-
come. Below we briefly discuss some of them; de-
tailed analysis of inter-language and inter-concept
variance is a major target for future work.
Web coverage of languages is not uniform (Pao-
lillo et al, 2005); e.g. Georgian has much less
web hits than English. Indeed, we observed a cor-
relation between reported web coverage and the
number of retrieved terms. Concept coverage and
181
English English as target
Language as source
Manual Manual WN
T[xx] P T[xx] P T P
Arabic 29 [12] 90 41 [35] 91 17 87
Armenian 27 [21] 93 40 [32] 92 15 86
Afrikaans 40 [29] 89 51 [28] 86 19 85
Bengali 23 [18] 95 42 [34] 93 18 88
Belorussian 23 [15] 91 43 [30] 93 17 87
Bulgarian 46 [36] 85 58 [33] 87 19 83
Catalan 45 [29] 81 56 [46] 88 21 86
Chinese 47 [34] 87 56 [22] 90 22 89
Croatian 46 [26] 90 57 [35] 92 16 89
Czech 58 [40] 89 65 [39] 94 23 88
Danish 48 [35] 94 59 [38] 97 17 90
Dutch 41 [28] 92 60 [36] 94 20 88
Estonian 35 [21] 96 47 [24] 96 16 90
Finnish 34 [21] 88 47 [29] 90 19 85
French 56 [30] 89 61 [31] 93 17 87
Georgian 22 [15] 95 39 [31] 96 16 90
German 54 [32] 91 62 [34] 92 21 83
Greek 27 [16] 93 44 [30] 95 17 91
Hebrew 38 [28] 93 45 [32] 93 18 92
Hindi 30 [10] 92 46 [28] 93 16 86
Hungarian 43 [27] 90 44 [28] 93 15 87
Italian 45 [26] 89 51 [29] 88 16 81
Icelandic 27 [21] 90 39 [27] 92 15 85
Indonesian 33 [25] 96 49 [25] 95 15 90
Japanese 40 [16] 89 50 [22] 91 20 83
Kazakh 22 [14] 96 43 [36] 97 16 92
Korean 33 [15] 88 46 [29] 89 16 85
Latvian 41 [30] 92 55 [46] 90 19 83
Lithuanian 36 [26] 94 44 [35] 95 16 89
Norwegian 37 [25] 89 46 [29] 93 15 85
Persian 17 [6] 98 40 [29] 96 15 92
Polish 38 [25] 89 55 [36] 92 17 96
Portuguese 55 [34] 87 64 [33] 90 21 85
Romanian 46 [29] 93 56 [25] 96 15 91
Russian 58 [40] 91 65 [35] 92 22 84
Serbian 19 [11] 93 36 [30] 95 17 90
Slovak 32 [20] 89 56 [39] 90 15 87
Slovenian 28 [16] 94 43 [36] 95 18 89
Spanish 53 [37] 90 66 [32] 91 23 85
Swedish 52 [33] 89 62 [39] 93 16 87
Thai 26 [13] 95 41 [34] 97 16 92
Turkish 42 [33] 92 50 [25] 93 16 88
Ukrainian 47 [33] 88 54 [28] 88 16 83
Vietnamese 26 [8] 84 48 [25] 89 15 82
Urdu 27 [14] 84 42 [36] 88 14 82
Average 38 [24] 91 50 [32] 92 17 87
Table 3: Concept translation and extension results. The
first column shows the 45 tested languages. Bold are lan-
guages evaluated with at least one native speaker. P: preci-
sion, T: number of retrieved terms. ?[xx]?: number of terms
added during the concept extension stage. Columns 1-4 show
results for manual evaluation on 24 concepts. Columns 5-6
show automated WN-based evaluation on 150 concepts. For
columns 1-2 the input category is given in English, in other
columns English served as the target language.
content is also different for each language. Thus,
concepts involving fantasy creatures were found
to have little coverage in Arabic and Hindi, and
wide coverage in European languages. For ve-
hicles, Snowmobile was detected in Finnish and
Language pair Regular Reduced Reduced
Source-Target data seed dict.
T[xx] P T P T P
Hebrew-French 43[28] 89 39 90 35 87
Arabic-Hebrew 31[24] 90 25 94 29 82
Chinese-Czech 35[29] 85 33 84 25 75
Hindi-Russian 45[33] 89 45 87 38 84
Danish-Turkish 28[20] 88 24 88 24 80
Russian-Arabic 28[18] 87 19 91 22 86
Hebrew-Russian 45[31] 92 44 89 35 84
Thai-Hebrew 28[25] 90 26 92 23 78
Finnish-Arabic 21[11] 90 14 92 16 84
Greek-Russian 48[36] 89 47 87 35 81
Average 35[26] 89 32 89 28 82
Table 4: Results for non-English pairs. P: precision, T:
number of terms. ?[xx]?: number of terms added in the exten-
sion stage. Columns 1-2 show results for normal experiment
settings, 3-4 show data for experiments where the 3 most fre-
quent terms were used as concept definitions, 5-6 describe
results for experiment with 1000-word dictionaries.
Swedish while Rickshaw appears in Hindi.
Morphology was completely neglected in this
research. To co-appear in a text, terms frequently
have to be in a certain form different from that
shown in dictionaries. Even in English, plurals
like spoons, forks co-appear more than spoon,
fork. Hence dictionaries that include morphol-
ogy may greatly improve the quality of our frame-
work. We have conducted initial experiments with
promising results in this direction, but we do not
report them here due to space limitations.
6 Conclusions
We proposed a framework that when given a set
of terms for a category in some source language
uses dictionaries and the web to retrieve a similar
category in a desired target language. We showed
that the same pattern-based method can success-
fully extend dozens of different concepts for many
languages with high precision. We observed that
even when we have very few ambiguous transla-
tions available, the target language concept can
be discovered in a fast and precise manner with-
out relying on any language-specific preprocess-
ing, databases or parallel corpora. The average
concept total processing time, including all web
requests, was below 2 minutes11. The short run-
ning time and the absence of language-specific re-
quirements allow processing queries within min-
utes and makes it possible to apply our method to
on-demand cross-language concept mining.
11We used a single PC with ADSL internet connection.
182
References
M. Fatih Amasyali, 2005. Automatic Construction of
Turkish WordNet. Signal Processing and Commu-
nications Applications Conference.
Sharon Caraballo, 1999. Automatic Construction of
a Hypernym-Labeled Noun Hierarchy from Text.
ACL ?99.
Thatsanee Charoenporn, Virach Sornlertlamvanich,
Chumpol Mokarat, Hitoshi Isahara, 2008. Semi-
Automatic Compilation of Asian WordNet. Pro-
ceedings of the 14th NLP-2008, University of Tokyo,
Komaba Campus, Japan.
James R. Curran, Marc Moens, 2002. Improvements
in Automatic Thesaurus Extraction. SIGLEX ?02,
59?66.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2008a. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ?08.
Dmitry Davidov, Ari Rappoport, 2008b. Classification
of Semantic Relationships between Nominals Using
Pattern Clusters. ACL ?08.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. In-
dexing by Latent Semantic Analysis. Journal of the
American Society for Info. Science, 41(6):391?407.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using Curvature and Markov Clustering in Graphs
for Lexical Acquisition and Word Sense Discrimi-
nation. MEANING ?05.
Oren Etzioni, Michael Cafarella, Doug Downey, S.
Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, Alexander Yates, 2005.
Unsupervised Named-Entity Extraction from the
Web: An Experimental Study. Artificial Intelli-
gence, 165(1):91134.
Dayne Freitag, 2004. Trained Named Entity Recogni-
tion Using Distributional lusters. EMNLP ?04.
James Gorman , James R. Curran, 2006. Scaling Dis-
tributional Similarity to Large Corpora COLING-
ACL ?06.
Marti Hearst, 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING ?92.
Jagadeesh Jagarlamudi, A Kumaran, 2007. Cross-
Lingual Information Retrieval System for Indian
Languages Working Notes for the CLEF 2007 Work-
shop.
Philipp Koehn, Kevin Knight, 2001. Knowl-
edge Sources for Word-Level Translation Models.
EMNLP ?01.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING ?98.
Margaret Matlin, 2005. Cognition, 6th edition. John
Wiley & Sons.
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
John Paolillo, Daniel Pimienta, Daniel Prado, et al,
2005. Measuring Linguistic Diversity on the In-
ternet. UNESCO Institute for Statistics Montreal,
Canada.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, Alpa Jain, 2006. Names and Similari-
ties on the Web: Fact Extraction in the Fast Lane.
COLING-ACL ?06.
Marius Pasca, Benjamin Van Durme, 2008. Weakly-
Supervised Acquisition of Open-Domain Classes
and Class Attributes from Web Documents and
Query Logs. ACL ?08.
Adam Pease, Christiane Fellbaum, Piek Vossen, 2008.
Building the Global WordNet Grid. CIL18.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional Clustering of English Words. ACL
?93.
Ellen Riloff, Rosie Jones, 1999. Learning Dictionar-
ies for Information Extraction by Multi-Level Boot-
strapping. AAAI ?99.
Martin Volk, Paul Buitelaar, 2002. A Systematic Eval-
uation of Concept-Based Cross-Language Informa-
tion Retrieval in the Medical Domain. In: Proc. of
3rd Dutch-Belgian Information Retrieval Workshop.
Leuven.
Dominic Widdows, Beate Dorow, 2002. A Graph
Model for Unsupervised Lexical Acquisition. COL-
ING ?02.
183
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 297?304,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Efficient Unsupervised Discovery of Word Categories
Using Symmetric Patterns and High Frequency Words
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem 91904, Israel
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem 91904, Israel
www.cs.huji.ac.il/?arir
Abstract
We present a novel approach for discov-
ering word categories, sets of words shar-
ing a significant aspect of their mean-
ing. We utilize meta-patterns of high-
frequency words and content words in or-
der to discover pattern candidates. Sym-
metric patterns are then identified using
graph-based measures, and word cate-
gories are created based on graph clique
sets. Our method is the first pattern-based
method that requires no corpus annota-
tion or manually provided seed patterns
or words. We evaluate our algorithm on
very large corpora in two languages, us-
ing both human judgments and WordNet-
based evaluation. Our fully unsupervised
results are superior to previous work that
used a POS tagged corpus, and computa-
tion time for huge corpora are orders of
magnitude faster than previously reported.
1 Introduction
Lexical resources are crucial in most NLP tasks
and are extensively used by people. Manual com-
pilation of lexical resources is labor intensive, er-
ror prone, and susceptible to arbitrary human deci-
sions. Hence there is a need for automatic author-
ing that would be as unsupervised and language-
independent as possible.
An important type of lexical resource is that
given by grouping words into categories. In gen-
eral, the notion of a category is a fundamental one
in cognitive psychology (Matlin, 2005). A lexi-
cal category is a set of words that share a signif-
icant aspect of their meaning, e.g., sets of words
denoting vehicles, types of food, tool names, etc.
A word can obviously belong to more than a single
category. We will use ?category? instead of ?lexi-
cal category? for brevity1.
Grouping of words into categories is useful in it-
self (e.g., for the construction of thesauri), and can
serve as the starting point in many applications,
such as ontology construction and enhancement,
discovery of verb subcategorization frames, etc.
Our goal in this paper is a fully unsupervised
discovery of categories from large unannotated
text corpora. We aim for categories containing sin-
gle words (multi-word lexical items will be dealt
with in future papers.) Our approach is based on
patterns, and utilizes the following stages:
1. Discovery of a set of pattern candidates that
might be useful for induction of lexical re-
lationships. We do this in a fully unsuper-
vised manner, using meta-patterns comprised
of high frequency words and content words.
2. Identification of pattern candidates that give
rise to symmetric lexical relationships. This
is done using simple measures in a word re-
lationship graph.
3. Usage of a novel graph clique-set alorithm
in order to generate categories from informa-
tion on the co-occurrence of content words in
the symmetric patterns.
We performed a thorough evaluation on two En-
glish corpora (the BNC and a 68GB web corpus)
and on a 33GB Russian corpus, and a sanity-check
test on smaller Danish, Irish and Portuguese cor-
pora. Evaluations were done using both human
1Some people use the term ?concept?. We adhere to the
cognitive psychology terminology, in which ?concept? refers
to the mental representation of a category (Matlin, 2005).
297
judgments and WordNet in a setting quite simi-
lar to that done (for the BNC) in previous work.
Our unsupervised results are superior to previous
work that used a POS tagged corpus, are less lan-
guage dependent, and are very efficient computa-
tionally2 .
Patterns are a common approach in lexical ac-
quisition. Our approach is novel in several as-
pects: (1) we discover patterns in a fully unsu-
pervised manner, as opposed to using a manually
prepared pattern set, pattern seed or words seeds;
(2) our pattern discovery requires no annotation of
the input corpus, as opposed to requiring POS tag-
ging or partial or full parsing; (3) we discover gen-
eral symmetric patterns, as opposed to using a few
hard-coded ones such as ?x and y?; (4) the clique-
set graph algorithm in stage 3 is novel. In addition,
we demonstrated the relatively language indepen-
dent nature of our approach by evaluating on very
large corpora in two languages3 .
Section 2 surveys previous work. Section 3 de-
scribes pattern discovery, and Section 4 describes
the formation of categories. Evaluation is pre-
sented in Section 5, and a discussion in Section 6.
2 Previous Work
Much work has been done on lexical acquisition
of all sorts. The three main distinguishing axes are
(1) the type of corpus annotation and other human
input used; (2) the type of lexical relationship tar-
geted; and (3) the basic algorithmic approach. The
two main approaches are pattern-based discovery
and clustering of context feature vectors.
Many of the papers cited below aim at the con-
struction of hyponym (is-a) hierarchies. Note that
they can also be viewed as algorithms for category
discovery, because a subtree in such a hierarchy
defines a lexical category.
A first major algorithmic approach is to repre-
sent word contexts as vectors in some space and
use similarity measures and automatic clustering
in that space (Curran and Moens, 2002). Pereira
(1993) and Lin (1998) use syntactic features in the
vector definition. (Pantel and Lin, 2002) improves
on the latter by clustering by committee. Cara-
ballo (1999) uses conjunction and appositive an-
notations in the vector representation.
2We did not compare against methods that use richer syn-
tactic information, both because they are supervised and be-
cause they are much more computationally demanding.
3We are not aware of any multilingual evaluation previ-
ously reported on the task.
The only previous works addressing our prob-
lem and not requiring any syntactic annotation are
those that decompose a lexically-defined matrix
(by SVD, PCA etc), e.g. (Schu?tze, 1998; Deer-
wester et al 1990). Such matrix decomposition
is computationally heavy and has not been proven
to scale well when the number of words assigned
to categories grows.
Agglomerative clustering (e.g., (Brown et al
1992; Li, 1996)) can produce hierarchical word
categories from an unannotated corpus. However,
we are not aware of work in this direction that has
been evaluated with good results on lexical cate-
gory acquisition. The technique is also quite de-
manding computationally.
The second main algorithmic approach is to
use lexico-syntactic patterns. Patterns have been
shown to produce more accurate results than fea-
ture vectors, at a lower computational cost on large
corpora (Pantel et al 2004). Hearst (1992) uses a
manually prepared set of initial lexical patterns in
order to discover hierarchical categories, and uti-
lizes those categories in order to automatically dis-
cover additional patterns.
(Berland and Charniak, 1999) use hand crafted
patterns to discover part-of (meronymy) relation-
ships, and (Chklovski and Pantel, 2004) discover
various interesting relations between verbs. Both
use information obtained by parsing. (Pantel et al
2004) reduce the depth of the linguistic data used
but still requires POS tagging.
Many papers directly target specific applica-
tions, and build lexical resources as a side effect.
Named Entity Recognition can be viewed as an in-
stance of our problem where the desired categories
contain words that are names of entities of a par-
ticular kind, as done in (Freitag, 2004) using co-
clustering. Many Information Extraction papers
discover relationships between words using syn-
tactic patterns (Riloff and Jones, 1999).
(Widdows and Dorow, 2002; Dorow et al 2005)
discover categories using two hard-coded symmet-
ric patterns, and are thus the closest to us. They
also introduce an elegant graph representation that
we adopted. They report good results. However,
they require POS tagging of the corpus, use only
two hard-coded patterns (?x and y?, ?x or y?), deal
only with nouns, and require non-trivial computa-
tions on the graph.
A third, less common, approach uses set-
theoretic inference, for example (Cimiano et al
298
2005). Again, that paper uses syntactic informa-
tion.
In summary, no previous work has combined
the accuracy, scalability and performance advan-
tages of patterns with the fully unsupervised,
unannotated nature possible with clustering ap-
proaches. This severely limits the applicability
of previous work on the huge corpora available at
present.
3 Discovery of Patterns
Our first step is the discovery of patterns that are
useful for lexical category acquisition. We use two
main stages: discovery of pattern candidates, and
identification of the symmetric patterns among the
candidates.
3.1 Pattern Candidates
An examination of the patterns found useful in
previous work shows that they contain one or more
very frequent word, such as ?and?, ?is?, etc. Our
approach towards unsupervised pattern induction
is to find such words and utilize them.
We define a high frequency word (HFW) as a
word appearing more than TH times per million
words, and a content word (CW) as a word appear-
ing less than TC times per a million words4.
Now define a meta-pattern as any sequence of
HFWs and CWs. In this paper we require that
meta-patterns obey the following constraints: (1)
at most 4 words; (2) exactly two content words; (3)
no two consecutive CWs. The rationale is to see
what can be achieved using relatively short pat-
terns and where the discovered categories contain
single words only. We will relax these constraints
in future papers. Our meta-patterns here are thus
of four types: CHC, CHCH, CHHC, and HCHC.
In order to focus on patterns that are more likely
to provide high quality categories, we removed
patterns that appear in the corpus less than TP
times per million words. Since we can ensure that
the number of HFWs is bounded, the total number
of pattern candidates is bounded as well. Hence,
this stage can be computed in time linear in the
size of the corpus (assuming the corpus has been
already pre-processed to allow direct access to a
word by its index.)
4Considerations for the selection of thresholds are dis-
cussed in Section 5.
3.2 Symmetric Patterns
Many of the pattern candidates discovered in the
previous stage are not usable. In order to find a us-
able subset, we focus on the symmetric patterns.
Our rationale is that two content-bearing words
that appear in a symmetric pattern are likely to
be semantically similar in some sense. This sim-
ple observation turns out to be very powerful, as
shown by our results. We will eventually combine
data from several patterns and from different cor-
pus windows (Section 4.)
For identifying symmetric patterns, we use a
version of the graph representation of (Widdows
and Dorow, 2002). We first define the single-
pattern graph G(P ) as follows. Nodes corre-
spond to content words, and there is a directed arc
A(x, y) from node x to node y iff (1) the words x
and y both appear in an instance of the pattern P
as its two CWs; and (2) x precedes y in P . Denote
by Nodes(G), Arcs(G) the nodes and arcs in a
graph G, respectively.
We now compute three measures on G(P ) and
combine them for all pattern candidates to filter
asymmetric ones. The first measure (M1) counts
the proportion of words that can appear in both
slots of the pattern, out of the total number of
words. The reasoning here is that if a pattern al-
lows a large percentage of words to participate in
both slots, its chances of being a symmetric pat-
tern are greater:
M1 :=
|{x|?yA(x, y) ? ?zA(z, x)}|
|Nodes(G(P ))|
M1 filters well patterns that connect words hav-
ing different parts of speech. However, it may
fail to filter patterns that contain multiple levels
of asymmetric relationships. For example, in the
pattern ?x belongs to y?, we may find a word B
on both sides (?A belongs to B?, ?B belongs to C?)
while the pattern is still asymmetric.
In order to detect symmetric relationships in a
finer manner, for the second and third measures
we define SymG(P ), the symmetric subgraph of
G(P ), containing only the bidirectional arcs and
nodes of G(P ):
SymG(P ) = {{x}, {(x, y)}|A(x, y) ? A(y, x)}
The second and third measures count the pro-
portion of the number of symmetric nodes and
edges in G(P ), respectively:
M2 :=
|Nodes(SymG(P ))|
|Nodes(G(P ))|
299
M3 :=
|Arcs(SymG(P ))|
|Arcs(G(P ))|
All three measures yield values in [0, 1], and
in all three a higher value indicates more symme-
try. M2 and M3 are obviously correlated, but they
capture different aspects of a pattern?s nature: M3
is informative for highly interconnected but small
word categories (e.g., month names), while M2 is
useful for larger categories that are more loosely
connected in the corpus.
We use the three measures as follows. For each
measure, we prepare a sorted list of all candidate
patterns. We remove patterns that are not in the
top ZT (we use 100, see Section 5) in any of the
three lists, and patterns that are in the bottom ZB
in at least one of the lists. The remaining patterns
constitute our final list of symmetric patterns.
We do not rank the final list, since the category
discovery algorithm of the next section does not
need such a ranking. Defining and utilizing such a
ranking is a subject for future work.
A sparse matrix representation of each graph
can be computed in time linear in the size of the in-
put corpus, since (1) the number of patterns |P | is
bounded, (2) vocabulary size |V | (the total number
of graph nodes) is much smaller than corpus size,
and (3) the average node degree is much smaller
than |V | (in practice, with the thresholds used, it
is a small constant.)
4 Discovery of Categories
After the end of the previous stage we have a set
of symmetric patterns. We now use them in order
to discover categories. In this section we describe
the graph clique-set method for generating initial
categories, and category pruning techniques for in-
creased quality.
4.1 The Clique-Set Method
Our approach to category discovery is based on
connectivity structures in the all-pattern word rela-
tionship graph G, resulting from merging all of the
single-pattern graphs into a single unified graph.
The graph G can be built in time O(|V | ? |P | ?
AverageDegree(G(P ))) = O(|V |) (we use V
rather than Nodes(G) for brevity.)
When building G, no special treatment is done
when one pattern is contained within another. For
example, any pattern of the form CHC is contained
in a pattern of the form HCHC (?x and y?, ?both x
and y?.) The shared part yields exactly the same
subgraph. This policy could be changed for a dis-
covery of finer relationships.
The main observation on G is that words that
are highly interconnected are good candidates to
form a category. This is the same general obser-
vation exploited by (Widdows and Dorow, 2002),
who try to find graph regions that are more con-
nected internally than externally.
We use a different algorithm. We find all strong
n-cliques (subgraphs containing n nodes that are
all bidirectionally interconnected.) A clique Q de-
fines a category that contains the nodes in Q plus
all of the nodes that are (1) at least unidirectionally
connected to all nodes in Q, and (2) bidirectionally
connected to at least one node in Q.
In practice we use 2-cliques. The strongly con-
nected cliques are the bidirectional arcs in G and
their nodes. For each such arc A, a category is gen-
erated that contains the nodes of all triangles that
contain A and at least one additional bidirectional
arc. For example, suppose the corpus contains the
text fragments ?book and newspaper?, ?newspaper
and book?, ?book and note?, ?note and book? and
?note and newspaper?. In this case the three words
are assigned to a category.
Note that a pair of nodes connected by a sym-
metric arc can appear in more than a single cate-
gory. For example, suppose a graph G containing
five nodes and seven arcs that define exactly three
strongly connected triangles, ABC,ABD,ACE.
The arc (A,B) yields a category {A,B,C,D},
and the arc (A,C) yields a category {A,C,B,E}.
Nodes A and C appear in both categories. Cate-
gory merging is described below.
This stage requires an O(1) computation for
each bidirectional arc of each node, so its com-
plexity is O(|V | ? AverageDegree(G)) =
O(|V |).
4.2 Enhancing Category Quality: Category
Merging and Corpus Windowing
In order to cover as many words as possible, we
use the smallest clique, a single symmetric arc.
This creates redundant categories. We enhance the
quality of the categories by merging them and by
windowing on the corpus.
We use two simple merge heuristics. First,
if two categories are identical we treat them as
one. Second, given two categories Q,R, we merge
them iff there?s more than a 50% overlap between
them: (|Q ? R| > |Q|/2) ? (|Q ? R| > |R|/2).
300
This could be added to the clique-set stage, but the
phrasing above is simpler to explain and imple-
ment.
In order to increase category quality and re-
move categories that are too context-specific, we
use a simple corpus windowing technique. In-
stead of running the algorithm of this section on
the whole corpus, we divide the corpus into win-
dows of equal size (see Section 5 for size deter-
mination) and perform the category discovery al-
gorithm of this section on each window indepen-
dently. Merging is also performed in each win-
dow separately. We now have a set of categories
for each window. For the final set, we select only
those categories that appear in at least two of the
windows. This technique reduces noise at the po-
tential cost of lowering coverage. However, the
numbers of categories discovered and words they
contain is still very large (see Section 5), so win-
dowing achieves higher precision without hurting
coverage in practice.
The complexity of the merge stage is O(|V |)
times the average number of categories per word
times the average number of words per category.
The latter two are small in practice, so complexity
amounts to O(|V |).
5 Evaluation
Lexical acquisition algorithms are notoriously
hard to evaluate. We have attempted to be as
thorough as possible, using several languages and
both automatic and human evaluation. In the auto-
matic part, we followed as closely as possible the
methodology and data used in previous work, so
that meaningful comparisons could be made.
5.1 Languages and Corpora
We performed in-depth evaluation on two lan-
guages, English and Russian, using three cor-
pora, two for English and one for Russian. The
first English corpus is the BNC, containing about
100M words. The second English corpus, Dmoz
(Gabrilovich and Markovitch, 2005), is a web cor-
pus obtained by crawling and cleaning the URLs
in the Open Directory Project (dmoz.org), result-
ing in 68GB containing about 8.2G words from
50M web pages.
The Russian corpus was assembled from many
web sites and carefully filtered for duplicates, to
yield 33GB and 4G words. It is a varied corpus
comprising literature, technical texts, news, news-
groups, etc.
As a preliminary sanity-check test we also ap-
plied our method to smaller corpora in Danish,
Irish and Portuguese, and noted some substantial
similarities in the discovered patterns. For exam-
ple, in all 5 languages the pattern corresponding to
?x and y? was among the 50 selected.
5.2 Thresholds, Statistics and Examples
The thresholds TH , TC , TP , ZT , ZB , were deter-
mined by memory size considerations: we com-
puted thresholds that would give us the maximal
number of words, while enabling the pattern ac-
cess table to reside in main memory. The resulting
numbers are 100, 50, 20, 100, 100.
Corpus window size was determined by starting
from a very small window size, defining at ran-
dom a single window of that size, running the al-
gorithm, and iterating this process with increased
window sizes until reaching a desired vocabulary
category participation percentage (i.e., x% of the
different words in the corpus assigned into cate-
gories. We used 5%.) This process has only a
negligible effect on running times, because each
iteration is run only on a single window, not on
the whole corpus.
The table below gives some statistics. V is the
total number of different words in the corpus. W
is the number of words belonging to at least one
of our categories. C is the number of categories
(after merging and windowing.) AS is the aver-
age category size. Running times are in minutes
on a 2.53Ghz Pentium 4 XP machine with 1GB
memory. Note how small they are, when com-
pared to (Pantel et al 2004), which took 4 days
for a smaller corpus using the same CPU.
V W C AS Time
Dmoz 16M 330K 142K 12.8 93m
BNC 337K 25K 9.6K 10.2 6.8m
Russian 10M 235K 115K 11.6 60m
Among the patterns discovered are the ubiqui-
tous ?x and y?, ?x or y? and many patterns con-
taining them. Additional patterns include ?from x
to y?, ?x and/or y? (punctuation is treated here as
white space), ?x and a y?, and ?neither x nor y?.
We discover categories of different parts of
speech. Among the noun ones, there are many
whose precision is 100%: 37 countries, 18 lan-
guages, 51 chemical elements, 62 animals, 28
types of meat, 19 fruits, 32 university names, etc.
A nice verb category example is {dive, snorkel,
swim, float, surf, sail, canoe, kayak, paddle, tube,
drift}. A nice adjective example is {amazing,
301
awesome, fascinating, inspiring, inspirational, ex-
citing, fantastic, breathtaking, gorgeous.}
5.3 Human Judgment Evaluation
The purpose of the human evaluation was dual: to
assess the quality of the discovered categories in
terms of precision, and to compare with those ob-
tained by a baseline clustering algorithm.
For the baseline, we implemented k-means as
follows. We have removed stopwords from the
corpus, and then used as features the words which
appear before or after the target word. In the calcu-
lation of feature values and inter-vector distances,
and in the removal of less informative features, we
have strictly followed (Pantel and Lin, 2002). We
ran the algorithm 10 times using k = 500 with
randomly selected centroids, producing 5000 clus-
ters. We then merged the resulting clusters us-
ing the same 50% overlap criterion as in our algo-
rithm. The result included 3090, 2116, and 3206
clusters for Dmoz, BNC and Russian respectively.
We used 8 subjects for evaluation of the English
categories and 15 subjects for evaluation of the
Russian ones. In order to assess the subjects? re-
liability, we also included random categories (see
below.)
The experiment contained two parts. In Part
I, subjects were given 40 triplets of words and
were asked to rank them using the following scale:
(1) the words definitely share a significant part
of their meaning; (2) the words have a shared
meaning but only in some context; (3) the words
have a shared meaning only under a very un-
usual context/situation; (4) the words do not share
any meaning; (5) I am not familiar enough with
some/all of the words.
The 40 triplets were obtained as follows. 20 of
our categories were selected at random from the
non-overlapping categories we have discovered,
and three words were selected from each of these
at random. 10 triplets were selected in the same
manner from the categories produced by k-means,
and 10 triplets were generated by random selec-
tion of content words from the same window in
the corpus.
In Part II, subjects were given the full categories
of the triplets that were graded as 1 or 2 in Part I
(that is, the full ?good? categories in terms of shar-
ing of meaning.) They were asked to grade the
categories from 1 (worst) to 10 (best) according to
how much the full category had met the expecta-
tions they had when seeing only the triplet.
Results are given in Table 1. The first line gives
the average percentage of triplets that were given
scores of 1 or 2 (that is, ?significant shared mean-
ing?.) The 2nd line gives the average score of
a triplet (1 is best.) In these lines scores of 5
were not counted. The 3rd line gives the average
score given to a full category (10 is best.) Inter-
evaluator Kappa between scores 1,2 and 3,4 was
0.56, 0.67 and 0.72 for Dmoz, BNC and Russian
respectively.
Our algorithm clearly outperforms k-means,
which outperforms random. We believe that the
Russian results are better because the percentage
of native speakers among our subjects for Russian
was larger than that for English.
5.4 WordNet-Based Evaluation
The major guideline in this part of the evalua-
tion was to compare our results with previous
work having a similar goal (Widdows and Dorow,
2002). We have followed their methodology as
best as we could, using the same WordNet (WN)
categories and the same corpus (BNC) in addition
to the Dmoz and Russian corpora5 .
The evaluation method is as follows. We took
the exact 10 WN subsets referred to as ?subjects?
in (Widdows and Dorow, 2002), and removed all
multi-word items. We now selected at random 10
pairs of words from each subject. For each pair,
we found the largest of our discovered categories
containing it (if there isn?t one, we pick another
pair. This is valid because our Recall is obviously
not even close to 100%, so if we did not pick an-
other pair we would seriously harm the validity of
the evaluation.) The various morphological forms
of the same word were treated as one during the
evaluation.
The only difference from the (Widdows and
Dorow, 2002) experiment is the usage of pairs
rather than single words. We did this in order to
disambiguate our categories. This was not needed
in (Widdows and Dorow, 2002) because they had
directly accessed the word graph, which may be
an advantage in some applications.
The Russian evaluation posed a bit of a prob-
lem because the Russian WordNet is not readily
available and its coverage is rather small. Fortu-
nately, the subject list is such that WordNet words
5(Widdows and Dorow, 2002) also reports results for an
LSA-based clustering algorithm that are vastly inferior to the
pattern-based ones.
302
Dmoz BNC Russian
us k-means random us k-means random us k-means random
avg ?shared meaning? (%) 80.53 18.25 1.43 86.87 8.52 0.00 95.00 18.96 7.33
avg triplet score (1-4) 1.74 3.34 3.88 1.56 3.61 3.94 1.34 3.32 3.76
avg category score (1-10) 9.27 4.00 1.8 9.31 4.50 0.00 8.50 4.66 3.32
Table 1: Results of evaluation by human judgment of three data sets (ours, that obtained by k-means, and
random categories) on the three corpora. See text for detailed explanations.
could be translated unambiguously to Russian and
words in our discovered categories could be trans-
lated unambiguously into English. This was the
methodology taken.
For each found category C containing N words,
we computed the following (see Table 2): (1) Pre-
cision: the number of words present in both C and
WN divided by N ; (2) Precision*: the number of
correct words divided by N . Correct words are ei-
ther words that appear in the WN subtree, or words
whose entry in the American Heritage Dictionary
or the Britannica directly defines them as belong-
ing to the given class (e.g., ?keyboard? is defined
as ?a piano?; ?mitt? is defined by ?a type of glove?.)
This was done in order to overcome the relative
poorness of WordNet; (3) Recall: the number of
words present in both C and WN divided by the
number of (single) words in WN; (4) The num-
ber of correctly discovered words (New) that are
not in WN. The Table also shows the number of
WN words (:WN), in order to get a feeling by how
much WN could be improved here. For each sub-
ject, we show the average over the 10 randomly
selected pairs.
Table 2 also shows the average of each measure
over the subjects, and the two precision measures
when computed on the total set of WN words. The
(uncorrected) precision is the only metric given in
(Widdows and Dorow, 2002), who reported 82%
(for the BNC.) Our method gives 90.47% for this
metric on the same corpus.
5.5 Summary
Our human-evaluated and WordNet-based results
are better than the baseline and previous work re-
spectively. Both are also of good standalone qual-
ity. Clearly, evaluation methodology for lexical
acquisition tasks should be improved, which is an
interesting research direction in itself.
Examining our categories at random, we found
a nice example that shows how difficult it is to
evaluate the task and how useful automatic cate-
gory discovery can be, as opposed to manual def-
inition. Consider the following category, discov-
ered in the Dmoz corpus: {nightcrawlers, chicken,
shrimp, liver, leeches}. We did not know why
these words were grouped together; if asked in an
evaluation, we would give the category a very low
score. However, after some web search, we found
that this is a ?fish bait? category, especially suitable
for catfish.
6 Discussion
We have presented a novel method for pattern-
based discovery of lexical semantic categories.
It is the first pattern-based lexical acquisition
method that is fully unsupervised, requiring no
corpus annotation or manually provided patterns
or words. Pattern candidates are discovered us-
ing meta-patterns of high frequency and content
words, and symmetric patterns are discovered us-
ing simple graph-theoretic measures. Categories
are generated using a novel graph clique-set alo-
rithm. The only other fully unsupervised lexical
category acquisition approach is based on decom-
position of a matrix defined by context feature vec-
tors, and it has not been shown to scale well yet.
Our algorithm was evaluated using both human
judgment and automatic comparisons with Word-
Net, and results were superior to previous work
(although it used a POS tagged corpus) and more
efficient computationally. Our algorithm is also
easy to implement.
Computational efficiency and specifically lack
of annotation are important criteria, because they
allow usage of huge corpora, which are presently
becoming available and growing in size.
There are many directions to pursue in the fu-
ture: (1) support multi-word lexical items; (2) in-
crease category quality by improved merge algo-
rithms; (3) discover various relationships (e.g., hy-
ponymy) between the discovered categories; (4)
discover finer inter-word relationships, such as
verb selection preferences; (5) study various prop-
erties of discovered patterns in a detailed manner;
and (6) adapt the algorithm to morphologically
rich languages.
303
Subject Prec. Prec.* Rec. New:WN
Dmoz
instruments 79.25 89.34 34.54 7.2:163
vehicles 80.17 86.84 18.35 6.3:407
academic 78.78 89.32 30.83 15.5:396
body parts 73.85 79.29 5.95 9.1:1491
foodstuff 83.94 90.51 28.41 26.3:1209
clothes 83.41 89.43 10.65 4.5:539
tools 83.99 89.91 21.69 4.3:219
places 76.96 84.45 25.82 6.3:232
crimes 76.32 86.99 31.86 4.7:102
diseases 81.33 88.99 19.58 6.8:332
set avg 79.80 87.51 22.77 9.1:509
all words 79.32 86.94
BNC
instruments 92.68 95.43 9.51 0.6:163
vehicles 94.16 95.23 3.81 0.2:407
academic 93.45 96.10 12.02 0.6:396
body parts 96.38 97.60 0.97 0.3:1491
foodstuff 93.76 94.36 3.60 0.6:1209
cloths 93.49 94.90 4.04 0.3:539
tools 96.84 97.24 6.67 0.1:219
places 87.88 97.25 6.42 1.5:232
crimes 83.79 91.99 19.61 2.6:102
diseases 95.16 97.14 5.54 0.5:332
set avg 92.76 95.72 7.22 0.73:509
all words 90.47 93.80
Russian
instruments 82.46 89.09 25.28 3.4:163
vehicles 83.16 89.58 16.31 5.1:407
academic 87.27 92.92 15.71 4.9:396
body parts 81.42 89.68 3.94 8.3:1491
foodstuff 80.34 89.23 13.41 24.3:1209
clothes 82.47 87.75 15.94 5.1:539
tools 79.69 86.98 21.14 3.7:219
places 82.25 90.20 33.66 8.5:232
crimes 84.77 93.26 34.22 3.3:102
diseases 80.11 87.70 20.69 7.7:332
set avg 82.39 89.64 20.03 7.43:509
all words 80.67 89.17
Table 2: WordNet evaluation. Note the BNC ?all
words? precision of 90.47%. This metric was re-
ported to be 82% in (Widdows and Dorow, 2002).
It should be noted that our algorithm can be
viewed as one for automatic discovery of word
senses, because it allows a word to participate in
more than a single category. When merged prop-
erly, the different categories containing a word can
be viewed as the set of its senses. We are planning
an evaluation according to this measure after im-
proving the merge stage.
References
Matthew Berland and Eugene Charniak, 1999. Finding
parts in very large corpora. ACL ?99.
Peter Brown, Vincent Della Pietra, Peter deSouza,
Jenifer Lai, Robert Mercer, 1992. Class-based n-
gram models for natural language. Comp. Linguis-
tics, 18(4):468?479.
Sharon Caraballo, 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. ACL
?99.
Timothy Chklovski, Patrick Pantel, 2004. VerbOcean:
mining the web for fine-grained semantic verb rela-
tions. EMNLP ?04.
Philipp Cimiano, Andreas Hotho, Steffen Staab, 2005.
Learning concept hierarchies from text corpora us-
ing formal concept analysis. J. of Artificial Intelli-
gence Research, 24:305?339.
James Curran, Marc Moens, 2002. Improvements in
automatic thesaurus extraction. ACL Workshop on
Unsupervised Lexical Acquisition, 2002.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. Index-
ing by latent semantic analysis. J. of the American
Society for Info. Science, 41(6):391?407.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using curvature and Markov clustering in graphs for
lexical acquisition and word sense discrimination.
MEANING ?05.
Dayne Freitag, 2004. Trained named entity recognition
using distributional clusters. EMNLP ?04.
Evgeniy Gabrilovich, Shaul Markovitch, 2005. Fea-
ture generation for text categorization using world
knowledge. IJCAI ?05.
Marti Hearst, 1992. Automatic acquisition of hy-
ponyms from large text corpora. COLING ?92.
Hang Li, Naoki Abe, 1996. Clustering words with the
MDL principle. COLING ?96.
Dekang Lin, 1998. Automatic retrieval and clustering
of similar words. COLING ?98.
Margaret Matlin, 2005. Cognition, 6th edition. John
Wiley & Sons.
Patrick Pantel, Dekang Lin, 2002. Discovering word
senses from text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards terascale knowledge acquisition.
COLING ?04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional clustering of English words. ACL ?93.
Ellen Riloff, Rosie Jones, 1999. Learning dictionaries
for information extraction by multi-level bootstrap-
ping. AAAI ?99.
Hinrich Schu?tze, 1998. Automatic word sense discrim-
ination. Comp. Linguistics, 24(1):97?123.
Dominic Widdows, Beate Dorow, 2002. A graph model
for unsupervised Lexical acquisition. COLING ?02.
304
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232?239,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Fully Unsupervised Discovery of Concept-Specific Relationships
by Web Mining
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem 91904, Israel
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem 91904, Israel
www.cs.huji.ac.il/?arir
Moshe Koppel
Dept. of Computer Science
Bar-Ilan University
Ramat-Gan 52900, Israel
koppel@cs.biu.ac.il
Abstract
We present a web mining method for discov-
ering and enhancing relationships in which a
specified concept (word class) participates.
We discover a whole range of relationships
focused on the given concept, rather than
generic known relationships as in most pre-
vious work. Our method is based on cluster-
ing patterns that contain concept words and
other words related to them. We evaluate the
method on three different rich concepts and
find that in each case the method generates a
broad variety of relationships with good pre-
cision.
1 Introduction
The huge amount of information available on the
web has led to a flurry of research on methods for
automatic creation of structured information from
large unstructured text corpora. The challenge is to
create as much information as possible while pro-
viding as little input as possible.
A lot of this research is based on the initial insight
(Hearst, 1992) that certain lexical patterns (?X is a
country?) can be exploited to automatically gener-
ate hyponyms of a specified word. Subsequent work
(to be discussed in detail below) extended this initial
idea along two dimensions.
One objective was to require as small a user-
provided initial seed as possible. Thus, it was ob-
served that given one or more such lexical patterns,
a corpus could be used to generate examples of hy-
ponyms that could then, in turn, be exploited to gen-
erate more lexical patterns. The larger and more reli-
able sets of patterns thus generated resulted in larger
and more precise sets of hyponyms and vice versa.
The initial step of the resulting alternating bootstrap
process ? the user-provided input ? could just as well
consist of examples of hyponyms as of lexical pat-
terns.
A second objective was to extend the information
that could be learned from the process beyond hy-
ponyms of a given word. Thus, the approach was
extended to finding lexical patterns that could pro-
duce synonyms and other standard lexical relations.
These relations comprise all those words that stand
in some known binary relation with a specified word.
In this paper, we introduce a novel extension of
this problem: given a particular concept (initially
represented by two seed words), discover relations
in which it participates, without specifying their
types in advance. We will generate a concept class
and a variety of natural binary relations involving
that class.
An advantage of our method is that it is particu-
larly suitable for web mining, even given the restric-
tions on query amounts that exist in some of today?s
leading search engines.
The outline of the paper is as follows. In the next
section we will define more precisely the problem
we intend to solve. In section 3, we will consider re-
lated work. In section 4 we will provide an overview
of our solution and in section 5 we will consider the
details of the method. In section 6 we will illustrate
and evaluate the results obtained by our method. Fi-
nally, in section 7 we will offer some conclusions
and considerations for further work.
232
2 Problem Definition
In several studies (e.g., Widdows and Dorow, 2002;
Pantel et al 2004; Davidov and Rappoport, 2006)
it has been shown that relatively unsupervised and
language-independent methods could be used to
generate many thousands of sets of words whose
semantics is similar in some sense. Although ex-
amination of any such set invariably makes it clear
why these words have been grouped together into
a single concept, it is important to emphasize that
the method itself provides no explicit concept defi-
nition; in some sense, the implied class is in the eye
of the beholder. Nevertheless, both human judgment
and comparison with standard lists indicate that the
generated sets correspond to concepts with high pre-
cision.
We wish now to build on that result in the fol-
lowing way. Given a large corpus (such as the web)
and two or more examples of some concept X , au-
tomatically generate examples of one or more rela-
tions R ? X ? Y , where Y is some concept and R
is some binary relationship between elements of X
and elements of Y .
We can think of the relations we wish to gener-
ate as bipartite graphs. Unlike most earlier work,
the bipartite graphs we wish to generate might be
one-to-one (for example, countries and their capi-
tals), many-to-one (for example, countries and the
regions they are in) or many-to-many (for example,
countries and the products they manufacture). For a
given class X , we would like to generate not one but
possibly many different such relations.
The only input we require, aside from a corpus,
is a small set of examples of some class. However,
since such sets can be generated in entirely unsuper-
vised fashion, our challenge is effectively to gener-
ate relations directly from a corpus given no addi-
tional information of any kind. The key point is that
we do not in any manner specify in advance what
types of relations we wish to find.
3 Related Work
As far as we know, no previous work has directly
addressed the discovery of generic binary relations
in an unrestricted domain without (at least implic-
itly) pre-specifying relationship types. Most related
work deals with discovery of hypernymy (Hearst,
1992; Pantel et al 2004), synonymy (Roark and
Charniak, 1998; Widdows and Dorow, 2002; Davi-
dov and Rappoport, 2006) and meronymy (Berland
and Charniak, 1999).
In addition to these basic types, several stud-
ies deal with the discovery and labeling of more
specific relation sub-types, including inter-verb re-
lations (Chklovski and Pantel, 2004) and noun-
compound relationships (Moldovan et al 2004).
Studying relationships between tagged named en-
tities, (Hasegawa et al 2004; Hassan et al 2006)
proposed unsupervised clustering methods that as-
sign given (or semi-automatically extracted) sets of
pairs into several clusters, where each cluster corre-
sponds to one of a known relationship type. These
studies, however, focused on the classification of
pairs that were either given or extracted using some
supervision, rather than on discovery and definition
of which relationships are actually in the corpus.
Several papers report on methods for using the
web to discover instances of binary relations. How-
ever, each of these assumes that the relations them-
selves are known in advance (implicitly or explic-
itly) so that the method can be provided with seed
patterns (Agichtein and Gravano, 2000; Pantel et al
2004), pattern-based rules (Etzioni et al 2004), rela-
tion keywords (Sekine, 2006), or word pairs exem-
plifying relation instances (Pasca et al 2006; Alfon-
seca et al 2006; Rosenfeld and Feldman, 2006).
In some recent work (Strube and Ponzetto, 2006),
it has been shown that related pairs can be gener-
ated without pre-specifying the nature of the rela-
tion sought. However, this work does not focus on
differentiating among different relations, so that the
generated relations might conflate a number of dis-
tinct ones.
It should be noted that some of these papers utilize
language and domain-dependent preprocessing in-
cluding syntactic parsing (Suchanek et al 2006) and
named entity tagging (Hasegawa et al 2004), while
others take advantage of handcrafted databases such
as WordNet (Moldovan et al 2004; Costello et al
2006) and Wikipedia (Strube and Ponzetto, 2006).
Finally, (Turney, 2006) provided a pattern dis-
tance measure which allows a fully unsupervised
measurement of relational similarity between two
pairs of words; however, relationship types were not
discovered explicitly.
233
4 Outline of the Method
We will use two concept words contained in a con-
cept class C to generate a collection of distinct re-
lations in which C participates. In this section we
offer a brief overview of our method.
Step 1: Use a seed consisting of two (or more) ex-
ample words to automatically obtain other examples
that belong to the same class. Call these concept
words. (For instance, if our example words were
France and Angola, we would generate more coun-
try names.)
Step 2: For each concept word, collect instances
of contexts in which the word appears together with
one other content word. Call this other word a tar-
get word for that concept word. (For example, for
France we might find ?Paris is the capital of France?.
Paris would be a target word for France.)
Step 3: For each concept word, group the contexts
in which it appears according to the target word that
appears in the context. (Thus ?X is the capital of Y ?
would likely be grouped with ?Y ?s capital is X?.)
Step 4: Identify similar context groups that ap-
pear across many different concept words. Merge
these into a single concept-word-independent clus-
ter. (The group including the two contexts above
would appear, with some variation, for other coun-
tries as well, and all these would be merged into
a single cluster representing the relation capital-
of(X,Y).)
Step 5: For each cluster, output the relation con-
sisting of all <concept word, target word> pairs that
appear together in a context included in the cluster.
(The cluster considered above would result in a set
of pairs consisting of a country and its capital. Other
clusters generated by the same seed might include
countries and their languages, countries and the re-
gions in which they are located, and so forth.)
5 Details of the Method
In this section we consider the details of each of
the above-enumerated steps. It should be noted
that each step can be performed using standard web
searches; no special pre-processed corpus is re-
quired.
5.1 Generalizing the seed
The first step is to take the seed, which might con-
sist of as few as two concept words, and generate
many (ideally, all, when the concept is a closed set
of words) members of the class to which they be-
long. We do this as follows, essentially implement-
ing a simplified version of the method of Davidov
and Rappoport (2006). For any pair of seed words
Si and Sj , search the corpus for word patterns of the
form SiHSj , where H is a high-frequency word in
the corpus (we used the 100 most frequent words
in the corpus). Of these, we keep all those pat-
terns, which we call symmetric patterns, for which
SjHSi is also found in the corpus. Repeat this pro-
cess to find symmetric patterns with any of the struc-
tures HSHS, SHSH or SHHS. It was shown in
(Davidov and Rappoport, 2006) that pairs of words
that often appear together in such symmetric pat-
terns tend to belong to the same class (that is, they
share some notable aspect of their semantics). Other
words in the class can thus be generated by search-
ing a sub-corpus of documents including at least two
concept words for those words X that appear in a
sufficient number of instances of both the patterns
SiHX and XHSi, where Si is a word in the class.
The same can be done for the other three pattern
structures. The process can be bootstrapped as more
words are added to the class.
Note that our method differs from that of Davidov
and Rappoport (2006) in that here we provide an ini-
tial seed pair, representing our target concept, while
there the goal is grouping of as many words as pos-
sible into concept classes. The focus of our paper is
on relations involving a specific concept.
5.2 Collecting contexts
For each concept word S, we search the corpus for
distinct contexts in which S appears. (For our pur-
poses, a context is a window with exactly five words
or punctuation marks before or after the concept
word; we choose 10,000 of these, if available.) We
call the aggregate text found in all these context win-
dows the S-corpus.
From among these contexts, we choose all pat-
terns of the form H1SH2XH3 or H1XH2SH3,
where:
234
? X is a word that appears with frequency below
f1 in the S-corpus and that has sufficiently high
pointwise mutual information with S. We use
these two criteria to ensure that X is a content
word and that it is related to S. The lower the
threshold f1, the less noise we allow in, though
possibly at the expense of recall. We used f1 =
1, 000 occurrences per million words.
? H2 is a string of words each of which occurs
with frequency above f2 in the S-corpus. We
want H2 to consist mainly of words common
in the context of S in order to restrict patterns
to those that are somewhat generic. Thus, in
the context of countries we would like to retain
words like capital while eliminating more spe-
cific words that are unlikely to express generic
patterns. We used f2 = 100 occurrences per
million words (there is room here for automatic
optimization, of course).
? H1 and H3 are either punctuation or words that
occur with frequency above f3 in the S-corpus.
This is mainly to ensure that X and S aren?t
fragments of multi-word expressions. We used
f3 = 100 occurrences per million words.
? We call these patterns, S-patterns and we call
X the target of the S-pattern. The idea is that S
and X very likely stand in some fixed relation
to each other where that relation is captured by
the S-pattern.
5.3 Grouping S-patterns
If S is in fact related to X in some way, there might
be a number of S-patterns that capture this relation-
ship. For each X , we group all the S-patterns that
have X as a target. (Note that two S-patterns with
two different targets might be otherwise identical,
so that essentially the same pattern might appear in
two different groups.) We now merge groups with
large (more than 2/3) overlap. We call the resulting
groups, S-groups.
5.4 Identifying pattern clusters
If the S-patterns in a given S-group actually capture
some relationship between S and the target, then
one would expect that similar groups would appear
for a multiplicity of concept words S. Suppose that
we have S-groups for three different concept words
S such that the pairwise overlap among the three
groups is more than 2/3 (where for this purpose two
patterns are deemed identical if they differ only at S
and X). Then the set of patterns that appear in two or
three of these S-groups is called a cluster core. We
now group all patterns in other S-groups that have an
overlap of more than 2/3 with the cluster core into a
candidate pattern pool P . The set of all patterns in
P that appear in at least two S-groups (among those
that formed P ) pattern cluster. A pattern cluster that
has patterns instantiated by at least half of the con-
cept words is said to represent a relation.
5.5 Refining relations
A relation consists of pairs (S, X) where S is a con-
cept word and X is the target of some S-pattern in a
given pattern cluster. Note that for a given S, there
might be one or many values of X satisfying the re-
lation. As a final refinement, for each given S, we
rank all such X according to pointwise mutual in-
formation with S and retain only the highest 2/3. If
most values of S have only a single corresponding X
satisfying the relation and the rest have none, we try
to automatically fill in the missing values by search-
ing the corpus for relevant S-patterns for the missing
values of S. (In our case the corpus is the web, so
we perform additional clarifying queries.)
Finally, we delete all relations in which all con-
cept words are related to most target words and all
relations in which the concept words and the target
words are identical. Such relations can certainly be
of interest (see Section 7), but are not our focus in
this paper.
5.6 Notes on required Web resources
In our implementation we use the Google search
engine. Google restricts individual users to 1,000
queries per day and 1,000 pages per query. In each
stage we conducted queries iteratively, each time
downloading all 1,000 documents for the query.
In the first stage our goal was to discover sym-
metric relationships from the web and consequently
discover additional concept words. For queries in
this stage of our algorithm we invoked two require-
ments.
First, the query should contain at least two con-
cept words. This proved very effective in reduc-
235
ing ambiguity. Thus of 1,000 documents for the
query bass, 760 deal with music, while if we add to
the query a second word from the intended concept
(e.g., barracuda), then none of the 1,000 documents
deal with music and the vast majority deal with fish,
as intended.
Second, we avoid doing overlapping queries. To
do this we used Google?s ability to exclude from
search results those pages containing a given term
(in our case, one of the concept words).
We performed up to 300 different queries for in-
dividual concepts in the first stage of our algorithm.
In the second stage, we used web queries to as-
semble S-corpora. On average, about 1/3 of the con-
cept words initially lacked sufficient data and we
performed up to twenty additional queries for each
rare concept word to fill its corpus.
In the last stage, when clusters are constructed,
we used web queries for filling missing pairs of one-
to-one or several-to-several relationships. The to-
tal number of filling queries for a specific concept
was below 1,000, and we needed only the first re-
sults of these queries. Empirically, it took between
0.5 to 6 day limits (i.e., 500?6,000 queries) to ex-
tract relationships for a concept, depending on its
size (the number of documents used for each query
was at most 100). Obviously this strategy can be
improved by focused crawling from primary Google
hits, which can drastically reduce the required num-
ber of queries.
6 Evaluation
In this section we wish to consider the variety of re-
lations that can be generated by our method from a
given seed and to measure the quality of these rela-
tions in terms of their precision and recall.
With regard to precision, two claims are being
made. One is that the generated relations correspond
to identifiable relations. The other claim is that to
the extent that a generated relation can be reason-
ably identified, the generated pairs do indeed belong
to the identified relation. (There is a small degree of
circularity in this characterization but this is proba-
bly the best we can hope for.)
As a practical matter, it is extremely difficult to
measure precision and recall for relations that have
not been pre-determined in any way. For each gen-
erated relation, authoritative resources must be mar-
shaled as a gold standard. For purposes of evalu-
ation, we ran our algorithm on three representative
domains ? countries, fish species and star constel-
lations ? and tracked down gold standard resources
(encyclopedias, academic texts, informative web-
sites, etc) for the bulk of the relations generated in
each domain.
This choice of domains allowed us to explore
different aspects of algorithmic behavior. Country
and constellation domains are both well defined and
closed domains. However they are substantially dif-
ferent.
Country names is a relatively large domain which
has very low lexical ambiguity, and a large number
of potentially useful relations. The main challenge
in this domain was to capture it well.
Constellation names, in contrast, are a relatively
small but highly ambiguous domain. They are used
in proper names, mythology, names of entertainment
facilities etc. Our evaluation examined how well the
algorithm can deal with such ambiguity.
The fish domain contains a very high number of
members. Unlike countries, it is a semi-open non-
homogenous domain with a very large number of
subclasses and groups. Also, unlike countries, it
does not contain many proper nouns, which are em-
pirically generally easier to identify in patterns. So
the main challenge in this domain is to extract un-
blurred relationships and not to diverge from the do-
main during the concept acquisition phase.
We do not show here all-to-all relationships such
as fish parts (common to all or almost all fish), be-
cause we focus on relationships that separate be-
tween members of the concept class, which are
harder to acquire and evaluate.
6.1 Countries
Our seed consisted of two country names. The in-
tended result for the first stage of the algorithm
was a list of countries. There are 193 countries in
the world (www.countrywatch.com) some of which
have multiple names so that the total number of
commonly used country names is 243. Of these,
223 names (comprising 180 countries) are charac-
ter strings with no white space. Since we consider
only single word names, these 223 are the names we
hope to capture in this stage.
236
Using the seed words France and Angola, we
obtained 202 country names (comprising 167 dis-
tinct countries) as well as 32 other names (consisting
mostly of names of other geopolitical entities). Us-
ing the list of 223 single word countries as our gold
standard, this gives precision of 0.90 and recall of
0.86. (Ten other seed pairs gave results ranging in
precision: 0.86-0.93 and recall: 0.79-0.90.)
The second part of the algorithm generated a set
of 31 binary relations. Of these, 25 were clearly
identifiable relations many of which are shown in
Table 1. Note that for three of these there are stan-
dard exhaustive lists against which we could mea-
sure both precision and recall; for the others shown,
sources were available for measuring precision but
no exhaustive list was available from which to mea-
sure recall, so we measured coverage (the number
of countries for which at least one target concept is
found as related).
Another eleven meaningful relations were gener-
ated for which we did not compute precision num-
bers. These include celebrity-from, animal-of, lake-
in, borders-on and enemy-of. (The set of relations
generated by other seed pairs differed only slightly
from those shown here for France and Angola.)
6.2 Fish species
In our second experiment, our seed consisted of two
fish species, barracuda and bluefish. There are 770
species listed in WordNet of which 447 names are
character strings with no white space. The first stage
of the algorithm returned 305 of the species listed
in Wordnet, another 37 species not listed in Word-
net, as well as 48 other names (consisting mostly
of other sea creatures). The second part of the al-
gorithm generated a set of 15 binary relations all of
which are meaningful. Those for which we could
find some gold standard are listed in Table 2.
Other relations generated include served-with,
bait-for, food-type, spot-type, and gill-type.
6.3 Constellations
Our seed consisted of two constellation names,
Orion and Cassiopeia. There are 88 standard
constellations (www.astro.wisc.edu) some of which
have multiple names so that the total number of com-
monly used constellations is 98. Of these, 87 names
(77 constellations) are strings with no white space.
Relationship Prec. Rec/Cov
Sample pattern
(Sample pair)
capital-of 0.92 R=0.79
in (x), capital of (y),
(Luanda, Angola)
language-spoken-in 0.92 R=0.60
to (x) or other (y) speaking
(Spain, Spanish)
in-region 0.73 R=0.71
throughout (x), from (y) to
(America, Canada)
city-in 0.82 C=0.95
west (x) ? forecast for (y).
(England, London)
river-in 0.92 C=0.68
central (x), on the (y) river
(China, Haine)
mountain-range-in 0.77 C=0.69
the (x) mountains in (y) ,
(Chella, Angola)
sub-region-of 0.81 C=0.81
the (y) region of (x),
(Veneto, Italy)
industry-of 0.70 C=0.90
the (x) industry in (y) ,
(Oil, Russia)
island-in 0.98 C=0.55
, (x) island , (y) ,
(Bathurst, Canada)
president-of 0.86 C=0.51
president (x) of (y) has
(Bush, USA)
political-position-in 0.81 C=0.75
former (x) of (y) face
(President, Ecuador)
political-party-of 0.91 C=0.53
the (x) party of (y) ,
(Labour, England)
festival-of 0.90 C=0.78
the (x) festival, (y) ,
(Tanabata, Japan)
religious-denomination-of 0.80 C=0.62
the (x) church in (y) ,
(Christian, Rome)
Table 1: Results on seed { France, Angola }.
237
Relationship Prec. Cov
Sample pattern
(Sample pair)
region-found-in 0.83 0.80
best (x) fishing in (y) .
(Walleye, Canada)
sea-found-in 0.82 0.64
of (x) catches in the (y) sea
(Shark, Adriatic)
lake-found-in 0.79 0.51
lake (y) is famous for (x) ,
(Marion, Catfish)
habitat-of 0.78 0.92
, (x) and other (y) fish
(Menhaden, Saltwater)
also-called 0.91 0.58
. (y) , also called (x) ,
(Lemonfish, Ling)
eats 0.90 0.85
the (x) eats the (y) and
(Perch, Minnow)
color-of 0.95 0.85
the (x) was (y) color
(Shark, Gray)
used-for-food 0.80 0.53
catch (x) ? best for (y) or
(Bluefish, Sashimi)
in-family 0.95 0.60
the (x) family , includes (y) ,
(Salmonid, Trout)
Table 2: Results on seed { barracud, bluefish }.
The first stage of the algorithm returned 81 constel-
lation names (77 distinct constellations) as well as
38 other names (consisting mostly of names of indi-
vidual stars). Using the list of 87 single word con-
stellation names as our gold standard, this gives pre-
cision of 0.68 and recall of 0.93.
The second part of the algorithm generated a set
of ten binary relations. Of these, one concerned
travel and entertainment (constellations are quite
popular as names of hotels and lounges) and another
three were not interesting. Apparently, the require-
ment that half the constellations appear in a relation
limited the number of viable relations since many
constellations are quite obscure. The six interesting
relations are shown in Table 3 along with precision
and coverage.
7 Discussion
In this paper we have addressed a novel type of prob-
lem: given a specific concept, discover in fully un-
supervised fashion, a range of relations in which it
participates. This can be extremely useful for study-
ing and researching a particular concept or field of
study.
As others have shown as well, two concept words
can be sufficient to generate almost the entire class
to which the words belong when the class is well-
defined. With the method presented in this paper,
using no further user-provided information, we can,
for a given concept, automatically generate a diverse
collection of binary relations on this concept. These
relations need not be pre-specified in any way. Re-
sults on the three domains we considered indicate
that, taken as an aggregate, the relations that are gen-
erated for a given domain paint a rather clear picture
of the range of information pertinent to that domain.
Moreover, all this was done using standard search
engine methods on the web. No language-dependent
tools were used (not even stemming); in fact, we re-
produced many of our results using Google in Rus-
sian.
The method depends on a number of numerical
parameters that control the subtle tradeoff between
quantity and quality of generated relations. There is
certainly much room for tuning of these parameters.
The concept and target words used in this paper
are single words. Extending this to multiple-word
expressions would substantially contribute to the ap-
plicability of our results.
In this research we effectively disregard many re-
lationships of an all-to-all nature. However, such
relationships can often be very useful for ontology
construction, since in many cases they introduce
strong connections between two different concepts.
Thus, for fish we discovered that one of the all-to-
all relationships captures a precise set of fish body
parts, and another captures swimming verbs. Such
relations introduce strong and distinct connections
between the concept of fish and the concepts of fish-
body-parts and swimming. Such connections may
be extremely useful for ontology construction.
238
Relationship Prec. Cov
Sample pattern
(Sample pair)
nearby-constellation 0.87 0.70
constellation (x), near (y),
(Auriga, Taurus)
star-in 0.82 0.76
star (x) in (y) is
(Antares , Scorpius)
shape-of 0.90 0.55
, (x) is depicted as (y).
(Lacerta, Lizard)
abbreviated-as 0.93 0.90
. (x) abbr (y),
(Hidra, Hya)
cluster-types-in 0.92 1.00
famous (x) cluster in (y),
(Praesepe, Cancer)
location 0.82 0.70
, (x) is a (y) constellation
(Draco, Circumpolar)
Table 3: Results on seed { Orion, Cassiopeia }.
References
Agichtein, E., Gravano, L., 2000. Snowball: Extracting
relations from large plain-text collections. Proceedings
of the 5th ACM International Conference on Digital
Libraries.
Alfonseca, E., Ruiz-Casado, M., Okumura, M., Castells,
P., 2006. Towards large-scale non-taxonomic relation
extraction: estimating the precision of rote extractors.
Workshop on Ontology Learning and Population at
COLING-ACL ?06.
Berland, M., Charniak, E., 1999. Finding parts in very
large corpora. ACL ?99.
Chklovski T., Pantel P., 2004. VerbOcean: mining the
web for fine-grained semantic verb relations. EMNLP
?04.
Costello, F., Veale, T., Dunne, S., 2006. Using Word-
Net to automatically deduce relations between words
in noun-noun compounds, COLING-ACL ?06.
Davidov, D., Rappoport, A., 2006. Efficient unsupervised
discovery of word categories using symmetric patterns
and high frequency words. COLING-ACL ?06.
Etzioni, O., Cafarella, M., Downey, D., Popescu, A.,
Shaked, T., Soderland, S., Weld, D., Yates, A., 2004.
Methods for domain-independent information extrac-
tion from the web: an experimental comparison. AAAI
?04.
Hasegawa, T., Sekine, S., Grishman, R., 2004. Discover-
ing relations among named entities from large corpora.
ACL ?04.
Hassan, H., Hassan, A., Emam, O., 2006. unsupervised
information extraction approach using graph mutual
reinforcement. EMNLP ?06.
Hearst, M., 1992. Automatic acquisition of hyponyms
from large text corpora. COLING ?92.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D.,
Girju, R., 2004. Models for the semantic classifica-
tion of noun phrases. Workshop on Comput. Lexical
Semantics at HLT-NAACL ?04.
Pantel, P., Ravichandran, D., Hovy, E., 2004. Towards
terascale knowledge acquisition. COLING ?04.
Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A., 2006.
Names and similarities on the web: fact extraction in
the fast lane. COLING-ACL ?06.
Roark, B., Charniak, E., 1998. Noun-phrase co-
occurrence statistics for semi-automatic semantic lex-
icon construction. ACL ?98.
Rosenfeld B., Feldman, R.: URES : an unsupervised
web relation extraction system. Proceedings, ACL ?06
Poster Sessions.
Sekine, S., 2006 On-demand information extraction.
COLING-ACL ?06.
Strube, M., Ponzetto, S., 2006. WikiRelate! computing
semantic relatedness using Wikipedia. AAAI ?06.
Suchanek F. M., G. Ifrim, G. Weikum. 2006. LEILA:
learning to extract information by linguistic analysis.
Workshop on Ontology Learning and Population at
COLING-ACL ?06.
Turney, P., 2006. Expressing implicit semantic relations
without supervision. COLING-ACL ?06.
Widdows, D., Dorow, B., 2002. A graph model for unsu-
pervised Lexical acquisition. COLING ?02.
239
Proceedings of ACL-08: HLT, pages 227?235,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Classification of Semantic Relationships between Nominals
Using Pattern Clusters
Dmitry Davidov
ICNC
Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
There are many possible different semantic re-
lationships between nominals. Classification
of such relationships is an important and dif-
ficult task (for example, the well known noun
compound classification task is a special case
of this problem). We propose a novel pat-
tern clusters method for nominal relationship
(NR) classification. Pattern clusters are dis-
covered in a large corpus independently of
any particular training set, in an unsupervised
manner. Each of the extracted clusters cor-
responds to some unspecified semantic rela-
tionship. The pattern clusters are then used
to construct features for training and classifi-
cation of specific inter-nominal relationships.
Our NR classification evaluation strictly fol-
lows the ACL SemEval-07 Task 4 datasets and
protocol, obtaining an f-score of 70.6, as op-
posed to 64.8 of the best previous work that
did not use the manually provided WordNet
sense disambiguation tags.
1 Introduction
Automatic extraction and classification of seman-
tic relationships is a major field of activity, of both
practical and theoretical interest. A prominent type
of semantic relationships is that holding between
nominals1. For example, in noun compounds many
different semantic relationships are encoded by the
same simple form (Girju et al, 2005): ?dog food? de-
notes food consumed by dogs, while ?summer morn-
1Our use of the term ?nominal? follows (Girju et al, 2007),
and includes simple nouns, noun compounds and multiword ex-
pressions serving as nouns.
ing? denotes a morning that happens in the summer.
These two relationships are completely different se-
mantically but are similar syntactically, and distin-
guishing between them could be essential for NLP
applications such as question answering and ma-
chine translation.
Relation classification usually relies on a train-
ing set in the form of tagged data. To improve re-
sults, some systems utilize additional manually con-
structed semantic resources such as WordNet (WN)
(Beamer et al, 2007). However, in many domains
and languages such resources are not available. Fur-
thermore, usage of such resources frequently re-
quires disambiguation and connection of the data to
the resource (word sense disambiguation in the case
of WordNet). Manual disambiguation is unfeasible
in many practical tasks, and an automatic one may
introduce errors and greatly degrade performance. It
thus makes sense to try to minimize the usage of
such resources, and utilize only corpus contexts in
which the relevant words appear.
A leading method for utilizing context informa-
tion for classification and extraction of relationships
is that of patterns (Hearst, 1992; Pantel and Pen-
nacchiotti, 2006). The standard classification pro-
cess is to find in an auxiliary corpus a set of patterns
in which a given training word pair co-appears, and
use pattern-word pair co-appearance statistics as fea-
tures for machine learning algorithms.
In this paper we introduce a novel approach,
based on utilizing pattern clusters that are prepared
separately and independently of the training set. We
do not utilize any manually constructed resource or
any manual tagging of training data beyond the cor-
227
rect classification, thus making our method applica-
ble to fully automated tasks and less domain and lan-
guage dependent. Moreover, our pattern clustering
algorithm is fully unsupervised.
Our method is based on the observation that while
each lexical pattern can be highly ambiguous, sev-
eral patterns in conjunction can reliably define and
represent a lexical relationship. Accordingly, we
construct pattern clusters from a large generic cor-
pus, each such cluster potentially representing some
important generic relationship. This step is done
without accessing any training data, anticipating that
most meaningful relationships, including those in a
given classification problem, will be represented by
some of the discovered clusters. We then use the
training set to label some of the clusters, and the la-
beled clusters to assign classes to tested items. One
of the advantages of our method is that it can be used
not only for classification, but also for further anal-
ysis and retrieval of the observed relationships2.
The semantic relationships between the compo-
nents of noun compounds and between nominals in
general are not easy to categorize rigorously. Sev-
eral different relationship hierarchies have been pro-
posed (Nastase and Szpakowicz, 2003; Moldovan et
al., 2004). Some classes, like Container-Contained,
Time-Event and Product-Producer, appear in sev-
eral classification schemes, while classes like Tool-
Object are more vaguely defined and are subdivided
differently. Recently, SemEval-07 Task 4 (Girju et
al., 2007) proposed a benchmark dataset that in-
cludes a subset of 7 widely accepted nominal rela-
tionship (NR) classes, allowing consistent evalua-
tion of different NR classification algorithms. In the
SemEval event, 14 research teams evaluated their al-
gorithms using this benchmark. Some of the teams
have used the manually annotated WN labels pro-
vided with the dataset, and some have not.
We evaluated our algorithm on SemEval-07 Task
4 data, showing superior results over participating
algorithms that did not utilize WordNet disambigua-
tion tags. We also show how pattern clusters can be
used for a completely unsupervised classification of
2In (Davidov and Rappoport, 2008) we focus on the pat-
tern cluster resource type itself, presenting an evaluation of its
intrinsic quality based on SAT tests. In the present paper we
focus on showing how the resource can be used to improve a
known NLP task.
the test set. Since in this case no training data is
used, this allows the automated discovery of a po-
tentially unbiased classification scheme.
Section 2 discusses related work, Section 3 out-
lines the pattern clustering algorithm, Section 4 de-
tails three classification methods, and Sections 5 and
6 describe the evaluation protocol and results.
2 Related Work
Numerous methods have been devised for classifica-
tion of semantic relationships, among which those
holding between nominals constitute a prominent
category. Major differences between these methods
include available resources, degree of preprocessing,
features used, classification algorithm and the nature
of training/test data.
2.1 Available Resources
Many relation classification algorithms utilize
WordNet. Among the 15 systems presented by
the 14 SemEval teams, some utilized the manually
provided WordNet tags for the dataset pairs (e.g.,
(Beamer et al, 2007)). In all cases, usage of WN
tags improves the results significantly. Some other
systems that avoided using the labels used WN as
a supporting resource for their algorithms (Costello,
2007; Nakov and Hearst, 2007; Kim and Baldwin,
2007). Only three avoided WN altogether (Hen-
drickx et al, 2007; Bedmar et al, 2007; Aramaki
et al, 2006).
Other resources used for relationship discovery
include Wikipedia (Strube and Ponzetto, 2006), the-
sauri or synonym sets (Turney, 2005) and domain-
specific semantic hierarchies like MeSH (Rosario
and Hearst, 2001).
While usage of these resources is beneficial in
many cases, high quality word sense annotation is
not easily available. Besides, lexical resources are
not available for many languages, and their coverage
is limited even for English when applied to some re-
stricted domains. In this paper we do not use any
manually annotated resources apart from the classi-
fication training set.
2.2 Degree of Preprocessing
Many relationship classification methods utilize
some language-dependent preprocessing, like deep
or shallow parsing, part of speech tagging and
228
named entity annotation (Pantel et al, 2004). While
the obtained features were shown to improve classi-
fication performance, they tend to be language de-
pendent and error-prone when working on unusual
text domains and are also highly computationally in-
tensive when processing large corpora. To make our
approach as language independent and efficient as
possible, we avoided using any such preprocessing
techniques.
2.3 Classification Features
A wide variety of features are used by different
algorithms, ranging from simple bag-of-words fre-
quencies to WordNet-based features (Moldovan et
al., 2004). Several studies utilize syntactic features.
Many other works manually develop a set of heuris-
tic features devised with some specific relationship
in mind, like a WordNet-based meronymy feature
(Bedmar et al, 2007) or size-of feature (Aramaki
et al, 2006). However, the most prominent feature
type is based on lexico-syntactic patterns in which
the related words co-appear.
Since (Hearst, 1992), numerous works have used
patterns for discovery and identification of instances
of semantic relationships (e.g., (Girju et al, 2006;
Snow et al, 2006; Banko et al 2007)). Rosenfeld
and Feldman (2007) discover relationship instances
by clustering entities appearing in similar contexts.
Strategies were developed for discovery of multi-
ple patterns for some specified lexical relationship
(Pantel and Pennacchiotti, 2006) and for unsuper-
vised pattern ranking (Turney, 2006). Davidov et
al. (2007) use pattern clusters to define general rela-
tionships, but these are specific to a given concept.
No study so far has proposed a method to define, dis-
cover and represent general relationships present in
an arbitrary corpus.
In (Davidov and Rappoport, 2008) we present
an approach to extract pattern clusters from an un-
tagged corpus. Each such cluster represents some
unspecified lexical relationship. In this paper, we
use these pattern clusters as the (only) source of ma-
chine learning features for a nominal relationship
classification problem. Unlike the majority of cur-
rent studies, we avoid using any other features that
require some language-specific information or are
devised for specific relationship types.
2.4 Classification Algorithm
Various learning algorithms have been used for re-
lation classification. Common choices include vari-
ations of SVM (Girju et al, 2004; Nastase et al,
2006), decision trees and memory-based learners.
Freely available tools like Weka (Witten and Frank,
1999) allow easy experimentation with common
learning algorithms (Hendrickx et al, 2007). In this
paper we did not focus on a single ML algorithm,
letting algorithm selection be automatically based
on cross-validation results on the training set, as in
(Hendrickx et al, 2007) but using more algorithms
and allowing a more flexible parameter choice.
2.5 Training Data
As stated above, several categorization schemes for
nominals have been proposed. Nastase and Sz-
pakowicz (2003) proposed a two-level hierarchy
with 5 (30) classes at the top (bottom) levels3. This
hierarchy and a corresponding dataset were used in
(Turney, 2005; Turney, 2006) and (Nastase et al,
2006) for evaluation of their algorithms. Moldovan
et al (2004) proposed a different scheme with 35
classes. The most recent dataset has been developed
for SemEval 07 Task 4 (Girju et al, 2007). This
manually annotated dataset includes a representative
rather than exhaustive list of 7 important nominal
relationships. We have used this dataset, strictly fol-
lowing the evaluation protocol. This made it possi-
ble to meaningfully compare our method to state-of-
the-art methods for relation classification.
3 Pattern Clustering Algorithm
Our pattern clustering algorithm is designed for the
unsupervised definition and discovery of generic se-
mantic relationships. The algorithm first discovers
and clusters patterns in which a single (?hook?) word
participates, and then merges the resulting clusters
to form the final structure. In (Davidov and Rap-
poport, 2008) we describe the algorithm at length,
discuss its behavior and parameters in detail, and
evaluate its intrinsic quality. To assist readers of
the present paper, in this section we provide an
overview. Examples of some resulting pattern clus-
ters are given in Section 6. We refer to a pattern
3Actually, there were 50 relationships at the bottom level,
but valid nominal instances were found only for 30.
229
contained in our clusters (a pattern type) as a ?pat-
tern? and to an occurrence of a pattern in the corpus
(a pattern token) as a ?pattern instance?.
The algorithm does not rely on any data from the
classification training set, hence we do not need to
repeat its execution for different classification prob-
lems. To calibrate its parameters, we ran it a few
times with varied parameters settings, producing
several different configurations of pattern clusters
with different degrees of noise, coverage and granu-
larity. We then chose the best configuration for our
task automatically without re-running pattern clus-
tering for each specific problem (see Section 5.3).
3.1 Hook Words and Hook Corpora
As a first step, we randomly sample a set of hook
words, which will be used in order to discover re-
lationships that generally occur in the corpus. To
avoid selection of ambiguous words or typos, we do
not select words with frequency higher than a pa-
rameter FC and lower than a threshold FB . We also
limit the total number N of hook words. For each
hook word, we now create a hook corpus, the set of
the contexts in which the word appears. Each con-
text is a window containing W words or punctuation
characters before and after the hook word.
3.2 Pattern Specification
To specify patterns, following (Davidov and Rap-
poport, 2006) we classify words into high-
frequency words (HFWs) and content words (CWs).
A word whose frequency is more (less) than FH
(FC) is considered to be a HFW (CW). Our patterns
have the general form
[Prefix] CW1 [Infix] CW2 [Postfix]
where Prefix, Infix and Postfix contain only HFWs.
We require Prefix and Postfix to be a single HFW,
while Infix can contain any number of HFWs (limit-
ing pattern length by window size). This form may
include patterns like ?such X as Y and?. At this stage,
the pattern slots can contain only single words; how-
ever, when using the final pattern clusters for nomi-
nal relationship classification, slots can contain mul-
tiword nominals.
3.3 Discovery of Target Words
For each of the hook corpora, we now extract all
pattern instances where one CW slot contains the
hook word and the other CW slot contains some
other (?target?) word. To avoid the selection of com-
mon words as target words, and to avoid targets ap-
pearing in pattern instances that are relatively fixed
multiword expressions, we sort all target words in
a given hook corpus by pointwise mutual informa-
tion between hook and target, and drop patterns ob-
tained from pattern instances containing the lowest
and highest L percent of target words.
3.4 Pattern Clustering
We now have for each hook corpus a set of patterns,
together with the target words used for their extrac-
tion, and we want to cluster pattern types. First,
we group in clusters all patterns extracted using the
same target word. Second, we merge clusters that
share more than S percent of their patterns. Some
patterns can appear in more than a single cluster.
Finally, we merge pattern clusters from different
hook corpora, to avoid clusters specific to a single
hook word. During merging, we define and utilize
core patterns and unconfirmed patterns, which are
weighed differently during cluster labeling (see Sec-
tion 4.2). We merge clusters from different hook
corpora using the following algorithm:
1. Remove all patterns originating from a single hook
corpus only.
2. Mark all patterns of all present clusters as uncon-
firmed.
3. While there exists some cluster C1 from corpus DX
containing only unconfirmed patterns:
(a) Select a cluster with a minimal number of pat-
terns.
(b) For each corpus D different from DX :
i. Scan D for clusters C2 that share at least
S percent of their patterns, and all of their
core patterns, with C1.
ii. Add all patterns of C2 to C1, setting all
shared patterns as core and all others as
unconfirmed.
iii. Remove cluster C2.
(c) If all of C1?s patterns remain unconfirmed re-
move C1.
4. If several clusters have the same set of core patterns
merge them according to rules (i,ii).
At the end of this stage, we have a set of pattern
clusters where for each cluster there are two subsets,
core patterns and unconfirmed patterns.
230
4 Relationship Classification
Up to this stage we did not access the training set in
any way and we did not use the fact that the target re-
lations are those holding between nominals. Hence,
only a small part of the acquired pattern clusters may
be relevant for a given NR classification task, while
other clusters can represent completely different re-
lationships (e.g., between verbs). We now use the
acquired clusters to learn a model for the given la-
beled training set and to use this model for classifi-
cation of the test set. First we describe how we deal
with data sparseness. Then we propose a HITS mea-
sure used for cluster labeling, and finally we present
three different classification methods that utilize pat-
tern clusters.
4.1 Enrichment of Provided Data
Our classification algorithm is based on contexts
of given nominal pairs. Co-appearance of nomi-
nal pairs can be very rare (in fact, some word pairs
in the Task 4 set co-appear only once in Yahoo
web search). Hence we need more contexts where
the given nominals or nominals similar to them co-
appear. This step does not require the training la-
bels (the correct classifications), so we do it for both
training and test pairs. We do it in two stages: ex-
tracting similar nominals, and obtaining more con-
texts.
4.1.1 Extracting more words
For each nominal pair (w1, w2) in a given sentence
S, we use a method similar to (Davidov and Rap-
poport, 2006) to extract words that have a shared
meaning with w1 or w2. We discover such words
by scanning our corpora and querying the web for
symmetric patterns (obtained automatically from the
corpus as in (Davidov and Rappoport, 2006)) that
contain w1 or w2. To avoid getting instances of
w1,2 with a different meaning, we also require that
the second word will appear in the same text para-
graph or the same web page. For example, if we are
given a pair <loans, students> and we see a sen-
tence ?... loans and scholarships for students and
professionals ...?, we use the symmetric pattern ?X
and Y? to add the word scholarships to the group of
loans and to add the word professionals to the group
of students. We do not take words from the sen-
tence ?In European soccer there are transfers and
loans...? since its context does not contain the word
students. In cases where there are only several or
zero instances where the two nominals co-appear,
we dismiss the latter rule and scan for each nominal
separately. Note that ?loans? can also be a verb, so
usage of a part-of-speech tagger might reduce noise.
If the number of instances for a desired nom-
inal is very low, our algorithm trims the first
words in these nominal and repeats the search (e.g.,
<simulation study, voluminous results> becomes
<study, results>). This step is the only one specific
to English, using the nature of English noun com-
pounds. Our desire in this case is to keep the head
words.
4.1.2 Extracting more contexts using the new
words
To find more instances where nominals similar to
w1 and w2 co-appear in HFW patterns, we construct
web queries using combinations of each nominal?s
group and extract patterns from the search result
snapshots (the two line summary provided by search
engines for each search result).
4.2 The HITS Measure
To use clusters for classification we define a HITS
measure similar to that of (Davidov et al, 2007), re-
flecting the affinity of a given nominal pair to a given
cluster. We use the pattern clusters from Section 3
and the additional data collected during the enrich-
ment phase to estimate a HITS value for each cluster
and each pair in the training and test sets. For a given
nominal pair (w1, w2) and cluster C with n core pat-
terns Pcore and m unconfirmed patterns Punconf ,
HITS(C, (w1, w2)) =
|{p; (w1, w2) appears in p ? Pcore}| /n+
?? |{p; (w1, w2) appears in p ? Punconf}| /m.
In this formula, ?appears in? means that the nomi-
nal pair appears in instances of this pattern extracted
from the original corpus or retrieved from the web
at the previous stage. Thus if some pair appears in
most of the patterns of some cluster it receives a high
HITS value for this cluster. ? (0..1) is a parameter
that lets us modify the relative weight of core and
unconfirmed patterns.
231
4.3 Classification Using Pattern Clusters
We present three ways to use pattern clusters for re-
lationship classification.
4.3.1 Classification by cluster labeling
One way to train a classifier in our case is to attach
a single relationship label to each cluster during the
training phase, and to assign each unlabeled pair to
some labeled cluster during the test phase. We use
the following normalized HITS measure to label the
involved pattern clusters. Denote by ki the number
of training pairs in class i in training set T . Then
Label(C) = argmaxi
?
p?T,Label(p)=i
hits(C, p)/ki
Clusters where the above sum is zero remain un-
labeled. In the test phase we assign to each test pair
p the label of the labeled cluster C that received the
highest HITS(C, p) value. If there are several clus-
ters with a highest HITS value, then the algorithm se-
lects a ?clarifying? set of patterns ? patterns that are
different in these best clusters. Then it constructs
clarifying web queries that contain the test nomi-
nal pair inside the clarifying patterns. The effect is
to increment the HITS value of the cluster contain-
ing a clarifying pattern if an appropriate pattern in-
stance (including the target nominals) was found on
the web. We start with the most frequent clarifying
pattern and perform additional queries until no clar-
ifying patterns are left or until some labeled cluster
obtains a highest HITS value. If no patterns are left
but there are still several winning clusters, we assign
to the pair the label of the cluster with the largest
number of pattern instances in the corpus.
One advantage of this method is that we get as
a by-product a set of labeled pattern clusters. Ex-
amination of this set can help to distinguish and an-
alyze (by means of patterns) which different rela-
tionships actually exist for each class in the train-
ing set. Furthermore, labeled pattern clusters can be
used for web queries to obtain additional examples
of the same relationship.
4.3.2 Classification by cluster HITS values as
features
In this method we treat the HITS measure for a clus-
ter as a feature for a machine learning classification
algorithm. To do this, we construct feature vectors
from each training pair, where each feature is the
HITS measure corresponding to a single pattern clus-
ter. We prepare test vectors similarly. Once we have
feature vectors, we can use a variety of classifiers
(we used those in Weka) to construct a model and to
evaluate it on the test set.
4.3.3 Unsupervised clustering
If we are not given any training set, it is still possi-
ble to separate between different relationship types
by grouping the feature vectors of Section 4.3.2 into
clusters. This can be done by applying k-means or
another clustering algorithm to the feature vectors
described above. This makes the whole approach
completely unsupervised. However, it does not pro-
vide any inherent labeling, making an evaluation dif-
ficult.
5 Experimental Setup
The main problem in a fair evaluation of NR classifi-
cation is that there is no widely accepted list of pos-
sible relationships between nominals. In our eval-
uation we have selected the setup and data from
SemEval-07 Task 4 (Girju et al, 2007). Selecting
this type of dataset alowed us to compare to 6 sub-
mitted state-of-art systems that evaluated on exactly
the same data and to 9 other systems that utilize
additional information (WN labels). We have ap-
plied our three different classification methods on
the given data set.
5.1 SemEval-07 Task 4 Overview
Task 4 (Girju et al, 2007) involves classification of
relationships between simple nominals other than
named entities. Seven distinct relationships were
chosen: Cause-Effect, Instrument-Agency, Product-
Producer, Origin-Entity, Theme-Tool, Part-Whole,
and Content-Container. For each relationship, the
provided dataset consists of 140 training and 70 test
examples. Examples were binary tagged as belong-
ing/not belonging to the tested relationship. The vast
majority of negative examples were near-misses, ac-
quired from the web using the same lexico-syntactic
patterns as the positives. Examples appear as sen-
tences with the nominal pair tagged. Nouns in this
pair were manually labeled with their correspond-
ing WordNet 3 labels and the web queries used to
232
obtain the sentences. The 15 submitted systems
were assigned into 4 categories according to whether
they use the WordNet and Query tags (some systems
were assigned to more than a single category, since
they reported experiments in several settings). In our
evaluation we do not utilize WordNet or Query tags,
hence we compare ourselves with the corresponding
group (A), containing 6 systems.
5.2 Corpus and Web Access
Our algorithm uses two corpora. We estimate fre-
quencies and perform primary search on a local web
corpus containing about 68GB untagged plain text.
This corpus was extracted from the web starting
from open directory links, comprising English web
pages with varied topics and styles (Gabrilovich and
Markovitch, 2005). To enrich the set of given word
pairs and patterns as described in Section 4.1 and
to perform clarifying queries, we utilize the Yahoo
API for web queries. For each query, if the desired
words/patterns were found in a page link?s snapshot,
we do not use the link, otherwise we download the
page from the retrieved link and then extract the re-
quired data. If only several links were found for a
given word pair we perform local crawling to depth
3 in an attempt to discover more instances.
5.3 Parameters and Learning Algorithm
Our algorithm utilizes several parameters. Instead
of calibrating them manually, we only provided
a desired range for each, and the final parameter
values were obtained during selection of the best-
performing setup using 10-fold cross-validation on
the training set. For each parameter we have esti-
mated its desired range using the (Nastase and Sz-
pakowicz, 2003) set as a development set. Note that
this set uses an entirely different relationship classi-
fication scheme. We ran the pattern clustering phase
on 128 different sets of parameters, obtaining 128
different clustering schemes with varied granularity,
noise and coverage.
The parameter ranges obtained are: FC (meta-
pattern content word frequency and upper bound for
hook word selection): 100?5000 words per million
(wpm); FH (meta-pattern HFW): 10 ? 100 wpm;
FB (low word count for hook word filtering): 1?50
wpm; N (number of hook words): 100 ? 1000; W
(window size): 5 or window = sentence; L (tar-
get word mutual information filter): 1/3 ? 1/5; S
(cluster overlap filter for cluster merging): 2/3; ?
(core vs. unconfirmed weight for HITS estimation):
0.1 ? 0.01; S (commonality for cluster merging):
2/3. As designed, each parameter indeed influences
a certain effect. Naturally, the parameters are not
mutually independent. Selecting the best configu-
ration in the cross-validation phase makes the algo-
rithm flexible and less dependent on hard-coded pa-
rameter values.
Selection of learning algorithm and its algorithm-
specific parameters were done as follows. For each
of the 7 classification tasks (one per relationship
type), for each of the 128 pattern clustering schemes,
we prepared a list of most of the compatible al-
gorithms available in Weka, and we automatically
selected the model (a parameter set and an algo-
rithm) which gave the best 10-fold cross-validation
results. The winning algorithms were LWL (Atke-
son et al, 1997), SMO (Platt, 1999), and K* (Cleary
and Trigg, 1995) (there were 7 tasks, and different
algorithms could be selected for each task). We then
used the obtained model to classify the testing set.
This allowed us to avoid fixing parameters that are
best for a specific dataset but not for others. Since
each dataset has only 140 examples, the computa-
tion time of each learning algorithm is negligible.
6 Results
The pattern clustering phase results in 90 to 3000
distinct pattern clusters, depending on the parameter
setup. Manual sampling of these clusters indeed re-
veals that many clusters contain patterns specific to
some apparent lexical relationship. For example, we
have discovered such clusters as: {?buy Y accessory
for X!?, ?shipping Y for X?, ?Y is available for X?, ?Y
are available for X?, ?Y are available for X systems?,
?Y for X? } and {?best X for Y?, ?X types for Y?, ?Y
with X?, ?X is required for Y?, ?X as required for Y?,
?X for Y?}. Note that some patterns (?Y for X?) can
appear in many clusters.
We applied the three classification methods de-
scribed in Section 4.3 to Task 4 data. For super-
vised classification we strictly followed the SemEval
datasets and rules. For unsupervised classification
we did not use any training data. Using the k-means
algorithm, we obtained two nearly equal unlabeled
233
Method P R F Acc
Unsupervised clustering (4.3.3) 64.5 61.3 62.0 64.5
Cluster Labeling (4.3.1) 65.1 69.0 67.2 68.5
HITS Features (4.3.2) 69.1 70.6 70.6 70.1
Best Task 4 (no WordNet) 66.1 66.7 64.8 66.0
Best Task 4 (with WordNet) 79.7 69.8 72.4 76.3
Table 1: Our SemEval-07 Task 4 results.
Relation Type F Acc C
Cause-Effect 69.7 71.4 2
Instrument-Agency 76.5 74.2 1
Product-Producer 76.4 83.8 1
Origin-Entity 65.4 62.6 4
Theme-Tool 59.4 58.7 6
Part-Whole 74.3 70.9 1
Content-Container 72.6 69.2 2
Table 2: By-relation Task 4 HITS-based results. C is the
number of clusters with positive labels.
clusters containing test samples. For evaluation we
assigned a negative/positive label to these two clus-
ters according to the best alignment with true labels.
Table 1 shows our results, along with the best Task
4 result not using WordNet labels (Costello, 2007).
For reference, the best results overall (Beamer et al,
2007) are also shown. The table shows precision (P)
recall (R), F-score (F), and Accuracy (Acc) (percent-
age of correctly classified examples).
We can see that while our algorithm is not as good
as the best method that utilizes WordNet tags, results
are superior to all participants who did not use these
tags. We can also see that the unsupervised method
results are above the random baseline (50%). In fact,
our results (f-score 62.0, accuracy 64.5) are better
than the averaged results (58.0, 61.1) of the group
that did not utilize WN tags.
Table 2 shows the HITS-based classification re-
sults (F-score and Accuracy) and the number of pos-
itively labeled clusters (C) for each relation. As ob-
served by participants of Task 4, we can see that dif-
ferent sets vary greatly in difficulty. However, we
also obtain a nice insight as to why this happens ?
relations like Theme-Tool seem very ambiguous and
are mapped to several clusters, while relations like
Product-Producer seem to be well-defined by the ob-
tained pattern clusters.
The SemEval dataset does not explicitly mark
items whose correct classification requires analysis
of the context of the whole sentence in which they
appear. Since our algorithm does not utilize test sen-
tence contextual information, we do not expect it to
show exceptional performance on such items. This
is a good topic for future research.
Since the SemEval dataset is of a very spe-
cific nature, we have also applied our classification
framework to the (Nastase and Szpakowicz, 2003)
dataset, which contains 600 pairs labeled with 5
main relationship types. We have used the exact
evaluation procedure described in (Turney, 2006),
achieving a class f-score average of 60.1, as opposed
to 54.6 in (Turney, 2005) and 51.2 in (Nastase et al,
2006). This shows that our method produces supe-
rior results for rather differing datasets.
7 Conclusion
Relationship classification is known to improve
many practical tasks, e.g., textual entailment (Tatu
and Moldovan, 2005). We have presented a novel
framework for relationship classification, based on
pattern clusters prepared as a standalone resource in-
dependently of the training set.
Our method outperforms current state-of-the-art
algorithms that do not utilize WordNet tags on Task
4 of SemEval-07. In practical situations, it would
not be feasible to provide a large amount of such
sense disambiguation tags manually. Our method
also shows competitive performance compared to
the majority of task participants that do utilize WN
tags. Our method can produce labeled pattern clus-
ters, which can be potentially useful for automatic
discovery of additional instances for a given rela-
tionship. We intend to pursue this promising direc-
tion in future work.
Acknowledgement. We would like to thank
the anonymous reviewers, whose comments have
greatly improved the quality of this paper.
References
Aramaki, E., Imai, T., Miyo, K., and Ohe, K., 2007.
UTH: semantic relation classification using physical
sizes. ACL SemEval ?07 Workshop.
Atkeson, C., Moore, A., and Schaal, S., 1997. Lo-
cally weighted learning. Artificial Intelligence Review,
11(1?5): 75?113.
234
Banko, M., Cafarella, M. J., Soderland, S., Broadhead,
M., and Etzioni, O., 2007. Open information extrac-
tion from the Web. IJCAI ?07.
Beamer, B., Bhat, S., Chee, B., Fister, A., Rozovskaya
A., and Girju, R., 2007. UIUC: A knowledge-rich ap-
proach to identifying semantic relations between nom-
inals. ACL SemEval ?07 Workshop.
Bedmar, I. S., Samy, D., and Martinez, J. L., 2007.
UC3M: Classification of semantic relations between
nominals using sequential minimal optimization. ACL
SemEval ?07 Workshop.
Cleary, J. G. , Trigg, L. E., 1995. K*: An instance-based
learner using and entropic distance measure. ICML
?95.
Costello, F. J., 2007. UCD-FC: Deducing semantic rela-
tions using WordNet senses that occur frequently in a
database of noun-noun compounds. ACL SemEval ?07
Workshop.
Davidov, D., Rappoport, A., 2006. Efficient unsuper-
vised discovery of word categories using symmetric
patterns and high frequency words. COLING-ACL ?06
Davidov D., Rappoport A. and Koppel M., 2007. Fully
unsupervised discovery of concept-specific relation-
ships by Web mining. ACL ?07.
Davidov, D., Rappoport, A., 2008. Unsupervised discov-
ery of generic relationships using pattern clusters and
its evaluation by automatically generated SAT analogy
questions. ACL ?08.
Gabrilovich, E., Markovitch, S., 2005. Feature gener-
ation for text categorization using world knowledge.
IJCAI ?05.
Girju, R., Giuglea, A., Olteanu, M., Fortu, O., Bolohan,
O., and Moldovan, D., 2004. Support vector ma-
chines applied to the classification of semantic rela-
tions in nominalized noun phrases. HLT/NAACL ?04
Workshop on Computational Lexical Semantics.
Girju, R., Moldovan, D., Tatu, M., and Antohe, D., 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19(4):479-496.
Girju, R., Badulescu, A., and Moldovan, D., 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1).
Girju, R., Hearst, M., Nakov, P., Nastase, V., Szpakowicz,
S., Turney, P., and Yuret, D., 2007. Task 04: Classi-
fication of semantic relations between nominal at Se-
mEval 2007. 4th Intl. Workshop on Semantic Evalua-
tions (SemEval ?07), in ACL ?07.
Hearst, M., 1992. Automatic acquisition of hyponyms
from large text corpora. COLING ?92
Hendrickx, I., Morante, R., Sporleder, C., and van den
Bosch, A., 2007. Machine learning of semantic rela-
tions with shallow features and almost no data. ACL
SemEval ?07 Workshop.
Kim, S.N., Baldwin, T., 2007. MELB-KB: Nominal
classification as noun compound interpretation. ACL
SemEval ?07 Workshop.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D., and
Girju, R., 2004. Models for the semantic classifica-
tion of noun phrases. HLT-NAACL ?04 Workshop on
Computational Lexical Semantics.
Nakov, P., and Hearst, M., 2007. UCB: System descrip-
tion for SemEval Task #4. ACL SemEval ?07 Work-
shop.
Nastase, V., Szpakowicz, S., 2003. Exploring noun-
modifier semantic relations. In Fifth Intl. Workshop
on Computational Semantics (IWCS-5).
Nastase, V., Sayyad-Shirabad, J., Sokolova, M., and Sz-
pakowicz, S., 2006. Learning noun-modifier semantic
relations with corpus-based and WordNet-based fea-
tures. In Proceedings of the 21st National Conference
on Artificial Intelligence, Boston, MA.
Pantel, P., Ravichandran, D., and Hovy, E., 2004. To-
wards terascale knowledge acquisition. COLING ?04.
Pantel, P., Pennacchiotti, M., 2006. Espresso: leveraging
generic patterns for automatically harvesting semantic
relations. COLING-ACL ?06.
Platt, J., 1999. Fast training of support vector machines
using sequential minimal optimization. In Scholkopf,
Burges, and Smola, Advances in Kernel Methods ?
Support Vector Learning, pp. 185?208. MIT Press.
Rosario, B., Hearst, M., 2001. Classifying the semantic
relations in noun compounds. EMNLP ?01.
Rosenfeld, B., Feldman, R., 2007. Clustering for unsu-
pervised relation identification. CIKM ?07.
Snow, R., Jurafsky, D., Ng, A.Y., 2006. Seman-
tic taxonomy induction from heterogeneous evidence.
COLING-ACL ?06.
Strube, M., Ponzetto, S., 2006. WikiRelate! computing
semantic relatedness using Wikipedia. AAAI ?06.
Tatu, M., Moldovan, D., 2005. A semantic approach to
recognizing textual entailment. HLT/EMNLP ?05.
Turney, P., 2005. Measuring semantic similarity by la-
tent relational analysis. IJCAI ?05.
Turney, P., 2006. Expressing implicit semantic relations
without supervision. COLING-ACL ?06.
Witten, H., Frank, E., 1999. Data Mining: Practical Ma-
chine Learning Tools and Techniques with Java Imple-
mentations. Morgan Kaufman, San Francisco, CA.
235
Proceedings of ACL-08: HLT, pages 692?700,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Discovery of Generic Relationships Using Pattern Clusters
and its Evaluation by Automatically Generated SAT Analogy Questions
Dmitry Davidov
ICNC
Hebrew University of Jerusalem
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
We present a novel framework for the dis-
covery and representation of general semantic
relationships that hold between lexical items.
We propose that each such relationship can be
identified with a cluster of patterns that cap-
tures this relationship. We give a fully unsu-
pervised algorithm for pattern cluster discov-
ery, which searches, clusters and merges high-
frequency words-based patterns around ran-
domly selected hook words. Pattern clusters
can be used to extract instances of the corre-
sponding relationships. To assess the quality
of discovered relationships, we use the pattern
clusters to automatically generate SAT anal-
ogy questions. We also compare to a set of
known relationships, achieving very good re-
sults in both methods. The evaluation (done
in both English and Russian) substantiates the
premise that our pattern clusters indeed reflect
relationships perceived by humans.
1 Introduction
Semantic resources can be very useful in many NLP
tasks. Manual construction of such resources is la-
bor intensive and susceptible to arbitrary human de-
cisions. In addition, manually constructed semantic
databases are not easily portable across text domains
or languages. Hence, there is a need for developing
semantic acquisition algorithms that are as unsuper-
vised and language independent as possible.
A fundamental type of semantic resource is that
of concepts (represented by sets of lexical items)
and their inter-relationships. While there is rel-
atively good agreement as to what concepts are
and which concepts should exist in a lexical re-
source, identifying types of important lexical rela-
tionships is a rather difficult task. Most established
resources (e.g., WordNet) represent only the main
and widely accepted relationships such as hyper-
nymy and meronymy. However, there are many
other useful relationships between concepts, such as
noun-modifier and inter-verb relationships. Identi-
fying and representing these explicitly can greatly
assist various tasks and applications. There are al-
ready applications that utilize such knowledge (e.g.,
(Tatu and Moldovan, 2005) for textual entailment).
One of the leading methods in semantics acqui-
sition is based on patterns (see e.g., (Hearst, 1992;
Pantel and Pennacchiotti, 2006)). The standard pro-
cess for pattern-based relation extraction is to start
with hand-selected patterns or word pairs express-
ing a particular relationship, and iteratively scan
the corpus for co-appearances of word pairs in pat-
terns and for patterns that contain known word pairs.
This methodology is semi-supervised, requiring pre-
specification of the desired relationship or hand-
coding initial seed words or patterns. The method
is quite successful, and examining its results in de-
tail shows that concept relationships are often being
manifested by several different patterns.
In this paper, unlike the majority of studies that
use patterns in order to find instances of given rela-
tionships, we use sets of patterns as the definitions
of lexical relationships. We introduce pattern clus-
ters, a novel framework in which each cluster cor-
responds to a relationship that can hold between the
lexical items that fill its patterns? slots. We present
a fully unsupervised algorithm to compute pat-
692
tern clusters, not requiring any, even implicit, pre-
specification of relationship types or word/pattern
seeds. Our algorithm does not utilize preprocess-
ing such as POS tagging and parsing. Some patterns
may be present in several clusters, thus indirectly ad-
dressing pattern ambiguity.
The algorithm is comprised of the following
stages. First, we randomly select hook words and
create a context corpus (hook corpus) for each hook
word. Second, we define a meta-pattern using high
frequency words and punctuation. Third, in each
hook corpus, we use the meta-pattern to discover
concrete patterns and target words co-appearing
with the hook word. Fourth, we cluster the patterns
in each corpus according to co-appearance of the tar-
get words. Finally, we merge clusters from different
hook corpora to produce the final structure. We also
propose a way to label each cluster by word pairs
that represent it best.
Since we are dealing with relationships that are
unspecified in advance, assessing the quality of the
resulting pattern clusters is non-trivial. Our evalu-
ation uses two methods: SAT tests, and compari-
son to known relationships. We used instances of
the discovered relationships to automatically gener-
ate analogy SAT tests in two languages, English and
Russian1. Human subjects answered these and real
SAT tests. English grades were 80% for our test and
71% for the real test (83% and 79% for Russian),
showing that our relationship definitions indeed re-
flect human notions of relationship similarity. In ad-
dition, we show that among our pattern clusters there
are clusters that cover major known noun-compound
and verb-verb relationships.
In the present paper we focus on the pattern clus-
ter resource itself and how to evaluate its intrinsic
quality. In (Davidov and Rappoport, 2008) we show
how to use the resource for a known task of a to-
tally different nature, classification of relationships
between nominals (based on annotated data), obtain-
ing superior results over previous work.
Section 2 discusses related work, and Section 3
presents the pattern clustering and labeling algo-
rithm. Section 4 describes the corpora we used and
the algorithm?s parameters in detail. Sections 5 and
1Turney and Littman (2005) automatically answers SAT
tests, while our focus is on generating them.
6 present SAT and comparison evaluation results.
2 Related Work
Extraction of relation information from text is a
large sub-field in NLP. Major differences between
pattern approaches include the relationship types
sought (including domain restrictions), the degrees
of supervision and required preprocessing, and eval-
uation method.
2.1 Relationship Types
There is a large body of related work that deals with
discovery of basic relationship types represented in
useful resources such as WordNet, including hyper-
nymy (Hearst, 1992; Pantel et al, 2004; Snow
et al, 2006), synonymy (Davidov and Rappoport,
2006; Widdows and Dorow, 2002) and meronymy
(Berland and Charniak, 1999; Girju et al, 2006).
Since named entities are very important in NLP,
many studies define and discover relations between
named entities (Hasegawa et al, 2004; Hassan et
al., 2006). Work was also done on relations be-
tween verbs (Chklovski and Pantel, 2004). There
is growing research on relations between nominals
(Moldovan et al, 2004; Girju et al, 2007).
2.2 Degree of Supervision and Preprocessing
While numerous studies attempt to discover one or
more pre-specified relationship types, very little pre-
vious work has directly attempted the discovery of
which main types of generic relationships actually
exist in an unrestricted domain. Turney (2006) pro-
vided a pattern distance measure that allows a fully
unsupervised measurement of relational similarity
between two pairs of words; such a measure could
in principle be used by a clustering algorithm in or-
der to deduce relationship types, but this was not
discussed. Unlike (Turney, 2006), we do not per-
form any pattern ranking. Instead we produce (pos-
sibly overlapping) hard clusters, where each pattern
cluster represents a relationship discovered in the
domain. Banko et al (2007) and Rosenfeld and
Feldman (2007) find relationship instances where
the relationships are not specified in advance. They
aim to find relationship instances rather than iden-
tify generic semantic relationships. Thus, their rep-
resentation is very different from ours. In addition,
(Banko et al, 2007) utilize supervised tools such
693
as a POS tagger and a shallow parser. Davidov et
al. (2007) proposed a method for unsupervised dis-
covery of concept-specific relations. That work, like
ours, relies on pattern clusters. However, it requires
initial word seeds and targets the discovery of rela-
tionships specific for some given concept, while we
attempt to discover and define generic relationships
that exist in the entire domain.
Studying relationships between tagged named en-
tities, (Hasegawa et al, 2004; Hassan et al, 2006)
proposed unsupervised clustering methods that as-
sign given sets of pairs into several clusters, where
each cluster corresponds to one of a known set of re-
lationship types. Their classification setting is thus
very different from our unsupervised discovery one.
Several recent papers discovered relations on the
web using seed patterns (Pantel et al, 2004), rules
(Etzioni et al, 2004), and word pairs (Pasca et al,
2006; Alfonseca et al, 2006). The latter used the
notion of hook which we also use in this paper.
Several studies utilize some preprocessing, includ-
ing parsing (Hasegawa et al, 2004; Hassan et al,
2006) and usage of syntactic (Suchanek et al, 2006)
and morphological (Pantel et al, 2004) informa-
tion in patterns. Several algorithms use manually-
prepared resources, including WordNet (Moldovan
et al, 2004; Costello et al, 2006) and Wikipedia
(Strube and Ponzetto, 2006). In this paper, we
do not utilize any language-specific preprocessing
or any other resources, which makes our algorithm
relatively easily portable between languages, as we
demonstrate in our bilingual evaluation.
2.3 Evaluation Method
Evaluation for hypernymy and synonymy usually
uses WordNet (Lin and Pantel, 2002; Widdows and
Dorow, 2002; Davidov and Rappoport, 2006). For
more specific lexical relationships like relationships
between verbs (Chklovski and Pantel, 2004), nom-
inals (Girju et al, 2004; Girju et al, 2007) or
meronymy subtypes (Berland and Charniak, 1999)
there is still little agreement which important rela-
tionships should be defined. Thus, there are more
than a dozen different type hierarchies and tasks pro-
posed for noun compounds (and nominals in gen-
eral), including (Nastase and Szpakowicz, 2003;
Girju et al, 2005; Girju et al, 2007).
There are thus two possible ways for a fair eval-
uation. A study can develop its own relationship
definitions and dataset, like (Nastase and Szpakow-
icz, 2003), thus introducing a possible bias; or it
can accept the definition and dataset prepared by
another work, like (Turney, 2006). However, this
makes it impossible to work on new relationship
types. Hence, when exploring very specific relation-
ship types or very generic, but not widely accepted,
types (like verb strength), many researchers resort
to manual human-based evaluation (Chklovski and
Pantel, 2004). In our case, where relationship types
are not specified in advance, creating an unbiased
benchmark is very problematic, so we rely on hu-
man subjects for relationship evaluation.
3 Pattern Clustering Algorithm
Our algorithm first discovers and clusters patterns in
which a single (?hook?) word participates, and then
merges the resulting clusters to form the final struc-
ture. In this section we detail the algorithm. The
algorithm utilizes several parameters, whose selec-
tion is detailed in Section 4. We refer to a pattern
contained in our clusters (a pattern type) as a ?pat-
tern? and to an occurrence of a pattern in the corpus
(a pattern token) as a ?pattern instance?.
3.1 Hook Words and Hook Corpora
As a first step, we randomly select a set of hook
words. Hook words were used in e.g. (Alfonseca
et al, 2006) for extracting general relations starting
from given seed word pairs. Unlike most previous
work, our hook words are not provided in advance
but selected randomly; the goal in those papers is
to discover relationships between given word pairs,
while we use hook words in order to discover rela-
tionships that generally occur in the corpus.
Only patterns in which a hook word actually par-
ticipates will eventually be discovered. Hence, in
principle we should select as many hook words as
possible. However, words whose frequency is very
high are usually ambiguous and are likely to produce
patterns that are too noisy, so we do not select words
with frequency higher than a parameter FC . In ad-
dition, we do not select words whose frequency is
below a threshold FB , to avoid selection of typos
and other noise that frequently appear on the web.
We also limit the total number N of hook words.
694
Our algorithm merges clusters originating from dif-
ferent hook words. Using too many hook words in-
creases the chance that some of them belong to a
noisy part in the corpus and thus lowers the quality
of our resulting clusters.
For each hook word, we now create a hook cor-
pus, the set of the contexts in which the word ap-
pears. Each context is a window containing W
words or punctuation characters before and after the
hook word. We avoid extracting text from clearly
unformatted sentences and our contexts do not cross
paragraph boundaries.
The size of each hook corpus is much smaller than
that of the whole corpus, easily fitting into main
memory; the corpus of a hook word occurring h
times in the corpus contains at most 2hW words.
Since most operations are done on each hook corpus
separately, computation is very efficient.
Note that such context corpora can in principle be
extracted by focused querying on the web, making
the system dynamically scalable. It is also possi-
ble to restrict selection of hook words to a specific
domain or word type, if we want to discover only
a desired subset of existing relationships. Thus we
could sample hook words from nouns, verbs, proper
names, or names of chemical compounds if we are
only interested in discovering relationships between
these. Selecting hook words randomly allows us to
avoid using any language-specific data at this step.
3.2 Pattern Specification
In order to reduce noise and to make the computa-
tion more efficient, we did not consider all contexts
of a hook word as pattern candidates, only contexts
that are instances of a specified meta-pattern type.
Following (Davidov and Rappoport, 2006), we clas-
sified words into high-frequency words (HFWs) and
content words (CWs). A word whose frequency is
more (less) than FH (FC) is considered to be a HFW
(CW). Unlike (Davidov and Rappoport, 2006), we
consider all punctuation characters as HFWs. Our
patterns have the general form
[Prefix] CW1 [Infix] CW2 [Postfix]
where Prefix, Infix and Postfix contain only HFWs.
To reduce the chance of catching CWi?s that are
parts of a multiword expression, we require Prefix
and Postfix to have at least one word (HFW), while
Infix is allowed to contain any number of HFWs (but
recall that the total length of a pattern is limited by
window size). A pattern example is ?such X as Y
and?. During this stage we only allow single words
to be in CW slots2.
3.3 Discovery of Target Words
For each of the hook corpora, we now extract all
pattern instances where one CW slot contains the
hook word and the other CW slot contains some
other (?target?) word. To avoid the selection of com-
mon words as target words, and to avoid targets ap-
pearing in pattern instances that are relatively fixed
multiword expressions, we sort all target words in
a given hook corpus by pointwise mutual informa-
tion between hook and target, and drop patterns ob-
tained from pattern instances containing the lowest
and highest L percent of target words.
3.4 Local Pattern Clustering
We now have for each hook corpus a set of patterns.
All of the corresponding pattern instances share the
hook word, and some of them also share a target
word. We cluster patterns in a two-stage process.
First, we group in clusters all patterns whose in-
stances share the same target word, and ignore the
rest. For each target word we have a single pattern
cluster. Second, we merge clusters that share more
than S percent of their patterns. A pattern can ap-
pear in more than a single cluster. Note that clusters
contain pattern types, obtained through examining
pattern instances.
3.5 Global Cluster Merging
The purpose of this stage is to create clusters of pat-
terns that express generic relationships rather than
ones specific to a single hook word. In addition,
the technique used in this stage reduces noise. For
each created cluster we will define core patterns and
unconfirmed patterns, which are weighed differently
during cluster labeling (see Section 3.6). We merge
clusters from different hook corpora using the fol-
lowing algorithm:
1. Remove all patterns originating from a single hook
corpus.
2While for pattern clusters creation we use only single words
as CWs, later during evaluation we allow multiword expressions
in CW slots of previously acquired patterns.
695
2. Mark all patterns of all present clusters as uncon-
firmed.
3. While there exists some cluster C1 from corpus DX
containing only unconfirmed patterns:
(a) Select a cluster with a minimal number of pat-
terns.
(b) For each corpus D different from DX :
i. Scan D for clusters C2 that share at least
S percent of their patterns, and all of their
core patterns, with C1.
ii. Add all patterns of C2 to C1, setting all
shared patterns as core and all others as
unconfirmed.
iii. Remove cluster C2.
(c) If all of C1?s patterns remain unconfirmed re-
move C1.
4. If several clusters have the same set of core patterns
merge them according to rules (i,ii).
We start from the smallest clusters because we ex-
pect these to be more precise; the best patterns for
semantic acquisition are those that belong to small
clusters, and appear in many different clusters. At
the end of this algorithm, we have a set of pattern
clusters where for each cluster there are two subsets,
core patterns and unconfirmed patterns.
3.6 Labeling of Pattern Clusters
To label pattern clusters we define a HITS measure
that reflects the affinity of a given word pair to a
given cluster. For a given word pair (w1, w2) and
cluster C with n core patterns Pcore and m uncon-
firmed patterns Punconf ,
Hits(C, (w1, w2)) =
|{p; (w1, w2) appears in p ? Pcore}| /n+
?? |{p; (w1, w2) appears in p ? Punconf}| /m.
In this formula, ?appears in? means that the word
pair appears in instances of this pattern extracted
from the original corpus or retrieved from the web
during evaluation (see Section 5.2). Thus if some
pair appears in most of patterns of some cluster it
receives a high HITS value for this cluster. The top
5 pairs for each cluster are selected as its labels.
? ? (0..1) is a parameter that lets us modify the
relative weight of core and unconfirmed patterns.
4 Corpora and Parameters
In this section we describe our experimental setup,
and discuss in detail the effect of each of the algo-
rithms? parameters.
4.1 Languages and Corpora
The evaluation was done using corpora in English
and Russian. The English corpus (Gabrilovich and
Markovitch, 2005) was obtained through crawling
the URLs in the Open Directory Project (dmoz.org).
It contains about 8.2G words and its size is about
68GB of untagged plain text. The Russian corpus
was collected over the web, comprising a variety of
domains, including news, web pages, forums, nov-
els and scientific papers. It contains 7.5G words of
size 55GB untagged plain text. Aside from remov-
ing noise and sentence duplicates, we did not apply
any text preprocessing or tagging.
4.2 Parameters
Our algorithm uses the following parameters: FC ,
FH , FB , W , N , L, S and ?. We used part of the
Russian corpus as a development set for determin-
ing the parameters. On our development set we have
tested various parameter settings. A detailed analy-
sis of the involved parameters is beyond the scope
of this paper; below we briefly discuss the observed
qualitative effects of parameter selection. Naturally,
the parameters are not mutually independent.
FC (upper bound for content word frequency in
patterns) influences which words are considered as
hook and target words. More ambiguous words gen-
erally have higher frequency. Since content words
determine the joining of patterns into clusters, the
more ambiguous a word is, the noisier the result-
ing clusters. Thus, higher values of FC allow more
ambiguous words, increasing cluster recall but also
increasing cluster noise, while lower ones increase
cluster precision at the expense of recall.
FH (lower bound for HFW frequency in patterns)
influences the specificity of patterns. Higher val-
ues restrict our patterns to be based upon the few
most common HFWs (like ?the?, ?of?, ?and?) and
thus yield patterns that are very generic. Lowering
the values, we obtain increasing amounts of pattern
clusters for more specific relationships. The value
we use for FH is lower than that used for FC , in or-
der to allow as HFWs function words of relatively
low frequency (e.g., ?through?), while allowing as
content words some frequent words that participate
in meaningful relationships (e.g., ?game?). However,
this way we may also introduce more noise.
696
FB (lower bound for hook words) filters hook
words that do not appear enough times in the cor-
pus. We have found that this parameter is essential
for removing typos and other words that do not qual-
ify as hook words.
N (number of hook words) influences relation-
ship coverage. With higher N values we discover
more relationships roughly of the same specificity
level, but computation becomes less efficient and
more noise is introduced.
W (window size) determines the length of the dis-
covered patterns. Lower values are more efficient
computationally, but values that are too low result in
drastic decrease in coverage. Higher values would
be more useful when we allow our algorithm to sup-
port multiword expressions as hooks and targets.
L (target word mutual information filter) helps in
avoiding using as targets common words that are
unrelated to hooks, while still catching as targets
frequent words that are related. Low L values de-
crease pattern precision, allowing patterns like ?give
X please Y more?, where X is the hook (e.g., ?Alex?)
and Y the target (e.g., ?some?). High values increase
pattern precision at the expense of recall.
S (minimal overlap for cluster merging) is a clus-
ters merge filter. Higher values cause more strict
merging, producing smaller but more precise clus-
ters, while lower values start introducing noise. In
extreme cases, low values can start a chain reaction
of total merging.
? (core vs. unconfirmed weight for HITS labeling)
allows lower quality patterns to complement higher
quality ones during labeling. Higher values increase
label noise, while lower ones effectively ignore un-
confirmed patterns during labeling.
In our experiments we have used the following
values (again, determined using a development set)
for these parameters: FC : 1, 000 words per mil-
lion (wpm); FH : 100 wpm; FB: 1.2 wpm; N : 500
words; W : 5 words; L: 30%; S: 2/3; ?: 0.1.
5 SAT-based Evaluation
As discussed in Section 2, the evaluation of semantic
relationship structures is non-trivial. The goal of our
evaluation was to assess whether pattern clusters in-
deed represent meaningful, precise and different re-
lationships. There are two complementary perspec-
tives that a pattern clusters quality assessment needs
to address. The first is the quality (precision/recall)
of individual pattern clusters: does each pattern clus-
ter capture lexical item pairs of the same semantic
relationship? does it recognize many pairs of the
same semantic relationship? The second is the qual-
ity of the cluster set as whole: does the pattern clus-
ters set alow identification of important known se-
mantic relationships? do several pattern clusters de-
scribe the same relationship?
Manually examining the resulting pattern clus-
ters, we saw that the majority of sampled clusters in-
deed clearly express an interesting specific relation-
ship. Examples include familiar hypernymy clusters
such as3 {?such X as Y?, ?X such as Y?, ?Y and other
X?,} with label (pets, dogs), and much more specific
clusters like { ?buy Y accessory for X!?, ?shipping Y
for X?, ?Y is available for X?, ?Y are available for X?,
?Y are available for X systems?, ?Y for X? }, labeled
by (phone, charger). Some clusters contain overlap-
ping patterns, like ?Y for X?, but represent different
relationships when examined as a whole.
We addressed the evaluation questions above us-
ing a SAT-like analogy test automatically generated
from word pairs captured by our clusters (see below
in this section). In addition, we tested coverage and
overlap of pattern clusters with a set of 35 known re-
lationships, and we compared our patterns to those
found useful by other algorithms (the next section).
Quantitatively, the final number of clusters is 508
(470) for English (Russian), and the average cluster
size is 5.5 (6.1) pattern types. 55% of the clusters
had no overlap with other clusters.
5.1 SAT Analogy Choice Test
Our main evaluation method, which is also a use-
ful application by itself, uses our pattern clusters to
automatically generate SAT analogy questions. The
questions were answered by human subjects.
We randomly selected 15 clusters. This allowed
us to assess the precision of the whole cluster set as
well as of the internal coherence of separate clus-
ters (see below). For each cluster, we constructed
a SAT analogy question in the following manner.
The header of the question is a word pair that is one
of the label pairs of the cluster. The five multiple
3For readability, we omit punctuations in Prefix and Postfix.
697
choice items include: (1) another label of the clus-
ter (the ?correct? answer); (2) three labels of other
clusters among the 15; and (3) a pair constructed by
randomly selecting words from those making up the
various cluster labels.
In our sample there were no word pairs assigned
as labels to more than one cluster4. As a baseline for
comparison, we have mixed these questions with 15
real SAT questions taken from English and Russian
SAT analogy tests. In addition, we have also asked
our subjects to write down one example pair of the
same relationship for each question in the test.
As an example, from one of the 15 clusters we
have randomly selected the label (glass, water). The
correct answer selected from the same cluster was
(schoolbag, book). The three pairs randomly se-
lected from the other 14 clusters were (war, death),
(request, license) and (mouse, cat). The pair ran-
domly selected from a cluster not among the 15 clus-
ters was (milk, drink). Among the subjects? propos-
als for this question were (closet, clothes) and (wal-
let, money).
We computed accuracy of SAT answers, and the
correlation between answers for our questions and
the real ones (Table 1). Three things are demon-
strated about our system when humans are capable
of selecting the correct answer. First, our clusters
are internally coherent in the sense of expressing a
certain relationship, because people identified that
the pairs in the question header and in the correct
answer exhibit the same relationship. Second, our
clusters distinguish between different relationships,
because the three pairs not expressing the same rela-
tionship as the header were not selected by the evalu-
ators. Third, our cluster labeling algorithm produces
results that are usable by people.
The test was performed in both English and Rus-
sian, with 10 (6) subjects for English (Russian).
The subjects (biology and CS students) were not in-
volved with the research, did not see the clusters,
and did not receive any special training as prepara-
tion. Inter-subject agreement and Kappa were 0.82,
0.72 (0.9, 0.78) for English (Russian). As reported
in (Turney, 2005), an average high-school SAT
grade is 57. Table 1 shows the final English and Rus-
4But note that a pair can certainly obtain a positive HITS
value for several clusters.
Our method Real SAT Correlation
English 80% 71% 0.85
Russian 83% 79% 0.88
Table 1: Pattern cluster evaluation using automatically
generated SAT analogy choice questions.
sian grade average for ours and real SAT questions.
We can see that for both languages, around 80%
of the choices were correct (the random choice base-
line is 20%). Our subjects are university students,
so results higher than 57 are expected, as we can
see from real SAT performance. The difference
in grades between the two languages might be at-
tributed to the presence of relatively hard and un-
common words. It also may result from the Russian
test being easier because there is less verb-noun am-
biguity in Russian.
We have observed a high correlation between true
grades and ours, suggesting that our automatically
generated test reflects the ability to recognize analo-
gies and can be potentially used for automated gen-
eration of SAT-like tests.
The results show that our pattern clusters indeed
mirror a human notion of relationship similarity and
represent meaningful relationships. They also show
that as intended, different clusters describe different
relationships.
5.2 Analogy Invention Test
To assess recall of separate pattern clusters, we have
asked subjects to provide (if possible) an additional
pair for each SAT question. On each such pair
we have automatically extracted a set of pattern in-
stances that capture this pair by using automated
web queries. Then we calculated the HITS value for
each of the selected pairs and assigned them to clus-
ters with highest HITS value. The numbers of pairs
provided were 81 for English and 43 for Russian.
We have estimated precision for this task as
macro-average of percentage of correctly assigned
pairs, obtaining 87% for English and 82% for Rus-
sian (the random baseline of this 15-class classifi-
cation task is 6.7%). It should be noted however
that the human-provided additional relationship ex-
amples in this test are not random so it may intro-
duce bias. Nevertheless, these results confirm that
our pattern clusters are able to recognize new in-
698
30 Noun Compound Relationships
Avg. num Overlap
of clusters
Russian 1.8 0.046
English 1.7 0.059
5 Verb Verb Relationships
Russian 1.4 0.01
English 1.2 0
Table 2: Patterns clusters discovery of known relation-
ships.
stances of relationships of the same type.
6 Evaluation Using Known Information
We also evaluated our pattern clusters using relevant
information reported in related work.
6.1 Discovery of Known Relationships
To estimate recall of our pattern cluster set, we
attempted to estimate whether (at least) a subset
of known relationships have corresponding pattern
clusters. As a testing subset, we have used 35 re-
lationships for both English and Russian. 30 rela-
tions are noun compound relationships as proposed
in the (Nastase and Szpakowicz, 2003) classifica-
tion scheme, and 5 relations are verb-verb relations
proposed by (Chklovski and Pantel, 2004). We
have manually created sets of 5 unambiguous sam-
ple pairs for each of these 35 relationships. For each
such pair we have assigned the pattern cluster with
best HITS value.
The middle column of Table 2 shows the average
number of clusters per relationship. Ideally, if for
each relationship all 5 pairs are assigned to the same
cluster, the average would be 1. In the worst case,
when each pair is assigned to a different cluster, the
average would be 5. We can see that most of the
pairs indeed fall into one or two clusters, success-
fully recognizing that similarly related pairs belong
to the same cluster. The column on the right shows
the overlap between different clusters, measured as
the average number of shared pairs in two randomly
selected clusters. The baseline in this case is essen-
tially 5, since there are more than 400 clusters for 5
word pairs. We see a very low overlap between as-
signed clusters, which shows that these clusters in-
deed separate well between defined relations.
6.2 Discovery of Known Pattern Sets
We compared our clusters to lists of patterns re-
ported as useful by previous papers. These lists
included patterns expressing hypernymy (Hearst,
1992; Pantel et al, 2004), meronymy (Berland and
Charniak, 1999; Girju et al, 2006), synonymy
(Widdows and Dorow, 2002; Davidov and Rap-
poport, 2006), and verb strength + verb happens-
before (Chklovski and Pantel, 2004). In all cases,
we discovered clusters containing all of the reported
patterns (including their refinements with domain-
specific prefix or postfix) and not containing patterns
of competing relationships.
7 Conclusion
We have proposed a novel way to define and identify
generic lexical relationships as clusters of patterns.
Each such cluster is set of patterns that can be used
to identify, classify or capture new instances of some
unspecified semantic relationship. We showed how
such pattern clusters can be obtained automatically
from text corpora without any seeds and without re-
lying on manually created databases or language-
specific text preprocessing. In an evaluation based
on an automatically created analogy SAT test we
showed on two languages that pairs produced by our
clusters indeed strongly reflect human notions of re-
lation similarity. We also showed that the obtained
pattern clusters can be used to recognize new ex-
amples of the same relationships. In an additional
test where we assign labeled pairs to pattern clus-
ters, we showed that they provide good coverage for
known noun-noun and verb-verb relationships for
both tested languages.
While our algorithm shows good performance,
there is still room for improvement. It utilizes a set
of constants that affect precision, recall and the gran-
ularity of the extracted cluster set. It would be ben-
eficial to obtain such parameters automatically and
to create a multilevel relationship hierarchy instead
of a flat one, thus combining different granularity
levels. In this study we applied our algorithm to a
generic domain, while the same method can be used
for more restricted domains, potentially discovering
useful domain-specific relationships.
699
References
Alfonseca, E., Ruiz-Casado, M., Okumura, M., Castells,
P., 2006. Towards large-scale non-taxonomic relation
extraction: estimating the precision of rote extractors.
COLING-ACL ?06 Ontology Learning & Population
Workshop.
Banko, M., Cafarella, M. J. , Soderland, S., Broadhead,
M., and Etzioni, O., 2007. Open information extrac-
tion from the Web. IJCAI ?07.
Berland, M., Charniak, E., 1999. Finding parts in very
large corpora. ACL ?99.
Chklovski, T., Pantel, P., 2004. VerbOcean: mining the
web for fine-grained semantic verb relations. EMNLP
?04.
Costello, F., Veale, T. Dunne, S., 2006. Using Word-
Net to automatically deduce relations between words
in noun-noun compounds. COLING-ACL ?06.
Davidov, D., Rappoport, A., 2006. Efficient unsuper-
vised discovery of word categories using symmetric
patterns and high frequency words. COLING-ACL
?06.
Davidov, D., Rappoport, A. and Koppel, M., 2007. Fully
unsupervised discovery of concept-specific relation-
ships by Web mining. ACL ?07.
Davidov, D., Rappoport, A., 2008. Classification of re-
lationships between nominals using pattern clusters.
ACL ?08.
Etzioni, O., Cafarella, M., Downey, D., Popescu, A.,
Shaked, T., Soderland, S., Weld, D., and Yates, A.,
2004. Methods for domain-independent information
extraction from the web: An experimental compari-
son. AAAI 04
Gabrilovich, E., Markovitch, S., 2005. Feature gener-
ation for text categorization using world knowledge.
IJCAI 2005.
Girju, R., Giuglea, A., Olteanu, M., Fortu, O., Bolohan,
O., and Moldovan, D., 2004. Support vector machines
applied to the classification of semantic relations in
nominalized noun phrases. HLT/NAACL Workshop on
Computational Lexical Semantics.
Girju, R., Moldovan, D., Tatu, M., and Antohe, D., 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19(4):479-496.
Girju, R., Badulescu, A., and Moldovan, D., 2006. Au-
tomatic discovery of part-whole relations. Computa-
tional Linguistics, 32(1).
Girju, R., Hearst, M., Nakov, P., Nastase, V., Szpakow-
icz, S., Turney, P., and Yuret, D., 2007. Task 04:
Classification of semantic relations between nominal
at SemEval 2007. ACL ?07 SemEval Workshop.
Hasegawa, T., Sekine, S., and Grishman, R., 2004. Dis-
covering relations among named entities from large
corpora. ACL ?04.
Hassan, H., Hassan, A. and Emam, O., 2006. Unsu-
pervised information extraction approach using graph
mutual reinforcement. EMNLP ?06.
Hearst, M., 1992. Automatic acquisition of hyponyms
from large text corpora. COLING ?92
Lin, D., Pantel, P., 2002. Concept discovery from text.
COLING 02.
Moldovan, D., Badulescu, A., Tatu, M., Antohe, D.,Girju,
R., 2004. Models for the semantic classification of
noun phrases. HLT-NAACL ?04 Workshop on Compu-
tational Lexical Semantics.
Nastase, V., Szpakowicz, S., 2003. Exploring noun mod-
ifier semantic relations. IWCS-5.
Pantel, P., Pennacchiotti, M., 2006. Espresso: leveraging
generic patterns for automatically harvesting semantic
relations. COLING-ACL 2006.
Pantel, P., Ravichandran, D. and Hovy, E.H., 2004. To-
wards terascale knowledge acquisition. COLING ?04.
Pasca, M., Lin, D., Bigham, J., Lifchits A., Jain, A.,
2006. Names and similarities on the web: fact extrac-
tion in the fast lane. COLING-ACL ?06.
Rosenfeld, B., Feldman, R., 2007. Clustering for unsu-
pervised relation identification. CIKM ?07.
Snow, R., Jurafsky, D., Ng, A.Y., 2006. Seman-
tic taxonomy induction from heterogeneous evidence.
COLING-ACL ?06.
Strube, M., Ponzetto, S., 2006. WikiRelate! computing
semantic relatedness using Wikipedia. AAAI ?06.
Suchanek, F., Ifrim, G., and Weikum, G., 2006. LEILA:
learning to extract information by linguistic analysis.
COLING-ACL ?06 Ontology Learning & Population
Workshop.
Tatu, M., Moldovan, D., 2005. A semantic approach to
recognizing textual entailment. HLT/EMNLP 2005.
Turney, P., 2005. Measuring semantic similarity by la-
tent relational analysis. IJCAI ?05.
Turney, P., Littman, M., 2005. Corpus-based learn-
ing of analogies and semantic selations. Machine
Learning(60):1?3:251?278.
Turney, P., 2006. Expressing implicit semantic relations
without supervision. COLING-ACL ?06.
Widdows, D., Dorow, B., 2002. A graph model for un-
supervised lexical acquisition. COLING ?02.
700
Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 36?44,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Unsupervised Concept Discovery In Hebrew Using Simple Unsupervised
Word Prefix Segmentation for Hebrew and Arabic
Elad Dinur1 Dmitry Davidov2
1Institute of Computer Science
2ICNC
The Hebrew University of Jerusalem
Ari Rappoport1
Abstract
Fully unsupervised pattern-based methods
for discovery of word categories have been
proven to be useful in several languages.
The majority of these methods rely on the
existence of function words as separate
text units. However, in morphology-rich
languages, in particular Semitic languages
such as Hebrew and Arabic, the equiva-
lents of such function words are usually
written as morphemes attached as prefixes
to other words. As a result, they are missed
by word-based pattern discovery methods,
causing many useful patterns to be unde-
tected and a drastic deterioration in per-
formance. To enable high quality lexical
category acquisition, we propose a sim-
ple unsupervised word segmentation algo-
rithm that separates these morphemes. We
study the performance of the algorithm for
Hebrew and Arabic, and show that it in-
deed improves a state-of-art unsupervised
concept acquisition algorithm in Hebrew.
1 Introduction
In many NLP tasks, we wish to extract informa-
tion or perform processing on text using minimal
knowledge on the input natural language. Towards
this goal, we sometimes find it useful to divide the
set of words in natural language to function words
and content words, a division that applies in the
vast majority of languages. Function words (or
grammatical words, e.g., a, an, the, in, of, etc) are
words that have little or highly ambiguous lexi-
cal meaning, and serve to express grammatical or
semantic relationships with the other words in a
sentence.
In some morphologically-rich languages, im-
portant function words are not written as space-
separated units but as morphemes attached as pre-
fixes to other words. This fact can cause prob-
lems when statistically analyzing text in these lan-
guages, for two main reasons: (1) the vocabulary
of the language grows, as our lexical knowledge
comes solely from a corpus (words appear with
and without the function morphemes); (2) infor-
mation derived from the presence of these mor-
phemes in the sentence is usually lost.
In this paper we address the important task of
a fully unsupervised acquisition of Hebrew lexical
categories (or concepts ? words sharing a signifi-
cant aspect of their meaning). We are not aware of
any previous work on this task for Hebrew. Due
to the problem above, the performance of many
acquisition algorithms deteriorates unacceptably.
This happens, for example, in the (Davidov and
Rappoport, 2006) algorithm that utilizes automati-
cally detected function words as the main building
block for pattern construction.
In order to overcome this problem, one should
separate such prefixes from the compound words
(words consisting of function morphemes attached
to content words) in the input corpus. When
we consider some particular word, there are fre-
quently many options to split it to smaller strings.
Fortunately, the set of function words is small and
closed, and the set of grammatical sequences of
function prefixes is also small. Hence we assume
it does not cost us much to know in advance what
are the possible sequences for a specific language.
Even when considering the small number of
possible function words, the task of separating
them is not simple, as some words may be ambigu-
ous. When reading a word that starts with a prefix
known to be a function morpheme, the word may
36
be a compound word, or it may be a meaningful
word by itself. For example, the word ?hsws? in
Hebrew1 can be interpreted as ?hsws? (hesitation),
or ?h sws? (the horse). The segmentation of the
word is context dependent ? the same string may
be segmented differently in different contexts.
One way of doing such word prefix segmenta-
tion is to perform a complete morphological dis-
ambiguation of the sentence. The disambigua-
tion algorithm finds for each word its morpho-
logical attributes (POS tag, gender, etc.), and de-
cides whether a word is a compound word or a
word without prefixes. A disambiguation algo-
rithm generally relies on a language-specific mor-
phological analyzer. It may also require a large
manually tagged corpus, construction of which for
some particular language or domain requires sub-
stantial human labor. We avoid the utilization of
such costly and language-specific disambiguation
algorithms and manually annotated data.
In this paper we present a novel method to sep-
arate function word prefixes, and evaluate it us-
ing manually labeled gold standards in Hebrew
and Arabic. We incorporate the method into a
pattern-based Hebrew concept acquisition frame-
work and show that it greatly improves state-of-art
results for unsupervised lexical category acquisi-
tion. This improvement allows the pattern-based
unsupervised framework to use one-tenth of the
Hebrew data in order to reach a similar level of
results.
Section 2 discusses related work, and Section 3
reviews the word categories discovery algorithm.
Section 4 presents the word prefix segmentation
algorithm. Results are given in Section 5.
2 Related Work
In this paper we develop an unsupervised frame-
work for segmentation of the function words for
languages where context is important for correct
segmentation. Our main target language is He-
brew, and we experimented with Arabic as well.
As far as we know, there is no work on unsu-
pervised segmentation of words in Hebrew which
does not utilize language-specific tools such as
morphological analyzers.
Lee et al (2003) addressed supervised word
segmentation in Arabic and have some aspects
similar to our approach. As in their study, we
1Transcription is according to (Ornan, 2005), except for
Shin which is denoted by ?$?.
also have a pre-supplied list of possible prefix
sequences and assume a trigram model in order
to find the most probable morpheme sequence.
Both studies evaluate performance on a segmented
text, and not just on words in the lexicon. How-
ever, their algorithm, while achieving good per-
formance (97% accuracy), relies on a training set
? a manually segmented corpus of about 110,000
words, while our unsupervised framework does
not require any annotation and is thus easier to im-
plement and to apply to different domains and lan-
guages.
Snyder and Barzilay (2008) study the task of un-
supervised morphological segmentation of multi-
ple languages. Their algorithm automatically in-
duces a segmentation and morpheme alignment of
short parallel phrases from a multilingual corpus.
Their corpus (The Hebrew Bible and translations)
contains parallel phrases in English, Arabic, He-
brew and Aramaic. They obtain 63.87 F-Score
for Hebrew words segmentation (prefix and suf-
fix), where recall and precision is calculated based
on all possible segmentation points.
Another type of segmentation algorithms in-
volves utilization of language-specific morpholog-
ical analyzers for complete morphological disam-
biguation. In Hebrew each word usually has more
than one possible POS (along with other attributes,
such as gender, number, etc.). Assuming we have
a morphological analyzer (producing the set of
possible analyses for a given word), we can try to
discover the correct segmentation of each word.
Levinger et al (1995) developed a method for
disambiguation of the results provided by a mor-
phological analyzer for Hebrew. Adler and El-
hadad (2006) proposed an unsupervised algorithm
for word segmentation. They estimate an initial
language model (using (Levinger et al, 1995))
and improve this model with EM. Direct compar-
ison to their work is problematic, however, since
we avoid utilization of a language-specific mor-
phology/POS analyzer. There are also studies of
this type that utilize labeled data (Bar-Haim et al,
2005), where the language model is learned from
the training data.
Extensive research has been done on word seg-
mentation, where, unlike in our study, the segmen-
tation is evaluated for every word, regardless of its
context. Creutz (2003) presents an algorithm for
unsupervised segmentation under these assump-
tions. He proposes a probabilistic model which
37
utilizes the distributions of morpheme length and
frequency to estimate the quality of the induced
morphemes. Dasgupta and Ng (2007) improves
over (Creutz, 2003) by suggesting a simpler ap-
proach. They segment a prefix using the word
frequency with and without a prefix. Other re-
cent studies that follow the context-independent
setup include (Creutz and Lagus, 2005; Keshava
and Pitler, 2005; Demberg, 2007). They test
their methods on English, Finnish and Turkish.
All of these studies, however, assume context-
independency of segmentation, disregarding the
ambiguity that may come from context. This
makes it problematic to apply the proposed meth-
ods to context-dependent morphology types as in
Hebrew and Arabic.
The guiding goal in the present paper is the con-
cept acquisition problem. Concept acquisition of
different kinds has been studied extensively. The
two main classification axes for this task are the
type of human input and annotation, and the basic
algorithmic approach used. The two main algo-
rithmic approaches are clustering of context fea-
ture vectors and pattern-based discovery.
The first approach is to map each word to a fea-
ture vector and cluster these vectors. Example of
such algorithms are (Pereira et al, 1993) and (Lin,
1998) that use syntactic features in the vector def-
inition. Pantel and Lin (2002) improves on the lat-
ter by clustering by committee.
Recently, there is a growing interest in the sec-
ond main algorithmic approach, usage of lexico-
syntactic patterns. Patterns have been shown to
produce more accurate results than feature vectors,
at a lower computational cost on large corpora
(Pantel et al, 2004). Thus (Dorow et al, 2005)
discover categories using two basic pre-specified
patterns (?x and y?, ?x or y?).
Some recent studies have proposed frameworks
that attempt to avoid any implicit or explicit pre-
specification of patterns. Davidov and Rappoport
(2006) proposed a method that detects function
words by their high frequency, and utilizes these
words for the discovery of symmetric patterns.
Their method is based on two assumptions: (1)
some function words in the language symmetri-
cally connect words belonging to the same cat-
egory; (2) such function words can be detected
as the most frequent words in language. While
these assumptions are reasonable for many lan-
guages, for some morphologically rich languages
the second assumption may fail. This is due to
the fact that some languages like Hebrew and Ara-
bic may express relationships not by isolated func-
tion words but by morphemes attached in writing
to other words.
As an example, consider the English word
?and?, which was shown to be very useful in con-
cept acquisition (Dorow et al, 2005). In Hebrew
this word is usually expressed as the morpheme
?w? attached to the second word in a conjunc-
tion (?... wsws? ? ?... and horse?). Patterns dis-
covered by such automatic pattern discovery al-
gorithms are based on isolated words, and hence
fail to capture ?and?-based relationships that are
very useful for detection of words belonging to the
same concept. Davidov and Rappoport (2006) re-
ports very good results for English and Russian.
However, no previous work applies a fully unsu-
pervised concept acquisition for Hebrew.
In our study we combine their concept ac-
quisition framework with a simple unsupervised
word segmentation technique. Our evaluation con-
firms the weakness of word-based frameworks for
morphology-rich languages such as Hebrew, and
shows that utilizing the proposed word segmen-
tation can overcome this weakness while keeping
the concept acquisition approach fully unsuper-
vised.
3 Unsupervised Discovery of Word
Categories
In this study we use word segmentation to improve
the (Davidov and Rappoport, 2006) method for
discovery of word categories, sets of words shar-
ing a significant aspect of their meaning. An ex-
ample for such a discovered category is the set of
verbs {dive, snorkel, swim, float, surf, sail, drift,
...}. Below we briefly describe this category ac-
quisition algorithm.
The algorithm consists of three stages as fol-
lows. First, it discovers a set of pattern candidates,
which are defined by a combination of high fre-
quency words (denoted by H) and slots for low
frequency (content) words (denoted by C). An ex-
ample for such a pattern candidate is ?x belongs to
y?, where ?x? and ?y? stand for content word slots.
The patterns are found according to a predefined
set of possible meta-patterns. The meta-patterns
are language-independent2 and consist of up to 4
2They do not include any specific words, only a relative
order of high/low frequency words, and hence can be used on
38
words in total, from which two are (non-adjacent)
content words. Four meta-patterns are used: CHC,
CHCH, CHHC, HCHC.
Second, those patterns which give rise to sym-
metric lexical relationships are identified. The
meaning of phrases constructed from those pat-
terns is (almost) invariant to the order of the con-
tent words contained in them. An example for
such a pattern is ?x and y?. In order to iden-
tify such useful patterns, for each pattern we build
a graph following (Widdows and Dorow, 2002).
The graph is constructed from a node for each con-
tent word, and a directed arc from the node ?x? to
?y? if the corresponding content words appear in
the pattern such that ?x? precedes ?y?. Then we
calculate several symmetry measures on the graph
structure and select the patterns with best values
for these measures.
The third stage is the generation of categories.
We extract tightly connected sets of words from
the unified graph which combines all graphs of se-
lected patterns. Such sets of words define the de-
sired categories.
The patterns which include the ?x and y? sub-
string are among the most useful patterns for gen-
eration of categories (they were used in (Dorow et
al., 2005) and discovered in all 5 languages tested
in (Davidov and Rappoport, 2006)). However, in
Hebrew such patterns can not be found in the same
way, since the function word ?and? is the prefix ?w?
and not a standalone high frequency word.
Another popular set of patterns are ones includ-
ing ?x or y?. Such patterns can be identified in
Hebrew, as ?or? in Hebrew is a separate word.
However, even in this case, the content word rep-
resented by ?x? or ?y? may appear with a pre-
fix. This damages the construction of the pattern
graph, since two different nodes may be created
instead of one ? one for a regular content word,
the other for the same word with a prefix. Conse-
quently, it is reasonable to assume that segmenting
the corpus in advance should improve the results
of discovery of word categories.
4 Word Segmentation Algorithm
We assume we know the small and closed set of
grammatical function word prefix sequences in the
language3. Our input is a sentence, and our ob-
any languages with explicit word segmentation.
3Unlike development of labeled training data, handcraft-
ing such a closed set is straightforward for many languages
and does not requires any significant time/human labor
jective is to return the correct segmentation of the
sentence. A sentence L is a sequence of words
{w1, w2, ..., wn}. A segmentation Si of L is a se-
quence of morphemes {m1, m2, ..., mk} and l(Si)
is the number of morphemes in the sequence. Note
that l(Si) may be different for each segmentation.
The best segmentation S will be calculated by:
P (Si) = p(m1)p(m2|m1)
l(Si)
?
i=3
p(mi|mi?1mi?2)
S = arg max
Si
P (Si)
Calculation of joint probabilities requires a tri-
gram model of the language. Below we describe
the construction of the trigram model and then we
detail the algorithm for efficient calculation of S.
4.1 Construction of trigram model
Creating the trigram language model is done in
two stages: (1) we segment a corpus automati-
cally, and (2) we learn a trigram language model
from the segmented corpus.
4.1.1 Initial corpus segmentation
For initial corpus segmentation, we define a sta-
tistical measure for the segmentation of individual
words. Let wx be a word, such that w is the pre-
fix of the word composed of a sequence of func-
tion word prefixes and x is a string of letters. Let
f(x) be the frequency of the word x in the cor-
pus. Denote by al the average length of the strings
(with prefixes) in the language. This can be eas-
ily estimated from the corpus ? every string that
appears in the corpus is counted once. l(x) is the
number of characters in the word x. We utilize
two parameters G, H , where G < H (we used
G = 2.5, H = 3.5) and define the following func-
tions :
factor(x) =
{
al?G?l(x)
al?H l(x) < al ? G
0 otherwise
Rank(wx) =
f(wx)
f(wx) + f(x)
+ factor(x)
Note that the expression f(wx)f(wx)+f(x) is a number
in (0, 1], inversely correlated with the frequency of
the prefixed word. Thus higher Rank(wx) values
indicate that the word is less likely to be composed
of the prefix w followed by the word x.
39
The expression al?G?l(x)al?H is a number in (0, 1],
therefore factor(x) ? [0, 1]. H is G ? 1 in order
to keep the expression smaller than 1. The term
factor(x) is greater as x is shorter. The factor
is meant to express the fact that short words are
less likely to have a prefix. We have examined
this in Hebrew ? as there are no words of length
1, two letter words have no prefix. We have ana-
lyzed 102 randomly chosen three letter words, and
found that only 19 of them were prefixed words.
We have analyzed 100 randomly chosen four let-
ter words, and found that 40 of them were pre-
fixed words. The result was about the same for
five letter words. In order to decide whether a
word needs to be separated, we define a thresh-
old T ? [0, 1]. We allow word separation only
when Rank(wx) is lower than T . When there
are more than two possible sequences of function
word prefixes (?mhsws?,?m hsws?, ?mh sws?),
we choose the segmentation with the lower rank.
4.1.2 Learning the trigram model
The learning of the language model is based on
counts of the corpus, assigning a special symbol,
?u/k? (unknown) for all words that do not appear
in the corpus. As estimated by (Lee et al, 2003),
we set the probability of ?u/k? to be 1E ? 9. The
value of the symbol ?u/k? was observed to be sig-
nificant. We found that the value proposed by (Lee
et al, 2003) for Arabic gives good results also for
Hebrew.
4.2 Dynamic programming approach for
word segmentation
The naive method to find S is to iterate over
all possible segmentations of the sentence. This
method may fail to handle long sentences, as
the number of segmentations grows exponentially
with the length of the sentence. To overcome this
problem, we use dynamic programming.
Each morpheme has an index i to its place in a
segmentation sequence. Iteratively, for index i, for
every morpheme which appears in some segmen-
tation in index i, we calculate the best segmen-
tation of the sequence m1 . . .mi. Two problems
arise here: (1) we need to calculate which mor-
phemes may appear in a given index; (2) we need
to constrain the calculation, such that only valid
segmentations would be considered.
To calculate which morphemes can appear in a
given index we define the object Morpheme. It
contains the morpheme (string), the index of a
word in the sentence the morpheme belongs to,
reference to the preceding Morpheme in the same
word, and indication whether it is the last mor-
pheme in the word. For each index of the sen-
tence segmentation, we create a list of Morphemes
(index-list).
For each word wi, and for segmentation
m1i , .., m
k
i , we create Morphemes M1i , .., Mki . We
traverse sequentially the words in the sentence,
and for each segmentation we add the sequence of
Morphemes to all possible index-lists. The index-
list for the first Morpheme M1i is the combination
of successors of all the index-lists that contain a
Morpheme Mki?1. The constraints are enforced
easily ? if a Morpheme M ji is the first in a word,
the preceding Morpheme in the sequence must be
the last Morpheme of the previous word. Oth-
erwise, the preceding Morpheme must be M j?1i ,
which is referenced by M ji .
4.3 Limitations
While our model handles the majority of cases, it
does not fully comply with a linguistic analysis of
Hebrew, as there are a few minor exceptions. We
assumed that there is no ambiguity in the function
word prefixes. This is not entirely correct, as in
Hebrew we have two different kinds of exceptions
for this rule. For example, the prefix ?k$? (when),
can also be interpreted as the prefix ?k? (as) fol-
lowed by the prefix ?$? (that). As the second in-
terpretation is rare, we always assumed it is the
prefix ?k$?. This rule was applied wherever an
ambiguity exists. However, we did not treat this
problem as it is very rare, and in the development
set and test set it did not appear even once.
A harder problem is encountered when process-
ing the word ?bbyt?. Two interpretations could
be considered here: ?b byt? (?in a house?), and
?b h byt? (?in the house?). Whether this actu-
ally poses a problem or not depends on the ap-
plication. We assume that the correct segmenta-
tion here is ?b byt?. Without any additional lin-
guistic knowledge (for example, diacritical vowel
symbols should suffice in Hebrew), solving these
problems requires some prior discriminative data.
5 Evaluation and Results
We evaluate our algorithm in two stages. First we
test the quality of our unsupervised word segmen-
tation framework on Hebrew and Arabic, compar-
ing our segmentation results to a manually anno-
40
With factor(x) Without factor(x)
T Prec. Recall F-Measure Accuracy Prec. Recall F-Measure Accuracy
0.70 0.844 0.798 0.820 0.875 0.811 0.851 0.830 0.881
0.73 0.841 0.828 0.834 0.883 0.808 0.866 0.836 0.884
0.76 0.837 0.846 0.841 0.886 0.806 0.882 0.842 0.887
0.79 0.834 0.870 0.851 0.893 0.803 0.897 0.847 0.890
0.82 0.826 0.881 0.852 0.892 0.795 0.904 0.846 0.888
0.85 0.820 0.893 0.854 0.892 0.787 0.911 0.844 0.886
0.88 0.811 0.904 0.855 0.891 0.778 0.917 0.841 0.882
Table 1: Ranks vs. Threshold T for Hebrew.
With factor(x) Without factor(x)
T Prec. Recall F-Measure Accuracy Prec. Recall F-Measure Accuracy
0.91 0.940 0.771 0.846 0.892 0.903 0.803 0.850 0.891
0.93 0.930 0.797 0.858 0.898 0.903 0.840 0.870 0.904
0.95 0.931 0.810 0.866 0.904 0.902 0.856 0.878 0.909
0.97 0.927 0.823 0.872 0.906 0.896 0.869 0.882 0.911
0.99 0.925 0.848 0.872 0.915 0.878 0.896 0.886 0.913
1.00 0.923 0.852 0.886 0.915 0.841 0.896 0.867 0.895
Table 2: Ranks vs. Threshold T for Arabic.
Algorithm P R F A
Rank seg. 0.834 0.870 0.851 0.893
Baseline 0.561 0.491 0.523 0.69
Morfessor 0.630 0.689 0.658 0.814
Table 3: Segmentation results comparison.
tated gold standard. Then we incorporate word
segmentation into a concept acquisition frame-
work and compare the performance of this frame-
work with and without word segmentation.
5.1 Corpora and annotation
For our experiments in Hebrew we used a 19MB
Hebrew corpus obtained from the ?Mila? Knowl-
edge Center for Processing Hebrew4. The cor-
pus consists of 143,689 different words, and a
total of 1,512,737 word tokens. A sample text
of size about 24,000 words was taken from the
corpus, manually segmented by human annotators
and used as a gold standard in our segmentation
evaluation. In order to estimate the quality of our
algorithm for Arabic, we used a 7MB Arabic news
items corpus, and a similarly manually annotated
test text of 4715 words. The Arabic corpus is too
small for meaningful category discovery, so we
used it only in the segmentation evaluation.
5.2 Evaluation of segmentation framework
In order to estimate the performance of word seg-
mentation as a standalone algorithm we applied
our algorithm on the Hebrew and Arabic corpora,
4http://mila.cs.technion.ac.il.
using different parameter settings. We first cal-
culated the word frequencies, then applied initial
segmentation as described in Section 4. Then we
used SRILM (Stolcke, 2002) to learn the trigram
model from the segmented corpus. We utilized
Good-Turing discounting with Katz backoff, and
we gave words that were not in the training set the
constant probability 1E ? 9. Finally we utilized
the obtained trigram model to select sentence seg-
mentations. To test the influence of the factor(x)
component of the Rank value, we repeated our
experiment with and without usage of this com-
ponent. We also ran our algorithm with a set of
different threshold T values in order to study the
influence of this parameter.
Tables 1 and 2 show the obtained results for He-
brew and Arabic respectively. Precision is the ra-
tio of correct prefixes to the total number of de-
tected prefixes in the text. Recall is the ratio of pre-
fixes that were split correctly to the total number
of prefixes. Accuracy is the number of correctly
segmented words divided by the total number of
words.
As can be seen from the results, the best F-score
with and without usage of the factor(x) compo-
nent are about the same, but usage of this compo-
nent gives higher precision for the same F-score.
From comparison of Arabic and Hebrew perfor-
mance we can also see that segmentation decisions
for the task in Arabic are likely to be easier, since
the accuracy for T=1 is very high. It means that,
unlike in Hebrew (where the best results were ob-
tained for T=0.79), a word which starts with a pre-
41
Method us k-means random
avg ?shared meaning?(%) 85 24.61 10
avg triplet score(1-4) 1.57 2.32 3.71
avg category score(1-10) 9.35 6.62 3.5
Table 4: Human evaluation results.
abuse, robbery, murder, assault, extortion
good, cheap, beautiful, comfortable
son, daughter, brother, parent
when, how, where
essential, important, central, urgent
Table 5: A sample from the lexical categories dis-
covered in Hebrew (translated to English).
fix should generally be segmented.
We also compared our best results to the base-
line and to previous work. The baseline draws a
segmentation uniformly for each word, from the
possible segmentations of the word. In an at-
tempt to partially reproduce (Creutz and Lagus,
2005) on our data, we also compared our results
to the results obtained from Morfessor Categories-
MAP, version 0.9.1 (Described in (Creutz and La-
gus, 2005)). The Morfessor Categories-MAP al-
gorithm gets a list of words and their frequen-
cies, and returns the segmentation for every word.
Since Morfessor may segment words with prefixes
which do not exist in our predefined list of valid
prefixes, we did not segment the words that had
illegal prefixes as segmented by Morfessor.
Results for this comparison are shown in Table
3. Our method significantly outperforms both the
baseline and Morfessor-based segmentation. We
have also tried to improve the language model by
a self training scheme on the same corpus but we
observed only a slight improvement, giving 0.848
Precision and 0.872 Recall.
5.3 Discovery of word categories
We divide the evaluation of the word categories
discovery into two parts. The first is evaluating
the improvement in the quantity of found lexical
categories. The second is evaluating the quality
of these categories. We have applied the algo-
rithm to a Hebrew corpus of size 130MB5, which
is sufficient for a proof of concept. We compared
the output of the categories discovery on two dif-
ferent settings, with function word separation and
without such separation. In both settings we omit-
5Again obtained from the ?Mila? Knowledge Center for
Processing Hebrew.
N A J
With Separation 148 4.1 1
No Separation 36 2.9 0
Table 6: Lexical categories discovery results com-
parison. N: number of categories. A: average cat-
egory size. J: ?junk? words.
ted all punctuation symbols. In both runs of the
algorithm we used the same parameters. Eight
symmetric patterns were automatically chosen for
each run. Two of the patterns that were chosen
by the algorithm in the unseparated case were also
chosen in the separated case.
5.3.1 Manual estimation of category quality
Evaluating category quality is challenging since
no exhaustive lists or gold standards are widely
accepted even in English, certainly so in resource-
poor languages such as Hebrew. Hence we follow
the human judgment evaluation scheme presented
in (Davidov and Rappoport, 2006), for the cate-
gories obtained from the segmented corpus.
We compared three methods of word categories
discovery. The first is random sampling of words
into categories. The second is k-means, where
each word is mapped to a vector, and similarity is
calculated as described in (Pantel and Lin, 2002).
We applied k-means to the set of vectors, with sim-
ilarity as a distance function. If a vector had low
similarity with all means, we leave it unattached.
Therefore some clusters contained only one vec-
tor. Running the algorithm 10 times, with different
initial means each time, produced 60 clusters with
three or more words. An interesting phenomenon
we observed is that this method produces very nice
clusters of named entities. The last method is the
one in (Davidov and Rappoport, 2006).
The experiment contained two parts. In Part
I, subjects were given 40 triplets of words and
were asked to rank them using the following scale:
(1) the words definitely share a significant part
of their meaning; (2) the words have a shared
meaning but only in some context; (3) the words
have a shared meaning only under a very un-
usual context/situation; (4) the words do not share
any meaning; (5) I am not familiar enough with
some/all of the words.
The 40 triplets were obtained as follows. 20 of
our categories were selected at random from the
non-overlapping categories we have discovered,
and three words were selected from each of these
42
at random. 10 triplets were selected in the same
manner from the categories produced by k-means,
and 10 triplets were selected at random from con-
tent words in the same document.
In Part II, subjects were given the full categories
represented by the triplets that were graded as 1 or
2 in Part I (the full ?good? categories in terms of
sharing of meaning). Subjects were asked to grade
the categories from 1 (worst) to 10 (best) accord-
ing to how much the full category had met the ex-
pectations they had when seeing only the triplet.
Nine people participated in the evaluation. A
summary of the results is given in Table 4.
The categories obtained from the unsegmented
corpus are too few and too small for a significant
evaluation. Therefore we applied the evaluation
scheme only for the segmented corpus.
The results from the segmented corpus contain
some interesting categories, with a 100% preci-
sion, like colors, Arab leaders, family members
and cities. An interesting category is {Arabic, En-
glish, Russian, French, German, Yiddish, Polish,
Math}. A sample of some other interesting cate-
gories can be seen in Table 5.
5.3.2 Segmentation effect on category
discovery
In Table 6, we find that there is a major improve-
ment in the number of acquired categories, and an
interesting improvement in the average category
size. One might expect that as a consequence of
an incorrect segmentation of a word, ?junk? words
may appear in the discovered categories. As can
be seen, only one ?junk? word was categorized.
Throughout this paper we have assumed that
function word properties of languages such as He-
brew and Arabic decrease performance of whole-
word pattern-based concept acquisition methods.
To check this assumption, we have applied the
concept acquisition algorithm on several web-
based corpora of several languages, while choos-
ing corpora size to be exactly equal to the size of
the Hebrew corpus (130Mb) and utilizing exactly
the same parameters. We did not perform quality
evaluation6, but measured the number of concepts
and concept size. Indeed the number of categories
was (190, 170, 159, 162, 150, 29) for Russian, En-
glish, Spanish, French, Turkish and Arabic respec-
tively, clearly inferior for Arabic in comparison to
these European and Slavic languages. A similar
6Brief manual examination suggests no significant drops
in concept quality.
tendency was observed for average concept size.
At the same time prefix separation does help to ex-
tract 148 concepts for Hebrew, making it nearly in-
line with other languages. In contrast, our prelim-
inary experiments on English and Russian suggest
that the effect of applying similar morphological
segmentation on these languages in insignificant.
In order to test whether more data can substi-
tute segmentation even for Hebrew, we have ob-
tained by means of crawling and web queries a
larger (while potentially much more noisy) web-
based 2GB Hebrew corpus which is based on fo-
rum and news contents. Our goal was to estimate
which unsegmented corpus size (if any) can bring
similar performance (in terms of concept number,
size and quality). We gradually increased corpus
size and applied the concept acquisition algorithm
on this corpus. Finally, we have obtained similar,
nearly matching, results to our 130MB corpus for
a 1.2GB Hebrew subcorpus of the 2GB Hebrew
corpus. The results remain stable for 4 different
1.2GB subsets taken from the same 2GB corpus.
This suggests that while segmentation can be sub-
stituted with more data, it may take roughly x10
more data for Hebrew to obtain the same results
without segmentation as with it.
6 Summary
We presented a simple method for separating func-
tion word prefixes from words. The method re-
quires very little language-specific knowledge (the
prefixes), and it can be applied to any morpholog-
ically rich language. We showed that this segmen-
tation dramatically improves lexical acquisition in
Hebrew, where nearly ?10 data is required to ob-
tain the same number of concepts without segmen-
tation.
While in this paper we evaluated our framework
on the discovery of concepts, we have recently
proposed fully unsupervised frameworks for the
discovery of different relationship types (Davidov
et al, 2007; Davidov and Rappoport, 2008a; Davi-
dov and Rappoport, 2008b). Many of these meth-
ods are mostly based on function words, and may
greatly benefit from the proposed segmentation
framework.
References
Meni Adler, Michael Elhadad, 2006. An Unsupervised
Morpheme-Based HMM for Hebrew Morphological
Disambiguation. ACL ?06.
43
Roy Bar-Haim, Khalil Simaan, Yoad Winter, 2005.
Choosing an Optimal Architecture for Segmentation
and POS-Tagging of Modern Hebrew. ACL Work-
shop on Computational Approaches to Semitic Lan-
guages ?05.
Mathias Creutz, 2003. Unsupervised Segmentation of
Words Using Prior Distributions of Morph Length
and Frequency. ACL ?03.
Mathias Creutz and Krista Lagus, 2005. Unsuper-
vised Morpheme Segmentation and Morphology In-
duction from Text Corpora Using Morfessor 1.0.
In Computer and Information Science, Report A81,
Helsinki University of Technology.
Sajib Dasgupta and Vincent Ng, 2007. High Perfor-
mance, Language-Independent Morphological Seg-
mentation. NAACL/HLT ?07.
Dmitry Davidov, Ari Rappoport, 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2008a. Classification
of Semantic Relationships between Nominals Using
Pattern Clusters. ACL ?08.
Dmitry Davidov, Ari Rappoport, 2008b. Unsupervised
Discovery of Generic Relationships Using Pattern
Clusters and its Evaluation by Automatically Gen-
erated SAT Analogy Questions. ACL ?08.
Vera Demberg, 2007. A Language-Independent Un-
supervised Model for Morphological Segmentation.
ACL ?07.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using Curvature and Markov Clustering in Graphs
for Lexical Acquisition and Word Sense Discrimi-
nation. MEANING ?05.
Samarth Keshava, Emily Pitler, 2006. A Simpler, Intu-
itive Approach to Morpheme Induction. In Proceed-
ings of 2nd Pascal Challenges Workshop, Venice,
Italy.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, Hany Hassan, 2003. Language Model
Based Arabic Word Segmentation. ACL ?03.
Moshe Levinger, Uzzi Ornan, Alon Itai, 1995. Learn-
ing Morpho-Lexical Probabilities from an Untagged
Corpus with an Application to Hebrew. Comput.
Linguistics, 21:383:404.
Dekang Lin, 1998. Automatic Retrieval and Cluster-
ing of Similar Words. COLING ?98.
Uzzi Ornan, 2005. Hebrew in Latin Script. Lesonenu,
LXIV:137:151 (in Hebrew).
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993.
Distributional Clustering of English Words. ACL
?93.
Benjamin Snyder, Regina Bazilay, 2008. Unsuper-
vised Multilingual Learning for Morphological Seg-
mentation. ACL/HLT ?08.
Andreas Stolcke, 2002. SRILM ? an Extensible Lan-
guage Modeling Toolkit. ICSLP, pages 901-904,
Denver, Colorado.
Dominic Widdows, Beate Dorow, 2002. A Graph
Model for Unsupervised Lexical Acquisition. COL-
ING ?02.
44
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 48?56,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Superior and Efficient Fully Unsupervised Pattern-based Concept
Acquisition Using an Unsupervised Parser
Dmitry Davidov1 Roi Reichart1 Ari Rappoport2
1ICNC , 2Institute of Computer Science
Hebrew University of Jerusalem
{dmitry@alice.nc|roiri@cs|arir@cs}.huji.ac.il
Abstract
Sets of lexical items sharing a significant
aspect of their meaning (concepts) are fun-
damental for linguistics and NLP. Unsuper-
vised concept acquisition algorithms have
been shown to produce good results, and are
preferable over manual preparation of con-
cept resources, which is labor intensive, er-
ror prone and somewhat arbitrary. Some ex-
isting concept mining methods utilize super-
vised language-specific modules such as POS
taggers and computationally intensive parsers.
In this paper we present an efficient fully
unsupervised concept acquisition algorithm
that uses syntactic information obtained from
a fully unsupervised parser. Our algorithm
incorporates the bracketings induced by the
parser into the meta-patterns used by a sym-
metric patterns and graph-based concept dis-
covery algorithm. We evaluate our algorithm
on very large corpora in English and Russian,
using both human judgments and WordNet-
based evaluation. Using similar settings as
the leading fully unsupervised previous work,
we show a significant improvement in con-
cept quality and in the extraction of multiword
expressions. Our method is the first to use
fully unsupervised parsing for unsupervised
concept discovery, and requires no language-
specific tools or pattern/word seeds.
1 Introduction
Comprehensive lexical resources for many domains
and languages are essential for most NLP applica-
tions. One of the most utilized types of such re-
sources is a repository of concepts: sets of lexical
items sharing a significant aspect of their meanings
(e.g., types of food, tool names, etc).
While handcrafted concept databases (e.g., Word-
Net) are extensively used in NLP, manual compila-
tion of such databases is labor intensive, error prone,
and somewhat arbitrary. Hence, for many languages
and domains great efforts have been made for au-
tomated construction of such databases from avail-
able corpora. While language-specific and domain-
specific studies show significant success in develop-
ment of concept discovery frameworks, the majority
of domains and languages remain untreated. Hence
there is a need for a framework that performs well
for many diverse settings and is as unsupervised and
language-independent as possible.
Numerous methods have been proposed for seed-
based concept extraction where a set of concept pat-
terns (or rules), or a small set of seed words for each
concept, is provided as input to the concept acqui-
sition system. However, even simple definitions for
concepts are not always available.
To avoid requiring this type of input, a number of
distributional and pattern-based methods have been
proposed for fully unsupervised seed-less acquisi-
tion of concepts from text. Pattern-based algorithms
were shown to obtain high quality results while be-
ing highly efficient in comparison to distributional
methods. Such fully unsupervised methods do not
incorporate any language-specific parsers or taggers,
so can be successfully applied to diverse languages.
However, unsupervised pattern-based methods
suffer from several weaknesses. Thus they are fre-
quently restricted to single-word terms and are un-
able to discover multiword expressions in efficient
and precise manner. They also usually ignore poten-
tially useful part-of-speech and other syntactic in-
formation. In order to address these weaknesses,
several studies utilize language-specific parsing or
48
tagging systems in concept acquisition. Unfortu-
nately, while improving results, this heavily affects
the language- and domain- independence of such
frameworks, and severely impacts efficiency since
even shallow parsing is computationally demanding.
In this paper we present a method to utilize the in-
formation induced by unsupervised parsers in an un-
supervised pattern-based concept discovery frame-
work. With the recent development of fast fully un-
supervised parsers, it is now possible to add parser-
based information to lexical patterns while keep-
ing the language-independence of the whole frame-
work and still avoiding heavy computational costs.
Specifically, we incorporate the bracketings induced
by the parser into the meta-patterns used by a sym-
metric patterns and graph-based unsupervised con-
cept discovery algorithm.
We performed a thorough evaluation on two En-
glish corpora (the BNC and a 68GB web corpus)
and on a 33GB Russian corpus. Evaluations were
done using both human judgments and WordNet, in
similar settings as that of the leading unsupervised
previous work. Our results show that utilization of
unsupervised parser both improves the assignment
of single-word terms to concepts and allows high-
precision discovery and assignment of of multiword
expressions to concepts.
2 Previous Work
Much work has been done on lexical acquisition of
all sorts and the acquisition of concepts in particu-
lar. Concept acquisition methods differ in the type of
corpus annotation and other human input used, and
in their basic algorithmic approach. Some methods
directly aim at concept acquisition, while the direct
goal in some is the construction of hyponym (?is-a?)
hierarchies. A subtree in such a hierarchy can be
viewed as defining a concept.
A major algorithmic approach is to represent
word contexts as vectors in some space and use dis-
tributional measures and clustering in that space.
Pereira (1993), Curran (2002) and Lin (1998) use
syntactic features in the vector definition. (Pantel
and Lin, 2002) improves on the latter by clustering
by committee. Caraballo (1999) uses conjunction
and appositive annotations in the vector representa-
tion. Several studies avoid requiring any syntactic
annotation. Some methods are based on decompo-
sition of a lexically-defined matrix (by SVD, PCA
etc), e.g. (Schu?tze, 1998; Deerwester et al, 1990).
While great effort has been made for improv-
ing the computational complexity of distributional
methods (Gorman and Curran, 2006), they still re-
main highly computationally intensive in compari-
son to pattern approaches (see below), and most of
them do not scale well for very large datasets.
The second main approach is to use lexico-
syntactic patterns. Patterns have been shown to pro-
duce more accurate results than feature vectors, at
a lower computational cost on large corpora (Pan-
tel et al, 2004). Since (Hearst, 1992), who used a
manually prepared set of initial lexical patterns, nu-
merous pattern-based methods have been proposed
for the discovery of concepts from seeds. Other
studies develop concept acquisition for on-demand
tasks where concepts are defined by user-provided
seeds. Many of these studies utilize information ob-
tained by language-specific parsing and named en-
tity recognition tools (Dorow et al, 2005). Pantel et
al. (2004) reduce the depth of linguistic data used,
but their method requires POS tagging.
TextRunner (Banko et al, 2007) utilizes a set
of pattern-based seed-less strategies in order to ex-
tract relational tuples from text. However, this sys-
tem contains many language-specific modules, in-
cluding the utilization of a parser in one of the pro-
cessing stages. Thus the majority of the existing
pattern-based concept acquisition systems rely on
pattern/word seeds or supervised language-specific
tools, some of which are very inefficient.
Davidov and Rappoport (2006) developed a
framework which discovers concepts based on high
frequency words and symmetry-based pattern graph
properties. This framework allows a fully unsuper-
vised seed-less discovery of concepts without rely-
ing on language-specific tools. However, it com-
pletely ignores potentially useful syntactic or mor-
phological information.
For example, the pattern ?X and his Y? is useful
for acquiring the concept of family member types,
as in ?his siblings and his parents?. Without syn-
tactic information, it can capture noise, as in ?... in
ireland) and his wife)? (parentheses denote syntac-
tic constituent boundaries). As another example, the
useful symmetric pattern ?either X or Y? can appear
in both good examples (?choose either Chihuahua
49
or Collie.?) and bad ones (?either Collie or Aus-
tralian Bulldog?). In the latter case, the algorithm
both captures noise (?Australlian? is now consid-
ered as a candidate for the ?dog type? concept), and
misses the discovery of a valid multiword candidate
(?Australlian Bulldog?). While symmetry-based fil-
tering greatly reduces such noise, the basic problem
remains. As a result, incorporating at least some
parsing information in a language-independent and
efficient manner could be beneficial.
Unsupervised parsing has been explored for sev-
eral decades (see (Clark, 2001; Klein, 2005) for re-
cent reviews). Recently, unsupervised parsers have
for the first time outperformed the right branch-
ing heuristic baseline for English. These include
CCM (Klein and Manning, 2002), the DMV and
DMV+CCM models (Klein and Manning, 2004),
(U)DOP based models (Bod, 2006a; Bod, 2006b;
Bod, 2007), an exemplar based approach (Den-
nis, 2005), guiding EM using contrastive estimation
(Smith and Eisner, 2006), and the incremental parser
of Seginer (2007) which we use here. These works
learn an unlabeled syntactic structure, dependency
or constituency. In this work we use constituency
trees as our syntactic representation.
Another important factor in concept acquisition
is the source of textual data used. To take advan-
tage of the rapidly expanding web, many of the pro-
posed frameworks utilize web queries rather than
local corpora (Etzioni et al, 2005; Davidov et al,
2007; Pasca and Van Durme, 2008; Davidov and
Rappoport, 2009). While these methods have a defi-
nite practical advantage of dealing with the most re-
cent and comprehensive data, web-based evaluation
has some methodological drawbacks such as limited
repeatability (Kilgarriff, 2007). In this study we ap-
ply our framework on offline corpora in settings sim-
ilar to that of previous work, in order to be able to
make proper comparisons.
3 Efficient Unsupervised Parsing
Our method utilizes the information induced by un-
supervised parsers. Specifically, we make use of the
bracketings induced by Seginer?s parser1 (Seginer,
2007). This parser has advantages in three major as-
1The parser is freely available at
http://staff.science.uva.nl/?yseginer/ccl
pects relevant to this paper.
First, it achieves state of the art unsupervised
parsing performance: its F-score2 is 75.9% for sen-
tences of up to 10 words from the PennTreebank
Wall Street Journal corpus (WSJ) (Marcus, 1993),
and 59% for sentences of the same length from the
German NEGRA (Brants, 1997) corpus. These cor-
pora consists of newspaper texts.
Second, to obtain good results, manually created
POS tags are used as input in all the unsupervised
parsers mentioned above except of Seginer?s, which
uses raw sentences as input. (Headden et al, 2008)
have shown that the performance of algorithms that
require POS tags substantially decreases when using
POS tags induced by unsupervised POS taggers in-
stead of manually created ones. Seginer?s incremen-
tal parser is therefore the only fully unsupervised
parser providing high quality parses.
Third, Seginer?s parser is extremely fast. During
its initial stage, the parser builds a lexicon. Our Pen-
tium 2.8GHB machines with 4GHB RAM can store
in memory the lexicon created by up to 0.2M sen-
tences. We thus divided our corpora to batches of
0.2M sentences and parsed each of them separately.
Note that in this setup parsing quality might be even
better than the quality reported in (Seginer, 2007),
since in the setup reported in that paper the parser
was applied to a few thousand sentences only. On
average, the parsing time of a single batch was 5
minutes (run time did not significantly differ across
batches and corpora).
Parser description. The parser utilizes the novel
common-cover link representation for syntactic
structure. This representation resembles depen-
dency structure but unlike the latter, it can be trans-
lated into a constituency tree, which is the syntactic
representation we use in this work.
The parsing algorithm creates the common-cover
links structure of a sentence in an incremental man-
ner. This means that the parser reads the words of
a sentence one after the other and, as each word is
read, it is only allowed to add links that have one of
their ends at that words (and update existing ones).
Words which have not yet been read are not avail-
2F = 2?R?PR+P , where R and P are the recall and precision of
the parsers? bracketing compared to manually created bracket-
ing of the same text. This is the accepted measure for parsing
performance (see (Klein, 2005)).
50
able to the parser at this stage. This restriction is
inspired by psycholinguistics research which sug-
gests that humans process language incrementally.
This results in a significant restriction of the parser?s
search space, which is the reason it is so fast.
During its initial stage the parser builds a lexicon
containing, for each word, statistics helping the deci-
sion of whether to link that word to other words. The
lexicon is updated as any new sentence is read. Lex-
icon updating is also done in an incremental manner
so this stage is also very fast.
4 Unsupervised Pattern Discovery
In the first stage of our algorithm, we run the unsu-
pervised parser on the corpus in order to produce a
bracketing structure for each sentence. In the sec-
ond stage, described here, we use these bracketings
in order to discover, in a fully unsupervised manner,
patterns that could be useful for concept mining.
Our algorithm is based on the concept acquisition
method of (Davidov and Rappoport, 2006). We dis-
cover patterns that connect terms belonging to the
same concept in two main stages: discovery of pat-
tern candidates, and identification of the symmetric
patterns among the candidates.
Pattern candidates. A major idea of (Davidov
and Rappoport, 2006) is that a few dozen high fre-
quency words (HFW) such as ?and? and ?is? con-
nect other, less frequent content terms into relation-
ships. They define meta-patterns, which are short
sequences of H?s and C?s, where H is a slot for
a HFW and C is a slot for a content word (later
to become a word belonging to a discovered con-
cept). Their method was shown to produce good
results. However, the fact that it does not consider
any syntactic information causes problems. Specif-
ically, it does not consider the constituent structure
of the sentence. Meta-patterns that cross constituent
boundaries are likely to generate noise ? two content
words (C?s) in a meta-pattern that belong to differ-
ent constituents are likely to belong to different con-
cepts as well. In addition, meta-patterns that do not
occupy a full constituent are likely to ?cut? multi-
word expressions (MWEs) into two parts, one part
that gets treated as a valid C word and one part that
is completely ignored.
The main idea in the present paper is to use the
bracketings induced by unsupervised parsers in or-
der to avoid the problems above. We utilize brack-
eting boundaries in our meta-patterns in addition
to HFW and C slots. In other words, their origi-
nal meta-patterns are totally lexical, while ours are
lexico-syntactic meta-patterns. We preserve the at-
tractive properties of meta-patterns, because both
HFWs and bracketings can be found or computed in
a language independent manner and very efficiently.
Concretely, we define a HFW as a word appearing
more than TH times per million words, and a C as
a word or multiword expression containing up to 4
words, appearing less than TC times per million.
We require that our patterns include two slots for
C?s, separated by at least a single HFW or bracket.
We allow separation by a single bracket because the
lowest level in the induced bracketing structure usu-
ally corresponds to lexical items, while higher levels
correspond to actual syntactic constituents.
In order to avoid truncation of multiword expres-
sions, we also require the meta pattern to start and
end by a HFW or bracket. Thus our meta-patterns
match the following regular expression:
{H|B}? C1 {H|B}+ C2 {H|B}?
where ?*? means zero or more times, and ?+? means
one or more time and B can be ?(?,?)? brackets pro-
duced by the parser (in these patterns we do not
need to guarantee that brackets match properly). Ex-
amples of such patterns include ?((C1)in C2))?,
?(C1)(such(as(((C2)?, and ?(C1)and(C2)?3. We
dismiss rare patterns that appear less than TP times
per million words.
Symmetric patterns. Many of the pattern candi-
dates discovered in the previous stage are not usable.
In order to find a usable subset, we focus on the sym-
metric patterns. We define a symmetric pattern as a
pattern in which the same pair of terms (C words)
is likely to appear in both left-to-right and right-to-
left orders. In order to identify symmetric patterns,
for each pattern we define a pattern graph G(P ), as
proposed by (Widdows and Dorow, 2002). If term
pair (C1, C2) appears in pattern P in some context,
3This paper does not use any punctuation since the parser
is provided with sentences having all non-alphabetic characters
removed. We assume word separation. C1,2 can be a word or a
multiword expression.
51
we add nodes c1, c2 to the graph and a directed edge
EP (c1, c2) between them. In order to select sym-
metric patterns, we create such a pattern graph for
every discovered pattern, and create a symmetric
subgraph SymG(P) in which we take only bidirec-
tional edges from G(P ). Then we compute three
measures for each pattern candidate as proposed by
(Davidov and Rappoport, 2006):
M1(P ) := |{c1|?c2EP (c1, c2) ? ?c3EP (c3, c1)}||Nodes(G(P ))|
M2(P ) := |Nodes(SymG(P ))||Nodes(G(P ))|
M3(P ) := |Edges(SymG(P ))||Edges(G(P ))|
For each measure, we prepare a sorted list of all can-
didate patterns. We remove patterns that are not in
the top ZT (we use 100, see Section 6) in any of the
three lists, and patterns that are in the bottom ZB in
at least one of the lists.
5 Concept Discovery
At the end of the previous stage we have a set of
symmetric patterns. We now use them in order to
discover concepts. The concept discovery algorithm
is essentially the same as used by (Davidov and Rap-
poport, 2006) and has some similarity with the one
used by (Widdows and Dorow, 2002). In this section
we outline the algorithm.
The clique-set method. The utilized approach to
concept discovery is based on connectivity struc-
tures in the all-pattern term relationship graph G,
resulting from merging all of the single-pattern
graphs for symmetric patterns selected in the previ-
ous stage. The main observation regarding G is that
highly interconnected words are good candidates to
form a concept. We find all strong n-cliques (sub-
graphs containing n nodes that are all interconnected
in both directions). A clique Q defines a concept that
contains all of the nodes in Q plus all of the nodes
that are (1) at least unidirectionally connected to all
nodes in Q, and (2) bidirectionally connected to at
least one node in Q. Using this definition, we create
a concept for each such clique.
Note that a single term can be assigned to several
concepts. Thus a clique based on a connection of the
word ?Sun? to ?Microsoft? can lead to a concept of
computer companies, while the connection of ?Sun?
to ?Earth? can lead to a concept of celestial bodies.
Reducing noise: merging and windowing. Since
any given term can participate in many cliques, the
algorithm creates overlapping categories, some of
which redundant. In addition, due to the nature of
language and the imperfection of the corpus some
noise is obviously to be expected. We enhance the
quality of the obtained concepts by merging them
and by windowing on the corpus. We merge two
concepts Q,R, iff there is more than a 50% overlap
between them: (|Q?R| > |Q|/2) ? (|Q?R| >
|R|/2). In order to increase concept quality and re-
move concepts that are too context-specific, we use
a simple corpus windowing technique. Instead of
running the algorithm of this section on the whole
corpus, we divide the corpus into windows of equal
size and perform the concept discovery algorithm of
this section (without pattern discovery) on each win-
dow independently. We now have a set of concepts
for each window. For the final set, we select only
those concepts that appear in at least two of the win-
dows. This technique reduces noise at the potential
cost of lowering coverage.
A decrease in the number of windows should pro-
duce more noisy results, while discovering more
concepts and terms. In the next section we show that
while windowing is clearly required for a large cor-
pus, incorporation of parser data increases the qual-
ity of the extracted corpus to the point where win-
dowing can be significantly reduced.
6 Results
In order to estimate the quality of concepts and to
compare it to previous work, we have performed
both automatic and human evaluation. Our basic
comparison was to (Davidov and Rappoport, 2006)
(we have obtained their data and utilized their al-
gorithm), where we can estimate if incorporation of
parser data can solve some fundamental weaknesses
of their framework. In the following description, we
call their algorithm P and our parser-based frame-
work P+. We have also performed an indirect com-
parison to (Widdows and Dorow, 2002).
While there is a significant number of other re-
lated studies4 on concept acquisition (see Section 2),
4Most are supervised and/or use language-specific tools.
52
direct or even indirect comparison to these works is
problematic due to difference in corpora, problem
definitions and evaluation strategies. Below we de-
scribe the corpora and parameters used in our evalu-
ation and then show and discuss WordNet-based and
Human evaluation settings and results.
Corpora. We performed in-depth evaluation in
two languages, English and Russian, using three
corpora, two for English and one for Russian.
The first English corpus is the BNC, containing
about 100M words. The second English corpus,
DMOZ(Gabrilovich and Markovitch, 2005), is a
web corpus obtained by crawling URLs in the Open
Directory Project (dmoz.org), resulting in 68GB
containing about 8.2G words from 50M web pages.
The Russian corpus (Davidov and Rappoport, 2006)
was assembled from web-based Russian reposito-
ries, to yield 33GB and 4G words. All of these cor-
pora were also used by (Davidov and Rappoport,
2006) and BNC was used in similar settings by
(Widdows and Dorow, 2002).
Algorithm parameters. The thresholds
TH , TC , TP , ZT , ZB , were determined mostly
by practical memory size considerations: we com-
puted thresholds that would give us the maximal
number of terms, while enabling the pattern access
table to reside in main memory. The resulting
numbers are 100, 50, 20, 100, 100. Corpus window
size was determined by starting from a small
window size, extracting at random a single window,
running the algorithm, and iterating this process
with increased ?2 window sizes until reaching a
desired vocabulary concept participation percentage
(before windowing) (i.e., x% of the different words
in the corpus participate in terms assigned into
concepts. We used 5%.). We also ran the algorithm
without windowing in order to check how well the
provided parsing information can help reduce noise.
Among the patterns discovered are the ubiquitous
ones containing ?and?,?or?, e.g. ?((X) or (a Y))?,
and additional ones such as ?from (X) to (Y)?.
Influence of parsing data on number of discov-
ered concepts. Table 1 compares the concept ac-
quisition framework with (P+) and without (P) uti-
lization of parsing data.
We can see that the amount of different words
V W C AS
P P+ P P+ P P+
DMOZ 16 330 504 142 130 12.8 16.0
BNC 0.3 25 42 9.6 8.9 10.2 15.6
Russ. 10 235 406 115 96 11.6 15.1
Table 1: Results for concept discovery with (P+) and
without (P) utilization of parsing data. V is the total num-
ber (millions) of different words in the corpus. W is the
number (thousands) of words belonging to at least one of
the terms for one of the concepts. C is the number (thou-
sands) of concepts (after merging and windowing). AS
is the average(words) category size.
covered by discovered concepts raises nearly 1.5-
fold when we utilize patterns based on parsing data
in comparison to pure HFW patterns used in previ-
ous work. We can also see nearly the same increase
in average concept size. At the same time we ob-
serve about 15% reduction in the total number of
discovered concepts.
There are two opposite factors in P+ which may
influence the number of concepts, their size and cov-
erage in comparison to P. On one hand, utilization of
more restricted patterns that include parsing infor-
mation leads to a reduced number of concept term
instances being discovered. Thus, the P+ pattern ?(X
(or (a Y))? will recognize ?(TV (or (a movie))? in-
stance and will miss ?(lunch) or (a snack))?, while
the P pattern ?X or a Y? will capture both. This leads
to a decrease in the number of discovered concepts.
On the other hand, P+ patterns, unlike P ones, al-
low the extraction of multiword expressions5, and
indeed more than third of the discovered terms us-
ing P+ were MWEs. Utilization of MWEs not only
allows to cover a greater amount of different words,
but also increases the number of discovered concepts
since new concepts can be found using cliques of
newly discovered MWEs. From the results, we can
see that for a given concept size and word coverage,
the ability to discover MWEs overcomes the disad-
vantage of ignoring potentially useful concepts.
Human judgment evaluation. Our human judge-
ment evaluation closely followed the protocol (Davi-
dov and Rappoport, 2006).
We used 4 subjects for evaluation of the English
5While P method can potentially be used to extract MWEs,
preliminary experimentation shows that without significant
modification, quality of MWEs obtained by P is very low in
comparison to P+
53
concepts and 4 subjects for Russian ones. In order
to assess subjects? reliability, we also included ran-
dom concepts (see below). The goal of the exper-
iment was to examine the differences between the
P+ and P concept acquisition frameworks. Subjects
were given 50 triplets of words and were asked to
rank them using the following scale: (1) the words
definitely share a significant part of their meaning;
(2) the words have a shared meaning but only in
some context; (3) the words have a shared mean-
ing only under a very unusual context/situation; (4)
the words do not share any meaning; (5) I am not
familiar enough with some/all of the words.
The 50 triplets were obtained as follows. We have
randomly selected 40 concept pairs (C+,C): C+ in
P+ and C in P using five following restrictions: (1)
concepts should contain at least 10 words; (2) for
a selected pair, C+ should share at least half of its
single-word terms with C, and C should share at
least half of its words with C+; (3) C+ should con-
tain at least 3 MWEs; (4) C should contain at least 3
words not appearing in C+; (5) C+ should contain at
least 3 single-word terms not appearing in C.
These restrictions allow to select concept pairs
such that C+ is similar to C while they still carry
enough differences which can be examined. We se-
lected the triplets as following: for pairs (C+, C) ten
triplets include terms appearing in both C+ and C
(Both column in Table 2), ten triplets include single-
word terms appearing in C+ but not C (P+ single
column), ten triplets include single-word terms ap-
pearing in C but not C+ (P column), ten triplets in-
clude MWEs appearing in C+ (P+ mwe column) and
ten triplets include random terms obtained from P+
concepts (Rand column).
P+ P Both Rand
mwe single
% shared
meaning
DMOZ 85 88 68 81 6
BNC 85 90 61 88 0
Russ. 89 95 70 93 11
triplet
score (1-4)
DMOZ 1.7 1.4 2.5 1.7 3.8
BNC 1.6 1.3 2.1 1.5 4.0
Russ. 1.5 1.1 2.0 1.3 3.7
Table 2: Results of evaluation by human judgment of
three data sets. P+ single/mwe: single-word/MWE terms
existing only in P+ concept; P: single-word terms existing
only in P concept; Both: terms existing in both concepts;
Rand: random terms. See text for detailed explanations.
The first part of Table 2 gives the average per-
centage of triplets that were given scores of 1 or 2
(that is, ?significant shared meaning?). The second
part gives the average score of a triplet (1 is best).
In these lines scores of 5 were not counted. Inter-
evaluator Kappa between scores are 0.68/0.75/0.76
for DMOZ, BNC and Russian respectively. We can
see that terms selected by P and skipped by P+
receive low scores, at the same time even single-
word terms selected by P+ and skipped by P show
very high scores. This shows that using parser data,
the proposed framework can successfully avoid se-
lection of erroneous terms, while discovering high-
quality terms missed by P. We can also see that P+
performance on MWEs, while being slightly infe-
rior to the one for single-word terms, still achieves
results comparable to those of single-word terms.
Thus our algorithm can greatly improve the re-
sults not only by discovering of MWEs but also by
improving the set of single word concept terms.
WordNet-based evaluation. The major guideline
in this part of the evaluation was to compare our re-
sults with previous work (Davidov and Rappoport,
2006; Widdows and Dorow, 2002) without the pos-
sible bias of human evaluation. We have followed
their methodology as best as we could, using the
same WordNet (WN) categories and the same cor-
pora. This also allows indirect comparison to several
other studies, thus (Widdows and Dorow, 2002) re-
ports results for an LSA-based clustering algorithm
that are vastly inferior to the pattern-based ones.
The evaluation method is as follows. We took
the exact 10 WN subsets referred to as ?subjects? in
(Widdows and Dorow, 2002), and removed all multi-
word items. We then selected at random 10 pairs of
words from each subject. For each pair, we found
the largest of our discovered concepts containing it.
The various morphological forms or clear typos of
the same word were treated as one in the evaluation.
We have improved the evaluation framework for
Russian by using the Russian WordNet (Gelfenbey-
nand et al, 2003) instead of back-translations as
done in (Davidov and Rappoport, 2006). Prelim-
inary examination shows that this has no apparent
effect on the results.
For each found concept C containing N words,
we computed the following: (1) Precision: the num-
54
ber of words present in both C and WN divided by
N ; (2) Precision*: the number of correct words di-
vided by N . Correct words are either words that
appear in the WN subtree, or words whose entry in
the American Heritage Dictionary or the Britannica
directly defines them as belonging to the given class
(e.g., ?murder? is defined as ?a crime?). This was
done in order to overcome the relative poorness of
WN; (3) Recall: the number of words present in
both C and WN divided by the number of words
in WN; (4) The percentage of correctly discovered
words (according to Precision*) that are not in WN.
Table 3 compares the macro-average of these 10
categories to corresponding related work. We do not
Prec. Prec.* Rec. %New
DMOZ
P 79.8 86.5 22.7 2.5
P+ 79.5 91.3 28.6 3.7
BNC
P 92.76 95.72 7.22 0.4
P+ 93.0 96.1 14.6 1.7
Widdows 82.0 - - -
Russian
P 82.39 89.64 20.03 2.1
P+ 83.5 92.6 29.6 4.0
Table 3: WordNet evaluation in comparison to P (Davi-
dov and Rappoport, 2006) and to Widdows(Widdows and
Dorow, 2002). Columns show average precision, preci-
sion* (as defined in text), recall, and % of new words
added to corresponding WN subtree.
observe apparent rise in precision when comparing
P+ and P, but we can see significant improvement
in both recall and precision* for all of three cor-
pora. In combination with human judgement results,
this suggests that the P+ framework successfully dis-
covers more correct terms not present in WN. This
causes precision to remain constant while precision*
improves significantly. Rise in recall also shows that
the P+ framework can discover significantly more
correct terms from the same data.
Windowing requirement. As discussed in Sec-
tion 5, windowing is required for successful noise
reduction. However, due to the increase in pattern
quality with parser data, it is likely that less noise
will be captured by the discovered patterns. Hence,
windowing could be relaxed allowing to obtain more
data with sufficiently high precision.
In order to test this issue we applied our algo-
rithms on the DMOZ corpus with 3 different win-
dowing settings: (1) choosing window size as de-
scribed above; (2) using ?4 larger window; (3)
avoiding windowing altogether. Each time we ran-
domly sampled a set of 100 concepts and tagged (by
the authors) noisy ones. A concept is considered to
be noisy if it has at least 3 words unrelated to each
other. Table 4 shows results of this test.
Reg. Window ?4 Window No windowing
P 4 18 33
P+ 4 5 21
Table 4: Percentage of noisy concepts as a function of
windowing.
We can see that while windowing is still essential
even with available parser data, using this data we
can significantly reduce windowing requirements,
allowing us to discover more concepts from the
same data.
Timing requirements are modest, considering we
parsed such large amounts of data. BNC pars-
ing took 45 minutes, and the total single-machine
processing time for the 68Gb DMOZ corpus was
4 days6. In comparison, a state-of-art supervised
parser (Charniak and Johnson, 2005) would process
the same amount of data in 1.3 years7.
7 Discussion
We have presented a framework which utilizes an
efficient fully unsupervised parser for unsupervised
pattern-based discovery of concepts. We showed
that utilization of unsupervised parser in pattern ac-
quisition not only allows successful extraction of
MWEs but also improves the quality of obtained
concepts, avoiding noise and adding new terms
missed by the parse-less approach. At the same time,
the framework remains fully unsupervised, allowing
its straightforward application to different languages
as supported by our bilingual evaluation.
This research presents one more step towards the
merging of fully unsupervised techniques for lex-
ical acquisition, allowing to extract semantic data
without strong assumptions on domain or language.
While we have aimed for concept acquisition, the
proposed framework can be also useful for extrac-
tion of different types of lexical relationships, both
among concepts and between concept terms.
6In fact, we used a PC cluster, and all 3 corpora were parsed
in 15 hours.
7Considering the reported parsing rate of 10 sentences per
second
55
References
Mishele Banko, Michael J Cafarella , Stephen Soderland,
Matt Broadhead, Oren Etzioni, 2007. Open Informa-
tion Extraction from the Web. IJCAI ?07.
Rens Bod, 2006a. An All-Subtrees Approach to Unsu-
pervised Parsing. ACL ?06.
Rens Bod, 2006b. Unsupervised Parsing with U-DOP.
CoNLL X.
Rens Bod, 2007. Is the End of Supervised Parsing in
Sight? ACL ?07.
Thorsten Brants, 1997. The NEGRA Export Format.
CLAUS Report, Saarland University.
Sharon Caraballo, 1999. Automatic Construction of a
Hypernym-labeled Noun Hierarchy from Text. ACL
?99.
Eugene Charniak and Mark Johnson, 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. ACL ?05.
Alexander Clark, 2001. Unsupervised Language Acqui-
sition: Theory and Practice. Ph.D. thesis, University
of Sussex.
James R. Curran, Marc Moens, 2002. Improvements in
Automatic Thesaurus Extraction SIGLEX 02?, 59?66.
Dmitry Davidov, Ari Rappoport, 2006. Efficient Un-
supervised Discovery of Word Categories using Sym-
metric Patterns and High Frequency Words. COLING-
ACL ?06.
Dmitry Davidov, Ari Rappoport, Moshe Koppel, 2007.
Fully Unsupervised Discovery of Concept-Specific
Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport, 2009. Translation and
Extension of Concepts Across Languages. EACL ?09.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, Richard Harshman, 1990. Index-
ing by Latent Semantic Analysis. J. of the American
Society for Info. Science, 41(6):391?407.
Simon Dennis, 2005. An exemplar-based approach to
unsupervised parsing. Proceedings of the 27th Con-
ference of the Cognitive Science Society.
Beate Dorow, Dominic Widdows, Katarina Ling, Jean-
Pierre Eckmann, Danilo Sergi, Elisha Moses, 2005.
Using curvature and Markov clustering in Graphs for
Lexical Acquisition and Word Sense Discrimination.
MEANING ?05.
Oren Etzioni, Michael Cafarella, Doug Downey, S. Kok,
Ana-Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel Weld, Alexander Yates, 2005. Unsupervised
Named-entity Extraction from the Web: An Experi-
mental Study. Artificial Intelligence, 165(1):91134.
Evgeniy Gabrilovich, Shaul Markovitch, 2005. Fea-
ture Generation for Text Categorization Using World
Knowledge. IJCAI ?05.
Ilya Gelfenbeyn, Artem Goncharuk, Vladislav Lehelt,
Anton Lipatov, Victor Shilo, 2003. Automatic Trans-
lation of WordNet Semantic Network to Russian Lan-
guage (in Russian) International Dialog 2003 Work-
shop.
James Gorman, James R. Curran, 2006. Scaling Distri-
butional Similarity to Large Corpora. COLING-ACL
?06.
William P. Headden III, David McClosky and Eugene
Charniak, 2008. Evaluating Unsupervised Part-of-
Speech tagging for Grammar Induction. COLING ?08.
Marti Hearst, 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. COLING ?92.
Adam Kilgarriff, 2007. Googleology is Bad Science.
Computational Linguistics ?08, Vol.33 No. 1,pp147-
151. .
Dan Klein and Christopher Manning, 2002. A genera-
tive constituent-context model for improved grammar
induction. Proc. of the 40th Meeting of the ACL.
Dan Klein and Christopher Manning, 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. ACL ?04.
Dan Klein, 2005. The unsupervised learning of natural
language structure. Ph.D. thesis, Stanford University.
Hang Li, Naoki Abe, 1996. Clustering Words with the
MDL Principle. COLING ?96.
Dekang Lin, 1998. Automatic Retrieval and Clustering
of Similar Words. COLING ?98.
Marcus Mitchell P., Beatrice Santorini and Mary Ann
Marcinkiewicz, 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330
Marius Pasca, Benjamin Van Durme, 2008. Weakly-
supervised Acquisition of Open-domain Classes and
Class Attributes from Web Documents and Query
Logs. ACL 08.
Patrick Pantel, Dekang Lin, 2002. Discovering Word
Senses from Text. SIGKDD ?02.
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards Terascale Knowledge Acquisition.
COLING ?04.
Fernando Pereira, Naftali Tishby, Lillian Lee, 1993. Dis-
tributional Clustering of English Words. ACL ?93.
Hinrich Schu?tze, 1998. Automatic Word Sense Discrim-
ination. Computational Linguistics , 24(1):97?123.
Yoav Seginer, 2007. Fast Unsupervised Incremental
Parsing. ACL ?07.
Noah A. Smith and Jason Eisner, 2006. Annealing Struc-
tural Bias in Multilingual Weighted Grammar Induc-
tion . ACL ?06.
Dominic Widdows, Beate Dorow, 2002. A Graph Model
for Unsupervised Lexical Acquisition. COLING ?02.
56
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 241?249,
Beijing, August 2010
Automated Translation of Semantic Relationships
Dmitry Davidov
ICNC
Hebrew University of Jerusalem
Ari Rappoport
Institute of Computer Science
Hebrew University of Jerusalem
arir@cs.huji.ac.il
Abstract
We present a method for translating se-
mantic relationships between languages
where relationships are defined as pattern
clusters. Given a pattern set which rep-
resents a semantic relationship, we use
the web to extract sample term pairs of
this relationship. We automatically trans-
late the obtained term pairs using multi-
lingual dictionaries and disambiguate the
translated pairs using web counts. Finally
we discover the set of most relevant tar-
get language patterns for the given rela-
tionship. The obtained pattern set can be
utilized for extraction of new relationship
examples for the target language.
We evaluate our method on 11 diverse tar-
get languages. To assess the quality of
the discovered relationships, we use an au-
tomatically generated cross-lingual SAT
analogy test, WordNet relationships, and
concept-specific relationships, achieving
high precision. The proposed framework
allows fully automated cross-lingual rela-
tionship mining and construction of mul-
tilingual pattern dictionaries without rely-
ing on parallel corpora.
1 Introduction
Acquiring and understanding semantic relation-
ships is crucial for many NLP applications. In
many cases, we would like to know if a given
term pair participates in a specified semantic re-
lationship or if two different term pairs encode
the same (possibly unspecified) type of relation-
ship. Beyond the well-known major relationship
types such as hyponymy (is-a) and meronymy
(part-of), there is a huge number of other rela-
tionships between objects and concepts. Exam-
ples include general relations such as larger-than,
contained-in, liked-by and domain specific ones
such as country-language, product-manufacturer,
product-seller, drug-disease etc.
The vast majority of NLP research is done in
a few languages for which extensive corpora (in-
cluding the web) are available. As a result, most
relationship retrieval studies and lexical database
compilation efforts target only a few languages.
However, due to the substantial growth of the mul-
tilingual web1 and a growing demand for NLP
application coverage for less common languages,
there is a need for relationship data in many less
studied languages.
In this paper we address the task of translating
relationships between languages, which has two
obvious benefits. First, it can directly help appli-
cations such as machine translation, cross-lingual
information retrieval, cross-lingual web mining
and the construction and enrichment of seman-
tic databases. Second, it can assist applications
in a single language, especially when compensat-
ing for a relative scarcity of resources in that lan-
guage. We focus on relations between two enti-
ties, which are the most common type.
When discussing the translation of relation-
ships, it is important to define how these are rep-
resented and in what way the task differs from
MT. While there is wide agreement on the def-
inition and representation of major relationship
types such as hypernymy and (to a lesser extent)
meronymy, there is no single accepted method (or
1http://www.internetworldstats.com/stats7.htm
241
resources) for other less common relationships.
Among the methods that have been proposed for
specifying lexical relationships are natural lan-
guage description and rules (Girju et al, 2007),
distributional means (Turney, 2005), sample term
pairs (Pasca et al 2006), relationship instances
(Banko et al, 2007) and pattern clusters (Davi-
dov and Rappoport, 2008a).
In this paper we utilize the last definition. Fol-
lowing (Davidov and Rappoport, 2008a) each se-
mantic relationship can be defined and repre-
sented by a set of lexical patterns such that the
represented relation holds between entities filling
the patterns? slots. We focus on pattern clusters re-
lationship definition due to several reasons. First,
as opposed to natural language descriptions, pat-
tern clusters are formal. Second, as opposed to
the other methods above, pattern clusters provide
a ?generative? model for the represented relation-
ship ? it is possible to obtain from them relation-
ship instances and term pairs, as we indeed uti-
lize in this paper. Third, pattern clusters can be
mined in a fully unsupervised manner, or in a
focused manner when the relationship desired is
known. Finally, pattern methods have proven to
be highly efficient and effective for lexical acqui-
sition tasks (Pantel et al 2004; Davidov and Rap-
poport, 2006).
The proposed framework comprises the follow-
ing stages. First, given a set of patterns defining a
relationship in a source language, we obtain from
the web a set of corresponding term pairs. Next,
for each of the terms in the obtained term pairs,
we retrieve sets of their translations to the target
language using available multilingual dictionar-
ies. Now that we have a set of translations for
each term in each pair, we retrieve search engine
snippets with the translated term pairs. We then
select appropriate word senses using web counts,
and extract a set of patterns which connect these
disambiguated terms. As a result we get a set
of relation-specific target language patterns, ef-
fectively obtaining the desired relationship defi-
nition. We can optionally use the retrieved pattern
sets to obtain term pairs of target language rela-
tionships from the web.
We performed a thorough evaluation for var-
ious relationships involving 11 languages. We
tested our framework on major relationships like
meronymy, specific relationships like country-
capital and unspecified unsupervisedly discovered
English relationships. The obtained relationships
were manually verified by human judges using
cross-lingual SAT analogy questions, and a few
specific factual relationships were evaluated using
a gold standard.
Our main contribution is a novel framework
for automated relationship translation across lan-
guages, where relationships are defined as pattern
clusters or as term pairs. This framework allows
fully automated cross-lingual relationship mining
and construction of multilingual pattern dictionar-
ies without relying on parallel corpora.
In Section 2 we discuss related work. Section 3
details the algorithm. Section 4 describes the eval-
uation, and Section 5 concludes.
2 Related work
Recently, with the development of practical appli-
cations which utilize WN-like databases in dozens
of languages, great effort has been made to manu-
ally construct and interconnect such databases for
different languages (Pease et al 2008; Charoen-
porn et al, 2007). Some studies (e.g., (Amasyali,
2005)) use semi-automated methods based on
language-specific heuristics and dictionaries.
At the same time, much work has been done
on automated lexical acquisition for a single lan-
guage, and in particular, on the web-based ac-
quisition of various types of semantic relation-
ships. There is a substantial amount of related
studies which deal with the discovery of vari-
ous relationship types represented in useful re-
sources such as WordNet, including hypernymy
(Pantel et al 2004; Snow et al, 2006), synonymy
(Davidov and Rappoport, 2006; Widdows and
Dorow, 2002) and meronymy (Berland and Char-
niak, 1999; Girju et al 2006). Since named
entities are very important in NLP, many studies
define and discover relations between named en-
tities (Hassan et al, 2006). Work was also done
on relations between verbs (Chklovski and Pan-
tel, 2004). There is growing research on relations
between nominals (Girju et al, 2007).
While the majority of studies focus on extract-
ing pre-specified semantic relationships, several
242
recent studies were done on the automated discov-
ery of unspecified relationship types. Thus Tur-
ney (2006) provided a pattern distance measure
that allows a fully unsupervised measurement of
relational similarity between two pairs of words
on the same language. Banko et al (2007) and
Rosenfeld and Feldman (2007) find relationship
instances where the relationships are not speci-
fied in advance. (Davidov and Rappoport, 2008a)
introduced the idea that salient semantic relation-
ships can be defined as pattern clusters, confirm-
ing it with SAT analogy test. As explained above,
we use this definition in the present study. We
also use pattern clusters given by (Davidov and
Rappoport, 2008a) as input in our evaluation.
Most of the relationship acquisition studies
were done in a single language. Those that ex-
periment in several languages usually treat each
language separately, while we extract a relation-
ship definition for one language using the pro-
vided definition for the other language.
Our study is related to cross-language infor-
mation retrieval (CLIR) frameworks. Both deal
with multilingual information extracted from the
Web. However, the majority of CLIR stud-
ies pursue different targets. Thus, one of the
main CLIR goals is the retrieval of documents
based on explicit queries, when the document
language is not the query language (Volk and
Buitelaar, 2002). These frameworks usually de-
velop language-specific tools and algorithms in-
cluding parsers, taggers and morphology analyz-
ers in order to integrate multilingual queries and
documents (Jagarlamudi and Kumaran, 2007).
Our goal is to develop and evaluate a language-
independent algorithm for the cross-lingual trans-
lation of relationship-defining structures. While
our targets are different from those of CLIR, CLIR
systems can greatly benefit from our framework,
since we can translate the relationships in CLIR
queries and subsequently check if the same rela-
tionships are present in the retrieved documents.
Another field indirectly related to our research
is Machine translation (MT). Many MT tasks re-
quire automated creation or improvement of dic-
tionaries (Koehn and Knight, 2001). However,
MT mainly deals with translation and disambigua-
tion of words at the sentence or document level,
while we translate relationship structures as a set
of patterns, defined independently of contexts.
We also perform pattern-set to pattern-set trans-
lation rather than the pattern-to-pattern or pair-to-
pair translation commonly explored in MT stud-
ies. This makes it difficult to perform meaning-
ful comparison to existing MT frameworks. How-
ever, the MT studies benefit from the proposed
framework by enhancement and verification of
translated relationship instances.
In (Davidov and Rappoport, 2009), we pro-
posed a framework for automated cross-lingual
concept mining. We incorporate several princi-
ples from this study including concept extension
and disambiguation of query language (See Sec-
tion 3.3). However our goals here are different
since we target cross-lingual acquisition of rela-
tionship structures rather then concept term lists.
3 Relationship Translation Framework
Our framework has the following stages: (1) given
a set of patterns in a source language defining
some lexical relationship, we use the web to ob-
tain source language term pairs participating in
this relationship; (2) we automatically translate
the obtained terms in each pair to the target lan-
guage using available multilingual dictionaries;
(3) we retrieve web snippets where these transla-
tions co-appear, disambiguating translations with
web counts and extracting the corresponding pat-
terns. As an optional final stage, the translated
pattern cluster can be used to extract and extend
a set of target language term pairs. Now we de-
scribe each of these stages in detail.
3.1 Acquisition of representative term pairs
We are provided with a pattern cluster, a set of pat-
terns representing a specific lexical relationship in
some language. The goal of the first stage is to
discover the most representative term pairs for this
cluster and language from the web. If the relation-
ship is already specified by a representative set of
term pairs, we skip this stage and continue to the
next stage. Note that the method described be-
low can also be used at the final stage to obtain
representative target language term pairs once we
obtain a target language pattern cluster.
The input lexical patterns are surface patterns
243
which include several fixed words or punctuation
symbols and two slots for content words, e.g. ?the
[X] of the [Y],?. Given a cluster of patterns defin-
ing a semantic relationship, we would like to ob-
tain from the web the most representative and fre-
quent examples of the represented relationship.
In order to do that we construct search engine
queries2 from the given patterns using wildcard
symbols to represent pattern slots. For example,
given a pattern ?the [X] of the [Y],? we construct
queries such as ?the * of the?; ?the * * of the?3.
We collect all the retrieved search engine snip-
pets and extract the appropriate term pairs found
in these snippets.
Now we would like to select the most useful of
the extracted pairs. Since the obtained pairs are
only useful if we can translate them into the tar-
get language, we dismiss all pairs in which one or
both terms have no translations to the target lan-
guage in our dictionaries (see Section 3.2). Since
each particular pattern can be ambiguous, we also
dismiss pairs which were found for only a single
pattern in the given cluster.
For the remaining term pairs we would like
to estimate their specificity for the given pattern
cluster. For each pattern, we retrieve and use two
web hit counts: Fterms(p, T1, T2), a hit count for
co-appearance of the pair in a way similar to that
in the pattern, and Fall(p, T1, T2), the hit count
of the full pattern instance.
For example, if for the pattern p=?the * of
the? we obtain a term pair (CEO, company), then
Fall(p)=Hits(?the CEO of the company?) and
Fterms(CEO, company)= Hits(?CEO * * com-
pany?). Given a pattern cluster C with patterns
{p1 . . . pn} ? C, we estimate the specificity of
a term pair (T1, T2) using the following simple
probabilistic metric, giving to all patterns in the
cluster an equal weight:
Spec(T1, T2) = 1n
?
pi?C
Fall(pi, T1, T2)
Fterms(pi, T1, T2)
We select the top 15 pairs with the highest speci-
ficity and use them in the next stage.
2We use Yahoo! Boss.
3Since the search engine API doesn?t allow punctuation,
we omit the punctuation in queries, but require a proper
punctuation when processing the obtained snippet data.
3.2 Translation of the term pairs
After the previous stage we have a good represen-
tative set of term pairs for the desired source lan-
guage relationship. Now we would like to trans-
late the words in these pairs to the target language.
In order to do that we use an extensive set of
1067 multilingual dictionaries developed for Star-
Dict4, including Wikipedia cross-language links
and Wiktionary. For each term we obtain a set
of its translations to the target language. If we
get more than five different translations, we select
the five having the highest number of dictionaries
where this translation appears.
As discussed in Section 3.1, we dismissed
terms for which no translation was found in any of
the available dictionaries, so each term in each of
the obtained pairs has at least a single translation
to the target language. However, in many cases the
available translations represent the wrong word
sense, since both the source terms and their trans-
lations can be ambiguous. Thus at this stage many
of the obtained term translations are irrelevant for
the given relationship and require disambiguation.
3.3 Web mining for translation contexts
For this stage, we need to restrict web mining
to specific target languages. This restriction is
straightforward if the alphabet or term translations
are language-specific or if the search API supports
restriction to this language. In case where there is
no such natural restrictions, we attempt to detect
and add to our queries a few language-specific fre-
quent words. Following (Davidov and Rappoport,
2009), we use our dictionaries to find 1?3 of the
15 most frequent words in a desired language5 that
are unique to that language and ?and? them with
the queries to ensure proper language selection.
This allows applying our algorithm to more than
60 diverse languages. The only data required for
each language is at least a partial coverage of the
obtained term pairs by some available dictionary.
Given a term pair (T1, T2) we obtain a set
of translations (T1?i?1...n, T2?j?1...m). For each
combination T1?i, T2?j of the obtained term trans-
lations, we construct and execute the following
4http://stardict.sourceforge.net/
5We estimated the word frequencies from text available
in the corresponding multilingual dictionaries.
244
four queries: {?T1?i ? T2?j?, ?T2?j ? T1?i?,
?T1?i ? ? T2?j?, ?T2?j ? ? T1?i?}6. Since
Y ahoo!Boss allows retrieval of up to the 1000
first results, we can collect up to four thousand
snippets for each combination. However, the ma-
jority of these combinations return no snippets at
all, effectively generating an average of a dozen
snippets per query.
3.4 Pattern extraction
Now for each pair of term translations we would
like to extract from the snippets all surface pat-
terns which connect the terms in this pair. We use
the basic two-slot meta-pattern type:
[Prefix] X [Infix] Y [Postfix]
X and Y should be the translated terms, Infix may
contain punctuation, spaces, and up to four words
(or up to eight symbols in languages without
space-separated words like Chinese). Prefix and
Postfix are limited to contain one or zero punctu-
ation characters and/or up to two words. We do
not allow empty Infix, Prefix of Postfix. If there
are several possible combinations of Prefix and
Postfix we generate a pattern set for all possible
combinations (e.g., if we retrieve a snippet . . . ?,
consider using [plexiglass] for [kitchen].?. . . , we
create patterns ?using X for Y.?, ?consider using
X for Y.? and ?, consider using X for Y.?).
Now we would like to find the patterns repre-
senting the relationship in the target language. We
do this in two stages. First we would like to detect
the most common patterns for the given relation-
ship. Let Sk be the union set of all patterns ob-
tained for all combinations of the extracted trans-
lations for a specific source language term pair
k ? 1 . . .K. Let Salience(p) = 1K |{k|p ? Sk}|
be the portion of source language term pairs which
lead to detection of the target language pattern
p. We compute salience for each pattern, and
select a subset of salient patterns, defined to be
those whose Salience exceeds a predefined thresh-
old (we used 1/3). If one salient pattern is a sub-
string of another salient pattern, we only select the
longer one.
6These are Yahoo! queries where enclosing words in ??
means searching for an exact phrase and ?*? means a wild-
card for exactly one arbitrary word.
In our salience estimation we mix data from
all combinations of translations including incor-
rect senses and wrong translations of ambiguous
terms. Now we would like to select a single cor-
rect target language pair for each source language
pair in order to find more refined relationship rep-
resenting patterns. For each source language term
pair, we select the target language translated pair
which captured the highest number of salient pat-
terns. In case there are several pairs with the same
number of salient patterns, we select a pair with
the greatest web hit count. We drop term pairs
with zero salient patterns.
Finally we would like to enhance the obtained
set of salient patterns with more precise and rep-
resentative relationship-specific patterns. Since
we disambiguated the translated pairs, target lan-
guage patterns captured by the remaining term
pairs should be more trusted. We compare the
target language pattern sets obtained for differ-
ent remaining term pairs, and collect all patterns
that were captured by at least three different term
pairs. As before, if one pattern is a substring of
another we retain only the longer one. As a result
we get a comprehensive target language pattern
cluster for the desired relationship.
3.5 Retrieval of target language term pairs
As an optional final stage, we can utilize the re-
trieved target language pattern clusters in order to
discover target language term pairs for the desired
relationship. We do this by utilizing the strategy
described in Section 3.1 on the obtained target
language pattern clusters. We do not dismiss ob-
tained terms having no available dictionary trans-
lations, and we do not limit our search to the 15
terms with highest specificity. Instead we either
select N term pairs with top specificity (where N
is provided by user as in our evaluation), or we
select all term pairs with specificity above some
threshold.
4 Evaluation
In order to test the quality of the translated pat-
tern clusters and the corresponding translated term
pairs, we need to check both flexibility and cor-
rectness. Flexibility measures how well the re-
trieval works well across languages and for many
245
types of semantic relationships. To do that, we
tested our framework on both generic and specific
relationships for 11 languages. Correctness ver-
ifies that the retrieved set of target language pat-
terns and the corresponding term pairs represent
the same semantic relationship as the given set
of source language term pairs or patterns. To do
that, we used both manual cross-lingual analogy-
based correctness evaluation and evaluation based
of factual data.
4.1 Languages and relationships
One of the main goals in this research was to pro-
vide a fully automated and flexible framework,
which requires minimal modifications when ap-
plied to different languages and relationships.
We examined an extensive set of target lan-
guages using English as a source language. Ta-
ble 1 shows 11 languages used in our experiments.
We included west European languages, Slavic lan-
guages like Russian, Semitic languages like He-
brew, and Asian languages such as Chinese. We
developed a set of tools for automatic off-line ac-
cess to an extensive set of 1067 multilingual dic-
tionaries created for the StarDict platform. These
dictionaries include recent dumps of Wikipedia
cross-language links and Wiktionary data.
In our experiments we used three sets of rela-
tionships: (1) Generic: 15 unsupervisedly dis-
covered English pattern clusters representing var-
ious generic relationships. (2) H-M-C: The
three most studied relationships: hypernymy,
meronymy and co-hyponymy.(3) Specific: Three
factual relationships: country-capital, country-
language and dog breed-origin. Below we de-
scribe the evaluation of each of these sets in de-
tail. Note that our framework allows two ways of
specifying a source language relationship ? a pat-
tern cluster and a set of term pairs.
4.2 Evaluation of generic pattern clusters
In our Generic evaluation setting, we utilized as
input a random sample of 15 automatically dis-
covered relationship definitions. We started from
a set of 508 English pattern clusters, unsuper-
visedly discovered using the method of (Davidov
and Rappoport, 2008a). Each of these clusters
is assumed to represent a distinct semantic rela-
tionship. We randomly selected 15 pattern clus-
ters from this set and executed our framework on
these clusters to obtain the corresponding target
language pattern clusters for each of the 11 tested
languages. An example of a partial set of patterns
in a cluster is: ?this [X] was kept in [Y],?;?the X that he
kept in [Y],?;?the [X] in the [Y] and?;?the [Y] containing
the [X]?. . . .
We then used the term pair selection algorithm
described in Section 3.1 to select the most spe-
cific term pair for each of the 15 source language
clusters and 10 pairs for each of the corresponding
translated target language clusters. Thus for each
of the 15 pattern clusters and for each of the 11
languages we produced a single source language
term pair and up to 10 corresponding target lan-
guage term pairs.
In order to check the correctness of transla-
tion of an unspecified semantic relationship we
need to compare source and target language rela-
tionships. Comparison of relationships is a chal-
lenging task, since there are no relationship re-
sources for most relationship types even in a sin-
gle language, and certainly so for their trans-
lations across languages. Thus various studies
define and split generic relationships differently
even when describing relatively restricted rela-
tionship domains (e.g., relationships holding be-
tween parts of noun phrases (Nastase and Sz-
pakowicz, 2003; Moldovan et al, 2004)). In order
to compare generic relationships we used a man-
ual cross-lingual SAT-like analogy human judg-
ment evaluation7. This allowed us to assess the
quality of the translated pattern clusters, in a sim-
ilar way as (Davidov and Rappoport, 2008a) did
for testing clusters in a single language.
For each of the 15 clusters we constructed a
cross-lingual analogy question in the following
manner. The header of the question was a term
pair obtained for the source language pattern clus-
ter. The six multiple choice items included: (1)
one of the 10 discovered translated term pairs of
the same cluster (the ?correct? answer)8; (2) three
7Using Amazon?s Mechanical Turk.
8We avoid selection of the target language pairs which
were obtained through direct translation of the source lan-
guage pair given at the header of the question. This is crucial
so that subjects will not judge correctness of translation but
correctness of the relationship.
246
of the translated pairs of the other clusters among
the 15; (3) a pair constructed by randomly select-
ing terms from different translated clusters; (4) the
6th option states that either the given options in-
clude broken words or incorrect language, or none
of the presented pairs even remotely exemplifies
the relationship in question. An example question
for English-Italian:
The English pair: (kennel, dog); (1) ?correct? pair: (ac-
quario, pesce ); (2)-(4) ?wrong? pairs: (topo, orecchio),
(mela, rossa), (occhio, grande); (5) ?random?: (scodella,
scatola); (6) Pairs comprise non-Italian/broken words or no
pair exemplifies the relationship
In order to check the English proficiency of the
subjects we added 5 ?easy? monolingual English
SAT analogy questions. We also added a single
hand-crafted cross-lingual question of an obvious
analogy case, making a total of 16 cross-lingual
questions. Subjects who failed more than one of
the easy English SAT questions or failed the obvi-
ous cross-lingual question were rejected from the
evaluation. Finally we have three subjects for each
of the tested languages. We also asked the sub-
jects to assign a confidence score from 0 (worst)
to 10 (best) to express how well the selected term
pair represents the source language relationship in
question.
Language P % 6th Scorec Scorew
Chinese 71 9 9.1 1.8
Czech 73 9 8.3 2.0
French 80 10 8.4 1.9
German 68 9 8.3 1.5
Greek 72 11 8.7 2.0
Hebrew 69 11 9.0 2.5
Hindi 62 12 7.4 1.9
Italian 70 10 8.5 1.5
Russian 75 8 9.0 1.6
Turkish 61 13 9.1 2.0
Ukrainian 73 11 9.3 2.3
Average 70 10 9.1 1.9
Table 1: Averaged results for manual evaluation of 15 pat-
tern clusters. P: precision (% of correct answers); % 6th: per-
centage of 6th selection; Scorec: averaged confidence score
for correct selections; Scorew: confidence score for wrong
selections.
We computed accuracy and agreement for the
given answers (Table 1). We can see that for all
languages above 61% of the choices were cor-
rect (comparing to 75% reported by (Davidov
and Rappoport, 2008a) for a similar monolingual
analogy test for the same set of pattern clusters).
While the results are obviously lower than the cor-
responding single-language test, they are signifi-
cantly above the random baseline of 20%9. Also
note that as reported in (Turney, 2006), an aver-
age single-language highschool SAT grade is 57,
which is lower than the scores obtained for our
cross-lingual test. We can also see that for the cor-
rectly selected pairs the confidence score was very
high, while the score for wrongly selected pairs
was significantly lower.
4.3 Evaluation of the H-M-C relationships
In order to test how well our algorithm performs
on the most common and useful relationships, hy-
pernymy, meronymy and co-hyponymy, we au-
tomatically sampled from WordNet a set of 10
source language term pairs for each of these re-
lationships and applied our framework to extract
up to 100 target language term pairs for each of
the three relationships as done above.
For each of the tested languages we presented
to three human subjects for each language a short
English definition of hypernymy, meronymy and
co-hyponymy, along with the corresponding ran-
domly selected 10 of 100 extracted pairs, and
asked them to rank how well (0 (worst) to 10
(best)) each pair represents the described relation-
ship. In order to reduce possible bias, we mixed in
each set 3 randomly selected term pairs obtained
for the other two relationships. Table 2 shows the
average scores for this task.
Language Hypernymy Meronymy Co-hyponymy Random
Chinese 8.0 7.1 8.1 1.9
Czech 8.4 7.0 8.5 2.3
French 8.1 7.5 8.4 1.8
German 8.4 7.1 8.6 2.4
Greek 8.7 7.5 8.6 1.8
Hebrew 8.6 7.9 8.3 1.6
Hindi 7.5 7.1 7.8 2.2
Italian 7.9 7.8 8.2 1.5
Russian 8.6 8.1 8.9 1.7
Turkish 8.3 7.2 8.6 1.7
Ukrainian 8.2 7.7 8.2 1.7
Average 8.3 7.5 8.4 1.9
Table 2: Averaged results for hypernymy, meronymy and
co-hyponymy translations. The three first columns show av-
erage scores for hypernymy, meronymy and co-hyponymy
relationships. The last column shows scores for the random
baseline.
We can see that our algorithm successfully de-
tects the common relationships, achieving high
scores. Also the results indicate that the patterns
9A reasonable random baseline omits the 6th option.
247
are sufficiently precise to extract at least 100 of
the instances for the given salient relationships.
4.4 Evaluation of the specific relationships
To check how well our algorithm performs on
some specific relationships, we examined its per-
formance on three specific relationships explored
in previous studies. We provided it with 10 source
language (English) term pair examples for each
of the (country, capital), (country, language) and
(dog breed, origin) relationships. For each of
these relationships we have factual information
for every tested target language available through
Wikipedia list articles. This allows us to perform
an unbiased automated evaluation of the quality of
the obtained target language data.
We applied our framework on these examples
and generated 30 target language pairs with high-
est specificity for each of these relationships and
languages. We compared the retrieved pairs to the
factual data. Table 3 shows the precision of the
results obtained for these patterns.
Language Capital Language Dog breed
Chinese 0.87 0.83 0.8
Czech 0.93 0.83 0.77
French 0.97 0.9 0.87
German 0.93 0.9 0.83
Greek 0.87 0.83 0.77
Hebrew 0.83 0.8 0.8
Hindi 0.83 0.8 0.77
Italian 0.93 0.87 0.83
Russian 0.97 0.9 0.87
Turkish 0.87 0.83 0.83
Ukrainian 0.93 0.87 0.8
Average 0.9 0.85 0.81
Table 3: Precision for three specific relationship
types: (country, capital), (country, language) and (dog
breed,origin).
The precision observed for this task is compara-
ble to precision obtained for Country-Capital and
Country-Language in a previous single-language
acquisition study (Davidov et al, 2007)10. The
high precision observed for this task indicates that
the obtained translated patterns are sufficiently
good as a seed for pattern-based mining of spe-
cific relationships.
10It should be noted however that unlike previous work,
we only examine the first 30 pairs and we do not use addi-
tional disambiguating words as input.
5 Conclusion
We proposed a framework which given a set of
patterns defining a semantic relationship in a spe-
cific source language uses multilingual dictionar-
ies and the web to discover a corresponding pat-
tern cluster for a target language. In the evaluation
we confirmed the applicability of our method for
different languages and relationships.
The obtained set of target language pattern clus-
ters can be used for acquisition of relationship in-
stances as shown in our evaluation. An interest-
ing direction for future work is to use the discov-
ered target language pattern clusters in NLP tasks
like textual entailment which require distinguish-
ing between semantic relationships.
Applying our framework to the set of unsuper-
visedly discovered relationships allows a fully au-
tomated construction of a relationship dictionary,
where pattern clusters in one language correspond
to patten clusters in many other languages. Un-
like the majority of existing machine translation
systems, construction of this dictionary does not
require parallel corpora. Such a dictionary can be
useful for machine translation, cross-lingual tex-
tual entailment and query translation, to name just
a few applications. In the future we plan to create
a multilingual pattern cluster dictionary which in-
terconnects pattern clusters from many languages
and allows cross-lingual definition of lexical rela-
tionships.
References
Amasyali Fatih, 2005. Automatic Construction of
Turkish Wordnet. Signal Processing and Commu-
nications Applications Conference.
Mishele Banko, Michael Cafarella , Stephen Soder-
land, Matt Broadhead, Oren Etzioni, 2007. Open
information extraction from the Web. IJCAI ?07.
Matthew Berland, Eugene Charniak, 1999. Finding
parts in very large corpora. ACL ?99.
Thatsanee Charoenporn, Virach Sornlertlamvanich,
Chumpol Mokarat, and Hitoshi Isahara, 2008.
Semi-automatic Compilation of Asian WordNet.
Proceedings of the 14th NLP-2008, University of
Tokyo, Komaba Campus, Japan.
Timothy Chklovski, Patrick Pantel, 2004. VerbOcean:
248
mining the web for fine-grained semantic verb rela-
tions. EMNLP ?04.
Dmitry Davidov, Ari Rappoport, 2006. Effi-
cient unsupervised discovery of word categories us-
ing symmetric patterns and high frequency words.
COLING-ACL ?06.
Dmitry Davidov, Ari Rappoport and Moshe Koppel,
2007. Fully Unsupervised Discovery of Concept-
Specific Relationships by Web Mining. ACL ?07.
Dmitry Davidov, Ari Rappoport. 2008a. Unsuper-
vised Discovery of Generic Relationships Using
Pattern Clusters and its Evaluation by Automatically
Generated SAT Analogy Questions. ACL ?08.
Dmitry Davidov and Ari Rappoport, 2008b. Classifi-
cation of relationships between nominals using pat-
tern clusters. ACL ?08.
Dmitry Davidov and Ari Rappoport, 2009. Transla-
tion and Extension of Concepts Across Languages.
EACL ?09.
Roxana Girju, Adriana Badulescu, and Dan Moldovan,
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1).
Roxana Girju, Marthy Hearst, Preslav Nakov, Vivi
Nastase, Stan Szpakowicz, Peter Turney and Yuret,
D., 2007. Task 04: Classification of semantic re-
lations between nominal at SemEval 2007. 4th Intl.
Workshop on Semantic Evaluations (SemEval ?07),
in ACL ?07.
Hany Hassan, Ahmed Hassan and Ossama Emam,
2006. Unsupervised information extraction ap-
proach using graph mutual reinforcement. EMNLP
?06.
Jagadeesh Jagarlamudi, A Kumaran, 2007 Cross-
Lingual Information Retrieval System for Indian
Languages Working Notes for the CLEF 2007
Workshop.
Philipp Koehn and Kevin Knight. 2001. Knowledge
sources for word-level translation models. EMNLP
?01.
Dan Moldovan, Adriana Badulescu, Marta Tatu,
Daniel Antohe, and Roxana Girju, 2004. Mod-
els for the semantic classification of noun phrases.
HLT-NAACL ?04 Workshop on Computational Lexi-
cal Semantics.
Vivi Nastase, Stan Szpakowicz, 2003. Exploring
noun-modifier semantic relations. In Fifth Intl.
Workshop on Computational Semantics (IWCS-5).
Patrick Pantel, Deepak Ravichandran, Eduard Hovy,
2004. Towards terascale knowledge acquisition.
COLING ?04.
Marius Pasca, Dekang Lin, Jeffrey Bigham, Andrei
Lifchits, Alpa Jain, 2006. Names and similari-
ties on the web: fact extraction in the fast lane.
COLING-ACL 06.
Adam Pease, Christiane Fellbaum, Piek Vossen, 2008.
Building the Global WordNet Grid. CIL18.
Benjamin Rosenfeld , Ronen Feldman, 2007. Cluster-
ing for unsupervised relation identification. CIKM
?07.
Rion Snow, Daniel Jurafsky, Andrew Ng, 2006. Se-
mantic taxonomy induction from heterogeneous ev-
idence. COLING-ACL ?06.
Peter Turney, 2005. Measuring semantic similarity by
latent relational analysis, IJCAI ?05.
Peter Turney, 2006. Expressing implicit semantic re-
lations without supervision. COLING-ACL ?06.
Martin Volk, Paul Buitelaar, 2002 A Systematic Eval-
uation of Concept-Based Cross-Language Informa-
tion Retrieval in the Medical Domain. In: Proc. of
3rd Dutch-Belgian Information Retrieval Workshop.
Leuven.
Dominic Widdows, Beate Dorow, 2002. A graph
model for unsupervised Lexical acquisition. COL-
ING ?02.
249
Coling 2010: Poster Volume, pages 241?249,
Beijing, August 2010
Enhanced Sentiment Learning Using Twitter Hashtags and Smileys
Dmitry Davidov? 1 Oren Tsur? 2
1ICNC / 2Institute of Computer Science
The Hebrew University
{oren,arir}@cs.huji.ac.il
Ari Rappoport 2
Abstract
Automated identification of diverse sen-
timent types can be beneficial for many
NLP systems such as review summariza-
tion and public media analysis. In some of
these systems there is an option of assign-
ing a sentiment value to a single sentence
or a very short text.
In this paper we propose a supervised
sentiment classification framework which
is based on data from Twitter, a popu-
lar microblogging service. By utilizing
50 Twitter tags and 15 smileys as sen-
timent labels, this framework avoids the
need for labor intensive manual annota-
tion, allowing identification and classifi-
cation of diverse sentiment types of short
texts. We evaluate the contribution of dif-
ferent feature types for sentiment classifi-
cation and show that our framework suc-
cessfully identifies sentiment types of un-
tagged sentences. The quality of the senti-
ment identification was also confirmed by
human judges. We also explore dependen-
cies and overlap between different sen-
timent types represented by smileys and
Twitter hashtags.
1 Introduction
A huge amount of social media including news,
forums, product reviews and blogs contain nu-
merous sentiment-based sentences. Sentiment is
defined as ?a personal belief or judgment that
?* Both authors equally contributed to this paper.
is not founded on proof or certainty?1. Senti-
ment expressions may describe the mood of the
writer (happy/sad/bored/grateful/...) or the opin-
ion of the writer towards some specific entity (X
is great/I hate X, etc.).
Automated identification of diverse sentiment
types can be beneficial for many NLP sys-
tems such as review summarization systems, dia-
logue systems and public media analysis systems.
Sometimes it is directly requested by the user to
obtain articles or sentences with a certain senti-
ment value (e.g Give me all positive reviews of
product X/ Show me articles which explain why
movie X is boring). In some other cases obtaining
sentiment value can greatly enhance information
extraction tasks like review summarization. While
the majority of existing sentiment extraction sys-
tems focus on polarity identification (e.g., positive
vs. negative reviews) or extraction of a handful of
pre-specified mood labels, there are many useful
and relatively unexplored sentiment types.
Sentiment extraction systems usually require
an extensive set of manually supplied sentiment
words or a handcrafted sentiment-specific dataset.
With the recent popularity of article tagging, some
social media types like blogs allow users to add
sentiment tags to articles. This allows to use blogs
as a large user-labeled dataset for sentiment learn-
ing and identification. However, the set of senti-
ment tags in most blog platforms is somewhat re-
stricted. Moreover, the assigned tag applies to the
whole blog post while a finer grained sentiment
extraction is needed (McDonald et al, 2007).
With the recent popularity of the Twitter micro-
blogging service, a huge amount of frequently
1WordNet 2.1 definitions.
241
self-standing short textual sentences (tweets) be-
came openly available for the research commu-
nity. Many of these tweets contain a wide vari-
ety of user-defined hashtags. Some of these tags
are sentiment tags which assign one or more senti-
ment values to a tweet. In this paper we propose a
way to utilize such tagged Twitter data for classi-
fication of a wide variety of sentiment types from
text.
We utilize 50 Twitter tags and 15 smileys as
sentiment labels which allow us to build a clas-
sifier for dozens of sentiment types for short tex-
tual sentences. In our study we use four different
feature types (punctuation, words, n-grams and
patterns) for sentiment classification and evaluate
the contribution of each feature type for this task.
We show that our framework successfully identi-
fies sentiment types of the untagged tweets. We
confirm the quality of our algorithm using human
judges.
We also explore the dependencies and overlap
between different sentiment types represented by
smileys and Twitter tags.
Section 2 describes related work. Section 3
details classification features and the algorithm,
while Section 4 describes the dataset and labels.
Automated and manual evaluation protocols and
results are presented in Section 5, followed by a
short discussion.
2 Related work
Sentiment analysis tasks typically combine two
different tasks: (1) Identifying sentiment expres-
sions, and (2) determining the polarity (sometimes
called valence) of the expressed sentiment. These
tasks are closely related as the purpose of most
works is to determine whether a sentence bears a
positive or a negative (implicit or explicit) opinion
about the target of the sentiment.
Several works (Wiebe, 2000; Turney, 2002;
Riloff, 2003; Whitelaw et al, 2005) use lexical re-
sources and decide whether a sentence expresses
a sentiment by the presence of lexical items (sen-
timent words). Others combine additional feature
types for this decision (Yu and Hatzivassiloglou,
2003; Kim and Hovy, 2004; Wilson et al, 2005;
Bloom et al, 2007; McDonald et al, 2007; Titov
and McDonald, 2008a; Melville et al, 2009).
It was suggested that sentiment words may have
different senses (Esuli and Sebastiani, 2006; An-
dreevskaia and Bergler, 2006; Wiebe and Mihal-
cea, 2006), thus word sense disambiguation can
improve sentiment analysis systems (Akkaya et
al., 2009). All works mentioned above identify
evaluative sentiment expressions and their polar-
ity.
Another line of works aims at identifying a
broader range of sentiment classes expressing var-
ious emotions such as happiness, sadness, bore-
dom, fear, and gratitude, regardless (or in addi-
tion to) positive or negative evaluations. Mihalcea
and Liu (2006) derive lists of words and phrases
with happiness factor from a corpus of blog posts,
where each post is annotated by the blogger with
a mood label. Balog et al (2006) use the mood
annotation of blog posts coupled with news data
in order to discover the events that drive the dom-
inant moods expressed in blogs. Mishne (2005)
used an ontology of over 100 moods assigned
to blog posts to classify blog texts according to
moods. While (Mishne, 2005) classifies a blog en-
try (post), (Mihalcea and Liu, 2006) assign a hap-
piness factor to specific words and expressions.
Mishne used a much broader range of moods.
Strapparava and Mihalcea (2008) classify blog
posts and news headlines to six sentiment cate-
gories.
While most of the works on sentiment analy-
sis focus on full text, some works address senti-
ment analysis in the phrasal and sentence level,
see (Yu and Hatzivassiloglou, 2003; Wilson et al,
2005; McDonald et al, 2007; Titov and McDon-
ald, 2008a; Titov and McDonald, 2008b; Wilson
et al, 2009; Tsur et al, 2010) among others.
Only a few studies analyze the sentiment and
polarity of tweets targeted at major brands. Jansen
et al (2009) used a commercial sentiment ana-
lyzer as well as a manually labeled corpus. Davi-
dov et al (2010) analyze the use of the #sarcasm
hashtag and its contribution to automatic recogni-
tion of sarcastic tweets. To the best of our knowl-
edge, there are no works employing Twitter hash-
tags to learn a wide range of emotions and the re-
lations between the different emotions.
242
3 Sentiment classification framework
Below we propose a set of classification features
and present the algorithm for sentiment classifica-
tion.
3.1 Classification features
We utilize four basic feature types for sentiment
classification: single word features, n-gram fea-
tures, pattern features and punctuation features.
For the classification, all feature types are com-
bined into a single feature vector.
3.1.1 Word-based and n-gram-based features
Each word appearing in a sentence serves as a
binary feature with weight equal to the inverted
count of this word in the Twitter corpus. We also
took each consecutive word sequence containing
2?5 words as a binary n-gram feature using a sim-
ilar weighting strategy. Thus n-gram features al-
ways have a higher weight than features of their
component words, and rare words have a higher
weight than common words. Words or n-grams
appearing in less than 0.5% of the training set sen-
tences do not constitute a feature. ASCII smileys
and other punctuation sequences containing two
or more consecutive punctuation symbols were
used as single-word features. Word features also
include the substituted meta-words for URLs, ref-
erences and hashtags (see Subsection 4.1).
3.1.2 Pattern-based features
Our main feature type is based on surface pat-
terns. For automated extraction of patterns, we
followed the pattern definitions given in (Davidov
and Rappoport, 2006). We classified words into
high-frequency words (HFWs) and content words
(CWs). A word whose corpus frequency is more
(less) than FH (FC) is considered to be a HFW
(CW).We estimate word frequency from the train-
ing set rather than from an external corpus. Unlike
(Davidov and Rappoport, 2006), we consider all
single punctuation characters or consecutive se-
quences of punctuation characters as HFWs. We
also consider URL, REF, and HASHTAG tags as
HFWs for pattern extraction. We define a pattern
as an ordered sequence of high frequency words
and slots for content words. Following (Davidov
and Rappoport, 2008), the FH and FC thresholds
were set to 1000 words per million (upper bound
for FC) and 100 words per million (lower bound
for FH )2.
The patterns allow 2?6 HFWs and 1?5 slots for
CWs. To avoid collection of patterns which cap-
ture only a part of a meaningful multiword ex-
pression, we require patterns to start and to end
with a HFW. Thus a minimal pattern is of the
form [HFW] [CW slot] [HFW]. For each sentence
it is possible to generate dozens of different pat-
terns that may overlap. As with words and n-gram
features, we do not treat as features any patterns
which appear in less than 0.5% of the training set
sentences.
Since each feature vector is based on a single
sentence (tweet), we would like to allow approx-
imate pattern matching for enhancement of learn-
ing flexibility. The value of a pattern feature is
estimated according the one of the following four
scenarios3:
?
????????????????????????
????????????????????????
1
count(p) : Exact match ? all the pattern components
appear in the sentence in correct
order without any additional words.
?
count(p) : Sparse match ? same as exact match
but additional non-matching words can
be inserted between pattern components.
??n
N?count(p) : Incomplete match ? only n > 1 of N
pattern components appear in
the sentence, while some non-matching
words can be inserted in-between.
At least one of the appearing components
should be a HFW.
0 : No match ? nothing or only a single
pattern component appears in the sentence.
0 ? ? ? 1 and 0 ? ? ? 1 are parameters we use
to assign reduced scores for imperfect matches.
Since the patterns we use are relatively long, ex-
act matches are uncommon, and taking advantage
of partial matches allows us to significantly re-
duce the sparsity of the feature vectors. We used
? = ? = 0.1 in all experiments.
This pattern based framework was proven effi-
cient for sarcasm detection in (Tsur et al, 2010;
2Note that the FH and FC bounds allow overlap between
some HFWs and CWs. See (Davidov and Rappoport, 2008)
for a short discussion.
3As with word and n-gram features, the maximal feature
weight of a pattern p is defined as the inverse count of a pat-
tern in the complete Twitter corpus.
243
Davidov et al, 2010).
3.1.3 Efficiency of feature selection
Since we avoid selection of textual features
which have a training set frequency below 0.5%,
we perform feature selection incrementally, on
each stage using the frequencies of the features
obtained during the previous stages. Thus first
we estimate the frequencies of single words in
the training set, then we only consider creation
of n-grams from single words with sufficient fre-
quency, finally we only consider patterns com-
posed from sufficiently frequent words and n-
grams.
3.1.4 Punctuation-based features
In addition to pattern-based features we used
the following generic features: (1) Sentence
length in words, (2) Number of ?!? characters in
the sentence, (3) Number of ??? characters in the
sentence, (4) Number of quotes in the sentence,
and (5) Number of capitalized/all capitals words
in the sentence. All these features were normal-
ized by dividing them by the (maximal observed
value times averaged maximal value of the other
feature groups), thus the maximal weight of each
of these features is equal to the averaged weight
of a single pattern/word/n-gram feature.
3.2 Classification algorithm
In order to assign a sentiment label to new exam-
ples in the test set we use a k-nearest neighbors
(kNN)-like strategy. We construct a feature vec-
tor for each example in the training and the test
set. We would like to assign a sentiment class to
each example in the test set. For each feature vec-
tor V in the test set, we compute the Euclidean
distance to each of the matching vectors in the
training set, where matching vectors are defined as
ones which share at least one pattern/n-gram/word
feature with v.
Let ti, i = 1 . . . k be the k vectors with low-
est Euclidean distance to v4 with assigned labels
Li, i = 1 . . . k. We calculate the mean distance
d(ti, v) for this set of vectors and drop from the set
up to five outliers for which the distance was more
then twice the mean distance. The label assigned
4We used k = 10 for all experiments.
to v is the label of the majority of the remaining
vectors.
If a similar number of remaining vectors have
different labels, we assigned to the test vector the
most frequent of these labels according to their
frequency in the dataset. If there are no matching
vectors found for v, we assigned the default ?no
sentiment? label since there is significantly more
non-sentiment sentences than sentiment sentences
in Twitter.
4 Twitter dataset and sentiment tags
In our experiments we used an extensive Twit-
ter data collection as training and testing sets. In
our training sets we utilize sentiment hashtags and
smileys as classification labels. Below we de-
scribe this dataset in detail.
4.1 Twitter dataset
We have used a Twitter dataset generously pro-
vided to us by Brendan O?Connor. This dataset
includes over 475 million tweets comprising
roughly 15% of all public, non-?low quality?
tweets created from May 2009 to Jan 2010.
Tweets are short sentences limited to 140 UTF-
8 characters. All non-English tweets and tweets
which contain less than 5 proper English words5
were removed from the dataset.
Apart of simple text, tweets may contain URL
addresses, references to other Twitter users (ap-
pear as @<user>) or a content tags (also called
hashtags) assigned by the tweeter (#<tag>)
which we use as labels for our supervised clas-
sification framework.
Two examples of typical tweets are: ?#ipad
#sucks and 6,510 people agree. See more on Ipad
sucks page: http://j.mp/4OiYyg??, and ?Pay no
mind to those who talk behind ur back, it sim-
ply means that u?re 2 steps ahead. #ihatequotes?.
Note that in the first example the hashtagged
words are a grammatical part of the sentence (it
becomes meaningless without them) while #ihate-
qoutes of the second example is a mere sentiment
label and not part of the sentence. Also note that
hashtags can be composed of multiple words (with
no spaces).
5Identification of proper English words was based on an
available WN-based English dictionary
244
Category # of tags % agreement
Strong sentiment 52 87
Likely sentiment 70 66
Context-dependent 110 61
Focused 45 75
No sentiment 3564 99
Table 1: Annotation results (2 judges) for the 3852 most
frequent tweeter tags. The second column displays the av-
erage number of tags, and the last column shows % of tags
annotated similarly by two judges.
During preprocessing, we have replaced URL
links, hashtags and references by URL/REF/TAG
meta-words. This substitution obviously had
some effect on the pattern recognition phase (see
Section 3.1.2), however, our algorithm is robust
enough to overcome this distortion.
4.2 Hashtag-based sentiment labels
The Twitter dataset contains above 2.5 million dif-
ferent user-defined hashtags. Many tweets include
more than a single tag and 3852 ?frequent? tags
appear in more than 1000 different tweets. Two
human judges manually annotated these frequent
tags into five different categories: 1 ? strong sen-
timent (e.g #sucks in the example above), 2 ?
most likely sentiment (e.g., #notcute), 3 ? context-
dependent sentiment (e.g., #shoutsout), 4 ? fo-
cused sentiment (e.g., #tmobilesucks where the
target of the sentiment is part of the hashtag), and
5 ? no sentiment (e.g. #obama). Table 1 shows
annotation results and the percentage of similarly
assigned values for each category.
We selected 50 hashtags annotated ?1? or ?2?
by both judges. For each of these tags we automat-
ically sampled 1000 tweets resulting in 50000 la-
beled tweets. We avoided sampling tweets which
include more than one of the sampled hashtags.
As a no-sentiment dataset we randomly sampled
10000 tweets with no hashtags/smileys from the
whole dataset assuming that such a random sam-
ple is unlikely to contain a significant amount of
sentiment sentences.
4.3 Smiley-based sentiment labels
While there exist many ?official? lists of possible
ASCII smileys, most of these smileys are infre-
quent or not commonly accepted and used as sen-
timent indicators by online communities. We used
the Amazon Mechanical Turk (AMT) service in
order to obtain a list of the most commonly used
and unambiguous ASCII smileys. We asked each
of ten AMT human subjects to provide at least 6
commonly used ASCII mood-indicating smileys
together with one or more single-word descrip-
tions of the smiley-related mood state. From the
obtained list of smileys we selected a subset of 15
smileys which were (1) provided by at least three
human subjects, (2) described by at least two hu-
man subject using the same single-word descrip-
tion, and (3) appear at least 1000 times in our
Twitter dataset. We then sampled 1000 tweets for
each of these smileys, using these smileys as sen-
timent tags in the sentiment classification frame-
work described in the previous section.
5 Evaluation and Results
The purpose of our evaluation was to learn how
well our framework can identify and distinguish
between sentiment types defined by tags or smi-
leys and to test if our framework can be success-
fully used to identify sentiment types in new un-
tagged sentences.
5.1 Evaluation using cross-validation
In the first experiment we evaluated the consis-
tency and quality of sentiment classification us-
ing cross-validation over the training set. Fully
automated evaluation allowed us to test the per-
formance of our algorithm under several dif-
ferent feature settings: Pn+W-M-Pt-, Pn+W+M-Pt-,
Pn+W+M+Pt-, Pn-W-M-Pt+ and FULL, where +/?
stands for utilization/omission of the following
feature types: Pn:punctuation, W:Word, M:n-
grams (M stands for ?multi?), Pt:patterns. FULL
stands for utilization of all feature types.
In this experimental setting, the training set was
divided to 10 parts and a 10-fold cross validation
test is executed. Each time, we use 9 parts as the
labeled training data for feature selection and con-
struction of labeled vectors and the remaining part
is used as a test set. The process was repeated ten
times. To avoid utilization of labels as strong fea-
tures in the test set, we removed all instances of
involved label hashtags/smileys from the tweets
used as the test set.
245
Setup Smileys Hashtags
random 0.06 0.02
Pn+W-M-Pt- 0.16 0.06
Pn+W+M-Pt- 0.25 0.15
Pn+W+M+Pt- 0.29 0.18
Pn-W-M-Pt+ 0.5 0.26
FULL 0.64 0.31
Table 2: Multi-class classification results for smileys and
hashtags. The table shows averaged harmonic f-score for 10-
fold cross validation. 51 (16) sentiment classes were used for
hashtags (smileys).
Multi-class classification. Under multi-class
classification we attempt to assign a single label
(51 labels in case of hashtags and 16 labels in case
of smileys) to each of vectors in the test set. Note
that the random baseline for this task is 0.02 (0.06)
for hashtags (smileys). Table 2 shows the perfor-
mance of our framework for these tasks.
Results are significantly above the random
baseline and definitely nontrivial considering the
equal class sizes in the test set. While still rel-
atively low (0.31 for hashtags and 0.64 for smi-
leys), we observe much better performance for
smileys which is expected due to the lower num-
ber of sentiment types.
The relatively low performance of hashtags can
be explained by ambiguity of the hashtags and
some overlap of sentiments. Examination of clas-
sified sentences reveals that many of them can
be reasonably assigned to more than one of the
available hashtags or smileys. Thus a tweet ?I?m
reading stuff that I DON?T understand again! ha-
haha...wth am I doing? may reasonably match
tags #sarcasm, #damn, #haha, #lol, #humor, #an-
gry etc. Close examination of the incorrectly
classified examples also reveals that substantial
amount of tweets utilize hashtags to explicitly in-
dicate the specific hashtagged sentiment, in these
cases that no sentiment value could be perceived
by readers unless indicated explicitly, e.g. ?De
Blob game review posted on our blog. #fun?.
Obviously, our framework fails to process such
cases and captures noise since no sentiment data
is present in the processed text labeled with a spe-
cific sentiment label.
Binary classification. In the binary classifica-
tion experiments, we classified a sentence as ei-
ther appropriate for a particular tag or as not bear-
Hashtags Avg #hate #jealous #cute #outrageous
Pn+W-M-Pt- 0.57 0.6 0.55 0.63 0.53
Pn+W+M-Pt- 0.64 0.64 0.67 0.66 0.6
Pn+W+M+Pt- 0.69 0.66 0.67 0.69 0.64
Pn-W-M-Pt+ 0.73 0.75 0.7 0.69 0.69
FULL 0.8 0.83 0.76 0.71 0.78
Smileys Avg :) ; ) X( : d
Pn+W-M-Pt- 0.64 0.66 0.67 0.56 0.65
Pn+W+M-Pt- 0.7 0.73 0.72 0.64 0.69
Pn+W+M+Pt- 0.7 0.74 0.75 0.66 0.69
Pn-W-M-Pt+ 0.75 0.78 0.75 0.68 0.72
FULL 0.86 0.87 0.9 0.74 0.81
Table 3: Binary classification results for smileys and hash-
tags. Avg column shows averaged harmonic f-score for 10-
fold cross validation over all 50(15) sentiment hashtags (smi-
leys).
ing any sentiment6. For each of the 50 (15) labels
for hashtags (smileys) we have performed a bi-
nary classification when providing as training/test
sets only positive examples of the specific senti-
ment label together with non-sentiment examples.
Table 3 shows averaged results for this case and
specific results for selected tags. We can see that
our framework successfully identifies diverse sen-
timent types. Obviously the results are much bet-
ter than those of multi-class classification, and the
observed > 0.8 precision confirms the usefulness
of the proposed framework for sentiment classifi-
cation of a variety of different sentiment types.
We can see that even for binary classification
settings, classification of smiley-labeled sentences
is a substantially easier task compared to classifi-
cation of hashtag-labeled tweets. Comparing the
contributed performance of different feature types
we can see that punctuation, word and pattern fea-
tures, each provide a substantial boost for classi-
fication quality while we observe only a marginal
boost when adding n-grams as classification fea-
tures. We can also see that pattern features con-
tribute the performance more than all other fea-
tures together.
5.2 Evaluation with human judges
In the second set of experiments we evaluated our
framework on a test set of unseen and untagged
tweets (thus tweets that were not part of the train-
6Note that this is a useful application in itself, as a filter
that extracts sentiment sentences from a corpus for further
focused study/processing.
246
ing data), comparing its output to tags assigned by
human judges. We applied our framework with
its FULL setting, learning the sentiment tags from
the training set for hashtags and smileys (sepa-
rately) and executed the framework on the reduced
Tweeter dataset (without untagged data) allowing
it to identify at least five sentences for each senti-
ment class.
In order to make the evaluation harsher, we re-
moved all tweets containing at least one of the
relevant classification hashtags (or smileys). For
each of the resulting 250 sentences for hashtags,
and 75 sentences for smileys we generated an ?as-
signment task?. Each task presents a human judge
with a sentence and a list of ten possible hash-
tags. One tag from this list was provided by our
algorithm, 8 other tags were sampled from the re-
maining 49 (14) available sentiment tags, and the
tenth tag is from the list of frequent non-sentiment
tags (e.g. travel or obama). The human judge was
requested to select the 0-2 most appropriate tags
from the list. Allowing assignment of multiple
tags conforms to the observation that even short
sentences may express several different sentiment
types and to the observation that some of the se-
lected sentiment tags might express similar senti-
ment types.
We used the Amazon Mechanical Turk service
to present the tasks to English-speaking subjects.
Each subject was given 50 tasks for Twitter hash-
tags or 25 questions for smileys. To ensure the
quality of assignments, we added to each test five
manually selected, clearly sentiment bearing, as-
signment tasks from the tagged Twitter sentences
used in the training set. Each set was presented to
four subjects. If a human subject failed to provide
the intended ?correct? answer to at least two of
the control set questions we reject him/her from
the calculation. In our evaluation the algorithm
is considered to be correct if one of the tags se-
lected by a human judge was also selected by the
algorithm. Table 4 shows results for human judge-
ment classification. The agreement score for this
task was ? = 0.41 (we consider agreement when
at least one of two selected items are shared).
Table 4 shows that the majority of tags selected
by humans matched those selected by the algo-
rithm. Precision of smiley tags is substantially
Setup % Correct % No sentiment Control
Smileys 84% 6% 92%
Hashtags 77% 10% 90%
Table 4: Results of human evaluation. The second col-
umn indicates percentage of sentences where judges find no
appropriate tags from the list. The third column shows per-
formance on the control set.
Hashtags #happy #sad #crazy # bored
#sad 0.67 - - -
#crazy 0.67 0.25 - -
#bored 0.05 0.42 0.35 -
#fun 1.21 0.06 1.17 0.43
Smileys :) ; ) : ( X(
; ) 3.35 - - -
: ( 3.12 0.53 - -
X( 1.74 0.47 2.18 -
: S 1.74 0.42 1.4 0.15
Table 5: Percentage of co-appearance of tags in tweeter
corpus.
higher than of hashtag labels, due to the lesser
number of possible smileys and the lesser ambi-
guity of smileys in comparison to hashtags.
5.3 Exploration of feature dependencies
Our algorithm assigns a single sentiment type
for each tweet. However, as discussed above,
some sentiment types overlap (e.g., #awesome and
#amazing). Many sentences may express several
types of sentiment (e.g., #fun and #scary in ?Oh
My God http://goo.gl/fb/K2N5z #entertainment
#fun #pictures #photography #scary #teaparty?).
We would like to estimate such inter-sentiment
dependencies and overlap automatically from the
labeled data. We use two different methods for
overlap estimation: tag co-occurrence and feature
overlap.
5.3.1 Tag co-occurrence
Many tweets contain more than a single hash-
tag or a single smiley type. As mentioned, we ex-
clude such tweets from the training set to reduce
ambiguity. However such tag co-appearances can
be used for sentiment overlap estimation. We cal-
culated the relative co-occurrence frequencies of
some hashtags and smileys. Table 5 shows some
of the observed co-appearance ratios. As expected
some of the observed tags frequently co-appear
with other similar tags.
247
Hashtags #happy #sad #crazy # bored
#sad 12.8 - - -
#crazy 14.2 3.5 - -
#bored 2.4 11.1 2.1 -
#fun 19.6 2.1 15 4.4
Smileys :) ; ) : ( X(
; ) 35.9 - - -
: ( 31.9 10.5 - -
X( 8.1 10.2 36 -
: S 10.5 12.6 21.6 6.1
Table 6: Percentage of shared features in feature vectors
for different tags.
Interestingly, it appears that a relatively high
ratio of co-appearance of tags is with opposite
meanings (e.g., ?#ilove eating but #ihate feeling
fat lol? or ?happy days of training going to end
in a few days #sad #happy?). This is possibly due
to frequently expressed contrast sentiment types
in the same sentence ? a fascinating phenomena
reflecting the great complexity of the human emo-
tional state (and expression).
5.3.2 Feature overlap
In our framework we have created a set of fea-
ture vectors for each of the Twitter sentiment tags.
Comparison of shared features in feature vector
sets allows us to estimate dependencies between
different sentiment types even when direct tag co-
occurrence data is very sparse. A feature is con-
sidered to be shared between two different senti-
ment labels if for both sentiment labels there is
at least a single example in the training set which
has a positive value of this feature. In order to au-
tomatically analyze such dependencies we calcu-
late the percentage of sharedWord/n-gram/Pattern
features between different sentiment labels. Table
6 shows the observed feature overlap values for
selected sentiment tags.
We observe the trend of results obtained by
comparison of shared feature vectors is similar to
those obtained by means of label co-occurrence,
although the numbers of the shared features are
higher. These results, demonstrating the pattern-
based similarity of conflicting, sometimes contra-
dicting, emotions are interesting from a psycho-
logical and cognitive perspective.
6 Conclusion
We presented a framework which allows an au-
tomatic identification and classification of various
sentiment types in short text fragments which is
based on Twitter data. Our framework is a su-
pervised classification one which utilizes Twitter
hashtags and smileys as training labels. The sub-
stantial coverage and size of the processed Twit-
ter data allowed us to identify dozens of senti-
ment types without any labor-intensive manually
labeled training sets or pre-provided sentiment-
specific features or sentiment words.
We evaluated diverse feature types for senti-
ment extraction including punctuation, patterns,
words and n-grams, confirming that each fea-
ture type contributes to the sentiment classifica-
tion framework. We also proposed two different
methods which allow an automatic identification
of sentiment type overlap and inter-dependencies.
In the future these methods can be used for au-
tomated clustering of sentiment types and senti-
ment dependency rules. While hashtag labels are
specific to Twitter data, the obtained feature vec-
tors are not heavily Twitter-specific and in the fu-
ture we would like to explore the applicability of
Twitter data for sentiment multi-class identifica-
tion and classification in other domains.
References
Akkaya, Cem, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
EMNLP.
Andreevskaia, A. and S. Bergler. 2006. Mining word-
net for fuzzy sentiment: Sentiment tag extraction
from wordnet glosses. In EACL.
Balog, Krisztian, Gilad Mishne, and Maarten de Ri-
jke. 2006. Why are they excited? identifying and
explaining spikes in blog mood levels. In EACL.
Bloom, Kenneth, Navendu Garg, and Shlomo Arga-
mon. 2007. Extracting appraisal expressions. In
HLT/NAACL.
Davidov, D. and A. Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In
COLING-ACL.
248
Davidov, D. and A. Rappoport. 2008. Unsuper-
vised discovery of generic relationships using pat-
tern clusters and its evaluation by automatically gen-
erated sat analogy questions. In ACL.
Davidov, D., O. Tsur, and A. Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In CoNLL.
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In LREC.
Jansen, B.J., M. Zhang, K. Sobel, and A. Chowdury.
2009. Twitter power: Tweets as electronic word of
mouth. Journal of the American Society for Infor-
mation Science and Technology.
Kim, S.M. and E. Hovy. 2004. Determining the senti-
ment of opinions. In COLING.
McDonald, Ryan, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models
for fine-to-coarse sentiment analysis. In ACL.
Melville, Prem, Wojciech Gryc, and Richard D.
Lawrence. 2009. Sentiment analysis of blogs by
combining lexical knowledge with text classifica-
tion. In KDD. ACM.
Mihalcea, Rada and Hugo Liu. 2006. A corpus-
based approach to finding happiness. In In AAAI
2006 Symposium on Computational Approaches to
Analysing Weblogs. AAAI Press.
Mishne, Gilad. 2005. Experiments with mood clas-
sification in blog posts. In Proceedings of the 1st
Workshop on Stylistic Analysis Of Text.
Riloff, Ellen. 2003. Learning extraction patterns for
subjective expressions. In EMNLP.
Strapparava, Carlo and Rada Mihalcea. 2008. Learn-
ing to identify emotions in text. In SAC.
Titov, Ivan and Ryan McDonald. 2008a. A joint
model of text and aspect ratings for sentiment sum-
marization. In ACL/HLT, June.
Titov, Ivan and Ryan McDonald. 2008b. Modeling
online reviews with multi-grain topic models. In
WWW, pages 111?120, New York, NY, USA. ACM.
Tsur, Oren, Dmitry Davidov, and Ari Rappoport.
2010. Icwsm ? a great catchy name: Semi-
supervised recognition of sarcastic sentences in
product reviews. In AAAI-ICWSM.
Turney, Peter D. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
fication of reviews. In ACL ?02, volume 40.
Whitelaw, Casey, Navendu Garg, and Shlomo Arga-
mon. 2005. Using appraisal groups for sentiment
analysis. In CIKM.
Wiebe, Janyce and Rada Mihalcea. 2006. Word sense
and subjectivity. In COLING/ACL, Sydney, AUS.
Wiebe, Janyce M. 2000. Learning subjective adjec-
tives from corpora. In AAAI.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399?433.
Yu, Hong and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating
facts from opinions and identifying the polarity of
opinion sentences. In EMNLP.
249
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1308?1317,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Extraction and Approximation of Numerical Attributes from the Web
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem, Israel
dmitry@alice.nc.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
arir@cs.huji.ac.il
Abstract
We present a novel framework for auto-
mated extraction and approximation of nu-
merical object attributes such as height
and weight from the Web. Given an
object-attribute pair, we discover and ana-
lyze attribute information for a set of com-
parable objects in order to infer the desired
value. This allows us to approximate the
desired numerical values even when no ex-
act values can be found in the text.
Our framework makes use of relation
defining patterns and WordNet similarity
information. First, we obtain from the
Web and WordNet a list of terms similar to
the given object. Then we retrieve attribute
values for each term in this list, and infor-
mation that allows us to compare different
objects in the list and to infer the attribute
value range. Finally, we combine the re-
trieved data for all terms from the list to
select or approximate the requested value.
We evaluate our method using automated
question answering, WordNet enrichment,
and comparison with answers given in
Wikipedia and by leading search engines.
In all of these, our framework provides a
significant improvement.
1 Introduction
Information on various numerical properties of
physical objects, such as length, width and weight
is fundamental in question answering frameworks
and for answering search engine queries. While
in some cases manual annotation of objects with
numerical properties is possible, it is a hard and
labor intensive task, and is impractical for dealing
with the vast amount of objects of interest. Hence,
there is a need for automated semantic acquisition
algorithms targeting such properties.
In addition to answering direct questions, the
ability to make a crude comparison or estimation
of object attributes is important as well. For ex-
ample, it allows to disambiguate relationships be-
tween objects such as X part-of Y or X inside Y.
Thus, a coarse approximation of the height of a
house and a window is sufficient to decide that
in the ?house window? nominal compound, ?win-
dow? is very likely to be a part of house and not
vice versa. Such relationship information can, in
turn, help summarization, machine translation or
textual entailment tasks.
Due to the importance of relationship and at-
tribute acquisition in NLP, numerous methods
were proposed for extraction of various lexical re-
lationships and attributes from text. Some of these
methods can be successfully used for extracting
numerical attributes. However, numerical attribute
extraction is substantially different in two aspects,
verification and approximation.
First, unlike most general lexical attributes, nu-
merical attribute values are comparable. It usually
makes no sense to compare the names of two ac-
tors, but it is meaningful to compare their ages.
The ability to compare values of different objects
allows to improve attribute extraction precision by
verifying consistency with attributes of other sim-
ilar objects. For example, suppose that for Toy-
ota Corolla width we found two different values,
1.695m and 27cm. The second value can be either
an extraction error or a length of a toy car. Ex-
tracting and looking at width values for different
car brands and for ?cars? in general we find:
? Boundaries: Maximal car width is 2.195m,
minimal is 88cm.
? Average: Estimated avg. car width is 1.7m.
? Direct/indirect comparisons: Toyota Corolla
is wider than Toyota Corona.
? Distribution: Car width is distributed nor-
mally around the average.
1308
Usage of all this knowledge allows us to select the
correct value of 1.695m and reject other values.
Thus we can increase the precision of value ex-
traction by finding and analyzing an entire group
of comparable objects.
Second, while it is usually meaningless and im-
possible to approximate general lexical attribute
values like an actor?s name, numerical attributes
can be estimated even if they are not explicitly
mentioned in the text.
In general, attribute extraction frameworks usu-
ally attempt to discover a single correct value (e.g.,
capital city of a country) or a set of distinct correct
values (e.g., actors of a movie). So there is es-
sentially nothing to do when there is no explicit
information present in the text for a given object
and an attribute. In contrast, in numerical attribute
extraction it is possible to provide an approxima-
tion even when no explicit information is present
in the text, by using values of comparable objects
for which information is provided.
In this paper we present a pattern-based frame-
work that takes advantage of the properties of sim-
ilar objects to improve extraction precision and
allow approximation of requested numerical ob-
ject properties. Our framework comprises three
main stages. First, given an object name we uti-
lize WordNet and pattern-based extraction to find
a list of similar objects and their category labels.
Second, we utilize a predefined set of lexical pat-
terns in order to extract attribute values of these
objects and available comparison/boundary infor-
mation. Finally, we analyze the obtained informa-
tion and select or approximate the attribute value
for the given (object, attribute) pair.
We performed a thorough evaluation using three
different applications: Question Answering (QA),
WordNet (WN) enrichment, and comparison with
Wikipedia and answers provided by leading search
engines. QA evaluation was based on a designed
dataset of 1250 questions on size, height, width,
weight, and depth, for which we created a gold
standard and compared against it automatically1.
For WN enrichment evaluation, our framework
discovered size and weight values for 300 WN
physical objects, and the quality of results was
evaluated by human judges. For interactive search,
we compared our results to information obtained
through Wikipedia, Google and Wolfram Alpha.
1This dataset is available in the authors? websites for the
research community.
Utilization of information about comparable ob-
jects provided a significant boost to numerical at-
tribute extraction quality, and allowed a meaning-
ful approximation of missing attribute values.
Section 2 discusses related work, Section 3 de-
tails the algorithmic framework, Section 4 de-
scribes the experimental setup, and Section 5
presents our results.
2 Related work
Numerous methods have been developed for ex-
traction of diverse semantic relationships from
text. While several studies propose relationship
identification methods using distributional analy-
sis of feature vectors (Turney, 2005), the major-
ity of the proposed open-domain relations extrac-
tion frameworks utilize lexical patterns connect-
ing a pair of related terms. (Hearst, 1992) man-
ually designed lexico-syntactic patterns for ex-
tracting hypernymy relations. (Berland and Char-
niak, 1999; Girju et al 2006) proposed a set of
patterns for meronymy relations. Davidov and
Rappoport (2008a) used pattern clusters to disam-
biguate nominal compound relations. Extensive
frameworks were proposed for iterative discov-
ery of any pre-specified (e.g., (Riloff and Jones,
1999; Chklovski and Pantel, 2004)) and unspec-
ified (e.g., (Banko et al, 2007; Rosenfeld and
Feldman, 2007; Davidov and Rappoport, 2008b))
relation types.
The majority of the above methods utilize the
following basic strategy. Given (or discovering
automatically) a set of patterns or relationship-
representing term pairs, these methods mine the
web for these patterns and pairs, iteratively obtain-
ing more instances. The proposed strategies gen-
erally include some weighting/frequency/context-
based algorithms (e.g. (Pantel and Pennacchiotti,
2006)) to reduce noise. Some of the methods are
suitable for retrieval of numerical attributes. How-
ever, most of them do not exploit the numerical
nature of the attribute data.
Our research is related to a sub-domain of ques-
tion answering (Prager, 2006), since one of the
applications of our framework is answering ques-
tions on numerical values. The majority of the
proposed QA frameworks rely on pattern-based
relationship acquisition (Ravichandran and Hovy,
2009). However, most QA studies focus on dif-
ferent types of problems than our paper, including
question classification, paraphrasing, etc.
1309
Several recent studies directly target the acqui-
sition of numerical attributes from the Web and
attempt to deal with ambiguity and noise of the
retrieved attribute values. (Aramaki et al, 2007)
utilize a small set of patterns to extract physical
object sizes and use the averages of the obtained
values for a noun compound classification task.
(Banerjee et al 2009) developed a method for
dealing with quantity consensus queries (QCQs)
where there is uncertainty about the answer quan-
tity (e.g. ?driving time from Paris to Nice?). They
utilize a textual snippet feature and snippet quan-
tity in order to select and rank intervals of the
requested values. This approach is particularly
useful when it is possible to obtain a substantial
amount of a desired attribute values for the re-
quested query. (Moriceau, 2006) proposed a rule-
based system which analyzes the variation of the
extracted numerical attribute values using infor-
mation in the textual context of these values.
A significant body of recent research deals with
extraction of various data from web tables and
lists (e.g., (Cafarella et al, 2008; Crestan and
Pantel, 2010)). While in the current research we
do not utilize this type of information, incorpo-
ration of the numerical data extracted from semi-
structured web pages can be extremely beneficial
for our framework.
All of the above numerical attribute extraction
systems utilize only direct information available
in the discovered object-attribute co-occurrences
and their contexts. However, as we show, indirect
information available for comparable objects can
contribute significantly to the selection of the ob-
tained values. Using such indirect information is
particularly important when only a modest amount
of values can be obtained for the desired object.
Also, since the above studies utilize only explic-
itly available information they were unable to ap-
proximate object values in cases where no explicit
information was found.
3 The Attribute Mining Framework
Our algorithm is given an object and an attribute.
In the WN enrichment scenario, it is also given
the object?s synset. The algorithm comprises three
main stages: (1) mining for similar objects and
determination of a class label; (2) mining for at-
tribute values and comparison statements; (3) pro-
cessing the results.
3.1 Similar objects and class label
To verify and estimate attribute values for the
given object we utilize similar objects (co-
hyponyms) and the object?s class label (hyper-
nym). In the WN enrichment scenario we can eas-
ily obtain these, since we get the object?s synset as
input. However, in Question Answering (QA) sce-
narios we do not have such information. To obtain
it we employ a strategy which uses WordNet alng
with pattern-based web mining.
Our web mining part follows common pattern-
based retrieval practice (Davidov et al, 2007). We
utilize Yahoo! Boss API to perform search engine
queries. For an object name Obj we query the
Web using a small set of pre-defined co-hyponymy
patterns like ?as * and/or [Obj]?2. In the WN en-
richment scenario, we can add the WN class la-
bel to each query in order to restrict results to the
desired word sense. In the QA scenario, if we
are given the full question and not just the (ob-
ject, attribute) pair we can add terms appearing in
the question and having a strong PMI with the ob-
ject (this can be estimated using any fixed corpus).
However, this is not essential.
We then extract new terms from the retrieved
web snippets and use these terms iteratively to re-
trieve more terms from the Web. For example,
when searching for an object ?Toyota?, we execute
a search engine query [ ?as * and Toyota?] and
we might retrieve a text snippet containing ?. . . as
Honda and Toyota . . . ?. We then extract from this
snippet the additional word ?Honda? and use it for
iterative retrieval of additional similar terms. We
attempt to avoid runaway feedback loop by requir-
ing each newly detected term to co-appear with the
original term in at least a single co-hyponymy pat-
tern.
WN class labels are used later for the retrieval
of boundary values, and here for expansion of the
similar object set. In the WN enrichment scenario,
we already have the class label of the object. In the
QA scenario, we automatically find class labels as
follows. We compute for each WN subtree a cov-
erage value, the number of retrieved terms found
in the subtree divided by the number of subtree
terms, and select the subtree having the highest
coverage. In all scenarios, we add all terms found
in this subtree to the retrieved term list. If no WN
subtree with significant (> 0.1) coverage is found,
2
?*? means a search engine wildcard. Square brackets
indicate filled slots and are not part of the query.
1310
we retrieve a set of category labels from the Web
using hypernymy detection patterns like ?* such
as [Obj]? (Hearst, 1992). If several label candi-
dates were found, we select the most frequent.
Note that we perform this stage only once for
each object and do not need to repeat it for differ-
ent attribute types.
3.2 Querying for values, bounds and
comparison data
Now we would like to extract the attribute values
for the given object and its similar objects. We
will also extract bounds and comparison informa-
tion in order to verify the extracted values and to
approximate the missing ones.
To allow us to extract attribute-specific informa-
tion, we provided the system with a seed set of ex-
traction patterns for each attribute type. There are
three kinds of patterns: value extraction, bounds
and comparison patterns. We used up to 10 pat-
terns of each kind. These patterns are the only
attribute-specific resource in our framework.
Value extraction. The first pattern group,
Pvalues, allows extraction of the attribute values
from the Web. All seed patterns of this group
contain a measurement unit name, attribute name,
and some additional anchoring words, e.g., ?Obj
is * [height unit] tall? or ?Obj width is * [width
unit]?. As in Section 3.1, we execute search en-
gine queries and collect a set of numerical val-
ues for each pattern. We extend this group it-
eratively from the given seed as commonly done
in pattern-based acquisition methods. To do this
we re-query the Web with the obtained (object, at-
tribute value, attribute name) triplets (e.g., ?[Toy-
ota width 1.695m]?). We then extract new pat-
terns from the retrieved search engine snippets and
re-query the Web with the new patterns to obtain
more attribute values.
We provided the framework with unit names
and with an appropriate conversion table which
allows to convert between different measurement
systems and scales. The provided names include
common abbreviations like cm/centimeter. All
value acquisition patterns include unit names, so
we know the units of each extracted value. At the
end of the value extraction stage, we convert all
values to a single unit format for comparison.
Boundary extraction. The second group,
Pboundary, consists of boundary-detection patterns
like ?the widest [label] is * [width unit]?. These
patterns incorporate the class labels discovered in
the previous stage. They allow us to find maximal
and minimal values for the object category defined
by labels. If we get several lower bounds and
several upper bounds, we select the highest upper
bound and the lowest lower bound.
Extraction of comparison information. The
third group, Pcompare, consists of comparison pat-
terns. They allow to compare objects directly
even when no attribute values are mentioned. This
group includes attribute equality patterns such as
?[Object1] has the same width as [Object2]?, and
attribute inequality ones such as ?[Object1] is
wider than [Object2]?. We execute search queries
for each of these patterns, and extract a set of or-
dered term pairs, keeping track of the relationships
encoded by the pairs.
We use these pairs to build a directed graph
(Widdows and Dorow, 2002; Davidov and Rap-
poport, 2006) in which nodes are objects (not nec-
essarily with assigned values) and edges corre-
spond to extracted co-appearances of objects in-
side the comparison patterns. The directions of
edges are determined by the comparison sign. If
two objects co-appear inside an equality pattern
we put a bidirectional edge between them.
3.3 Processing the collected data
As a result of the information collection stage, for
each object and attribute type we get:
? A set of attribute values for the requested ob-
ject.
? A set of objects similar or comparable to
the requested object, some of them annotated
with one or many attribute values.
? Upper and lowed bounds on attribute values
for the given object category.
? A comparison graph connecting some of the
retrieved objects by comparison edges.
Obviously, some of these components may be
missing or noisy. Now we combine these informa-
tion sources to select a single attribute value for
the requested object or to approximate this value.
First we apply bounds, removing out-of-range val-
ues, then we use comparisons to remove inconsis-
tent comparisons. Finally we examine the remain-
ing values and the comparison graph.
Processing bounds. First we verify that indeed
most (? 50%) of the retrieved values fit the re-
trieved bounds. If the lower and/or upper bound
1311
contradicts more than half of the data, we reject
the bound. Otherwise we remove all values which
do not satisfy one or both of the accepted bounds.
If no bounds are found or if we disable the bound
retrieval (see Section 4.1), we assign the maximal
and minimal observed values as bounds.
Since our goal is to obtain a value for the single
requested object, if at the end of this stage we re-
main with a single value, no further processing is
needed. However, if we obtain a set of values or
no values at all, we have to utilize comparison data
to select one of the retrieved values or to approx-
imate the value in case we do not have an exact
answer.
Processing comparisons. First we simplify the
comparison graph. We drop all graph components
that are not connected (when viewing the graph as
undirected) to the desired object.
Now we refine the graph. Note that each graph
node may have a single value, many assigned val-
ues, or no assigned values. We define assigned
nodes as nodes that have at least one value. For
each directed edge E(A ? B), if both A and
B are assigned nodes, we check if Avg(A) ?
Avg(B)3. If the average values violate the equa-
tion, we gradually remove up to half of the highest
values for A and up to half of the lowest values
for B till the equation is satisfied. If this cannot
be done, we drop the edge. We repeat this process
until every edge that connects two assigned nodes
satisfies the inequality.
Selecting an exact attribute value. The goal
now is to select an attribute value for the given
object. During the first stage it is possible that
we directly extract from the text a set of values
for the requested object. The bounds processing
step rejects some of these values, and the com-
parisons step may reject some more. If we still
have several values remaining, we choose the most
frequent value based on the number of web snip-
pets retrieved during the value acquisition stage.
If there are several values with the same frequency
we select the median of these values.
Approximating the attribute value. In the case
when we do not have any values remaining after
the bounds processing step, the object node will
remain unassigned after construction of the com-
parison graph, and we would like to estimate its
value. Here we present an algorithm which allows
3Avg. is of values of an object, without similar objects.
us to set the values of all unassigned nodes, includ-
ing the node of the requested object.
In the algorithm below we treat all node groups
connected by bidirectional (equality) edges as a
same-value group, i.e., if a value is assigned to one
node in the group, the same value is immediately
assigned to the rest of the nodes in the same group.
We start with some preprocessing. We create
dummy lower and upper bound nodes L and U
with corresponding upper/lower bound values ob-
tained during the previous stage. These dummy
nodes will be used when we encounter a graph
which ends with one or more nodes with no avail-
able numerical information. We then connect
them to the graph as follows: (1) if A has no in-
coming edges, we add an edge L ? A; (2) if A
has no outgoing edges, we add an edge A ? U .
We define a legal unassigned path as a di-
rected path A0 ? A1 ? . . . ? An ? An+1
where A0 and An+1 are assigned satisfying
Avg(A0) ? Avg(An+1) and A1 . . . An are
unassigned. We would like to use dummy bound
nodes only in cases when no other information is
available. Hence we consider paths L ? . . . ? U
connecting both bounds are illegal. First we
assign values for all unassigned nodes that belong
to a single legal unassigned path, using a simple
linear combination:
V al(Ai)i?(1...n) =
n + 1? i
n + 1 Avg(A0) +
i
n + 1Avg(An+1)
Then, for all unassigned nodes that belong to
multiple legal unassigned paths, we compute node
value as above for each path separately and assign
to the node the average of the computed values.
Finally we assign the average of all extracted
values within bounds to all the remaining unas-
signed nodes. Note that if we have no compari-
son information and no value information for the
requested object, the requested object will receive
the average of the extracted values of the whole set
of the retrieved comparable objects and the com-
parison step will be essentially empty.
4 Experimental Setup
We performed automated question answering
(QA) evaluation, human-based WN enrichment
evaluation, and human-based comparison of our
results to data available through Wikipedia and to
the top results of leading search engines.
1312
4.1 Experimental conditions
In order to test the main system components, we
ran our framework under five different conditions:
? FULL: All system components were used.
? DIRECT: Only direct pattern-based acqui-
sition of attribute values (Section 3.2, value
extraction) for the given object was used, as
done in most general-purpose attribute acqui-
sition systems. If several values were ex-
tracted, the most common value was used as
an answer.
? NOCB: No boundary and no comparison
data were collected and processed (Pcompare
and Pbounds were empty). We only collected
and processed a set of values for the similar
objects.
? NOB: As in FULL but no boundary data was
collected and processed (Pbounds was empty).
? NOC: As in FULL but no comparison data
was collected and processed (Pcompare was
empty).
4.2 Automated QA Evaluation
We created two QA datasets, Web and TREC
based.
Web-based QA dataset. We created QA
datasets for size, height, width, weight, and depth
attributes. For each attribute we extracted from
the Web 250 questions in the following way.
First, we collected several thousand questions,
querying for the following patterns: ?How
long/tall/wide/heavy/deep/high is?,?What is the
size/width/height/depth/weight of?. Then we
manually filtered out non-questions and heavily
context-specific questions, e.g., ?what is the width
of the triangle?. Next, we retained only a single
question for each entity by removing duplicates.
For each of the extracted questions we manu-
ally assigned a gold standard answer using trusted
resources including books and reliable Web data.
For some questions, the exact answer is the only
possible one (e.g., the height of a person), while
for others it is only the center of a distribution
(e.g., the weight of a coffee cup). Questions
with no trusted and exact answers were eliminated.
From the remaining questions we randomly se-
lected 250 questions for each attribute.
TREC-based QA dataset. As a small comple-
mentary dataset we used relevant questions from
the TREC Question Answering Track 1999-2007.
From 4355 questions found in this set we collected
55 (17 size, 2 weight, 3 width, 3 depth and 30
height) questions.
Examples. Some example questions from our
datasets are (correct answers are in parentheses):
How tall is Michelle Obama? (180cm); How tall
is the tallest penguin? (122cm); What is the height
of a tennis net? (92cm); What is the depth of the
Nile river? (1000cm = 10 meters); How heavy
is a cup of coffee? (360gr); How heavy is a gi-
raffe? (1360000gr = 1360kg); What is the width
of a DNA molecule? (2e-7cm); What is the width
of a cow? (65cm).
Evaluation protocol. Evaluation against the
datasets was done automatically. For each ques-
tion and each condition our framework returned
a numerical value marked as either an exact an-
swer or as an approximation. In cases where no
data was found for an approximation (no similar
objects with values were found), our framework
returned no answer.
We computed precision4, comparing results to
the gold standard. Approximate answers are con-
sidered to be correct if the approximation is within
10% of the gold standard value. While a choice of
10% may be too strict for some applications and
too generous for others, it still allows to estimate
the quality of our framework.
4.3 WN enrichment evaluation
We manually selected 300 WN entities from about
1000 randomly selected objects below the object
tree in WN, by filtering out entities that clearly
do not possess any of the addressed numerical at-
tributes.
Evaluation was done using human subjects. It
is difficult to do an automated evaluation, since
the nature of the data is different from that of the
QA dataset. Most of the questions asked over the
Web target named entities like specific car brands,
places and actors. There is usually little or no vari-
ability in attribute values of such objects, and the
major source of extraction errors is name ambigu-
ity of the requested objects.
WordNet physical objects, in contrast, are much
less specific and their attributes such as size and
4Due to the nature of the task recall/f-score measures are
redundant here
1313
weight rarely have a single correct value, but usu-
ally possess an acceptable numerical range. For
example, the majority of the selected objects like
?apple? are too general to assign an exact size.
Also, it is unclear how to define acceptable val-
ues and an approximation range. Crudeness of
desired approximation depends both on potential
applications and on object type. Some objects
show much greater variability in size (and hence a
greater range of acceptable approximations) than
others. This property of the dataset makes it diffi-
cult to provide a meaningful gold standard for the
evaluation. Hence in order to estimate the quality
of our results we turn to an evaluation based on
human judges.
In this evaluation we use only approximate re-
trieved values, keeping out the small amount of
returned exact values5.
We have mixed (Object, Attribute name, At-
tribute value) triplets obtained through each of the
conditions, and asked human subjects to assign
these to one of the following categories:
? The attribute value is reasonable for the given
object.
? The value is a very crude approximation of
the given object attribute.
? The value is incorrect or clearly misleading.
? The object is not familiar enough to me so I
cannot answer the question.
Each evaluator was provided with a random sam-
ple of 40 triplets. In addition we mixed in 5 manu-
ally created clearly correct triplets and 5 clearly in-
correct ones. We used five subjects, and the agree-
ment (inter-annotator Kappa) on shared evaluated
triplets was 0.72.
4.4 Comparisons to search engine output
Recently there has been a significant improvement
both in the quality of search engine results and in
the creation of manual well-organized and anno-
tated databases such as Wikipedia.
Google and Yahoo! queries frequently provide
attribute values in the top snippets or in search
result web pages. Many Wikipedia articles in-
clude infoboxes with well-organized attribute val-
ues. Recently, the Wolfram Alpha computational
knowledge engine presented the computation of
attribute values from a given query text.
5So our results are in fact higher than shown.
Hence it is important to test how well our frame-
work can complement the manual extraction of at-
tributes from resources such as Wikipedia and top
Google snippets. In order to test this, we randomly
selected 100 object-attribute pairs from our Web
QA and WordNet datasets and used human sub-
jects to test the following:
1. Go1: Querying Google for [object-name
attribute-name] gives in some of the first
three snippets a correct value or a good ap-
proximation value6 for this pair.
2. Go2: Querying Google for [object-name
attribute-name] and following the first three
links gives a correct value or a good approxi-
mation value.
3. Wi: There is a Wikipedia page for the given
object and it contains an appropriate attribute
value or an approximation in an infobox.
4. Wf: A Wolfram Alpha query for [object-
name attribute-name] retrieves a correct
value or a good approximation value
5 Results
5.1 QA results
We applied our framework to the above QA
datasets. Table 1 shows the precision and the per-
centage of approximations and exact answers.
Looking at %Exact+%Approx, we can see that
for all datasets only 1-9% of the questions re-
main unanswered, while correct exact answers
are found for 65%/87% of the questions for
Web/TREC (% Exact and Prec(Exact) in the ta-
ble). Thus approximation allows us to answer 13-
24% of the requested values which are either sim-
ply missing from the retrieved text or cannot be de-
tected using the current pattern-based framework.
Comparing performance of FULL to DIRECT, we
see that our framework not only allows an approx-
imation when no exact answer can be found, but
also significantly increases the precision of exact
answers using the comparison and the boundary
information. It is also apparent that both bound-
ary and comparison features are needed to achieve
good performance and that using both of them
achieves substantially better results than each of
them separately.
6As defined in the human subject questionnaire.
1314
FULL DIRECT NOCB NOB NOC
Web QA
Size
%Exact 80 82 82 82 80
Prec(Exact) 76 40 40 54 65
%Approx 16 - 14 14 16
Prec(Appr) 64 - 34 53 46
Height
%Exact 79 84 84 84 79
Prec(Exact) 86 56 56 69 70
%Approx 16 - 11 11 16
Prec(Appr) 72 - 25 65 53
Width
%Exact 74 76 76 76 74
Prec(Exact) 86 45 45 60 72
%Approx 17 - 15 15 17
Prec(Appr) 75 - 26 63 55
Weight
%Exact 71 73 73 73 71
Prec(Exact) 82 57 57 64 70
Prec(Appr) 24 - 22 22 24
%Approx 61 - 39 51 46
Depth
%Exact 82 82 82 82 82
Prec(Exact) 89 60 60 71 78
%Approx 19 - 19 19 19
Prec(Appr) 92 - 58 76 63
Total average
%Exact 77 79 79 79 77
Prec(Exact) 84 52 52 64 71
%Approx 18 - 16 16 19
Prec(Appr) 72 - 36 62 53
TREC QA
%Exact 87 90 90 90 87
Prec(Exact) 100 62 62 84 76
%Approx 13 - 9 9 13
Prec(Appr) 57 - 20 40 57
Table 1: Precision and amount of exact and approximate
answers for QA datasets.
Comparing results for different question types
we can see substantial performance differences be-
tween the attribute types. Thus depth shows much
better overall results than width. This is likely due
to a lesser difficulty of depth questions or to a more
exact nature of available depth information com-
pared to width or size.
5.2 WN enrichment
As shown in Table 2, for the majority of examined
WN objects, the algorithm returned an approxi-
mate value, and only for 13-15% of the objects (vs.
70-80% in QA data) the algorithm could retrieve
exact answers.
Note that the common pattern-based acquisition
framework, presented as the DIRECT condition,
could only extract attribute values for 15% of the
objects since it does not allow approximations and
FULL DIRECT NOCB NOB NOC
Size
%Exact 15.3 18.0 18.0 18.0 15.3
%Approx 80.3 - 38.2 20.0 23.6
Weight
%Exact 11.8 12.5 12.5 12.5 11.8
%Approx 71.7 - 38.2 20.0 23.6
Table 2: Percentage of exact and approximate values for the
WordNet enrichment dataset.
FULL NOCB NOB NOC
Size
%Correct 73 21 49 28
%Crude 15 54 31 49
%Incorrect 8 21 16 19
Weight
%Correct 64 24 46 38
%Crude 24 45 30 41
%Incorrect 6 25 18 15
Table 3: Human evaluation of approximations for the WN
enrichment dataset (the percentages are averaged over the hu-
man subjects).
may only extract values from the text where they
explicitly appear.
Table 3 shows human evaluation results. We
see that the majority of approximate values were
clearly accepted by human subjects, and only 6-
8% were found to be incorrect. We also observe
that both boundary and comparison data signifi-
cantly improve the approximation results. Note
that DIRECT is missing from this table since no
approximations are possible in this condition.
Some examples for WN objects and approx-
imate values discovered by the algorithm are:
Sandfish, 15gr; skull, 1100gr; pilot, 80.25kg. The
latter value is amusing due to the high variabil-
ity of the value. However, even this value is valu-
able, as a sanity check measure for automated in-
ference systems and for various NLP tasks (e.g.,
?pilot jacket? likely refers to a jacket used by pi-
lots and not vice versa).
5.3 Comparison with search engines and
Wikipedia
Table 4 shows results for the above datasets in
comparison to the proportion of correct results and
the approximations returned by our framework un-
der the FULL condition (correct exact values and
approximations are taken together).
We can see that our framework, due to its ap-
proximation capability, currently shows signifi-
cantly greater coverage than manual extraction of
data from Wikipedia infoboxes or from the first
1315
FULL Go1 Go2 Wi Wf
Web QA 83 32 40 15 21
WordNet 87 24 27 18 5
Table 4: Comparison of our attribute extraction framework
to manual extraction using Wikipedia and search engines.
search engine results.
6 Conclusion
We presented a novel framework which allows
an automated extraction and approximation of nu-
merical attributes from the Web, even when no ex-
plicit attribute values can be found in the text for
the given object. Our framework retrieves simi-
larity, boundary and comparison information for
objects similar to the desired object, and com-
bines this information to approximate the desired
attribute.
While in this study we explored only several
specific numerical attributes like size and weight,
our framework can be easily augmented to work
with any other consistent and comparable attribute
type. The only change required for incorpora-
tion of a new attribute type is the development of
attribute-specific Pboundary , Pvalues, and Pcompare
pattern groups; the rest of the system remains un-
changed.
In our evaluation we showed that our frame-
work achieves good results and significantly out-
performs the baseline commonly used for general
lexical attribute retrieval7.
While there is a growing justification to rely
on extensive manually created resources such as
Wikipedia, we have shown that in our case auto-
mated numerical attribute acquisition could be a
preferable option and provides excellent coverage
in comparison to handcrafted resources or man-
ual examination of the leading search engine re-
sults. Hence a promising direction would be to
use our approach in combination with Wikipedia
data and with additional manually created attribute
rich sources such as Web tables, to achieve the best
possible performance and coverage.
We would also like to explore the incorpora-
tion of approximate discovered numerical attribute
data into existing NLP tasks such as noun com-
pound classification and textual entailment.
7It should be noted, however, that in our DIRECT base-
line we used a basic pattern-based retrieval strategy; more
sophisticated strategies for value selection might bring better
results.
References
Eiji Aramaki, Takeshi Imai, Kengo Miyo and Kazuhiko
Ohe. 2007 UTH: SVM-based Semantic Relation
Classification using Physical Sizes. Proceedings
of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007).
Somnath Banerjee, Soumen Chakrabarti and Ganesh
Ramakrishnan. 2009. Learning to Rank for Quan-
tity Consensus Queries. SIGIR ?09.
Michele Banko, Michael J Cafarella , Stephen Soder-
land, Matt Broadhead and Oren Etzioni. 2007.
Open information extraction from the Web. IJCAI
?07.
Matthew Berland, Eugene Charniak, 1999. Finding
parts in very large corpora. ACL ?99.
Michael Cafarella, Alon Halevy, Yang Zhang, Daisy
Zhe Wang and Eugene Wu. 2008. WebTables: Ex-
ploring the Power of Tables on the Web. VLDB ?08.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: mining the Web for fine-grained semantic verb
relations. EMNLP ?04.
Eric Crestan and Patrick Pantel. 2010. Web-Scale
Knowledge Extraction from Semi-Structured Ta-
bles. WWW ?10.
Dmitry Davidov and Ari Rappoport. 2006. Efficient
Unsupervised Discovery of Word Categories Us-
ing Symmetric Patterns and High Frequency Words.
ACL-Coling ?06.
Dmitry Davidov, Ari Rappoport and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. ACL ?07.
Dmitry Davidov and Ari Rappoport. 2008a. Classifi-
cation of Semantic Relationships between Nominals
Using Pattern Clusters. ACL ?08.
Dmitry Davidov and Ari Rappoport. 2008b. Unsu-
pervised Discovery of Generic Relationships Using
Pattern Clusters and its Evaluation by Automatically
Generated SAT Analogy Questions. ACL ?08.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1).
Marty Hearst, 1992. Automatic acquisition of hy-
ponyms from large text corpora. COLING ?92.
Veronique Moriceau, 2006. Numerical Data Integra-
tion for Cooperative Question-Answering. EACL -
KRAQ06 ?06.
John Prager, 2006. Open-domain question-answering.
In Foundations and Trends in Information Re-
trieval,vol. 1, pp 91-231.
1316
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: leveraging generic patterns for automat-
ically harvesting semantic relations. COLING-ACL
?06.
Deepak Ravichandran and Eduard Hovy. 2002 Learn-
ing Surface Text Patterns for a Question Answering
System. ACL ?02.
Ellen Riloff and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level
Bootstrapping. AAAI ?99.
Benjamin Rosenfeld and Ronen Feldman. 2007.
Clustering for unsupervised relation identification.
CIKM ?07.
Peter Turney, 2005. Measuring semantic similarity by
latent relational analysis, IJCAI ?05.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised Lexical acquisition. COL-
ING ?02.
1317
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 107?116,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Semi-Supervised Recognition of Sarcastic Sentences
in Twitter and Amazon
Dmitry Davidov
ICNC
The Hebrew University
Jerusalem, Israel
dmitry@alice.nc.huji.ac.il
Oren Tsur
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
oren@cs.huji.ac.il
Ari Rappoport
Institute of Computer Science
The Hebrew University
Jerusalem, Israel
arir@cs.huji.ac.il
Abstract
Sarcasm is a form of speech act in which
the speakers convey their message in an
implicit way. The inherently ambiguous
nature of sarcasm sometimes makes it hard
even for humans to decide whether an ut-
terance is sarcastic or not. Recognition of
sarcasm can benefit many sentiment analy-
sis NLP applications, such as review sum-
marization, dialogue systems and review
ranking systems.
In this paper we experiment with semi-
supervised sarcasm identification on two
very different data sets: a collection of
5.9 million tweets collected from Twit-
ter, and a collection of 66000 product re-
views from Amazon. Using the Mechani-
cal Turk we created a gold standard sam-
ple in which each sentence was tagged by
3 annotators, obtaining F-scores of 0.78 on
the product reviews dataset and 0.83 on
the Twitter dataset. We discuss the dif-
ferences between the datasets and how the
algorithm uses them (e.g., for the Amazon
dataset the algorithm makes use of struc-
tured information). We also discuss the
utility of Twitter #sarcasm hashtags for the
task.
1 Introduction
Sarcasm (also known as verbal irony) is a sophis-
ticated form of speech act in which the speakers
convey their message in an implicit way. One in-
herent characteristic of the sarcastic speech act is
that it is sometimes hard to recognize. The dif-
ficulty in recognition of sarcasm causes misun-
derstanding in everyday communication and poses
problems to many NLP systems such as online
review summarization systems, dialogue systems
or brand monitoring systems due to the failure of
state of the art sentiment analysis systems to detect
sarcastic comments. In this paper we experiment
with a semi-supervised framework for automatic
identification of sarcastic sentences.
One definition for sarcasm is: the activity of
saying or writing the opposite of what you mean,
or of speaking in a way intended to make someone
else feel stupid or show them that you are angry
(Macmillan English Dictionary (2007)). Using the
former definition, sarcastic utterances appear in
many forms (Brown, 1980; Gibbs and O?Brien,
1991). It is best to present a number of examples
which show different facets of the phenomenon,
followed by a brief review of different aspects of
the sarcastic use. The sentences are all taken from
our experimental data sets:
1. ?thank you Janet Jackson for yet another
year of Super Bowl classic rock!? (Twitter)
2. ?He?s with his other woman: XBox 360. It?s
4:30 fool. Sure I can sleep through the gun-
fire? (Twitter)
3. ?Wow GPRS data speeds are blazing fast.?
(Twitter)
4. ?[I] Love The Cover? (book, amazon)
5. ?Defective by design? (music player, ama-
zon)
Example (1) refers to the supposedly lame mu-
sic performance in super bowl 2010 and attributes
it to the aftermath of the scandalous performance
of Janet Jackson in the previous year. Note that the
previous year is not mentioned and the reader has
to guess the context (use universal knowledge).
The words yet and another might hint at sarcasm.
107
Example (2) is composed of three short sentences,
each of them sarcastic on its own. However, com-
bining them in one tweet brings the sarcasm to
its extreme. Example (3) is a factual statement
without explicit opinion. However, having a fast
connection is a positive thing. A possible sar-
casm emerges from the over exaggeration (?wow?,
?blazing-fast?).
Example (4) from Amazon, might be a genuine
compliment if it appears in the body of the review.
However, recalling the expression ?don?t judge a
book by its cover?, choosing it as the title of the
review reveals its sarcastic nature. Although the
negative sentiment is very explicit in the iPod re-
view (5), the sarcastic effect emerges from the pun
that assumes the knowledge that the design is one
of the most celebrated features of Apple?s prod-
ucts. (None of the above reasoning was directly
introduced to our algorithm.)
Modeling the underlying patterns of sarcastic
utterances is interesting from the psychological
and cognitive perspectives and can benefit var-
ious NLP systems such as review summariza-
tion (Popescu and Etzioni, 2005; Pang and Lee,
2004; Wiebe et al, 2004; Hu and Liu, 2004) and
dialogue systems. Following the ?brilliant-but-
cruel? hypothesis (Danescu-Niculescu-Mizil et al,
2009), it can help improve ranking and recommen-
dation systems (Tsur and Rappoport, 2009). All
systems currently fail to correctly classify the sen-
timent of sarcastic sentences.
In this paper we utilize the semi-supervised sar-
casm identification algorithm (SASI) of (Tsur et
al., 2010). The algorithm employs two modules:
semi supervised pattern acquisition for identify-
ing sarcastic patterns that serve as features for a
classifier, and a classification stage that classifies
each sentence to a sarcastic class. We experiment
with two radically different datasets: 5.9 million
tweets collected from Twitter, and 66000 Amazon
product reviews. Although for the Amazon dataset
the algorithm utilizes structured information, re-
sults for the Twitter dataset are higher. We discuss
the possible reasons for this, and also the utility
of Twitter #sarcasm hashtags for the task. Our al-
gorithm performed well in both domains, substan-
tially outperforming a strong baseline based on se-
mantic gap and user annotations. To further test its
robustness we also trained the algorithm in a cross
domain manner, achieving good results.
2 Data
The datasets we used are interesting in their own
right for many applications. In addition, our algo-
rithm utilizes some aspects that are unique to these
datasets. Hence, before describing the algorithm,
we describe the datasets in detail.
Twitter Dataset. Since Twitter is a relatively
new service, a somewhat lengthy description of
the medium and the data is appropriate.
Twitter is a very popular microblogging service.
It allows users to publish and read short messages
called tweets (also used as a verb: to tweet: the act
of publishing on Twitter). The tweet length is re-
stricted to 140 characters. A user who publishes a
tweet is referred to as a tweeter and the readers are
casual readers or followers if they are registered to
get al tweets by this tweeter.
Apart from simple text, tweets may contain ref-
erences to url addresses, references to other Twit-
ter users (these appear as @<user>) or a con-
tent tag (called hashtags) assigned by the tweeter
(#<tag>). An example of a tweet is: ?listen-
ing to Andrew Ridgley by Black Box Recorder on
@Grooveshark: http://tinysong.com/cO6i #good-
music?, where ?grooveshark? is a Twitter user
name and #goodmusic is a tag that allows to
search for tweets with the same tag. Though fre-
quently used, these types of meta tags are optional.
In order to ignore specific references we substi-
tuted such occurrences with special tags: [LINK],
[USER] and [HASHTAG] thus we have ?listen-
ing to Andrew Ridgley by Black Box Recorder on
[USER]: [LINK] [HASHTAG]?. It is important
to mention that hashtags are not formal and each
tweeter can define and use new tags as s/he likes.
The number of special tags in a tweet is only
subject to the 140 characters constraint. There is
no specific grammar that enforces the location of
special tags within a tweet.
The informal nature of the medium and the 140
characters length constraint encourages massive
use of slang, shortened lingo, ascii emoticons and
other tokens absent from formal lexicons.
These characteristics make Twitter a fascinat-
ing domain for NLP applications, although posing
great challenges due to the length constraint, the
complete freedom of style and the out of discourse
nature of tweets.
We used 5.9 million unique tweets in our
dataset: the average number of words is 14.2
108
words per tweet, 18.7% contain a url, 35.3% con-
tain reference to another tweeter and 6.9% contain
at least one hashtag1.
The #sarcasm hashtag One of the hashtags
used by Twitter users is dedicated to indicate sar-
castic tweets. An example of the use of the tag
is: ?I guess you should expect a WONDERFUL
video tomorrow. #sarcasm?. The sarcastic hashtag
is added by the tweeter. This hashtag is used in-
frequently as most users are not aware of it, hence,
the majority of sarcastic tweets are not explicitly
tagged by the tweeters. We use tagged tweets as
a secondary gold standard. We discuss the use of
this tag in Section 5.
Amazon dataset. We used the same dataset
used by (Tsur et al, 2010), containing 66000 re-
views for 120 products from Amazon.com. The
corpus contained reviews for books from differ-
ent genres and various electronic products. Ama-
zon reviews are much longer than tweets (some
reach 2000 words, average length is 953 charac-
ters), they are more structured and grammatical
(good reviews are very structured) and they come
in a known context of a specific product. Reviews
are semi-structured as besides the body of the re-
view they all have the following fields: writer,
date, star rating (the overall satisfaction of the re-
view writer) and a one line summary.
Reviews refer to a specific product and rarely
address each other. Each review sentence is, there-
fore, part of a context ? the specific product, the
star rating, the summary and other sentences in
that review. In that sense, sentences in the Ama-
zon dataset differ radically from the contextless
tweets. It is worth mentioning that the majority
of reviews are on the very positive side (star rating
average of 4.2 stars).
3 Classification Algorithm
Our algorithm is semi-supervised. The input is
a relatively small seed of labeled sentences. The
seed is annotated in a discrete range of 1 . . . 5
where 5 indicates a clearly sarcastic sentence and
1 indicates a clear absence of sarcasm. A 1 . . . 5
scale was used in order to allow some subjectiv-
ity and since some instances of sarcasm are more
explicit than others.
1The Twitter data was generously provided to us by Bren-
dan O?Connor.
Given the labeled sentences, we extracted a set
of features to be used in feature vectors. Two basic
feature types are utilized: syntactic and pattern-
based features. We constructed feature vectors for
each of the labeled examples in the training set and
used them to build a classifier model and assign
scores to unlabeled examples. We next provide a
description of the algorithmic framework of (Tsur
et al, 2010).
Data preprocessing A sarcastic utterance usu-
ally has a target. In the Amazon dataset these
targets can be exploited by a computational al-
gorithm, since each review targets a product, its
manufacturer or one of its features, and these are
explicitly represented or easily recognized. The
Twitter dataset is totally unstructured and lacks
textual context, so we did not attempt to identify
targets.
Our algorithmic methodology is based on
patterns. We could use patterns that include
the targets identified in the Amazon dataset.
However, in order to use less specific patterns,
we automatically replace each appearance
of a product, author, company, book name
(Amazon) and user, url and hashtag (Twitter)
with the corresponding generalized meta tags
?[PRODUCT]?,?[COMPANY]?,?[TITLE]? and
?[AUTHOR]? tags2 and ?[USER]?,?[LINK]? and
?[HASHTAG]?. We also removed all HTML tags
and special symbols from the review text.
Pattern extraction Our main feature type is
based on surface patterns. In order to extract such
patterns automatically, we followed the algorithm
given in (Davidov and Rappoport, 2006). We clas-
sified words into high-frequency words (HFWs)
and content words (CWs). A word whose cor-
pus frequency is more (less) than FH (FC) is con-
sidered to be a HFW (CW). Unlike in (Davidov
and Rappoport, 2006), we consider all punctuation
characters as HFWs. We also consider [product],
[company], [title], [author] tags as HFWs for pat-
tern extraction. We define a pattern as an ordered
sequence of high frequency words and slots for
content words. The FH and FC thresholds were
set to 1000 words per million (upper bound for
FC) and 100 words per million (lower bound for
FH )3.
2Appropriate names are provided with each review so this
replacement can be done automatically.
3Note that FH and FC set bounds that allow overlap be-
tween some HFWs and CWs.
109
The patterns allow 2-6 HFWs and 1-6 slots for
CWs. For each sentence it is possible to gener-
ate dozens of patterns that may overlap. For ex-
ample, given a sentence ?Garmin apparently does
not care much about product quality or customer
support?, we have generated several patterns in-
cluding ?[COMPANY] CW does not CW much?,
?does not CW much about CW CW or?, ?not CW
much? and ?about CW CW or CW CW.?. Note
that ?[COMPANY]? and ?.? are treated as high
frequency words.
Pattern selection The pattern extraction stage
provides us with hundreds of patterns. However,
some of them are either too general or too specific.
In order to reduce the feature space, we have used
two criteria to select useful patterns.
First, we removed all patterns which appear
only in sentences originating from a single prod-
uct/book (Amazon). Such patterns are usually
product-specific. Next we removed all patterns
which appear in the seed both in some example la-
beled 5 (clearly sarcastic) and in some other exam-
ple labeled 1 (obviously not sarcastic). This filters
out frequent generic and uninformative patterns.
Pattern selection was performed only on the Ama-
zon dataset as it exploits review?s meta data.
Pattern matching Once patterns are selected,
we have used each pattern to construct a single en-
try in the feature vectors. For each sentence we
calculated a feature value for each pattern as fol-
lows:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1 : Exact match ? all the pattern components
appear in the sentence in correct
order without any additional words.
? : Sparse match ? same as exact match
but additional non-matching words can be
inserted between pattern components.
? ? n/N : Incomplete match ? only n > 1 of N pattern
components appear in the sentence,
while some non-matching words can
be inserted in-between. At least one of the
appearing components should be a HFW.
0 : No match ? nothing or only a single
pattern component appears in the sentence.
0 ? ? ? 1 and 0 ? ? ? 1 are parameters we use
to assign reduced scores for imperfect matches.
Since the patterns we use are relatively long, ex-
act matches are uncommon, and taking advantage
of partial matches allows us to significantly re-
duce the sparsity of the feature vectors. We used
?\? 0.05 0.1 0.2
0.05 0.48 0.45 0.39
0.1 0.50 0.51 0.40
0.2 0.40 0.42 0.33
Table 1: Results (F-Score for ?no enrichment? mode) of
cross validation with various values for ? and ? on Twit-
ter+Amazon data
? = ? = 0.1 in all experiments. Table 1 demon-
strates the results obtained with different values
for ? and ?.
Thus, for the sentence ?Garmin apparently does
not care much about product quality or customer
support?, the value for ?[company] CW does not?
would be 1 (exact match); for ?[company] CW
not? would be 0.1 (sparse match due to insertion
of ?does?); and for ?[company] CW CW does not?
would be 0.1 ? 4/5 = 0.08 (incomplete match
since the second CW is missing).
Punctuation-based features In addition to
pattern-based features we used the following
generic features: (1) Sentence length in words,
(2) Number of ?!? characters in the sentence, (3)
Number of ??? characters in the sentence, (4)
Number of quotes in the sentence, and (5) Num-
ber of capitalized/all capitals words in the sen-
tence. All these features were normalized by di-
viding them by the (maximal observed value ? av-
eraged maximal value of the other feature groups),
thus the maximal weight of each of these fea-
tures is equal to the averaged weight of a single
pattern/word/n-gram feature.
Data enrichment Since we start with only a
small annotated seed for training (particularly, the
number of clearly sarcastic sentences in the seed is
modest) and since annotation is noisy and expen-
sive, we would like to find more training examples
without requiring additional annotation effort.
To achieve this, we posited that sarcastic sen-
tences frequently co-appear in texts with other sar-
castic sentences (i.e. example (2) in Section 1).
We performed an automated web search using the
Yahoo! BOSS API4, where for each sentence s in
the training set (seed), we composed a search en-
gine query qs containing this sentence5. We col-
lected up to 50 search engine snippets for each
example and added the sentences found in these
snippets to the training set. The label (level of sar-
4http://developer.yahoo.com/search/boss.
5If the sentence contained more than 6 words, only the
first 6 words were included in the search engine query.
110
casm) Label(sq) of a newly extracted sentence sq
is similar to the label Label(s) of the seed sen-
tence s that was used for the query that acquired it.
The seed sentences together with newly acquired
sentences constitute the (enriched) training set.
Data enrichment was performed only for the
Amazon dataset where we have a manually tagged
seed and the sentence structure is closer to stan-
dard English grammar. We refer the reader to
(Tsur et al, 2010) for more details about the en-
richment process and for a short discussion about
the usefulness of web-based data enrichment in the
scope of sarcasm recognition.
Classification In order to assign a score to new
examples in the test set we use a k-nearest neigh-
bors (kNN)-like strategy. We construct feature
vectors for each example in the training and test
sets. We would like to calculate the score for each
example in the test set. For each feature vector v in
the test set, we compute the Euclidean distance to
each of the matching vectors in the extended train-
ing set, where matching vectors share at least one
pattern feature with v.
Let ti, i = 1..k be the k vectors with lowest
Euclidean distance to v6. Then v is classified with
a label l as follows:
Count(l) = Fraction of training vectors with label l
Label(v) =
[
1
k
?
i
Count(Label(ti)) ? Label(ti)
?
j Count(label(tj))
]
Thus the score is a weighted average of the k clos-
est training set vectors. If there are less than k
matching vectors for the given example then fewer
vectors are used in the computation. If there are
no matching vectors found for v, we assigned the
default value Label(v) = 1, since sarcastic sen-
tences are fewer in number than non-sarcastic ones
(this is a ?most common tag? strategy).
4 Evaluation Setup
Seed and extended training sets (Amazon). As
described in the previous section, SASI is semi su-
pervised, hence requires a small seed of annotated
data. We used the same seed of 80 positive (sar-
castic) examples and 505 negative examples de-
scribed at (Tsur et al, 2010).
After automatically expanding the training set,
our training data now contains 471 positive exam-
ples and 5020 negative examples. These ratios are
6We used k = 5 for all experiments.
to be expected, since non-sarcastic sentences out-
number sarcastic ones, definitely when most on-
line reviews are positive (Liu et al, 2007). This
generally positive tendency is also reflected in our
data ? the average number of stars is 4.12.
Seed training set with #sarcasm (Twitter). We
used a sample of 1500 tweets marked with the
#sarcasm hashtag as a positive set that represents
sarcasm styles special to Twitter. However, this set
is very noisy (see discussion in Section 5).
Seed training set (cross domain). Results ob-
tained by training on the 1500 #sarcasm hash-
tagged tweets were not promising. Examination of
the #sarcasm tagged tweets shows that the annota-
tion is biased and noisy as we discuss in length
in Section 5. A better annotated set was needed
in order to properly train the algorithm. Sarcas-
tic tweets are sparse and hard to find and annotate
manually. In order to overcome sparsity we used
the positive seed annotated on the Amazon dataset.
The training set was completed by manually se-
lected negative example from the Twitter dataset.
Note that in this setting our training set is thus of
mixed domains.
4.1 Star-sentiment baseline
Many studies on sarcasm suggest that sarcasm
emerges from the gap between the expected utter-
ance and the actual utterance (see echoic mention,
allusion and pretense theories in Related Work
Section( 6)). We implemented a baseline designed
to capture the notion of sarcasm as reflected by
these models, trying to meet the definition ?saying
the opposite of what you mean in a way intended
to make someone else feel stupid or show you are
angry?.
We exploit the meta-data provided by Amazon,
namely the star rating each reviewer is obliged
to provide, in order to identify unhappy review-
ers. From this set of negative reviews, our base-
line classifies as sarcastic those sentences that ex-
hibit strong positive sentiment. The list of positive
sentiment words is predefined and captures words
typically found in reviews (for example, ?great?,
?excellent?, ?best?, ?top?, ?exciting?, etc).
4.2 Evaluation procedure
We used two experimental frameworks to test
SASI?s accuracy. In the first experiment we eval-
uated the pattern acquisition process, how consis-
tent it is and to what extent it contributes to correct
111
classification. We did that by 5-fold cross valida-
tion over the seed data.
In the second experiment we evaluated SASI on
a test set of unseen sentences, comparing its out-
put to a gold standard annotated by a large number
of human annotators (using the Mechanical Turk).
This way we verify that there is no over-fitting and
that the algorithm is not biased by the notion of
sarcasm of a single seed annotator.
5-fold cross validation (Amazon). In this ex-
perimental setting, the seed data was divided to 5
parts and a 5-fold cross validation test is executed.
Each time, we use 4 parts of the seed as the train-
ing data and only this part is used for the feature
selection and data enrichment. This 5-fold pro-
cess was repeated ten times. This procedure was
repeated with different sets of optional features.
We used 5-fold cross validation and not the
standard 10-fold since the number of seed exam-
ples (especially positive) is relatively small hence
10-fold is too sensitive to the broad range of possi-
ble sarcastic patterns (see the examples in Section
1).
Classifying new sentences (Amazon & Twitter).
Evaluation of sarcasm is a hard task due to the
elusive nature of sarcasm, as discussed in Sec-
tion 1. In order to evaluate the quality of our al-
gorithm, we used SASI to classify all sentences
in both corpora (besides the small seed that was
pre-annotated and was used for the evaluation in
the 5-fold cross validation experiment). Since it
is impossible to created a gold standard classifica-
tion of each and every sentence in the corpus, we
created a small test set by sampling 90 sentences
which were classified as sarcastic (labels 3-5) and
90 sentences classified as not sarcastic (labels 1,2).
The sampling was performed on the whole corpus
leaving out only the seed data.
Again, the meta data available in the Amazon
dataset alows us a stricter evaluation. In order
to make the evaluation harder for our algorithm
and more relevant, we introduced two constraints
to the sampling process: i) we sampled only sen-
tences containing a named-entity or a reference to
a named entity. This constraint was introduced in
order to keep the evaluation set relevant, since sen-
tences that refer to the named entity (the target of
the review) are more likely to contain an explicit
or implicit sentiment. ii) we restricted the non-
sarcastic sentences to belong to negative reviews
(1-3 stars) so that all sentences in the evaluation
set are drawn from the same population, increas-
ing the chances they convey various levels of di-
rect or indirect negative sentiment7.
Experimenting with the Twitter dataset, we sim-
ply classified each tweet into one of 5 classes
(class 1: not sarcastic, class 5: clearly sarcastic)
according to the label given by the algorithm. Just
like the evaluation of the algorithm on the Amazon
dataset, we created a small evaluation set by sam-
pling 90 sentences which were classified as sarcas-
tic (labels 3-5) and 90 sentences classified as not
sarcastic (labels 1,2).
Procedure Each evaluation set was randomly
divided to 5 batches. Each batch contained 36 sen-
tences from the evaluation set and 4 anchor sen-
tences: two with sarcasm and two sheer neutral.
The anchor sentences were not part of the test set
and were the same in all five batches. The purpose
of the anchor sentences is to control the evaluation
procedure and verify that annotation is reasonable.
We ignored the anchor sentences when assessing
the algorithm?s accuracy.
We used Amazon?s Mechanical Turk8 service
in order to create a gold standard for the evalua-
tion. We employed 15 annotators for each eval-
uation set. We used a relatively large number of
annotators in order to overcome the possible bias
induced by subjectivity (Muecke, 1982). Each an-
notator was asked to assess the level of sarcasm of
each sentence of a set of 40 sentences on a scale of
1-5. In total, each sentence was annotated by three
different annotators.
Inter Annotator Agreement. To simplify the
assessment of inter-annotator agreement, the scal-
ing was reduced to a binary classification where 1
and 2 were marked as non-sarcastic and 3-5 as sar-
castic (recall that 3 indicates a hint of sarcasm and
5 indicates ?clearly sarcastic?). We checked the
Fleiss? ? statistic to measure agreement between
multiple annotators. The inter-annotator agree-
ment statistic was ? = 0.34 on the Amazon dataset
and ? = 0.41 on the Twitter dataset.
These agreement statistics indicates a fair
agreement. Given the fuzzy nature of the task at
7Note that the second constraint makes the problem less
easy. If taken from all reviews, many of the sentences would
be positive sentences which are clearly non-sarcastic. Doing
this would bias selection to positive vs. negative samples in-
stead of sarcastic-nonsarcastic samples.
8https://www.mturk.com/mturk/welcome
112
Prec. Recall Accuracy F-score
punctuation 0.256 0.312 0.821 0.281
patterns 0.743 0.788 0.943 0.765
pat+punct 0.868 0.763 0.945 0.812
enrich punct 0.4 0.390 0.832 0.395
enrich pat 0.762 0.777 0.937 0.769
all: SASI 0.912 0.756 0.947 0.827
Table 2: 5-fold cross validation results on the Amazon gold
standard using various feature types. punctuation: punctua-
tion mark;, patterns: patterns; enrich: after data enrichment;
enrich punct: data enrichment based on punctuation only; en-
rich pat: data enrichment based on patterns only; SASI: all
features combined.
hand, this ? value is certainly satisfactory. We at-
tribute the better agreement on the twitter data to
the fact that in twitter each sentence (tweet) is con-
text free, hence the sentiment in the sentence is ex-
pressed in a way that can be perceived more easily.
Sentences from product reviews come as part of a
full review, hence the the sarcasm sometimes re-
lies on other sentences in the review. In our evalu-
ation scheme, our annotators were presented with
individual sentences, making the agreement lower
for those sentences taken out of their original con-
text. The agreement on the control set (anchor sen-
tences) had ? = 0.53.
Using Twitter #sarcasm hashtag. In addition to
the gold standard annotated using the Mechanical
Turk, we collected 1500 tweets that were tagged
#sarcastic by their tweeters. We call this sample
the hash-gold standard. It was used to further eval-
uate recall. This set (along with the negative sam-
ple) was used for a 5-fold cross validation in the
same manner describe for Amazon.
5 Results and discussion
5-fold cross validation (Amazon). Results are
analyzed and discussed in detail in (Tsur et al,
2010), however, we summarize it here (Table 2)
in order to facilitate comparison with the results
obtained on the Twitter dataset. SASI, including
all components, exhibits the best overall perfor-
mances with 91.2% precision and with F-Score
of 0.827. Interestingly, although data enrichment
brings SASI to the best performance in both preci-
sion and F-score, patterns+punctuations achieves
almost comparable results.
Newly introduced sentences (Amazon). In the
second experiment we evaluated SASI based on a
gold standard annotation created by 15 annotators.
Table 3 presents the results of our algorithm as
well as results of the heuristic baseline that makes
Prec. Recall FalsePos FalseNeg F Score
Star-sent. 0.5 0.16 0.05 0.44 0.242
SASI (AM) 0.766 0.813 0.11 0.12 0.788
SASI (TW) 0.794 0.863 0.094 0.15 0.827
Table 3: Evaluation on the Amazon (AM) and the Twitter
(TW) evaluation sets obtained by averaging on 3 human an-
notations per sentence. TW results were obtained with cross-
domain training.
Prec. Recall Accuracy F-score
punctuation 0.259 0.26 0.788 0.259
patterns 0.765 0.326 0.889 0.457
enrich punct 0.18 0.316 0.76 0.236
enrich pat 0.685 0.356 0.885 0.47
all no enrich 0.798 0.37 0.906 0.505
all SASI: 0.727 0.436 0.896 0.545
Table 4: 5-fold cross validation results on the Twitter hash-
gold standard using various feature types. punctuation: punc-
tuation marks; patterns: patterns; enrich: after data enrich-
ment; enrich punct: data enrichment based on punctuation
only; enrich pat: data enrichment based on patterns only;
SASI: all features combined.
use of meta-data, designed to capture the gap be-
tween an explicit negative sentiment (reflected by
the review?s star rating) and explicit positive senti-
ment words used in the review. Precision of SASI
is 0.766, a significant improvement over the base-
line with precision of 0.5.
The F-score shows more impressive improve-
ment as the baseline shows decent precision but a
very limited recall since it is incapable of recog-
nizing subtle sarcastic sentences. These results fit
the works of (Brown, 1980; Gibbs and O?Brien,
1991) claiming many sarcastic utterances do not
conform to the popular definition of ?saying or
writing the opposite of what you mean?. Table 3
also presents the false positive and false negative
ratios. The low false negative ratio of the baseline
confirms that while recognizing a common type
of sarcasm, the naive definition of sarcasm cannot
capture many other types sarcasm.
Newly introduced sentences (Twitter). Results
on the Twitter dataset are even better than those
obtained on the Amazon dataset, with accuracy of
0.947 (see Table 3 for precision and recall).
Tweets are less structured and are context free,
hence one would expect SASI to perform poorly
on tweets. Moreover, the positive part of the seed
is taken from the Amazon corpus hence might
seem tailored to sarcasm type targeted at prod-
ucts and part of a harsh review. On top of that,
the positive seed introduces some patterns with
tags that never occur in the Twitter test set ([prod-
uct/company/title/author]).
113
Our explanation of the excellent results is three-
fold: i) SASI?s robustness is achieved by the sparse
match (?) and incomplete match (?) that toler-
ate imperfect pattern matching and enable the use
of variations of the patterns in the learned feature
vector. ? and ? allow the introduction of patterns
with components that are absent from the posi-
tive seed, and can perform even with patterns that
contain special tags that are not part of the test
set. ii) SASI learns a model which spans a feature
space with more than 300 dimensions. Only part
of the patterns consist of meta tags that are spe-
cial to product reviews, the rest are strong enough
to capture the structure of general sarcastic sen-
tences and not product-specific sarcastic sentences
only. iii) Finally, in many cases, it might be that
the contextless nature of Twitter forces tweeters to
express sarcasm in a way that is easy to understand
from individual sentence. Amazon sentences co-
appear with other sentences (in the same review)
thus the sarcastic meaning emerges from the con-
text. Our evaluation scheme presents the annota-
tors with single sentences therefore Amazon sen-
tences might be harder to agree on.
hash gold standard (Twitter). In order to fur-
ther test out algorithm we built a model consist-
ing of the positive sample of the Amazon training,
the #sarcasm hash-tagged tweets and a sample of
non sarcastic tweets as the negative training set.
We evaluated it in a 5-fold cross validation man-
ner (only against the hash-gold standard). While
precision is still high with 0.727, recall drops to
0.436 and the F-Score is 0.545.
Looking at the hash-gold standard set, we ob-
served three main uses for the #sarcasm hashtag.
Differences between the various uses can explain
the relatively low recall. i) The tag is used as a
search anchor. Tweeters add the hashtag to tweets
in order to make them retrievable when searching
for the tag. ii) The tag is often abused and added
to non sarcastic tweets, typically to clarify that a
previous tweet should have been read sarcastically,
e.g.: ?@wrightfan05 it was #Sarcasm ?. iii) The
tag serves as a sarcasm marker in cases of a very
subtle sarcasm where the lack of context, the 140
length constraint and the sentence structure make
it impossible to get the sarcasm without the ex-
plicit marker. Typical examples are: ?#sarcasm
not at all.? or ?can?t wait to get home tonite #sar-
casm.?, which cannot be decided sarcastic without
the full context or the #sarcasm marker.
These three observations suggest that the hash-
gold standard is noisy (containing non-sarcastic
tweets) and is biased toward the hardest (insepa-
rable) forms of sarcasm where even humans get
it wrong without an explicit indication. Given
the noise and the bias, the recall is not as bad as
the raw numbers suggest and is actually in synch
with the results obtained on the Mechanical Turk
human-annotated gold standard. Table 4 presents
detailed results and the contribution of each type
of feature to the classification.
We note that the relative sparseness of sarcas-
tic utterances in everyday communication as well
as in these two datasets make it hard to accurately
estimate the recall value over these huge unanno-
tated data sets. Our experiment, however, indi-
cates that we achieve reasonable recall rates.
Punctuation Surprisingly, punctuation marks
serve as the weakest predictors, in contrast to Tep-
permann et al (2006). An exception is three con-
secutive dots, which when combine with other fea-
tures constitute a strong predictor. Interestingly
though, while in the cross validation experiments
SASI performance varies greatly (due to the prob-
lematic use of the #sarcasm hashtag, described
previously), performance based only on punctua-
tion are similar (Table 2 and Table 4).
Tsur et al (2010) presents some additional ex-
amples for the contribution of each type of feature
and their combinations.
6 Related Work
While the use of irony and sarcasm is well stud-
ied from its linguistic and psychologic aspects
(Muecke, 1982; Stingfellow, 1994; Gibbs and Col-
ston, 2007), automatic recognition of sarcasm is a
novel task, addressed only by few works. In the
context of opinion mining, sarcasm is mentioned
briefly as a hard nut that is yet to be cracked, see
comprehensive overview by (Pang and Lee, 2008).
Tepperman et al (2006) identify sarcasm in
spoken dialogue systems, their work is restricted
to sarcastic utterances that contain the expres-
sion ?yeah-right? and it depends heavily on cues
in the spoken dialogue such as laughter, pauses
within the speech stream, the gender (recognized
by voice) of the speaker and prosodic features.
Burfoot and Baldwin (2009) use SVM to deter-
mine whether newswire articles are true or satir-
ical. They introduce the notion of validity which
models absurdity via a measure somewhat close to
114
PMI. Validity is relatively lower when a sentence
includes a made-up entity or when a sentence con-
tains unusual combinations of named entities such
as, for example, those in the satirical article be-
ginning ?Missing Brazilian balloonist Padre spot-
ted straddling Pink Floyd flying pig?. We note
that while sarcasm can be based on exaggeration
or unusual collocations, this model covers only a
limited subset of the sarcastic utterances.
Tsur et al (2010) propose a semi supervised
framework for recognition of sarcasm. The pro-
posed algorithm utilizes some features specific to
(Amazon) product reviews. This paper continues
this line, proposing SASI a robust algorithm that
successfully captures sarcastic sentences in other,
radically different, domains such as twitter.
Utsumi (1996; 2000) introduces the implicit dis-
play theory, a cognitive computational framework
that models the ironic environment. The complex
axiomatic system depends heavily on complex for-
malism representing world knowledge. While
comprehensive, it is currently impractical to im-
plement on a large scale or for an open domain.
Mihalcea and Strapparava (2005) and Mihalcea
and Pulman (2007) present a system that identi-
fies humorous one-liners. They classify sentences
using naive Bayes and SVM. They conclude that
the most frequently observed semantic features are
negative polarity and human-centeredness. These
features are also observed in some sarcastic utter-
ances.
Some philosophical, psychological and linguis-
tic theories of irony and sarcasm are worth refer-
encing as a theoretical framework: the constraints
satisfaction theory (Utsumi, 1996; Katz, 2005),
the role playing theory (Clark and Gerrig, 1984),
the echoic mention framework (Wilson and Sper-
ber, 1992) and the pretence framework (Gibbs,
1986). These are all based on violation of the max-
ims proposed by Grice (1975).
7 Conclusion
We used SASI, the first robust algorithm for recog-
nition of sarcasm, to experiment with a novel
Twitter dataset and compare performance with an
Amazon product reviews dataset. Evaluating in
various ways and with different parameters con-
figurations, we achieved high precision, recall and
F-Score on both datasets even for cross-domain
training and with no need for domain adaptation.
In the future we will test the contribution of
sarcasm recognition for review ranking and sum-
marization systems and for brand monitoring sys-
tems.
References
R. L. Brown. 1980. The pragmatics of verbal irony.
In R. W. Shuy and A. Snukal, editors, Language use
and the uses of language, pages 111?127. George-
town University Press.
Clint Burfoot and Timothy Baldwin. 2009. Automatic
satire detection: Are you having a laugh? In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, pages 161?164, Suntec, Singapore, August.
Association for Computational Linguistics.
H. Clark and R. Gerrig. 1984. On the pretence the-
ory of irony. Journal of Experimental Psychology:
General, 113:121?126.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opinions
are received by online communities: A case study on
amazon.com helpfulness votes. Jun.
D. Davidov and A. Rappoport. 2006. Efficient
unsupervised discovery of word categories using
symmetric patterns and high frequency words. In
COLING-ACL.
Macmillan English Dictionary. 2007. Macmillan En-
glish Dictionary. Macmillan Education, 2 edition.
Raymond W Gibbs and Herbert L. Colston, editors.
2007. Irony in Language and Thought. Routledge
(Taylor and Francis), New York.
R. W. Gibbs and J. E. O?Brien. 1991. Psychological
aspects of irony understanding. Journal of Pragmat-
ics, 16:523?530.
R. Gibbs. 1986. On the psycholinguistics of sar-
casm. Journal of Experimental Psychology: Gen-
eral, 105:3?15.
H. P. Grice. 1975. Logic and conversation. In Peter
Cole and Jerry L. Morgan, editors, Syntax and se-
mantics, volume 3. New York: Academic Press.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In KDD ?04: Proceed-
ings of the tenth ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 168?177, New York, NY, USA. ACM.
A. Katz. 2005. Discourse and social-cultural factors
in understanding non literal language. In Colston H.
and Katz A., editors, Figurative language compre-
hension: Social and cultural influences, pages 183?
208. Lawrence Erlbaum Associates.
115
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou Huang,
and Ming Zhou. 2007. Low-quality product re-
view detection in opinion summarization. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 334?342.
Rada Mihalcea and Stephen G. Pulman. 2007. Char-
acterizing humour: An exploration of features in hu-
morous texts. In CICLing, pages 337?347.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: Investigations in automatic humor
recognition. pages 531?538, Vancouver, Canada.
D.C. Muecke. 1982. Irony and the ironic. Methuen,
London, New York.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the ACL, pages 271?278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Now Publishers Inc, July.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 339?346,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Frank Jr. Stingfellow. 1994. The Meaning of Irony.
State University of NY, New York.
J. Tepperman, D. Traum, and S. Narayanan. 2006.
Yeah right: Sarcasm recognition for spoken dialogue
systems. In InterSpeech ICSLP, Pittsburgh, PA.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most help-
ful book reviews. In International AAAI Conference
on Weblogs and Social Media.
Oren Tsur, Dmitry Davidiv, and Ari Rappoport. 2010.
Icwsm ? a great catchy name: Semi-supervised
recognition of sarcastic sentences in product re-
views. In International AAAI Conference on We-
blogs and Social Media.
Akira Utsumi. 1996. A unified theory of irony and
its computational formalization. In COLING, pages
962?967.
Akira Utsumi. 2000. Verbal irony as implicit dis-
play of ironic environment: Distinguishing ironic
utterances from nonirony. Journal of Pragmatics,
32(12):1777?1806.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277? 308, January.
D. Wilson and D. Sperber. 1992. On verbal irony. Lin-
gua, 87:53?76.
116
