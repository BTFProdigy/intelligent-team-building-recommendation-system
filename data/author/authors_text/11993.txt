Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 289?292,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Automatic Cost Estimation for Tree Edit Distance Using Particle Swarm
Optimization
Yashar Mehdad
University of Trento and FBK - Irst
Trento, Italy
mehdad@fbk.eu
Abstract
Recently, there is a growing interest in
working with tree-structured data in differ-
ent applications and domains such as com-
putational biology and natural language
processing. Moreover, many applications
in computational linguistics require the
computation of similarities over pair of
syntactic or semantic trees. In this context,
Tree Edit Distance (TED) has been widely
used for many years. However, one of the
main constraints of this method is to tune
the cost of edit operations, which makes
it difficult or sometimes very challenging
in dealing with complex problems. In this
paper, we propose an original method to
estimate and optimize the operation costs
in TED, applying the Particle Swarm Op-
timization algorithm. Our experiments on
Recognizing Textual Entailment show the
success of this method in automatic esti-
mation, rather than manual assignment of
edit costs.
1 Introduction
Among many tree-based algorithms, Tree Edit
Distance (TED) has offered many solutions for
various NLP applications such as information re-
trieval, information extraction, similarity estima-
tion and textual entailment. Tree edit distance is
defined as the minimum costly set of basic oper-
ations transforming one tree to another. In com-
mon, TED approaches use an initial fixed cost for
each operation.
Generally, the initial assigned cost to each edit
operation depends on the nature of nodes, appli-
cations and dataset. For example the probabil-
ity of deleting a function word from a string is
not the same as deleting a symbol in RNA struc-
ture. According to this fact, tree comparison may
be affected by application and dataset. A solu-
tion to this problem is assigning the cost to each
edit operation empirically or based on the expert
knowledge and recommendation. These methods
emerge a critical problem when the domain, field
or application is new and the level of expertise and
empirical knowledge is very limited.
Other approaches towards this problem tried to
learn a generative or discriminative probabilistic
model (Bernard et al, 2008) from the data. One
of the drawbacks of those approaches is that the
cost values of edit operations are hidden behind
the probabilistic model. Additionally, the cost can
not be weighted or varied according to the tree
context and node location.
In order to overcome these drawbacks, we are
proposing a stochastic method based on Particle
Swarm Optimization (PSO) to estimate the cost of
each edit operation based on the user defined ap-
plication and dataset. A further advantage of the
method, besides automatic learning of the opera-
tion costs, is to investigate the cost values in order
to better understand how TED approaches the ap-
plication and data in different domains.
As for the experiments, we learn a model for
recognizing textual entailment, based on TED,
where the input is a pair of strings represented as
syntactic dependency trees. Our results illustrate
that optimizing the cost of each operation can dra-
matically affect the accuracy and achieve a better
model for recognizing textual entailment.
2 Tree Edit Distance
Tree edit distance measure is a similarity metric
for rooted ordered trees. Assuming that we have
two rooted and ordered trees, it means that one
node in each tree is assigned as a root and the
children of each node are ordered. The edit op-
erations on the nodes a and b between trees are
defined as: Insertion (? ? a), Deletion (a ? ?)
and Substitution (a ? b). Each edit operation has
289
an associated cost (denoted as ?(a ? b)). An
edit script on two trees is a sequence of edit op-
erations changing a tree to another. Consequently,
the cost of an edit script is the sum of the costs of
its edit operations. Based on the main definition
of this approach, TED is the cost of minimum cost
edit script between two trees (Zhang and Shasha,
1989).
In the classic TED, a cost value is assigned to
each operation initially, and the distance is com-
puted based on the initial cost values. Considering
that the distance can vary in different domains and
datasets, converging to an optimal set of values for
operations is almost empirically impossible. In
the following sections, we propose a method for
estimating the optimum set of values for opera-
tion costs in TED algorithm. Our method is built
on adapting the PSO optimization approach as a
search process to automate the procedure of cost
estimation.
3 Particle Swarm Optimization
PSO is a stochastic optimization technique which
was introduced recently based on the social be-
haviour of bird flocking and fish schooling (Eber-
hart et al, 2001). PSO is one of the population-
based search methods which takes advantage of
the concept of social sharing of information. In
this algorithm each particle can learn from the ex-
perience of other particles in the same population
(called swarm). In other words, each particle in
the iterative search process would adjust its fly-
ing velocity as well as position not only based on
its own acquaintance but also other particles? fly-
ing experience in the swarm. This algorithm has
found efficient in solving a number of engineering
problems. PSO is mainly built on the following
equations.
X
i
= X
i
+ V
i
(1)
V
i
= ?V
i
+ c
1
r
1
(X
bi
?X
i
)
+ c
2
r
2
(X
gi
?X
i
) (2)
To be concise, for each particle at each itera-
tion, the position X
i
(Equation 1) and velocity V
i
(Equation 2) is updated. X
bi
is the best position
of the particle during its past routes and X
gi
is
the best global position over all routes travelled
by the particles of the swarm. r
1
and r
2
are ran-
dom variables drawn from a uniform distribution
in the range [0,1], while c
1
and c
2
are two accel-
eration constants regulating the relative velocities
with respect to the best local and global positions.
The weight ? is used as a tradeoff between the
global and local best positions. It is usually se-
lected slightly less than 1 for better global explo-
ration (Melgani and Bazi, 2008). Position opti-
mally is computed based on the fitness function
defined in association with the related problem.
Both position and velocity are updated during the
iterations until convergence is reached or iterations
attain the maximum number defined by the user.
4 Automatic Cost Optimization for TED
In this section we proposed a system for estimat-
ing and optimizing the cost of each edit operation
for TED. As mentioned earlier, the aim of this sys-
tem is to find the optimal set of operation costs to:
1) improve the performance of TED in different
applications, and 2) provide some information on
how different operations in TED approach an ap-
plication or dataset. In order to obtain this, the
system is developed using an optimization frame-
work based on PSO.
4.1 PSO Setup
One of the most important steps in applying PSO
is to define a fitness function, which could lead
the swarm to the optimized particles based on the
application and data. The choice of this function
is very crucial since, based on this, PSO evalu-
ates the quality of each candidate particle for driv-
ing the solution space to optimization. Moreover,
this function should be, possibly, application and
data independent, as well as flexible enough to be
adapted to the TED based problems. With the in-
tention of accomplishing these goals, we define
two main fitness functions as follows:
1) Bhattacharyya Distance: This statistical
measure determines the similarity of two discrete
probability distributions (Bhattacharyya, 1943).
In classification, this method is used to mea-
sure the distance between two different classes.
Put it differently, maximizing the Bhattacharyya
distance would increase the separability of two
classes.
2) Accuracy: By maximizing the accuracy ob-
tained from 10 fold cross-validation on the devel-
opment set, as the fitness function, we estimate the
optimized cost of the edit operations.
290
4.2 Integrating TED with PSO
The procedure to estimate and optimize the cost
of edit operations in TED applying the PSO algo-
rithm, is as follows.
a) Initialization
1) Generate a random swarm of size n (cost of
edit operations).
2) For each position of the particle from the
swarm, obtain the fitness function value.
3) Set the best position of each particle with its
initial position (X
bi
).
b) Search
4) Detect the best global position (X
gi
) in the
swarm based on maximum value of the fit-
ness function over all explored routes.
5) Update the velocity of each particle (V
i
).
6) Update the position of each particle (X
i
).
7) For each candidate particle calculate the fit-
ness function.
8) Update the best position of each particle if
the current position has a larger value.
c) Convergence
9) Run till the maximum number of iteration
(in our case set to 10) is reached or start the
search process.
5 Experimental Design
Our experiments were conducted on the basis of
Recognizing Textual Entailment (RTE) datasets
1
.
Textual Entailment can be explained as an associ-
ation between a coherent text(T) and a language
expression, called hypothesis(H). The entailment
function for the pair T-H returns the true value
when the meaning of H can be inferred from the
meaning of T and false otherwise. In another
word, Textual Entailment can be defined as hu-
man reading comprehension task. One of the ap-
proaches to textual entailment problem is based on
the distance between T and H.
In this approach, the entailment score for a pair
is calculated on the minimal set of edit operations
that transform T into H. An entailment relation is
assigned to a T-H pair in the case that overall cost
of the transformations is below a certain thresh-
old. The threshold, which corresponds to tree edit
1
http://www.pascal-network.org/Challenges/RTE1-4
distace, is empirically estimated over the dataset.
This method was implemented by (Kouylekov and
Magnini, 2005), based on TED algorithm (Zhang
and Shasha, 1989). Each RTE dataset includes
its own development and test set, however, RTE-4
was released only as a test set and the data from
RTE-1 to RTE-3 were exploited as development
set for evaluating RTE-4 data.
In order to deal with TED approach to textual
entailment, we used EDITS
2
package (Edit Dis-
tance Textual Entailment Suite) (Magnini et al,
2009). In addition, We partially exploit JSwarm-
PSO
3
package with some adaptations as an im-
plementation of PSO algorithm. Each pair in the
datasets converted to two syntactic dependency
trees using Stanford statistical parser
4
, developed
in the Stanford university NLP group by (Klein
and Manning, 2003).
We conducted six different experiments in two
sets on each RTE dataset. The costs were esti-
mated on the training set, then we evaluate the es-
timated costs on the test set. In the first set of ex-
periments, we set a simple cost scheme based on
three operations. Implementing this cost scheme,
we expect to optimize the cost of each edit opera-
tion without considering that the operation costs
may vary based on different characteristics of a
node, such as size, location or content. The results
were obtained using: 1) The random cost assign-
ment, 2) Assigning the cost based on the exper-
tise knowledge and intuition (So called Intuitive),
and 3) Automatic estimated and optimized cost for
each operation. In the second case, we applied the
same cost values which was used in EDITS by its
developers (Magnini et al, 2009).
In the second set of experiments, we tried to
take advantage of an advanced cost scheme with
more fine-grained operations to assign a weight to
the edit operations based on the characteristics of
the nodes (Magnini et al, 2009). For example if a
node is in the list of stop-words, the deletion cost
should be different from the cost of deleting a con-
tent word. By this intuition, we tried to optimize 9
specialized costs for edit operations (A swarm of
size 9). At each experiment, both fitness functions
were applied and the best results were chosen for
presentation.
2
http://edits.fbk.eu/
3
http://jswarm-pso.sourceforge.net/
4
http://nlp.stanford.edu/software/lex-parser.shtml
291
Data set
Model RTE4 RTE3 RTE2 RTE1
Simple
Random 49.6 53.62 50.37 50.5
Intuitive 51.3 59.6 56.5 49.8
Optimized 56.5 61.62 58 58.12
Adv.
Random 53.60 52.0 54.62 53.5
Intuitive 57.6 59.37 57.75 55.5
Optimized 59.5 62.4 59.87 58.62
Baseline 57.19
RTE-4 Challenge 57.0
Table 1: Comparison of accuracy on all RTE
datasets based on optimized and unoptimized cost
schemes.
6 Results
Our results are summarized in Table 1. We show
the accuracy gained by a distance-based base-
line for textual entailment (Mehdad and Magnini,
2009) in compare with the results achieved by the
random, intuitive and optimized cost schemes us-
ing EDITS system. For the better comparison,
we also present the results of the EDITS system
(Cabrio et al, 2008) in RTE-4 challenge using
combination of different distances as features for
classification (Cabrio et al, 2008).
Table 1 shows that, in all datasets, accuracy im-
proved up to 9% by optimizing the cost of each
edit operation. Results prove that, the optimized
cost scheme enhances the quality of the system
performance even more than the cost scheme used
by the experts (Intuitive cost scheme). Further-
more, using the fine-grained and weighted cost
scheme for edit operations we could achieve the
highest results in accuracy. Moreover, by explor-
ing the estimated optimal cost of each operation,
we could find even some linguistics phenomena
which exists in the dataset. For instance, in most
of the cases, the cost of deletion was estimated
zero, which shows that deleting the words from
the text does not effect the distance in the entail-
ment pairs. In addition, the optimized model can
reflect more consistency and stability (from 58 to
62 in accuracy) than other models, while in unop-
timized models the result varies more, on different
datasets (from 50 in RTE-1 to 59 in RTE-3).
7 Conclusion
In this paper, we proposed a novel approach for es-
timating the cost of edit operations in TED. This
model has the advantage of being efficient and
more transparent than probabilistic approaches as
well as having less complexity. The easy imple-
mentation of this approach, besides its flexibility,
makes it suitable to be applied in real world appli-
cations. The experimental results on textual entail-
ment, as one of the challenging problems in NLP,
confirm our claim.
Acknowledgments
Besides my special thanks to F. Melgani, B.
Magnini and M. Kouylekov for their academic and
technical support, I acknowledge the reviewers for
their comments. The EDITS system has been sup-
ported by the EU-funded project QALL-ME (FP6
IST-033860).
References
M. Bernard, L. Boyer, A. Habrard, and M. Sebban.
2008. Learning probabilistic models of tree edit dis-
tance. Pattern Recogn., 41(8):2611?2629.
A. Bhattacharyya. 1943. On a measure of diver-
gence between two statistical populations defined by
probability distributions. Bull. Calcutta Math. Soc.,
35:99109.
E. Cabrio, M. Kouylekovand, and B. Magnini. 2008.
Combining specialized entailment engines for rte-4.
In Proceedings of TAC08, 4th PASCAL Challenges
Workshop on Recognising Textual Entailment.
R. C. Eberhart, Y. Shi, and J. Kennedy. 2001. Swarm
Intelligence. The Morgan Kaufmann Series in Arti-
ficial Intelligence.
D. Klein and C. D. Manning. 2003. Fast exact in-
ference with a factored model for natural language
parsing. In Advances in Neural Information Pro-
cessing Systems 15, Cambridge, MA. MIT Press.
M. Kouylekov and B. Magnini. 2005. Recognizing
textual entailment with tree edit distance algorithms.
In PASCAL Challenges on RTE, pages 17?20.
B. Magnini, M. Kouylekov, and E. Cabrio. 2009. Edits
- Edit Distance Textual Entailment Suite User Man-
ual. Available at http://edits.fbk.eu/.
Y. Mehdad and B. Magnini. 2009. A word overlap
baseline for the recognizing textual entailment task.
Available at http://edits.fbk.eu/.
F. Melgani and Y. Bazi. 2008. Classification of elec-
trocardiogram signals with support vector machines
and particle swarm optimization. IEEE Transac-
tions on Information Technology in Biomedicine,
12(5):667?677.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262.
292
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 36?43,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Optimizing Textual Entailment Recognition Using Particle Swarm
Optimization
Yashar Mehdad
University of Trento and FBK - Irst
Trento, Italy
mehdad@fbk.eu
Bernardo Magnini
FBK - Irst
Trento, Italy
magnini@fbk.eu
Abstract
This paper introduces a new method to im-
prove tree edit distance approach to tex-
tual entailment recognition, using particle
swarm optimization. Currently, one of the
main constraints of recognizing textual en-
tailment using tree edit distance is to tune
the cost of edit operations, which is a dif-
ficult and challenging task in dealing with
the entailment problem and datasets. We
tried to estimate the cost of edit operations
in tree edit distance algorithm automati-
cally, in order to improve the results for
textual entailment. Automatically estimat-
ing the optimal values of the cost opera-
tions over all RTE development datasets,
we proved a significant enhancement in
accuracy obtained on the test sets.
1 Introduction
One of the main aspects of natural languages is to
express the same meaning in many possible ways,
which directly increase the language variability
and emerges the complex structure in dealing with
human languages. Almost all computational lin-
guistics tasks such as Information Retrieval (IR),
Question Answering (QA), Information Extrac-
tion (IE), text summarization and Machine Trans-
lation (MT) have to cope with this notion. Textual
Entailment Recognition was proposed by (Dagan
and Glickman, 2004), as a generic task in order to
conquer the problem of lexical, syntactic and se-
mantic variabilities in languages.
Textual Entailment can be explained as an as-
sociation between a coherent text (T) and a lan-
guage expression, called hypothesis (H) such that
entailment function for the pair T-H returns the
true value when the meaning of H can be inferred
from the meaning of T and false, otherwise.
Amongst the approaches to the problem of tex-
tual entailment, some methods utilize the no-
tion of distance between the pair of T and H as
the main feature which separates the entailment
classes (positive and negative). One of the suc-
cessful algorithms implemented Tree Edit Dis-
tance (TED), based on the syntactic features that
are represented in the structured parse tree of each
string (Kouylekov and Magnini, 2005). In this
method the distance is computed as the cost of
the edit operations (insertion, deletion and substi-
tution) that transform the text T into the hypothesis
H. Each edit operation has an associated cost and
the entailment score is calculated such that the set
of operations would lead to the minimum cost.
Generally, the initial cost is assigned to each
edit operation empirically, or based on the ex-
pert knowledge and experience. These methods
emerge a critical problem when the domain, field
or application is new and the level of expertise and
empirical knowledge is very limited. In dealing
with textual entailment, (Kouylekov and Magnini,
2006) tried to experiment different cost values
based on various linguistics knowledge and prob-
abilistics estimations. For instance, they defined
the substitution cost as a function of similarity
between two nodes, or, for insertion cost, they
employed Inverse Document Frequency (IDF) of
the inserted node. However, the results could not
proven to be optimal.
Other approaches towards estimating the cost
of operations in TED tried to learn a generic or
discriminative probabilistic model (Bernard et al,
2008; Neuhaus and Bunke, 2004) from the data,
without concerning the optimal value of each op-
eration. One of the drawbacks of those approaches
is that the cost values of edit operations are hidden
behind the probabilistic model. Additionally, the
cost can not be weighted or varied according to
the tree context and node location (Bernard et al,
2008).
In order to overcome these drawbacks, we are
proposing a stochastic method based on Particle
36
Swarm Optimization (PSO), to estimate the cost
of each edit operation for textual entailment prob-
lem. Implementing PSO, we try to learn the op-
timal cost for each operation in order to improve
the prior textual entailment model. In this paper,
the goal is to automatically estimate the best possi-
ble operation costs on the development set. A fur-
ther advantage of such method, besides automatic
learning of the operation costs, is being able to in-
vestigate the cost values to better understand how
TED approaches the data in textual entailment.
The rest of the paper is organized as follows:
After describing the TED approach to textual en-
tailment in the next section, PSO optimization al-
gorithm and our method in applying it to the prob-
lem are explained in sections 4 and 5. Then we
present our experimental setup as well as the re-
sults, in detail. Finally, in the conclusion, the main
advantages of our approach are reviewed and fur-
ther developments are proposed accordingly.
2 Tree Edit Distance and Textual
Entailment
One of the approaches to textual entailment is
based on the Tree Edit Distance (TED) between
T and H. The tree edit distance measure is a simi-
larity metric for rooted ordered trees. This metric
was initiated by (Tai, 1979) as a generalization of
the string edit distance problem and was improved
by (Zhang and Shasha, 1989) and (Klein, 1998).
The distance is computed as the cost of editing
operations (i.e. insertion, deletion and substitu-
tion), which are required to transform the text T
into the hypothesis H, while each edit operation on
two text fragments A and B (denoted as A ? B)
has an associated cost (denoted as ? (A ? B)). In
textual entailment context, the edit operations are
defined in the following way based on the depen-
dency parse tree of T and H:
? Insertion (? ? A): insert a node A from
the dependency tree of H into the depen-
dency tree of T. When a node is inserted it
is attached to the dependency relation of the
source label.
? Deletion (A ? ?): delete a node A from
the dependency tree of T. When A is deleted
all its children are attached to the parent of
A. It is not required to explicitly delete the
children of A, as they are going to be either
deleted or substituted in a following step.
? Substitution (A ? B): change the label of
a node A in the source tree into a label of a
node B of the target tree. In the case of substi-
tution, the relation attached to the substituted
node is changed with the relation of the new
node.
According to (Zhang and Shasha, 1989), the min-
imum cost mappings of all the descendants of
each node has to be computed before the node
is encountered, so the least-cost mapping can be
selected right away. To accomplish this the al-
gorithm keeps track of the keyroots of the tree,
which are defined as a set that contains the root
of the tree plus all nodes which have a left sibling.
This problem can be easily solved using recursive
methods (Selkow, 1977), or as it was suggested in
(Zhang and Shasha, 1989) by dynamic program-
ming. (Zhang and Shasha, 1989) defined the rel-
evant subproblems of tree T as the prefixes of all
special subforests rooted in the keyroots. This ap-
proach computes the TED (?) by the following
equations:
?(F
T
, ?) =
?(F
T
? r
F
T
, ?) + ?(r
F
T
? ?) (1)
?(?, F
H
) =
?(?, F
H
? r
F
H
) + ?(? ? r
F
H
) (2)
?(F
T
, F
H
) =
min
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?(F
T
? r
F
T
, F
H
) + ?(r
F
T
? ?)
?(F
T
, F
H
? r
F
H
) + ?(? ? r
F
H
)
?(F
T
(r
F
T
), F
H
(r
F
H
))+
?(F
T
? T (r
F
T
), F
H
?H(r
F
H
))+
?(r
F
T
? r
F
H
)
(3)
where F
T
and F
H
are forests of T and H , while
r
F
T
and r
F
H
are the rightmost roots of the trees
in F
T
and F
H
respectively. ? is an empty forest.
Moreover, F
T
(r
F
T
) and F
H
(r
F
H
) are the forests
rooted in r
F
T
and r
F
H
respectively.
Estimating ? as the bottom line of the compu-
tation is directly related to the cost of each oper-
ation. Moreover, the cost of edit operations can
simply change the way that a tree is transformed
to another. As Figure 1
1
shows (Demaine et al,
2007), there could exist more than one edit script
for transforming each tree to another. Based on the
1
The example adapted from (Demaine et al, 2007)
37
Figure 1: Two possible edit scripts to transform one tree to another.
main definition of this approach, TED is the cost
of minimum cost edit script between two trees.
The entailment score for a pair is calculated on
the minimal set of edit operations that transform
the dependency parse tree of T into H. An entail-
ment relation is assigned to a T-H pair where the
overall cost of the transformations is below a cer-
tain threshold. The threshold, which corresponds
to tree edit distace, is empirically estimated over
the dataset. This method was implemented by
(Kouylekov and Magnini, 2005), based on the al-
gorithm by (Zhang and Shasha, 1989).
In this method, a cost value is assigned to each
operation initially, and the distance is computed
based on the initial cost values. Considering that
the distance can vary in different datasets, con-
verging to an optimal set of values for operations
is almost empirically impossible. In the follow-
ing sections, we propose a method for estimat-
ing the optimum set of values for operation costs
in TED algorithm dealing with textual entailment
problem. Our method is built on adapting PSO
optimization approach as a search process to auto-
mate the procedure of the cost estimation.
3 Particle Swarm Optimization
PSO is a stochastic optimization technique which
was introduced based on the social behaviour of
bird flocking and fish schooling (Eberhart et al,
2001). It is one of the population-based search
methods which takes advantage of the concept of
social sharing of information. The main struc-
ture of this algorithm is not very different from
other evolutionary techniques such as Genetic Al-
gorithms (GA); however, the easy implementation
and less complexity of PSO, as two main charac-
teristics, are good motivations to apply this opti-
mization approach in many areas.
In this algorithm each particle can learn from
the experience of other particles in the same pop-
ulation (called swarm). In other words, each parti-
cle in the iterative search process, would adjust its
flying velocity as well as position not only based
on its own acquaintance, but also other particles?
flying experience in the swarm. This algorithm has
found efficient in solving a number of engineering
problems. In the following, we briefly explain the
main concepts of PSO.
To be concise, for each particle at each itera-
tion, the position X
i
(Equation 4) and velocity V
i
(Equation 5) is updated. X
bi
is the best position
of the particle during its past routes and X
gi
is
the best global position over all routes travelled by
the particles of the swarm. r
1
and r
2
are random
variables drawn from a uniform distribution in the
range [0,1], while c
1
and c
2
are two acceleration
constants regulating the relative velocities with re-
spect to the best local and global positions. The
weight ? is used as a tradeoff between the global
and local best positions and its value is usually
selected slightly less than 1 for better global ex-
ploration (Melgani and Bazi, 2008). The optimal
position is computed based on the fitness func-
tion defined in association with the related prob-
lem. Both position and velocity are updated dur-
ing the iterations until convergence is reached or
iterations attain the maximum number defined by
the user. This search process returns the best fit-
ness function over the particles, which is defined
as the optimized solution.
X
i
= X
i
+ V
i
(4)
V
i
= ?V
i
+ c
1
r
1
(X
bi
?X
i
)
+ c
2
r
2
(X
gi
?X
i
) (5)
Algorithm 1 shows a simple pseudo code of
how this optimization algorithm works. In the rest
of the paper, we describe our method to integrate
this algorithm with TED.
38
Algorithm 1 PSO algorithm
for all particles do
Initialize particle
end for
while Convergence or maximum iteration
do
for all particles do
Calculate fitness function
if fitness function value > X
bi
then
X
bi
? fitness function value
end if
end for
choose the best particle amongst all in X
gi
for all particles do
calculate V
i
update X
i
end for
end while
return best particle
4 Automatic Cost Estimation
One of the challenges in applying TED for rec-
ognizing textual entailment is estimating the cost
of each edit operation which transforms the text T
into the hypothesis H in an entailment pair. Since
the cost of edit operations can directly affect the
distance, which is the main criteria to measure the
entailment, it is not trivial to estimate the cost of
each operation. Moreover, considering that imply-
ing different costs for edit operations can affect the
results in different data sets and approaches, it mo-
tivates the idea of optimizing the cost values.
4.1 PSO Setup
One of the most important steps in applying PSO
is to define a fitness function which could lead the
swarm to the optimized particles based on the ap-
plication and data. The choice of this function
is very crucial, since PSO evaluates the quality
of each candidate particle for driving the solution
space to optimization, on the basis of the fitness
function. Moreover, this function should possibly
improve the textual entailment recognition model.
In order to attain these goals, we tried to define
two main fitness functions as follows.
1. Bhattacharyya Distance: This measure was
proposed by (Bhattacharyya, 1943) as a sta-
tistical measure to determine the similarity
or distance between two discrete probabil-
ity distributions. In binary classification, this
method is widely used to measure the dis-
tance between two different classes. In the
studies by (Fukunaga, 1990), Bhattacharyya
distance was occluded to be one of the most
effective measure specifically for estimating
the separability of two classes. Figure 2
shows the intuition behind this measure.
Figure 2: Bhattacharyya distance between two
classes with similar variances.
Bhattacharyya distance is calculated based on
the covariance (?) and mean (?) of each dis-
tribution based on its simplest formulation in
Equation 6 (Reyes-Aldasoro and Bhalerao,
2006). Maximizing the distance between
the classes would result a better separability
which aims to a better classification results.
Furthermore, estimating the costs using this
function would indirectly improve the perfor-
mance specially in classification problems. It
could be stated that, maximizing the Bhat-
tacharyya distance would increase the separa-
bility of two entailment classes which result
in a better performance.
BD(c
1
, c
2
) =
1
4
ln{
1
4
(
?
2
c
1
?
2
c
2
+
?
2
c
2
?
2
c
1
+ 2)}
+
1
4
{
(?
c
1
? ?
c
2
)
2
?
2
c
1
+ ?
2
c
2
} (6)
2. Accuracy: Accuracy or any performance
measure obtained from a TED based system,
can define a good fitness function in optimiz-
ing the cost values. Since maximizing the
accuracy would directly increase the perfor-
mance of the system or enhance the model
to solve the problem, this measure is a pos-
sible choice to adapt in order to achieve our
aim. In this method, trying to maximize the
fitness function will compute the best model
based on the optimal cost values in the parti-
cle space of PSO algorithm.
In other words, by defining the accuracy ob-
tained from 10 fold cross-validation over the
39
development set, as the fitness function, we
could estimate the optimized cost of the edit
operations. Maximizing the accuracy gained
in this way, would lead to find the set of edit
operation costs which directly increases our
accuracy, and consequently guides us to the
main goal of optimization.
In the following section, the procedure of esti-
mating the optimal costs are described in detail.
4.2 Integrating TED with PSO for Textual
Entailment Problem
The procedure describing the proposed system to
optimize and estimate the cost of edit operations
in TED applying PSO algorithm is as follows.
a) Initialization
Step 1) Generate a random swarm of particles
(in a simple case each particle is de-
fined by the cost of three operations).
Step 2) For each position of the particle from
the swarm, obtain the fitness function
value (Bhattacharyya distance or accu-
racy) over the training data.
Step 3) Set the best position of each particle
with its initial position (X
bi
).
b) Search
Step 4) Detect the best global position (X
gi
)
in the swarm based on maximum value
of the fitness function over all explored
routes.
Step 5) Update the velocity of each particle
(V
i
).
Step 6) Update the position of each particle
(X
i
). In this step, by defining the
boundaries, we could stop the particle
to exit the allowed search space.
Step 7) For each candidate particle calculate
the fitness function (Bhattacharyya
distance or accuracy).
Step 8) Update the best position of each parti-
cle if the current position has a larger
value.
c) Convergence
Step 9) Run till the maximum number of iter-
ation (in our case set to 10) is reached
or start the search process.
d) Results
Step 10) Return the best fitness function value
and the best particle. In this step the
optimum costs are returned.
Following the steps above, in contrary to de-
termine the entailment relation applying tree edit
distance, the operation costs can be automatically
estimated and optimized. In this process, both fit-
ness functions could be easily compared and the
cost values leading to the better model would be
selected. In the following section, the experimen-
tal procedure for obtaining the optimal costs by
exploiting the PSO approach to TE is described.
5 Experimental Design
In our experiments we show an increase in the per-
formance of TED based approach to textual en-
tailment, by optimizing the cost of edit operations.
In the following subsections, the framework and
dataset of our experiments are elaborated.
5.1 Dataset Description
Our experiments were conducted on the basis
of the Recognizing Textual Entailment (RTE)
datasets
2
, which were developed under PASCAL
RTE challenge. Each RTE dataset includes its own
development and test set, however, RTE-4 was re-
leased only as a test set and the data from RTE-1
to RTE-3 were used as development set. More de-
tails about the RTE datasets are illustrated in Table
5.1.
Number of pairs
Development Test
Datasets YES NO YES NO
RTE-1 283 284 400 400
RTE-2 400 400 400 400
RTE-3 412 388 410 390
RTE-4 ? ? 500 500
Table 1: RTE-1 to RTE-4 datasets.
5.2 Experimental Framework
In our experiments, in order to deal with TED
approach to textual entailment, we used EDITS
3
package (Edit Distance Textual Entailment Suite)
2
http://www.pascal-network.org/Challenges/RTE1-4
3
The EDITS system has been supported by the EU-
funded project QALL-ME (FP6 IST-033860). Available at
http://edits.fbk.eu/
40
(Magnini et al, 2009). This system is an open
source software based on edit distance algorithms,
and computes the T-H distance as the cost of the
edit operations (i.e. insertion, deletion and substi-
tution) that are necessary to transform T into H.
By defining the edit distance algorithm and a cost
scheme (assigning a cost to the edit operations),
this package is able to learn a TED threshold, over
a set of string pairs, to decide if the entailment ex-
ists in a pair.
In addition, we partially exploit the JSwarm-
PSO
4
(Cingolani, 2005) package, with some adap-
tations, as an implementation of PSO algorithm.
Each pair in the datasets is converted to two syn-
tactic dependency parse trees using the Stanford
statistical parser
5
, developed in the Stanford uni-
versity NLP group by (Klein and Manning, 2003).
Figure 3: Five main steps of the experimental
framework.
In order to take advantage of PSO optimization
approach, we integrated EDITS and JSwarm-PSO
to provide a flexible framework for the experi-
ments (Figure 5.3). In this way, we applied the
defined fitness functions in the integrated system.
The Bhattacharyya distance between two classes
(YES and NO), in each experiment, could be com-
puted based on the TED score of each pair in the
dataset. Moreover, the accuracy, by default, is
computed by EDITS over the training set based
on 10-fold cross-validation.
5.3 Experimental Scheme
We conducted six different experiments in two sets
on each RTE dataset. The costs were estimated on
the training set and the results obtained based on
the estimated costs over the test set. In the first
4
http://jswarm-pso.sourceforge.net/
5
http://nlp.stanford.edu/software/lex-parser.shtml
set of experiments, we set a simple cost scheme
based on three operations. Implementing this cost
scheme, we expect to optimize the cost of each
edit operation without considering that the opera-
tion costs may vary based on different character-
istics of a node, such as size, location or content.
The results were obtained considering three dif-
ferent settings: 1) the random cost assignment; 2)
assigning the cost based on the human expertise
knowledge and intuition (called Intuitive), and 3)
automatic estimated and optimized cost for each
operation. In the second case, we used the same
scheme which was used in EDITS by its develop-
ers (Magnini et al, 2009).
In the second set of experiments, we tried to
compose an advanced cost scheme with more
fine-grained operations to assign a weight to the
edit operations based on the characteristics of the
nodes. For example if a node is in the list of stop-
words, the deletion cost is set to zero. Otherwise,
the cost of deletion would be equal to the number
of words in H multiplied by word?s length (num-
ber of characters). Similarly, the cost of inserting
a word w in H is set to 0 if w is a stop word,
and to the number of words in T multiplied by
words length otherwise. The cost of substituting
two words is the Levenshtein distance (i.e. the edit
distance calculated at the level of characters) be-
tween their lemmas, multiplied by the number of
words in T, plus number of words in H. By this in-
tuition, we tried to optimize nine specialized costs
for edit operations (i.e. each particle is defined by
9 parameters to be optimized). We conducted the
experiments using all three cases mentioned in the
simple cost scheme.
In each experiment, we applied both fitness
functions in the optimization; however, at the final
phase, the costs which led to the maximum results
were chosen as the estimated operation costs. In
order to save breath and time, we set the number
of iterations to 10, in addition, the weight ? was
set to 0.95 for better global exploration (Melgani
and Bazi, 2008).
6 Results
Our results are summarized in Table 2. We show
the accuracy gained by a distance-based (word-
overlap) baseline for textual entailment (Mehdad
and Magnini, 2009) to be compared with the re-
sults achieved by the random, intuitive and op-
timized cost schemes using EDITS system. For
41
Data set
Model RTE-4 RTE-3 RTE-2 RTE-1
Simple
Random 49.6 53.62 50.37 50.5
Intuitive 51.3 59.6 56.5 49.8
Optimized 56.5 61.62 58 58.12
Advanced
Random 53.60 52.0 54.62 53.5
Intuitive 57.6 59.37 57.75 55.5
Optimized 59.5 62.4 59.87 58.62
Baseline 55.2 60.9 54.8 51.4
RTE-4 Challenge 57.0
Table 2: Comparison of accuracy on all RTE datasets based on optimized and unoptimized cost schemes.
the better comparison, we also present the results
of the EDITS system in RTE-4 challenge using a
combination of different distances as features for
classification (Cabrio et al, 2008).
In the first experiment, we estimated the cost of
each operation using the simple cost scheme. Ta-
ble 2 shows that in all datasets, accuracy improved
up to 9% by optimizing the cost of each edit opera-
tion. Results prove that the optimized cost scheme
enhances the quality of the system performance,
even more than the cost scheme used by experts
(Intuitive cost scheme) (Magnini et al, 2009).
Furthermore, in the second set of experiments,
using the fine-grained and weighted cost scheme
for edit operations we could achieve the highest re-
sults in accuracy. The chart in Figure 4, illustares
that all optimized results outperform the word-
overlap baseline for textual entailment as well as
the accuracy obtained in RTE-4 challenge using
combination of different distances as features for
classification (Cabrio et al, 2008).
By exploring the estimated optimal cost of each
operation, another interesting point was discov-
ered. The estimated cost of deletion in the first
set of experiments was 0, which means that delet-
ing a node from the dependency tree of T does not
effect the quality of results. This proves that by
setting different cost schemes, we could explore
even some linguistics phenomena which exists in
the entailment dataset. Studying the dataset from
this point of view might be interesting to find some
hidden information which can not be explored eas-
ily.
In addition, the optimized model can reflect
more consistency and stability (from 58 to 62 in
accuracy) than other models, while in unoptimized
models the result varies more, on different datasets
(from 50 in RTE-1 to 59 in RTE-3). Moreover, we
believe that by changing some parameters such as
maximum number of iterations, or by defining a
better cost scheme, there could be still a room for
improvement.
Figure 4: Accuracy obtained by different experi-
mental setups.
7 Conclusion
In this paper, we proposed a novel approach for es-
timating the cost of edit operations for the tree edit
distance approach to textual entailment. With this
work we illustrated another step forward in im-
proving the foundation of working with distance-
based algorithms for textual entailment. The ex-
perimental results confirm our working hypothe-
sis that by improving the results in applying tree
edit distance for textual entailment, besides out-
performing the distance-based baseline for recog-
42
nizing textual entailment.
We believe that for further development, ex-
tending the cost scheme to find weighted and
specialized cost operations to deal with different
cases, can lead to more interesting results. Besides
that, exploring and studying the estimated cost of
operations, could be interesting from a linguistics
point of view.
Acknowledgments
Besides my special thanks to Farid Melgani
for his helpful ideas, I acknowledge Milen
Kouylekov for his academic and technical sup-
ports. This work has been partially sup-
ported by the three-year project LiveMemories
(http://www.livememories.org/), funded by the
Provincia Autonoma di Trento.
References
Marc Bernard, Laurent Boyer, Amaury Habrard, and
Marc Sebban. 2008. Learning probabilistic models
of tree edit distance. Pattern Recogn., 41(8):2611?
2629.
A. Bhattacharyya. 1943. On a measure of diver-
gence between two statistical populations defined by
probability distributions. Bull. Calcutta Math. Soc.,
35:99109.
Elena Cabrio, Milen Kouylekovand, and Bernardo
Magnini. 2008. Combining specialized entailment
engines for rte-4. In Proceedings of TAC08, 4th
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment.
Pablo Cingolani. 2005. Jswarm-pso: Particle swarm
optimization package. Available at http://jswarm-
pso.sourceforge.net/.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling of
language variability. In Proceedings of the PAS-
CAL Workshop of Learning Methods for Text Under-
standing and Mining.
E. Demaine, S. Mozes, B. Rossman, and O. Weimann.
2007. An optimal decomposition algorithm for tree
edit distance. In Proceedings of the 34th Inter-
national Colloquium on Automata, Languages and
Programming (ICALP), pages 146?157.
Russell C. Eberhart, Yuhui Shi, and James Kennedy.
2001. Swarm Intelligence. The Morgan Kaufmann
Series in Artificial Intelligence. Morgan Kaufmann.
Keinosuke Fukunaga. 1990. Introduction to statisti-
cal pattern recognition (2nd ed.). Academic Press
Professional, Inc., San Diego, CA, USA.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15, pages 3?10, Cambridge,
MA. MIT Press.
Philip N. Klein. 1998. Computing the edit-distance
between unrooted ordered trees. In ESA ?98: Pro-
ceedings of the 6th Annual European Symposium on
Algorithms, pages 91?102, London, UK. Springer-
Verlag.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In PASCAL Challenges on RTE, pages
17?20.
Milen Kouylekov and Bernardo Magnini. 2006. Tree
edit distance for recognizing textual entailment: Es-
timating the cost of insertion. In PASCAL RTE-2
Challenge.
Bernardo Magnini, Milen Kouylekov, and Elena
Cabrio. 2009. Edits - edit distance tex-
tual entailment suite user manual. Available at
http://edits.fbk.eu/.
Yashar Mehdad and Bernardo Magnini. 2009. A word
overlap baseline for the recognizing textual entail-
ment task. Available at http://edits.fbk.eu/.
Farid Melgani and Yakoub Bazi. 2008. Classi-
fication of electrocardiogram signals with support
vector machines and particle swarm optimization.
IEEE Transactions on Information Technology in
Biomedicine, 12(5):667?677.
Michel Neuhaus and Horst Bunke. 2004. A proba-
bilistic approach to learning costs for graph edit dis-
tance. In ICPR ?04, pages 389?393, Washington,
DC, USA. IEEE Computer Society.
C. C. Reyes-Aldasoro and A. Bhalerao. 2006. The
bhattacharyya space for feature selection and its ap-
plication to texture segmentation. Pattern Recogn.,
39(5):812?826.
Stanley M. Selkow. 1977. The tree-to-tree editing
problem. Inf. Process. Lett., 6(6):184?186.
Kuo-Chung Tai. 1979. The tree-to-tree correction
problem. J. ACM, 26(3):422?433.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262.
43
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Divide and Conquer:
Crowdsourcing the Creation of Cross-Lingual Textual Entailment Corpora
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Yashar Mehdad
FBK-irst and University of Trento
Trento, Italy
mehdad@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Abstract
We address the creation of cross-lingual tex-
tual entailment corpora by means of crowd-
sourcing. Our goal is to define a cheap and
replicable data collection methodology that
minimizes the manual work done by expert
annotators, without resorting to preprocess-
ing tools or already annotated monolingual
datasets. In line with recent works empha-
sizing the need of large-scale annotation ef-
forts for textual entailment, our work aims to:
i) tackle the scarcity of data available to train
and evaluate systems, and ii) promote the re-
course to crowdsourcing as an effective way
to reduce the costs of data collection without
sacrificing quality. We show that a complex
data creation task, for which even experts usu-
ally feature low agreement scores, can be ef-
fectively decomposed into simple subtasks as-
signed to non-expert annotators. The resulting
dataset, obtained from a pipeline of different
jobs routed to Amazon Mechanical Turk, con-
tains more than 1,600 aligned pairs for each
combination of texts-hypotheses in English,
Italian and German.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al, 2010; Mehdad
et al, 2011) as an extension of Textual Entailment
(Dagan and Glickman, 2004). The task consists of
deciding, given a text (T) and an hypothesis (H) in
different languages, if the meaning of H can be in-
ferred from the meaning of T. As in other NLP appli-
cations, both for monolingual and cross-lingual TE,
the availability of large quantities of annotated data
is an enabling factor for systems development and
evaluation. Until now, however, the scarcity of such
data on the one hand, and the costs of creating new
datasets of reasonable size on the other, have repre-
sented a bottleneck for a steady advancement of the
state of the art.
In the last few years, monolingual TE corpora for
English and other European languages have been
created and distributed in the framework of sev-
eral evaluation campaigns, including the RTE Chal-
lenge1, the Answer Validation Exercise at CLEF2,
and the Textual Entailment task at EVALITA3. De-
spite the differences in the design of the tasks, all
the released datasets were collected through simi-
lar procedures, always involving expensive manual
work done by expert annotators. Moreover, in the
data creation process, large amounts of hand-crafted
T-H pairs often have to be discarded in order to re-
tain only those featuring full agreement, in terms of
the assigned entailment judgements, among multiple
annotators. The amount of discarded pairs is usually
high, contributing to increase the costs of creating
textual entailment datasets4.
The issues related to the shortage of datasets and
the high costs for their creation are more evident
1http://www.nist.gov/tac/2011/RTE/
2http://nlp.uned.es/clef-qa/ave/
3http://www.evalita.it/2009/tasks/te
4For instance, in the first five RTE Challenges, the aver-
age effort needed to create 1,000 pairs featuring full agreement
among 3 annotators was around 2.5 person-months. Typically,
around 25% of the original pairs had to be discarded during the
process, due to low inter-annotator agreement (Bentivogli et al,
2009).
670
in the CLTE scenario, where: i) the only dataset
currently available is an English-Spanish corpus ob-
tained by translating the RTE-3 corpus (Negri and
Mehdad, 2010), and ii) the application of the stan-
dard methods adopted to build RTE pairs requires
proficiency in multiple languages, thus significantly
increasing the costs of the data creation process.
To address these issues, in this paper we devise
a cost-effective methodology to create cross-lingual
textual entailment corpora. In particular, we focus
on the following problems:
(1) Is it possible to collect T-H pairs minimizing
the intervention of expert annotators? To address
this question, we explore the feasibility of crowd-
sourcing the corpus creation process. As a contri-
bution beyond the few works on TE/CLTE data ac-
quisition, we define an effective methodology that:
i) does not involve experts in the most complex (and
costly) stages of the process, ii) does not require pre-
processing tools, and iii) does not rely on the avail-
ability of already annotated RTE corpora.
(2) How can we guarantee good quality of the col-
lected data at a low cost? We address the quality
control issue through the decomposition of a com-
plex task (i.e. creating and annotating entailment
pairs) into smaller sub-tasks. Complex tasks are usu-
ally hard to explain in a simple way understandable
to non-experts, difficult to accomplish, and not suit-
able for the application of the quality-check mecha-
nisms provided by current crowdsourcing services.
Our ?divide and conquer? solution represents the
first attempt to address a complex task involving
content generation and labelling through the defini-
tion of a cheap and reliable pipeline of simple tasks
which are easy to define, accomplish, and control.
(3) Can we adapt such methodology to collect
cross-lingual T-H pairs? We tackle this question
by separating the problem of creating and annotating
TE pairs from the issues related to the multilingual
dimension. Our solution builds on the assumption
that entailment annotations can be projected across
aligned T-H pairs in different languages. In this
case, a complex multilingual task is reduced to a se-
quence of simpler subtasks where the most difficult
one, the generation of entailment pairs, is entirely
monolingual. Besides ensuring cost-effectiveness,
our solution allows us to overcome the problem of
finding workers that are proficient in multiple lan-
guages. Moreover, since the core monolingual tasks
of the process are carried out by manipulating En-
glish texts, we are able to address the very large
community of English speaking workers, with a
considerable reduction of costs and execution time.
Finally, as a by-product of our method, the acquired
pairs are fully aligned for all language combinations,
thus enabling meaningful comparisons between sce-
narios of different complexity (monolingual TE, and
CLTE between close or distant languages).
We believe that, in the same spirit of recent works
promoting large-scale annotation efforts around en-
tailment corpora (Sammons et al, 2010; Bentivogli
et al, 2010), the proposed approach and the resulting
dataset5 will contribute to meeting the strong need
for resources to develop and evaluate novel solutions
for textual entailment.
2 Related Works
Crowdsourcing services, such as Amazon Mechan-
ical Turk6 (MTurk) and CrowdFlower7, have been
recently used with success for a variety of NLP ap-
plications (Callison-Burch and Dredze, 2010). The
idea is that the acquisition and annotation of large
amounts of data needed to train and evaluate NLP
tools can be carried out in a cost-effective manner
by defining simple Human Intelligence Tasks (HITs)
routed to a crowd of non-expert workers (aka ?Turk-
ers?) hired through on-line marketplaces.
As regards textual entailment, the first work ex-
ploring the use of crowdsourcing services for data
annotation is described in (Snow et al, 2008), which
shows high agreement between non-expert annota-
tions of the RTE-1 dataset and existing gold standard
labels assigned by expert labellers.
Focusing on the actual generation of monolingual
entailment pairs, (Wang and Callison-Burch, 2010)
experiments the use of MTurk to collect facts and
counter facts related to texts extracted from an ex-
isting RTE corpus annotated with named entities.
Taking a step beyond the task of annotating exist-
5The CLTE corpora described in this paper will be made
freely available for research purposes through the website of
the funding EU Project CoSyne (http://www.cosyne.eu/).
6https://www.mturk.com/
7Although MTurk is directly accessible only to US citizens,
the CrowdFlower service (http://crowdflower.com/) provides an
interface to MTurk for non-US citizens.
671
ing datasets, and showing the feasibility of involving
non-experts also in the generation of TE pairs, this
approach is more relevant to our objectives. How-
ever, at least two major differences with our work
have to be remarked. First, they still use avail-
able RTE data to obtain a monolingual TE corpus,
whereas we pursue the more ambitious goal of gen-
erating from scratch aligned CLTE corpora for dif-
ferent language combinations. To this aim, we do
not resort to already annotated data, nor language-
specific preprocessing tools. Second, their approach
involves qualitative analysis of the collected data
only a posteriori, after manual removal of invalid
and trivial generated hypotheses. In contrast, our
approach integrates quality control mechanisms at
all stages of the data collection/annotation process,
thus minimizing the recourse to experts to check the
quality of the collected material.
Related research in the CLTE direction is re-
ported in (Negri and Mehdad, 2010), which de-
scribes the creation of an English-Spanish corpus
obtained from the RTE-3 dataset by translating the
English hypotheses into Spanish. Translations have
been crowdsourced adopting a methodology based
on translation-validation cycles, defined as separate
HITs. Although simplifying the CLTE corpus cre-
ation problem, which is recast as the task of translat-
ing already available annotated data, this solution is
relevant to our work for the idea of combining gold
standard units and ?validation HITS? as a way to
control the quality of the collected data at runtime.
3 Quality Control of Crowdsourced Data
The design of data acquisition HITs has to take into
account several factors, each having a considerable
impact on the difficulty of instructing the workers,
the quality and quantity of the collected data, the
time and overall costs of the acquisition. A major
distinction has to be made between jobs requiring
data annotation, and those involving content gener-
ation. In the former case, Turkers are presented with
the task of labelling input data referring to a fixed
set of possible values (e.g. making a choice between
multiple alternatives, assigning numerical scores to
rank the given data). In the latter case, Turkers are
faced with creative tasks consisting in the production
of textual material (e.g. writing a correct translation,
or a summary of a given text).
The ease of controlling the quality of the acquired
data depends on the nature of the job. For annotation
jobs, quality control mechanisms can be easily set up
by calculating Turkers? agreement, by applying vot-
ing schemes, or by adding hidden gold units to the
data to be annotated8. In contrast, the quality of the
results of content generation jobs is harder to assess,
due to the fact that multiple valid results are accept-
able (e.g. the same content can be expressed, trans-
lated, or summarized in different ways). In such sit-
uations the standard quality control mechanisms are
not directly applicable, and the detection of errors
requires either costly manual verification at the end
of the acquisition process, or more complex and cre-
ative solutions integrating HITs for quality check.
Most of the approaches to content generation pro-
posed so far rely on post hoc verification to fil-
ter out undesired low-quality data (Mrozinski et al,
2008; Mihalcea and Strapparava, 2009; Wang and
Callison-Burch, 2010). The few solutions integrat-
ing validation HITs address the translation of sin-
gle sentences, a task that is substantially different
from ours (Negri and Mehdad, 2010; Bloodgood and
Callison-Burch, 2010). Compared to sentence trans-
lation, the task of creating CLTE pairs is both harder
to explain without recurring to notions that are dif-
ficult to understand to non-experts (e.g. ?seman-
tic equivalence?, ?unidirectional entailment?), and
harder to execute without mastering these notions.
To tackle these issues the ?divide and conquer? ap-
proach described in the next section consists in the
decomposition of a difficult content generation job
into easier subtasks that are: i) self-contained and
easy to explain, ii) easy to execute without any NLP
expertise, and iii) suitable for the integration of a va-
riety of runtime control mechanisms (regional qual-
ifications, gold units, ?validation HITs?) able to en-
sure a good quality of the collected material.
8Both MTurk and CrowdFlower provide means to check
workers? reliability, and weed out untrusted ones without money
waste. These include different types of qualification mecha-
nisms, the possibility of giving work only to known trusted
Turkers (only with MTurk), and the possibility of adding hid-
den gold standard units in the data to be annotated (offered as a
built-in mechanism only by CrowdFlower).
672
4 CLTE Corpus Creation Methodology
Our approach builds on a pipeline of HITs routed to
MTurk?s workforce through the CrowdFlower inter-
face. The objective is to collect aligned T-H pairs
for different language combinations, reproducing an
RTE-like annotation style. However, our annotation
is not limited to the standard RTE framework, where
only unidirectional entailment from T to H is con-
sidered. As a useful extension, we annotate any pos-
sible entailment relation between the two text frag-
ments, including: i) bidirectional entailment (i.e.
semantic equivalence between T and H), ii) unidi-
rectional entailment from T to H, and iii) unidirec-
tional entailment from H to T. The resulting pairs
can be easily used to generate not only standard RTE
datasets9, but also general-purpose collections fea-
turing multi-directional entailment relations.
4.1 Data Acquisition and Annotation
We collect large amounts of CLTE pairs carrying out
the most difficult part of the process (the creation of
entailment-annotated pairs) at a monolingual level.
Starting from a set of parallel sentences in n lan-
guages, (e.g. L1, L2, L3), n entailment corpora are
created: one monolingual (L1/L1), and n-1 cross-
lingual (L1/L2, and L1/L3).
The monolingual corpus is obtained by modify-
ing the sentences only in one language (L1). Orig-
inal and modified sentences are then paired and an-
notated to form an entailment dataset for L1. The
CLTE corpora are obtained by combining the mod-
ified sentences in L1 with the original sentences in
L2 and L3, and projecting to the multilingual pairs
the annotations assigned to the monolingual pairs.
In principle, only two stages of the process re-
quire crowdsourcing multilingual tasks, but do not
concern entailment annotations. The first one, at the
beginning of the process, aims to obtain a set of par-
allel sentences to start with, and can be done in dif-
ferent ways (e.g. crowdsourcing the translation of
a set of sentences). The second one, at the end of
the process, consists of translating the modified L1
sentences into other languages (e.g. L2) in order to
extend the corpus to cover new language combina-
9With the positive examples drawn from bidirectional and
unidirectional entailments from T to H, and the negative ones
drawn from unidirectional entailments from H to T.
tions (e.g. L2/L2, L2/L3).
The execution of the two ?multilingual? stages is
not strictly necessary but depends on: i) the avail-
ability of parallel sentences to start the process, and
ii) the actual objectives in terms of language combi-
nations to be covered10.
As regards the first stage, in this work we started
from a set of 467 English/Italian/German aligned
sentences extracted from parallel documents down-
loaded from the Cafebabel European Magazine11.
Concerning the second multilingual stage, we per-
formed only one round of translations from En-
glish to Italian to extend the 3 combinations ob-
tained without translations (ENG/ENG, ENG/ITA,
and ENG/GER) with the new language combina-
tions ITA/ITA, ITA/ENG, and ITA/GER.
STEP1:	 ?Sentence	 ?modifica?on	 ?(monolingual)	 ?
STEP3:	 ?Transla?on	 ?(mul?lingual)	 ?
GER	 ? ENG	 ?
ENG1	 ?
ITA	 ?
ITA1	 ? ITA	 ?ENG	 ? ENG1	 ?
STEP2:	 ?TE	 ?annota?on	 ?(monolingual)	 ?
Monolingual	 ?TE	 ?corpus	 ?
Cross-??lingual	 ?TE	 ?corpus	 ?
ENG1	 ?GER	 ?
ENG1	 ?ITA	 ?
TE	 ?annota?ns	 ?projec?n	 ?	 ?	 ?
ITA1	 ? GER	 ?
ITA1	 ? ENG	 ?
Figure 1: Corpus creation process.
The main steps of our corpus creation process,
depicted in Figure 1, can be summarized as follows:
Step1: Sentence modification. The original
English sentences (ENG) are modified through
(monolingual) generation HITs asking Turkers to:
i) preserve the meaning of the original sentences
using different surface forms, or ii) slightly change
their meaning by adding or removing content. Our
assumption, in line with (Bos et al, 2009), is that
10Starting from parallel sentences in n languages, the n cor-
pora obtained without recurring to translations can be aug-
mented, by means of translation HITs, to create the full set of
language combinations. Each round of translation adds 1 mono-
lingual corpus, and n-1 CLTE corpora.
11http://www.cafebabel.com/
673
another way to think about entailment is to consider
whether one text T1 adds new information to the
content of another text T: if so, then T is entailed by
T1.
The result of this phase is a set of texts (ENG1)
that can be of three types:
1. Paraphrases of the original ENG texts, that will
be used to create bidirectional entailment pairs
(ENG?ENG1);
2. More specific sentences (the outcome of
content addition operations), used to create
ENG?ENG1 unidirectional entailment pairs;
3. More general sentences (the outcome of
content removal operations), used to create
ENG?ENG1 unidirectional entailment pairs.
Step2: TE Annotation. Entailment pairs com-
posed of the original sentences (ENG) and the modi-
fied ones (ENG1) are used as input of (monolingual)
annotation HITs asking Turkers to decide which of
the two texts contains more information. As a re-
sult, each ENG/ENG1 pair is annotated as an ex-
ample of uni-/bidirectional entailment, and stored in
the monolingual English corpus. Since the original
ENG texts are aligned with the ITA and GER texts,
the entailment annotations of ENG/ENG1 pairs can
be projected to the other language pairs and the
ITA/ENG1 and GER/ENG1 pairs are stored in the
CLTE corpus. The possibility of projecting TE an-
notations is based on the assumption that the seman-
tic information is mostly preserved during the trans-
lation process. This particularly holds at the deno-
tative level (i.e. regarding the truth values of the
sentence) which is crucial to semantic inference. At
other levels (e.g. lexical) there might be slight se-
mantic variations which, however, are very unlikely
to play a crucial role in determining entailment rela-
tions.
Step3: Translation. The modified sentences
(ENG1) are translated into Italian (ITA1) through
(multilingual) generation HITs reproducing the ap-
proach described in (Negri and Mehdad, 2010). As
a result, three new datasets are produced by au-
tomatically projecting annotations: the monolin-
gual ITA/ITA1, and the cross-lingual ENG/ITA1 and
GER/ITA1.
Since the solution adopted for sentence transla-
tion does not present novelty factors, the remainder
of this paper will omit further details on it. Instead,
the following sections will focus on the more chal-
lenging tasks of sentence modification and TE anno-
tation.
4.2 Crowdsourcing Sentence Modification and
TE Annotation
Sentence modification and TE annotation have been
decomposed into a pipeline of simpler monolingual
English sub-tasks. Such pipeline, depicted in Figure
2, involves several types of generation/annotation
HITs designed to be easily understandable to non-
experts. Each HIT consists of: i) a set of instruc-
tions for a specific task (e.g. paraphrasing a text),
ii) the data to be manipulated (e.g. an English sen-
tence), and iii) a test to check workers? reliability.
To cope with the quality control issues discussed in
Section 3, such tests are realized using gold stan-
dard units, either hidden in the data to be annotated
(annotation HITs) or defined as test questions that
workers must correctly answer (generation HITs).
Moreover, regional qualifications are applied to all
HITs. As a further quality check, all the annotation
HITs consider Turkers? agreement as a way to filter
out low quality results (only annotations featuring
agreement among 4 out of 5 workers are retained).
The six HITs defined for each subtask can be de-
scribed as follows:
1. Paraphrase (generation). Modify an En-
glish text (ENG), in order to produce a semantically
equivalent variant (ENG1). As a reliability test, be-
fore creating the paraphrase workers are asked to
judge if two English sentences contain the same in-
formation.
2. Grammaticality (annotation). Decide if an
English sentence is grammatically correct. This val-
idation HIT represents a quality check of the out-
put of each generation task (i.e. paraphrasing, and
add/remove information HITs).
3. Bidirectional Entailment (annotation). De-
cide whether two English sentences, the original
ENG and the modified ENG1, contain the same in-
formation (i.e. are semantically equivalent).
4a. Add Information (generation). Modify an
English text to create a more specific one by adding
content. As a reliability test, before generating the
674
Figure 2: Sentence modification and TE annotation pipeline.
new sentence workers are asked to judge which of
two given English sentences is more detailed.
4b. Remove Information (generation). Mod-
ify an English text to create a more general one by
removing part of its content. As a reliability test, be-
fore generating the new sentence workers are asked
to judge which of two given English sentences is less
detailed.
5. Unidirectional Entailment (annotation). De-
cide which of two English sentences (the original
ENG, and a modified ENG1) provides more infor-
mation.
These HITs are combined in an iterative pro-
cess that alternates text generation, grammaticality
check, and entailment annotation steps. As a result,
for each original ENG text we obtain multiple ENG1
variants of the three types (paraphrases, more gen-
eral texts, and more specific texts) and, in turn, a set
of annotated monolingual (ENG/ENG1) TE pairs.
As described in Section 4.1, the resulting mono-
lingual English TE corpus (ENG/ENG1) is used to
create the following mono/cross-lingual TE corpora:
? ITA/ENG1, and GER/ENG1 (by projecting TE
annotations)
? ITA/ITA1, GER/ITA1, and ENG/ITA1 (by
translating the ENG1 texts into Italian, and pro-
jecting TE annotations)
5 The Resulting CLTE Corpora
This section provides a quantitative and qualita-
tive analysis of the results of our corpus creation
methodology, focusing on the collected ENG-ENG1
monolingual dataset. It has to be remarked that, as
an effect of the adopted methodology, all the obser-
vations and the conclusions drawn hold for the col-
lected CLTE corpora as well.
5.1 Quantitative Analysis
Table 1 provides some details about each step of the
pipeline shown in Figure 2. For each HIT the table
presents: i) the number of items (sentences, or pairs
of sentences) given in input, ii) the number of items
(sentences or annotations) produced as output, iii)
the number of items discarded when the agreement
threshold was not reached, iv) the number of entail-
ment pairs added to the corpus, v) the time (days and
hours) required by the MTurk workforce to complete
the job, and vi) the cost of the job.
In HIT-1 (Paraphrase) 1,414 paraphrases were
collected asking three different meaning-preserving
modifications of each of the 467 original sen-
tences12. From a practical point of view, such redun-
dancy aims to ensure a sufficient number of gram-
matically correct and semantically equivalent mod-
ified sentences. From a theoretical point of view,
12Often, crowdsourced jobs return a number of output items
that is slightly larger than required, due to the labour distribution
mechanism internal to MTurk.
675
HIT # Input items # Output items # Discarded items # Pairs to corpus MTurk time Cost ($)
1. Paraphrase 467 1,414 5d+10.5h 45.48
2. Grammaticality 1,414 1,326 88 (6.22%) 1d+15h 56.88
3. Bidirectional Ent. 1,326 1,213 113 (8.52%) 301 3d+2h 53.47
(yes=1,205 no=8)
4a. Add Info 452 916 3d 37.02
4b. Remove Info 452 923 2d+22h 29.73
2. Grammaticality 1,839 1,749 90 (4.89%) 2d+5h 64.37
3. Bidirectional Ent. 1,749 1,438 311 (17.78%) 148 3d+20.5h 70.52
(yes=148 no=1,290)
5. Unidirectional Ent. 1,298 1,171 127 (9.78%) 1,171 8.5h 78.24
(491 + 680)
TOTAL 721 1,620 22d+11h 435.71
Table 1: The monolingual dataset creation pipeline.
collecting many variants of a small pool of origi-
nal sentences aims to create pairs featuring different
entailment relations with similar superficial forms.
This, in principle, should allow to obtain a dataset
which requires TE systems to focus more on deeper
semantic phenomena than on the surface realization
of the pairs.
The collected paraphrases were sent as input to
HIT-2 (Grammaticality). After this validation HIT,
the number of acceptable paraphrases was reduced
to 1,326 (with 88 discarded sentences, correspond-
ing to 6.22% of the total).
The retained paraphrases were paired with their
corresponding original sentences, and sent to HIT-3
(Bidirectional Entailment) to be judged for semantic
equivalence. The pairs marked as bidirectional en-
tailments (1,205) were divided in three groups: 25%
of the pairs (301) were directly stored in the final
corpus, while the ENG1 paraphrases of the remain-
ing 75% (904) were equally distributed to the next
modification steps.
In both HIT-4a (Add Information) and HIT-4b
(Remove information) two new modified sentences
were asked for each of the 452 paraphrases received
as input. The sentences collected in these generation
tasks were respectively 916 and 923.
The new modified sentences were sent back to
HIT-2 (Grammaticality) and HIT-3 (Bidirectional
Entailment). As a result 1,438 new pairs were cre-
ated; out of these, 148 resulted to be bidirectional
entailments and were stored in the corpus.
Finally, the 1,298 entailment pairs judged as non-
bidirectional in the two previously completed HIT-
3 (8+1,290) were given as input to HIT-5 (Unidi-
rectional Entailment). The pairs which passed the
agreement threshold were classified according to the
judgement received, and stored in the corpus as uni-
directional entailment pairs.
The analysis of Table 1 allows to formulate
some considerations. First, the percentage of dis-
carded items confirms the effectiveness of decom-
posing complex generation tasks into simpler sub-
tasks that integrate validation HITs and quality
checks based on non-experts? agreement. In fact, on
average, around 9.5% of the generated items were
discarded without experts? intervention13. Second,
the amount of discarded items gives evidence about
the relative difficulty of each HIT. As expected,
we observe lower rejection rates, corresponding to
higher inter-annotator agreement, for grammatical-
ity HITs (5.55% on average) than for more complex
entailment-related tasks (12.02% on average).
Looking at costs and execution time, it is hard
to draw definite conclusions due to several factors
that influence the progress of the crowdsourced jobs
(e.g. the fluctuations of Turkers? performances, the
time of the day at which jobs are posted, the dif-
ficulty to set the optimal cost for a given HIT14).
On the one hand, as expected, the more creative
?Add Info? task proved to be more demanding than
the ?Remove Info?: even though it was paid more,
13Moreover, it is worthwhile noticing that around 20% of the
collected items were automatically rejected (and not paid) due
to failures on the gold standard controls created both for gener-
ation and annotation tasks.
14The payment for each HIT was set on the basis of a pre-
vious feasibility study aimed at determining the best trade-off
between cost and execution time. However, replicating our ap-
proach would not necessarily result in the same costs.
676
it still took little more time to be completed. On
the other hand, although the ?Unidirectional Entail-
ment? task was expected to be more difficult and
thus rewarded more than the ?Bidirectional Entail-
ment? one, in the end it took notably less time to
be completed. Nevertheless, the overall figures (435
USD, and about 22.5 days of MTurk work to com-
plete the process)15 clearly demonstrate the effec-
tiveness of the approach. Even considering the time
needed for an expert to manage the pipeline (i.e. one
week to prepare gold units, and to handle the I/O of
each HIT), these figures show that our methodology
provides a cheaper and faster way to collect entail-
ment data in comparison with the RTE average costs
reported in Section 1.
As regards the amount of data collected, the re-
sulting corpus contains 1,620 pairs with the fol-
lowing distribution of entailment relations: i) 449
bidirectional entailments, ii) 491 ENG?ENG1 uni-
directional entailments, and iii) 680 ENG?ENG1
unidirectional entailments.
It must be noted that our methodology does not
lead to the creation of pairs where some information
is provided in one text and not in the other, and vice-
versa, as Example 1 shows:
Example 1.
ENG: New theories were emerging in the field of psychology.
ENG1: New theories were rising, which announced a kind of
veiled racism.
These negative examples in both directions repre-
sent a natural extension of the dataset, relevant also
for specific application-oriented scenarios, and their
creation will be addressed in future work.
Besides the achievement of our primary objec-
tives, the adopted approach led to some interesting
by-products. First, the generated corpora are per-
fectly suitable to produce entailment datasets simi-
lar to those used in the traditional RTE evaluation
framework. In particular, considering any possible
entailment relation between two text fragments, our
annotation subsumes the one proposed in RTE cam-
paigns. This allows for the cost-effective genera-
tion of RTE-like annotations from the acquired cor-
15Although by projecting annotations the ENG1/ITA and
ENG1/GER CLTE corpora came for free, the ITA1/ITA,
ITA1/ENG, and ITA1/GER combinations created by crowd-
sourcing translations added 45 USD and approximately 5 days
to these figures.
pora by combining ENG?ENG1 and ENG?ENG1
pairs to form 940 positive examples (449+491),
keeping the 680 ENG?ENG1 as negative exam-
ples. Moreover, by swapping ENG and ENG1 in the
unidirectional entailment pairs, 491 additional nega-
tive examples and 680 positive examples can be eas-
ily obtained.
Finally, the output of HITs 1-2-3 in Table 1 rep-
resents per se a valuable collection of 1,205 para-
phrases. This suggests the great potential of crowd-
sourcing for paraphrase acquisition.
5.2 Qualitative Analysis
Through manual verification of more than 50% of
the corpus (900 pairs), a total number of 53 pairs
(5.9%) were found incorrect. The different errors
were classified as follows:
Type 1: Sentence modification errors. Generation
HITs are a minor source of errors, being responsible
for 10 problematic pairs. These errors are either in-
troduced by generating a false statement (Example
2), or by forming a not fully understandable, awk-
ward, or non-natural sentence (Example 3).
Example 2.
ENG: Kosovo was the subject of major riots in 1989.
ENG1: The Russian city of Kosovo was the subject of ...
Example 3.
ENG: Balat is the Kurdish-Armenian district of Instanbul.
ENG1: Balat is a place, which is the Kurdish-Armenian ...
Type 2: TE annotation errors. The notion of con-
taining more/less information, used in the ?Unidi-
rectional Entailment? HIT, can mostly be applied
straightforwardly to the entailment definition. How-
ever, the concept of ?more/less detailed?, which gen-
erally works for factual statements, in some cases is
not applicable. In fact, the MTurk workers have reg-
ularly interpreted the instructions about the amount
of information as concerning the quantity of con-
cepts contained in a sentence. This is not always cor-
responding to the actual entailment relation between
the sentences. As a consequence, 43 pairs featur-
ing wrong entailment annotations were encountered.
These errors can be classified as follows:
a) 13 pairs, where the added/removed information
changes the meaning of the sentence. In these cases,
the modified sentence was judged more/less specific
677
than the original one, leading to unidirectional en-
tailment annotation. On the contrary, in terms of
the standard entailment definition, the correct anno-
tation is ?no entailment? (as in Example 4, which
was annotated as ENG?ENG1):
Example 4.
ENG: If you decide to live in Bulgaria, you have to like
difficulties because they are not difficulties, they are challenges.
ENG1: You have to like difficulties as they are not difficulties,
they are challenges.
b) 10 pairs where the incorrect annotation is due to
a coreference problem, as in:
Example 5.
ENG: John Smith is the new CEO of the company.
ENG1: He is the new CEO of the company.
These pairs were labelled as unidirectional entail-
ments (in the example above ENG?ENG1), under
the assumption that a proper name is more specific
and informative than a pronoun. However, adher-
ing to the TE definition, co-referring expressions are
equivalent, and their realization does not play any
role in the entailment decision. This implies that the
correct entailment annotation is ?bidirectional?.
c) 9 pairs where the sentences are semantically
equivalent, but contain a piece of information which
is explicit in one sentence, and implicit in the other.
In these cases, Turkers judged the sentence contain-
ing the explicit mention as more specific, and thus
the pair was annotated as unidirectional entailment.
Example 6.
ENG: I hear the click of the trigger and the burst of bullets
reach me immediately.
ENG1: I hear the trigger and the burst of bullets reach me
instantly.
In Example 6, the expression ?the trigger? in ENG1
implicitly means ?the click of the trigger?, mak-
ing the two sentences equivalent, and the entailment
bidirectional (instead of ENG?ENG1).
d) 7 pairs where the information removed from or
added to the sentence is not relevant to the entail-
ment relation. In these cases, the modified sen-
tence was judged less/more specific than the origi-
nal one (and thus considered as unidirectional entail-
ment), even though the correct judgement is ?bidi-
rectional?, as in:
Example 7.
ENG: At the same time, AKP is struggling with its approach to
the EU.
ENG1: AKP is struggling with its approach to the European
Union.
e) 4 pairs where the added/removed information
concerns universally quantified general statements,
about which the interpretation of ?more/less spe-
cific? given by Turkers resulted in the wrong anno-
tation.
Example 8.
ENG: I think the success of multicultural couples depends on
the size of the cultural gap between the two partners
ENG1: I believe the success of the couples depends on the size
of the cultural gap between the 2 partners.
In Example 8, the additional information (?mul-
ticultural?) restricts the set to which it refers
(?couples?) making ENG entailed by ENG1, and
not vice versa as resulted from Turkers? annotation.
In light of this analysis, we conclude that the sen-
tence modification methodology proved to be suc-
cessful, as the low number of Type 1 errors shows.
Considering that the most expensive phase in the
creation of a TE dataset is the generation of the
pairs, this is a significant achievement. Differently,
the entailment assessment phase appears to be more
problematic, accounting for the majority of errors.
As shown by Type 2 errors, this is due to a par-
tial misalignment between the instructions given in
our HITs, and the formal definition of textual en-
tailment. For this reason, further experimentation
will explore different ways to instruct workers (e.g.
asking to consider proper names and pronouns as
equivalent) in order to reduce the amount of errors
produced. As a final remark, considering that in the
creation of a TE dataset the manual check of the an-
notated pairs represents a minor cost, even the in-
volvement of experts to filter out wrong annotations
would not decrease the cost-effectiveness of the pro-
posed methodology.
6 Conclusions
There is an increasing need of annotated data to
develop new solutions to the Textual Entailment
problem, explore new entailment-related tasks, and
set up experimental frameworks targeting real-world
applications. Following the recent trends promot-
ing annotation efforts that go beyond the estab-
lished RTE Challenge framework (unidirectional en-
tailment between monolingual T-H pairs), in this
678
paper we addressed the multilingual dimension of
the problem. Our primary goal was the creation of
large-scale collections of entailment pairs for differ-
ent language combinations. Besides that, we consid-
ered cost effectiveness and replicability as additional
requirements. To achieve our objectives, we devel-
oped a ?divide and conquer? methodology based on
crowdsourcing. Our approach presents several key
innovations with respect to the related works on TE
data acquisition. These include the decomposition
of a complex content generation task in a pipeline
of simpler subtasks accessible to a large crowd of
non-experts, and the integration of quality control
mechanisms at each stage of the process. The result
of our work is the first large-scale dataset contain-
ing both monolingual and cross-lingual corpora for
several combinations of texts-hypotheses in English,
Italian, and German. Among the advantages of our
method it is worth mentioning: i) the full alignment
between the created corpora, ii) the possibility to
easily extend the dataset to new languages, and iii)
the feasibility of creating general-purpose corpora,
featuring multi-directional entailment relations, that
subsume the traditional RTE-like annotation.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853). The au-
thors would like to thank Emanuele Pianta for the
helpful discussions, and Giovanni Moretti for the
valuable support in the creation of the CLTE dataset.
References
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The Fifth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of TAC 2009.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, and Bernardo Magnini.
2010. Building Textual Entailment Specialized Data
Sets: a Methodology for Isolating Linguistic Phenom-
ena Relevant to Inference. Proceedings of LREC 2010.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using Mechanical Turk to Build Machine Translation
Evaluation Sets. Proceedings of the NAACL 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk.
Johan Bos, Fabio Massimo Zanzotto, and Marco Pennac-
chiotti. 2009. Textual Entailment at EVALITA 2009.
Proceedings of EVALITA 2009.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing Speech and Language Data With Amazons Me-
chanical Turk. Proceedings NAACL-2010 Workshop
on Creating Speech and Language Data With Amazons
Mechanical Turk.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
Proceedings of NAACL-HLT 2010.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. Proceedings of ACL-HLT
2011.
Rada Mihalcea and Carlo Strapparava. 2009. The Lie
Detector: Explorations in the Automatic Recognition
of Deceptive Language. Proceedings of ACL 2009.
Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.
2008. Collecting a Why-Question Corpus for Devel-
opment and Evaluation of an Automatic QA-System.
Proceedings of ACL 2008.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. Proceed-
ings of the NAACL 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical Turk.
Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.
2010. Ask Not What Textual Entailment Can Do for
You... Proceedings of ACL 2010.
Rion Snow, Brendan O?Connor, Daniel Jurafsky and An-
drew Y. Ng. 2008. Cheap and Fast - But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. Proceedings of EMNLP 2008.
Rui Wang and Chris Callison-Burch. 2010. Cheap Facts
and Counter-Facts. Proceedings of the NAACL 2010
Workshop on Creating Speech and Language Data
With Amazons Mechanical Turk.
679
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1602?1613,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Abstractive Summarization of Product Reviews Using Discourse Structure
Shima Gerani
?? ?
Yashar Mehdad
? ?
Giuseppe Carenini
?
Raymond T. Ng
?
Bita Nejat
?
?
University of Lugano
?
University of British Columbia
Switzerland Vancouver, BC, Canada
{gerani,mehdad,carenini,rng,nejatb}@cs.ubc.ca
Abstract
We propose a novel abstractive summa-
rization system for product reviews by tak-
ing advantage of their discourse structure.
First, we apply a discourse parser to each
review and obtain a discourse tree repre-
sentation for every review. We then mod-
ify the discourse trees such that every leaf
node only contains the aspect words. Sec-
ond, we aggregate the aspect discourse
trees and generate a graph. We then select
a subgraph representing the most impor-
tant aspects and the rhetorical relations be-
tween them using a PageRank algorithm,
and transform the selected subgraph into
an aspect tree. Finally, we generate a
natural language summary by applying a
template-based NLG framework. Quan-
titative and qualitative analysis of the re-
sults, based on two user studies, show that
our approach significantly outperforms ex-
tractive and abstractive baselines.
1 Introduction
Most existing works on sentiment summarization
focus on predicting the overall rating on an en-
tity (Pang et al., 2002; Pang and Lee, 2004) or
estimating ratings for product features (Lu et al.,
2009; Lerman et al., 2009; Snyder and Barzilay,
2007; Titov and McDonald, 2008)). However, the
opinion summaries in such systems are extractive,
meaning that they generate a summary by concate-
nating extracts that are representative of opinion
on the entity or its aspects.
Comparing extractive and abstractive sum-
maries for evaluative texts has shown that an ab-
stractive approach is more appropriate for sum-
marizing evaluative text (Carenini et al., 2013;
?
The contribution of the first two authors to this paper
was equal.
Di Fabbrizio et al., 2014). This finding is also
supported by a previous study in the context of
summarizing news articles (Barzilay et al., 1999).
To the best of our knowledge, there are only three
previous works on abstractive opinion summariza-
tion (Ganesan et al., 2010; Carenini et al., 2013;
Di Fabbrizio et al., 2014). The first work (Gane-
san et al., 2010) proposes a graph-based method
for generating ultra concise opinion summaries
that are more suitable for viewing on devices with
small screens. This method does not provide a
well-formed grammatical abstract and the gener-
ated summary only contains words that occur in
the original texts. Therefore, this approach is more
extractive than abstractive. Another limitation is
that the generated summaries do not contain any
information about the distribution of opinions.
In the second work, (Carenini et al., 2013) ad-
dresses some of the aforementioned problems and
generates well-formed grammatical abstracts that
describe the distribution of opinion over the en-
tity and its features. However, for each product,
this approach requires a feature taxonomy hand-
crafted by humans as an input, which is not scal-
able. To partially address this problem (Mukherjee
and Joshi, 2013) has proposed a method for the au-
tomatic generation of a product attribute hierarchy
that leverages ConceptNet (Liu and Singh, 2004).
However, the resulting ontology tree has been used
only for sentiment classification and not for clas-
sification.
In the third and most recent study, (Di Fabbrizio
et al., 2014) proposed Starlet-H as a hybrid ab-
stractive/extractive sentiment summarizer. Starlet-
H uses extractive summarization techniques to se-
lect salient quotes from the input reviews and em-
beds them into the abstractive summary to exem-
plify, justify or provide evidence for the aggregate
positive or negative opinions. However, Starlet-H
assumes a limited number of aspects as input and
needs a large amount of training data to learn the
1602
ordering of aspects for summary generation.
Highlighting the reasons behind opinions in re-
views was also previously proposed in (Kim et al.,
2013). However, their approach is extractive and
similar to (Ganesan et al., 2010) does not cover the
distribution of opinions. Furthermore, it aims to
explain the opinion on only one aspect, rather than
explaining the overall opinion on the product, its
aspects and how they affect each other.
To address some of the above mentioned limita-
tions , in this paper we propose a novel abstrac-
tive summarization framework that generates an
aspect-based abstract from multiple reviews of a
product. In our framework, anything that is eval-
uated in the review is considered an aspect, in-
cluding the product itself. We propose a natural
language generation (NLG) framework that takes
aspects and their structured relation as input and
generates an abstractive summary. However, un-
like (Carenini et al., 2013), our method assumes no
domain knowledge about the entity in terms of a
user-defined feature taxonomy. On the other hand,
in contrast with Starlet-H, we do not limit the in-
put reviews to a small number of aspects and our
aspect ordering method takes advantage of rhetor-
ical information and does not require any training
data. Our method relies on the discourse struc-
ture and discourse relations of reviews to infer the
importance of aspects as well as the association
between them (e.g., which aspects relate to each
other).
Researchers have recently started using the dis-
course structure of text in sentiment analysis and
have shown its advantage in improving sentiment
classification accuracy (e.g., (Lazaridou et al.,
2013; Trivedi and Eisenstein, 2013; Somasun-
daran et al., 2009; Asher et al., 2008)). However,
to the best of our knowledge, none of the existing
works have looked into exploiting discourse struc-
ture in abstractive review summarization.
In our work, importance of aspects, derived
from the reviews? discourse structure and rela-
tions, is used to rank and select aspects to be in-
cluded in the summary. More specifically, we start
with the most important (highest ranked) aspects
to generate a summary and add more aspects to
the system until a summary of desired length is
obtained. Aspect association is considered to bet-
ter explain how the opinions on aspects affect each
other (e.g., opinion over specific aspects affect the
opinion over the more general ones). Consider
the following sentence as an example summary
generated by our system for the entity Camera
Canon G3: ?All reviewers who commented on the
camera, thought that it was really good mainly be-
cause of the photo quality.? This summary encap-
sulates all the following key pieces of information:
1) camera and photo quality are the most impor-
tant aspects, 2) People have positive opinion on
camera in general and on photo quality as one of
its features, and finally 3) photo quality is the main
reason behind users satisfaction on camera. Such
summary helps users understand the reason behind
a rating of a product or its aspects without going
through all reviews or reading scattered opinions
on different aspects in multiple sentences of an ex-
tractive summary.
This paper makes the following contributions:
1. We propose a novel content selection and struc-
turing strategy for review summarization, that as-
sumes no prior domain knowledge, by taking ad-
vantage of the discourse structure of reviews.
2. We propose a novel product-independent
template-based NLG framework to generate an ab-
stract based on the selected content, without re-
lying on deep syntactic knowledge or sophisti-
cated NLG methods. Our framework, similarly to
(Carenini et al., 2013), can effectively convey the
distribution of opinions.
3. We present the first study that investigates the
use of discourse structure information in both con-
tent selection and abstract generation for multi-
document summarization.
Quantitative and qualitative analysis over eval-
uation results of two user studies on a set of user
reviews on twelve different products show that our
system is an effective abstractive system for re-
view summarization.
2 Summarization Framework
At a high-level, our summarization framework in-
volves generating a summary from multiple in-
put reviews based on an Aspect Hierarchy Tree
(AHT) that reflects the importance of aspects as
well as the relationships between them. In our
framework, an AHT is generated automatically
from the set of input reviews, where each sen-
tence of every review is marked by the aspects pre-
sented in that sentence and the polarity of opin-
ions over them. There are various methods for
extracting the aspects and predicting the polar-
ity of opinion (Hu and Liu, 2004b; Hu and Liu,
1603
2006; Kim et al., 2011). In this paper we do not
focus on aspect extraction and sentiment predic-
tion but rather consider the aspect and their po-
larity/strength (P/S) information given as input to
the system. P/S scores are integer values in the
range [-3, +3], where +3 is the most positive and
-3 is the most negative polarity value. We also
do not attempt to automatically resolve corefer-
ences between aspects. For example, the aspect
?g3?, ?canon g3? and ?canon? were manually
collapsed as into ?camera?. This preprocessing
step helps to reduce the noise generated by inac-
curate aspect labeling in our reviews. Figure 1
shows two sample input reviews where the aspects
and their P/S scores are identified. For example, in
R1, aspects camera, photo quality and auto mode
are mentioned. The P/S values for the three as-
pects are [+2], [+3] and [+2] respectively which
indicate positive opinion on all aspects.
The first component of our system applies a dis-
course parser to each review and obtains a dis-
course tree representation for every review (e.g.
Figure 1 (a) and (b)). The discourse trees are then
modified such that every leaf node only contains
the aspect words. The output of the first compo-
nent is an aspect-based discourse tree (ADT) for
every review (e.g. Figure 1 (c) and (d)). In the
second component, we aggregate the ADTs and
generate a graph called Aggregated Rhetorical Re-
lation Graph (ARRG) (e.g. Figure 1 (f)). The
third component of our framework, is responsi-
ble for content selection and structuring. It takes
ARRG as input, runs Weighted PageRank, and se-
lects a subgraph (e.g. Figure 1 (g)) representing
the most important aspects. Finally it transforms
the selected subgraph into a tree and provides an
AHT as output (e.g. Figure 1 (h)). The gener-
ated AHT is the input of the last component which
generates a natural language summary by apply-
ing micro planning and sentence realization. We
now describe each component of our framework
in more detail.
3 Discourse Parsing
Any coherent text is structured so that we can
derive and interpret the information. This struc-
ture shows how discourse units (text spans such
as sentences or clauses) are connected and relate
to each other. Discourse analysis aims to reveal
this structure. Several theories have been pro-
posed in the past to describe the discourse struc-
ture, among which the Rhetorical Structure The-
ory (RST) (Mann and Thompson, 1988) is one of
the most popular. RST divides a text into min-
imal atomic units, called Elementary Discourse
Units (EDUs). It then forms a tree representa-
tion of a discourse called a Discourse Tree (DT)
using rhetorical relations (e.g., Elaboration, Ex-
planation, etc) as edges, and EDUs as leaves.
EDUs linked by a rhetorical relation are also dis-
tinguished based on their relative importance in
conveying the author?s message: nucleus is the
central part, whereas satellite is the peripheral
part.
We use a publicly available state-of-the-art dis-
course parser (Joty et al., 2013)
1
to generate a
DT for each product review. Figure 1 (a) and (b)
show DTs for two sample reviews where dotted
edges identify the satellite spans. DT1 in Figure 1
(a) shows that review R1 consists of three EDUs
with two relations Elaboration and Background
between them. It also shows that the first EDU
(i.e. I love camera) is the nucleus (shown by solid
line) of the relation Elaboration and so the rest of
the document (EDUs 2 and 3) is less important and
aims at elaborating on what the author meant in
the first EDU. Similarly, the structure shows that
the third EDU is mentioned as background infor-
mation for EDU2 and so is less important for real-
izing the core meaning of the document.
After obtaining the DTs, we remove all words
from the text spans of each EDU, except the aspect
words. Thus, for each review, we have a DT where
a leaf node represents the aspects occurring in the
corresponding EDU. Note that there may be EDUs
containing no aspects in a review. In such cases,
we keep the corresponding node and mark it with
no aspect. We call the resulting tree an Aspect-
based Discourse Tree (ADT) which will be used
in the next components. Figure 1 (c) and (d) show
ADTs generated from DTs.
4 Aspect Rhetorical Relation Graph
(ARRG)
In the second component, we aim at generat-
ing an ARRG for a product, based on the ADTs
which are the output from the previous compo-
nent. There are two motivations behind aggregat-
ing the ADTs and building the ARRG: i) while
each ADT can be rather noisy because of the infor-
mal language of the reviews and inaccuracies from
1
http://alt.qcri.org/discourse/Discourse Parser Dist.tar.gz
1604
Background
Elaboration
I love this 
camera
I  am amazed at 
the quality of 
photos
that I have took 
simply by using the 
auto mode
great camera !
It gives tons of 
control for 
photo buffs
but still has an 
auto mode
for the novice 
to use
INPUT:
R1: camera[+2], photo quality[+3], auto mode[+2]##I love this camera, I am amazed at the quality of photos that I have took simply using the auto mode
R2: camera[+2], control[+2], auto mode [+1]#great camera! It gives tons of control for photo buffs but still has an auto mode for the novice to use
camera
 photo 
quality
auto mode
Background
Elaboration
camera
control
auto mode
camera
-
(camera, Elaboration, photo quality, 0.5)
(camera, Elaboration, auto mode, 0.33)
(photo quality, Background, auto mode, 0.75)
(camera, Elaboration, auto mode, 0.375)
(camera, Elaboration, control, 0.5)
(control, Contrast, auto mode, 0.66)
Elaboration
Contrast
Elaboration
Elaboration
Contrast
Elaboration
camera
 photo quality
auto mode
control
E
l
a
b
o
r
a
t
i
o
n
,
0
.
5
Background,0.75
E
l
a
b
o
r
a
t
i
o
n
,
0
.
7
0
5
Elaboration,0.5
C
o
n
t
r
a
s
t
,
0
.
6
6
8 4
9
5
camera
 photo quality
control
E
l
a
b
o
r
a
t
i
o
n
,
0
.
5
Elaboration,0.5
camera
 photo quality
control
E
l
a
b
o
r
a
t
i
o
n
E
l
a
b
o
r
a
t
i
o
n
(d) ADT2
(c) ADT1(a) DT1
(b) DT2 (f) ARRG
(h) AHT
(g) ARRG-subgraph
(e) aspect relation tuples extracted from ADTs
OUTPUT:
All reviewers who commented on the camera, thought that it was really good mainly because of the photo quality. Accordingly, about half of the reviewers commented 
about the control and they thought it was fine.
(
i
)
 
M
i
c
r
o
p
l
a
n
n
i
n
g
S
e
n
t
e
n
c
e
 
r
e
a
l
i
z
a
t
i
o
n
Wednesday, May 28, 14
Figure 1: A simple example illustrating different components of our summarization framework.
automatic discourse parsing, aggregating all the
ADTs can reveal more reliable information; and
ii) the aggregated information highlights the most
important aspects overall as well as the strongest
connection between the aspects. This information
can effectively drive the content selection and ab-
stract generation phases.
ARRG is a directed graph in which we allow
multiple edges between two vertices. In ARRG,
vertices represent aspects. We associate to each
aspect/node an importance measure that aggre-
gates all the P/S values that the aspect receives
in all the reviews. By following (Carenini et al.,
2013), let PS(a) be the set of P/S values that an
aspect a receives. The direct measure of impor-
tance of the aspect is defined as:
dir-moi(a) =
?
ps?PS(a)
ps
2
(1)
In ARRG, edges indicate existence of a
rhetorical relation between text spans of a re-
view in which the aspects occurred. Edges are
labeled with the type of the relation as well
as a weight indicating our confidence in the
presence of the relation between the two aspects.
In ARRG, an edge with label r, w from node
u to node v, u
r, w
???? v, indicates the existance
of a relation r with confidence w between two
aspects u and v. Also, the direction of the arrow
indicates that u and v occurred in the satellite
and nucleus spans respectively. For example,
photo quality
elaboration, 0.8
???????????? camera indicates
that there is a high confidence (0.8) that aspect
photo quality was used in a text span to elaborate
aspect camera. Moreover, camera is a more
important aspect compared to photo quality.
To build ARRG, we use all the ADTs that are
output of the previous component (one for each
review). From each ADT
j
, we extract all tuples
of the form (u, r, v, w) in which u is an aspect oc-
curring in a satellite span, v is an aspect occurring
in a nucleus span, r is a relation type and w is the
weight of the tuple computed as follows:
w = 1?0.5
|EDUs between u and v|
|total EDUs in ADT
j
|
?0.5
d
r
d
(2)
where, |.| indicates cardinality of a set. d indi-
cates the depth of the ADT
j
and d
r
indicates the
depth of the sub-tree of ADT
j
rooted at relation
r. Equation 2 weighs a tuple based on two factors:
(i) the relative distance of the EDUs in which the
two aspects u and v participating in relation r oc-
cur. The intuition is that aspects occurring in close
proximity to each other are more related; and (ii)
the depth of the sub-tree at the point of the rela-
tion relative to the depth of the whole ADT
j
. This
is because as we move from leaves to the root of
a DT, the accuracy of the rhetorical structure has
been shown to decrease. Also, at higher levels
of an ADT (intra-sentential relations), it is more
1605
likely that aspects are related through non adjacent
EDUs and so are less strongly related. Figure 1 (e)
shows tuples extracted from sample ADTs.
Notice that every two aspects u and v may be
related by the same relation more than once in an
ADT for a review. Thus, we might have i tuples
with the same u,r, and v but confidence weights
which are not necessarily the same. From every
ADT
j
, we extract all (u, r, v, w
ij
) and select the
one with maximum confidence. We then aggre-
gate the selected tuples extracted from different
reviews. Putting these two steps together, for ev-
ery two aspects u and v related by relation r, we
obtain a single tuple (u, r, v, w?) where
w? =
?
j
max
i
w
ij
(3)
Figure 1 (f) shows an example ARRG built for the
sample reviews.
5 Content Selection and Structuring
The content of the summary is selected by extract-
ing from ARRG a subgraph containing the most
important aspects. Such content is then structured
by transforming the subgraph into an aspect hier-
archy.
5.1 Subgraph Extraction
In ARRG aspects/nodes are weighted by how fre-
quently and strongly they are evaluated in the re-
views (i.e, dir-moi) and edges are weighted by
how frequently and strongly the corresponding as-
pects are rhetorically related in the discourse trees
(Equation 3). In content selection, we want to
extract aspects that not only have high weight,
but that are also linked with heavy edges to other
heavy aspects. This problem can be effectively
addressed by Weighted Page Rank (WPR) (Xing
and Ghorbani, 2004). WPR takes the importance
of both the in-links and out-links of the aspects
into account and distributes rank scores based on
the weights of relations between aspects. In this
way, the heavier aspect nodes, that are either in
the nuclei of many relations or in the satellites of
relations with other heavy aspects, are promoted.
We then update the weight of nodes (aspects) with
the new score from WPR. Finally, we rank nodes
based on their updated score moi and select the
top N aspects.
moi(a) = ?dir-moi(a) + (1??)WPR(a) (4)
Here ? is a coefficient that can be tuned on a de-
velopment set or can be set to 0.5 without tuning.
Figure 1 (g) shows an example subgraph selected
from the sample ARRG.
5.2 Aspects Subgraph to Aspects Hierarchy
Transformation
In this step, we generate a hierarchical tree struc-
ture for aspects. Such a tree structure helps to
navigate over aspects and can be easily traversed
to find certain aspects and their relation to their
parent or children. The hierarchy of aspects also
matches the intuition that the root node is the most
frequent and general aspect (often the product) and
as the depth increases, nodes represent more spe-
cific aspects of the product with less frequency and
weight.
To obtain a hierarchical tree structure from the
extracted subgraph, we first build an undirected
graph as follows: we merge the edges connecting
two nodes and consider the sum of their weights
as the weight of the merged graph. We also ignore
the relation direction for the purpose of generat-
ing the tree. We then find the Maximum Span-
ning Tree of the undirected subgraph and set the
highest weighted aspect as the root of the tree.
This process results in a useful knowledge struc-
ture of aspects with their associated weight and
sentiment polarity connected with the rhetorical
relations called Aspect Hierarchical Tree (AHT).
Figure 1 (h) shows the generated AHT from the
sub-graph.
6 Abstract Generation
The automatic generation of a natural language
summary in our system involves the following
tasks (Reiter and Dale, 2000): (i) microplanning,
which covers lexical selection; and (ii) sentence
realization, which produces english text from the
output of the microplanner.
6.1 Microplanning
Once the content is selected and structured, it is
passed to the microplanning module which per-
forms lexical choice. Lexical choice is an impor-
tant component of microplanning. Lexical choice
is formulated in our system based on a ?formal?
style, language ?variability? and ?fluent? connec-
tivity among other lexical units. Table 1 demon-
strates our lexical choice strategy.
1606
Quantifiers:
if (relative-number == 1) : [?All users (x people) who commented about the aspect?, ?All costumers (x people) that reviewed the aspect?, ...]
if (relative-number >= 0.8) : [?Almost all users commented about the aspect and they?, ?Almost all costumers mentioned the aspect and they?, ...]
if (relative-number >= 0.6) : [?Most users commented about the aspect and they mainly?, ?Most shoppers mentioned aspect and they?, ...]
if (relative-number >= 0.45) : [?Almost half of the users commented about the aspect and they?, ?Almost 50% of the shoppers mentioned the aspect and they?, ...]
if (relative-number >= 0.2) : [?About y% of the reviewers commented about the aspect and they?, ?Around y% of the shoppers mentioned the aspect and they?, ...]
if (relative-number >= 0.0) : [?z reviewers commented about the aspect and in overall they?, ?z shoppers mentioned about the aspect and they?, ...]
Polarity verbs:
if (controversial(aspect)) : [?had controversial opinions about it?, ?expressed controversial opinions about this feature?, ...]
else: if (average <= ?2) : [?hated it?, ?felt that it was very poor?, ?thought that it was very poor?, ...]
if (average <= ?1) : [?disliked it?, ?felt that it was poor?, ?thought that it was poor?, ...]
if (average < 0) : [?did not like it?, ?felt that it was weak?, ?thought that it was weak?, ...]
if (average == 0) : [?did not express any strong positive or negative opinion about it?, ...]
if (average <= +1) : [?liked it?, ?felt that it was fine?, ?thought that it was satisfactory?, ...]
if (average <= +2) : [?absolutely liked it?, ?really liked this feature?, ?felt that it was a really good feature?, ?thought that it was really good?, ...]
if (average <= +3) : [?loved it?, ?felt that it was great?, ?thought that it was great?, ...]
Connectives
[?Also, related to the aspect?, ?Accordingly, ?, ?Moreover, regarding the aspect, ? ,?In relation to the aspect, ?, ?Talking about the aspect, ?, ...]
Table 1: Microplanning strategy for lexical choice. The selected lexical items will fill the template in the
realization step.
Sentence realization templates:
First sentence templates:
if (polarity-agreement(root,highest-weighted-child) & connecting-relation == [elaboration, explain, cause, summary, same-unit, background, evidence, justify]):
?quantifier + polarity-verb + ?mainly because of the? + highest-weighted-child?
else: ?quantifier + polarity-verb?
First level children (aspects) sentences templates:
?connective + ?, ? + quantifier + ? ? + polarity-verb?
Supporting sentences templates:
if (#children(aspect)==1): ?connective + quantifier + verb ?
elseif (#children(aspect)>1 & polarity-agreement(children)): ?connective + quantifier + verb + [and, similarly, while, ...] + quantifier + verb?
elseif (#children(aspect)>1 & !polarity-agreement(children)): ?connective + quantifier + verb + [but, in contrast, on contrary, ...] + quantifier + verb?
Table 2: Sentence realization templates.
Quantifiers: for each aspect, a quantifier is se-
lected based on both the absolute and relative
number of users whose opinions contributed to the
evaluation of the aspect.
Polarity verbs: for each aspect, a polarity verb is
selected based on the average sentiment polarity
strength for that aspect. Although the average, in
most cases, can be a good metric to evaluate the
polarity of an aspect, it fails when the distribution
of evaluations is centered on zero, for instance, if
there are equal numbers of positive and negative
evaluations (i.e., controversial). To partially solve
this problem, we first check whether the aspect
evaluation is controversial by applying the formula
proposed by (Carenini and Cheung, 2008). In the
case of controversiality, our microplanner selects
a lexical item to express the controversiality of the
aspect. In other cases, we use the average and se-
lect the polarity verb based on that.
Connectives: in order to form more fluent and
readable sentences and to increase the language
variability, we randomly select our connectives
from the list shown in Table 1. Moreover, when
a parent aspect (excluding the root in AHT) has
two children, they are connected by one of the co-
ordinating conjunction ?[and, similarly]? if they
agree on polarity, and they will be connected by
a choice of ?[on the contrary, in contrast]? other-
wise (see Supporting sentences templates in Table
2). As an alternative we could have selected con-
nectives based on the discourse relations specified
in the aspects tree. However, this is left as future
work.
6.2 Sentence Realization
The realization of our abstract generation is per-
formed by applying a rather simple and compre-
hensive template-based strategy. Depending on
the specific lexical choice in microplanning step,
an appropriate template and corresponding fillers
are selected as shown in Table 2. We develop three
different templates: i) generates the first abstract
sentence; ii) generates the abstract sentence for the
aspects with no children; and iii) generates sup-
porting sentences for aspects with children.
For illustration, assuming that we apply this
strategy to a 5-node variation of the AHT in Figure
1 (h), where the aspect ?control? has two children
?auto mode? and ?setting?, we obtain ?All review-
ers (45 people) who commented on the camera,
thought that it was really good mainly because of
the photo quality. Accordingly, about 24% of the
reviewers commented about the control and they
1607
thought it was fine. Also, related to the control, 7
users expressed their opinion about the auto mode
and they liked it, similarly, 6 shoppers commented
about the setting and they thought that it was sat-
isfactory.?
7 Experimental Setup
7.1 Dataset and Baselines
We conduct our experiments using the customer
reviews of twelve products obtained from (Hu and
Liu, 2004a): 4 digital cameras, 1 DVD player, 1
MP3 player, 2 routers, 2 phones, 1 diaper and 1
antivirus. The reviews were collected from Ama-
zon.com and Cnet.com. We use manually anno-
tated aspects and their associated sentiment from
the same dataset.
We compare the summaries generated by our
system with two state-of-the-art extractive base-
lines and a simpler version of our abstractive sys-
tem, as follows:
1) MEAD-LexRank (LR): we use the LexRank
(Erkan and Radev, 2004) implementation inside
the MEAD summarization framework (Radev et
al., 2004), which outperforms other algorithms
implemented in the MEAD framework.
2) MEADStar (MEAD*): a state-of-the-art ex-
tractive opinion summarization system (Carenini
et al., 2013), which is adapted from the
open source summarization framework MEAD.
MEAD* orders aspects by the number of sen-
tences evaluating that aspect, and selects a sen-
tence from each aspect until it reaches the word
limit. The sentence that is selected for each aspect
is the one with the highest sum of polarity/strength
evaluations for any aspect.
3) Simple Abstractive (SA): we sort the aspects
of each product based on dir-moi (Equation 1).
Then, for each aspect, we generate a sentence
based on a simple template ?quantifier + polarity-
verb? until the summary reaches the word limit.
We limit the length of our summaries to 150
words. In our experiment we use the default pa-
rameter in Equation 4 without tuning (i.e. ? =
0.5). Our system starts the content selection pro-
cess with 10 aspects and generates a summary
based on a AHT with 10 aspects. We add one as-
pect, reproduce the AHT and regenerate the sum-
mary. We repeat this process until the word limit
is reached.
7.2 Evaluation Framework
On one hand, the lack of product reviews datasets
with human written summaries, and on the other
hand, the difficulty of generating human-written
summaries for reviews, makes review summary
evaluation a very challenging task.
We evaluate the summaries generated by our
system by performing two user studies based on
pairwise preferences using a popular crowdsourc-
ing service.
2
The user preference evaluation is an
effective method for opinion summarization (e.g.,
(Lerman et al., 2009)). The main motivations be-
hind pairwise preferences evaluation is two-fold:
i) raters can make a preference decision more ef-
ficiently than a scoring judgment; and ii) rater
agreement is higher in preference decisions than
in scoring judgments (Ariely et al., 2003).
In both user studies, for each product, we run
six pairwise comparisons for four summaries. In
each rating assignment, two summaries of the
same product were placed in random order. Raters
were shown the name of each product along with
the relevant summaries and were asked to express
their preference for one summary over the other
using a simple set of criteria. For two summaries
S
1
and S
2
raters should choose one of the follow-
ing three options: 1) Prefer S
1
, 2) Prefer S
2
, 3) No
preference.
Raters were specifically instructed that their rat-
ing should express ?overall satisfaction with the
information provided by the summary?. Raters
were also asked to provide a brief comment jus-
tifying their choice. Over 48 raters participated in
each study, and each comparison was evaluated by
at least five raters generating more than 360 judg-
ments for each user study. We pre-select the high
skilled raters to ensure a higher quality results.
The main difference between the two user stud-
ies is that in ?user study 1?, we show two sum-
maries to the raters and ask them to choose the one
they prefer without showing them the original re-
views. In contrast, in ?user study 2?, we show two
summaries with links to the full text of the reviews
for the raters to explore. In order to make sure that
the raters read the reviews, we ask them to write
a short summary of the reviews before rating the
automatic summaries. We ran two different user
studies because: i) for each product there might be
many reviews to be included; ii) there is no guar-
anty that raters, in various evaluation settings, read
2
www.crowdflower.com
1608
System I vs System II Agreement No preference Preferred Sys I Preferred Sys II
User Studies
1 2 1 2 1 2 1 2
LR vs MEAD*
0.33 0.75 7% 6% 35% 20% 58% 74%
LR vs SA
0.42 0.83 0% 0% 38% 21% 62% 79%
LR vs Our System
0.50 1.00 0% 3% 26% 13% 74% 84%
MEAD* vs SA
0.58 0.83 0% 0% 38% 20% 62% 80%
MEAD* vs Our System
0.67 0.50 0% 3% 25% 30% 75% 67%
SA vs Our System
0.42 0.50 12% 11% 23% 32% 65% 57%
Table 3: Results of pairwise preference user studies. Statistically significant improvements (p < 0.01)
over the baselines are demonstrated by bold fonts. Italic fonts indicate statistical significance (p < 0.01)
of abstractive methods (SA and Our System) over extractive approaches (LR and MEAD*).
Systems LR MEAD* SA Our System
User Studies
1 2 1 2 1 2 1 2
Preference
33% 18% 41% 41% 49% 63% 71% 69%
Table 4: System preference results. Statistically significant improvements (p < 0.01) over the baselines
are demonstrated by bold fonts.
the reviews (partially or completely); and iii) there
is no evidence regarding the depth that each rater
would look into the reviews. Therefore, choosing
between user study 1 and 2 is not a straightforward
decision. In other words, designing the two user
studies in this way helps us to answer the ques-
tion: ?Does the fact that raters can read all the
reviews affect their ratings??.
8 Results
This section provides a quantitative and qualitative
analysis of the evaluation results
3
.
8.1 Quantitative Analysis
Quantitative results for both user studies are
shown in Table 3. The second column indicates
the percentage of judgments for which the raters
were in agreement. Agreement here is a weak
agreement, where four (out of five) raters are de-
fined to be in agreement if they all gave the same
rating. The next three columns indicate the per-
centage of judgments for each preference cate-
gory, grouped into two user studies. In addi-
tion, we measure the preference for each system
in both user studies (Table 4). For each system,
the preference is the number of times raters prefer
the system, divided by the total number of judg-
ments for that system (e.g., if A is preferred over
3
The evaluation results and summaries obtained
from CrowdFlower are publicly available and can
be downloaded from: https://www.cs.ubc.
ca/cs-research/lci/research-groups/
natural-language-processing/reviews/
user_study_results.zip
B 10 out of 30 times, and A is preferred over C
15 out of 20 times, the overall preference of A is
(10+15)/(30+20)=50%)
Abstractive vs. Extractive: the results of our sys-
tem and SA in Table 3 show statistically signifi-
cant improvements in pairwise preference over ex-
tractive baselines (LR and MEAD*) in both user
studies.
4
Moreover, the results of overall prefer-
ence in Table 4 demonstrates that two abstractive
systems are preferred over the extractive ones in
both studies. This further supports the findings in
the previous studies (e.g., (Carenini et al., 2013))
that users prefer abstractive summarization. We
can observe that, in both user studies, raters prefer
our system over other abstractive and extractive
baselines. Also, the highest pairwise preference
percentages occur comparing an extractive and an
abstractive system (e.g., LR vs Our System).
Abstractive Systems: the raters prefer our system
over SA in both user studies (65% and 57%), and
our system ranks first in our pairwise preference
user studies. Knowing that both systems are ab-
stractive and the differences between them comes
from using the rhetorical structure in the content
selection and abstract generation phases, proves
the effectiveness of using rhetorical structure and
relations in abstractive summarization of reviews.
Extractive Systems: the result in Table 3 and 4
demonstrate that raters prefer MEAD* over LR.
Although both systems are extractive, the MEAD*
system has been proposed for extractive opinion
4
The statistical significance tests was calculated by ap-
proximate randomization, as described in (Yeh, 2000).
1609
Preference Sys 1 to Sys 2 Reasons Examples of preference justification taken from the raters comments
Our System to LR and MEAD* Readability, coverage of aspects,
aggregation of opinions
better wording, more objective, more depth, I like the stats, more detail about
people opinion, less personal experience, detail comparison from different
reviews, a summary in a summary, mentions more features, ...
LR and MEAD* to Our System Descriptiveness, personal point of
views, product capabilities
explain how the product is positive, good characteristics about the product,
has lot more to tell, more descriptive about features, personal perspective,
not only characteristics but also ability, more true to the product itself, ...
Our System to SA The relations between the aspects,
more language variability
provides a bit more information, is very complete, not repetitive, more ele-
gant, coherent, .....
SA to Our System Simpler structure, more aspects written better, has touched variety of features, ...
Table 5: System preference results. The reasons are classified based on raters justifications preferring
the underlined systems.
summarization. In contrast, LR is a generic ex-
tractive summarization system which is not opti-
mized for opinion summarization. This also fur-
ther demonstrates the need for opinion and reviews
summarization systems.
User Study 1 vs. User Study 2: the first in-
teresting observation is that, although the over-
all ranking of systems in both user studies does
not change, there are some changes in the re-
sults. This indicates that reading the reviews ef-
fects preference decisions. We can observe that
in all cases except one (MEAD* vs Our System)
the agreement between the raters increases sig-
nificantly when they are given the reviews. This
can be interpreted as reading the reviews helps
the rater to choose a better summary easier and
more effectively. Moreover, we calculate the over-
all agreement for both user studies.
5
Case study 2
reports a higher overall agreement (70%) in com-
parison with the user study 1 (65%). This further
proves our finding that showing the reviews can
help the raters with their preference judgment.
In Table 3, the preference of sys 2 (last col-
umn) significantly rises for all cases when com-
pared with the LR system. This proves that raters
strongly prefer the summaries that cover opinion-
ated sentences, specifically when they are exposed
to the reviews. The same result is reflected in Ta-
ble 4, where the overall preference of LR drops
when the raters are given the reviews. We also ob-
serve a significant rise in preference of sys 2 when
MEAD* is compared with SA (Table 3) and in the
overall preference of SA (Table 4) in user study
2. This proves that raters become more confident
in preferring an abstractive summary over an ex-
tractive one when the reviews are given to them.
In contrast, we notice that the preference of sys
2 drops comparing ?MEAD* vs Our System? and
?SA vs Our System?. Knowing that the drop is
5
The agreement is calculated based on 100 randomly sam-
pled units selected from our crowdsourcing job.
not significant and the the overall ranking of sys-
tems remains unchanged, this case is less straight
forward to interpret.
8.2 Qualitative Analysis
We collect and group the rater justifications in the
results we obtain by crowdsourcing our evaluation
framework, when preferring a summary over an-
other, in Table 5. To make the comparison more
clear, Example 1 shows the summaries generated
by MEAD* and our system.
Comparing our system with the extractive base-
lines, raters? justifications are classified in three
main categories. Although the language of the ex-
tractive summaries is less formal, raters often pre-
fer our system in terms of presentation and lan-
guage. They justify their selections by expressing
phrases such as ?better grammar? or ?fewer er-
rors?. They also comment about the coverage of
aspects in the summaries generated by our system
and they realize that our system was capable of
aggregating the opinions for each aspect. In con-
trast, when they prefer the extractive summaries,
they like the descriptive language of the summary
and the technical details of the products that were
missing in our system summaries.
We also notice that raters realize the usage of
structure (AHT) in our system (both of content se-
lection and summary generation) and they appre-
ciate it by expressing phrases such as ?very com-
plete?, ?more elegant? or ?related features?. In
contrast, they sometimes appreciate a simpler lan-
guage in summaries generated by SA. Moreover,
few raters prefer the higher coverage in SA sum-
maries. This is mainly because not using connec-
tives and structure in SA leaves more space to in-
clude more aspects.
1610
Product: Nikon Coolpix 4300
MEAD*: it is very compact but the controls are so well designed that
they ?re still easy to use . It ?s easy for beginners to use , but has features
that more serious photographers will love , so it ?s an excellent camera
to grow into . But overall this is a good camera with a ? really good ?
picture clarity ; an exceptional close-up shooting capability .The battery
life is very good , i got about 90 minutes with the lcd turned on all
the time , the first time around , and i have been using it with the lcd
off every now and then , and have yet needed to recharge it . Yes ,
the picture quality and features which are too numerous to mention are
unmatched for any camera in this price range.
Our System: All reviewers (34 people), who commented on the cam-
era, felt that it was really good mainly because of the picture. Around
26% of the reviewers expressed their opinion about the picture quality
and they really liked it. Around 24% of the reviewers noted the use
and they thought that it was satisfactory. Talking about the use, around
24% of the reviewers expressed their opinion about the size and they
felt that it was fine. Only 6 reviewers commented about the scene mode
and in overall they thought that it was satisfactory. Moreover, regarding
the scene mode, 4 shoppers mentioned about the manual mode and they
thought that it was satisfactory, and similarly only 4 reviewers com-
mented about the auto mode and in overall they did not express any
strong positive or negative opinion about it. Only 4 costumers men-
tioned the software and they felt that it was really good.
Example 1. Summaries generated by our system
and MEAD* baseline for the Nikon Coolpix 4300
camera. For brevity we exclude other baselines.
9 Conclusions
We have presented a framework for abstractive
summarization of product reviews based on dis-
course structure. For content selection, we pro-
pose a graph model based on the importance
and association relations between aspects, that as-
sumes no prior domain knowledge, by taking ad-
vantage of the discourse structure of reviews. For
abstract generation, we propose a product inde-
pendent template-based natural language genera-
tion (NLG) framework that takes aspects and their
structured relation as input and generates an ab-
stractive summary. Quantitative evaluation results,
based on two pairwise preference user studies,
show substantial improvement over extractive and
abstractive baselines, including MEAD*, which
is considered a state-of-the-art opinion extractive
summarization system, and a simpler version of
our abstractive system. In future work, we plan
to extend the microplanning phase by taking ad-
vantage of the highly weighted rhetorical relations
between the aspects and select connective phrases
based on the discourse relations specified in the
aspects tree. In addition, we plan to develop and
evaluate an end-to-end system, in which the aspect
extraction and polarity estimation of aspects are
automated. In this way, we can achieve an end-to-
end automatic summarizaion system for product
reviews.
Acknowledgments
This work was supported in part by Swiss Na-
tional Science Foundation (PBTIP2-145659) and
NSERC Business Intelligence Network. We
would like to thank the anonymous reviewers for
their valuable comments. We also acknowledge
Shafiq Rayhan Joty for his help regarding rhetori-
cal parser.
References
Dan Ariely, George Loewenstein, and Drazen Prelec.
2003. ?Coherent Arbitrariness?: Stable Demand
Curves Without Stable Preferences. Quarterly Jour-
nal of Economics, 118:73?105.
Nicholas Asher, Farah Benamara, and Yvette Yannick
Mathieu. 2008. Distilling opinion in discourse: A
preliminary study. In Coling 2008: Companion vol-
ume: Posters and Demonstrations, pages 5?8.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings
of the 37th annual meeting of the Association for
Computational Linguistics on Computational Lin-
guistics, ACL ?99, pages 550?557, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Giuseppe Carenini and Jackie Chi Kit Cheung. 2008.
Extractive vs. nlg-based abstractive summarization
of evaluative text: the effect of corpus controversial-
ity. In INLG ?08: Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
pages 33?41, Morristown, NJ, USA. Association for
Computational Linguistics.
Giuseppe Carenini, Jackie Chi Kit Cheung, and Adam
Pauls. 2013. Multi-document summarization
of evaluative text. Computational Intelligence,
29(4):545?576.
Giuseppe Di Fabbrizio, Amanda Stent, and Robert
Gaizauskas. 2014. A hybrid approach to multi-
document summarization of opinions in reviews. In
Proceedings of the 8th International Natural Lan-
guage Generation conference, INLG 2014.
G?unes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22(1):457?479,
December.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
340?348, Stroudsburg, PA, USA. Association for
Computational Linguistics.
1611
Minqing Hu and Bing Liu. 2004a. Mining and sum-
marizing customer reviews. In Proceedings of ACM
SIGKDD Conference on Knowledge Discovery and
Data Mining 2004 (KDD 2004), pages 168?177,
Seattle, Washington.
Minqing Hu and Bing Liu. 2004b. Mining opinion
features in customer reviews. In Proceedings of the
Nineteenth National Conference on Artificial Intelli-
gence (AAAI-2004).
Minqing Hu and Bing Liu. 2006. Opinion feature
extraction using class sequential rules. In Pro-
ceedings of AAAI 2006 Spring Sympoia on Compu-
tational Approaches to Analyzing Weblogs (AAAI-
CAAW 2006).
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 486?496,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Hyun Duk Kim, Kavita Ganesan, Parikshit Sondhi, and
ChengXiang Zhai. 2011. Comprehensive review of
opinion summarization.
Hyun Duk Kim, Malu Castellanos, Meichun Hsu,
ChengXiang Zhai, Umeshwar Dayal, and Riddhi-
man Ghosh. 2013. Compact explanatory opinion
summarization. In Proceedings of the 22Nd ACM
International Conference on Conference on Infor-
mation &#38; Knowledge Management, CIKM ?13,
pages 1697?1702, New York, NY, USA. ACM.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1630?1639, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-
Donald. 2009. Sentiment summarization: Evaluat-
ing and learning user preferences. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
?09, pages 514?522, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Hugo Liu and Push Singh. 2004. Conceptnet: A prac-
tical commonsense reasoning toolkit. BT Technol-
ogy Journal, 22(4):211?226.
Yue Lu, ChengXiang Zhai, and Neel Sundaresan.
2009. Rated aspect summarization of short com-
ments. In Proceedings of the 18th International
Conference on World Wide Web, WWW ?09, pages
131?140, New York, NY, USA. ACM.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Subhabrata Mukherjee and Sachindra Joshi. 2013.
Sentiment aggregation using conceptnet ontology.
In Proceedings of the 6th International Joint Con-
ference on Natural Language Processing, IJCNLP
2013.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42Nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 Conference on Empirical Methods in Natu-
ral Language Processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C?elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
Winkel, and Zhu Zhang. 2004. MEAD ? A plat-
form for multidocument multilingual text summa-
rization. In Conference on Language Resources and
Evaluation (LREC), Lisbon, Portugal.
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation Systems. Cambridge Univer-
sity Press, New York, NY, USA.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In
HLT-NAACL, pages 300?307.
Swapna Somasundaran, Galileo Namata, Janyce
Wiebe, and Lise Getoor. 2009. Supervised and
unsupervised methods in employing discourse rela-
tions for improving opinion polarity classification.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
1 - Volume 1, EMNLP ?09, pages 170?179, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Ivan Titov and Ryan T. McDonald. 2008. A joint
model of text and aspect ratings for sentiment sum-
marization. In Proceedings of the 46th Annual
Meeting of the Association for Computational Lin-
guistics, June 15-20, 2008, Columbus, Ohio, USA,
ACL 2008, pages 308?316. Association for Compu-
tational Linguistics.
Rakshit S. Trivedi and Jacob Eisenstein. 2013. Dis-
course connectors for latent subjectivity in sentiment
analysis. In HLT-NAACL, pages 808?813. The As-
sociation for Computational Linguistics.
1612
Wenpu Xing and Ali Ghorbani. 2004. Weighted pager-
ank algorithm. In Proceedings of the Second Annual
Conference on Communication Networks and Ser-
vices Research, CNSR ?04, pages 305?314. IEEE
Computer Society.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th conference on Computational
linguistics - Volume 2, COLING ?00, pages 947?
953, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
1613
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 321?324,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Towards Cross-Lingual Textual Entailment
Yashar Mehdad1,2, Matteo Negri1, Marcello Federico1
FBK-Irst1, University of Trento2
Trento, Italy
{mehdad,negri,federico}@fbk.eu
Abstract
This paper investigates cross-lingual textual
entailment as a semantic relation between two
text portions in different languages, and pro-
poses a prospective research direction. We
argue that cross-lingual textual entailment
(CLTE) can be a core technology for sev-
eral cross-lingual NLP applications and tasks.
Through preliminary experiments, we aim at
proving the feasibility of the task, and provid-
ing a reliable baseline. We also introduce new
applications for CLTE that will be explored in
future work.
1 Introduction
Textual Entailment (TE) (Dagan and Glickman,
2004) has been proposed as a generic framework for
modeling language variability. Given two texts T
and H, the task consists in deciding if the meaning
of H can be inferred from the meaning of T. So far,
TE has been only applied in a monolingual setting,
where both texts are assumed to be written in the
same language. In this work, we propose and inves-
tigate a cross-lingual extension of TE, where we as-
sume that T and H are written in different languages.
The great potential of integrating (monolingual)
TE recognition components into NLP architectures
has been reported in several works, such as ques-
tion answering (Harabagiu and Hickl, 2006), infor-
mation retrieval (Clinchant et al, 2006), informa-
tion extraction (Romano et al, 2006), and document
summarization (Lloret et al, 2008).
To the best of our knowledge, mainly due to
the absence of cross-lingual TE (CLTE) recognition
components, similar improvements have not been
achieved yet in any cross-lingual application. As
a matter of fact, despite the great deal of attention
that TE has received in recent years (also witnessed
by five editions of the Recognizing Textual Entail-
ment Challenge1), interest for cross-lingual exten-
sions has not been in the mainstream of TE research,
which until now has been mainly focused on the En-
glish language.
Nevertheless, the strong interest towards cross-
lingual NLP applications (both from the market and
research perspectives, as demonstrated by success-
ful evaluation campaigns such as CLEF2) is, to our
view, a good reason to start investigating CLTE, as
well. Along such direction, research can now ben-
efit from recent advances in other fields, especially
machine translation (MT), and the availability of: i)
large amounts of parallel and comparable corpora in
many languages, ii) open source software to com-
pute word-alignments from parallel corpora, and iii)
open source software to set-up strong MT baseline
systems. We strongly believe that all these resources
can potentially help in developing inference mecha-
nisms on multilingual data.
Building on these considerations, this paper aims
to put the basis for future research on the cross-
lingual Textual Entailment task, in order to allow
for semantic inference across languages in different
NLP tasks. Among these, as a long-term goal, we
plan to adopt CLTE to support the alignment of text
portions that express the same meaning in different
languages. As a possible application scenario, CLTE
1http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
2www.clef-campaign.org/
321
can be used to address content merging tasks in tidy
multilingual environments, such as commercial Web
sites, digital libraries, or user generated content col-
lections. Within such framework, as it will be dis-
cussed in the last section of this paper, CLTE com-
ponents can be used for automatic content synchro-
nization in a concurrent, collaborative, and multilin-
gual editing setting, e.g. Wikipedia.
2 Cross Lingual Textual Entailment
Adapting the definition of TE we define CLTE as
a relation between two natural language portions in
different languages, namely a text T (e.g. in En-
glish), and a hypothesis H (e.g. in French), that
holds if a human after reading T would infer that H
is most likely true, or otherwise stated, the meaning
of H can be entailed (inferred) from T .
We can see two main orthogonal directions for ap-
proaching CLTE: i) simply bring CLTE back to the
monolingual case by translating H into the language
of T, or vice-versa; ii) try to embed cross-lingual
processing techniques inside the TE recognition pro-
cess. In the following, we briefly overview and mo-
tivate each approach.
Basic approaches. The simplest approach is to
add a MT component to the front-end of an existing
TE engine. For instance, let the French hypothesis
H be translated into English and then run the TE en-
gine on T and the translation of H. There are sev-
eral good reasons to follow this divide-and-conquer
approach, as well as some drawbacks. Decoupling
the cross-lingual and the entailment components re-
sults in a simple and modular architecture that, ac-
cording to well known software engineering princi-
ples, results easier to develop, debug, and maintain.
Moreover, a decoupled CLTE architecture would al-
low for easy extensions to other languages as it just
requires extra MT systems. Along the same idea of
pivoting through English, in fact, the same TE sys-
tem can be employed to perform CLTE between any
language pair, once MT is available from each lan-
guage into English. A drawback of the decoupled
approach is that as MT is still far from being perfect,
translation errors are propagated to the TE engine
and might likely affect performance. To cope with
this issue, we explored the alternative approach of
applying TE on a list of n-best translations provided
by the MT engine, and take a final decision based on
some system combination criterion. This latter ap-
proach potentially reduces the impact of translation
errors, but might significantly increase the computa-
tional requirements of CLTE.
Advanced approaches. The idea is to move to-
wards a cross-lingual TE approach that takes advan-
tage of a tighter integration of MT and TE algo-
rithms and techniques. This could result in methods
for recognizing TE across languages without trans-
lating the texts and, in principle, with a lower com-
plexity. When dealing with phrase-based statistical
MT (Koehn et al, 2007), a possible approach is to
extract information from the phrase-table to enrich
the inference and entailment rules which could be
used in a distance based entailment system. As an
example the entailment relations between the French
phrase ?ordinateur portable? and the English phrase
?laptop?, or between the German phrase ?europaeis-
chen union? and the English word ?Europe? could
be captured from parallel corpora through statistical
phrase-based MT approaches.
There are several implications that make this ap-
proach interesting. First of all, we believe that re-
search on CLTE can employ inference mechanisms
and semantic knowledge sources to augment exist-
ing MT methods, leading to improvements in the
translation quality (e.g. (Pado? et al, 2009)). In
addition, the acquired rules could as well enrich
the available multilingual resources and dictionaries
such as MultiWordNet3.
3 Feasibility studies
The main purpose of our preliminary experiments is
to verify the feasibility of CLTE, as well as setting
baseline results to be further improved over time. To
this aim, we started by adopting the basic approach
previously discussed. In particular, starting from an
English/French corpus of T-H pairs, we automati-
cally translated each H fragment from French into
English.
Our decisions build on several motivations. First
of all, the reason for setting English and French
as a first language pair for experiments is to rely
on higher quality translation models, and larger
amounts of parallel data for future improvements.
3http://multiwordnet.fbk.eu/
322
Second, the reason for translating the hypotheses is
that, according to the notion of TE, they are usually
shorter, less detailed, and barely complex in terms of
syntax and concepts with respect to the texts. This
makes them easier to translate preserving the origi-
nal meaning. Finally, from an application-oriented
perspective, working with English Ts seems more
promising due the richness of English data available
(e.g. in terms of language variability, and more de-
tailed elaboration of concepts). This increases the
probability to discover entailment relations with Hs
in other languages.
In order to create a realistic and standard setting,
we took advantage of the available RTE data, select-
ing the RTE3 development set and manually trans-
lating the hypotheses into French. Since the man-
ual translation requires trained translators, and due
to time and logistics constraints, we obtained 520
translated hypotheses (randomly selected from the
entire RTE3 development set) which built our bi-
lingual entailment corpus for evaluation.
In the initial step, following our basic approach,
we translated the French hypotheses to English us-
ing Google4 and Moses5. We trained a phrase-
base translation model using Europarl6 and News
Commentary parallel corpora in Moses, applying a
6-gram language model trained on the New York
Times portion of the English Gigaword corpus7.
As a TE engine , we used the EDITS8 package
(Edit Distance Textual Entailment Suite). This sys-
tem is an open source software package based on
edit distance algorithms, which computes the T-H
distance as the cost of the edit operations (i.e. in-
sertion, deletion and substitution) that are necessary
to transform T into H. By defining the edit distance
algorithm and a cost scheme (i.e. which defines the
costs of each edit operation), this package is able to
learn a distance model over a set of training pairs,
which is used to decide if an entailment relation
holds over each test pair.
In order to obtain a monolingual TE model, we
trained and tuned (Mehdad, 2009) our model on the
RTE3 test set, to reduce the overfitting bias, since
4http://translate.google.com
5http://www.statmt.org/moses/
6http://www.statmt.org/europarl/
7http://www.ldc.upenn.edu
8http://edits.fbk.eu/
our original data was created over the RTE3 devel-
opment set. Moreover, we used a set of lexical en-
tailment rules extracted from Wikipedia and Word-
Net, as described in (Mehdad et al, 2009). To be-
gin with, we used this model to classify the cre-
ated cross-lingual entailment corpus in three differ-
ent settings: 1) hypotheses translated by Google, 2)
hypotheses translated by Moses (1st best), and 3) the
original RTE3 monolingual English pairs.
Results reported in Table 1 show that using
Google as a translator, in comparison with the orig-
inal manually-created data, does not cause any drop
in performance. This confirms that merely trans-
lating the hypothesis using a very good translation
model (Google) is a feasible and promising direc-
tion for CLTE. Knowing that Google has one of the
best French-English translation models, the down-
trend of results using Moses translator, in contrast
with Google, is not out of our expectation. Trying
to bridge this gap brings us to the next round of
experiments, where we extracted the n-best trans-
Orig. Google Moses Moses Moses
1st best 30 best > 0.4
Acc. 63.48 63.48 61.37 62.90 62.90
Table 1: Results comparison over 520 test pairs.
lations produced by Moses, to have a richer lexical
variability, beneficial for improving the TE recogni-
tion. The graph in Figure 1 shows an incremental
improvement when the n-best translated hypotheses
are used. Besides that, trying to reach a more mono-
tonic distribution of the results, we normalized the
ranking score (from 0 to 1) given by Moses, and in
each step we chose the first n results over a normal-
ized score. In this way, having the hypotheses with
the score of above 0.4, we achieved the highest accu-
racy of 62.9%. This is exactly equal to adopting the
30-best hypotheses translated by Moses. Using this
method, we could improve the performance up to
1.5% above the 1st best results, achieving almost the
same level of performance obtained with Google.
4 A possible application scenario
Among the many possible applications, the task of
managing textual information in multiple languages
represents an ideal application scenario for CLTE.
Along such direction, our long-term goal is to use
323
Figure 1: Accuracy gained by n-best Moses translations.
CLTE components in the task of synchronizing the
content of documents about the same topic (e.g.
Wikipedia articles), written in different languages.
Currently, multilingual Wikis rely on users to manu-
ally translate different Wiki pages on the same sub-
ject. This is not only a time-consuming procedure
but also the source of many inconsistencies, as users
update the different language versions separately,
and every update would require translators to com-
pare the different language versions and synchronize
the updates. Our goal is to automate this process
by integrating MT and CLTE in a two-step process
where: i) CLTE is used to identify text portions that
should ?migrate? from one page to the other, and ii)
MT is used to actually translate these portions in the
appropriate target language.
The adoption of entailment-based techniques to
address the multilingual content synchronization
task looks promising, as several issues inherent to
such task can be formalized as TE-related problems.
Given two pages (P1 and P2), these issues include
identifying (and then properly managing):
1. Text portions in P1 and P2 that express exactly
the same meaning (bi-directional entailment, or se-
mantic equivalence) and which should not migrate
across pages;
2. Text portions in P1 that are more specific than
portions of P2 (unidirectional entailment between
P2 and P1 or vice-versa) and should replace them;
3. Text portions in P1 describing facts that are not
present in P2, and which should be added in P2 or
vice-versa (the ?unknown? cases in RTE parlance);
4. Meaning discrepancies between text portions
in P1 and text portions in P2 (?contradictions? in
RTE parlance).
5 Conclusion
This paper presented a preliminary investigation to-
wards cross-lingual Textual Entailment, focusing on
possible research directions and alternative method-
ologies. Baseline results have been provided to
demonstrate the potentialities of a simple approach
that integrates MT and monolingual TE compo-
nents. Overall, our work sets a novel framework
for further studies and experiments to improve cross-
lingual NLP tasks. In particular, CLTE can be scaled
to more complex problems, such as cross-lingual
content merging and synchronization.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853)
References
S. Clinchant, C. Goutte, and E. Gaussier. 2006. Lex-
ical entailment for information retrieval. In Proc.
ECIR?06.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proc. of the PASCAL Workshop of Learn-
ing Methods for Text Understanding and Mining.
S. Harabagiu and A. Hickl. 2006. Methods for using tex-
tual entailment in open-domain question answering.
In Proc. COLING/ACL 2006.
P. Koehn et al 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. ACL07 Demo
and Poster Sessions.
E. Lloret, O?. Ferra?ndez, R. Mun?oz, and M. Palomar.
2008. A text summarization approach under the in-
fluence of textual entailment. In Proc. NLPCS 2008.
Y. Mehdad, M. Negri, E. Cabrio, M. Kouylekov, and
B. Magnini. 2009. Edits: An open source framework
for recognizing textual entailment. In Proc. TAC 2009.
To appear.
Yashar Mehdad. 2009. Automatic cost estimation for
tree edit distance using particle swarm optimization.
In Proc. ACL ?09.
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Textual entailment features for machine trans-
lation evaluation. In Proc. StatMT ?09.
L. Romano, M. Kouylekov, I. Szpektor, I. Dagan, and
A. Lavelli. 2006. Investigating a generic paraphrase-
based approach for relation extraction. In Proc. EACL
2006.
324
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1020?1028,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Syntactic/Semantic Structures for Textual Entailment Recognition
Yashar Mehdad
FBK-IRST, DISI
University of Trento
Povo (TN) - Italy
mehdad@fbk.eu
Alessandro Moschitti
DISI
University of Trento
Povo (TN) - Italy
moschitti@disi.unitn.it
Fabio Massimo Zanzotto
DISP
University of Rome ?Tor Vergata?
Roma - Italy
zanzotto@info.uniroma2.it
Abstract
In this paper, we describe an approach based
on off-the-shelf parsers and semantic re-
sources for the Recognizing Textual Entail-
ment (RTE) challenge that can be generally
applied to any domain. Syntax is exploited
by means of tree kernels whereas lexical se-
mantics is derived from heterogeneous re-
sources, e.g. WordNet or distributional se-
mantics through Wikipedia. The joint syn-
tactic/semantic model is realized by means of
tree kernels, which can exploit lexical related-
ness to match syntactically similar structures,
i.e. whose lexical compounds are related. The
comparative experiments across different RTE
challenges and traditional systems show that
our approach consistently and meaningfully
achieves high accuracy, without requiring any
adaptation or tuning.
1 Introduction
Recognizing Textual Entailment (RTE) is rather
challenging as effectively modeling syntactic and
semantic for this task is difficult. Early deep seman-
tic models (e.g., (Norvig, 1987)) as well as more re-
cent ones (e.g., (Tatu and Moldovan, 2005; Bos and
Markert, 2005; Roth and Sammons, 2007)) rely on
specific world knowledge encoded in rules for draw-
ing decisions. Shallower models exploit matching
methods between syntactic/semantic graphs of texts
and hypotheses (Haghighi et al, 2005). The match-
ing step is carried out after the application of some
lexical-syntactic rules that are used to transform the
text T or the hypothesis H (Bar-Haim et al, 2009)
at surface form level. For all these methods, the ef-
fective use of syntactic and semantic information de-
pends on the coverage and the quality of the specific
rules. Lexical-syntactic rules can be automatically
extracted from plain corpora (e.g., (Lin and Pantel,
2001; Szpektor and Dagan, 2008)) but the quality
(also in terms of little noise) and the coverage is low.
In contrast, rules written at the semantic level are
more accurate but their automatic design is difficult
and so they are typically hand-coded for the specific
phenomena.
In this paper, we propose models for effectively
using syntactic and semantic information in RTE,
without requiring either large automatic rule acqui-
sition or hand-coding. These models exploit lexi-
cal similarities to generalize lexical-syntactic rules
automatically derived by supervised learning meth-
ods. In more detail, syntax is encoded in the form of
parse trees whereas similarities are defined by means
of WordNet simlilarity measures or Latent Seman-
tic Analysis (LSA) applied to Wikipedia or to the
British National Corpus (BNC). The joint syntac-
tic/semantic model is realized by means of novel tree
kernels, which can match subtrees whose leaves are
lexically similar (so not just identical).
To assess the benefit of our approach, we carried
out comparative experiments with previous work:
especially with the method described in (Zanzotto
and Moschitti, 2006; Zanzotto et al, 2009). This
constitutes our strong baseline as, although it can
only exploit lexical-syntactic rules, it has achieved
top accuracy in all RTE challenges. The results,
across different RTE challenges, show that our ap-
proach constantly and significantly improves the
1020
baseline model. Moreover, our approach does not
require any adaptation or tuning and uses a compu-
tation for the similarity function based on Wikipedia
which is faster than the computation of tools based
on WordNet or other resources (Basili et al, 2006).
The remainder of the paper is organized as fol-
lows: Section 2 critically reviews the previous work
by highlighting the need of generalizing lexico-
syntactic rules. Section 3 describes lexical similar-
ity approaches, which can serve the generalization
purpose. Section 4 describes how to integrate lex-
ical similarity in syntactic structures using syntac-
tic/semantic tree kernels (SSTK) whereas Section 5
shows how to use SSTK in a kernel-based RTE sys-
tem. Section 6 describes the experiments and re-
sults. Section 7 discusses the efficiency and accu-
racy of our system compared with other RTE sys-
tems. Finally, we draw the conclusions in Section
8.
2 Related work
Lexical-syntactic rules are largely used in textual en-
tailment recognition systems (e.g., (Bar-Haim et al,
2007; Dinu and Wang, 2009)) as they conveniently
encode world knowledge into linguistic structures.
For example, to decide whether the simple sentences
are in the entailment relation:
T2 ??H2
T2 ?In 1980 Chapman killed Lennon.?
H2 ?John Lennon died in 1980.?
we need a lexical-syntactic rule such as:
?3 = XkilledY ? Ydied
along with such rules, the temporal information
should be taken into consideration.
Given the importance of lexical-syntactic rules in
RTE, many methods have been proposed for their
extraction from large corpora (e.g., (Lin and Pantel,
2001; Szpektor and Dagan, 2008)). Unfortunately,
these unsupervised methods in general produce rules
that can hardly be used: noise and coverage are the
most critical issues.
Supervised approaches were experimented in
(Zanzotto and Moschitti, 2006; Zanzotto et al,
2009), where lexical-syntactic rules were derived
from examples in terms of complex relational fea-
tures. This approach can easily miss some useful
information and rules. For example, given the pair
?T2,H2?, to derive the entailment value of the fol-
lowing case:
T4 ??H4
T4 ?In 1963 Lee Harvey Oswald mur-
dered JFK?
H4 ?JFK died in 1963?
we can only rely on this relatively interesting
lexical-syntactic rule (i.e. which is in common be-
tween the two examples):
?5 = (V P (V BZ)(NP X )) ? (S(NP X )(V P (V BZ died)))
Unfortunately, this can be extremely misleading
since it also derives similar decisions for the follow-
ing example:
T6 ??H6
T6 ?In 1956 JFK met Marilyn Monroe?
H6 ?Marilyn Monroe died in 1956?
The problem is that the pairs ?T2,H2? and
?T4,H4? share more meaningful features than the
rule ?5, which should make the difference with re-
spect to the relation between the pairs ?T2,H2? and
?T6,H6?. Indeed, the word ?kill? is more semanti-
cally related to ?murdered? than to ?meet?. Using
this information, it is possible to derive more effec-
tive rules from training examples.
There are several solutions for taking this infor-
mation into account, e.g. by using FrameNet se-
mantics (e.g., like in (Burchardt et al, 2007)), it is
possible to encode a lexical-syntactic rule using the
KILLING and the DEATH frames, i.e.:
?7 = KILLING(Killer :
X ,
V ictim : Y ) ?
DEATH(
Protagonist : Y )
However, to use this model, specific rules and a
semantic role labeler on the specific corpora are
needed.
3 Lexical similarities
Previous research in computational linguistics has
produced many effective lexical similarity mea-
sures based on many different resources or corpora.
For example, WordNet similarities (Pedersen et al,
2004) or Latent Semantic Analysis over a large cor-
pus are widely used in many applications and for
1021
the definition of kernel functions, e.g. (Basili et al,
2006; Basili et al, 2005; Bloehdorn et al, 2006).
In this section we present the main component of
our new kernel, i.e. a lexical similarity derived from
different resources. This is used inside the syntac-
tic/semantic tree kernel defined in (Bloehdorn and
Moschitti, 2007a; Bloehdorn and Moschitti, 2007b)
to enhance the basic tree kernel functions.
3.1 WordNet Similarities
WordNet similarities have been heavily used in pre-
vious NLP work (Chan and Ng, 2005; Agirre et al,
2009). All WordNet similarities apply to pairs of
synonymy sets (synsets) and return a value indicat-
ing their semantic relatedness. For example, the fol-
lowing measures, that we use in this study, are based
on path lengths between concepts in the Wordnet Hi-
erarchy:
Path the measure is equal to the inverse of the
shortest path length (path length) between two
synsets c1 and c2 in WordNet
SimPath =
1
path length(c1, c2)
(1)
WUP the Wu and Palmer (Wu and Palmer, 1994)
similarity metric is based on the depth of two given
synsets c1 and c2 in the WordNet taxonomy, and the
depth of their least common subsumer (lcs). These
are combined into a similarity score:
SimWUP =
2? depth(lcs)
depth(c1) + depth(c2)
(2)
Wordnet similarity measures on synsets can be
extended to similarity measures between words as
follows:
?S(w1, w2) = max(c1,c2)?C1?C2SimS(c1, c2)
(3)
where S is Path or WUP and Ci is the set of the
synsets related to the word wi.
3.2 Distributional Semantic Similarity
Latent Semantic Analysis (LSA) is one of the
corpus-based measure of distributional semantic
similarity, proposed by (Landauer et al, 1998).
Words ~wi are represented in a document space. Each
feauture is a document and its value is the frequency
of the word in the document. The similarity is gen-
erally computed as a cosine similarity:
?LSI(w1, w2) =
~w1 ~w2
|| ~w1|| ? || ~w2||
(4)
In our approach we define a proximity matrix P
where pi,j represents ?LSI(wi, wj) The core of our
approach lies on LSI (Latent Semantic Indexing)
over a large corpus. We used singular value de-
composition (SVD) to build the proximity matrix
P = DDT from a large corpus, represented by its
word-by-document matrix D.
SVD decomposes D (weighted matrix of term
frequencies in a collection of text) into three matri-
ces U?V T , where U (matrix of term vectors) and
V (matrix of document vectors) are orthogonal ma-
trices whose columns are the eigenvectors of DDT
and DTD respectively, and ? is the diagonal matrix
containing the singular value of D.
Given such decomposition, P can be obtained as
Uk?2kUTk , where Uk is the matrix containing the first
k columns of U and k is the dimensionality of the
latent semantic space. This is efficiently used to re-
duce the memory requirements while retaining the
information. Finally we computed the term simi-
larity using the cosine measure in the vector space
model (VSM).
Generally, LSA can be observed as a way to over-
come some of the drawbacks of the standard vector
space model, such as sparseness and dimensionality.
In other words, the LSA similarity is computed in
a lower dimensional space, in which second-order
relations among words and documents are exploited
(Mihalcea et al, 2006).
It is worth mentioning that the LSA similarity
measure depends on the selected corpus but it ben-
efits from a higher computation speed in compari-
son to the construction of the similarity matrix based
on the WordNet Similarity package (Pedersen et al,
2004).
4 Lexical similarity in Syntactic Tree
Kernels
Section 2 has shown that the role of the syntax is im-
portant in extracting generalized rules for RTE but it
is not enough. Therefore, the lexical similarity de-
scribed in the previous section should be taken into
1022
SPP
IN
CD
1963
NP
NNP
Lee
NNP
Harvey
NNP
Oswald
VP
VBN
murdered
NNP
JFK
?
S
PP
IN
CD
NP VP
VBN
murdered
NNP
S
PP
IN
CD
NP VP
VBN NNP
S
PP
IN
NP VP
VBN NNP
VP
VBN
murdered
NNP
JFK
VP
VBN
murdered
NNP
VP
VBN NNP
JFK
VP
VBN
killed
NNP
Kennedy
Figure 1: A syntactic parse tree (on the left) along with some of its fragments. After the bar there is an important
fragment from a semantically similar sentence, which cannot be matched by STK but it is matched by SSTK.
account in the model definition. Since tree kernels
have been shown to be very effective for exploit-
ing syntactic information in natural language tasks, a
promising idea is to merge together the two different
approaches, i.e. tree kernels and semantic similari-
ties.
4.1 Syntactic Tree Kernel (STK)
Tree kernels compute the number of common sub-
structures between two trees T1 and T2 without ex-
plicitly considering the whole fragment space. The
standard definition of the STK, given in (Collins and
Duffy, 2002), allows for any set of nodes linked by
one or more entire production rules to be valid sub-
structures. The formal characterization is given in
(Collins and Duffy, 2002) and is reported hereafter:
Let F = {f1, f2, . . . , f|F|} be the set of tree
fragments and ?i(n) be an indicator function,
equal to 1 if the target fi is rooted at node n
and equal to 0 otherwise. A tree kernel func-
tion over T1 and T2 is defined as TK(T1, T2) =
?
n1?NT1
?
n2?NT2
?(n1, n2), where NT1 and NT2
are the sets of nodes in T1 and T2, respectively and
?(n1, n2) =
?|F|
i=1 ?i(n1)?i(n2).
? function counts the number of subtrees rooted
in n1 and n2 and can be evaluated as follows:
1. if the productions at n1 and n2 are different
then ?(n1, n2) = 0;
2. if the productions at n1 and n2 are the same,
and n1 and n2 have only leaf children (i.e. they
are pre-terminal symbols) then ?(n1, n2) = ?;
3. if the productions at n1 and n2 are the same,
and n1 and n2 are not pre-terminals then
?(n1, n2) = ?
?l(n1)
j=1 (1 + ?(cn1(j), cn2(j))),
where l(n1) is the number of children of n1,
cn(j) is the j-th child of node n and ? is a de-
cay factor penalizing larger structures.
Figure 1 shows some fragments (out of the over-
all 472) of the syntactic parse tree on the left, which
is derived from the text T4. These fragments sat-
isfy the constraint that grammatical rules cannot be
broken. For example, (VP (VBN (murdered) NNP
(JFK))) is a valid fragment whereas (VP (VBN (mur-
dered)) is not. One drawback of such kernel is that
two sentences expressing similar semantics but with
different lexicals produce structures which will not
be matched. For example, after the vertical bar
there is a fragment, extracted from the parse tree
of a semantically identical sentences: In 1963
Oswald killed Kennedy. In this case, much
less matches will be counted by the kernel function
applied to such parse trees and the one of T4. In par-
ticular, the complete VP subtree will not be matched.
To tackle this problem the Syntactic Semantic
Tree Kernel (SSTK) was defined in (Bloehdorn and
Moschitti, 2007a); hereafter, we report its definition.
4.2 Syntactic Semantic Tree kernels (SSTK)
An SSTK produces all the matches of STK. More-
over, the fragments, which are identical but for their
lexical nodes, produce a match proportional to the
product of the similarity between their correspond-
ing words. This is a sound definition. Indeed, since
the structures are the same, each word in position i
of the first fragment can be associated with a word
located in the same position i of the second frag-
ment. More formally, the fast evaluation of ? for
STK can be used for computing the semantic ? for
SSTK by simply adding the following step
0. if n1 and n2 are pre-terminals and label(n1) =
label(n2) then ?(n1, n2) = ??S(ch1n1 , ch1n2),
where label(ni) is the label of node ni and ?S is
a term similarity kernel, e.g. based on Wikipedia,
Wordnet or BNC, defined in Section 3. Note that:
(a) since n1 and n2 are pre-terminals of a parse tree
1023
they can have only one child (i.e. ch1n1 and ch1n2 )
and such children are words and (b) Step 2 of the
original ? evaluation is no longer necessary.
For example, the fragments: (VP (VBN (murdered)
NNP (JFK))) has a match with (VP (VBN (killed)
NNP (Kennedy))) equal to ?S(murdered, kill) ?
?S(JFK,Kennedy).
Beside the novelty of taking into account tree
fragments that are not identical it should be noted
that the lexical semantic similarity is constrained
in syntactic structures, which limit errors/noise due
to incorrect (or, as in our case, not provided) word
sense disambiguation.
Finally, it should be noted that when a valid ker-
nel is used in place of ?S , SSTK is a valid kernel for
definition of convolution kernels (Haussler, 1999).
Since the matrix P derived by applying LSA pro-
duces a semi-definite matrix (see (Cristianini and
Holloway, 2001)) we can always use the similarity
matrix derived by LSA in SSTK. In case of Wordnet,
the validity of the kernel will depend of the kind of
similarity used. In our experiments, we have carried
out single value decomposition and we have verified
that our Wordenet matrices, Path and WUP, are in-
deed positive semi-definite.
5 Kernels for Textual Entailment
Recognition
In this section, we describe how we use the syntac-
tic tree kernel (STK) and the semantic/syntactic tree
kernel (SSTK) for modeling lexical-syntactic ker-
nels for textual entailment recognition. We build
on the kernel described in (Zanzotto and Moschitti,
2006; Zanzotto et al, 2009) that can model lexical-
syntactic rules with variables (i.e., first-order rules).
5.1 Anchoring and pruning
Kernels for modeling lexical-syntactic rules with
variables presuppose that words in texts T are ex-
plicitly related to words in hypotheses H . This cor-
relation is generally called anchoring and it is imple-
mented with placeholders that co-index the syntactic
trees derived from T and H . Words and intermediate
nodes are co-indexed when they are equal or similar.
For example, in the pair:
T8 ??H8
T8 ?Lee Harvey Oswald was born in
New Orleans, Louisiana, and was
of English, German, French and
Irish ancestry. In 1963 1 Oswald
murdered JFK 2 ?
H8 ?JFK 2 died in 1963 1 ?
Moreover, the set of anchors also allows us to
prune fragments of the text T that are irrelevant
for the final decision: we can discard sentences
or phrases uncovered by placeholders. For exam-
ple, in the pair ?T8,H8?, we can infer that ?Lee
H. . . ancestry? is not a relevant fragment and remove
it. This allows us to focus on the critical part for de-
termining the entailment value.
5.2 Kernels for capturing lexical-syntactic
rules
Once placeholders are available in the entailment
pairs, we can apply the model proposed in (Zan-
zotto et al, 2009). This derives the maximal simi-
larity between pairs of T and H based on the lexico-
syntactic information encoded by the syntactic parse
trees of T and H enriched with placeholders. More
formally, the original kernel is based on the follow-
ing equation:
maxSTK(?T,H?, ?T ?, H ??) = maxc?C (5)
(STK(t(T, c), t(T ?, i)) + STK(t(H, c), t(H ?, i)),
where: (i) C is the set of all bijective mappings be-
tween the placeholders (i.e., the possible variables)
from ?T,H? into ?T ?,H ??; (ii) c ? C is a substitu-
tion function, which implements such mapping; (iii)
t(?, c) returns the syntactic tree enriched with place-
holders replaced by means of the substitution c; and
(iv) STK(?1, ?2) is a tree kernel function.
The new semantic-syntactic kernel for lexical-
syntactic rules, maxSSTK, substitutes STK with
SSTK in Eq. 5 thus enlarging the coverage of the
matching between the pairs of texts and the pairs of
hypotheses.
6 Experiments
The aim of the experiments is to investigate if our
RTE system exploiting syntactic semantic kernels
(SSTK) can effectively derive generalized lexico-
syntactic rules. In more detail, first, we determine
the best lexical similarity suitable for the task, i.e.
1024
No Semantic Wiki BNC Path WUP
RTE2 j = 1 63.12 63.5 62.75 62.88 63.88
j = 0.9 63.38 64.75 62.26 63.88 64.25
RTE3 j = 1 66.88 67.25 67.25 66.88 66.5
j = 0.9 67.25 67.75 67.5 67.12 67.38
RTE5 j = 1 65.5 66.5 65.83 66 66
j = 0.9 65.5 66.83 65.67 66 66.33
Table 1: Accuracy of plain (WOK+STK+maxSTK) and Semantic Lexico-Syntactic (WOK+SSTK+maxSSTK) Ker-
nels. The latter according to different similarities
distributional vs. Wordnet-based approaches. Sec-
ond, we derive qualitative and quantitative proper-
ties, which justify the selection of one with respect
to the other.
For this purpose, we tested four different version
of SSTK, i.e. using Path, WUP, BNC and WIKI
lexical similarities on three different RTE datasets.
These correspond to the three different challenges in
which the development set was provided.
6.1 Experimental Setup
We used the data from three recognizing textual en-
tailment challenge: RTE2 (Bar-Haim et al, 2006),
RTE3 (Giampiccolo et al, 2007), and RTE5, along
with the standard split between training and test sets.
We did not use RTE1 as it was differently built from
the others and RTE4 as it does not contain the devel-
opment set.
We used the following publicly available tools:
the Charniak Parser (Charniak, 2000) for pars-
ing sentences and SVM-light-TK (Moschitti, 2006;
Joachims, 1999), in which we coded our new kernels
for RTE. Additionally, we used the Jiang&Conrath
(J&C) distance (Jiang and Conrath, 1997) com-
puted with wn::similarity package (Pedersen
et al, 2004) to measure the similarity between T and
H . This similarity is also used to define the text-
hypothesis word overlap kernel (WOK).
The distributional semantics is captured by means
of LSA: we used the java Latent Semantic Indexing
(jLSI) tool (Giuliano, 2007). In particular, we pre-
computed the word-pair matrices for RTE2, RTE3,
and RTE5. We built different LSA matrices from
the British National Corpus (BNC) and Wikipedia
(Wiki). The British National Corpus (BNC) is a bal-
anced synchronic text corpus containing 100 mil-
lion words with morpho-syntactic annotation. For
Wikipedia, we created a model from the 200,000
most visited Wikipedia articles, after cleaning the
unnecessary markup tags. Articles are our doc-
uments for creating the term-by-document matrix.
Wikipedia provides the largest coverage knowledge
resource developed by a community, besides the no-
ticeable coverage of named entities. This further
motivates the design of a similarity measure. We
also consider two typical WordNet similarities (i.e.,
Path and WUP, respectively) as described in Sec.
3.1.
The main RTE model that we consider is consti-
tuted by three main kernels:
? WOK, i.e. the kernel based on only the text-
hypothesis lexical overlapping words (this is an
intra-pair similarity);
? STK, i.e. the sum of the standard tree kernel
(see Section 4.1) applied to the two text parse-
trees and the two hypothesis parse trees;
? SSTK, i.e. the same as STK with the use of
lexical similarities as explained in Section 4.2;
? maxSTK and maxSSTK, i.e. the kernel for
RTE, illustrated in Section 5.2, where the lat-
ter exploits similarity since it uses SSTK in Eq.
5.
Note that the model presented in (Zanzotto et al,
2009), our baseline, corresponds to the combination
kernel: WOK+maxSTK. In this paper, in addition to
the role of lexical similarities, we also study several
combinations (we just need to sum the separated ker-
nels), i.e. WOK+STK+maxSTK, SSTK+maxSSTK,
WOK+SSTK+maxSSTK and WOK+maxSSTK.
Finally, we measure the performance of our sys-
tem with the standard accuracy and then we deter-
mine the statistical significance by using the model
1025
STK SSTK maxSTK maxSSTK STK+maxSTK SSTK+maxSSTK ?
RTE2 +WOK 61.5 61.12 63.88 64.12 63.12 63.50 60.62
52.62 52.75 61.25 59.38 61.25 58.75 -
RTE3 +WOK 66.38 66.5 66.5 67.0 66.88 67.25 66.75
53.25 54.5 62.25 64.38 63.12 63.62 -
RTE5 +WOK 62.0 62.0 64.83 64.83 65.5 66.5 60.67
54.33 57.33 63.33 62.67 61.83 62.67 -
Table 2: Comparing different lexico-syntactic kernels with Wiki-based semantic kernels
described in (Yeh, 2000) and implemented in (Pado?,
2006).
6.2 Distributional vs. WordNet-based
Semantics
The first experiment compares the basic kernel, i.e.
WOK+STK+maxSTK, with the new semantic ker-
nel, i.e. WOK+SSTK+maxSSTK, where SSTK
and maxSSTK encode four different kinds of sim-
ilarities, BNC, WIKI, WUP and Path. The aim
is twofold: understanding if semantic similarities
can be effectively used to derive generalized lexico-
syntactic rules and to determine the best similarity
model.
Table 1 shows the results according to No Seman-
tics, Wiki, BNC, Path and WUP. The three pairs of
rows represent the results over the three different
datasets, i.e., RTE2, RTE3, and RTE5. For each
pair, we have two rows representing a different j
parameter of SVM. An increase of j augments the
weight of positive with respect to negative examples
and during learning it tunes-up the Recall/Precision
rate. We use two values j = 1 (the default value)
and j = 0.9 (selected during a preliminary experi-
ment on a validation set on RTE2). j = 0.9 was used
to minimally increase the Precision, considering that
the semantic model tends to improve the Recall.
The results show that:
? WIKI semantics constantly improves the basic
kernel (no Semantics) for any datasets or pa-
rameter.
? The distributional semantics is almost always
better than the WordNet-based one.
? In one case WUP improves WIKI, i.e. 63.88 vs
63.5 and in another case BNC reaches WIKI,
i.e. 67.25 but this happens for the default values
of the j parameters, i.e. j = 1, which was not
selected by our limited parameter validation.
Finally, the difference between the accuracy of the
best WIKI kernels and the No Semantic kernels are
statistically significant (p << 0.05).
6.3 Kernel Comparisons
The previous experiments (Sec. 6.2) show that
Wikipedia-based distributional semantics provides
an effective similarity to generalize lexico-syntactic
rules (features). As our RTE kernel is a composition
of other basic kernels, we experimented with dif-
ferent combinations to understand the role of each
component. Moreover, to obtain results independent
of parameterization we used the default parameter j.
Table 2 reports the accuracy of different kernels
and their combinations on different RTE datasets.
Each row describes the results for each dataset and
it is split in two according to the use of WOK or not
in the RTE model. In the each column, the different
kernels are reported. For example, the entry in the
4th column and the 2nd row refers to the accuracy of
SSTK in combination with WOK, i.e. WOK+SSTK
for the RTE2.
We observe that: first WOK produces a very high
accuracy in RTE challenges, i.e. 60.62, 66.75 and
60.67 and it is an essential component of RTE sys-
tems since its ablation always causes a large accu-
racy decrease. This is reasonable as the major source
of information to establish entailment between sen-
tences is their word overlap.
Second, STK and SSTK, when added to WOK,
improve it on RTE2 and RTE5 but do not improve
it on RTE3. This suggests a difficulty of exploiting
syntactic information for RTE3.
Third, maxSTK+WOK relevantly improves
WOK on RTE2 and RTE5 but fails in RTE3. Again,
the syntactic rules (with variables) which this kernel
1026
BNC WN WIKI
RTE2 0.55 0.42 0.83
RTE3 0.54 0.41 0.83
RTE5 0.45 0.34 0.82
Table 3: Coverage of the different resources for the words
of the three datasets
can provide are not enough general for RTE3. In
contrast, maxSSTK+WOK improves WOK on all
datasets thanks to its generalization ability.
Finally, STK and SSTK added to maxSTK+WOK
or to maxSSTK+WOK tend to produce an accuracy
increase, although not in every condition.
7 Discussion
7.1 Coverage and efficiency
As already mentioned, the practical use of
Wikipedia to design lexical similarities is motivated
by a large coverage. Deriving similarities from other
resources such as WordNet is more time-consuming.
To prove our claim, we performed an analysis on the
coverage and efficiency in computing the pair term
similarity.
Table 3 shows the coverage of the content words
of the three datasets. The coverage of Wikipedia is
about two times more than the other resources in all
experimented datasets.
Speed Milliseconds
LSA 0.54
WN with POS 5.3
WN without POS 15.2
Table 4: The comparison in terms of speed calculated
over 10000 pairs after loading the model.
Moreover, Table 4 shows that the computation
of the LSA matrix on Wikipedia is faster than us-
ing the WordNet similarity software (Pedersen et al,
2004). Even if the accuracy of some WordNet mod-
els can reach the one based on Wikipedia, the latter
is preferable for the smaller computational cost.
7.2 Comparison with previous work
The results of our models show that lexical se-
mantics for building more effective lexical-syntactic
rules is promising. Here, we compare our ap-
proaches with other RTE systems to show that our
Average Acc. Our rank # participants
RTE2 59.8 3rd 23
RTE3 64.5 4th 26
RTE5 61.5 4th 20
Table 5: Comparison with other approaches to RTE
results are indeed state-of-the-art. Unfortunately,
deriving a reasonable accuracy value to represent the
state-of-the-art is extremely difficult as many fac-
tors can determine the final score. For example, the
best systems in RTE2 and RTE3 (Giampiccolo et al,
2007) have an accuracy 10% higher than the others
but they generally use resources that are not publicly
available.
Table 5 shows the average accuracy of the partici-
pant systems, the rank of our system that we propose
in this paper and the number of participants. Our
model accuracy is absolutely above the average and
it is ranked at the top positions. We can also carry
out a finer comparison with respect to RTE2 (Bar-
Haim et al, 2006). Our system results are the best
when compared with systems using semantic mod-
els based on FrameNet, indeed the best ranked sys-
tem in this class, i.e., (Burchardt et al, 2007), scores
only 62.5. Among systems using logical inference,
our model is instead the 3rd out of 8 systems using
logical inference that perform worse than ours. Fi-
nally, it is the 2nd among systems using supervised
machine learning models.
8 Conclusion
In this paper we presented a model to effectively in-
clude semantics in lexical-syntactic features for tex-
tual entailment recognition. We have experimentally
shown that LSA-derived lexical semantics embed-
ded in syntactic structures is a promising approach.
The model that we have presented is one of the
best system in the RTE challenges. Additionally, in
contrast to many other methods it does not require
large sets of handcrafted or corpus extracted lexical-
syntactic rules.
Acknowledgements
The research of Alessandro Moschitti has been par-
tially supported by Trustworthy Eternal Systems via
Evolving Software, Data and Knowledge (EternalS,
project number FP7 247758).
1027
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca,
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and wordnet-based ap-
proaches. In NAACL ?09: Proc. HLT/NAACL.
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Giampic-
colo, and I. Magnini, B. Szpektor. 2006. The ii
PASCAL recognising textual entailment challenge. In
Proc. of the II PASCAL Challenges Workshop.
R. Bar-Haim, I. Dagan, I. Greental, and E. Shnarch.
2007. Semantic inference at the lexical-syntactic level.
In AAAI?07: Proc. of the 22nd national conference on
Artificial intelligence.
R. Bar-Haim, J. Berant, and I. Dagan. 2009. A com-
pact forest for scalable inference over entailment and
paraphrase rules. In Proc. of EMNLP.
R. Basili, M. Cammisa, and A. Moschitti. 2005. Effec-
tive use of wordnet semantics via kernel-based learn-
ing. In CoNLL.
R. Basili, M. Cammisa, and A. Moschitti. 2006. A se-
mantic kernel to classify texts with very few training
examples. In Informatica.
S. Bloehdorn and A. Moschitti. 2007a. Combined syn-
tactic and semantic kernels for text classification. In
ECIR.
S. Bloehdorn and A. Moschitti. 2007b. Structure and se-
mantics for expressive text kernels. In Proc. of CIKM
?07.
S. Bloehdorn, R. Basili, M. Cammisa, and A. Moschitti.
2006. Semantic kernels for text classification based on
topological measures of feature similarity. In Proc. of
ICDM 06, Hong Kong, 2006.
J. Bos and K. Markert. 2005. Recognising textual entail-
ment with logical inference. In HLT ?05: Proc. of the
conference on HLT and EMNLP.
A. Burchardt, N. Reiter, S. Thater, and A. Frank. 2007.
Semantic Approach to Textual Entailment: System
Evaluation and Task Analysis. In Proc. of the 3rd-
PASCAL Workshop on Textual Entailment, Prague.
Y. S. Chan and H. T. Ng. 2005. Word sense disambigua-
tion with distribution estimation. In Proc. of IJCAI?05.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the 1st NAACL conference.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: kernels over discrete struc-
tures, and the voted perceptron. In Proc. of ACL ?02.
N. Cristianini and R. Holloway. 2001. Latent semantic
kernels.
G. Dinu and R. Wang. 2009. Inference rules and their
application to recognizing textual entailment. In Proc.
of the EACL ?09.
D. Giampiccolo, B. Magnini, Ido Dagan, and B. Dolan.
2007. The third pascal recognizing textual entailment
challenge. In Proc. of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing.
Claudio Giuliano. 2007. jLSI a for latent se-
mantic indexing. http://tcc.itc.it/research/textec/tools-
resources/jLSI.html.
A. D. Haghighi, A. Y. Ng, and C. D. Manning. 2005.
Robust textual inference via graph matching. In HLT
?05: Proc. of the conference on HLT and EMNLP.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report.
J. J. Jiang and D. W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proc. of the 10th ROCLING.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical.
Landauer, Foltz, and Laham. 1998. Introduction to latent
semantic analysis. In Discourse Processes 25.
D. Lin and P. Pantel. 2001. DIRT-discovery of inference
rules from text. In Proc. of the ACM KDD-01.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of text
semantic similarity. In Proc. of AAAI06.
Alessandro Moschitti. 2006. Making tree kernels practi-
cal for natural language learning. In Proc. of EACL.
Peter Norvig. 1987. A unified theory of inference for
text understanding. Technical report, USA.
Sebastian Pado?, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity - measuring the relatedness of
concepts. In Proc. of 5th NAACL.
D. Roth and M. Sammons. 2007. Semantic and logi-
cal inference model for textual entailment. In Proc.
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
I. Szpektor and I. Dagan. 2008. Learning entailment
rules for unary templates. In Proc. of COLING ?08.
M. Tatu and D. Moldovan. 2005. A semantic approach
to recognizing textual entailment. In HLT ?05: Proc.
of HLT/EMNLP.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical
selection. In Proc. of ACL.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proc. of ACL
2000, Morristown, NJ, USA.
F. M. Zanzotto and A. Moschitti. 2006. Automatic learn-
ing of textual entailments with cross-pair similarities.
In Proc. of ACL ?06.
F. M. Zanzotto, M. Pennacchiotti, and A. Moschitti.
2009. A machine learning approach to textual en-
tailment recognition. NATURAL LANGUAGE ENGI-
NEERING.
1028
Proceedings of NAACL-HLT 2013, pages 179?189,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Towards Topic Labeling with Phrase Entailment and Aggregation
Yashar Mehdad Giuseppe Carenini Raymond T. Ng Shafiq Joty
Department of Computer Science, University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
{mehdad, carenini, rng, rjoty}@cs.ubc.ca
Abstract
We propose a novel framework for topic la-
beling that assigns the most representative
phrases for a given set of sentences cover-
ing the same topic. We build an entailment
graph over phrases that are extracted from the
sentences, and use the entailment relations to
identify and select the most relevant phrases.
We then aggregate those selected phrases by
means of phrase generalization and merging.
We motivate our approach by applying over
conversational data, and show that our frame-
work improves performance significantly over
baseline algorithms.
1 Introduction
Given text segments about the same topic written in
different ways (i.e., language variability), topic la-
beling deals with the problem of automatically gen-
erating semantically meaningful labels for those text
segments. The potential of integrating topic label-
ing as a prerequisite for higher-level analysis has
been reported in several areas, such as summariza-
tion (Harabagiu and Lacatusu, 2010; Kleinbauer et
al., 2007; Dias et al, 2007), information extraction
(Allan, 2002) and conversation visualization (Liu
et al, 2012). Moreover, the huge amount of tex-
tual data generated everyday specifically in conver-
sations (e.g., emails and blogs) calls for automated
methods to analyze and re-organize them into mean-
ingful coherent clusters.
Table 1 shows an example of two human written
topic labels for a topic cluster collected from a blog1,
1http://slashdot.org
Text: a: Where do you think the term ?Horse laugh? comes
from?
b: And that rats also giggled when tickled.
c: My hypothesis- if an animal can play, it can ?laugh? or at
least it is familiar with the concept of ?laughing?.
Many animals play. There are various sorts of humour though.
Some involve you laughing because your brain suddenly made
a lots of unexpected connections.
Possible extracted phrases: animals play, rats have, laugh,
Horse laugh, rats also giggle, rats
Human-authored topic labels: animals which laugh, animal
laughter
Table 1: Topic labeling example.
and possible phrases that can be extracted from the
topic cluster using different approaches. This ex-
ample demonstrates that although most approaches
(Mei et al, 2007; Lau et al, 2011; Branavan et al,
2007) advocate extracting phrase-level topic labels
from the text segments, topically related text seg-
ments do not always contain one keyword or key
phrase that can capture the meaning of the topic. As
shown in this example, such labels do not exist in the
original text and cannot be extracted using the exist-
ing probabilistic models (e.g., (Mei et al, 2007)).
The same problem can be observed with many other
examples. This suggests the idea of aggregating and
generating topic labels, instead of simply extracting
them, as a challenging scenario for this field of re-
search.
Moreover, to generate a label for a topic we have
to be able to capture the overall meaning of a topic.
However, most current methods disregard semantic
relations, in favor of statistical models of word dis-
tributions and frequencies. This calls for the integra-
179
tion of semantic models for topic labeling.
Towards the solution of the mentioned problems,
in this paper we focus on two novel contributions:
1. Phrase aggregation. We propose to generate
topic labels using the extracted information by pro-
ducing the most representative phrases for each text
segment. We perform this task in two steps. First,
we generalize some lexically diverse concepts in
the extracted phrases. Second, we aggregate and
generate new phrases that can semantically imply
more than one original extracted phrase. For ex-
ample, the phrase ?rats also giggle? and ?horse
laugh? should be merged into a new phrase ?animals
laugh?. Although our method is still relying on ex-
tracting phrases, we move beyond current extractive
approaches, by generating new phrases through gen-
eralization and aggregation of the extracted ones.
2. Building a multidirectional entailment graph
over the extracted phrases to identify and select the
relevant information. We set such problem as an
application-oriented variant of the Textual Entail-
ment (TE) recognition task (Dagan and Glickman,
2004), to identify the information that are seman-
tically equivalent, novel, or more informative with
respect to the content of the others. In this way, we
prune the redundant and less informative text por-
tions (e.g., phrases), and produce semantically in-
formed phrases for the generation phase. In the case
of the example in Table 1, we eliminate phrases such
as ?rats have?, ?rats? and ?laugh? while keeping
?animal play?, ?Horse laugh? and ?rats also gig-
gle?.
The experimental results over conversational data
sets show that, in all cases, our approach outper-
forms other models significantly. Although conver-
sational data are known to be challenging (Carenini
et al, 2011), we choose to test our method on con-
versations because this is a genre in which topic
modeling is critically needed, as conversations lack
the structure and organization of, for instance, edited
monologues. The results indicate that our frame-
work is sufficiently robust to deal with topic labeling
in less structured, informal genres (when compared
with edited monologues). As an additional result of
our experiments, we show that the identification and
selection phase using semantic relations (entailment
graph) is a necessary step to perform the final step
(i.e., the phrase aggregation).
2 Topic Labeling Framework
Each topic cluster contains the sentences that can
semantically represent a topic. The task of cluster-
ing the sentences into a set of coherent topic clus-
ters is called topic segmentation (Joty et al, 2011),
which is out of the scope of this paper. Our goal is to
generate an understandable label (i.e., a sequence of
words) that could capture the semantic of the topic,
and distinguish a topic from other topics (based on
definition of a good topic label by (Mei et al, 2007)),
given a set of topic clusters. Among possible choices
of word sequences as topic labels, in order to balance
the granularity, we set phrases as valid topic labels.
Extract all
Filter/select
Entailment
Identify
Phrase extraction Entailment Graph
Generalize
Merge
Phraseaggregation? ? ?- -
1Figure 1: Topic labeling framework.
As shown in Figure 1, our framework consists of
three main components that we describe in more de-
tails in the following sections.
2.1 Phrase extraction
We tokenize and preprocess each cluster in the col-
lection of topic clusters with lemmas, stems, part-of-
speech tags, sense tags and chunks. We also extract
n-grams up to length 5 which do not start or end with
a stop word. In this phase, we do not include any
frequency count feature in our candidate extraction
pipeline. Once we have built the candidates pool,
the next step is to identify a subset containing the
most significant of those candidates. Since most top
systems in key phrase extraction use supervised ap-
proaches, we follow the same method (Kim et al,
2010b; Medelyan et al, 2008; Frank et al, 1999).
Initially, we consider a set of features used in the
other systems to determine whether a phrase is likely
to be a key phrase. However, since our dataset is
conversational (more details in Section 3), and the
text segments are not long, we aim for a classifier
with high recall. Thus, we only use TFxIDF (Salton
and McGill, 1986), position of the first occurrence
(Frank et al, 1999) and phrase length as our fea-
tures. We merge the training and test data released
180
for SemEval-2010 Task #5 (Kim et al, 2010b),
which consists of 244 scientific articles and 3705
key phrases, to train a Naive Bayes classifier in or-
der to learn a supervised model. We then apply our
model to extract the candidate phrases from the col-
lected candidates pool.
As a further step, to increase the coverage (re-
call) of our extracted phrases and to reduce the num-
ber of very short phrases (frequent keywords), we
choose the chunks containing any of the extracted
keywords. We add those chunks to our extracted
phrases and eliminate the associated keywords.
2.2 Entailment graph
So far, we have extracted a pool of key phrases from
each topic cluster. Many such phrases include re-
dundant information which are semantically equiv-
alent but vary in lexical choices. By identifying the
semantic relations between the phrases we can dis-
cover the information in one phrase that is seman-
tically equivalent, novel, or more/less informative
with respect to the content of the other phrase.
We set this problem as a variant of the Textual
Entailment (TE) recognition task (Mehdad et al,
2010b; Adler et al, 2012; Berant et al, 2011). We
build an entailment graph for each topic cluster,
where nodes are the extracted phrases and edges are
the entailment relations between nodes. Given two
phrases (ph1 and ph2), we aim at identifying and
handling the following cases:
i) ph1 and ph2 express the same meaning (bidirec-
tional entailment). In such cases one of the phrases
should be eliminated;
ii) ph1 is more informative than ph2 (unidirectional
entailment). In such cases, the entailing phrase
should replace or complement the entailed one;
iii) ph1 contains facts that are not present in ph2,
and vice-versa (the ?unknown? cases in TE par-
lance). In such cases, both phrases should remain.
Figure 2 shows how entailment relations can help
in selecting the phrases by removing the redun-
dant and less informative ones. For example, the
phrase ?animals laugh? entails ?rats giggle?, ?Horse
laugh? and ?Mice chuckle?,2 but not ?Animals play?.
2Assuming that ?animals laugh? is interpreted as ?all ani-
mals laugh?.
rats
giggle
Horse
laugh
laugh
rats
Mice
chuckle
animals
laugh
Animals
playx
x
1
Figure 2: Building an entailment graph over phrases. Ar-
rows and ?x? represent the entailment direction and un-
known cases respectively.
So we can keep ?animals laugh? and ?Animals play?
and eliminate others. In this way, TE-based phrase
identification method can be designed to distinguish
meaning-preserving variations from true divergence,
regardless of lexical choices and structures.
Similar to previous approaches in TE (e.g., (Be-
rant et al, 2011; Mehdad et al, 2010b; Mehdad et
al., 2010a)), we use supervised method. To train and
build the entailment graph, we perform the follow-
ing three steps.
2.2.1 Training set collection
In the last few years, TE corpora have been cre-
ated and distributed in the framework of several
evaluation campaigns, including the Recognizing
Textual Entailment (RTE) Challenge3 and Cross-
lingual textual entailment for content synchroniza-
tion4 (Negri et al, 2012). However, such datasets
cannot directly support our application. Specifi-
cally, our entailment graph is built over the extracted
phrases (with max. length of 5 tokens per phrase),
while the RTE datasets are composed of longer sen-
tences and paragraphs (Bentivogli et al, 2009; Negri
et al, 2011).
In order to collect a dataset which is more similar
to the goal of our entailment framework, we decide
to select a subset of the sixth and seventh RTE chal-
lenge main task (i.e., RTE within a Corpus). Our
3http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
4http://www.cs.york.ac.uk/semeval-2013/task8/
181
dataset choice is based on the following reasons: i)
the length of sentence pairs in RTE6 and RTE7 is
shorter than the others, and ii) RTE6 and RTE7 main
task datasets are originally created for summariza-
tion purpose which is closer to our work. We sort
the RTE6 and RTE7 dataset pairs based on the sen-
tence length and choose the first 2000 samples with
a equal number of positive and negative examples.
The average length of words in our training data is
6.7 words. There are certainly some differences be-
tween our training set and our phrases. However, the
collected training samples was the closest available
dataset to our purpose.
2.2.2 Feature representation and training
Working at the phrase level imposes another con-
straint. Phrases are short and in terms of syntactic
structure, they are not as rich as sentences. This lim-
its our features to the lexical level. Lexical mod-
els, on the other hand, are less computationally ex-
pensive and easier to implement and often deliver a
strong performance for RTE (Sammons et al, 2011).
Our entailment decision criterion is based on
similarity scores calculated with a phrase-to-phrase
matching process. Each example pair of phrases
(ph1 and ph2) is represented by a feature vector,
where each feature is a specific similarity score esti-
mating whether ph1 entails ph2.
We compute 18 similarity scores for each pair of
phrases. In order to adapt the similarity scores to the
entailment score, we normalize the similarity scores
by the length of ph2 (in terms of lexical items), when
checking the entailment direction from ph1 to ph2.
In this way, we can check the portion of informa-
tion/facts in ph2 which is covered by ph1.
The first 5 scores are computed based on the exact
lexical overlap between the phrases: word overlap,
edit distance, ngram-overlap, longest common sub-
sequence and Lesk (Lesk, 1986). The other scores
were computed using lexical resources: Word-
Net (Fellbaum, 1998), VerbOcean (Chklovski and
Pantel, 2004), paraphrases (Denkowski and Lavie,
2010) and phrase matching (Mehdad et al, 2011).
We used WordNet to compute the word similarity
as the least common subsumer between two words
considering the synonymy-antonymy, hypernymy-
hyponymy, and meronymy relations. Then, we cal-
culated the sentence similarity as the sum of the sim-
ilarity scores of the word pairs in Text and Hypothe-
sis, normalized by the number of words in Hypothe-
sis. We also use phrase matching features described
in (Mehdad et al, 2011) which consists of phrasal
matching at the level on ngrams (1 to 5 grams). The
rationale behind using different entailment features
is that combining various scores will yield a better
model (Berant et al, 2011).
To combine the entailment scores and optimize
their relative weights, we train a Support Vector Ma-
chine binary classifier, SVMlight (Joachims, 1999),
over an equal number of positive and negative exam-
ples. This results in an entailment model with 95%
accuracy over 2-fold and 5-fold cross-validation,
which further proves the effectiveness of our fea-
ture set for this lexical entailment model. The reason
that we gained a very high accuracy is because our
selected sentences are a subset of RTE6 and RTE7
with a shorter length (less number of words) which
makes the entailment recognition task much easier
than recognizing entailment between paragraphs or
complex long sentences.
2.2.3 Graph edge labeling
We set the edge labeling problem as a two-way
classification task. Two-way classification casts
multidirectional entailment as a unidirectional prob-
lem, where each pair is analyzed checking for en-
tailment in both directions (Mehdad et al, 2012). In
this condition, each original test example is correctly
classified if both pairs originated from it are cor-
rectly judged (?YES-YES? for bidirectional,?YES-
NO? and ?NO-YES? for unidirectional entailment
and ?NO-NO? for unknown cases). Two-way clas-
sification represents an intuitive solution to capture
multidimensional entailment relations. Moreover,
since our training examples are labeled with binary
judgments, we are not able to train a three-way clas-
sifier.
2.2.4 Identification and selection
Assigning all entailment relations between the ex-
tracted phrase pairs, we are aiming at identifying
relevant phrases and eliminating the redundant (in
terms of meaning) and less informative ones. In or-
der to perform this task we follow a set of rules based
on the graph edge labels. Note that since entailment
182
# Merging patterns
1 merge ( cw11(CPOS=[N|V |J]) ..w1n , cw21(CPOS=[N|V |J]) ..w2n ) = w11..w1n and w22..w2n
E.g. merge ( challenging situation , challenging problem ) = challenging situation and problem
2 merge ( w11..cw1n(CPOS=[N|V |J]) , w21..cw2n(CPOS=[N|V |J]) ) = w11..w1n?1 and w21..w2n
E.g. merge ( wet Mars , warm Mars ) = wet and warm Mars
3 merge ( w11..cw1n(CPOS=[N|V |J]) , cw21(CPOS=[N|V |J]) ..w2n ) = w11..w1n w22..w2n
E.g. merge ( interesting story , story continues ) = interesting story continues
4 merge ( cw11(CPOS=[N|V |J]) ..w1n , w21..cw2n(CPOS=[N|V |J]) ) = w21..w2n w12..w1n
E.g. merge ( LHC shutting down , details about LHC ) = details about LHC shutting down
5 merge ( w11Cpos , cw12(CPOS=[N|V |J]) , w13Cpos , w21Cpos , cw22(CPOS=[N|V |J]) , w23Cpos ) = w11 and w21 w22 w23
and w13
E.g. merge ( technology grow fast , media grow exponentially ) = technology and media grow exponentially and fast
Table 2: Phrase merging patterns.
is a transitive relation, our entailment graph is transi-
tive i.e., if entail(ph1,ph2) and entail(ph2,ph3) then
entail(ph1,ph3) (Berant et al, 2011).
Rule 1) If there is a chain of entailing nodes, we
keep the one which is in the root of the chain and
eliminate others (e.g. ?animals laugh? in Figure 2);
Rule 2) Among the nodes that are connected
with bidirectional entailment (semantically equiva-
lent nodes) we keep only the one with more outgoing
bidirectional and unidirectional entailment relations,
respectively;
Rule 3) Among the nodes that are connected with
unknown entailment (novel information with respect
to others) we keep the ones with no incoming entail-
ment relation (e.g., ?Animals play? in Figure 2).
Although deleting might be harsh, in our current
framework, we only rely on the performance of an
entailment model which gives us a yes/no entailment
decision. In future, we are planning to improve our
entailment graph by weighting the edges. In this
way, we can take advantage of the weights to make
a more conservative decision in pruning the entail-
ment chains.
2.3 Phrase aggregation
Once we have identified and selected the informa-
tive phrases, the generation of topic labels can be
done in two steps. First, we generalize the phrases
containing the concepts that are lexically connected.
Second, we merge the phrases with a set of hand
written linguistically motivated patterns.
2.3.1 Phrase generalization
In this step, we generalize phrases that contain
concepts which are lexically connected. For this
purpose, we search in phrases for different words
with the same part-of-speech and sense tag. Then,
we find the link between those words in WordNet. If
they are connected and the shortest path connecting
them is less than 3 (estimated over the development
set), we replace both by their common parent in the
WordNet. In the case that they belong to the same
synset, we can replace one by another. Note that we
limit our search to nouns and verbs. For example,
?rat? and ?horse? can be replaced by ?animal?, or
?giggle? and ?chuckle? can be replaced by ?laugh?.
The motivation behind the generalization step is to
enrich the common terms between the phrases in fa-
vor of increasing the chance that they could merge
to a single phrase. This also helps to move beyond
the limitation of original lexical choices.
2.3.2 Phrase merging
The goal is to merge the phrases that are con-
nected, and to generate a human readable phrase that
contains more information than a single extracted
phrase. Several approaches have been proposed to
aggregate and merge sentences in Natural Language
Generation (NLG) (e.g. (Barzilay and Lapata, 2006;
Cheng and Mellish, 2000)), however most of them
use syntactic structure of the sentences. To merge
phrases at the lexical level, we set few common lin-
guistically motivated aggregation patterns such as:
simple conjunction, and conjunction via shared par-
ticipants (Reiter and Dale, 2000).
Table 2 demonstrates the merging patterns, where
wij is the jth word (or segment) in phrase i, cw
is the common word (or segment) in both phrases
and CPOS is the common part-of-speech tag of
the corresponding word. To illustrate, pattern 1
183
looks for the first segment of each phrase (wi1).
If they are same (cwi1) and share the same POS
tag (CPOS), then we aggregate the first phrase
(w11..w1n) and the second phrase removing the first
element (w22..w2n) by using the connective ?and?.
For instance, the aggregation of ?animals laugh? and
?animals play? results in ?animals laugh and play?.
The rest of the patterns follow the same logic and for
the sake of brevity we avoid illustrating each pattern.
These patterns are among the most common domain
and application independent methods by which two
phrases/sentences can be aggregated, as described in
the NLG literature (Reiter and Dale, 2000).
In our aggregation pipeline, we group the phrases
based on their lexical overlap (number of common
words). The merging process is conducted over each
group in descending order (larger number of words
in common), in order to increase the chance of merg-
ing rules application. Then, we perform the merg-
ing over the resulting generated phrases from each
group. If our phrases cannot be merged (i.e., do not
match merging patterns), we select them as labels
for the topic cluster.
3 Datasets and Evaluation Metrics
3.1 Datasets
To verify the effectiveness of our approach, we ex-
periment with two different conversational datasets.
Our interest in dealing with conversational texts de-
rives from two reasons. First, the huge amount of
textual data generated everyday in these conversa-
tions validates the need of text analysis frameworks
to process such conversational texts effectively. Sec-
ond, conversational texts pose challenges to the tra-
ditional techniques, including redundancies, disflu-
encies, higher language variabilities and ill-formed
sentence structure (Liu et al, 2011).
Our conversational datasets are from two differ-
ent asynchronous media: email and blog. For email,
we use the dataset presented in (Joty et al, 2010),
where three individuals annotated the publicly avail-
able BC3 email corpus (Ulrich et al, 2008) with top-
ics. The corpus contains 40 email threads (or conver-
sations) at an average of 5 emails per thread. On av-
erage it has 26.3 sentences and 2.5 topics per thread.
A topic has an average length of 12.6 sentences. In
total, the three annotators found 269 topics in a cor-
pus of 1,024 sentences.
There are no publicly available blog corpora an-
notated with topics. For this study, we build our
own blog corpus containing 20 blog conversations of
various lengths from Slashdot, each annotated with
topics by three human annotators.5 The number of
comments per conversation varies from 30 to 101
with an average of 60.3 and the number of sentences
per conversation varies from 105 to 430 with an av-
erage of 220.6. The annotators first read a conversa-
tion and list the topics discussed in the conversation
by a short description (e.g., Game contents or size,
Bugs or faults) which provides a high-level overview
of the topic. Then, they assign the most appropriate
topic to each sentence in the conversation. The short
high-level descriptions of the topics serve as refer-
ence (or gold) topic labels in our experiments. The
target number of topics was not given in advance and
the annotators were instructed to find as many topics
as needed to convey the overall content structure of
the conversation. The annotators found 5 to 23 top-
ics per conversation with an average of 10.77. The
number of sentences per topic varies from 11.7 to
61.2 with an average of 27.16. In total, the three
annotators found 512 topics in our blog corpus con-
taining 4,411 sentences overall.
Note that our annotators performed topic segmen-
tation and labeling independently. In the email cor-
pus, the three annotators found 100, 77 and 92 top-
ics respectively (269 in total), and in the blog corpus,
they found 251, 119 and 192 topics respectively (562
in total). For the evaluation, there is a single gold
standard per topic written by each annotator. Table
1 shows a case in which two annotators selected the
same topical cluster and so we have two labels for
the same cluster.
3.2 Evaluation metrics
Traditionally, key phrase extraction is evaluated us-
ing precision, recall and f-measure based on exact
matches on all the extracted key phrases with gold
standards for a given text. However, as claimed
by (Kim et al, 2010a), this approach is not flexible
enough as it ignores the near-misses. Moreover, in
the case of topic labeling, most of the human written
5The new blog corpus annotated with topics will be made
publicly available for research purposes.
184
topic labels cannot be found in the text. Recently,
(Kim et al, 2010a) evaluated the utility of differ-
ent n-gram-based metrics for key phrase extraction
and showed that the metric R-precision correlates
most with human judgments. R-precision normal-
izes the approximate matching score by the maxi-
mum number of words in the reference and candi-
date phrases. Since this penalize our aggregation
phase, where the phrases tend to be longer than orig-
inal extracted phrase, we decide to use R-f1 as our
evaluation metric which considers length of both ref-
erence and candidate phrases.
R?precision = 1k
k?
i=1
overlap(candi, ref)
#words(candi)
R?recall = 1k
k?
i=1
overlap(candi, ref)
#words(ref)
R?f1 = 2 ?R?precision ?R?recall(R?precision + R?recall)
The metric described above only considers word
overlap and ignores other semantic relations (e.g.,
synonymy, hypernymy) between words. However,
annotators write labels of their own and may use
words that are not directly from the conversation but
are semantically related. Therefore, we propose to
also use another variant of R-f1 that incorporates se-
mantic relation between words. To calculate the Se-
mantic R-f1, we count the number of overlaps not
only when they have the same form, but also when
they are connected in WordNet with a synonymy,
hypernymy, hyponymy and entailment relation.
Its worth noting that the generalizations phase and
the evaluation method are completely independent.
In the generalization step, we try to generalize the
phrases which are automatically extracted from the
text segments. While, in the evaluation, we compare
the human written gold standards with the system
output. Therefore, using WordNet in the generaliza-
tion step does not bias the results in the evaluation.
4 Experiments and Results
4.1 Experimental settings
We conduct our experiments over the blog and email
datasets described in Section 3.1, after eliminating
the development set from the test datasets. In our ex-
periments, the development set was used for the pat-
tern extraction and the shortest path threshold con-
necting the words in Wordnet in the generalization
phase. Our test dataset consists of 461 topics (i.e.,
clusters and their associated topic labels) from 20
blog conversations and 242 topics from 40 email
conversations.
For preprocessing our dataset we use OpenNLP6
for tokenization, part-of-speech tagging and chunck-
ing. For sense disambiguation, we use the extended
gloss overlap measure with the window size of 5,
developed by (Pedersen et al, 2005). We also apply
Snowball algorithm (Porter, 2001) for stemming.
We compare our approach with two strong base-
lines. The first baseline Freq-BL ranks the words
according to their frequencies and select the top 5
candidates applying Maximum Marginal Relevance
algorithm (Carbonell and Goldstein, 1998) using
the same pre- and post-processing as the work by
(Mihalcea and Tarau, 2004). The second baseline
Lead-BL, ranks the words based on their relevance
to the leading sentences.7 The ranking criteria is
log(tfw,Lt + 1)? log(tfw,t + 1), where tfw,Lt and
tfw,t are the number of times word w appears in a
set of leading sentences Lt and topic cluster t, re-
spectively (Allan, 2002). The log expressions, as the
ranking criterion, assign more weights to the words
in the topic segment, that also appear in the leading
sentences. This is because topics tend to be intro-
duced in the first few sentences of a topical cluster.
We also measure the performance of our framework
at each step in order to compare the effectiveness of
each phase independently or in combination.
4.2 Results
We evaluate the performance of different models us-
ing the metrics R-f1 and Semantic R-f1 (Sem-R-f1),
described in Section 3.2. Table 4 shows the results
in percentage for different models. The results show
that our framework outperforms the baselines signif-
6http://opennlp.sourceforge.net/
7The key intuitions for this baseline is the leading sentences
of a topic cluster carry the most informative clues for the topic
labels. Based on our development set, when we consider the
first three sentences, the coverage of content words that appear
in human labeled topics are 39% and 49% for blog and email,
respectively.
185
Blog Email
Human-authored system generated Human-authored system generated
Shutting down the LHC story about the LHC shutting down (#3) How it affects coding it screws my coding
typical shutdown and upgrade times typical and scheduled shutdown (#2) Opinions and preferences of tools opinion about what tools
MARS was warm and wet 3B years ago Mars was warm and wet early history (#3) white on black for disabled users white text on black background (#3)
Moon Treaty and outer space treaty Moon and Outer Space Treaty (#2) Contact with Steven email to Steven Pemberton (#3)
Table 3: Successful examples of human-authored and system generated labels for blog and email datasets. The number
near some examples refers to the aggregation patterns in Table 2.
Models
R-f1 Sem-R-f1
blog email blog email
Lead-BL 13.5 14.0 34.5 30.1
Freq-BL 15.3 13.1 34.7 29.1
Extraction-BL 13.9 16.0 31.6 33.2
Entailment 12.2 15.6 30.8 33.3
Extraction+Aggregation 15.1 18.5 35.5 37.6
Extraction+Entailment+
Aggregation 17.9 20.4 38.7 41.6
Table 4: Results for candidate topic labels on blog and
email corpora.
icantly8 in both datasets.
On the blog corpus, our key phrase extraction
method (Extraction-BL) fails to beat the other base-
lines (Lead-BL and Freq-BL) in majority of cases
(except R-f1 for Lead-BL). However, in the email
dataset, it improves the performance over both base-
lines in both evaluation metrics. This might be due
to the shorter topic clusters (in terms of number of
sentences) in email corpus which causes a smaller
number of phrases to be extracted.
We also observe the effectiveness of the aggre-
gation phase. In all cases, there is a significant
improvement (p < 0.05) after applying the ag-
gregation phase over the extracted phrases (Extrac-
tion+Aggregation).
Note that there is no improvement over the ex-
traction phase after the entailment (Entailment row).
This is mainly due to the fact that the entailment
phase filters the equivalent phrases. This affects the
results negatively when such filtered phrases share
many common words with our human-authored
phrases. However, the results improve more sig-
nificantly (p < 0.01) when the aggregation is con-
ducted after the entailment. This demonstrates that,
the combination of these two steps are beneficial for
topic labeling over conversational datasets.
In addition, the differences between the results us-
8The statistical significance tests was calculated by approx-
imate randomization described in (Yeh, 2000).
ing R-f1 and Sem-R-f1 metrics suggests the need for
more flexible automatic evaluation methods for this
task. Moreover, although the same trend of improve-
ment is observed in blog and email corpora, the dif-
ferences between their performance suggest the in-
vestigation of specialized methods for various con-
versational modalities.
0 100 200 300 400 500 600
0
0.2
0.4
0.6
0.8
1
Blog
Se
m-
R-
f1
Ext
Ext+Ent
Ext+Agg
Ext+Ent+Agg
1
Figure 3: Sem-R-f1 results distribution after each phase
of our pipeline for blog corpus. The x-axis represents the
examples sorted based on their Sem-R-f1 score.
To further analyze the performance, in Figure 3,
we show the Sem-R-f1 results distribution for our
blog dataset.9 We can observe that the aggrega-
tion after the entailment phase (bold curve) clearly
increase the number of correct labels, while such
improvement can be only achieved when the en-
tailment relations is used to identify the relevant
phrases. This further highlights the need of seman-
tics in this task. Comparing both datasets, this ef-
fect is more dominant in blogs. We believe that this
is due to the length of topic clusters. Presumably,
building an entailment graph over a greater pool of
9For brevity?s sake we do not show the email dataset graph.
186
original phrases is more effective to filter the redun-
dant information and identify the relevant phrases.
5 Discussion
After analyzing the results and through manual veri-
fication of some cases, we observe that our approach
led to some interestingly successful examples. Table
3 shows few generated labels and the human written
topics for such cases.
In general, given that the results are expressed in
percentage, it appears that the performance is still
far from satisfactory level. This leaves an interesting
challenge for the research community to tackle.
However, this is not always due to the weakness
of our proposed model. We have identified three
different system independent sources of error:10
Type 1: Abstractive human-authored labels: the
nature of our method is based on extraction (with
the exception of our simple generalization phase)
and in many cases the human-written labels cannot
be extracted from the text and require more complex
generalizations. In fact, only 9.81% of the labels
in blog and 12.74% of the labels in email appear
verbatim in their respective conversations. For
example:
Human-authored label: meeting schedule and location
Generated phrases: meeting, Boston area, mid October
Type 2: Evaluation methods: in this work, we
proposed a semantic method to evaluate our system.
However, the current evaluation methods fail to
capture the meaning. For example:
Human-authored label: Food choices
Generated phrase: I would ask what people want to eat
Type 3: Subjective topic labels: often is not easy
for human to agree on one label for a topic cluster.11
For example:
Human-authored label 1: Member introduction
Human-authored label 2: Bio of Len
Generated phrases: own intro, Len Kasday, chair
In light of this analysis, we conclude that a more
comprehensive evaluation method (e.g., human eval-
uation) could better reveal the potential of our sys-
10There are many examples of such cases, however for
brevity we just mention one example for each type.
11The mean R-precision agreements computed based on one-
to-one mappings of the topic clusters are 20.22 and 36.84 on
blog and email data sets, respectively.
tem in dealing with topic labeling, specially on con-
versational data.
6 Conclusion
In this paper, we study the problem of automatic
topic labeling, and propose a novel framework to la-
bel topic clusters with meaningful readable phrases.
Within such framework, this paper makes two main
contributions. First, in contrast with most current
methods based on fully extractive models, we pro-
pose to aggregate topic labels by means of gener-
alizing and merging techniques. Second, beyond
current approaches which disregard semantic infor-
mation, we integrate semantics by means of build-
ing textual entailment graphs over the topic clusters.
To achieve our objectives, we successfully applied
our framework over two challenging conversational
datasets. Coherent results on both datasets demon-
strate the potential of our approach in dealing with
topic labeling task.
Future work will address both the improvement of
our aggregation phase and ranking the output candi-
date phrases for each topic cluster. On one hand,
we plan to accommodate more sophisticated NLG
techniques for the aggregation and generation phase.
Incorporating a better source of prior knowledge in
the generalization phase (e.g., YAGO or DBpedia) is
also an interesting research direction towards a bet-
ter phrase aggregation step. On the other hand, we
plan to apply a ranking strategy to select the top can-
didate phrases generated by our framework.
Acknowledgments
We would like to thank the anonymous reviewers
and Frank Tompa for their valuable comments and
suggestions to improve the paper, and the NSERC
Business Intelligence Network for financial support.
Yashar Mehdad also would like to acknowledge the
early discussions on the related topics with Matteo
Negri.
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application to
the health-care domain. In Proceedings of the ACL
2012 System Demonstrations, ACL ?12, pages 79?84,
187
Stroudsburg, PA, USA. Association for Computational
Linguistics.
James Allan. 2002. Topic detection and tracking: event-
based information organization.
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, HLT-NAACL ?06, pages 359?366, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In In
Proc Text Analysis Conference (TAC09.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
SRK Branavan, Pawan Deshpande, and Regina Barzilay.
2007. Generating a table-of-contents. In ACL, vol-
ume 45, page 544.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?98, pages 335?336, New York, NY, USA.
ACM.
Giuseppe Carenini, Gabriel Murray, and Raymond Ng.
2011. Methods for mining and summarizing text con-
versations.
Hua Cheng and Chris Mellish. 2000. Capturing the in-
teraction between aggregation and text planning in two
generation systems. In In Proceedings of the 1st In-
ternational Natural Language Generation Conference,
186193, Mitzpe.
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. In Dekang Lin and Dekai Wu, editors, Proceed-
ings of EMNLP 2004, pages 33?40, Barcelona, Spain,
July. Association for Computational Linguistics.
I. Dagan and O. Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: improved evaluation
support for five target languages. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 339?342,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Gae?l Dias, Elsa Alves, and Jose? Gabriel Pereira Lopes.
2007. Topic segmentation algorithms for text summa-
rization and passage retrieval: an exhaustive evalua-
tion. In Proceedings of the 22nd national conference
on Artificial intelligence - Volume 2, AAAI?07, pages
1334?1339. AAAI Press.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceedings
of the Sixteenth International Joint Conference on Ar-
tificial Intelligence, IJCAI ?99, pages 668?673, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Sanda Harabagiu and Finley Lacatusu. 2010. Us-
ing topic themes for multi-document summarization.
ACM Trans. Inf. Syst., 28(3):13:1?13:47, July.
T. Joachims. 1999. Making large-scale svm learning
practical. LS8-Report 24, Universita?t Dortmund, LS
VIII-Report.
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and
Raymond T. Ng. 2010. Exploiting conversation struc-
ture in unsupervised topic segmentation for emails.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Shafiq Joty, Gabriel Murray, and Raymond T. Ng. 2011.
Supervised topic segmentation of email conversations.
In In ICWSM11. AAAI.
Su Nam Kim, Timothy Baldwin, and Min-Yen Kan.
2010a. Evaluating n-gram based evaluation metrics
for automatic keyphrase extraction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 572?580, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-
thy Baldwin. 2010b. Semeval-2010 task 5: Automatic
keyphrase extraction from scientific articles. In Pro-
ceedings of the 5th International Workshop on Seman-
tic Evaluation, SemEval ?10, pages 21?26, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Thomas Kleinbauer, Stephanie Becker, and Tilman
Becker. 2007. Combining multiple information lay-
ers for the automatic generation of indicative meeting
abstracts. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, ENLG
?07, pages 151?154, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Jey Han Lau, Karl Grieser, David Newman, and Timothy
Baldwin. 2011. Automatic labelling of topic models.
In ACL, pages 1536?1545.
188
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, pages 24?26, New York, NY,
USA. ACM.
F. Liu, Y. Liu, C. Busso, S. Harabagiu, and V. Ng. 2011.
Identifying the Gist of Conversational Text: Automatic
Keyword Extraction and Summarization. Ph.D. thesis,
THE UNIVERSITY OF TEXAS AT DALLAS.
Shixia Liu, Michelle X. Zhou, Shimei Pan, Yangqiu
Song, Weihong Qian, Weijia Cai, and Xiaoxiao Lian.
2012. Tiara: Interactive, topic-based visual text sum-
marization and analysis. ACM Trans. Intell. Syst.
Technol., 3(2):25:1?25:28, February.
O. Medelyan, I.H. Witten, and D. Milne. 2008. Topic
indexing with wikipedia. In Proceedings of the AAAI
WikiAI workshop.
Y. Mehdad, A. Moschitti, and F.M. Zanzotto. 2010a.
Syntactic semantic structures for textual entailment
recognition. Association for Computational Linguis-
tics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010b. Towards cross-lingual textual entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 321?324, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
ACL ?12, pages 120?124, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD inter-
national conference on Knowledge discovery and data
mining, KDD ?07, pages 490?499, New York, NY,
USA. ACM.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of EMNLP-04and
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, July.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: cross-lingual textual entailment
for content synchronization. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics - Volume 1: Proceedings of the main con-
ference and the shared task, and Volume 2: Proceed-
ings of the Sixth International Workshop on Seman-
tic Evaluation, SemEval ?12, pages 399?407, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
T. Pedersen, S. Banerjee, and S. Patwardhan. 2005.
Maximizing Semantic Relatedness to Perform Word
Sense Disambiguation. Research Report UMSI
2005/25, University of Minnesota Supercomputing In-
stitute, March.
Martin F. Porter. 2001. Snowball: A language for stem-
ming algorithms.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems.
Gerard Salton and Michael J. McGill. 1986. Introduction
to modern information retrieval.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan Roth.
2011. Recognizing Textual Entailment. In Daniel M.
Bikel and Imed Zitouni, editors, Multilingual Natu-
ral Language Applications: From Theory to Practice.
Prentice Hall, Jun.
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for su-
pervised email summarization.
Alexander Yeh. 2000. More accurate tests for the statis-
tical significance of result differences. In Proceedings
of the 18th conference on Computational linguistics -
Volume 2, COLING ?00, pages 947?953, Stroudsburg,
PA, USA. Association for Computational Linguistics.
189
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1336?1345,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Using Bilingual Parallel Corpora
for Cross-Lingual Textual Entailment
Yashar Mehdad
FBK - irst and Uni. of Trento
Povo (Trento), Italy
mehdad@fbk.eu
Matteo Negri
FBK - irst
Povo (Trento), Italy
negri@fbk.eu
Marcello Federico
FBK - irst
Povo (Trento), Italy
federico@fbk.eu
Abstract
This paper explores the use of bilingual par-
allel corpora as a source of lexical knowl-
edge for cross-lingual textual entailment. We
claim that, in spite of the inherent difficul-
ties of the task, phrase tables extracted from
parallel data allow to capture both lexical re-
lations between single words, and contextual
information useful for inference. We experi-
ment with a phrasal matching method in or-
der to: i) build a system portable across lan-
guages, and ii) evaluate the contribution of
lexical knowledge in isolation, without inter-
action with other inference mechanisms. Re-
sults achieved on an English-Spanish corpus
obtained from the RTE3 dataset support our
claim, with an overall accuracy above average
scores reported by RTE participants on mono-
lingual data. Finally, we show that using par-
allel corpora to extract paraphrase tables re-
veals their potential also in the monolingual
setting, improving the results achieved with
other sources of lexical knowledge.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
proposed by (Mehdad et al, 2010) as an extension
of Textual Entailment (Dagan and Glickman, 2004)
that consists in deciding, given two texts T and H in
different languages, if the meaning of H can be in-
ferred from the meaning of T. The task is inherently
difficult, as it adds issues related to the multilingual
dimension to the complexity of semantic inference
at the textual level. For instance, the reliance of cur-
rent monolingual TE systems on lexical resources
(e.g. WordNet, VerbOcean, FrameNet) and deep
processing components (e.g. syntactic and semantic
parsers, co-reference resolution tools, temporal ex-
pressions recognizers and normalizers) has to con-
front, at the cross-lingual level, with the limited
availability of lexical/semantic resources covering
multiple languages, the limited coverage of the ex-
isting ones, and the burden of integrating language-
specific components into the same cross-lingual ar-
chitecture.
As a first step to overcome these problems,
(Mehdad et al, 2010) proposes a ?basic solution?,
that brings CLTE back to the monolingual scenario
by translating H into the language of T. Despite the
advantages in terms of modularity and portability of
the architecture, and the promising experimental re-
sults, this approach suffers from one main limitation
which motivates the investigation on alternative so-
lutions. Decoupling machine translation (MT) and
TE, in fact, ties CLTE performance to the availabil-
ity of MT components, and to the quality of the
translations. As a consequence, on one side trans-
lation errors propagate to the TE engine hampering
the entailment decision process. On the other side
such unpredictable errors reduce the possibility to
control the behaviour of the engine, and devise ad-
hoc solutions to specific entailment problems.
This paper investigates the idea, still unexplored,
of a tighter integration of MT and TE algorithms and
techniques. Our aim is to embed cross-lingual pro-
cessing techniques inside the TE recognition pro-
cess in order to avoid any dependency on external
MT components, and eventually gain full control of
the system?s behaviour. Along this direction, we
1336
start from the acquisition and use of lexical knowl-
edge, which represents the basic building block of
any TE system. Using the basic solution proposed
by (Mehdad et al, 2010) as a term of comparison,
we experiment with different sources of multilingual
lexical knowledge to address the following ques-
tions:
(1) What is the potential of the existing mul-
tilingual lexical resources to approach CLTE?
To answer this question we experiment with lex-
ical knowledge extracted from bilingual dictionar-
ies, and from a multilingual lexical database. Such
experiments show two main limitations of these re-
sources, namely: i) their limited coverage, and ii)
the difficulty to capture contextual information when
only associations between single words (or at most
named entities and multiword expressions) are used
to support inference.
(2) Does MT provide useful resources or tech-
niques to overcome the limitations of existing re-
sources? We envisage several directions in which
inputs from MT research may enable or improve
CLTE. As regards the resources, phrase and para-
phrase tables extracted from bilingual parallel cor-
pora can be exploited as an effective way to cap-
ture both lexical relations between single words, and
contextual information useful for inference. As re-
gards the algorithms, statistical models based on co-
occurrence observations, similar to those used inMT
to estimate translation probabilities, may contribute
to estimate entailment probabilities in CLTE. Focus-
ing on the resources direction, the main contribu-
tion of this paper is to show that the lexical knowl-
edge extracted from parallel corpora allows to sig-
nificantly improve the results achieved with other
multilingual resources.
(3) In the cross-lingual scenario, can we achieve
results comparable to those obtained in mono-
lingual TE? Our experiments show that, although
CLTE seems intrinsically more difficult, the results
obtained using phrase and paraphrase tables are bet-
ter than those achieved by average systems on mono-
lingual datasets. We argue that this is due to the
fact that parallel corpora are a rich source of cross-
lingual paraphrases with no equivalents in monolin-
gual TE.
(4) Can parallel corpora be useful also for mono-
lingual TE? To answer this question, we experiment
on monolingual RTE datasets using paraphrase ta-
bles extracted from bilingual parallel corpora. Our
results improve those achieved with the most widely
used resources in monolingual TE, namely Word-
Net, Verbocean, and Wikipedia.
The remainder of this paper is structured as fol-
lows. Section 2 shortly overviews the role of lexical
knowledge in textual entailment, highlighting a gap
between TE and CLTE in terms of available knowl-
edge sources. Sections 3 and 4 address the first three
questions, giving motivations for the use of bilingual
parallel corpora in CLTE, and showing the results of
our experiments. Section 5 addresses the last ques-
tion, reporting on our experiments with paraphrase
tables extracted from phrase tables on the monolin-
gual RTE datasets. Section 6 concludes the paper,
and outlines the directions of our future research.
2 Lexical resources for TE and CLTE
All current approaches to monolingual TE, ei-
ther syntactically oriented (Rus et al, 2005), or
applying logical inference (Tatu and Moldovan,
2005), or adopting transformation-based techniques
(Kouleykov and Magnini, 2005; Bar-Haim et al,
2008), incorporate different types of lexical knowl-
edge to support textual inference. Such information
ranges from i) lexical paraphrases (textual equiva-
lences between terms) to ii) lexical relations pre-
serving entailment between words, and iii) word-
level similarity/relatedness scores. WordNet, the
most widely used resource in TE, provides all the
three types of information. Synonymy relations
can be used to extract lexical paraphrases indicat-
ing that words from the text and the hypothesis en-
tail each other, thus being interchangeable. Hy-
pernymy/hyponymy chains can provide entailment-
preserving relations between concepts, indicating
that a word in the hypothesis can be replaced
by a word from the text. Paths between con-
cepts and glosses can be used to calculate simi-
larity/relatedness scores between single words, that
contribute to the computation of the overall similar-
ity between the text and the hypothesis.
Besides WordNet, the RTE literature documents
the use of a variety of lexical information sources
(Bentivogli et al, 2010; Dagan et al, 2009).
These include, just to mention the most popular
1337
ones, DIRT (Lin and Pantel, 2001), VerbOcean
(Chklovski and Pantel, 2004), FrameNet (Baker et
al., 1998), and Wikipedia (Mehdad et al, 2010;
Kouylekov et al, 2009). DIRT is a collection of sta-
tistically learned inference rules, that is often inte-
grated as a source of lexical paraphrases and entail-
ment rules. VerbOcean is a graph of fine-grained
semantic relations between verbs, which are fre-
quently used as a source of precise entailment rules
between predicates. FrameNet is a knowledge-base
of frames describing prototypical situations, and the
role of the participants they involve. It can be
used as an alternative source of entailment rules,
or to determine the semantic overlap between texts
and hypotheses. Wikipedia is often used to extract
probabilistic entailment rules based word similar-
ity/relatedness scores.
Despite the consensus on the usefulness of lexi-
cal knowledge for textual inference, determining the
actual impact of these resources is not straightfor-
ward, as they always represent one component in
complex architectures that may use them in differ-
ent ways. As emerges from the ablation tests re-
ported in (Bentivogli et al, 2010), even the most
common resources proved to have a positive impact
on some systems and a negative impact on others.
Some previous works (Bannard and Callison-Burch,
2005; Zhao et al, 2009; Kouylekov et al, 2009)
indicate, as main limitations of the mentioned re-
sources, their limited coverage, their low precision,
and the fact that they are mostly suitable to capture
relations mainly between single words.
Addressing CLTE we have to face additional and
more problematic issues related to: i) the stronger
need of lexical knowledge, and ii) the limited avail-
ability of multilingual lexical resources. As regards
the first issue, it?s worth noting that in the monolin-
gual scenario simple ?bag of words? (or ?bag of n-
grams?) approaches are per se sufficient to achieve
results above baseline. In contrast, their applica-
tion in the cross-lingual setting is not a viable so-
lution due to the impossibility to perform direct lex-
ical matches between texts and hypotheses in differ-
ent languages. This situation makes the availability
of multilingual lexical knowledge a necessary con-
dition to bridge the language gap. However, with
the only exceptions represented by WordNet and
Wikipedia, most of the aforementioned resources
are available only for English. Multilingual lexi-
cal databases aligned with the EnglishWordNet (e.g.
MultiWordNet (Pianta et al, 2002)) have been cre-
ated for several languages, with different degrees of
coverage. As an example, the 57,424 synsets of the
Spanish section of MultiWordNet algned to English
cover just around 50% of the WordNet?s synsets,
thus making the coverage issue even more problem-
atic than for TE. As regards Wikipedia, the cross-
lingual links between pages in different languages
offer a possibility to extract lexical knowledge use-
ful for CLTE. However, due to their relatively small
number (especially for some languages), bilingual
lexicons extracted from Wikipedia are still inade-
quate to provide acceptable coverage. In addition,
featuring a bias towards named entities, the infor-
mation acquired through cross-lingual links can at
most complement the lexical knowledge extracted
from more generic multilingual resources (e.g bilin-
gual dictionaries).
3 Using Parallel Corpora for CLTE
Bilingual parallel corpora represent a possible solu-
tion to overcome the inadequacy of the existing re-
sources, and to implement a portable approach for
CLTE. To this aim, we exploit parallel data to: i)
learn alignment criteria between phrasal elements
in different languages, ii) use them to automatically
extract lexical knowledge in the form of phrase ta-
bles, and iii) use the obtained phrase tables to create
monolingual paraphrase tables.
Given a cross-lingual T/H pair (with the text in
l1 and the hypothesis in l2), our approach leverages
the vast amount of lexical knowledge provided by
phrase and paraphrase tables to map H into T. We
perform such mapping with two different methods.
The first method uses a single phrase table to di-
rectly map phrases extracted from the hypothesis to
phrases in the text. In order to improve our system?s
generalization capabilities and increase the cover-
age, the second method combines the phrase table
with two monolingual paraphrase tables (one in l1,
and one in l2). This allows to:
1. use the paraphrase table in l2 to find para-
phrases of phrases extracted from H;
2. map them to entries in the phrase table, and ex-
tract their equivalents in l1;
1338
3. use the paraphrase table in l1 to find para-
phrases of the extracted fragments in l1;
4. map such paraphrases to phrases in T.
With the second method, phrasal matches between
the text and the hypothesis are indirectly performed
through paraphrases of the phrase table entries.
The final entailment decision for a T/H pair is as-
signed considering a model learned from the similar-
ity scores based on the identified phrasal matches.
In particular, ?YES? and ?NO? judgements are as-
signed considering the proportion of words in the
hypothesis that are found also in the text. This way
to approximate entailment reflects the intuition that,
as a directional relation between the text and the hy-
pothesis, the full content of H has to be found in T.
3.1 Extracting Phrase and Paraphrase Tables
Phrase tables (PHT) contain pairs of correspond-
ing phrases in two languages, together with associa-
tion probabilities. They are widely used in MT as a
way to figure out how to translate input in one lan-
guage into output in another language (Koehn et al,
2003). There are several methods to build phrase ta-
bles. The one adopted in this work consists in learn-
ing phrase alignments from a word-aligned bilingual
corpus. In order to build English-Spanish phrase ta-
bles for our experiments, we used the freely avail-
able Europarl V.4, News Commentary and United
Nations Spanish-English parallel corpora released
for the WMT101. We run TreeTagger (Schmid,
1994) for tokenization, and used the Giza++ (Och
and Ney, 2003) to align the tokenized corpora at
the word level. Subsequently, we extracted the bi-
lingual phrase table from the aligned corpora using
the Moses toolkit (Koehn et al, 2007). Since the re-
sulting phrase table was very large, we eliminated
all the entries with identical content in the two lan-
guages, and the ones containing phrases longer than
5 words in one of the two sides. In addition, in or-
der to experiment with different phrase tables pro-
viding different degrees of coverage and precision,
we extracted 7 phrase tables by pruning the initial
one on the direct phrase translation probabilities of
0.01, 0.05, 0.1, 0.2, 0.3, 0.4 and 0.5. The resulting
1http://www.statmt.org/wmt10/
phrase tables range from 76 to 48 million entries,
with an average of 3.9 words per phrase.
Paraphrase tables (PPHT) contain pairs of corre-
sponding phrases in the same language, possibly as-
sociated with probabilities. They proved to be use-
ful in a number of NLP applications such as natural
language generation (Iordanskaja et al, 1991), mul-
tidocument summarization (McKeown et al, 2002),
automatic evaluation of MT (Denkowski and Lavie,
2010), and TE (Dinu and Wang, 2009).
One of the proposed methods to extract para-
phrases relies on a pivot-based approach using
phrase alignments in a bilingual parallel corpus
(Bannard and Callison-Burch, 2005). With this
method, all the different phrases in one language that
are aligned with the same phrase in the other lan-
guage are extracted as paraphrases. After the extrac-
tion, pruning techniques (Snover et al, 2009) can
be applied to increase the precision of the extracted
paraphrases.
In our work we used available2 paraphrase
databases for English and Spanish which have been
extracted using the method previously outlined.
Moreover, in order to experiment with different
paraphrase sets providing different degrees of cov-
erage and precision, we pruned the main paraphrase
table based on the probabilities, associated to its en-
tries, of 0.1, 0.2 and 0.3. The number of phrase pairs
extracted varies from 6 million to about 80000, with
an average of 3.2 words per phrase.
3.2 Phrasal Matching Method
In order to maximize the usage of lexical knowledge,
our entailment decision criterion is based on similar-
ity scores calculated with a phrase-to-phrase match-
ing process.
A phrase in our approach is an n-gram composed
of up to 5 consecutive words, excluding punctua-
tion. Entailment decisions are estimated by com-
bining phrasal matching scores (Scoren) calculated
for each level of n-grams , which is the number
of 1-grams, 2-grams,..., 5-grams extracted from H
that match with n-grams in T. Phrasal matches are
performed either at the level of tokens, lemmas, or
stems, can be of two types:
2http://www.cs.cmu.edu/ alavie/METEOR
1339
1. Exact: in the case that two phrases are identical
at one of the three levels (token, lemma, stem);
2. Lexical: in the case that two different phrases
can be mapped through entries of the resources
used to bridge T and H (i.e. phrase tables, para-
phrases tables, dictionaries or any other source
of lexical knowledge).
For each phrase in H, we first search for exact
matches at the level of token with phrases in T. If
no match is found at a token level, the other levels
(lemma and stem) are attempted. Then, in case of
failure with exact matching, lexical matching is per-
formed at the same three levels. To reduce redun-
dant matches, the lexical matches between pairs of
phrases which have already been identified as exact
matches are not considered.
Once matching for each n-gram level has been
concluded, the number of matches (Mn) and the
number of phrases in the hypothesis (Nn) are used
to estimate the portion of phrases in H that are
matched at each level (n). The phrasal matching
score for each n-gram level is calculated as follows:
Scoren =
Mn
Nn
To combine the phrasal matching scores obtained
at each n-gram level, and optimize their relative
weights, we trained a Support Vector Machine clas-
sifier, SVMlight (Joachims, 1999), using each score
as a feature.
4 Experiments on CLTE
To address the first two questions outlined in Sec-
tion 1, we experimented with the phrase matching
method previously described, contrasting the effec-
tiveness of lexical information extracted from par-
allel corpora with the knowledge provided by other
resources used in the same way.
4.1 Dataset
The dataset used for our experiments is an English-
Spanish entailment corpus obtained from the orig-
inal RTE3 dataset by translating the English hy-
pothesis into Spanish. It consists of 1600 pairs
derived from the RTE3 development and test sets
(800+800). Translations have been generated by
the CrowdFlower3 channel to Amazon Mechanical
Turk4 (MTurk), adopting the methodology proposed
by (Negri and Mehdad, 2010). The method relies
on translation-validation cycles, defined as separate
jobs routed to MTurk?s workforce. Translation jobs
return one Spanish version for each hypothesis. Val-
idation jobs ask multiple workers to check the cor-
rectness of each translation using the original En-
glish sentence as reference. At each cycle, the trans-
lated hypothesis accepted by the majority of trust-
ful validators5 are stored in the CLTE corpus, while
wrong translations are sent back to workers in a
new translation job. Although the quality of the re-
sults is enhanced by the possibility to automatically
weed out untrusted workers using gold units, we per-
formed a manual quality check on a subset of the ac-
quired CLTE corpus. The validation, carried out by
a Spanish native speaker on 100 randomly selected
pairs after two translation-validation cycles, showed
the good quality of the collected material, with only
3 minor ?errors? consisting in controversial but sub-
stantially acceptable translations reflecting regional
Spanish variations.
The T-H pairs in the collected English-Spanish
entailment corpus were annotated using TreeTagger
(Schmid, 1994) and the Snowball stemmer6 with to-
ken, lemma, and stem information.
4.2 Knowledge sources
For comparison with the extracted phrase and para-
phrase tables, we use a large bilingual dictionary
and MultiWordNet as alternative sources of lexical
knowledge.
Bilingual dictionaries (DIC) allow for precise
mappings between words in H and T. To create
a large bilingual English-Spanish dictionary we
processed and combined the following dictionaries
and bilingual resources:
- XDXF Dictionaries7: 22,486 entries.
3http://crowdflower.com/
4https://www.mturk.com/mturk/
5Workers? trustworthiness can be automatically determined
by means of hidden gold units randomly inserted into jobs.
6http://snowball.tartarus.org/
7http://xdxf.revdanica.com/
1340
Figure 1: Accuracy on CLTE by pruning the phrase table
with different thresholds.
- Universal dictionary database8: 9,944 entries.
- Wiktionary database9: 5,866 entries.
- Omegawiki database10: 8,237 entries.
- Wikipedia interlanguage links11: 7,425 entries.
The resulting dictionary features 53,958 entries,
with an average length of 1.2 words.
MultiWordNet (MWN) allows to extract mappings
between English and Spanish words connected by
entailment-preserving semantic relations. The ex-
traction process is dataset-dependent, as it checks
for synonymy and hyponymy relations only between
terms found in the dataset. The resulting collection
of cross-lingual words associations contains 36,794
pairs of lemmas.
4.3 Results and Discussion
Our results are calculated over 800 test pairs of our
CLTE corpus, after training the SVM classifier over
800 development pairs. This section reports the
percentage of correct entailment assignments (accu-
racy), comparing the use of different sources of lex-
ical knowledge.
Initially, in order to find a reasonable trade-off be-
tween precision and coverage, we used the 7 phrase
tables extracted with different pruning thresholds
8http://www.dicts.info/
9http://en.wiktionary.org/
10http://www.omegawiki.org/
11http://www.wikipedia.org/
MWN DIC PHT PPHT Acc. ?
x 55.00 0.00
x 59.88 +4.88
x 62.62 +7.62
x x 62.88 +7.88
Table 1: Accuracy results on CLTE using different lexical
resources.
(see Section 3.1). Figure 1 shows that with the prun-
ing threshold set to 0.05, we obtain the highest re-
sult of 62.62% on the test set. The curve demon-
strates that, although with higher pruning thresholds
we retain more reliable phrase pairs, their smaller
number provides limited coverage leading to lower
results. In contrast, the large coverage obtained with
the pruning threshold set to 0.01 leads to a slight
performance decrease due to probably less precise
phrase pairs.
Once the threshold has been set, in order to
prove the effectiveness of information extracted
from bilingual corpora, we conducted a series of ex-
periments using the different resources mentioned in
Section 4.2.
As it can be observed in Table 1, the highest
results are achieved using the phrase table, both
alone and in combination with paraphrase tables
(62.62% and 62.88% respectively). These results
suggest that, with appropriate pruning thresholds,
the large number and the longer entries contained
in the phrase and paraphrase tables represent an ef-
fective way to: i) obtain high coverage, and ii) cap-
ture cross-lingual associations between multiple lex-
ical elements. This allows to overcome the bias to-
wards single words featured by dictionaries and lex-
ical databases.
As regards the other resources used for compari-
son, the results show that dictionaries substantially
outperform MWN. This can be explained by the
low coverage of MWN, whose entries also repre-
sent weaker semantic relations (preserving entail-
ment, but with a lower probability to be applied)
than the direct translations between terms contained
in the dictionary.
Overall, our results suggest that the lexical knowl-
edge extracted from parallel data can be successfully
used to approach the CLTE task.
1341
Dataset WN VO WIKI PPHT PPHT 0.1 PPHT 0.2 PPHT 0.3 AVG
RTE3 61.88 62.00 61.75 62.88 63.38 63.50 63.00 62.37
RTE5 62.17 61.67 60.00 61.33 62.50 62.67 62.33 61.41
RTE3-G 62.62 61.5 60.5 62.88 63.50 62.00 61.5 -
Table 2: Accuracy results on monolingual RTE using different lexical resources.
5 Using parallel corpora for TE
This section addresses the third and the fourth re-
search questions outlined in Section 1. Building
on the positive results achieved on the cross-lingual
scenario, we investigate the possibility to exploit
bilingual parallel corpora in the traditional monolin-
gual scenario. Using the same approach discussed
in Section 4, we compare the results achieved with
English paraphrase tables with those obtained with
other widely used monolingual knowledge resources
over two RTE datasets.
For the sake of completeness, we report in this
section also the results obtained adopting the ?basic
solution? proposed by (Mehdad et al, 2010). Al-
though it was presented as an approach to CLTE,
the proposed method brings the problem back to the
monolingual case by translating H into the language
of T. The comparison with this method aims at ver-
ifying the real potential of parallel corpora against
the use of a competitive MT system (Google Trans-
late) in the same scenario.
5.1 Dataset
We experiment with the original RTE3 and RTE5
datasets, annotated with token, lemma, and stem in-
formation using the TreeTagger and the Snowball
stemmer.
In addition to confront our method with the solu-
tion proposed by (Mehdad et al, 2010) we translated
the Spanish hypotheses of our CLTE dataset into En-
glish using Google Translate. The resulting dataset
was annotated in the same way.
5.2 Knowledge sources
We compared the results achieved with paraphrase
tables (extracted with different pruning thresh-
olds12) with those obtained using the three most
12We pruned the paraphrase table (PPHT), with probabilities
set to 0.1 (PPHT 0.1), 0.2 (PPHT 0.2), and 0.3 (PPHT 0.3)
widely used English resources for Textual Entail-
ment (Bentivogli et al, 2010), namely:
WordNet (WN). WordNet 3.0 has been used
to extract a set of 5396 pairs of words connected by
the hyponymy and synonymy relations.
VerbOcean (VO). VerbOcean has been used
to extract 18232 pairs of verbs connected by the
?stronger-than? relation (e.g. ?kill? stronger-than
?injure?).
Wikipedia (WIKI). We performed Latent Se-
mantic Analysis (LSA) over Wikipedia using the
jLSI tool (Giuliano, 2007) to measure the relat-
edness between words in the dataset. Then, we
filtered all the pairs with similarity lower than 0.7 as
proposed by (Kouylekov et al, 2009). In this way
we obtained 13760 word pairs.
5.3 Results and Discussion
Table 2 shows the accuracy results calculated over
the original RTE3 and RTE5 test sets, training our
classifier over the corresponding development sets.
The first two rows of the table show that pruned
paraphrase tables always outperform the other lexi-
cal resources used for comparison, with an accuracy
increase up to 3%. In particular, we observe that us-
ing 0.2 as a pruning threshold provides a good trade-
off between coverage and precision, leading to our
best results on both datasets (63.50% for RTE3, and
62.67% for RTE5). It?s worth noting that these re-
sults, compared with the average scores reported by
participants in the two editions of the RTE Challenge
(AVG column), represent an accuracy improvement
of more than 1%. Overall, these results confirm our
claim that increasing the coverage using context sen-
sitive phrase pairs obtained from large parallel cor-
pora, results in better performance not only in CLTE,
1342
but also in the monolingual scenario.
The comparison with the results achieved on
monolingual data obtained by automatically trans-
lating the Spanish hypotheses (RTE3-G row in Ta-
ble 2) leads to four main observations. First, we no-
tice that dealing with MT-derived inputs, the optimal
pruning threshold changes from 0.2 to 0.1, leading
to the highest accuracy of 63.50%. This suggests
that the noise introduced by incorrect translations
can be tackled by increasing the coverage of the
paraphrase table. Second, in line with the findings
of (Mehdad et al, 2010), the results obtained over
the MT-derived corpus are equal to those we achieve
over the original RTE3 dataset (i.e. 63.50%). Third,
the accuracy obtained over the CLTE corpus using
combined phrase and paraphrase tables (62.88%, as
reported in Table 1) is comparable to the best re-
sult gained over the automatically translated dataset
(63.50%). In all the other cases, the use of phrase
and paraphrase tables on CLTE data outperforms
the results achieved on the same data after transla-
tion. Finally, it?s worth remarking that applying our
phrase matching method on the translated dataset
without any additional source of knowledge would
result in an overall accuracy of 62.12%, which is
lower than the result obtained using only phrase ta-
bles on cross-lingual data (62.62%). This demon-
strates that phrase tables can successfully replace
MT systems in the CLTE task.
In light of this, we suggest that extracting lexi-
cal knowledge from parallel corpora is a preferable
solution to approach CLTE. One of the main rea-
sons is that placing a black-box MT system at the
front-end of the entailment process reduces the pos-
sibility to cope with wrong translations. Further-
more, the access to MT components is not easy (e.g.
Google Translate limits the number and the size of
queries, while open source MT tools cover few lan-
guage pairs). Moreover, the task of developing a
full-fledged MT system often requires the availabil-
ity of parallel corpora, and is much more complex
than extracting lexical knowledge from them.
6 Conclusion and Future Work
In this paper we approached the cross-lingual Tex-
tual Entailment task focusing on the role of lexi-
cal knowledge extracted from bilingual parallel cor-
pora. One of the main difficulties in CLTE raises
from the lack of adequate knowledge resources to
bridge the lexical gap between texts and hypothe-
ses in different languages. Our approach builds on
the intuition that the vast amount of knowledge that
can be extracted from parallel data (in the form of
phrase and paraphrase tables) offers a possible so-
lution to the problem. To check the validity of our
assumptions we carried out several experiments on
an English-Spanish corpus derived from the RTE3
dataset, using phrasal matches as a criterion to ap-
proximate entailment. Our results show that phrase
and paraphrase tables allow to: i) outperform the re-
sults achieved with the few multilingual lexical re-
sources available, and ii) reach performance levels
above the average scores obtained by participants in
the monolingual RTE3 challenge. These improve-
ments can be explained by the fact that the lexi-
cal knowledge extracted from parallel data provides
good coverage both at the level of single words, and
at the level of phrases.
As a further contribution, we explored the appli-
cation of paraphrase tables extracted from parallel
data in the traditional monolingual scenario. Con-
trasting results with those obtained with the most
widely used resources in TE, we demonstrated the
effectiveness of paraphrase tables as a mean to over-
come the bias towards single words featured by the
existing resources.
Our future work will address both the extraction
of lexical information from bilingual parallel cor-
pora, and its use for TE and CLTE. On one side,
we plan to explore alternative ways to build phrase
and paraphrase tables. One possible direction is to
consider linguistically motivated approaches, such
as the extraction of syntactic phrase tables as pro-
posed by (Yamada and Knight, 2001). Another in-
teresting direction is to investigate the potential of
paraphrase patterns (i.e. patterns including part-
of-speech slots), extracted from bilingual parallel
corpora with the method proposed by (Zhao et al,
2009). On the other side we will investigate more
sophisticated methods to exploit the acquired lexi-
cal knowledge. As a first step, the probability scores
assigned to phrasal entries will be considered to per-
form weighted phrase matching as an improved cri-
terion to approximate entailment.
1343
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853).
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. Proceedings
of COLING-ACL.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL 2005).
Roy Bar-haim , Jonathan Berant , Ido Dagan , Iddo
Greental , Shachar Mirkin , Eyal Shnarch , and Idan
Szpektor. 2008. Efficient semantic deduction and ap-
proximate matching over compact parse forests. Pro-
ceedings of the TAC 2008 Workshop on Textual Entail-
ment.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of the the Text Analysis Conference (TAC
2010).
Timothy Chklovski and Patrick Pantel. 2004. Verbocean:
Mining the web for fine-grained semantic verb rela-
tions. Proceedings of Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP-04).
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Journal of Natural Language
Engineering , Volume 15, Special Issue 04, pp i-xvii.
Michael Denkowski and Alon Lavie. 2010. Extending
the METEOR Machine Translation Evaluation Metric
to the Phrase Level. Proceedings of Human Language
Technologies (HLT-NAACL 2010).
Georgiana Dinu and Rui Wang. 2009. Inference Rules
and their Application to Recognizing Textual Entail-
ment. Proceedings of the 12th Conference of the Eu-
ropean Chapter of the ACL (EACL 2009).
Claudio Giuliano. 2007. jLSI a tool for la-
tent semantic indexing. Software avail-
able at http://tcc.itc.it/research/textec/tools-
resources/jLSI.html.
Lidija Iordanskaja, Richard Kittredge, and Alain Polg re..
1991. Lexical selection and paraphrase in a meaning
text generation model. Natural Language Generation
in Articial Intelligence and Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical.
Philipp Koehn, Franz Josef Och, and Daniel Marcu 2003.
Statistical Phrase-Based Translation. Proceedings of
HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. Proceed-
ings of the Conference of the Association for Compu-
tational Linguistics (ACL).
Milen Kouleykov and Bernardo Magnini. 2005. Tree
edit distance for textual entailment. Proceedings of
RALNP-2005, International Conference on Recent Ad-
vances in Natural Language Processing.
Milen Kouylekov, Yashar Mehdad, and Matteo Negri.
2010. Mining Wikipedia for Large-Scale Repositories
of Context-Sensitive Entailment Rules. Proceedings
of the Language Resources and Evaluation Conference
(LREC 2010).
Yashar Mehdad, Alessandro Moschitti and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. Proceedings of the
11th Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010).
Dekang Lin and Patrick Pantel. 2001. DIRT - Discovery
of Inference Rules from Text.. Proceedings of ACM
Conference on Knowledge Discovery and Data Mining
(KDD-01).
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news on
a daily basis with Columbias Newsblaster. Proceed-
ings of the Human Language Technology Conference..
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment.
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Dan Moldovan and Adrian Novischi. 2002. Lexical
chains for question answering. Proceedings of COL-
ING.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. Proceed-
ings of the NAACL 2010 Workshop on Creating Speech
and Language Data With Amazons Mechanical Turk .
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):1951.
1344
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: Developing and Aligned
Multilingual Database. Proceedings of the First Inter-
national Conference on Global WordNet.
Vasile Rus, Art Graesser, and Kirtan Desai 2005.
Lexico-Syntactic Subsumption for Textual Entailment.
Proceedings of RANLP 2005.
Helmut Schmid 2005. Probabilistic Part-of-Speech Tag-
ging Using Decision Trees. Proceedings of the In-
ternational Conference on New Methods in Language
Processing.
Marta Tatu andDan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. Proceed-
ings of the Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing (HLT/EMNLP 2005).
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, Adequacy, or
HTER? Exploring Different Human Judgments with
a Tunable MT Metric. Proceedings of WMT09.
Rui Wang and Yi Zhang,. 2009. Recognizing Tex-
tual Relatedness with Predicate-Argument Structures.
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP 2009).
Kenji Yamada and Kevin Knight 2001. A Syntax-Based
Statistical Translation Model. Proceedings of the Con-
ference of the Association for Computational Linguis-
tics (ACL).
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting Paraphrase Patterns from Bilingual
Parallel Corpora. Journal of Natural Language Engi-
neering , Volume 15, Special Issue 04, pp 503-526.
1345
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 120?124,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Detecting Semantic Equivalence and Information Disparity
in Cross-lingual Documents
Yashar Mehdad Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{mehdad|negri|federico}@fbk.eu
Abstract
We address a core aspect of the multilingual
content synchronization task: the identifica-
tion of novel, more informative or semanti-
cally equivalent pieces of information in two
documents about the same topic. This can be
seen as an application-oriented variant of tex-
tual entailment recognition where: i) T and
H are in different languages, and ii) entail-
ment relations between T and H have to be
checked in both directions. Using a combi-
nation of lexical, syntactic, and semantic fea-
tures to train a cross-lingual textual entailment
system, we report promising results on differ-
ent datasets.
1 Introduction
Given two documents about the same topic writ-
ten in different languages (e.g. Wiki pages), con-
tent synchronization deals with the problem of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions. A roadmap to-
wards the solution of this problem has to take into
account, among the many sub-tasks, the identifica-
tion of information in one page that is semantically
equivalent, novel, or more informative with respect
to the content of the other page. In this paper we
set such problem as an application-oriented, cross-
lingual variant of the Textual Entailment (TE) recog-
nition task (Dagan and Glickman, 2004). Along this
direction, we make two main contributions:
(a) Experiments with multi-directional cross-
lingual textual entailment. So far, cross-lingual
textual entailment (CLTE) has been only applied
to: i) available TE datasets (uni-directional rela-
tions between monolingual pairs) transformed into
their cross-lingual counterpart by translating the hy-
potheses into other languages (Negri and Mehdad,
2010), and ii) machine translation (MT) evaluation
datasets (Mehdad et al, 2012). Instead, we ex-
periment with the only corpus representative of the
multilingual content synchronization scenario, and
the richer inventory of phenomena arising from it
(multi-directional entailment relations).
(b) Improvement of current CLTE methods. The
CLTE methods proposed so far adopt either a ?piv-
oting approach? based on the translation of the two
input texts into the same language (Mehdad et al,
2010), or an ?integrated solution? that exploits bilin-
gual phrase tables to capture lexical relations and
contextual information (Mehdad et al, 2011). The
promising results achieved with the integrated ap-
proach, however, still rely on phrasal matching tech-
niques that disregard relevant semantic aspects of
the problem. By filling this gap integrating linguis-
tically motivated features, we propose a novel ap-
proach that improves the state-of-the-art in CLTE.
2 CLTE-based content synchronization
CLTE has been proposed by (Mehdad et al, 2010) as
an extension of textual entailment which consists of
deciding, given a text T and an hypothesis H in dif-
ferent languages, if the meaning of H can be inferred
from the meaning of T. The adoption of entailment-
based techniques to address content synchronization
looks promising, as several issues inherent to such
task can be formalized as entailment-related prob-
120
lems. Given two pages (P1 and P2), these issues
include identifying, and properly managing:
(1) Text portions in P1 and P2 that express the same
meaning (bi-directional entailment). In such cases
no information has to migrate across P1 and P2, and
the two text portions will remain the same;
(2) Text portions in P1 that are more informa-
tive than portions in P2 (forward entailment). In
such cases, the entailing (more informative) portions
from P1 have to be translated and migrated to P2 in
order to replace or complement the entailed (less in-
formative) fragments;
(3) Text portions in P2 that are more informa-
tive than portions in P1 (backward entailment), and
should be translated to replace or complement them;
(4) Text portions in P1 describing facts that are not
present in P2, and vice-versa (the ?unknown? cases
in RTE parlance). In such cases, the novel infor-
mation from both sides has to be translated and mi-
grated in order to mutually enrich the two pages;
(5) Meaning discrepancies between text portions in
the two pages (?contradictions? in RTE parlance).
CLTE has been previously modeled as a phrase
matching problem that exploits dictionaries and
phrase tables extracted from bilingual parallel cor-
pora to determine the number of word sequences in
H that can be mapped to word sequences in T. In
this way a semantic judgement about entailment is
made exclusively on the basis of lexical evidence.
When only unidirectional entailment relations from
T to H have to be determined (RTE-like setting), the
full mapping of the hypothesis into the text usually
provides enough evidence for a positive entailment
judgement. Unfortunately, when dealing with multi-
directional entailment, the correlation between the
proportion of matching terms and the correct entail-
ment decisions is less strong. In such framework, for
instance, the full mapping of the hypothesis into the
text is per se not sufficient to discriminate between
forward entailment and semantic equivalence. To
cope with these issues, we explore the contribution
of syntactic and semantic features as a complement
to lexical ones in a supervised learning framework.
3 Beyond lexical CLTE
In order to enrich the feature space beyond pure lex-
ical match through phrase table entries, our model
builds on two additional feature sets, derived from i)
semantic phrase tables, and ii) dependency relations.
Semantic Phrase Table (SPT) matching repre-
sents a novel way to leverage the integration of se-
mantics and MT-derived techniques. SPT matching
extends CLTE methods based on pure lexical match
by means of ?generalized? phrase tables annotated
with shallow semantic labels. SPTs, with entries in
the form ?[LABEL] word1...wordn [LABEL]?, are
used as a recall-oriented complement to the phrase
tables used in MT. A motivation for this augmenta-
tion is that semantic tags allow to match tokens that
do not occur in the original bilingual parallel cor-
pora used for phrase table extraction. Our hypothe-
sis is that the increase in recall obtained from relaxed
matches through semantic tags in place of ?out of
vocabulary? terms (e.g. unseen person names) is an
effective way to improve CLTE performance, even
at the cost of some loss in precision.
Like lexical phrase tables, SPTs are extracted
from parallel corpora. As a first step we annotate
the parallel corpora with named-entity taggers for
the source and target languages, replacing named
entities with general semantic labels chosen from
a coarse-grained taxonomy (person, location, orga-
nization, date and numeric expression). Then, we
combine the sequences of unique labels into one sin-
gle token of the same label, and we run Giza++ (Och
and Ney, 2000) to align the resulting semantically
augmented corpora. Finally, we extract the seman-
tic phrase table from the augmented aligned corpora
using the Moses toolkit (Koehn et al, 2007). For
the matching phase, we first annotate T and H in the
same way we labeled our parallel corpora. Then, for
each n-gram order (n=1 to 5) we use the SPT to cal-
culate a matching score as the number of n-grams in
H that match with phrases in T divided by the num-
ber of n-grams in H.1
Dependency Relation (DR) matching targets the
increase of CLTE precision. Adding syntactic con-
straints to the matching process, DR features aim to
reduce the amount of wrong matches often occur-
ring with bag-of-words methods (both at the lexi-
cal level and with recall-oriented SPTs). For in-
stance, the contradiction between ?Yahoo acquired
1When checking for entailment from H to T, the normaliza-
tion is carried out dividing by the number of n-grams in T.
121
Overture? and ?Overture compro? Yahoo?, which is
evident when syntax is taken into account, can not
be caught by shallow methods. We define a de-
pendency relation as a triple that connects pairs of
words through a grammatical relation. DR matching
captures similarities between dependency relations,
combining the syntactic and lexical level. In a valid
match, while the relation has to be the same, the con-
nected words can be either the same, or semantically
equivalent terms in the two languages (e.g. accord-
ing to a bilingual dictionary). Given the dependency
tree representations of T and H, for each grammati-
cal relation (r) we calculate a DR matching score as
the number of matching occurrences of r in T and
H, divided by the number of occurrences of r in H.
Separate DR matching scores are calculated for each
relation r appearing both in T and H.
4 Experiments and results
4.1 Content synchronization scenario
In our first experiment we used the English-German
portion of the CLTE corpus described in (Negri et
al., 2011), consisting of 500 multi-directional entail-
ment pairs which we equally divided into training
and test sets. Each pair in the dataset is annotated
with ?Bidirectional?, ?Forward?, or ?Backward? en-
tailment judgements. Although highly relevant for
the content synchronization task, ?Contradiction?
and ?Unknown? cases (i.e. ?NO? entailment in both
directions) are not present in the annotation. How-
ever, this is the only available dataset suitable to
gather insights about the viability of our approach to
multi-directional CLTE recognition.2 We chose the
ENG-GER portion of the dataset since for such lan-
guage pair MT systems performance is often lower,
making the adoption of simpler solutions based on
pivoting more vulnerable.
To build the English-German phrase tables we
combined the Europarl, News Commentary and ?de-
news?3 parallel corpora. After tokenization, Giza++
and Moses were respectively used to align the cor-
pora and extract a lexical phrase table (PT). Simi-
larly, the semantic phrase table (SPT) has been ex-
2Recently, a new dataset including ?Unknown? pairs has
been used in the ?Cross-Lingual Textual Entailment for Content
Synchronization? task at SemEval-2012 (Negri et al, 2012).
3http://homepages.inf.ed.ac.uk/pkoehn/
tracted from the same corpora annotated with the
Stanford NE tagger (Faruqui and Pado?, 2010; Finkel
et al, 2005). Dependency relations (DR) have been
extracted running the Stanford parser (Rafferty and
Manning, 2008; De Marneffe et al, 2006). The dic-
tionary created during the alignment of the parallel
corpora provided the lexical knowledge to perform
matches when the connected words are different, but
semantically equivalent in the two languages. To
combine and weight features at different levels we
used SVMlight (Joachims, 1999) with default pa-
rameters.
In order to experiment under testing conditions
of increasing complexity, we set the CLTE problem
both as a two-way and as a three-way classification
task. Two-way classification casts multi-directional
entailment as a unidirectional problem, where each
pair is analyzed checking for entailment both from
left to right and from right to left. In this condi-
tion, each original test example is correctly clas-
sified if both pairs originated from it are correctly
judged (?YES-YES? for bidirectional, ?YES-NO?
for forward, and ?NO-YES? for backward entail-
ment). Two-way classification represents an intu-
itive solution to capture multidirectional entailment
relations but, at the same time, a suboptimal ap-
proach in terms of efficiency since two checks are
performed for each pair. Three-way classification is
more efficient, but at the same time more challeng-
ing due to the higher difficulty of multiclass learn-
ing, especially with small datasets.
Results are compared with two pivoting ap-
proaches, checking for entailment between the orig-
inal English texts and the translated German hy-
potheses.4 The first (Pivot-EDITS), uses an op-
timized distance-based model implemented in the
open source RTE system EDITS (Kouylekov and
Negri, 2010; Kouylekov et al, 2011). The second
(Pivot-PPT) exploits paraphrase tables for phrase
matching, and represents the best monolingual
model presented in (Mehdad et al, 2011). Table
1 demonstrates the success of our results in prov-
ing the two main claims of this paper. (a) In both
settings all the feature sets used outperform the ap-
proaches taken as terms of comparison. The 61.6%
accuracy achieved in the most challenging setting
4Using Google Translate.
122
PT PT+DR PT+SPT PT+SPT+DR Pivot-EDITS Pivot-PPT
Cont. Synch. (2-way) 57.8 58.6 62.4 63.3 27.4 57.0
Cont. Synch. (3-way) 57.4 57.8 58.7 61.6 25.3 56.1
RTE-3 AVG Pivot PPT
RTE3-derived 62.6 63.6 63.5 64.5 62.4 63.5
Table 1: CLTE accuracy results over content synchronization and RTE3-derived datasets.
(3-way) demonstrates the effectiveness of our ap-
proach to capture meaning equivalence and informa-
tion disparity in cross-lingual texts.
(b) In both settings the combination of lexical, syn-
tactic and semantic features (PT+SPT+DR) signif-
icantly improves5 the state-of-the-art CLTE model
(PT). Such improvement is motivated by the joint
contribution of SPTs (matching more and longer n-
grams, with a consequent recall improvement), and
DR matching (adding constraints, with a consequent
gain in precision). However, the performance in-
crease brought by DR features over PT is mini-
mal. This might be due to the fact that both PT and
DR features are precision-oriented, and their effec-
tiveness becomes evident only in combination with
recall-oriented features (SPT).
Cross-lingual models also significantly outper-
form pivoting methods. This suggests that the noise
introduced by incorrect translations makes the pivot-
ing approach less attractive in comparison with the
more robust cross-lingual models.
4.2 RTE-like CLTE scenario
Our second experiment aims at verifying the effec-
tiveness of the improved model over RTE-derived
CLTE data. To this aim, we compare the results ob-
tained by the new CLTE model with those reported
in (Mehdad et al, 2011), calculated over an English-
Spanish entailment corpus derived from the RTE-3
dataset (Negri and Mehdad, 2010).
In order to build the English-Spanish lexical
phrase table (PT), we used the Europarl, News Com-
mentary and United Nations parallel corpora. The
semantic phrase table (SPT) was extracted from the
same corpora annotated with FreeLing (Carreras et
al., 2004). Dependency relations (DR) have been ex-
tracted parsing English texts and Spanish hypotheses
with DepPattern (Gamallo and Gonzalez, 2011).
5p < 0.05, calculated using the approximate randomization
test implemented in (Pado?, 2006).
Accuracy results have been calculated over 800
test pairs of the CLTE corpus, after training the SVM
binary classifier over the 800 development pairs.
Our new features have been compared with: i) the
state-of-the-art CLTE model (PT), ii) the best mono-
lingual model (Pivot-PPT) presented in (Mehdad et
al., 2011), and iii) the average result achieved by
participants in the monolingual English RTE-3 eval-
uation campaign (RTE-3 AVG). As shown in Ta-
ble 1, the combined feature set (PT+SPT+DR) sig-
nificantly5 outperforms the lexical model (64.5%
vs 62.6%), while SPT and DR features separately
added to PT (PT+SPT, and PT+DR) lead to marginal
improvements over the results achieved by the PT
model alone (about 1%). This confirms the con-
clusions drawn from the previous experiment, that
precision-oriented and recall-oriented features lead
to a larger improvement when they are used in com-
bination.
5 Conclusion
We addressed the identification of semantic equiv-
alence and information disparity in two documents
about the same topic, written in different languages.
This is a core aspect of the multilingual content syn-
chronization task, which represents a challenging
application scenario for a variety of NLP technolo-
gies, and a shared research framework for the inte-
gration of semantics and MT technology. Casting
the problem as a CLTE task, we extended previous
lexical models with syntactic and semantic features.
Our results in different cross-lingual settings prove
the feasibility of the approach, with significant state-
of-the-art improvements also on RTE-derived data.
Acknowledgments
This work has been partially supported by the EU-
funded project CoSyne (FP7-ICT-4-248531).
123
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language Ana-
lyzers. In Proceedings of the 4th Language Resources
and Evaluation Conference (LREC 2004), volume 4.
I. Dagan and O. Glickman. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. In Proceedings of the PASCAL Workshop
of Learning Methods for Text Understanding and Min-
ing.
M.C. De Marneffe, B. MacCartney, and C.D. Man-
ning. 2006. Generating Typed Dependency Parses
from Phrase Structure Parses. In Proceedings of the
5th Language Resources and Evaluation Conference
(LREC 2006), volume 6, pages 449?454.
M. Faruqui and S. Pado?. 2010. Training and Evaluat-
ing a German Named Entity Recognizer with Seman-
tic Generalization. In Proceedings of the 10th Con-
ference on Natural Language Processing (KONVENS
2010), Saarbru?cken, Germany.
J.R. Finkel, T. Grenager, and C. Manning. 2005. Incor-
porating Non-local Information into Information Ex-
traction Systems by Gibbs Sampling. In Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics (ACL 2005).
P. Gamallo and I. Gonzalez. 2011. A grammatical for-
malism based on patterns of part of speech tags. Inter-
national Journal of Corpus Linguistics, 16(1):45?71.
T. Joachims. 1999. Advances in kernel methods. chap-
ter Making large-scale support vector machine learn-
ing practical, pages 169?184. MIT Press, Cambridge,
MA, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings of
the 45th Annual Meeting on Association for Computa-
tional Linguistics, Demonstration Session (ACL 2007).
M. Kouylekov and M. Negri. 2010. An Open-Source
Package for Recognizing Textual Entailment. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, system demonstrations
(ACL 2010).
M. Kouylekov, Y. Mehdad, and M. Negri. 2011. Is it
Worth Submitting this Run? Assess your RTE Sys-
tem with a Good Sparring Partner. Proceedings of the
EMNLP TextInfer 2011 Workshop on Textual Entail-
ment.
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012. Match
without a Referee: Evaluating MT Adequacy without
Reference Translations. In Proceedings of the Ma-
chine Translation Workshop (WMT2012).
M. Negri and Y. Mehdad. 2010. Creating a Bi-lingual
Entailment Corpus through Translations with Mechan-
ical Turk: $100 for a 10-day Rush. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazons? Mechanical Turk.
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and Conquer: Crowd-
sourcing the Creation of Cross-Lingual Textual Entail-
ment Corpora. Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011).
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
S. Pado?, 2006. User?s guide to sigf: Significance test-
ing by approximate randomisation.
A.N. Rafferty and C.D. Manning. 2008. Parsing Three
German Treebanks: Lexicalized and Unlexicalized
Baselines. In In Proceedings of the ACL 2008 Work-
shop on Parsing German.
124
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 486?496,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Combining Intra- and Multi-sentential Rhetorical Parsing for
Document-level Discourse Analysis
Shafiq Joty?
sjoty@qf.org.qa
Qatar Computing Research Institute
Qatar Foundation
Doha, Qatar
Giuseppe Carenini, Raymond Ng, Yashar Mehdad
{carenini, rng, mehdad}@cs.ubc.ca
Department of Computer Science
University of British Columbia
Vancouver, Canada
Abstract
We propose a novel approach for develop-
ing a two-stage document-level discourse
parser. Our parser builds a discourse tree
by applying an optimal parsing algorithm
to probabilities inferred from two Con-
ditional Random Fields: one for intra-
sentential parsing and the other for multi-
sentential parsing. We present two ap-
proaches to combine these two stages of
discourse parsing effectively. A set of
empirical evaluations over two different
datasets demonstrates that our discourse
parser significantly outperforms the state-
of-the-art, often by a wide margin.
1 Introduction
Discourse of any kind is not formed by inde-
pendent and isolated textual units, but by related
and structured units. Discourse analysis seeks
to uncover such structures underneath the surface
of the text, and has been shown to be benefi-
cial for text summarization (Louis et al, 2010;
Marcu, 2000b), sentence compression (Sporleder
and Lapata, 2005), text generation (Prasad et al,
2005), sentiment analysis (Somasundaran, 2010)
and question answering (Verberne et al, 2007).
Rhetorical Structure Theory (RST) (Mann and
Thompson, 1988), one of the most influential the-
ories of discourse, represents texts by labeled hier-
archical structures, called Discourse Trees (DTs),
as exemplified by a sample DT in Figure 1. The
leaves of a DT correspond to contiguous Elemen-
tary Discourse Units (EDUs) (six in the exam-
ple). Adjacent EDUs are connected by rhetori-
cal relations (e.g., Elaboration, Contrast), form-
ing larger discourse units (represented by internal
?This work was conducted at the University of British
Columbia, Vancouver, Canada.
nodes), which in turn are also subject to this re-
lation linking. Discourse units linked by a rhetori-
cal relation are further distinguished based on their
relative importance in the text: nucleus being the
central part, whereas satellite being the peripheral
one. Discourse analysis in RST involves two sub-
tasks: discourse segmentation is the task of identi-
fying the EDUs, and discourse parsing is the task
of linking the discourse units into a labeled tree.
While recent advances in automatic discourse
segmentation and sentence-level discourse parsing
have attained accuracies close to human perfor-
mance (Fisher and Roark, 2007; Joty et al, 2012),
discourse parsing at the document-level still poses
significant challenges (Feng and Hirst, 2012) and
the performance of the existing document-level
parsers (Hernault et al, 2010; Subba and Di-
Eugenio, 2009) is still considerably inferior com-
pared to human gold-standard. This paper aims
to reduce this performance gap and take discourse
parsing one step further. To this end, we address
three key limitations of existing parsers as follows.
First, existing discourse parsers typically model
the structure and the labels of a DT separately
in a pipeline fashion, and also do not consider
the sequential dependencies between the DT con-
stituents, which has been recently shown to be crit-
ical (Feng and Hirst, 2012). To address this limi-
tation, as the first contribution, we propose a novel
document-level discourse parser based on proba-
bilistic discriminative parsing models, represented
as Conditional Random Fields (CRFs) (Sutton et
al., 2007), to infer the probability of all possible
DT constituents. The CRF models effectively rep-
resent the structure and the label of a DT con-
stituent jointly, and whenever possible, capture the
sequential dependencies between the constituents.
Second, existing parsers apply greedy and sub-
optimal parsing algorithms to build the DT for a
document. To cope with this limitation, our CRF
models support a probabilistic bottom-up parsing
486
But he added:
"Some people use the purchasers?
 index as a leading indicator, some use it as a coincident indicator. But the thing it?s supposed to measure -- manufacturing strength --
it missed altogether last month." <P>Elaboration
Same-UnitContrast
Contrast
Attribution
(1)
(2) (3)
(4) (5)
(6)
Figure 1: Discourse tree for two sentences in RST-DT. Each of the sentences contains three EDUs. The
second sentence has a well-formed discourse tree, but the first sentence does not have one.
algorithm which is non-greedy and optimal.
Third, existing discourse parsers do not dis-
criminate between intra-sentential (i.e., building
the DTs for the individual sentences) and multi-
sentential parsing (i.e., building the DT for the
document). However, we argue that distinguish-
ing between these two conditions can result in
more effective parsing. Two separate parsing
models could exploit the fact that rhetorical re-
lations are distributed differently intra-sententially
vs. multi-sententially. Also, they could indepen-
dently choose their own informative features. As
another key contribution of our work, we devise
two different parsing components: one for intra-
sentential parsing, the other for multi-sentential
parsing. This provides for scalable, modular and
flexible solutions, that can exploit the strong cor-
relation observed between the text structure (sen-
tence boundaries) and the structure of the DT.
In order to develop a complete and robust dis-
course parser, we combine our intra-sentential
and multi-sentential parsers in two different ways.
Since most sentences have a well-formed dis-
course sub-tree in the full document-level DT (for
example, the second sentence in Figure 1), our first
approach constructs a DT for every sentence us-
ing our intra-sentential parser, and then runs the
multi-sentential parser on the resulting sentence-
level DTs. However, this approach would disre-
gard those cases where rhetorical structures vio-
late sentence boundaries. For example, consider
the first sentence in Figure 1. It does not have a
well-formed sub-tree because the unit containing
EDUs 2 and 3 merges with the next sentence and
only then is the resulting unit merged with EDU
1. Our second approach, in an attempt of dealing
with these cases, builds sentence-level sub-trees
by applying the intra-sentential parser on a sliding
window covering two adjacent sentences and by
then consolidating the results produced by over-
lapping windows. After that, the multi-sentential
parser takes all these sentence-level sub-trees and
builds a full rhetorical parse for the document.
While previous approaches have been tested on
only one corpus, we evaluate our approach on
texts from two very different genres: news articles
and instructional how-to-do manuals. The results
demonstrate that our contributions provide con-
sistent and statistically significant improvements
over previous approaches. Our final result com-
pares very favorably to the result of state-of-the-art
models in document-level discourse parsing.
In the rest of the paper, after discussing related
work in Section 2, we present our discourse pars-
ing framework in Section 3. In Section 4, we de-
scribe the intra- and multi-sentential parsing com-
ponents. Section 5 presents the two approaches
to combine the two stages of parsing. The exper-
iments and error analysis, followed by future di-
rections are discussed in Section 6. Finally, we
summarize our contributions in Section 7.
2 Related work
The idea of staging document-level discourse
parsing on top of sentence-level discourse parsing
was investigated in (Marcu, 2000a; LeThanh et al,
2004). These approaches mainly rely on discourse
markers (or cues), and use hand-coded rules to
build DTs for sentences first, then for paragraphs,
and so on. However, often rhetorical relations
are not explicitly signaled by discourse markers
(Marcu and Echihabi, 2002), and discourse struc-
tures do not always correspond to paragraph struc-
tures (Sporleder and Lascarides, 2004). Therefore,
rather than relying on hand-coded rules based on
discourse markers, recent approaches employ su-
pervised machine learning techniques with a large
set of informative features.
Hernault et al, (2010) presents the publicly
available HILDA parser. Given the EDUs in a doc-
487
Elaboration Joint AttributionSame-Unit Contrast Explanation
0
5
10
15
20
25
30 Multi-sentential
Intra-sentential
Figure 2: Distributions of six most frequent relations in
intra-sentential and multi-sentential parsing scenarios.
ument, HILDA iteratively employs two Support
Vector Machine (SVM) classifiers in pipeline to
build the DT. In each iteration, a binary classifier
first decides which of the adjacent units to merge,
then a multi-class classifier connects the selected
units with an appropriate relation label. They eval-
uate their approach on the RST-DT corpus (Carl-
son et al, 2002) of news articles. On a different
genre of instructional texts, Subba and Di-Eugenio
(2009) propose a shift-reduce parser that relies on
a classifier for relation labeling. Their classifier
uses Inductive Logic Programming (ILP) to learn
first-order logic rules from a set of features includ-
ing compositional semantics. In this work, we ad-
dress the limitations of these models (described in
Section 1) introducing our novel discourse parser.
3 Our Discourse Parsing Framework
Given a document with sentences already seg-
mented into EDUs, the discourse parsing prob-
lem is determining which discourse units (EDUs
or larger units) to relate (i.e., the structure), and
how to relate them (i.e., the labels or the discourse
relations) in the resulting DT. Since we already
have an accurate sentence-level discourse parser
(Joty et al, 2012), a straightforward approach to
document-level parsing could be to simply apply
this parser to the whole document. However this
strategy would be problematic because of scalabil-
ity and modeling issues. Note that the number of
valid trees grows exponentially with the number
of EDUs in a document.1 Therefore, an exhaus-
tive search over the valid trees is often unfeasible,
even for relatively small documents.
For modeling, the problem is two-fold. On the
one hand, it appears that rhetorical relations are
distributed differently intra-sententially vs. multi-
sententially. For example, Figure 2 shows a com-
parison between the two distributions of six most
1For n + 1 EDUs, the number of valid discourse trees is
actually the Catalan number Cn.
model
AlgorithmSentences segmented into EDUs
Document-level
 discourse tree
Intra-sententialparser Multi-sententialparser
model
Algorithm
Figure 3: Discourse parsing framework.
frequent relations on a development set containing
20 randomly selected documents from RST-DT.
Notice that relations Attribution and Same-Unit
are more frequent than Joint in intra-sentential
case, whereas Joint is more frequent than the other
two in multi-sentential case. On the other hand,
different kinds of features are applicable and in-
formative for intra-sentential vs. multi-sentential
parsing. For example, syntactic features like dom-
inance sets (Soricut and Marcu, 2003) are ex-
tremely useful for sentence-level parsing, but are
not even applicable in multi-sentential case. Like-
wise, lexical chain features (Sporleder and Las-
carides, 2004), that are useful for multi-sentential
parsing, are not applicable at the sentence level.
Based on these observations, our discourse
parsing framework comprises two separate mod-
ules: an intra-sentential parser and a multi-
sentential parser (Figure 3). First, the intra-
sentential parser produces one or more discourse
sub-trees for each sentence. Then, the multi-
sentential parser generates a full DT for the doc-
ument from these sub-trees. Both of our parsers
have the same two components: a parsing model
assigns a probability to every possible DT, and
a parsing algorithm identifies the most probable
DT among the candidate DTs in that scenario.
While the two models are rather different, the
same parsing algorithm is shared by the two mod-
ules. Staging multi-sentential parsing on top of
intra-sentential parsing in this way allows us to ex-
ploit the strong correlation between the text struc-
ture and the DT structure as explained in detail in
Section 5. Before describing our parsing models
and the parsing algorithm, we introduce some ter-
minology that we will use throughout the paper.
Following (Joty et al, 2012), a DT can be for-
mally represented as a set of constituents of the
form R[i,m, j], referring to a rhetorical relation
R between the discourse unit containing EDUs i
through m and the unit containing EDUs m+1
through j. For example, the DT for the sec-
ond sentence in Figure 1 can be represented as
488
{Elaboration-NS[4,4,5], Same-Unit-NN[4,5,6]}.
Notice that a relation R also specifies the nuclear-
ity statuses of the discourse units involved, which
can be one of Nucleus-Satellite (NS), Satellite-
Nucleus (SN) and Nucleus-Nucleus (NN).
4 Parsing Models and Parsing Algorithm
The job of our intra-sentential and multi-sentential
parsing models is to assign a probability to each
of the constituents of all possible DTs at the sen-
tence level and at the document level, respectively.
Formally, given the model parameters ?, for each
possible constituent R[i,m, j] in a candidate DT
at the sentence or document level, the parsing
model estimates P (R[i,m, j]|?), which specifies
a joint distribution over the label R and the struc-
ture [i,m, j] of the constituent.
4.1 Intra-Sentential Parsing Model
Recently, we proposed a novel parsing model
for sentence-level discourse parsing (Joty et
al., 2012), that outperforms previous approaches
by effectively modeling sequential dependencies
along with structure and labels jointly. Below we
briefly describe the parsing model, and show how
it is applied to obtain the probabilities of all possi-
ble DT constituents at the sentence level.
Figure 4 shows the intra-sentential parsing
model expressed as a Dynamic Conditional Ran-
dom Field (DCRF) (Sutton et al, 2007). The ob-
served nodes Uj in a sequence represent the dis-
course units (EDUs or larger units). The first layer
of hidden nodes are the structure nodes, where
Sj?{0, 1} denotes whether two adjacent discourse
units Uj?1 and Uj should be connected or not.
The second layer of hidden nodes are the relation
nodes, with Rj?{1 . . .M} denoting the relation
between two adjacent unitsUj?1 andUj , whereM
is the total number of relations in the relation set.
The connections between adjacent nodes in a hid-
den layer encode sequential dependencies between
the respective hidden nodes, and can enforce con-
straints such as the fact that a Sj= 1 must not fol-
low a Sj?1= 1. The connections between the two
hidden layers model the structure and the relation
of a DT (sentence-level) constituent jointly.
To obtain the probability of the constituents
of all candidate DTs for a sentence, we apply
the parsing model recursively at different levels
of the DT and compute the posterior marginals
over the relation-structure pairs. To illustrate the
U UU U U
2
2
2
3 j t-1 t
SS S S S
R R R R R
3
3 j
j t-1
t-1 t
Unit sequenceat level i
Structure sequence
Relationsequence
U1
t
Figure 4: A chain-structured DCRF as our intra-
sentential parsing model.
process, let us assume that the sentence contains
four EDUs. At the first (bottom) level, when all
the units are the EDUs, there is only one possible
unit sequence to which we apply our DCRF
model (Figure 5(a)). We compute the posterior
marginals P (R2, S2=1|e1, e2, e3, e4,?), P (R3,
S3=1|e1, e2, e3, e4,?) and P (R4, S4=1|e1, e2, e3,
e4,?) to obtain the probability of the con-
stituents R[1, 1, 2], R[2, 2, 3] and R[3, 3, 4],
respectively. At the second level, there are
three possible unit sequences (e1:2, e3, e4),
(e1,e2:3, e4) and (e1,e2,e3:4). Figure 5(b) shows
their corresponding DCRFs. The posterior
marginals P (R3, S3=1|e1:2,e3,e4,?), P (R2:3
S2:3=1|e1,e2:3,e4,?), P (R4, S4=1|e1,e2:3,e4,?)
and P (R3:4, S3:4=1|e1,e2,e3:4,?) computed from
the three sequences correspond to the probability
of the constituents R[1, 2, 3], R[1, 1, 3], R[2, 3, 4]
and R[2, 2, 4], respectively. Similarly, we attain
the probability of the constituents R[1, 1, 4],
R[1, 2, 4] and R[1, 3, 4] by computing their
respective posterior marginals from the three
possible sequences at the third (top) level.
e 1 e e2
2
2
3
S S3
R R3
(a)
e 1e
S
R
1:2 3
3
3
e e
S
R
2:3
2:3
(b)
2:3e4
S4
R4
e4
S4
R4
e4
S4
R4
1e e
S
R
2
2
2 e3:4
S3:4
R3:4
1 e
S
R
1:3 4
4
4
e e
S
R
2:4
2:4
(c)
2:4 ee
S
R
1:2e
3:4
3:4
3:4
(i) (ii)
(iii)(i) (ii) (iii)
Figure 5: Our parsing model applied to the sequences at
different levels of a sentence-level DT. (a) Only possible se-
quence at the first level, (b) Three possible sequences at the
second level, (c) Three possible sequences at the third level.
At this point what is left to be explained is
how we generate all possible sequences for a
given number of EDUs in a sentence. Algorithm
1 demonstrates how we do that. More specifi-
cally, to compute the probabilities of each DT con-
489
stituent R[i, k, j], we need to generate sequences
like (e1, ? ? ? , ei?1, ei:k, ek+1:j , ej+1, ? ? ? , en) for
1 ? i ? k < j ? n. In doing so, we may
generate some duplicate sequences. Clearly, the
sequence (e1, ? ? ? , ei?1, ei:i, ei+1:j , ej+1, ? ? ? , en)
for 1 ? i ? k < j < n is already considered
for computing the probability of R[i+ 1, j, j+ 1].
Therefore, it is a duplicate sequence that we ex-
clude from our list of all possible sequences.
Input: Sequence of EDUs: (e1, e2, ? ? ? , en)
Output: List of sequences: L
for i = 1? n? 1 do
for j = i+ 1? n do
if j == n then
for k = i? j ? 1 do
L.append
((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))
end
else
for k = i+ 1? j ? 1 do
L.append
((e1, .., ei?1, ei:k, ek+1:j , ej+1, .., en))
end
end
end
end
Algorithm 1: Generating all possible sequences
for a sentence with n EDUs.
Once we obtain the probability of all possible
DT constituents, the discourse sub-trees for the
sentences are built by applying an optimal prob-
abilistic parsing algorithm (Section 4.4) using one
of the methods described in Section 5.
4.2 Multi-Sentential Parsing Model
Given the discourse units (sub-trees) for all the
sentences of a document, a simple approach to
build the rhetorical tree of the document would be
to apply a new DCRF model, similar to the one
in Figure 4 (with different parameters), to all the
possible sequences generated from these units to
infer the probability of all possible higher-order
constituents. However, the number of possible se-
quences and their length increase with the number
of sentences in a document. For example, assum-
ing that each sentence has a well-formed DT, for
a document with n sentences, Algorithm 1 gener-
ates O(n3) sequences, where the sequence at the
bottom level has n units, each of the sequences at
the second level has n-1 units, and so on. Since
the model in Figure 4 has a ?fat? chain structure,
U Ut-1 t
S
R t
Adjacent Unitsat level i
Structure
Relation
t
Figure 6: A CRF as a multi-sentential parsing model.
we could use forwards-backwards algorithm for
exact inference in this model (Sutton and McCal-
lum, 2012). However, forwards-backwards on a
sequence containing T units costs O(TM2) time,
where M is the number of relations in our rela-
tion set. This makes the chain-structured DCRF
model impractical for multi-sentential parsing of
long documents, since learning requires to run in-
ference on every training sequence with an overall
time complexity of O(TM2n3) per document.
Our model for multi-sentential parsing is shown
in Figure 6. The two observed nodes Ut?1 and
Ut are two adjacent discourse units. The (hidden)
structure node S?{0, 1} denotes whether the two
units should be connected or not. The hidden node
R?{1 . . .M} represents the relation between the
two units. Notice that like the previous model, this
is also an undirected graphical model. It becomes
a CRF if we directly model the hidden (output)
variables by conditioning its clique potential (or
factor) ? on the observed (input) variables:
P (Rt, St|x,?) = 1Z(x,?)?(Rt, St|x,?) (1)
where x represents input features extracted from
the observed variables Ut?1 and Ut, and Z(x,?)
is the partition function. We use a log-linear rep-
resentation of the factor:
?(Rt, St|x,?) = exp(?T f(Rt, St, x)) (2)
where f(Rt, St, x) is a feature vector derived from
the input features x and the labels Rt and St, and
? is the corresponding weight vector. Although,
this model is similar in spirit to the model in Fig-
ure 4, we now break the chain structure, which
makes the inference much faster (i.e., complex-
ity of O(M2)). Breaking the chain structure also
allows us to balance the data for training (equal
number instances with S=1 and S=0), which dra-
matically reduces the learning time of the model.
We apply our model to all possible adjacent
units at all levels for the multi-sentential case, and
490
compute the posterior marginals of the relation-
structure pairs P (Rt, St=1|Ut?1, Ut,?) to obtain
the probability of all possible DT constituents.
4.3 Features Used in our Parsing Models
Table 1 summarizes the features used in our pars-
ing models, which are extracted from two adjacent
unitsUt?1 andUt. Since most of these features are
adopted from previous studies (Joty et al, 2012;
Hernault et al, 2010), we briefly describe them.
Organizational features include the length of
the units as the number of EDUs and tokens.
It also includes the distances of the units from
the beginning and end of the sentence (or text in
the multi-sentential case). Text structural fea-
tures indirectly capture the correlation between
text structure and rhetorical structure by counting
the number of sentence and paragraph boundaries
in the units. Discourse markers (e.g., because, al-
though) carry informative clues for rhetorical re-
lations (Marcu, 2000a). Rather than using a fixed
list of discourse markers, we use an empirically
learned lexical N-gram dictionary following (Joty
et al, 2012). This approach has been shown to
be more robust and flexible across domains (Bi-
ran and Rambow, 2011; Hernault et al, 2010). We
also include part-of-speech (POS) tags for the be-
ginning and end N tokens in a unit.
8 Organizational features Intra & Multi-Sentential
Number of EDUs in unit 1 (or unit 2).
Number of tokens in unit 1 (or unit 2).
Distance of unit 1 in EDUs to the beginning (or to the end).
Distance of unit 2 in EDUs to the beginning (or to the end).
4 Text structural features Multi-Sentential
Number of sentences in unit 1 (or unit 2).
Number of paragraphs in unit 1 (or unit 2).
8 N-gram features N?{1, 2, 3} Intra & Multi-Sentential
Beginning (or end) lexical N-grams in unit 1.
Beginning (or end) lexical N-grams in unit 2.
Beginning (or end) POS N-grams in unit 1.
Beginning (or end) POS N-grams in unit 2.
5 Dominance set features Intra-Sentential
Syntactic labels of the head node and the attachment node.
Lexical heads of the head node and the attachment node.
Dominance relationship between the two units.
8 Lexical chain features Multi-Sentential
Number of chains start in unit 1 and end in unit 2.
Number of chains start (or end) in unit 1 (or in unit 2).
Number of chains skipping both unit 1 and unit 2.
Number of chains skipping unit 1 (or unit 2).
2 Contextual features Intra & Multi-Sentential
Previous and next feature vectors.
2 Substructure features Intra & Multi-Sentential
Root nodes of the left and right rhetorical sub-trees.
Table 1: Features used in our parsing models.
Lexico-syntactic features dominance sets
(Soricut and Marcu, 2003) are very effective for
intra-sentential parsing. We include syntactic
labels and lexical heads of head and attachment
nodes along with their dominance relationship
as features. Lexical chains (Morris and Hirst,
1991) are sequences of semantically related words
that can indicate topic shifts. Features extracted
from lexical chains have been shown to be useful
for finding paragraph-level discourse structure
(Sporleder and Lascarides, 2004). We compute
lexical chains for a document following the ap-
proach proposed in (Galley and McKeown, 2003),
that extracts lexical chains after performing word
sense disambiguation. Following (Joty et al,
2012), we also encode contextual and rhetorical
sub-structure features in our models. The rhetori-
cal sub-structure features incorporate hierarchical
dependencies between DT constituents.
4.4 Parsing Algorithm
Given the probability of all possible DT con-
stituents in the intra-sentential and multi-sentential
scenarios, the job of the parsing algorithm is to
find the most probable DT for that scenario. Fol-
lowing (Joty et al, 2012), we implement a prob-
abilistic CKY-like bottom-up algorithm for com-
puting the most likely parse using dynamic pro-
gramming. Specifically, with n discourse units,
we use the upper-triangular portion of the n?n
dynamic programming table D. Given Ux(0) and
Ux(1) are the start and end EDU Ids of unit Ux:
D[i, j] = P (R[Ui(0), Uk(1), Uj(1)]) (3)
where, k = argmax
i?p?j
P (R[Ui(0), Up(1), Uj(1)]).
Note that, in contrast to previous studies on
document-level parsing (Hernault et al, 2010;
Subba and Di-Eugenio, 2009; Marcu, 2000b),
which use a greedy algorithm, our approach finds
a discourse tree that is globally optimal.
5 Document-level Parsing Approaches
Now that we have presented our intra-sentential
and our multi-sentential parsers, we are ready to
describe how they can be effectively combined to
perform document-level discourse analysis. Re-
call that a key motivation for a two-stage parsing is
that it allows us to capture the correlation between
text structure and discourse structure in a scalable,
modular and flexible way. Below we describe two
different approaches to model this correlation.
491
5.1 1S-1S (1 Sentence-1 Sub-tree)
A key finding from several previous studies on
sentence-level discourse analysis is that most sen-
tences have a well-formed discourse sub-tree in
the full document-level DT (Joty et al, 2012;
Fisher and Roark, 2007). For example, Figure 7(a)
shows 10 EDUs in 3 sentences (see boxes), where
the DTs for the sentences obey their respective
sentence boundaries. The 1S-1S approach aims to
maximally exploit this finding. It first constructs
a DT for every sentence using our intra-sentential
parser, and then it provides our multi-sentential
parser with the sentence-level DTs to build the
rhetorical parse for the whole document.
1     2  3S 1
8  9   10S 34   5      6   7S 2
1    2   3S 1
8   9    10S 34   5    6    7S 2(a) (b)
???
Figure 7: Two possible DTs for three sentences.
5.2 Sliding Window
While the assumption made by 1S-1S clearly sim-
plifies the parsing process, it totally ignores the
cases where discourse structures violate sentence
boundaries. For example, in the DT shown in Fig-
ure 7(b), sentence S2 does not have a well-formed
sub-tree because some of its units attach to the
left (4-5, 6) and some to the right (7). Vliet and
Redeker (2011) call these cases as ?leaky? bound-
aries. Even though less than 5% of the sentences
have leaky boundaries in RST-DT, in other corpora
this can be true for a larger portion of the sen-
tences. For example, we observe over 12% sen-
tences with leaky boundaries in the Instructional
corpus of (Subba and Di-Eugenio, 2009). How-
ever, we notice that in most cases where discourse
structures violate sentence boundaries, its units are
merged with the units of its adjacent sentences, as
in Figure 7(b). For example, this is true for 75%
cases in our development set containing 20 news
articles from RST-DT and for 79% cases in our
development set containing 20 how-to-do manuals
from the Instructional corpus. Based on this obser-
vation, we propose a sliding window approach.
In this approach, our intra-sentential parser
works with a window of two consecutive sen-
tences, and builds a DT for the two sentences. For
example, given the three sentences in Figure 7, our
intra-sentential parser constructs a DT for S1-S2
and a DT for S2-S3. In this process, each sentence
in a document except the first and the last will be
associated with two DTs: one with the previous
sentence (say DTp) and one with the next (say
DTn). In other words, for each non-boundary sen-
tence, we will have two decisions: one from DTp
and one from DTn. Our parser consolidates the
two decisions and generates one or more sub-trees
for each sentence by checking the following three
mutually exclusive conditions one after another:
? Same in both: If the sentence has the same (in
terms of both structure and labels) well-formed
sub-tree in both DTp and DTn, we take this sub-
tree for the sentence. For example, in Figure 8(a),
S2 has the same sub-tree in the two DTs, i.e. a DT
for S1-S2 and a DT for S2-S3. The two decisions
agree on the DT for the sentence.
? Different but no cross: If the sentence has a
well-formed sub-tree in both DTp and DTn, but
the two sub-trees vary either in structure or in la-
bels, we pick the most probable one. For example,
consider the DT for S1-S2 in Figure 8(a) and the
DT for S2-S3 in Figure 8(b). In both cases S2 has
a well-formed sub-tree, but they differ in structure.
We pick the sub-tree which has the higher proba-
bility in the two dynamic programming tables.
1     2  3S1 8  9   10S34   5      6   7S2
1    2   3S1 8   9    10S3
4   5    6    7S2
(a)
(c)
8  9   10S 34   5    6     7S2 (b)
4   5    6    7S2(i) (ii)
4   5      6   7S2
Figure 8: Extracting sub-trees for S2.
? Cross: If either or both of DTp and DTn seg-
ment the sentence into multiple sub-trees, we pick
the one with more sub-trees. For example, con-
sider the two DTs in Figure 8(c). In the DT for
S1-S2, S2 has three sub-trees (4-5,6,7), whereas
in the DT for S2-S3, it has two (4-6,7). So, we ex-
tract the three sub-trees for S2 from the first DT. If
the sentence has the same number of sub-trees in
both DTp and DTn, we pick the one with higher
probability in the dynamic programming tables.
At the end, the multi-sentential parser takes all
these sentence-level sub-trees for a document, and
builds a full rhetorical parse for the document.
492
6 Experiments
6.1 Corpora
While previous studies on document-level parsing
only report their results on a particular corpus, to
show the generality of our method, we experiment
with texts from two very different genres. Our
first corpus is the standard RST-DT (Carlson et
al., 2002), which consists of 385 Wall Street Jour-
nal articles, and is partitioned into a training set
of 347 documents and a test set of 38 documents.
53 documents, selected from both sets were anno-
tated by two annotators, based on which we mea-
sure human agreement. In RST-DT, the original 25
rhetorical relations defined by (Mann and Thomp-
son, 1988) are further divided into a set of 18
coarser relation classes with 78 finer-grained rela-
tions. Our second corpus is the Instructional cor-
pus prepared by (Subba and Di-Eugenio, 2009),
which contains 176 how-to-do manuals on home-
repair. The corpus was annotated with 26 informa-
tional relations (e.g., Preparation-Act, Act-Goal).
6.2 Experimental Setup
We experiment with our discourse parser on the
two datasets using our two different parsing ap-
proaches, namely 1S-1S and the sliding window.
We compare our approach with HILDA (Hernault
et al, 2010) on RST-DT, and with the ILP-based
approach of (Subba and Di-Eugenio, 2009) on the
Instructional corpus, since they are the state-of-
the-art on the respective genres. On RST-DT, the
standard split was used for training and testing
purposes. The results for HILDA were obtained
by running the system with default settings on the
same inputs we provided to our system. Since we
could not run the ILP-based system of (Subba and
Di-Eugenio, 2009) (not publicly available) on the
Instructional corpus, we report the performances
presented in their paper. They used 151 documents
for training and 25 documents for testing. Since
we did not have access to their particular split,
we took 5 random samples of 151 documents for
training and 25 documents for testing, and report
the average performance over the 5 test sets.
To evaluate the parsing performance, we use
the standard unlabeled (i.e., hierarchical spans)
and labeled (i.e., nuclearity and relation) preci-
sion, recall and F-score as described in (Marcu,
2000b). To compare with previous studies, our
experiments on RST-DT use the 18 coarser rela-
tions. After attaching the nuclearity statuses (NS,
SN, NN) to these relations, we get 41 distinct re-
lations. Following (Subba and Di-Eugenio, 2009)
on the Instructional corpus, we use 26 relations,
and treat the reversals of non-commutative rela-
tions as separate relations. That is, Goal-Act and
Act-Goal are considered as two different relations.
Attaching the nuclearity statuses to these relations
gives 76 distinct relations. Analogous to previous
studies, we map the n-ary relations (e.g., Joint)
into nested right-branching binary relations.
6.3 Results and Error Analysis
Table 2 presents F-score parsing results for our
parsers and the existing systems on the two cor-
pora.2 On both corpora, our parser, namely, 1S-1S
(TSP 1-1) and sliding window (TSP SW), outper-
form existing systems by a wide margin (p<7.1e-
05).3 On RST-DT, our parsers achieve absolute
F-score improvements of 8%, 9.4% and 11.4%
in span, nuclearity and relation, respectively, over
HILDA. This represents relative error reductions
of 32%, 23% and 21% in span, nuclearity and rela-
tion, respectively. Our results are also close to the
upper bound, i.e. human agreement on this corpus.
On the Instructional genre, our parsers deliver
absolute F-score improvements of 10.5%, 13.6%
and 8.14% in span, nuclearity and relations, re-
spectively, over the ILP-based approach. Our
parsers, therefore, reduce errors by 36%, 27% and
13% in span, nuclearity and relations, respectively.
If we compare the performance of our parsers
on the two corpora, we observe higher results
on RST-DT. This can be explained in at least
two ways. First, the Instructional corpus has a
smaller amount of data with a larger set of rela-
tions (76 when nuclearity attached). Second, some
frequent relations are (semantically) very similar
(e.g., Preparation-Act, Step1-Step2), which makes
it difficult even for the human annotators to distin-
guish them (Subba and Di-Eugenio, 2009).
Comparison between our two models reveals
that TSP SW significantly outperforms TSP 1-1
only in finding the right structure on both corpora
(p<0.01). Not surprisingly, the improvement is
higher on the Instructional corpus. A likely ex-
planation is that the Instructional corpus contains
more leaky boundaries (12%), allowing the sliding
2Precision, Recall and F-score are the same when manual
segmentation is used (see Marcu, (2000b), page 143).
3Since we did not have access to the output or to the sys-
tem of (Subba and Di-Eugenio, 2009), we were not able to
perform a significance test on the Instructional corpus.
493
RST-DT Instructional
Metrics HILDA TSP 1-1 TSP SW Human ILP TSP 1-1 TSP SW
Span 74.68 82.47* 82.74*? 88.70 70.35 79.67 80.88?
Nuclearity 58.99 68.43* 68.40* 77.72 49.47 63.03 63.10
Relation 44.32 55.73* 55.71* 65.75 35.44 43.52 43.58
Table 2: Parsing results of different models using manual (gold) segmentation. Performances significantly superior to HILDA
(with p<7.1e-05) are denoted by *. Significant differences between TSP 1-1 and TSP SW (with p<0.01) are denoted by ?.
T-CT-OT-CMM-MCMPEVSUCNDENCATEEXBACOJOS-UATEL
T-C  T-O  T-CM      M-M     CMP  EV  SU     CND  EN     CA  TE    EX     BA   CO      JO   S-U   AT     EL10732111227114121291309359
00010203133350012722
0000100000001018532
 0020121007936757108
0000000302112332000
0001300102901921001
0001320004112121008
000000000070431001
000010001305111006
00000000242210000014
000000800000001000
0000100221010122010
0001010000011110000
000040000000020000
000000000000000000
001000000000000000
020000000000000001
000000000000000000
Figure 9: Confusion matrix for relation labels on the
RST-DT test set. Y-axis represents true and X-axis repre-
sents predicted relations. The relations are Topic-Change
(T-C), Topic-Comment (T-CM), Textual Organization (T-
O), Manner-Means (M-M), Comparison (CMP), Evaluation
(EV), Summary (SU), Condition (CND), Enablement (EN),
Cause (CA), Temporal (TE), Explanation (EX), Background
(BA), Contrast (CO), Joint (JO), Same-Unit (S-U), Attribu-
tion (AT) and Elaboration (EL).
window approach to be more effective in finding
those, without inducing much noise for the labels.
This clearly demonstrates the potential of TSP SW
for datasets with even more leaky boundaries e.g.,
the Dutch (Vliet and Redeker, 2011) and the Ger-
man Potsdam (Stede, 2004) corpora.
Error analysis reveals that although TSP SW
finds more correct structures, a corresponding im-
provement in labeling relations is not present be-
cause in a few cases, it tends to induce noise from
the neighboring sentences for the labels. For ex-
ample, when parsing was performed on the first
sentence in Figure 1 in isolation using 1S-1S, our
parser rightly identifies the Contrast relation be-
tween EDUs 2 and 3. But, when it is considered
with its neighboring sentences by the sliding win-
dow, the parser labels it as Elaboration. A promis-
ing strategy to deal with this and similar problems
that we plan to explore in future, is to apply both
approaches to each sentence and combine them by
consolidating three probabilistic decisions, i.e. the
one from 1S-1S and the two from sliding window.
To further analyze the errors made by our parser
on the hardest task of relation labeling, Figure 9
presents the confusion matrix for TSP 1-1 on the
RST-DT test set. The relation labels are ordered
according to their frequency in the RST-DT train-
ing set. In general, the errors are produced by two
different causes acting together: (i) imbalanced
distribution of the relations, and (ii) semantic sim-
ilarity between the relations. The most frequent
relation Elaboration tends to mislead others es-
pecially, the ones which are semantically similar
(e.g., Explanation, Background) and less frequent
(e.g., Summary, Evaluation). The relations which
are semantically similar mislead each other (e.g.,
Temporal:Background, Cause:Explanation).
These observations suggest two ways to im-
prove our parser. We would like to employ a more
robust method (e.g., ensemble methods with bag-
ging) to deal with the imbalanced distribution of
relations, along with taking advantage of a richer
semantic knowledge (e.g., compositional seman-
tics) to cope with the errors caused by semantic
similarity between the rhetorical relations.
7 Conclusion
In this paper, we have presented a novel discourse
parser that applies an optimal parsing algorithm
to probabilities inferred from two CRF models:
one for intra-sentential parsing and the other for
multi-sentential parsing. The two models exploit
their own informative feature sets and the distribu-
tional variations of the relations in the two parsing
conditions. We have also presented two novel ap-
proaches to combine them effectively. Empirical
evaluations on two different genres demonstrate
that our approach yields substantial improvement
over existing methods in discourse parsing.
Acknowledgments
We are grateful to Frank Tompa and the anony-
mous reviewers for their comments, and the
NSERC BIN and CGS-D for financial support.
494
References
O. Biran and O. Rambow. 2011. Identifying Justi-
fications in Written Dialogs by Classifying Text as
Argumentative. International Journal of Semantic
Computing, 5(4):363?381.
L. Carlson, D. Marcu, and M. Okurowski. 2002. RST
Discourse Treebank (RST-DT) LDC2002T07. Lin-
guistic Data Consortium, Philadelphia.
V. Feng and G. Hirst. 2012. Text-level Discourse Pars-
ing with Rich Linguistic Features. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, ACL ?12, pages 60?68,
Jeju Island, Korea. Association for Computational
Linguistics.
S. Fisher and B. Roark. 2007. The Utility of Parse-
derived Features for Automatic Discourse Segmen-
tation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics, ACL
?07, pages 488?495, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
M. Galley and K. McKeown. 2003. Improving Word
Sense Disambiguation in Lexical Chaining. In Pro-
ceedings of the 18th International Joint Conference
on Artificial Intelligence, IJCAI ?07, pages 1486?
1488, Acapulco, Mexico.
H. Hernault, H. Prendinger, D. duVerle, and
M. Ishizuka. 2010. HILDA: A Discourse Parser
Using Support Vector Machine Classification. Dia-
logue and Discourse, 1(3):1?33.
S. Joty, G. Carenini, and R. T. Ng. 2012. A Novel
Discriminative Framework for Sentence-Level Dis-
course Analysis. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, EMNLP-CoNLL ?12, pages 904?
915, Jeju Island, Korea. Association for Computa-
tional Linguistics.
H. LeThanh, G. Abeysinghe, and C. Huyck. 2004.
Generating Discourse Structures for Written Texts.
In Proceedings of the 20th international confer-
ence on Computational Linguistics, COLING ?04,
Geneva, Switzerland. Association for Computa-
tional Linguistics.
A. Louis, A. Joshi, and A. Nenkova. 2010. Discourse
Indicators for Content Selection in Summarization.
In Proceedings of the 11th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
SIGDIAL ?10, pages 147?156, Tokyo, Japan. Asso-
ciation for Computational Linguistics.
W. Mann and S. Thompson. 1988. Rhetorical Struc-
ture Theory: Toward a Functional Theory of Text
Organization. Text, 8(3):243?281.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?02, pages
368?375. Association for Computational Linguis-
tics.
D. Marcu. 2000a. The Rhetorical Parsing of Unre-
stricted Texts: A Surface-based Approach. Compu-
tational Linguistics, 26:395?448.
D. Marcu. 2000b. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press,
Cambridge, MA, USA.
J. Morris and G. Hirst. 1991. Lexical Cohesion
Computed by Thesaural Relations as an Indicator
of Structure of Text. Computational Linguistics,
17(1):21?48.
R. Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki,
and B. Webber. 2005. The Penn Discourse Tree-
Bank as a Resource for Natural Language Gener-
ation. In Proceedings of the Corpus Linguistics
Workshop on Using Corpora for Natural Language
Generation, pages 25?32, Birmingham, U.K.
S. Somasundaran, 2010. Discourse-Level Relations for
Opinion Analysis. PhD thesis, University of Pitts-
burgh.
R. Soricut and D. Marcu. 2003. Sentence Level
Discourse Parsing Using Syntactic and Lexical In-
formation. In Proceedings of the 2003 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics on Human Lan-
guage Technology, NAACL-HLT ?03, pages 149?
156, Edmonton, Canada. Association for Computa-
tional Linguistics.
C. Sporleder and M. Lapata. 2005. Discourse Chunk-
ing and its Application to Sentence Compression.
In Proceedings of the conference on Human Lan-
guage Technology and Empirical Methods in Nat-
ural Language Processing, pages 257?264, Van-
couver, British Columbia, Canada. Association for
Computational Linguistics.
C. Sporleder and A. Lascarides. 2004. Combining Hi-
erarchical Clustering and Machine Learning to Pre-
dict High-Level Discourse Structure. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Geneva, Switzer-
land. Association for Computational Linguistics.
M. Stede. 2004. The Potsdam Commentary Corpus.
In Proceedings of the ACL-04 Workshop on Dis-
course Annotation, Barcelona. Association for Com-
putational Linguistics.
R. Subba and B. Di-Eugenio. 2009. An Effective Dis-
course Parser that Uses Rich Linguistic Information.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT-NAACL ?09, pages 566?574, Boul-
der, Colorado. Association for Computational Lin-
guistics.
495
C. Sutton and A. McCallum. 2012. An Introduction
to Conditional Random Fields. Foundations and
Trends in Machine Learning, 4(4):267?373.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic Conditional Random Fields: Factorized
Probabilistic Models for Labeling and Segmenting
Sequence Data. Journal of Machine Learning Re-
search (JMLR), 8:693?723.
S. Verberne, L. Boves, N. Oostdijk, and P. Coppen.
2007. Evaluating Discourse-based Answer Extrac-
tion for Why-question Answering. In Proceedings
of the 30th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 735?736, Amsterdam, The Nether-
lands. ACM.
N. Vliet and G. Redeker. 2011. Complex Sentences as
Leaky Units in Discourse Parsing. In Proceedings
of Constraints in Discourse, Agay-Saint Raphael,
September.
496
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1220?1230,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Abstractive Summarization of Spoken and Written Conversations
Based on Phrasal Queries
Yashar Mehdad Giuseppe Carenini Raymond T. Ng
Department of Computer Science, University of British Columbia
Vancouver, BC, V6T 1Z4, Canada
{mehdad, carenini, rng}@cs.ubc.ca
Abstract
We propose a novel abstractive query-
based summarization system for conversa-
tions, where queries are defined as phrases
reflecting a user information needs. We
rank and extract the utterances in a con-
versation based on the overall content and
the phrasal query information. We clus-
ter the selected sentences based on their
lexical similarity and aggregate the sen-
tences in each cluster by means of a word
graph model. We propose a ranking strat-
egy to select the best path in the con-
structed graph as a query-based abstract
sentence for each cluster. A resulting sum-
mary consists of abstractive sentences rep-
resenting the phrasal query information
and the overall content of the conversa-
tion. Automatic and manual evaluation
results over meeting, chat and email con-
versations show that our approach signifi-
cantly outperforms baselines and previous
extractive models.
1 Introduction
Our lives are increasingly reliant on multimodal
conversations with others. We email for business
and personal purposes, attend meetings in per-
son, chat online, and participate in blog or forum
discussions. While this growing amount of per-
sonal and public conversations represent a valu-
able source of information, going through such
overwhelming amount of data, to satisfy a partic-
ular information need, often leads to an informa-
tion overload problem (Jones et al, 2004). Au-
tomatic summarization has been proposed in the
past as a way to address this problem (e.g., (Sakai
and Sparck-Jones, 2001)). However, often a good
summary cannot be generic and should be a brief
and well-organized paragraph that answer a user?s
information need.
The Document Understanding Conference
(DUC)
1
has launched query-focused multidocu-
ment summarization as its main task since 2004,
by focusing on complex queries with very specific
answers. For example, ?How were the bombings
of the US embassies in Kenya and Tanzania
conducted? How and where were the attacks
planned??. Such complex queries are appropriate
for a user who has specific information needs and
can formulate the questions precisely. However,
especially when dealing with conversational data
that tend to be less structured and less topically
focused, a user is often initially only exploring the
source documents, with less specific information
needs. Moreover, following the common practice
in search engines, users are trained to form
simpler and shorter queries (Meng and Yu, 2010).
For example, when a user is interested in certain
characteristics of an entity in online reviews (e.g.,
?location? or ?screen?) or a specific entity in a
blog discussion (e.g., ?new model of iphone?), she
would not initially compose a complex query.
To address these issues, in this work, we tackle
the task of conversation summarization based on
phrasal queries. We define a phrasal query as a
concatenation of two or more keywords, which is
a more realistic representation of a user?s informa-
tion needs. For conversational data, this definition
is more similar to the concept of search queries in
information retrieval systems as well as to the con-
cept of topic labels in the task of topic modeling.
Example 1 shows two queries and their associated
human written summaries based on a single chat
log. We can observe that the two summaries, al-
though generated from the same chat log, are to-
tally distinct. This further demonstrates the impor-
tance of phrasal query-based summarization sys-
tems for long conversations.
To date, most systems in the area of summa-
1
http://www-nlpir.nist.gov/projects/duc/index.html
1220
Query-1: Test/Sample database for GNUe
Abstract-1: James Thompson asked Reinhard: I was going to work
on the sample tonight. You mentioned wanting a fishhook and all data
types. Any other things you want to see in there? Reinhard said that
master/detail would be good, as there have been bugs only appearing in
3-level case. James said he already included that and I know I need to
add a boolean. Did you want date as well as date-time? Reinhard said
yes - we also have time values (time without date). They are especially
interesting. James had not ever had use for something like that so I?m
not sure where I would graft that in.
Query-2: Passing parameters to Forms
Abstract-2: James Thompson (jamest) asked how did parameter sup-
port in forms change recently? He reported the trigger namespace func-
tion referencesGFForm.parameters - which no longer exists. Reinhard
said every GFForm should have a parameters. James said he was using
parameters in on-startup. Reinhard said that?s probably the only place
where they don?t work. James said that I?m thinking about moving that
to on-activation instead of on-startup anyway as it should still work for
a main form - but i still wonder if the on-startup parameter issue should
be considered a bug - as it shouldn?t choke. Reinhard was sure it should
be considered a bug but I have no idea how to fix it. We haven?t found a
way to deal with parameters that works for every case. I don?t know if
there is any chance to pass the parameters to the form before it is acti-
vated. James asked how are parameters handled now? Reinhard replied
that they are passed to activateForm so they are available from activa-
tion for the ?main? form, the command line parameters are passed and
for dialogs, the parameters are passed that were given in runDialog.
Example 1: Sample queries and associated
human-written query-based summaries for a chat
log.
rization focus on news or other well-written docu-
ments, while research on summarizing multiparty
written conversations (e.g., chats, emails) has been
limited. This is because traditional NLP ap-
proaches developed for formal texts often are not
satisfactory when dealing with multiparty written
conversations, which are typically in a casual style
and do not display a clear syntactic structure with
proper grammar and spelling. Even though some
works try to address the problem of summarizing
multiparty written conversions (e.g., (Mehdad et
al., 2013b; Wang and Cardie, 2013; Murray et
al., 2010; Zhou and Hovy, 2005; Gillick et al,
2009)), they do so in a generic way (not query-
based) and focus on only one conversational do-
main (e.g., meetings). Moreover, most of the pro-
posed systems for conversation summarization are
extractive.
To address such limitations, we propose a fully
automatic unsupervised abstract generation frame-
work based on phrasal queries for multimodal con-
versation summarization. Our key contributions in
this work are as follows:
1) To the best of our knowledge, our framework
is the first abstractive system that generates sum-
maries based on users phrasal queries, instead of
well-formed questions. As a by-product of our
approach, we also propose an extractive summa-
rization model based on phrasal queries to select
the summary-worthy sentences in the conversation
based on query terms and signature terms (Lin and
Hovy, 2000).
2) We propose a novel ranking strategy to select
the best path in the constructed word graph by tak-
ing the query content, overall information content
and grammaticality (i.e., fluency) of the sentence
into consideration.
3) Although most of the current summarization
approaches use supervised algorithms as a part
of their system (e.g., (Wang et al, 2013)), our
method can be totally unsupervised and does not
depend on human annotation.
4) Although different conversational modali-
ties (e.g., email vs. chat vs. meeting) underline
domain-specific characteristics, in this work, we
take advantage of their underlying similarities to
generalize away from specific modalities and de-
termine effective method for query-based summa-
rization of multimodal conversations.
We evaluate our system over GNUe Traffic
archive
2
Internet Relay Chat (IRC) logs, AMI
meetings corpus (Carletta et al, 2005) and BC3
emails dataset (Ulrich et al, 2008). Automatic
evaluation on the chat dataset and manual eval-
uation over the meetings and emails show that
our system uniformly and statistically significantly
outperforms baseline systems, as well as a state-
of-the-art query-based extractive summarization
system.
2 Phrasal Query Abstraction
Framework
Our phrasal query abstraction framework gener-
ates a grammatical abstract from a conversation
following three steps, as shown in Figure 1.
2.1 Utterance Extraction
Abstractive summary sentences can be created by
aggregating and merging multiple sentences into
an abstract sentence. In order to generate such
a sentence, we need to identify which sentences
from the original document should be extracted
and combined to generate abstract sentences. In
other words, we want to identify the summary-
worthy sentences in the text that can be combined
into an abstract sentence. This task can be con-
sidered as content selection. Moreover, this step,
stand alone, corresponds to an extractive summa-
rization system.
2
http://kt.earth.li/GNUe/index.html
1221
Original 
conversation  
Query 
Extracted 
utterances 
Filtered 
utterances 
Ex
tra
ctio
n 
Re
dun
dan
cy 
Re
mo
val
 
Ge
ner
atio
n 
Clusters Word graphs 
Top ranked 
sentences Query-based 
abstract 
Clustering Word Graph Ranking Construction 
Figure 1: Phrasal query abstraction framework. The steps (arrows) influenced by the query are high-
lighted.
Signature terms: navigator, functionality, reports, UI, schema, gnu
Chat log:
- but watching them build a UI in the flash demo?s is pretty damn im-
pressive... and have started moving my sales app to all UI being built
via ...
- i?ll be expanding the technotes in navigator for a while ...
- ... in terms of functionality of the underlying databases ...
- you mean if I start GNU again I have to read bug reports too?
- no, just in case you want to enter bug report
- ...I expand the schema before populating with test data ...
- i?m willing to scrap it if there is a better schema hidden in gnue some-
where :)
Example 2: Sample signature terms for a part of a
chat log.
In order to select and extract the informative
summary-worthy utterances, based on the phrasal
query and the original text, we consider two cri-
teria: i) utterances should carry the essence of the
original text; and ii) utterances should be relevant
to the query. To fulfill such requirements we define
the concepts of signature terms and query terms.
2.1.1 Signature Terms
Signature terms are generally indicative of the
content of a document or collection of docu-
ments. To identify such terms, we can use fre-
quency, word probability, standard statistic tests,
information-theoretic measures or log-likelihood
ratio. In this work, we use log-likelihood ratio to
extract the signature terms from chat logs, since
log-likelihood ratio leads to better results (Gupta
et al, 2007). We use a method described in (Lin
and Hovy, 2000) in order to identify such terms
and their associated weight. Example 2 demon-
strates a chat log and associated signature terms.
2.1.2 Query Terms
Query terms are indicative of the content in a
phrasal query. In order to identify such terms,
we first extract all content terms from the query.
Then, following previous studies (e.g., (Gonzalo
et al, 1998)), we use the synsets relations in Word-
Net for query expansion. We extract all concepts
that are synonyms to the query terms and add
them to the original set of query terms. Note that
we limit our synsets to the nouns since verb syn-
onyms do not prove to be effective in query ex-
pansion (Hunemark, 2010). While signature terms
are weighted, we assume that all query terms are
equally important and they all have wight equal to
1.
2.1.3 Utterance Scoring
To estimate the utterance score, we view both
the query terms and the signature terms as the
terms that should appear in a human query-based
summary. To achieve this, the most relevant
(summary-worthy) utterances that we select are
the ones that maximize the coverage of such terms.
Given the query terms and signature terms, we can
estimate the utterance score as follows:
Score
Q
=
1
n
n
?
i=1
t(q)
i
(1)
Score
S
=
1
n
n
?
i=1
t(s)
i
? w(s)
i
(2)
Score = ? ? Score
Q
+ ? ? Score
S
(3)
where n is number of content words in the ut-
terance, t(q)
i
= 1 if the term t
i
is a query term
and 0 otherwise, and t(s)
i
= 1 if t
i
is a signature
term and 0 otherwise, and w(s)
i
is the normalized
associated weight for signature terms. The param-
eters ? and ? are tuned on a development set and
sum up to 1.
After all the utterances are scored, the top
scored utterances are selected to be sent to the next
step. We estimate the percentage of the retrieved
utterances based on the development set.
1222
2.2 Redundancy Removal
Utterances selected in previous step often in-
clude redundant information, which is semanti-
cally equivalent but may vary in lexical choices.
By identifying the semantic relations between the
sentences, we can discover what information in
one sentence is semantically equivalent, novel, or
more/less informative with respect to the content
of the other sentences. Similar to earlier work
(Berant et al, 2011; Adler et al, 2012), we set
this problem as a variant of the Textual Entail-
ment (TE) recognition task (Dagan and Glickman,
2004). Using entailment in this phase is moti-
vated by taking advantage of semantic relations
instead of pure statistical methods (e.g., Maximal
Marginal Relevance) and shown to be more effec-
tive (Mehdad et al, 2013a). We follow the same
practice as (Mehdad et al, 2013a) to build an en-
tailment graph for all selected sentences to identify
relevant sentences and eliminate the redundant (in
terms of meaning) and less informative ones.
2.3 Abstract Generation
In this phase, our goal is to generate understand-
able informative abstract sentences that capture
the content of the source sentences and represents
the information needs defined by queries. There
are several ways of generating abstract sentences
(e.g. (Barzilay and McKeown, 2005; Liu and Liu,
2009; Ganesan et al, 2010; Murray et al, 2010));
however, most of them rely heavily on the sen-
tence structure. We believe that such approaches
are suboptimal, especially in dealing with conver-
sational data, because multiparty written conversa-
tions are often poorly structured. Instead, we ap-
ply an approach that does not rely on syntax, nor
on a standard NLG architecture. Moreover, since
dealing with user queries efficiency is an impor-
tant aspect, we aim for an approach that is also
motivated by the speed with which the abstracts
are obtained. We perform the task of abstract gen-
eration in three steps, as follows:
2.3.1 Clustering
In order to generate an abstract summary, we need
to identify which sentences from the previous step
(i.e., redundancy removal) can be clustered and
combined in generated abstract sentences. This
task can be viewed as sentence clustering, where
each sentence cluster can provide the content for
an abstract sentence.
We use the K-mean clustering algorithm by co-
sine similarity as a distance function between sen-
tence vectors composed of tf.idf scores. Also no-
tice that the lexical similarity between sentences in
one cluster facilitates both the construction of the
word graph and finding the best path in the word
graph, as described next.
2.3.2 Word Graph
In order to construct a word graph, we adopt
the method recently proposed by (Mehdad et al,
2013a; Filippova, 2010) with some optimizations.
Below, we show how the word graph is applied to
generate the abstract sentences.
Let G = (W,L) be a directed graph with the
set of nodes W representing words and a set of
directed edges L representing the links between
words. Given a cluster of related sentences S =
{s
1
, s
2
, ..., s
n
}, a word graph is constructed by it-
eratively adding sentences to it. In the first step,
the graph represents one sentence plus the start
and end symbols. A node is added to the graph for
each word in the sentence, and words adjacent are
linked with directed edges. When adding a new
sentence, a word from the sentence is merged in
an existing node in the graph providing that they
have the same POS tag and they satisfy one of the
following conditions:
i) They have the same word form;
ii) They are connected in WordNet by the syn-
onymy relation. In this case the lexical choice for
the node is selected based on the tf.idf score of
each node;
iii) They are from a hypernym/hyponym pair or
share a common direct hypernym. In this case,
both words are replaced by the hypernym;
iv) They are in an entailment relation. In this
case, the entailing word is replaced by the entailed
one.
The motivation behind merging non-identical
words is to enrich the common terms between
the phrases to increase the chance that they could
merge into a single phrase. This also helps to
move beyond the limitation of original lexical
choices. In case the merging is not possible a
new node is created in the graph. When a node
can be merged with multiple nodes (i.e., merging
is ambiguous), either the preceding and following
words in the sentence and the neighboring nodes
in the graph or the frequency is used to select the
candidate node.
We connect adjacent words with directed edges.
1223
For the new nodes or unconnected nodes, we draw
an edge with a weight of 1. In contrast, when two
already connected nodes are added (merged), the
weight of their connection is increased by 1.
2.3.3 Path Ranking
A word graph, as described above, may contain
many sequences connecting start and end. How-
ever, it is likely that most of the paths are not read-
able. We are aiming at generating an informative
abstractive sentence for each cluster based on a
user query. Moreover, the abstract sentence should
be grammatically correct.
In order to satisfy both requirements, we have
devised the following ranking strategy. First, we
prune the paths in which a verb does not exist,
to filter ungrammatical sentences. Then we rank
other paths as follows:
Query focus: to identify the summary sentence
with the highest coverage of query content, we
propose a score that counts the number of query
terms that appear in the path. In order to reward
the ranking score to cover more salient terms in
the query content, we also consider the tf.idf score
of query terms in the coverage formulation.
Q(P ) =
?
q
i
?P
tfidf (q
i
)
?
q
i
?G
tfidf (q
i
)
where the q
i
are the query terms.
Fluency: in order to improve the grammaticality
of the generated sentence, we coach our ranking
model to select more fluent (i.e., grammatically
correct) paths in the graph. We estimate the gram-
maticality of generated paths (Pr(P )) using a lan-
guage model.
Path weight: The purpose of this function is two-
fold: i) to generate a grammatical sentence by fa-
voring the links between nodes (words) which ap-
pear often; and ii) to generate an informative sen-
tence by increasing the weight of edges connecting
salient nodes. For a path P with m nodes, we de-
fine the edge weightw(n
i
, n
j
) and the path weight
W (P ) as below:
w(n
i
, n
j
) =
freq(n
i
) + freq(n
j
)
?
P
?
?G
n
i
,n
j
?P
?
diff (P
?
, n
i
, n
j
)
?1
W (P ) =
?
m?1
i=1
w(n
i
, n
i+1
)
m? 1
where the function diff(P
?
, n
i
, n
j
) refers to the
distance between the offset positions pos(P
?
, n
i
)
of nodes n
i
and n
j
in path P
?
(any path in G con-
taining n
i
and n
j
) and is defined as |pos(P
?
, n
j
)?
pos(P
?
, n
i
)|.
Overal ranking score: In order to generate a
query-based abstract sentence that combines the
scores above, we employ a ranking model. The
purpose of such a model is three-fold: i) to cover
the content of query information optimally; ii) to
generate a more readable and grammatical sen-
tence; and iii) to favor strong connections between
the concepts. Therefore, the final ranking score of
path P is calculated over the normalized scores as:
Score(P ) = ? ?Q(P ) + ? ? Pr(P )? ? ?W (P )
Where ?, ? and ? are the coefficient factors to
tune the ranking score and they sum up to 1. In or-
der to rank the graph paths, we select all the paths
that contain at least one verb and rerank them us-
ing our proposed ranking function to find the best
path as the summary of the original sentences in
each cluster.
3 Experimental Setup
In this section, we show the evaluation results of
our proposed framework and its comparison to the
baselines and a state-of-the-art query-focused ex-
tractive summarization system.
3.1 Datasets
One of the challenges of this work is to find suit-
able conversational datasets that can be used for
evaluating our query-based summarization sys-
tem. Most available conversational corpora do not
contain any human written summaries, or the gold
standard human written summaries are generic
(Carletta et al, 2005; Joty et al, 2013). In this
work, we use available corpora for emails and
chats for written conversations, while for spoken
conversation, we employ an available corpus in
multiparty meeting conversations.
Chat: to the best of our knowledge, the only pub-
licly available chat logs with human written sum-
maries can be downloaded from the GNUe Traffic
archive (Zhou and Hovy, 2005; Uthus and Aha,
2011; Uthus and Aha, 2013). Each chat log has
a human created summary in the form of a digest.
Each digest summarizes IRC logs for a period and
consists of few summaries over each chat log with
a unique title for the associated human written
summary. In this way, the title of each summary
1224
can be counted as a phrasal query and the cor-
responding summary is considered as the query-
based abstract of the associated chat log includ-
ing only the information most relevant to the title.
Therefore, we can use the human-written query-
based abstract as gold standards and evaluate our
system automatically. Our chat dataset consists of
66 query-based (title-based) human written sum-
maries with their associated queries (titles) and
chat logs, created from 40 original chat logs. The
average number of tokens are 1840, 325 and 6 for
chat logs, query-based summaries and queries, re-
spectively.
Meeting: we use the AMI meeting corpus (Car-
letta et al, 2005) that consists of 140 multiparty
meetings with a wide range of annotations, includ-
ing generic abstractive summaries for each meet-
ing. In order to create queries, we extract three
key-phrases from generic abstractive summaries
using TextRank algorithm (Mihalcea and Tarau,
2004). We use the extracted key-phrases as queries
to generate query-based abstracts. Since there is
no human-written query-based summary for AMI
corpus, we randomly select 10 meetings and eval-
uate our system manually.
Email: we use BC3 (Ulrich et al, 2008), which
contains 40 threads from the W3C corpus. BC3
corpus is annotated with generic human-written
abstractive summaries, and it has been used in sev-
eral previous works (e.g., (Joty et al, 2011)). In
order to adapt this corpus to our framework, we
followed the same query generation process as for
the meeting dataset. Finally, we randomly select
10 emails threads and evaluate the results manu-
ally.
3.2 Baselines
We compare our approach with the following
baselines:
1) Cosine-1st: we rank the utterances in the chat
log based on the cosine similarity between the ut-
terance and query. Then, we select the first ut-
trance as the summary;
2) Cosine-all: we rank the utterances in the chat
log based on the cosine similarity between the ut-
terance and query and then select the utterances
with a cosine similarity greater than 0;
3) TextRank: a widely used graph-based rank-
ing model for single-document sentence extraction
that works by building a graph of all sentences in a
document and use similarity as edges to compute
the salience of sentences in the graph (Mihalcea
and Tarau, 2004);
4) LexRank: another popular graph-based con-
tent selection algorithm for multi-document sum-
marization (Erkan and Radev, 2004);
5) Biased LexRank: is a state-of-the-art query-
focused summarization that uses LexRank algo-
rithm in order to recursively retrieve additional
passages that are similar to the query, as well as
to the other nodes in the graph (Otterbacher et al,
2009).
Moreover, we compare our abstractive system
with the first part of our framework (utterance ex-
traction in Figure 1), which can be presented as an
extractive query-based summarization system (our
extractive system). We also show the results of the
version we use in our pipeline (our pipeline ex-
tractive system). The only difference between the
two versions is the length of the generated sum-
maries. In our pipeline we aim at higher recall,
since we later filter sentences and aggregate them
to generate new abstract sentences. In contrast,
in the stand alone version (extractive system) we
limit the number of retrieved sentences to the de-
sired length of the summary. We also compare the
results of our full system (i.e., with tuning) with
a non-optimized version when the ranking coef-
ficients are distributed equally (? = ? = ? =
0.33). For parameters estimation, we tune all pa-
rameters (utterance selection and path ranking) ex-
haustively with 0.1 intervals using our develop-
ment set.
For manual evaluation of query-based abstracts
(meeting and email datasets), we perform a sim-
ple user study assessing the following aspects: i)
Overall quality given a query (5-point scale)?; and
ii) Responsiveness: how responsive is the gener-
ated summary to the query (5-point scale)? Each
query-based abstract was rated by two annotators
(native English speaker). Evaluators are presented
with the original conversation, query and gener-
ated summary. For the manual evaluation, we
only compare our full system with LexRank (LR)
and Biased LexRank (Biased LR). We also ask
the evaluators to select the best summary for each
query and conversation, given our system gener-
ated summary and the two baselines.
To evaluate the grammaticality of our generated
summaries, following common practice (Barzilay
and McKeown, 2005), we randomly selected 50
sentences from original conversations and system
1225
Models ROUGE-1 (%) ROUGE-2 (%)
Prc Rec F-1 Prc Rec F-1
Cosine-1st 71 5 8 30 3 5
Cosine-all 30 68 38 18 40 22
TextRank 25 76 34 15 44 20
LexRank 36 50 37 14 20 15
Biased LexRank 36 51 38 15 21 16
Utterance extraction (our extractive system) 34 66
?
40
??
20
??
40
?
24
??
Utterance extraction (our pipeline extractive system) 30 73
?
38 19
??
44
?
24
??
Our abstractive system (without tuning) 38
?
59
?
41
??
18
?
27
?
19
?
Our abstractive system (with tuning) 40
??
56
?
42
??
20
??
25
?
22
??
Table 1: Performance of different summarization algorithms on chat logs for query-based chat sum-
marization. Statistically significant improvements (p < 0.01) over the biased LexRank system are
marked with *. ? indicates statistical significance (p < 0.01) over extractive approaches (TextRank
and LexRank). Systems in italics use the query in generating the summary.
generated abstracts, for each dataset. Then, we
asked annotators to give one of three possible rat-
ings for each sentence based on grammaticality:
perfect (2 pts), only one mistake (1 pt) and not ac-
ceptable (0 pts), ignoring capitalization or punc-
tuation. Each sentence was rated by two annota-
tors. Note that each sentence was evaluated indi-
vidually, so the human judges were not affected
by intra-sentential problems posed by coreference
and topic shifts.
3.3 Experimental Settings
For preprocessing our dataset we use OpenNLP
3
for tokenization, stemming and part-of-speech
tagging. We use six randomly selected query-
logs from our chat dataset (about 10% of the
dataset) for tuning the coefficient parameters. We
set the k parameter in our clustering phase to
10 based on the average number of sentences
in the human written summaries. For our lan-
guage model, we use a tri-gram smoothed lan-
guage model trained using the newswire text pro-
vided in the English Gigaword corpus (Graff and
Cieri, 2003). For the automatic evaluation we use
the official ROUGE software with standard op-
tions and report ROUGE-1 and ROUGE-2 preci-
sion, recall and F-1 scores.
3.4 Results
3.4.1 Automatic Evaluation (Chat dataset)
Abstractive vs. Extractive: our full query-
based abstractive summariztion system show sta-
tistically significant improvements over baselines
3
http://opennlp.apache.org/
and other pure extractive summarization systems
for ROUGE-1
4
. This means our systems can ef-
fectively aggregate the extracted sentences and
generate abstract sentences based on the query
content. We can also observe that our full system
produces the highest ROUGE-1 precision score
among all models, which further confirms the suc-
cess of this model in meeting the user informa-
tion needs imposed by queries. The absolute im-
provement of 10% in precision for ROUGE-1 in
our abstractive model over our extractive model
(our pipeline) further confirms the effectiveness of
our ranking method in generating the abstract sen-
tences considering the query related information.
Our extractive query-based method beats all
other extractive systems with a higher ROUGE-
1 and ROUGE-2 which shows the effectiveness of
our utterance extraction model in comparison with
other extractive models. In other words, using
our extractive model described in section 2.1, as
a stand alone system, is an effective query-based
extractive summarization model. We also observe
that our extractive model outperforms our abstrac-
tive model for ROUGE-2 score. This can be due
to word merging and word replacement choices
in the word graph construction, which sometimes
change or remove a word in a bigram and conse-
quently may decrease the bigram overlap score.
Query Relevance: another interesting observa-
tion is that relying only on the cosine similarity
(i.e., cosine-all) to measure the query relevance
presents a quite strong baseline. This proves the
importance of query content in our dataset and fur-
ther supports the main claim of our work that a
4
The statistical significance tests was calculated by ap-
proximate randomization, as described in (Yeh, 2000).
1226
Dataset Overal Quality Responsiveness Preference
Our Sys Biased LR LR Our Sys Biased LR LR Our Sys Biased LR LR
Meeting 2.9 2.5 2.1 3.8 3.2 1.8 70% 30% 0%
Email 2.7 1.8 1.7 3.7 3.0 1.5 60% 30% 10%
Table 2: Manual evaluation scores for our phrasal query abstraction system in comparison with Biased
LexRank and LexRank (LR).
Dataset Grammar G=2 G=1 G=0
Orig Sys Orig Sys Orig Sys Orig Sys
Chat 1.8 1.6 84% 73% 16% 24% 0% 3%
Meeting 1.5 1.3 50% 40% 50% 55% 0% 5%
Email 1.9 1.6 85% 60% 15% 35% 0% 5%
Table 3: Average rating and distribution over grammaticality scores for phrasal query abstraction system
in comparison with original sentences.
good summary should express a brief and well-
organized abstract that answers the user?s query.
Moreover, a precision of 71% for ROUGE-1 from
the simple cosine-1st baseline confirms that some
utterances contain more query relevant informa-
tion in conversational discussions.
Query-based vs. Generic: the high recall
and low precision in TextRank baseline, both for
the ROUGE-1 and ROUGE-2 scores, shows the
strength of the model in extracting the generic in-
formation from chat conversations while missing
the query-relevant content. The LexRank baseline
improves the results of the TextRank system by
increasing the precision and balancing the preci-
sion and recall scores for ROUGE-1 score. We
believe that this is due to the robustness of the
LexRank method in dealing with noisy texts (chat
conversations) (Erkan and Radev, 2004). In addi-
tion, the Biased LexRank model slightly improves
the generic LexRank system. Considering this
marginal improvement and relatively high results
of pure extractive systems, we can infer that the
Biased LexRank extracted summaries do not carry
much query relevant content. In contrast, the sig-
nificant improvement of our model over the ex-
tractive methods demonstrates the success of our
approach in presenting the query related content
in generated abstracts.
An example of a short chat log, its related query
and corresponding manual and automatic sum-
maries are shown in Example 3.
3.4.2 Manual Evaluation
Content and User Preference: Table 2 demon-
strates overall quality, responsiveness (query re-
latedness) and user preference scores for the ab-
stracts generated by our system and two base-
lines. Results indicate that our system signif-
icantly outperforms baselines in overall quality
and responsiveness, for both meeting and email
datasets. This confirms the validity of the re-
sults we obtained by conducting automatic evalu-
ation over the chat dataset. We also can observe
that the absolute improvements in overall qual-
ity and responsiveness for emails (0.9 and 0.7) is
greater than for meetings (0.4 and 0.6). This is
expected since dealing with spoken conversations
is more challenging than written ones. Note that
the responsiveness scores are greater than over-
all scores. This further proves the effectiveness of
our approach in dealing with phrasal queries. We
also evaluate the users? summary preferences. For
both datasets (meeting and email), in majority of
cases (70% and 60% respectively), the users prefer
the query-based abstractive summary generated by
our system.
Grammaticality: Table 3 shows grammaticality
scores and distributions over the three possible
scores for all datasets. The chat dataset results
demonstrate the highest scores: 73% of the sen-
tences generated by our phrasal query abstrac-
tion model are grammatically correct and 24% of
the generated sentences are almost correct with
only one grammatical error, while only 3% of
the abstract sentences are grammatically incor-
rect. However, the results varies moving to other
datasets. For meeting dataset, the percentage of
completely grammatical sentences drops dramati-
cally. This is due to the nature of spoken conver-
sations which is more error prone and ungrammat-
ical. The grammaticality score of the original sen-
tences also proves that the sentences from meet-
1227
Query: Trigger namespace and the self property
Chat log:
A: good morning
B: good morning
C: good morning everyone
D: good morning
D: good night all
F: New GNUe Traffic online
F: loadsa deep metaphyisical stuff this week
F: D & E discuss the meaning of ?self? ;-)
E: yes, and he took the more metaphysical route, where I took the more
scientific route
E: I say self?s meaning is derived from one?s ancestry
E: self?s meaning is derived from how others use you
E: okay, analogy extended too far, I guess :)
F: is this a friends vs family debate?
E: also noted that the cool part about that is if you have code that needs
to happen both on a pre-insert and a pre-update - but only a few lines of
the code is different
E: you could have one block of trigger code that used self.action to find
out why it had been called and branch accordingly.
E: there was a big jump from the previous paragraph to that
E: that took that out of context
E: iirc, I was saying an alternative was that ?self? could refer to neither
the trigger?s owner nor to the trigger?s caller
E: but to the event itself
E: so self.action could be what action is being performed
E: self.parent could be the trigger?s owner
E: and self.context could be the caller
E: and self.name could be the name of the trigger
F: umm, I?m afraid apparant non-sequiturs are always a hazard of doing
summaries ;-)
E: iow, I was presenting a 3rd alternative where self referred to some-
thing specific to the trigger
F: I normally try to write around them, but not 100% successful
E: I?m just convulsing my thoughts to the irc log
E: for an errata next week :)
E: I think convulsing is a good description
F: heh
TexRank: There was a big jump from the previous paragraph to that.
that took that out of context iirc, I was saying an alternative was that
?self? could refer to neither the trigger?s owner nor to the trigger?s caller.
but to the event itself. so self.action could be what action is being per-
formed, self.parent could be the trigger?s owner, and self.context could
be the caller and self.name could be the name of the trigger. umm, I?m
afraid apparant non-sequiturs are always a hazard of doing summaries
;-)
LexRank: good morning everyone. heh. I?m just convulsing my
thoughts to the irc log good morning. Jason also noted that the cool
part about that is if you have code that needs to happen both on a pre-
insert and a pre-update - but only a few lines of the code is different -
you could have one block of trigger code that used self.action to find out
why it had been called and branch accordingly. for an errata next week
:) self?s meaning is derived from how others use you. I think convulsing
is a good description reinhard & jcater discuss the meaning of ?self? ;-)
Biased-LexRank: good morning everyone. heh. I?m just convulsing
my thoughts to the irc log. Jason also noted that the cool part about
that is if you have code that needs to happen both on a pre-insert and
a pre-update - but only a few lines of the code is different - you could
have one block of trigger code that used self.action to find out why it
had been called and branch accordingly. yes, and he took the more
metaphysical route, where I took the more scientific route there was
a big jump from the previous paragraph to that but to the event itself.
iow, I was presenting a 3rd alternative where self referred to something
specific to the trigger.
Our system: self could refer to neither the triggers owner nor caller.
I was saying an alternative where self referred to something specific to
the trigger. and self.name could be the name.
so self.action could be what action is being performed, self.parent the
triggers owner and self.context caller.
Gold: Further to, E clarified that he had suggested that ?self? could
refer to neither the trigger?s owner nor to the trigger?s caller - but to
the event itself. So self.action could be what action is being performed,
self.parent could be the trigger?s owner, and self.context could be the
caller. In other words, I was presenting a 3rd alternative where self
referred to something specific to the trigger.
Example 3. Summaries generated by our system
and other baselines in comparison with the human-
written summary for a short chat log. Speaker in-
formation have been anonymized.
ing transcripts, although generated by humans, are
not fully grammatical. In comparison with the
original sentences, for all datasets, our model re-
ports slightly lower results for the grammaticality
score. Considering the fact that the abstract sen-
tences are automatically generated and the orig-
inal sentences are human-written, the grammat-
icality score and the percentage of fully gram-
matical sentences generated by our system, with
higher ROUGE or quality scores in comparison
with other methods, demonstrates that our system
is an effective phrasal query abstraction frame-
work for both spoken and written conversations.
4 Conclusion
We have presented an unsupervised framework for
abstractive summarization of spoken and written
conversations based on phrasal queries. For con-
tent selection, we propose a sentence extraction
model that incorporates query relevance and con-
tent importance into the extraction process. For
the generation phase, we propose a ranking strat-
egy which selects the best path in the constructed
word graph based on fluency, query relevance
and content. Both automatic and manual evalua-
tion of our model show substantial improvement
over extraction-based methods, including Biased
LexRank, which is considered a state-of-the-art
system. Moreover, our system also yields good
grammaticality score for human evaluation and
achieves comparable scores with the original sen-
tences. Our future work is four-fold. First, we
are trying to improve our model by incorporating
conversational features (e.g., speech acts). Sec-
ond, we aim at implementing a strategy to or-
der the clusters for generating more coherent ab-
stracts. Third, we try to improve our generated
summary by resolving coreferences and incorpo-
rating speaker information (e.g., names) in the
clustering and sentence generation phases. Fi-
nally, we plan to take advantage of topic shifts to
better segment the relevant parts of conversations
in relation to phrasal queries.
Acknowledgments
We would like to thank the anonymous review-
ers for their valuable comments and suggestions
to improve the paper, and the NSERC Business In-
telligence Network for financial support. We also
would like to acknowledge the early discussions
on the related topics with Frank Tompa.
1228
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application
to the health-care domain. In Proceedings of
the ACL 2012 System Demonstrations, ACL ?12,
pages 79?84, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence Fusion for Multidocument News Sum-
marization. Comput. Linguist., 31(3):297?328,
September.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global Learning of Typed Entailment Rules.
In Proceedings of ACL, Portland, OR.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005.
The AMI meeting corpus: A pre-announcement. In
Proc. MLMI, pages 28?39.
I. Dagan and O. Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
G?unes Erkan and Dragomir R. Radev. 2004. Lexrank:
graph-based lexical centrality as salience in text
summarization. J. Artif. Int. Res., 22(1):457?479,
December.
Katja Filippova. 2010. Multi-sentence compression:
finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 322?
330, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
340?348, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-tr. 2009. A global optimization
framework for meeting summarization. In Proc.
IEEE ICASSP, pages 4769?4772.
Julio Gonzalo, Felisa Verdejo, Irina Chugur, and
Juan M. Cigarrn. 1998. Indexing with wordnet
synsets can improve text retrieval. CoRR.
David Graff and Christopher Cieri. 2003. English Gi-
gaword Corpus. Technical report, Linguistic Data
Consortium, Philadelphia.
Surabhi Gupta, Ani Nenkova, and Dan Jurafsky. 2007.
Measuring importance and query relevance in topic-
focused multi-document summarization. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 193?196, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Lisa Hunemark. 2010. Query expansion using search
logs and WordNet. Technical report, Uppsala Uni-
versity, mar. Masters thesis in Computational Lin-
guistics.
Quentin Jones, Gilad Ravid, and Sheizaf Rafaeli. 2004.
Information overload and the message dynamics
of online interaction spaces: A theoretical model
and empirical exploration. Info. Sys. Research,
15(2):194?210, June.
Shafiq Joty, Gabriel Murray, and Raymond T. Ng.
2011. Supervised topic segmentation of email con-
versations. In ICWSM11. AAAI.
Shafiq R. Joty, Giuseppe Carenini, and Raymond T.
Ng. 2013. Topic segmentation and labeling in asyn-
chronous conversations. J. Artif. Intell. Res. (JAIR),
47:521?573.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In Proc. Of the COLING Conference, pages
495?501.
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: can it be done by sen-
tence compression? In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 261?264, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Yashar Mehdad, Giuseppe Carenini, and Raymond
NG T. 2013a. Towards Topic Labeling with Phrase
Entailment and Aggregation. In Proceedings of
NAACL 2013, pages 179?189, Atlanta, USA, June.
Association for Computational Linguistics.
Yashar Mehdad, Giuseppe Carenini, Frank Tompa, and
Raymond T. NG. 2013b. Abstractive meeting sum-
marization with entailment and fusion. In Proceed-
ings of the 14th EuropeanWorkshop on Natural Lan-
guage Generation, pages 136?146, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Weiyi Meng and Clement T. Yu. 2010. Advanced
Metasearch Engine Technology. Synthesis Lectures
on Data Management. Morgan and Claypool Pub-
lishers.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, July.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010. Generating and validating abstracts of meet-
ing conversations: a user study. In Proceedings of
1229
the 6th International Natural Language Generation
Conference, INLG ?10, pages 105?113, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jahna Otterbacher, Gnes Erkan, and Dragomir R.
Radev. 2009. Biased lexrank: Passage retrieval us-
ing random walks with question-based priors. Inf.
Process. Manage., 45(1):42?54.
Tetsuya Sakai and Karen Sparck-Jones. 2001. Generic
summaries for indexing in information retrieval. In
Proceedings of the 24th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ?01, pages 190?198,
New York, NY, USA. ACM.
J. Ulrich, G. Murray, and G. Carenini. 2008. A
publicly available annotated corpus for supervised
email summarization. In AAAI08 EMAIL Workshop,
Chicago, USA. AAAI.
David C. Uthus and David W. Aha. 2011. Plans toward
automated chat summarization. In Proceedings of
the Workshop on Automatic Summarization for Dif-
ferent Genres, Media, and Languages, WASDGML
?11, pages 1?7, Stroudsburg, PA, USA. Association
for Computational Linguistics.
David C. Uthus and David W. Aha. 2013. The ubuntu
chat corpus for multiparticipant chat analysis. In
AAAI Spring Symposium: Analyzing Microtext.
Lu Wang and Claire Cardie. 2013. Domain-
independent abstract generation for focused meet-
ing summarization. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1395?
1405, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Lu Wang, Hema Raghavan, Vittorio Castelli, Radu Flo-
rian, and Claire Cardie. 2013. A sentence com-
pression based framework to query-focused multi-
document summarization. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1384?1394, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th Conference on Computational
Linguistics - Volume 2, COLING ?00, pages 947?
953. Association for Computational Linguistics.
Liang Zhou and Eduard Hovy. 2005. Digesting vir-
tual ?geek? culture: The summarization of technical
internet relay chats. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 298?305, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
1230
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 399?407,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Semeval-2012 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
FBK-irst
Trento, Italy
mehdad@fbk.eu
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the first round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2012. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (10 teams,
92 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization.
Cross-linguality represents a dimension of the TE
recognition problem that has been so far only par-
tially investigated. The great potential for integrat-
ing monolingual TE recognition components into
NLP architectures has been reported in several ar-
eas, including question answering, information re-
trieval, information extraction, and document sum-
marization. However, mainly due to the absence of
cross-lingual textual entailment (CLTE) recognition
components, similar improvements have not been
achieved yet in any cross-lingual application. The
CLTE task aims at prompting research to fill this
gap. Along such direction, research can now ben-
efit from recent advances in other fields, especially
machine translation (MT), and the availability of: i)
large amounts of parallel and comparable corpora in
many languages, ii) open source software to com-
pute word-alignments from parallel corpora, and iii)
open source software to set up MT systems. We
believe that all these resources can positively con-
tribute to develop inference mechanisms for multi-
lingual data.
Content synchronization represents a challenging
application scenario to test the capabilities of ad-
vanced NLP systems. Given two documents about
the same topic written in different languages (e.g.
Wiki pages), the task consists of automatically de-
tecting and resolving differences in the information
they provide, in order to produce aligned, mutually
enriched versions of the two documents. Towards
this objective, a crucial requirement is to identify the
information in one page that is either equivalent or
novel (more informative) with respect to the content
of the other. The task can be naturally cast as an
entailment recognition problem, where bidirectional
and unidirectional entailment judgments for two text
fragments are respectively mapped into judgments
about semantic equivalence and novelty. Alterna-
tively, the task can be seen as a machine translation
evaluation problem, where judgments about seman-
tic equivalence and novelty depend on the possibility
to fully or partially translate a text fragment into the
other.
399
Figure 1: ?bidirectional?, ?forward?, ?backward? and
?no entailment? judgments for SP/EN CLTE pairs.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as useful evidence to
draw entailment decisions), the standard sentence
and word alignment programs used in SMT offer a
strong baseline for CLTE. However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and large
room for mutual improvement.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgments (see Figure 1 for
Spanish/English examples of each judgment):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in both direc-
tions;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset created for
the first round of the task.
3 Dataset description
Four CLTE corpora have been created for the fol-
lowing language combinations: Spanish/English
(SP-EN), Italian/English (IT-EN), French/English
(FR-EN), German/English (DE-EN). The datasets
are released in the XML format shown in Figure 1.
3.1 Data collection and annotation
The dataset was created following the crowdsourc-
ing methodology proposed in (Negri et al, 2011),
which consists of the following steps:
1. First, English sentences were manually ex-
tracted from copyright-free sources (Wikipedia
and Wikinews). The selected sentences repre-
sent one of the elements (T1) of each entail-
ment pair;
2. Next, each T1 was modified through crowd-
sourcing in various ways in order to ob-
tain a corresponding T2 (e.g. introduc-
ing meaning-preserving lexical and syntactic
changes, adding and removing portions of
text);
3. Each T2 was then paired to the original T1,
and the resulting pairs were annotated with one
of the four entailment judgments. In order to
reduce the correlation between the difference
in sentences? length and entailment judgments,
400
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.1 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgments, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were manually checked and corrected
when necessary. Only pairs with agreement between
two expert annotators were retained. The final result
is a multilingual parallel entailment corpus, where
T1s are in 5 different languages (i.e. English, Span-
ish, German, Italian, and French), and T2s are in En-
glish. It?s worth mentioning that the monolingual
English corpus, a by-product of our data collection
methodology, will be publicly released as a further
contribution to the research community.2
3.2 Dataset statistics
Each dataset consists of 1,000 pairs (500 for training
and 500 for test), balanced across the four entail-
ment judgments (bidirectional, forward, backward,
and no entailment).
For each language combination, the distribu-
tion of the four entailment judgments according to
length diff is shown in Figure 2. Vertical bars rep-
resent, for each length diff value, the proportion
of pairs belonging to the four entailment classes.
As can be seen, the length diff constraint applied
to the length difference in the monolingual English
1Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
2The cross-lingual datasets are already available for research
purposes at http://www.celct.it/resourcesList.
php. The monolingual English dataset will be publicly released
to non participants in July 2012.
pairs (step 3 of the creation process) is substantially
reflected in the cross-lingual datasets for all lan-
guage combinations. In fact, as shown in Table 1,
the majority of the pairs is always included in the
same length diff range (approximately [-5,+5]) and,
within this range, the distribution of the four classes
is substantially uniform. Our assumption is that such
data distribution makes entailment judgments based
on mere surface features such as sentence length in-
effective, thus encouraging the development of alter-
native, deeper processing strategies.
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
Table 1: CLTE pairs distribution within the -5/+5
length diff range.
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgments re-
turned by each system with those manually assigned
by human annotators. The metric used for systems?
ranking is accuracy over the whole test set, i.e. the
number of correct judgments out of the total number
of judgments in the test set. Additionally, we calcu-
lated precision, recall, and F1 measures for each of
the four entailment judgment categories taken sep-
arately. These scores aim at giving participants the
possibility to gain clearer insights into their system?s
behavior on the entailment phenomena relevant to
the task.
For each language combination, two baselines
considering the length difference between T1 and T2
have been calculated (besides the trivial 0.25 accu-
racy score obtained by assigning each test pair in the
balanced dataset to one of the four classes):
? Composition of binary judgments (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary entailment
decisions (?YES?, ?NO?). The classifier uses
length(T1)/length(T2) as a single feature to
check for entailment from T1 to T2, and
length(T2)/length(T1) for the opposite direc-
tion. For each test pair, the unidirectional
401
010
20
30
40
50
60
70
80
-21 -18 -15 -12 -9 -6 -3 0 3 6 9
no_entailmentforwardbidirectionalbackward
(a) SP-EN
0
10
20
30
40
50
60
70
80
-17 -14 -11 -8 -5 -2 1 4 7 10
no_entailmentforwardbidirectionalbackward
(b) IT-EN
0
10
20
30
40
50
60
70
80
90
-21 -18 -15 -12 -9 -6 -3 0 3 6 9 12
no_entailmentforwardbidirectionalbackward
(c) FR-EN
0
10
20
30
40
50
60
70
80
90
-14 -10 -7 -4 -1 2 5 8 11 14 17
no_entailmentforwardbidirectionalbackward
(d) DE-EN
Figure 2: CLTE pairs distribution for different length diff values across all datasets.
judgments returned by the two classifiers are
composed into a single multi-directional judg-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgments.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using a
linear kernel with default parameters. Baseline re-
sults are reported in Table 2.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages.
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.34 0.39 0.39 0.40
Multi-class 0.43 0.44 0.42 0.42
Table 2: Baseline accuracy results.
5 Submitted runs and results
Participants were allowed to submit up to five runs
for each language combination. A total of 17 teams
registered to participate in the task and downloaded
the training set. Out of them, 12 downloaded the
test set and 10 (including one of the task organizers)
submitted valid runs. Eight teams produced submis-
sions for all the language combinations, while two
teams participated only in the SP-EN task. In total,
92 runs have been submitted and evaluated (29 for
SP-EN, and 21 for each of the other language pairs).
402
Despite the novelty and the difficulty of the problem,
these numbers demonstrate the interest raised by the
task, and the overall success of the initiative.
System name SP-EN IT-EN FR-EN DE-EN
BUAP run1 0.350 0.336 0.334 0.330
BUAP run2 0.366 0.344 0.342 0.268
celi run1 0.276 0.278 0.278 0.280
celi run2 0.336 0.338 0.300 0.352
celi run3 0.322 0.334 0.298 0.350
celi run4 0.268 0.280 0.280 0.274
DirRelCond3 run1 0.300 0.280 0.362 0.336
DirRelCond3 run2 0.300 0.284 0.360 0.336
DirRelCond3 run3 0.300 0.338 0.384 0.364
DirRelCond3 run4 0.344 0.316 0.384 0.374
FBK run1* 0.502 - - -
FBK run2* 0.490 - - -
FBK run3* 0.504 - - -
FBK run4* 0.500 - - -
HDU run1 0.630 0.554 0.564 0.558
HDU run2 0.632 0.562 0.570 0.552
ICT run1 0.448 0.454 0.456 0.460
JU-CSE-NLP run1 0.274 0.316 0.288 0.262
JU-CSE-NLP run2 0.266 0.326 0.294 0.296
JU-CSE-NLP run3 0.272 0.314 0.296 0.264
Sagan run1 0.342 0.352 0.346 0.342
Sagan run2 0.328 0.352 0.336 0.310
Sagan run3 0.346 0.356 0.330 0.332
Sagan run4 0.340 0.330 0.310 0.310
SoftCard run1 0.552 0.566 0.570 0.550
UAlacant run1 LATE 0.598 - - -
UAlacant run2 0.582 - - -
UAlacant run3 LATE 0.510 - - -
UAlacant run4 0.514 - - -
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 3: Accuracy results (92 runs) over the 4 lan-
guage combinations. Highest, average, median and low-
est scores are calculated considering the best run for each
team (*task organizers? system).
Accuracy results are reported in Table 3. As can
be seen from the table, overall accuracy scores are
quite different across language pairs, with the high-
est result on SP-EN (0.632), which is considerably
higher than the highest score on DE-EN (0.558).
This might be due to the fact that most of the partic-
ipating systems rely on a ?pivoting? approach that
addresses CLTE by automatically translating T1 in
the same language of T2 (see Section 6). Regard-
ing the DE-EN dataset, pivoting methods might be
penalized by the lower quality of MT output when
German T1s are translated into English.
The comparison with baselines results leads to in-
teresting observations. First of all, while all systems
significantly outperform the lowest 1-class baseline
(0.25), both other baselines are surprisingly hard to
beat. This shows that, despite the effort in keep-
ing the distribution of the entailment classes uni-
form across different length diff values, eliminating
the correlation between sentences? length and cor-
rect entailment decisions is difficult. As a conse-
quence, although disregarding semantic aspects of
the problem, features considering such information
are quite effective.
In general, systems performed better on the SP-
EN dataset, with most results above the binary base-
line (8 out of 10), and half of the systems above the
multi-class baseline. For the other language pairs
the results are lower, with only 3 out of 8 partici-
pants above the two baselines in all datasets. Aver-
age results reflect this situation: the average scores
are always above the binary baseline, whereas only
the SP-EN average result is higher than the multi-
class baseline(0.44 vs. 0.43).
To better understand the behaviour of each sys-
tem (also in relation to the different language com-
binations), Table 4 provides separate precision, re-
call, and F1 scores for each entailment judgment,
calculated over the best runs of each participating
team. Overall, the results suggest that the ?bidi-
rectional? and ?no entailment? categories are more
problematic than ?forward? and ?backward? judg-
ments. For most datasets, in fact, systems? perfor-
mance on ?bidirectional? and ?no entailment? is sig-
nificantly lower, typically on recall. Except for the
DE-EN dataset (more problematic on ?forward?),
also average F1 results on these judgments are lower.
This might be due to the fact that, for all datasets, the
vast majority of ?bidirectional? and ?no entailment?
judgments falls in a length diff range where the dis-
tribution of the four classes is more uniform (see
Figure 2).
Similar reasons can justify the fact that ?back-
ward? entailment results are consistently higher on
all datasets. Compared with ?forward? entailment,
these judgments are in fact less scattered across the
entire length diff range (i.e. less intermingled with
the other classes).
403
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods as-
sign entailment judgments without preliminary
translation.
? Composition of binary judgments vs. Multi-
class classification. Compositional approaches
map unidirectional entailment decisions taken
separately into single judgments (similar to the
Binary baseline in Section 4). Methods based
on multi-class classification directly assign one
of the four entailment judgments to each test
pair (similar to our Multi-class baseline).
Concerning the former dimension, most of the
systems (6 out of 10) adopted a pivoting approach,
relying on Google Translate (4 systems), Microsoft
Bing Translator (1), or a combination of Google,
Bing, and other MT systems (1) to produce English
T2s. Regarding the latter dimension, the composi-
tional approach was preferred to multi-class classi-
fication (6 out of 10). The best performing system
relies on a ?hybrid? approach (combining monolin-
gual and cross-lingual alignments) and a compo-
sitional strategy. Besides the frequent recourse to
MT tools, other resources used by participants in-
clude: on-line dictionaries for the translation of sin-
gle words, word alignment tools, part-of-speech tag-
gers, NP chunkers, named entity recognizers, stem-
mers, stopwords lists, and Wikipedia as an external
multilingual corpus. More in detail:
BUAP [pivoting, compositional] (Vilarin?o et al,
2012) adopts a pivoting method based on translating
T1 into the language of T2 and vice versa (Google
Translate3 and the OpenOffice Thesaurus4). Simi-
larity measures (e.g. Jaccard index) and rules are
3http://translate.google.com/
4http://extensions.services.openoffice.
org/en/taxonomy/term/233
respectively used to annotate the two resulting sen-
tence pairs with entailment judgments and combine
them in a single decision.
CELI [cross lingual, compositional & multi-
class] (Kouylekov, 2012) uses dictionaries for word
matching, and a multilingual corpus extracted from
Wikipedia for term weighting. Word overlap and
similarity measures are then used in different ap-
proaches to the task. In one run (Run 1), they are
used to train a classifier that assigns separate en-
tailment judgments for each direction. Such judg-
ments are finally composed into a single one for each
pair. In the other runs, the same features are used for
multi-class classification.
DirRelCond3 [cross lingual, compositional]
(Perini, 2012) uses bilingual dictionaries (Freedict5
and WordReference6) to translate content words into
English. Then, entailment decisions are taken com-
bining directional relatedness scores between words
in both directions (Perini, 2011).
FBK [cross lingual, compositional & multi-
class] (Mehdad et al, 2012a) uses cross-lingual
matching features extracted from lexical phrase ta-
bles, semantic phrase tables, and dependency rela-
tions (Mehdad et al, 2011; Mehdad et al, 2012b;
Mehdad et al, 2012c). The features are used for
multi-class and binary classification using SVMs.
HDU [hybrid, compositional] (Wa?schle and
Fendrich, 2012) uses a combination of binary clas-
sifiers for each entailment direction. The classifiers
use both monolingual alignment features based on
METEOR (Banerjee and Lavie, 2005) alignments
(translations obtained from Google Translate), and
cross-lingual alignment features based on GIZA++
(Och and Ney, 2000) (word alignments learned on
Europarl).
ICT [pivoting, compositional] (Meng et al,
2012) adopts a pivoting method (using Google
Translate and an in-house hierarchical MT system),
and the open source EDITS system (Kouylekov and
Negri, 2010) to calculate similarity scores between
monolingual English pairs. Separate unidirectional
entailment judgments obtained from binary classi-
fier are combined to return one of the four valid
CLTE judgments.
5http://www.freedict.com/
6http://www.wordreference.com/
404
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP spa-eng run2 0,337 0,664 0,447 0,406 0,568 0,473 0,333 0,088 0,139 0,391 0,144 0,211
celi spa-eng run2 0,324 0,368 0,345 0,411 0,368 0,388 0,306 0,296 0,301 0,312 0,312 0,312
DirRelCond3 spa-eng run4 0,358 0,608 0,451 0,444 0,448 0,446 0,286 0,032 0,058 0,243 0,288 0,264
FBK spa-eng run3 0,515 0,704 0,595 0,546 0,568 0,557 0,447 0,304 0,362 0,482 0,440 0,460
HDU spa-eng run2 0,607 0,656 0,631 0,677 0,704 0,690 0,602 0,592 0,597 0,643 0,576 0,608
ICT spa-eng run1 0,750 0,240 0,364 0,440 0,472 0,456 0,395 0,560 0,464 0,436 0,520 0,474
JU-CSE-NLP spa-eng run1 0,211 0,288 0,243 0,272 0,296 0,284 0,354 0,232 0,280 0,315 0,280 0,297
Sagan spa-eng run3 0,225 0,200 0,212 0,269 0,224 0,245 0,418 0,448 0,432 0,424 0,512 0,464
SoftCard spa-eng run1 0,602 0,616 0,609 0,650 0,624 0,637 0,471 0,448 0,459 0,489 0,520 0,504
UAlacant spa-eng run1 LATE 0,689 0,568 0,623 0,645 0,728 0,684 0,507 0,544 0,525 0,566 0,552 0,559
AVG. 0,462 0,491 0,452 0,476 0,5 0,486 0,412 0,354 0,362 0,43 0,414 0,415
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP ita-eng run2 0,324 0,456 0,379 0,327 0,672 0,440 0,538 0,056 0,101 0,444 0,192 0,268
celi ita-eng run2 0,349 0,360 0,354 0,455 0,36 0,402 0,294 0,320 0,307 0,287 0,312 0,299
DirRelCond3 ita-eng run3 0,323 0,488 0,389 0,480 0,288 0,360 0,331 0,368 0,348 0,268 0,208 0,234
HDU ita-eng run2 0,564 0,600 0,581 0,628 0,648 0,638 0,551 0,520 0,535 0,500 0,480 0,490
ICT ita-eng run1 0,661 0,296 0,409 0,554 0,368 0,442 0,427 0,448 0,438 0,383 0,704 0,496
JU-CSE-NLP ita-eng run2 0,240 0,280 0,258 0,339 0,480 0,397 0,412 0,280 0,333 0,359 0,264 0,304
Sagan ita-eng run3 0,306 0,296 0,301 0,252 0,216 0,233 0,395 0,512 0,446 0,455 0,400 0,426
SoftCard ita-eng run1 0,602 0,616 0,609 0,617 0,696 0,654 0,560 0,448 0,498 0,481 0,504 0,492
AVG. 0,421 0,424 0,410 0,457 0,466 0,446 0,439 0,369 0,376 0,397 0,383 0,376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP fra-eng run2 0,447 0,272 0,338 0,291 0,760 0,420 0,250 0,016 0,030 0,449 0,320 0,374
celi fra-eng run2 0,316 0,296 0,306 0,378 0,360 0,369 0,270 0,296 0,282 0,244 0,248 0,246
DirRelCond3 fra-eng run3 0,393 0,576 0,468 0,441 0,512 0,474 0,387 0,232 0,290 0,278 0,216 0,243
HDU fra-eng run2 0,564 0,672 0,613 0,582 0,736 0,650 0,676 0,384 0,490 0,500 0,488 0,494
ICT fra-eng run1 0,750 0,192 0,306 0,517 0,496 0,506 0,385 0,656 0,485 0,444 0,480 0,462
JU-CSE-NLP fra-eng run3 0,215 0,208 0,211 0,289 0,296 0,292 0,341 0,496 0,404 0,333 0,184 0,237
Sagan fra-eng run1 0,244 0,168 0,199 0,297 0,344 0,319 0,394 0,568 0,466 0,427 0,304 0,355
SoftCard fra-eng run1 0,551 0,608 0,578 0,649 0,696 0,672 0,560 0,488 0,521 0,513 0,488 0,500
AVG. 0,435 0,374 0,377 0,431 0,525 0,463 0,408 0,392 0,371 0,399 0,341 0,364
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
BUAP deu-eng run1 0,395 0,120 0,184 0,248 0,224 0,235 0,344 0,688 0,459 0,364 0,288 0,321
celi deu-eng run2 0,347 0,416 0,378 0,402 0,392 0,397 0,339 0,312 0,325 0,319 0,288 0,303
DirRelCond3 deu-eng run4 0,429 0,312 0,361 0,408 0,552 0,469 0,367 0,320 0,342 0,298 0,312 0,305
HDU deu-eng run1 0,559 0,528 0,543 0,600 0,696 0,644 0,540 0,488 0,513 0,524 0,520 0,522
ICT deu-eng run1 0,718 0,224 0,341 0,493 0,552 0,521 0,390 0,512 0,443 0,439 0,552 0,489
JU-CSE-NLP deu-eng run2 0,182 0,048 0,076 0,307 0,496 0,379 0,315 0,560 0,403 0,233 0,080 0,119
Sagan deu-eng run1 0,250 0,168 0,201 0,239 0,256 0,247 0,405 0,600 0,484 0,443 0,344 0,387
SoftCard deu-eng run1 0,568 0,568 0,568 0,611 0,640 0,625 0,521 0,488 0,504 0,496 0,504 0,500
AVG. 0,431 0,298 0,332 0,414 0,476 0,440 0,403 0,496 0,434 0,390 0,361 0,368
Table 4: precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
JU-CSE-NLP [pivoting, compositional] (Neogi
et al, 2012) uses Microsoft Bing translator7 to pro-
duce monolingual English pairs. Separate lexical
mapping scores are calculated (from T1 to T2 and
vice-versa) considering different types of informa-
tion and similarity metrics. Binary entailment de-
7http://www.microsofttranslator.com/
cisions are then heuristically combined into single
decisions.
Sagan [pivoting, multi-class] (Castillo and Car-
denas, 2012) adopts a pivoting method using Google
Translate, and trains a monolingual system based on
a SVM multi-class classifier. A CLTE corpus de-
rived from the RTE-3 dataset is also used as a source
of additional training material.
405
SoftCard [pivoting, multi-class] (Jimenez et al,
2012) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity (computed with
edit-distance)
UAlacant [pivoting, multi-class] (Espla`-Gomis
et al, 2012) exploits translations obtained from
Google Translate, Microsoft Bing translator, and the
Apertium open-source MT platform (Forcada et al,
2011).8 Then, a multi-class SVM classifier is used
to take entailment decisions using information about
overlapping sub-segments as features.
7 Conclusion
Despite the novelty of the problem and the diffi-
culty to capture multi-directional entailment rela-
tions across languages, the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
tion task organized within SemEval-2012 was a suc-
cessful experience. This year a new interesting chal-
lenge has been proposed, a benchmark for four lan-
guage combinations has been released, baseline re-
sults have been proposed for comparison, and a
monolingual English dataset has been produced as
a by-product which can be useful for monolingual
TE research. The interest shown by participants
was encouraging: 10 teams submitted a total of 92
runs for all the language pairs proposed. Overall,
the results achieved on all datasets are encourag-
ing, with best systems significantly outperforming
the proposed baselines. It is worth observing that the
nature of the task, which lies between semantics and
machine translation, led to the participation of teams
coming from both these communities, showing in-
teresting opportunities for integration and mutual
improvement. The proposed approaches reflect this
situation, with teams traditionally working on MT
now dealing with entailment, and teams tradition-
ally participating in the RTE challenges now dealing
with cross-lingual alignment techniques. Our ambi-
tion, for the future editions of the CLTE task, is to
further consolidate the bridge between the semantics
and MT communities.
8http://www.apertium.org/
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853). The
authors would also like to acknowledge Giovanni
Moretti from CELCT for evaluation scripts and
technical assistance, and the volunteer translators
that contributed to the creation of the dataset:
Mar??a Sol Accossato, Laura Barthe?le?my, Clau-
dia Biacchi, Jane Brendler, Amandine Chantrel,
Hanna Cheda Patete, Ellen Clancy, Rodrigo Damian
Tejeda, Daniela Dold, Valentina Frattini, Debora
Hedy Amato, Geniz Hernandez, Be?ne?dicte Jean-
nequin, Beate Jones, Anne Kauffman, Marcia Laura
Zanoli, Jasmin Lewis, Alicia Lo?pez, Domenico Los-
eto, Sabrina Luja?n Sa?nchez, Julie Mailfait, Gabriele
Mark, Nunzio Pruiti, Lourdes Rey Cascallar, Sylvie
Martlew, Aleane Salas Velez, Monica Scalici, An-
dreas Schwab, Marianna Sicuranza, Chiara Sisler,
Stefano Tordazzi, Yvonne.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 65?72.
Julio Castillo and Marina Cardenas. 2012. Sagan: A
Cross Lingual Textual Entailment system based on
Machine Traslation. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012).
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Miquel Espla`-Gomis, Felipe Sa?nchez-Mart??nez, and
Mikel L. Forcada. 2012. UAlacant: Using Online
Machine Translation for Cross-Lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Mikel L. Forcada, Ginest??-Rosell Mireia, Nordfalk Jacob,
O?Regan Jim, Ortiz-Rojas Sergio, Pe?rez-Ortiz Juan A.,
Sa?nchez-Mart??nez Felipe, Ram??rez-Sa?nchez Gema,
406
and Tyers Francis M. 2011. Apertium: a Free/Open-
Source Platform for Rule-Based Machine Translation.
Machine Translation, 25(2):127?144. Special Issue:
Free/Open-Source Machine Translation.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft Cardinality + ML: Learning Adap-
tive Similarity Functions for Cross-lingual Textual En-
tailment. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2012. CELI: An Experiment with
Cross Language Textual Entailment. In Proceedings
of the 6th International Workshop on Semantic Evalu-
ation (SemEval 2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Jose? G. C. de Souza.
2012a. FBK: Cross-Lingual Textual Entailment With-
out Translation. In Proceedings of the 6th Interna-
tional Workshop on Semantic Evaluation (SemEval
2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012b. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012c. Match without a Referee: Evaluating MT Ade-
quacy without Reference Translations. In Proceedings
of the 7th Workshop on Statistical Machine Translation
(WMT 2012).
Fandong Meng, Hao Xiong, and Qun Liu. 2012. ICT:
A Translation based Cross-lingual Textual Entailment.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Snehasis Neogi, Partha Pakray, Sivaji Bandyopadhyay,
and Alexander Gelbukh. 2012. JU-CSE-NLP: Lan-
guage Independent Cross-lingual Textual Entailment
System. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Franz J. Och and Hermann Ney. 2000. Improved Sta-
tistical Alignment Models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL 2000).
Alpa?r Perini. 2011. Detecting textual entailment
with conditions on directional text relatedness scores.
Studia Universitatis Babes-Bolyai Series Informatica,
LVI(2):13?18.
Alpa?r Perini. 2012. DirRelCond3: Detecting Textual
Entailment Across Languages With Conditions On Di-
rectional Text Relatedness Scores. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n,
and Esteban Castillo. 2012. BUAP: Lexical and
Semantic Similarity for Cross-lingual Textual Entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012).
Katharina Wa?schle and Sascha Fendrich. 2012. HDU:
Cross-lingual Textual Entailment with SMT Features.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
407
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 624?630,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
FBK: Machine Translation Evaluation and Word Similarity metrics
for Semantic Textual Similarity
Jose? Guilherme C. de Souza
Fondazione Bruno Kessler
University of Trento
Povo, Trento, Italy
desouza@fbk.eu
Matteo Negri
Fondazione Bruno Kessler
Povo, Trento
Italy
negri@fbk.eu
Yashar Mehdad
Fondazione Bruno Kessler
Povo, Trento
Italy
mehdad@fbk.eu
Abstract
This paper describes the participation of FBK
in the Semantic Textual Similarity (STS) task
organized within Semeval 2012. Our ap-
proach explores lexical, syntactic and se-
mantic machine translation evaluation metrics
combined with distributional and knowledge-
based word similarity metrics. Our best
model achieves 60.77% correlation with hu-
man judgements (Mean score) and ranked 20
out of 88 submitted runs in the Mean rank-
ing, where the average correlation across all
the sub-portions of the test set is considered.
1 Introduction
The Semantic Textual Similarity (STS) task pro-
posed at SemEval 2012 consists of examining the
degree of semantic equivalence between two sen-
tences and assigning a score to quantify such sim-
ilarity ranging from 0 (the two texts are about dif-
ferent topics) to 5 (the two texts are semantically
equivalent). The complete description of the task,
the datasets and the evaluation methodology adopted
can be found in (Agirre et al, 2012).
Typical approaches to measure semantic textual
similarity exploit information at the lexical level.
The proposed solutions range from calculating the
overlap of common words between the two text seg-
ments (Salton et al, 1997) to the application of
knowledge-based and corpus-based word similarity
metrics to cope with the low recall achieved by on
simple lexical matching (Mihalcea et al, 2006).
Our participation in the STS task is inspired by
previous work on paraphrase recognition, in which
machine translation (MT) evaluation metrics are
used to identify whether a pair of sentences are
semantically equivalent or not (Finch and Hwang,
2005; Wan et al, 2006). Our approach to semantic
textual similarity makes use of not only lexical in-
formation but also syntactic and semantic informa-
tion. To this aim, our metrics are based on different
natural language processing tools that provide syn-
tactic and semantic annotation. These include shal-
low parsing, constituency parsing, dependency pars-
ing, semantic roles labeling, discourse representa-
tion analyzer, and named entities recognition. In ad-
dition, we employed distributional and knowledge-
based word similarity metrics in an attempt to im-
prove the results given by the MT metrics. The com-
puted scores are used as features to train a regression
model in a supervised learning framework.
Our best run model achieves 60.77% correlation
with human judgements when evaluating the seman-
tic similarity of texts from the entire test set and
was ranked in the 20th position (out of 88 submit-
ted runs) in the Mean ranking.
2 System Description
The system has been designed following a ma-
chine learning based approach in which a regres-
sion model is induced using different shallow and
deep linguistic features extracted from the datasets.
The STS training corpora are first preprocessed us-
ing different tools that annotate the texts at different
levels. Using the preprocessed data, the features are
extracted for each pair and used to train a model that
will be applied to unseen test pairs. The training
set is composed by three datasets (MSRpar, MSRvid
and SMTeuroparl) which combined contain a total
of 2234 instances. The test data is composed by a
different sample of the same three datasets plus in-
stances derived from two additional corpora (OnWN
624
and SMTnews). The datasets construction and anno-
tation are described in (Agirre et al, 2012).
Our system exploits two sets of features which re-
spectively build on MT evaluation metrics (2.1) and
word similarity metrics (2.2). The whole feature set
is summarized in figure 1.
2.1 Machine Translation Evaluation Metrics
MT evaluation metrics are designed to assess
whether the output of a MT system is semantically
equivalent to a set of reference translations. The
MT evaluation metrics described in this section, im-
plemented in the Asiya Open Toolkit for Automatic
Machine Translation (Meta-) Evaluation1 (Gime?nez
and Ma`rquez, 2010) are used to extract features at
different linguistic levels: lexical, syntactic and se-
mantic. For the syntactic and semantic levels, Asiya
calculates similarity measures based on the linguis-
tic elements provided by each kind of annotation.
Linguistic elements are defined as ?the linguistic
units, structures, or relationships? (Gime?nez, 2008)
(e.g. dependency relations, discourse relations,
named entities, part-of-speech tags, among others).
(Gime?nez, 2008) defines two simple measures us-
ing the linguistic elements of a given linguistic level:
overlapping and matching. Overlapping is a
measure of the proportion of items inside the lin-
guistic elements of a certain type shared by both
texts. Matching is defined in the same way with
the difference that the order between the items inside
a linguistic element is taken into consideration. That
is, the items of a linguistic element are concatenated
in a single unit from left to right.
2.1.1 Lexical Level
At the lexical level we explored different n-gram
and edit distance based metrics. The difference
among them is in the way each algorithm calcu-
lates the lexical similarity, which yields to differ-
ent results. We used the following n-gram-based
metrics: BLEU (Papineni et al, 2002), NIST (Dod-
dington, 2002), ROUGE (Lin and Och, 2004), GTM
(Melamed et al, 2003), METEOR (Banerjee and
Lavie, 2005). Besides those, we also used metrics
based on edit distance. Such metrics calculate the
number of edit operations (e.g. insertions, deletions,
and substitutions) necessary to transform one text
1http://nlp.lsi.upc.edu/asiya/
into the other (the lower the number of edit oper-
ations, the higher the similarity score). The edit-
distance-based metrics used were: WER (Nie? en et
al., 2000), PER (Tillmann et al, 1997), TER (Snover
et al, 2006) and TER-Plus (Snover et al, 2009). The
lexical metrics form a group of metrics that we here-
after call lex.
2.1.2 Syntactic Level
The syntactic level was explored by running con-
stituency parsing (cp), dependency parsing (dp),
and shallow parsing (sp). Constituency trees were
produced by the Max-Ent reranking parser (Char-
niak, 2005). The constituency parse trees were
exploited by using three different classes of met-
rics that were designed to calculate the similarities
between the trees of two texts: overlapping in
function of a given part-of-speech; matching in
function of a given constituency type; and syntactic
tree matching (STM) metric proposed by (Liu and
Gildea, 2005).
Dependency trees were obtained using MINI-
PAR (Lin, 2003). Two types of metrics were used
to calculate the similarity between two texts using
dependency trees. In the first, different similarity
measures were calculated taking into consideration
three different perspectives: overlap of words that
hang in the same level or in a deeper level of the
dependency tree; overlap between words that hang
directly from terminal nodes given a specified part-
of-speech; and overlap between words that are ruled
by non-terminal nodes given a specified grammat-
ical relation (subject, object, relative clause, among
others). The second type is an implementation of the
head-word chain matching introduced in (Liu and
Gildea, 2005).
The shallow syntax approach proposed by
(Gime?nez, 2008) uses three different tools to ex-
plore the parts-of-speech, word lemmas and base
phrases chunks, respectively: SVMTool (Gime?nez
and Ma`rquez, 2004), Freeling (Carreras et al, 2004)
and Phreco (Carreras et al, 2005). In this type of
metrics the idea is to measure the similarity between
the two texts using parts-of-speech and chunk types.
The following metrics were used: overlapping
according to the part-of-speech; overlapping ac-
cording to the chunk type; the accumulated NIST
metric (Doddington, 2002) scores over different
625
Figure 1: A summary of the class of features explored.
sequences (lemmas, parts-of-speech, base phrase
chunks and chunk IOB labels).
2.1.3 Semantic Level
At the semantic level we aplored three different
types of information, namely: discourse represen-
tations, named entities and semantic roles. Here-
after they are respectively referred to as dr, ne, and
sr features. The discourse relations are automat-
ically annotated using the C&C Tools (Clark and
Curran, 2004). The following metrics using seman-
tic tree representations were proposed by (Gime?nez,
2008). A metric similar to the STM in which se-
mantic trees are used instead of constituency trees;
the overlapping between discourse representa-
tion structures according to their type; and the mor-
phosyntactic overlapping of discourse represen-
tation structures that share the same type.
Named entities metrics are calculated by com-
paring the entities that appear in each text. The
named entities were annotated using the BIOS pack-
age (Surdeanu et al, 2005). Two types of metrics
were used: the overlapping between the named
entities in each sentence according to their type and
the matching between the named entities in func-
tion of their type.
Semantic roles were automatically annotated us-
ing the SwiRL package (Surdeanu and Turmo,
2005). The arguments and adjuncts annotated in
each sentence are compared according to three dif-
ferent metrics: overlapping between the seman-
tic roles according to their type; the matching be-
tween the semantic roles according to their type; and
the overlapping of the roles without taking into
consideration their lexical realization.
2.2 Word Similarity Metrics
Besides the MT evaluation metrics, we experi-
mented with lexical semantics by calculating word
similarity metrics. For that, we followed a distri-
butional and a knowledge-based word similarity ap-
proach.
2.2.1 Distributional Word Similarity
As some previous work on semantic textual tex-
tual similarity (Mihalcea et al, 2006) and textual
entailment (Kouylekov et al, 2010; Mehdad et al,
2010) have shown, distributional word similarity
measures can improve the performance of both tasks
by allowing matches between terms that are lexically
different. We measure the word similarity comput-
ing a set of Latent Semantic Analysis (LSA) metrics
over Wikipedia. The 200,000 most visited articles
of Wikipedia were extracted and cleaned to build the
626
term-by-document matrix using the jLSI tool2.
Using this model we designed three different sim-
ilarity metrics that compute the similarity between
all elements in one text with all elements in the other
text. For two metrics we calculate the similarities
between different parts-of-speech: (i) similarity over
nouns and adjectives, and (ii) similarity over verbs.
The third metric computes the similarity between
all words in the two sentences. The similarity is
computed by averaging the pairwise similarity using
the LSA model between the elements of each text.
These metrics are hereafter called lsa.
2.2.2 Knowledge-based Word Similarity
In order to incorporate world knowledge informa-
tion about entities (persons, organizations, locations,
among others) into our model we experimented with
knowledge-based (thesaurus-based) word similarity
metrics. Usually such approaches have a very lim-
ited coverage of concepts due to the reduced size of
the available thesauri. In order to increase the cov-
erage we extracted concepts from the YAGO2 se-
mantic knowledge base (Hoffart et al, 2011) derived
from Wikipedia, Wordnet (Miller, 1995) and Geon-
ames3. YAGO2 contains knowledge about 10 mil-
lion entities and more than 120 million facts about
these entities.
In order to link the entities in the text to the enti-
ties in YAGO2 we have used ?The Wiki Machine?
(TWM) tool4. The tool solves the linking problem
by disambiguating each entity mention in the text
(excluding pronouns) using Wikipedia to provide the
sense inventory and the training data (Giuliano et
al., 2009). After preprocessing the datasets with
TWM the entities are annotated with their respective
Wikipedia entries represented by their URLs. Using
the entity?s URL it is possible to retrieve the Word-
net synsets related to the entity?s entry in YAGO2
and explore different knowledge-based metrics to
compute word similarity between entities.
In our experiments we selected three differ-
ent algorithms to calculate word similarity using
YAGO2: Wu-Palmer (Zhibiao and Palmer, 1994),
the Leacock-Chodorow (Leacock et al, 1998) and
2http://hlt.fbk.eu/en/technology/jlsi
3http://www.geonames.org/
4http://thewikimachine.fbk.eu/html/
index.html
the path distance (score based on the shortest path
that connects the senses in the Wordnet hyper-
nym/hyponym taxonomy). Two classes of metrics
were designed: (i) the average of the similarity be-
tween all the entities in each sentence and (ii) the
similarity of the pair of elements which have the
shortest path in the Wordnet taxonomy among all
possible pairs. There are six different metrics using
the three algorithms in total. An extra metric was
designed using only TWM. The metric is calculated
by taking the number of common entities in the two
sentences divided by the total number of entities an-
notated in the two sentences. The metrics described
in this section are part of the yago group.
3 Experiments and Discussion
In this section we present our experiments settings,
the configuration of the runs submitted and discuss
the results obtained. All our experiments were made
using half of the training set for training and half
for testing (development). Ten different random-
izations were run over the training data in order
to obtain ten different pairs of train/development
sets and reduce overfitting. We tried several differ-
ent regression algorithms and the best performance
was achieved with the implementation of Support
Vector Machines (SVM) of the SVMLight package
(Joachims, 1998). We used the radial basis function
kernel with default parameters without any special
tuning for the different datasets.
3.1 Submitted Runs and Results
Based on the results achieved with different feature
sets over training data we have selected the best
combinations for our submission. The feature sets
for each run are:
Run 1: lex, lsa, yago, and a selection of
features in the cp, dp, sp, dr, ne and sr
groups, forming a total of 286 features.
Run 2: lex, lsa, and yago, in a total of 50
features.
Run 3: lex and lsa, forming a total of 43
features.
The results obtained by our three submitted runs
are summarized in table 1. The table reports the
627
Runs submitted
Run 1 Run 2 Run 3 Base PE
Development 0.885 0.863 0.859 - -
Test
MSp 0.249 0.512 0.516 0.433 0.577
MSv 0.611 0.780 0.777 0.299 0.818
SMTe 0.149 0.379 0.441 0.454 0.450
Wn 0.421 0.622 0.629 0.586 0.629
SMTn 0.243 0.547 0.608 0.390 0.608
All 0.563 0.643 0.651 0.310 0.789
Allnrm 0.712 0.808 0.810 0.673 0.633
Mean 0.362 0.588 0.607 0.435 0.829
Table 1: Results of each run for each dataset (MSRpar,
MSRvid, SMTeuroparl, OnWn, SMTnews) calculated
with the Pearson correlation between the system?s out-
puts and the gold standard annotation. Official scores ob-
tained using the three evaluation scores All, Allnrm and
Mean. Development row presents the average results for
each run in the whole training dataset. Base is the of-
ficial baseline system. Post Evaluation is the experiment
ran after the evaluation period with models trained for the
specific datasets.
Pearson correlation between the system output and
the gold standard annotation provided by the task or-
ganizers. The table also presents the official scores
used to rank the systems and described in (Agirre et
al., 2012). Our best model, Run 3, was ranked 20th
according to the Mean score, 25th according to the
RankNrm score and 32th according to the All score
among 88 submitted runs.
The ?Development? row reports the results of our
three best models in the development phase. The
results obtained for the three training datasets are
higher than the results obtained for the testing. One
hypothesis that might explain this behavior is over-
fitting during the training phase due to the way we
divided the training set and carried out the experi-
ments. A different experiment setting to carry out
the development should be tried to evaluate this hy-
pothesis.
To our surprise, in the test datasets the results of
Run 1 and Run 3 swapped positions: in the train-
ing setting Run 1 was the best model and Run 3 the
third best. The performance of Run 3 was relatively
stable across the five datasets ranging from about
the 30th to the 48th position the exception being
the SMTnews dataset. In this dataset Run 3 was the
best performing run of the evaluation exercise (and
Run 2 the second). One possible explanation for this
behavior is the fact that Run 3 is based on lexical
features that do not take into consideration the syn-
tactic structure of the two texts and therefore is not
penalized by the noise introduced by the texts gen-
erated by MT systems. This hypothesis, however,
does not explain why Run 3 score for the SMTeu-
roparl dataset was below the baseline score. Error
analysis of the effects of different group of features
in the test datasets is required to better understand
such behaviors.
3.2 Post-evaluation Experiments
After the evaluation period, as a first step towards
the required error analysis and a better comprehen-
sion of the potential of our approach, we performed
an experiment to assess the impact of having mod-
els trained for specific datasets. In this experiment,
each training dataset (MSRpar, MSRvid and SMTeu-
roparl) was used to train a model. Each dataset?s
model was tested on its respective test dataset. The
model for the surprise datasets (OnWn and SMT-
news) were trained using the whole training dataset.
We used the Run 3 feature set (the best run in the
official evaluation). The results of the experiment
are reported in the column ?Exp? of table 1. The
impact of having specific models for each dataset
is high. The Mean score goes from .607 to .829
and improvements are also observed in the All score
(0.789). These scores would rank our system at the
7th position in the Mean rank. However, it is impor-
tant to notice that in a real-world setting, knowledge
about the source of data is not always available. We
consider that having a general model that does not
rely on this kind of information represents a more re-
alistic way to confront with real-world applications.
4 Final Remarks
In this paper we described FBK?s participation in
the STS Semeval 2012 task. Our approach is based
on a combination of MT evaluation metrics, distri-
butional, and knowledge-based word similarity met-
rics. Our best run achieved the 20th position among
88 runs in the Mean overall ranking. An error analy-
sis of the problematic test pairs is required to under-
stand the potential of our feature sets and improve
the overall performance of our approach. Along this
direction, a first experiment with our best features
and a different strategy already led to significant im-
provements in the Mean and All scores (from .651 to
628
.789 and from .607 to .829, respectively).
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853).
The authors would like to thank Claudio Giuliano
for kindly helping us to preprocess the datasets with
the Wiki Machine.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez. 2012. SemEval-2012 Task 6: A Pilot on Semantic
Textual Similarity. In 6th International Workshop on
Semantic Evaluation (SemEval 2012), in conjunction
with the First Joint Conference on Lexical and Com-
putational Semantics (*SEM 2012).
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization.
Xavier Carreras, Isaac Chao, Llu??s Padro, and Muntsa
Padro?. 2004. Freeling: An open-source suite of lan-
guage analyzers. In 4th International Conference on
Language Resources and Evaluation (LREC), pages
239?242.
Xavier Carreras, Llu??s Ma`rquez, and Jorge Catro. 2005.
Filtering-Ranking Perceptron Learning. Machine
Learning, 60:41?75.
Eugene Charniak. 2005. Coarse-to-fine n-best parsing
and MaxEnt discriminative reranking. In Proceedings
of the 43rd Annual Meeting on, volume 1, pages 173?
180.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In ACL ?04
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the Second Interna-
tional Conference on Human Language Technology
Research, pages 138?145. Morgan Kaufmann Publish-
ers Inc.
Andrew Finch and YS Hwang. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Third Inter-
national Workshop on Paraphrasing, pages 17?24.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In 4th International Conference on Lan-
guage Resources and Evaluation (LREC), pages 43?
46.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-) Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77?86.
J. Gime?nez. 2008. Empirical Machine Translation and
its Evaluation. Ph.D. thesis.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo
Strapparava. 2009. Kernel methods for minimally su-
pervised wsd. Computational Linguistics, 35(4):513?
528.
Johannes Hoffart, Fabian M. FM Suchanek, Klaus
Berberich, Edwin Lewis Kelham, Gerard de Melo, and
Gerhard Weikum. 2011. YAGO2: Exploring and
Querying World Knowledge in Time, Space, Context,
and Many Languages. In 20th International World
Wide Web Conference (WWW 2011), pages 229?232.
Thorsten Joachims. 1998. Making Large-Scale SVM
Learning Practical. In Bernhard Scholkopf, Christo-
pher J. C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods - Support Vector Learn-
ing, pages 41?56. MIT Press, Cambridge, USA.
Milen Kouylekov, Yashar Mehdad, and Matteo Negri.
2010. Mining Wikipedia for Large-Scale Reposito-
ries of Context-Sensitive Entailment Rules. In Seventh
international conference on Language Resources and
Evaluation (LREC 2010), pages 3550?3553, La Val-
letta, Malta.
Claudia Leacock, George A. Miller, and Martin
Chodorow. 1998. Using corpus statistics and Word-
Net relations for sense identification. Computational
Linguistics, 24(1):147?166.
C.Y. Lin and F.J. Och. 2004. Automatic evaluation
of machine translation quality using longest common
subsequence and skip-bigram statistics. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 605. Association for
Computational Linguistics.
Dekang Lin. 2003. Dependency-Based Evaluation of
Minipar. Text, Speech and Language Technology,
20:317?329.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization, num-
ber June, pages 25?32.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the ACL, number June,
pages 1020?1028.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation. In
629
Proceedings of the Joint Conference on Human Lan-
guage Technology and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL).
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence, pages
775?780.
George A. Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):39?
41.
Sonja Nie? en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for machine
translation: Fast evaluation for MT research. In Lan-
guage Resources and Evaluation, pages 0?6.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), number July, pages 311?318.
Gerard Salton, Amit Singhal, and Mandar Mitra. 1997.
Automatic text structuring and summarization. Infor-
mation Processing &amp;, 33(2):193?207.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Association for Machine Translation in the
Americas.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3):117?127,
December.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic
role labeling using complete syntactic analysis. In
9th Conference on Computational Natural Language
Learning (CoNLL), number June, pages 221?224.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
domain Speech. In 9th International Conference on
Speech Communication and Technology (Interspeech),
pages 3433?3436.
C Tillmann, S Vogel, H Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP Based Search for Statistical
Translation. In Fifth European Conference on Speech
Communication and Technology, pages 2667?2670.
Stephen Wan, Mark Dras, Robert Dale, and Ce?cile Paris.
2006. Using Dependency-Based Features to Take the
?Para-farce? out of Paraphrase. In 2006 Australasian
Language Technology Workshop (ALTW2006), num-
ber 2005, pages 131?138.
Wu Zhibiao and Martha Palmer. 1994. Verb Seman-
tics and Lexical Selection. In ACL ?94 Proceedings
of the 32nd annual meeting on Association for Com-
putational Linguistics, pages 133?138.
630
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 701?705,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
FBK: Cross-Lingual Textual Entailment Without Translation
Yashar Mehdad
FBK-irst
Trento , Italy
mehdad@fbk.eu
Matteo Negri
FBK-irst
Trento , Italy
negri@fbk.eu
Jose? Guilherme C. de Souza
FBK-irst & University of Trento
Trento, Italy
desouza@fbk.eu
Abstract
This paper overviews FBK?s participation
in the Cross-Lingual Textual Entailment
for Content Synchronization task organized
within SemEval-2012. Our participation is
characterized by using cross-lingual matching
features extracted from lexical and semantic
phrase tables and dependency relations. The
features are used for multi-class and binary
classification using SVMs. Using a combi-
nation of lexical, syntactic, and semantic fea-
tures to create a cross-lingual textual entail-
ment system, we report on experiments over
the provided dataset. Our best run achieved
an accuracy of 50.4% on the Spanish-English
dataset (with the average score and the me-
dian system respectively achieving 40.7% and
34.6%), demonstrating the effectiveness of a
?pure? cross-lingual approach that avoids in-
termediate translations.
1 Introduction
So far, cross-lingual textual entailment (CLTE)
(Mehdad et al, 2010) has been applied to: i)
available TE datasets (?YES?/?NO? uni-directional
relations between monolingual pairs) transformed
into their cross-lingual counterpart by translating
the hypotheses into other languages (Negri and
Mehdad, 2010), and ii) machine translation evalu-
ation datasets (Mehdad et al, 2012b). The content
synchronization task represents a challenging appli-
cation scenario to test the capabilities of CLTE sys-
tems, by proposing a richer inventory of phenomena
(i.e. ?Bidirectional?/?Forward?/?Backward?/?No
entailment? multi-directional entailment relations).
Multi-directional CLTE recognition can be seen
as the identification of semantic equivalence and in-
formation disparity between two topically related
sentences, at the cross-lingual level. This is a core
aspect of the multilingual content synchronization
task, which represents a challenging application sce-
nario for a variety of NLP technologies, and a shared
research framework for the integration of semantics
and MT technology.
The CLTE methods proposed so far adopt either
a ?pivoting approach? (translation of the two in-
put texts into the same language, as in (Mehdad et
al., 2010)), or an ?integrated solution? that exploits
bilingual phrase tables to capture lexical relations
and contextual information (Mehdad et al, 2011).
The promising results achieved with the integrated
approach still rely on phrasal matching techniques
that disregard relevant semantic aspects of the prob-
lem. By filling this gap integrating linguistically
motivated features, in our participation, we propose
an approach that combines lexical, syntactic and se-
mantic features within a machine learning frame-
work (Mehdad et al, 2012a).
Our submitted runs have been produced by train-
ing and optimizing multiclass and binary SVM clas-
sifiers, over the Spanish-English (Spa-Eng) devel-
opment set. In both cases, our results were posi-
tive, showing significant improvements over the me-
dian systems and average scores obtained by partic-
ipants. The overall results confirm the difficulty of
the task, and the potential of our approach in com-
bining linguistically motivated features in a ?pure?
cross-lingual approach that avoids the recourse to
external MT components.
701
2 Experiments
In our experiment we used the Spa-Eng portion of
the dataset described in (Negri et al, 2012; Negri
et al, 2011), consisting of 500 multi-directional en-
tailment pairs which was provided to train the sys-
tems and 500 pairs for the submission. Each pair in
the dataset is annotated with ?Bidirectional?, ?For-
ward?, ?Backward? or ?No entailment? judgements.
2.1 Approach
Our system builds on the integration of lexical,
syntactic and semantic features in a supervised
learning framework. Our model builds on three
main feature sets, respectively derived from: i)
phrase tables, ii) dependency relations, and iii)
semantic phrase tables.
1. Phrase Table (PT) matching: through
these features, a semantic judgement about entail-
ment is made exclusively on the basis of lexical
evidence. The matching features are calculated
with a phrase-to-phrase matching process. A phrase
in our approach is an n-gram composed of one
or more (up to 5) consecutive words, excluding
punctuation. Entailment decisions are assigned
combining phrasal matching scores calculated for
each level of n-grams (i.e. considering the number
of 1-grams, 2-grams,..., 5-grams extracted from H
that match with n-grams in T). Phrasal matches,
performed either at the level of tokens, lemmas, or
stems, can be of two types:
1. Exact: in the case that two phrases are identical
at one of the three levels (token, lemma, stem).
2. Lexical: in the case that two different phrases
can be mapped through entries of the resources
used to bridge T and H (i.e. phrase tables).
For each phrase in H, we first search for exact
matches at the level of token with phrases in T. If
no match is found at a token level, the other levels
(lemma and stem) are attempted. Then, in case of
failure with exact matching, lexical matching is per-
formed at the same three levels. To reduce redun-
dant matches, the lexical matches between pairs of
phrases which have already been identified as exact
matches are not considered.
Once the matching phase for each n-gram
level has been concluded, the number of matches
Matchn and the number of phrases in the hypoth-
esis H(n) is used to estimate the portion of phrases
in H that are matched at each level n (Equation 1).1
Since languages can express the same meaning with
different amounts of words, a phrase with length n
in H can match a phrase with any length in T.
Matchn =
Matchn
|H(n)|
(1)
In order to build English-Spanish phrase tables
for our experiments, we used the freely available
Europarl V.4, News Commentary and United
Nations Spanish-English parallel corpora released
for the WMT10 Shared Translation Task.2 We
run the TreeTagger (Schmid, 1995) and Snowball
stemmer (Porter, 2001) for preprocessing, and used
the Giza++ (Och and Ney, 2000) toolkit to align the
tokenized corpora at the word level. Subsequently,
we extracted the bi-lingual phrase table from the
aligned corpora using the Moses toolkit (Koehn et
al., 2007).
2. Dependency Relation (DR) matching tar-
gets the increase of CLTE precision. By adding
syntactic constraints to the matching process,
DR features aim to reduce wrong matches often
occurring at the lexical level. For instance, the con-
tradiction between ?Yahoo acquired Overture? and
?Overture compro? Yahoo? is evident when syntax
(in this case subject-object inversion) is taken into
account, but can not be caught by bag-of-words
methods.
We define a dependency relation as a triple that
connects pairs of words through a grammatical rela-
tion. For example, ?nsubj (loves, John)? is a depen-
dency relation with head loves and dependent John
connected by the relation nsubj, which means that
?John? is the subject of ?loves?. DR matching cap-
tures similarities between dependency relations, by
combining the syntactic and lexical level. In a valid
match, while the relation has to be the same (?exact?
1When checking for entailment from H to T, the normaliza-
tion is carried out dividing the number of n-grams in H by the
number of n-grams in T. The same holds for dependency rela-
tion and semantic phrase table matching.
2http://www.statmt.org/wmt10/
702
match), the connected words must be either the same
or semantically equivalent in the two languages. For
example, ?nsubj (loves, John)? can match ?nsubj
(ama, John)? and ?nsubj (quiere, John)? but not
?dobj (quiere, John)?.
Given the dependency tree representations of T
and H, for each grammatical relation (r) we calcu-
late a DR matching score (Matchr, see Equation 2)
as the number of matching occurrences of r in T and
H (respectively DRr(T ) and DRr(H)), divided by
the number of occurrences of r in H.
matchr =
|match(DRr(T ), DRr(H))|
|DRr(H)|
(2)
In our experiments, in order to extract de-
pendency relation (DR) matching features, the
dependency tree representations of English and
Spanish texts have been produced with DepPattern
(Otero and Lopez, 2011). We then mapped the
sets of dependency relation labels for the English-
Spanish parser output into: Adjunct, Determiner,
Object, Subject and Preposition. The dictionary,
containing about 9M bilingual word pairs, created
during the alignment of the English-Spanish parallel
corpora provided the lexical knowledge to perform
matches when the connected words are different.
3. Semantic Phrase Table (SPT) matching:
represents a novel way to leverage the integration
of semantics and MT-derived techniques. To this
aim, SPT improves CLTE methods relying on pure
lexical match, by means of ?generalized? phrase
tables annotated with shallow semantic labels.
Semantically enhanced phrase tables, with entries in
the form ?[LABEL] word1...wordn [LABEL]? (e.g.
?[ORG] acquired [ORG]?), are used as a recall-
oriented complement to the lexical phrase tables
used in machine translation (token-based entries like
?Yahoo acquired Overture?). The main motivation
for this augmentation is that word replacement with
semantic tags allows to match T-H tokens that do
not occur in the original bilingual parallel corpora
used for phrase table extraction. Our hypothesis
is that the increase in recall obtained from relaxed
matches through semantic tags in place of ?out of
vocabulary? terms (e.g. unseen person, location, or
organization names) is an effective way to improve
CLTE performance, even at the cost of some loss in
precision. Semantic phrase tables, however, have
two additional advantages. The first is related to
their smaller size and, in turn, its positive impact
on system?s efficiency, due to the considerable
search space reduction. Semantic tags allow to
merge different sequences of tokens into a single tag
and, consequently, different phrase entries can be
unified to one semantic phrase entry. As a result, for
instance, the SPT used in our experiments is more
than 30% smaller than the original token-based one.
The second advantage relates to their potential im-
pact on the confidence of CLTE judgements. Since
a semantic tag might cover more than one token
in the original entry phrase, SPT entries are often
short generalizations of longer original phrases.
Consequently, the matching process can benefit
from the increased probability of mapping higher
order n-grams (i.e. those providing more contextual
information) from H into T and vice-versa.
Like lexical phrase tables, SPTs are extracted
from parallel corpora. As a first step, we annotate
the corpora with named-entity taggers (FreeLing in
our case (Carreras et al, 2004)) for the source and
target languages, replacing named entities with gen-
eral semantic labels chosen from a coarse-grained
taxonomy including the categories: person, location,
organization, date and numeric expression. Then,
we combine the sequences of unique labels into one
single token of the same label, and we run Giza++
(Och and Ney, 2000) to align the resulting seman-
tically augmented corpora. Finally, we extract the
semantic phrase table from the augmented aligned
corpora using the Moses toolkit (Koehn et al, 2007).
For the matching phase, we first annotate T and
H in the same way we labeled our parallel corpora.
Then, for each n-gram order (n=1 to 5, excluding
punctuation), we use the SPT to calculate a matching
score (SPT matchn, see Equation 3), as the num-
ber of n-grams in H that match with phrases in T
divided by the number of n-grams in H. The match-
ing algorithm is same as the phrase table matching
one.
SPT matchn =
|SPTn(H) ? SPT (T )|
|SPTn(H)|
(3)
703
Run Features Classification Parameter selection Result
1 PT+SPT+DR Multiclass Entire training set 0.502
2 PT+SPT+DR Multiclass 2-fold cross validation 0.490
3 PT+SPT+DR Binary Entire training set 0.504
4 PT+SPT+DR Binary 2-fold cross validation 0.500
Table 1: Summary of the submitted runs and results for Spa-Eng dataset.
Forward Backward No entailment Bidirectional
P R F1 P R F1 P R F1 P R F1
0.515 0.704 0.595 0.546 0.568 0.557 0.447 0.304 0.362 0.482 0.440 0.460
Table 2: Best run?s Precision/Recall/F1 scores.
In our supervised learning framework, the com-
puted PT, SPT and DR scores are used as sepa-
rate features, giving to an SVM classifier, LIBSVM
(Chang and Lin, 2011), the possibility to learn opti-
mal feature weights from training data.
2.2 Submitted runs
In order to test our models under different condi-
tions, we set the CLTE problem both as two-way and
multiclass classification tasks.
Two-way classification casts multidirectional en-
tailment as a unidirectional problem, where each
pair is analyzed checking for entailment both from
left to right and from right to left. In this condi-
tion, each original test example is correctly clas-
sified if both pairs originated from it are correctly
judged (?YES-YES? for bidirectional, ?YES-NO?
for forward, ?NO-YES? for backward entailment,
and ?NO-NO? for no entailment). Two-way clas-
sification represents an intuitive solution to capture
multidirectional entailment relations but, at the same
time, a suboptimal approach in terms of efficiency
since two checks are performed for each pair.
Multiclass classification is more efficient, but at
the same time more challenging due to the higher
difficulty of multiclass learning, especially with
small datasets. We also tried to use the parameter se-
lection tool for C-SVM classification using the RBF
(radial basis function) kernel, available in LIBSVM
package. Our submitted runs and results have been
obtained with the settings summarized in table 1.
As can be seen from the table, our best result has
been achieved by Run 3 (50.4% accuracy), which
is significantly higher than the average and median
score over the best runs obtained by participants
(44.0% and 40.7% respectively). The detailed re-
sults achieved by the best run are reported in Table
2. We can observe that our system is performing
well for recognizing the unidirectional entailment
(i.e. forward and backward), while the performance
drops over no entailment pairs. The low results for
bidirectional cases also reflect the difficulty of dis-
criminating the no entailment pairs from the bidi-
rectional ones. Looking at the detailed results, we
can observe a high recall in the forward and back-
ward entailment cases, which could be explained by
the effectiveness of the semantic phrase table match-
ing features aiming at coverage increase over lexi-
cal methods. Adding more linguistically motivated
features and weighting the non-matched phrases can
be a starting point to improve the overall results for
other cases (bidirectional and no entailment).
3 Conclusion
In this paper we described our participation to the
cross-lingual textual entailment for content synchro-
nization task at SemEval-2012. We approached this
task by combining lexical, syntactic and semantic
features, at the cross-lingual level without recourse
to intermediate translation steps. In spite of the
difficulty and novelty of the task, our results on
the Spanish-English dataset (0.504) prove the effec-
tiveness of the approach with significant improve-
ments over the reported average and median accu-
racy scores for the 29 submitted runs (respectively
40.7% and 34.6%).
Acknowledgments
This work has been partially supported by the EU-
funded project CoSyne (FP7-ICT-4-248531).
704
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04).
C.C. Chang and C.J. Lin. 2011. LIBSVM: A Library
for Support Vector Machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3).
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions (ACL 2007).
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012a. Detect-
ing Semantic Equivalence and Information Disparity
in Cross-lingual Documents. In Proceedings of the
ACL?12.
Y. Mehdad, M. Negri, and M. Federico. 2012b. Match
without a Referee: Evaluating MT Adequacy without
Reference Translations. In Proceedings of the Ma-
chine Translation Workshop (WMT2012).
M. Negri and Y. Mehdad. 2010. Creating a Bi-lingual
Entailment Corpus through Translations with Mechan-
ical Turk: $100 for a 10-day rush. In Proceedings of
the NAACL HLT 2010 Workshop on Creating Speech
and Language Data with Amazon?s Mechanical Turk,
pages 212?216. Association for Computational Lin-
guistics.
M. Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, and
A. Marchetti. 2011. Divide and conquer: Crowd-
sourcing the creation of cross-lingual textual entail-
ment corpora. In Proceedings of EMNLP 2011.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
P.G. Otero and I.G. Lopez. 2011. A Grammatical For-
malism Based on Patterns of Part-of-Speech Tags. In-
ternational journal of corpus linguistics, 16(1).
M. Porter. 2001. Snowball: A language for stemming
algorithms.
H. Schmid. 1995. Treetaggera language indepen-
dent part-of-speech tagger. Institut fu?r Maschinelle
Sprachverarbeitung, Universita?t Stuttgart.
705
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 25?33, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
Semeval-2013 Task 8:
Cross-lingual Textual Entailment for Content Synchronization
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Alessandro Marchetti
CELCT
Trento, Italy
amarchetti@celct.it
Yashar Mehdad
UBC
Vancouver, Canada
mehdad@cs.ubc.ca
Luisa Bentivogli
FBK-irst
Trento, Italy
bentivo@fbk.eu
Danilo Giampiccolo
CELCT
Trento, Italy
giampiccolo@celct.it
Abstract
This paper presents the second round of the
task on Cross-lingual Textual Entailment for
Content Synchronization, organized within
SemEval-2013. The task was designed to pro-
mote research on semantic inference over texts
written in different languages, targeting at the
same time a real application scenario. Par-
ticipants were presented with datasets for dif-
ferent language pairs, where multi-directional
entailment relations (?forward?, ?backward?,
?bidirectional?, ?no entailment?) had to be
identified. We report on the training and test
data used for evaluation, the process of their
creation, the participating systems (six teams,
61 runs), the approaches adopted and the re-
sults achieved.
1 Introduction
The cross-lingual textual entailment task (Mehdad et
al., 2010) addresses textual entailment (TE) recog-
nition (Dagan and Glickman, 2004) under the new
dimension of cross-linguality, and within the new
challenging application scenario of content synchro-
nization. Given two texts in different languages, the
cross-lingual textual entailment (CLTE) task con-
sists of deciding if the meaning of one text can be
inferred from the meaning of the other text. Cross-
linguality represents an interesting direction for re-
search on recognizing textual entailment (RTE), es-
pecially due to its possible application in a vari-
ety of tasks. Among others (e.g. question answer-
ing, information retrieval, information extraction,
and document summarization), multilingual content
synchronization represents a challenging application
scenario to evaluate CLTE recognition components
geared to the identification of sentence-level seman-
tic relations.
Given two documents about the same topic writ-
ten in different languages (e.g. Wikipedia pages),
the content synchronization task consists of au-
tomatically detecting and resolving differences in
the information they provide, in order to produce
aligned, mutually enriched versions of the two docu-
ments (Monz et al, 2011; Bronner et al, 2012). To-
wards this objective, a crucial requirement is to iden-
tify the information in one page that is either equiv-
alent or novel (more informative) with respect to the
content of the other. The task can be naturally cast
as an entailment recognition problem, where bidi-
rectional and unidirectional entailment judgements
for two text fragments are respectively mapped into
judgements about semantic equivalence and novelty.
The task can also be seen as a machine translation
evaluation problem, where judgements about se-
mantic equivalence and novelty depend on the pos-
sibility to fully or partially translate a text fragment
into the other.
The recent advances on monolingual TE on the
one hand, and the methodologies used in Statisti-
cal Machine Translation (SMT) on the other, offer
promising solutions to approach the CLTE task. In
line with a number of systems that model the RTE
task as a similarity problem (i.e. handling similar-
ity scores between T and H as features contributing
to the entailment decision), the standard sentence
and word alignment programs used in SMT offer
a strong baseline for CLTE (Mehdad et al, 2011;
25
Figure 1: Example of SP-EN CLTE pairs.
Mehdad et al, 2012). However, although repre-
senting a solid starting point to approach the prob-
lem, similarity-based techniques are just approx-
imations, open to significant improvements com-
ing from semantic inference at the multilingual
level (e.g. cross-lingual entailment rules such as
?perro???animal?). Taken in isolation, similarity-
based techniques clearly fall short of providing an
effective solution to the problem of assigning direc-
tions to the entailment relations (especially in the
complex CLTE scenario, where entailment relations
are multi-directional). Thanks to the contiguity be-
tween CLTE, TE and SMT, the proposed task pro-
vides an interesting scenario to approach the issues
outlined above from different perspectives, and of-
fers large room for mutual improvement.
Building on the success of the first CLTE evalua-
tion organized within SemEval-2012 (Negri et al,
2012a), the remainder of this paper describes the
second evaluation round organized within SemEval-
2013. The following sections provide an overview
of the datasets used, the participating systems, the
approaches adopted, the achieved results, and the
lessons learned.
2 The task
Given a pair of topically related text fragments (T1
and T2) in different languages, the CLTE task con-
sists of automatically annotating it with one of the
following entailment judgements (see Figure 1 for
Spanish/English examples of each judgement):
? bidirectional (T1?T2 & T1?T2): the two
fragments entail each other (semantic equiva-
lence);
? forward (T1?T2 & T16?T2): unidirectional
entailment from T1 to T2;
? backward (T16?T2 & T1?T2): unidirectional
entailment from T2 to T1;
? no entailment (T16?T2 & T16?T2): there is
no entailment between T1 and T2 in either di-
rection;
In this task, both T1 and T2 are assumed to be
true statements. Although contradiction is relevant
from an application-oriented perspective, contradic-
tory pairs are not present in the dataset.
3 Dataset description
The CLTE-2013 dataset is composed of four CLTE
corpora created for the following language combi-
nations: Spanish/English (SP-EN), Italian/English
(IT-EN), French/English (FR-EN), German/English
(DE-EN). Each corpus consists of 1,500 sentence
pairs (1,000 for training and 500 for test), balanced
across the four entailment judgements.
In this year?s evaluation, as training set we used
the CLTE-2012 corpus1 that was created for the
SemEval-2012 evaluation exercise2 (including both
training and test sets). The CLTE-2013 test set was
created from scratch, following the methodology de-
scribed in the next section.
3.1 Data collection and annotation
To collect the entailment pairs for the 2013 test set
we adopted a slightly modified version of the crowd-
sourcing methodology followed to create the CLTE-
2012 corpus (Negri et al, 2011). The main differ-
ence with last year?s procedure is that we did not
take advantage of crowdsourcing for the whole data
collection process, but only for part of it.
As for CLTE-2012, the collection and annotation
process consists of the following steps:
1. First, English sentences were manually ex-
tracted from Wikipedia and Wikinews. The se-
lected sentences represent one of the elements
(T1) of each entailment pair;
1http://www.celct.it/resources.php?id page=CLTE
2http://www.cs.york.ac.uk/semeval-2012/task8/
26
2. Next, each T1 was modified in various ways
in order to obtain a corresponding T2. While
in the CLTE-2012 dataset the whole T2 cre-
ation process was carried out through crowd-
sourcing, for the CLTE-2013 test set we crowd-
sourced only the first phase of T1 modification,
namely the creation of paraphrases. Focusing
on the creation of high quality paraphrases, we
followed the crowdsourcing methodology ex-
perimented in Negri et al (2012b), in which
a paraphrase is obtained through an itera-
tive modification process of an original sen-
tence, by asking workers to introduce meaning-
preserving lexical and syntactic changes. At
each round of the iteration, new workers are
presented with the output of the previous iter-
ation in order to increase divergence from the
original sentence. At the end of the process,
only the more divergent paraphrases according
to the Lesk score (Lesk, 1986) are selected. As
for the second phase of T2 creation process,
this year it was carried out by expert annota-
tors, who followed the same criteria used last
year for the crowdsourced tasks, i.e. i) remove
information from the input (paraphrased) sen-
tence and ii) add information from sentences
surrounding T1 in the source article;
3. Each T2 was then paired to the original T1, and
the resulting pairs were annotated with one of
the four entailment judgements. In order to re-
duce the correlation between the difference in
sentences? length and entailment judgements,
only the pairs where the difference between the
number of words in T1 and T2 (length diff ) was
below a fixed threshold (10 words) were re-
tained.3 The final result is a monolingual En-
glish dataset annotated with multi-directional
entailment judgements, which are well dis-
tributed over length diff values ranging from 0
to 9;
4. In order to create the cross-lingual datasets,
each English T1 was manually translated into
3Such constraint has been applied in order to focus as much
as possible on semantic aspects of the problem, by reduc-
ing the applicability of simple association rules such as IF
length(T1)>length(T2) THEN T1?T2.
four different languages (i.e. Spanish, German,
Italian and French) by expert translators;
5. By pairing the translated T1 with the cor-
responding T2 in English, four cross-lingual
datasets were obtained.
To ensure the good quality of the datasets, all the
collected pairs were cross-annotated and filtered to
retain only those pairs with full agreement in the
entailment judgement between two expert annota-
tors. The final result is a multilingual parallel en-
tailment corpus, where T1s are in 5 different lan-
guages (i.e. English, Spanish, German, Italian, and
French), and T2s are in English. It is worth men-
tioning that the monolingual English corpus, a by-
product of our data collection methodology, will be
publicly released as a further contribution to the re-
search community.
3.2 Dataset statistics
As described in section 3.1, the methodology fol-
lowed to create the training and test sets was the
same except for the crowdsourced tasks. This al-
lowed us to obtain two datasets with the same bal-
ance across the entailment judgements, and to keep
under control the distribution of the pairs for differ-
ent length diff values in each language combination.
Training Set. The training set is composed of
1,000 CLTE pairs for each language combina-
tion, balanced across the four entailment judge-
ments (bidirectional, forward, backward, and
no entailment). As shown in Table 1, our data col-
lection procedure led to a dataset where the major-
ity of the pairs falls in the +5 -5 length diff range
for each language pair (67.2% on average across the
four language pairs). This characteristic is partic-
ularly relevant as our assumption is that such data
distribution makes entailment judgements based on
mere surface features such as sentence length inef-
fective, thus encouraging the development of alter-
native, deeper processing strategies.
Test Set. The test set is composed of 500 entail-
ment pairs for each language combination, balanced
across the four entailment judgements. As shown
in Table 2, also in this dataset the majority of the
collected entailment pairs is uniformly distributed
27
(a) SP-EN (b) IT-EN
(c) FR-EN (d) DE-EN
Figure 2: Pair distribution in the 2013 test set: total number of pairs (y-axis) for different length diff values (x-axis).
SP-EN IT-EN FR-EN DE-EN
Forward 104 132 121 179
Backward 202 182 191 123
No entailment 163 173 169 174
Bidirectional 175 199 193 209
ALL 644 686 674 685
% (out of 1,000) 64.4 68.6 67.4 68.5
Table 1: Training set pair distribution within the -5/+5
length diff range.
in the [-5,+5] length diff range (68.1% on average
across the four language pairs).
However, comparing training and test set for
each language pair, it can be seen that while the
Spanish-English and Italian-English datasets are ho-
mogeneous with respect to the length diff feature,
the French-English and German-English datasets
present noticeable differences between training and
test set. These figures show that, despite the consid-
erable effort spent to produce comparable training
SP-EN IT-EN FR-EN DE-EN
backward 82 89 82 102
bidirectional 89 92 90 106
forward 69 78 76 98
no entailment 71 80 59 100
ALL 311 339 307 406
% (out of 500) 62.2 67.8 61.4 81.2
Table 2: Test set pair distribution within the -5/+5
length diff range.
and test sets, the ideal objective of a full homogene-
ity between the datasets for these two languages was
difficult to reach.
Complete details about the distribution of the
pairs in terms of length diff for the four cross-
lingual corpora in the test set are provided in Figure
2. Vertical bars represent, for each length diff value,
the proportion of pairs belonging to the four entail-
ment classes.
28
4 Evaluation metrics and baselines
Evaluation results have been automatically com-
puted by comparing the entailment judgements re-
turned by each system with those manually assigned
by human annotators in the gold standard. The met-
rics used for systems? ranking is accuracy over the
whole test set, i.e. the number of correct judge-
ments out of the total number of judgements in the
test set. Additionally, we calculated precision, re-
call, and F1 measures for each of the four entail-
ment judgement categories taken separately. These
scores aim at giving participants the possibility to
gain clearer insights into their system?s behaviour on
the entailment phenomena relevant to the task.
To allow comparison with the CLTE-2012 re-
sults, the same three baselines were calculated on the
CLTE-2013 test set for each language combination.
The first one is the 0.25 accuracy score obtained by
assigning each test pair in the balanced dataset to
one of the four classes. The other two baselines con-
sider the length difference between T1 and T2:
? Composition of binary judgements (Bi-
nary). To calculate this baseline an SVM
classifier is trained to take binary en-
tailment decisions (?YES?, ?NO?). The
classifier uses length(T1)/length(T2) and
length(T2)/length(T1) as features respectively
to check for entailment from T1 to T2 and vice-
versa. For each test pair, the unidirectional
judgements returned by the two classifiers are
composed into a single multi-directional judge-
ment (?YES-YES?=?bidirectional?, ?YES-
NO?=?forward?, ?NO-YES?=?backward?,
?NO-NO?=?no entailment?);
? Multi-class classification (Multi-class). A
single SVM classifier is trained with the same
features to directly assign to each pair one of
the four entailment judgements.
Both the baselines have been calculated with the
LIBSVM package (Chang and Lin, 2011), using de-
fault parameters. Baseline results are reported in Ta-
ble 3.
Although the four CLTE datasets are derived from
the same monolingual EN-EN corpus, baseline re-
sults present slight differences due to the effect of
translation into different languages. With respect to
last year?s evaluation, we can observe a slight drop
in the binary classification baseline results. This
might be due to the fact that the length distribution
of examples is slightly different this year. How-
ever, there are no significant differences between the
multi-class baseline results of this year in compar-
ison with the previous round results. This might
suggest that multi-class classification is a more ro-
bust approach for recognizing multi-directional en-
tailment relations. Moreover, both baselines failed
in capturing the ?no-entailment? examples in all
datasets (F1no?entailment = 0).
SP-EN IT-EN FR-EN DE-EN
1-class 0.25 0.25 0.25 0.25
Binary 0.35 0.39 0.37 0.39
Multi-class 0.43 0.44 0.42 0.42
Table 3: Baseline accuracy results.
5 Submitted runs and results
Like in the 2012 round of the CLTE task, partici-
pants were allowed to submit up to five runs for each
language combination. A total of twelve teams reg-
istered for participation and downloaded the train-
ing set. Out of them, six4 submitted valid runs.
Five teams produced submissions for all the four
language combinations, while one team participated
only in the DE-EN task. In total, 61 runs have been
submitted and evaluated (16 for DE-EN, and 15 for
each of the other language pairs).
Accuracy results are reported in Table 4. As can
be seen from the table, the performance of the best
systems is quite similar across the four language
combinations, with the best submissions achieving
results in the 43.4-45.8% accuracy interval. Simi-
larly, also average and median results are close to
each other, with a small drop on DE-EN. This drop
might be explained by the difference between the
training and test set with respect to the length diff
feature. Moreover, the performance of DE-EN auto-
matic translation might affect approaches based on
?pivoting?, (i.e. addressing CLTE by automatically
translating T1 in the same language of T2, as de-
scribed in Section 6).
4Including the task organizers.
29
System name SP-EN IT-EN FR-EN DE-EN
altn run1* 0.428 0.432 0.420 0.388
BUAP run1 0.364 0.358 0.368 0.322
BUAP run2 0.374 0.358 0.364 0.318
BUAP run3 0.380 0.358 0.362 0.316
BUAP run4 0.364 0.388 0.392 0.350
BUAP run5 0.386 0.360 0.372 0.318
celi run1 0.340 0.324 0.334 0.342
celi run2 0.342 0.324 0.340 0.342
ECNUCS run1 0.428 0.426 0.438 0.422
ECNUCS run2 0.404 0.420 0.450 0.436
ECNUCS run3 0.408 0.426 0.458 0.432
ECNUCS run4 0.422 0.416 0.436 0.452
ECNUCS run5 0.392 0.402 0.442 0.426
SoftCard run1 0.434 0.454 0.416 0.414
SoftCard run2 0.432 0.448 0.426 0.402
umelb run1 ? ? ? 0.324
Highest 0.434 0.454 0.458 0.452
Average 0.404 0.404 0.401 0.378
Median 0.428 0.426 0.420 0.369
Lowest 0.342 0.324 0.340 0.324
Table 4: CLTE-2013 accuracy results (61 runs) over the
4 language combinations. Highest, average, median and
lowest scores are calculated considering only the best run
for each team (*task organizers? system).
Compared to the results achieved last year, shown
in Table 5, a sensible decrease in the highest scores
can be observed. While in CLTE-2012 the top sys-
tems achieved an accuracy well above 0.5 (with a
maximum of 0.632 in SP-EN), the results for this
year are far below such level (the peak is now at
45,8% for FR-EN). A slight decrease with respect
to 2012 can also be noted for average performances.
However, it?s worth remarking the general increase
of the lowest and median scores, which are less sen-
sitive to isolate outstanding results achieved by sin-
gle teams. This indicates that a progress in CLTE
research has been made building on the lessons
learned after the first round of the initiative.
To better understand the behaviour of each sys-
tem, Table 6 provides separate precision, recall, and
F1 scores for each entailment judgement, calculated
over the best runs of each participating team. In
contrast to CLTE-2012, where the ?bidirectional?
and ?no entailment? categories consistently proved
to be more problematic than ?forward? and ?back-
ward? judgements, this year?s results are more ho-
mogeneous across the different classes. Neverthe-
less, on average, the classification of ?bidirectional?
pairs is still worse for three language pairs (SP-EN,
IT-EN and FR-EN), and results for ?no entailment?
are lower for two of them (SP-EN and DE-EN).
SP-EN IT-EN FR-EN DE-EN
Highest 0.632 0.566 0.570 0.558
Average 0.440 0.411 0.408 0.408
Median 0.407 0.350 0.365 0.363
Lowest 0.274 0.326 0.296 0.296
Table 5: CLTE-2012 accuracy results. Highest, average,
median and lowest scores are calculated considering only
the best run for each team.
As regards the comparison with the baselines,
this year?s results confirm that the length diff -based
baselines are hard to beat. More specifically, most
of the systems are slightly above the binary classi-
fication baseline (with the exception of the DE-EN
dataset where only two systems out of six outper-
formed it), whereas for all the language combina-
tions the multi-class baseline was beaten only by the
best participating system.
This shows that, despite the effort in keeping the
distribution of the entailment classes uniform across
different length diff values, eliminating the correla-
tion between sentence length and correct entailment
decisions is difficult. As a consequence, although
disregarding semantic aspects of the problem, fea-
tures considering length information are quite ef-
fective in terms of overall accuracy. Such features,
however, perform rather poorly when dealing with
challenging cases (e.g. ?no-entailment?), which are
better handled by participating systems.
6 Approaches
A rough classification of the approaches adopted by
participants can be made along two orthogonal di-
mensions, namely:
? Pivoting vs. Cross-lingual. Pivoting meth-
ods rely on the automatic translation of one of
the two texts (either single words or the en-
tire sentence) into the language of the other
(typically English) in order perform monolin-
gual TE recognition. Cross-lingual methods
assign entailment judgements without prelim-
inary translation.
? Composition of binary judgements vs.
Multi-class classification. Compositional ap-
proaches map unidirectional (?YES?/?NO?)
30
SP-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full spa-eng 0.509 0.464 0.485 0.440 0.264 0.330 0.464 0.416 0.439 0.357 0.568 0.438
BUAP spa-eng run5 0.446 0.360 0.398 0.521 0.296 0.378 0.385 0.456 0.418 0.300 0.432 0.354
celi spa-eng run2 0.396 0.352 0.373 0.431 0.400 0.415 0.325 0.328 0.327 0.245 0.288 0.265
ECNUCS spa-eng run1 0.458 0.432 0.444 0.533 0.320 0.400 0.406 0.416 0.411 0.380 0.544 0.447
SoftCard spa-eng run1 0.462 0.344 0.394 0.619 0.480 0.541 0.418 0.472 0.444 0.325 0.440 0.374
AVG. 0.454 0.390 0.419 0.509 0.352 0.413 0.400 0.418 0.408 0.321 0.454 0.376
IT-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full ita-eng 0.448 0.376 0.409 0.417 0.344 0.377 0.512 0.496 0.504 0.374 0.512 0.432
BUAP ita-eng run4 0.418 0.328 0.368 0.462 0.384 0.419 0.379 0.440 0.407 0.327 0.400 0.360
celi ita-eng run1 0.288 0.256 0.271 0.395 0.408 0.402 0.336 0.304 0.319 0.279 0.328 0.301
ECNUCS ita-eng run1 0.422 0.456 0.438 0.592 0.336 0.429 0.440 0.440 0.440 0.349 0.472 0.401
SoftCard ita-eng run1 0.514 0.456 0.483 0.612 0.480 0.538 0.392 0.464 0.425 0.364 0.416 0.388
AVG. 0.418 0.374 0.394 0.496 0.390 0.433 0.412 0.429 0.419 0.339 0.426 0.376
FR-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full fra-eng 0.405 0.392 0.398 0.420 0.296 0.347 0.500 0.440 0.468 0.381 0.552 0.451
BUAP fra-eng run4 0.407 0.472 0.437 0.431 0.376 0.402 0.379 0.376 0.378 0.352 0.344 0.348
celi fra-eng run2 0.394 0.344 0.368 0.364 0.376 0.370 0.352 0.352 0.352 0.263 0.288 0.275
ECNUCS fra-eng run3 0.422 0.432 0.427 0.667 0.352 0.461 0.514 0.432 0.470 0.383 0.616 0.472
SoftCard fra-eng run2 0.477 0.416 0.444 0.556 0.400 0.465 0.412 0.432 0.422 0.335 0.456 0.386
AVG. 0.421 0.411 0.415 0.488 0.360 0.409 0.431 0.406 0.418 0.343 0.451 0.386
DE-EN
Forward Backward No entailment Bidirectional
System name P R F1 P R F1 P R F1 P R F1
altn full deu-eng 0.432 0.408 0.420 0.378 0.272 0.316 0.445 0.392 0.417 0.330 0.480 0.391
BUAP deu-eng run4 0.364 0.344 0.354 0.389 0.280 0.326 0.352 0.352 0.352 0.317 0.424 0.363
celi deu-eng run1 0.346 0.352 0.349 0.414 0.424 0.419 0.351 0.264 0.301 0.272 0.328 0.297
ECNUCS deu-eng run4 0.429 0.432 0.430 0.611 0.352 0.447 0.415 0.392 0.403 0.429 0.632 0.511
SoftCard deu-eng run1 0.511 0.368 0.428 0.527 0.384 0.444 0.417 0.400 0.408 0.317 0.504 0.389
umelb deu-eng run1 0.323 0.320 0.321 0.240 0.184 0.208 0.362 0.376 0.369 0.347 0.416 0.378
AVG. 0.401 0.371 0.384 0.426 0.316 0.360 0.390 0.363 0.375 0.335 0.464 0.389
Table 6: Precision, recall and F1 scores, calculated for each team?s best run for all the language combinations.
entailment decisions taken separately into sin-
gle judgements (similar to the Binary baseline
in Section 4). Methods based on multi-class
classification directly assign one of the four en-
tailment judgements to each test pair (similar to
our Multi-class baseline).
In contrast with CLTE-2012, where the combina-
tion of pivoting and compositional methods was the
option adopted by the majority of the approaches,
this year?s solutions do not show a clear trend. Con-
cerning the former dimension, participating systems
are equally distributed in cross-lingual and pivoting
methods relying on external automatic translation
tools. Regarding the latter dimension, in addition
to compositional and multi-class strategies, also al-
ternative solutions that leverage more sophisticated
meta-classification strategies have been proposed.
Besides the recourse to MT tools (e.g. Google
Translate), other tools and resources used by partic-
ipants include: WordNet, word alignment tools (e.g.
Giza++), part-of-speech taggers (e.g. Stanford POS
Tagger), stemmers (e.g. Snowball), machine learn-
ing libraries (e.g. Weka, SVMlight), parallel corpora
(e.g. Europarl), and stopword lists. More in detail:
ALTN [cross-lingual, compositional] (Turchi
and Negri, 2013) adopts a supervised learning
method based on features that consider word align-
ments between the two sentences obtained with
GIZA++ (Och et al, 2003). Binary entailment
judgements are taken separately, and combined into
final CLTE decisions.
BUAP [pivoting, multi-class and meta-
classifier] (Vilarin?o et al, 2013) adopts a pivoting
method based on translating T1 into the language of
31
T2 and vice versa (using Google Translate5). Sim-
ilarity measures (e.g. Jaccard index) and features
based on n-gram overlap, computed at the level of
words and part of speech categories, are used (either
alone or in combination) by different classification
strategies including: multi-class, a meta-classifier
(i.e. combining the output of 2/3/4-class classifiers),
and majority voting.
CELI [cross-lingual, meta-classifier]
(Kouylekov, 2013) uses dictionaries for word
matching, and a multilingual corpus extracted
from Wikipedia for term weighting. A variety of
distance measures implemented in the RTE system
EDITS (Kouylekov and Negri, 2010; Negri et
al., 2009) are used to extract features to train a
meta-classifier. Such classifier combines binary
decisions (?YES?/?NO?) taken separately for each
of the four CLTE judgements.
ECNUCS [pivoting, multi-class] (Jiang and
Man, 2013) uses Google Translate to obtain the En-
glish translation of each T1. After a pre-processing
step aimed at maximizing the commonalities be-
tween the two sentences (e.g. abbreviation replace-
ment), a number of features is extracted to train
a multi-class SVM classifier. Such features con-
sider information about sentence length, text sim-
ilarity/difference measures, and syntactic informa-
tion.
SoftCard [pivoting, multi-class] (Jimenez et al,
2013) after automatic translation with Google Trans-
late, uses SVMs to learn entailment decisions based
on information about the cardinality of: T1, T2, their
intersection and their union. Cardinalities are com-
puted in different ways, considering tokens in T1 and
T2, their IDF, and their similarity.
Umelb [cross-lingual, pivoting, compositional]
(Graham et al, 2013) adopts both pivoting and
cross-lingual approaches. For the latter, GIZA++
was used to compute word alignments between the
input sentences. Word alignment features are used
to train binary SVM classifiers whose decisions are
eventually composed into CLTE judgements.
7 Conclusion
Following the success of the first round of the Cross-
lingual Textual Entailment for Content Synchroniza-
5http://translate.google.com/
tion task organized within SemEval-2012, a second
evaluation task has been organized within SemEval-
2013. Despite the decrease in the number of partic-
ipants (six teams - four less than in the first round
- submitted a total of 61 runs) the new experience
is still positive. In terms of data, a new test set
has been released, extending the old one with 500
new CLTE pairs. The resulting 1,500 cross-lingual
pairs, aligned over four language combinations (in
addition to the monolingual English version), and
annotated with multiple entailment relations, repre-
sent a significant contribution to the research com-
munity and a solid starting point for further develop-
ments.6 In terms of results, in spite of a significant
decrease of the top scores, the increase of both me-
dian and lower results demonstrates some encour-
aging progress in CLTE research. Such progress is
also demonstrated by the variety of the approaches
proposed. While in the first round most of the
teams adopted more intuitive and ?simpler? solu-
tions based on pivoting (i.e. translation of T1 and
T2 in the same language) and compositional entail-
ment decision strategies, this year new ideas and
more complex solutions have emerged. Pivoting and
cross-lingual approaches are equally distributed, and
new classification methods have been proposed. Our
hope is that the large room for improvement, the in-
crease of available data, and the potential of CLTE
as a way to address complex NLP tasks and applica-
tions will motivate further research on the proposed
problem.
Acknowledgments
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-248531). The
authors would also like to acknowledge Pamela
Forner and Giovanni Moretti from CELCT, and the
volunteer translators that contributed to the creation
of the dataset: Giusi Calo, Victoria D??az, Bianca
Jeremias, Anne Kauffman, Laura Lo?pez Ortiz, Julie
Mailfait, Laura Mora?n Iglesias, Andreas Schwab.
6Together with the datasets derived from translation of the
RTE data (Negri and Mehdad, 2010), this is the only material
currently available to train and evaluate CLTE systems.
32
References
Amit Bronner, Matteo Negri, Yashar Mehdad, Angela
Fahrni, and Christof Monz. 2012. Cosyne: Synchro-
nizing multilingual wiki content. In Proceedings of
WikiSym 2012.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Proceedings of the PASCAL
Workshop of Learning Methods for Text Understand-
ing and Mining.
Yvette Graham, Bahar Salehi, and Tim Baldwin. 2013.
Unimelb: Cross-lingual Textual Entailment with Word
Alignment and String Similarity Features. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation (SemEval 2013).
Zhao Jiang and Lan Man. 2013. ECNUCS: Recognizing
Cross-lingual Textual Entailment Using Multiple Fea-
ture Types. . In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. Soft Cardinality-CLTE: Learning to Iden-
tify Directional Cross-Lingual Entailmens from Car-
dinalities and SMT. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Milen Kouylekov and Matteo Negri. 2010. An open-
source package for recognizing textual entailment. In
Proceedings of the ACL 2010 System Demonstrations.
Milen Kouylekov. 2013. Celi: EDITS and Generic Text
Pair Classification. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013).
Michael Lesk. 1986. Automated Sense Disambiguation
Using Machine-readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. In Proceedings
of the 5th annual international conference on Systems
documentation (SIGDOC86).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Proceedings of the 11th Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL HLT 2010).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using Bilingual Parallel Corpora for Cross-
Lingual Textual Entailment. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL HLT 2011).
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting Semantic Equivalence and Informa-
tion Disparity in Cross-lingual Documents. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012).
Christof Monz, Vivi Nastase, Matteo Negri, Angela
Fahrni, Yashar Mehdad, and Michael Strube. 2011.
Cosyne: a framework for multilingual content syn-
chronization of wikis. In Proceedings of WikiSym
2011.
Matteo Negri and Yashar Mehdad. 2010. Creating a Bi-
lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Cre-
ating Speech and Language Data with Amazons? Me-
chanical Turk.
Matteo Negri, Milen Kouylekov, Bernardo Magnini,
Yashar Mehdad, and Elena Cabrio. 2009. Towards ex-
tensible textual entailment engines: the edits package.
In AI* IA 2009: Emergent Perspectives in Artificial In-
telligence, pages 314?323. Springer.
Matto Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and Conquer: Crowdsourcing the Creation of
Cross-Lingual Textual Entailment Corpora. Proceed-
ings of the 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012a.
Semeval-2012 Task 8: Cross-lingual Textual Entail-
ment for Content Synchronization. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012).
Matteo Negri, Yashar Mehdad, Alessandro Marchetti,
Danilo Giampiccolo, and Luisa Bentivogli. 2012b.
Chinese Whispers: Cooperative Paraphrase Acqui-
sition. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC12), volume 2, pages 2659?2665.
F. Och, H. Ney, F. Josef, and O. H. Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics.
Marco Turchi and Matteo Negri. 2013. ALTN: Word
Alignment Features for Cross-Lingual Textual Entail-
ment. In Proceedings of the 7th International Work-
shop on Semantic Evaluation (SemEval 2013).
Darnes Vilarin?o, David Pinto, Saul Leo?n, Yuridiana
Alema?n, and Helena Go?mez-Adorno. 2013. BUAP:
N -gram based Feature Evaluation for the Cross-
Lingual Textual Entailment Task. In Proceedings of
the 7th International Workshop on Semantic Evalua-
tion (SemEval 2013).
33
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 212?216,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Creating a Bi-lingual Entailment Corpus through Translations with
Mechanical Turk: $100 for a 10-day Rush
Matteo Negri1 and Yashar Mehdad1,2
FBK-Irst1, University of Trento2
Trento, Italy
{negri,mehdad}@fbk.eu
Abstract
This paper reports on experiments in the cre-
ation of a bi-lingual Textual Entailment cor-
pus, using non-experts? workforce under strict
cost and time limitations ($100, 10 days). To
this aim workers have been hired for transla-
tion and validation tasks, through the Crowd-
Flower channel to Amazon Mechanical Turk.
As a result, an accurate and reliable corpus of
426 English/Spanish entailment pairs has been
produced in a more cost-effective way com-
pared to other methods for the acquisition of
translations based on crowdsourcing. Focus-
ing on two orthogonal dimensions (i.e. relia-
bility of annotations made by non experts, and
overall corpus creation costs), we summarize
the methodology we adopted, the achieved re-
sults, the main problems encountered, and the
lessons learned.
1 Introduction
Textual Entailment (TE) (Dagan and Glickman,
2004) has been proposed as a generic framework for
modelling language variability. Given a text T and
an hypothesis H, the task consists in deciding if the
meaning of H can be inferred from the meaning of
T. At the monolingual level, the great potential of
integrating TE recognition (RTE) components into
NLP architectures has been demonstrated in several
areas, including question answering, information re-
trieval, information extraction, and document sum-
marization. In contrast, mainly due to the absence of
cross-lingual TE (CLTE) recognition components,
similar improvements have not been achieved yet
in any cross-lingual application. Along such di-
rection, focusing on feasibility and architectural is-
sues, (Mehdad et al, 2010) recently proposed base-
line results demonstrating the potential of a simple
approach that integrates Machine Translation and
monolingual TE components.
As a complementary research problem, this paper
addresses the data collection issue, focusing on the
definition of a fast, cheap, and reliable methodology
to create CLTE corpora. The main motivation is that,
as in many other NLP areas, the availability of large
quantities of annotated data represents a critical bot-
tleneck in the systems? development/evaluation cy-
cle. Our first step in this direction takes advantage
of an already available monolingual corpus, casting
the problem as a translation one. The challenge con-
sists in taking a publicly available RTE dataset of
English T-H pairs (i.e. the PASCAL-RTE3 dataset1),
and create its English-Spanish CLTE equivalent by
translating the hypotheses into Spanish. To this
aim non-expert workers have been hired through
the CrowdFlower2 channel to Amazon Mechanical
Turk3 (MTurk), a crowdsourcing marketplace re-
cently used with success for a variety of NLP tasks
(Snow et al, 2008; Callison-Burch, 2009; Mihalcea
and Strapparava, 2009; Marge et al, 2010; Ambati
et al, 2010).
The following sections overview our experiments,
carried out under strict time (10 days) and cost
($100) limitations. In particular, Section 2 describes
our data acquisition process; Section 3 summarizes
1Available at: http://www.nist.gov/tac/data/RTE/index.html
2http://crowdflower.com/
3https://www.mturk.com/mturk/
212
the successive approximations that led to the defini-
tion of our methodology, and the lessons learned at
each step; Section 4 concludes the paper and pro-
vides directions for future work.
2 Corpus creation cycles
Starting from the RTE3 Development set (800 En-
glish T-H pairs), our corpus creation process has
been organized in sentence translation-validation
cycles, defined as separate ?jobs? routed to Crowd-
Fower?s workforce. At the first stage of each cycle,
the original English hypotheses are used to create a
translation job for collecting their Spanish equiva-
lents. At the second stage, the collected translations
are used to create a validation job, where multiple
judges are asked to check the correctness of each
translation, given the English source. Translated hy-
potheses that are positively evaluated by the major-
ity of trustful validators (i.e. those judged correct
with a confidence above 0.8) are retained, and di-
rectly stored in our CLTE corpus together with the
corresponding English texts. The remaining ones
are used to create a new translation job. The proce-
dure is iterated until substantial agreement for each
translated hypothesis is reached.
As regards the first phase of the cycle, we defined
our translation HIT as follows:
In this task you are asked to:
? First, judge if the Spanish sentence is a correct
translation of the English sentence. If the En-
glish sentence and its Spanish translation are blank
(marked as -), you can skip this step.
? Then, translate the English sentence above the text
box into Spanish.
Please make sure that your translation is:
1. Faithful to the original phrase in both meaning and
style.
2. Grammatically correct.
3. Free of spelling errors and typos.
Don?t use any automatic (machine) translation tool! You
can have a look at any on-line dictionary or reference
for the meaning of a word.
This HIT asks workers to first check the qual-
ity of an English-Spanish translation (used as a gold
unit), and then write the Spanish translation of a
new English sentence. The quality check allows
to collect accurate translations, by filtering out
judgments made by workers missing more than
20% of the gold units.
As regards the second phase of the cycle, our
validation HIT has been defined as follows:
Su tarea es verificar si la traduccio?n dada de una
frase del Ingle?s al espaol es correcta o no. La traduccio?n
es correcta si:
1. El estilo y sentido de la frase son fieles a los de la
original.
2. Es gramaticalmente correcta.
3. Carece de errores ortogra?ficos y tipogra?ficos.
Nota: el uso de herramientas de traduccio?n automa?tica
(ma?quina) no esta? permitido!
This HIT asks workers to take binary decisions
(Yes/No) for a set of English-Spanish translations
including gold units. The title and the description
are written in Spanish in order to weed out untrusted
workers (i.e. those speaking only English), and
attract the attention of Spanish speakers.
In our experiments, both the translation and vali-
dation jobs have been defined in several ways, trying
to explore different strategies to quickly collect reli-
able data in a cost effective way. Such cost reduction
effort led to the following differences between our
work and similar related approaches documented in
literature (Callison-Burch, 2009; Snow et al, 2008):
? Previous works built on redundancy of the col-
lected translations (up to 5 for each source
sentence), thus resulting in more costly jobs.
For instance, adopting a redundancy-based ap-
proach to collect 5 translations per sentence at
the cost of $0.01 each, and 5 validations per
translation at the cost of $0.002 each, would re-
sult in $80 for 800 sentences.
Assuming that the translation process is com-
plex and expensive, our cycle-based technique
builds on simple and cheap validation mech-
anisms that drastically reduce the amount of
translations required. In our case, 1 translation
per sentence at the cost of $0.01, and 5 valida-
tions per translation at the cost of $0.002 each,
213
would result in $32 for 800 sentences, making
a conservative assumption of up to 8 iterations
with 50% wrong translations at each cycle (i.e.
800 sentences in the first cycle, 400 in the sec-
ond, 200 in the third, etc.).
? Previous works involving validation of the col-
lected data are based on ranking/voting mecha-
nisms, where workers are asked to order a num-
ber of translations, or select the best one given
the source. Our approach to validation is based
on asking workers to take binary decisions over
source-target pairs. This results in an easier,
faster, and eventually cheaper task.
? Previous works did not use any specific method
to qualify the workers? knowledge, apart from
post-hoc agreement computation. Our ap-
proach systematically includes gold units to fil-
ter out untrusted workers during the process.
As a result we pay only for qualified judgments.
3 Experiments and lessons learned
The overall methodology, and the definition of the
HITs described in Section 2, are the result of suc-
cessive approximations that took into account two
correlated aspects: the quality of the collected trans-
lations, and the current limitations of the Crowd-
Flower service. On one side, simpler, cheaper, and
faster jobs launched in the beginning of our experi-
ments had to be refined to improve the quality of the
retained translations. On the other side, ad-hoc solu-
tions had to be found to cope with the limited quality
control functionalities provided by CrowdFlower. In
particular, the lack of regional qualifications of the
workers, and of any qualification tests mechanism
(useful features of MTurk) raised the need of defin-
ing more controlled, but also more expensive jobs.
Table 1 and the rest of this section summarize the
progress of our work in defining the methodology
adopted, the main improvements experimented at
each step, the overall costs, and the lessons learned.
Step 1: a na??ve approach. Initially, transla-
tion/validation jobs were defined without using qual-
ification mechanisms, giving permission to any
worker to complete our HITs. In this phase, our goal
was to estimate the trade-off between the required
development time, the overall costs, and the qual-
ity of translations collected in the most na??ve condi-
tions.
As expected, the job accomplishment time was
negligible, and the overall cost very low. More
specifically, it took about 1 hour for translating the
800 hypotheses at the cost of $12, and less than 6
hours to obtain 5 validations per each translation at
the same cost of $12.
Nevertheless, as revealed by further experiments
with the introduction of gold units, the quality of the
collected translations was poor. In particular, 61% of
them should have been rejected, often due to gross
mistakes. As an example, among the collected mate-
rial several translations in languages other than En-
glish revealed a massive and defective use of on-line
translation tools by untrusted workers, as also ob-
served by (Callison-Burch, 2009).
Step 2: reducing validation errors. A first im-
provement addressed the validation phase, where
we introduced gold units as a mechanism to qual-
ify the workers, and consequently prune the un-
trusted ones. To this aim, we launched the valida-
tion HIT described in Section 2, adding around 50
English-Spanish control pairs. The pairs (equally
distributed into positive and negative samples) have
been extracted from the collected data, and manually
checked by a Spanish native speaker.
The positive effect of using gold units has been
verified in two ways. First, we checked the quality
of the translations collected in the first na??ve transla-
tion job, by counting the number of rejections (61%)
after running the improved validation job. Then, we
manually checked the quality of the translations re-
tained with the new job. A manual check on 20% of
the retained translations was carried out by a Span-
ish native speaker, resulting in 97% Accuracy. The
3% errors encountered are equally divided into mi-
nor translation errors, and controversial (but sub-
stantially acceptable) cases due to regional Spanish
variations.
The considerable quality improvement observed
has been obtained with a small increase of 25% in
the cost (less than $3). However, as regards the ac-
complishment time, adding the gold units to qualify
workers led to a considerable increase in duration
(about 4 days for the first iteration). This is mainly
214
due to the high number of automatically rejected
judgments, obtained from untrusted workers miss-
ing the gold units. Because of the discrepancy be-
tween trusted and untrusted judgments, we faced an-
other limitation of the CrowdFlower service, which
further delayed our experiments. Often, in fact, the
rapid growth of untrusted judgments activates auto-
matic pausing mechanisms, based on the assumption
that gold units are not accurate. This, however, is a
strong assumption which does not take into account
the huge amount of non-qualified workers accepting
(or even just playing with) the HITs. For instance,
in our case the vast majority of errors came from
workers located in specific regions where the native
language is not Spanish nor English.
Step 3: reducing translation errors. The ob-
served improvement obtained by introducing gold
units in the validation phase, led us to the definition
of a new translation task, also involving a similar
qualification mechanism. To this aim, due to lan-
guage variability, it was clearly impossible to use
reference translations as gold units. Taking into ac-
count the limitations of the CrowdFlower interface,
which does not allow to set qualification tests or
split the jobs into sequential subtasks (other effec-
tive and widely used features of MTurk), we solved
the problem by defining the translation HITs as de-
scribed in Section 2. This solution combines a va-
lidity check and a translation task, and proved to be
effective with a decrease in the translations eventu-
ally rejected (45%).
Step 4: reducing time. Considering the extra time
required by using gold units, we decided to spend
more money on each HIT to boost the speed of our
jobs. In addition, to overcome the delays caused by
the automatic pausing mechanism, we obtained from
CrowdFlower the possibility to pose regional quali-
fication, as commonly used in MTurk.
As expected, both solutions proved to be effective,
and contributed to the final definition of our method-
ology. On one side, doubling the payment for each
task (from $0.01 to $0.02 for each translation and
from from $0.002 to $0.005 for each validation), we
halved the required time to finish each job. On the
other side, by imposing the regional qualification,
we eventually avoided unexpected automatic pauses.
4 Conclusion and future work
We presented a set of experiments targeting the cre-
ation of bi-lingual Textual Entailment corpora by
means of non experts? workforce (i.e. the Crowd-
Flower channel to Amazon Mechanical Turk).
As a first step in this direction, we took advantage
of an already existing monolingual English RTE cor-
pus, casting the problem as a translation task where
Spanish translations of the hypotheses are collected
and validated by the workers. Strict time and cost
limitations on one side, and the current limitations
of the CrowdFlower service on the other side, led
us to the definition of an effective corpus creation
methodology. As a result, less than $100 were spent
in 10 days to define such methodology, leading to
collect 426 pairs as a by-product. However, it?s
worth remarking that applying this technique to cre-
ate the full corpus would cost about $30.
The limited costs, together with the short time re-
quired to acquire reliable results, demonstrate the
effectiveness of crowdsourcing services for simple
sentence translation tasks. However, while MTurk is
already a well tested, stable, and rich of functional-
ities platform, some limitations emerged during our
experience with the more recent CrowdFlower ser-
vice (currently the only one accessible to non-US
citizens). Some of these limitations, such as the
regional qualification mechanism, have been over-
come right after the end of our experimentation with
the introduction of new functionalities provided as
?Advanced Options?. Others (such as the lack of
other qualification mechanisms, and the automatic
pausing of the HITs in case of high workers? error
rates on the gold units) at the moment still represent
a possible complication, and have to be carefully
considered when designing experiments and inter-
preting the results4.
In light of this positive experience, next steps
in our research will further explore crowdsourcing-
based data acquisition methods to address the com-
plementary problem of collecting new entailment
pairs from scratch. This will allow to drastically re-
duce data collection bottlenecks, and boost research
both on cross-lingual and mono-lingual Textual En-
4However, when asked through the provided support ser-
vice, the CrowdFlower team proved to be quite reactive in pro-
viding ad-hoc solutions to specific problems.
215
Elapsed time Running cost Focus Lessons learned
1 day $24 Approaching CrowdFlower,
defining a na??ve methodology
Need of qualification mechanism,
task definition in Spanish.
7 days $58 Improving validation Qualification mechanisms (gold units
and regional) are effective, need of
payment increase to boost speed.
9 days $99.75 Improving translation Combined HIT for qualification, pay-
ment increase worked!
10 days $99.75 Obtaining bi-lingual RTE corpus Fast, cheap, and reliable method.
Table 1: $100 for a 10-day rush (summary and lessons learned)
tailment.
Acknowledgments
We would like to thank MTurk and CrowdFlower
for providing the $100 credit used for the experi-
ments, and our colleague Silvana Bernaola Biggio,
who kindly accepted to validate our results.
The research leading to these results has re-
ceived funding from the European Community?s
Seventh Framework Programme (FP7/2007-2013)
under Grant Agreement n. 248531.
References
V. Ambati, S. Vogel and J. Carbonell 2010. Active
Learning and Crowd-Sourcing for Machine Transla-
tion. To appear in Proceedings of LREC 2010.
C. Callison-Burch 2009. Fast, Cheap, and Creative:
Evaluating Translation Quality Using Amazon?s Me-
chanical Turk. In Proceedings of EMNLP 2009.
I. Dagan and O. Glickman 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. In Proceedings of the PASCAL Workshop
of Learning Methods for Text Understanding and Min-
ing.
M. Marge, S. Banerjee and A. Rudnicky 2010. Using the
Amazon Mechanical Turk for Transcription of Spoken
Language. In Proceedings of the 2010 IEEE Interna-
tional Conference on Acoustics, Speech and Spoken
Language (ICASSP 2010).
Y. Mehdad, M. Negri, and M. Federico 2010. Towards
Cross-Lingual Textual Entailment. To appear in Pro-
ceedings of NAACL HLT 2010.
R. Mihalcea and C. Strapparava 2009. The Lie Detector:
Explorations in the Automatic Recognition of Decep-
tive Language. In Proceedings of ACL 2009.
R. Snow, B. O?Connor, D. Jurafsky, and A. Y. Ng 2008.
Cheap and Fast - but is it Good? Evaluating Non-
expert Annotations for Natural Language Tasks. In
Proceedings of EMNLP 2008.
216
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 30?34,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Is it Worth Submitting this Run?
Assess your RTE System with a Good Sparring Partner
Milen Kouylekov
CELI s.r.l.
Turin, Italy
kouylekov@celi.it
Yashar Mehdad
FBK-irst and University of Trento
Trento, Italy
mehdad@fbk.eu
Matteo Negri
FBK-irst
Trento, Italy
negri@fbk.eu
Abstract
We address two issues related to the devel-
opment of systems for Recognizing Textual
Entailment. The first is the impossibility to
capitalize on lessons learned over the different
datasets available, due to the changing nature
of traditional RTE evaluation settings. The
second is the lack of simple ways to assess
the results achieved by our system on a given
training corpus, and figure out its real potential
on unseen test data. Our contribution is the ex-
tension of an open-source RTE package with
an automatic way to explore the large search
space of possible configurations, in order to
select the most promising one over a given
dataset. From the developers? point of view,
the efficiency and ease of use of the system,
together with the good results achieved on all
previous RTE datasets, represent a useful sup-
port, providing an immediate term of compar-
ison to position the results of their approach.
1 Introduction
Research on textual entailment (TE) has received a
strong boost by the Recognizing Textual Entailment
(RTE) Challenges, organized yearly to gather the
community around a shared evaluation framework.
Within such framework, besides the intrinsic diffi-
culties of the task (i.e. deciding, given a set of Text-
Hypothesis pairs, if the hypotheses can be inferred
from the meaning of the texts), the development of
RTE systems has to confront with a number of ad-
ditional problems and uncertainty factors. First of
all, since RTE systems are usually based on com-
plex architectures that integrate a variety of tools and
resources, it is per se very difficult to tune them and
define the optimal configuration given a new dataset.
In general, when participating to the evaluation chal-
lenges there?s no warranty that the submitted runs
are those obtained with the best possible configura-
tion allowed by the system. Second, the evaluation
settings change along the years. Variations in the
length of the texts, the origin of the pairs, the bal-
ance between positive and negative examples, and
the type of entailment decisions allowed, reflect the
need to move from easier and more artificial settings
to more complex and natural ones. However, in con-
trast with other more stable tasks in terms of eval-
uation settings and metrics (e.g. machine transla-
tion), such changes make it difficult to capitalize on
the experience obtained by participants throughout
the years. Third, looking at RTE-related literature
and the outcomes of the six campaigns organised so
far, the conclusions that can be drawn are often con-
troversial. For instance, it is not clear whether the
availability of larger amounts of training data corre-
lates with better performance (Hickl et al, 2006) or
not (Zanzotto et al, 2007; Hickl and Bensley, 2007),
even within the same evaluation setting. In addi-
tion, ablation tests carried out in recent editions of
the challenge do not allow for definite conclusions
about the actual usefulness of tools and resources,
even the most popular ones (Bentivogli et al, 2009).
Finally, the best performing systems often have dif-
ferent natures from one year to another, showing al-
ternations of deep (Hickl and Bensley, 2007; Tatu
and Moldovan, 2007) and shallow approaches (Jia
et al, 2010) ranked at the top positions. In light
of these considerations, it would be useful for sys-
30
tems developers to have: i) automatic ways to sup-
port systems? tuning at a training stage, and ii) reli-
able terms of comparison to validate their hypothe-
ses, and position the results of their work before sub-
mitting runs for evaluation. In this paper we address
these needs by extending an open-source RTE pack-
age (EDITS1) with a mechanism that automatizes
the selection of the most promising configuration
over a training dataset. We prove the effectiveness
of such extension showing that it allows not only to
achieve good performance on all the available RTE
Challenge datasets, but also to improve the official
results, achieved with the same system, through ad
hoc configurations manually defined by the devel-
opers team. Our contribution is twofold. On one
side, in the spirit of the collaborative nature of open
source projects, we extend an existing tool with a
useful functionality that was still missing. On the
other side, we provide a good ?sparring partner? for
system developers, to be used as a fast and free term
of comparison to position the results of their work.
2 ?Coping? with configurability
EDITS (Kouylekov and Negri, 2010) is an open
source RTE package, which offers a modular, flex-
ible, and adaptable working environment to experi-
ment with the RTE task over different datasets. The
package allows to: i) create an entailment engine
by defining its basic components (i.e. algorithms,
cost schemes, rules, and optimizers); ii) train such
entailment engine over an annotated RTE corpus to
learn a model; and iii) use the entailment engine and
the model to assign an entailment judgement and a
confidence score to each pair of an un-annotated test
corpus. A key feature of EDITS is represented by its
high configurability, allowed by the availability of
different algorithms, the possibility to integrate dif-
ferent sets of lexical entailment/contradiction rules,
and the variety of parameters for performance opti-
mization (see also Mehdad, 2009). Although config-
urability is per se an important aspect (especially for
an open-source and general purpose system), there
is another side of the coin. In principle, in order to
select the most promising configuration over a given
development set, one should exhaustively run a huge
number of training/evaluation routines. Such num-
1http://edits.fbk.eu/
ber corresponds to the total number of configura-
tions allowed by the system, which result from the
possible combinations of parameter settings. When
dealing with enlarging dataset sizes, and the tight
time constraints usually posed by the evaluation
campaigns, this problem becomes particularly chal-
lenging, as developers are hardly able to run exhaus-
tive training/evaluation routines. As recently shown
by the EDITS developers team, such situation re-
sults in running a limited number of experiments
with the most ?reasonable? configurations, which
consequently might not lead to the optimal solution
(Kouylekov et al, 2010).
The need of a mechanism to automatically ob-
tain the most promising solution on one side, and
the constraints posed by the evaluation campaigns
on the other side, arise the necessity to optimize
this procedure. Along this direction, the objective
is good a trade-off between exhaustive experimen-
tation with all possible configurations (unfeasible),
and educated guessing (unreliable). The remainder
of this section tackles this issue introducing an op-
timization strategy based on genetic algorithms, and
describing its adaptation to extend EDITS with the
new functionality.
2.1 Genetic algorithm
Genetic algorithms (GA) are well suited to effi-
ciently deal with large search spaces, and have been
recently applied with success to a variety of opti-
mization problems and specific NLP tasks (Figueroa
and Neumann, 2008; Otto and Riff, 2004; Aycinena
et al, 2003). GA are a direct stochastic method for
global search and optimization, which mimics natu-
ral evolution. To this aim, they work with a popu-
lation of individuals, representing possible solutions
to the given task. Traditionally, solutions are rep-
resented in binary as strings of 0s and 1s, but other
encodings (e.g. sequences of real values) are possi-
ble. The evolution usually starts from a population
of randomly generated individuals, and at each gen-
eration selects the best-suited individuals based on
a fitness function (which measures the optimality of
the solution obtained by the individual). Such selec-
tion is then followed by modifications of the selected
individuals obtained by recombining (crossover) and
performing random changes (mutation) to form a
new population, which will be used in the next iter-
31
ation. Finally, the algorithm is terminated when the
maximum number of generations, or a satisfactory
fitness level has been reached for the population.
2.2 EDITS-GA
Our extension to the EDITS package, EDITS-GA,
consists in an iterative process that starts with an
initial population of randomly generated configura-
tions. After a training phase with the generated con-
figurations, the process is evaluated by means of the
fitness function, which is manually defined by the
user2. This measure is used by the genetic algo-
rithm to iteratively build new populations of config-
urations, which are trained and evaluated. This pro-
cess can be seen as the combination of: i) a micro
training/evaluation routine for each generated con-
figuration of the entailment engine; and ii) a macro
evolutionary cycle, as illustrated in Figure 1. The
fitness function is an important factor for the evalu-
ation and the evolution of the generated configura-
tions, as it drives the evolutionary process by deter-
mining the best-suited individuals used to generate
new populations. The procedure to estimate and op-
timize the best configuration applying the GA, can
be summarized as follows.
(1) Initialization: generate a random initial popula-
tion (i.e. a set of configurations).
(2) Selection:
2a. The fitness function (accuracy, or F-measure)
is evaluated for each individual in the population.
2b. The individuals are selected according to their
fitness function value.
(3) Reproduction: generate a new population of
configurations from the selected one, through ge-
netic operators (cross-over and mutation).
(4) Iteration: repeat the Selection and Reproduction
until Termination.
(5) Termination: end if the maximum number of
iterations has been reached, or the population has
converged towards a particular solution.
In order to extend EDITS with genetic algo-
rithms, we used a GA implementation available in
the JGAP tool3. In our settings, each individual con-
tains a sequence of boolean parameters correspond-
2For instance, working on the RTE Challenge ?Main? task
data, the fitness function would be the accuracy for RTE1 to
RTE5, and the F-measure for RTE6.
3http://jgap.sourceforge.net/
Figure 1: EDITS-GA framework.
ing to the activation/de-activation of the system?s
basic components (algorithms, cost schemes, rules,
and optimizers). The configurations corresponding
to such individuals constitute the populations itera-
tively evaluated by EDITS-GA on a given dataset.
3 Experiments
Our experiments were carried out over the datasets
used in the six editions of the RTE Challenge
(?Main? task data from RTE1 to RTE6). For each
dataset we obtained the best model by training
EDITS-GA over the development set, and evaluat-
ing the resulting model on the test pairs. To this
aim, the optimization process is iterated over all
the available algorithms in order to select the best
combination of parameters. As termination crite-
rion, we set to 20 the maximum number of itera-
tions. To increase efficiency, we extended EDITS
to pre-process each dataset using the tokenizer and
stemmer available in Lucene4. This pre-processing
phase is automatically activated when the EDITS-
GA has to process non-annotated datasets. How-
ever, we also annotated the RTE corpora with the
Stanford parser plugin (downloadable from the ED-
ITS websitein order to run the syntax-based algo-
rithms available (e.g. tree edit distance). The num-
ber of boolean parameters used to generate the con-
figurations is 18. In light of this figure, it becomes
evident that the number of possible configurations
is too large (218=262,144) for an exhaustive train-
ing/evaluation routine over each dataset5. However,
4http://lucene.apache.org/
5In an exploratory experiment we measured in around 4
days the time required to train EDITS, with all possible con-
32
# Systems Best Lowest Average EDITS (rank) EDITS-GA (rank) % Impr. Comp. Time
RTE1 15 0.586 0.495 0.544 0.559 (8) 0.5787 (3) +3.52% 8m 24s
RTE2 23 0.7538 0.5288 0.5977 0.605 (6) 0.6225 (5) +2.89% 9m 8s
RTE3 26 0.8 0.4963 0.6237 - 0.6875 (4) - 9m
RTE4 26 0.746 0.516 0.5935 0.57 (17) 0.595 (10) +4.38% 30m 54s
RTE5 20 0.735 0.5 0.6141 0.6017 (14) 0.6233 (9) +3.58% 8m 23s
RTE6 18 0.4801 0.116 0.323 0.4471 (4) 0.4673 (3) +4.51% 1h 54m 20s
Table 1: RTE results (acc. for RTE1-RTE5, F-meas. for RTE6). For each participant, only the best run is considered.
with an average of 5 reproductions on each iteration,
EDITS-GA makes an average of 100 configurations
for each algorithm. Thanks to EDITS-GA, the aver-
age number of evaluated configurations for a single
dataset is reduced to around 4006.
Our results are summarized in Table 1, showing
the total number of participating systems in each
RTE Challenge, together with the highest, lowest,
and average scores they achieved. Moreover, the of-
ficial results obtained by EDITS are compared with
the performance achieved with EDITS-GA on the
same data. We can observe that, for all datasets,
the results achieved by EDITS-GA significantly im-
prove (up to 4.51%) the official EDITS results. It?s
also worth mentioning that such scores are always
higher than the average ones obtained by partici-
pants. This confirms that EDITS-GA can be poten-
tially used by RTE systems developers as a strong
term of comparison to assess the capabilities of
their own system. Since time is a crucial factor for
RTE systems, it is important to remark that EDITS-
GA allows to converge on a promising configura-
tion quite efficiently. As can be seen in Table 1,
the whole process takes around 9 minutes7 for the
smaller datasets (RTE1 to RTE5), and less than 2
hours for a very large dataset (RTE6). Such time
analysis further proves the effectiveness of the ex-
tended EDITS-GA framework. For the sake of com-
pleteness we gave a look at the differences between
the ?educated guessing? done by the EDITS de-
velopers for the official RTE submissions, and the
?optimal? configuration automatically selected by
EDITS-GA. Surprisingly, in some cases, even a mi-
nor difference in the selected parameters leads to
figurations, over small datasets (RTE1 to RTE5).
6With these settings, training EDITS-GA over small datasets
(RTE1 to RTE5) takes about 9 minutes each.
7All time figures are calculated on an Intel(R) Xeon(R),
CPU X3440 @ 2.53GHz, 8 cores with 8 GB RAM.
significant gaps in the results. For instance, in RTE6
dataset, the ?guessed? configuration (Kouylekov et
al., 2010) was based on the lexical overlap algo-
rithm, setting the cost of replacing H terms with-
out an equivalent in T to the minimal Levenshtein
distance between such words and any word in T.
EDITS-GA estimated, as a more promising solution,
a combination of lexical overlap with a different cost
scheme (based on the IDF of the terms in T). In ad-
dition, in contrast with the ?guessed? configuration,
stop-words filtering was selected as an option, even-
tually leading to a 4.51% improvement over the of-
ficial RTE6 result.
4 Conclusion
?Is it worth submitting this run??,?How good is my
system??. These are the typical concerns of system
developers approaching the submission deadline of
an RTE evaluation campaign. We addressed these is-
sues by extending an open-source RTE system with
a functionality that allows to select the most promis-
ing configuration over an annotated training set. Our
contribution provides developers with a good ?spar-
ring partner? (a free and immediate term of compar-
ison) to position the results of their approach. Ex-
perimental results prove the effectiveness of the pro-
posed extension, showing that it allows to: i) achieve
good performance on all the available RTE datasets,
and ii) improve the official results, achieved with the
same system, through ad hoc configurations manu-
ally defined by the developers team.
Acknowledgments
This work has been partially supported by the EC-
funded projects CoSyne (FP7-ICT-4-24853), and
Galateas (CIP-ICT PSP-2009-3-250430).
33
References
Margaret Aycinena, Mykel J. Kochenderfer, and David
Carl Mulford. 2003. An Evolutionary Approach to
Natural Language Grammar Induction. Stanford CS
224N Natural Language Processing.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, Bernardo Magnini. 2009. The Fifth
PASCAL Recognizing Textual Entailment Challenge.
Proceedings of the TAC 2009 Workshop.
Ido Dagan and Oren Glickman. 2004. Probabilistic tex-
tual entailment: Generic applied modeling of language
variability. Proceedings of the PASCAL Workshop of
Learning Methods for Text Understanding and Min-
ing.
Alejandro G. Figueroa and Gu?nter Neumann. 2008. Ge-
netic Algorithms for Data-driven Web Question An-
swering. Evolutionary Computation 16(1) (2008) pp.
89-125.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing Textual Entailment with LCCs Groundhog Sys-
tem. Proceedings of the Second PASCAL Challenges
Workshop.
Andrew Hickl and Jeremy Bensley. 2007. A discourse
commitment-based framework for recognizing textual
entailment. Proceedings of the ACL-PASCAL Work-
shop on Textual Entailment and Paraphrasing.
Houping Jia, Xiaojiang Huang, Tengfei Ma, Xiaojun
Wan, and Jianguo Xiao. 2010. PKUTM Participation
at TAC 2010 RTE and Summarization Track. Proceed-
ings of the Sixth Recognizing Textual Entailment Chal-
lenge.
Milen Kouylekov and Matteo Negri. 2010. An Open-
source Package for Recognizing Textual Entailment.
Proceedings of ACL 2010 Demo session.
Milen Kouylekov, Yashar Mehdad, Matteo Negri, and
Elena Cabrio. 2010. FBK Participation in RTE6:
Main and KBP Validation Task. Proceedings of the
Sixth Recognizing Textual Entailment Challenge.
Yashar Mehdad 2009. Automatic Cost Estimation for
Tree Edit Distance Using Particle Swarm Optimiza-
tion. Proceedings of ACL-IJCNLP 2009.
Eridan Otto and Mar??a Cristina Riff 2004. Towards an
efficient evolutionary decoding algorithm for statisti-
cal machine translation. LNAI, 2972:438447..
Marta Tatu and Dan Moldovan. 2007. COGEX at RTE3.
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing.
Fabio Massimo Zanzotto, Marco Pennacchiotti and
Alessandro Moschitti. 2007. Shallow Semantics in
Fast Textual Entailment Rule Learners. Proceedings
of the Third Recognizing Textual Entailment Chal-
lenge.
34
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 171?180,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Match without a Referee:
Evaluating MT Adequacy without Reference Translations
Yashar Mehdad Matteo Negri Marcello Federico
Fondazione Bruno Kessler, FBK-irst
Trento , Italy
{mehdad|negri|federico}@fbk.eu
Abstract
We address two challenges for automatic ma-
chine translation evaluation: a) avoiding the
use of reference translations, and b) focusing
on adequacy estimation. From an economic
perspective, getting rid of costly hand-crafted
reference translations (a) permits to alleviate
the main bottleneck in MT evaluation. From
a system evaluation perspective, pushing se-
mantics into MT (b) is a necessity in order
to complement the shallow methods currently
used overcoming their limitations. Casting
the problem as a cross-lingual textual entail-
ment application, we experiment with differ-
ent benchmarks and evaluation settings. Our
method shows high correlation with human
judgements and good results on all datasets
without relying on reference translations.
1 Introduction
While syntactically informed modelling for statis-
tical MT is an active field of research that has re-
cently gained major attention from the MT commu-
nity, work on integrating semantic models of ade-
quacy into MT is still at preliminary stages. This sit-
uation holds not only for system development (most
current methods disregard semantic information, in
favour of statistical models of words distribution),
but also for system evaluation. To realize its full po-
tential, however, MT is now in the need of semantic-
aware techniques, capable of complementing fre-
quency counts with meaning representations.
In order to integrate semantics more deeply into
MT technology, in this paper we focus on the eval-
uation dimension. Restricting our investigation to
some of the more pressing issues emerging from this
area of research, we provide two main contributions.
1. An automatic evaluation method that avoids
the use of reference translations. Most current
metrics are based on comparisons between auto-
matic translations and human references, and reward
lexical similarity at the n-gram level (e.g. BLEU
(Papineni et al, 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), TER (Snover
et al, 2006)). Due to the variability of natural lan-
guages in terms of possible ways to express the same
meaning, reliable lexical similarity metrics depend
on the availability of multiple hand-crafted (costly)
realizations of the same source sentence in the tar-
get language. Our approach aims to avoid this bot-
tleneck by adapting cross-lingual semantic inference
capabilities and judging a translation only given the
source sentence.
2. A method for evaluating translation adequacy.
Most current solutions do not consistently reward
translation adequacy (semantic equivalence between
source sentence and target translation). The scarce
integration of semantic information in MT, specif-
ically at the multilingual level, led to MT systems
that are ?illiterate? in terms of semantics and mean-
ing. Moreover, current metrics are often difficult to
interpret. In contrast, our method targets the ade-
quacy dimension, producing easily interpretable re-
sults (e.g. judgements in a 4-point scale).
Our approach builds on recent advances in
cross-lingual textual entailment (CLTE) recognition,
which provides a natural framework to address MT
adequacy evaluation. In particular, we approach
the problem as an application of CLTE where bi-
171
directional entailment between source and target is
considered as evidence of translation adequacy. Be-
sides avoiding the use of references, the proposed
solution differs from most previous methods which
typically rely on surface-level features, often ex-
tracted from the source or the target sentence taken
in isolation. Although some of these features might
correlate well with adequacy, they capture seman-
tic equivalence only indirectly, and at the level of
a probabilistic prediction. Focusing on a combina-
tion of surface, syntactic and semantic features, ex-
tracted from both source and target (e.g. ?source-
target length ratio?, ?dependency relations in com-
mon?), our approach leads to informed adequacy
judgements derived from the actual observation of
a translation given the source sentence.
2 Background
Some recent works proposed metrics able to approx-
imately assess meaning equivalence between can-
didate and reference translations. Among these,
(Gime?nez and Ma`rquez, 2007) proposed a hetero-
geneous set comprising overlapping and matching
metrics, compiled from a rich set of variants at five
different linguistic levels: lexical, shallow-syntactic,
syntactic, shallow-semantic and semantic. More
similar to our approach, (Pado? et al, 2009) proposed
semantic adequacy metrics that exploit feature rep-
resentations motivated by Textual Entailment (TE).
Both metrics, however, highly depend on the avail-
ability of multiple reference translations.
Early attempts to avoid reference translations ad-
dressed quality estimation (QE) by means of large
numbers of source, target, and system-dependent
features to discriminate between ?good? and ?bad?
translations (Blatz et al, 2004; Quirk, 2004). More
recently (Specia et al, 2010b; Specia and Farzindar,
2010; Specia, 2011) conducted a series of experi-
ments using features designed to estimate translation
post-editing effort (in terms of volume and time) as
an indicator of MT output quality. Good results in
QE have been achieved by adding linguistic infor-
mation such as shallow parsing, POS tags (Xiong
et al, 2010), or dependency relations (Bach et al,
2011; Avramidis et al, 2011) as features. However,
in general these approaches do not distinguish be-
tween fluency (i.e. syntactic correctness of the out-
put translation) and adequacy, and mostly rely on
fluency-oriented features (e.g. ?number of punctu-
ation marks?). As a result, a simple surface form
variation is given the same importance of a content
word variation that changes the meaning of the sen-
tence. To the best of our knowledge, only (Specia et
al., 2011) proposed an approach to frame MT evalu-
ation as an adequacy estimation problem. However,
their method still includes many features which are
not focused on adequacy, and often look either at the
source or at the target in isolation (see for instance
?source complexity? and ?target fluency? features).
Moreover, the actual contribution of the adequacy
features used is not always evident and, for some
testing conditions, marginal.
Our approach to adequacy evaluation builds on
and extends the above mentioned works. Similarly
to (Pado? et al, 2009) we rely on the notion of textual
entailment, but we cast it as a cross-lingual problem
in order to bypass the need of reference translations.
Similarly to (Blatz et al, 2004; Quirk, 2004), we try
to discriminate between ?good? and ?bad? transla-
tions, but we focus on adequacy. To this aim, like
(Xiong et al, 2010; Bach et al, 2011; Avramidis et
al., 2011; Specia et al, 2010b; Specia et al, 2011)
we rely on a large number of features, but focusing
on source-target dependent ones, aiming at informed
adequacy evaluation of a translation given the source
instead of a more generic quality assessment based
on surface features.
3 CLTE for adequacy evaluation
We address adequacy evaluation by adapting cross-
lingual textual entailment recognition as a way to
measure to what extent a source sentence and its au-
tomatic translation are semantically similar. CLTE
has been proposed by (Mehdad et al, 2010) as an ex-
tension of textual entailment (Dagan and Glickman,
2004) that consists in deciding, given a text T and a
hypothesis H in different languages, if the meaning
of H can be inferred from the meaning of T.
The main motivation in approaching adequacy
evaluation using CLTE is that an adequate trans-
lation and the source text should convey the same
meaning. In terms of entailment, this means that an
adequate MT output and the source sentence should
entail each other (bi-directional entailment). Los-
172
ing or altering part of the meaning conveyed by the
source sentence (i.e. having more, or different infor-
mation in one of the two sides) will change the en-
tailment direction and, consequently, the adequacy
judgement. Framed in this way, CLTE-based ade-
quacy evaluation methods can be designed to dis-
tinguish meaning-preserving variations from true di-
vergence, regardless of reference translations.
Similarly to many monolingual TE approaches,
CLTE solutions proposed so far adopt supervised
learning methods, with features that measure to what
extent the hypotheses can be mapped into the texts.
The underlying assumption is that the probability of
entailment is proportional to the number of words in
H that can be mapped to words in T (Mehdad et al,
2011). Such mapping can be carried out at differ-
ent word representation levels (e.g. tokens, lemmas,
stems), possibly with the support of lexical knowl-
edge in order to cross the language barrier between
T and H (e.g. dictionaries, phrase tables).
Under the same assumption, since in the adequacy
evaluation framework the entailment relation should
hold in both directions, the mapping is performed
both from the source to the target and vice-versa,
building on features extracted from both sentences.
Moreover, to improve over previous CLTE methods
and boost MT adequacy evaluation performance, we
explore the joint contribution of a number of lexi-
cal, syntactic and semantic features (Mehdad et al,
2012).
Concerning the features used, it?s worth observ-
ing that the cost of implementing our approach (in
terms of required resources and linguistic proces-
sors), and the need of reference translations are in-
trinsically different bottlenecks for MT. While the
limited availability of processing tools for some lan-
guage pairs is a ?temporary? bottleneck, the acqui-
sition of multiple references is a ?permanent? one.
The former cost is reducing over time due to the
progress in NLP research; the latter represents a
fixed cost that has to be eliminated. Similar consid-
erations hold regarding the need of annotated data to
develop our supervised learning approach. Concern-
ing this, the cost of labelling source-target pairs with
adequacy judgments is significantly lower compared
to the creation of multiple references.
3.1 Features
In order to learn models for classification and regres-
sion we used the Support Vector Machine (SVM)
algorithms implemented in the LIBSVM package
(Chang and Lin, 2011) with a linear kernel and de-
fault parameters setting. Aiming at objective ade-
quacy evaluation, our method limits the recourse to
MT system-dependent features to reduce the bias
of evaluating MT technology with its own core
methods. The experiments described in the follow-
ing sections are carried out on publicly available
English-Spanish datasets, exploring the potential of
a combination of surface, syntactic and semantic
features. Language-dependent ones are extracted
by exploiting processing tools for the two lan-
guages (part-of-speech taggers, dependency parsers
and named entity recognizers), most of which are
available for many languages.
Our feature set can be described as follows:
Surface Form (F) features consider the num-
ber of words, punctuation marks and non-word
markers (e.g. quotations and brackets) in source
and target, as well as their ratios (source/target and
target/source), and the number of out of vocabulary
terms encountered.
Shallow Syntactic (SSyn) features consider
the number and ratios of common part-of-speech
(POS) tags in source and target. Since the list of
valid POS tags varies for different languages, we
mapped English and Spanish tags into a common
list using the FreeLing tagger (Carreras et al, 2004).
Syntactic (Syn) features consider the number
and ratios of dependency roles common to source
and target. To create a unique list of roles, we used
the DepPattern (Otero and Lopez, 2011) package,
which provides English and Spanish dependency
parsers.
Phrase Table (PT) matching features are cal-
culated as in (Mehdad et al, 2011), with a phrasal
matching algorithm that takes advantage of a lexical
phrase table extracted from a bilingual parallel
corpus. The algorithm determines the number of
phrases in the source (1 to 5-grams, at the level of
173
tokens, lemmas and stems) that can be mapped into
target word sequences, and vice-versa. To build our
English-Spanish phrase table, we used the Europarl,
News Commentary and United Nations Spanish-
English parallel corpora. After tokenization, the
Giza++ (Och and Ney, 2000) and the Moses toolkit
(Koehn et al, 2007) were respectively used to
align the corpora and extract the phrase table.
Although the phrase table was generated using MT
technology, its use to compute our features is still
compatible with a system-independent approach
since the extraction is carried out without tuning the
process towards any particular task. Moreover, our
phrase matching algorithm integrates matches from
overlapping n-grams of different size and nature
(tokens, lemmas and stems) which current MT
decoding algorithms cannot explore for complexity
reasons.
Dependency Relation (DR) matching fea-
tures target the increase of CLTE precision by
adding syntactic constraints to the matching pro-
cess. These features capture similarities between
dependency relations, combining syntactic and
lexical levels. We define a dependency relation
as a triple that connects pairs of words through a
grammatical relation. In a valid match, while the
relation has to be the same, the connected words
can be either the same, or semantically equivalent
terms in the two languages. For example, ?nsubj
(loves, John)? can match ?nsubj (ama, John)?
and ?nsubj (quiere, John)? but not ?dobj (quiere,
John)?. Term matching is carried out by means
of a bilingual dictionary extracted from parallel
corpora during PT creation. Given the dependency
tree representations of source and target produced
with DepPattern, for each grammatical relation r we
calculate two DR matching scores as the number
of matching occurrences of r in both source and
target, respectively normalized by: i) the number of
occurrences of r in the source, and ii) the number of
occurrences of r in the target.
Semantic Phrase Table (SPT) matching features
represent a novel way to leverage the integration of
semantics and MT-derived techniques. Semantically
enhanced phrase tables are used as a recall-oriented
complement to the lexical PT matching features.
SPTs are extracted from the same parallel corpora
used to build lexical PTs, augmented with shallow
semantic labels. To this aim, we first annotate the
corpora with the FreeLing named-entity tagger,
replacing named entities with general semantic
labels chosen from a coarse-grained taxonomy
(person, location, organization, date and numeric
expression). Then, we combine the sequences of
unique labels into one single token of the same
label. Finally, we extract the semantic phrase
table from the augmented corpora in the same way
mentioned above. The resulting SPTs are used to
map phrases between NE-annotated source-target
pairs, similar to PT matching. SPTs offer three
main advantages: i) semantic tags allow to match
tokens that do not occur in the original parallel
corpora used to extract the phrase table, ii) SPT
entries are often short generalizations of longer
original phrases, so the matching process can
benefit from the increased probability of mapping
higher order n-grams (i.e. those providing more
contextual information), and iii) their smaller size
has positive impact on system?s efficiency, due to
the considerable search space reduction.
4 Experiments and results
4.1 Datasets
Datasets with manual evaluation of MT output have
been made available through a number of shared
evaluation tasks. However, most of these datasets
are not specifically annotated for adequacy measure-
ment purposes, and the available adequacy judge-
ments are limited to few hundred sentences for some
language pairs. Moreover, most datasets are created
by comparing reference translations with MT sys-
tems? output, disregarding the input sentences. Such
judgements are hence biased towards the reference.
Furthermore, the inter-annotator agreement is often
low (Callison-Burch et al, 2007). In light of these
limitations, most of the available datasets are per se
not fully suitable for adequacy evaluation methods
based on supervised learning, nor to provide sta-
ble and meaningful results. To partially cope with
these problems, our experiments have been carried
out over two different datasets:
? 16K: 16.000 English-Spanish pairs, with
Spanish translations produced by multiple MT
174
systems, annotated by professional translators
with quality scores in a 4-point scale (Specia et
al., 2010a).
? WMT07: 703 English-Spanish pairs derived
from MT systems? output, with explicit ade-
quacy judgements on a 5-point scale.
The two datasets present complementary advan-
tages and disadvantages. On the one hand, al-
though it is not annotated to explicitly capture
meaning-related aspects of MT output, the quality
oriented dataset has the main advantage of being
large enough for supervised approaches. Moreover,
it should allow to check the effectiveness of our fea-
ture set in estimating adequacy as a latent aspect of
the more general notion of MT output quality. On
the other hand, the smaller dataset is less suitable
for supervised learning, but represents an appropri-
ate benchmark for MT adequacy evaluation.
4.2 Adequacy and quality prediction
To experiment with our CLTE-based evaluation
method minimizing overfitting, we randomized each
dataset 5 times (D1 to D5), and split them into 80%
for training and 20% for testing. Using different
feature sets, we then trained and tested various re-
gression models over each of the five splits, and
computed correlation coefficients between the CLTE
model predictions and the human gold standard an-
notations ([1-4] for quality, and [1-5] for adequacy).
16K quality-based dataset
In Table 1 we compare the Pearson?s correlation
coefficient of our SVM regression models against
the results reported in (Specia et al, 2010b), calcu-
lated with the same three common MT evaluation
metrics with a single reference: BLEU, TER and
Meteor. For the sake of comparison, we also re-
port the average quality correlation (QE) obtained
by (Specia et al, 2010b) over the same dataset.1
The results show that the integration of syntac-
tic and semantic information allows our adequacy-
oriented model to achieve a correlation with hu-
man quality judgements that is always significantly
1We only show the average results reported in (Specia et al,
2010b), since the distributions of the 16K dataset is different
from our randomized distribution.
higher2 than the correlation obtained by the MT
evaluation metrics used for comparison. As ex-
pected a considerable improvement over surface fea-
tures is achieved by the integration of syntactic in-
formation. A further increase, however, is brought
by the complementary contribution of SPT (recall-
oriented, due to the higher coverage of semantics-
aware phrase tables with respect to lexical PTs), and
DR matching features (precision-oriented, due to
the syntactic constraints posed to matching text por-
tions). Although they are meant to capture meaning-
related aspects of MT output, our features allow
to outperform the results obtained by the generic
quality-oriented features used by (Specia et al,
2010b), which do not discriminate between ade-
quacy and fluency.3 When dependency relations and
phrase tables (both lexical and semantics-aware) are
used in combination, our scores also outperform the
average QE score. Finally, looking at the different
random splits of the same dataset (D1 to D5), our
correlation scores remain substantially stable, prov-
ing the robustness of our approach not only for ade-
quacy, but also for quality estimation.
WMT07 adequacy-based dataset
In Table 2 we compare our regression model,
obtained in the same way previously described,
against three commonly used MT evaluation metrics
(Callison-Burch et al, 2007). In this case, the re-
ported results do not show the same consistency over
the 5 randomized datasets (D1 to D5). However, it is
worth pointing out that: i) the small dataset is partic-
ularly challenging to train models with higher corre-
lation with humans, ii) our aim is checking how far
we get using only adequacy-oriented features rather
than outperforming BLEU/TER/Meteor at any cost,
and iii) our results are not far from those achieved
by metrics that rely on reference translations. Com-
pared with Meteor, the correlation is even higher
proving the effectiveness of the proposed method.
2p < 0.05, calculated using the approximate randomization
test implemented in (Pado?, 2006).
3As reported in (Specia et al, 2010b), more than 50% (39
out of 74) of the features used is translation-independent (only
source-derived features).
175
Features D1 D2 D3 D4 D5 AVG
F 0.2506 0.2578 0.2436 0.2527 0.2443 0.25
SSyn+Syn 0.4387 0.4114 0.3994 0.4114 0.3793 0.41
F+SSyn+Syn 0.4215 0.4398 0.4059 0.4464 0.4255 0.428
F+SSyn+Syn+DR 0.4668 0.4602 0.4386 0.4437 0.4454 0.451
F+SSyn+Syn+DR+PT 0.4724 0.4715 0.4852 0.5028 0.4653 0.48
F+SSyn+Syn+DR+PT+SPT 0.4967 0.4802 0.4688 0.4894 0.4887 0.485
BLEU 0.2268
TER 0.1938
METEOR 0.2713
QE (Specia et al, 2010b) 0.4792
Table 1: Pearson?s correlation between SVM regression and human quality annotation over 16K dataset.
Features D1 D2 D3 D4 D5 AVG
F 0.10 0.03 0.04 0.10 0.14 0.083
SSyn+Syn 0.299 0.351 0.1834 0.2962 0.2417 0.274
F+SSyn+Syn 0.2648 0.2870 0.4061 0.3601 0.1327 0.29
F+SSyn+Syn+DR 0.3196 0.4568 0.2860 0.5057 0.4066 0.395
F+SSyn+Syn+DR+PT 0.3254 0.4710 0.3921 0.4599 0.3501 0.40
F+SSyn+Syn+DR+PT+SPT 0.3487 0.4032 0.4803 0.4380 0.3929 0.413
BLEU 0.466
TER 0.437
METEOR 0.357
Table 2: Pearson?s correlation between SVM regression and human adequacy annotation over WMT07.
4.3 Multi-class classification
To further explore the potential of our CLTE-based
MT evaluation method, we trained an SVM multi-
class classifier to predict the exact adequacy and
quality scores assigned by human judges. The eval-
uation was carried out measuring the accuracy of our
models with 10-fold cross validation to minimize
overfitting. As a baseline, we calculated the per-
formance of the Majority Class (MjC) classifier pro-
posed in (Specia et al, 2011), which labels all exam-
ples with the most frequent class among all classes.
The performance improvement over the result ob-
tained by the MjC baseline (?) has been calculated
to assess the contribution of different feature sets.
16K quality-based dataset
The accuracy results reported in Table 3a show
that also in this testing condition, syntactic and se-
mantic features improve over surface form ones. Be-
sides that, we observe a steady improvement over
the MjC baseline (from 5% to 12%). This demon-
strates the effectiveness of our adequacy-based fea-
tures to predict exact quality scores in a 4-point
scale, although this is a more challenging and dif-
ficult task than regression and binary classification.
Such improvement is even more interesting consid-
ering that (Specia et al, 2010b) reported discour-
aging results with multi-class classification to pre-
dict quality scores. Moreover, while they claimed
that removing target-independent features (i.e. those
only looking at the source text) significantly de-
grades their QE performance, we achieved good re-
sults without using any of these features.
WMT07 adequacy-based dataset
As we can observe in Table 3b, all variations
of adequacy estimation models significantly outper-
form the MjC baseline, with improvements rang-
176
Features 10-fold acc. ?
F 42.16% 5.16
Syn+SSyn 46.61% 9.61
F+Syn+SSyn 47.10% 10.10
F+Syn+SSyn+DR 47.26% 10.26
F+Syn+SSyn+DR+PT 48.15% 11.15
F+Syn+SSyn+DR+PT+SPT 48.74% 11.74
MjC 37% -
(a) 16K dataset.
Features 10-fold acc. ?
F 50.07% 14.07
Syn+SSyn 54.19% 18.19
F+Syn+SSyn 54.34% 18.34
F+Syn+SSyn+DR 56.47% 20.47
F+Syn+SSyn+DR+PT 56.61% 20.61
F+Syn+SSyn+DR+PT+SPT 56.75% 20.75
MjC 36% -
(b) WMT07 dataset
Table 3: Multi-class classification accuracy of the quality/adequacy scores.
Features 10-fold acc. ?
F 65.85% 11.85
Syn+SSyn 69.59% 15.59
F+Syn+SSyn 70.89% 16.89
F+Syn+SSyn+DR 71.39% 17.39
F+Syn+SSyn+DR+PT 71.92% 17.92
F+Syn+SSyn+DR+PT+SPT 72.21% 18.21
MjC 54% -
(a) 16k dataset.
Features 10-fold acc. ?
F 83.24% 12.84
Syn+SSyn 83.67% 13.27
F+Syn+SSyn 84.31% 13.91
F+Syn+SSyn+DR 84.86% 14.46
F+Syn+SSyn+DR+PT 84.96% 14.56
F+Syn+SSyn+DR+PT+SPT 85.20% 14.80
MjC 70.4% -
(b) WMT07 dataset.
Table 4: Accuracy of the binary classification into ?good? or ?adequate?, and ?bad? or ?inadequate?.
ing from 14% to 20%. Interestingly, although the
dataset is small and the number of classes is higher
(5-point scale), the improvement and overall results
are better than those obtained on the 16K dataset.
Such result confirms our hypothesis that adequacy-
based features extracted from both source and target
perform better on a dataset explicitly annotated with
adequacy judgements. In addition, the improvement
over the MjC baseline (?) of our best model is much
higher (20%) than the one reported in (Specia et al,
2011) on adequacy estimation (6%). We are aware
that their results are calculated over a dataset for a
different language pair (i.e. English-Arabic) which
brings up more challenges. However, our smaller
dataset (700 vs 2580 pairs) and the higher number
of classes (5 vs 4) compensate to some extent the
difficulty of dealing with English-Arabic pairs.
4.4 Recognizing ?good? vs ?bad? translations
Last but not least, we considered the traditional sce-
nario for quality and confidence estimation, which
is a binary classification of translations into ?good?
and ?bad? or, from the meaning point of view, ?ade-
quate? and ?inadequate?. Adequacy-oriented binary
classification has many potential applications in the
translation industry, ranging from the design of con-
fidence estimation methods that reward meaning-
preserving translations, to the optimization of the
translation workflow. For instance, an ?adequate?
translation can be just post-edited in terms of fluency
by a target language native speaker, without having
any knowledge of the source language. On the other
hand, an ?inadequate? translation should be sent to a
human translator or to another MT system, in order
to reach acceptable adequacy. Effective automatic
binary classification has an evident positive impact
on such workflow.
16K quality-based dataset
We grouped the quality scores in the 4-point scale
into two classes, where scores {1,2} are considered
as ?bad? or ?inadequate?, while {3,4} are taken as
?good? or ?adequate?. We carried out learning and
177
classification using different sets of features with 10-
fold cross validation. We also compared our accu-
racy with the MjC baseline, and calculated the im-
provement of each model (?) against it.
The results reported in Table 4a demonstrate that
the accuracy of our models is always significantly
superior to the MjC baseline. Moreover, also in this
case there is a steady improvement using syntactic
and semantic features over the results obtained by
surface form features. Additionally, it is worth men-
tioning that the best model improvement over the
baseline (?) is much higher (about 18%) than the
improvement reported in (Specia et al, 2010b) over
the same dataset (about 8%), considering the aver-
age score obtained with their data distribution. This
confirms the effectiveness of our CLTE approach
also in classifying ?good? and ?bad? translations.
WMT07 adequacy-based dataset
We mapped the 5-point scale adequacy scores into
two classes, with {1,2,3} judgements assigned to the
?inadequate? class, and {4,5} judgements assigned
to the ?adequate? class. The main motivation for this
distribution was to separate the examples in a way
that adequate translations are substantially accept-
able, while inadequate translations present evident
meaning discrepancies with the source.
The results reported in Table 4b show that the
accuracy of the binary classifiers to distinguish be-
tween ?adequate? and ?inadequate? classes was sig-
nificantly superior (up to about 15%) to the MjC
baseline. We also notice that surface form fea-
tures have a significant contribution to deal with the
adequacy-oriented dataset, while the gain obtained
using syntactic and semantic features (2%) is lower
than the improvement observed in the 16K dataset.
This might be due to the more unbalanced distribu-
tion of the classes which: i) leads to a high baseline,
and ii) together with the small size of the WMT07
dataset, makes supervised learning more challeng-
ing. Finally, the improvement of all models (?) over
the MjC baseline is much higher than the gain re-
ported in (Specia et al, 2011) over their adequacy-
oriented dataset (around 2%).
5 Conclusions
In the effort of integrating semantics into MT tech-
nology, we focused on automatic MT evaluation, in-
vestigating the potential of applying cross-lingual
textual entailment techniques for adequacy assess-
ment. The underlying assumption is that MT output
adequacy can be determined by verifying that an en-
tailment relation holds from the source to the target,
and vice-versa. Within such framework, this paper
makes two main contributions.
First, in contrast with most current metrics based
on the comparison between automatic translations
and multiple references, we avoid the bottleneck
represented by the manual creation of such refer-
ences.
Second, beyond current approaches biased to-
wards fluency or general quality judgements, we
tried to isolate the adequacy dimension of the prob-
lem, exploring the potential of adequacy-oriented
features extracted from the observation of source
and target.
To achieve our objectives, we successfully ex-
tended previous CLTE methods with a variety of lin-
guistically motivated features. Altogether, such fea-
tures led to reliable judgements that show high cor-
relation with human evaluation. Coherent results on
different datasets and classification schemes demon-
strate the effectiveness of the approach and its poten-
tial for different applications.
Future works will address both the improvement
of our adequacy evaluation method and its integra-
tion in SMT for optimization purposes. On one
hand, we plan to explore new features capturing
other semantic dimensions. A possible direction is
to consider topic modelling techniques to measure
the relatedness of source and target. Another inter-
esting direction is to investigate the use of Wikipedia
entity linking tools to support the mapping between
source and target terms. On the other hand, we plan
to explore the integration of our model as an error
criterion in SMT system training.
Acknowledgments
This work has been partially supported by the
CoSyne project (FP7-ICT-4-24853) and T4ME net-
work of excellence (FP7-IST-249119), funded by
the European Commission under the 7th Frame-
work Programme. The authors would like to thank
Hanna Bechara, Antonio Valerio Miceli Barone and
Daniele Pighin for their contributions during the MT
Marathon 2011.
178
References
E. Avramidis, M. Popovic, V. Vilar Torres, and A. Bur-
chardt. 2011. Evaluate with Confidence Estimation:
Machine Ranking of Translation Outputs using Gram-
matical Features. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation (WMT ?11).
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: A Method for Measuring Machine Translation
Confidence. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2011).
S. Banerjee and A. Lavie. 2005. METEOR: An Auto-
matic Metric for MT Evaluation with Improved Corre-
lation with Human Judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Con-
fidence Estimation for Machine Translation. In Pro-
ceedings of the 20th international conference on Com-
putational Linguistics (COLING ?04). Association for
Computational Linguistics.
C. Callison-Burch, C. Fordyce, P. Koehn, C. Monz, and
J. Schroeder. 2007. (Meta-) Evaluation of Machine
Translation. In Proceedings of the Second Workshop
on Statistical Machine Translation (WMT ?07).
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC?04).
C.C. Chang and C.J. Lin. 2011. LIBSVM: A Library
for Support Vector Machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3).
I. Dagan and O. Glickman. 2004. Probabilistic Textual
Entailment: Generic Applied Modeling of Language
Variability. In Proceedings of the PASCAL Workshop
of Learning Methods for Text Understanding and Min-
ing.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality Using N-gram Co-Occurrence
Statistics. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, HLT ?02.
J. Gime?nez and L. Ma`rquez. 2007. Linguistic Features
for Automatic Evaluation of Heterogenous MT Sys-
tems. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation (StatMT ?07).
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit for
Statistical Machine Translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions (ACL 2007).
Y. Mehdad, M. Negri, and M. Federico. 2010. Towards
Cross-Lingual Textual Entailment. In Proceedings of
the 11th Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL HLT 2010).
Y. Mehdad, M. Negri, and M. Federico. 2011. Using
Bilingual Parallel Corpora for Cross-Lingual Textual
Entailment. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies (ACL HLT 2011).
Y. Mehdad, M. Negri, and M. Federico. 2012. Detect-
ing Semantic Equivalence and Information Disparity
in Cross-lingual Documents. In Proceedings of the
ACL?12.
F.J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of the 38th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2000).
P.G. Otero and I.G. Lopez. 2011. A Grammatical For-
malism Based on Patterns of Part-of-Speech Tags. In-
ternational journal of corpus linguistics, 16(1).
S. Pado?, M. Galley, D. Jurafsky, and C. D. Manning.
2009. Textual Entailment Features for Machine Trans-
lation Evaluation. In Proceedings of the Fourth Work-
shop on Statistical Machine Translation (StatMT ?09).
S. Pado?, 2006. User?s guide to sigf: Significance test-
ing by approximate randomisation.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation (ACL 2002. In Proceedings of the
40th annual meeting on association for computational
linguistics.
C.B. Quirk. 2004. Training a Sentence-Level Machine
Translation Confidence Measure. In Proceedings of
LREC 2004.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Edit Rate
with Targeted Human Annotation. In Proceedings of
Association for Machine Translation in the Americas
(AMTA 2006).
L. Specia and A. Farzindar. 2010. Estimating Machine
Translation Post-Editing Effort with HTER. In Pro-
ceedings of the AMTA-2010 Workshop, Bringing MT
to the User: MT Research and the Translation Indus-
try.
L. Specia, N. Cancedda, and M. Dymetman. 2010a.
A Dataset for Assessing Machine Translation Eval-
uation Metrics. In Proceedings of the 7th interna-
tional conference on Language Resources and Eval-
uation (LREC10).
179
L. Specia, D. Raj, and M. Turchi. 2010b. Machine Trans-
lation Evaluation Versus Quality Estimation. Machine
translation, 24(1).
L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2011.
Predicting Machine Translation Adequacy. In Pro-
ceedings of the 13th Machine Translation Summit (MT-
Summit 2011).
L. Specia. 2011. Exploiting Objective Annotations for
Minimising Translation Post-editing Effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation (EAMT 2011).
D. Xiong, M. Zhang, and H. Li. 2010. Error Detection
for Statistical Machine Translation Using Linguistic
Features. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL
2010). Association for Computational Linguistics.
180
Proceedings of the 14th European Workshop on Natural Language Generation, pages 136?146,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Abstractive Meeting Summarization with Entailment and Fusion
Yashar Mehdad
?
Giuseppe Carenini
?
Frank W. Tompa
??
Raymond T. NG
?
Department of Computer Science
?University of British Columbia ??University of Waterloo
{mehdad, carenini, rng}@cs.ubc.ca fwtompa@cs.uwaterloo.ca
Abstract
We propose a novel end-to-end frame-
work for abstractive meeting summariza-
tion. We cluster sentences in the in-
put into communities and build an entail-
ment graph over the sentence communi-
ties to identify and select the most relevant
sentences. We then aggregate those se-
lected sentences by means of a word graph
model. We exploit a ranking strategy to
select the best path in the word graph as
an abstract sentence. Despite not relying
on the syntactic structure, our approach
significantly outperforms previous models
for meeting summarization in terms of in-
formativeness. Moreover, the longer sen-
tences generated by our method are com-
petitive with shorter sentences generated
by the previous word graph model in terms
of grammaticality.
1 Introduction
The huge amount of data generated every day in
meetings calls for developing automated methods
to efficiently process these data to meet users?
needs. Automatic summarization is a popular task
that can help users to browse a large amount of
recorder speech in text format. This paper tackles
the task of recorded meeting summarization, ad-
dressing the key limitations of existing approaches
by proposing the following contributions:
1) Various approaches that were recently devel-
oped for meeting summarization (such as (Gillick
et al, 2009; Garg et al, 2009)) focus on extract-
ing important sentences (or dialogue acts) from
speech transcripts, either manual transcripts or au-
tomatic speech recognition (ASR) output. How-
ever, it has been observed in the context of meet-
ing summarization users generally prefer concise
abstracts over extracts, and abstracts lead to higher
objective task scores; likewise automatic abstrac-
tive summaries are preferred in comparison with
human extracts (Murray et al, 2010). Moreover,
most of the abstractive summarization approaches
focus on one component of the system, such as
generation (e.g., (Genest and Lapalme, 2010)) or
content selection (e.g., (Murray et al, 2012)), in-
stead of developing the full framework for abstrac-
tive summarization. To address these limitations,
as the main contribution of this paper, we pro-
pose a full pipeline to generate an abstractive sum-
mary for each meeting transcript. Our system is
similar to that of Murray et al (2010) in terms
of generating abstractive summaries for meeting
transcripts. However, we take a lighter supervi-
sion for the content selection phase and a different
approach towards the language generation phase,
which does not rely on the conventional Natural
Language Generation (NLG) architecture (Reiter
and Dale, 2000).
2) We propose a word graph based approach
to aggregate and generate the abstractive sentence
summary. Our work extends the word graph
method proposed by Filippova (2010) with the fol-
lowing novel contributions: i) We take advantage
of lexical knowledge to merge similar nodes by
finding their relations in WordNet; ii) We gener-
ate new sentences through generalization and ag-
gregation of the original ones, which means that
our generated sentences are not necessarily com-
posed of the original words; and iii) We adopt a
new ranking strategy to select the best path in the
graph by taking the information content and the
grammaticality (i.e. fluency) of the sentence into
consideration.
3) In order to generate an abstract summary for
a meeting, we have to be able to capture the over-
all content of the conversation. This can be done
by identifying the essential content from the most
informative sentences using the semantic relations
among all sentences in a meeting transcript. How-
136
ever, most current methods disregard such rela-
tions in favor of statistical models of word distri-
butions and frequencies. Moreover, the data from
meeting transcripts often contains many highly re-
dundant sentences. As one of the key contribu-
tions of this paper, we propose to build a multi-
directional entailment graph over the sentences to
identify and select relevant information from the
most informative sentences.
4) The textual data from meeting conversa-
tion transcripts are typically in a casual style and
do not exhibit a clear syntactic structure with
proper grammar and spelling. Therefore, abstrac-
tive summarization approaches developed for for-
mal texts, such as scientific or news articles, of-
ten are not satisfactory when dealing with infor-
mal texts. Our proposed method for abstractive
meeting summarization requires minimal syntac-
tic and structural information and is robust in deal-
ing with text that suffers from transcription errors,
ill-formed sentences and unknown lexical choices
such as typically formed in meeting transcripts.
We evaluate our system over meeting tran-
scripts. Our result compares favorably to the result
of previous extractive and abstractive approaches
in terms of information content. Moreover, we
show that our method can generate longer sen-
tences with competitive grammaticality scores, in
comparison with previous abstractive approaches.
Furthermore, we evaluate the impact of each com-
ponent of our system through an ablation test.
As an additional result of our experiments, we
also show that using semantic relations (entail-
ment graph) is important in efficiently performing
the final step of our summarization pipeline (i.e.,
the sentence fusion).
2 Abstractive Summarization
Framework
Similar to Murray et al (2010), our goal is to
generate a meeting summary, i.e. a set of sen-
tences, that could capture the semantics of the
meeting. While (Murray et al, 2010) requires
extensive annotations to train several classifiers
to detect important sentences, opinions and dia-
log acts, we only use a subset of that annotation,
which includes a human abstract and links from
each sentence in the abstract to the source meet-
ing sentences. In addition, instead of generat-
ing an abstractive sentence via the conventional
NLG pipeline (Reiter and Dale, 2000), we exploit
a word graph approach.
Linking
Detection
Entailment
Identify
Community
Detection Entailment Graph
Word Graph
Ranking
Sentence Fusion? ? ?
- -
1Figure 1: Meeting summarization framework.
As shown in Figure 1, our framework consists
of three main components, which we describe in
more detail in the following sections.
2.1 Community Detection
While some abstractive summary sentences are
very similar to original sentences from the meeting
transcript, others can be created by aggregating
and merging multiple sentences into an abstract
sentence. In order to generate such a sentence, we
need to identify which sentences from the original
meeting transcript should be combined in gener-
ated abstract sentences. This task can be consid-
ered as the first step of abstractive meeting sum-
marization and is called ?abstractive community
detection (ACD)? (Murray et al, 2012). To per-
form ACD, we follow the same method proposed
by Murray et al (2012), in two steps:
First, we classify sentence pairs according to
whether or not they should be realized by a com-
mon abstractive sentence. For each pair, we ex-
tract its structural and linguistic features, and we
train a logistic regression classifier over all our
training data (described in Section 3.1) exploiting
such features. We run the trained classier over sen-
tence pairs, predicting abstractive links between
sentences in the document. The result can be rep-
resented as an undirected graph where nodes are
the sentences, and edges represent whether two
nodes are linked.
Second, we have to identify which nodes (i.e.,
sentences from the meeting transcript) can be clus-
tered as a community to generate an abstract sen-
tence. For this purpose, we apply the CONGA al-
gorithm (Gregory, 2007) for community detection
that uses betweenness to detect communities in a
graph. The betweenness score for an edge is the
number of shortest paths between pairs of nodes
in the graph that run along that edge.
If a sentence is not connected to at least one
other sentence in the first step, it?s assigned to its
own singleton community.
137
CD
E
F
GA Bx
x
1
Figure 2: Building an entailment graph over sentences. Ar-
rows and ?x? represent the entailment direction and unknown
cases respectively.
2.2 Entailment Graph
Sentences in a community often include redundant
information which are semantically equivalent but
vary in lexical choices. By identifying the seman-
tic relations between the sentences in each com-
munity, we can discover the information in one
sentence that is semantically equivalent, novel, or
more/less informative with respect to the content
of the other sentences.
Similar to earlier work (Lloret et al, 2008;
Mehdad et al, 2010; Berant et al, 2011; Adler et
al., 2012; Mehdad et al, 2013), we set this prob-
lem as a variant of the Textual Entailment (TE)
recognition task (Dagan and Glickman, 2004). We
build an entailment graph for each community of
sentences, where nodes are the linked sentences
and edges are the entailment relations between
nodes. Given two sentences (s1 and s2), we aim
at identifying the following cases:
i) s1 and s2 express the same meaning (bidirec-
tional entailment). In such cases one of the sen-
tences should be eliminated;
ii) s1 is more informative than s2 (unidirectional
entailment). In such cases, the entailing sentence
should replace or complement the entailed one;
iii) s1 contains facts that are not present in s2, and
vice-versa (the ?unknown? cases in TE parlance).
In such cases, both sentences should remain.
Figure 2 shows how entailment relations can
help in selecting the sentences by removing the re-
dundant and less informative ones. As we show in
the figure, the sentence ?A? entails ?E?, ?F? and
?G?, but not ?B?. So we can keep ?A? and ?B?
and eliminate others. For example, the sentence
?we should discuss about the remote control and
its color? entails ?about the remote?, ?let?s talk
about the remote? and ?um remote?s color?, but
not ?remote?s size is also important?. So we can
keep ?we should discuss about the remote con-
trol and its color? and ?remote?s size is also im-
portant? and eliminate the others. In this way,
TE-based sentence identification can be designed
to distinguish meaning-preserving variations from
true divergence, regardless of lexical choices and
structures.
Similar to previous approaches in TE (e.g., (Be-
rant et al, 2011)), we use a supervised method. To
train and build the entailment graph, we perform
three steps described in the following subsections.
2.2.1 Training set collection
In the last few years, TE corpora have been cre-
ated and distributed in the framework of several
evaluation campaigns, including the Recognizing
Textual Entailment (RTE) Challenge1 and Cross-
lingual textual entailment for content synchroniza-
tion2 (Negri et al, 2012). However, such datasets
cannot directly support our application, since the
RTE datasets are often composed of longer well-
formed sentences and paragraphs (Bentivogli et
al., 2009; Negri et al, 2011).
In order to collect a dataset that is more sim-
ilar to the goal of our entailment framework, we
select a subset of the sixth and seventh RTE chal-
lenge main task (i.e., RTE within a Corpus). Our
dataset choice is based on the following reasons:
i) the length of sentence pairs in RTE6 and RTE7
is shorter than the others, and ii) RTE6 and RTE7
main task datasets are originally created for sum-
marization purpose, which is closer to our work.
We sort the RTE6 and RTE7 dataset pairs based
on the sentence length and choose the first 2000
samples with an equal number of positive and neg-
ative examples. The average length of words in
our training data is 7 words. There are certainly
some differences between our training set and our
sentences from the meeting corpus. However, the
collected training samples was the closest avail-
able dataset to our needs.
2.2.2 Feature representation and training
Working with meeting transcripts imposes some
constraints on feature selection. Meeting tran-
scripts are not often well-formed in terms of sen-
1http://pascallin.ecs.soton.ac.uk/Challenges/RTE/
2http://www.cs.york.ac.uk/semeval-2013/task8/
138
tence structure and contain errors. This limits
our features to the lexical level. Fortunately, lexi-
cal models are less computationally expensive and
easier to implement and often deliver a strong per-
formance for RTE (Sammons et al, 2011).
Our entailment decision criterion is based on
similarity scores calculated with a sentence-to-
sentence matching process. Each example pair of
sentences (s1 and s2) is represented by a feature
vector, where each feature is a specific similarity
score estimating whether s1 entails s2.
We compute 18 similarity scores for each pair
of sentences. Before aggregating the similarity
scores to form an entailment score, we normalize
the similarity scores by the length of s2 (in terms
of lexical items), when checking the entailment di-
rection from s1 to s2. In this way, we can estimate
the portion of information/facts in s2 which is cov-
ered by s1.
The first five scores are computed based on the
exact lexical overlap between the phrases: word
overlap, edit distance, ngram-overlap, longest
common subsequence and Lesk (Lesk, 1986).
The other scores were computed using lexical
resources: WordNet (Fellbaum, 1998), VerbO-
cean (Chklovski and Pantel, 2004), paraphrases
(Denkowski and Lavie, 2010) and phrase match-
ing (Mehdad et al, 2011). We used WordNet
to compute the word similarity as the least com-
mon subsumer between two words considering the
synonymy-antonymy, hypernymy-hyponymy, and
meronymy relations. Then, we calculated the sen-
tence similarity as the sum of the similarity scores
of the word pairs in Text and Hypothesis, nor-
malized by the number of words in Hypothesis.
We also use phrase matching features described in
(Mehdad et al, 2011) which consists of phrasal
matching at the level on ngrams (1 to 5 tokens).
The rationale behind using different entailment
features is that combining various scores will yield
a better model (Berant et al, 2011).
To combine the entailment scores and optimize
their relative weights, we train a Support Vector
Machine binary classifier, SVMlight (Joachims,
1999), over an equal number of positive and nega-
tive examples. This results in an entailment model
with 95% accuracy over 2-fold and 5-fold cross-
validation, which further proves the effectiveness
of our feature set for this lexical entailment model.
The reason that we gained a very high accuracy
is because our selected sentences are a subset
of RTE6 and RTE7 with a shorter length (fewer
words) which makes the entailment recognition
task much easier than recognizing entailment be-
tween paragraphs or long sentences.
2.2.3 Entailment graph edge labeling
Since our training examples are labeled with bi-
nary judgments, we are not able to train a three-
way classifier. Therefore, we set the edge label-
ing problem as a two-way classification task that
casts multidirectional entailment as a unidirec-
tional problem, where each pair is analyzed check-
ing for entailment in both directions (Mehdad et
al., 2012). In this condition, each original test
example is correctly classified if both pairs origi-
nated from it are correctly judged (?YES-YES? for
bidirectional,?YES-NO? and ?NO-YES? for unidi-
rectional entailment and ?NO-NO? for unknown
cases). Two-way classification represents an intu-
itive solution to capture multidimensional entail-
ment relations.
2.2.4 Identification and selection
By assigning all entailment relations between the
extracted sentence pairs, we identify relevant sen-
tences to eliminate the redundant (in terms of
meaning) and less informative ones. In order to
perform this task we follow a set of rules based
on the graph edge labels. Note that since entail-
ment is a transitive relation, our entailment graph
is transitive i.e., if entail(s1,s2) and entail(s2,s3)
then entail(s1,s3) (Berant et al, 2011).
Rule 1) Among the nodes that are connected with
bidirectional entailment (semantically equivalent
nodes) we keep only the one with more outgo-
ing bidirectional and unidirectional entailment re-
lations;
Rule 2) If there is a chain of entailing nodes, we
keep the one that is the root of the chain and elim-
inate others.
2.3 Multi-sentence Fusion
Sentence fusion is a well-known challenge for
summarization systems. In this phase, our goal
is to generate an understandable informative sen-
tence that maximally captures the content of the
sentences in a sentence community.
There are several ways of generating an abstract
sentence (e.g. (Barzilay and McKeown, 2005; Liu
and Liu, 2009; Ganesan et al, 2010; Murray et
al., 2010)); however, most of them rely heavily
on the syntactic structure. We believe that such
139
we
um
should
/must
choose/
deter-
mine
The remote control is
beca-
use
the
cost/
price
important
/crucial
use of uh
Start End
1
Figure 3: Word graph constructed from sentences (1-4) and possible fusion paths. Double line nodes represent merged words
in the graph.
approaches are suboptimal, especially in dealing
with written conversational data (e.g., email) or
the data from speech transcripts, whether manual
transcription or automatic speech recognition out-
put. Instead, we apply an approach that does not
rely on syntax, nor on a standard NLG architec-
ture. More specifically, we build a word graph
from all the words of the sentences in a commu-
nity and aggregate them in order to generate a new
abstractive sentence.
We perform the task of multi-sentence fusion in
two steps. First, we construct a word graph over
sentences in each community that were selected
from the entailment graph. Second, we rank the
valid paths in the word graph and select the top
path as the abstract sentence summary.
2.3.1 Constructing a Word Graph
In order to construct a word graph, our model ex-
tends the word graph method proposed by Filip-
pova (2010) with the following novel contribu-
tions:
1- The basic word graph method disregards se-
mantic and lexical relations between the words
in constructing the word graph, in favor of re-
dundancy and word frequencies. To move be-
yond such limitation, we take advantage of lexi-
cal knowledge to map the similar nodes by finding
their relations in WordNet. In this way, for exam-
ple, two synonym words can be mapped into the
same node.
2- Filippova?s approach is essentially extractive
in nature, which means the generated sentence is
composed by the same words from the original
sentences. We move beyond this by generating
new sentences through generalization and aggre-
gation of the original ones. This means that our
generated sentences are not necessarily composed
of the original words. In this way, we are one step
closer to abstractive summarization.
3- Our proposed method aggregates and gen-
erates new readable sentences, regardless of their
lengths, that can semantically imply several orig-
inal sentences, by taking the information content
and the readability (i.e. fluency) of the sentence
into consideration.
Following Filippova?s method, given a set of re-
lated sentences, we build a word graph by itera-
tively adding sentences to it. Figure 1 illustrates
a small graph composed of 4 sentences, including
the start and end nodes.
1- we must determine the use of uh remote.
2- The remote control is important because the
cost.
3- um we should choose the control.
4- The remote price is crucial.
As one of the main steps of word graph con-
struction, we merge the words that have the same
POS tags under the following conditions:
1) The words are identical (e.g. ?remote?).
2) The words are synonyms. The replacement
choice is based on the word?s commonality, i.e.
tfidf (e.g. ?important? and ?crucial?).
3) The words form a hypernym/hyponym pair or
share a common hypernym. Both words are re-
placed by the hypernym (e.g. ?price? and ?cost?).
4) The words are in an entailment relation. Both
words are replaced by the entailed one (e.g. ?pay?
and ?buy?).
Note that, similar to Filippova?s approach,
where merging is ambiguous we check the context
(a word before and after in the sentence and the
neighboring nodes in the graph) and select the can-
didate that has larger overlap in the context, or the
one with a greater frequency. Similar to the origi-
nal word graph model, we connect adjacent words
with directed edges. For the new nodes or uncon-
nected nodes, we draw an edge with a weight of
1. In contrast, weights between already connected
nodes are increased by 1.
140
2.3.2 Path Selection and Ranking
The word graph, as described above, will generate
many sequences connecting start and end. How-
ever, it is likely that most of the paths are not read-
able. Since we are aiming at generating a good
abstractive sentence, some constraints need to be
considered.
A good abstractive sentence should cover most
of the concepts that exist in the original sentences.
Moreover, it should be grammatically correct.
In order to satisfy these constraints we adopt a
ranking strategy that combines the characteristics
of a good summary sentence. To filter ungram-
matical sentences, we prune the paths in which a
verb does not exist. Our ranking formulation is
summarized as below:
Fluency: Our word graph process generates
many possible paths as abstractive summaries.
We need now to decide which of these paths
are more readable and fluent. As in other areas
of NLP (e.g. machine translation and speech
recognition), the answer can be estimated by a
language model. We assign a probability Pr(P )
to each path P based on a n-gram language model.Pr(P ) = mY
i=1
Pr(pi|pi 11 ) ? mY
i=1
Pr(pi|pi 1i n+1)
?
mX
i=1
 logPr(pi|pi 1i n+1)
Coverage: To identify the summary with the high-
est coverage, we propose a second score that esti-
mates the number of nouns that appear in the path.
In order to reward the ranking score to cover more
salient nouns, we also consider the tfidf score of
nouns in the coverage formulation.
C overage(P ) = Ppi2P tfidf (pi)P
pi2G tfidf (pi)
where the pi are nouns and G is the graph.
Edge weight: As a third score, we adopt the Filip-
pova?s edge weighting formulation w(pi, pj) and
define the edge weight of the path W (P ) as be-
low: w(pi, pj) = freq(pi) + freq(pj)P
P2G
pi,pj2P
di? (P, pi, pj) 1W (P ) = Pm 1i=1 w(pi, pi+1)m  1
where the function diff(P, pi, pj) refers to the
distance between the offset positions pos(P, pi)
of words pi and pj in path P and is defined as
|pos(P, pj)   pos(P, pi)| and m is the number of
words in path P .
Ranking score: In order to generate a summary
sentence that combines the scores above, we
employ a ranking model. The purpose of such
a model is three-fold: i) to generate a more
readable and grammatical sentence; ii) to cover
the content of original sentences optimally; and
iii) to favor strong connections between the
concepts. Therefore, the final ranking score of
path P is calculated over the normalized scores as:Score(P ) = Pr(P )? Coverage(P )W (P )
We select all the paths that contain at least one
verb and rerank them using our proposed ranking
function to find the best path as the summary of
the original sentences.
3 Experiments and Results
We now describe a preliminary, formative evalua-
tion of our framework, whose main goal is to as-
sess strengths and weaknesses of the various com-
ponents and generate ideas for further develop-
ments.
3.1 Dataset
To verify the effectiveness of our approach, we ex-
periment with the AMI meeting corpus (Carletta
et al, 2005) that consists of 140 multi-party meet-
ings with a wide range of annotations, including
abstactive summaries for each meeting and links
from each sentence in the summary to the set of
sentences in the original transcripts that sentence
is summarizing. We use this information as our
gold standard since it provides many examples in
which a set of sentences in the meeting (a commu-
nity) is linked to a human written sentence sum-
marizing that community.
141
In our experiments, we use human authored
transcripts. Note that our approach is not specific
to conversations, however it is designed to deal
with ill-formed sentences and structural errors.
Moreover, the first two components of our sys-
tem are supervised, while the word graph model
is completely unsupervised and domain indepen-
dent.
In order to train our logistic regression classifier
for the first phase of our pipeline, we randomly
select a training set that consists of 98 meetings.
Note that there are about one million sentence pair
instances in the training set since we consider ev-
ery pairing of sentences within a meeting. The rest
is selected as a test set for the evaluation phase.
3.2 Experimental Settings
For preprocessing our dataset we use OpenNLP3
for tokenization and part-of-speech tagging. When
the number of sentences in each community is
more than 10 (which happens in around 10% of
the cases), the community is first clustered using
the MajorClust (Stein and Niggemann, 1999) al-
gorithm when sentences are represented as nor-
malized tfidf vectors and the similarity between
the sentences is measured using cosine similarity.
Then, each cluster is treated as a separate com-
munity. The clustering guarantees a higher over-
lap between the sentences as well as a word graph
with fewer paths. Next, we construct a word graph
over each cluster and rank the valid paths. We
choose the first ranked path as the abstractive sum-
mary of the cluster. For our language model, we
use a tri-gram smoothed language model trained
using the newswire text provided in the English
Gigaword corpus (Graff and Cieri, ).
3.3 Evaluation Metrics
To evaluate performance, we use the ROUGE-1
and ROUGE-2 (unigram and bigram overlap) F1
score, which correlate well with human rankings
of summary quality (Lin and Hovy, 2003). We
also ignore stopwords to reduce the impact of high
overlap when matching them.
Furthermore, to evaluate the grammaticality of
our generated summaries in comparison with the
original word graph method, following common
practice (Barzilay and McKeown, 2005), we ran-
domly selected 10 meeting summaries (total 150
sentences). Then, we asked annotators to give one
3http://opennlp.apache.org/
Models ROUGE-1 ROUGE-2
MMR-centroid 18 3
MMR-cosine 21 -
ILP 24 -
TextRank 25.0 4.4
ClusterRank 27.5 5.1
Orig. word graph 26.9 3.8
Our model (-ent) 32.3 4.8
Our model (GC) 32.1 4.0
Our model (full) 28.7 4.2
Table 1: Performance of different summarization algorithms
on human transcripts for meeting conversations. 5
of three possible ratings for each sentence in a
summary based on grammaticality: perfect (2 pts),
only one mistake (1 pt) and not acceptable (0 pts),
ignoring the capitalization or punctuation. Each
meeting was rated by two annotators (Computer
Science graduate students).
3.4 Baselines
We compare our approach with various extrac-
tive baselines: 1) MMR-centroid system (Car-
bonell and Goldstein, 1998); 2) MMR-cosine sys-
tem (Gillick et al, 2009); 3) ILP-based system
(Gillick et al, 2009); 4) TextRank system (Mihal-
cea and Tarau, 2004); and 5) ClusterRank system
(Garg et al, 2009) and with one abstractive base-
line: 6) Original word graph model (Orig. word
graph) (Filippova, 2010).
In order to measure the effectiveness of dif-
ferent components, we also evaluated our sys-
tem using human-annotated sentence communities
(GC) in comparison with our community detection
model (full). Moreover, we measure the perfor-
mance of our system (GC) ablating the entailment
module (-ent).
3.5 Results
Table 1 shows the results for our proposed ap-
proach in comparison with these strong baselines
for meeting summarization. The results show that
our model outperforms the baselines significantly4
for ROUGE-1 over human transcripts for meet-
ing conversations, which proves the effectiveness
of our approach in dealing with summarization of
5The MMR-cosine and ILP systems did not report the
ROUGE-2 score.
4The statistical significance tests was calculated by ap-
proximate randomization described in (Yeh, 2000).
142
Models Read. R=2 R=1 R=0 Avg Len.
Orig. word graph 1.41 55% 32% 13% 8
Our model 1.34 47% 39% 14% 14
Table 2: Average rating and distribution over rating scores for abstractive word graph models.
meeting conversations. However, the ClusterRank
and TextRank systems outperform our model for
ROUGE-2 score. This can be due to word merging
and word replacement choices in the word graph
construction (see Section 2.3.1), which sometimes
changes a word in a bigram and consequently de-
creases the bigram overlap score. A more detailed
analysis of this problem is left as future work.
Note that there is a drop in ROUGE score when
we use entailment in our system in comparison
with ablating the entailment phase (-ent). This is
mainly due to the fact that the entailment phase
filters equivalent sentences. This affects the re-
sults negatively when such filtered sentences share
many common words with our human-authored
abstracts. We believe that this drop is partly as-
sociated with our evaluation metric rather than
meaning. In other words, we expect no difference
in performance when a human evaluation is ap-
plied. However, the entailment phase helps in im-
proving the efficiency of our pipeline significantly.
If each graph has e edges, n nodes, and p paths,
then finding all the paths results in time complex-
ity O((np + 1)(e + n)), using depth-first search.
Decreasing the number of sentences will reduce
the number of nodes and edges, which leads to
the smaller number of paths. This is even more
significant when there are many sentences in a
community in comparison with the gold standard.
Note that it?s impossible to finish the graph build-
ing phase after 12 hours on a 2.3 GHz quad-core
machine without performing the entailment phase,
when we use our community detection model.
This would be especially problematic in a real-
time setting.
Comparing the gold standard sentence commu-
nities (GC) and our fully automatic system, we can
notice that inaccuracies in the community detec-
tion phase affects the overall performance. How-
ever, using our community detection model, we
still outperform the previous models significantly.
Table 2 shows grammaticality scores, distribu-
tions over the three scores and average sentence
lengths in tokens. The results demonstrate that
47% of the sentences generated by our method are
grammatically correct and 39% of the generated
sentences are almost correct. In comparison with
the original word graph method, our model reports
slightly lower results for the grammaticality score
and the percentage of correct sentences. How-
ever, considering the correlation between sentence
length and grammatical complexity, our model is
capable of generating longer sentences with more
information content (according to ROUGE) and
competitive grammaticality scores.
4 Discussion
After analyzing the results and through manual
verification of some cases, we observe that our ap-
proach produces some interestingly successful ex-
amples. Nevertheless, it appears that the perfor-
mance is still far from satisfactory. This leaves an
interesting challenge for the research community
to tackle. We have identified five different sources
of error:
Type 1: Abstractive human-authored summaries:
the nature of our method is based on extracting
the relevant sentences and generating an abstract
sentence by aggregating such sentences. Also due
to this, our generated abstracts are often infor-
mal and closer to the transcripts? style. However,
in many cases, the human-written summaries are
composed by understanding the original sentences
and produce a formal style abstract sentence, of-
ten using a different vocabulary and structure. For
example:
Human-authored: The industrial designer and user in-
terface designer presented the prototype they created,
which was designed to look like a banana.
System: Working on the principle of a fruit it?s basically
designed around a banana.
Type 2: Evaluation method: The current evalu-
ation methods fail to capture the meaning and re-
lies only on matching the words at uni- or bigram
level. Therefore, we believe that a manual eval-
uation can reveal more potential of our system in
generating abstractive summaries that are closer to
human-written summaries.
143
Human-authored: the project manager recapped the de-
cisions made in the previous meeting.
System: I told you guys about the three new require-
ments ... so that was the last meeting.
Type 3: Subjective abstractive summaries: of-
ten it is not easy for humans to agree on one sum-
mary for a meeting. It is well known that inter-
annotator agreement is quite low for the summa-
rization task (Mani, 2001). For example:
Human-authored 1: They do tool training with a white-
board and each person introduces themselves and draws
their favorite animal on the board.
Human-authored 2: The group introduced themselves to
each other and acquainted themselves with the meeting-
room materials by drawing on the whiteboard.
System: We are gonna know each other and then draw
your little animal.
Type 4: Speaker information: since the nature of
our method is based on extracting the relevant sen-
tences or speaker utterances, we do not take the
speaker information into consideration. However,
the human-written summaries for meetings take
the speaker into account. We plan to extend our
framework to include this feature. For example:
Human-authored: The project manager opened the
meeting and stated the agenda to the team members.
System: I hope you?re ready for this functional design
meeting know at the end projects requirement.
Type 5: Transcription errors: as mentioned be-
fore, the meeting transcripts often contain struc-
ture, grammar, vocabulary choice and dictation er-
rors. This always raises more challenges for algo-
rithms dealing with such texts. For example:
Transcript: if it i if it isn?t more expensive for us to k
make because as far as I understand it.
In light of this analysis, we conclude that a
more comprehensive evaluation method (e.g., hu-
man evaluation), including speaker information in
the pipeline and using text normalization tech-
niques to reduce the effects of noisy transcripts can
better reveal the potential of our system in dealing
with meeting summarization.
5 Conclusion and Future Work
In this paper, we study the problem of abstrac-
tive meeting summarization, and propose a novel
framework to generate summaries composed of
grammatical sentences. Within this framework,
this paper makes three main contributions. First,
in contrast with most current methods based on
fully extractive models, we propose to take advan-
tage of a word graph model for sentence fusion
to generate abstractive summary sentences. Sec-
ond, beyond most of the current approaches which
disregard semantic information, we integrate se-
mantics by means of building textual entailment
graphs over sentence communities. Third, our
framework uses minimal syntactic information in
comparison with previous methods and does not
require a domain specific, engineered conven-
tional NLP component.
We successfully applied our framework over
a challenging meeting dataset, the AMI corpus.
Some significant improvements over our dataset,
in comparison with previous methods, demon-
strates the potential of our approach in dealing
with meeting summarization. Moreover, we prove
that our model can generate longer sentences with
only a minimal loss in grammaticality.
In light of the results of our preliminary forma-
tive evaluation, future work will address the im-
provement of the community detection and sen-
tence fusion phases. On the one hand, we plan to
improve our community detection graph by adding
more relevant features into our current supervised
model. On the other hand, we plan to incorporate
a better source of lexical knowledge in the word
graph construction (e.g., YAGO or DBpedia). We
are also interested in improving our ranking model
by assigning tuned weights to each component. In
addition, we are exploring the replacement of pro-
nouns by their referents (e.g., replacing ?I? by the
name or role of the speaker) to improve both the
entailment and word graph models. Once we will
have explored all these improvements, we plan to
run more comprehensive human evaluations.
Acknowledgments
We would like to thank the anonymous review-
ers for their valuable comments and suggestions
to improve the paper, our annotators for their valu-
able work, and the NSERC Business Intelligence
Network for financial support.
144
References
Meni Adler, Jonathan Berant, and Ido Dagan. 2012.
Entailment-based text exploration with application
to the health-care domain. In Proceedings of
the ACL 2012 System Demonstrations, ACL ?12,
pages 79?84, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence Fusion for Multidocument News Sum-
marization. Comput. Linguist., 31(3):297?328,
September.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The
Fifth PASCAL Recognizing Textual Entailment
Challenge. In Proc Text Analysis Conference
(TAC09.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global Learning of Typed Entailment Rules.
In Proceedings of ACL, Portland, OR.
Jaime Carbonell and Jade Goldstein. 1998. The use
of MMR, diversity-based reranking for reordering
documents and producing summaries. In Proceed-
ings of the 21st annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?98, pages 335?336, New
York, NY, USA. ACM.
Jean Carletta, Simone Ashby, Sebastien Bourban,
Mike Flynn, Thomas Hain, Jaroslav Kadlec, Vasilis
Karaiskos, Wessel Kraaij, Melissa Kronenthal, Guil-
laume Lathoud, Mike Lincoln, Agnes Lisowska, and
Mccowan Wilfried Post Dennis Reidsma. 2005.
The AMI meeting corpus: A pre-announcement. In
Proc. MLMI, pages 28?39.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In Dekang Lin and Dekai Wu, ed-
itors, Proceedings of EMNLP 2004, pages 33?40,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
I. Dagan and O. Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic applied modeling of lan-
guage variability. In PASCAL Workshop on Learn-
ing Methods for Text Understanding and Mining.
Michael Denkowski and Alon Lavie. 2010.
METEOR-NEXT and the METEOR paraphrase ta-
bles: improved evaluation support for five target
language. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, WMT ?10, pages 339?342, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Katja Filippova. 2010. Multi-sentence compression:
finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 322?
330, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ?10, pages
340?348, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Nikhil Garg, Benoit Favre, Korbinian Reidhammer,
and Dilek Hakkani Tu?r. 2009. ClusterRank: A
Graph Based Method for Meeting Summarization.
Idiap-RR Idiap-RR-09-2009, Idiap, P.O. Box 592,
CH-1920 Martigny, Switzerland, 6.
Pierre-Etienne Genest and Guy Lapalme. 2010. Text
Generation for Abstractive Summarization. In Pro-
ceedings of the Third Text Analysis Conference,
Gaithersburg, Maryland, USA. National Institute
of Standards and Technology, National Institute of
Standards and Technology.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-tr. 2009. A global optimization
framework for meeting summarization. In Proc.
IEEE ICASSP, pages 4769?4772.
David Graff and Christopher Cieri. English Gigaword
Corpus?, year = 2003, institution = Linguistic Data
Consortium, address = Philadelphia,. Technical re-
port.
Steve Gregory. 2007. An Algorithm to Find Over-
lapping Community Structure in Networks. In
Proceedings of the 11th European conference on
Principles and Practice of Knowledge Discovery in
Databases, PKDD 2007, pages 91?102, Berlin, Hei-
delberg. Springer-Verlag.
T. Joachims. 1999. Making large-Scale SVMLearning
Practical. LS8-Report 24, Universita?t Dortmund, LS
VIII-Report.
Michael Lesk. 1986. Automatic sense disambigua-
tion using machine readable dictionaries: how to tell
a pine cone from an ice cream cone. In Proceed-
ings of the 5th annual international conference on
Systems documentation, SIGDOC ?86, pages 24?26,
New York, NY, USA. ACM.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using N-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology - Volume 1, NAACL ?03,
pages 71?78, Stroudsburg, PA, USA. Association
for Computational Linguistics.
145
Fei Liu and Yang Liu. 2009. From extractive to ab-
stractive meeting summaries: can it be done by sen-
tence compression? In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
?09, pages 261?264, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Elena Lloret, O?scar Ferra?ndez, Rafael Mun?oz, and
Manuel Palomar. 2008. A Text Summarization Ap-
proach under the Influence of Textual Entailment. In
NLPCS, pages 22?31.
I. Mani. 2001. Automatic summarization. Natural
Language Processing, 3. J. Benjamins Publishing
Company.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards cross-lingual textual entailment.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 321?324, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2011. Using bilingual parallel corpora for
cross-lingual textual entailment. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1336?1345,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Short Papers
- Volume 2, ACL ?12, pages 120?124, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Yashar Mehdad, Giuseppe Carenini, and Raymond
NG T. 2013. Towards Topic Labeling with Phrase
Entailment and Aggregation. In Proceedings of
NAACL 2013, pages 179?189, Atlanta, USA, June.
Association for Computational Linguistics.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, July.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2010. Generating and validating abstracts of meet-
ing conversations: a user study. In Proceedings of
the 6th International Natural Language Generation
Conference, INLG ?10, pages 105?113, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Gabriel Murray, Giuseppe Carenini, and Raymond Ng.
2012. Using the omega index for evaluating ab-
stractive community detection. In Proceedings of
Workshop on Evaluation Metrics and System Com-
parison for Automatic Summarization, pages 10?18,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad,
Danilo Giampiccolo, and Alessandro Marchetti.
2011. Divide and conquer: crowdsourcing the cre-
ation of cross-lingual textual entailment corpora. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 670?679, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
Semeval-2012 task 8: cross-lingual textual entail-
ment for content synchronization. In Proceedings of
the First Joint Conference on Lexical and Compu-
tational Semantics - Volume 1: Proceedings of the
main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop
on Semantic Evaluation, SemEval ?12, pages 399?
407, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge Univer-
sity Press, New York, NY, USA.
Mark Sammons, V.G.Vinod Vydiswaran, and Dan
Roth. 2011. Recognizing textual entailment. In
Multilingual Natural Language Applications: From
Theory to Practice. Prentice Hall, Jun.
Benno Stein and Oliver Niggemann. 1999. On the Na-
ture of Structure and Its Identification. In Proceed-
ings of the 25th International Workshop on Graph-
Theoretic Concepts in Computer Science, WG ?99,
pages 122?134, London, UK, UK. Springer-Verlag.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Pro-
ceedings of the 18th Conference on Computational
Linguistics - Volume 2, COLING ?00, pages 947?
953. Association for Computational Linguistics.
146
Proceedings of the SIGDIAL 2013 Conference, pages 117?121,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Dialogue Act Recognition in
Synchronous and Asynchronous Conversations
Maryam Tavafi?, Yashar Mehdad?, Shafiq Joty?, Giuseppe Carenini?, Raymond Ng?
?Department of Computer Science, University of British Columbia, Vancouver, Canada
?Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
?{tavafi, mehdad, carenini, rng}@cs.ubc.ca ?sjoty@qf.org.qa
Abstract
In this work, we study the effectiveness of
state-of-the-art, sophisticated supervised
learning algorithms for dialogue act mod-
eling across a comprehensive set of differ-
ent spoken and written conversations in-
cluding: emails, forums, meetings, and
phone conversations. To this aim, we com-
pare the results of SVM-multiclass and
two structured predictors namely SVM-
hmm and CRF algorithms. Extensive em-
pirical results, across different conversa-
tional modalities, demonstrate the effec-
tiveness of our SVM-hmm model for di-
alogue act recognition in conversations.
1 Introduction
Revealing the underlying conversational struc-
ture in dialogues is important for detecting the
human social intentions in spoken conversations
and in many applications including summariza-
tion (Murray, 2010), dialogue systems and di-
alogue games (Carlson, 1983) and flirt detec-
tion (Ranganath, 2009). As an additional example,
Ravi and Kim (2007) show that dialogue acts can
be used for analyzing the interaction of students in
educational forums.
Recently, there have been increasing interests
for dialogue act (DA) recognition in spoken and
written conversations, which include meetings,
phone conversations, emails and blogs. However,
most of the previous works are specific to one of
these domains. There are potentially useful fea-
tures and algorithms for each of these domains,
but due to the underlying similarities between
these types of conversations, we aim to identify a
domain-independent DA modeling approach that
can achieve good results across all types of con-
versations. Such a domain-independent dialogue
act recognizer makes it possible to automatically
recognize dialogue acts in a wide variety of con-
versational data, as well as in conversations span-
ning multiple domains/modalities; for instance a
conversation that starts in a meeting and then con-
tinues via email.
While previous work in DA modeling has fo-
cused on studying only one (Carvalho, 2005;
Shrestha, 2004; Ravi, 2007; Ferschke, 2012; Kim,
2010a; Sun, 2012) or, in a few cases, a couple of
conversational domains (Jeong, 2009; Joty, 2011),
in this paper, we analyze the performance of su-
pervised DA modeling on a comprehensive set
of different spoken and written conversations that
includes: emails, forums, meetings, and phone
conversations. More specifically, we compare
the performance of three state-of-the-art, sophis-
ticated machine learning algorithms, which in-
clude SVM-multiclass and two structured predic-
tors SVM-hmm and Conditional Random Fields
(CRF) for DA modeling. We present an exten-
sive set of experiments studying the effectiveness
of DA modeling on different types of conversa-
tions such as emails, forums, meeting, and phone
discussions. The experimental results show that
the SVM-hmm algorithm outperforms other su-
pervised algorithms across all datasets.
2 Related Work
There have been several studies on supervised
dialogue act (DA) modeling. To the best of
our knowledge, none of them compare the per-
formance of DA recognition on different syn-
chronous (e.g., meeting and phone) and asyn-
chronous (e.g., email and forum) conversations.
Most of the works analyze DA modeling in a spe-
cific domain. Carvalho and Cohen (2005) propose
classifying emails into their dialogue acts accord-
ing to two ontologies for nouns and verbs. The
ontologies are used for determining the speech
acts of each single email with verb-noun pairs.
Shrestha and McKeown (2004) also study the
117
problem of DA modeling in email conversations
considering the two dialogue acts of question and
answer. Likewise, Ravi and Kin (2007) present
a DA recognition method for detecting questions
and answers in educational discussions. Ferschke
et al (2012) apply DA modeling to Wikipedia dis-
cussions to analyze the collaborative process of
editing Wikipedia pages. Kim et al (2010a) study
the task of supervised classification of dialogue
acts in one-to-one online chats in the shopping do-
main.
All these previous studies focus on DA recog-
nition in one or two domains, and do not sys-
tematically analyze the performance of different
dialog act modeling approaches on a compre-
hensive set of conversation domains. As far as
we know, the present work is the first that pro-
poses domain-independent supervised DA model-
ing techniques, and analyzes their effectiveness on
different modalities of conversations.
3 Dialogue Act Recognition
3.1 Conversational structure
Adjacent utterances in a conversation have a
strong correlation in terms of their dialogue acts.
As an example, if speaker 1 asks a question to
speaker 2, it is a high probability that the next ut-
terance of the conversation would be an answer
from speaker 2. Therefore, the conversational
structure is a paramount factor that should be taken
into account for automatic DA modeling. The con-
versational structure differs in spoken and written
discussions. In spoken conversations, the discus-
sion between the speakers is synchronized. The
speakers hear each other?s ideas and then state
their opinions. So the temporal order of the ut-
terances can be considered as the conversational
structure in these types of conversations. How-
ever, in written conversations such as email and
forum, authors contribute to the discussion in dif-
ferent order, and sometimes they do not pay atten-
tion to the content of previous posts. Therefore,
the temporal order of the conversation cannot be
used as the conversational structure in these do-
mains, and appropriate techniques should be used
to extract the underlying structure in these conver-
sations.
To this aim, when reply links are available in
the dataset, we use them to capture the conversa-
tion structure. To obtain a conversational structure
that is often even more refined than the reply links,
we build the Fragment Quotation Graph. To this
end, we follow the procedure proposed by Joty et
al. (2011) to extract the graph structure of a thread.
3.2 Features
In defining the feature set, we have two primary
criteria, being domain independent and effective-
ness in previous works. Lexical features such as
unigrams and bigrams have been shown to be use-
ful for the task of DA modeling in previous stud-
ies (Sun, 2012; Ferschke, 2012; Kim, 2010a; Ravi,
2007; Carvalho, 2005). In addition, unigrams have
been shown to be the most effective among the
two. So, as the lexical feature, we include the fre-
quency of unigrams in our feature set.
Moreover, length of the utterance is another
beneficial feature for DA recognition (Ferschke,
2012; Shrestha, 2004; Joty, 2011), which we add
to our feature set. The speaker of an utterance
has shown its utility for recognizing speech acts
(Sun, 2012; Kim, 2010a; Joty, 2011). Sun and
Morency (2012) specifically employ a speaker-
adaptation technique to demonstrate the effective-
ness of this feature for DA modeling. We also
include the relative position of a sentence in a
post for DA modeling since most of previous stud-
ies (Ferschke, 2012; Kim, 2010a; Joty, 2011)
prove the efficiency of this feature.
3.3 Algorithms
Since most top performing DA models use su-
pervised approaches (Carvalho, 2005; Shrestha,
2004; Ravi, 2007; Ferschke, 2012; Kim, 2010a),
to analyze the performance of DA modeling on a
comprehensive set of different spoken and written
conversations, we compare the state-of-the-art su-
pervised algorithms.
We employ three state-of-the-art, sophisticated
supervised learning algorithms:
SVM-hmm predicts labels for the examples
in a sequence (Tsochantaridis, 2004). This
approach uses the Viterbi algorithm to find the
highest scoring tag sequence for a given obser-
vation sequence. Being a Hidden Markov Model
(HMM), the model makes the Markov assump-
tion, which means that the label of a particular
example is assigned only by considering the
label of the previous example. This approach is
considered an SVM because the parameters of the
model are trained discriminatively to separate the
label of sequences by a large margin.
118
CRF is a probabilistic framework to label and
segment sequence data (Lafferty, 2001). The
main advantage of CRF over HMM is that it re-
laxes the assumption of conditional independence
of observed data. HMM is a generative model
that assigns a joint distribution over label and
observation sequences. Whereas, CRF defines the
conditional probability distribution over label se-
quences given a particular observation sequence.
SVM-multiclass is a generalization of binary
SVM to a multiclass predictor (Crammer, 2001).
The SVM-multiclass does not consider the
sequential dependency between the examples.
4 Corpora
Gathering conversational corpora for DA model-
ing is an expensive and time-consuming task. Due
to the privacy issues, there are few available con-
versational datasets.
For asynchronous conversations, we use avail-
able corpora for email and forum discussions. For
synchronous domains we employ available cor-
pora in multi-party meeting and phone conversa-
tions.
BC3 (Email): As the labeled dataset for email
conversations, we use BC3 (Ulrich, 2008), which
contains 40 threads from W3C corpus. The
BC3 corpus is annotated with twelve domain-
independent dialogue acts, which are mainly
adopted from the MRDA tagset, and it has been
used in several previous works (e.g., (Joty, 2011)).
CNET (Forum): As the labeled forum dataset,
we use the available CNET corpus, which is an-
notated with eleven domain-independent dialogue
acts in a post-level (Kim et al 2010b). This corpus
consists of 320 threads and a total of 1332 posts,
which are mostly from technical forums.
MRDA (Meeting): ICSI-MRDA dataset is
used as labeled data for meeting conversation,
which contains 75 meetings with 53 unique speak-
ers (Shriberg, 2004). The ICSI-MRDA dataset re-
quires one general tag per sentence followed by
variable number of specific tags. There are 11
general tags and 39 specific tags in the annotation
scheme. We reduce their tagset to the eleven gen-
eral tags to be consistent with the other datasets.
SWBD (Phone): In addition to multi-party
meeting conversations, we also report our experi-
mental results on Switchboard-DAMSL (SWBD),
which is a large-scale corpus containing telephone
speech (Jurafsky, 1997). This corpus is annotated
with the SWBD-DAMSL tagset, which consists of
220 tags. We use the mapping table presented by
Jeong (2009) to reduce the tagset to 16 domain-
independent dialogue acts.
All the available corpora are annotated with di-
alogue acts at the sentence-level. The only excep-
tion is the CNET forum dataset, on which we ap-
ply DA classification at the post-level.
5 Experiments and Results
5.1 Experimental settings
In our experiments, we use the SVM-hmm1 and
SVM-multiclass2 packages developed with the
SVM-light software. We use the Mallet package3
for the CRF algorithm. The results of supervised
classifications are compared to the baseline, which
is the majority class of each dataset. We apply
5-fold cross-validation for the supervised learn-
ing methods to each dataset, and compare the re-
sults of different methods using micro-averaged
and macro-averaged accuracies.
5.2 Results
Table 1 shows the results of supervised classifi-
cation on different conversation modalities. We
observe that SVM-hmm and CRF classifiers out-
perform SVM-multiclass classifier in all conversa-
tional domains. Both SVM-hmm and CRF classi-
fiers consider the sequential structure of conversa-
tions, while this is ignored in the SVM-multiclass
classifier. This shows that the sequential structure
of the conversation is beneficial independently of
the conversational modality. We can also observe
that the SVM-hmm algorithm results in the highest
performance in all datasets. As shown in (Altun,
2003), generalization performace of SVM-hmm
is superior to CRF. This superiority also applies
to the DA modeling task across all the conversa-
tional modalities. However, as it was investigated
by Keerthi and Sundararajan (2007), the discrep-
ancy in the performance of these methods may
arise from different feature functions that these
two methods use, and they might perform simi-
larly when they use the same feature functions.
Comparing the results across different datasets,
we can also note that the largest improvement
of SVM-hmm and CRF is on the SWBD, the
1http://www.cs.cornell.edu/people/tj/svm_light/svm_hmm.html
2http://svmlight.joachims.org/svm_multiclass.html
3http://mallet.cs.umass.edu
119
Corpus Baseline SVM-multiclass SVM-hmm CRFMicro Macro Micro Macro Micro Macro Micro Macro
BC3 69.56 8.34 73.57 (4.01) 8.34 (0) 77.75 (8.19) 18.20 (9.86) 72.18 (2.62) 14.9 (6.56)
CNET 36.75 9.09 34.8 (-1.95) 9.3 (0.21) 58.7 (21.95) 17.1 (8.01) 40.3 (3.55) 11.5 (2.41)
MRDA 66.47 9.09 66.47 (0) 9.09 (0) 80.5 (14.03) 32.4 (23.31) 77.8 (11.33) 22.9 (13.81)
SWBD 46.44 6.25 46.5 (0.06) 6.25 (0) 74.32 (27.88) 30.13 (23.88) 73.04 (26.6) 24.05 (17.8)
Table 1: Results of supervised DA modeling; columns are micro-averaged and macro-averaged accura-
cies with difference with baseline in parentheses.
phone conversation dataset. Moreover, super-
vised DA recognition on synchronous conversa-
tions achieves a better performance than on asyn-
chronous conversations. We can argue that this is
due to the less complex sequential structure of syn-
chronous conversations. A lower macro-averaged
accuracy in asynchronous conversations (i.e., fo-
rums and emails) can be justified in the same way.
By looking at the results in asynchronous con-
versations, we observe a larger improvement of
micro-averaged accuracy over the CNET corpus.
This might be due to two reasons: i) the DA tagsets
in both corpora are different (i.e., no overlap in
tagsets); and ii) the conversational structure in fo-
rums and emails is different.
5.3 Discussion
We analyze the strengths and weakness of super-
vised DA modeling with SVM-hmm in different
conversations individually.
BC3: SVM-hmm succeeds in classifying most
of the statement and yes-no question speech acts in
the BC3 corpus. However, it does not show a high
accuracy for classifying polite mechanisms such
as ?thanks? and ?regards?. Through the error anal-
ysis, we observed that in most of these cases the
error arose from the voting algorithm. Moreover,
the improvement of supervised DA modeling on
the BC3 corpus is smaller than the other datasets.
This may suggest that email conversation is a chal-
lenging domain for DA recognition.
CNET: The inventory of dialogue acts in the
CNET dataset can be considered as two groups of
question and answer dialogue acts, and we would
need more sophisticated features in order to clas-
sify the posts into the fine-grained dialogue acts.
The SVM-hmm succeeds in predicting the labels
of question-question and answer-answer dialogue
acts, but it performs poorly for the other labels.
The improvement of DA modeling over the base-
line is significant for this dataset. To further im-
prove the performance, a hierarchical DA classifi-
cation can be applied. In this way, the posts would
be classified into question and non-question dia-
logue acts in the first level.
MRDA: SVM-hmm performs well for pre-
dicting the classes of statement, floor holder,
backchannel, and wh-question. Floor holders and
backchannels are mostly the short utterances such
as ?ok?, ?um?, and ?so?, and we believe the length
and unigrams features are very effective for pre-
dicting these dialogue acts. On the other hand,
SVM-hmm fails in predicting the other types of
questions such as rhetorical questions and open-
ended questions by classifying them as statements.
Arguably by adding more sophisticated features
such as POS tags, SVM-hmm would perform bet-
ter for classifying these speech acts.
SWBD: The improvement of supervised DA
recognition on the SWBD is higher than the other
domains. Supervised DA classification correctly
predicts most of the classes of statement, reject re-
sponse, wh-question, and backchannel. However,
SVM-hmm cannot predict some specific dialogue
acts of phone conversations such as self-talk and
signal-non-understanding. There are a few utter-
ances in the corpus with these dialogue acts, and
most of them are classified as statements.
6 Conclusion and Future Work
We have studied the effectiveness of sophisticated
supervised learning algorithms for DA modeling
across a comprehensive set of different spoken and
written conversations. Through an extensive ex-
periment, we have shown that our proposed SVM-
hmm algorithm with the domain-independent fea-
ture set can achieve high results on different syn-
chronous and asynchronous conversations.
In future, we will incorporate other lexical and
syntactic features in our supervised framework.
We also plan to augment our feature set with
domain-specific features like prosodic features for
spoken conversations. We will also investigate the
performance of our domain-independent approach
in a semi-supervised framework.
120
References
Congkai Sun and Louise-Philippe Morency. 2012. Di-
alogue Act Recognition using Reweighted Speaker
Adaptation. 13th Annual SIGdial Meeting on Dis-
course and Dialogue.
Dan Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL labeling project
coder?s manual, draft 13. Technical report, Univ. of
Colorado Institute of Cognitive Science.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI Meet-
ing Recorder Dialog Act (MRDA) Corpus. HLT-
NAACL SIGDIAL Workshop.
Gabriel Murray, Giuseppe Carenini, and Raymond T.
Ng. 2010. Generating and validating abstracts of
meeting conversations: a user study. INLG?10.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and struc-
tured output spaces. Proceedings of the 21st Inter-
national Conference on Machine Learning (ICML).
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for
supervised email summarization. EMAIL?08 Work-
shop. AAAI.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. Intl. Conf. on Machine Learning.
Koby Crammer and Yoram Singer. 2001. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search.
Lari Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. D. Reidel.
Lokesh Shrestha and Kathleen McKeown. 2004. De-
tection of question-answer pairs in email conversa-
tions. Proceedings of the 20th Biennial Int. Conf. on
Computational Linguistics.
Minwoo Jeong, Chin-Yew Lin, and Gary G. Lee.
2009. The Semi-supervised speech act recognition
in emails and forums. Proceedings of the 2009 Conf.
Empirical Methods in Natural Language Processing.
Oliver Ferschke, Iryna Gurevych, and Yevgen Cheb-
otar. 2012. Behind the Article: Recognizing Dia-
log Acts in Wikipedia Talk Pages. Proceedings of
the 13th Conference of the European Chapter of the
ACL.
Rajesh Ranganath, Dan Jurafsky, and Dan Mcfarland.
2009. Its not you, its me: Detecting flirting and its
misperception in speed-dates. EMNLP-09.
S. S. Keerthi and S. Sundararajan. 2007. CRF versus
SVM-Struct for sequence labeling. Technical report,
Yahoo Research.
Shafiq R. Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in
asynchronous conversations. IJCAI.
Su N. Kim, Lawrence Cavedon, and Timothy Baldwin.
2010a. Classifying dialogue acts in one-on-one live
chats. EMNLP?10.
Su N. Kim, Li Wang, and Timothy Baldwin. 2010b.
Tagging and linking web forum posts. Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, CoNLL ?10.
Sujith Ravi and Jihie Kim. 2007. Profiling student
interactions in threaded discussions with speech act
classifiers. AIED?07, LA, USA.
Vitor R. Carvalho and William W. Cohen. 2005. On
the collective classification of email "speech acts".
Proceedings of the 31st Annual Int. ACM SIGIR
Conf. on Research and Development in Information
Retrieval.
Yasemin Altun and Ioannis Tsochantaridis and Thomas
Hofmann. 2003. Hidden Markov Support Vector
Machines. Proceedings of the 20th International
Conference on Machine Learning.
7 Appendix A. Frequency of Dialogue
Acts in the Corpora
Tag Dialogue Acts Email(BC3) Forum(CNET) Meeting(MRDA) Phone(SWBD)A Accept response 2.07% ? ? 6.96%AA Acknowledge and appreciate 1.24% ? ? 2.12%AC Action motivator 6.09% ? ? 0.38%P Polite mechanism 6.97% ? ? 0.12%QH Rhetorical question 0.75% ? 0.34% 0.25%QO Open-ended question 1.32% ? 0.17% 0.3%QR Or/or-clause question 1.10% ? ? 0.2%QW Wh-question 2.29% ? 1.63% 0.95%QY Yes-no question 6.75% ? 4.75% 2.62%R Reject response 1.06% ? ? 1.03%S Statement 69.56% ? 66.47% 46.44%U Uncertain response 0.79% ? ? 0.15%Z Hedge ? ? ? 11.55%B Backchannel ? ? 14.44% 26.62%D Self-talk ? ? ? 0.1%C Signal-non-understanding ? ? ? 0.14%FH Floor holder ? ? 7.96% ?FG Floor grabber ? ? 2.96% ?H Hold ? ? 0.76% ?QRR Or clause after yes-no question ? ? 0.38% ?QR Or question ? ? 0.2% ?QQ Question-question ? 27.92% ? ?QA Question-add ? 11.67% ? ?QCN Question-confirmation ? 3.89% ? ?QCC Question-correction ? 0.36% ? ?AA Answer-answer ? 36.75% ? ?AD Answer-add ? 8.84% ? ?AC Answer-confirmation ? 0.36% ? ?RP Reproduction ? 0.71% ? ?AO Answer-objection ? 1.07% ? ?RS Resolution ? 7.78% ? ?O Other ? 0.71% ? ?
Table 2: Dialogue act categories and their relative
frequency.
Table 2 indicates the dialogue acts of each cor-
pus and their relative frequencies in that dataset.
The table shows that the distribution of dialogue
acts in the datasets are not balanced. Most of the
utterances in the datasets are labeled as statements.
Consequently, during the classification step, most
of the utterances are labeled as the statement dia-
logue act. This always affects the performance of
a classifier in dealing with low frequency classes.
A possible approach to tackle this problem is to
cluster the correlative dialogue acts into the same
group and apply a DA modeling approach in a hi-
erarchical manner.
121
Proceedings of the 8th International Natural Language Generation Conference, pages 45?53,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
A Template-based Abstractive Meeting Summarization: Leveraging 
Summary and Source Text Relationships 
 
Tatsuro Oya, Yashar Mehdad, Giuseppe Carenini, Raymond Ng 
Department of Computer Science 
University of British Columbia, Vancouver, Canada 
{toya, mehdad, carenini, rng}@cs.ubc.ca 
 
 
 
Abstract 
In this paper, we present an automatic 
abstractive summarization system of 
meeting conversations. Our system ex-
tends a novel multi-sentence fusion algo-
rithm in order to generate abstract tem-
plates. It also leverages the relationship 
between summaries and their source 
meeting transcripts to select the best 
templates for generating abstractive 
summaries of meetings. Our manual and 
automatic evaluation results demonstrate 
the success of our system in achieving 
higher scores both in readability and in-
formativeness. 
1. Introduction 
People spend a vast amount of time in meetings 
and these meetings play a prominent role in their 
lives. Consequently, study of automatic meeting 
summarization has been attracting peoples? atten-
tion as it can save a great deal of their time and 
increase their productivity. 
The most common approaches to automatic 
meeting summarization have been extractive. 
Since extractive approaches do not require natu-
ral language generation techniques, they are ar-
guably simpler to apply and have been extensive-
ly investigated. However, a user study conducted 
by Murray et al. (2010) indicates that users pre-
fer abstractive summaries to extractive ones. 
Thereafter, more attention has been paid to ab-
stractive meeting summarization systems (Me-
hdad et al.  2013; Murray et al. 2010; Wang and 
Cardie 2013). However, the approaches intro-
duced in previous studies create summaries by 
either heavily relying on annotated data or by 
fusing human utterances which may contain 
grammatical mistakes. In this paper, we address 
these issues by introducing a novel summariza-
tion approach that can create readable summaries 
with less need for annotated data. Our system 
first acquires templates from human-authored 
summaries using a clustering and multi-sentence 
fusion algorithm. It then takes a meeting tran-
script to be summarized, segments the transcript 
based on topics, and extracts important phrases 
from it. Finally, our system selects templates by 
referring to the relationship between human-
authored summaries and their sources and fills 
the templates with the phrases to create summar-
ies. 
The main contributions of this paper are: 1) 
The successful adaptation of a word graph algo-
rithm to generate templates from human-
authored summaries; 2) The implementation of a 
novel template selection algorithm that effective-
ly leverages the relationship between human-
authored summary sentences and their source 
transcripts; and 3) A comprehensive testing of 
our approach, comprising both automatic and 
manual evaluations. 
 We instantiate our framework on the AMI 
corpus (Carletta et al., 2005) and compare our 
summaries with those created from a state-of-
the-art systems. The evaluation results demon-
strate that our system successfully creates in-
formative and readable summaries. 
2. Related Work 
Several studies have been conducted on creating 
automatic abstractive meeting summarization 
systems. One of them includes the system pro-
posed by Mehdad et al., (2013). Their approach 
first clusters human utterances into communities 
(Murray et al., 2012) and then builds an entail-
ment graph over each of the latter in order to se-
lect the salient utterances. It then applies a se-
mantic word graph algorithm to them and creates 
abstractive summaries. Their results show some 
improvement in creating informative summaries. 
45
However, since they create these summaries by 
merging human utterances, their summaries are 
still partially extractive.  
Recently, there have been some studies on 
creating abstract summaries of specific aspects of 
meetings such as decisions, actions and problems 
(Murray et al. 2010; Wang and Cardie, 2013). 
These summaries are called the Focused Meeting 
Summaries (Carenini et al., 2011). 
The system introduced by Murray et al. first 
classifies human utterances into specific aspects 
of meetings, e.g. decisions, problem, and action, 
and then maps them onto ontologies. It then se-
lects the most informative subsets from these on-
tologies and finally generates abstractive sum-
maries of them, utilizing a natural language gen-
eration tool, simpleNLG (Gatt and Reiter, 2009). 
Although their approach is essentially focused 
meeting summarization, after creating summaries 
of specific aspects, they aggregate them into one 
single summary covering the whole meeting. 
Wang and Cardie introduced a template-based 
focused abstractive meeting summarization sys-
tem. Their system first clusters human-authored 
summary sentences and applies a Multiple-
Sequence Alignment algorithm to them to gener-
ate templates. Then, given a meeting transcript to 
be summarized, it identifies a human utterance 
cluster describing a specific aspect and extracts 
all summary-worthy relation instances, i.e. indi-
cator-argument pairs, from it. Finally, the tem-
plates are filled with these relation instances and 
ranked accordingly, to generate summaries of a 
specific aspect of the meeting.  
Although the two approaches above are both 
successful in creating readable summaries, they 
rely on much annotated information, such as dia-
log act and sentiment types, and also require the 
accurate classification of human utterances that 
contain much noise and much ill-structured 
grammar. 
Our approach is inspired by the works intro-
duced here but improves on their shortcomings. 
Unlike those of Murray et al. (2010) and Wang 
and Cardie (2013), our system relies less on an-
notated training data and does not require a clas-
sifier. In addition, our evaluation indicates that 
our system can create summaries of the entire 
conversations that are more informative and 
readable than those of Mehdad et al.(2013). 
3. Framework 
In order for summaries to be readable and in-
formative, they should be grammatically correct 
and contain important information in meetings. 
To this end, we have created our framework con-
sisting of the following two components: 1) An 
off-line template generation module, which gen-
eralizes collected human-authored summaries 
and creates templates from them; and 2) An on-
line summary generation module, which seg-
ments meeting transcripts based on the topics 
discussed, extracts the important phrases from 
these segments, and generate abstractive sum-
maries of them by filling the phrases into the ap-
propriate templates. Figure 1 depicts our frame-
work. In the following sections, we describe each 
of the two components in detail. 
 
Figure 1: Our meeting summarization framework. Top: off-line Template generation module. Bottom: on-line 
Summary Generation module. 
46
3.1 Template Generation Module 
Our template generation module attempts to sat-
isfy two possibly conflicting objectives. First, 
templates should be quite specific such that they 
accept only the relevant fillers. Second, our 
module should generate generalized templates 
that can be used in many situations. We assume 
that the former is achieved by labeling phrases 
with their hypernyms that are not too general and 
the latter by merging related templates. Based on 
these assumptions, we divide our module into the 
three tasks: 1) Hypernym labeling; 2) Clustering; 
and 3) Template fusion. 
3.1.1 Hypernym Labeling 
Templates are derived from human-authored 
meeting summaries in the training data. We first 
collect sentences whose subjects are meeting par-
ticipant(s) and that contain active root verbs, 
from the summaries. This is achieved by utilizing 
meeting participant information provided in the 
corpus and parsing sentences with the Stanford 
Parser (Marneffe et al., 2006). The motivation 
behind this process is to collect sentences that are 
syntactically similar. We then identify all noun 
phrases in these sentences using the Illinois 
Chunker (Punyakanok and Roth, 2001). This 
chunker extracts all noun phrases as well as part 
of speech (POS) for all words. To add further in-
formation on each noun phrase, we label the right 
most nouns (the head nouns) in each phrase with 
their hypernyms using WordNet (Fellbaum, 
1998). In WordNet, hypernyms are organized in-
to hierarchies ranging from the most abstract to 
the most specific. For our work, we utilize the 
fourth most abstract hypernyms in light of the 
first goal discussed at the beginning of Section 
3.1, i.e. not too general. For disambiguating the 
sense of the nouns, we simply select the sense 
that has the highest frequency in WordNet.  
At this stage, all noun phrases in sentences 
are tagged with their hypernyms defined in 
WordNet, such as ?artifact.n.01?, and ?act.n.02?, 
where n?s stands for nouns and the two digit 
numbers represent their sense numbers. We treat 
these hypernym-labeled sentences as templates 
and the phrases as blanks. 
In addition, we also create two additional 
rules for tagging noun phrases: 1) Since the sub-
jects of all collected sentences are meeting par-
ticipant(s), we label all subject noun phrases as 
?speaker?; and 2) If the noun phrases consist of 
meeting specific terms such as ?the meeting? or 
?the group?, we do not convert them into blanks. 
These two rules guarantee the creation of tem-
plates suitable for meetings. 
 
Figure 2: Some examples of hypernym labeling task 
3.1.2 Clustering 
Next, we cluster the templates into similar 
groups. We utilize root verb information for this 
process assuming that these verbs such as ?dis-
cuss? and ?suggest? that appear in summaries are 
the most informative factors in describing meet-
ings. Therefore, after extracting root verbs in 
summary sentences, we create fully connected 
graphs where each node represents the root verbs 
and each edge represents a score denoting how 
similar the two word senses are. To measure the 
similarity of two verbs, we first identify the verb 
senses based on their frequency in WordNet and 
compute the similarity score based on the short-
est path that connects the senses in the hypernym 
taxonomy. We then convert the graph into a 
similarity matrix and apply a Normalized Cuts 
method (Shi and Malik, 2000) to cluster the root 
verbs. Finally, all templates are organized into 
the groups created by their root verbs. 
 
Figure 3: A word graph generated from related templates and the highest scored path (shown in bold) 
47
3.1.3 Template Fusion 
We further generalize the clustered templates by 
applying a word graph algorithm. The algorithm 
was originally proven to be effective in summa-
rizing a cluster of related sentences (Boudin and 
Morin, 2013; Filippova, 2010; Mehdad et al., 
2013). We extend it so that it can be applied to 
templates. 
Word Graph Construction  
In our system, a word graph is a directed graph 
with words or blanks serving as nodes and edges 
representing adjacency relations.  
Given a set of related templates in a group, 
the graph is constructed by first creating a start 
and end node, and then iteratively adding tem-
plates to it. When adding a new template, the al-
gorithm first checks each word in the template to 
see if it can be mapped onto existing nodes in the 
graph. The word is mapped onto a node if the 
node consists of the same word and the same 
POS tag, and no word from this template has 
been mapped onto this node yet. Then, it checks 
each blank in the template and maps it onto a 
node if the node consists of the same hypernym-
labeled blank and no blank from this template 
has been mapped onto this node yet.  
When more than one node refer to the same 
word or blank in the template, or when more than 
one word or blank in the template can be mapped 
to the same node in the graph, the algorithm 
checks the neighboring nodes in the current 
graph as well as the preceding and the subse-
quent words or blanks in the template. Then, 
those word-node or blank-node pairs with higher 
overlap in the context are selected for mapping. 
Otherwise, a new node is created and added to 
the graph. As a simplified illustration, we show a 
word graph in Figure 3 obtained from the follow-
ing four templates. 
  
? After introducing [situation.n.01], [speaker] then dis-
cussed [content.n.05] . 
? Before beginning [act.n.02] of [artifact.n.01], [speaker] 
discussed [act.n.02] and [content.n.05] for [arti-
fact.n.01] . 
? [speaker] discussed [content.n.05] of [artifact.n.01] and 
[material.n.01] . 
? [speaker] discussed [act.n.02] and [asset.n.01] in attract-
ing [living_thing.n.01] . 
Path Selection  
The word graph generates many paths connect-
ing its start and end nodes, not all of which are 
readable and cannot be used as templates. Our 
aim is to create concise and generalized tem-
plates. Therefore, we create the following rank-
ing strategy to be able to select the ideal paths. 
First, to filter ungrammatical or complex tem-
plates, the algorithm prunes away the paths hav-
ing more than three blanks; having subordinate 
clauses; containing no verb; having two consecu-
tive blanks; containing blanks which are not la-
beled by any hypernym; or whose length are 
shorter than three words. Note that these rules, 
which were defined based on close observation 
of the results obtained from our development set, 
greatly reduce the chance of selecting ill-
structured templates. Second, the remaining 
paths are reranked by 1) A normalized path 
weight and 2) A language model learned from 
hypernym-labeled human-authored summaries in 
our training data, each of which is described be-
low. 
1) Normalized Path Weight 
We adapt Filippova (2010)?s approach to com-
pute the edge weight. The formula is shown as: 
         
                  
?                 
                
    
where ei,j  is an edge that connects the nodes i 
and j in a graph, freq(i) is the number of words 
and blanks in the templates that are mapped to 
node i and diff(p,i,j) is the distance between the 
offset positions of nodes i and j in path p. This 
weight is defined so that the paths that are in-
formative and that contain salient (frequent) 
words are selected. To calculate a path score, 
W(p), all the edge weights on the path are 
summed and normalized by its length. 
2) Language Model 
Although the goal is to create concise templates, 
these templates must be grammatically correct. 
Hence, we train an n-gram language model using 
all templates generated from the training data in 
the hypernym labeling stage. Then for each path, 
we compute a sum of negative log probabilities 
of n-gram occurrences and normalize the score 
by its length, which is represented as H(p).  
The final score of each path is calculated as 
follows: 
                                   
where ? and ? are the coefficient factors which 
are tuned using our development set. For each 
48
group of clusters, the top ten best scored paths 
are selected as templates and added to its group. 
As an illustration, the path shown in bold in 
Figure 3 is the highest scored path obtained from 
this path ranking strategy.  
3.2 Summary Generation Module 
This section explains our summary generation 
module consisting of four tasks: 1) Topic seg-
mentation; 2) Phrase and speaker extraction; 3) 
Template selection and filling; and 4) Sentence 
ranking.  
3.2.1 Topic Segmentation 
It is important for a summary to cover all topics 
discussed in the meeting. Therefore, given a 
meeting transcript to be summarized, after re-
moving speech disfluencies such as ?uh?, and 
?ah?, we employ a topic segmenter, LCSeg (Gal-
ley et al., 2003) which create topic segments by 
observing word repetitions.  
One shortcoming of LCSeg is that it ignores 
speaker information when segmenting transcripts. 
Important topics are often discussed by one or 
two speakers. Therefore, in order to take ad-
vantage of the speaker information, we extend 
LCSeg by adding the following post-process 
step: If a topic segment contains more than 25 ut-
terances, we subdivide the segment based on the 
speakers. These subsegments are then compared 
with one another using cosine similarity, and if 
the similarity score is greater than that of the 
threshold (0.05), they are merged. The two num-
bers, i.e. 25 and 0.05, were selected based on the 
development set so that, when segmenting a tran-
script, the system can effectively take into ac-
count speaker information without creating too 
many segments. 
3.2.2 Phrase And Speaker Extraction 
All salient phrases are then extracted from each 
topic segment in the same manner as performed 
in the template generation module in Section 3.1, 
by: 1) Extracting all noun phrases; and 2) Label-
ing each phrase with the hypernym of its head 
noun. Furthermore, to be able to select salient 
phrases, these phrases are subsequently scored 
and ranked based on the sum of the frequency of 
each word in the segment. Finally, to handle re-
dundancy, we remove phrases that are subsets of 
others. 
In addition, for each utterance in the meeting, 
the transcript contains its speaker?s name. There-
fore, we extract the most dominant speakers? 
name(s) for each topic segment and label them as 
?speaker?. These phrases and this speaker infor-
mation will later be used in the template filling 
process. Table 1 below shows an example of 
dominant speakers and high scored phrases ex-
tracted from a topic segment. 
Dominant speakers 
Project Manager (speaker) 
Industrial Designer (speaker) 
High scored phrases (hypernyms) 
the whole look (appearance.n.01) 
the company logo (symbol.n.01) 
the product (artifact.n.01) 
the outside (region.n.01) 
electronics (content.n.05) 
the fashion (manner.n.01) 
Table 1: Dominant speakers and high scored 
phrases extracted from a topic segment 
3.2.3 Template Selection and Filling 
In terms of our training data, all human-authored 
abstractive summary sentences have links to the 
subsets of their source transcripts which support 
and convey the information in the abstractive 
sentences as illustrated in Figure 4. These subsets 
are called communities. Since each community is 
used to create one summary sentence, we hy-
pothesize that each community covers one spe-
cific topic.  
Thus, to find the best templates for each topic 
segment, we refer to our training data. In particu-
lar, we first find communities in the training set 
that are similar to the topic segment and identify 
the templates derived from the summary sen-
tences linked to these communities.   
 
Figure 4: A link from an abstractive summary sentence to a subset of a meeting transcript that conveys or sup-
ports the information in the abstractive sentence 
49
This process is done in two steps, by: 1) As-
sociating the communities in the training data 
with the groups containing templates that were 
created in our template generation module; and 
2) Finding templates for each topic segment by 
comparing the similarities between the segments 
and all sets of communities associated with the 
template groups. Below, we describe the two 
steps in detail. 
1) Recall that in the template generation 
module in Section 3.1, we label human-authored 
summary sentences in training data with hyper-
nyms and cluster them into similar groups. Thus, 
as shown in Figure 5, we first associate all sets of 
communities in the training data into these 
groups by determining to which groups the 
summary sentences linked by these communities 
belong. 
 
Figure 5: An example demonstrating how each com-
munity in training data is associated with a group con-
taining templates  
2) Next, for each topic segment, we compute 
average cosine similarity between the segment 
and all communities in all of the groups.  
 
Figure 6: Computing the average cosine similarities 
between a topic segment and all sets of com munities 
in each group 
At this stage, each community is already as-
sociated with a group that contains ranked tem-
plates. In addition, each segment has a list of av-
erage-scores that measures how similar the seg-
ment is to the communities in each group. Hence, 
the templates used for each segment are decided 
by selecting the ones from the groups with higher 
scores.  
Our system now contains for each segment a 
set of phrases and ideal templates, both of which 
are scored, as well as the most dominant speakers? 
name(s). Thus, candidate sentences are generated 
for each segment by: first, selecting speakers? 
name(s), then selecting phrases and templates 
based on their scores; and finally filling the tem-
plates with matching labels. Here, we limit the 
maximum number of sentences created for each 
topic segment to 30. This number is defined so 
that the system can avoid generating sentences 
consisting of low scored phrases and templates. 
Finally, these candidate sentences are passed to 
our sentence ranking module. 
3.2.4 Sentence Ranking 
Our system will create many candidate sentenc-
es, and most of them will be redundant. Hence, 
to be able to select the most fluent, informative 
and appropriate sentences, we create a sentence 
ranking model considering 1) Fluency, 2) Cover-
age, and 3) The characteristics of the meeting, 
each of which are summarized below: 
1) Fluency 
We estimate the fluency of the generated sen-
tences in the same manner as in Section 3.1.3. 
That is, we train a language model on human-
authored abstract summaries from the training 
portions of meeting data and then compute a 
normalized sum of negative log probabilities of 
n-gram occurrences in the sentence. The fluency 
score is represented as H(s) in the equation be-
low. 
2) Coverage 
To select sentences that cover important topics, 
we give special rewards to the sentences that 
contain the top five ranked phrases.   
3) The Characteristics of the Meeting 
We also add three additional scoring rules that 
are specific to the meeting summaries. In particu-
lar, these three rules are created based on phrases 
often used in the opening and closing of meet-
ings in a development set: 1) If sentences derived 
50
from the first segment contain the words ?open? 
or ?meeting?, they will be rewarded; 2) If sen-
tences derived from the last segment contain the 
words ?close? or ?meeting?, the sentences will 
again be rewarded; and 3)  If sentences not de-
rived  from the first or last segment contains the 
words ?open? or ?close?,  they will be penalized. 
The final ranking score of the candidate sen-
tences is computed using the follow formula: 
      s  ?  s  ? ?
i
 
i 1  i s  ?  i
 
i 1  i s     
where, Ri (s) is a binary that indicates whether 
the top i ranked phrase exists in sentence s; Mi (s) 
is also a binary that indicates whether the i th 
meeting specific rule can be met for sentence s; 
and ?, ? i and ? i are the coefficient factors to tune 
the ranking score, all of which are tuned using 
our development set. 
Finally, the sentence ranked the highest in 
each segment is selected as the summary sen-
tence, and the entire meeting summary is created 
by collecting these sentences and sorting them by 
the chronological order of the topic segments. 
4. Evaluation 
In this section, we describe an evaluation of our 
system. First, we describe the corpus data. Next, 
the results of the automatic and manual evalua-
tions of our system against various baseline ap-
proaches are discussed. 
4.1 Data 
For our meeting summarization experiments, we 
use manually transcripted meeting records and 
their human-authored summaries in the AMI 
corpus. The corpus contains 139 meeting records 
in which groups of four people play different 
roles in a fictitious team. We reserved 20 meet-
ings for development and implemented a three-
fold cross-validation using the remaining data.   
4.2 Automatic Evaluation 
We report the F1-measure of ROUGE-1, 
ROUGE-2 and ROUGE-SU4 (Lin and Hovy, 
2003) to assess the performance of our system. 
The scores of automatically generated summaries 
are calculated by comparing them with human-
authored ones.  
For our baselines, we use the system intro-
duced by Mehdad et al. (2013) (FUSION), which 
creates abstractive summaries from extracted 
sentences and was proven to be effective in cre-
ating abstractive meeting summaries; and Tex-
tRank (Mihalcea and Tarau, 2004), a graph based 
sentence ranker that is suitable for creating ex-
tractive summaries. Our system can create sum-
maries of any length by adjusting the number of 
segments to be created by LCSeg. Thus, we cre-
ate summaries of three different lengths (10, 15, 
and 20 topic segments) with the average number 
of words being 100, 137, and 173, respectively. 
These numbers generally corresponds to human-
authored summary length in the corpus which 
varies from 82 to 200 words.  
Table 2 shows the results of our system in 
comparison with those of the two baselines. The 
results show that our model significantly outper-
forms the two baselines. Compared with FU-
SION, our system with 20 segments achieves 
about 3 % of improvement in all ROUGE scores. 
This indicates that our system creates summaries 
that are more lexically similar to human-authored 
ones. Surprisingly, there was not a significant 
change in our ROUGE scores over the three dif-
ferent summary lengths. This indicates that our 
system can create summaries of any length with-
out losing its content. 
Models Rouge-1 Rouge-2 Rouge-SU4 
TextRank 21.7 2.5 6.5 
FUSION 27.9 4.0 8.1 
Our System 10 Seg. 28.4 6.7 10.1 
Our System 15 Seg. 30.6 6.8 10.9 
Our System 20 Seg. 31.5 6.7 11.4 
Table 2: An evaluation of summarization performance 
using the F1 measure of ROUGE-1 2, and SU4 
4.3 Manual Evaluation 
We also conduct manual evaluations utilizing a 
crowdsourcing tool1. In this experiment, our sys-
tem with 15 segments is compared with FUSION, 
human-authored summaries (ABS) and, human-
annotated extractive summaries (EXT).  
After randomly selecting 10 meetings, 10 par-
ticipants were selected for each meeting and giv-
en instructions to browse the transcription of the 
meeting so as to understand its gist. They were 
then asked to read all different types of summar-
ies described above and rate each of them on a 1-
5 scale for the following three items: 1) The 
summary?s overall quality, with ?5? being the 
best and ?1? being the worst possible quality; 2) 
The summary?s fluency, ignoring the capitaliza-
tion or punctuation, with ?5? indicating no 
grammatical mistakes and ?1? indicating too 
many; and 3) The summary?s informativeness, 
with ?5? indicating that the summary covers all 
meeting content and ?1? indicating that the 
                                                 
1 http://www.crowdflower.com/ 
51
summary does not cover the content at all.  
The results are described in Table 3. Overall, 
58 people worldwide, who are among the most 
reliable contributors accounting for 7 % of over-
all members and who maintain the highest levels 
of accuracy on test questions provided in pervi-
ous crowd sourcing jobs, participated in this rat-
ing task.  As to statistical significance, we use the 
2-tail pairwise t-test to compare our system with 
the other three approaches. The results are sum-
marized in Table 4. 
Models Quality Fluency Informativeness 
Our System  3.52 3.69 3.54 
ABS 3.96 4.03 3.87 
EXT 3.02 3.16 3.30 
FUSION 3.16 3.14 3.05 
Table 3: Average rating scores. 
Models  
Compared 
Quality 
(P-value) 
Fluency 
(P-value) 
Informativeness 
(P-value) 
Our System 
vs. ABS 
0.000162 0.000437 0.00211 
Our System 
vs. FUSION 
0.00142 0.0000135 0.000151 
Our System 
vs. EXT. 
0.000124 0.0000509 0.0621 
Table 4: T-test results of manual evaluation 
As expected, for all of the three items, ABS 
received the highest of all ratings, while our sys-
tem received the second highest. The t-test re-
sults indicate that the difference in the rating data 
is statistically significant for all cases except that 
of informativeness between ours and the extrac-
tive summaries. This can be understood because 
the extractive summaries were manually created 
by an annotator and contain all of the important 
information in the meetings. 
From this observation, we can conclude that 
users prefer our template-based summaries over 
human-annotated extractive summaries and ab-
stractive summaries created from extracted sali-
ent sentences. Furthermore, it demonstrates that 
our summaries are as informative as human-
annotated extractive ones. 
Finally, we show in Figure 7 one of the sum-
maries created by our system in line-with a hu-
man-authored one.  
5. Conclusion and Future Work 
In this paper, we have demonstrated a robust ab-
stractive meeting summarization system. Our ap-
proach makes three main contributions. First, we 
have proposed a novel approach for generating 
templates leveraging a multi-sentence fusion al-
gorithm and lexico-semantic information. Sec-
ond, we have introduced an effective template 
selection method, which utilize the relationship 
between human-authored summaries and their 
source transcripts. Finally, comprehensive evalu-
ation demonstrated that summaries created by 
our system are preferred over human-annotated 
extractive ones as well as those created from a 
state-of-the-art meeting summarization system.  
The current version of our system uses only 
hypernym information in WordNet to label 
phrases. Considering limited coverage in Word-
Net, future work includes extending our frame-
work by applying a more sophisticated labeling 
task utilizing a richer knowledge base (e.g., YA-
GO). Also, we plan to apply our framework to 
different multi-party conversational domains 
such as chat logs and forum discussions.  
Human-Authored Summary 
The project manager opened the meeting and had the 
team members introduce themselves and describe their 
roles in the upcoming project. The project manager then 
described the upcoming project. The team then discussed 
their experiences with remote controls. They also 
discussed the project budget and which features they 
would like to see in the remote control they are to create. 
The team discussed universal usage, how to find remotes 
when misplaced, shapes and colors, ball shaped remotes, 
marketing strategies, keyboards on remotes, and remote 
sizes. team then discussed various features to consider in 
making the remote. 
 
Summary Created by Our System with 15 Segment 
project manager summarized their role of the meeting .  
user interface expert and project manager talks about a 
universal remote . the group recommended using the 
International Remote Control Association rather than a 
remote control . project manager offered the ball 
idea .user interface expert suggested few buttons . user 
interface expert and industrial designer then asked a 
member about a nice idea for The idea . project manager 
went over a weak point . the group announced the one-
handed design . project manager and industrial designer 
went over their remote control idea . project manager 
instructed a member to research the ball function .  
industrial designer went over stability point .industrial 
designer went over definite points . 
Figure 7: A comparison between a human-authored 
summary and a summary created by our system 
Acknowledgements 
We would like to thank all members in UBC 
NLP group for their comments and UBC LCI 
group and ICICS for financial support. 
 References 
Florian Boudin and Emmanuel Morin. 2013. 
Keyphrase Extraction for N-best Reranking in Mul-
ti-Sentence Compression. In  Proceedings of the 
52
2013 Conference of the North American Chapter of 
the Association for Computational Linguistics: 
Human Language Technologies (NAACL-HLT 
2013), 2013. 
Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 
2011. Methods for Mining and Summarizing Text 
Conversations. Morgan Claypool. 
J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guil-
lemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, 
M. Kronenthal, G. Lathoud, M. Lincoln, A. 
Lisowska, I. McCowan, W. Post, D. Reidsma, and 
P. Wellner. 2005. The AMI meeting corpus: A pre-
announcement. In Proceeding of MLMI 2005, Ed-
inburgh, UK, pages 28?39. 
Christiane Fellbaum 1998. WordNet, An Electronic 
Lexical Database. The MIT Press. Cambridge, MA. 
Katja Filippova. 2010. Multi-sentence compression: 
finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 322?
330, Stroudsburg, PA, USA. Association for Com-
putational Linguistic 
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of the 41st Annual Meeting of the Association 
for Computational Linguistics (ACL 2003). 562?
569. Sapporo, Japan. 
Albert Gatt and Ehud Reiter. 2009. SimpleNLG: a 
Realisation Engine for Practical Applications. In 
ENLG?09: Proceedings of the 12th European 
Workshop on Natural Language Generation, pages 
90?93, Morristown, NJ, USA. Association for 
Computational Linguistics. 
Ravi Kondadadi, Blake Howald and Frank Schilder. 
2013. A Statistical NLG Framework for Aggregat-
ed Planning and Realization. In Proceeding of the 
Annual Conferene for the Association of Computa-
tional Linguistic (ACL 2013).  
Marie-Catherine de Marneffe, Bill MacCartney and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
In Proceedings of the Fifth International Confer-
ence on Language Resources and Evaluation 
(LREC'06). 
Yashar Mehdad, Giuseppe Carenini, Frank Tompa. 
2013. Abstractive Meeting Summarization with En-
tailment and Fusion. In Proceedings of the 14th Eu-
ropean Natural Language Generation (ENLG - 
SIGGEN 2013), Sofia, Bulgaria. 
Rata Mihalcea and Paul Tarau 2004. TextRank: 
Bringing order into texts. In Proceedings of the 
2004 Conference on Empirical Methods in Natural 
Language Processing, July. 
Gabriel Murray, Giuseppe Carenini, and Raymond T. 
Ng. 2010. Generating and validating abstracts of 
meeting conversations: a user study. In INLG 2010. 
Gabriel Murray, Giuseppe Carenini and Raymond Ng. 
2012. Using the Omega Index for Evaluating Ab-
stractive Community Detection, NAACL 2012, 
Workshop on Evaluation Metrics and System Com-
parison for Automatic Summarization, Montreal, 
Canada. 
Vasin Punyakanok and Dan Roth. 2001. The Use of 
Classifiers in Sequential Inference. NIPS (2001) pp. 
995-1001. 
Jianbo Shi and Jitendra Malik. 2000. Normalized Cuts 
& Image Segmentation. IEEE Trans. of PAMI, Aug 
2000. 
David C. Uthus and David W. Aha. 2011. Plans to-
ward automated chat summarization. In Proceed-
ings of the Workshop on Automatic Summarization 
for Different Genres, Media, and Languages, 
WASDGML?11, pages 1-7, Stroudsburg, PA, USA. 
Association for Computational Linguistics. 
Lu Wang and Claire Cardie. 2013. Domain-
Independent Abstract Generation for Focused 
Meeting Summarization.  In ACL 2013. 
Liang Zhou and Eduard Hovy. 2005. Digesting virtual 
?geek? culture: The summarization of technical in-
ternet relay chats. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational 
Linguistics (ACL?05), pages 298-305, Ann Arbor, 
Michigan, June. Association for Computational 
Linguistics. 
53
