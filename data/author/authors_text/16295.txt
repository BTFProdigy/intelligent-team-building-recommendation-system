Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 138?142,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
A Probabilistic Approach to Persian Ezafe Recognition 
 
Habibollah Asghari 
Department of ECE, 
University of Tehran,  
Tehran, Iran 
habib.asghari@ut.ac.ir 
Jalal Maleki 
Department of CIS, 
Link?pings Universitet 
SE-581 83 Link?ping, Sweden 
jalal.maleki@liu.se 
Heshaam Faili 
Department of ECE, 
University of Tehran,  
Tehran, Iran 
hfaili@ut.ac.ir 
 
  
 
Abstract 
In this paper, we investigate the problem of 
Ezafe recognition in Persian language. Ezafe is 
an unstressed vowel that is usually not written, 
but is intelligently recognized and pronounced 
by human. Ezafe marker can be placed into 
noun phrases, adjective phrases and some 
prepositional phrases linking the head and 
modifiers. Ezafe recognition in Persian is 
indeed a homograph disambiguation problem, 
which is a useful task for some language 
applications in Persian like TTS. In this paper, 
Part of Speech tags augmented by Ezafe 
marker (POSE) have been used to train a 
probabilistic model for Ezafe recognition. In 
order to build this model, a ten million word 
tagged corpus was used for training the 
system. For building the probabilistic model, 
three different approaches were used; 
Maximum Entropy POSE tagger, Conditional 
Random Fields (CRF) POSE tagger and also a 
statistical machine translation approach based 
on parallel corpus. It is shown that comparing 
to previous works, the use of CRF POSE 
tagger can achieve outstanding results.  
1 Introduction 
In Persian language, Ezafe is an unstressed 
short vowel /-e/ (or /-ye/ after vowels) which is 
used to link two words in some contexts. 
Although Ezafe is an important part of the 
Persian phonology and morphology, it does not 
have a specific character representation, and so is 
not usually written. However, it is pronounced as 
a short vowel /e/. Sometimes, for disambiguation 
purposes it is preferred to explicitly mark its 
presence by a written symbol (the diacritic Kasre) 
after some words in order to facilitate the correct 
pronunciation. 
The most important application of Ezafe 
recognition is a text to phoneme tool for Text To 
Speech (TTS) Systems. Other application of 
Ezafe recognition is identifying the dependency 
of a word in a Noun Phrase. (Oskouipour, 2011, 
Mavvaji and Eslami, 2012) 
In this research, we would like to investigate 
various approaches to correctly recognize 
genitive cases in Persian language. Shortly, the 
contributions of this paper are as follow: 
? Modeling the Ezafe recognition task as a 
sequence labeling system. 
? Using HMM and CRF as sequence labelers. 
? Modeling the Ezafe recognition task as a 
monotone translation problem which can be 
tackled by phrase based SMT approach. 
? Using a big amount of test and gold data, so 
the results are considerably reliable. 
? To enhance the results of the system, five 
Persian-specific features which discriminate 
the results in high-precision low-recall fashion, 
have been proposed. 
? The recognition rate has achieved outstanding 
results in comparison to the previous works.  
This task is closely related to the task of 
determining short vowels in Arabic language. So, 
although the aim of this paper is to recognize 
Ezafe in Persian language, but all the methods 
investigated here is applicable to determine short 
vowels in Arabic language. 
In the next section a clear definition of the 
problem is presented and the characteristics of 
Persian language are introduced. In Section 3 we 
will give a precise definition of Ezafe. Section 4 
provides an overview of previous works on 
Ezafe recognition. Our approach will be 
described in Section 5 followed by two sections 
including corpus selection process and 
implementation of proposed method. Conclusion 
and recommendations for future works will be 
presented in the last section. 
138
2 An Overview of Persian Language 
Persian Language belongs to Arabic script-
based languages. This category of languages 
includes Kurdish, Urdu, Arabic, Pashtu and 
Persian. They all have common scripting, and 
somehow similar writing system. 
In Arabic script-based languages, the most 
common features are absence of capitalization, 
right to left direction, lack of clear word 
boundaries, complex word structure, encoding 
issues in computer environment, and a high 
degree of ambiguity due to non-representation of 
short vowels in writing (Farghaly, 2004). Note 
that Ezafe recognition and homograph 
disambiguation problem mostly deals with the 
last mentioned feature. 
One of the problems in Persian language 
processing is long-distance dependencies. This 
phenomenon complicates Ezafe recognition task 
even for humans (Ghomeshi, 1996). Another 
problem is how to determine phrase/word 
boundaries. In Persian language, affixes can be 
written in three formats; completely separated by 
a space delimiter, separated by half-space1 , or 
can be attached to its main word. So, determining 
word and phrase boundaries are somehow a 
complicated task in Persian.The third challenge 
arises by pronoun drop due to the morphology of 
Persian language.  
3 Ezafe Definition 
Historically, Persian Ezafe had a 
demonstrative morpheme in old Iran (Estaji and 
Jahangiri, 2006). It was related to a demonstrative 
/hya/, which links the head noun to adjectival 
modifiers, to the possessor NP (Samvelian, P., 
2007). In evolution of Persian language, /hya/ 
became /?i/ in Middle Persian and progressively 
lost its demonstrative value to end as a simple 
linker. In recognizing Ezafe, we should consider 
all morphological, syntactic, semantic and 
discourse views (Parsafar, 2010). It should be 
noted that Ezafe can be iterated within the NP, 
occurring as many times as there are modifiers.  
4 Previous Works  
As a first attempt to recognize Ezafe in Persian 
text, Bijankhan (Bijankhan, 2005) used a pattern 
matching algorithm for Ezafe recognition. He 
has used POS tags and also semantic labels (such 
as place, time, ordinal numbers ...) to obtain a 
                                                          
1A Non-Joint Zero Width (NJZW) letter 
statistical view of Ezafe markers. He manually 
derived 80 most frequent patterns such as Noun-
Noun and Noun-Adjective etc. The most 
frequent combinations were extracted based on a 
10 million-wordscorpus.  
In a research accomplished by (Isapour, et al., 
2007), the researchers rely on the fact that Ezafe 
can relate between head and its modifiers so as to 
help to build NPs. So by parsing sentences and 
finding Phrase borders, the location of Ezafe in 
the sentence can be found. In this work, the 
sentences were analyzed using a Probabilistic 
Context Free Grammar (PCFG) to derive phrase 
borders. Then based on the extracted parse tree, 
the head and modifiers in each phrase can be 
determined. In the last phase, a rule based 
approach was also applied to increase the 
accuracy in Ezafe marker labeling. For training 
the algorithm, 1000 sentences were selected and 
a parse tree was built for each of them. Because 
of the limited number of parsed sentences for 
training, the results cannot be extended for 
general applications. 
There were also other attempts to effectively 
recognize Ezafe marker in Persian text, such as 
(Zahedi, 1998) based on fuzzy sets. Also, 
(Oskouipour, 2011) developed a system based on 
Hidden Markove Model to correctly identify 
Ezafe markers. (Mavvaji and Eslami, 2012) had 
another attempt by syntactic analysis. There are 
also some implementations using neural 
networks (Razi and Eshqi, 2012). Some of the 
results can be seen in Table 4. 
5 Our Approach 
In this paper we have investigated two types 
of POS taggers, and also a MT-based approach. 
In the following section, these approaches will be 
explained and the results will be compared to 
previous work. 
A. Ezafe recognition as a POS tagging problem 
Part Of Speech tagging is an effective way for 
automatically assigning grammatical tags to 
words in a text. In Persian, POS tagging can be 
applied as a homograph disambiguation problem 
for correct pronunciation of words in a sentence 
(Yarowsky, 1996). There are powerful POS 
tagger algorithms such as statistical, rule based, 
transformation based and memory based learning 
methods. In this research we have used two 
schemes of statistical POS tagging for Ezafe 
recognition. The first one is a Maximum Entropy 
tagger that has been investigated by (Toutanova 
and Manning. 2000) and (Toutanova, et al. 
139
2003). In order to implement this approach, we 
have used Stanford toolkit as a MaxEnt tagger. 
The second approach is based on Conditional 
Random Fields (CRF) model, that was first 
introduced by (Lafferty, et al., 2001) and then 
(Sha and Pereira. 2003).  
B. Ezafe recognition as a translation problem 
We can consider the Ezafe recognition 
problem as a monotone translation problem. In 
other words, it can be considered as a noisy 
channel problem. The original training text 
without the Ezafe marker can be used as source 
language, and the tagged text can be used as 
destination language. So, we can apply these 
parallel corpora as inputs to a phrase-based 
Statistical Machine Translation (SMT) system.  
In the experiments, we have used monotone 
SMT with distortion limit equal to zero. For 
implementing SMT, we have used Moses toolkit. 
It should be mentioned that in the case of Ezafe 
recognition, we can use a SMT system without 
re-ordering. By using phrase-based SMT, the 
local dependencies between the neighboring 
words are handled by the phrase table. Also some 
of the dependencies between different phrases 
can be tackled by the language model. 
6 Data Preparation 
In this work, we have used Bijankhan corpus 
(Bijankhan, 2004, Amiri, et al, 2007). The 
content of this corpus is gathered from daily news 
and common texts, covering 4300 different 
subjects. It contains about 10 million tagged 
words in about 370000 sentences. The words in 
the corpus have been tagged by 550 tags based on 
a hierarchical order, with more fine-grained POS 
tags like ?noun-plural-subj?. About 23% of words 
in the corpus are tagged with Ezafe. We have 
used an extended version of POS tags, named 
POSE (Part of Speech tags + Ezafe tag) that can 
be constructed by adding Ezafe markers to 
original first level tags. Table 1 shows the 
statistics of POSE tags. 
 
POSE Frequency %  in Ezafe 
markers 
%  in 
all corpus 
N-e 1817472 81.87 18.39 
ADJ-e 223003 10.05 2.26 
P-e 111127 5.01 1.125 
NUM-e 27860 1.26 0.28 
others 40477 1.81 0.41 
Total 2219939 100 % 22.46 
 
Table 1 - Ezafe Statistics in Bijankhan Corpus 
7 Performance Metrics 
The ordinary measures that can be used based 
on confusion matrix are Precision, Recall and F1 
measure. Another measure that can be used in 
this binary classification problem is Mathews 
Correlation Coefficient (MCC). This measure 
indicates the quality of the classifier for binary 
class problems especially when two classes are of 
very different sizes.We have also considered two 
other measures; true positive rate as Ezafe 
presence accuracy, and false positive rate as 
Ezafe absence accuracy. The total average can be 
calculated using a weighted average of the two 
last mentioned measures.  
8 Experiments and Results 
As mentioned, the system was trained on 
Bijankhan corpus. Only the first level of POS 
tags was used for the training phase, except for 
the words with Ezafe, that the POS plus Ezafe 
marker was chosen. The more fine-grained POS 
tags were removed to achieve more accuracy. 
We used a ten-fold cross-validation scheme. 
For calculating the total accuracy, Ezafe presence 
accuracy and Ezafe absence accuracy should be 
weighted by 16.8% (ratio of words with Ezafe 
marker in test corpus) and 83.2% (ratio of words 
without Ezafe marker) respectively.  
A. Evaluating fine-grained tags 
The first experiment was done in order to test 
the ability of other fine grained POS tags in Ezafe 
recognition. In this test that was done on 30% of 
the corpus, all of the fine grained POS tags of the 
words plus Ezafe marker were used to train a 
Maximum Entropy POSE tagger.  As shown in 
Table 2, the accuracy of the system decreased 
when we used complete features hierarchy. So, in 
consequent experiments, we used only first level 
tag features. 
Conditions 
Performance measures  
(Run on 30% of corpus) 
Precision Recall F-
measure 
MCC Accur
acy 
MaxEnt+ 
POSE 87.95 93.14 0.91 0.89 96.71 
MaxEnt+ 
POSE+ fine 
grained tags 
89.56 88.69 0.89 0.87 96.37 
 
Table 2: Experiment Based on Full Tag Hierarchy 
 
B. Evaluating MaxEnt tagger 
In the next experiment, we used a MaxEnt 
tagger applied on whole corpus. With first level 
hierarchy of POSE tags, a total accuracy of 
97.21% was resulted. As shown in the Table 3, 
while we have a good recall rate, the precision 
140
reached a fair value. Both F-measure and MCC 
have values greater than 0.9. 
The effect of eliminating titles which are 
incomplete sentences was also experimented. 
Table 3 shows that eliminating the 
achieve a good improvement in accuracy
 
C. Using Persian-specific features 
Augmenting the system with some Persian
specific features to decrease FP and FN can 
significantly increase the total accuracy. As 
shown in Table 3, by using five 
accuracy can be increased by more than 0.6%. 
The features are as follow: 
? Punctuations cannot take Ezafe. B
feature, these FP errors will be removed
? Noun words which are followed by adjectives 
and adverbs should take Ezafe marker.
? Adjectives which are followed by nouns and 
adverbs should take Ezafe marker.
? Adverbs which are followed by nouns and 
adverbs should take Ezafe marker.
? Nouns, adverbs and adjectives which are 
followed by verbs do not take Ezafe.
 
Conditions 
Performance measures 
Precision Recall F-measure 
MAXent+POSE 89.44 94.48 0.919 
MAXent+POSE 
without title 89.53 94.47 0.919 
Maxent+POSE+
Persian Specific 
Features 
91.37 95.92 0.936 
 
Table 3 - Results of Experiments on complete corpus Size
Note that the false positive rate of the above 
mentioned experiment is about twice
negative rate. So, we tried to extract more 
features based on investigating words in FP table 
and confusion matrix. 
D. Evaluating CRF Tagger 
The next experiment was based on C
In order to compare the results with M
tagger, the experiment was performed on 
corpus using 10-fold cross validation
In this experiment, we used a CRF tagger
applied a window on the text to see the effect of 
neighboring words as input features 
recognition. As shown in Figure 1, 
of system varies by changing the size of the 
window from 1 to 9. The graph shows that the 
experiments with a CRF tagger can achieve its 
best accuracy with window of size 
performance was achieved by augmenting the 
CRF model with the five mentioned Persian
specific features.  
titlesdoes not 
. 
-
features, the 
y this simple 
. 
 
 
 
 
(%) 
MCC Accuracy 
0.903 97.21 
0.903 97.23 
0.923 97.80 
 
 of the false 
RF tagger. 
axEnt 
whole 
 method.  
 and 
in Ezafe 
the accuracy 
5. Better 
-
Fig. 1. Ezafe Recognition Accuracy vs. Window Size
Table 4 shows the results comparing 
previous works in this regard. As shown in the 
table, the accuracy of CRF model 
mentioned featuresets can achieve best 
comparing to other approaches. 
Conditions 
Ezafe 
presence 
accuracy 
Ezafe 
Presence 
Error 
Ezafe 
Absence 
Accuracy 
Ezafe Absence 
Rule based and 
syntactic  
(Oskouipour, 2011) 
10.37 89.63 83.20 
PCFG with 1000 
sentences  
(Isapour, 2007) 
86.74 13.26 95.62 
Pattern based 
method patterns 
with freq>1%  
(Bijankhan, 2005) 
79.69 20.31 92.95 
HMM with 3gram  
(Oskouipour, 2011) 78.55 21.45 95.31 
SMT based 
approach 75.96 24.05 89.99 
MaxEnt with POSE  94.48 5.52 97.75 
MaxEnt with POSE 
+ Persian Specific 
Features 
95.92 4.08 98.18 
CRF Winsize=5 95.15 4.85 98.36 
CRF Winsize=5 
+Persian Specific 
Features 
96.42 3.58 98.367 
 
Table 4 - Comparison of results (%)
9 Conclusions 
In this paper, we proposed a POSE tagging 
approach to recognize Ezafe in Persian sentences. 
Besides to this probabilistic approach, some 
features were extracted to increase the 
recognition accuracy. Experimental results show 
that CRF tagger acts pretty well in Persian Ezafe
recognition. The obtained 
outstanding performance compar
approaches and the accuracy is quite reliable 
because of training based on a 10 million
corpus.Future research can be done based on 
other taggers such as log-linear and TnT taggers. 
Moreover, Ezafe recognition can be viewed as a 
spell checking problem.So, a spell checker can 
also be used as another approach.  
 
 
with 
augmented by 
results 
Error 
Total 
Accuracy 
16.80 70.06 
4.38 93.29 
7.05 89.86 
4.68 91.69 
10.01 88.86 
2.25 97.21 
1.82 97.80 
1.63 97.83 
1.63 98.04 
 
 
results show 
ing to earlier 
-words 
141
References  
Amiri, Hadi, Hojjat, Hossein, and Oroumchian, 
Farhad.,2007.Investigation on a Feasible Corpus 
for Persian POS Tagging. 12th international CSI 
computer conference, Iran. 
Bijankhan, Mahmoud.,The Role of the Corpus 
in Writing a Grammar: An Introduction to a 
Software, Iranian Journal of Linguistics, vol. 19, 
no. 2, fall and winter 2004. 
Bijankhan, Mahmoud., 2005. A feasibility 
study on Ezafe Domain Analysis based on 
pattern matching method. Published by Research 
Institute on Culture, Art, and Communication,  
Tehran, Iran. 
Estaji, Azam., Jahangiri, Nader., 2006 The 
origin of kasre ezafe in persian language. 
Journal of Persian language and literature, Vol. 
47, pp 69-82, Isfahan University, Iran. 
Farghaly, Ali., 2004. Computer Processing of 
Arabic Script-based Languages: Current State 
and Future Directions. Workshop on 
Computational Approaches to Arabic Script-
based Languages, COLING 2004, University of 
Geneva, Geneva, Switzerland, August 28, 2004. 
Ghomeshi, Jila.  1996. Projection and 
Inflection: A Study of Persian Phrase Structure. 
PhD. Thesis, Graduate Department of 
Linguistics, University of Toronto. 
Isapour, Shahriyar., Homayounpour, 
Mohammad Mehdi, and Bijankhan, Mahmoud., 
2007. Identification of ezafe location in Persian 
language with Probabilistic Context Free 
Grammar, 13th Computer association 
Conference, Kish Island, Iran. 
Kahnemuyipour, Arsalan., 2003. Syntactic 
categories and Persian stress.  Natural Language 
& Linguistic Theory 21.2: 333-379. 
Lafferty, John.,McCallum, Andrew., and Pereira. 
Fernando, C.N., 2001 Conditional random fields: 
Probabilistic models for segmenting and labeling 
sequence data, In Proc. of ICML, pp.282-289,  
Mavvaji, Vahid., and Eslami, Moharram., 2012. 
Converting persian text to phoneme stream 
based on a syntactic analyser. The first 
internationasl conference on persian text and 
speech, September 5,6, 2012, Semnan, Iran. 
Namnabat, Majid., and Homayounpour, 
Mohamad Mehdi., 2006. Text to phoneme 
conversion in Persian language using multi-
layer perceptron neural network, Iranian Journal 
of electrical and computer engineering, Vol. 5, 
No. 3, Autumn 2007. 
Oskouipour, Navid., 2011. Converting Text to 
phoneme stream with the ability to recognizing 
ezafe marker and homographs applied to Persian 
speech synthesis. Msc. Thesis, Sharif University 
of Technology, Iran. 
Pantcheva, Marina Blagoeva., 2006. Persian 
Preposition Classes. Nordlyd; Volume 33 (1). 
ISSN 0332-7531.s 1 - 25. 
Parsafar Parviz. 2010, Syntax, Morphology, and 
Semantics of Ezafe.Iranian Studies[serial online]. 
December 2010;43(5):637-666. Available in 
Academic Search Complete, Ipswich, MA. 
Razi, Behnam, and Eshqi, Mohammad, 2012. 
Design of a POS tagger for Persian speech 
based on Neural Networks, 20th conference on 
Electrical Engineering,15-17 may 2012,Tehran, Iran. 
Samvelian, Pollet. 2007. The Ezafe as a head-
marking inflectional affix: Evidence from 
Persian and Kurmanji Kurdish. Aspects of 
Iranian Linguistics: Papers in Honor of 
Mohammad Reza Bateni, 339-361. 
Sha, Fei., and Pereira, Fernando, 
2003. Shallow parsing with conditional random 
fields, In Proc. of HLT/NAACL 2003. 
Shakeri, Zakieh, et al. 2012. Use of linguistic 
features for improving English-Persian SMT. 
Konvens 2012, The 11th Conference on Natural 
Language Processing, Vienna, Sept 19-21, 2012 
Toutanova, Kristina Klein, and Manning. 
ChristopherD., 2000. Enriching the Knowledge 
Sources Used in a Maximum Entropy Part-of-
Speech Tagger. Proceedings of the Joint 
SIGDAT Conference on Empirical Methods in 
Natural Language Processing and Very Large 
Corpora (EMNLP/VLC-2000), pp. 63-70. 
Toutanova, Kristina Klein, Manning, Christopher 
D., and Singer, Yoram. 2003. Feature-Rich Part-
of-Speech Tagging with a Cyclic Dependency 
Network. In Proceedings of HLT-NAACL 2003, 
pp. 252-259. 
Yarowsky, David. 1996. Homograph 
disambiguation in text-to-speech synthesis; 
Progress in Speech Synthesis. eds. van Santen, J., 
Sproat, R., Olive, J. and Hirschberg, J : 157-172. 
Zahedi, Morteza., 1998. Design and 
implementation of an intelligent program for 
recognizing short vowels in Persian text. Msc. 
Thesis, University of Tehran. 
 
142
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?9,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Fast Unsupervised Dependency Parsing with Arc-Standard Transitions
Mohammad Sadegh Rasooli
Department of Computer Engineering
Iran University of Science and Technology
Narmak, Tehran, Iran
rasooli@comp.iust.ac.ir
rasooli.ms@gmail.com
Heshaam Faili
School of Electrical
and Computer Engineering
University of Tehran
Amir-Abaad, Tehran, Iran
hfaili@ut.ac.ir
Abstract
Unsupervised dependency parsing is one of
the most challenging tasks in natural lan-
guages processing. The task involves find-
ing the best possible dependency trees from
raw sentences without getting any aid from
annotated data. In this paper, we illus-
trate that by applying a supervised incre-
mental parsing model to unsupervised pars-
ing; parsing with a linear time complex-
ity will be faster than the other methods.
With only 15 training iterations with linear
time complexity, we gain results compara-
ble to those of other state of the art methods.
By employing two simple universal linguis-
tic rules inspired from the classical depen-
dency grammar, we improve the results in
some languages and get the state of the art
results. We also test our model on a part of
the ongoing Persian dependency treebank.
This work is the first work done on the Per-
sian language.
1 Introduction
Unsupervised learning of grammars has achieved
considerable focus in recent years. The lack
of sufficient manually tagged linguistic data and
the considerable successes of unsupervised ap-
proaches on some languages have motivated re-
searchers to test different models of unsupervised
learning on different linguistic representations.
Since the introduction of the dependency model
with valence (DMV) proposed by Klein and Man-
ning (2004), dependency grammar induction has
received great attention by researchers. DMV
was the first model to outperform the right attach-
ment accuracy in English. Since this achievement,
the model has been used by many researchers
(e.g. (Cohen and Smith, 2010); (Gillenwater et al,
2011); (Headden III et al, 2009); and (Spitkovsky
et al, 2011b)).
The main task of unsupervised dependency
parsing is to obtain the most likely dependency
tree of a sentence without using any annotated
training data. In dependency trees, each word has
only one head and the head of the sentence is a de-
pendent of an artificial root word. Problems such
as data sparsity and a large search space that in-
creases the ambiguity have made the task difficult.
Even deciding the direction of the link between
two words in a dependency relation has made the
task more difficult than finding phrase structures
themselves (Klein and Manning, 2004).
In this paper, we propose a model based on
Arc-Standard Transition System of Nivre (2004),
which is known as an incremental greedy projec-
tive parsing model that parses sentences in lin-
ear time. To the best of our knowledge, the only
incremental unsupervised dependency parsing is
the model of Daume? III (2009) with Shift-Reduce
parsing model (Nivre, 2003).1
Our model is not lexicalized, has a simple fea-
ture space and converges in 15 iterations with
a linear (O(n)) parsing and training time, while
other methods based on DMV in the best case
work inO(n3) time complexity withO(n3) mem-
ory use for sentences with of length n. We be-
lieve that the output of this model can also im-
prove DMV.2 In addition, we use punctuation
clues (Spitkovsky et al, 2011c), tying feature sim-
ilarity in the transition system configuration, and
1The other study is in Seginer (2007) that is for con-
stituency parsing (phrase structure extraction).
2For the effect of model initialization in unsupervised de-
pendency parsing, see Gimpel and Smith (2011).
1
?baby steps? notion (Spitkovsky et al, 2009) to
improve the model accuracy.
We test our model on 9 CoNLL 2006 and 2007
shared task data sets (Buchholz and Marsi, 2006;
Nivre et al, 2007) and WSJ part of Penn treebank
and show that in some languages our model is bet-
ter than the recent models. We also test our model
on a part of an ongoing first Persian dependency
corpus (Rasooli et al, 2011). Our study may be
the first work to test dependency parsing on the
Persian language.
The remainder of this paper is organized as fol-
lows. In Section 2, related work on unsupervised
dependency parsing is reviewed. In Section 3, we
describe our dependency parsing model. In Sec-
tion 4 and Section 5, after the reporting experi-
mental results on several languages, the conclu-
sion is made.
2 Related Work
The first considerable work on unsupervised de-
pendency parsing which outperforms the base-
line (right attachment) accuracy in English was
proposed by Klein and Manning (2004). The
model is called dependency model with valence
(DMV). In the DMV, each word can be the
head of the sentence with the probability of
P (root|X). Each word X , decides to get a
child Y from a direction (right or left), with
the probability PCHOOSE(X|Y, dir, adj), where
adj is a Boolean value indicating whether the
word has gotten a child in the direction dir or
not. The other probability used in the DMV
is PSTOP (X|Y, dir, adj) that means whether to
stop getting dependents from the direction with
adjacency value or not. All the probabilities in the
model are assumed to be independent and the de-
pendency tree likelihood is a product of all prob-
abilities. Only part of speech (POS) tags are used
as features and the probabilities are multinomial.
The model uses the inside-outside algorithm to
find all possible subtrees efficiently in Expecta-
tion Maximization (EM) algorithm.
Several researchers have tried to improve and
modify the DMV. In Headden III et al (2009), by
using the lexical values with the frequency more
than 100 and defining tied probabilistic context
free grammar (PCFG) and Dirichlet priors, the ac-
curacy is improved. In Smith and Eisner (2005),
by producing artificial neighbors of the feature
space via actions such as deletion of one word,
substitution of adjacent words and adding a word,
the likelihood of the true feature space in all
neighbors is calculated. That method is known
as contrastive estimation (CE).
In Spitkovsky et al (2009), the idea of learning
the initial parameters of the model from shorter
sentences leads to a method named ?baby steps?.
In ?baby steps?, the model prior of each training
set with the sentence length less than or equal to
N , is achieved by training DMV on the training
set with the sentence length less than or equal to
N ? 1. The other method used in the mentioned
work, is ?less is more? which hypothesize that
training on a subset of all data (with the length
of less than or equal to 15) in batch mode is more
useful than training on all data. In Spitkovsky et
al. (2010a), a combination of ?baby steps? and
?less is more?, named ?leapfrog? is applied to
the DMV. In Spitkovsky et al (2011b), a mixture
of EMs is used to improve the DMV by trying
to escape from local maxima; i.e., changing the
EM policy in some iterations in order to escape
from local maxima. The model is termed ?lateen?
EM. In Spitkovsky et al (2010b), HTML hyper-
text tags are used as indicators of phrases in or-
der to localize the search space of the dependency
model. In Spitkovsky et al (2011c), punctuation
marks are used as indicators of local dependencies
of the words in the sentence.
In Cohen and Smith (2010), shared logistic nor-
mal distribution is used to tie grammatical roles
that are not assumed to be independent from each
other. In the study, the bilingual similarity of each
POS tag probability in the dependency model is
applied to the probability model. In Blunsom and
Cohn (2010), Pitman-Yor priors (PYP) are ap-
plied to the DMV. Furthermore, tree substitution
grammar (TSG) is used as an intermediate repre-
sentation of the tree. In Gillenwater et al (2011),
a mathematical model is employed to overcome
the posterior sparsity in the DMV, by defining
constraints on the probability model.
There are also some models different from
DMV. In Daume? III (2009), based on a stochas-
tic search method, Shift-Reduce transition pars-
ing model of Nivre (2003) is applied. The model
is greedy and selects an action stochastically ac-
cording to each action probability at the time. The
advantage of the model lies on its parsing and
training speed. In Naseem and Barzilay (2011),
sparse semantic annotations in texts are used as
2
Initialization ?nil,W,??
Termination ?S, nil, A?
Left-Reduce ?wiwj |S, I, A? ? ?wj |S, I, A ? ?wj , wi??
Right-Reduce ?wiwj |S, I, A? ? ?wi|S, I, A ? ?wi, wj??
Shift ?S,wi|I, A? ? ?wi|S, I, A?
Figure 1: Actions in Arc-Standard Transition System (Nivre, 2004)
clues to unsupervised parsing. In Marec?ek and
Z?abokrtsky? (2011), by applying Gibbs sampling
method to count the data occurrences, a simple
probability model (the fraction of each depen-
dency relation divided by the number of head POS
tags) is used. In that model, non-projective depen-
dency trees are allowed and all noun-root depen-
dency probabilities are multiplied by a small num-
ber, to decrease the chance of choosing a noun-
root dependency. There are also some studies in
which labeled data in one language is employed
to guide unsupervised parsing in the others (Co-
hen et al, 2011).
3 Fast Unsupervised Parsing
In this section, after a brief description of the Arc-
Standard parsing model, our probability model,
and the unsupervised search-based structure pre-
diction (Daume? III, 2009) are reviewed. After
these descriptions, we go through ?baby steps,?
the use of curricula in unsupervised learning (Tu
and Honavar, 2011), and the use of punctuation
in unsupervised parsing. Finally, we describe our
tied feature model that tries to overcome the data
sparsity. In this paper, a mixture of ?baby steps?
and punctuation clues along with search-based
structure prediction is applied to the Arc-Standard
model.
3.1 Arc-Standard Transition Model
The parser in this model has a configuration repre-
sented by ?S, I, A?, where S is a stack of words,
I is a buffer of input words which are not pro-
cessed yet andA is the list of all arcs that are made
until now. The parser initializes with ?nil,W, ??
in which W is a string of all words in the sen-
tence, nil shows a stack with a root word and ?
shows an empty set. The termination configura-
tion is shown as ?S, nil, A?, where S shows an
empty stack with only root word, nil shows an
empty buffer and A is the full arc set. An arc in
which wj is the head of wi is shown by wj ? wi
or (wj , wi).
As shown in Figure 1, there are three actions
in this model. In the shift action, the top-most
input word goes to the top of the stack. In the left-
reduce action, the top-most stack word becomes
the head of the second item in the stack and the
second item is removed from the stack. On the
other hand, in the right-reduce action, the second
word in the stack becomes the head of the top item
in the stack and the top item is removed from the
stack.
3.2 Feature Space and Probability Model
The feature space that we use in this model is
a tuple of three POS tags; i.e., the first item in
the buffer, the top-most and the second item in
the stack. The probability of each action is in-
spired from Chelba and Jelinek (2000) as in equa-
tion (1). In each step in the configuration, the
parser chooses an action based on the probability
in equation (1), where feat is the feature value
and act is an action.
P (act, feat) = P (act) ? P (feat|act) (1)
The action selection in the training phase is
done stochastically. In other words, in every step
there is a maximum of 3 actions and a minimum
of one action.3 After calculating all probabili-
ties, a roulette wheel is made to do multinomial
sampling. The sampling is done with stochas-
tic EM (Celeux and Diebolt, 1985) in a roulette
wheel selection model.
The probabilities are initialized equally (except
that P (shift) = 0.5 and P (right ? reduce) =
P (left? reduce) = 0.25). After sampling from
the data, we update the model as in equations 2?
4, where ? is a smoothing variable and Nf is the
number of all possible unique features in the data
set. C(?) is a function that counts the data from
3For example, in the first state only shift is possible and
in the last state only right-reduce is possible.
3
samples. In equations 3 and 4, sh, r ? r and
l?r are shift, right-arc and left-arc actions respec-
tively. C(Action, Feature) is obtained from the
samples drawn in the training phase. For exam-
ple, if the right-reduce action is selected, we add
its probability to C(right? reduce, feature).
P (feat|act) =
C(act, feat) + ?
C(act) +Nf?
(2)
P (sh) = 0.5 (3)
P (act) =
C(act) + ?
C(r ? r) + C(l ? r) + 2?
;
act 6= Shift
(4)
3.3 Unsupervised Search-Based Structure
Prediction
Since there are 32n+1 possible actions for a sen-
tence with the length of n it seems impractical
to track the search space for even middle-length
sentences. Accordingly, in Daume? III (2009) a
stochastic search model is designed to improve
the model accuracy based on random actions. In
that work, with each configuration step, the trainer
selects one of the actions according to the proba-
bility of each action stochastically. By choosing
actions stochastically, a set of samples is drawn
from the data and the model parameters are up-
dated based on the pseudo-code in Figure 2. In
Figure 2, pi is known as the policy of the prob-
ability model and ? is a constant number which
changes in each iteration based on the iteration
number (? = 1iteration#3 ). We employ this model
in our work to learn probability values in equa-
tion 1. The learning from samples is done via
equations (2?4).
3.4 ?Baby Steps? Incremental Parsing
In Spitkovsky et al(2009), the idea that shorter
sentences are less ambiguous, hence more in-
formative, is applied to the task. Spitkovsky et
al. (2009) emphasize that starting from sentences
with a length of 1 and iterating on sentences with
the length ? N from the probabilities gained
from the sentences with the length ? N ? 1,
leads to better results.
Initialize pi = pi?
while not converge
Take samples stochastically
h? learn from samples
pi = ?pi + (1? ?)h
end while
return pi
Figure 2: Pseudo-code of the search-based structure
prediction model in Daume? III (2009)
We also use ?baby steps? on our incremental
model. For the sentences having length 1 through
5, we only iterate once in each sentence length.
At those sentence lengths, the full search space is
explored (all trees are made by doing all possi-
ble actions in each state of all possible configura-
tions), while for sentence length 6 towards 15, we
iterate at each step 3 times, only choosing one ac-
tion stochastically at each state. The procedure is
done similarly for all languages with the same pa-
rameters. In fact, the greedy nature of the model
encourages us to bail out of each sentence length
quickly. In other words, we want to jump out of
early local maxima, as in early-terminating lateen
EM (Spitkovsky et al, 2011b).
In curricula (Tu and Honavar, 2011), smooth-
ing variable for shorter sentences is larger than
smoothing variable for longer sentences. With re-
gards to the idea, we start with smoothing variable
equal to 1 and multiply it on each sentence length
by a constant value equal to e?1.
3.5 Punctuation Clues
Spitkovsky et al (2011c) show that about 74.0%
of words in English texts occurring between two
punctuation marks have only one word linking
with other words of the sentence. This character-
istic is known as ?loose?. We apply this restriction
on our model to improve the parsing accuracy and
decrease the total search space. We show that this
clue not only does not improve the dependency
parsing accuracy, but also decreases it in some oc-
casions.
3.6 Tying Probabilities with Feature
Similarity Measure
We assume that the most important features in
the feature set for right-reduce and left-reduce ac-
tions are the two top words in the stack. On the
other hand, for the shift action, the most impor-
4
tant words are first buffer and top stack words. In
order to solve the sparsity problem, we modify
the probability of each action based on equation
(5). In this equation, neigh(act, feat) is gained
via searching over all features with the same top
and second stack item for left-reduce and right-
reduce, and all features with the same top stack
and first buffer item for the shift action.
P ?(feat|act) =
P (feat|act) +
?
f ??neigh(act,feat) P (f
?|act)
C(neigh(act,feat))
2
(5)
3.7 Universal Linguistic Heuristics to
Improve Parsing Accuracy
Based on the nature of dependency grammar, we
apply two heuristics. In the first heuristic, we mul-
tiply the probability of the last verb reduction by
10?10 in order to keep verbocentricity of the de-
pendency grammar. The last verb reduction oc-
curs when there is neither a verb in the buffer
nor in the stack except the one that is going to
be reduced by one of the right-arc or left-arc ac-
tions. In other words, the last verb remaining
on the stack should be less likely to be removed
than the other actions in the current configura-
tion.4 In the second heuristic, in addition to the
first heuristic, we multiply each noun ? verb,
adjective ? verb, and adjective ? noun by
0.1 in order to keep the nature of dependency
grammar in which nouns and adjective in most
cases are not able to be the head of a verb and an
adjective is not able to be the head of a noun.5 We
show in the experiments that, in most languages,
considering this nature will help improve the pars-
ing accuracy.
We have tested our model on 9 CoNLL data
sets (Buchholz and Marsi, 2006; Nivre et al,
2007). The data sets include Arabic, Czech, Bul-
garian, Danish, Dutch, Portuguese, Slovenian,
Spanish, and Swedish. We have also tested our
model on a part of the ongoing project of Persian
dependency treebank. The data set includes 2,113
4It is important to note that the only reason that we
choose a very small number is to decrease the chance of
verb-reduction among three possible actions. Using other
values? 0.01 does not change results significantly.
5The are some exceptions too. For example, in the sen-
tence: ?I am certain your work is good.? Because of that, we
do not choose a very small number.
train and 235 test sentences.6
As shown in Figure 3, we use the same proce-
dure as in Daume? III (2009), except that we re-
strict ? to not be less than 0.005 in order to in-
crease the chance of finding new search spaces
stochastically. As in previous works, e.g., Smith
and Eisner (2005), punctuation is removed for
evaluation.
iteration# = 0
for i=1 to 15 do
Train-set=all sentences-length?i
max-iter=3
if(i? 5)
max-iter=1
end-if
for j=1 to max-iter do
? = max( 1iteration#3 , 0.005)
iteration#? iteration# + 1
if(i ? 5)
samples? find all subtrees
end-if
else
samples? sample instances stochastically
end-else
h? learn from samples
pi = ?pi + (1? ?)h
end-for
? = ? ? e?1
end-for
Figure 3: Pseudo-code of the unsupervised Arc-
Standard training model
4 Evaluation Results
Although training is done on sentences of length
less than 16, the test was done on all sentences in
the test data without dropping any sentences form
the test data. Results are shown in Table 1 on 9
languages. In Table 1, ?h1? and ?h2? refer to the
two linguistic heuristics that are used in this pa-
per. We also compare our work with Spitkovsky
et al (2011b) and Marec?ek and Z?abokrtsky? (2011)
6This dataset is obtained via contacting with the project
team at http://www.dadegan.ir/en/. Recently an official
pre-version of the dataset is released, consisting more
than 12,000 annotated sentences (Dadegan Research Group,
2012). We wish to report results on the dataset in our future
publications.
5
Baselines Using Heuristic 1 and 2 Using Heuristic 1 Without any heuristic
Language Rand LA RA fs+punc fs punc fs+punc punc punc+fs simp.
Arabic?07 3.90 59.00 06.00 52.05 52.05 52.05 54.55 54.55 55.64 55.64
Bulgarian 8.00 38.80 17.90 52.48 53.86 46.36 42.75 37.35 35.99 35.99
Czech?07 7.40 29.60 24.20 42.37 42.40 39.31 30.21 27.94 25.17 25.17
Danish 6.70 47.80 13.10 52.14 53.11 52.14 51.10 51.70 46.01 46.01
Dutch 7.50 24.50 28.00 48.14 48.80 48.20 28.30 28.36 23.47 23.45
Persian 9.50 03.90 29.16 51.65 51.37 50.99 49.78 50.99 26.87 26.87
Portuguese 5.80 31.20 25.80 54.86 55.84 46.82 33.84 33.62 28.83 28.83
Slovenian 7.90 26.60 24.30 22.44 22.44 22.43 21.31 21.30 19.47 19.45
Spanish 4.30 29.80 24.70 30.88 31.16 30.88 32.33 32.33 29.63 29.69
Swedish 7.80 27.80 25.90 32.74 34.33 33.52 28.48 28.48 25.74 25.74
Turkish 6.40 01.50 65.40 33.83 27.39 38.13 61.27 47.92 30.56 34.52
Average 6.84 29.14 25.86 43.05 42.98 41.89 39.45 37.69 31.58 31.94
Table 1: Results tested on CoNLL data sets and the Persian data set. ?Rand?, ?LA? and ?RA? stand for random,
left-attach and right-attach, respectively; ?punc? refers to punctuation clues and fs refers to feature similarity cue;
?all? refers to using both heuristics h1 and h2; and ?simp.? refers to the simple model.
in Table 2. As shown in Table 2, our model out-
performs the accuracy in 7 out of 9 languages.
The Effect of Feature Similarity
As shown in Table 1, feature similarity cannot
have any effect on the simple model. When we
add linguistic information to the model, this fea-
ture similarity measure keeps the trainer from
diverging. In other words, the greedy nature
of the model becomes endangered when incom-
plete knowledge (as in our linguistic heuristics)
is used. Incomplete knowledge may cause early
divergence. In other words, the greedy algo-
rithm tracks the knowledge which it has and does
not consider other probable search areas. This
phenomenon may cause early divergence in the
model. By using feature similarity, we try to es-
cape from this event.
The Effect of Punctuation Clues
As shown in Table 1, in most languages punc-
tuation clues do not improve the accuracy. This
maybe arises out of the fact that ?loose? is not a
good clue for incremental parsing. The other clue
is ?sprawl? in which the external link restriction
is lifted. This restriction is in 92.9% of fragments
in English texts (Spitkovsky et al, 2011c), but it
is not implemented and tested in this paper.
4.1 Evaluation on English
We also test our data on Penn Treebank but we do
not gain better results than state of the art meth-
ods. We use the same train and test set as in
Model WSJ?10 WSJ ??
h1+fs 45.16 31.97
h1+fs+punc 44.17 30.17
Stoch. EM(1-5) 40.86 33.65
Stoch. EM(1-5)+h1 52.70 42.85
Stoch. EM(1-5)+h1+h2 50.30 41.37
A1+fs+h1 49.9 43.3
Klein and Manning (2004) 43.2 -
Daume? III (2009) 45.4 -
Blunsom and Cohn (2010) 67.7 55.7
Spitkovsky et al (2011a) - 59.1
Table 3: Results of our model on WSJ, compared
to its counterpart Daume? III (2009) and other DMV-
based models. Since in Blunsom and Cohn (2010) and
Spitkovsky et al (2011b), other results are reported,
we only limit our report to some of the results on WSJ.
In the Table, ?h1? shows heuristic 1 and ?fs? shows
the use of feature similarity. Stochastic EM(1-5) is
one test that have done only by applying baby steps
on sentences with the length 1 to 5 without using un-
supervised search-based model. A1 refers to a change
in the model in which smoothing variable in steps 1 to
5 is multiplied by 10.
Spitkovsky et al (2009). We convert the Penn
treebank data via automatic ?head-percolation?
rules (Collins, 1999). We have also tested our
model via simple stochastic EM (without using
unsupervised structure prediction) and show that
the main problem with this method in English is
its fast divergence when jumping from sentence
length 5 to 6. In the model settings tested for
English, the model with heuristic 1 with the fea-
ture similarity is the best setting that we find. By
testing with a smoothing variable ten times bigger
6
``````````````Language
Method Name
MZ-NR MZ Spi5 Spi6 Our Best
Arabic?07 24.8 25.0 22.0 49.5 55.64
Bulgarian 51.4 25.4 44.3 43.9 53.86
Czech?07 33.3 24.3 31.4 28.4 42.40
Danish 38.6 30.2 44.0 38.3 53.11
Dutch 43.4 32.2 32.5 27.8 48.80
Persian - - - - 51.65
Portuguese 41.8 43.2 34.4 36.7 55.84
Slovenian 34.6 25.4 33.6 32.2 22.44
Spanish 54.6 53.0 33.3 50.6 32.33
Swedish 26.9 23.3 42.5 50.0 34.33
Turkish 32.1 32.2 33.4 35.9 61.27
Average (except Persian) 38.1 31.4 35.1 39.3 46.00
Table 2: Comparison of our work to those of Table 5 (?Spi5?) and Table 6 (?Spi6?) in Spitkovsky et al (2011b)
and Marec?ek and Z?abokrtsky? (2011) with Noun-Root constraint (?MZ-NR?) and no constraint (?MZ?). The
comparison results are from Table 4 in Marec?ek and Z?abokrtsky? (2011). ?Our Best? refers to the bold scores in
Table 1.
in the first 5 steps, we have seen that the results
change significantly. The results are shown in Ta-
ble 3.
One main problem of the converted depen-
dencies in English is their conversion errors like
multi-root trees.7 There are many trees in the
corpus that have wrong multi-root dependencies.
Such problems lead us to believe that we should
not rely too much on the results on WSJ part of
the Penn treebank.
5 Analysis and Conclusion
One main aspect of incremental methods is their
similarity to the way that humans read and learn
sentences in language. The interesting character-
istic of incremental parsing lies on its speed and
low memory use. In this paper, we use one new
incremental parsing model on unsupervised pars-
ing for the first time. The simple mathematical
model and its linear training order has made the
model flexible to be used for bigger data sets.
In addition to testing recently used heuristics in
unsupervised parsing, by inspiring basic depen-
dency theory from linguistics, parsing accuracy
has been increased in some languages. We see
that this model is capable of detecting many true
dependencies in many languages.
Some observations show that choosing inap-
7We assume to find only trees that are projective and
single-rooted.
propriate parameters for the model may lead to
unwanted divergence in the model. The diver-
gence is mostly seen in English, where we see
a significant accuracy decrease at the last step in
comparison to step 5 instead of seeing an increase
in the accuracy. With one setting in English, we
reach the accuracy equal to 43% (13% more than
the accuracy of the model reported in this paper).
In some languages like Slovenian, we see that
even with a good undirected accuracy, the model
does not succeed in finding the dependency di-
rection with heuristics. While in Czech, Dutch,
and Bulgarian the second heuristic works well,
it does not change accuracy a lot in other lan-
guages (in languages like Turkish and English this
heuristic decreases accuracy). We believe that
choosing better linguistic knowledge like the ones
in Naseem et al (2010), tying grammatical rules
from other languages similar to the work in Cohen
and Smith (2010), and choosing better probability
models that can be enriched with lexical features
and broad context consideration (like the works in
supervised incremental dependency parsing) will
help the model perform better on different lan-
guages.
Despite the fact that our study is the first work
done on Persian, we believe that the results that
we achieve for Persian is very considerable, re-
garding the free-word order nature of the Persian
language.
7
Acknowledgments
The paper is funded by Computer Research Cen-
ter of Islamic Sciences (CRCIS). We would like
to appreciate Valentin Spitkovsky and Shay Co-
hen for their technical comments on the re-
search and paper draft. We would also thank
Maryam Faal-Hamedanchi, Manouchehr Kouhes-
tani, Amirsaeid Moloodi, Hamid Reza Ghader,
Maryam Aminian and anonymous reviewers for
their comments on the draft version. We would
also like to thank Jason Eisner, Mark Johnson,
Noah Smith and Joakim Nivre for their help in
answering our questions and Saleh Ziaeinejad and
Younes Sangsefidi for their help on the project.
References
Phil Blunsom and Trevor Cohn. 2010. Unsupervised
induction of tree substitution grammars for depen-
dency parsing. In 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2010), pages 1204?1213.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceeding of the Tenth Conforence on Computa-
tional Natural Language Learning (CoNLL).
Gilles Celeux and Jean Diebolt. 1985. The SEM al-
gorithm: A probabilistic teacher algorithm derived
from the em algorithm for the mixture problem.
Computational Statistics Quarterly, 2:73?82.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech & Lan-
guage, 14(4):283?332.
Shay B. Cohen and Noah A. Smith. 2010. Co-
variance in unsupervised learning of probabilistic
grammars. Journal of Machine Learning Research
(JMLR), 11:3117?3151.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011. Unsupervised structure prediction with Non-
Parallel multilingual guidance. In Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP 2011).
Michael Collins. 1999. Head-driven statistical mod-
els for natural language parsing. Ph.D. thesis, Uni-
versity of Pennsylvania.
Dadegan Research Group. 2012. Persian Dependency
Treebank Version 0.1, Annotation Manual and User
Guide. http://dadegan.ir/en/.
Hal Daume? III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75(3):297?325.
Hal Daume? III. 2009. Unsupervised search-based
structured prediction. In 26th International Confer-
ence on Machine Learning (ICML), pages 209?216.
ACM.
Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a,
Fernando Pereira, and Ben Taskar. 2011. Poste-
rior sparsity in unsupervised dependency parsing.
Journal of Machine Learning Research (JMLR),
12:455?490.
Kevin Gimpel and Noah A. Smith. 2011. Concav-
ity and initialization for unsupervised dependency
grammar induction. Technical report.
William P. Headden III, Mark Johnson, and David
McClosky. 2009. Improving unsupervised depen-
dency parsing with richer contexts and smoothing.
In Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of
the ACL, pages 101?109.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure:
Models of dependency and constituency. In Asso-
ciation for Computational Linguistics (ACL).
David Marec?ek and Zdene?k Z?abokrtsky?. 2011. Gibbs
sampling with treeness constraint in unsupervised
dependency parsing. In RANLP Workshop on Ro-
bust Unsupervised and Semisupervised Methods in
Natural Language Processing.
Tahira Naseem and Regina Barzilay. 2011. Using se-
mantic cues to learn syntax. In 25th Conference on
Artificial Intelligence (AAAI-11).
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2010).
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceeding of CoNLL 2007.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In International Work-
shop on Parsing Technologies, pages 149?160.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Workshop on Incremental
Parsing: Bringing Engineering and Cognition To-
gether, pages 50?57.
Mohammad Sadegh Rasooli, Amirsaeid Moloodi,
Manouchehr Kouhestani, and Behrouz Minaei-
Bidgoli. 2011. A syntactic valency lexicon for
Persian verbs: The first steps towards Persian de-
pendency treebank. In 5th Language & Technology
Conference (LTC): Human Language Technologies
as a Challenge for Computer Science and Linguis-
tics, pages 227?231.
Yoav Seginer. 2007. Fast unsupervised incremen-
tal parsing. In 45th Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL), pages
384?391.
Noah A. Smith and Jason Eisner. 2005. Guiding un-
supervised grammar induction using contrastive es-
timation. In IJCAI Workshop on Grammatical In-
ference Applications, pages 73?82.
8
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2009. Baby steps: How ?Less is more? in
unsupervised dependency parsing. In NIPS 2009
Workshop on Grammar Induction, Representation
of Language and Language Learning.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2010a. From baby steps to leapfrog: How
?Less is more? in unsupervised dependency pars-
ing. In Human Language Technologies: The 11th
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL HLT 2010).
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010b. Profiting from Mark-Up: Hyper-
Text annotations for guided parsing. In 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2010).
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X
Chang, and Daniel Jurafsky. 2011a. Unsupervised
dependency parsing without gold Part-of-Speech
tags. In 2011 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2011b. Lateen EM: unsupervised train-
ing with multiple objectives, applied to dependency
grammar induction. In 2011 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP 2011).
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel
Jurafsky. 2011c. Punctuation: Making a point
in unsupervised dependency parsing. In Fifteenth
Conference on Computational Natural Language
Learning (CoNLL-2011).
Kewei Tu and Vasant Honavar. 2011. On the utility of
curricula in unsupervised learning of probabilistic
grammars. In 22nd International Joint Conference
on Artificial Intelligence (IJCAI 2011).
9
