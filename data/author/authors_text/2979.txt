An Evaluation Corpus For Temporal Summarization
Vikash Khandelwal, Rahul Gupta, and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
fvikas,rgupta,allang@cs.umass.edu
ABSTRACT
In recent years, a lot of work has been done in the eld of
Topic Tracking. The focus of this work has been on iden-
tifying stories belonging to the same topic. This might re-
sult in a very large number of stories being reported to the
user. It might be more useful to a user if a summary of the
main events in the topic rather than the entire collection of
stories related to the topic were presented. Though work
on such a ne-grained level has been started, there is cur-
rently no standard evaluation testbed available to measure
the accuracy of such techniques. We describe a scheme for
developing a testbed of user judgments which can be used
to evaluate the above mentioned techniques. The corpus
that we have created can also be used to evaluate single or
multi-document summaries.
1. THE PROBLEM
In recent years, a lot of progress has been made in the
eld of Topic Tracking ([2], [3], [8], etc). The focus of this
work has been on identifying news stories belonging to the
same topic. This might result in a very large number of
stories being reported to the user. It would be more useful
to a user if a summary of the main events/developments in
the topic rather than the entire collection of stories related
to the topic were presented. We can formulate the problem
as follows.
We are given a stream of chronologically ordered and top-
ically related stories. We strive to identify the shifts in the
topic which represent the developments within the topic.
For example, consider the topic \2000 Presidential Elec-
tions". On the night of November 7, there were reports of
Gore conceding defeat to Bush. The next morning, there
were reports claiming his retraction of the previous conces-
sion. Most of the stories on the next day would also contain
old information including details of Gore's rst phone call
to Bush. We want to present only the new development
(i.e., Gore's retraction) on the next day.
.
We assume that sentence extracts can identify such topic
shifts. At the very least, they can convey enough informa-
tion to a user to keep track of the developments within that
topic. For example, in Figure 1, the mappings indicate how
the sentences in a story correspond to events.
Human judgments are required to evaluate accuracy of
extracts. The approach usually taken is to have each such
extract evaluated by human beings but such a process is ex-
pensive and time consuming. We need an evaluation corpus
similar to the TDT or TREC corpora that can be used over
and over again to do such evaluations automatically. We
propose a new scheme for building such a corpus.
Summarization evaluation is di?cult because summaries
can be created for a range of purposes. The Tipster SUM-
MAC evaluation [7] required human assessors to evaluate
each summary, and most other evaluations have also re-
quired human checking of every summary [6]. There are
others who have attempted automatic evaluations ([5], [9])
but none of these evaluation schemes captures all the desir-
able properties in a summary.
The particular problem of summarizing shifts in a news
topic was attacked slightly dierently at a Summer 1999
workshop on Novelty Detection [4]. Those eorts towards
\new information detection" were a dead end because the
granularity of new information was too small, e.g., a men-
tion of a person's age might count as new information even
when it is not the focus of the story. Swan and Allan also
created an event-level summary \timeline" ([10], [11]) but
they did not develop any evaluation corpus for their work.
This paper is organised as follows. In Section 2, we dis-
cuss the desirable properties of such an evaluation corpus.
Section 3 discusses the entire annotation process, as well
as the interesting practical issues, the problems faced and
then the statistics of the corpus that we have built. Finally,
in Section 4, we discuss one possible way of utilising this
corpus.
2. DESIRABLE PROPERTIES OF THE
EVALUATION CORPUS
Any evaluation corpus of sentence extracts and events
which is to be used for the purpose of evaluating summaries
of topic shifts in a news stream should have the following
properties:
 It should be possible to identify all new events on a
periodic basis. This would be required to estimate the
recall of a system.
The Navy has ordered the discharge of sailor Timothy Mcveigh 
after he described himself gay in his America online user profile.
Civil rights groups and gay campaigners are outraged.
Mcveigh, who?s no relation of the convicted Oklahoma bomber,
is Lodging an appeal.
Paul Miller has more.
Timothy R. Mcveigh put "gay" in the marital status part of an aol
For "the world," I am Paul Miller in Washington.
user profile.
He did not use his full name or say he was in the Navy, referring
to himself only as "Tim of Honolulu".
The Navy?s personnel department says that?s violation of the 
Clinton administration?s Don?t ask/Don?t tell policy of Homo-
sexuals in the military and Mcveigh has been dismissed.
Many people are upset that the Navy asked aol for information
about the supposedly anonymous user and, a Naval investigat-
or says, the online service provided it.
Gay rights groups say it?s discrimination.
Privacy advocates say it?s a breach of confidentiality.
      
Sailor Mcveigh dis-
charged from Navy
Navy claims that he 
violated "Don?t ask/
Don?t tell" policy.
Discrimination against
                gays.
Breach of privacy by
            Navy.
           event
Not related to any
Figure 1: An example showing how sentence extracts can indicate events
 It should be possible to quantify the precision of a
summary, i.e., it should be possible to nd the pro-
portion of relevant sentences in the summary,
 It should be possible to identify redundancy in the
system output being evaluated. There should be some
way of assigning a marginal utility to sentences con-
taining relevant but redundant information
 It should be possible to quantify the \usefulness" of
a summary taking recall, precision as well as redun-
dancy into account.
 Sentence boundaries should be uniquely identied
(though they need not be perfect) because the aim
of the system is to identify the relevant portions in
the summary.
3. BUILDING AN EVALUATION CORPUS
3.1 The annotation process
We collect a stream of stories related to a certain topic
from the TDT-2 corpus of stories from January 1 to June
30 1998. We used stories that were judged \on-topic" by
annotators from LDC. The topics were selected from the
TDT 1998 and 1999 evaluations. The stories are parsed to
obtain sentence boundaries and all the sentences are given
unique identiers. We proceed with collecting the human
judgments in the following four steps.
1. Each judge reads all the stories and identies the im-
portant events.
2. The judges sit together to merge the events identied
by them, to form a single list of events for that topic.
All the events are given unique identiers.
3. Each judge goes through the stories again, connecting
sentences to the relevant events. Obviously, not all
sentences need to be related to any event. However,
if some sentence is relevant to more than one event, it
is linked to all those events.
4. Another judge now veries the mapping between the
sentences and the events. This gives us the nal map-
ping from sentences to events.
This way we obtain all the events mentioned within a
story and we can also nd out the events which nd their
rst mention within this story. The advantage of building
the evaluation corpus in this way is that these judgments
can be used both for summarizing topic shifts as well as
summarizing any given story by itself.
We have built a user interface in Java to allow judges to do
the above work systematically. Figure 2 shows a snapshot
of the interface used by the judges.
3.2 Statistics of the judgments obtained
We have obtained judgments for 22 topics. Three judges
worked on each topic. We summarize the results of the
annotation process for a subset of the topics in Table 1.
We dene the interjudge agreement for an event to be the
ratio of the number of sentences linked to that event, as
agreed upon by the third judge, to the number of sentences
in the union of the sentences individually marked by the
rst two judges for that event. For a topic, the interjudge
agreement is dened to be the average of the agreement for
all the events in that topic. It is to be noted that the Kappa
statistic is not applicable here in any standard form.
We found a large variance in the number of sentences
linked to dierent events. As an example, in Table 2, we
show the statistics for a group of news stories describing
Figure 2: A snapshot of the user interface used for annotating the topics
Topic id # of # of Time taken Inter-judge
stories events (in hours) Agreement
20008 49 10 4.5 0.91
20020 34 23 4.5 0.98
20021 48 9 2.5 0.97
20022 27 10 3.5 0.85
20024 38 12 2.75 0.98
20026 68 11 2.5 0.87
20031 34 15 2.5 0.62
20041 24 11 2 0.94
20042 28 14 2.5 0.66
20057 19 9 2 0.66
20065 57 16 2.33 0.94
20074 51 13 3 0.96
Average 39.75 12.75 2.88 0.86
Table 1: Annotation statistics for some of the topics
the damage due to tornados in Florida. We see that event 5
(\Relief agencies needed more than $300,000 to provide re-
lief") is linked to 4 sentences while event 1 (\At least 40 peo-
ple died in Florida due to 10-15 tornados.") is linked to 43
sentences. We may be able to use the number of sentences
linked to a event as an indicator of the weight/importance
of the event.
We have divided our corpus into two parts - one each for
training and testing respectively. Each part consists of 11
topics. Care was taken to ensure that both the parts had
topics of roughly the same size and time of occurrence. The
statistics of both parts of the corpus are given in Table 3.
3.3 Problems faced
 Sometimes our sentence parser broke up a valid sen-
tence into multiple parts. One judge linked only the
Event id # of Inter-judge
sentences Agreement
1 43 1.0
2 9 1.0
3 33 0.97
4 8 1.0
5 4 0.8
6 5 1.0
7 14 1.0
8 19 1.0
9 9 1.0
Table 2: Variance in the number of sentences linked
to dierent events for topic 20021
relevant part of the sentence to the corresponding
event while another linked all the parts to that event.
This happened in the case of three of the topics (top-
ics 20031, 20042 and 20057) before we detected the
problem.
 Sometimes when similar sentences occur in dierent
stories, one of the judges neglected the later occur-
rences of the sentence.
3.4 Interesting issues/judges? comments
We asked the judges for feedback on the annotation pro-
cess and the di?culties faced. Here are some of the inter-
esting issues which cropped up :
 Some ideas/events cannot be covered by any single
sentence but only by a group of sentences. By them-
selves, none of the sentences might be relevant to the
event. For example, Suppose, the event is The Navy
and AOL contradict each other and we have two sen-
tences - \the navy has said in sworn testimony that
Training Test All
Number of topics 11 11 22
Number of stories 474 470 944
per topic 43.1 42.7 42.9
Number of events 162 181 343
per topic 14.7 16.5 15.6
Number of sentences 8043 9006 17049
per topic 731.2 818.7 775.0
per story 17.0 19.2 18.1
O-event sentences 72% 70% 71%
Single-event sentences 24% 26% 25%
Multi-event sentences 4% 4% 4%
Table 3: Characteristics of the corpus. All numbers
except for the number of topics are averaged over
all topics included in that column.
this did happen." and \america online is saying this
never happened." Clearly, any one sentence does not
adequately represent the event. This can be easily
taken care of by considering groups of sentences rather
than single sentences.
 Abstract ideas : Sometimes the meaning of individ-
ual sentences is totally dierent from overall idea they
convey. Satirical articles are an example of this. These
kind of ideas cannot be represented by sentence ex-
tracts. We omitted such events.
 Sometimes dierent stories totally contradict each
other. For example, some stories (on the same day)
claim a lead for Bush while others claim Gore to be far
ahead. This is more of a summarization issue though
and need not be dealt with while building the evalua-
tion corpus.
4. USING THE EVALUATION CORPUS
We have used the corpus for evaluating our system which
produces temporal summaries in news stream ([1]). The
problem of temporal summarization can be formalized as
follows. A news topic is made up of a set of events and
is discussed in a sequence of news stories. Most sentences
of the news stories discuss one or more of the events in the
topic. Some sentences are not germane to any of the events.
Those sentences are called \o-event" sentences and con-
trast with \on-event" sentences. The task of the system is
to assign a score to every sentence that indicates the impor-
tance of the sentence in the summary. This scoring yields
a ranking on all sentences in the topic, including o- and
on-event sentences.
We will use measures that are analogues of recall and
precision. We are interested in multiple properties:
 Useful sentences are those that have the potential to
be a meaningful part of the summary. O-event sen-
tences are not useful, but all other sentences are.
 Novel sentences are those that are not redundant|
i.e., are new in the presentation. The rst sentence
about an event is clearly novel, but all following sen-
tences discussing the same event are not.
Figure 3: nu-recall vs nu-precision plot for the task
of summarizing topic shifts in a news stream
 Size of the summary is a typical measure used in sum-
marization research and we include it here.
Based on those properties, we could dene the following
measure to capture the combination of usefulness and nov-
elty:
nu   recall =
P
I(r(e
i
) > 0)
E
nu  precision =
P
I(r(e
i
) > 0)
S
r
where S
r
is the number of sentences retrieved, E is the
number of events in the topic, e
i
is event number i (1  i 
E), r(e
i
) is the number of sentences retrieved for event e
i
,
I(exp) is 1 if exp is true and 0 if not. All summations are
as i ranges over the set of events. Note that S
r
6=
P
r(e
i
)
since completely o-topic sentences might be retrieved.
The nu-recall measure is the proportion of the events that
have been mentioned in the summary, and nu-precision is
the proportion of sentences retrieved that are the rst men-
tions of an event.
We used this measure to evaluate the performance of
our system over the entire training corpus. The results for
the training corpus are shown in the nu-recall/nu-precision
graph in gure 3. This work is described in detail else-
where([1]).
This is just one of the possible ways of using the corpus.
We can dene a number of other similar measures which
could be easily computed using the data provided by such a
corpus. These same measures can also be used to evaluate a
system producing single or multi-document summaries too.
5. FUTURE WORK
We intend to complete collecting user judgments for more
topics soon. After analyzing the reliablity of these judg-
ments and correcting the few mistakes that we had made
initially, we will collect annotations for more topics. Ini-
tially, we had used a simple barebones sentence parser, since
that is mostly su?cient for the work such a corpus would be
put to. Nevertheless, in future annotations, we will need to
improve the sentence parser. We intend to continue using
these judgments to evaluate the performance of the systems
that we are currently building to identify and summarize
topic shifts in news streams.
Acknowledgements
This material is based on work supported in part by the
Library of Congress and Department of Commerce under
cooperative agreement number EEC-9209623 and in part
by SPAWARSYSCEN-SD grant number N66001-99-1-8912.
Any opinions, ndings and conclusions or recommendations
expressed in this material are the author(s) and do not nec-
essarily reect those of the sponsor.
6. REFERENCES
[1] J. Allan, R. Gupta, and V. Khandelwal. Temporal
Summaries of News Topics. Proceedings of SIGIR
2001 Conference, New Orleans, LA., 2001.
[2] J. Allan, V. Lavrenko, D. Frey, and V. Khandelwal.
UMASS at TDT2000. TDT 2000 Workshop notebook,
2000.
[3] J. Allan, R. Papka, and V. Lavrenko. On-line New
Event Detection and Tracking. Proceedings of SIGIR
1998, pp. 37-45, 1998.
[4] J. Allan et al Topic-based novelty detection. 1999
Summer Workshop at CLSP Final Report. Available
at http://www.clsp.jhu.edu/ws99/tdt, 1999.
[5] J. Goldstein, M. Kantrowitz, V. Mittal, and
J. Carbonell. Summarizing text documents: Sentence
Selection and Evaluation Metrics. Proceedings of
SIGIR 1999, August 1999.
[6] H. Jing, R. Barzilay, K. McKeown, and M. Elhadad.
Summarization Evaluation Methods: Experiments
and Analysis. Working notes, AAAI Spring
Symposium on Intelligent Text Summarization,
Stanford, CA, April, 1998.
[7] Inderjeet Mani and et al The TIPSTER SUMMAC
Text Summarization Evaluation Final Report. 1998.
[8] R. Papka, J. Allan, and V. Lavrenko. UMASS
Approaches to Detection and Tracking at TDT2.
Proceedings of the DARPA Broadcast News
Workshop, Herndon,VA, pp. 111-125, 1999.
[9] D. R. Radev, H. Jing, and M. Budzikowska.
Summarization of multiple documents: clustering,
sentence extraction, and evaluation. ANLP/NAACL
Workshop on Summarization, Seattle, WA, 2000.
[10] R. Swan and J. Allan. Extracting Signicant Time
Varying Features from Text. Proceedings of the Eighth
International Conference on Information and
Knowledge Management, pp.38-45, 1999.
[11] R. Swan and J. Allan. Automatic Generation of
Overview Timelines. Proceedings of SIGIR 2000
Conference, Athens, pp.49-56, 2000.
Monitoring the News: a TDT demonstration system
David Frey, Rahul Gupta, Vikas Khandelwal,
Victor Lavrenko, Anton Leuski, and James Allan
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
ABSTRACT
We describe a demonstration system built upon Topic Detection
and Tracking (TDT) technology. The demonstration system moni-
tors a stream of news stories, organizes them into clusters that rep-
resent topics, presents the clusters to a user, and visually describes
the changes that occur in those clusters over time. A user may also
mark certain clusters as interesting, so that they can be ?tracked?
more easily.
1. TDT BACKGROUND
The Topic Detection and Tracking (TDT) research program in-
vestigates methods for organizing an arriving stream of news sto-
ries by the topics the stories discuss.[1, 4, 7, 8] Topics are de?ned
to be the set of stories that follow from some seminal event in the
world?this is in contrast to a broader subject-based notion of topic.
That is, stories about a particular airline crash fall into one topic,
and stories from other airline crashes will be in their own topics.
All organization is done as stories arrive, though variations of
the task allow ?nal organizational decisions to be postponed for
minutes, hours, or even days. The formal TDT evaluation program
includes the following research tasks:
1. Segmentation is used to separate a television or radio pro-
gram into distinct news stories. This process is not needed
for newswire services, since those stories arrive pre-segmented.
2. Detection is the task of putting all arriving news stories into
bins that represent broad news topics. If a new topic appears
in the news, the system must create a new bin. Neither the
set of bins nor the total number of them is known in advance.
This task is carried out without any supervision?i.e., the
system never knows whether or not the stories it is putting
together actually belong together.
3. Tracking is the task of ?nding all stories that follow are on
the same topic as an initial small set. This task is different
from detection in that the starting stories are known to be on
the same topic. Typically tracking is evaluated with 2-4 on-
topic stories.
.
The TDT research workshops also include a few other tasks (?rst
story detection, and story link detection). TDT has also inspired
other event-based organization methods, including automatic time-
line generation to visualize the temporal locality of topics[10], and
the identi?cation of new information within a topic?s discussion[3].
This demonstration system illustrates event-based news organi-
zation by visualizing the creation of, changes within, and relation-
ships between clusters created by the detection task. It leverages
the segmentation results so that audio stories are distinct stories,
but does not directly visualize the detection. Tracking is implicity
presented by allowing clusters to be marked so that they receive
special attention by the user.
2. ARCHITECTURE
The TDT demonstration system is based upon Lighthouse, an
interactive information retrieval system developed by Leuski.[6]
Lighthouse provides not only a typical ranked list search result, but
a visualization of inter-document similarities in 2- or 3-dimensions.
The user interface is a Java client that can run as an application or
an applet. Lighthouse uses http protocols to send queries to a server
and receive the ranked list, summary information about the docu-
ments, and the visualization data.
The TDTLighthouse system requires a TDT system running in
the background. In this version of the demonstration, the TDT sys-
tem is only running the segmentation and detection tasks described
above. Stories arrive and are put into clusters (bins).
The TDTLighthouse client can query its server to receive up-to-
date information about the clusters that the TDT system has found.
The server in turn queries the TDT system to get that information
and maintains state information so that changes (cluster growth,
additional clusters, etc.) can be highlighted.
3. DEMONSTRATION DATA
The data for this demonstration was taken from the our TDT
2000 evaluation output on the TDT cluster detection task [8]. The
sytem is running on the TDT-3 evaluation collection of news arti-
cles, approximately 40,000 news stories spanning October 1 through
December 31, 1998.
We simulated incremental arrival of the data as follows. At the
end of each day in the collection, we looked at the incremental
output of the TDT detection system. At this point, every story has
been classi?ed into a cluster. Every story seen to date is in one of
the clusters for that day, even if the cluster has the same contents as
it did yesterday.
The demonstration is designed to support text summarization
tools that could help a user understand the content of the cluster.
For our purposes, each cluster was analyzed to construct the fol-
lowing information:
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 1: TDT demonstration system running on TDT-3 data, approximately four weeks into the collection.
1. The title was generated by selecting the 10 most commonly
occurring non-stopwords throughout the cluster. A better ti-
tle would probably be the headline of the most ?representa-
tive? news story, though this is an open research question.
2. The summary was generated by selecting the ?ve sentences
that were most representative of the entire cluster. Better ap-
proaches might generate a summary from the multiple doc-
uments [9] or summarize the changes from the previous day
[5, 2].
3. The contents of the cluster is just a list of every story in the
cluster, presented in reverse chronological order. Various
alternative presentations are possible, including leveraging
the multimedia (radio and television) that is the basis for the
TDT data.
The demonstration system was setup so that it could move from
between the days. All of the input to the client was generated au-
tomatically, but we saved the information so that it could be shown
more quickly. It typically takes a few minutes to generate all of the
presentation information for a single day?s clusters.
4. DEMONSTRATION SYSTEM
Figure 1 shows the client window. This snapshot shows the sys-
tem on October 31 at 10:00pm, approximately four weeks into the
data. The status line on the lower-left shows that at this point the
system has already encountered almost 16,000 stories and has bro-
ken them into about 2400 topic clusters.
The system is showing the 50 topics with the largest number of
stories. The ranked list (by size) starts on the upper-left, shows the
?rst 25, and the continues in the upper-right. The ?title? for each
of those topics is generated in this case by the most common words
within the cluster. Any system that does a better job of building
a title for a large cluster of stories could be used to improve this
capability.
In addition to the ranked list of topics, the system computes inter-
topic similarities and depicts that using the spheres in the middle.
If two topics are highly similar, their spheres will appear near each
other in the visualization. This allows related topics to be detected
quickly. Because the 50 largest topics are shown, the topics are
more unalike than they would be with a wider range, but it is still
possible to see, for example, that topics about the Clinton pres-
idency are near each other (the cyan pair of spheres overlapping
rank number 9, topic rank numbers 5 and 29). The spheres and the
ranked list are tightly integrated, so selecting one causes the other
to be highlighted.
Topics can be assigned colors to make them easier to pick out in
future sessions. In this case, the user has chosen to use the same
color for a range of related topics?e.g., red for sports topics, green
for weather topics, etc. The color selection is in the control of
the user and is not done automatically. However, once a color is
assigned to a topic, the color is ?sticky? for future sessions. A user
might choose to color a critical topic bright red so that changes to
it stand out in the future.
Figure 2 shows the same visualization, but here a summary of
a selected topic is shown in a pop-up balloon. This summary was
generated by selecting sentences that contained large numbers of
key concepts from the topic. Any summarization of a cluster could
be used here if it provided more useful information.
To illustrate how the demonstration system shows changes in
TDT clusters over time, Figure 3 shows an updated visualization
for two weeks later (November 14, 1998). The topic colors are
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 2: Similar to Figure 1, but showing a pop-up balloon.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 4: A 3-dimensional version of Figure 3.
persistent from Figure 1, though one of the marked topics (?Straw-
berry cancer colon Yankee?) is no longer in the largest 50 so does
not appear.
Most of the spheres include a small ?wedge? of yellow in them.
That indicates the proportion of the topic that is new stories (since
Figure 1). Some topics have large numbers of new stories, so have
a large yellow slice, whereas a few have a very small number of
new stories, so have only a thin wedge. The yellow wedge can be
as much as 50% of the sphere (which would represent an entirely
new topic), and only covers the top of the sphere. This restriction
ensures that the topic color is still visible.
The controls at the top of the screen are for moving between
queries, issuing a query, and returning the visualization to a ?home?
point. The next ?ve controls affect the layout of the display, includ-
ing allowing a 3-D display: a 3-D version of Figure 3 is shown in
Figure 4. The ?nal control enables a browsing wizard that can be
used to ?nd additional topics that are very similar to a selected topic
color (that set is chosen using the pull-down menu that has ?none?
in it).
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
Figure 3: TDT demonstration system running on TDT-3 data, approximately six weeks into the collection.
5. CONCLUSION AND FUTURE WORK
The demonstration system described above illustrates the effect
of TDT technology. It is also interesting in its own right, allow-
ing a user to track news topics of interest and to see how changes
occur over time. There is no reason that the same system could
not be used for non-TDT environments: any setting that clusters
documents might be appropriate for this system.
We are working to extend the demonstration system to include
some additional features.
? Considering the large number of topics (almost 3,000 in Fig-
ure 3), it is unlikely that all ?interesting? topics will be ?nd-
able. The query box at the top of the display will be used to
allow the user to ?nd topics that match a request. The ranked
list will display the top 50 topics that match the query.
? Related to querying, we hope to include an ?alert? feature
that will ?ag newly-created topics that match a query. For
example, an analyst interested in the Middle East might de-
velop a query that would identify topics in that region. When
such a topic appeared, it would be ?agged for the user (prob-
ably with a ?hot topic? color).
? We hope to allow user ?correction? of the topic breakdown
provided by the TDT system. The state-of-the-art in TDT
still makes mistakes, sometimes pulling two similar topics
together, and sometimes breaking a single topic into multiple
clusters. We intend that a user who sees such a mistake be
able to indicate it to the system. That information will, in
turn, to be relayed back to the TDT system to affect future
processing.
? We will be implementing an ?explode this topic? feature that
will show the stories within a topic analogously to the way
the current system shows the topics within the news. If the
topic is small enough, for example, the spheres would repre-
sent stories within the topic. If the topic is larger, the spheres
might represnt sub-clusters within the topic.
Acknowledgments
This material is based on work supported in part by the Library of
Congress and Department of Commerce under cooperative agree-
ment number EEC-9209623, and in part by SPAWARSYSCEN-SD
contract number N66001-99-1-8912. Any opinions, ?ndings and
conclusions or recommendations expressed in this material are the
authors? and do not necessarily re?ect those of the sponsor.
6. REFERENCES
[1] J. Allan, J. Carbonell, G. Doddington, J. Yamron, and
Y. Yang. Topic detection and tracking pilot study: Final
report. In Proceedings of the DARPA Broadcast News
Transcription and Understanding Workshop, pages 194?218,
1998.
[2] J. Allan, R. Gupta, and K. Khandelwal. Temporal summaries
of news topics. Technical Report IR-226, University of
Massachusetts, CIIR, 2001.
[3] J. Allan, H. Jin, M. Rajman, C. Wayne, D. Gildea,
V. Lavrenko, R. Hoberman, and D. Caputo. Topic-based
novelty detection: 1999 summer workshop at CLSP, ?nal
report. Available at http://www.clsp.jhu.edu/ws99/tdt, 1999.
[4] DARPA, editor. Proceedings of the DARPA Broadcast news
Workshop, Herndon, Virginia, February 1999.
[5] V. Khandelwal, R. Gupta, and J. Allan. An evaluation
scheme for summarizing topic shifts in news streams. In
Notebook proceedings of HLT 2001, 2001.
[6] A. Leuski and J. Allan. Lighthouse: Showing the way to
relevant information. In Proceedings of the IEEE Symposium
on Information Visualization (InfoVis), pages 125?130, 2000.
[7] NIST. Proceedings of the TDT 1999 workshop. Notebook
publication for participants only, March 2000.
[8] NIST. Proceedings of the TDT 2000 workshop. Notebook
publication for participants only, November 2000.
[9] D. R. Radev, H. Jing, and M. Budzikowska. Summarization
of multiple documents: clustering, sentence extraction, an d
evaluation. ANLP/NAACL Workshop on Summarization,
Seattle, WA, 2000.
[10] Russell Swan and James Allan. Automatic generation of
overview timelines. In Proceedings of SIGIR, pages 49?56,
Athens, Greece, 2000. ACM.
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 325?335,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
ReNoun: Fact Extraction for Nominal Attributes
Mohamed Yahya
?
Max Planck Institute for Informatics
myahya@mpi-inf.mpg.de
Steven Euijong Whang, Rahul Gupta, Alon Halevy
Google Research
{swhang,grahul,halevy}@google.com
Abstract
Search engines are increasingly relying on
large knowledge bases of facts to provide
direct answers to users? queries. How-
ever, the construction of these knowledge
bases is largely manual and does not scale
to the long and heavy tail of facts. Open
information extraction tries to address this
challenge, but typically assumes that facts
are expressed with verb phrases, and there-
fore has had difficulty extracting facts for
noun-based relations.
We describe ReNoun, an open information
extraction system that complements pre-
vious efforts by focusing on nominal at-
tributes and on the long tail. ReNoun?s ap-
proach is based on leveraging a large on-
tology of noun attributes mined from a text
corpus and from user queries. ReNoun
creates a seed set of training data by us-
ing specialized patterns and requiring that
the facts mention an attribute in the ontol-
ogy. ReNoun then generalizes from this
seed set to produce a much larger set of ex-
tractions that are then scored. We describe
experiments that show that we extract facts
with high precision and for attributes that
cannot be extracted with verb-based tech-
niques.
1 Introduction
One of the major themes driving the current evo-
lution of search engines is to make the search
experience more efficient and mobile friendly
for users by providing them concrete answers to
queries. These answers, that apply to queries
about entities that the search engine knows about
(e.g., famous individuals, organizations or loca-
tions) complement the links that the search en-
?
Work done during an internship at Google Research.
gine typically returns (Sawant and Chakrabati,
2013; Singhal, 2012; Yahya et al., 2012). To
support such answers, the search engine main-
tains a knowledge base that describes various at-
tributes of an entity (e.g., (Nicolas Sarkozy,
wife, Carla Bruni)). Upon receiving a query,
the search engine tries to recognize whether the
answer is in its knowledge base.
For the most part, the aforementioned knowl-
edge bases are constructed using manual tech-
niques and carefully supervised information ex-
traction algorithms. As a result, they obtain high
coverage on head attributes, but low coverage on
tail ones, such as those shown in Table 1. For ex-
ample, they may have the answer for the query
?Sarkozy?s wife?, but not for ?Hollande?s ex-
girlfriend? or ?Google?s philanthropic arm?. In
addition to broadening the scope of query answer-
ing, extending the coverage of the knowledge base
to long tail attributes can also facilitate providing
Web answers to the user. Specifically, the search
engine can use lower-confidence facts to corrob-
orate an answer that appears in text in one of the
top Web results and highlight them to the user.
This paper describes ReNoun, an open-
information extraction system that focuses on ex-
tracting facts for long tail attributes. The obser-
vation underlying our approach is that attributes
from the long tail are typically expressed as nouns,
whereas most previous work on open-information
extraction (e.g., Mausam et al. (2012)) extend
techniques for extracting attributes expressed in
verb form. Hence, the main contribution of our
work is to develop an extraction system that com-
plements previous efforts, focuses on nominal at-
tributes and is effective for the long tail. To that
end, ReNoun begins with a large but imperfect on-
tology of nominal attributes that is extracted from
text and the query stream (Gupta et al., 2014).
ReNoun proceeds by using a small set of high-
precision extractors that exploit the nominal na-
325
Attribute Fact Phrase Verb form seen
legal affairs (NPR, legal affairs NPR welcomed Nina Totenberg as 7
correspondent correspondent, Nina Totenberg) its new legal affairs correspondent.
economist (Princeton, economist, Princeton economist Paul Krugman 7
Paul Krugman) was awarded the Nobel prize in 2008.
ex-boyfriend (Trierweiler, ex-boyfriend, Trierweiler did not have any children 3
Hollande with her ex-boyfriend Hollande.
staff writer (The New Yorker, staff writer, Adam Gopnik is one of The New 3
Adam Gopnik) Yorker?s best staff writers.
Table 1: Examples of noun phrases as attributes, none which are part of a verb phrase. Additionally, the first two attributes do
not occur within a verb phrase in a large corpus (see ? 2 for details) in a setting where they can be associated with a triple.
ture of the attributes to obtain a training set, and
then generalizes from the training set via distant
supervision to find a much larger set of extraction
patterns. Finally, ReNoun scores extracted facts
by considering how frequently their patterns ex-
tract triples and the coherence of these patterns,
i.e., whether they extract triples for semantically
similar attributes. Our experiments demonstrate
that ReNoun extracts a large body of high preci-
sion facts, and that these facts are not extracted
with techniques based on verb phrases.
2 Preliminaries
The goal of ReNoun is to extract triples of the form
(S,A,O), where S is subject, A is the attribute, and
O is the object. In our setting, the attribute is al-
ways a noun phrase. We refer to the subject and
object as the arguments of the attribute.
ReNoun takes as input a set of attributes, which
can be collected using the methods described in
Gupta el al. (2014), Lee et al. (2012), and Pasca
and van Durme (2007). In this work, we use Biper-
pedia (Gupta et al., 2014), which is an ontology
of nominal attributes automatically extracted from
Web text and user queries. For every attribute,
Biperpedia supplies the Freebase (Bollacker et al.,
2008) domain type (e.g., whether the attribute ap-
plies to people, organizations or hotels). Since the
attributes themselves are the result of an extraction
algorithm, they may include false positives (i.e.,
attributes that do not make sense).
The focus of ReNoun is on attributes whose val-
ues are concrete objects (e.g., wife, protege,
chief-economist). Other classes of attributes
that we do not consider in this work are (1) nu-
meric (e.g., population, GDP) that are better ex-
tracted from Web tables (Cafarella et al., 2008),
and (2) vague (e.g., culture, economy) whose
value is a narrative that would not fit the current
mode of query answering on search engines.
We make the distinction between the fat head
and long tail of attributes. To define these two sets,
we ordered the attributes in decreasing order of the
number of occurrences in the corpus
1
. We defined
the fat head to be the attributes until the point N
in the ordering such that the sum of the total num-
ber of occurrences of attributes before N equaled
the number of total occurrences of the attributes
after N . In our news corpus, the fat head included
218 attributes (i.e., N = 218) and the long tail
included 60K attributes. Table 2 shows examples
from both.
Fat head
daughter, headquarters
president, spokesperson,
Long tail
chief economist, defender,
philanthropic arm, protege
Table 2: Examples of fat head and long tail attributes.
The output of ReNoun is a set of facts, where
each fact could be generated by multiple extrac-
tions. We store the provenance of each extraction
and the number of times each fact was extracted.
Noun versus verb attributes
ReNoun?s goal is to extract facts for attributes ex-
pressed as noun phrases. A natural question is
whether we can exploit prior work on open in-
formation extraction, which focused on extracting
relations expressed as verbs. For example, if we
can extract facts for the attribute advised or is
advisor of, we can populate the noun attribute
advisor with the same facts. In Section 7.2 we
demonstrate that this approach is limited for sev-
eral reasons.
First, attributes in knowledge bases are typically
expressed as noun phrases. Table 3 shows that
1
The occurrences were weighted by the number of se-
mantic classes they occur with in the ontology because many
classes overlap.
326
Knowledge Base %Nouns %Verbs
Freebase 97 3
DBpedia 96 4
Table 3: Percentage of attributes expressed as nouns phrases
among the 100 attributes with the most facts.
the vast majority of the attributes in both Freebase
and DBpedia (Auer et al., 2007) are expressed as
nouns even for the fat head (and even more so for
the long tail). Hence, if we extract the verb form
of attributes we would need to translate them into
noun form, which would require us to solve the
paraphrasing problem and introduce more sources
of error (Madnani and Dorr, 2010). Second, as we
dig deeper into the long tail, attributes tend to be
expressed in text more in noun form rather than
verb form. One of the reasons is that the attribute
names tend to get longer and therefore unnatural
to express as verbs (e.g. chief privacy officer, au-
tomotive division). Finally, there is often a sub-
tle difference in meaning between verb forms and
noun forms of attributes. For example, it is com-
mon to see the phrase ?Obama advised Merkel on
saving the Euro,? but that would not necessarily
mean we want to say that Obama is an advisor of
Angela Merkel, in the common sense of advisor.
Processed document corpus
ReNoun extracts facts from a large corpus of
400M news articles. We exploit rich synactic
and linguistic cues, by processing these docu-
ments with a natural language processing pipeline
comprising of ? dependency parsing, noun phrase
chunking, named entity recognition, coreference
resolution, and entity resolution to Freebase. The
chunker identifies nominal mentions in the text
that include our attributes of interest. As discussed
later in the paper, we exploit the dependency
parse, coreference and entity resolution heavily
during various stages of our pipeline.
3 Overview of ReNoun
Since ReNoun aims at extracting triples for at-
tributes not present in head-heavy knowledge
bases, one key challenge is that we do not have any
labeled data (i.e. known facts) for such attributes,
especially in the long tail. Therefore ReNoun has
an initial seed fact extraction step that automati-
cally generates a small corpus of relatively precise
seed facts for all attributes, so that distant supervi-
sion can be employed. The second big challenge
is to filter the noise from the resulting extractions.
ReNoun?s extraction pipeline, shown in Fig-
ure 1, is composed of four stages.
Seed fact extraction: We begin by extracting a
small number of high-precision facts for our at-
tributes. For this step, we rely on manually spec-
ified lexical patterns that are specifically tailored
for noun phrases, but are general enough to be in-
dependent of any specific attributes. When apply-
ing such patterns, we exploit coreference to make
the generated seed facts more precise by requiring
the attribute and object noun phrases of a seed fact
to refer to the same real-world entity. This is elab-
orated further in Section 4.
Extraction pattern generation: Utilizing the
seed facts, we use distant supervision (Mintz et al.,
2009) to learn a set of dependency parse patterns
that are used to extract a lot more facts from the
text corpus.
Candidate generation: We apply the learned de-
pendency parse patterns from the previous stage
to generate a much larger set of extractions. We
aggregate all the extractions that give rise to the
same fact and store with them the provenance of
the extraction. The extractions generated here are
called candidates because they are assigned scores
that determine how they are used. The applica-
tion consuming an extraction can decide whether
to discard an extraction or use it, and in this case
the manner in which it is used, based on the scores
we attach to it and the application?s precision re-
quirements.
Scoring: In the final stage, we score the facts, re-
flecting our confidence in their correctness. In-
tuitively, we give a pattern a high score if it ex-
tracts many facts that have semantically similar at-
tributes, and then propagate this score to the facts
extracted by the pattern (Section 6).
4 Seed fact extraction
Since we do not have facts, but only attributes, the
first phase of ReNoun?s pipeline is to extract a set
of high-precision seed facts that are used to train
more general extraction patterns. ReNoun extracts
seed facts using a manually crafted set of extrac-
tion rules (see Table 4). However, the extraction
rules and the application of these rules are tailored
to our task of extracting noun-based attributes.
Specifically, when we apply an extraction rule
to generate a triple (S,A,O), we require that (1) A
is an attribute in our ontology, and (2) the value of
A and the object O corefer to the same real-world
327
	
 	


 	
 

	




Figure 1: Extraction Pipeline: we begin with a set of high-precision extractors and use distant supervision to train other
extractors. We then apply the new extractors and score the resulting triples based on the frequency and coherence of the
patterns that produce them.
1. the A of S, O ? the CEO of Google, Larry Page
2. the A of S is O ? the CEO of Google is Larry Page
3. O, S A ? Larry Page, Google CEO
4. O, S?s A ? Larry Page, Google?s CEO
5. O, [the] A of S ? Larry Page, [the] CEO of Google
6. SAO ? Google CEO Larry Page
7. S A, O ? Google CEO, Larry Page
8. S?s A, O ? Google?s CEO, Larry Page
Table 4: High precision patterns used for seed fact extraction
along with an example of each. Here, the object (O) and the
attribute (A) corefer and the subject (S) is in close proxim-
ity. In all examples, the resulting fact is (Google, CEO,
Larry Page). Patterns are not attribute specific.
entity. For example, in Figure 2, CEO is in our on-
tology and we can use a coreference resolver to in-
fer that CEO and Larry Page refer to the same en-
tity. The use of coreference follows from the sim-
ple observation that objects will often be referred
to by nominals, many of which are our attributes of
interest. Since the sentence matches our sixth ex-
traction rule, ReNoun extracts the triple (Google,
CEO, Larry Page).
Document:
?[Google]
1
[CEO]
2
[Larry Page]
2
started his term in 2011,
when [he]
2
succeeded [Eric Schmidt]
3
. [Schmidt]
3
has
since assumed the role of executive chairman of [the
company]
1
.?
(a)
Coreference clusters:
# Phrases Freebase ID
1 Google, the company /m/045c7b
2 Larry Page, CEO, he /m/0gjpq
3 Eric Schmidt, Schmidt /m/01gqf4
(b)
Figure 2: Coreference clusters: (a) a document annotated
with coreference clusters; (b) a table showing each cluster
with the representative phrases in bold and the Freebase ID
to which each cluster maps.
We rely on a coreference resolver in the spirit of
Haghighi and Klein (2009). The resolver clusters
the mentions of entities in a document so the ref-
erences in each cluster are assumed to refer to the
same real-world entity. The resolver also chooses
for each cluster a representative phrase, which is a
proper noun or proper adjective (e.g., Canadian).
Other phrases in the same cluster can be other
proper nouns or adjectives, common nouns like
CEO or pronouns like he in the example. Each
cluster is possibly linked by an entity resolver to
a Freebase entity using a unique Freebase ID. Fig-
ure 2(b) shows the coreference clusters from the
sample document, with representative phrases in
bold, along with the Freebase ID of each clus-
ter. Note that in our example the phrase execu-
tive chairman, which is also in our ontology of
attributes, is not part of any coreference cluster.
Therefore, the fact centered around this attribute
in the example will not be part of the seed extrac-
tions, but could be extracted in the next phase. The
resulting facts use Freebase IDs for the subject and
object (for readability, we will use entity names
in the rest of this work). In summary, our seed
extraction proceeds in two steps. First, we find
sentences with candidate attribute-object pairs that
corefer and in which the attribute is in our ontol-
ogy. Second, we match these sentences against our
hand-crafted rules to generate the extractions. In
Section 7 we show that the precision of our seed
facts is 65% for fat head attributes and 80% for
long tail ones.
5 Pattern and candidate fact generation
In this section we describe how ReNoun uses the
seed facts to learn a much broader set of extrac-
tion patterns. ReNoun uses the learned patterns
to extract many more candidate facts that are then
assigned scores reflecting their quality.
5.1 Dependency patterns
We use the seed facts to learn patterns over de-
pendency parses of text sentences. A dependency
parse of a sentence is a directed graph whose ver-
tices correspond to tokens labeled with the word
and the POS tag, and the edges are syntactic rela-
tions between the corresponding tokens (de Marn-
effe et al., 2006). A dependency pattern is a sub-
graph of a dependency parse where some words
have been replaced by variables, but the POS tags
328
have been retained (called delexicalization). A de-
pendency pattern enables us to extract sentences
with the same dependency parse as the sentence
that generated the pattern, modulo the delexical-
ized words. We note that one big benefit of using
dependency patterns is that they generalize well,
as they ignore extra tokens in the sentence that do
not belong to the dependency subgraph of interest.
5.2 Generating dependency patterns
The procedure for dependency pattern generation
is shown in Algorithm 1, and Figure 3 shows an
example of its application. The input to the algo-
rithm is the ontology of attributes, the seed facts
(Section 4), and our processed text corpus (Sec-
tion 2).
Algorithm 1: Dependency pattern generation
input : Set of attributes A, Seed facts I , Corpus D.
P := An empty set of dependency pattern-attribute pairs.
foreach sentence s ? D do
foreach triple t = (S,A,O) found in s do
if t ? I then
G(s) = dependency parse of s
P
?
= minimal subgraph of G(s)
containing the head tokens of S, A and O
P = Delexicalize(P
?
, S, A, O)
P = P ? {?P,A?}
return P
Attributes: A ={executive chairman}
Seed fact: I = {(Google, executive chairman, Eric Schmidt)}
Sentence: s =?An executive chairman, like Eric Schmidt of Google, wields influence
over company operations.?An/DET
executive/NNchairman/NN detnn
like/INprep Schmidt/NNPpobj Eric/NNPnn of/INprep Google/NNPpobj
(a)
chairman/NN like/INprep Schmidt/NNPpobj of/INprep Google/NNPpobj
(b)
{A/N} like/INprep {O/N}pobj of/INprep {S/N}pobj
(c)
Figure 3: Dependency pattern generation using seed facts,
corresponding to Algorithm 1: (a) shows the input to the pro-
cedure (dependency parse partially shown); (b) P
?
; (c) P .
The procedure iterates over the sentences in the
corpus, looking for matches between a sentence
s and a seed fact f . A sentence s matches f if
s contains (i) the attribute in f , and (ii) phrases in
coreference clusters that map to the same Freebase
IDs as the subject and object of f . When a match
is found, we generate a pattern as follows.
We denote by P
?
the minimal subgraph of the
dependency parse of s containing the head tokens
of the subject, attribute and object (Figure 3 (b)).
We delexicalize the three vertices corresponding
to the head tokens of the subject, attribute and ob-
ject by variables indicating their roles. The POS
tag associated with the attribute token is always a
noun. The subject and object are additionally al-
lowed to have pronouns and adjectives associated
with their tokens. All POS tags corresponding to
nouns are lifted to N, in order to match the vari-
ous types of nouns. We denote the resulting de-
pendency pattern by P and add it to our output,
associated with the matched attribute. We note
that in principle, the vertex corresponding to the
head of the attribute does not need to be delexi-
calized. However, we do this to improve the ef-
ficiency of pattern-matching, since we will often
have patterns for different attributes differing only
at the attribute vertex.
It is important to note that because of the man-
ner in which the roles of subject and object were
assigned during seed fact extraction, the patterns
ReNoun generates clearly show which argument
will take the role of the subject, and which will
take the role of the object. This is in contrast
to previous work such as Ollie (Mausam at al.,
2012), where the assignment depends on the order
in which the arguments are expressed in the sen-
tence from which the fact is being extracted. For
example, from the sentence ?Opel was described
as GM?s most successful subsidiary.? and the seed
fact (GM, subsidiary, Opel), the pattern that
ReNoun generates will consistently extract facts
like (BMW, subsidiary, Rolls-Royce), and not
the incorrect inverse, regardless of the relative or-
dering of the two entities in the sentence.
At this point we have dependency patterns ca-
pable of generating more extractions for their seed
fact attributes. For efficient matching, we use the
output of Algorithm 1 to generate a map from de-
pendency patterns to their attributes with entries
like that shown in Figure 4(a). This way, a pat-
tern match can be propagated to all its mapped at-
tributes in one shot, as we explain in Section 5.3.
Finally, we discard patterns that do not pass a sup-
port threshold, where support is the number of dis-
tinct seed facts from which a pattern could be gen-
erated.
329
{A/N} like/INprep {O/N}pobj of/INprep {S/N}pobj
attributes: {executive chairman, creative director, ...}
(a)
?An executive chairman, like Steve Chase of AOL, is
responsible for representing the company.?
?
(AOL, executive chairman, Steve Chase)
(b)
?A creative director, like will.i.am of 3D Systems, may also
be referred to as chief creative officer.?
?
(3D Systems, creative director, will.i.am)
(c)
Figure 4: A dependency pattern and its use in extraction: (a)
the pattern in our running example and the set of attributes to
which it applies; (b) and (c) sentences matching the pattern
and the resulting extractions.
5.3 Applying the dependency patterns
Given the learned patterns, we can now generate
new extractions. Each match of a pattern against
the corpus will indicate the heads of the poten-
tial subject, attribute and object. The noun phrase
headed by the token matching the {A/N} vertex is
checked against the set of attributes to which the
pattern is mapped. If the noun phrase is found
among these attributes, then a triple (S, A, O) is
constructed from the attribute and the Freebase en-
tities to which the tokens corresponding to the S
and O nodes in the pattern are resolved. This triple
is then emitted as an extraction along with the pat-
tern that generated it. Figure 4(b) and (c) show two
sentences that match the dependency pattern in our
running example and the resulting extractions.
Finally, we aggregate our extractions by their
generated facts. For each fact f , we save the dis-
tinct dependency patterns that yielded f and the
total number of times it was found in the corpus.
6 Scoring extracted facts
In this section we describe how we score the can-
didate facts extracted by applying the dependency
patterns in Section 5. Recall that a fact may be
obtained from multiple extractions, and assigning
scores to each fact (rather than each extraction) en-
ables us to consider all extractions of a fact in ag-
gregate.
We score facts based on the patterns which ex-
tract them. Our scheme balances two character-
istics of a pattern: its frequency and coherence.
Pattern frequency is defined as the number of ex-
has/VBZ {S/N}nsubj children/NNSdobj with/INprep {A/N}pobj {O/N}appos
attributes: {ex-wife, boyfriend, ex-partner}
frequency(P ) = 574, coherence(P ) = 0.429
Example: ?Putin has two children with his ex-wife,
Lyudmila.?
(a)
{A/N} {S/N}nn{O/N} nn
attributes: {ex-wife, general manager, subsidiary,... }
frequency(P ) = 52349038, coherence(P ) = 0.093
Example: ?Chelsea F.C. general manager Jos?e Mourinho...?
(b)
Figure 5: (a) a coherent pattern extracting facts for semanti-
cally similar attributes and (b) an incoherent pattern.
tractions produced by the pattern. Our first ob-
servation is that patterns with a large number of
extractions are always able to produce correct ex-
tractions (in addition to incorrect ones). We also
observe that generic patterns produce more er-
roneous facts compared to more targeted ones.
To capture this, we introduce pattern coherence,
which reflects how targeted a pattern is based on
the attributes to which it applies. For example,
we observed that if an extraction pattern yields
facts for the coherent set of attributes ex-wife,
boyfriend, and ex-partner, then its output is
consistently good. On the other hand, a pattern
that yields facts for a less coherent set of attributes
ex-wife, general manager, and subsidiary is
more likely to produce noisy extractions. Generic,
more incoherent patterns are more sensitive to
noise in the linguistic annotation of a document.
Figure 5 shows an example pattern for each case,
along with its frequency and coherence.
We capture coherence of attributes using word-
vector representations of attributes that are cre-
ated over large text corpora (Mikolov et al., 2013).
The word-vector representation v(w) for a word
w (multi-word attributes can be preprocessed into
single words) is computed in two steps. First, the
algorithm counts the number of occurrences of a
word w
1
that occurs within the text window cen-
tered at w (typically a window of size 10), pro-
ducing an intermediate vector that potentially has
a non-zero value for every word in the corpus.
The intermediate vector is then mapped to a much
smaller dimension (typically less than 1000) to
produce v(w). As shown in (Mikolov et al., 2013),
two words w
1
and w
2
for which the cosine dis-
330
tance between v(w
1
) and v(w
2
) is small tend to
be semantically similar. Therefore, a pattern is co-
herent if it applies to attributes deemed similar as
per their word vectors.
Given an extraction pattern P that extracts facts
for a set of attributes A, we define the coherence
of P to be the average pairwise coherence of all at-
tributes inA, where the pairwise coherence of two
attributes a
1
and a
2
is the cosine distance between
v(a
1
) and v(a
2
).
Finally, we compute the score of a fact f by
summing the product of frequency and coherence
for each pattern of f as shown in Equation 1.
S(f) =
?
P?Pat(f)
frequency(P )? coherence(P ) (1)
7 Experimental Evaluation
We describe a set of experiments that validate the
contributions of ReNoun. In Sections 7.2 and 7.3
we validate our noun-centric approach: we show
that extractions based on verb phrases cannot yield
the results of ReNoun and that NomBank, the re-
source used by state of the art in semantic role-
labeling for nouns, will not suffice either. In Sec-
tions 7.4-7.6 we evaluate the different components
of ReNoun and its overall quality, and in Sec-
tion 7.7 we discuss the cases in which ReNoun was
unable to extract any facts.
7.1 Setting
We used the fat head (FH) and long tail (LT) at-
tributes and annotated news corpus described in
Section 2. When evaluating facts, we used major-
ity voting among three human judges, unless oth-
erwise noted. The judges were instructed to con-
sider facts with inverted subjects and objects as in-
correct. For example, while (GM, subsidiary,
Opel) is correct, its inverse is incorrect.
7.2 Verb phrases are not enough
State-of-art open information extraction systems
like Ollie (Mausam at al., 2012) assume that a re-
lation worth extracting is expressed somewhere in
verb form. We show this is not the case and jus-
tify our noun-centric approach. In this experiment
we compare ReNoun to a custom implementation
of Ollie that uses the same corpus as ReNoun and
supports multi-word attributes. While Ollie does
try to find relations expressed as nouns, its seed
facts are relations expressed as verbs.
We randomly sampled each of FH and LT for
100 attributes for which ReNoun extracts facts and
ReNoun Ollie
flagship company -
railway minister -
legal affairs correspondent -
spokesperson be spokesperson of
president-elect be president elect of
co-founder be co-founder of
Table 5: ReNoun attributes with and without a corresponding
Ollie relation.
asked a judge to find potentially equivalent Ol-
lie relations. Note that we did not require the
judge to find exactly the same triple (thereby bias-
ing the experiment towards finding more attribute
matches). Furthermore, the judge was instructed
that a verb phrase like advised by should be con-
sidered a match to the ReNoun attribute advisor.
However, looking at the data, most facts involving
the relation advised are not synonymous with the
advisor relation as we think of it (e.g., ?Obama
advised Merkel on saving the Euro?). This obser-
vation suggests that there is an even more subtle
difference between the meaning of verb expres-
sions and noun-based expressions in text. This ex-
periment, therefore, gives an upper bound on the
number of ReNoun attributes that Ollie can cover.
For FH, not surprisingly, we could find matches
for 99 of the 100 attributes. However, for LT, only
31 of the 100 attributes could be found, even under
our permissive setting. Most attributes that could
not be matched were multi-word noun phrases.
While in principle, one could use the Ollie patterns
that apply to the head of a multi-word attribute, we
found that we generate more interesting patterns
for specific multi-word attributes. Table 5 shows
examples of attributes with and without verb map-
pings in Ollie.
We also compare in the other direction and esti-
mate the portion of Ollie relations centered around
nouns for which ReNoun fails to extract facts. For
this experiment, we randomly sampled 100 Ollie
relations that contained common nouns whose ob-
jects are concrete values, and looked for equivalent
attributes in ReNoun extractions. ReNoun extracts
facts for 48 of the Ollie relations. Among the 52
relations with no facts, 25 are not in Biperpedia
(which means that ReNoun cannot extract facts for
them no matter what). For the other 27 relations,
ReNoun did not extract facts for the following
reasons. First, some relations expressed actions,
which cannot be expressed using nouns only, and
are not considered attributes describing the subject
entity (e.g., citation of in ?Obama?s citation
331
of the Bible?). Second, some relations have the
object (a common noun) embedded within them
(e.g., have microphone in) and do not have cor-
responding attributes that can be expressed us-
ing nouns only. The remaining relations either
have meaningless extractions or use common noun
phrases as arguments. ReNoun only uses proper
nouns (i.e., entities) for arguments because facts
with common noun arguments are rarely interest-
ing without more context. We note that the major-
ity of the 25 Ollie relations without corresponding
Biperpedia attributes also fall into one of the three
categories above.
7.3 Comparison against NomBank
In principle, the task of extracting noun-mediated
relations can be compared to that of semantic role
labeling (SRL) for nouns. The task in SRL is to
identify a relation, expressed either through a verb
or a noun, map it to a semantic frame, and map
the arguments of the relation to the various roles
within the frame. State of the art SRL systems,
such as that of Johansson and Nugues (2008), are
trained on NomBank (Meyers et al., 2004) for
handling nominal relations, which also means that
they are limited by the knowledge it has. We asked
a judge to manually search NomBank for 100 at-
tributes randomly drawn from each of FH and LT
for which ReNoun extracts facts. For multi-word
attributes, we declare a match if its head word was
found. We were able to find 80 matches for the
FH attributes and 42 for LT ones. For example,
we could not find entries for the noun attributes
coach or linebacker (of a football team). This
result is easy to explain by the fact that NomBank
only has 4700 attributes.
In addition, for some nouns, the associated
frames do not allow for the extraction of triples.
For example, all frames for the noun member spec-
ify one argument only, so in the sentence ?John
became a member of ACM?, the output relation is
(ACM, member) instead of the desired triple (ACM,
member, John).
As we did with Ollie, we also looked at nouns
from NomBank for which ReNoun does not ex-
tract facts. Out of a random sample of 100 Nom-
Bank nouns, ReNoun did not extract facts for
29 nouns (four of which are not in Biperpedia).
The majority of the missed nouns cannot be used
by ReNoun because they either take single ar-
guments (instead of two) or take either preposi-
tional phrases or common nouns (instead of proper
nouns correponding to entities) as one their argu-
ments.
7.4 Quality of seed facts
In Section 4, we described our method for ex-
tracting seed facts for our attributes. Applying
the method to our corpus resulted in 139M extrac-
tions, which boiled down to about 680K unique
facts covering 11319 attributes. We sampled 100
random facts from each of FH and LT, and ob-
tained 65% precision for FH seed facts and 80%
precision for LT ones. This leads us to two obser-
vations.
First, the precision of seed facts for LT attributes
is high, which makes them suitable for use as
a building block in a distant supervision scheme
to learn dependency parse patterns. We are pri-
marily interested in LT attributes, which earlier
approaches cannot deal with satisfactorily as we
demonstrated above.
Second, LT attributes have higher precision than
FH attributes. One reason is that multi-word at-
tributes (which tend to be in LT) are sometimes
incorrectly chunked, and only their head words are
recognized as attributes (which are more likely to
be in FH). For example, in the phrase ?America?s
German coach, Klinsmann?, the correct attribute
is German coach (LT), but bad chunking may pro-
duce the attribute coach (FH) with Germany as the
subject. Another reason is that FH attributes are
likely to occur in speculative contexts where the
presence of the attribute is not always an asser-
tion of a fact. (While both FH and LT attributes
can be subject to speculative contexts, we observe
this more for FH than LT in our data.) For ex-
ample, before a person is a railway minister
of a country, there is little mention of her along
with the attribute. However, before a person is
elected president, there is more media about her
candidacy. Speculative contexts, combined with
incorrect linguistic analysis of sentences, can re-
sult in incorrect seed facts (e.g., from ?Republi-
can favorite for US president, Mitt Romney, vis-
ited Ohio?, we extract the incorrect seed fact (US,
president, Mitt Romney)).
7.5 Candidate generation
Using the seed facts, we ran our candidate gen-
eration algorithm (Section 5). In the first step of
the algorithm we produced a total of about 2 mil-
lion unique dependency patterns. A third of these
332
patterns could extract values for exactly one at-
tribute. Manual inspection of these long tail pat-
terns showed that they were either noise, or do not
generalize. We kept patterns supported by at least
10 seed facts, yielding more than 30K patterns.
We then applied the patterns to the corpus. The
result was over 460M extractions, aggregated into
about 40M unique facts. Of these, about 22M facts
were for LT attributes, and 18M for FH. We now
evaluate the quality of these facts.
7.6 Scoring extracted facts
In Section 6, we presented a scheme for scoring
facts using pattern frequency and coherence. To
show its effectiveness we (i) compare it against
other scoring schemes, and (ii) show the quality
of the top-k facts produced using this scheme, for
various k. To compute coherence, we generated
attribute word vectors using the word2vec
2
tool
trained on a dump of Wikipedia.
First, we compare the quality of our scoring
scheme (FREQ COH) with three other schemes as
shown in Table 6. The scheme FREQ is identical
to FREQ COH except that all coherences are set
to 1. PATTERN counts the number of distinct pat-
terns that extract the fact while PATTERN COH
sums the pattern coherences. We generated a ran-
dom sample of 252 FH and LT nouns with no en-
tity disambiguation errors by the underlying nat-
ural language processing pipeline. The justifi-
cation is that none of the schemes we consider
here capture such errors. Accounting for such
errors requires elaborate signals from the entity
linking system, which we leave for future work.
For each scoring scheme, we computed the Spear-
man?s rank correlation coefficient ? between the
scores and manual judgments (by three judges). A
larger ? indicates more correlation, and comput-
ing ? was statistically significant (p-value<0.01)
for all schemes.
Scheme Spearman?s ?
FREQ 0.486
FREQ COH 0.495
PATTERN 0.265
PATTERN COH 0.257
Table 6: Scoring schemes
FREQ and FREQ COH dominate, which shows
that considering the frequency with which patterns
perform extraction helps. The two schemes, how-
ever, are very close to each other. We observed
2
https://code.google.com/p/word2vec/
FH LT
k Precision #Attr Precision #Attr
10
2
1.00 8 1.00 50
10
3
0.98 36 1.00 294
10
4
0.96 78 0.98 1548
10
5
0.82 106 0.96 5093
10
6
0.74 124 0.70 7821
All 0.18 141 0.26 11178
Table 7: Precision of random samples of the top-k scoring
facts, along with the attribute yield.
that adding coherence helps when two facts have
similar frequencies, but this effect is tempered
when considering a large number of facts.
Second, we evaluate the scoring of facts gener-
ated by ReNoun by the precision of top-k results
for several values of k. In this evaluation, facts
with disambiguation errors are counted as wrong.
The particular context in which ReNoun is applied
will determine where in the ordering to set the
threshold of facts to consider. We compute pre-
cision based on a sample of 50 randomly chosen
facts for each k. Table 7 shows the precision re-
sults, along with the number of distinct attributes
(#Attr) for which values are extracted at each k.
As we can see, ReNoun is capable of generat-
ing a large number of high quality facts (?70%
precise at 1M), which our scoring method man-
ages to successfully surface to the top. The ma-
jor sources of error were (i) incorrect dependency
parsing mainly due to errors in boilerplate text re-
moval from news documents, (ii) incorrect coref-
erence resolution of pronouns, (iii) incorrect entity
resolution against Freebase, and (iv) cases where
a triple is not sufficient (e.g., ambassador where
both arguments are countries.)
7.7 Missed extractions
We analyze why ReNoun does not extract facts for
certain attributes. For FH, we investigate all the 77
attributes for which ReNoun is missing facts. For
LT, there are about 50K attributes without corre-
sponding facts, and we use a random sample of
100 of those attributes.
Cause FH LT Example
Vague 23 37 culture
Numeric 4 26 rainfall
Object not KB entity 11 6 email
Plural 30 15 member firms
Bad attribute / misspell 3 4 newsies
Value expected 6 12 nationality
Total 77 100
Table 8: Analysis of attributes with no extractions.
333
Table 8 shows the categorization of the missed
attributes. The first three categories are cases that
are currently outside the scope of ReNoun: vague
attributes whose values are long narratives, nu-
meric attributes, and typed attributes (e.g., email)
whose values are not modeled as Freebase enti-
ties. The next two categories are due to limitations
of the ontology, e.g., plural forms of attributes are
not always synonymized with singular forms and
some attributes are bad. Finally, the ?Value ex-
pected? category contains the attributes for which
ReNoun should have extracted values. One reason
for missing values is that the corpus itself does not
contain values of all attributes. Another reason is
that some attributes are not verbalized in text. For
example, attributes like nationality are usually
not explicitly stated when expressed in text.
8 Related Work
Open information extraction (OIE) was introduced
by Banko et al. (2007). For a pair of noun phrases,
their system, TEXTRUNNER, looks for the at-
tribute (or more generally the relation) in the text
between them and uses a classifier to judge the
trustworthiness of an extraction. WOE
parse
(Wu
and Weld, 2010) extends this by using dependency
parsing to connect the subject and object. Both
systems assume that the attribute is between its
two arguments, an assumption that ReNoun drops
since it is not suitable for nominal attributes.
Closest to our work are ReVerb (Fader et al.,
2011) and Ollie (Mausam at al., 2012). ReVerb
uses POS tag patterns to locate verb relations and
then looks at noun phrases to the left and right for
arguments. Ollie uses the ReVerb extractions as
its seeds to train patterns that can further extract
triples. While Ollie?s patterns themselves are not
limited to verb relations (they also support noun
relations), the ReVerb seeds are limited to verbs,
which makes Ollie?s coverage on noun relations
also limited. In comparison, ReNoun take a noun-
centric approach and extracts many facts that do
not exist in Ollie.clo
ClausIE (Del Corro and Gemulla, 2013) is an
OIE framework that exploits knowledge about the
grammar of the English language to find clauses
in a sentence using its dependency parse. The
clauses are subsequently used to generate extrac-
tions at multiple granularities, possibly with more
than triples. While ClausIE comes with a prede-
fined set of rules on how to extract facts from a
dependency parse, ReNoun learns such rules from
its seed facts.
Finally, Nakashole et al. (2014) and Mintz et al.
(2009) find additional facts for attributes that al-
ready have facts in a knowledge base. In contrast,
ReNoun is an OIE framework whose goal is to find
facts for attributes without existing facts.
9 Conclusions
We described ReNoun, an open information ex-
traction system for nominal attributes that focuses
on the long tail. The key to our approach is to start
from a large ontology of nominal attributes and ap-
ply noun-specific manual patterns on a large pre-
processed corpus (via standard NLP components)
to extract precise seed facts. We then learn a set of
dependency patterns, which are used to generate a
much larger set of candidate facts. We proposed a
scoring function for filtering candidate facts based
on pattern frequency and coherence. We demon-
strated that the majority of long tail attributes in
ReNoun do not have corresponding verbs in Ol-
lie. Finally, our experiments show that our scor-
ing function is effective in filtering candidate facts
(top-1M facts are ?70% precise).
In the future, we plan to extend ReNoun to ex-
tract triples whose components are not limited to
Freebase IDs. As an example, extending ReNoun
to handle numerical or typed attributes would in-
volve extending our extraction pattern learning
to accommodate units (e.g., kilograms) and other
special data formats (e.g., addresses).
Acknowledgments
We would like to thank Luna Dong, Anjuli Kan-
nan, Tara McIntosh, and Fei Wu for many discus-
sions about the paper.
334
References
S?oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary G. Ives
2007. DBpedia: A Nucleus for a Web of Open Data.
In Proceedings of the International Semantic Web
Conference.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni 2007.
Open Information Extraction from the Web. In Pro-
ceedings of the International Joint Conference on
Artificial Intelligence .
Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a Col-
laboratively Created Graph Database for Structuring
Human Knowledge. In Proceedings of the Interna-
tional Conference on Management of Data.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang,
Eugene Wu, and Yang Zhang 2008. WebTables:
Exploring the Power of Tables on the Web. In Pro-
ceedings of the VLDB Endowment.
Luciano Del Corro and Rainer Gemulla. 2013.
ClausIE: Clause-based Open Information Extrac-
tion. In Proceedings of the International World Wide
Web Conference.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning 2006. Generating Typed
Dependency Parses from Phrase Structure Parses.
In Proceedings of Language Resources and Evalu-
ation.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying Relations for Open Information
Extraction. In Proceedings of Empirical Methods in
Natural Language Processing.
Rahul Gupta, Alon Halevy, Xuezhi Wang, Steven
Whang, and Fei Wu. 2014. Biperpedia: An On-
tology for Search Applications. In Proceedings of
the VLDB Endowment.
Aria Haghighi and Dan Klein 2009. Simple Corefer-
ence Resolution with Rich Syntactic and Semantic
Features. In Proceedings of Empirical Methods in
Natural Language Processing.
Richard Johansson and Pierre Nugues 2008. The Ef-
fect of Syntactic Representation on Semantic Role
Labeling. In Proceedings of the International Con-
ference on Computational Linguistics.
Taesung Lee, Zhongyuan Wang, Haixun Wang, and
Seung-won Hwang 2013. Attribute Extraction and
Scoring: A Probabilistic Approach. In Proceedings
of the International Conference on Data Engineer-
ing .
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open Language
Learning for Information Extraction. In Proceed-
ings of Empirical Methods in Natural Language
Processing.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Repre-
sentations in Vector Space. arXiv.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant Supervision for Relation Ex-
traction Without Labeled Data. In Proceedings of
the Association for Computational Linguistics.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. Annotating Noun Ar-
gument Structure for NomBank. In Proceedings of
Language Resources and Evaluation.
Nitin Madnani and Bonnie J. Dorr. 2010. Generat-
ing Phrasal and Sentential Paraphrases: A Survey of
Data-Driven Methods. In Computational Linguis-
tics 36(3).
Ndapandula Nakashole, Martin Theobald, and Gerhard
Weikum. 2011. Scalable Knowledge Harvesting
with High Precision and High Recall. In Proceed-
ings of Web Search and Data Mining.
Marius Pasca. 2014. Acquisition of Open-domain
Classes via Intersective Semantics. In Proceedings
of the International World Wide Web Conference.
Marius Pasca and Benjamin Van Durme. 2007. What
You Seek Is What You Get: Extraction of Class At-
tributes from Query Logs. In Proceedings of the
International Joint Conference on Artificial Intelli-
gence .
Uma Sawant and Soumen Chakrabarti. 2013. Learn-
ing Joint Query Interpretation and Response Rank-
ing. In Proceedings of the International World Wide
Web Conference.
Amit Singhal. 2012 Introducing the
Knowledge Graph: things, not strings
http://googleblog.blogspot.com/2012/
05/introducing-knowledge-graph-
things-not.html
Fei Wu and Daniel S. Weld. 2010. Open Information
Extraction Using Wikipedia. In Proceedings of the
the Association for Computational Linguistics.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural Language Questions for the
Web of Data. In Proceedings of Empirical Methods
in Natural Language Processing.
335
