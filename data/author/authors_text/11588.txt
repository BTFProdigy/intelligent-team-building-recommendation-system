Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 406?414,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using a maximum entropy model to build segmentation lattices for MT
Chris Dyer
Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland
College Park, MD 20742, USA
redpony AT umd.edu
Abstract
Recent work has shown that translating seg-
mentation lattices (lattices that encode alterna-
tive ways of breaking the input to an MT sys-
tem into words), rather than text in any partic-
ular segmentation, improves translation qual-
ity of languages whose orthography does not
mark morpheme boundaries. However, much
of this work has relied on multiple segmenters
that perform differently on the same input to
generate sufficiently diverse source segmen-
tation lattices. In this work, we describe a
maximum entropy model of compound word
splitting that relies on a few general features
that can be used to generate segmentation lat-
tices for most languages with productive com-
pounding. Using a model optimized for Ger-
man translation, we present results showing
significant improvements in translation qual-
ity in German-English, Hungarian-English,
and Turkish-English translation over state-of-
the-art baselines.
1 Introduction
Compound words pose significant challenges to the
lexicalized models that are currently common in sta-
tistical machine translation. This problem has been
widely acknowledged, and the conventional solu-
tion, which has been shown to work well for many
language pairs, is to segment compounds into their
constituent morphemes using either morphological
analyzers or empirical methods and then to trans-
late from or to this segmented variant (Koehn et al,
2008; Dyer et al, 2008; Yang and Kirchhoff, 2006).
But into what units should a compound word be
segmented? Taken as a stand-alone task, the goal of
a compound splitter is to produce a segmentation for
some input that matches the linguistic intuitions of a
native speaker of the language. However, there are
often advantages to using elements larger than sin-
gle morphemes as the minimal lexical unit for MT,
since they may correspond more closely to the units
of translation. Unfortunately, determining the op-
timal segmentation is challenging, typically requir-
ing extensive experimentation (Koehn and Knight,
2003; Habash and Sadat, 2006; Chang et al, 2008).
Recent work has shown that by combining a vari-
ety of segmentations of the input into a segmentation
lattice and effectively marginalizing over many dif-
ferent segmentations, translations superior to those
resulting from any single single segmentation of the
input can be obtained (Xu et al, 2005; Dyer et al,
2008; DeNeefe et al, 2008). Unfortunately, this ap-
proach is difficult to utilize because it requires mul-
tiple segmenters that behave differently on the same
input.
In this paper, we describe a maximum entropy
word segmentation model that is trained to assign
high probability to possibly several segmentations of
an input word. This model enables generation of di-
verse, accurate segmentation lattices from a single
model that are appropriate for use in decoders that
accept word lattices as input, such as Moses (Koehn
et al, 2007). Since our model relies a small num-
ber of dense features, its parameters can be tuned
using very small amounts of manually created ref-
erence lattices. Furthermore, since these parame-
ters were chosen to have valid interpretation across
a variety of languages, we find that the weights esti-
mated for one apply quite well to another. We show
that these lattices significantly improve translation
quality when translating into English from three lan-
guages exhibiting productive compounding: Ger-
man, Turkish, and Hungarian.
The paper is structured as follows. In the next sec-
406
tion, we describe translation from segmentation lat-
tices and give a motivating example, Section 3 de-
scribes our segmentation model and its tuning and
how it is used to generate segmentation lattices, Sec-
tion 5 presents experimental results, Section 6 re-
views relevant related work, and in Section 7 we
conclude and discuss future work.
2 Segmentation lattice translation
In this section we give a brief overview of lattice
translation and then describe the characteristics of
segmentation lattices that are appropriate for trans-
lation.
2.1 Lattice translation
Word lattices have been used to represent ambiguous
input to machine translation systems for a variety of
tasks, including translating automatic speech recog-
nition transcriptions and translating from morpho-
logically complex languages (Bertoldi et al, 2007;
Dyer et al, 2008). The intuition behind using lat-
tices in both approaches is to avoid the error propa-
gation effects that are found when a one-best guess
is used. By carrying a certain amount of uncertainty
forward in the processing pipeline, information con-
tained in the translation models can be leveraged to
help resolve the upstream ambiguity. In our case, we
want to propagate uncertainty about the proper seg-
mentation of a compound forward to the decoder,
which can use its full translation model to select
proper segmentation for translation. Mathemati-
cally, this can be understood as follows: whereas the
goal in conventional machine translation is to find
the sentence e?I1 that maximizes Pr(eI1|fJ1 ), the lat-
tice adds a latent variable, the path f? from a des-
ignated start start to a designated goal state in the
lattice G:
e?I1 = arg maxeI1
Pr(eI1|G) (1)
= arg max
eI1
?
f??G
Pr(eI1|f?)Pr(f? |G) (2)
? arg max
eI1
max
f??G
Pr(eI1|f?)Pr(f? |G) (3)
If the transduction formalism used is a synchronous
probabilistic context free grammar or weighted finite
tonband aufnahme
ton
band
auf
nahme
tonbandaufnahme
wieder aufnahme
wie
der
auf
nahme
wiederaufnahme
Figure 1: Segmentation lattice examples. The dotted
structure indicates linguistically implausible segmenta-
tion that might be generated using dictionary-driven ap-
proaches.
state transducer, the search represented by equation
(3) can be carried out efficiently using dynamic pro-
gramming (Dyer et al, 2008).
2.2 Segmentation lattices
Figure 1 shows two lattices that encode the
most linguistically plausible ways of segment-
ing two prototypical German compounds with
compositional meanings. However, while these
words are structurally quite similar, translating
them into English would seem to require differ-
ent amounts of segmentation. For example, the
dictionary fragment shown in Table 1 illustrates
that tonbandaufnahme can be rendered into En-
glish by following 3 different paths in the lat-
tice, ton/audio band/tape aufnahme/recording, ton-
band/tape aufnahme/recording, and tonbandauf-
nahme/tape recording. In contrast, wiederaufnahme
can only be translated correctly using the unseg-
mented form, even though in German the meaning
of the full form is a composition of the meaning of
the individual morphemes.1
It should be noted that phrase-based models can
translate multiple words as a unit, and therefore cap-
ture non-compositional meaning. Thus, by default if
the training data is processed such that, for example,
aufnahme, in its sense of recording, is segmented
into two words, then more paths in the lattices be-
1The English word resumption is likewise composed of two
morphemes, the prefix re- and a kind of bound morpheme
that never appears in other contexts (sometimes called a ?cran-
berry? morpheme), but the meaning of the whole is idiosyncratic
enough that it cannot be called compositional.
407
German English
auf on, up, in, at, ...
aufnahme recording, entry
band reel, tape, band
der the, of the
nahme took (3P-SG-PST)
ton sound, audio, clay
tonband tape, audio tape
tonbandaufnahme tape recording
wie how, like, as
wieder again
wiederaufnahme resumption
Table 1: German-English dictionary fragment for words
present in Figure 1.
come plausible translations. However, using a strat-
egy of ?over segmentation? and relying on phrase
models to learn the non-compositional translations
has been shown to degrade translation quality sig-
nificantly on several tasks (Xu et al, 2004; Habash
and Sadat, 2006). We thus desire lattices containing
as little oversegmentation as possible.
We have now have a concept of a ?gold standard?
segmentation lattice for translation: it should con-
tain all linguistically motivated segmentations that
also correspond to plausible word-for-word transla-
tions into English. Figure 2 shows an example of the
reference lattice for the two words we just discussed.
For the experiments in this paper, we generated a
development and test set by randomly choosing 19
German newspaper articles, identifying all words
greater than 6 characters is length, and segmenting
each word so that the resulting units could be trans-
lated compositionally into English. This resulted in
489 training sentences corresponding to 564 paths
for the dev set (which was drawn from 15 articles),
and 279 words (302 paths) for the test set (drawn
from the remaining 4 articles).
3 A maximum entropy segmentation
model
We now turn to the problem of modeling word seg-
mentation in a way that facilitates lattice construc-
tion. As a starting point, we consider the work
of Koehn and Knight (2003) who observe that in
most languages that exhibit compounding, the mor-
tonband aufnahme
ton
band
wiederaufnahme
Figure 2: Manually created reference lattices for the two
words from Figure 1. Although only a subset of all
linguistically plausible segmentations, each path corre-
sponds to a plausible segmentation for word-for-word
German-English translation.
phemes used to construct compounds frequently
also appear as individual tokens. Based on this ob-
servation, they propose a model of word segmenta-
tion that splits compound words into pieces found
in the dictionary based on a variety heuristic scoring
criteria. While these models have been reasonably
successful (Koehn et al, 2008), they are problem-
atic for two reasons. First, there is no principled way
to incorporate additional features (such as phonotac-
tics) which might be useful to determining whether
a word break should occur. Second, the heuristic
scoring offers little insight into which segmentations
should be included in a lattice.
We would like our model to consider a wide vari-
ety of segmentations of any word (including perhaps
hypothesized morphemes that are not in the dictio-
nary), to make use of a rich set of features, and to
have a probabilistic interpretation of each hypothe-
sized split (to incorporate into the downstream de-
coder). We decided to use the class of maximum
entropy models, which are probabilistically sound,
can make use of possibly many overlapping features,
and can be trained efficiently (Berger et al, 1996).
We thus define a model of the conditional proba-
bility distribution Pr(sN1 |w), where w is a surface
form and sN1 is the segmented form consisting of N
segments as:
Pr(sN1 |w) = exp
?
i ?ihi(sN1 , w)?
s? exp
?
i ?ihi(s?, w)
(4)
To simplify inference and to make the lattice repre-
sentation more natural, we only make use of local
feature functions that depend on properties of each
segment:
408
Pr(sN1 |w) ? exp
?
i
?i
N?
j
hi(sj , w) (5)
3.1 From model to segmentation lattice
The segmentation model just introduced is equiva-
lent to a lattice where each vertex corresponds to
a particular coverage (in terms of letters consumed
from left to right) of the input word. Since we only
make use of local features, the number of vertices
in a lattice for word w is |w| ? m, where m is the
minimum segment length permitted. In all experi-
ments reported in this paper, we use m = 3. Each
edge is labeled with a morpheme s (corresponding
to the morpheme associated with characters delim-
ited by the start and end nodes of the edge) as well
as a weight, ?i ?ihi(s, w). The cost of any path
from the start to the goal vertex will be equal to the
numerator in equation (4). The value of the denomi-
nator can be computed using the forward algorithm.
In most of our experiments, s will be identical
to the substring of w that the edge is designated to
cover. However, this is not a requirement. For exam-
ple, German compounds frequently have so-called
Fugenelemente, one or two characters that ?glue
together? the primary morphemes in a compound.
Since we permit these characters to be deleted, then
an edge where they are deleted will have fewer char-
acters than the coverage indicated by the edge?s
starting and ending vertices.
3.2 Lattice pruning
Except for the minimum segment length restriction,
our model defines probabilities for all segmentations
of an input word, making the resulting segmenta-
tion lattices are quite large. Since large lattices
are costly to deal with during translation (and may
lead to worse translations because poor segmenta-
tions are passed to the decoder), we prune them us-
ing forward-backward pruning so as to contain just
the highest probability paths (Sixtus and Ortmanns,
1999). This works by computing the score of the
best path passing through every edge in the lattice
using the forward-backward algorithm. By finding
the best score overall, we can then prune edges us-
ing a threshold criterion; i.e., edges whose score is
some factor ? away from the global best edge score.
3.3 Maximum likelihood training
Our model defines a conditional probability distribu-
tion over virtually all segmentations of a word w. To
train our model, we wish to maximize the likelihood
of the segmentations contained in the reference lat-
tices by moving probability mass away from the seg-
mentations that are not in the reference lattice. Thus,
we wish to minimize the following objective (which
can be computed using the forward algorithm over
the unpruned hypothesis lattices):
L = ? log?
i
?
s?Ri
p(s|wi) (6)
The gradient with respect to the feature weights for
a log linear model is simply:
?L
??k
= ?
i
Ep(s|wi)[hk]? Ep(s|wi,Ri)[hk] (7)
To compute these values, the first expectation is
computed using forward-backward inference over
the full lattice. To compute the second expecta-
tion, the full lattice is intersected with the reference
lattice Ri, and then forward-backward inference
is redone.2 We use the standard quasi-Newtonian
method L-BFGS to optimize the model (Liu et al,
1989). Training generally converged in only a few
hundred iterations.
3.3.1 Training to minimize 1-best error
In some cases, such as when performing word
alignment for translation model construction, lat-
tices cannot be used easily. In these cases, a 1-
best segmentation (which can be determined from
the lattice using the Viterbi algorithm) may be de-
sired. To train the parameters of the model for this
condition (which is arguably slightly different from
the lattice generation case we just considered), we
used the minimum error training (MERT) algorithm
on the segmentation lattices to find the parameters
that minimized the error on our dev set (Macherey
2The second expectation corresponds to the empirical fea-
ture observations in a standard maximum entropy model. Be-
cause this is an expectation and not an invariant observation,
the log likelihood function is not guaranteed to be concave and
the objective surface may have local minima. However, exper-
imentation revealed the optimization performance was largely
invariant with respect to its starting point.
409
et al, 2008). The error function we used was WER
(the minimum number of insertions, substitutions,
and deletions along any path in the reference lattice,
normalized by the length of this path). The WER on
the held-out test set for a system tuned using MERT
is 9.9%, compared to 11.1% for maximum likeli-
hood training.
3.4 Features
We remark that since we did not have the resources
to generate training data in all the languages we
wished to generate segmentation lattices for, we
have confined ourselves to features that we expect to
be reasonably informative for a broad class of lan-
guages. A secondary advantage of this is that we
used denser features than are often used in maxi-
mum entropy modeling, meaning that we could train
our model with relatively less training data than
might otherwise be required.
The features we used in our compound segmen-
tation model for the experiments reported below are
shown in Table 2. Building on the prior work that
relied heavily on the frequency of the hypothesized
constituent morphemes in a monolingual corpus, we
included features that depend on this value, f(si).
|si| refers to the number of letters in the ith hypothe-
sized segment. Binary predicates evaluate to 1 when
true and 0 otherwise. f(si) is the frequency of the
token si as an independent word in a monolingual
corpus. p(#|si1 ? ? ? si4) is the probability of a word
start preceding the letters si1 ? ? ? si4. We found it
beneficial to include a feature that was the probabil-
ity of a certain string of characters beginning a word,
for which we used a reverse 5-gram character model
and predicted the word boundary given the first five
letters of the hypothesized word split.3 Since we did
have expertise in German morphology, we did build
a special German model. For this, we permitted the
strings s, n, and es to be deleted between words.
Each deletion fired a count feature (listed as fugen
in the table). Analysis of errors indicated that the
segmenter would periodically propose an incorrect
segmentation where a single word could be divided
into a word and a nonword consisting of common in-
3In general, this helped avoid situations where a word may
be segemented into a frequent word and then a non-word string
of characters since the non-word typically violated the phono-
tactics of the language in some way.
Feature de-only neutral
?si ? N -3.55 ?
f(si) > 0.005 -3.13 -3.31
f(si) > 0 3.06 3.64
log p(#|si1si2si3si4) -1.58 -2.11
segment penalty 1.18 2.04
|si| ? 12 -0.9 -0.79
oov -0.88 -1.09
?fugen -0.76 ?
|si| ? 4 -0.66 -1.18
|si| ? 10, f(si) > 2?10 -0.51 -0.82
log f(si) -0.32 -0.36
2?10 < f(si) < 0.005 -0.26 -0.45
Table 2: Features and weights learned by maximum like-
lihood training, sorted by weight magnitude.
flectional suffixes. To address this, an additional fea-
ture was added that fired when a proposed segment
was one of a setN of 30 nonwords that we saw quite
frequently. The weights shown in Table 2 are those
learned by maximum likelihood training on models
both with and without the special German features,
which are indicated with ?.
4 Model evalatuion
To give some sense of the performance of the model
in terms of its ability to generate lattices indepen-
dently of a translation task, we present precision and
recall of segmentations for pruning parameters (cf.
Section 3.2) ranging from ? = 0 to ? = 5. Pre-
cision measures the number of paths in the hypoth-
esized lattice that correspond to paths in the refer-
ence lattice; recall measures the number of paths in
the reference lattices that are found in the hypothesis
lattice. Figure 3 shows the effect of manipulating the
density parameter on the precision and recall of the
German lattices. Note that very high recall is possi-
ble; however, the German-only features have a sig-
nificant impact, especially on recall, because the ref-
erence lattices include paths where Fugenelemente
have been deleted.
5 Translation experiments
We now review experiments using segmentation lat-
tices produced by the segmentation model we just
introduced in German-English, Hungarian-English,
410
 0.92
 0.93
 0.94
 0.95
 0.96
 0.97
 0.98
 0.99
 1
 0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1
Reca
ll
Precision
MLMERTML, no special German
Figure 3: The effect of the lattice density parameter on
precision and recall.
and Turkish-English translation tasks and then show
results elucidating the effect of the lattice density pa-
rameter. We begin with a description of our MT sys-
tem.
5.1 Data preparation and system description
For all experiments, we used a 5-gram English lan-
guage model trained on the AFP and Xinua por-
tions of the Gigaword v3 corpus (Graff et al, 2007)
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995). The training, development, and test
data for German-English and Hungarian-English
systems used were distributed as part of the 2009
EACL Workshop on Machine Translation,4 and the
Turkish-English data corresponds to the training and
test sets used in the work of Oflazer and Durgar El-
Kahlout (2007). Corpus statistics for all language
pairs are summarized in Table 3. We note that in all
language pairs, the 1BEST segmentation variant of
the training data results in a significant reduction in
types.
Word alignment was carried out by running
Giza++ implementation of IBM Model 4 initialized
with 5 iterations of Model 1, 5 of the HMM aligner,
and 3 iterations of Model 4 (Och and Ney, 2003)
in both directions and then symmetrizing using the
grow-diag-final-and heuristic (Koehn et al,
2003). For each language pair, the corpus was
aligned twice, once in its non-segmented variant and
once using the single-best segmentation variant.
For translation, we used a bottom-up parsing de-
coder that uses cube pruning to intersect the lan-
4http://www.statmt.org/wmt09
guage model with the target side of the synchronous
grammar. The grammar rules were extracted from
the word aligned parallel corpus and scored as de-
scribed in Chiang (2007). The features used by the
decoder were the English language model log prob-
ability, log f(e?|f?), the ?lexical translation? log prob-
abilities in both directions (Koehn et al, 2003), and
a word count feature. For the lattice systems, we
also included the unnormalized log p(f? |G), as it is
defined in Section 3, as well as an input word count
feature. The feature weights were tuned on a held-
out development set so as to maximize an equally
weighted linear combination of BLEU and 1-TER
(Papineni et al, 2002; Snover et al, 2006) using the
minimum error training algorithm on a packed for-
est representation of the decoder?s hypothesis space
(Macherey et al, 2008). The weights were indepen-
dently optimized for each language pair and each ex-
perimental condition.
5.2 Segmentation lattice results
In this section, we report the results of an experiment
to see if the compound lattices constructed using our
maximum entropy model yield better translations
than either an unsegmented baseline or a baseline
consisting of a single-best segmentation.
For each language pair, we define three condi-
tions: BASELINE, 1BEST, and LATTICE. In the
BASELINE condition, a lowercased and tokenized
(but not segmented) version of the test data is
translated using the grammar derived from a non-
segmented training data. In the 1BEST condition,
the single best segmentation s?N1 that maximizes
Pr(sN1 |w) is chosen for each word using the MERT-
trained model (the German model for German, and
the language-neutral model for Hungarian and Turk-
ish). This variant is translated using a grammar
induced from a parallel corpus that has also been
segmented according to the same decision rule. In
the LATTICE condition, we constructed segmenta-
tion lattices using the technique described in Sec-
tion 3.1. For all languages pairs, we used d = 2 as
the pruning density parameter (which corresponds to
the highest F-score on the held out test set). Addi-
tionally, if the unsegmented form of the word was
removed from the lattice during pruning, it was re-
stored to the lattice with zero weight.
Table 4 summarizes the results of the translation
411
f -tokens f -types e-tokens. e-types
DE-BASELINE 38M 307k 40M 96k
DE-1BEST 40M 136k ? ?
HU-BASELINE 25M 646k 29M 158k
HU-1BEST 27M 334k ? ?
TR-BASELINE 1.0M 56k 1.3M 23k
TR-1BEST 1.1M 41k ? ?
Table 3: Training corpus statistics.
BLEU TER
DE-BASELINE 21.0 60.6
DE-1BEST 20.7 60.1
DE-LATTICE 21.6 59.8
HU-BASELINE 11.0 71.1
HU-1BEST 10.7 70.4
HU-LATTICE 12.3 69.1
TR-BASELINE 26.9 61.0
TR-1BEST 27.8 61.2
TR-LATTICE 28.7 59.6
Table 4: Translation results for German (DE)-English,
Hungarian (HU)-English, and Turkish (TR)-English.
Scores were computed using a single reference and are
case insensitive.
experiments comparing the three input variants. For
all language pairs, we see significant improvements
in both BLEU and TER when segmentation lattices
are used.5 Additionally, we also confirmed previous
findings that showed that when a large amount of
training data is available, moving to a one-best seg-
mentation does not yield substantial improvements
(Yang and Kirchhoff, 2006). Perhaps most surpris-
ingly, the improvements observed when using lat-
tices with the Hungarian and Turkish systems were
larger than the corresponding improvement in the
German system, but German was the only language
for which we had segmentation training data. The
smaller effect in German is probably due to there be-
ing more in-domain training data in the German sys-
tem than in the (otherwise comparably sized) Hun-
garian system.
5Using bootstrap resampling (Koehn, 2004), the improve-
ments in BLEU, TER, as well as the linear combination used in
tuning are statistically significant at at least p < .05.
Targeted analysis of the translation output shows
that while both the 1BEST and LATTICE systems
generally produce adequate translations of com-
pound words that are out of vocabulary in the BASE-
LINE system, the LATTICE system performs bet-
ter since it recovers from infelicitous splits that the
one-best segmenter makes. For example, one class
of error we frequently observe is that the one-best
segmenter splits an OOV proper name into two
pieces when a portion of the name corresponds to a
known word in the source language (e.g. tom tan-
credo?tom tan credo which is then translated as
tom tan belief ).6
5.3 The effect of the density parameter
Figure 4 shows the effect of manipulating the den-
sity parameter (cf. Section 3.2) on the performance
and decoding time of the Turkish-English transla-
tion system. It further confirms the hypothesis that
increased diversity of segmentations encoded in a
segmentation lattice can improve translation perfor-
mance; however, it also shows that once the den-
sity becomes too great, and too many implausible
segmentations are included in the lattice, translation
quality will be harmed.
6 Related work
Aside from improving the vocabulary coverage of
machine translation systems (Koehn et al, 2008;
Yang and Kirchhoff, 2006; Habash and Sadat,
2006), compound word segmentation (also referred
to as decompounding) has been shown to be help-
ful in a variety of NLP tasks including mono- and
6We note that our maximum entropy segmentation model
could easily address this problem by incorporating information
about whether a word is likely to be a named entity as a feature.
412
 84
 84.2
 84.4
 84.6
 84.8
 85
 1  1.5  2  2.5  3  3.5
 2
 4
 6
 8
 10
 12
 14
 16
1-(TE
R-BLE
U)/2
sec
s/sen
tence
Segmentation lattice density
Translation qualityDecoding time
Figure 4: The effect of the lattice density parameter on
translation quality and decoding time.
crosslingual IR (Airio, 2006) and speech recognition
(Hessen and Jong, 2003). A number of researchers
have demonstrated the value of using lattices to en-
code segmentation alternatives as input to a machine
translation system (Dyer et al, 2008; DeNeefe et al,
2008; Xu et al, 2004), but this is the first work to
do so using a single segmentation model. Another
strand of inquiry that is closely related is the work on
adjusting the source language segmentation to match
the granularity of the target language as a way of im-
proving translation. The approaches suggested thus
far have been mostly of a heuristic nature tailored to
Chinese-English translation (Bai et al, 2008; Ma et
al., 2007).
7 Conclusions and future work
In this paper, we have presented a maximum entropy
model for compound word segmentation and used it
to generate segmentation lattices for input into a sta-
tistical machine translation system. These segmen-
tation lattices improve translation quality (over an
already strong baseline) in three typologically dis-
tinct languages (German, Hungarian, Turkish) when
translating into English. Previous approaches to
generating segmentation lattices have been quite la-
borious, relying either on the existence of multiple
segmenters (Dyer et al, 2008; Xu et al, 2005) or
hand-crafted rules (DeNeefe et al, 2008). Although
the segmentation model we propose is discrimina-
tive, we have shown that it can be trained using a
minimal amount of annotated training data. Further-
more, when even this minimal data cannot be ac-
quired for a particular language (as was the situa-
tion we faced with Hungarian and Turkish), we have
demonstrated that the parameters obtained in one
language work surprisingly well for others. Thus,
with virtually no cost, this model can be used with a
variety of diverse languages.
While these results are already quite satisfying,
there are a number of compelling extensions to this
work that we intend to explore in the future. First,
unsupervised segmentation approaches offer a very
compelling alternative to the manually crafted seg-
mentation lattices that we created. Recent work
suggests that unsupervised segmentation of inflec-
tional affixal morphology works quite well (Poon et
al., 2009), and extending this work to compounding
morphology should be feasible, obviating the need
for expensive hand-crafted reference lattices. Sec-
ond, incorporating target language information into
a segmentation model holds considerable promise
for inducing more effective translation models that
perform especially well for segmentation lattice in-
puts.
Acknowledgments
Special thanks to Kemal Oflazar and Reyyan Yen-
iterzi of Sabanc? University for providing the
Turkish-English corpus and to Philip Resnik, Adam
Lopez, Trevor Cohn, and especially Phil Blunsom
for their helpful suggestions. This research was sup-
ported by the Army Research Laboratory. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do
not necessarily reflect the view of the sponsors.
References
Eija Airio. 2006. Word normalization and decompound-
ing in mono- and bilingual IR. Information Retrieval,
9:249?271.
Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.
2008. Improving word alignment by adjusting Chi-
nese word segmentation. In Proceedings of the Third
International Joint Conference on Natural Language
Processing.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech
413
translation by confusion network decoding. In Pro-
ceeding of ICASSP 2007, Honolulu, Hawaii, April.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Man-
ning. 2008. Optimizing Chinese word segmentation
for machine translation performance. In Proceedings
of the Third Workshop on Statistical Machine Transla-
tion, Prague, Czech Republic, June.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
S. DeNeefe, U. Hermjakob, and K. Knight. 2008. Over-
coming vocabulary sparsity in mt using lattices. In
Proceedings of AMTA, Honolulu, HI.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proceedings of HLT-ACL.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2007. English
gigaword third edition.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
Arjan Van Hessen and Franciska De Jong. 2003. Com-
pound decomposition in dutch large vocabulary speech
recognition. In Proceedings of Eurospeech 2003, Gen-
eve, pages 225?228.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of IEEE
Internation Conference on Acoustics, Speech, and Sig-
nal Processing, pages 181?184.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proc. of the EACL 2003.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
2003, pages 48?54, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Annual Meeting
of the Association for Computation Linguistics (ACL),
Demonstration Session, pages 177?180, June.
Philipp Koehn, Abhishek Arun, and Hieu Hoang. 2008.
Towards better machine translation quality for the
German-English language pairs. In ACL Workshop on
Statistical Machine Translation.
P. Koehn. 2004. Statistical signficiance tests for machine
translation evluation. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 388?395.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory BFGS method
for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 304?311,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proceedings of EMNLP, Honolulu, HI.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proceedings
of the Second Workshop on Statistical Machine Trans-
lation, pages 25?32, Prague, Czech Republic, June.
Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 311?318.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proc. of NAACL 2009.
S. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Proceed-
ings of ICASSP, Phoenix, AZ.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas.
J. Xu, R. Zens, and H. Ney. 2004. Do we need Chi-
nese word segmentation for statistical machine trans-
lation? In Proceedings of the Third SIGHAN Work-
shop on Chinese Language Learning, pages 122?128,
Barcelona, Spain.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005, Pittsburgh.
M. Yang and K. Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected
languages. In Proceedings of the EACL 2006, pages
41?48.
414
Proceedings of NAACL HLT 2009: Tutorials, pages 1?2,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Data?Intensive?Text?Processing?with?MapReduce?
?
Jimmy?Lin?and?Chris?Dyer?
University?of?Maryland,?College?Park?
{jimmylin,redpony}@umd.edu?
Overview?
This?half?day?tutorial?introduces?participants?to?data?intensive?text?processing?with?the?MapReduce?
programming?model?[1],?using?the?open?source?Hadoop?implementation.?The?focus?will?be?on?scalability?
and?the?tradeoffs?associated?with?distributed?processing?of?large?datasets.?Content?will?include?general?
discussions?about?algorithm?design,?presentation?of?illustrative?algorithms,?case?studies?in?HLT?
applications,?as?well?as?practical?advice?in?writing?Hadoop?programs?and?running?Hadoop?clusters.?
Amazon?has?generously?agreed?to?provide?each?participant?with?$100?in?Amazon?Web?Services?(AWS)?
credits?that?can?used?toward?its?Elastic?Compute?Cloud?(EC2)??utility?computing??service?(sufficient?for?
1000?instance?hours).?EC2?allows?anyone?to?rapidly?provision?Hadoop?clusters??on?the?fly??without?
upfront?hardware?investments,?and?provides?a?low?cost?vehicle?for?exploring?Hadoop.?
Intended?Audience?
The?tutorial?is?targeted?at?any?NLP?researcher?interested?in?data?intensive?processing?and?scalability?
issues?in?general.?No?background?in?parallel?or?distributed?computing?is?necessary,?but?a?prior?knowledge?
of?HLT?is?assumed.?
Course?Objectives?
? Acquire?understanding?of?the?MapReduce?programming?model?and?how?it?relates?to?alternative?
approaches?to?concurrent?programming.?
? Acquire?understanding?of?how?data?intensive?HLT?problems?(e.g.,?text?retrieval,?iterative?
optimization?problems,?etc.)?can?be?solved?using?MapReduce.?
? Acquire?understanding?of?the?tradeoffs?involved?in?designing?MapReduce?algorithms?and?
awareness?of?associated?engineering?issues.?
Tutorial?Topics?
The?following?lists?topics?that?will?be?covered:?
? MapReduce?algorithm?design?
? Distributed?counting?applications?(e.g.,?relative?frequency?estimation)?
? Applications?to?text?retrieval?
? Applications?to?graph?algorithms?
? Applications?to?iterative?optimization?algorithms?(e.g.,?EM)?
? Practical?Hadoop?issues?
? Limitations?of?MapReduce?
Instructor?Bios?
Jimmy?Lin?is?an?assistant?professor?in?the?iSchool?at?the?University?of?Maryland,?College?Park.?He?joined?
the?faculty?in?2004?after?completing?his?Ph.D.?in?Electrical?Engineering?and?Computer?Science?at?MIT.?Dr.?
Lin?s?research?interests?lie?at?the?intersection?of?natural?language?processing?and?information?retrieval.?
1
He?leads?the?University?of?Maryland?s?effort?in?the?Google/IBM?Academic?Cloud?Computing?Initiative.?Dr.?
Lin?has?taught?two?semester?long?Hadoop?courses?[2]?and?has?given?numerous?talks?about?MapReduce?
to?a?wide?audience.?
Chris?Dyer?is?a?Ph.D.?student?at?the?University?of?Maryland,?College?Park,?in?the?Department?of?
Linguistics.??His?current?research?interests?include?statistical?machine?translation,?machine?learning,?and?
the?relationship?between?artificial?language?processing?systems?and?the?human?linguistic?processing?
system.?He?has?served?on?program?committees?for?AMTA,?ACL,?COLING,?EACL,?EMNLP,?NAACL,?ISWLT,?
and?the?ACL?Workshops?on?Machine?translation,?and?is?one?of?the?developers?of?the?Moses?open?source?
machine?translation?toolkit.?He?has?practical?experience?solving?NLP?problems?with?both?the?Hadoop?
MapReduce?framework?and?Google?s?MapReduce?implementation,?which?was?made?possible?by?an?
internship?with?Google?Research?in?2008.??
Acknowledgments?
This?work?is?supported?by?NSF?under?awards?IIS?0705832?and?IIS?0836560;?the?Intramural?Research?
Program?of?the?NIH,?National?Library?of?Medicine;?DARPA/IPTO?Contract?No.?HR0011?06?2?0001?under?
the?GALE?program.?Any?opinions,?findings,?conclusions,?or?recommendations?expressed?here?are?the?
instructors??and?do?not?necessarily?reflect?those?of?the?sponsors.?We?are?grateful?to?Amazon?for?its?
support?of?tutorial?participants.?
References?
[1]??Dean,?Jeffrey?and?Sanjay?Ghemawat.?MapReduce:?Simplified?Data?Processing?on?Large?Clusters.?
Proceedings?of?the?6th?Symposium?on?Operating?System?Design?and?Implementation?(OSDI?2004),?p.?
137?150,?2004,?San?Francisco,?California.?
[2]??Jimmy?Lin.?Exploring?Large?Data?Issues?in?the?Curriculum:?A?Case?Study?with?MapReduce.?
Proceedings?of?the?Third?Workshop?on?Issues?in?Teaching?Computational?Linguistics?(TeachCL?08)?at?
ACL?2008,?p.?54?61,?2008,?Columbus,?Ohio.?
?
2
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, June 2007. c?2007 Association for Computational Linguistics 
Moses: Open Source Toolkit for Statistical Machine Translation 
Philipp Koehn 
Hieu Hoang  
Alexandra Birch 
Chris Callison-Burch 
University of Edin-
burgh1 
Marcello Federico 
Nicola Bertoldi 
ITC-irst2 
Brooke Cowan 
Wade Shen 
Christine Moran 
MIT3 
Richard Zens 
RWTH Aachen4 
Chris Dyer 
University of Maryland5 
 
Ond?ej Bojar 
Charles University6 
Alexandra Constantin 
Williams College7 
Evan Herbst 
Cornell8 
1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk. 
2{federico, bertoldi}@itc.it. 3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu. 4 
zens@i6.informatik.rwth-aachen.de. 5 redpony@umd.edu. 6 bojar@ufal.ms.mff.cuni.cz. 7 
07aec_2@williams.edu. 8 evh4@cornell.edu 
 
Abstract 
We describe an open-source toolkit for sta-
tistical machine translation whose novel 
contributions are (a) support for linguisti-
cally motivated factors, (b) confusion net-
work decoding, and (c) efficient data for-
mats for translation models and language 
models. In addition to the SMT decoder, 
the toolkit also includes a wide variety of 
tools for training, tuning and applying the 
system to many translation tasks.  
1 Motivation 
Phrase-based statistical machine translation 
(Koehn et al 2003) has emerged as the dominant 
paradigm in machine translation research. How-
ever, until now, most work in this field has been 
carried out on proprietary and in-house research 
systems. This lack of openness has created a high 
barrier to entry for researchers as many of the 
components required have had to be duplicated. 
This has also hindered effective comparisons of the 
different elements of the systems. 
By providing a free and complete toolkit, we 
hope that this will stimulate the development of the 
field. For this system to be adopted by the commu-
nity, it must demonstrate performance that is com-
parable to the best available systems. Moses has 
shown that it achieves results comparable to the 
most competitive and widely used statistical ma-
chine translation systems in translation quality and 
run-time (Shen et al 2006). It features all the ca-
pabilities of the closed sourced Pharaoh decoder 
(Koehn 2004). 
Apart from providing an open-source toolkit 
for SMT, a further motivation for Moses is to ex-
tend phrase-based translation with factors and con-
fusion network decoding. 
The current phrase-based approach to statisti-
cal machine translation is limited to the mapping of 
small text chunks without any explicit use of lin-
guistic information, be it morphological, syntactic, 
or semantic. These additional sources of informa-
tion have been shown to be valuable when inte-
grated into pre-processing or post-processing steps. 
Moses also integrates confusion network de-
coding, which allows the translation of ambiguous 
input. This enables, for instance, the tighter inte-
gration of speech recognition and machine transla-
tion. Instead of passing along the one-best output 
of the recognizer, a network of different word 
choices may be examined by the machine transla-
tion system. 
Efficient data structures in Moses for the 
memory-intensive translation model and language 
model allow the exploitation of much larger data 
resources with limited hardware. 
177
 2 Toolkit 
The toolkit is a complete out-of-the-box trans-
lation system for academic research. It consists of 
all the components needed to preprocess data, train 
the language models and the translation models. It 
also contains tools for tuning these models using 
minimum error rate training (Och 2003) and evalu-
ating the resulting translations using the BLEU 
score (Papineni et al 2002).  
Moses uses standard external tools for some of 
the tasks to avoid duplication, such as GIZA++ 
(Och and Ney 2003) for word alignments and 
SRILM for language modeling.  Also, since these 
tasks are often CPU intensive, the toolkit has been 
designed to work with Sun Grid Engine parallel 
environment to increase throughput.  
In order to unify the experimental stages, a 
utility has been developed to run repeatable ex-
periments. This uses the tools contained in Moses 
and requires minimal changes to set up and cus-
tomize. 
The toolkit has been hosted and developed un-
der sourceforge.net since inception. Moses has an 
active research community and has reached over 
1000 downloads as of 1st March 2007.  
The main online presence is at  
http://www.statmt.org/moses/ 
where many sources of information about the 
project can be found. Moses was the subject of this 
year?s Johns Hopkins University Workshop on 
Machine Translation (Koehn et al 2006). 
The decoder is the core component of Moses. 
To minimize the learning curve for many research-
ers, the decoder was developed as a drop-in re-
placement for Pharaoh, the popular phrase-based 
decoder. 
In order for the toolkit to be adopted by the 
community, and to make it easy for others to con-
tribute to the project, we kept to the following 
principles when developing the decoder: 
? Accessibility 
? Easy to Maintain 
? Flexibility 
? Easy for distributed team development 
? Portability 
It was developed in C++ for efficiency and fol-
lowed modular, object-oriented design. 
3 Factored Translation Model 
Non-factored SMT typically deals only with 
the surface form of words and has one phrase table, 
as shown in Figure 1. 
i am buying you a green cat
using phrase dictionary:
i
 am buying
you
a
green
cat
je
ach?te
vous
un
vert
chat
a une
je vous ach?te un chat vert
Translate:
 
In factored translation models, the surface 
forms may be augmented with different factors, 
such as POS tags or lemma. This creates a factored 
representation of each word, Figure 2.  
1 1 1 / sing /
                                 
je vous achet un chat
PRO PRO VB ART NN
je vous acheter un chat
st st st present masc masc
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?
1 1 / 1 sing sing
i buy you a cat
PRO VB PRO ART NN
i tobuy you a cat
st st present st
? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?? ?  
 
Mapping of source phrases to target phrases 
may be decomposed into several steps. Decompo-
sition of the decoding process into various steps 
means that different factors can be modeled sepa-
rately. Modeling factors in isolation allows for 
flexibility in their application. It can also increase 
accuracy and reduce sparsity by minimizing the 
number dependencies for each step. 
For example, we can decompose translating 
from surface forms to surface forms and lemma, as 
shown in Figure 3. 
Figure 2. Factored translation 
Figure 1. Non-factored translation 
178
  
Figure 3. Example of graph of decoding steps 
By allowing the graph to be user definable, we 
can experiment to find the optimum configuration 
for a given language pair and available data.  
The factors on the source sentence are consid-
ered fixed, therefore, there is no decoding step 
which create source factors from other source fac-
tors. However, Moses can have ambiguous input in 
the form of confusion networks. This input type 
has been used successfully for speech to text 
translation (Shen et al 2006). 
Every factor on the target language can have its 
own language model. Since many factors, like 
lemmas and POS tags, are less sparse than surface 
forms, it is possible to create a higher order lan-
guage models for these factors. This may encour-
age more syntactically correct output. In Figure 3 
we apply two language models, indicated by the 
shaded arrows, one over the words and another 
over the lemmas. Moses is also able to integrate 
factored language models, such as those described 
in (Bilmes and Kirchhoff 2003) and (Axelrod 
2006). 
4 Confusion Network Decoding 
Machine translation input currently takes the 
form of simple sequences of words. However, 
there are increasing demands to integrate machine 
translation technology into larger information 
processing systems with upstream NLP/speech 
processing tools (such as named entity recognizers, 
speech recognizers, morphological analyzers, etc.). 
These upstream processes tend to generate multiple, 
erroneous hypotheses with varying confidence. 
Current MT systems are designed to process only 
one input hypothesis, making them vulnerable to 
errors in the input.  
In experiments with confusion networks, we 
have focused so far on the speech translation case, 
where the input is generated by a speech recog-
nizer. Namely, our goal is to improve performance 
of spoken language translation by better integrating 
speech recognition and machine translation models. 
Translation from speech input is considered more 
difficult than translation from text for several rea-
sons. Spoken language has many styles and genres, 
such as, formal read speech, unplanned speeches, 
interviews, spontaneous conversations; it produces 
less controlled language, presenting more relaxed 
syntax and spontaneous speech phenomena. Fi-
nally, translation of spoken language is prone to 
speech recognition errors, which can possibly cor-
rupt the syntax and the meaning of the input. 
There is also empirical evidence that better 
translations can be obtained from transcriptions of 
the speech recognizer which resulted in lower 
scores. This suggests that improvements can be 
achieved by applying machine translation on a 
large set of transcription hypotheses generated by 
the speech recognizers and by combining scores of 
acoustic models, language models, and translation 
models. 
Recently, approaches have been proposed for 
improving translation quality through the process-
ing of multiple input hypotheses. We have imple-
mented in Moses confusion network decoding as 
discussed in (Bertoldi and Federico 2005), and de-
veloped a simpler translation model and a more 
efficient implementation of the search algorithm. 
Remarkably, the confusion network decoder re-
sulted in an extension of the standard text decoder. 
5 Efficient Data Structures for Transla-
tion Model and Language Models 
With the availability of ever-increasing 
amounts of training data, it has become a challenge 
for machine translation systems to cope with the 
resulting strain on computational resources. Instead 
of simply buying larger machines with, say, 12 GB 
of main memory, the implementation of more effi-
cient data structures in Moses makes it possible to 
exploit larger data resources with limited hardware 
infrastructure. 
A phrase translation table easily takes up giga-
bytes of disk space, but for the translation of a sin-
gle sentence only a tiny fraction of this table is 
needed. Moses implements an efficient representa-
tion of the phrase translation table. Its key proper-
ties are a prefix tree structure for source words and 
on demand loading, i.e. only the fraction of the 
phrase table that is needed to translate a sentence is 
loaded into the working memory of the decoder. 
179
 For the Chinese-English NIST  task, the mem-
ory requirement of the phrase table is reduced from 
1.7 gigabytes to less than 20 mega bytes, with no 
loss in translation quality and speed (Zens and Ney 
2007). 
The other large data resource for statistical ma-
chine translation is the language model. Almost 
unlimited text resources can be collected from the 
Internet and used as training data for language 
modeling. This results in language models that are 
too large to easily fit into memory. 
The Moses system implements a data structure 
for language models that is more efficient than the 
canonical SRILM (Stolcke 2002) implementation 
used in most systems. The language model on disk 
is also converted into this binary format, resulting 
in a minimal loading time during start-up of the 
decoder.  
An even more compact representation of the 
language model is the result of the quantization of 
the word prediction and back-off probabilities of 
the language model. Instead of representing these 
probabilities with 4 byte or 8 byte floats, they are 
sorted into bins, resulting in (typically) 256 bins 
which can be referenced with a single 1 byte index. 
This quantized language model, albeit being less 
accurate, has only minimal impact on translation 
performance (Federico and Bertoldi 2006). 
6 Conclusion and Future Work 
This paper has presented a suite of open-source 
tools which we believe will be of value to the MT 
research community. 
We have also described a new SMT decoder 
which can incorporate some linguistic features in a 
consistent and flexible framework. This new direc-
tion in research opens up many possibilities and 
issues that require further research and experimen-
tation. Initial results show the potential benefit of 
factors for statistical machine translation, (Koehn 
et al 2006) and (Koehn and Hoang 2007). 
References 
Axelrod, Amittai. "Factored Language Model for Sta-
tistical Machine Translation." MRes Thesis. 
Edinburgh University, 2006. 
Bertoldi, Nicola, and Marcello Federico. "A New De-
coder for Spoken Language Translation Based 
on Confusion Networks." Automatic Speech 
Recognition and Understanding Workshop 
(ASRU), 2005. 
Bilmes, Jeff A, and Katrin Kirchhoff. "Factored Lan-
guage Models and Generalized Parallel Back-
off." HLT/NACCL, 2003. 
Koehn, Philipp. "Pharaoh: A Beam Search Decoder for 
Phrase-Based Statistical Machine Translation 
Models." AMTA, 2004. 
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola 
Bertoldi, Ondrej Bojar, Chris Callison-Burch, 
Brooke Cowan, Chris Dyer, Hieu Hoang, 
Richard Zens, Alexandra Constantin, Christine 
Corbett Moran, and Evan Herbst. "Open 
Source Toolkit for Statistical Machine Transla-
tion". Report of the 2006 Summer Workshop at 
Johns Hopkins University, 2006. 
Koehn, Philipp, and Hieu Hoang. "Factored Translation 
Models." EMNLP, 2007. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
"Statistical Phrase-Based Translation." 
HLT/NAACL, 2003. 
Och, Franz Josef. "Minimum Error Rate Training for 
Statistical Machine Translation." ACL, 2003. 
Och, Franz Josef, and Hermann Ney. "A Systematic 
Comparison of Various Statistical Alignment 
Models." Computational Linguistics 29.1 
(2003): 19-51. 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. "BLEU: A Method for Automatic 
Evaluation of Machine Translation." ACL, 
2002. 
Shen, Wade, Richard Zens, Nicola Bertoldi, and 
Marcello Federico. "The JHU Workshop 2006 
Iwslt System." International Workshop on Spo-
ken Language Translation, 2006. 
Stolcke, Andreas. "SRILM an Extensible Language 
Modeling Toolkit." Intl. Conf. on Spoken Lan-
guage Processing, 2002. 
Zens, Richard, and Hermann Ney. "Efficient Phrase-
Table Representation for Machine Translation 
with Applications to Online MT and Speech 
Recognition." HLT/NAACL, 2007. 
180
Proceedings of ACL-08: HLT, pages 1012?1020,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generalizing Word Lattice Translation
Christopher Dyer?, Smaranda Muresan, Philip Resnik?
Laboratory for Computational Linguistics and Information Processing
Institute for Advanced Computer Studies
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
redpony, smara, resnik AT umd.edu
Abstract
Word lattice decoding has proven useful in
spoken language translation; we argue that it
provides a compelling model for translation of
text genres, as well. We show that prior work
in translating lattices using finite state tech-
niques can be naturally extended to more ex-
pressive synchronous context-free grammar-
based models. Additionally, we resolve a
significant complication that non-linear word
lattice inputs introduce in reordering mod-
els. Our experiments evaluating the approach
demonstrate substantial gains for Chinese-
English and Arabic-English translation.
1 Introduction
When Brown and colleagues introduced statistical
machine translation in the early 1990s, their key in-
sight ? harkening back to Weaver in the late 1940s ?
was that translation could be viewed as an instance
of noisy channel modeling (Brown et al, 1990).
They introduced a now standard decomposition that
distinguishes modeling sentences in the target lan-
guage (language models) from modeling the rela-
tionship between source and target language (trans-
lation models). Today, virtually all statistical trans-
lation systems seek the best hypothesis e for a given
input f in the source language, according to
e? = arg max
e
Pr(e|f) (1)
An exception is the translation of speech recogni-
tion output, where the acoustic signal generally un-
derdetermines the choice of source word sequence
f . There, Bertoldi and others have recently found
that, rather than translating a single-best transcrip-
tion f , it is advantageous to allow the MT decoder to
consider all possibilities for f by encoding the alter-
natives compactly as a confusion network or lattice
(Bertoldi et al, 2007; Bertoldi and Federico, 2005;
Koehn et al, 2007).
Why, however, should this advantage be limited
to translation from spoken input? Even for text,
there are often multiple ways to derive a sequence
of words from the input string. Segmentation of
Chinese, decompounding in German, morpholog-
ical analysis for Arabic ? across a wide range
of source languages, ambiguity in the input gives
rise to multiple possibilities for the source word se-
quence. Nonetheless, state-of-the-art systems com-
monly identify a single analysis f during a prepro-
cessing step, and decode according to the decision
rule in (1).
In this paper, we go beyond speech translation
by showing that lattice decoding can also yield im-
provements for text by preserving alternative anal-
yses of the input. In addition, we generalize lattice
decoding algorithmically, extending it for the first
time to hierarchical phrase-based translation (Chi-
ang, 2005; Chiang, 2007).
Formally, the approach we take can be thought of
as a ?noisier channel?, where an observed signal o
gives rise to a set of source-language strings f ? ?
F(o) and we seek
e? = arg max
e
max
f ??F(o)
Pr(e, f ?|o) (2)
= arg max
e
max
f ??F(o)
Pr(e)Pr(f ?|e, o) (3)
= arg max
e
max
f ??F(o)
Pr(e)Pr(f ?|e)Pr(o|f ?).(4)
Following Och and Ney (2002), we use the maxi-
mum entropy framework (Berger et al, 1996) to di-
rectly model the posterior Pr(e, f ?|o) with parame-
ters tuned to minimize a loss function representing
1012
the quality only of the resulting translations. Thus,
we make use of the following general decision rule:
e? = arg max
e
max
f ??F(o)
M?
m=1
?m?m(e, f
?, o) (5)
In principle, one could decode according to (2)
simply by enumerating and decoding each f ? ?
F(o); however, for any interestingly large F(o) this
will be impractical. We assume that for many in-
teresting cases of F(o), there will be identical sub-
strings that express the same content, and therefore
a lattice representation is appropriate.
In Section 2, we discuss decoding with this model
in general, and then show how two classes of trans-
lation models can easily be adapted for lattice trans-
lation; we achieve a unified treatment of finite-state
and hierarchical phrase-based models by treating
lattices as a subcase of weighted finite state au-
tomata (FSAs). In Section 3, we identify and solve
issues that arise with reordering in non-linear FSAs,
i.e. FSAs where every path does not pass through
every node. Section 4 presents two applications of
the noisier channel paradigm, demonstrating sub-
stantial performance gains in Arabic-English and
Chinese-English translation. In Section 5 we discuss
relevant prior work, and we conclude in Section 6.
2 Decoding
Most statistical machine translation systems model
translational equivalence using either finite state
transducers or synchronous context free grammars
(Lopez, to appear 2008). In this section we discuss
the issues associated with adapting decoders from
both classes of formalism to process word lattices.
The first decoder we present is a SCFG-based de-
coder similar to the one described in Chiang (2007).
The second is a phrase-based decoder implementing
the model of Koehn et al (2003).
2.1 Word lattices
A word lattice G = ?V,E? is a directed acyclic
graph that formally is a weighted finite state automa-
ton (FSA). We further stipulate that exactly one node
has no outgoing edges and is designated the ?end
node?. Figure 1 illustrates three classes of word
lattices.
0
1x 2a
y
3bc
0 1
ax
?
2b 3dc
0 1a 2b 3c
Figure 1: Three examples of word lattices: (a) sentence,
(b) confusion network, and (c) non-linear word lattice.
A word lattice is useful for our purposes because
it permits any finite set of strings to be represented
and allows for substrings common to multiple mem-
bers of the set to be represented with a single piece
of structure. Additionally, all paths from one node to
another form an equivalence class representing, in
our model, alternative expressions of the same un-
derlying communicative intent.
For translation, we will find it useful to encode
G in a chart based on a topological ordering of the
nodes, as described by Cheppalier et al (1999). The
nodes in the lattices shown in Figure 1 are labeled
according to an appropriate numbering.
The chart-representation of the graph is a triple of
2-dimensional matrices ?F,p,R?, which can be con-
structed from the numbered graph. Fi,j is the word
label of the jth transition leaving node i. The cor-
responding transition cost is pi,j . Ri,j is the node
number of the node on the right side of the jth tran-
sition leaving node i. Note that Ri,j > i for all i, j.
Table 1 shows the word lattice from Figure 1 repre-
sented in matrix form as ?F,p,R?.
0 1 2
a 1 1 b 1 2 c 1 3
a 13 1 b 1 2 c
1
2 3
x 13 1 d
1
2 3
 13 1
x 12 1 y 1 2 b
1
2 3
a 12 2 c
1
2 3
Table 1: Topologically ordered chart encoding of the
three lattices in Figure 1. Each cell ij in this table is a
triple ?Fij ,pij ,Rij?
1013
2.2 Parsing word lattices
Chiang (2005) introduced hierarchical phrase-based
translation models, which are formally based
on synchronous context-free grammars (SCFGs).
Translation proceeds by parsing the input using the
source language side of the grammar, simultane-
ously building a tree on the target language side via
the target side of the synchronized rules. Since de-
coding is equivalent to parsing, we begin by present-
ing a parser for word lattices, which is a generaliza-
tion of a CKY parser for lattices given in Cheppalier
et al (1999).
Following Goodman (1999), we present our lat-
tice parser as a deductive proof system in Figure 2.
The parser consists of two kinds of items, the first
with the form [X ? ? ? ?, i, j] representing rules
that have yet to be completed and span node i to
node j. The other items have the form [X, i, j] and
indicate that non-terminal X spans [i, j]. As with
sentence parsing, the goal is a deduction that covers
the spans of the entire input lattice [S, 0, |V | ? 1].
The three inference rules are: 1) match a terminal
symbol and move across one edge in the lattice 2)
move across an -edge without advancing the dot in
an incomplete rule 3) advance the dot across a non-
terminal symbol given appropriate antecedents.
2.3 From parsing to MT decoding
A target language model is necessary to generate flu-
ent output. To do so, the grammar is intersected with
an n-gram LM. To mitigate the effects of the combi-
natorial explosion of non-terminals the LM intersec-
tion entails, we use cube-pruning to only consider
the most promising expansions (Chiang, 2007).
2.4 Lattice translation with FSTs
A second important class of translation models in-
cludes those based formally on FSTs. We present a
description of the decoding process for a word lattice
using a representative FST model, the phrase-based
translation model described in Koehn et al (2003).
Phrase-based models translate a foreign sentence
f into the target language e by breaking up f into
a sequence of phrases f
I
1, where each phrase f i can
contain one or more contiguous words and is trans-
lated into a target phrase ei of one or more contigu-
ous words. Each word in f must be translated ex-
actly once. To generalize this model to word lattices,
it is necessary to choose both a path through the lat-
tice and a partitioning of the sentence this induces
into a sequence of phrases f
I
1. Although the number
of source phrases in a word lattice can be exponen-
tial in the number of nodes, enumerating the possible
translations of every span in a lattice is in practice
tractable, as described by Bertoldi et al (2007).
2.5 Decoding with phrase-based models
We adapted the Moses phrase-based decoder to
translate word lattices (Koehn et al, 2007). The
unmodified decoder builds a translation hypothesis
from left to right by selecting a range of untrans-
lated words and adding translations of this phrase to
the end of the hypothesis being extended. When no
untranslated words remain, the translation process is
complete.
The word lattice decoder works similarly, only
now the decoder keeps track not of the words that
have been covered, but of the nodes, given a topo-
logical ordering of the nodes. For example, assum-
ing the third lattice in Figure 1 is our input, if the
edge with word a is translated, this will cover two
untranslated nodes [0,1] in the coverage vector, even
though it is only a single word. As with sentence-
based decoding, a translation hypothesis is complete
when all nodes in the input lattice are covered.
2.6 Non-monotonicity and unreachable nodes
The changes described thus far are straightfor-
ward adaptations of the underlying phrase-based
sentence decoder; however, dealing properly with
non-monotonic decoding of word lattices introduces
some minor complexity that is worth mentioning. In
the sentence decoder, any translation of any span of
untranslated words is an allowable extension of a
partial translation hypothesis, provided that the cov-
erage vectors of the extension and the partial hypoth-
esis do not intersect. In a non-linear word lattice,
a further constraint must be enforced ensuring that
there is always a path from the starting node of the
translation extension?s source to the node represent-
ing the nearest right edge of the already-translated
material, as well as a path from the ending node of
the translation extension?s source to future translated
spans. Figure 3 illustrates the problem. If [0,1] is
translated, the decoder must not consider translating
1014
Axioms:
[X ? ??, i, i] : w
(X
w
?? ??, ??) ? G, i ? [0, |V | ? 2]
Inference rules:
[X ? ? ? Fj,k?, i, j] : w
[X ? ?Fj,k ? ?, i,Rj,k] : w ? pj,k
[X ? ? ? ?, i, j] : w
[X ? ? ? ?, i,Rj,k] : w ? pj,k
Fj,k = 
[Z ? ? ?X?, i, k] : w1 [X ? ??, k, j] : w2
[Z ? ?X ? ?, i, j] : w1 ? w2
Goal state:
[S ? ??, 0, |V | ? 1]
Figure 2: Word lattice parser for an unrestricted context free grammar G.
0 1x
2
ay
Figure 3: The span [0, 3] has one inconsistent covering,
[0, 1] + [2, 3].
[2,3] as a possible extension of this hypothesis since
there is no path from node 1 to node 2 and therefore
the span [1,2] would never be covered. In the parser
that forms the basis of the hierarchical decoder de-
scribed in Section 2.3, no such restriction is neces-
sary since grammar rules are processed in a strictly
left-to-right fashion without any skips.
3 Distortion in a non-linear word lattice
In both hierarchical and phrase-based models, the
distance between words in the source sentence is
used to limit where in the target sequence their trans-
lations will be generated. In phrase based transla-
tion, distortion is modeled explicitly. Models that
support non-monotonic decoding generally include
a distortion cost, such as |ai ? bi?1 ? 1| where ai is
the starting position of the foreign phrase f i and bi?1
is the ending position of phrase f i?1 (Koehn et al,
2003). The intuition behind this model is that since
most translation is monotonic, the cost of skipping
ahead or back in the source should be proportional
to the number of words that are skipped. Addition-
ally, a maximum distortion limit is used to restrict
0 1x
2a y3
b cd
Figure 4: Distance-based distortion problem. What is the
distance between node 4 to node 0?
the size of the search space.
In linear word lattices, such as confusion net-
works, the distance metric used for the distortion
penalty and for distortion limits is well defined;
however, in a non-linear word lattice, it poses the
problem illustrated in Figure 4. Assuming the left-
to-right decoding strategy described in the previous
section, if c is generated by the first target word, the
distortion penalty associated with ?skipping ahead?
should be either 3 or 2, depending on what path is
chosen to translate the span [0,3]. In large lattices,
where a single arc may span many nodes, the possi-
ble distances may vary quite substantially depending
on what path is ultimately taken, and handling this
properly therefore crucial.
Although hierarchical phrase-based models do
not model distortion explicitly, Chiang (2007) sug-
gests using a span length limit to restrict the win-
dow in which reordering can take place.1 The de-
coder enforces the constraint that a synchronous rule
learned from the training data (the only mechanism
by which reordering can be introduced) can span
1This is done to reduce the size of the search space and be-
cause hierarchical phrase-based translation models are inaccu-
rate models of long-distance distortion.
1015
Distance metric MT05 MT06
Difference 0.2943 0.2786
Difference+LexRO 0.2974 0.2890
ShortestP 0.2993 0.2865
ShortestP+LexRO 0.3072 0.2992
Table 2: Effect of distance metric on phrase-based model
performance.
maximally ? words in f . Like the distortion cost
used in phrase-based systems, ? is also poorly de-
fined for non-linear lattices.
Since we want a distance metric that will restrict
as few local reorderings as possible on any path,
we use a function ?(a, b) returning the length of the
shortest path between nodes a and b. Since this func-
tion is not dependent on the exact path chosen, it can
be computed in advance of decoding using an all-
pairs shortest path algorithm (Cormen et al, 1989).
3.1 Experimental results
We tested the effect of the distance metric on trans-
lation quality using Chinese word segmentation lat-
tices (Section 4.1, below) using both a hierarchical
and phrase-based system modified to translate word
lattices. We compared the shortest-path distance
metric with a baseline which uses the difference in
node number as the distortion distance. For an ad-
ditional datapoint, we added a lexicalized reorder-
ing model that models the probability of each phrase
pair appearing in three different orientations (swap,
monotone, other) in the training corpus (Koehn et
al., 2005).
Table 2 summarizes the results of the phrase-
based systems. On both test sets, the shortest path
metric improved the BLEU scores. As expected,
the lexicalized reordering model improved transla-
tion quality over the baseline; however, the improve-
ment was more substantial in the model that used the
shortest-path distance metric (which was already a
higher baseline). Table 3 summarizes the results of
our experiment comparing the performance of two
distance metrics to determine whether a rule has ex-
ceeded the decoder?s span limit. The pattern is the
same, showing a clear increase in BLEU for the
shortest path metric over the baseline.
Distance metric MT05 MT06
Difference 0.3063 0.2957
ShortestP 0.3176 0.3043
Table 3: Effect of distance metric on hierarchical model
performance.
4 Exploiting Source Language Alternatives
Chinese word segmentation. A necessary first
step in translating Chinese using standard models
is segmenting the character stream into a sequence
of words. Word-lattice translation offers two possi-
ble improvements over the conventional approach.
First, a lattice may represent multiple alternative
segmentations of a sentence; input represented in
this way will be more robust to errors made by the
segmenter.2 Second, different segmentation granu-
larities may be more or less optimal for translating
different spans. By encoding alternatives in the in-
put in a word lattice, the decision as to which granu-
larity to use for a given span can be resolved during
decoding rather than when constructing the system.
Figure 5 illustrates a lattice based on three different
segmentations.
Arabic morphological variation. Arabic orthog-
raphy is problematic for lexical and phrase-based
MT approaches since a large class of functional el-
ements (prepositions, pronouns, tense markers, con-
junctions, definiteness markers) are attached to their
host stems. Thus, while the training data may pro-
vide good evidence for the translation of a partic-
ular stem by itself, the same stem may not be at-
tested when attached to a particular conjunction.
The general solution taken is to take the best pos-
sible morphological analysis of the text (it is of-
ten ambiguous whether a piece of a word is part
of the stem or merely a neighboring functional el-
ement), and then make a subset of the bound func-
tional elements in the language into freestanding to-
kens. Figure 6 illustrates the unsegmented Arabic
surface form as well as the morphological segmen-
tation variant we made use of. The limitation of this
approach is that as the amount and variety of train-
ing data increases, the optimal segmentation strat-
egy changes: more aggressive segmentation results
2The segmentation process is ambiguous, even for native
speakers of Chinese.
1016
01
?
2
??
4
????
?
3
?
??
?
5
?
6
??
?
7
"
8
?
9
??
?
10
?
11
??
?
12
"
Figure 5: Sample Chinese segmentation lattice using three segmentations.
in fewer OOV tokens, but automatic evaluation met-
rics indicate lower translation quality, presumably
because the smaller units are being translated less
idiomatically (Habash and Sadat, 2006). Lattices al-
low the decoder to make decisions about what gran-
ularity of segmentation to use subsententially.
4.1 Chinese Word Segmentation Experiments
In our experiments we used two state-of-the-art Chi-
nese word segmenters: one developed at Harbin
Institute of Technology (Zhao et al, 2001), and
one developed at Stanford University (Tseng et al,
2005). In addition, we used a character-based seg-
mentation. In the remaining of this paper, we use cs
for character segmentation, hs for Harbin segmenta-
tion and ss for Stanford segmentation. We built two
types of lattices: one that combines the Harbin and
Stanford segmenters (hs+ss), and one which uses
all three segmentations (hs+ss+cs).
Data and Settings. The systems used in these
experiments were trained on the NIST MT06 Eval
corpus without the UN data (approximatively 950K
sentences). The corpus was analyzed with the three
segmentation schemes. For the systems using word
lattices, the training data contained the versions of
the corpus appropriate for the segmentation schemes
used in the input. That is, for the hs+ss condition,
the training data consisted of two copies of the cor-
pus: one segmented with the Harbin segmenter and
the other with the Stanford segmenter.3 A trigram
English language model with modified Kneser-Ney
smoothing (Kneser and Ney, 1995) was trained on
the English side of our training data as well as por-
tions of the Gigaword v2 English Corpus, and was
used for all experiments. The NIST MT03 test set
was used as a development set for optimizing the in-
terpolation weights using minimum error rate train-
3The corpora were word-aligned independently and then
concatenated for rule extraction.
ing (Och, 2003). The testing was done on the NIST
2005 and 2006 evaluation sets (MT05, MT06).
Experimental results: Word-lattices improve
translation quality. We used both a phrase-based
translation model, decoded using our modified ver-
sion of Moses (Koehn et al, 2007), and a hierarchi-
cal phrase-based translation model, using our modi-
fied version of Hiero (Chiang, 2005; Chiang, 2007).
These two translation model types illustrate the ap-
plicability of the theoretical contributions presented
in Section 2 and Section 3.
We observed that the coverage of named entities
(NEs) in our baseline systems was rather poor. Since
names in Chinese can be composed of relatively
long strings of characters that cannot be translated
individually, when generating the segmentation lat-
tices that included cs arcs, we avoided segmenting
NEs of type PERSON, as identified using a Chinese
NE tagger (Florian et al, 2004).
The results are summarized in Table 4. We see
that using word lattices improves BLEU scores both
in the phrase-based model and hierarchical model as
compared to the single-best segmentation approach.
All results using our word-lattice decoding for the
hierarchical models (hs+ss and hs+ss+cs) are sig-
nificantly better than the best segmentation (ss).4
For the phrase-based model, we obtain significant
gains using our word-lattice decoder using all three
segmentations on MT05. The other results, while
better than the best segmentation (hs) by at least
0.3 BLEU points, are not statistically significant.
Even if the results are not statistically significant
for MT06, there is a high decrease in OOV items
when using word-lattices. For example, for MT06
the number of OOVs in the hs translation is 484.
4Significance testing was carried out using the bootstrap re-
sampling technique advocated by Koehn (2004). Unless other-
wise noted, all reported improvements are signficant at at least
p < 0.05.
1017
surface wxlAl ftrp AlSyf kAn mEZm AlDjyj AlAElAmy m&ydA llEmAd .
segmented w- xlAl ftrp Al- Syf kAn mEZm Al- Djyj Al- AElAmy m&ydA l- Al- EmAd .
(English) During the summer period , most media buzz was supportive of the general .
Figure 6: Example of Arabic morphological segmentation.
The number of OOVs decreased by 19% for hs+ss
and by 75% for hs+ss+cs. As mentioned in Section
3, using lexical reordering for word-lattices further
improves the translation quality.
4.2 Arabic Morphology Experiments
We created lattices from an unsegmented version of
the Arabic test data and generated alternative arcs
where clitics as well as the definiteness marker and
the future tense marker were segmented into tokens.
We used the Buckwalter morphological analyzer and
disambiguated the analysis using a simple unigram
model trained on the Penn Arabic Treebank.
Data and Settings. For these experiments we
made use of the entire NIST MT08 training data,
although for training of the system, we used a sub-
sampling method proposed by Kishore Papineni that
aims to include training sentences containing n-
grams in the test data (personal communication).
For all systems, we used a 5-gram English LM
trained on 250M words of English training data.
The NIST MT03 test set was used as development
set for optimizing the interpolation weights using
MER training (Och, 2003). Evaluation was car-
ried out on the NIST 2005 and 2006 evaluation sets
(MT05, MT06).
Experimental results: Word-lattices improve
translation quality. Results are presented in Table
5. Using word-lattices to combine the surface forms
with morphologically segmented forms significantly
improves BLEU scores both in the phrase-based and
hierarchical models.
5 Prior work
Lattice Translation. The ?noisier channel? model
of machine translation has been widely used in spo-
ken language translation as an alternative to select-
ing the single-best hypothesis from an ASR system
and translating it (Ney, 1999; Casacuberta et al,
2004; Zhang et al, 2005; Saleem et al, 2005; Ma-
tusov et al, 2005; Bertoldi et al, 2007; Mathias,
2007). Several authors (e.g. Saleem et al (2005)
and Bertoldi et al (2007)) comment directly on
the impracticality of using n-best lists to translate
speech.
Although translation is fundamentally a non-
monotonic relationship between most language
pairs, reordering has tended to be a secondary con-
cern to the researchers who have worked on lattice
translation. Matusov et al (2005) decodes monoton-
ically and then uses a finite state reordering model
on the single-best translation, along the lines of
Bangalore and Riccardi (2000). Mathias (2007)
and Saleem et al (2004) only report results of
monotonic decoding for the systems they describe.
Bertoldi et al (2007) solve the problem by requiring
that their input be in the format of a confusion net-
work, which enables the standard distortion penalty
to be used. Finally, the system described by Zhang
et al (2005) uses IBM Model 4 features to translate
lattices. For the distortion model, they use the maxi-
mum probability value over all possible paths in the
lattice for each jump considered, which is similar
to the approach we have taken. Mathias and Byrne
(2006) build a phrase-based translation system as a
cascaded series of FSTs which can accept any input
FSA; however, the only reordering that is permitted
is the swapping of two adjacent phrases.
Applications of source lattices outside of the do-
main of spoken language translation have been far
more limited. Costa-jussa` and Fonollosa (2007) take
steps in this direction by using lattices to encode
multiple reorderings of the source language. Dyer
(2007) uses confusion networks to encode mor-
phological alternatives in Czech-English translation,
and Xu et al (2005) takes an approach very similar
to ours for Chinese-English translation and encodes
multiple word segmentations in a lattice, but which
is decoded with a conventionally trained translation
model and without a sophisticated reordering model.
The Arabic-English morphological segmentation
lattices are similar in spirit to backoff translation
models (Yang and Kirchhoff, 2006), which consider
alternative morphological segmentations and simpli-
1018
MT05 MT06
(Source Type) BLEU BLEU
cs 0.2833 0.2694
hs 0.2905 0.2835
ss 0.2894 0.2801
hs+ss 0.2938 0.2870
hs+ss+cs 0.2993 0.2865
hs+ss+cs.lexRo 0.3072 0.2992
MT05 MT06
(Source Type) BLEU BLEU
cs 0.2904 0.2821
hs 0.3008 0.2907
ss 0.3071 0.2964
hs+ss 0.3132 0.3006
hs+ss+cs 0.3176 0.3043
(a) Phrase-based model (b) Hierarchical model
Table 4: Chinese Word Segmentation Results
MT05 MT06
(Source Type) BLEU BLEU
surface 0.4682 0.3512
morph 0.5087 0.3841
morph+surface 0.5225 0.4008
MT05 MT06
(Source Type) BLEU BLEU
surface 0.5253 0.3991
morph 0.5377 0.4180
morph+surface 0.5453 0.4287
(a) Phrase-based model (b) Hierarchical model
Table 5: Arabic Morphology Results
fications of a surface token when the surface token
can not be translated.
Parsing and formal language theory. There has
been considerable work on parsing word lattices,
much of it for language modeling applications in
speech recognition (Ney, 1991; Cheppalier and Raj-
man, 1998). Additionally, Grune and Jacobs (2008)
refines an algorithm originally due to Bar-Hillel for
intersecting an arbitrary FSA (of which word lattices
are a subset) with a CFG. Klein and Manning (2001)
formalize parsing as a hypergraph search problem
and derive an O(n3) parser for lattices.
6 Conclusions
We have achieved substantial gains in translation
performance by decoding compact representations
of alternative source language analyses, rather than
single-best representations. Our results generalize
previous gains for lattice translation of spoken lan-
guage input, and we have further generalized the
approach by introducing an algorithm for lattice
decoding using a hierarchical phrase-based model.
Additionally, we have shown that although word
lattices complicate modeling of word reordering, a
simple heuristic offers good performance and en-
ables many standard distortion models to be used
directly with lattice input.
Acknowledgments
This research was supported by the GALE program
of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-0001. The authors wish
to thank Niyu Ge for the Chinese named-entity anal-
ysis, Pi-Chuan Chang for her assistance with the
Stanford Chinese segmenter, and Tie-Jun Zhao and
Congui Zhu for making the Harbin Chinese seg-
menter available to us.
References
S. Bangalore and G. Riccardi. 2000. Finite state models
for lexical reordering in spoken language translation.
In Proc. Int. Conf. on Spoken Language Processing,
pages 422?425, Beijing, China.
A.L. Berger, V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Comput. Linguist., 22(1):39?71.
N. Bertoldi and M. Federico. 2005. A new decoder for
spoken language translation based on confusion net-
works. In Proceedings of the IEEE Automatic Speech
Recognition and Understanding Workshop.
N. Bertoldi, R. Zens, and M. Federico. 2007. Speech
translation by confusion network decoding. In Pro-
ceeding of ICASSP 2007, Honolulu, Hawaii, April.
P.F. Brown, J. Cocke, S. Della-Pietra, V.J. Della-Pietra,
F. Jelinek, J.D. Lafferty, R.L. Mercer, and P.S.
Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16:79?85.
F. Casacuberta, H. Ney, F. J. Och, E. Vidal, J. M. Vilar,
S. Barrachina, I. Garcia-Varea, D. Llorens, C. Mar-
1019
tinez, S. Molau, F. Nevado, M. Pastor, D. Pico, A. San-
chis, and C. Tillmann. 2004. Some approaches to
statistical and finite-state speech-to-speech translation.
Computer Speech & Language, 18(1):25?47, January.
J. Cheppalier and M. Rajman. 1998. A generalized CYK
algorithm for parsing stochastic CFG. In Proceedings
of the Workshop on Tabulation in Parsing and Deduc-
tion (TAPD98), pages 133?137, Paris, France.
J. Cheppalier, M. Rajman, R. Aragues, and A. Rozen-
knop. 1999. Lattice parsing for speech recognition.
In Sixth Conference sur le Traitement Automatique du
Langage Naturel (TANL?99), pages 95?104.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL?05), pages 263?270.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
T.H. Cormen, C. E. Leiserson, and R. L. Rivest, 1989.
Introduction to Algorithms, pages 558?565. The MIT
Press and McGraw-Hill Book Company.
M. Costa-jussa` and J.A.R. Fonollosa. 2007. Analy-
sis of statistical and morphological classes to gener-
ate weighted reordering hypotheses on a statistical ma-
chine translation system. In Proc. of the Second Work-
shop on SMT, pages 171?176, Prague.
C. Dyer. 2007. Noisier channel translation: translation
from morphologically complex languages. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, Prague, June.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Proc. of HLT-NAACL 2004, pages 1?8.
J. Goodman. 1999. Semiring parsing. Computational
Linguistics, 25:573?605.
D. Grune and C.J. H. Jacobs. 2008. Parsing as intersec-
tion. Parsing Techniques, pages 425?442.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
D. Klein and C. D. Manning. 2001. Parsing with hyper-
graphs. In Proceedings of IWPT 2001.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of IEEE
Internation Conference on Acoustics, Speech, and Sig-
nal Processing, pages 181?184.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
2003, pages 48?54.
P. Koehn, A. Axelrod, A. Birch Mayne, C. Callison-
Burch, M. Osborne, and D. Talbot. 2005. Edinburgh
system description for the 2005 IWSLT speech trans-
lation evaluation. In Proc. of IWSLT 2005, Pittsburgh.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-
Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Annual Meeting
of the Association for Computation Linguistics (ACL),
Demonstration Session, pages 177?180, Jun.
P. Koehn. 2004. Statistical significance tests for machine
translation evluation. In Proc. of the 2004 Conf. on
EMNLP, pages 388?395.
A. Lopez. to appear 2008. Statistical machine transla-
tion. ACM Computing Surveys.
L. Mathias and W. Byrne. 2006. Statistical phrase-
based speech translation. In IEEE Conf. on Acoustics,
Speech and Signal Processing.
L. Mathias. 2007. Statistical Machine Translation
and Automatic Speech Recognition under Uncertainty.
Ph.D. thesis, The Johns Hopkins University.
E. Matusov, S. Kanthak, and H. Ney. 2005. On the in-
tegration of speech recognition and statistical machine
translation. In Proceedings of Interspeech 2005.
H. Ney. 1991. Dynamic programming parsing for
context-free grammars in continuous speech recogni-
tion. IEEE Transactions on Signal Processing, 39(2).
H. Ney. 1999. Speech translation: Coupling of recogni-
tion and translation. In Proc. of ICASSP, pages 517?
520, Phoenix.
F. Och and H. Ney. 2002. Discriminitive training
and maximum entropy models for statistical machine
translation. In Proceedings of the 40th Annual Meet-
ing of the ACL, pages 295?302.
S. Saleem, S.-C. Jou, S. Vogel, and T. Schulz. 2005. Us-
ing word lattice information for a tighter coupling in
speech translation systems. In Proc. of ICSLP, Jeju
Island, Korea.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter. In Fourth SIGHANWorkshop on Chinese Lan-
guage Processing.
J. Xu, E. Matusov, R. Zens, and H. Ney. 2005. Inte-
grated Chinese word segmentation in statistical ma-
chine translation. In Proc. of IWSLT 2005, Pittsburgh.
M. Yang and K. Kirchhoff. 2006. Phrase-based back-
off models for machine translation of highly inflected
languages. In Proceedings of the EACL 2006, pages
41?48.
R. Zhang, G. Kikui, H. Yamamoto, and W. Lo. 2005.
A decoding algorithm for word lattice translation in
speech translation. In Proceedings of the 2005 Inter-
national Workshop on Spoken Language Translation.
T. Zhao, L. Yajuan, Y. Muyun, and Y. Hao. 2001. In-
creasing accuracy of chinese segmentation with strat-
egy of multi-step processing. In J Chinese Information
Processing (Chinese Version), volume 1, pages 13?18.
1020
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 163?171,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Efficient Minimum Error Rate Training and
Minimum Bayes-Risk Decoding for
Translation Hypergraphs and Lattices
Shankar Kumar1 and Wolfgang Macherey1 and Chris Dyer2 and Franz Och1
1Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{shankarkumar,wmach,och}@google.com
2Department of Linguistics
University of Maryland
College Park, MD 20742, USA
redpony@umd.edu
Abstract
Minimum Error Rate Training (MERT)
and Minimum Bayes-Risk (MBR) decod-
ing are used in most current state-of-the-
art Statistical Machine Translation (SMT)
systems. The algorithms were originally
developed to work with N -best lists of
translations, and recently extended to lat-
tices that encode many more hypotheses
than typical N -best lists. We here extend
lattice-based MERT and MBR algorithms
to work with hypergraphs that encode a
vast number of translations produced by
MT systems based on Synchronous Con-
text Free Grammars. These algorithms
are more efficient than the lattice-based
versions presented earlier. We show how
MERT can be employed to optimize pa-
rameters for MBR decoding. Our exper-
iments show speedups from MERT and
MBR as well as performance improve-
ments from MBR decoding on several lan-
guage pairs.
1 Introduction
Statistical Machine Translation (SMT) systems
have improved considerably by directly using the
error criterion in both training and decoding. By
doing so, the system can be optimized for the
translation task instead of a criterion such as like-
lihood that is unrelated to the evaluation met-
ric. Two popular techniques that incorporate the
error criterion are Minimum Error Rate Train-
ing (MERT) (Och, 2003) and Minimum Bayes-
Risk (MBR) decoding (Kumar and Byrne, 2004).
These two techniques were originally developed
for N -best lists of translation hypotheses and re-
cently extended to translation lattices (Macherey
et al, 2008; Tromble et al, 2008) generated by a
phrase-based SMT system (Och and Ney, 2004).
Translation lattices contain a significantly higher
number of translation alternatives relative to N -
best lists. The extension to lattices reduces the
runtimes for both MERT and MBR, and gives per-
formance improvements from MBR decoding.
SMT systems based on synchronous context
free grammars (SCFG) (Chiang, 2007; Zollmann
and Venugopal, 2006; Galley et al, 2006) have
recently been shown to give competitive perfor-
mance relative to phrase-based SMT. For these
systems, a hypergraph or packed forest provides a
compact representation for encoding a huge num-
ber of translation hypotheses (Huang, 2008).
In this paper, we extend MERT and MBR
decoding to work on hypergraphs produced by
SCFG-based MT systems. We present algorithms
that are more efficient relative to the lattice al-
gorithms presented in Macherey et al (2008;
Tromble et al (2008). Lattice MBR decoding uses
a linear approximation to the BLEU score (Pap-
ineni et al, 2001); the weights in this linear loss
are set heuristically by assuming that n-gram pre-
cisions decay exponentially with n. However, this
may not be optimal in practice. We employ MERT
to select these weights by optimizing BLEU score
on a development set.
A related MBR-inspired approach for hyper-
graphs was developed by Zhang and Gildea
(2008). In this work, hypergraphs were rescored to
maximize the expected count of synchronous con-
stituents in the translation. In contrast, our MBR
algorithm directly selects the hypothesis in the
hypergraph with the maximum expected approx-
imate corpus BLEU score (Tromble et al, 2008).
will soon announce 
X1  X2
X1  X2
X1  X2
X1  X2
X1  X2
X1 its future in the 
X1 its future in the 
Suzuki
soon
its future in
X1 announces
Rally World Championship
Figure 1: An example hypergraph.
163
2 Translation Hypergraphs
A translation lattice compactly encodes a large
number of hypotheses produced by a phrase-based
SMT system. The corresponding representation
for an SMT system based on SCFGs (e.g. Chi-
ang (2007), Zollmann and Venugopal (2006), Mi
et al (2008)) is a directed hypergraph or a packed
forest (Huang, 2008).
Formally, a hypergraph is a pair H = ?V, E?
consisting of a vertex set V and a set of hyperedges
E ? V? ? V . Each hyperedge e ? E connects a
head vertex h(e) with a sequence of tail vertices
T (e) = {v1, ..., vn}. The number of tail vertices
is called the arity (|e|) of the hyperedge. If the ar-
ity of a hyperedge is zero, h(e) is called a source
vertex. The arity of a hypergraph is the maximum
arity of its hyperedges. A hyperedge of arity 1 is a
regular edge, and a hypergraph of arity 1 is a regu-
lar graph (lattice). Each hyperedge is labeled with
a rule re from the SCFG. The number of nontermi-
nals on the right-hand side of re corresponds with
the arity of e. An example without scores is shown
in Figure 1. A path in a translation hypergraph in-
duces a translation hypothesis E along with its se-
quence of SCFG rules D = r1, r2, ..., rK which,
if applied to the start symbol, derives E. The se-
quence of SCFG rules induced by a path is also
called a derivation tree for E.
3 Minimum Error Rate Training
Given a set of source sentences FS1 with corre-
sponding reference translations RS1 , the objective
of MERT is to find a parameter set ??M1 which min-
imizes an automated evaluation criterion under a
linear model:
??M1 = argmin
?M1
? SX
s=1
Err
`
Rs, E?(Fs; ?
M
1 )
?
ff
E?(Fs; ?
M
1 ) = argmax
E
? SX
s=1
?mhm(E, Fs)
ff
.
In the context of statistical machine translation,
the optimization procedure was first described in
Och (2003) for N -best lists and later extended to
phrase-lattices in Macherey et al (2008). The al-
gorithm is based on the insight that, under a log-
linear model, the cost function of any candidate
translation can be represented as a line in the plane
if the initial parameter set ?M1 is shifted along a
direction dM1 . Let C = {E1, ..., EK} denote a set
of candidate translations, then computing the best
scoring translation hypothesis E? out of C results in
the following optimization problem:
E?(F ; ?) = argmax
E?C
n
(?M1 + ? ? d
M
1 )
> ? hM1 (E,F )
o
= argmax
E?C
?
X
m
?mhm(E,F )
| {z }
=a(E,F )
+ ? ?
X
m
dmhm(E,F )
| {z }
=b(E,F )
ff
= argmax
E?C
?
a(E,F ) + ? ? b(E,F )
| {z }
(?)
?
Hence, the total score (?) for each candidate trans-
lation E ? C can be described as a line with
? as the independent variable. For any particu-
lar choice of ?, the decoder seeks that translation
which yields the largest score and therefore corre-
sponds to the topmost line segment. If ? is shifted
from ?? to +?, other translation hypotheses
may at some point constitute the topmost line seg-
ments and thus change the decision made by the
decoder. The entire sequence of topmost line seg-
ments is called upper envelope and provides an ex-
haustive representation of all possible outcomes
that the decoder may yield if ? is shifted along
the chosen direction. Both the translations and
their corresponding line segments can efficiently
be computed without incorporating any error crite-
rion. Once the envelope has been determined, the
translation candidates of its constituent line seg-
ments are projected onto their corresponding error
counts, thus yielding the exact and unsmoothed er-
ror surface for all candidate translations encoded
in C. The error surface can now easily be traversed
in order to find that ?? under which the new param-
eter set ?M1 + ?? ? d
M
1 minimizes the global error.
In this section, we present an extension of the
algorithm described in Macherey et al (2008)
that allows us to efficiently compute and repre-
sent upper envelopes over all candidate transla-
tions encoded in hypergraphs. Conceptually, the
algorithm works by propagating (initially empty)
envelopes from the hypergraph?s source nodes
bottom-up to its unique root node, thereby ex-
panding the envelopes by applying SCFG rules to
the partial candidate translations that are associ-
ated with the envelope?s constituent line segments.
To recombine envelopes, we need two operators:
the sum and the maximum over convex polygons.
To illustrate which operator is applied when, we
transform H = ?V, E? into a regular graph with
typed nodes by (1) marking all vertices v ? V with
the symbol ? and (2) replacing each hyperedge
e ? E , |e| > 1, with a small subgraph consisting
of a new vertex v?(e) whose incoming and out-
going edges connect the same head and tail nodes
164
Algorithm 1 ?-operation (Sum)
input: associative map a: V ? Env(V), hyperarc e
output: Minkowski sum of envelopes over T (e)
for (i = 0; i < |T (e)|; ++i) {
v = Ti(e);
pq.enqueue(? v, i, 0?);
}
L = ?;
D = ? e, ?1 ? ? ? ?|e|?
while (!pq.empty()) {
? v, i, j? = pq.dequeue();
` = A[v][j];
D[i+1] = `.D;
if (L.empty() ? L.back().x < `.x) {
if (0 < j) {
`.y += L.back().y - A[v][j-1].y;
`.m += L.back().m - A[v][j-1].m;
}
L.push_back(`);
L.back().D = D;
} else {
L.back().y += `.y;
L.back().m += `.m;
L.back().D[i+1] = `.D;
if (0 < j) {
L.back().y -= A[v][j-1].y;
L.back().m -= A[v][j-1].m;
}
}
if (++j < A[v].size())
pq.enqueue(? v, i, j?);
}
return L;
in the transformed graph as were connected by e
in the original graph. The unique outgoing edge
of v?(e) is associated with the rule re; incoming
edges are not linked to any rule. Figure 2 illus-
trates the transformation for a hyperedge with ar-
ity 3. The graph transformation is isomorphic.
The rules associated with every hyperedge spec-
ify how line segments in the envelopes of a hyper-
edge?s tail nodes can be combined. Suppose we
have a hyperedge e with rule re : X ? aX1bX2c
and T (e) = {v1, v2}. Then we substitute X1 and
X2 in the rule with candidate translations associ-
ated with line segments in envelopes Env(v1) and
Env(v2) respectively.
To derive the algorithm, we consider the gen-
eral case of a hyperedge e with rule re : X ?
w1X1w2...wnXnwn+1. Because the right-hand
side of re has n nonterminals, the arity of e is
|e| = n. Let T (e) = {v1, ..., vn} denote the
tail nodes of e. We now assume that each tail
node vi ? T (e) is associated with the upper en-
velope over all candidate translations that are in-
duced by derivations of the corresponding nonter-
minal symbol Xi. These envelopes shall be de-
Algorithm 2 ?-operation (Max)
input: array L[0..K-1] containing line objects
output: upper envelope of L
Sort(L:m);
j = 0; K = size(L);
for (i = 0; i < K; ++i) {
` = L[i];
`.x = -?;
if (0 < j) {
if (L[j-1].m == `.m) {
if (`.y <= L[j-1].y) continue;
--j;
}
while (0 < j) {
`.x = (`.y - L[j-1].y)/
(L[j-1].m - `.m);
if (L[j-1].x < `.x) break;
--j;
}
if (0 == j) `.x = -?;
L[j++] = `;
} else L[j++] = `;
}
L.resize(j);
return L;
noted by Env(vi). To decompose the problem of
computing and propagating the tail envelopes over
the hyperedge e to its head node, we now define
two operations, one for either node type, to spec-
ify how envelopes associated with the tail vertices
are propagated to the head vertex.
Nodes of Type ???: For a type ? node, the
resulting envelope is the Minkowski sum over
the envelopes of the incoming edges (Berg et
al., 2008). Since the envelopes of the incoming
edges are convex hulls, the Minkowski sum pro-
vides an upper bound to the number of line seg-
ments that constitute the resulting envelope: the
bound is the sum over the number of line seg-
ments in the envelopes of the incoming edges, i.e.:?
?Env(v?(e))
?
? ?
?
v??T (e)
?
?Env(v?)
?
?.
Algorithm 1 shows the pseudo code for comput-
ing the Minkowski sum over multiple envelopes.
The line objects ` used in this algorithm are
encoded as 4-tuples, each consisting of the x-
intercept with `?s left-adjacent line stored as `.x,
the slope `.m, the y-intercept `.y, and the (partial)
derivation tree `.D. At the beginning, the leftmost
line segment of each envelope is inserted into a
priority queue pq. The priority is defined in terms
of a line?s x-intercept such that lower values imply
higher priority. Hence, the priority queue enumer-
ates all line segments from left to right in ascend-
ing order of their x-intercepts, which is the order
needed to compute the Minkowski sum.
Nodes of Type ???: The operation performed
165
=?
= max
Figure 2: Transformation of a hypergraph into
a factor graph and bottom-up propagation of en-
velopes.
at nodes of type ??? computes the convex hull
over the union of the envelopes propagated over
the incoming edges. This operation is a ?max?
operation and it is identical to the algorithm de-
scribed in (Macherey et al, 2008) for phrase lat-
tices. Algorithm 2 contains the pseudo code.
The complete algorithm then works as follows:
Traversing all nodes in H bottom-up in topolog-
ical order, we proceed for each node v ? V over
its incoming hyperedges and combine in each such
hyperedge e the envelopes associated with the tail
nodes T (e) by computing their sum according to
Algorithm 1 (?-operation). For each incoming
hyperedge e, the resulting envelope is then ex-
panded by applying the rule re to its constituent
line segments. The envelopes associated with dif-
ferent incoming hyperedges of node v are then
combined and reduced according to Algorithm 2
(?-operation). By construction, the envelope at
the root node is the convex hull over the line seg-
ments of all candidate translations that can be de-
rived from the hypergraph.
The suggested algorithm has similar properties
as the algorithm presented in (Macherey et al,
2008). In particular, it has the same upper bound
on the number of line segments that constitute the
envelope at the root node, i.e, the size of this enve-
lope is guaranteed to be no larger than the number
of edges in the transformed hypergraph.
4 Minimum Bayes-Risk Decoding
We first review Minimum Bayes-Risk (MBR) de-
coding for statistical MT. An MBR decoder seeks
the hypothesis with the least expected loss under a
probability model (Bickel and Doksum, 1977). If
we think of statistical MT as a classifier that maps
a source sentence F to a target sentence E, the
MBR decoder can be expressed as follows:
E? = argmin
E??G
?
E?G
L(E,E?)P (E|F ), (1)
where L(E,E?) is the loss between any two hy-
potheses E and E?, P (E|F ) is the probability
model, and G is the space of translations (N -best
list, lattice, or a hypergraph).
MBR decoding for translation can be performed
by reranking an N -best list of hypotheses gener-
ated by an MT system (Kumar and Byrne, 2004).
This reranking can be done for any sentence-
level loss function such as BLEU (Papineni et al,
2001), Word Error Rate, or Position-independent
Error Rate.
Recently, Tromble et al (2008) extended
MBR decoding to translation lattices under an
approximate BLEU score. They approximated
log(BLEU) score by a linear function of n-gram
matches and candidate length. If E and E? are the
reference and the candidate translations respec-
tively, this linear function is given by:
G(E,E?) = ?0|E
?|+
?
w
?|w|#w(E
?)?w(E), (2)
where w is an n-gram present in either E or E?,
and ?0, ?1, ..., ?N are weights which are deter-
mined empirically, where N is the maximum n-
gram order.
Under such a linear decomposition, the MBR
decoder (Equation 1) can be written as
E? = argmax
E??G
?0|E
?|+
?
w
?|w|#w(E
?)p(w|G), (3)
where the posterior probability of an n-gram in the
lattice is given by
p(w|G) =
?
E?G
1w(E)P (E|F ). (4)
Tromble et al (2008) implement the MBR
decoder using Weighted Finite State Automata
(WFSA) operations. First, the set of n-grams
is extracted from the lattice. Next, the posterior
probability of each n-gram is computed. A new
automaton is then created by intersecting each n-
gram with weight (from Equation 2) to an un-
weighted lattice. Finally, the MBR hypothesis is
extracted as the best path in the automaton. We
will refer to this procedure as FSAMBR.
The above steps are carried out one n-gram at
a time. For a moderately large lattice, there can
be several thousands of n-grams and the proce-
dure becomes expensive. We now present an alter-
nate approximate procedure which can avoid this
166
enumeration making the resulting algorithm much
faster than FSAMBR.
4.1 Efficient MBR for lattices
The key idea behind this new algorithm is to
rewrite the n-gram posterior probability (Equa-
tion 4) as follows:
p(w|G) =
?
E?G
?
e?E
f(e, w,E)P (E|F ) (5)
where f(e, w,E) is a score assigned to edge e on
path E containing n-gram w:
f(e, w,E) =
?
?
?
1 w ? e, p(e|G) > p(e?|G),
e? precedes e on E
0 otherwise
(6)
In other words, for each pathE, we count the edge
that contributes n-gramw and has the highest edge
posterior probability relative to its predecessors on
the path E; there is exactly one such edge on each
lattice path E.
We note that f(e, w,E) relies on the full path
E which means that it cannot be computed based
on local statistics. We therefore approximate the
quantity f(e, w,E) with f?(e, w,G) that counts
the edge e with n-gram w that has the highest arc
posterior probability relative to predecessors in the
entire lattice G. f?(e, w,G) can be computed lo-
cally, and the n-gram posterior probability based
on f? can be determined as follows:
p(w|G) =
X
E?G
X
e?E
f?(e, w,G)P (E|F ) (7)
=
X
e?E
1w?ef
?(e, w,G)
X
E?G
1E(e)P (E|F )
=
X
e?E
1w?ef
?(e, w,G)P (e|G),
where P (e|G) is the posterior probability of a lat-
tice edge. The algorithm to perform Lattice MBR
is given in Algorithm 3. For each node t in the lat-
tice, we maintain a quantity Score(w, t) for each
n-gram w that lies on a path from the source node
to t. Score(w, t) is the highest posterior probabil-
ity among all edges on the paths that terminate on t
and contain n-gram w. The forward pass requires
computing the n-grams introduced by each edge;
to do this, we propagate n-grams (up to maximum
order ?1) terminating on each node.
4.2 Extension to Hypergraphs
We next extend the Lattice MBR decoding algo-
rithm (Algorithm 3) to rescore hypergraphs pro-
duced by a SCFG based MT system. Algorithm 4
is an extension to the MBR decoder on lattices
Algorithm 3 MBR Decoding on Lattices
1: Sort the lattice nodes topologically.
2: Compute backward probabilities of each node.
3: Compute posterior prob. of each n-gram:
4: for each edge e do
5: Compute edge posterior probability P (e|G).
6: Compute n-gram posterior probs. P (w|G):
7: for each n-gram w introduced by e do
8: Propagate n? 1 gram suffix to he.
9: if p(e|G) > Score(w, T (e)) then
10: Update posterior probs. and scores:
p(w|G) += p(e|G) ? Score(w, T (e)).
Score(w, he) = p(e|G).
11: else
12: Score(w, he) = Score(w, T (e)).
13: end if
14: end for
15: end for
16: Assign scores to edges (given by Equation 3).
17: Find best path in the lattice (Equation 3).
(Algorithm 3). However, there are important dif-
ferences when computing the n-gram posterior
probabilities (Step 3). In this inside pass, we now
maintain both n-gram prefixes and suffixes (up to
the maximum order?1) on each hypergraph node.
This is necessary because unlike a lattice, new n-
grams may be created at subsequent nodes by con-
catenating words both to the left and the right side
of the n-gram. When the arity of the edge is 2,
a rule has the general form aX1bX2c, where X1
and X2 are sequences from tail nodes. As a result,
we need to consider all new sequences which can
be created by the cross-product of the n-grams on
the two tail nodes. E.g. if X1 = {c, cd, d} and
X2 = {f, g}, then a total of six sequences will
result. In practice, such a cross-product is not pro-
Algorithm 4 MBR Decoding on Hypergraphs
1: Sort the hypergraph nodes topologically.
2: Compute inside probabilities of each node.
3: Compute posterior prob. of each hyperedge P (e|G).
4: Compute posterior prob. of each n-gram:
5: for each hyperedge e do
6: Merge the n-grams on the tail nodes T (e). If the
same n-gram is present on multiple tail nodes, keep
the highest score.
7: Apply the rule on e to the n-grams on T (e).
8: Propagate n? 1 gram prefixes/suffixes to he.
9: for each n-gram w introduced by this hyperedge do
10: if p(e|G) > Score(w, T (e)) then
11: p(w|G) += p(e|G) ? Score(w, T (e))
Score(w, he) = p(e|G)
12: else
13: Score(w, he) = Score(w, T (e))
14: end if
15: end for
16: end for
17: Assign scores to hyperedges (Equation 3).
18: Find best path in the hypergraph (Equation 3).
167
hibitive when the maximum n-gram order in MBR
does not exceed the order of the n-gram language
model used in creating the hypergraph. In the lat-
ter case, we will have a small set of unique prefixes
and suffixes on the tail nodes.
5 MERT for MBR Parameter
Optimization
Lattice MBR Decoding (Equation 3) assumes a
linear form for the gain function (Equation 2).
This linear function contains n + 1 parameters
?0, ?1, ..., ?N , where N is the maximum order of
the n-grams involved. Tromble et al (2008) ob-
tained these factors as a function of n-gram preci-
sions derived from multiple training runs. How-
ever, this does not guarantee that the resulting
linear score (Equation 2) is close to the corpus
BLEU. We now describe how MERT can be used
to estimate these factors to achieve a better ap-
proximation to the corpus BLEU.
We recall that MERT selects weights in a lin-
ear model to optimize an error criterion (e.g. cor-
pus BLEU) on a training set. The lattice MBR
decoder (Equation 3) can be written as a lin-
ear model: E? = argmaxE??G
?N
i=0 ?igi(E
?, F ),
where g0(E?, F ) = |E?| and gi(E?, F ) =?
w:|w|=i #w(E
?)p(w|G).
The linear approximation to BLEU may not
hold in practice for unseen test sets or language-
pairs. Therefore, we would like to allow the de-
coder to backoff to the MAP translation in such
cases. To do that, we introduce an additional fea-
ture function gN+1(E,F ) equal to the original de-
coder cost for this sentence. A weight assignment
of 1.0 for this feature function and zeros for the
other feature functions would imply that the MAP
translation is chosen. We now have a total ofN+2
feature functions which we optimize using MERT
to obtain highest BLEU score on a training set.
6 Experiments
We now describe our experiments to evaluate
MERT and MBR on lattices and hypergraphs, and
show how MERT can be used to tune MBR pa-
rameters.
6.1 Translation Tasks
We report results on two tasks. The first one is
the constrained data track of the NIST Arabic-
to-English (aren) and Chinese-to-English (zhen)
translation task1. On this task, the parallel and the
1http://www.nist.gov/speech/tests/mt
Dataset # of sentences
aren zhen
dev 1797 1664
nist02 1043 878
nist03 663 919
Table 1: Statistics over the NIST dev/test sets.
monolingual data included all the allowed train-
ing sets for the constrained track. Table 1 reports
statistics computed over these data sets. Our de-
velopment set (dev) consists of the NIST 2005 eval
set; we use this set for optimizing MBR parame-
ters. We report results on NIST 2002 and NIST
2003 evaluation sets.
The second task consists of systems for 39
language-pairs with English as the target language
and trained on at most 300M word tokens mined
from the web and other published sources. The de-
velopment and test sets for this task are randomly
selected sentences from the web, and contain 5000
and 1000 sentences respectively.
6.2 MT System Description
Our phrase-based statistical MT system is simi-
lar to the alignment template system described in
(Och and Ney, 2004; Tromble et al, 2008). Trans-
lation is performed using a standard dynamic pro-
gramming beam-search decoder (Och and Ney,
2004) using two decoding passes. The first de-
coder pass generates either a lattice or an N -best
list. MBR decoding is performed in the second
pass.
We also train two SCFG-based MT systems:
a hierarchical phrase-based SMT (Chiang, 2007)
system and a syntax augmented machine transla-
tion (SAMT) system using the approach described
in Zollmann and Venugopal (2006). Both systems
are built on top of our phrase-based systems. In
these systems, the decoder generates an initial hy-
pergraph or anN -best list, which are then rescored
using MBR decoding.
6.3 MERT Results
Table 2 shows runtime experiments for the hyper-
graph MERT implementation in comparison with
the phrase-lattice implementation on both the aren
and the zhen system. The first two columns show
the average amount of time in msecs that either
algorithm requires to compute the upper envelope
when applied to phrase lattices. Compared to the
algorithm described in (Macherey et al, 2008)
which is optimized for phrase lattices, the hyper-
graph implementation causes a small increase in
168
Avg. Runtime/sent [msec]
(Macherey 2008) Suggested Alg.
aren zhen aren zhen
phrase lattice 8.57 7.91 10.30 8.65
hypergraph ? ? 8.19 8.11
Table 2: Average time for computing envelopes.
running time. This increase is mainly due to the
representation of line segments; while the phrase-
lattice implementation stores a single backpointer,
the hypergraph version stores a vector of back-
pointers.
The last two columns show the average amount
of time that is required to compute the upper en-
velope on hypergraphs. For comparison, we prune
hypergraphs to the same density (# of edges per
edge on the best path) and achieve identical run-
ning times for computing the error surface.
6.4 MBR Results
We first compare the new lattice MBR (Algo-
rithm 3) with MBR decoding on 1000-best lists
and FSAMBR (Tromble et al, 2008) on lattices
generated by the phrase-based systems; evaluation
is done using both BLEU and average run-time per
sentence (Table 3). Note that N -best MBR uses
a sentence BLEU loss function. The new lattice
MBR algorithm gives about the same performance
as FSAMBR while yielding a 20X speedup.
We next report the performance of MBR on hy-
pergraphs generated by Hiero/SAMT systems. Ta-
ble 4 compares Hypergraph MBR (HGMBR) with
MAP and MBR decoding on 1000 best lists. On
some systems such as the Arabic-English SAMT,
the gains from Hypergraph MBR over 1000-best
MBR are significant. In other cases, Hypergraph
MBR performs at least as well as N -best MBR.
In all cases, we observe a 7X speedup in run-
time. This shows the usefulness of Hypergraph
MBR decoding as an efficient alternative to N -
best MBR.
6.5 MBR Parameter Tuning with MERT
We now describe the results by tuning MBR n-
gram parameters (Equation 2) using MERT. We
first compute N + 1 MBR feature functions on
each edge of the lattice/hypergraph. We also in-
clude the total decoder cost on the edge as as addi-
tional feature function. MERT is then performed
to optimize the BLEU score on a development set;
For MERT, we use 40 random initial parameters as
well as parameters computed using corpus based
statistics (Tromble et al, 2008).
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
MAP 54.2 64.2 40.1 39.0 -
N -best MBR 54.3 64.5 40.2 39.2 3.7
Lattice MBR
FSAMBR 54.9 65.2 40.6 39.5 3.7
LatMBR 54.8 65.2 40.7 39.4 0.2
Table 3: Lattice MBR for a phrase-based system.
BLEU (%) Avg.
aren zhen time
nist03 nist02 nist03 nist02 (ms.)
Hiero
MAP 52.8 62.9 41.0 39.8 -
N -best MBR 53.2 63.0 41.0 40.1 3.7
HGMBR 53.3 63.1 41.0 40.2 0.5
SAMT
MAP 53.4 63.9 41.3 40.3 -
N -best MBR 53.8 64.3 41.7 41.1 3.7
HGMBR 54.0 64.6 41.8 41.1 0.5
Table 4: Hypergraph MBR for Hiero/SAMT systems.
Table 5 shows results for NIST systems. We
report results on nist03 set and present three sys-
tems for each language pair: phrase-based (pb),
hierarchical (hier), and SAMT; Lattice MBR is
done for the phrase-based system while HGMBR
is used for the other two. We select the MBR
scaling factor (Tromble et al, 2008) based on the
development set; it is set to 0.1, 0.01, 0.5, 0.2, 0.5
and 1.0 for the aren-phrase, aren-hier, aren-samt,
zhen-phrase zhen-hier and zhen-samt systems re-
spectively. For the multi-language case, we train
phrase-based systems and perform lattice MBR
for all language pairs. We use a scaling factor of
0.7 for all pairs. Additional gains can be obtained
by tuning this factor; however, we do not explore
that dimension in this paper. In all cases, we prune
the lattices/hypergraphs to a density of 30 using
forward-backward pruning (Sixtus and Ortmanns,
1999).
We consider a BLEU score difference to be a)
gain if is at least 0.2 points, b) drop if it is at most
-0.2 points, and c) no change otherwise. The re-
sults are shown in Table 6. In both tables, the fol-
lowing results are reported: Lattice/HGMBR with
default parameters (?5, 1.5, 2, 3, 4) computed us-
ing corpus statistics (Tromble et al, 2008),
Lattice/HGMBR with parameters derived from
MERT both without/with the baseline model cost
feature (mert?b, mert+b). For multi-language
systems, we only show the # of language-pairs
with gains/no-changes/drops for each MBR vari-
ant with respect to the MAP translation.
169
We observed in the NIST systems that MERT
resulted in short translations relative to MAP on
the unseen test set. To prevent this behavior,
we modify the MERT error criterion to include
a sentence-level brevity scorer with parameter ?:
BLEU+brevity(?). This brevity scorer penalizes
each candidate translation that is shorter than the
average length over its reference translations, us-
ing a penalty term which is linear in the difference
between either length. We tune ? on the develop-
ment set so that the brevity score of MBR transla-
tion is close to that of the MAP translation.
In the NIST systems, MERT yields small im-
provements on top of MBR with default param-
eters. This is the case for Arabic-English Hi-
ero/SAMT. In all other cases, we see no change
or even a slight degradation due to MERT.
We hypothesize that the default MBR parame-
ters (Tromble et al, 2008) are well tuned. There-
fore there is little gain by additional tuning using
MERT.
In the multi-language systems, the results show
a different trend. We observe that MBR with de-
fault parameters results in gains on 18 pairs, no
differences on 9 pairs, and losses on 12 pairs.
When we optimize MBR features with MERT, the
number of language pairs with gains/no changes/-
drops is 22/5/12. Thus, MERT has a bigger impact
here than in the NIST systems. We hypothesize
that the default MBR parameters are sub-optimal
for some language pairs and that MERT helps to
find better parameter settings. In particular, MERT
avoids the need for manually tuning these param-
eters by language pair.
Finally, when baseline model costs are added
as an extra feature (mert+b), the number of pairs
with gains/no changes/drops is 26/8/5. This shows
that this feature can allow MBR decoding to back-
off to the MAP translation. When MBR does not
produce a higher BLEU score relative to MAP
on the development set, MERT assigns a higher
weight to this feature function. We see such an
effect for 4 systems.
7 Discussion
We have presented efficient algorithms
which extend previous work on lattice-based
MERT (Macherey et al, 2008) and MBR de-
coding (Tromble et al, 2008) to work with
hypergraphs. Our new MERT algorithm can work
with both lattices and hypergraphs. On lattices, it
achieves similar run-times as the implementation
System BLEU (%)
MAP MBR
default mert-b mert+b
aren.pb 54.2 54.8 54.8 54.9
aren.hier 52.8 53.3 53.5 53.7
aren.samt 53.4 54.0 54.4 54.0
zhen.pb 40.1 40.7 40.7 40.9
zhen.hier 41.0 41.0 41.0 41.0
zhen.samt 41.3 41.8 41.6 41.7
Table 5: MBR Parameter Tuning on NIST systems
MBR wrt. MAP default mert-b mert+b
# of gains 18 22 26
# of no-changes 9 5 8
# of drops 12 12 5
Table 6: MBR on Multi-language systems.
described in Macherey et al (2008). The new
Lattice MBR decoder achieves a 20X speedup
relative to either FSAMBR implementation
described in Tromble et al (2008) or MBR on
1000-best lists. The algorithm gives comparable
results relative to FSAMBR. On hypergraphs
produced by Hierarchical and Syntax Augmented
MT systems, our MBR algorithm gives a 7X
speedup relative to 1000-best MBR while giving
comparable or even better performance.
Lattice MBR decoding is obtained under a lin-
ear approximation to BLEU, where the weights
are obtained using n-gram precisions derived from
development data. This may not be optimal in
practice for unseen test sets and language pairs,
and the resulting linear loss may be quite differ-
ent from the corpus level BLEU. In this paper, we
have described how MERT can be employed to
estimate the weights for the linear loss function
to maximize BLEU on a development set. On an
experiment with 40 language pairs, we obtain im-
provements on 26 pairs, no difference on 8 pairs
and drops on 5 pairs. This was achieved with-
out any need for manual tuning for each language
pair. The baseline model cost feature helps the al-
gorithm effectively back off to the MAP transla-
tion in language pairs where MBR features alone
would not have helped.
MERT and MBR decoding are popular tech-
niques for incorporating the final evaluation met-
ric into the development of SMT systems. We be-
lieve that our efficient algorithms will make them
more widely applicable in both SCFG-based and
phrase-based MT systems.
170
References
M. Berg, O. Cheong, M. Krefeld, and M. Overmars,
2008. Computational Geometry: Algorithms and
Applications, chapter 13, pages 290?296. Springer-
Verlag, 3rd edition.
P. J. Bickel and K. A. Doksum. 1977. Mathematical
Statistics: Basic Ideas and Selected topics. Holden-
Day Inc., Oakland, CA, USA.
D. Chiang. 2007. Hierarchical phrase based transla-
tion . Computational Linguistics, 33(2):201 ? 228.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable Inference
and Training of Context-Rich Syntactic Translation
Models. . In COLING/ACL, Sydney, Australia.
L. Huang. 2008. Advanced Dynamic Programming
in Semiring and Hypergraph Frameworks. In COL-
ING, Manchester, UK.
S. Kumar and W. Byrne. 2004. Minimum Bayes-
Risk Decoding for Statistical Machine Translation.
In HLT-NAACL, Boston, MA, USA.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based Minimum Error Rate Train-
ing for Statistical Machine Translation. In EMNLP,
Honolulu, Hawaii, USA.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-Based
Translation. In ACL, Columbus, OH, USA.
F. Och and H. Ney. 2004. The Alignment Template
Approach to Statistical Machine Translation. Com-
putational Linguistics, 30(4):417 ? 449.
F. Och. 2003. Minimum Error Rate Training in Statis-
tical Machine Translation. In ACL, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. Technical Report RC22176
(W0109-022), IBM Research Division.
A. Sixtus and S. Ortmanns. 1999. High Quality
Word Graphs Using Forward-Backward Pruning. In
ICASSP, Phoenix, AZ, USA.
R. Tromble, S. Kumar, F. Och, andW.Macherey. 2008.
Lattice Minimum Bayes-Risk Decoding for Statis-
tical Machine Translation. In EMNLP, Honolulu,
Hawaii.
H. Zhang and D. Gildea. 2008. Efficient Multi-pass
Decoding for Synchronous Context Free Grammars.
In ACL, Columbus, OH, USA.
A. Zollmann and A. Venugopal. 2006. Syntax Aug-
mented Machine Translation via Chart Parsing. In
HLT-NAACL, New York, NY, USA.
171
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782?790,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Gibbs Sampler for Phrasal Synchronous Grammar Induction
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Trevor Cohn?
tcohn@inf.ed.ac.uk
Miles Osborne?
miles@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
We present a phrasal synchronous gram-
mar model of translational equivalence.
Unlike previous approaches, we do not
resort to heuristics or constraints from
a word-alignment model, but instead
directly induce a synchronous grammar
from parallel sentence-aligned corpora.
We use a hierarchical Bayesian prior
to bias towards compact grammars with
small translation units. Inference is per-
formed using a novel Gibbs sampler
over synchronous derivations. This sam-
pler side-steps the intractability issues of
previous models which required inference
over derivation forests. Instead each sam-
pling iteration is highly efficient, allowing
the model to be applied to larger transla-
tion corpora than previous approaches.
1 Introduction
The field of machine translation has seen many
advances in recent years, most notably the shift
from word-based (Brown et al, 1993) to phrase-
based models which use token n-grams as trans-
lation units (Koehn et al, 2003). Although very
few researchers use word-based models for trans-
lation per se, such models are still widely used in
the training of phrase-based models. These word-
based models are used to find the latent word-
alignments between bilingual sentence pairs, from
which a weighted string transducer can be induced
(either finite state (Koehn et al, 2003) or syn-
chronous context free grammar (Chiang, 2007)).
Although wide-spread, the disconnect between the
translation model and the alignment model is arti-
ficial and clearly undesirable. Word-based mod-
els are incapable of learning translational equiv-
alences between non-compositional phrasal units,
while the algorithms used for inducing weighted
transducers from word-alignments are based on
heuristics with little theoretical justification. A
model which can fulfil both roles would address
both the practical and theoretical short-comings of
the machine translation pipeline.
The machine translation literature is littered
with various attempts to learn a phrase-based
string transducer directly from aligned sentence
pairs, doing away with the separate word align-
ment step (Marcu and Wong, 2002; Cherry and
Lin, 2007; Zhang et al, 2008b; Blunsom et al,
2008). Unfortunately none of these approaches
resulted in an unqualified success, due largely
to intractable estimation. Large training sets with
hundreds of thousands of sentence pairs are com-
mon in machine translation, leading to a parameter
space of billions or even trillions of possible bilin-
gual phrase-pairs. Moreover, the inference proce-
dure for each sentence pair is non-trivial, prov-
ing NP-complete for learning phrase based models
(DeNero and Klein, 2008) or a high order poly-
nomial (O(|f |3|e|3))1 for a sub-class of weighted
synchronous context free grammars (Wu, 1997).
Consequently, for such models both the param-
eterisation and approximate inference techniques
are fundamental to their success.
In this paper we present a novel SCFG transla-
tion model using a non-parametric Bayesian for-
mulation. The model includes priors to impose a
bias towards small grammars with few rules, each
of which is as simple as possible (e.g., terminal
productions consisting of short phrase pairs). This
explicitly avoids the degenerate solutions of max-
imum likelihood estimation (DeNero et al, 2006),
without resort to the heuristic estimator of Koehn
et al (2003). We develop a novel Gibbs sampler
to perform inference over the latent synchronous
derivation trees for our training instances. The
sampler reasons over the infinite space of possi-
ble translation units without recourse to arbitrary
restrictions (e.g., constraints drawn from a word-
alignment (Cherry and Lin, 2007; Zhang et al,
2008b) or a grammar fixed a priori (Blunsom et al,
1f and e are the input and output sentences respectively.
782
2008)). The sampler performs local edit operations
to nodes in the synchronous trees, each of which
is very fast, leading to a highly efficient inference
technique. This allows us to train the model on
large corpora without resort to punitive length lim-
its, unlike previous approaches which were only
applied to small data sets with short sentences.
This paper is structured as follows: In Sec-
tion 3 we argue for the use of efficient sam-
pling techniques over SCFGs as an effective solu-
tion to the modelling and scaling problems of
previous approaches. We describe our Bayesian
SCFG model in Section 4 and a Gibbs sampler
to explore its posterior. We apply this sampler
to build phrase-based and hierarchical translation
models and evaluate their performance on small
and large corpora.
2 Synchronous context free grammar
A synchronous context free grammar (SCFG,
(Lewis II and Stearns, 1968)) generalizes context-
free grammars to generate strings concurrently in
two (or more) languages. A string pair is gener-
ated by applying a series of paired rewrite rules
of the form, X ? ?e, f ,a?, where X is a non-
terminal, e and f are strings of terminals and non-
terminals and a specifies a one-to-one alignment
between non-terminals in e and f . In the context of
SMT, by assigning the source and target languages
to the respective sides of a probabilistic SCFG it
is possible to describe translation as the process
of parsing the source sentence, which induces a
parallel tree structure and translation in the tar-
get language (Chiang, 2007). Figure 1 shows an
example derivation for Japanese to English trans-
lation using an SCFG. For efficiency reasons we
only consider binary or ternary branching rules
and don?t allow rules to mix terminals and non-
terminals. This allows our sampler to more effi-
ciently explore the space of grammars (Section
4.2), however more expressive grammars would be
a straightforward extension of our model.
3 Related work
Most machine translation systems adopt the
approach of Koehn et al (2003) for ?training?
a phrase-based translation model.2 This method
starts with a word-alignment, usually the latent
state of an unsupervised word-based aligner such
2We include grammar based transducers, such as Chiang
(2007) and Marcu et al (2006), in our definition of phrase-
based models.
Grammar fragment:
X ? ?X
1
X
2
X
3
, X
1
X
3
X
2
?
X ? ?John-ga, John?
X ? ?ringo-o, an apple?
X ? ?tabeta, ate?
Sample derivation:
?S
1
,S
1
? ? ?X
2
, X
2
?
? ?X
3
X
4
X
5
, X
3
X
5
X
4
?
? ?John-ga X
4
X
5
, John X
5
X
4
?
? ?John-ga ringo-o X
5
, John X
5
an apple?
? ?John-ga ringo-o tabeta, John ate an apple?
Figure 1: A fragment of an SCFG with a ternary
non-terminal expansion and three terminal rules.
as GIZA++. Various heuristics are used to com-
bine source-to-target and target-to-source align-
ments, after which a further heuristic is used to
read off phrase pairs which are ?consistent? with
the alignment. Although efficient, the sheer num-
ber of somewhat arbitrary heuristics makes this
approach overly complicated.
A number of authors have proposed alterna-
tive techniques for directly inducing phrase-based
translation models from sentence aligned data.
Marcu and Wong (2002) proposed a phrase-based
alignment model which suffered from a massive
parameter space and intractable inference using
expectation maximisation. Taking a different tack,
DeNero et al (2008) presented an interesting new
model with inference courtesy of a Gibbs sampler,
which was better able to explore the full space of
phrase translations. However, the efficacy of this
model is unclear due to the small-scale experi-
ments and the short sampling runs. In this work we
also propose a Gibbs sampler but apply it to the
polynomial space of derivation trees, rather than
the exponential space of the DeNero et al (2008)
model. The restrictions imposed by our tree struc-
ture make sampling considerably more efficient
for long sentences.
Following the broad shift in the field from finite
state transducers to grammar transducers (Chiang,
2007), recent approaches to phrase-based align-
ment have used synchronous grammar formalisms
permitting polynomial time inference (Wu, 1997;
783
Cherry and Lin, 2007; Zhang et al, 2008b; Blun-
som et al, 2008). However this asymptotic time
complexity is of high enough order (O(|f |3|e|3))
that inference is impractical for real translation
data. Proposed solutions to this problem include
imposing sentence length limits, using small train-
ing corpora and constraining the search space
using a word-alignment model or parse tree. None
of these limitations are particularly desirable as
they bias inference. As a result phrase-based align-
ment models are not yet practical for the wider
machine translation community.
4 Model
Our aim is to induce a grammar from a train-
ing set of sentence pairs. We use Bayes? rule
to reason under the posterior over grammars,
P (g|x) ? P (x|g)P (g), where g is a weighted
SCFG grammar and x is our training corpus. The
likelihood term, P (x|g), is the probability of the
training sentence pairs under the grammar, while
the prior term, P (g), describes our initial expec-
tations about what consitutes a plausible gram-
mar. Specifically we incorporate priors encoding
our preference for a briefer and more succinct
grammar, namely that: (a) the grammar should be
small, with few rules rewriting each non-terminal;
and (b) terminal rules which specify phrasal trans-
lation correspondence should be small, with few
symbols on their right hand side.
Further, Bayesian non-parametrics allow the
capacity of the model to grow with the data.
Thereby we avoid imposing hard limits on the
grammar (and the thorny problem of model selec-
tion), but instead allow the model to find a gram-
mar appropriately sized for its training data.
4.1 Non-parametric form
Our Bayesian model of SCFG derivations resem-
bles that of Blunsom et al (2008). Given a gram-
mar, each sentence is generated as follows. Start-
ing with a root non-terminal (z1), rewrite each
frontier non-terminal (zi) using a rule chosen from
our grammar expanding zi. Repeat until there are
no remaining frontier non-terminals. This gives
rise to the following derivation probability:
p(d) = p(z1)
?
ri?d
p(ri|zi)
where the derivation is a sequence of rules d =
(r1, . . . , rn), and zi denotes the root node of ri.
We allow two types of rules: non-terminal and
terminal expansions. The former rewrites a non-
terminal symbol as a string of two or three non-
terminals along with an alignment, specifying
the corresponding ordering of the child trees in
the source and target language. Terminal expan-
sions rewrite a non-terminal as a pair of terminal
n-grams, representing a phrasal translation pair,
where either but not both may be empty.
Each rule in the grammar, ri, is generated from
its root symbol, zi, by first choosing a rule type
ti ? {TERM, NON-TERM} from a Bernoulli distribu-
tion, ri ? Bernoulli(?). We treat ? as a random
variable with its own prior, ? ? Beta(?R, ?R) and
integrate out the parameters, ?. This results in the
following conditional probability for ti:
p(ti|r?i, zi, ?R) =
n?iti,zi + ?
R
n?i?,zi + 2?R
where n?iri,zi is the number of times ri has been
used to rewrite zi in the set of all other rules, r?i,
and n?i?,zi =
?
r n
?i
r,zi is the total count of rewriting
zi. The Dirichlet (and thus Beta) distribution are
exchangeable, meaning that any permutation of its
events are equiprobable. This allows us to reason
about each event given previous and subsequent
events (i.e., treat each item as the ?last?.)
When ti = NON-TERM, we generate a binary
or ternary non-terminal production. The non-
terminal sequence and alignment are drawn from
(z, a) ? ?Nzi and, as before, we define a prior over
the parameters, ?Nzi ? Dirichlet(?
T ), and inte-
grate out ?Nzi . This results in the conditional prob-
ability:
p(ri|ti = NON-TERM, r?i, zi, ?N ) =
nN,?iri,zi + ?
N
nN,?i?,zi + |N |?N
where nN,?iri,zi is the count of rewriting zi with non-
terminal rule ri, n
N,?i
?,zi the total count over all non-
terminal rules and |N | is the number of unique
non-terminal rules.
For terminal productions (ti = TERM) we first
decide whether to generate a phrase in both lan-
guages or in one language only, according to a
fixed probability pnull.3 Contingent on this deci-
sion, the terminal strings are then drawn from
3To discourage null alignments, we used pnull = 10?10
for this value in the experiments we report below.
784
either ?Pzi for phrase pairs or ?
null for single lan-
guage phrases. We choose Dirichlet process (DP)
priors for these parameters:
?Pzi ? DP(?
P , PP1 )
?nullzi ? DP(?
null, Pnull1 )
where the base distributions, PP1 and P
null
1 , range
over phrase pairs or monolingual phrases in either
language, respectively.
The most important choice for our model is
the priors on the parameters of these terminal
distributions. Phrasal SCFG models are subject
to a degenerate maximum likelihood solution in
which all probability mass is placed on long, or
whole sentence, phrase translations (DeNero et al,
2006). Therefore, careful consideration must be
given when specifying the P1 distribution on ter-
minals in order to counter this behavior.
To construct a prior over string pairs, first we
define the probability of a monolingual string (s):
PX0 (s) = PPoisson(|s|; 1)?
1
V |s|X
where the PPoisson(k; 1) is the probability under a
Poisson distribution of length k given an expected
length of 1, while VX is the vocabulary size of
language X . This distribution has a strong bias
towards short strings. In particular note that gener-
ally a string of length k will be less probable than
two of length k2 , a property very useful for finding
?minimal? translation units. This contrasts with a
geometric distribution in which a string of length
k will be more probable than its segmentations.
We define Pnull1 as the string probability of the
non-null part of the rule:
Pnull1 (z ? ?e, f?) =
{ 1
2P
E
0 (e) if |f | = 0
1
2P
F
0 (f) if |e| = 0
The terminal translation phrase pair distribution
is a hierarchical Dirichlet Process in which each
phrase are independently distributed according to
DPs:4
PP1 (z ? ?e, f?) = ?
E
z (e)? ?
F
z (f)
?Ez ? DP(?
PE , PE0 )
4This prior is similar to one used by DeNero et al (2008),
who used the expected table count approximation presented
in Goldwater et al (2006). However, Goldwater et al (2006)
contains two major errors: omitting P0, and using the trun-
cated Taylor series expansion (Antoniak, 1974) which fails
for small ?P0 values common in these models. In this work
we track table counts directly.
and ?Fz is defined analogously. This prior encour-
ages frequent phrases to participate in many differ-
ent translation pairs. Moreover, as longer strings
are likely to be less frequent in the corpus this has
a tendency to discourage long translation units.
4.2 A Gibbs sampler for derivations
Markov chain Monte Carlo sampling allows us to
perform inference for the model described in 4.1
without restricting the infinite space of possible
translation rules. To do this we need a method for
sampling a derivation for a given sentence pair
from p(d|d?). One possible approach would be
to first build a packed chart representation of the
derivation forest, calculate the inside probabilities
of all cells in this chart, and then sample deriva-
tions top-down according to their inside probabil-
ities (analogous to monolingual parse tree sam-
pling described in Johnson et al (2007)). A prob-
lem with this approach is that building the deriva-
tion forest would take O(|f |3|e|3) time, which
would be impractical for long sentences.
Instead we develop a collapsed Gibbs sam-
pler (Teh et al, 2006) which draws new sam-
ples by making local changes to the derivations
used in a previous sample. After a period of burn
in, the derivations produced by the sampler will
be drawn from the posterior distribution, p(d|x).
The advantage of this algorithm is that we only
store the current derivation for each training sen-
tence pair (together these constitute the state of
the sampler), but never need to reason over deriva-
tion forests. By integrating over (collapsing) the
parameters we only store counts of rules used
in the current sampled set of derivations, thereby
avoiding explicitly representing the possibly infi-
nite space of translation pairs.
We define two operators for our Gibbs sam-
pler, each of which re-samples local derivation
structures. Figures 2 and 4 illustrate the permu-
tations these operators make to derivation trees.
The omitted tree structure in these figures denotes
the Markov blanket of the operator: the structure
which is held constant when enumerating the pos-
sible outcomes for an operator.
The Split/Join operator iterates through the
positions between each source word sampling
whether a terminal boundary should exist at
that position (Figure 2). If the source position
785
... ... ...
... ...
... ...
... ...
Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The
dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a
solid line is a null alignment.
...
......
...
...
......
...
Figure 4: Rule insert/delete sampler. A pair of
adjacent nodes in a ternary rule can be re-parented
as a binary rule, or vice-versa.
falls between two existing terminals whose tar-
get phrases are adjacent, then any new target seg-
mentation within those target phrases can be sam-
pled, including null alignments. If the two exist-
ing terminals also share the same parent, then any
possible re-ordering is also a valid outcome, as
is removing the terminal boundary to form a sin-
gle phrase pair. Otherwise, if the visited boundary
point falls within an existing terminal, then all tar-
get split and re-orderings are possible outcomes.
The probability for each of these configurations
is evaluated (see Figure 3) from which the new
configuration is sampled.
While the first operator is theoretically capa-
ble of exploring the entire derivation forest (by
flattening the tree into a single phrase and then
splitting), the series of moves required would be
highly improbable. To allow for faster mixing we
employ the Insert/Delete operator which adds and
deletes the parent non-terminal of a pair of adja-
cent nodes. This is illustrated in Figure 4. The
update equations are analogous to those used for
the Split/Join operator in Figure 3. In order for this
operator to be effective we need to allow greater
than binary branching nodes, otherwise deleting a
nodes would require sampling from a much larger
set of outcomes. Hence our adoption of a ternary
branching grammar. Although such a grammar
would be very inefficient for a dynamic program-
ming algorithm, it allows our sampler to permute
the internal structure of the trees more easily.
4.3 Hyperparameter Inference
Our model is parameterised by a vector of hyper-
parameters, ? = (?R, ?N , ?P , ?PE , ?PF , ?null),
which control the sparsity assumption over var-
ious model parameters. We could optimise each
concentration parameter on the training corpus by
hand, however this would be quite an onerous task.
Instead we perform inference over the hyperpa-
rameters following Goldwater and Griffiths (2007)
by defining a vague gamma prior on each con-
centration parameter, ?x ? Gamma(10?4, 104).
This hyper-prior is relatively benign, allowing the
model to consider a wide range of values for
the hyperparameter. We sample a new value for
each ?x using a log-normal distribution with mean
?x and variance 0.3, which is then accepted into
the distribution p(?x|d, ??) using the Metropolis-
Hastings algorithm. Unlike the Gibbs updates, this
calculation cannot be distributed over a cluster
(see Section 4.4) and thus is very costly. Therefore
for small corpora we re-sample the hyperparame-
ter after every pass through the corpus, for larger
experiments we only re-sample every 20 passes.
4.4 A Distributed approximation
While employing a collapsed Gibbs sampler
allows us to efficiently perform inference over the
786
p(JOIN) ? p(ti = TERM|zi, r?)? p(ri = (zi ? ?e, f?)|zi, r?) (1)
p(SPLIT) ? p(ti = NON-TERM|zi, r?)? p(ri = (zi ? ?zl, zr, ai?)|zi, r
?) (2)
? p(tl = TERM|ti, zi, r
?)? p(rl = (zl ? ?el, fl?)|zl, r
?)
? p(tr = TERM|ti, tl, zi, r
?)? p(rr = (zr ? ?er, fr?)|zl, r
? ? (zl ? ?el, fl?))
Figure 3: Gibbs sampling equations for the competing configurations of the Split/Join sampler, shown in
Figure 2. Eq. (1) corresponds to the top-left configuration, and (2) the remaining configurations where the
choice of el, fl, er, fr and ai specifies the string segmentation and the alignment (monotone or reordered).
massive space of possible grammars, it induces
dependencies between all the sentences in the
training corpus. These dependencies make it diffi-
cult to scale our approach to larger corpora by dis-
tributing it across a number of processors. Recent
work (Newman et al, 2007; Asuncion et al, 2008)
suggests that good practical parallel performance
can be achieved by having multiple processors
independently sample disjoint subsets of the cor-
pus. Each process maintains a set of rule counts for
the entire corpus and communicates the changes
it has made to its section of the corpus only
after sampling every sentence in that section. In
this way each process is sampling according to
a slightly ?out-of-date? distribution. However, as
we confirm in Section 5 the performance of this
approximation closely follows the exact collapsed
Gibbs sampler.
4.5 Extracting a translation model
Although we could use our model directly as a
decoder to perform translation, its simple hier-
archical reordering parameterisation is too weak
to be effective in this mode. Instead we use our
sampler to sample a distribution over translation
models for state-of-the-art phrase based (Moses)
and hierarchical (Hiero) decoders (Koehn et al,
2007; Chiang, 2007). Each sample from our model
defines a hierarchical alignment on which we can
apply the standard extraction heuristics of these
models. By extracting from a sequence of samples
we can directly infer a distribution over phrase
tables or Hiero grammars.
5 Evaluation
Our evaluation aims to determine whether the
phrase/SCFG rule distributions created by sam-
pling from the model described in Section 4
impact upon the performance of state-of-the-
art translation systems. We conduct experiments
translating both Chinese (high reordering) and
Arabic (low reordering) into English. We use the
GIZA++ implementation of IBM Model 4 (Brown
et al, 1993; Och and Ney, 2003) coupled with the
phrase extraction heuristics of Koehn et al (2003)
and the SCFG rule extraction heuristics of Chiang
(2007) as our benchmark. All the SCFG models
employ a single X non-terminal, we leave experi-
ments with multiple non-terminals to future work.
Our hypothesis is that our grammar based
induction of translation units should benefit lan-
guage pairs with significant reordering more than
those with less. While for mostly monotone trans-
lation pairs, such as Arabic-English, the bench-
mark GIZA++-based system is well suited due to
its strong monotone bias (the sequential Markov
model and diagonal growing heuristic).
We conduct experiments on both small and
large corpora to allow a range of alignment quali-
ties and also to verify the effectiveness of our dis-
tributed approximation of the Bayesian inference.
The samplers are initialised with trees created
from GIZA++ Model 4 alignments, altered such
that they are consistent with our ternary grammar.
This is achieved by using the factorisation algo-
rithm of Zhang et al (2008a) to first create ini-
tial trees. Where these factored trees contain nodes
with mixed terminals and non-terminals, or more
than three non-terminals, we discard alignment
points until the node factorises correctly. As the
alignments contain many such non-factorisable
nodes, these trees are of poor quality. However,
all samplers used in these experiments are first
?burnt-in? for 1000 full passes through the data.
This allows the sampler to diverge from its ini-
tialisation condition, and thus gives us confidence
that subsequent samples will be drawn from the
posterior. An expectation over phrase tables and
Hiero grammars is built from every 50th sample
after the burn-in, up until the 1500th sample.
We evaluate the translation models using IBM
BLEU (Papineni et al, 2001). Table 1 lists the
statistics of the corpora used in these experiments.
787
IWSLT NIST
English?Chinese English?Chinese English?Arabic
Sentences 40k 300k 290k
Segs./Words 380k 340k 11.0M 8.6M 9.3M 8.5M
Av. Sent. Len. 9 8 36 28 32 29
Longest Sent. 75 64 80 80 80 80
Table 1: Corpora statistics.
System Test 05
Moses (Heuristic) 47.3
Moses (Bayes SCFG) 49.6
Hiero (Heuristic) 48.3
Hiero (Bayes SCFG) 51.8
Table 2: IWSLT Chinese to English translation.
5.1 Small corpus
Firstly we evaluate models trained on a small
Chinese-English corpus using a Gibbs sampler on
a single CPU. This corpus consists of transcribed
utterances made available for the IWSLT work-
shop (Eck and Hori, 2005). The sparse counts and
high reordering for this corpus means the GIZA++
model produces very poor alignments.
Table 2 shows the results for the benchmark
Moses and Hiero systems on this corpus using
both the heuristic phrase estimation, and our pro-
posed Bayesian SCFG model. We can see that
our model has a slight advantage. When we look
at the grammars extracted by the two models we
note that the SCFG model creates considerably
more translation rules. Normally this would sug-
gest the alignments of the SCFG model are a lot
sparser (more unaligned tokens) than those of the
heuristic, however this is not the case. The pro-
jected SCFG derivations actually produce more
alignment points. However these alignments are
much more locally consistent, containing fewer
spurious off-diagonal alignments, than the heuris-
tic (see Figure 5), and thus produce far more valid
phrases/rules.
5.2 Larger corpora
We now test our model?s performance on a larger
corpus, representing a realistic SMT experiment
with millions of words and long sentences. The
Chinese-English training data consists of the FBIS
corpus (LDC2003E14) and the first 100k sen-
tences from the Sinorama corpus (LDC2005E47).
The Arabic-English training data consists of
the eTIRR corpus (LDC2004E72), the Arabic
l
l
l
l
l
l
l
l
l l
l l
Number of Sampling Passes
Negative 
Log?Post
erior
l l
l
l
l
l
l
l l
l l
476
478
480
482
484
486
488
490
20 40 60 80 100 120 140 160 180 200 220 240
single (exact)distributed
Figure 6: The posterior for the single CPU sampler
and distributed approximation are roughly equiva-
lent over a sampling run.
news corpus (LDC2004T17), the Ummah cor-
pus (LDC2004T18), and the sentences with confi-
dence c > 0.995 in the ISI automatically extracted
web parallel corpus (LDC2006T02). The Chinese
text was segmented with a CRF-based Chinese
segmenter optimized for MT (Chang et al, 2008).
The Arabic text was preprocessed according to the
D2 scheme of Habash and Sadat (2006), which
was identified as optimal for corpora this size. The
parameters of the NIST systems were tuned using
Och?s algorithm to maximize BLEU on the MT02
test set (Och, 2003).
To evaluate whether the approximate distributed
inference algorithm described in Section 4.4 is
effective, we compare the posterior probability of
the training corpus when using a single machine,
and when the inference is distributed on an eight
core machine. Figure 6 plots the mean posterior
and standard error for five independent runs for
each scenario. Both sets of runs performed hyper-
parameter inference every twenty passes through
the data. It is clear from the training curves that the
distributed approximation tracks the corpus prob-
ability of the correct sampler sufficiently closely.
This concurs with the findings of Newman et al
788
?? ? ?? ?? ? ?? ?? ? ?? ??balance
of
rights
and
obligations
an
important
wto
characteristic
(a) Giza++
?? ? ?? ?? ? ?? ?? ? ?? ??balance
of
rights
and
obligations
an
important
wto
characteristic
(b) Gibbs
Figure 5: Alignment example. The synchronous tree structure is shown for (b) using brackets to indicate
constituent spans; these are omitted for single token constituents. The right alignment is roughly correct,
except that ?of? and ?an? should be left unaligned (? ?to be? is missing from the English translation).
System MT03 MT04 MT05
Moses (Heuristic) 26.2 30.0 25.3
Moses (Bayes SCFG) 26.4 30.2 25.8
Hiero (Heuristic) 26.4 30.8 25.4
Hiero (Bayes SCFG) 26.7 30.9 26.0
Table 3: NIST Chinese to English translation.
System MT03 MT04 MT05
Moses (Heuristic) 48.5 43.9 49.2
Moses (Bayes SCFG) 48.5 43.5 48.7
Hiero (Heuristic) 48.1 43.5 48.4
Hiero (Bayes SCFG) 48.4 43.4 47.7
Table 4: NIST Arabic to English translation.
(2007) who also observed very little empirical dif-
ference between the sampler and its distributed
approximation.
Tables 3 and 4 show the result on the two NIST
corpora when running the distributed sampler on
a single 8-core machine.5 These scores tally with
our initial hypothesis: that the hierarchical struc-
ture of our model suits languages that exhibit less
monotone reordering.
Figure 5 shows the projected alignment of a
headline from the thousandth sample on the NIST
Chinese data set. The effect of the grammar based
alignment can clearly be seen. Where the combi-
nation of GIZA++ and the heuristics creates out-
lier alignments that impede rule extraction, the
SCFG imposes a more rigid hierarchical struc-
ture on the alignments. We hypothesise that this
property may be particularly useful for syntac-
tic translation models which often have difficulty
5Producing the 1.5K samples for each experiment took
approximately one day.
with inconsistent word alignments not correspond-
ing to syntactic structure.
The combined evidence of the ability of our
Gibbs sampler to improve posterior likelihood
(Figure 6) and our translation experiments demon-
strate that we have developed a scalable and effec-
tive method for performing inference over phrasal
SCFG, without compromising the strong theoreti-
cal underpinnings of our model.
6 Discussion and Conclusion
We have presented a Bayesian model of SCFG
induction capable of capturing phrasal units of
translational equivalence. Our novel Gibbs sam-
pler over synchronous derivation trees can effi-
ciently draw samples from the posterior, overcom-
ing the limitations of previous models when deal-
ing with long sentences. This avoids explicitly
representing the full derivation forest required by
dynamic programming approaches, and thus we
are able to perform inference without resorting to
heuristic restrictions on the model.
Initial experiments suggest that this model per-
forms well on languages for which the monotone
bias of existing alignment and heuristic phrase
extraction approaches fail. These results open the
way for the development of more sophisticated
models employing grammars capable of capturing
a wide range of translation phenomena. In future
we envision it will be possible to use the tech-
niques developed here to directly induce gram-
mars which match state-of-the-art decoders, such
as Hiero grammars or tree substitution grammars
of the form used by Galley et al (2004).
789
Acknowledgements
The authors acknowledge the support of
the EPSRC (Blunsom & Osborne, grant
EP/D074959/1; Cohn, grant GR/T04557/01)
and the GALE program of the Defense Advanced
Research Projects Agency, Contract No. HR0011-
06-2-001 (Dyer).
References
C. E. Antoniak. 1974. Mixtures of dirichlet processes with
applications to bayesian nonparametric problems. The
Annals of Statistics, 2(6):1152?1174.
A. Asuncion, P. Smyth, M. Welling. 2008. Asynchronous
distributed learning of topic models. In NIPS. MIT Press.
P. Blunsom, T. Cohn, M. Osborne. 2008. Bayesian syn-
chronous grammar induction. In Proceedings of NIPS 21,
Vancouver, Canada.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, R. L. Mercer.
1993. The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Linguistics,
19(2):263?311.
P.-C. Chang, D. Jurafsky, C. D. Manning. 2008. Optimizing
Chinese word segmentation for machine translation per-
formance. In Proc. of the Third Workshop on Machine
Translation, Prague, Czech Republic.
C. Cherry, D. Lin. 2007. Inversion transduction grammar for
joint phrasal translation modeling. In Proc. of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST 2007), Rochester, USA.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. DeNero, D. Klein. 2008. The complexity of phrase align-
ment problems. In Proceedings of ACL-08: HLT, Short
Papers, 25?28, Columbus, Ohio. Association for Compu-
tational Linguistics.
J. DeNero, D. Gillick, J. Zhang, D. Klein. 2006. Why gener-
ative phrase models underperform surface heuristics. In
Proc. of the HLT-NAACL 2006 Workshop on Statistical
Machine Translation, 31?38, New York City.
J. DeNero, A. Bouchard-Co?te?, D. Klein. 2008. Sampling
alignment structure under a Bayesian translation model.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, 314?323, Hon-
olulu, Hawaii. Association for Computational Linguistics.
M. Eck, C. Hori. 2005. Overview of the IWSLT 2005 eval-
uation campaign. In Proc. of the International Workshop
on Spoken Language Translation, Pittsburgh.
M. Galley, M. Hopkins, K. Knight, D. Marcu. 2004. What?s
in a translation rule? In Proc. of the 4th International Con-
ference on Human Language Technology Research and
5th Annual Meeting of the NAACL (HLT-NAACL 2004),
Boston, USA.
S. Goldwater, T. Griffiths. 2007. A fully bayesian approach
to unsupervised part-of-speech tagging. In Proc. of the
45th Annual Meeting of the ACL (ACL-2007), 744?751,
Prague, Czech Republic.
S. Goldwater, T. Griffiths, M. Johnson. 2006. Contex-
tual dependencies in unsupervised word segmentation. In
Proc. of the 44th Annual Meeting of the ACL and 21st
International Conference on Computational Linguistics
(COLING/ACL-2006), Sydney.
N. Habash, F. Sadat. 2006. Arabic preprocessing schemes
for statistical machine translation. In Proc. of the 6th
International Conference on Human Language Technol-
ogy Research and 7th Annual Meeting of the NAACL
(HLT-NAACL 2006), New York City. Association for
Computational Linguistics.
M. Johnson, T. Griffiths, S. Goldwater. 2007. Bayesian
inference for PCFGs via Markov chain Monte Carlo. In
Proc. of the 7th International Conference on Human Lan-
guage Technology Research and 8th Annual Meeting of the
NAACL (HLT-NAACL 2007), 139?146, Rochester, New
York.
P. Koehn, F. J. Och, D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of the 3rd International Con-
ference on Human Language Technology Research and
4th Annual Meeting of the NAACL (HLT-NAACL 2003),
81?88, Edmonton, Canada.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens,
C. Dyer, O. Bojar, A. Constantin, E. Herbst. 2007. Moses:
Open source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the ACL (ACL-2007),
Prague.
P. M. Lewis II, R. E. Stearns. 1968. Syntax-directed trans-
duction. J. ACM, 15(3):465?488.
D. Marcu, W. Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Proc. of the
2002 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2002), 133?139, Philadelphia.
Association for Computational Linguistics.
D. Marcu, W. Wang, A. Echihabi, K. Knight. 2006. SPMT:
Statistical machine translation with syntactified target lan-
guage phrases. In Proc. of the 2006 Conference on Empir-
ical Methods in Natural Language Processing (EMNLP-
2006), 44?52, Sydney, Australia.
D. Newman, A. Asuncion, P. Smyth, M. Welling. 2007.
Distributed inference for latent dirichlet alocation. In
NIPS. MIT Press.
F. J. Och, H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?52.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of the 41st Annual Meeting
of the ACL (ACL-2003), 160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, W. Zhu. 2001. Bleu: a
method for automatic evaluation of machine translation,
2001.
Y. W. Teh, M. I. Jordan, M. J. Beal, D. M. Blei. 2006.
Hierarchical Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
D. Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23(3):377?403.
H. Zhang, D. Gildea, D. Chiang. 2008a. Extracting syn-
chronous grammar rules from word-level alignments in
linear time. In Proc. of the 22th International Con-
ference on Computational Linguistics (COLING-2008),
1081?1088, Manchester, UK.
H. Zhang, C. Quirk, R. C. Moore, D. Gildea. 2008b.
Bayesian learning of non-compositional phrases with syn-
chronous parsing. In Proc. of the 46th Annual Conference
of the Association for Computational Linguistics: Human
Language Technologies (ACL-08:HLT), 97?105, Colum-
bus, Ohio.
790
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation
?
Zhifei Li, Chris Callison-Burch, Chris Dyer
?
, Juri Ganitkevitch
+
, Sanjeev Khudanpur,
Lane Schwartz
?
, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University
? Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
? Natural Language Processing Lab, University of Minnesota
Abstract
We describe Joshua (Li et al, 2009a)
1
,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit?s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al, 2007) does for regular phrase-based
machine translation.
?
This research was supported in part by the Defense Ad-
vanced Research Projects Agency?s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors? alone.
1
Please cite Li et al (2009a) if you use Joshua in your
research, and not this demonstration description paper.
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua?s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
25
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
Table 1: BLEU scores for top primary systems on
the WMT-09 French-English Task from Callison-
Burch et al (2009), who also provide human eval-
uation results.
2.1 Joshua Toolkit Features
Here is a short description of Joshua?s main fea-
tures, described in more detail in Li et al (2009a):
? Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al, 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
? Suffix-array Grammar Extraction: Gram-
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
? Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
? Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
? k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
? Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
? Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
? Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams
2
; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
? Minimum Error Rate Training: Joshua?s
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).
3
2
The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3
The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
26
? Variational Decoding: spurious ambiguity
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al, 2009b).
3 Demonstration Outline
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module?s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
?
ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och?s efficient line
search method.
4 Demonstration Requirements
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live ?instant translation? user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
27
We will rely on 3 workstations: 
one for the instant translation 
demo, where arbitrary input is 
translated from/to a language pair 
of choice (top); one for runtime 
demonstration of the system, with 
a terminal window for each of the 
three main components of the 
systems, as well as visual aids, 
such as derivation trees (left); and 
one (not shown) designated for 
technical discussion of the code.
Remote server 
hosting trained 
translation models
JHU
Grammar extraction
Decoder
M
E
R
T
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
28
Proceedings of the Second Workshop on Statistical Machine Translation, pages 207?211,
Prague, June 2007. c?2007 Association for Computational Linguistics
The ?noisier channel?: translation from morphologically complex languages
Christopher J. Dyer
Department of Linguistics
University of Maryland
College Park, MD 20742
redpony@umd.edu
Abstract
This paper presents a new paradigm for
translation from inflectionally rich lan-
guages that was used in the University
of Maryland statistical machine transla-
tion system for the WMT07 Shared Task.
The system is based on a hierarchical
phrase-based decoder that has been aug-
mented to translate ambiguous input given
in the form of a confusion network (CN),
a weighted finite state representation of a
set of strings. By treating morphologi-
cally derived forms of the input sequence
as possible, albeit more ?costly? paths that
the decoder may select, we find that sig-
nificant gains (10% BLEU relative) can
be attained when translating from Czech,
a language with considerable inflectional
complexity, into English.
1 Introduction
Morphological analysis occupies a tenuous position
statistical machine translation systems. Conven-
tional translation models are constructed with no
consideration of the relationships between lexical
items and instead treat different inflected (observed)
forms of identical underlying lemmas as completely
independent of one another. While the variously
inflected forms of one lemma may express differ-
ences in meaning that are crucial to correct transla-
tion, the strict independence assumptions normally
made exacerbate data sparseness and lead to poorly
estimated models and suboptimal translations. A va-
riety of solutions have been proposed: Niessen and
Ney (2001) use of morphological information to im-
prove word reordering before training and after de-
coding. Goldwater and McClosky (2005) show im-
provements in a Czech to English word-based trans-
lation system when inflectional endings are simpli-
fied or removed entirely. Their method can, how-
ever, actually harm performance since the discarded
morphemes carry some information that may have
bearing on the translation (cf. Section 3.3). To avoid
this pitfall, Talbot and Osborne (2006) use a data-
driven approach to cluster source-language morpho-
logical variants that are meaningless in the target
language, and Yang and Kirchhoff (2006) propose
the use of a backoff model that uses morphologically
reduced forms only when the translation of the sur-
face form is unavailable. All of these approaches
have in common that the decisions about whether to
use morphological information are made in either a
pre- or post-processing step.
Recent work in spoken language translation sug-
gests that allowing decisions about the use of mor-
phological information to be made along side other
translation decisions (i.e., inside the decoder), will
yield better results. At least as early as Ney (1999),
it has been shown that when translating the out-
put from automatic speech regonition (ASR) sys-
tems, the quality can be improved by considering
multiple (rather than only a single best) transcrip-
tion hypothesis. Although state-of-the-art statistical
machine translation systems have conventionally as-
sumed unambiguous input; recent work has demon-
strated the possibility of efficient decoding of am-
207
biguous input (represented as confusion networks or
word lattices) within standard phrase-based models
(Bertoldi et al, to appear 2007) as well as hierarchi-
cal phrase-based models (Dyer and Resnik, 2007).
These hybrid decoders search for the target language
sentence e? that maximizes the following probability,
where G(o) represents the set of weighted transcrip-
tion hypotheses produced by an ASR decoder:
e? = argmax
e
max
f ??G(o)
P (e, f ?|o) (1)
The conditional probability p(e, f |o) that is maxi-
mized is modeled directly using a log-linear model
(Och and Ney, 2002), whose parameters can be
tuned to optimize either the probability of a devel-
opment set or some other objective (such as max-
imizing BLEU). In addition to the standard trans-
lation model features, the ASR system?s posterior
probability is another feature. The decoder thus
finds a translation hypothesis e? that maximizes the
joint translation/transcription probability, which is
not necessarily the one that corresponds to the best
single transcription hypothesis.
2 Noisier channel translation
We extend the concept of translating from an am-
biguous set of source hypotheses to the domain of
text translation by redefining G(?) to be a set of
weighted sentences derived by applying morpholog-
ical transformations (such as stemming, compound
splitting, clitic splitting, etc.) to a given source sen-
tence f . This model for translation extends the usual
noisy channel metaphor by suggesting that an ?En-
glish? source signal is first distorted into a morpho-
logically neutral ?French? and then morphological
processes represent a further distortion of the signal,
which can be modeled independently. Whereas in
the context of an ASR transcription hypothesis, G(?)
assigns a posterior probability to each sentence, we
redefine of this value to be a backoff penalty. This
can be intuitively thought of as a measure of the
?distance? that a given morphological alternative is
from the observed input sentence.
The remainder of the paper is structured as fol-
lows. In Section 2, we describe the basic hierarchi-
cal translation model. In Section 3, we describe the
data and tools used and present experimental results
for Czech-English. Section 4 concludes.
3 Hierarchical phrase-based decoding
Chiang (2005; to appear 2007) introduced hierar-
chical phrase-based translation models, which are
formally based on synchronous context-free gram-
mars. These generalize phrase-based translation
models by allowing phrase pairs to contain vari-
ables. Like phrase correspondences, the correspond-
ing synchronous grammar rules can be learned auto-
matically from aligned, but otherwise unannotated,
training bitext. For details about the extraction algo-
rithm, refer to Chiang (to appear 2007).
The rules of the induced grammar consist of pairs
of strings of terminals and non-terminals in the
source and target languages, as well one-to-one cor-
respondences between non-terminals on the source
and target side of each pair (shown as indexes in
the examples below). Thus they encapsulate not
only meaning translation (of possibly discontinuous
spans), but also typical reordering patterns. For ex-
ample, the following two rules were extracted from
the Spanish ? English segment of the Europarl cor-
pus (Koehn, 2003):
X ? ?la X
1
de X
2
,X
2
?s X
1
? (2)
X ? ?el X
1
verde, the green X
1
? (3)
Rule (2) expresses the fact that possessors can
be expressed prior to the possessed object in En-
glish but must follow in Spanish. Rule (3) shows
that the adjective verde follows the modified expres-
sion in Spanish whereas the corresponding English
lexical item green precedes what it modifies. Al-
though the rules given here correspond to syntactic
constituents, this is accidental. The grammars ex-
tracted make use of only a single non-terminal cate-
gory and variables are posited that may or may not
correspond to linguistically meaningful spans.
Given a synchronous grammar G, the translation
process is equivalent to parsing an input sentence
with the source side of G and thereby inducing a
target sentence. The decoder we used is based on
the CKY+ algorithm, which permits the parsing of
rules that are not in Chomsky normal form (Chep-
palier and Rajman, 1998) and that has been adapted
to admit input that is in the form of a confusion net-
work (Dyer and Resnik, 2007). To incorporate target
208
Language Tokens Types Singletons
Czech surface 1.2M 88037 42341
Czech lemmas 1.2M 34227 13129
Czech truncated 1.2M 37263 13093
English 1.4M 31221 10508
Spanish 1.4M 47852 20740
French 1.2M 38241 15264
German 1.4M 75885 39222
Table 1: Corpus statistics, by language, for the
WMT07 training subset of the News Commentary
corpus.
language model probabilities into the model, which
is important for translation quality, the grammar is
intersected during decoding with an m-gram lan-
guage model. This process significantly increases
the effective size of the grammar, and so a beam-
search heuristic called cube pruning is used, which
has been experimentally determined to be nearly as
effective as an exhaustive search but far more effi-
cient.
4 Experiments
We carried out a series of experiments using differ-
ent strategies for making use of morphological in-
formation on the News Commentary Czech-English
data set provided for the WMT07 Shared Task.
Czech was selected because it exhibits a rich inflec-
tional morphology, but its other morphological pro-
cesses (such as compounding and cliticization) that
affect multiple lemmas are relatively limited. This
has the advantage that a morphologically simpli-
fied (i.e., lemmatized) form of a Czech sentence has
the same number of tokens as the surface form has
words, which makes representing G(f) as a confu-
sion network relatively straightforward. The relative
morphological complexity of Czech, as well as the
potential benefits that can be realized by stemming,
can be inferred from the corpus statistics given in
Table 1.
4.1 Technical details
A trigram English language model with modified
Kneser-Ney smoothing (Kneser and Ney, 1995) was
trained using the SRI Language Modeling Toolkit
(Stolcke, 2002) on the English side of the News
Commentary corpus as well as portions of the
GigaWord v2 English Corpus and was used for
all experiments. Recasing was carried out using
SRI?s disambig tool using a trigram language
model. The feature set used included bidirectional
translation probabilities for rules, lexical transla-
tion probabilities, a target language model proba-
bility, and count features for target words, num-
ber of non-terminal symbols used, and finally the
number of morphologically simplified forms se-
lected in the CN. Feature weight tuning was carried
out using minimum error rate training, maximizing
BLEU scores on a held-out development set (Och,
2003). Translation scores are reported using case-
insensitive BLEU (Papineni et al, 2002) with a sin-
gle reference translation. Significance testing was
done using bootstrap resampling (Koehn, 2004).
4.2 Data preparation and training
We used a Czech morphological analyzer by Hajic?
and Hladka? (1998) to extract the lemmas from the
Czech portions of the training, development, and
test data (the Czech-English portion of the News
Commentary corpus distributed as as part of the
WMT07 Shared Task). Data sets consisting of trun-
cated forms were also generated; using a length limit
of 6, which Goldwater and McClosky (2005) exper-
imentally determined to be optimal for translation
performance. We refer to the three data sets and the
models derived from them as SURFACE, LEMMA,
and TRUNC. Czech?English grammars were ex-
tracted from the three training sets using the meth-
ods described in Chiang (to appear 2007). Two ad-
ditional grammars were created by combining the
rules from the SURFACE grammar and the LEMMA
or TRUNC grammar and renormalizing the condi-
tional probabilities, yielding the combined models
SURFACE+LEMMA and SURFACE+TRUNC.
Confusion networks for the development and test
sets were constructed by providing a single back-
off form at each position in the sentence where the
lemmatizer or truncation process yielded a different
word form. The backoff form was assigned a cost of
1 and the surface form a cost of 0. Numbers and
punctuation were not truncated. A ?backoff? set,
corresponding approximately to the method of Yang
and Kirchhoff (2006) was generated by lemmatiz-
ing only unknown words. Figure 1 shows a sample
surface+lemma CN from the test set.
209
1 2 3 4 5 6 7 8 9 10 11 12
z americke?ho br?ehu atlantiku se veskera? takova? odu?vodne?n?? jev?? jako naprosto bizarn??
americky? br?eh atlantik s takovy? jevit
Figure 1: Example confusion network generated by lemmatizing the source sentence to generate alternates at
each position in the sentence. The upper element in each column is the surface form and the lower element,
when present, is the lemma.
Input BLEU Sample translation
SURFACE 22.74 From the US side of the Atlantic all such odu?vodne?n?? appears to be a totally bizarre.
LEMMA 22.50 From the side of the Atlantic with any such justification seem completely bizarre.
TRUNC (l=6) 22.07 From the bank of the Atlantic, all such justification appears to be totally bizarre.
backoff (SURFACE+LEMMA) 23.94 From the US bank of the Atlantic, all such justification appears to be totally bizarre.
CN (SURFACE+LEMMA) 25.01 From the US side of the Atlantic all such justification appears to be a totally bizarre.
CN (SURFACE+TRUNC) 23.57 From the US Atlantic any such justification appears to be a totally bizarre.
Table 2: Czech-English results on WMT07 Shared Task DEVTEST set. The sample translations are transla-
tions of the sentence shown in Figure 1.
4.3 Experimental results
Table 2 summarizes the performance of the six
Czech?English models on the WMT07 Shared
Task development set. The basic SURFACE model
tends to outperform both the LEMMA and TRUNC
models, although the difference is only marginally
significant. This suggests that the Goldwater and
McClosky (2005) results are highly dependent on
the kind of translation model and quantity of data.
The backoff model, a slightly modified version
of the method proposed by Yang and Kirchhoff
(2006),1 does significantly better than the baseline
(p < .05). However, the joint (SURFACE+LEMMA)
model outperforms both surface and backoff base-
lines (p < .01 and p < .05, respectively). The SUR-
FACE+TRUNC model is an improvement over the
SURFACE model, but it performances significantly
worse than the SURFACE+LEMMA model.
5 Conclusion
We presented a novel model-driven method for us-
ing morphologically reduced forms when translat-
ing from a language with complex inflectional mor-
1Our backoff model has two primary differences frommodel
described by Y&K. The first is that our model effectively cre-
ates backoff forms for every surface string, whereas Y&K do
this only for forms that are not found in the surface string. This
means that in our model, the probabilities of a larger number
of surface rules have been altered by backoff discounting than
would be the case in the more conservative model. Second, the
joint model we used has the benefit of using morphologically
simpler forms to improve alignment.
phology. By allowing the decoder to select among
the surface form of a word or phrase and variants
of morphological alternatives on the source side,
we outperform baselines where hard decisions about
what form to use are made in advance of decod-
ing, as has typically been done in systems that make
use of morphological information. This ?decoder-
guided? incorporation of morphology was enabled
by adopting techniques for translating from ambigu-
ous sources that were developed to address problems
specific to spoken language translation. Although
the results presented here were obtained using a hi-
erarchical phrase-based system, the model general-
izes to any system where the decoder can accept a
weighted word graph as its input.
Acknowledgements
The author would like to thank David Chiang for
making the Hiero decoder sources available to us
and Daniel Zeman for his assistance in the prepara-
tion of the Czech data. This work was generously
supported by the GALE program of the Defense
Advanced Research Projects Agency, Contract No.
HR0011-06-2-0001.
References
N. Bertoldi, R. Zens, and M. Federico. to appear 2007. Speech
translation by confusion network decoding. In 32nd Inter-
national Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), Honolulu, Hawaii, April.
210
J. Cheppalier and M. Rajman. 1998. A generalized CYK
algorithm for parsing stochastic CFG. In Proceedings
of the Workshop on Tabulation in Parsing and Deduction
(TAPD98), pages 133?137, Paris, France.
D. Chiang. to appear 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
C. Dyer and P. Resnik. 2007. Word Lattice Parsing for Sta-
tistical Machine Translation. Technical report, University of
Maryland, College Park, April.
S. Goldwater and D. McClosky. 2005. Improving statistical
mt through morphological analysis. In Proceedings of Hu-
man Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing, pages
676?683, Vancouver, British Columbia.
J. Hajic? and B. Hladka?. 1998. Tagging inflective languages:
Prediction of morphological categories for a rich, structured
tagset. In Proceedings of the COLING-ACL Conference,
pages 483?490.
R. Kneser and H. Ney. 1995. Improved backing-off for m-
gram language modeling. In Proceedings of IEEE Interna-
tion Conference on Acoustics, Speech, and Signal Process-
ing, pages 181?184.
P. Koehn. 2003. Europarl: A multilingual corpus for evaluation
of machine translation. Draft, unpublished.
P. Koehn. 2004. Statistical signficiance tests for machine
translation evluation. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP), pages 388?395.
H. Ney. 1999. Speech translation: Coupling of recognition
and translation. In IEEE International Conference on Acous-
tic, Speech and Signal Processing, pages 517?520, Phoenix,
AR, March.
S. Niessen and H. Ney. 2001. Morpho-syntactic analysis for
reordering in statistical machine translation. In Proceedings
of MT Summit VIII, Santiago de Compostela, Galicia, Spain.
F. Och and H. Ney. 2002. Discriminitive training and maxi-
mum entropy models for statistical machine translation. In
Proceedings of the 40th Annual Meeting of the ACL, pages
295?302.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics (ACL),
pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: a
method for automatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the ACL, pages
311?318.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
D. Talbot and M. Osborne. 2006. Modelling lexical redun-
dancy for machine translation. In Proceedings of ACL 2006,
Sydney, Australia.
M. Yang and K. Kirchhoff. 2006. Phrase-based backoff mod-
els for machine translation of highly inflected languages. In
Proceedings of the EACL 2006, pages 41?48.
211
Proceedings of the Third Workshop on Statistical Machine Translation, pages 199?207,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Fast, Easy, and Cheap: Construction of
Statistical Machine Translation Models with MapReduce
Christopher Dyer, Aaron Cordova, Alex Mont, Jimmy Lin
Laboratory for Computational Linguistics and Information Processing
University of Maryland
College Park, MD 20742, USA
redpony@umd.edu
Abstract
In recent years, the quantity of parallel train-
ing data available for statistical machine trans-
lation has increased far more rapidly than
the performance of individual computers, re-
sulting in a potentially serious impediment
to progress. Parallelization of the model-
building algorithms that process this data on
computer clusters is fraught with challenges
such as synchronization, data exchange, and
fault tolerance. However, the MapReduce
programming paradigm has recently emerged
as one solution to these issues: a powerful
functional abstraction hides system-level de-
tails from the researcher, allowing programs to
be transparently distributed across potentially
very large clusters of commodity hardware.
We describe MapReduce implementations of
two algorithms used to estimate the parame-
ters for two word alignment models and one
phrase-based translation model, all of which
rely on maximum likelihood probability esti-
mates. On a 20-machine cluster, experimental
results show that our solutions exhibit good
scaling characteristics compared to a hypo-
thetical, optimally-parallelized version of cur-
rent state-of-the-art single-core tools.
1 Introduction
Like many other NLP problems, output quality of
statistical machine translation (SMT) systems in-
creases with the amount of training data. Brants et
al. (2007) demonstrated that increasing the quantity
of training data used for language modeling signifi-
cantly improves the translation quality of an Arabic-
English MT system, even with far less sophisticated
backoff models. However, the steadily increas-
ing quantities of training data do not come with-
out cost. Figure 1 shows the relationship between
the amount of parallel Arabic-English training data
used and both the translation quality of a state-of-
the-art phrase-based SMT system and the time re-
quired to perform the training with the widely-used
Moses toolkit on a commodity server.1 Building
a model using 5M sentence pairs (the amount of
Arabic-English parallel text publicly available from
the LDC) takes just over two days.2 This represents
an unfortunate state of affairs for the research com-
munity: excessively long turnaround on experiments
is an impediment to research progress.
It is clear that the needs of machine translation re-
searchers have outgrown the capabilities of individ-
ual computers. The only practical recourse is to dis-
tribute the computation across multiple cores, pro-
cessors, or machines. The development of parallel
algorithms involves a number of tradeoffs. First is
that of cost: a decision must be made between ?ex-
otic? hardware (e.g., large shared memory machines,
InfiniBand interconnect) and commodity hardware.
There is significant evidence (Barroso et al, 2003)
that solutions based on the latter are more cost ef-
fective (and for resource-constrained academic in-
stitutions, often the only option).
Given appropriate hardware, MT researchers
must still contend with the challenge of developing
software. Quite simply, parallel programming is dif-
ficult. Due to communication and synchronization
1http://www.statmt.org/moses/
2All single-core timings reported in this paper were per-
formed on a 3GHz 64-bit Intel Xeon server with 8GB memory.
199
15 min
30 min45 min
1.5 hrs
3 hrs
6 hrs
12 hrs
1 day
2 days
 10000  100000  1e+06  1e+07 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
Tim
e (se
cond
s)
Tran
slati
on q
ualit
y (BL
EU)
Corpus size (sentences)
Training timeTranslation quality
Figure 1: Translation quality and training time as a func-
tion of corpus size.
issues, concurrent operations are notoriously chal-
lenging to reason about. In addition, fault tolerance
and scalability are serious concerns on commodity
hardware prone to failure. With traditional paral-
lel programming models (e.g., MPI), the developer
shoulders the burden of handling these issues. As a
result, just as much (if not more) effort is devoted to
system issues as to solving the actual problem.
Recently, Google?s MapReduce framework (Dean
and Ghemawat, 2004) has emerged as an attractive
alternative to existing parallel programming models.
The MapReduce abstraction shields the programmer
from having to explicitly worry about system-level
issues such as synchronization, data exchange, and
fault tolerance (see Section 2 for details). The run-
time is able to transparently distribute computations
across large clusters of commodity hardware with
good scaling characteristics. This frees the program-
mer to focus on actual MT issues.
In this paper we present MapReduce implementa-
tions of training algorithms for two kinds of models
commonly used in statistical MT today: a phrase-
based translation model (Koehn et al, 2003) and
word alignment models based on pairwise lexi-
cal translation trained using expectation maximiza-
tion (Dempster et al, 1977). Currently, such models
take days to construct using standard tools with pub-
licly available training corpora; our MapReduce im-
plementation cuts this time to hours. As an benefit
to the community, it is our intention to release this
code under an open source license.
It is worthwhile to emphasize that we present
these results as a ?sweet spot? in the complex design
space of engineering decisions. In light of possible
tradeoffs, we argue that our solution can be consid-
ered fast (in terms of running time), easy (in terms
of implementation), and cheap (in terms of hard-
ware costs). Faster running times could be achieved
with more expensive hardware. Similarly, a custom
implementation (e.g., in MPI) could extract finer-
grained parallelism and also yield faster running
times. In our opinion, these are not worthwhile
tradeoffs. In the first case, financial constraints
are obvious. In the second case, the programmer
must explicitly manage all the complexities that
come with distributed processing (see above). In
contrast, our algorithms were developed within a
matter of weeks, as part of a ?cloud computing?
course project (Lin, 2008). Experimental results
demonstrate that MapReduce provides nearly opti-
mal scaling characteristics, while retaining a high-
level problem-focused abstraction.
The remainder of the paper is structured as fol-
lows. In the next section we provide an overview of
MapReduce. In Section 3 we describe several gen-
eral solutions to computing maximum likelihood es-
timates for finite, discrete probability distributions.
Sections 4 and 5 apply these techniques to estimate
phrase translation models and perform EM for two
word alignment models. Section 6 reviews relevant
prior work, and Section 7 concludes.
2 MapReduce
MapReduce builds on the observation that many
tasks have the same basic structure: a computation is
applied over a large number of records (e.g., parallel
sentences) to generate partial results, which are then
aggregated in some fashion. The per-record compu-
tation and aggregation function are specified by the
programmer and vary according to task, but the ba-
sic structure remains fixed. Taking inspiration from
higher-order functions in functional programming,
MapReduce provides an abstraction at the point of
these two operations. Specifically, the programmer
defines a ?mapper? and a ?reducer? with the follow-
ing signatures (square brackets indicate a list of ele-
ments):
map: ?k1, v1? ? [?k2, v2?]
reduce: ?k2, [v2]? ? [?k3, v3?]
200
inp
ut
inp
ut
inp
ut
inp
ut
ma
p
ma
p
ma
p
ma
p
inp
ut
inp
ut
inp
ut
inp
ut
Bar
rier
:?gr
oup
?val
ues
?by
?key
s
red
uce
red
uce
red
uce
out
put
out
put
out
put
Figure 2: Illustration of the MapReduce framework: the
?mapper? is applied to all input records, which generates
results that are aggregated by the ?reducer?.
Key/value pairs form the basic data structure in
MapReduce. The ?mapper? is applied to every input
key/value pair to generate an arbitrary number of in-
termediate key/value pairs. The ?reducer? is applied
to all values associated with the same intermediate
key to generate output key/value pairs. This two-
stage processing structure is illustrated in Figure 2.
Under this framework, a programmer need only
provide implementations of map and reduce. On top
of a distributed file system (Ghemawat et al, 2003),
the runtime transparently handles all other aspects
of execution, on clusters ranging from a few to a few
thousand workers on commodity hardware assumed
to be unreliable, and thus is tolerant to various faults
through a number of error recovery mechanisms.
The runtime also manages data exchange, includ-
ing splitting the input across multiple map workers
and the potentially very large sorting problem be-
tween the map and reduce phases whereby interme-
diate key/value pairs must be grouped by key.
For the MapReduce experiments reported in this
paper, we used Hadoop version 0.16.0,3 which is
an open-source Java implementation of MapRe-
duce, running on a 20-machine cluster (1 master,
19 slaves). Each machine has two processors (run-
ning at either 2.4GHz or 2.8GHz), 4GB memory
(map and reduce tasks were limited to 768MB), and
100GB disk. All software was implemented in Java.
3http://hadoop.apache.org/
Method 1
Map1 ?A,B? ? ??A,B?, 1?
Reduce1 ??A,B?, c(A,B)?
Map2 ??A,B?, c(A,B)? ? ??A,? ?, c(A,B)?
Reduce2 ??A,? ?, c(A)?
Map3 ??A,B?, c(A,B)? ? ?A, ?B, c(A,B)??
Reduce3 ?A, ?B,
c(A,B)
c(A) ??
Method 2
Map1 ?A,B? ? ??A,B?, 1?; ??A,? ?, 1?
Reduce1 ??A,B?,
c(A,B)
c(A) ?
Method 3
Map1 ?A,Bi? ? ?A, ?Bi : 1??
Reduce1 ?A, ?B1 :
c(A,B1)
c(A) ?, ?B2 :
c(A,B2)
c(A) ? ? ? ? ?
Table 1: Three methods for computing PMLE(B|A).
The first element in each tuple is a key and the second
element is the associated value produced by the mappers
and reducers.
3 Maximum Likelihood Estimates
The two classes of models under consideration are
parameterized with conditional probability distribu-
tions over discrete events, generally estimated ac-
cording to the maximum likelihood criterion:
PMLE(B|A) =
c(A,B)
c(A)
=
c(A,B)
?
B? c(A,B
?)
(1)
Since this calculation is fundamental to both ap-
proaches (they distinguish themselves only by where
the counts of the joint events come from?in the case
of the phrase model, they are observed directly, and
in the case of the word-alignment models they are
the number of expected events in a partially hidden
process given an existing model of that process), we
begin with an overview of how to compute condi-
tional probabilities in MapReduce.
We consider three possible solutions to this prob-
lem, shown in Table 1. Method 1 computes the count
for each pair ?A,B?, computes the marginal c(A),
and then groups all the values for a given A together,
such that the marginal is guaranteed to be first and
then the pair counts follow. This enables Reducer3
to only hold the marginal value in memory as it pro-
cesses the remaining values. Method 2 works simi-
larly, except that the original mapper emits two val-
ues for each pair ?A,B? that is encountered: one that
201
will be the marginal and one that contributes to the
pair count. The reducer groups all pairs together by
the A value, processes the marginal first, and, like
Method 1, must only keep this value in memory as
it processes the remaining pair counts. Method 2 re-
quires more data to be processed by the MapReduce
framework, but only requires a single sort operation
(i.e., fewer MapReduce iterations).
Method 3 works slightly differently: rather than
computing the pair counts independently of each
other, the counts of all the B events jointly occurring
with a particular A = a event are stored in an asso-
ciative data structure in memory in the reducer. The
marginal c(A) can be computed by summing over
all the values in the associative data structure and
then a second pass normalizes. This requires that
the conditional distribution P (B|A = a) not have
so many parameters that it cannot be represented
in memory. A potential advantage of this approach
is that the MapReduce framework can use a ?com-
biner? to group many ?A,B? pairs into a single value
before the key/value pair leaves for the reducer.4 If
the underlying distribution from which pairs ?A,B?
has certain characteristics, this can result in a signifi-
cant reduction in the number of keys that the mapper
emits (although the number of statistics will be iden-
tical). And since all keys must be sorted prior to the
reducer step beginning, reducing the number of keys
can have significant performance impact.
The graph in Figure 3 shows the performance
of the three problem decompositions on two model
types we are estimating, conditional phrase trans-
lation probabilities (1.5M sentences, max phrase
length=7), and conditional lexical translation prob-
abilities as found in a word alignment model (500k
sentences). In both cases, Method 3, which makes
use of more memory to store counts of all B events
associated with event A = a, completes at least 50%
more quickly. This efficiency is due to the Zipfian
distribution of both phrases and lexical items in our
corpora: a few frequent items account for a large
portion of the corpus. The memory requirements
were also observed to be quite reasonable for the
4Combiners operate like reducers, except they run directly
on the output of a mapper before the results leave memory.
They can be used when the reduction operation is associative
and commutative. For more information refer to Dean and Ghe-
mawat (2004).
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
Met
hod
 1
Met
hod
 2
Mat
hod
 3
Tim
e (se
cond
s)
Estimation method
Phrase pairsWord pairs
Figure 3: PMLE computation strategies.
Figure 4: A word-aligned sentence. Examples
of consistent phrase pairs include ?vi, i saw?,
?la mesa pequen?a, the small table?, and
?mesa pequen?a, small table?; but, note that, for
example, it is not possible to extract a consistent phrase
corresponding to the foreign string la mesa or the English
string the small.
models in question: representing P (B|A = a) in the
phrase model required at most 90k parameters, and
in the lexical model, 128k parameters (i.e., the size
of the vocabulary for language B). For the remainder
of the experiments reported, we confine ourselves to
the use of Method 3.
4 Phrase-Based Translation
In phrase-based translation, the translation process
is modeled by splitting the source sentence into
phrases (a contiguous string of words) and translat-
ing the phrases as a unit (Och et al, 1999; Koehn
et al, 2003). Phrases are extracted from a word-
aligned parallel sentence according to the strategy
proposed by Och et al (1999), where every word in
a phrase is aligned only to other words in the phrase,
and not to any words outside the phrase bounds. Fig-
ure 4 shows an example aligned sentence and some
of the consistent subphrases that may be extracted.
202
1.5 min
5 min
20 min
60 min
3 hrs
12 hrs
2 days
 10000  100000  1e+06  1e+07
Tim
e (se
cond
s)
Corpus size (sentences)
Moses training timeMapReduce training (38 M/R)Optimal (Moses/38)
Figure 5: Phrase model extraction and scoring times at
various corpus sizes.
Constructing a model involves extracting all the
phrase pairs ?e, f? and computing the conditional
phrase translation probabilities in both directions.5
With a minor adjustment to the techniques intro-
duced in Section 3, it is possible to estimate P (B|A)
and P (A|B) concurrently.
Figure 5 shows the time it takes to construct
a phrase-based translation model using the Moses
tool, running on a single core, as well as the time
it takes to build the same model using our MapRe-
duce implementation. For reference, on the same
graph we plot a hypothetical, optimally-parallelized
version of Moses, which would run in 138 of the time
required for the single-core version on our cluster.6
Although these represent completely different im-
plementations, this comparison offers a sense of
MapReduce?s benefits. The framework provides a
conceptually simple solution to the problem, while
providing an implementation that is both scalable
and fault tolerant?in fact, transparently so since
the runtime hides all these complexities from the re-
searcher. From the graph it is clear that the overhead
associated with the framework itself is quite low, es-
pecially for large quantities of data. We concede that
it may be possible for a custom solution (e.g., with
MPI) to achieve even faster running times, but we
argue that devoting resources to developing such a
solution would not be cost-effective.
Next, we explore a class of models where the stan-
5Following Och and Ney (2002), it is customary to combine
both these probabilities as feature values in a log-linear model.
6In our cluster, only 19 machines actually compute, and each
has two single-core processors.
dard tools work primarily in memory, but where the
computational complexity of the models is greater.
5 Word Alignment
Although word-based translation models have been
largely supplanted by models that make use of larger
translation units, the task of generating a word align-
ment, the mapping between the words in the source
and target sentences that are translationally equiva-
lent, remains crucial to nearly all approaches to sta-
tistical machine translation.
The IBM models, together with a Hidden Markov
Model (HMM), form a class of generative mod-
els that are based on a lexical translation model
P (fj |ei) where each word fj in the foreign sentence
fm1 is generated by precisely one word ei in the sen-
tence el1, independently of the other translation de-
cisions (Brown et al, 1993; Vogel et al, 1996; Och
and Ney, 2000). Given these assumptions, we let
the sentence translation probability be mediated by
a latent alignment variable (am1 in the equations be-
low) that specifies the pairwise mapping between
words in the source and target languages. Assum-
ing a given sentence length m for fm1 , the translation
probability is defined as follows:
P (fm1 |e
l
1) =
?
am1
P (fm1 , a
m
1 |e
l
1)
=
?
am1
P (am1 |e
l
1, f
m
1 )
m?
j=1
P (fj |eaj )
Once the model parameters have been estimated, the
single-best word alignment is computed according
to the following decision rule:
a?m1 = argmax
am1
P (am1 |e
l
1, f
m
1 )
m?
j=1
P (fj |eaj )
In this section, we consider the MapReduce imple-
mentation of two specific alignment models:
1. IBM Model 1, where P (am1 |e
l
1, f
m
1 ) is uniform
over all possible alignments.
2. The HMM alignment model where
P (am1 |e
l
1, f
m
1 ) =
?m
j=1 P (aj |aj?1).
203
Estimating the parameters for these models is more
difficult (and more computationally expensive) than
with the models considered in the previous section:
rather than simply being able to count the word pairs
and alignment relationships and estimate the mod-
els directly, we must use an existing model to com-
pute the expected counts for all possible alignments,
and then use these counts to update the new model.7
This training strategy is referred to as expectation-
maximization (EM) and is guaranteed to always im-
prove the quality of the prior model at each iteration
(Brown et al, 1993; Dempster et al, 1977).
Although it is necessary to compute a sum over all
possible alignments, the independence assumptions
made in these models allow the total probability of
generating a particular observation to be efficiently
computed using dynamic programming.8 The HMM
alignment model uses the forward-backward algo-
rithm (Baum et al, 1970), which is also an in-
stance of EM. Even with dynamic programming,
this requires O(Slm) operations for Model 1, and
O(Slm2) for the HMM model, where m and l are
the average lengths of the foreign and English sen-
tences in the training corpus, and S is the number of
sentences. Figure 6 shows measurements of the av-
erage iteration run-time for Model 1 and the HMM
alignment model as implemented in Giza++ (Och
and Ney, 2003), a state-of-the-art C++ implemen-
tation of the IBM and HMM alignment models that
is widely used. Five iterations are generally neces-
sary to train the models, so the time to carry out full
training of the models is approximately five times the
per-iteration run-time.
5.1 EM with MapReduce
Expectation-maximization algorithms can be ex-
pressed quite naturally in the MapReduce frame-
work (Chu et al, 2006). In general, for discrete gen-
erative models, mappers iterate over the training in-
stances and compute the partial expected counts for
all the unobservable events in the model that should
7For the first iteration, when there is no prior model, a
heuristic, random, or uniform distribution may be chosen.
8For IBM Models 3-5, which are not our primary focus, dy-
namic programming is not possible, but the general strategy for
computing expected counts from a previous model and updat-
ing remains identical and therefore the techniques we suggest
in this section are applicable to those models as well.
3 s
10 s
30 s
90 s
3m20s
20 min
60 min
3 hrs
 10000  100000  1e+06
Ave
rage
 itera
tion 
laten
cy (se
cond
s)
Corpus size (sentences)
Model 1HMM
Figure 6: Per-iteration average run-times for Giza++ im-
plementations of Model 1 and HMM training on corpora
of various sizes.
be associated with the given training instance. Re-
ducers aggregate these partial counts to compute
the total expected joint counts. The updated model
is estimated using the maximum likelihood crite-
rion, which just involves computing the appropri-
ate marginal and dividing (as with the phrase-based
models), and the same techniques suggested in Sec-
tion 3 can be used with no modification for this
purpose. For word alignment models, Method 3
is possible since word pairs distribute according to
Zipf?s law (meaning there is ample opportunity for
the combiners to combine records), and the number
of parameters for P (e|fj = f) is at most the num-
ber of items in the vocabulary of E, which tends to
be on the order of hundreds of thousands of words,
even for large corpora.
Since the alignment models we are considering
are fundamentally based on a lexical translation
probability model, i.e., the conditional probability
distribution P (e|f), we describe in some detail how
EM updates the parameters for this model.9 Using
the model parameters from the previous iteration (or
starting from an arbitrary or heuristic set of param-
eters during the first iteration), an expected count is
computed for every l ?m pair ?ei, fj? for each par-
allel sentence in the training corpus. Figure 7 illus-
9Although computation of expected count for a word pair
in a given training instance obviously depends on which model
is being used, the set of word pairs for which partial counts are
produced for each training instance, as well as the process of ag-
gregating the partial counts and updating the model parameters,
is identical across this entire class of models.
204
the
blue
house
maison la bleue fleur
flower
la maison
the house
la maison bleue la fleur
the blue house the flower
(a)
(b)
Figure 7: Each cell in (a) contains the expected counts for
the word pair ?ei, fj?. In (b) the example training data is
marked to show which training instances contribute par-
tial counts for the pair ?house, maison?.
3 s
10 s
30 s
90 s
3m20s
20 min
60 min
3 hrs
 10000  100000  1e+06
Tim
e (se
cond
s)
Corpus size (sentences)
Optimal Model 1 (Giza/38)Optimal HMM (Giza/38)MapReduce Model 1 (38 M/R)MapReduce HMM (38 M/R)
Figure 8: Average per-iteration latency to train HMM
and Model 1 using the MapReduce EM trainer, compared
to an optimal parallelization of Giza++ across the same
number of processors.
trates the relationship between the individual train-
ing instances and the global expected counts for a
particular word pair. After collecting counts, the
conditional probability P (f |e) is computed by sum-
ming over all columns for each f and dividing. Note
that under this training regime, a non-zero probabil-
ity P (fj |ei) will be possible only if ei and fj co-
occur in at least one training instance.
5.2 Experimental Results
Figure 8 shows the timing results of the MapReduce
implementation of Model 1 and the HMM alignment
model. Similar to the phrase extraction experiments,
we show as reference the running time of a hy-
pothetical, optimally-parallelized version of Giza++
on our cluster (i.e., values in Figure 6 divided by
38). Whereas in the single-core implementation the
added complexity of the HMM model has a signif-
icant impact on the per-iteration running time, the
data exchange overhead dominates in the perfor-
mance of both models in a MapReduce environment,
making running time virtually indistinguishable. For
these experiments, after each EM iteration, the up-
dated model parameters (which are computed in a
distributed fashion) are compiled into a compressed
representation which is then distributed to all the
processors in the cluster at the beginning of the next
iteration. The time taken for this process is included
in the iteration latencies shown in the graph. In fu-
ture work, we plan to use a distributed model repre-
sentation to improve speed and scalability.
6 Related work
Expectation-maximization algorithms have been
previously deployed in the MapReduce framework
in the context of several different applications (Chu
et al, 2006; Das et al, 2007; Wolfe et al, 2007).
Wolfe et al (2007) specifically looked at the perfor-
mance of Model 1 on MapReduce and discuss how
several different strategies can minimize the amount
of communication required but they ultimately ad-
vocate abandoning the MapReduce model. While
their techniques do lead to modest performance im-
provements, we question the cost-effectiveness of
the approach in general, since it sacrifices many of
the advantages provided by the MapReduce envi-
ronment. In our future work, we instead intend to
make use of an approach suggested by Das et al
(2007), who show that a distributed database run-
ning in tandem with MapReduce can be used to
provide the parameters for very large mixture mod-
els efficiently. Moreover, since the database is dis-
tributed across the same nodes as the MapReduce
jobs, many of the same data locality benefits that
Wolfe et al (2007) sought to capitalize on will be
available without abandoning the guarantees of the
MapReduce paradigm.
Although it does not use MapReduce, the MTTK
tool suite implements distributed Model 1, 2 and
HMM training using a ?home-grown? paralleliza-
tion scheme (Deng and Byrne, 2006). However, the
tool relies on a cluster where all nodes have access to
the same shared networked file storage, a restriction
that MapReduce does not impose.
205
There has been a fair amount of work inspired by
the problems of long latencies and excessive space
requirements in the construction of phrase-based
and hierarchical phrase-based translation models.
Several authors have advocated indexing the train-
ing data with a suffix array and computing the nec-
essary statistics during or immediately prior to de-
coding (Callison-Burch et al, 2005; Lopez, 2007).
Although this technique works quite well, the stan-
dard channel probability P (f |e) cannot be com-
puted, which is not a limitation of MapReduce.10
7 Conclusions
We have shown that an important class of model-
building algorithms in statistical machine transla-
tion can be straightforwardly recast into the MapRe-
duce framework, yielding a distributed solution
that is cost-effective, scalable, robust, and exact
(i.e., doesn?t resort to approximations). Alterna-
tive strategies for parallelizing these algorithms ei-
ther impose significant demands on the developer,
the hardware infrastructure, or both; or, they re-
quire making unwarranted independence assump-
tions, such as dividing the training data into chunks
and building separate models. We have further
shown that on a 20-machine cluster of commodity
hardware, the MapReduce implementations have ex-
cellent performance and scaling characteristics.
Why does this matter? Given the difficulty of im-
plementing model training algorithms (phrase-based
model estimation is difficult because of the size of
data involved, and word-based alignment models are
a challenge because of the computational complex-
ity associated with computing expected counts), a
handful of single-core tools have come to be widely
used. Unfortunately, they have failed to scale with
the amount of training data available. The long la-
tencies associated with these tools on large datasets
imply that any kind of experimentation that relies on
making changes to variables upstream of the word
alignment process (such as, for example, altering the
training data f ? f ?, building a new model P (f ?|e),
and reevaluating) is severely limited by this state of
affairs. It is our hope that by reducing the cost of this
10It is an open question whether the channel probability
and inverse channel probabilities are both necessary. Lopez
(2008) presents results suggesting that P (f |e) is not necessary,
whereas Subotin (2008) finds the opposite.
these pieces of the translation pipeline, we will see a
greater diversity of experimental manipulations. To-
wards that end, we intend to release this code under
an open source license.
For our part, we plan to continue pushing the lim-
its of current word alignment models by moving to-
wards a distributed representation of the model pa-
rameters used in the expectation step of EM and
abandoning the compiled model representation. Fur-
thermore, initial experiments indicate that reorder-
ing the training data can lead to better data local-
ity which can further improve performance. This
will enable us to scale to larger corpora as well as
to explore different uses of translation models, such
as techniques for processing comparable corpora,
where a strict sentence alignment is not possible un-
der the limitations of current tools.
Finally, we note that the algorithms and tech-
niques we have described here can be readily ex-
tended to problems in other areas of NLP and be-
yond. HMMs, for example, are widely used in
ASR, named entity detection, and biological se-
quence analysis. In these areas, model estimation
can be a costly process, and therefore we believe
this work will be of interest for these applications
as well. It is our expectation that MapReduce will
also provide solutions that are fast, easy, and cheap.
Acknowledgments
This work was supported by the GALE program of
the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-0001. We would also
like to thank the generous hardware support of IBM
and Google via the Academic Cloud Computing Ini-
tiative. Specifically, thanks go out to Dennis Quan
and Eugene Hung from IBM for their tireless sup-
port of our efforts. Philip Resnik and Miles Osborne
provided helpful comments on an early draft. The
last author would like to thank Esther and Kiri for
their kind support.
References
Luiz Andre? Barroso, Jeffrey Dean, and Urs Ho?lzle. 2003.
Web search for a planet: The Google cluster architec-
ture. IEEE Micro, 23(2):22?28.
Leonard E. Baum, Ted Petrie, George Soules, and Nor-
man Weiss. 1970. A maximization technique occur-
206
ring in the statistical analysis of probabilistic functions
of Markov chains. Annals of Mathematical Statistics,
41(1):164?171.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 858?867, Prague, Czech Re-
public.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics (ACL
2005), pages 255?262, Ann Arbor, Michigan.
Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu,
Gary R. Bradski, Andrew Y. Ng, and Kunle Oluko-
tun. 2006. Map-Reduce for machine learning on mul-
ticore. In Advances in Neural Information Processing
Systems 19 (NIPS 2006), pages 281?288, Vancouver,
British Columbia, Canada.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and
Shyam Rajaram. 2007. Google news personalization:
scalable online collaborative filtering. In Proceedings
of the 16th International Conference on World Wide
Web (WWW 2007), pages 271?280, Banff, Alberta,
Canada.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce:
Simplified data processing on large clusters. In Pro-
ceedings of the 6th Symposium on Operating System
Design and Implementation (OSDI 2004), pages 137?
150, San Francisco, California.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistics Society,
39(1):1?38.
Yonggang Deng and William J. Byrne. 2006. MTTK:
An alignment toolkit for statistical machine transla-
tion. In Proceedings of the 2006 Human Language
Technology Conference of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL 2006), Companion Volume, pages 265?
268, New York, New York.
Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Le-
ung. 2003. The Google File System. In Proceedings
of the 19th ACM Symposium on Operating Systems
Principles (SOSP-03), pages 29?43, Bolton Landing,
New York.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (HLT/NAACL
2003), pages 48?54, Edmonton, Alberta, Canada.
Jimmy Lin. 2008. Exploring large-data issues in the cur-
riculum: A case study with MapReduce. In Proceed-
ings of the Third Workshop on Issues in Teaching Com-
putational Linguistics at ACL 2008, Columbus, Ohio.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 976?985, Prague, Czech
Republic.
Adam Lopez. 2008. Machine Translation by Pattern
Matching. Ph.D. dissertation, University of Maryland,
College Park, MD.
Franz Josef Och and Hermann Ney. 2000. A comparison
of alignment models for statistical machine translation.
In Proceedings of the 18th International Conference
on Computational Linguistics (COLING 2000), pages
1086?1090, Saarbrucken, Germany.
Franz Josef Och and Hermann Ney. 2002. Discrimini-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL 2002), pages 295?302, Philadelphia,
Pennsylvania.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the 1999 Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
20?28, College Park, Maryland.
Michael Subotin. 2008. Exponential models for machine
translation. Master?s thesis, University of Maryland,
College Park, MD.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics (COLING 1996), pages 836?
841, Copenhagen, Denmark.
Jason Wolfe, Aria Delier Haghighi, and Daniel Klein.
2007. Fully distributed EM for very large datasets.
Technical Report UCB/EECS-2007-178, EECS De-
partment, University of California, Berkeley.
207
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135?139,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Joshua: An Open Source Toolkit for Parsing-based Machine Translation
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,+ Sanjeev Khudanpur,
Lane Schwartz,? Wren N. G. Thornton, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe Joshua, an open source
toolkit for statistical machine transla-
tion. Joshua implements all of the algo-
rithms required for synchronous context
free grammars (SCFGs): chart-parsing, n-
gram language model integration, beam-
and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array
grammar extraction and minimum error
rate training. It uses parallel and dis-
tributed computing techniques for scala-
bility. We demonstrate that the toolkit
achieves state of the art translation per-
formance on the WMT09 French-English
translation task.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high bar-
rier to entry for other researchers, and makes ex-
periments difficult to duplicate and compare. In
this paper, we describe Joshua, a general-purpose
open source toolkit for parsing-based machine
translation, serving the same role as Moses (Koehn
et al, 2007) does for regular phrase-based ma-
chine translation.
Our toolkit is written in Java and implements
all the essential algorithms described in Chiang
(2007): chart-parsing, n-gram language model in-
tegration, beam- and cube-pruning, and k-best ex-
traction. The toolkit also implements suffix-array
grammar extraction (Lopez, 2007) and minimum
error rate training (Och, 2003). Additionally, par-
allel and distributed computing techniques are ex-
ploited to make it scalable (Li and Khudanpur,
2008b). We have also made great effort to ensure
that our toolkit is easy to use and to extend.
The toolkit has been used to translate roughly
a million sentences in a parallel corpus for large-
scale discriminative training experiments (Li and
Khudanpur, 2008a). We hope the release of the
toolkit will greatly contribute the progress of the
syntax-based machine translation research.1
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: The Joshua code is organized
into separate packages for each major aspect of
functionality. In this way it is clear which files
contribute to a given functionality and researchers
can focus on a single package without worrying
about the rest of the system. Moreover, to mini-
mize the problems of unintended interactions and
unseen dependencies, which is common hinder-
ance to extensibility in large projects, all exten-
sible components are defined by Java interfaces.
Where there is a clear point of departure for re-
search, a basic implementation of each interface is
provided as an abstract class to minimize the work
necessary for new extensions.
End-to-end Cohesion: There are many compo-
nents to a machine translation pipeline. One of the
great difficulties with current MT pipelines is that
these diverse components are often designed by
separate groups and have different file format and
interaction requirements. This leads to a large in-
vestment in scripts to convert formats and connect
the different components, and often leads to unten-
able and non-portable projects as well as hinder-
1The toolkit can be downloaded at http://www.
sourceforge.net/projects/joshua, and the in-
structions in using the toolkit are at http://cs.jhu.
edu/?ccb/joshua.
135
ing repeatability of experiments. To combat these
issues, the Joshua toolkit integrates most critical
components of the machine translation pipeline.
Moreover, each component can be treated as a
stand-alone tool and does not rely on the rest of
the toolkit we provide.
Scalability: Our third design goal was to en-
sure that the decoder is scalable to large models
and data sets. The parsing and pruning algorithms
are carefully implemented with dynamic program-
ming strategies, and efficient data structures are
used to minimize overhead. Other techniques con-
tributing to scalability includes suffix-array gram-
mar extraction, parallel and distributed decoding,
and bloom filter language models.
Below we give a short description about the
main functions implemented in our Joshua toolkit.
2.1 Training Corpus Sub-sampling
Rather than inducing a grammar from the full par-
allel training data, we made use of a method pro-
posed by Kishore Papineni (personal communica-
tion) to select the subset of the training data con-
sisting of sentences useful for inducing a gram-
mar to translate a particular test set. This method
works as follows: for the development and test
sets that will be translated, every n-gram (up to
length 10) is gathered into a map W and asso-
ciated with an initial count of zero. Proceeding
in order through the training data, for each sen-
tence pair whose source-to-target length ratio is
within one standard deviation of the average, if
any n-gram found in the source sentence is also
found in W with a count of less than k, the sen-
tence is selected. When a sentence is selected, the
count of every n-gram in W that is found in the
source sentence is incremented by the number of
its occurrences in the source sentence. For our
submission, we used k = 20, which resulted in
1.5 million (out of 23 million) sentence pairs be-
ing selected for use as training data. There were
30,037,600 English words and 30,083,927 French
words in the subsampled training corpus.
2.2 Suffix-array Grammar Extraction
Hierarchical phrase-based translation requires a
translation grammar extracted from a parallel cor-
pus, where grammar rules include associated fea-
ture values. In real translation tasks, the grammars
extracted from large training corpora are often far
too large to fit into available memory.
In such tasks, feature calculation is also very ex-
pensive in terms of time required; huge sets of
extracted rules must be sorted in two directions
for relative frequency calculation of such features
as the translation probability p(f |e) and reverse
translation probability p(e|f) (Koehn et al, 2003).
Since the extraction steps must be re-run if any
change is made to the input training data, the time
required can be a major hindrance to researchers,
especially those investigating the effects of tok-
enization or word segmentation.
To alleviate these issues, we extract only a sub-
set of all available rules. Specifically, we follow
Callison-Burch et al (2005; Lopez (2007) and use
a source language suffix array to extract only those
rules which will actually be used in translating a
particular set of test sentences. This results in a
vastly smaller rule set than techniques which ex-
tract all rules from the training set.
The current code requires suffix array rule ex-
traction to be run as a pre-processing step to ex-
tract the rules needed to translate a particular test
set. However, we are currently extending the de-
coder to directly access the suffix array. This will
allow the decoder at runtime to efficiently extract
exactly those rules needed to translate a particu-
lar sentence, without the need for a rule extraction
pre-processing step.
2.3 Decoding Algorithms2
Grammar formalism: Our decoder assumes a
probabilistic synchronous context-free grammar
(SCFG). Currently, it only handles SCFGs of the
kind extracted by Heiro (Chiang, 2007), but is eas-
ily extensible to more general SCFGs (e.g., (Gal-
ley et al, 2006)) and closely related formalisms
like synchronous tree substitution grammars (Eis-
ner, 2003).
Chart parsing: Given a source sentence to de-
code, the decoder generates a one-best or k-best
translations using a CKY algorithm. Specifically,
the decoding algorithm maintains a chart, which
contains an array of cells. Each cell in turn main-
tains a list of proven items. The parsing process
starts with the axioms, and proceeds by applying
the inference rules repeatedly to prove new items
until proving a goal item. Whenever the parser
proves a new item, it adds the item to the appro-
priate chart cell. The item also maintains back-
2More details on the decoding algorithms are provided in
(Li et al, 2009a).
136
pointers to antecedent items, which are used for
k-best extraction.
Pruning: Severe pruning is needed in order to
make the decoding computationally feasible for
SCFGs with large target-language vocabularies.
In our decoder, we incorporate two pruning tech-
niques: beam and cube pruning (Chiang, 2007).
Hypergraphs and k-best extraction: For each
source-language sentence, the chart-parsing algo-
rithm produces a hypergraph, which represents
an exponential set of likely derivation hypotheses.
Using the k-best extraction algorithm (Huang and
Chiang, 2005), we extract the k most likely deriva-
tions from the hypergraph.
Parallel and distributed decoding: We also
implement parallel decoding and a distributed
language model by exploiting multi-core and
multi-processor architectures and distributed com-
puting techniques. More details on these two fea-
tures are provided by Li and Khudanpur (2008b).
2.4 Language Models
In addition to the distributed LM mentioned
above, we implement three local n-gram language
models. Specifically, we first provide a straightfor-
ward implementation of the n-gram scoring func-
tion in Java. This Java implementation is able to
read the standard ARPA backoff n-gram models,
and thus the decoder can be used independently
from the SRILM toolkit.3 We also provide a na-
tive code bridge that allows the decoder to use the
SRILM toolkit to read and score n-grams. This
native implementation is more scalable than the
basic Java LM implementation. We have also im-
plemented a Bloom Filter LM in Joshua, following
Talbot and Osborne (2007).
2.5 Minimum Error Rate Training
Johsua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu. The optimization
consists of a series of line-optimizations along
the dimensions corresponding to the parameters.
The search across a dimension uses the efficient
method of Och (2003). Each iteration of our
MERT implementation consists of multiple weight
3This feature allows users to easily try the Joshua toolkit
without installing the SRILM toolkit and compiling the native
bridge code. However, users should note that the basic Java
LM implementation is not as scalable as the native bridge
code.
updates, each reflecting a greedy selection of the
dimension giving the most gain. Each iteration
also optimizes several random ?intermediate ini-
tial? points in addition to the one surviving from
the previous iteration, as an approximation to per-
forming multiple random restarts. More details on
the MERT method and the implementation can be
found in Zaidan (2009).4
3 WMT-09 Translation Task Results
3.1 Training and Development Data
We assembled a very large French-English train-
ing corpus (Callison-Burch, 2009) by conducting
a web crawl that targted bilingual web sites from
the Canadian government, the European Union,
and various international organizations like the
Amnesty International and the Olympic Commit-
tee. The crawl gathered approximately 40 million
files, consisting of over 1TB of data. We converted
pdf, doc, html, asp, php, etc. files into text, and
preserved the directory structure of the web crawl.
We wrote set of simple heuristics to transform
French URLs onto English URLs, and considered
matching documents to be translations of each
other. This yielded 2 million French documents
paired with their English equivalents. We split the
sentences and paragraphs in these documents, per-
formed sentence-aligned them using software that
IBM Model 1 probabilities into account (Moore,
2002). We filtered and de-duplcated the result-
ing parallel corpus. After discarding 630 thousand
sentence pairs which had more than 100 words,
our final corpus had 21.9 million sentence pairs
with 587,867,024 English words and 714,137,609
French words.
We distributed the corpus to the other WMT09
participants to use in addition to the Europarl
v4 French-English parallel corpus (Koehn, 2005),
which consists of approximately 1.4 million sen-
tence pairs with 39 million English words and 44
million French words. Our translation model was
trained on these corpora using the subsampling de-
scried in Section 2.1.
For language model training, we used the
monolingual news and blog data that was as-
sembled by the University of Edinburgh and dis-
tributed as part of WMT09. This data consisted
4The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
137
of 21.2 million English sentences with half a bil-
lion words. We used SRILM to train a 5-gram
language model using a vocabulary containing the
500,000 most frequent words in this corpus. Note
that we did not use the English side of the parallel
corpus as language model training data.
To tune the system parameters we used News
Test Set from WMT08 (Callison-Burch et al,
2008), which consists of 2,051 sentence pairs
with 43 thousand English words and 46 thou-
sand French words. This is in-domain data that
was gathered from the same news sources as the
WMT09 test set.
3.2 Translation Scores
The translation scores for four different systems
are reported in Table 1.5
Baseline: In this system, we use the GIZA++
toolkit (Och and Ney, 2003), a suffix-array archi-
tecture (Lopez, 2007), the SRILM toolkit (Stol-
cke, 2002), and minimum error rate training (Och,
2003) to obtain word-alignments, a translation
model, language models, and the optimal weights
for combining these models, respectively.
Minimum Bayes Risk Rescoring: In this sys-
tem, we re-ranked the n-best output of our base-
line system using Minimum Bayes Risk (Kumar
and Byrne, 2004). We re-score the top 300 trans-
lations to minimize expected loss under the Bleu
metric.
Deterministic Annealing: In this system, in-
stead of using the regular MERT (Och, 2003)
whose training objective is to minimize the one-
best error, we use the deterministic annealing
training procedure described in Smith and Eisner
(2006), whose objective is to minimize the ex-
pected error (together with the entropy regulariza-
tion technique).
Variational Decoding: Statistical models in
machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
(e.g., during decoding) is then computationally in-
tractable. Therefore, most systems use a simple
Viterbi approximation that measures the goodness
5Note that the implementation of the novel techniques
used to produce the non-baseline results is not part of the cur-
rent Joshua release, though we plan to incorporate it in the
next release.
System BLEU-4
Joshua Baseline 25.92
Minimum Bayes Risk Rescoring 26.16
Deterministic Annealing 25.98
Variational Decoding 26.52
Table 1: The uncased BLEU scores on WMT-09
French-English Task. The test set consists of 2525
segments, each with one reference translation.
of a string using only its most probable deriva-
tion. Instead, we develop a variational approxima-
tion, which considers all the derivations but still
allows tractable decoding. More details will be
provided in Li et al (2009b). In this system, we
have used both deterministic annealing (for train-
ing) and variational decoding (for decoding).
4 Conclusions
We have described a scalable toolkit for parsing-
based machine translation. It is written in Java
and implements all the essential algorithms de-
scribed in Chiang (2007) and Li and Khudanpur
(2008b): chart-parsing, n-gram language model
integration, beam- and cube-pruning, and k-best
extraction. The toolkit also implements suffix-
array grammar extraction (Callison-Burch et al,
2005; Lopez, 2007) and minimum error rate train-
ing (Och, 2003). Additionally, parallel and dis-
tributed computing techniques are exploited to
make it scalable. The decoder achieves state of
the art translation performance.
Acknowledgments
This research was supported in part by the Defense
Advanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001 and
the National Science Foundation under grants
No. 0713448 and 0840112. The views and find-
ings are the authors? alone.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
138
Chris Callison-Burch. 2009. A 109 word parallel cor-
pus. In preparation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, , and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008a. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In Proceedings of AMTA.
Zhifei Li and Sanjeev Khudanpur. 2008b. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009a. Decoding in joshua:
Open source, parsing-based machine translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In preparation.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the ACL/Coling.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
139
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 145?149,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The University of Maryland Statistical Machine Translation System for
the Fourth Workshop on Machine Translation
Chris Dyer??, Hendra Setiawan?, Yuval Marton??, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park, MD 20742, USA
{redpony,hendra,ymarton,resnik} AT umd.edu
Abstract
This paper describes the techniques we
explored to improve the translation of
news text in the German-English and
Hungarian-English tracks of the WMT09
shared translation task. Beginning with a
convention hierarchical phrase-based sys-
tem, we found benefits for using word seg-
mentation lattices as input, explicit gen-
eration of beginning and end of sentence
markers, minimum Bayes risk decoding,
and incorporation of a feature scoring the
alignment of function words in the hy-
pothesized translation. We also explored
the use of monolingual paraphrases to im-
prove coverage, as well as co-training to
improve the quality of the segmentation
lattices used, but these did not lead to im-
provements.
1 Introduction
For the shared translation task of the Fourth Work-
shop on Machine Translation (WMT09), we fo-
cused on two tasks: German to English and Hun-
garian to English translation. Despite belonging to
different language families, German and Hungar-
ian have three features in common that complicate
translation into English:
1. productive compounding (especially of
nouns),
2. rich inflectional morphology,
3. widespread mid- to long-range word order
differences with respect to English.
Since these phenomena are poorly addressed with
conventional approaches to statistical machine
translation, we chose to work primarily toward
mitigating their negative effects when construct-
ing our systems. This paper is structured as fol-
lows. In Section 2 we describe the baseline model,
Section 3 describes the various strategies we em-
ployed to address the challenges just listed, and
Section 4 summarizes the final translation system.
2 Baseline system
Our translation system makes use of a hierarchical
phrase-based translation model (Chiang, 2007),
which we argue is a strong baseline for these
language pairs. First, such a system makes use
of lexical information when modeling reorder-
ing (Lopez, 2008), which has previously been
shown to be useful in German-to-English trans-
lation (Koehn et al, 2008). Additionally, since
the decoder is based on a CKY parser, it can con-
sider all licensed reorderings of the input in poly-
nomial time, and German and Hungarian may re-
quire quite substantial reordering. Although such
decoders and models have been common for sev-
eral years, there have been no published results for
these language pairs.
The baseline system translates lowercased and
tokenized source sentences into lowercased target
sentences. The features used were the rule transla-
tion relative frequency P (e?|f?), the ?lexical? trans-
lation probabilities Plex(e?|f?) and Plex(f? |e?), a rule
count, a target language word count, the target
(English) language model P (eI1), and a ?pass-
through? penalty for passing a source language
word to the target side.1 The rule feature values
were computed online during decoding using the
suffix array method described by Lopez (2007).
1The ?pass-through? penalty was necessary since the En-
glish language modeling data contained a large amount of
source-language text.
145
2.1 Training and development data
To construct the translation suffix arrays used to
compute the translation grammar, we used the par-
allel training data provided. The preprocessed
training data was filtered for length and aligned
using the GIZA++ implementation of IBM Model
4 (Och and Ney, 2003) in both directions and sym-
metrized using the grow-diag-final-and
heuristic. We trained a 5-gram language model
from the provided English monolingual training
data and the non-Europarl portions of the parallel
training data using modified Kneser-Ney smooth-
ing as implemented in the SRI language modeling
toolkit (Kneser and Ney, 1995; Stolcke, 2002). We
divided the 2008 workshop ?news test? sets into
two halves of approximately 1000 sentences each
and designated one the dev set and the other the
dev-test set.
2.2 Automatic evaluation metric
Since the official evaluation criterion for WMT09
is human sentence ranking, we chose to minimize
a linear combination of two common evaluation
metrics, BLEU and TER (Papineni et al, 2002;
Snover et al, 2006), during system development
and tuning:
TER ? BLEU
2
Although we are not aware of any work demon-
strating that this combination of metrics correlates
better than either individually in sentence ranking,
Yaser Al-Onaizan (personal communication) re-
ports that it correlates well with the human evalua-
tion metric HTER. In this paper, we report uncased
TER and BLEU individually.
2.3 Forest minimum error training
To tune the feature weights of our system, we used
a variant of the minimum error training algorithm
(Och, 2003) that computes the error statistics from
the target sentences from the translation search
space (represented by a packed forest) that are ex-
actly those that are minimally discriminable by
changing the feature weights along a single vector
in the dimensions of the feature space (Macherey
et al, 2008). The loss function we used was the
linear combination of TER and BLEU described in
the previous section.
3 Experimental variations
This section describes the experimental variants
explored.
3.1 Word segmentation lattices
Both German and Hungarian have a large number
of compound words that are created by concate-
nating several morphemes to form a single ortho-
graphic token. To deal with productive compound-
ing, we employ word segmentation lattices, which
are word lattices that encode alternative possible
segmentations of compound words. Doing so en-
ables us to use possibly inaccurate approaches to
guess the segmentation of compound words, al-
lowing the decoder to decide which to use during
translation. This is a further development of our
general source-lattice approach to decoding (Dyer
et al, 2008).
To construct the segmentation lattices, we de-
fine a log-linear model of compound word seg-
mentation inspired by Koehn and Knight (2003),
making use of features including number of mor-
phemes hypothesized, frequency of the segments
as free-standing morphemes in a training corpus,
and letters in each segment. To tune the model
parameters, we selected a set of compound words
from a subset of the German development set,
manually created a linguistically plausible seg-
mentation of these words, and used this to select
the parameters of the log-linear model using a lat-
tice minimum error training algorithm to minimize
WER (Macherey et al, 2008). We reused the same
features and weights to create the Hungarian lat-
tices. For the test data, we created a lattice of ev-
ery possible segmentation of any word 6 charac-
ters or longer and used forward-backward pruning
to prune out low-probability segmentation paths
(Sixtus and Ortmanns, 1999). We then concate-
nated the lattices in each sentence.
Source Condition BLEU TER
German
baseline 20.8 60.7
lattice 21.3 59.9
Hungarian
baseline 11.0 71.1
lattice 12.3 70.4
Table 1: Impact of compound segmentation lat-
tices.
To build the translation model for lattice sys-
tem, we segmented the training data using the one-
best split predicted by the segmentation model,
146
and word aligned this with the English side. This
variant version of the training data was then con-
catenated with the baseline system?s training data.
3.1.1 Co-training of segmentation model
To avoid the necessity of manually creating seg-
mentation examples to train the segmentation
model, we attempted to generate sets of training
examples by selecting the compound splits that
were found along the path chosen by the decoder?s
one-best translation. Unfortunately, the segmen-
tation system generated in this way performed
slightly worse than the one-best baseline and so
we continued to use the parameter settings derived
from the manual segmentation.
3.2 Modeling sentence boundaries
Incorporating an n-gram language model proba-
bility into a CKY-based decoder is challenging.
When a partial hypothesis (also called an ?item?)
has been completed, it has not yet been determined
what strings will eventually occur to the left of
its first word, meaning that the exact computation
must deferred, which makes pruning a challenge.
In typical CKY decoders, the beginning and ends
of the sentence (which often have special charac-
teristics) are not conclusively determined until the
whole sentence has been translated and the proba-
bilities for the beginning and end sentence proba-
bilities can be added. However, by this point it is
often the case that a possibly better sentence be-
ginning has been pruned away. To address this,
we explicitly generate beginning and end sentence
markers as part of the translation process, as sug-
gested by Xiong et al (2008). The results of doing
this are shown in Table 2.
Source Condition BLEU TER
German
baseline 21.3 59.9
+boundary 21.6 60.1
Hungarian
baseline 12.3 70.4
+boundary 12.8 70.4
Table 2: Impact of modeling sentence boundaries.
3.3 Source language paraphrases
In order to deal with the sparsity associated with
a rich source language morphology and limited-
size parallel corpora (bitexts), we experimented
with a novel approach to paraphrasing out-of-
vocabulary (OOV) source language phrases in
our Hungarian-English system, using monolingual
contextual similarity rather than phrase-table piv-
oting (Callison-Burch et al, 2006) or monolin-
gual bitexts (Barzilay and McKeown, 2001; Dolan
et al, 2004). Distributional profiles for source
phrases were represented as context vectors over
a sliding window of size 6, with vectors defined
using log-likelihood ratios (cf. Rapp (1999), Dun-
ning (1993)) but using cosine rather than city-
block distance to measure profile similarity.
The 20 distributionally most similar source
phrases were treated as paraphrases, considering
candidate phrases up to a width of 6 tokens and fil-
tering out paraphrase candidates with cosine simi-
larity to the original of less than 0.6. The two most
likely translations for each paraphrase were added
to the grammar in order to provide mappings to
English for OOV Hungarian phrases.
This attempt at monolingually-derived source-
side paraphrasing did not yield improvements over
baseline. Preliminary analysis suggests that the
approach does well at identifying many content
words in translating extracted paraphrases of OOV
phrases (e.g., a kommunista part vezetaje ? ,
leader of the communist party or a ra tervezett?
until the planned to), but at the cost of more fre-
quently omitting target words in the output.
3.4 Dominance feature
Although our baseline hierarchical system permits
long-range reordering, it lacks a mechanism to
identify the most appropriate reordering for a spe-
cific sentence translation. For example, when the
most appropriate reordering is a long-range one,
our baseline system often also has to consider
shorter-range reorderings as well. In the worst
case, a shorter-range reordering has a high proba-
bility, causing the wrong reordering to be chosen.
Our baseline system lacks the capacity to address
such cases because all the features it employs are
independent of the phrases being moved; these are
modeled only as an unlexicalized generic nonter-
minal symbol.
To address this challenge, we included what we
call a dominance feature in the scoring of hypothe-
sis translations. Briefly, the premise of this feature
is that the function words in the sentence hold the
key reordering information, and therefore function
words are used to model the phrases being moved.
The feature assesses the quality of a reordering by
looking at the phrase alignment between pairs of
147
function words. In our experiments, we treated
the 128 most frequent words in the corpus as func-
tion words, similar to Setiawan et al (2007). Due
to space constraints, we will discuss the details in
another publication. As Table 3 reports, the use of
this feature yields positive results.
Source Condition BLEU TER
German
baseline 21.6 60.1
+dom 22.2 59.8
Hungarian
baseline 12.8 70.4
+dom 12.6 70.0
Table 3: Impact of alignment dominance feature.
3.5 Minimum Bayes risk decoding
Although during minimum error training we as-
sume a decoder that uses the maximum derivation
decision rule, we find benefits to translating using
a minimum risk decision rule on a test set (Kumar
and Byrne, 2004). This seeks the translation E of
the input lattice F that has the least expected loss,
measured by some loss function L:
E? = arg min
E?
EP (E|F)[L(E,E
?)] (1)
= arg min
E?
?
E
P (E|F)L(E,E?) (2)
We approximate the posterior distribution
P (E|F) and the set of possible candidate transla-
tions using the unique 500-best translations of a
source lattice F . If H(E,F) is the decoder?s path
weight, this is:
P (E|F) ? exp?H(E,F)
The optimal value for the free parameter ?must
be experimentally determined and depends on the
ranges of the feature functions and weights used in
the model, as well as the amount and kind of prun-
ing using during decoding.2 For our submission,
we used ? = 1. Since our goal is to minimize
TER?BLEU
2 we used this as the loss function in (2).
Table 4 shows the results on the dev-test set for
MBR decoding.
2If the free parameter ? lies in (1,?) the distribution is
sharpened, if it lies in [0, 1), the distribution is flattened.
Source Decoder BLEU TER
German
Max-D 22.2 59.8
MBR 22.6 59.4
Hungarian
Max-D 12.6 70.0
MBR 12.8 69.8
Table 4: Performance of maximum derivation vs.
MBR decoders.
4 Conclusion
Table 5 summarizes the impact on the dev-test set
of all features included in the University of Mary-
land system submission.
Condition
German Hungarian
BLEU TER BLEU TER
baseline 20.8 60.7 11.0 71.1
+lattices 21.3 59.9 12.3 70.4
+boundary 21.6 60.1 12.8 70.4
+dom 22.2 59.8 12.6 70.0
+MBR 22.6 59.4 12.8 69.8
Table 5: Summary of all features
Acknowledgments
This research was supported in part by the
GALE program of the Defense Advanced Re-
search Projects Agency, Contract No. HR0011-
06-2-001, and the Army Research Laboratory.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the view of the
sponsors. Discussions with Chris Callison-Burch
were helpful in carrying out the monolingual para-
phrase work.
References
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In In
Proceedings of ACL-2001.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings NAACL-
2006.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
148
exploiting massively parallel news sources. In Pro-
ceedings of the 20th International Conference on
Computational Linguistics of the Association for
Computational Linguistics, Geneva, Switzerland.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
Chris Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT. Association for Compu-
tational Linguistics, June.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
P. Koehn and K. Knight. 2003. Empirical methods
for compound splitting. In Proceedings of the EACL
2003.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In ACL Work-
shop on Statistical Machine Translation.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 976?
985.
Adam Lopez. 2008. Tera-scale translation models
via pattern matching. In Proceedings of COLING,
Manchester, UK.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proceedings of EMNLP, Honolulu, HI.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual
Meeting of the ACL, pages 311?318.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th Annual
Conference of the Association for Computational
Linguistics., pages 519?525.
Hendra Setiawan, Min-Yen Kan, and Haizhao Li.
2007. Ordering phrases with function words. In
Proceedings of ACL.
S. Sixtus and S. Ortmanns. 1999. High quality word
graphs using forward-backward pruning. In Pro-
ceedings of ICASSP, Phoenix, AZ.
Matthew Snover, Bonnie J. Dorr, Richard Schwartz,
Linnea Micciulla, and John Makhoul. 2006. A
study of translation edit rate with targeted human an-
notation. In Proceedings of Association for Machine
Translation in the Americas.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Intl. Conf. on Spoken Language
Processing.
Deyi Xiong, Min Zhang, Ai Ti Aw, Haitao Mi, Qun
Liu, and Shouxun Lin. 2008. Refinements in BTG-
based statistical machine translation. In Proceed-
ings of IJCNLP 2008.
149
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102?110,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Monte Carlo inference and maximization for phrase-based translation
Abhishek Arun?
a.arun@sms.ed.ac.uk
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Adam Lopez?
alopez@inf.ed.ac.uk
Barry Haddow?
bhaddow@inf.ed.ac.uk
Philipp Koehn?
pkoehn@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
Recent advances in statistical machine
translation have used beam search for
approximate NP-complete inference within
probabilistic translation models. We present
an alternative approach of sampling from the
posterior distribution defined by a translation
model. We define a novel Gibbs sampler
for sampling translations given a source
sentence and show that it effectively explores
this posterior distribution. In doing so
we overcome the limitations of heuristic
beam search and obtain theoretically sound
solutions to inference problems such as
finding the maximum probability translation
and minimum expected risk training and
decoding.
1 Introduction
Statistical machine translation (SMT) poses the
problem: given a foreign sentence f , find the
translation e? that maximises the conditional
posterior probability p(e|f). This probabilistic
formulation of translation has driven development
of state-of-the-art systems which are able to learn
from parallel corpora which were generated for
other purposes ? a direct result of employing a
mathematical framework that we can reason about
independently of any particular model.
For example, we can train SMT models using
maximum likelihood estimation (Brown et al, 1993;
Och and Ney, 2000; Marcu and Wong, 2002). Alter-
natively, we can train to minimise probabilistic con-
ceptions of risk (expected loss) with respect to trans-
lation metrics, thereby obtaining better results for
those metrics (Kumar and Byrne, 2004; Smith and
Eisner, 2006; Zens and Ney, 2007). We can also use
Bayesian inference techniques to avoid resorting to
heuristics that damage the probabilistic interpreta-
tion of the models (Zhang et al, 2008; DeNero et
al., 2008; Blunsom et al, 2009).
Most models define multiple derivations for each
translation; the probability of a translation is thus
the sum over all of its derivations. Unfortunately,
finding the maximum probability translation is NP-
hard for all but the most trivial of models in this
setting (Sima?an, 1996). It is thus necessary to resort
to approximations for this sum and the search for its
maximum e?.
The most common of these approximations is
the max-derivation approximation, which for many
models can be computed in polynomial time via
dynamic programming (DP). Though effective for
some problems, it has many serious drawbacks for
probabilistic inference:
1. It typically differs from the true model maxi-
mum.
2. It often requires additional approximations in
search, leading to further error.
3. It introduces restrictions on models, such as
use of only local features.
4. It provides no good solution to compute the
normalization factor Z(f) required by many prob-
abilistic algorithms.
In this work, we solve these problems using a
Monte Carlo technique with none of the above draw-
backs. Our technique is based on a novel Gibbs
sampler that draws samples from the posterior dis-
tribution of a phrase-based translation model (Koehn
et al, 2003) but operates in linear time with respect
to the number of input words (Section 2). We show
102
that it is effective for both decoding (Section 3) and
minimum risk training (Section 4).
2 A Gibbs sampler for phrase-based
translation models
We begin by assuming a phrase-based translation
model in which the input sentence, f , is segmented
into phrases, which are sequences of adjacent
words.1 Each foreign phrase is translated into the
target language, to produce an output sentence e
and an alignment a representing the mapping from
source to target phrases. Phrases are allowed to be
reordered during translation.
The model is defined with a log-linear form,
with feature function vector h and parametrised by
weight vector ?, as described in Koehn et al (2003).
P (e, a|f ;?) = exp [? ? h(e, a, f)]?
?e?,a?? exp [? ? h(e?, a?, f)]
(1)
The features h of the model are usually few and
are themselves typically probabilistic models
indicating e.g, the relative frequency of a target
phrase translation given a source phrase (translation
model), the fluency of the target phrase (language
model) and how phrases reorder with respect
to adjacent phrases (reordering model). There
is a further parameter ? that limits how many
source language words may intervene between
two adjacent target language phrases. For the
experiments in this paper, we use ? = 6.
2.1 Gibbs sampling
We use Markov chain Monte Carlo (MCMC) as an
alternative to DP search (Geman and Geman, 1984;
Metropolis and Ulam, 1949). MCMC probabilis-
tically generates sample derivations from the com-
plete search space. The probability of generating
each sample is conditioned on the previous sam-
ple, forming a Markov chain. After a long enough
interval (referred to as the burn-in) this chain returns
samples from the desired distribution.
Our MCMC sampler uses Gibbs sampling, which
obtains samples from the joint distribution of a set
of random variables X = {X1, . . . , Xn}. It starts
with some initial state (X1 = x10, . . . , Xn = xn0),
and generates a Markov chain of samples, where
1These phrases are not necessarily linguistically motivated.
each sample is the result of applying a set of Gibbs
operators to the previous sample. Each operator is
defined by specifying a subset of the random vari-
ables Y ? X , which the operator updates by sam-
pling from the conditional distribution P (Y |X \Y ).
The set X \ Y is referred to as the Markov blanket
and is unchanged by the operator.
In the case of translation, we require a Gibbs sam-
pler that produces a sequence of samples, SN1 =
(e1, a1) . . . (eN , aN ), that are drawn from the dis-
tribution P (e, a|f). These samples can thus be used
to estimate the expectation of a function h(e, a, f)
under the distribution as follows:
EP (a,e|f)[h] = limN??
1
N
N?
i=1
h(ai, ei, f) (2)
Taking h to be an indicator function
h = ?(a, a?)?(e, e?) provides an estimate of
P (a?, e?|f), and using h = ?(e, e?) marginalises over
all derivations a?, yielding an estimate of P (e?|f).
2.2 Gibbs operators
Our sampler consists of three operators. Examples
of these are depicted in Figure 1.
The RETRANS operator varies the translation of a
single source phrase. Segmentation, alignment, and
all other translations are held constant.
The MERGE-SPLIT operator varies the source
segmentation at a single word boundary. If the
boundary is a split point in the current hypothesis,
the adjoining phrases can be merged, provided
that the corresponding target phrases are adjacent
and the phrase table contains a translation of the
merged phrase. If the boundary is not a split point,
the covering phrase may be split, provided that
the phrase table contains a translation of both new
phrases. Remaining segmentation points, phrase
alignment and phrase translations are held constant.
The REORDER operator varies the target phrase
order for a pair of source phrases, provided that
the new alignment does not violate reordering limit
?. Segmentation, phrase translations, and all other
alignments are held constant.
To illustrate the RETRANS operator, we will
assume a simplified model with two features: a
bigram language model Plm and a translation model
Ptm. Both features are assigned a weight of 1.
103
c?est un re?sultat remarquable
it is some result remarkable
(a)
Initial
c?est un re?sultat remarquable
but some result remarkable
(b)
Retrans
c?est un re?sultat remarquable
it is a result remarkable
(c)
Merge
c?est un re?sultat remarquable
it is a remarkable result
(d)
Reorder
1
Figure 1: Example evolution of an initial hypothesis via
application of several operators, with Markov blanket
indicated by shading.
We denote the start of the sentence with S and the
language model context with C. Assuming the
French phrase c?est can be translated either as it is or
but, the RETRANS operator at step (b) stochastically
chooses an English phrase, e? in proportion to the
phrases? conditional probabilities.
P (but|c?est,C) = Ptm(but|c?est) ? Plm(S but some)Z
and
P (it is|c?est,C) = Ptm(it is|c?est) ? Plm(S it is some)Z
where
Z = Ptm(but|c?est) ? Plm(S but some) +
Ptm(it is|c?est) ? Plm(S it is some)
Conditional distributions for the MERGE-SPLIT and
REORDER operators can be derived in an analogous
fashion.
A complete iteration of the sampler consists of
applying each operator at each possible point in the
sentence, and a sample is collected after each opera-
tor has performed a complete pass.
2.3 Algorithmic complexity
Since both the RETRANS and MERGE-SPLIT oper-
ators are applied by iterating over source side word
positions, their complexity is linear in the size of the
input.
The REORDER operator iterates over the positions
in the input and for the source phrase found at that
position considers swapping its target phrase with
that of every other source phrase, provided that the
reordering limit is not violated. This means that it
can only consider swaps within a fixed-length win-
dow, so complexity is linear in sentence length.
2.4 Experimental verification
To verify that our sampler was behaving as expected,
we computed the KL divergence between its
inferred distribution q?(e|f) and the true distribution
over a single sentence (Figure 2). We computed
the true posterior distribution p(e|f) under an
Arabic-English phrase-based translation model
with parameters trained to maximise expected
BLEU (Section 4), summing out the derivations for
identical translations and computing the partition
term Z(f). As the number of iterations increases,
the KL divergence between the distributions
approaches zero.
3 Decoding
The task of decoding amounts to finding the single
translation e? that maximises or minimises some cri-
terion given a source sentence f . In this section
we consider three common approaches to decod-
ing, maximum translation (MaxTrans), maximum
derivation (MaxDeriv), and minimum risk decoding
(MinRisk):
e? =
?
?
?
argmax(e,a) p(e, a|f) (MaxDeriv)
argmaxe p(e|f) (MaxTrans)
argmine?e? `e?(e)p(e?|f) (MinRisk)
In the minimum risk decoder, `e?(e) is any real-
valued loss (error) function that computes the error
of one hypothesis e with respect to some reference
e?. Our loss is a sentence-level approximation of
(1 ? BLEU).
As noted in section 2, the Gibbs sampler can
be used to provide an estimate of the probability
distribution P (a, e|f) and therefore to determine
the maximum of this distribution, in other words
the most likely derivation. Furthermore, we can
marginalise over the alignments to estimate P (e|f)
104
Iterations
KL d
iverg
ence
l l l
l l
l
l l
l
l
l
l
l l l
l l
10 100 1000 10000 100000 1000000
0.00
1
0.01
0.1
KL Divergence
Figure 2: The KL divergence of the true posterior distri-
bution and the distribution estimated by the Gibbs sam-
pler at different numbers of iterations for the Arabic
source sentence r}ys wzrA? mAlyzyA yzwr Alflbyn (in
English, The prime minister of Malaysia visits the Philip-
pines).
and so obtain the most likely translation. The Gibbs
sampler can therefore be used as a decoder, either
running in max-derivation and max-translation
mode. Using the Gibbs sampler in this way makes
max-translation decoding tractable, and so will
help determine whether max-translation offers any
benefit over the usual max-derivation. Using the
Gibbs sampler as a decoder also allows us to verify
that it is producing valid samples from the desired
distribution.
3.1 Training data and preparation.
The experiments in this section were performed
using the French-English and German-English
parallel corpora from the WMT09 shared translation
task (Callison-Burch et al, 2009), as well as 300k
parallel Arabic-English sentences from the NIST
MT evaluation training data.2 For all language
pairs, we constructed a phrase-based translation
model as described in Koehn et al (2003), limiting
the phrase length to 5. The target side of the parallel
corpus was used to train a 3-gram language model.
2The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
sentences with confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For the German and French systems, the DEV2006
set was used for model tuning and the TEST2007
(in-domain) and NEWS-DEV2009B (out-of-domain)
sets for testing. For the Arabic system, the MT02
set (10 reference translations) was used for tuning
and MT03 (4 reference translations) was used for
evaluation. To reduce the size of the phrase table,
we used the association-score technique suggested
by Johnson et al (2007a). Translation quality is
reported using case-insensitive BLEU (Papineni et
al., 2002).
3.2 Translation performance
For the experiments reported in this section, we
used feature weights trained with minimum error
rate training (MERT; Och, 2003) . Because MERT
ignores the denominator in Equation 1, it is invari-
ant with respect to the scale of the weight vector
? ? the Moses implementation simply normalises
the weight vector it finds by its `1-norm. However,
when we use these weights in a true probabilistic
model, the scaling factor affects the behaviour of
the model since it determines how peaked or flat the
distribution is. If the scaling factor is too small, then
the distribution is too flat and the sampler spends
too much time exploring unimportant probability
regions. If it is too large, then the distribution is too
peaked and the sampler may concentrate on a very
narrow probability region. We optimised the scaling
factor on a 200-sentence portion of the tuning set,
finding that a multiplicative factor of 10 worked best
for fr-en and a multiplicative factor of 6 for de-en. 3
The first experiment shows the effect of different
initialisations and numbers of sampler iterations on
max-derivation decoding performance of the sam-
pler. The Moses decoder (Koehn et al, 2007) was
used to generate the starting hypothesis, either in
full DP max-derivation mode, or alternatively with
restrictions on the features and reordering, or with
zero weights to simulate a random initialisation, and
the number of iterations varied from 100 to 200,000,
with a 100 iteration burn-in in each case. Figure 3
shows the variation of model score with sampler iter-
ation, for the different starting points, and for both
language pairs.
3We experimented with annealing, where the scale factor is
gradually increased to sharpen the distribution while sampling.
However, we found no improvements with annealing.
105
?
20.1
?
20.0
?
19.9
?
19.8
?
19.7
?
19.6
Iterations
Mod
el sc
ore
100 1000 10000
French?English
Initialisationfull
mono
nolm
zero
?
40.6
?
40.4
?
40.2
?
40.0
?
39.8
Iterations
Mod
el sc
ore
100 1000 10000 100000
German?English
Initialisationfull
mono
nolm
zero
Figure 3: Mean maximum model score, as a function of iteration number and starting point. The starting point can
either be the full max-derivation translation (full), the monotone translation (mono), the monotone translation with no
language model (nolm) or the monotone translation with all weights set to zero (zero).
Comparing the best model scores found by the
sampler, with those found by the Moses decoder
with its default settings, we found that around
50,000 sampling iterations were required for
fr-en and 100,000 for de-en, for the sampler to
give equivalent model scores to Moses. From
Figure 3 we can see that the starting point did not
have an appreciable effect on the model score of
the best derivation, except with low numbers of
iterations. This indicates that the sampler is able
to move fairly quickly towards the maximum of
the distribution from any starting point, in other
words it has good mobility. Running the sampler
for 100,000 iterations took on average 1670 seconds
per sentence on the French-English data set and
1552 seconds per sentence on German-English.
A further indication of the dependence of sampler
accuracy on the iteration count is provided by Fig-
ure 4. In this graph, we show the mean Spearman?s
rank correlation between the nbest lists of deriva-
tions when ranked by (i) model score and (ii) the
posterior probability estimated by the sampler. This
measure of sampler accuracy also shows a logarith-
mic dependence on the sample size.
3.3 Minimum risk decoding
The sampler also allows us to perform minimum
Bayes risk (MBR) decoding, a technique introduced
by Kumar and Byrne (2004). In their work, as an
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iterations
Corr
elati
on
100 1000 10000 100000
Language Pairsfr?ende?en
Figure 4: Mean Spearman?s rank correlation of 1000-best
list of derivations ranked according to (i) model score and
(ii) posterior probability estimated by sampler. This was
measured on a 200 sentence subset of DEV2006.
approximation of the model probability distribution,
the expected loss of the decoder is calculated by
summing over an n-best list. With the Gibbs sam-
pler, however, we should be able to obtain a much
more accurate view of the model probability distri-
bution. In order to compare max-translation, max-
derivation and MBR decoding with the Gibbs sam-
pler, and the Moses baseline, we ran experiments
106
fr-en de-en
in out in out
Moses 32.7 19.1 27.4 15.9
MaxD 32.6 19.1 27.0 15.5
MaxT 32.6 19.1 27.4 16.0
MBR 32.6 19.2 27.3 16.0
Table 1: Comparison of the BLEU score of the Moses
decoder with the sampler running in max-derivation
(MaxD), max-translation (MaxT) and minumum Bayes
risk (MBR) modes. The test sets are TEST2007 (in) and
NEWS-DEV2009B (out)
on both European language pairs, using both the in-
domain and out-of-domain test sets. The sampler
was initialised with the output of Moses with the
feature weights set to zero and restricted to mono-
tone, and run for 100,000 iterations with a 100 iter-
ation burn-in. The scale factors were set to the same
values as in the previous experiment. The relative
translation quality (measured according to BLEU) is
shown in Table 1.
3.4 Discussion
These results show very little difference between the
decoding methods, indicating that the Gibbs sam-
pling decoder can perform as well as a standard DP
based max-derivation decoder with these models,
and that there is no gain from doing max-translation
or MBR decoding. However it should be noted that
the model used for these experiments was optimised
by MERT, for max-derivation decoding, and so the
experiments do not rule out the possibility that max-
translation and MBR decoding will offer an advan-
tage on an appropriately optimised model.
4 Minimum risk training
In the previous section, we described how our sam-
pler can be used to search for the best translation
under a variety of decoding criteria (max deriva-
tion, translation, and minimum risk). However, there
appeared to be little benefit to marginalizing over
the latent derivations. This is almost certainly a side
effect of the MERT training approach that was used
to construct the models so as to maximise the per-
formance of the model on its single best derivation,
without regard to the shape of the rest of the dis-
tribution (Blunsom et al, 2008). In this section we
describe a further application of the Gibbs sampler:
to do unbiased minimum risk training.
While there have been at least two previous
attempts to do minimum risk training for MT, both
approaches relied on biased k-best approximations
(Smith and Eisner, 2006; Zens and Ney, 2007).
Since we sample from the whole distribution, we
will have a more accurate risk assessment.
The risk, or expected loss, of a probabilistic trans-
lation model on a corpus D, defined with respect to
a particular loss function `e?(e), where e? is the refer-
ence translation and e is a hypothesis translation
L = ?
?e?,f??D
?
e
p(e|f)`e?(e) (3)
This value can be trivially computed using equa-
tion (2). In this section, we are concerned with find-
ing the parameters ? that minimise (3). Fortunately,
with the log-linear parameterization of p(e|f), L is
differentiable with respect to ?:
?L
??k
= ?
?e?,f??D
?
e
p(e|f)`e?(e)
(
hk ? Ep(e|f)[hk]
)
(4)
Equation (4) is slightly more complicated to com-
pute using the sampler since it requires the feature
expectation in order to evaluate the final term. How-
ever, this can be done simply by making two passes
over the samples, computing the feature expecta-
tions on the first pass and the gradient on the second.
We have now shown how to compute our
objective (3), the expected loss, and a gradient
with respect to the model parameters we want
to optimise, (4), so we can use any standard
first-order optimization technique. Since the
sampler introduces stochasticity into the gradient
and objective, we use stochastic gradient descent
methods which are more robust to noise than
more sophisticated quasi-Newtonian methods
like L-BFGS (Schraudolph et al, 2007). For the
experiments below, we updated the learning rate
after each step proportionally to difference in
successive gradients (Schraudolph, 1999).
For the experiments reported in this section, we
used sample sizes of 8000 and estimated the gradi-
ent on sets of 100 sentences drawn randomly (with
replacement) from the development corpus. For a
107
Training Decoder MT03
Moses Max Derivation 44.6
MERT Moses MBR 44.8
Gibbs MBR 44.9
Moses Max Derivation 40.6
MinRisk MaxTrans 41.8
Gibbs MBR 42.9
Table 2: Decoding with minimum risk trained systems,
compared with decoding with MERT-trained systems on
Arabic to English MT03 data
loss function we use 4-gram (1 ? BLEU) computed
individually for each sentence4. By examining per-
formance on held-out data, we find the model con-
verges typically in fewer than 20 iterations.
4.1 Training experiments
During preliminary experiments with training, we
observed on a held-out data set (portions of MT04)
that the magnitude of the weights vector increased
steadily (effectively sharpening the distribution), but
without any obvious change in the objective. Since
this resulted in poor generalization we added a reg-
ularization term of ||~? ? ~?||2/2?2 to L. We initially
set the means to zero, but after further observing that
the translations under all decoding criteria tended to
be shorter than the reference (causing a significant
drop in performance when evaluated using BLEU),
we found that performance could be improved by
setting ?WP = ?0.5, indicating a preference for a
lower weight on this parameter.
Table 2 compares the performance on Arabic to
English translation of systems tuned with MERT
(maximizing corpus BLEU) with systems tuned to
maximise expected sentence-level BLEU. Although
the performance of the minimum risk model under
all decoding criteria is lower than that of the orig-
inal MERT model, we note that the positive effect
of marginalizing over derivations as well as using
minimum risk decoding for obtaining good results
on this model. A full exploration of minimum risk
training is beyond the scope of this paper, but these
initial experiments should help emphasise the versa-
tility of the sampler and its utility in solving a variety
of problems. In the conclusion, we will, however,
4The ngram precision counts are smoothed by adding 0.01
for n > 1
discuss some possible future directions that can be
taken to make this style of training more competitive
with standard baseline systems.
5 Discussion and future work
We have described an algorithmic technique that
solves certain problems, but also verifies the utility
of standard approximation techniques. For exam-
ple, we found that on standard test sets the sampler
performs similarly to the DP max-derivation solu-
tion and equally well regardless of how it is ini-
tialised. From this we conclude that at least for
MERT-trained models, the max-derivation approx-
imation is adequate for finding the best translation.
Although the training approach presented in
Section 4 has a number of theoretical advantages,
its performance in a one-best evaluation falls short
when compared with a system tuned for optimal
one-best performance using MERT. This contradicts
the results of Zens and Ney (2007), who optimise
the same objective and report improvements over a
MERT baseline. We conjecture that the difference
is due to the biased k-best approximation they used.
By considering only the most probable derivations,
they optimise a smoothed error surface (as one
does in minimum risk training), but not one that
is indicative of the true risk. If our hypothesis
is accurate, then the advantage is accidental and
ultimately a liability. Our results are in line with
those reported by Smith and Eisner (2006) who
find degradation in performance when minimizing
risk, but compensate by ?sharpening? the model
distribution for the final training iterations,
effectively maximising one-best performance
rather minimising risk over the full distribution
defined by their model. In future work, we will
explore possibilities for artificially sharpening the
distribution during training so as to better anticipate
the one-best evaluation conditions typical of MT.
However, for applications which truly do require a
distribution over translations, such as re-ranking,
our method for minimising expected risk would be
the objective of choice.
Using sampling for model induction has two fur-
ther advantages that we intend to explore. First,
although MERT performs quite well on models with
108
small numbers of features (such as those we consid-
ered in this paper), in general the algorithm severely
limits the number of features that can be used since
it does not use gradient-based updates during opti-
mization, instead updating one feature at a time. Our
training method (Section 4) does not have this limi-
tation, so it can use many more features.
Finally, for the DP-based max-derivation approx-
imation to be computationally efficient, the features
characterizing the steps in the derivation must be
either computable independently of each other or
with only limited local context (as in the case of the
language model or distortion costs). This has led to
a situation where entire classes of potentially use-
ful features are not considered because they would
be impractical to integrate into a DP based trans-
lation system. With the sampler this restriction is
mitigated: any function of h(e, f, a) may partici-
pate in the translation model subject only to its own
computability. Freed from the rusty manacles of
dynamic programming, we anticipate development
of many useful features.
6 Related work
Our sampler is similar to the decoder of Germann
et al (2001), which starts with an approximate solu-
tion and then incrementally improves it via operators
such as RETRANS and MERGE-SPLIT. It is also
similar to the estimator of Marcu and Wong (2002),
who employ the same operators to search the align-
ment space from a heuristic initialisation. Although
the operators are similar, the use is different. These
previous efforts employed their operators in a greedy
hill-climbing search. In contrast, our operators are
applied probabilistically, making them theoretically
well-founded for a variety of inference problems.
Our use of Gibbs sampling follows from its
increasing use in Bayesian inference problems in
NLP (Finkel et al, 2006; Johnson et al, 2007b).
Most closely related is the work of DeNero
et al (2008), who derive a Gibbs sampler for
phrase-based alignment, using it to infer phrase
translation probabilities. The use of Monte Carlo
techniques to calculate posteriors is similar to that
of Chappelier and Rajman (2000) who use those
techniques to find the best parse under models where
the derivation and the parse are not isomorphic.
To our knowledge, we are the first to apply Monte
Carlo methods to maximum translation and mini-
mum risk translation. Approaches to the former
(Blunsom et al, 2008; May and Knight, 2006) rely
on dynamic programming techniques which do not
scale well without heuristic approximations, while
approaches to the latter (Smith and Eisner, 2006;
Zens et al, 2007) use biased k-best approximations.
7 Conclusion
We have described a Gibbs sampler for approxi-
mating two intractable problems in SMT: maximum
translation decoding (and its variant, minimum risk
decoding) and minimum risk training. By using
Monte Carlo techniques we avoid the biases associ-
ated with the more commonly used DP based max-
derivation (or k-best derivation) approximation. In
doing so we provide a further tool to the translation
community that we envision will allow the devel-
opment and analysis of increasing theoretically well
motivated techniques.
Acknowledgments
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001; and by the
EuroMatrix project funded by the European Commission
(6th Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/).
The ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. In Advances in Neu-
ral Information Processing Systems 21, pages 161?
168.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder,
editors. 2009. Proc. of Workshop on Machine Trans-
lations, Athens.
J.-C. Chappelier and M. Rajman. 2000. Monte-Carlo
sampling for NP-hard maximization problems in the
109
framework of weighted parsing. In Natural Language
Processing ? NLP 2000, number 1835 in Lecture Notes
in Artificial Intelligence, pages 106?117. Springer.
J. DeNero, A. Bouchard, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. of EMNLP.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
U. Germann, M. Jahr, K. Knight, D. Marcu, and
K. Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proceedings of ACL.
Association for Computational Linguistics, July.
J. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007a.
Improving translation quality by discarding most of
the phrasetable. In Proc. of EMNLP-CoNLL, Prague.
M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL-HLT, pages 139?
146, Rochester, New York, April.
P. Koehn, F. Och, and D.Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48?
54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demonstration Session, pages 177?180, June.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP, pages 133?139.
J. May and K. Knight. 2006. A better n-best list: Prac-
tical determinization of weighted finite tree automata.
In Proc. of NAACL-HLT.
N. Metropolis and S. Ulam. 1949. The Monte Carlo
method. Journal of the American Statistical Associa-
tion, 44(247):335?341.
F. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Proc. of
COLING, Saarbrucken, Germany, July.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
N. N. Schraudolph, J. Yu, and S. Gu?nter. 2007. A
stochastic quasi-Newton method for online convex
optimization. In Proc. of Artificial Intelligence and
Statistics.
N. N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-
09-99, IDSIA.
K. Sima?an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree grammars. In
Proc. of COLING, Copenhagen.
D. A. Smith and J. Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING-ACL, pages 787?794.
R. Zens and H. Ney. 2007. Efficient phrase-table repre-
sentation for machine translation with applications to
online MT and speech translation. In Proc. of NAACL-
HLT, Rochester, New York.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP, pages 524?532, Prague,
Czech Republic.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proc. of ACL: HLT, pages
97?105, Columbus, Ohio.
110
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1059?1070, Dublin, Ireland, August 23-29 2014.
Automatic Classification of Communicative Functions of Definiteness
Archna Bhatia
?,?
Chu-Cheng Lin
?
Nathan Schneider
?
Yulia Tsvetkov
?
Fatima Talib Al-Raisi
?
Laleh Roostapour
?
Jordan Bender
?
Abhimanu Kumar
?
Lori Levin
?
Mandy Simons
?
Chris Dyer
?
?
Carnegie Mellon University
?
University of Pittsburgh
Pittsburgh, PA 15213 Pittsburgh, PA 15260
?archnab@cs.cmu.edu
Abstract
Definiteness expresses a constellation of semantic, pragmatic, and discourse properties?the
communicative functions?of an NP. We present a supervised classifier for English NPs that
uses lexical, morphological, and syntactic features to predict an NP?s communicative function in
terms of a language-universal classification scheme. Our classifiers establish strong baselines for
future work in this neglected area of computational semantic analysis. In addition, analysis of
the features and learned parameters in the model provides insight into the grammaticalization of
definiteness in English, not all of which is obvious a priori.
1 Introduction
Definiteness is a morphosyntactic property of noun phrases (NPs) associated with semantic and pragmatic
characteristics of entities and their discourse status. Lyons (1999), for example, argues that definite
markers prototypically reflect identifiability (whether a referent for the NP can be identified by the
discourse participants or not); other aspects identified in the literature include uniqueness of the entity
in the world and whether the hearer is already familiar with the entity given the context and preceding
discourse (Roberts, 2003; Abbott, 2006). While some morphosyntactic forms of definiteness are employed
by all languages?namely, demonstratives, personal pronouns, and possessives?languages display a vast
range of variation with respect to the form and meaning of definiteness. For example, while languages
like English make use of definite and indefinite articles to distinguish between the discourse status of
various entities (the car vs. a car vs. cars), many other languages?including Czech, Indonesian, and
Russian?do not have articles (although they do have demonstrative determiners). Sometimes definiteness
is marked with affixes or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in
Chinese (a language without articles), where the existential construction can be used to express indefinite
subjects and the ba- construction can be used to express definite direct objects (Chen, 2004).
Aside from this variation in the form of (in)definite NPs within and across languages, there is also vari-
ability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites
expressing these functions. We refer to these as communicative functions of definiteness, following
Bhatia et al. (2014). Croft (2003, pp. 6?7) shows that even when two languages have access to the
same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite
or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French
translations (both languages use definite as well as indefinite articles) such as:
(1) He showed extreme care. (unmarked)
Il montra un soin extr?me. (indef.)
(2) I love artichokes and asparagus. (unmarked)
J?aime les artichauts et les asperges. (def.)
(3) His brother became a soldier. (indef.)
Son fr?re est devenu soldat. (unmarked)
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
1059
? NONANAPHORA [?A,?B] 999
? UNIQUE [+U] 287
*
UNIQUE_HEARER_OLD [+F,?G,+S] 251
? UNIQUE_PHYSICAL_COPRESENCE [+R] 13
? UNIQUE_LARGER_SITUATION [+R] 237
? UNIQUE_PREDICATIVE_IDENTITY [+P] 1
*
UNIQUE_HEARER_NEW [?F] 36
? NONUNIQUE [?U] 581
*
NONUNIQUE_HEARER_OLD [+F] 169
? NONUNIQUE_PHYSICAL_COPRESENCE [?G,+R,+S] 39
? NONUNIQUE_LARGER_SITUATION [?G,+R,+S] 117
? NONUNIQUE_PREDICATIVE_IDENTITY [+P] 13
*
NONUNIQUE_HEARER_NEW_SPEC [?F,?G,+R,+S] 231
*
NONUNIQUE_NONSPEC [?G,?S] 181
? GENERIC [+G,?R] 131
*
GENERIC_KIND_LEVEL 0
*
GENERIC_INDIVIDUAL_LEVEL 131
? ANAPHORA [+A] 1574
? BASIC_ANAPHORA [?B,+F] 795
*
SAME_HEAD 556
*
DIFFERENT_HEAD 329
? EXTENDED_ANAPHORA [+B] 779
*
BRIDGING_NOMINAL [?G,+R,+S] 43
*
BRIDGING_EVENT [+R,+S] 10
*
BRIDGING_RESTRICTIVE_MODIFIER [?G,+S] 614
*
BRIDGING_SUBTYPE_INSTANCE [?G] 0
*
BRIDGING_OTHER_CONTEXT [+F] 112
? MISCELLANEOUS [?R] 732
? PLEONASTIC [?B,?P] 53
? QUANTIFIED 248
? PREDICATIVE_EQUATIVE_ROLE [?B,+P] 58
? PART_OF_NONCOMPOSITIONAL_MWE 100
? MEASURE_NONREFERENTIAL 125
? OTHER_NONREFERENTIAL 148
+ ? 0 + ? 0 + ? 0 + ? 0
Anaphoric 1574 999 732 Generic 131 1476 1698 Predicative 72 53 3180 Specific 1305 181 1819
Bridging 779 1905 621 Familiar 1327 267 1711 Referential 690 863 1752 Unique 287 581 2437
Figure 1: CFD (Communicative Functions of Definiteness) annotation scheme, with frequencies in the
corpus. Internal (non-leaf) labels are in bold; these are not annotated or predicted. +/? values are shown
for ternary attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique; these are inherited from supercategories, but otherwise default to 0. Thus, for example, the
full attribute specification for UNIQUE_PHYSICAL_COPRESENCE is [?A,?B,+F,?G,0P,+R,+S,+U].
Counts for these attributes are shown in the table at bottom.
A cross-linguistic classification of communicative functions should be able to characterize the aspects
of meaning that account for the different patterns of definiteness marking exhibited in (1?3): e.g., that
(2) concerns a generic class of entities while (3) concerns a role filled by an individual. For more on
communicative functions, see ?2.
This paper develops supervised classifiers to predict communicative function labels for English NPs
using lexical, morphological, and syntactic features. The contribution of our work is in both the output of
the classifiers and the models themselves (features and weights). Each classifier predicts communicative
function labels that capture aspects of discourse-newness, uniqueness, specificity, and so forth. Such
functions are useful in a variety of language processing applications. For example, they should usually be
preserved in translation, even when the grammatical mechanisms for expressing them are different. The
communicative function labels also represent the discourse status of entities, making them relevant for
entity tracking, knowledge base construction, and information extraction.
Our log-linear model is a form-meaning mapping that relates syntactic, lexical, and morphological
features to properties of communicative functions. The learned weights of this model can, e.g., gener-
ate plausible hypotheses regarding the form-meaning relationship which can then be tested rigorously
through controlled experiments. This hypothesis generation is linguistically significant as it indicates new
grammatical mechanisms beyond the obvious a and the articles that are used for expressing definiteness
in English.
To build our models, we leverage a cross-lingual definiteness annotation scheme (?2) and annotated
English corpus (?3) developed in prior work (Bhatia et al., 2014). The classifiers, ?4, are supervised
models with features that combine lexical and morphosyntactic information and the prespecified attributes
or groupings of the communicative function labels (such as Anaphoric, Bridging, Specific in fig. 1) to
predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (?5) include one that
exploits these label groupings to award partial credit according to relatedness. ?6 presents experiments
comparing several models and discussing their strengths and weaknesses; computational work and
applications related to definiteness are addressed in ?7.
1060
2 Annotation scheme
The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoric-
ity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel
et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell,
1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define
it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003)
proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite
descriptions. However, possessive definite descriptions (John?s daughter) and the weak definites (the son
of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they
are spoken. In contrast to the reductionist approaches are approaches to grammaticalization (Hopper and
Traugott, 2003) in which grammar develops over time in such a way that each grammatical construction
has some prototypical communicative functions, but may also have many non-prototypical communica-
tive functions. The scheme we are adopting for this work?the annotation scheme for Communicative
Functions of Definiteness (CFD) as described in Bhatia et al. (2014)?assumes that there may be multiple
functions to definiteness. CFD is based on a combination of these functions and is summarized in fig. 1. It
was developed by annotating texts in two languages (English and Hindi) for four different genres?namely
TED talks, a presidential inaugural speech, news articles, and fictional narratives?keeping in mind the
communicative functions that have been associated with definiteness in the linguistic literature.
CFD is hierarchically organized. This hierarchical organization serves to reduce the number of decisions
that an annotator needs to make for speed and consistency. We now highlight some of the major distinctions
in the hierarchy.
At the highest level, the distinction is made between Anaphora, Nonanaphora, and Miscellaneous
functions of an NP (the annotatable unit). Anaphora and Nonanaphora respectively describe whether
an entity is old or new in the discourse; the Miscellaneous function is mainly assigned to various kinds of
nonreferential NPs.
The Anaphora category has two subcategories: Basic_Anaphora and Extended_Anaphora. Ba-
sic_Anaphora applies to NPs referring to entities that have been mentioned before. Extended_Anaphora
applies to any NP whose referent has not been mentioned itself, but is evoked by a previously mentioned
entity. For example, after mentioning a wedding, the bride, the groom, and the cake are considered to be
Extended_Anaphora.
Within the Nonanaphora category, a first distinction is made between Unique, Nonunique, and
Generic. The Unique function applies to NPs whose referent becomes unique in a context for any of
several reasons. For example, Obama can safely be considered unique in contemporary political discourse
in the United States. The function Nonunique applies to NPs that start out with multiple possible referents
and that may or may not become identifiable in a speech situation. For example, a little riding hood of
red velvet in fig. 2 could be annotated with the label Nonunique. Finally, Generic NPs refer to classes
or types of entities rather than specific entities. For example, Dinosaurs in Dinosaurs are extinct. is a
Generic NP.
Another important distinction CFD makes is between Hearer_Old for references to entities that are
familiar to the hearer (e.g., if they are physically present in the speech situation), versus Hearer_New
for nonfamiliar references. This distinction cuts across the two subparts of the hierarchy, Anaphora
and Nonanaphora; thus, labels marking Hearer_Old or Hearer_New also encode other distinctions
(e.g., Unique_Hearer_Old, Unique_Hearer_New, Nonunique_Hearer_Old). For further details on
the annotation scheme, see fig. 1 and Bhatia et al. (2014).
Because the ordering of distinctions determines the tree structure of the hierarchy, the same commu-
nicative functions could have been organized in a superficially different way. In fact, Komen (2013) has
proposed a hierarchy with similar leaf nodes, but different internal structure. Since it is possible that
some natural groupings of labels are not reflected in the hierarchy we used, we also decompose each
label into fundamental communicative functions, which we call attributes. Each label type is associated
with values for attributes Anaphoric, Bridging, Familiar, Generic, Predicative, Referential, Specific, and
Unique. These attributes can have values of +, ?, or 0, as shown in fig. 1. For instance, with the Anaphoric
1061
Once upon a time there was a dear little girl who was loved by everyone who looked at her, but most of all by her grandmother,
and there was nothing that she would not have given to the child.
Once she
SAME_HEAD
gave her
DIFFERENT_HEAD
a little riding hood of red velvet
OTHER_NONREFERENTIAL
NONUNIQUE_HEARER_NEW_SPEC
, which suited her
SAME_HEAD
so well that
she
SAME_HEAD
would never wear anything else
QUANTIFIED
; so she
SAME_HEAD
was always called ?Little Red Riding Hood
UNIQUE_HEARER_NEW
.?
Figure 2: An annotated sentence from ?Little Red Riding Hood.? The previous sentence is shown for
context.
attribute, a value of + applies to labels that can never mark NPs new to the discourse, ? applies to labels
that can only apply if the NP is new in the discourse, and 0 applies to labels such as Pleonastic (where
anaphoricity is not applicable because there is no discourse referent).
3 Data
We use the English definiteness corpus of Bhatia et al. (2014), which consists of texts from multiple genres
annotated with the scheme described in ?2.
1
The 17 documents consist of prepared speeches (TED talks
and a presidential address), published news articles, and fictional narratives. The TED data predominates
(75% of the corpus);
2
the presidential speech represents about 16%, fictional narratives 5%, and news
articles 4%. All told, the corpus contains 13,860 words (868 sentences), with 3,422 NPs (the annotatable
units). Bhatia et al. (2014) report high inter-annotator agreement, estimating Cohen?s ? = 0.89 within the
TED genre as well as for all genres.
Figure 2 is an excerpt from the ?Little Red Riding Hood? annotated with the CFD scheme.
4 Classification framework
To model the relationship between the grammar of definiteness and its communicative functions in a
data-driven fashion, we work within the supervised framework of feature-rich discriminative classification,
treating the functional categories from ?2 as output labels y and various lexical, morphological, and
syntactic characteristics of the language as features of the input x. Specifically, we learn two kinds
of probabilistic models. The first is a log-linear model similar to multiclass logistic regression, but
deviating in that logistic regression treats each output label (response) as atomic, whereas we decompose
each into attributes based on their linguistic definitions, enabling commonalities between related labels
to be recognized. Each weight in the model corresponds to a feature that mediates between percepts
(characteristics of the input NP) and attributes (characteristics of the label). This is aimed at attaining
better predictive accuracy as well as feature weights that better describe the form?function interactions we
are interested in recovering. We also train a random forest model on the hypothesis that it would allow us
to sacrifice interpretability of the learned parameters for predictive accuracy.
Our setup is formalized below, where we discuss the mathematical models and linguistically motivated
features.
4.1 Models
We experiment with two classification methods: a log-linear model and a nonlinear tree-based ensemble
model. Due to their consistency and interpretability, linear models are a valuable tool for quantifying and
analyzing the effects of individual features. Non-linear models, while less interpretable, often outperform
logistic regression (Perlich et al., 2003), and thus could be desirable when the predictions are needed for a
downstream task.
1
The data can be obtained from http://www.cs.cmu.edu/~ytsvetko/definiteness_corpus.
2
The TED talks are from a large parallel corpus obtained from http://www.ted.com/talks/.
1062
4.1.1 Log-linear model
At test time, we model the probability of communicative function label y conditional on an NP x as
follows:
p
?
(y?x) = log
exp?
?
f(x,y)
?
y
??Y exp?
?
f(x,y?)
(1)
where ? ?Rd is a vector of parameters (feature weights), and f ?X ?Y ?Rd is the feature function over
input?label pairs. The feature function is defined as follows:
f(x,y) = ? (x)? ??(y) (2)
where the percept function ? ?X ?Rc produces a vector of real-valued characteristics of the input, and
the attribute function
?
? ?Y ? {0,1}a encodes characteristics of each label. There is a feature for every
percept?attribute pairing: so d = c ?a and f(i?1)a+ j(x,y) = ?i(x) ?? j(y),1 ? i ? c,1 ? j ? a.
3
The contents of
the percept and attribute functions are detailed in ?4.2 and ?4.3 respectively.
For prediction, having learned weights
?
? we use the Bayes-optimal decision rule for minimizing
misclassification error, selecting the y that maximizes this probability:
y?? argmax
y?Y
p
?
?
(y?x) (3)
Training optimizes
?
? so as to maximize a convex L
2
-regularized
4
learning objective over the training data
D:
?
? = argmax
?
?? ??? ??
2
2
+ ?
?x,y??D
log
exp?
?
f(x,y)
?
y
??Y exp(?
?
f(x,y?))
(4)
With
?
?(y) = the identity of the label, this reduces to standard logistic regression.
4.1.2 Non-linear model
We employ a random forest classifier (Breiman, 2001), an ensemble of decision tree classifiers learned
from many independent subsamples of the training data. Given an input, each tree classifier assigns a
probability to each label; those probabilities are averaged to compute the probability distribution across
the ensemble.
An important property of the random forests, in addition to being an effective tool in prediction, is
their immunity to overfitting: as the number of trees increases, they produce a limiting value of the
generalization error.
5
Thus, no hyperparameter tuning is required. Random forests are known to be
robust to sparse data and to label imbalance (Chen et al., 2004), both of which are challenges with the
definiteness dataset.
4.2 Percepts
The characteristics of the input that are incorporated in the model, which we call percepts to distinguish
them from model features linking inputs to outputs, see ?4.1, are intended to capture the aspects of English
morphosyntax that may be relevant to the communicative functions of definiteness.
After preprocessing the text with a dependency parser and coreference resolver, which is described in
?6.1, we extract several kinds of percepts for each NP.
4.2.1 Basic
Words of interest. These are the head within the NP, all of its dependents, and its governor (external to
the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing
the dependency path upward from the head. For each of these words, we have separate percepts capturing:
the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a
3
Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection.
4
As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are
excluded from regularization.
5
See Theorem 1.2 in Breiman (2001) for details.
1063
binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we
have additional features specific to the first and the last one. Moreover, to better capture tense, aspect
and modality, we collect the attached verb?s auxiliaries. We also make note of the negative particle (with
dependency label neg) if it is a dependent of the verb.
Structural. The structural percepts are: the path length from the head up to the root, and to the attached
verb. We also have percepts for the number of dependents, and the number of dependency relations that
link non-neighbors. Integer values were binarized with thresholding.
Positional. These percepts are the token length of the NP, the NP?s location in the sentence (first or
second half), and the attached verb?s position relative to the head (left or right). 12 additional percept
templates record the POS and lemma of the left and right neighbors of the head, governor, and attached
verb.
4.2.2 Contextual NPs
When extracting features for a given NP (call it the ?target?), we also consider NPs in the following
relationship with the target NP: its immediate parent, which is the smallest NP whose span fully subsumes
that of the target; the immediate child, which is the largest NP subsumed within the target; the immediate
precedent and immediate successor within the sentence; and the nearest preceding coreferent mention.
For each of these related NPs, we include all of their basic percepts conjoined with the nature of the
relation to the target.
4.3 Attributes
As noted above, though CFD labels are organized into a tree hierarchy, there are actually several dimensions
of commonality that suggest different groupings. These attributes are encoded as ternary characteristics;
for each label (including internal labels), every one of the 8 attributes is assigned a value of +, ?, or 0
(refer to fig. 1). In light of sparse data, we design features to exploit these similarities via the attribute
vector function
?(y) = [y,A(y),B(y),F(y),G(y),P(y),R(y),S(y),U(y)]
?
(5)
where A ?Y ? {+,?,0} returns the value for Anaphoric, B(y) for Bridging, etc. The identity of the label
is also included in the vector so that different labels are always recognized as different by the attribute
function. The categorical components of this vector are then binarized to form
?
?(y); however, instead
of a binary component that fires for the 0 value of each ternary attribute, there is a component that fires
for any value of the attribute?a sort of bias term. The weights assigned to features incorporating + or ?
attribute values, then, are easily interpreted as deviations relative to the bias.
5 Evaluation
The following measures are used to evaluate our predictor against the gold standard for the held-out
evaluation (dev or test) set E :
? Exact Match: This accuracy measure gives credit only where the predicted and gold labels are identical.
? By leaf label: We also compute precision and recall of each leaf label to determine which categories
are reliably predicted.
? Soft Match: This accuracy measure gives partial credit where the predicted and gold labels are
related. It is computed as the proportion of attributes-plus-full-label whose (categorical) values match:
??(y)??(y?)?/9.
6 Experiments
6.1 Experimental Setup
Data splits. The annotated corpus of Bhatia et al. (2014) (?3) contains 17 documents in 3 genres:
13 prepared speeches (mostly TED talks),
6
2 newspaper articles, and 2 fictional narratives. We arbitrarily
choose some documents to hold out from each genre; the resulting test set consists of 2 TED talks
6
We have combined the TED talks and presidential speech genres since both involved prepared speeches.
1064
Condition ?? ? ? Exact Match Acc. Soft Match Acc.
Majority baseline ? ? 12.1 47.8
Log-linear classifier, attributes only 473,064 100 38.7 77.1
Log-linear classifier, labels only 413,931 100 40.8 73.6
Full log-linear classifier (labels + attributes) 926,417 100 43.7 78.2
Random forest classifier 20,363 ? 49.7 77.5
Table 1: Classifiers and baseline, as measured on the test set. The first two columns give the number of
parameters and the tuned regularization hyperparameter, respectively; the third and fourth columns give
accuracies as percentages. The best in each column is bolded.
(?Alisa_News?, ?RobertHammond_park?), 1 newspaper article (?crime1_iPad_E?), and 1 narrative
(?Little Red Riding Hood?). The test set then contains 19,28 tokens (111 sentences), in which there are
511 annotated NPs; while the training set contains 2,911 NPs among 11,932 tokens (757 sentences).
Preprocessing. Automatic dependency parses and coreference information were obtained with the
parser and coreference resolution system in Stanford CoreNLP v. 3.3.0 (Socher et al., 2013; Recasens
et al., 2013) for use in features (?4.2). Syntactic features were extracted from the Basic dependencies
output by the parser. To evaluate the performance of Stanford system on our data, we manually inspected
the dependencies and coreference information for a subset of sentences from our corpus (using texts
from TED talks and fictional narratives genres) and recorded the errors. We found that about 70% of the
sentences had all correct dependencies, and only about 0.04% of the total dependencies were incorrect
for our data. However, only 62.5% of the coreference links were correctly identified by the coreference
resolver. The rest of them were either missing or incorrectly identified. We believe this may have caused a
portion of the classifier errors while predicting the Ananphoic labels.
Throughout our experiments (training as well as testing), we use the gold NP boundaries identified by
the human annotators. The automatic dependency parses are used to extract percepts for each gold NP.
If there is a conflict between the gold NP boundaries and the parsed NP boundaries, to avoid extracting
misleading percepts, we assign a default value.
Learning. The log-linear model variants are trained with an in-house implementation of supervised
learning with L
2
-regularized AdaGrad (Duchi et al., 2011). Hyperparameters are tuned on a development
set formed by holding out every tenth instance from the training set (test set experiments use the full
training set): the power of 10 giving the highest Soft Match accuracy was chosen for ? .
7
The Python
scikit-learn toolkit (Pedregosa et al., 2011) was used for the random forest classifier.8
6.2 Results
Measurements of overall classification performance appear in table 1. While far from perfect, our
classifiers achieve promising accuracy levels given the small size of the training data and the number of
labels in the annotation scheme. The random forest classifier is the most accurate in Exact Match, likely
due to the robustness of that technique under conditions where the data are small and the frequencies
of individual labels are imbalanced. By the Soft Match measure, our attribute-aware log-linear models
perform very well. The most successful of the log-linear models is the richest model, which combines the
fine-grained communicative function labels with higher-level attributes of those labels. But notably the
attribute-only model, which decomposes the semantic labels into attributes without directly considering
the full label, performs almost as well as the random forest classifier in Soft Match. This is encouraging
because it suggests that the model has correctly exploited known linguistic generalizations to account for
the grammaticalization of definiteness in English.
Table 2 reports the precision and recall of each leaf label predicted. Certain leaf labels are found
to be easier for the classifier to predict: e.g., the communicative function label Pleonastic has a high
F
1
score. This is expected as the Ploenastic CFD for English is quite regular and captured by the EX
7
Preliminary experiments with cross-validation on the training data showed that the value of ? was stable across folds.
8
Because it is a randomized algorithm, the results may vary slightly between runs; however, a cross-validation experiment on
the training data found very little variance in accuracy.
1065
Leaf label N P R F
1
Leaf label N P R F
1
Pleonastic 44 100 78 88 Part_of_Noncompositional_MWE 88 20 17 18
Bridging_Restrictive_Modifier 552 58 84 68 Bridging_Nominal 33 33 10 15
Quantified 213 57 57 57 Generic_Individual_Level 113 14 11 13
Unique_Larger_Situation 97 52 58 55 Nonunique_Nonspec 173 9 25 13
Same_Head 452 41 41 41 Bridging_Other_Context 96 33 6 11
Measure_Nonreferential 98 88 26 40 Bridging_Event 9 ? 0 ?
Nonunique_Hearer_New_Spec 190 36 46 40 Nonunique_Physical_Copresence 36 0 0 ?
Other_Nonreferential 134 39 36 37 Nonunique_Predicative_Identity 10 ? 0 ?
Different_Head 271 32 33 32 Predicative_Nonidentity 57 0 0 ?
Nonunique_Larger_Situation 97 29 25 27 Unique_Hearer_New 26 ? 0 ?
Table 2: Number of training set instances and precision, recall, and F
1
percentages for leaf labels.
part-of-speech tag. The classifier finds predictions of certain CFD labels, such as Bridging_Event,
Bridging_Nominal and Nonunique_Nonspecific, to be more difficult due to data sparseness: it appears
that there were not enough training instances for the classifier to learn the generalizations corresponding
to these CFDs. Bridging_Other_Context was hard to predict as this was a category which referred not
to the entities previously mentioned but to the whole speech event from the past. There seem to be no
clear morphosyntactic cues associated with this CFD, so to train a classifier to predict this category label,
we would need to model more complex semantic and discourse information. This also applies to the
classifier confusion between the Same_Head and Different_Head, since both of these labels share all
the semantic attributes used in this study.
An advantage of log-linear models is that inspecting the learned feature weights can provide useful
insights into the model?s behavior. Figure 3 lists 10 features that received the highest positive weights
in the full model for the + and ? values of the Specific attribute. These confirm some known properties
of English definites and indefinites. The definite article, possessives (PRP$), proper nouns (NNP), and the
second person pronoun are all associated with specific NPs, while the indefinite article is associated with
nonspecific NPs. The model also seems to have picked up on the less obvious but well-attested tendency
of objects to be nonspecific (Aissen, 2003).
In addition to confirming known grammaticalization patterns of definiteness, we can mine the highly-
weighted features for new hypotheses: e.g., in figs. 3 and 4, the model thinks that objects of ?from? are
especially likely to be Specific, and that NPs with comparative adjectives (JJR) are especially likely to be
nonspecific (fig. 3). From fig. 3, we also know that Num. of dependents, dependent?s POS: 1,PRP$ has
a higher weight than, say, Num. of dependents, dependent?s POS: 2,PRP$. This observation suggests a
hypothesis that in English the NPs which have possessive pronouns immediately preceding the head are
more likely to be specific than the NPs which have intervening words between the possessive pronoun
and the head. Similarly, looking at another example in fig. 4, the following two percepts get high weights
for the NP the United States of America to be Specific: last dependent?s POS: NNP and first dependent?s
lemma: the. Since frequency and other factors affect the feature weights learned by the classifier, these
differences in weights may or may not reflect an inherent association with Specificity. Whether these
are general trends, or just an artifact of the sentences that happened to be in the training data and our
statistical learning procedure, will require further investigation, ideally with additional datasets and more
rigorous hypothesis testing.
Finally, we can remove features to test their impact on predictive performance. Notably, in experiments
ablating features indicating articles?the most obvious exponents of definiteness in English?we see
a decrease in performance, but not a drastic one. This suggests that the expression of communicative
functions of definiteness is in fact much richer than morphological definiteness.
Errors. Several labels are unattested or virtually unattested in the training data, so the models unsurpris-
ingly fail to predict them correctly at test time. Same_Head and Different_Head, though both common,
are confused quite frequently. Whether the previous coreferent mention has the same or different head is a
simple distinction for humans; low model accuracy is likely due to errors propagated from coreference
resolution. This problem is so frequent that merging these two categories and retraining the random
forest model improves Exact Match accuracy by 8% absolute and Soft Match accuracy by 5% absolute.
1066
Percepts
+Specific ?Specific
First dependent?s POS PRP$ First dependent?s lemma a
Head?s left neighbor?s POS PRP$ Last dependent?s lemma a
Last dependent?s lemma you Num. of dependents, dependent?s lemma 1,a
Num. of dependents, dependent?s lemma 1,you Head?s left neighbor?s POS JJR
Num. of dependents, dependent?s POS 1,PRP$ Last dependent?s POS JJR
Governor?s right neighbor?s POS PRP$ Num. of dependents, dependent?s lemma 2,a
Last dependent?s POS NNP First dependent?s lemma new
Last dependent?s POS PRP$ Last dependent?s lemma new
First dependent?s lemma the Num. of dependents, dependent?s POS 2,JJR
Governor?s lemma from Governor?s left neighbor?s POS VB
Figure 3: Percepts receiving highest positive weights in association with values of the Specific attribute.
Example Relevant percepts from fig. 3 CFD annotation
This is just for the United States of America. Last dependent?s POS: NNP
First dependent?s lemma: the
Unique_Larger_Situation
We were driving from our home in Nashville
to a little farm we have 50 miles east of
Nashville ? driving ourselves.
First dependent?s POS: PRP$
Head?s left neighbor?s POS: PRP$
Governor?s right neighbor?s POS: PRP$
Governor?s lemma: from
Bridging_Restrictive_Modifier
Figure 4: Sentences from our corpus illustrating percepts fired for gold NPs and their CFD annotations.
Another common confusion is between the highly frequent category Unique_Larger_Situation and the
rarer category Unique_Hearer_New; the latter is supposed to occur only for the first occurrence of a
proper name referring to a entity that is not already part of the knowledge of the larger community. In
other words, this distinction requires world knowledge about well-known entities, which could perhaps be
mined from the Web or other sources.
7 Related Work
Because semantic/pragmatic analysis of referring expressions is important for many NLP tasks, a compu-
tational model of the communicative functions of definiteness has the potential to leverage diverse lexical
and grammatical cues to facilitate deeper inferences about the meaning of linguistic input. We have used
a coreference resolution system to extract features for modeling definiteness, but an alternative would be
to predict definiteness functions as input to (or jointly with) the coreference task. Applications such as
information extraction and dialogue processing could be expected to benefit not only from coreference
information, but also from some of the semantic distinctions made in our framework, including specificity
and genericity.
Better computational processing of definiteness in different languages stands to help machine translation
systems. It has been noted that machine translation systems face problems when the source and the target
language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov
et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either
(a) preprocessing the source language to make it look more like the target language (Collins et al., 2005;
Habash, 2007; Nie?en and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine
translation output to match the target language, (e.g., Popovi
?
c et al., 2006). Attempts have also been made
to use syntax on the source and/or the target sides to capture the syntactic differences between languages
(Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite
articles has been found beneficial in a variety of applications, including postediting of MT output (Knight
and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction
of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013)
trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and
used this classifier to improve the quality of statistical machine translation.
While definiteness morpheme prediction has been thoroughly studied in computational linguistics,
1067
studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit
linguistically-motivated features in a supervised approach to distinguish between generic and specific
NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve
the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong
et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been
conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness
more broadly.
Our work is related to research in linguistics on the modeling of syntactic constructions such as dative
shift and the expression of possession with ?of? or ??s?. Bresnan and Ford (2010) used logistic regression
with semantic features to predict syntactic constructions. Although we are doing the opposite (using
syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as
mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper
and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in
multiple communicative functions for each grammatical construction. Other attempts have also been made
to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have
been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation,
Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by
prepositions.
8 Conclusion
We have presented a data-driven approach to modeling the relationship between universal communicative
functions associated with (in)definiteness and their lexical/grammatical realization in a particular language.
Our feature-rich classifiers can give insights into this relationship as well as predict communicative
functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear
classifier compares favorably to the random forest classifier in Soft Match accuracy. Further improvements
to the classifier may come from additional features or better preprocessing. This work has focused on
English, but in future work we plan to build similar models for other languages?including languages
without articles, under the hypothesis that such languages will rely on other, subtler devices to encode
many of the functions of definiteness.
Acknowledgments
This work was sponsored by the U. S. Army Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533. We thank the reviewers for their useful comments.
References
Barbara Abbott. 2006. Definite and indefinite. In Keith Brown, editor, Encyclopedia of Language and Linguistics,
pages 3?392. Elsevier.
Judith Aissen. 2003. Differential object marking: iconicity vs. economy. Natural Language & Linguistic Theory,
21(3):435?483.
Archna Bhatia, Mandy Simons, Lori Levin, Yulia Tsvetkov, Chris Dyer, and Jordan Bender. 2014. A unified anno-
tation scheme for the semantic/pragmatic components of definiteness. In Proc. of LREC. Reykjav?k, Iceland.
Betty Birner and Gregory Ward. 1994. Uniqueness, familiarity and the definite article in English. In Proc. of the
Twentieth Annual Meeting of the Berkeley Linguistics Society, pages 93?102.
Leo Breiman. 2001. Random forests. Machine Learning, 45(1):5?32.
Joan Bresnan and Marilyn Ford. 2010. Predicting syntax: Processing dative constructions in American and Aus-
tralian varieties of English. Language, 86(1):168?213.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer. 2013. Translating into morphologically rich
languages with synthetic phrases. In Proc. of EMNLP, pages 1677?1687. Seattle, Washington, USA.
1068
Chao Chen, Andy Liaw, and Leo Breiman. 2004. Using random forest to learn imbalanced data. University of
California, Berkeley.
Ping Chen. 2004. Identifiability and definiteness in Chinese. Linguistics, 42:1129?1184.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine translation.
In Proc. of ACL, pages 531?540. Ann Arbor, Michigan.
Cleo Condoravdi. 1992. Strong and weak novelty and familiarity. In Proc. of SALT II, pages 17?37.
William Croft. 2003. Typology and Universals. Cambridge University Press.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12(Jul):2121?2159.
Michael Elhadad. 1993. Generating argumentative judgment determiners. In Proc. of AAAI, pages 344?349.
Gareth Evans. 1977. Pronouns, quantifiers and relative clauses. Canadian Journal of Philosophy, 7(3):46.
Gareth Evans. 1980. Pronouns. Linguistic Inquiry, 11.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1988. The generation and interpretation of demonstrative
expressions. In Proc. of XIIth International Conference on Computational Linguistics, pages 216?221.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski. 1993. Cognitive status and the form of referring expres-
sions in discourse. Language, 69:274?307.
Nizar Habash. 2007. Syntactic preprocessing for statistical machine translation. In MT Summit XI, pages 215?222.
Copenhagen.
Na-Rae Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting errors in english article usage by non-native
speakers. Natural Language Engineering, 12:115?129.
Irene Heim. 1990. E-type pronouns and donkey anaphora. Linguistics and Philosophy, 13:137?177.
Iris Hendrickx, Orph?e De Clercq, and V?ronique Hoste. 2011. Analysis and reference resolution of bridge
anaphora across different text genres. In Iris Hendrickx, Sobha Lalitha Devi, Antonio Horta Branco, and Ruslan
Mitkov, editors, DAARC, volume 7099 of Lecture Notes in Computer Science, pages 1?11. Springer.
Paul J. Hopper and Elizabeth Closs Traugott. 2003. Grammaticalization. Cambridge University Press.
Nirit Kadmon. 1987. On unique and non-unique reference and asymmetric quantification. Ph.D. thesis, University
of Massachusetts.
Nirit Kadmon. 1990. Uniqueness. Linguistics and Philosophy, 13:273?324.
Kevin Knight and Ishwar Chander. 1994. Automated postediting of documents. In Proc. of the National Conference
on Artificial Intelligence, pages 779?779. Seattle, WA.
Erwin Ronald Komen. 2013. Finding focus: a study of the historical development of focus in English. LOT,
Utrecht.
Fang Kong, Guodong Zhou, Longhua Qian, and Qiaoming Zhu. 2010. Dependency-driven anaphoricity determi-
nation for coreference resolution. In Proc. of COLING, pages 599?607. Beijing, China.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proc. of COLING/ACL, pages 609?616. Sydney, Australia.
Christopher Lyons. 1999. Definiteness. Cambridge University Press.
Guido Minnen, Francis Bond, and Ann Copestake. 2000. Memory-based learning for article generation. In Proc. of
1069
the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language
Learning, pages 43?48.
Vincent Ng and Claire Cardie. 2002. Identifying anaphoric and non-anaphoric noun phrases to improve coreference
resolution. In Proc. of COLING. Taipei, Taiwan.
Sonja Nie?en and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In Proc. of
COLING, pages 1081?1085.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Math-
ieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cour-
napeau, Matthieu Brucher, M. Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python.
Journal of Machine Learning Research, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Simonoff. 2003. Tree induction vs. logistic regression: a learning-
curve analysis. Journal of Machine Learning Research, 4:211?255.
Maja Popovi?c, Daniel Stein, and Hermann Ney. 2006. Statistical machine translation of German compound words.
In Advances in Natural Language Processing, pages 616?624. Springer.
Vinodkumar Prabhakaran, Michael Bloodgood, Mona Diab, Bonnie Dorr, Lori Levin, Christine D. Piatko, Owen
Rambow, and Benjamin Van Durme. 2012. Statistical modality tagging from rule-based annotations and crowd-
sourcing. In Proc. of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,
ExProM ?12, pages 57?64.
Ellen F. Prince. 1992. The ZPG letter: Subjects, definiteness and information status. In S. Thompson and W. Mann,
editors, Discourse description: diverse analyses of a fund raising text, pages 295?325. John Benjamins.
Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse
entities: identifying singleton mentions. In Proc. of NAACL-HLT, pages 627?633. Atlanta, Georgia, USA.
Roi Reichart and Ari Rappoport. 2010. Tense sense disambiguation: A new syntactic polysemy task. In Proc. of
EMNLP, EMNLP ?10, pages 325?334.
Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proc. of ACL, pages 40?49. Uppsala,
Sweden.
Craig Roberts. 2003. Uniqueness in definite noun phrases. Linguistics and Philosophy, 26:287?350.
Alla Rozovskaya and Dan Roth. 2010. Training paradigms for correcting errors in grammar and usage. In Proc.
of NAACL-HLT, pages 154?162.
Bertrand Russell. 1905. On denoting. Mind, New Series, 14:479?493.
Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector
grammars. In Proc. of ACL, pages 455?465. Sofia, Bulgaria.
Vivek Srikumar and Dan Roth. 2013. An inventory of preposition relations. CoRR, abs/1305.5785.
Sara Stymne. 2009. Definite noun phrases in statistical machine translation into Danish. In Proc. of Workshop on
Extracting and Using Constructions in NLP, pages 4?9.
Yulia Tsvetkov, Chris Dyer, Lori Levi, and Archna Bhatia. 2013. Generating English determiners in phrase-based
translation with synthetic translation options. In Proc. of WMT.
Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proc. of ACL, pages 303?310.
Philadelphia, Pennsylvania, USA.
Yuqi Zhang, Richard Zens, and Hermann Ney. 2007. Improved chunk-level reordering for statistical machine
translation. In IWSLT 2007: International Workshop on Spoken Language Translation, pages 21?28.
1070
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 534?544,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Discriminative Word Alignment with a Function Word Reordering Model
Hendra Setiawan
UMIACS
University of Maryland
hendra@umiacs.umd.edu
Chris Dyer
Language Technologies Institute
Carnegie Mellon University
cdyer@cs.cmu.edu
Philip Resnik
Linguistics and UMIACS
University of Maryland
resnik@umd.edu
Abstract
We address the modeling, parameter estima-
tion and search challenges that arise from the
introduction of reordering models that capture
non-local reordering in alignment modeling.
In particular, we introduce several reordering
models that utilize (pairs of) function words
as contexts for alignment reordering. To ad-
dress the parameter estimation challenge, we
propose to estimate these reordering models
from a relatively small amount of manually-
aligned corpora. To address the search chal-
lenge, we devise an iterative local search al-
gorithm that stochastically explores reorder-
ing possibilities. By capturing non-local re-
ordering phenomena, our proposed alignment
model bears a closer resemblance to state-
of-the-art translation model. Empirical re-
sults show significant improvements in align-
ment quality as well as in translation perfor-
mance over baselines in a large-scale Chinese-
English translation task.
1 Introduction
In many Statistical Machine Translation (SMT) sys-
tems, alignment represents an important piece of in-
formation, from which translation rules are learnt.
However, while translation models have evolved
from word-based to syntax-based modeling, the de
facto alignment model remains word-based (Brown
et al, 1993; Vogel et al, 1996). This gap be-
tween alignment modeling and translation modeling
is clearly undesirable as it often generates tensions
that would prevent the extraction of many useful
translation rules (DeNero and Klein, 2007). Recent
work, e.g. by Blunsom et al (2009) and Haghihi et
al. (2009) just to name a few, show that alignment
models that bear closer resemblance to state-of-the-
art translation model consistently yields not only a
better alignment quality but also an improved trans-
lation quality.
In this paper, we follow this recent effort to nar-
row the gap between alignment model and trans-
lation model to improve translation quality. More
concretely, we focus on the reordering component
since we observe that the treatment of reordering re-
mains significantly different when comparing align-
ment versus translation: the reordering component
in state-of-the-art translation models has focused
on long-distance reordering, but its counterpart in
alignment models has remained focused on local
reordering, typically modeling distortion based en-
tirely on positional information. This leaves most
alignment decisions to association-based scores.
Why is employing stronger reordering models
more challenging in alignment than in translation?
One answer can be attributed to the fact that align-
ment points are unobserved in parallel text, thus so
are their reorderings. As such, introducing stronger
reordering often further exacerbates the computa-
tional complexity to do inference over the model.
Some recent alignment models appeal to external
linguistic knowledge, mostly by using monolingual
syntactic parses (Cherry and Lin, 2006; Pauls et al,
2010), which at the same time, provides an approx-
imation of the bilingual syntactic divergences that
drive the reordering. To our knowledge, however,
this approach has been used mainly to constrain re-
ordering possibilities, or to add to the generalization
ability of association-based scores, not to directly
model reordering in the context of alignment.
534
In this paper, we introduce a new approach to im-
proving the modeling of reordering in alignment. In-
stead of relying on monolingual parses, we condi-
tion our reordering model on the behavior of func-
tion words and the phrases that surround them.
Function words are the ?syntactic glue? of sen-
tences, and in fact many syntacticians believe that
functional categories, as opposed to substantive cat-
egories like noun and verb, are primarily responsi-
ble for cross-language syntactic variation (Ouhalla,
1991). Our reordering model can be seen as offering
a reasonable approximation to more fully elaborated
bilingual syntactic modeling, and this approxima-
tion is also highly practical, as it demands no exter-
nal knowledge (other than a list of function words)
and avoids the practical issues associated with the
use of monolingual parses, e.g. whether the mono-
lingual parser is robust enough to produce reliable
output for every sentence in training data.
At a glance, our reordering model enumerates
the function words on both source and target sides,
modeling their reordering relative to their neighbor-
ing phrases, their neighboring function words, and
the sentence boundaries. Because the frequency of
function words is high, we find that by predicting the
reordering of function words accurately, the reorder-
ing of the remaining words improves in accuracy as
well. In total, we introduce six sub-models involving
function words, and these serve as features in a log
linear model. We train model weights discrimina-
tively using Minimum Error Rate Training (MERT)
(Och, 2003), optimizing F-measure.
The parameters of our sub-models are estimated
from manually-aligned corpora, leading the reorder-
ing model more directly toward reproducing human
alignments, rather than maximizing the likelihood
of unaligned training data. This use of manual data
for parameter estimation is a reasonable choice be-
cause these models depend on a small, fixed number
of lexical items that occur frequently in language,
hence only small training corpora are required. In
addition, the availability of manually-aligned cor-
pora has been growing steadily.
The remainder of the paper proceeds as follows.
In Section 2, we provide empirical motivation for
our approach. In Section 3, we discuss six sub-
models based on function word relationships and
how their parameters are estimated; these are com-
????
?
??
?
?
?
???
one
of
few
that
have
dipl. rels.
with
countries
Australia
is
?
??
North Korea
the
1
1 2 3 4 5 6 7 8 9 10 11
2
3
4
5
6
7
8
9
10
11
12
Figure 1: An aligned Chinese-English sentence pair.
bined with additional features in Section 4 to pro-
duce a single discriminative alignment model. Sec-
tion 5 describes a simple decoding algorithm to find
the most probable alignment under the combined
model, Section 6 describes the training of our dis-
criminative model and Section 7 presents experi-
mental results for the model using this algorithm.
We wrap up in Sections 8 and 9 with a discussion
of related work and a summary of our conclusions.
2 Empirical Motivation
Fig. 1 shows an example of a Chinese-English sen-
tence pair together with correct alignment points.
Predicting the alignment for this particular Chinese-
English sentence pair is challenging, since the sig-
nificantly different syntactic structures of these two
languages lead to non-monotone reordering. For ex-
ample, an accurate alignment model should account
for the fact that prepositional phrases in Chinese ap-
pear in a different order than in English, as illus-
trated by the movement of the phrase ????/with
North Korea? from the beginning of the Chinese
noun phrase to the end of the corresponding English.
The central question that concerns us here is how
to define and infer regularities that can be useful
to predict alignment reorderings. The approach we
take here is supported by empirical results from a
pilot study, conducted as an inquiry into the idea of
focusing on function words to model alignment re-
ordering, which we briefly describe.
We took a Chinese-English manually-aligned cor-
pus of approximately 21 thousand sentence pairs,
535
?
??
?
?
?
?
?
? ?
one
of
few
that
have
dipl. rels.
with
countries
Australia
is
?
?
North Korea
the
1
1 2 3 4 5 6 7 8 9 10 11
2
3
4
5
6
7
8
9
10
11
12
Figure 2: The all-monotone phrase pairs, indicated as
rectangular areas in bold, that can be extracted from the
Fig. 1 example.
and divided each sentence pair into all-monotone
phrase pairs. Visually, an all-monotone phrase pair
corresponds to a maximal block in the alignment
matrix for which internal alignment points appear
in monotone order from the top-left corner to the
bottom-right corner. Fig. 2 illustrates seven such
pairs that can be extracted from the example in
Fig. 1. In total, there are 154,517 such phrase pairs
in our manually-aligned corpus.
The alignment configuration internal to all-
monotone phrase pair blocks is, obviously, mono-
tonic, which is a configuration that is effectively
modeled by traditional alignments models. On the
other hand, the reordering between two adjacent
blocks is the focus of our efforts since existing mod-
els are less effective at modeling non-monotonic
alignment configurations. To measure the function
words? potential to predict non-monotone reorder-
ings, we examined the border words where two ad-
jacent blocks meet. In particular, we are interested
in how many adjacent blocks whose border words
are function words.
The results of this pilot study were quite encour-
aging. If we consider only the Chinese side of the
phrase pairs, 88.35% adjacent blocks have function
words as their boundary words. If we consider only
the English side, function words appear at the bor-
ders of 93.91% adjacent blocks. If we consider
both the Chinese and English sides, the percentage
increases to 95.53%. Notice that in Fig. 2, func-
tion words appear at the borders of all adjacent all-
monotone phrase pairs, if both Chinese and English
sides are considered. Clearly with such high cov-
erage, function words are central in predicting non-
monotone reordering in alignment.
3 Reordering with Function Words
The reordering models we describe follow our previ-
ous work using function word models for translation
(Setiawan et al, 2007; Setiawan et al, 2009). The
core hypothesis in this work is that function words
provide robust clues to the reordering patterns of the
phrases surrounding them. To make this insight use-
ful for alignment, we develop features that score the
alignment configuration of the neighboring phrases
of a function word (which functions as an anchor)
using two kinds of information: 1) the relative order-
ing of the phrases with respect to the function word
anchor; and 2) the span of the phrases. This sec-
tion provides a high level overview of our reordering
model, which attempts to leverage this information.
To facilitate subsequent discussions, we introduce
the notion of monolingual function word phrase
FWi, which consists of the tuple (Yi, Li, Ri), where
Yi is the i-th function word and Li,Ri are its left and
right neighboring phrases, respectively. Note that
this notion of ?phrase? is defined only for reorder-
ing purposes in our model, and does not necessar-
ily correspond to a linguistic phrase. We define
such phrases on both sides to cover as many non-
monotone reorderings as possible, as suggested by
the pilot study. To denote the side, we append a sub-
script: FWi,S = (Yi,S , Li,S , Ri,S) refers to a func-
tion word phrase on the source side, and FWi,T =
(Yi,T , Li,T , Ri,T ) to one on the target side. In our
subsequent discussion, we will mainly use FWi,S ,
and we will omit subscripts S or T if they are clear
from context.
The primary objective of our reordering model
is to predict the projection of monolingual func-
tion word phrases from one language to the
other, inferring bilingual function word phrase pairs
FWi,S?T = (Yi,S?T , Li,S?T , Ri,S?T ), which en-
code the two aforementioned pieces of informa-
tion.1 To infer these phrases, we take a probabilis-
1The subscript S ? T denotes the projection direction from
source to target. The subscript for the other direction is T ? S.
536
tic approach. For instance, to estimate the spans of
Li,S?T , Ri,S?T , our reordering model assumes that
any span to the left of Yi,S is a possible Li,S and
any span to the right of Yi,S is a possible Ri,S , de-
ciding which is most probable via features, rather
than committing to particular spans (e.g. as defined
by a monolingual text chunker or parser). We only
enforce one criterion on Li,S?T and Ri,S?T : they
have to be the maximal alignment blocks satisfying
the consistent heuristic (Och and Ney, 2004) that end
or start with Yi,S?T on the source S side respec-
tively.2
To infer these phrases, we decompose Li,S?T
into (o(Li,S?T ), d(FWi?1,S?T ), b(?s?)); sim-
ilarly, Ri,S?T into (o(Ri,S?T ),d(FWi+1,S?T ),
b(?/s?) )). Taking the decomposition of Li,S?T as
a case in point, here o(Li,S?T ) describes the re-
ordering of the left neighbor Li,S?T with respect
to the function word Yi,S?T , while d(FWi?1,S?T )
and b(?s?)) probe the span of Li,S?T , i.e. whether
it goes beyond the preceding function word phrase
pairs FWi?1,S?T and up to the beginning-of-
sentence marker ?s? respectively. The same defini-
tion applies to the decomposition of Ri,S?T , where
FWi+1,S?T is the succeeding function word phrase
pair and ?/s? is the end-of-sentence marker.
3.1 Six (Sub-)Models
To model o(Li,S?T ), o(Ri,S?T ), i.e. the re-
ordering of the neighboring phrases of a func-
tion word, we employ the orientation model in-
troduced by Setiawan et al (2007). Formally,
this model takes the form of probability distribution
Pori(o(Li,S?T ), o(Ri,S?T )|Yi,S?T ), which condi-
tions the reordering on the lexical identity of the
function word alignment (but independent of the lex-
ical identity of its neighboring phrases). In particu-
lar, o maps the reordering into one of the following
four orientation values (borrowed from Nagata et al
(2006)) with respect to the function word: Mono-
tone Adjacent (MA), Monotone Gap (MG), Reverse
Adjacent (RA) and Reverse Gap (RG). The Mono-
tone/Reverse distinction indicates whether the pro-
jected order follows the original order, while the
Adjacent/Gap distinction indicates whether the pro-
2This heuristic is commonly used in learning phrase pairs
from parallel text. The maximality ensures the uniqueness of L
and R.
jections of the function word and the neighboring
phrase are adjacent or separated by an intervening
phrase.
To model d(FWi?1,S?T ), d(FWi+1,S?T ), i.e.
whether Li,S?T and Ri,S?T extend beyond the
neighboring function word phrase pairs, we uti-
lize the pairwise dominance model of Setiawan
et al (2009). Taking d(FWi?1,S?T ) as
a case in point, this model takes the form
Pdom(d(FWi?1,S?T )|Yi?1,S?T , Yi,S?T ), where d
takes one of the following four dominance val-
ues: leftFirst, rightFirst, dontCare, or neither.
We will detail the exact formulation of these val-
ues in the next subsection. However, to provide
intuition, the value of either leftFirst or neither
for d(FWi?1,S?T ) would suggest that the span of
Li,S?T doesn?t extend to Yi?1,S?T ; the further dis-
tinction between leftFirst and neither concerns with
whether the span of Ri?1,S?T extends to FWi,S?T .
To model b(?s?), b(?/s?), i.e. whether the span of
Li,S?T and Ri,S?T extends up to sentence mark-
ers, we introduce the borderwise dominance model.
Formally, this model is similar to the pairwise domi-
nance model, except that we use the sentence bound-
aries as the anchors instead of the neighboring
phrase pairs. This model captures longer distance
dependencies compared to the previous two mod-
els; in the Chinese-English case, in particular, it is
useful to discourage word alignments from crossing
clause or sentence boundaries. The sentence bound-
ary issue is especially important in machine trans-
lation (MT) experimentation, since the Chinese side
of English-Chinese parallel text often includes long
sentences that are composed of several independent
clauses joined together; in such cases, words from
one clause should be discouraged from aligning to
words from other clauses. In Fig. 1, this model is
potentially useful to discourage words from cross-
ing the copula ??/is?.
We define each model for all (pairs of) function
word phrase pairs, forming features over a set of
word alignments (A) between source (S) and target
537
(T ) sentence pair, as follows:
fori =
N
?
i=1
Pori(o(Li), o(Ri)|Yi) (1)
fdom =
N
?
i=2
Pdom(d(FWi?1)|Yi?1, Yi) (2)
fbdom =
N
?
i=1
Pbdom(b (?s?)|?s?, Yi) ?
Pbdom(b (?/s?)|Yi, ?/s?) (3)
where N is the number of function words (of the
source side, in the S ? T case). As the bilingual
function word phrase pairs are uni-directional, we
employ these three models in both directions, i.e.
T ? S as well as S ? T . As a result, there are
six reordering models based on function words.
3.2 Prediction and Parameter Estimation
Given FWi?1,S?T (and all other FW?i?/i,S?T ),
our reordering model has to decompose Li,S?T into
(o(Li,S?T ), d(FWi?1,S?T ), b(?s?)); and Ri,S?T
into (o(Ri,S?T ),d(FWi+1,S?T ), b(?/s?) )) during
prediction and parameter estimation. In prediction
mode (described in Section 5), it has to make the de-
composition on the current state of alignment, while
during parameter estimation, it has to make the
same decomposition on the manually-aligned cor-
pora. Since the process is identical, we proceed with
the discussion in the context of parameter estima-
tion, where the decomposition is performed to col-
lect counts to estimate the parameters of our models.
Orientation model. Using Li,S?T as a case in
point and given (Yi,S?T =sll/tmm, Li,S?T =sl2l1/tm2m1 ,
Ri,S?T =s
l4
l3
/tm4m3)
3
, the value of o(Li,S?T ) in terms
of Monotone/Reverse is:
Monotone/Reverse =
{
M, m2 < m,
R, m < m1.
(4)
while its value in terms of Adjacent/Gap values is:
Adjacent/Gap =
{
A, |m ? m1| ? |m ? m2| = 1,
G, otherwise.
(5)
3We use subscripts to indicate the starting index, and super-
scripts the ending index.
By adjusting the indices, the computation of
o(Ri,S?T ) follows similarly to the procedure above.
Suppose we want to estimate the probability of
Li,S?T =MA for a particular Yi. Note that here, we
are interested in the lexical identity of Yi, thus the
index i is irrelevant. We first gather the counts of the
orientation value for all Li,S?T of Yi in the corpus:
c(o(Li,S?T ) ? {MA, RA, MG, RG}, Yi). Then
Pori(MA|Yi) is estimated as follows:
Pori(MA|Yi) =
c(MA, Yi)
c(Yi)
(6)
where c(Yi) is the frequency of Yi in the corpus. The
estimation of other orientation values as well as the
T ? S version of the model, follows the same pro-
cedure.
Pairwise and Borderwise dominance models.
Given Ri,S?T = sl2l1/t
m2
m1 and Li+1,S?T =
sl4l3/t
m4
m3 , i.e. the spans of the neighbors of a
pair of neighboring function word phrase pairs
(Yi = sl5l5/tm5m5 , Yi+1 = s
l6
l6
/tm6m6), the value of
d(FWi+1,S?T ) is:
=
?
?
?
?
?
?
?
?
?
?
?
leftFirst, l2 ? l6
?
l3 > l5
rightFirst, l2 < l6
?
l3 ? l5
dontCare, l2 ? l6
?
l3 ? l5
neither, l2 < l6
?
l3 > l5
(7)
Note that the neighbors of the sentence markers for
the borderwise models span the whole sentence, thus
value of neither is impossible for these models.
Suppose we want to estimate the probability of Yi
and Yi+1 having a dontCare dominance value. Note
that here we are interested in the lexical identity of
Yi and Yi+1, thus the models are insensitive to the in-
dices. We first gather the counts of the Yi and Yi+1
having the dontCare value c(dontCare, Yi, Yi+1);
then Pdom(dontCare|Yi, Yi+1) is estimated as fol-
lows:
Pdom(dontCare|Yi, Yi+1) =
c(dontCare, Yi, Yi+1)
c(Yi, Yi+1)
(8)
where c(Yi, Yi+1) is the count of Yi appears after
Yi+1 in the training corpus without any other func-
tion word comes in between.
538
4 Alignment Model
To use the function word alignment features de-
scribed in the previous section to predict alignments,
we use a linear model of the following form:
A? = arg max
A?A(S,T )
? ? f(A, S, T ) (9)
where A(S, T ) is the set of all possible alignments
of a source sentence S and target sentence T , and
f(A, S, T ) is a vector of feature functions on A, S,
and T , and ? is a parameter vector.
In addition to the six reordering models, our
model employs several association-based scores that
look at alignments in isolation. These features in-
clude:
1. Normalized log-likelihood ratio (LLR). This
feature represents an association score, derived from
statistical testing statistics. LLR (Dunning, 1993)
has been widely used especially to measure lexical
association. Since the values of LLR are unnormal-
ized, we normalize them on a per-sentence basis, so
that the normalized LLRs of, say, a particular source
word to the target words in a particular sentence sum
up to one.
2. Translation table from IBM model 4. This
feature represents another association score, derived
from a generative model, in particular the word-
based IBM model 4. The use of this feature is
widespread in recent alignment models, since it pro-
vides a relatively accurate initial prediction.
3. Translation table from manually-aligned
corpora. This feature represents a gold-standard as-
sociation score, based on human annotation. While
attractive, this feature suffers from data sparse-
ness issues since the lexical coverage of manually-
aligned corpora, especially over content words, is
very low. To overcome this issue, we design this
feature to have two levels of granularity; as such, a
fine-grained one is applied for function words and
the coarse-grained one for content words.
4. Grow-diag-final alignments bonus. This fea-
ture encourages our alignment model to reuse align-
ment points that are part of the alignments created
by the grow-diag-final heuristic, which we used as
the baseline of our machine translation experiments.
5. Fertility model from IBM model 4. This fea-
ture, which is another by-product of IBM model 4,
measures the probability of a certain word aligning
to zero, one, or two or more words.
6. Null-alignment probability. This bino-
mial feature models preference towards not aligning
words, i.e. aligning to the NULL token. The intu-
ition is to penalize NULL alignments depending on
word class, by assigning lower probability mass to
unaligned content words than to unaligned function
words. In our experiment, we assign feature value
10?3 for a function word aligning to NULL, and
10?5 for a content word aligning to NULL.
Note that with the exception of the alignment
bonus feature (4), all features are uni-directional,
and therefore we employ these features in both di-
rections just as was done for the reordering models.
5 Search
To find A? using the model in Eq. 9, it is neces-
sary to search 2|S|?|T | different alignment config-
urations, and, because of the non-local dependen-
cies in some of our features, it is not possible to use
dynamic programming to perform this search effi-
ciently. We therefore employ an approximate search
for the best alignment. We use a local search pro-
cedure which starts from some alignment (in our
case, a symmetrized Model 4 alignment) and make
local changes to it. Rather than taking a pure hill-
climbing approach which greedily moves to locally
better configurations (Brown et al, 1993), we use
a stochastic search procedure which can move into
lower-scoring states with some probability, similar
to the Monte Carlo techniques used to draw sam-
ples from analytically intractable probability distri-
butions.
5.1 Algorithm
To find A?, our search algorithm starts with an initial
alignment A(1) and iteratively draws a new set by
making a few small changes to the current set. For
each step i = [1, n], with alignment A(i), a set of
neighboring alignments N (A(i)) is induced by ap-
plying small transformations (discussed below) to
the current alignment. The next alignment A(i+1)
539
is sampled from the following distribution:
p(A(i+1)|S, T, A(i)) = exp? ? f(A
(i+1), S, T )
Z(A(i), S, T )
where Z(A(i), S, T ) =
?
A??N (A(i))
exp? ? f(A?, S, T )
In addition to the current ?active? alignment configu-
ration A(i), the algorithm keeps track of the highest
scoring alignment observed so far, Amax. After n
steps, the algorithm returns Amax as its approxima-
tion of A?. In the experiments reported below, we
initialized A(1) with the Model 4 alignments sym-
metrized by using the grow-diag-final-and heuristic
(Koehn et al, 2003).
5.2 Alignment Neighborhoods
We now turn to a discussion of how the alignment
neighborhoods used by our stochastic search algo-
rithm are generated. We define three local transfor-
mation operations that apply to single columns of
the alignment grid (which represent all of the align-
ments to the lth source word), rows, or existing align-
ment points (l, m). Our three neighborhood gener-
ating operators are ALIGN, ALIGNEXCLUSIVE, and
SWAP. The ALIGN operator applies to the lth col-
umn of A and can either add an alignment point
(l, m?) or move an existing one (including to null,
thus deleting it). ALIGNEXCLUSIVE adds an align-
ment point (l, m) and deletes all other points from
row m. Finally, the SWAP operator swaps (l, m) and
(l?, m?), resulting in new alignment points (l, m?)
and (l?, m). We increase the decoder?s mobility
by traversing the target side and applying the same
steps above for each target word. Fig. 3 illustrates
the three operators. By iterating over all columns l
and rows m, the full alignment space A(S, T ) can
be explored.4
To further reduce the search space, an alignment
point (l, m) is only admitted into a neighborhood if
it is found in the high-recall alignment set R(S, T ),
which we define to be the model 4 union alignments
(bidirectional model 4 symmetrized via union) plus
the 5 best alignments according to the log-likelihood
ratio.
4Using only the ALIGN operator, it is possible to explore
the full alignment space; however, using all three operators in-
creases mobility.
(a)
(b)
(c)
l l' l l'
m
m'
m
m'
m
m'
Figure 3: Illustrations for (a) ALIGN, (b) ALIGNEXCLU-
SIVE, and (c) SWAP operators, as applied to align the dot-
ted, smaller circle (l,m) to (l,m?). The left hand side rep-
resents A(i), while the right hand side represents a can-
didate for A(i+1). The solid circles represent the new
alignment points added to A(i+1).
6 Discriminative Training
To set the model parameters ?, we used the min-
imum error rate training (MERT) algorithm (Och,
2003) to maximize the F-measure of the 1-best
alignment of the model on a development set con-
sisting of sentence pairs with manually generated
alignments. The candidate set used by MERT to ap-
proximate the model is simply the set of alignments
{A(1), A(2), . . . , A(n)} encountered in the stochastic
search.
While MERT does not scale to large numbers of
features, the scarcity of manually aligned training
data also means that models with large numbers of
sparse features would be difficult to learn discrimi-
natively, so this limitation is somewhat inherent in
the problem space. Additionally, MERT has sev-
eral advantages that make it particularly useful for
our task. First, we can optimize F-measure of the
alignments directly, which has been shown to corre-
late with translation quality in a downstream system
(Fraser and Marcu, 2007b). Second, we are opti-
mizing the quality of the 1-best alignments under the
model. Since translation pipelines typically use only
a single word alignment, this criterion is appropri-
ate. Finally, and very importantly for us, MERT re-
quires only an approximation of the model?s hypoth-
esis space to carry out optimization. Since we are
using a stochastic search, this is crucial, since sub-
540
sequent evaluations of the same sentence pair (even
with the same weights) may result in a different can-
didate set.
Although MERT is a non-probabilistic optimizer,
we explore the alignment space stochastically. This
is necessary to make sure that the weights we use
correspond to a probability distribution that is not
overly peaked (which would result in a greedy hill-
climbing search) or flat (which would explore the
model space without information from the model).
We found that normalizing the weights by the Eu-
clidean norm resulted in a distribution that was well-
balanced between the two extremes.
7 Experiments
We evaluated our proposed alignment model intrin-
sically on an alignment task and extrinsically on a
large-scale translation task, focusing on Chinese-
English as the language pair. Our training data
consists of manually aligned corpora available from
LDC (LDC2006E93 and LDC2008E57) and un-
aligned corpora, which include FBIS, ISI, HKNews
and Xinhua. In total, the manually aligned corpora
consist of more than 21 thousand sentence pairs,
while the unaligned corpora consist of more than
710 thousand sentence pairs. The manually-aligned
corpora are primarily used for training the reorder-
ing models and for discriminative training purposes.
For translation experiments, we used cdec (Dyer
et al, 2010), a fast implementation of hierarchi-
cal phrase-based translation models (Chiang, 2005),
which represents a state-of-the-art translation sys-
tem.
We constructed the list of function words in En-
glish manually and in Chinese from (Howard, 2002).
Punctuation marks were added to the list, result-
ing in 883 and 359 tokens in the Chinese and En-
glish lists, respectively. For the alignment experi-
ments, we took the first 500 sentence pairs from the
newswire genre of the manually-aligned corpora and
used the first 250 sentences as the development set,
with the remaining 250 as the test set. To ensure
blind experimentation, we excluded these sentence
pairs from the training of the features, including the
reordering models.
7.1 Alignment Quality
We used GIZA++, the implementation of the de-
facto standard IBM alignment model, as our base-
line alignment model. In particular, we used
GIZA++ to align the concatenation of the develop-
ment set, the test set, and the unaligned corpora, with
5, 5, 3 and 3 iterations of model 1, HMM, model
3, and model 4 respectively. Since the IBM model
is asymmetric, we followed the standard practice of
running GIZA++ twice, once in each direction, and
combining the resulting outputs heuristically. We
chose to use the grow-diag-final-and heuristic as it
worked well for hierarchical phrase-based transla-
tion in our early experiments. We recorded the align-
ment quality of the test set as our baseline perfor-
mance.
For our alignment model, we used the same set of
training data. To align the test set, we first tuned
the weights of the features in our discriminative
alignment model using minimum error rate training
(MERT) (Och, 2003) with F?=0.1 as the optimiza-
tion criterion. At each iteration, our aligner outputs
k-best alignments under current set of weights, from
which MERT proceeds to compute the next set of
weights. MERT terminates once the improvement
over the previous iteration is lower than a predefined
value. Once tuned, we ran our aligner on the test set
and measured the quality of the resulting alignment
as the performance of our model.
Model P R F0.5 F0.1
gdfa 70.97 63.83 67.21 64.48
association 73.70 76.85 75.24 76.52
+ori 74.09 78.29 76.13 77.85
+dom 75.06 78.98 76.97 78.57
+bdom 75.41 80.53 77.89 79.99
Table 1: Alignment quality results (F0.1) for our discrim-
inative reordering models with various features (lines 2-
5) versus the baseline IBM word-based Model 4 sym-
metrized using the grow-diag-final-and heuristic. The
balanced F0.5 measure is reported for reference. The best
scores are bolded.
Table 1 reports the results of our experiments,
which are conducted in an incremental fashion pri-
marily to highlight the role of reordering model-
ing. The first line (gdfa) reports the baseline perfor-
541
mance. In the first experiment (association), we em-
ployed only the association-based features described
in Section 4. As shown, we obtain a significant im-
provement over baseline. This result is consistent
with recent literature (Fraser and Marcu, 2007a) that
shows that a discriminatively trained model outper-
forms baseline unsupervised models like GIZA++.
In the second set of experiments, we added the re-
ordering models into our discriminative model one
by one, starting with the orientation models, then
the pairwise dominance model and finally the bor-
derwise dominance model, reported in lines +ori,
+dom and +bdom respectively. As shown, each ad-
ditional reordering model provides a significant ad-
ditional improvement. The best result is obtained by
employing all reordering models. These results em-
pirically confirm our hypothesis that we can improve
alignment quality by employing reordering models
that capture non-local reordering phenomena.
7.2 Translation Quality
For translation experiments, we used the products
from our intrinsic experiments to learn translation
rules for the hierarchical phrase-based decoder, i.e.
the features weights of the +bdom experiment to
align the MT training data using our discriminative
model. For our translation model, we used the stan-
dard features based on the relative frequency counts,
including a 5-gram language model feature trained
on the English portion of the whole training data
plus portions of the Gigaword v2 corpus. Specif-
ically, we tuned the weights of these features via
MERT on the NIST MT06 set and we report the re-
sult on the NIST MT02, MT03, MT04 and MT05
sets.
MT02 MT03 MT04 MT05
gdfa 25.61 32.05 31.80 29.34
this work 26.56 33.79 32.61 30.47
Table 2: The translation performance (BLEU) of hierar-
chical phrase-based translation trained on training data
aligned by IBM model 4 symmetrized with the grow-
diag-final-and heuristic, versus being trained on align-
ments by our discriminative alignment model. Bolded
scores indicate that the improvement is statistically sig-
nificant.
Table 2 shows the result of our translation exper-
iments. In our alignment model, we employed the
whole set of reordering models, i.e. the one reported
in the +bdom line in Table 1. As shown, our dis-
criminative alignment model produces a consistent
and significant improvement over the baseline IBM
model 4 (p < 0.01), ranging between 0.81 and 1.71
BLEU points.
8 Related Work
The focus of our work is to strengthen the reordering
component of alignment modeling. Although the de
facto standard, the IBM models do not generalize
well in practice: the IBM approach employs a series
of reordering models based on the word?s position,
but reordering depends on syntactic context rather
than absolute position in the sentence. Over the
years, there have been many proposals to improve
these reordering models, most notably Vogel et al
(1996), which adds a first-order dependency. Never-
theless, the use of these distortion-based models re-
mains widespread (Marcu and Wong, 2002; Moore,
2004).
Alignment modeling is challenging because it
often has to consider a prohibitively large align-
ment space. Efforts to constrain the space gen-
erally comes from the use of Inversion Transduc-
tion Grammar (ITG) (Wu, 1997). Recent propos-
als that use ITG constraints include (Haghighi et
al., 2009; Blunsom et al, 2009) just to name a few.
More recent models have begun to use linguistically-
motivated constraints, often in combination with
ITG, primarily exploiting monolingual syntactic in-
formation (Burkett et al, 2010; Pauls et al, 2010).
Our reordering model is closely related to the
model proposed by Zhang and Gildea (2005; 2006;
2007a), with respect to conditioning the reordering
predictions on lexical items. These related models
treat their lexical items as latent variables to be es-
timated from training data, while our model uses
a fixed set of lexical items that correspond to the
class of function words. With respect to the focus
on function words, our reordering model is closely
related to the UALIGN system (Hermjakob, 2009).
However, UALIGN uses deep syntactic analysis and
hand-crafted heuristics in its model.
542
9 Conclusions
Languages exhibit regularities of word order that
are preserved when projected to another language.
We use the notion of function words to infer such
regularities, resulting in several reordering models
that are employed as features in a discriminative
alignment model. In particular, our models pre-
dict the reordering of function words by looking
at their dependencies with respect to their neigh-
boring phrases, their neighboring function words,
and the sentence boundaries. By capturing such
long-distance dependencies, our proposed align-
ment model contributes to the effort to unify align-
ment and translation. Our experiments demonstrate
that our alignment approach achieves both its intrin-
sic and extrinsic goals.
Acknowledgements
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001. Any
opinions, findings, conclusions or recommendations
expressed in this paper are those of the authors and
do not necessarily reflect the view of the sponsors.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In ACL, pages 782?790, Sun-
tec, Singapore, August. Association for Computational
Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In HLT-NAACL, pages 127?135, Los An-
geles, California, June. Association for Computational
Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discriminative
training. In COLING/ACL, pages 105?112, Sydney,
Australia, July. Association for Computational Lin-
guistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In ACL, pages
263?270, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL,
pages 17?24, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74, March.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In ACL, Up-
psala, Sweden.
Alexander Fraser and Daniel Marcu. 2007a. Getting
the structure right for word alignment: LEAF. In
EMNLP-CoNLL, pages 51?60, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007b. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
itg models. In ACL, pages 923?931, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
Ulf Hermjakob. 2009. Improved word alignment with
statistics and linguistic heuristics. In EMNLP, pages
229?237, Singapore, August. Association for Compu-
tational Linguistics.
Jiaying Howard. 2002. A Student Handbook for Chinese
Function Words. The Chinese University Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HTL-NAACL,
pages 127?133, Edmonton, Alberta, Canada, May. As-
sociation for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP, July 23.
Robert C. Moore. 2004. Improving ibm word alignment
model 1. In ACL, pages 518?525, Barcelona, Spain,
July.
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,
and Kazuteru Ohashi. 2006. A clustered global phrase
reordering model for statistical machine translation. In
ACL, pages 713?720, Sydney, Australia, July. Associ-
ation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Jamal Ouhalla. 1991. Functional Categories and Para-
metric Variation. Routledge.
543
Adam Pauls, Dan Klein, David Chiang, and Kevin
Knight. 2010. Unsupervised syntactic alignment with
inversion transduction grammars. In HLT-NAACL,
pages 118?126, Los Angeles, California, June. Asso-
ciation for Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li. 2007.
Ordering phrases with function words. In ACL, pages
712?719, Prague, Czech Republic, June. Association
for Computational Linguistics.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological ordering of function
words in hierarchical phrase-based translation. In
ACL, pages 324?332, Suntec, Singapore, August. As-
sociation for Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In COLING, pages 836?841, Copenhagen.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment. In
ACL. The Association for Computer Linguistics.
Hao Zhang and Daniel Gildea. 2006. Inducing word
alignments with bilexical synchronous trees. In ACL.
The Association for Computer Linguistics.
544
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594?604,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Predicting a Scientific Community?s Response to an Article
Dani Yogatama Michael Heilman Brendan O?Connor Chris Dyer
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.edu
Bryan R. Routledge
Tepper School of Business
Carnegie Mellon University
Pittsburgh, PA 15213, USA
routledge@cmu.edu
Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
nasmith@cs.cmu.edu
Abstract
We consider the problem of predicting mea-
surable responses to scientific articles based
primarily on their text content. Specif-
ically, we consider papers in two fields
(economics and computational linguistics)
and make predictions about downloads and
within-community citations. Our approach is
based on generalized linear models, allowing
interpretability; a novel extension that cap-
tures first-order temporal effects is also pre-
sented. We demonstrate that text features
significantly improve accuracy of predictions
over metadata features like authors, topical
categories, and publication venues.
1 Introduction
Written communication is an essential component
of the complex social phenomenon of science. As
such, natural language processing is well-positioned
to provide tools for understanding the scientific pro-
cess, by analyzing the textual artifacts (papers, pro-
ceedings, etc.) that it produces. This paper is about
modeling collections of scientific documents to un-
derstand how their textual content relates to how a
scientific community responds to them. While past
work has often focused on citation structure (Borner
et al, 2003; Qazvinian and Radev, 2008), our em-
phasis is on the text content, following Ramage et
al. (2010) and Gerrish and Blei (2010).
Instead of task-independent exploratory data anal-
ysis (e.g., topic modeling) or multi-document sum-
marization, we consider supervised models of the
collective response of a scientific community to a
published article. There are many measures of im-
pact of a scientific paper; ours come from direct
measurements of the number of downloads (from
an established website where prominent economists
post papers before formal publication) and citations
(within a fixed scientific community). We adopt a
discriminative approach based on generalized lin-
ear models that can make use of any text or meta-
data features, and show that simple lexical fea-
tures offer substantial power in modeling out-of-
sample response and in forecasting response for fu-
ture articles. Realistic forecasting evaluations re-
quire methodological care beyond the usual best
practices of train/test separation, and we elucidate
these issues.
In addition, we introduce a new regularization
technique that leverages the intuition that the rela-
tionship between observable features and response
should evolve smoothly over time. This regularizer
allows the learner to rely more strongly on more re-
cent evidence, while taking into account a long his-
tory of training data. Our time series-inspired regu-
larizer is computationally efficient in learning and is
a significant advance over earlier text-driven fore-
casting models that ignore the time variable alto-
gether (Kogan et al, 2009; Joshi et al, 2010).
We evaluate our approaches in two novel experi-
mental settings: predicting downloads of economics
articles and predicting citation of papers at ACL
conferences. Our approaches substantially outper-
594
0
15
00
4 9
log(# downloads)
# d
oc
s.
0
25
00
0 18
# citations
# d
oc
s.
Figure 1: Left: the distribution of log download counts
for papers in the NBER dataset one year after post-
ing. Right: the distribution of within-dataset citations of
ACL papers within three years of publication (outliers ex-
cluded for readability).
form text-ignorant baselines on ground-truth predic-
tions. Our time series models permit flexibility in
features and offer a novel and perhaps more inter-
pretable view of the data than summary statistics.
2 Data
We make use of two collections of scientific litera-
ture, one from the economics domain, and the other
from computational linguistics and natural language
processing. Statistics are summarized in Table 1.
2.1 NBER
Our first dataset consists of research papers in eco-
nomics from the National Bureau of Economic
Research (NBER) from 1999 to 2009 (http://
www.nber.org). Approximately 1,000 research
economists are affiliated with the NBER. New
NBER working papers are posted to the website
weekly. The papers are not yet peer-reviewed, but
given the prominence of many economists affiliated
with the NBER, many of the papers are widely read.
Text from the abstracts of the papers and related
metadata are publicly available. Full text is available
to subscribers (universities typically have access).
The NBER provided us with download statistics
for these papers. For each paper, we computed
the total number of downloads in the first year af-
ter each paper?s posting.1 The download counts are
log-normally distributed, as shown in Figure 1, and
so our regression models (?3) minimize squared er-
rors in the log space. Our download logs begin in
1For the vast majority of papers, most of the downloads oc-
cur soon after the paper?s posting. We explored different mea-
sures with different download windows (two years, for exam-
ple) with broadly similar results. We leave a more detailed anal-
ysis of the time series patterns of downloads to future work.
Dataset # Docs. Avg. #
Words
Response
NBER 8,814 155 # downloads in first
year (mean 761)
ACL 4,026 3,966 at least 1 citation in
first 3 years? (54% no)
Table 1: Descriptive statistics about the datasets.
1999. We use the 8,814 papers from 1999?2009 pe-
riod (there are 16,334 papers in the full dataset dat-
ing back to 1985). We only use text from the ab-
stracts, since we were able to obtain full texts for
just a portion of the papers, and since the OCR of
the full texts we do have is very noisy.
2.2 ACL
Our second dataset consists of research papers
from the Association for Computational Linguis-
tics (ACL) from 1980 to 2006 (Radev et al, 2009a;
Radev et al, 2009b). We have the full texts for pa-
pers (OCR output) as well as structured citation data.
There are 15,689 papers in the whole dataset. For
the citation prediction task, we include conference
papers from ACL, EACL, HLT, and NAACL.2 We
remove journal papers, since they are characteristi-
cally different from conference papers, as well as
workshop papers. We do include short papers, in-
teractive demo session papers, and student research
papers that are included in the companion volumes
for these conferences (such papers are cited less than
full papers, but many are still cited). The resulting
dataset contains 4,026 papers. The number of pa-
pers in each year varies because not all conferences
are annual.
We look at citations in the three-year window fol-
lowing publication, excluding self-citations and only
considering citations from papers within these con-
ferences. Figure 1 shows a histogram; note that
many papers (54%) are not cited at all, and the dis-
tribution of citations per paper is neither normal nor
log-normal. We organize the papers into two classes:
those with zero citations and those with non-zero ci-
tations in the three-year window.
2EMNLP is a relatively recent conference, and, in this col-
lection, complete data for its papers postdate the end of the last
training period, so we chose to exclude it from our dataset.
595
3 Model
Our forecasting approach is based on generalized
linear models for regression and classification. The
models are trained with an `2-penalty, often called
a ?ridge? model (Hoerl and Kennard, 1970).3 For
the NBER data, where (log) number of downloads is
nearly a continuous measure, we use linear regres-
sion. For the ACL data, where response is the bi-
nary cited-or-not variable we use logistic regression,
often referred to as a ?maximum entropy? model
(Berger et al, 1996) or a log-linear model. We
briefly review the class of models. Then, we de-
scribe a time series model appropriate for time series
data.
3.1 Generalized Linear Models
Consider a model that predicts a response y given a
vector input x = ?x1, . . . , xd? ? Rd. Our models
are linear functions of x and parameterized by the
vector ?. Given a corpus of M document features,
X , and responses Y , we estimate:
?? = argmin? R(?) + L(?,X, Y ) (1)
where L is a model-dependent loss function and R
is a regularization penalty to encourage models with
small weight vectors. We describe models and loss
functions first and then turn to regularization.
For the NBER data, the (log) number of down-
loads is continuous, and so we use least-squares
linear regression model. The loss function is the
sum of the squared errors for the M documents in
our training data: L(?,X, Y ) = ?Mi=1(yi ? y?i)2,
where the prediction rule for new documents is:
y? =
?d
j=0 ?jxj . Probabilistically, this equates to an
assumption that ?>x is the mean of a normal (i.e.,
Gaussian) distribution from which random variable
y is drawn.
For the ACL data, we predict y from a discrete
set C (specifically, the binary set of zero citations or
more than zero citations), and we use logistic regres-
sion. This model assumes that for the ith training
input xi, the output yi is drawn according to:
p(yi | xi) =
(
exp?>c xi
) /(?
c??C exp?>c?xi
)
3Preliminary experiments found no consistent benefit from
`1 (?lasso?) models, though we note that `1-regularization leads
to sparse, compact models that may be more interpretable.
where there is a feature vector ?c for each class
c ? C. Under this interpretation, parameter esti-
mation is maximum a posteriori inference for ?,
and R(?) is a log-prior for the weights. The loss
function is the negative log likelihood for the M
documents: L(?,X, Y ) = ??Mi=1 log p(yi | xi).
The prediction rule for a new document is: y? =
argmaxc?C
?d
j=0 ?c,jxj . Generalized linear mod-
els and penalized regression are well-studied with
an extensive literature (Mccullagh and Nelder, 1989;
Hastie et al, 2009). We leave other types of mod-
els, such as Poisson (Cameron and Trivedi, 1998)
or ordinal (McCullagh, 1980) regression models, to
future work.
3.2 Ridge Regression
With large numbers of features, regularization is
crucial to avoid overfitting. In ridge regression (Ho-
erl and Kennard, 1970), a standard method to which
we compare the time series regularization discussed
in ?3.3, the penalty R(?) is proportional to the `2-
norm of ?:
R(?) = ????2 = ?
?
j ?2j
where ? is a regularization hyperparameter that is
tuned on development data or by cross-validation.4
This penalty pushes many ?j close (but not com-
pletely) to zero. In practice, we multiply the penalty
by the number of examples M to facilitate tuning of
?.
The ridge linear regression model can be inter-
preted probabilistically as each coefficient ?j is
drawn i.i.d. from a normal distribution with mean
0 and variance 2??1.
3.3 Time Series Regularization
A simple way to capture temporal variation is to con-
join traditional features with a time variable. Here,
we divide the dataset into T time steps (years). In the
new representation, the feature space expands from
Rd to RT?d. For a document published at year t, the
elements of x are non-zero only for those features
that correspond to year-t; that is xt?,j = 0 for all
t? 6= t.
4The linear regression has a bias ?0 that is always active.
The logistic regression also has an unpenalized bias ?c,0 for
each class c. This weight is not regularized.
596
Estimating this model with the new features using
the `2-penalty would be effectively estimating sepa-
rate models for each year under the assumption that
each ?t,j is independent; even for features that dif-
fered only temporally (e.g., ?t,j and ?t+1,j).
In this work, we apply time series regularization
to GLMs, enabling models that have coefficients that
change over time but prefer gradual changes across
time steps. Boyd and Vandenberghe (2004, ?6.3) de-
scribe a general version of this sort of regularizer.
To our knowledge, such regularizers have not previ-
ously been applied to temporal modeling of text.
The time series regularization penalty becomes:
R(?) = ?
T?
t=1
d?
j=1
?2t,j+??
T?
t=2
d?
j=0
(?t,j ? ?t?1,j)2
It includes a standard `2-penalty on the coefficients,
and a penalty for differences between coefficients
for adjacent time steps to induce smooth changes.5
Similar to the previous model, in practice, we mul-
tiply the regularization constant ? by MT to facili-tate tuning of ? for datasets with different numbers
of examples M and numbers of time steps T . The
new parameter, ?, controls the smoothness of the es-
timated coefficients. Setting ? to zero imposes no
penalty for time-variation in the coefficients and re-
sults in independent ridge regressions at each time
step. Also, when the number of examples is con-
stant across time steps, setting a large ? parameter
(? ? ?) results in a single ridge regression over all
years since it imposes ?t,j = ?t+1,j for all t ? T .
The partial derivative is:
?R/??t,j = 2??t,j
+ 1{t > 1}2??(?t,j ? ?t?1,j)
+ 1{t < T}2??(?t,j ? ?t+1,j)
This time series regularization can be applied more
generally, not just to linear and logistic regression.
With either ridge regularization or this time se-
ries regularization scheme, Eq. 1 is an unconstrained
convex optimization problem for the linear models
5Our implementation of the time series regularizer does not
penalize the magnitude of the weight for the bias feature (as in
ridge regression). It does, however, penalize the difference in
the bias weight between time steps (as with other features).
?
1
?
2
?
3
?
T
Y
1
Y
2
Y
3
Y
T
...
X
1
X
2
X
3
X
T
?,?
Figure 2: Time series regression as a graphical model;
the variables Xt and Yt are the sets of feature vectors
and response variables from documents dated t.
we describe here. There exist a number of optimiza-
tion procedures for it; we use the L-BFGS quasi-
Newton algorithm (Liu and Nocedal, 1989).
Probabilistic Interpretation
We can interpret the time series regularization prob-
abilistically as follows. Let the coefficient for the
jth feature over time be ?j = ??1,j , ?2,j , ..., ?T,j?.
?j are draws from a multivariate normal distribu-
tion with a tridiagonal precision matrix ??1 = ? ?
RT?T :
? = ?
?
??????
1 + ? ?? 0 0 . . .
?? 1 + 2? ?? 0 . . .
0 ?? 1 + 2? ?? . . .
0 0 ?? 1 + 2? . . .
... ... ... ... . . .
?
??????
The form of R(?) follows from noting:
?2 log p(?j ;?, ?) = ?>j ??j + constant
The squared difference between adjacent time steps
comes from the off-diagonal entries in the preci-
sion matrix.6 Figure 2 shows a graphical represen-
tation of the time series regularization in our model.
Its Markov chain structure corresponds to the off-
diagonals.
There is a rich literature on time series analysis
(Box et al, 2008; Hamilton, 1994). The prior dis-
tribution over the sequence ??1,j , . . . , ?T,j? that our
regularizer posits is closely linked to a first-order au-
toregressive process, AR(1).
6Consistent with the previous section, we assume that pa-
rameters for different features, ?j and ?k, are independent.
597
NBER ACL
Response log(#downloads+1) 1{#citations > 0}
GLM type normal / squared-loss logistic / log-loss
Metric 1 mean absolute error accuracy
Metric 2 Kendall?s ? Kendall?s ?
Table 2: Summary of the setup for the NBER download
and ACL citation prediction experiments.
4 Features
NBER metadata features
? Authors? last names. We treat each name as a bi-
nary feature. If a paper has multiple authors, all
authors are used and they have equal weights re-
gardless of their ordering.
? NBER program(s).7 There are 19 major re-
search programs at the NBER (e.g., Monetary
Economics, Health Economics, etc.).
ACL metadata features
? Authors? last names as binary features.
? Conference venues. We use first letter of the ACL
anthology paper ID, which correlates with its con-
ference venue (e.g., P for the ACL main confer-
ence, H for the HLT conference, etc.).8
Text features
? Binary indicator features for the presence of each
unigram, bigram, and trigram. For the NBER
data, we have separate features for titles and ab-
stracts. For the ACL data, we have separate fea-
tures for titles and full texts. We pruned text fea-
tures by document frequency (details in ?5).
? Log transformed word counts. We include fea-
tures for the numbers of words in the title and the
abstract (NBER) or the full text (ACL).
7Almost all NBER papers are tagged with one or more pro-
grams (we assign untagged papers a ?null? tag). The complete
list of NBER programs can be found at http://www.nber.
org/programs
8Papers in the ACL dataset have a tag which shows which
workshop, conference, or journal they appeared in. However,
sometimes a conference is jointly held with another confer-
ence, such that meta information in the dataset is different even
though the conference is the same. For this reason, we simply
use the first letter of the paper ID.
5 Experiments
For each of the datasets in ?2, we test our models
for two tasks: forecasting about future papers (i.e.,
making predictions about papers that appeared af-
ter a training dataset) and modeling held-out papers
from the past (i.e., making predictions within the
same time period as the training dataset, on held-out
examples).
For the NBER dataset, the task is to predict the
number of downloads a paper will receive in its first
year after publication. For the ACL dataset, the task
is to predict whether a paper will be cited at all (by
another ACL paper in our dataset) within the first
three years after its publication. To our knowledge,
clean, reliable citation counts are not available for
the NBER dataset; nor are download statistics avail-
able for the ACL dataset. Table 2 summarizes the
variables of interest, model types, and evaluation
metrics for the tasks.
5.1 Extrapolation
The lag between a paper?s publication and when its
outcome (download or citation count) can be ob-
served poses a unique methodological challenge.
Consider predicting the number of downloads over
g future time steps. If t is the time of forecasting,
we can observe the texts of all articles published be-
fore t. However, any article published in the interval
[t ? g, t] is too recent for the outcome measurement
of y to be taken. We refer to the interval [t? g, t] as
the ?forecast gap?. Since recent articles are some-
times the most relevant predictions at t, we do not
want to ignore them. Consider a paper at time step
t?, t?g < t? < t. To extrapolate its number of down-
loads, we consider the observed number in [t?, t], and
then estimate the ratio r of downloads that occur in
the first t?t? time steps, against the first g time steps,
using the fully observed portion of the training data.
We then scale the observed downloads during [t?, t]
by r?1 to extrapolate. The same method is used to
extrapolate citation counts.
In preliminary experiments, we observed that ex-
trapolating responses for papers in the forecast gap
led to better performance in general. For example,
for the ridge regressions trained on all past years
with the full feature set, the error dropped from 262
to 259 when using extrapolation compared to with-
598
out extrapolation. Also, the extrapolated download
counts were quite close to the true values (which we
have but do not use because of the forecast gap): for
example, the mean absolute error of the extrapolated
responses was 99 when extrapolated based on the
median of the fully observed portion of the training
data (measured monthly).
5.2 Forecasting NBER Downloads
In our first set of experiments, we predict the number
of downloads of an NBER paper within one year of
its publication.
We compare four approaches for predicting
downloads. The first is a baseline that simply uses
the median of the log of the training and develop-
ment data as the prediction. The second and third
use GLMs with ridge regression-style regularization
(?3.2), trained on all past years (?all years?) and on
the single most recent past year (?one year?), respec-
tively. The last model (?time series?) is a GLM with
time series regularization (?3.3).
We divided papers by year. Figure 3 illustrates the
experimental setup. We held out a random 20% of
papers for each year from 1999?2007 as a test set for
the task of modeling the past. To define the feature
set and tune hyperparameters, we used the remain-
ing 80% of papers from 1999?2005 as our training
data and the remaining papers in 2006 as our devel-
opment data. After pruning,9 we have 37,251 to-
tal features, of which 2,549 are metadata features.
When tuning hyperparameters, we simulated the ex-
istence of a forecast gap by using extrapolated re-
sponses for papers in the last year of the training
data instead of their true responses. We considered
? ? 5{2,1,...,?5,?6}, and ? ? 5{3,2,...,?1,?2} and se-
lected those that led to the best performance on the
development set.
We then used the selected feature set and hyperpa-
rameters to test the forecasting and modeling capa-
bilities of each model. For forecasting, we predicted
numbers of downloads of papers in 2008 and 2009.
We used the baseline median, ridge regression, and
time series regularization models trained on papers
in 1999?2007 and 1999?2008, respectively. We
treated the last year of the training data (2007 and
9For NBER, text features appearing in less than 0.1% or
more than 99.9% of the training documents were removed. For
ACL, the thresholds were 2% and 98%.
training
modeling test (unused)
gap dev.
trr taitan tag
training gap test
trr tai tam tao
ddd
ddd
modeling test (unused)
oae
lae
oae
lae
training gap test
trr tamddd
modeling test
oae
lae
tao tar
NBER Experiments
ACL Experiments
training
modeling test (unused)
dev.
toa tro ta ddd
oae
lae
s(u)p)v.(uu)p)vProceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 223?232, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Bayesian Model for Learning SCFGs with Discontiguous Rules
Abby Levenberg
Dept. of Computer Science
University of Oxford
ablev@cs.ox.ac.uk
Chris Dyer
School of Computer Science
Carnegie Mellon Univeristy
cdyer@cs.cmu.edu
Phil Blunsom
Dept. of Computer Science
University of Oxford
pblunsom@cs.ox.ac.uk
Abstract
We describe a nonparametric model
and corresponding inference algorithm
for learning Synchronous Context Free
Grammar derivations for parallel text. The
model employs a Pitman-Yor Process prior
which uses a novel base distribution over
synchronous grammar rules. Through both
synthetic grammar induction and statistical
machine translation experiments, we show
that our model learns complex translational
correspondences? including discontiguous,
many-to-many alignments?and produces
competitive translation results. Further,
inference is efficient and we present results on
significantly larger corpora than prior work.
1 Introduction
In the twenty years since Brown et al1992) pio-
neered the first word-based statistical machine trans-
lation (SMT) models substantially more expressive
models of translational equivalence have been devel-
oped. The prevalence of complex phrasal, discon-
tiguous, and non-monotonic translation phenomena
in real-world applications of machine translation has
driven the development of hierarchical and syntac-
tic models based on synchronous context-free gram-
mars (SCFGs). Such models are now widely used in
translation and represent the state-of-the-art in most
language pairs (Galley et al2004; Chiang, 2007).
However, while the models used for translation have
evolved, the way in which they are learnt has not:
na??ve word-based models are still used to infer trans-
lational correspondences from parallel corpora.
In this work we bring the learning of the minimal
units of translation in step with the representational
power of modern translation models. We present a
nonparametric Bayesian model of translation based
on SCFGs, and we use its posterior distribution to
infer synchronous derivations for a parallel corpus
using a novel Gibbs sampler. Our model is able
to: 1) directly model many-to-many alignments,
thereby capturing non-compositional and idiomatic
translations; 2) align discontiguous phrases in both
the source and target languages; 3) have no restric-
tions on the length of a rule, the number of nonter-
minal symbols per rule, or their configuration.
Learning synchronous grammars is hard due to
the high polynomial complexity of dynamic pro-
gramming and the exponential space of possible
rules. As such most prior work for learning SCFGs
has relied on inference algorithms that were heuristi-
cally constrained or biased by word-based alignment
models and small experiments (Wu, 1997; Zhang et
al., 2008; Blunsom et al2009; Neubig et al2011).
In contrast to these previous attempts, our SCFG
model scales to large datasets (over 1.3M sentence
pairs) without imposing restrictions on the form of
the grammar rules or otherwise constraining the set
of learnable rules (e.g., with a word alignment).
We validate our sampler by demonstrating its
ability to recover grammars used to generate
synthetic datasets. We then evaluate our model by
inducing word alignments for SMT experiments
in several typologically diverse language pairs and
across a range of corpora sizes. Our results attest to
our model?s ability to learn synchronous grammars
encoding complex translation phenomena.
223
2 Prior Work
The goal of directly inducing phrasal translation
models from parallel corpora has received a lot of
attention in the NLP and SMT literature. Marcu
and Wong (2002) presented an ambitious maximum
likelihood model and EM inference algorithm for
learning phrasal translation representations. The
first issue this model faced was a massive parameter
space and intractable inference. However a more
subtle issue is that likelihood based models of this
form suffer from a degenerate solution, resulting
in the model learning whole sentences as phrases
rather than minimal units of translation. DeNero
et al2008) recognised this problem and proposed
a nonparametric Bayesian prior for contiguous
phrases. This had the dual benefits of biasing the
model towards learning minimal translation units,
and integrating out the parameters such that a much
smaller set of statistics would suffice for inference
with a Gibbs sampler. However this work fell short
by not evaluating the model independently, instead
only presenting results in which it was combined
with a standard word-alignment initialisation, thus
leaving open the question of its efficacy.
The fact that flat phrasal models lack a structured
approach to reordering has led many researchers to
pursue SCFG induction instead (Wu, 1997; Cherry
and Lin, 2007; Zhang et al2008; Blunsom et
al., 2009). The asymptotic time complexity of
the inside algorithm for even the simplest SCFG
models is O(|s|3|t|3), too high to be practical for
most real translation data. A popular solution to
this problem is to heuristically restrict inference
to derivations which agree with an independent
alignment model (Cherry and Lin, 2007; Zhang et
al., 2008). However this may have the unintended
effect of biasing the model back towards the initial
alignments that they attempt to improve upon.
More recently Neubig et al2011) reported a
novel Bayesian model for phrasal alignment and
extraction that was able to model phrases of multiple
granularities via a synchronous Adaptor Grammar.
However this model suffered from the common
problem of intractable inference and results were
presented for a very small number of samples from
a heuristically pruned beam, making interpreting
the results difficult.
Blunsom et al2009) presented an approach
similar to ours that implemented a Gibbs sampler
for a nonparametric Bayesian model of ITG. While
that work managed to scale to a non-trivially sized
corpus, like other works it relied on a state-of-the-art
word alignment model for initialisation. Our model
goes further by allowing discontiguous phrasal
translation units. Surprisingly, the freedom
that this extra power affords allows the Gibbs
sampler we propose to mix more quickly, allowing
state-of-the-art results from a simple initialiser.
3 Model
We use a nonparametric generative model based on
the 2-parameter Pitman-Yor process (PYP) (Pitman
and Yor, 1997), a generalisation of the Dirichlet Pro-
cess, which has been used for various NLP modeling
tasks with state-of-the-art results such as language
modeling, word segmentation, text compression and
part of speech induction (Teh, 2006; Goldwater et
al., 2006; Wood et al2011; Blunsom and Cohn,
2011). In this section we first provide a brief defi-
nition of the SCFG formalism and then describe our
PYP prior for them.
3.1 Synchronous Context-Free Grammar
An synchronous context-free grammar (SCFG) is a
5-tuple ??,?, V, S,R? that generalises context-free
grammar to generate strings concurrently in two lan-
guages (Lewis and Stearns, 1968). ? is a finite set of
source language terminal symbols, ? is a finite set
of target language terminal symbols, V is a set of
nonterminal symbols, with a designated start sym-
bol S, and R is a set of synchronous rewrite rules.
A string pair is generated by starting with the pair
?S1 | S1? and recursively applying rewrite rules of
the form X ? ?s, t, a? where the left hand side
(LHS) X is a nonterminal in V , s is a string in
(? ? V )?, t is a string in (? ? V )? and a specifies
a one-to-one mapping (bijection) between nontermi-
nal symbols in s and t. The following are examples:1
VP ? ? schlage NP1 NP2 vor | suggest NP2 to NP1 ?
NP ? ? die Kommission | the commission ?
1The nonterminal alignment a is indicated through sub-
scripts on the nonterminals.
224
In a probabilistic SCFG, rules are associated with
probabilities such that the probabilities of all
rewrites of a particular LHS category sum to 1.
Translation with SCFGs is carried out by parsing
the source language with the monolingual source
language projection of the grammar (using standard
monolingual parsing algorithms), which induces
a parallel tree structure and translation in the
target language (Chiang, 2007). Alignment or
synchronous parsing is the process of concurrently
parsing both the source and target sentences,
uncovering the derivation or derivations that give
rise to a string pair (Wu, 1997; Dyer, 2010).
Our goal is to infer the most probable SCFG
derivations that explain a corpus of parallel sen-
tences, given a nonparametric prior over probabilis-
tic SCFGs. In this work we will consider grammars
with a single nonterminal category X.
3.2 Pitman-Yor Process SCFG
Before training we have no way of knowing how
many rules will be needed in our grammar to ade-
quately represent the data. By using the Pitman-
Yor process as a prior on the parameters of a syn-
chronous grammar we can formulate a model which
prefers smaller numbers of rules that are reused
often, thereby avoiding degenerate grammars con-
sisting of large, overly specific rules. However, as
the data being fit grows, the model can become more
complex. The PYP is parameterised by a discount
parameter d, a strength parameter ?, and the base
distribution G0, which gives the prior probability
of an event (in our case, events are rules) before
any observations have occurred. The discount is
subtracted from each positive rule count and damp-
ens the rich get richer effect where frequent rules
are given higher probability compared to infrequent
ones. The strength parameter controls the variance,
or concentration, about the base distribution.
In our model, a draw from a PYP is a distribution
over SCFG rules with a particular LHS (in fact, it is
a distribution over all well-formed rules). From this
distribution we can in turn draw individual rules:
GX ? PY(d, ?,G0),
X ? ?s, t, a? ? GX .
Although the PYP has no known analytical form,
we can marginalise out the GX ?s and reason about
Step 1: Generate source side length.
Step 2: Generate source side configuration of 
terminals (and non-terminal placeholders).
Step 3: Generate target length.
Step 4. Generate target side configuration of 
terminals (and non-terminal placeholders).
Step 5. Generate the words.
X < _ _ _  ||| ? >
X < X1 _ X2 ||| ? >
X < X1 _ X2 ||| _ _ _  >
X < X1 _ X2 ||| _ X1 X2  >
X < X1 ? X2 ||| you X1 X2  >
Figure 1: Example generation of a synchronous
grammar rule in our G0.
individual rules directly using the process described
by Teh (2006). In this process, at time n a rule rn
is generated by stochastically deciding whether to
make another copy of a previously generated rule
or to draw a new one from the base distribution, G0.
Let ? = (?1, ?2, . . .) be the sequence of draws from
G0; thus |?| is the total number of draws from G0. A
rule rn corresponds to a selection of a ?k. Let ck
be a counter indicating the number of times ?k has
been selected. In particular, we set rn to ?k with
probability
ck ? d
? + n ,
and increment ck, or with probability
? + d ? |?|
? + n ,
we draw a new rule from G0, append it to ?, and use
it for rn.
3.3 Base Distribution
The base distribution G0 for the PYP assigns prob-
ability to a rule based our belief about what consti-
tutes a good rule independent of observing any of
225
the data. We describe a novel generative process for
all rules X ? ?s, t, a? that encodes these beliefs.
We describe the generative process generally here
in text, and readers may refer to the example in Fig-
ure 1. The process begins by generating the source
length (total number of terminal and nonterminal
symbols, written |s|) by drawing from a Poisson dis-
tribution with mean 1:
|s| ? Poisson(1) .
This assigns high probability to shorter rules,
but arbitrarily long rules are possible with a low
probability. Then, for every position in s, we decide
whether it will contain a terminal or nonterminal
symbol by repeated, independent draws from a
Bernoulli distribution. Since we believe that shorter
rules should be relatively more likely to contain
terminal symbols than longer rules, we define the
probability of a terminal symbol to be ?|s| where
0 < ? < 1 is a hyperparameter.
si ? Bernoulli(?|s|) ? i ? [1, |s|] .
We next generate the length of the target side of
the rule. Let #NT(s) denote the number of nonter-
minal symbols we generated in s, i.e., the arity of
the rule. Our intuition here is that source and target
lengths should be similar. However, to ensure that
the rule is well-formed, t must contain exactly as
many nonterminal symbols as the source does. We
therefore draw the number of target terminal sym-
bols from a Poisson whose mean is the number of
terminal symbols in the source, plus a small constant
?0 to ensure that it is greater than zero:
|t| ? #NT(s) ? Poisson (|s| ? #NT(s) + ?0) .
We then determine whether each position in t is
a terminal or nonterminal symbol by drawing uni-
formly from the bag of #NT(s) source nontermi-
nals and |t| ? #NT(s) terminal indicators, with-
out replacement. At this point we have created a
rule template which indicates how large the rule is,
whether each position contains a terminal or non-
terminal symbol, and the reordering of the source
nonterminals a. To conclude the process we must
select the terminal types from the source and target
vocabularies. To do so, we use the following distri-
bution:
Pterminals(s, t) =
PM1?(s, t) + PM1?(s, t)
2
where PM1?(s, t) (PM1?(s, t)) first generates the
source (target) terminals from uniform draws from
the vocabulary, then generates the string in the other
language according to IBM MODEL 1, marginaliz-
ing over the alignments (Brown et al1993).
4 Gibbs Sampler
In this section we introduce a Gibbs sampler that
enables us to perform posterior inference given a
corpus of sentence pairs. Our innovation is to repre-
sent the synchronous derivation of a sentence pair in
a hierarchical 4-dimensional binary alignment grid,
with elements z[s,t,u,v] ? {0, 1}.
The settings of the grid variables completely
determine the SCFG rules in the current derivation.
A setting of a binary variable z[s,t,u,v] = 1 represents
a constituent linking the source span [s, t] and the
target span [u, v] in the current derivation; variables
with a value of 0 indicate no link between spans
[s, t] and [u, v].2 This relationship from our grid
representation is illustrated in Figure 2a.
Our Gibbs sampler operates over the space of all
the random variables z[s,t,u,v], resampling one at a
time. Changes to a single variable imply that at most
two additional rules must be generated, as illustrated
in Figure 2b. The probability of choosing a binary
setting of 0 or 1 for a variable is proportional to the
probability of generating the two derivations under
the model described in the previous section. Note
that for a given sentence, most of the bispan vari-
ables must be set to 0 otherwise they would violate
the strict nesting constraint required for valid SCFG
derivations. We discuss below how to exploit this
fact to limit the number of binary variables that must
be resampled for each sentence.
To be valid, a Gibbs sampler must be ergodic and
satisfy detailed balance. Ergodicity requires that
there is non-zero probability that any state in the
sampler be reachable from any other state. Clearly
2Our grid representation is the synchronous generalisation
of the well-known correspondence between CFG derivations
and Boolean matrices; see Lee (2002) for an overview.
226
Amna will
{mna
succeed
kAmyAb
hw
gy
AmnA
awiilaaswu
cl
eduu
(a) An example grid representation of a syn-
chronous derivation. The SCFG rules (annotated
with their bispans) that correspond to this setting
of the grid are:
X[0,4,0,3] ?
? X[0,1,0,1] X[1,4,1,3] | X[0,1,0,1] X[1,4,1,3] ?
X[0,1,0,1] ? ? {mna | Amna ?
X[1,4,1,3] ? ? kAmyAb hw gy | will succeed ?
Amna will
{mna
succeed
kAmyAb
hw
gy
AmnA
awiilaaswu
cl
eduu
(b) The toggle operator resamples a bispan vari-
able (here, z[1,3,2,3], shown in blue) to determine
whether it should be subtracted from the immedi-
ately dominating rule (bispan in red) and made into
a child rule in the derivation. This would require
the addition of the following two rules:
X[1,4,1,3] ? ? X[1,3,2,3] gy | will X[1,3,2,3]?
X[1,3,2,3] ? ? kAmyAb hw | succeed ?
Alternatively, the active bispan variable can be set
so it is not a constituent, which would require the
single rule:
X[1,4,1,3] ? ? kAmyAb hw gy | will succeed ?
Figure 2: A single operation of the Gibbs sampler for a binary alignment grid.
our operator satisfies this since given any configu-
ration of the alignment grid we can use the toggle
operator to flatten the derivation to a single rule and
then break it back down to reach any derivation.
Detailed balance requires that the probability of
transitioning between two possible adjacent sampler
states respects their joint probabilities in the station-
ary distribution. One way to ensure this is to make
the order in which bispan variables are visited deter-
ministic and independent of the variables? current
settings. Then, the probability of the sampler tar-
geting any bispan in the grid is equal regardless of
the current configuration of the alignment grid.
A naive instantiation of this strategy is to visit all
|s|2|t|2 bispans in some order. However, since we
wish to be able to draw many samples, this is not
computationally feasible. A much more efficient
approach avoids resampling variables that would
result in violations without visiting each of them
individually. However, to ensure detailed balanced
is maintained, the order that we resample bispans
has to match the order we would sample them using
any exhaustive approach. We achieve this by always
checking a derivation top-down, from largest to
smallest bispan. Under this ordering, whether or not
a smaller bispan is visited will be independent of
how the larger ones were resampled. Furthermore,
the set of variables that may be resampled is fixed
given this ordering. Therefore, the probability of
sampling any possible bispan in the sentence pair is
still uniform (ensuring detailed balance), while our
sampler remains fast.
5 Evaluation
The preceding sections have introduced a model,
and accompanying inference technique, designed to
induce a posterior distribution over SCFG deriva-
tions containing discontiguous and phrasal transla-
tion rules. The evaluation that follows aims to deter-
mine our models ability to meet these design goals,
and to do so in a range of translation scenarios.
In order to validate both the model and the sam-
pler?s ability to learn an SCFG we first conduct a
synthetic experiment in which the true grammar is
227
known. Subsequently we conduct a series of experi-
ments on real parallel corpora of increasing sizes to
explore the empirical properties of our model.
5.1 Synthetic Data Experiments
Prior work on SCFG induction for SMT has val-
idated modeling claims by reporting BLEU scores
on real translation tasks. However, the combination
of noisy data and the complexity of SMT pipelines
conspire to obscure whether models actually achieve
their design goals, normally stated in terms of an
ability to induce SCFGs with particular properties.
Here we include a small synthetic data experiment
to clearly validate our models ability to learn an
SCFG that includes discontiguous and phrasal trans-
lation rules with non-monotonic word order.
Using the probabilistic SCFG shown in the top
half of Table 1 we stochastically generated three
thousand parallel sentence pairs as training data for
our model. We then ran the Gibbs sampler for fifty
iterations through the data.
The bottom half of Table 1 lists the five rules
with the highest marginal probability estimated by
the sampler. Encouragingly our model was able to
recover a grammar very close to the original. Even
for such a small grammar the space of derivations
is enormous and the task of recovering it from a
data sample is non-trivial. The divergence from the
true probabilities is due to the effect of the prior
assigning shorter rules higher probability. With a
larger data sample we would expect the influence of
the prior in the posterior to diminish.
5.2 Machine Translation Evaluation
Ultimately the efficacy of a model for SCFG induc-
tion will be judged on its ability to underpin a state-
of-the-art SMT system. Here we evaluate our model
by applying it to learning word alignments for par-
allel corpora from which SMT systems are induced.
We train models across a range of corpora sizes and
for language pairs that exhibit the type of complex
alignment phenomena that we are interested in mod-
eling: Chinese ? English (ZH-EN), Urdu ? English
(UR-EN) and German ? English (DE-EN).
Data and Baselines
The UR-EN corpus is the smallest of those used in
our experiments and is taken from the NIST 2009
GRAMMAR RULE TRUE PROBABILITY
X? ? X1 a X2 |X1 X2 1 ? 0.2
X? ? b c d | 3 2 ? 0.2
X? ? b d | 3 ? 0.2
X? ? d | 3 ? 0.2
X? ? c d | 3 1 ? 0.2
SAMPLED RULE SAMPLED PROBABILITY
X? ? d | 3 ? 0.25
X? ? b d | 3 ? 0.24
X? ? c d | 3 1 ? 0.24
X? ? b c d | 3 2 ? 0.211
X? ? X1 a X2 |X1 X2 1 ? 0.012
Table 1: Manually created SCFG used to generate
synthetic data, and the five most probable inferred
rules by our model.
ZH-EN
NIST
UR-EN
NIST
DE-EN
EUROPARL
TRAIN (SRC) 8.6M 1.2M 34M
TRAIN (TRG) 9.5M 1.0M 36M
DEV (SRC) 22K 18K 26K
DEV (TRG) 27K 16K 28K
Table 2: Corpora statistics (in words).
translation evaluation.3 The ZH-EN data is of a
medium scale and comes from the FBIS corpus.
The DE-EN pair constitutes the largest corpus and
is taken from Europarl, the proceedings of the Euro-
pean Parliament (Koehn, 2003). Statistics for the
data are shown in Table 2. We measure translation
quality via the BLEU score (Papineni et al2001).
All translation systems employ a Hiero
translation model during decoding. Baseline
word alignments were obtained by running
GIZA++ in both directions and symmetrizing using
the grow-diag-final-and heuristic (Och and
Ney, 2003; Koehn et al2003). Decoding was
performed with the cdec decoder (Dyer et al
2010) with the synchronous grammar extracted
using the techniques developed by Lopez (2008).
All translation systems include a 5-gram language
model built from a five hundred million token subset
3http://www.itl.nist.gov/iad/mig/tests/
mt/2009/
228
LANGUAGE TEST MODEL 4 MODEL 1 PYP-SCFG
PAIR SET BASELINE INITIALISATION WEAK M1 INIT. STRONG HMM INIT.
UR-EN MT09 23.1 18.5 23.7 24.0
ZH-EN MT03-08 29.4 19.8 28.3 29.8
DE-EN EUROPARL 28.4 25.5 27.8 29.2
Table 3: Results for the SMT experiments in BLEU . The baseline is produced using a full GIZA++ run. The
MODEL 1 INITIALISATION column is from the initialisation alignments using MODEL 1 and no sampling.
The PYP-SCFG columns show results for the 500th sample for both MODEL 1 and HMM initialisations.
of all the English data made available for the NIST
2009 shared task (Graff, 2003).
Experimental Setup
To obtain the PYP-SCFG word alignments we
ran the sampler for five hundred iterations for each
of the language pairs and experimental conditions
described below. We used the approach of Newman
et al2007) to distribute the sampler across multi-
ple threads. The strength ? and discount d hyper-
parameters of the Pitman-Yor Processes, and the ter-
minal penalty ? (Section 3.3), were inferred using
slice sampling (Neal, 2000).
The Gibbs sampler requires an initial set of
derivations from which to commence sampling. In
our experiments we investigated both weak and
a strong initialisations, the former based on word
alignments from IBM Model 1 and the latter on
alignments from an HMM model (Vogel et al
1996). For decoding we used the word alignments
implied by the derivations in the final sample to
extract a Hiero grammar with the same standard set
of relative frequency, length, and language model
features used for the baseline.
Weak Initialisation
Our first translation experiments ascertain the
degree to which our proposed Gibbs sampling
inference algorithm is able to learn good
synchronous derivations for the PYP-SCFG model.
A number of prior works on alignment with Gibbs
samplers have only evaluated models initialised
with the more complex GIZA++ alignment models
(Blunsom et al2009; DeNero et al2008), as a
result it can be difficult to separate the performance
of the sampler from that of the initialisation.
In order to do this, we initialise the sampler
PYP-SCFG
LANGUAGE PAIR MODEL 1 INIT. HMM INIT.
UR-EN 1.93/2.08 1.45/1.58
ZH-EN 3.47/4.28 1.69/2.37
DE-EN 4.05/4.77 1.50/2.04
Table 4: Average source/target rule lengths in the
PYP-SCFG models after the 500th sample for the
different initialisations.
using just the MODEL 1 distribution used in the
PYP-SCFG model?s base distribution. We denote
this a weak initialisation as no alignment models
outside of those included in the PYP-SCFG model
influence the resulting word alignments. The
BLEU scores for translation systems built from the
five hundredth sample are show in the WEAK M1
INIT. column of Table 3. Additionally we build a
translation system from the MODEL 1 alignment
used to initialise the sampler without using using our
PYP-SCFG model or sampling. BLEU scores are
shown in the MODEL 1 INITIALISATION column
of Table 3. Firstly it is clear MODEL 1 is indeed a
weak initialiser as the resulting translation systems
achieve uniformly low BLEU scores. In contrast, the
models built from the output of the Gibbs sampler
for the PYP-SCFG model achieve BLEU scores
comparable to those of the MODEL 4 BASELINE.
Thus the sampler has moved a good distance from
its initialisation, and done so in a direction that
results in better synchronous derivations.
Strong Initialisation
Given we have established that the sampler can
produce state-of-the-art translation results from a
229
weak initialisation, it is instructive to investigate
whether initialising the model with a strong
alignment system, the GIZA++ HMM (Vogel et
al., 1996), leads to further improvements. Column
HMM INIT. of Table 3 shows the results for
initialising with the HMM word alignments and
sampling for 500 iterations. Starting with a stronger
initial sample results in both quicker mixing and
better translation quality for the same number of
sampling iterations.
Table 4 compares the average lengths of the rules
produced by the sampler with both the strong and
weak initialisers. As the size of the training corpora
increases (UR-EN ? ZH-EN ? DE-EN) we see that
the average size of the rules produced by the weakly
initialised sampler also increases, while that of the
strongly initialised model stays relatively uniform.
Initially both samplers start out with a large num-
ber of long rules and as the sampling progresses
the rules are broken down into smaller, more gen-
eralisable, pieces. As such we conclude from these
metrics that after five hundred samples the strongly
initialised model has converged to sampling from a
mode of the distribution while the weakly initialised
model converges more slowly and on the longer cor-
pora is still travelling towards a mode. This sug-
gests that longer sampling runs, and Gibbs operators
that make simultaneous updates to multiple parts
of a derivation, would enable the weakly initialised
model to obtain better translation results.
Grammar Analysis
The BLEU scores are informative as a measure of
translation quality but we also explored some of the
differences in the grammars obtained from the PYP-
SCFG model compared to the standard approach. In
Figures 3 and 4 we show some basic statistics of
the grammars our model produces. From Figure 3
we see that the number of unique rules in the PYP-
SCFG grammar decreases steadily as the sampler
iterates through the data, so the model is finding an
increasingly sparser distribution with fewer but bet-
ter quality rules as sampling progresses. Note that
the gradient of the curves appears to be a function of
the size of the corpus and suggests that the model
built from the large DE-EN corpus would benefit
from a longer sampling run. Figure 4 shows the dis-
tribution of rules with a given arity as a percentage
 140
 160
 180
 200
 220
 240
 260
 280
 300
 320
 340
 0  20  40  60  80  100
u
n
iq
ue
 g
ra
m
m
ar
 ru
le
s i
n 
PY
P
samples
ur-en (* 1k)
zh-en (* 3k)
de-en (* 10k)
Figure 3: Unique grammar rules for each language
pair as a function of the number of samples. The
number of rule types decreases monotonically as
sampling continues. Rule counts are displayed by
normalised corpus size (see Table 2).
X? ?? | end of ?
X? ??? | ninth ?*
X? ??? X | charter X ?
X? ??? | confidence in ?
X? ????? X | the chinese government X ?
X? ??? | are ?
X? ?????? X | beijing , X ?*
X? ????? | departments concerned ?
X? ??????? X | washington , X ?*
X? ???? X1? X2 , | he X1 X2 , ?*
Table 5: The five highest ZH-EN probability rules in
the Hiero grammar built from the PYP-SCFG that
are not in the baseline Hiero grammar (top), and the
top five rules in the baseline Hiero grammar that
are not in the PYP-SCFG grammar (bottom). An
* indicates a bad translation rule.
of the full grammar after the final sampling iteration.
The model prior biases the results to shorter rules as
the vast majority of the model probability mass is on
rules with zero, one or two nonterminals.
Tables 5 and 6 show the most probable rules in the
Hiero translation system obtained using the PYP-
SCFG alignments that are not present in the TM
from the GIZA++ alignments and visa versa. For
both language pairs, four of the top five rules in
230
X? ? yh | it is ?
X? ? zmyn | the earth ?
X? ? yhy X | the same X ?
X? ? X1 nhyN X2 gy | X2 not be X1 ?
X? ? X1 gY kh X2 | recommend that X2 X1 ?*
X? ? hwN gY | will ?
X? ? Gyr mlky | international ?*
X? ? X1 *rAye kY X2 | X2 to X1 sources ?*
X? ? nY X1 nhyN kyA X2 | did not X1 X2 ?*
X? ? xAtwn X1 ky X2 | woman X2 the X1?
Table 6: Five of the top scoring rules in the UR-EN
Hiero grammar from sampled PYP-SCFG align-
ments (top) versus the baseline UR-EN Hiero gram-
mar rules not in the sampled grammar (bottom). An
* indicates a bad translation rule.
 0
 0.1
 0.2
 0.3
 0.4
 0  1  2 3+
%
 o
f r
ul
es
arity
zh-en
ur-en
de-en
Figure 4: The percentage of rules with a given arity
in the final grammar of the PYP-SCFG model.
the PYP-SCFG grammar that are not in the heuris-
tically extracted grammar are correct and minimal
phrasal units of translation, whereas only two of the
top probability rules in the GIZA++ grammar are of
good translation quality.
6 Conclusion and Further Work
In this paper we have presented a nonparametric
Bayesian model for learning SCFGs directly
from parallel corpora. We have also introduced
a novel Gibbs sampller that allows for efficient
posterior inference. We show state-of-the-art
results and learn complex translation phenomena,
including discontiguous and many-to-many
phrasal alignments, without applying any heuristic
restrictions on the model to make learning tractable.
Our evaluation shows that we can use a principled
approach to induce SCFGs designed specifically
to utilize the full power of grammar based SMT
instead of relying on complex word alignment
heuristics with inherent bias.
Future work includes the obvious extension to
learning SCFGs that contain multiple nonterminals
instead of a single nonterminal grammar. We also
expect that expanding our sampler beyond strict
binary sampling may allow us to explore the space
of hierarchical word alignments more quickly
allowing for faster mixing. We expect with these
extensions our model of grammar induction may
further improve translation output.
Acknowledgements
This work was supported by a grant from Google,
Inc. and EPRSRC grant no. EP/I010858/1 (Leven-
berg and Blunsom), the U. S. Army Research Lab-
oratory and U. S. Army Research Office under con-
tract/grant no. W911NF-10-1-0533 (Dyer).
References
P. Blunsom and T. Cohn. 2011. A hierarchical pitman-
yor process hmm for unsupervised part of speech
induction. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 865?874, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
P. Blunsom, T. Cohn, C. Dyer, and M. Osborne. 2009.
A gibbs sampler for phrasal synchronous grammar
induction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the AFNLP, pages 782?790, Suntec, Singa-
pore, August. Association for Computational Linguis-
tics.
P. F. Brown, V. J. D. Pietra, R. L. Mercer, S. A. D. Pietra,
and J. C. Lai. 1992. An estimate of an upper bound
for the entropy of english. Computational Linguistics,
18(1):31?40.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
C. Cherry and D. Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
231
In Proceedings of SSST, NAACL-HLT 2007 / AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation, pages 17?24, Rochester, New York, April.
Association for Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 314?323, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proceedings of the ACL 2010 System
Demonstrations, ACLDemos ?10, pages 7?12.
C. Dyer. 2010. Two monolingual parses are better than
one (synchronous parse). In Proc. of NAACL.
M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004.
What?s in a translation rule? In D. M. Susan Dumais
and S. Roukos, editors, HLT-NAACL 2004: Main
Proceedings, pages 273?280, Boston, Massachusetts,
USA, May 2 - May 7. Association for Computational
Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computational
Linguistics, Syndney, Australia.
D. Graff. 2003. English Gigaword. Linguistic Data Con-
sortium (LDC-2003T05).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceedings
of the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
P. Koehn. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
L. Lee. 2002. Fast context-free grammar parsing
requires fast Boolean matrix multiplication. Journal
of the ACM, 49(1):1?15.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15:465?488, July.
A. Lopez. 2008. Machine Translation by Pattern Match-
ing. Ph.D. thesis, University of Maryland.
D. Marcu and D. Wong. 2002. A phrase-based,joint
probability model for statistical machine translation.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing, pages 133?
139. Association for Computational Linguistics, July.
R. Neal. 2000. Slice sampling. Annals of Statistics,
31:705?767.
G. Neubig, T. Watanabe, E. Sumita, S. Mori, and
T. Kawahara. 2011. An unsupervised model for joint
phrase alignment and extraction. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 632?641, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
D. Newman, A. Asuncion, P. Smyth, and M. Welling.
2007. Distributed inference for latent dirichlet al
cation. In NIPS. MIT Press.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In ACL ?02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 311?318, Morristown, NJ, USA.
Association for Computational Linguistics.
J. Pitman and M. Yor. 1997. The two-parameter Poisson-
Dirichlet distribution derived from a stable subordina-
tor. Ann. Probab., 25:855?900.
Y. W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages
985?992.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proceed-
ings of the 16th conference on Computational linguis-
tics, pages 836?841, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
F. Wood, J. Gasthaus, C. Archambeau, L. James, and
Y. W. Teh. 2011. The sequence memoizer. Commu-
nications of the Association for Computing Machines,
54(2):91?98.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23:377?403, September.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL-08:
HLT, pages 97?105, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
232
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 73?84,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Paraphrasing 4 Microblog Normalization
Wang Ling Chris Dyer Alan W Black Isabel Trancoso
L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
Compared to the edited genres that have
played a central role in NLP research, mi-
croblog texts use a more informal register with
nonstandard lexical items, abbreviations, and
free orthographic variation. When confronted
with such input, conventional text analysis
tools often perform poorly. Normalization
? replacing orthographically or lexically id-
iosyncratic forms with more standard variants
? can improve performance. We propose a
method for learning normalization rules from
machine translations of a parallel corpus of
microblog messages. To validate the utility of
our approach, we evaluate extrinsically, show-
ing that normalizing English tweets and then
translating improves translation quality (com-
pared to translating unnormalized text) using
three standard web translation services as well
as a phrase-based translation system trained
on parallel microblog data.
1 Introduction
Microblogs such as Twitter, Sina Weibo (a popular
Chinese microblog service) and Facebook have re-
ceived increasing attention in diverse research com-
munities (Han and Baldwin, 2011; Hawn, 2009, in-
ter alia). In contrast to traditional text domains that
use carefully controlled, standardized language, mi-
croblog content is often informal, with less adher-
ence to conventions regarding punctuation, spelling,
and style, and with a higher proportion of dialect
or pronouciation-derived orthography. While this
diversity itself is an important resource for study-
ing, e.g., sociolinguistic variation (Eisenstein et al,
2011; Eisenstein, 2013), it poses challenges to NLP
applications developed for more formal domains. If
retaining variation due to sociolinguistic or phono-
logical factors is not crucial, text normalization can
improve performance on downstream tasks (?2).
This paper introduces a data-driven approach to
learning normalization rules by conceiving of nor-
malization as a kind of paraphrasing and taking
inspiration from the bilingual pivot approach to
paraphrase detection (Bannard and Callison-Burch,
2005) and the observation that translation is an
inherently ?simplifying? process (Laviosa, 1998;
Volansky et al, 2013). Starting from a parallel cor-
pus of microblog messages consisting of English
paired with several other languages (Ling et al,
2013), we use standard web machine translation sys-
tems to re-translate the non-English segment, pro-
ducing ?English original,English MT? pairs (?3).
These are our normalization examples, with MT out-
put playing the role of normalized English. Sev-
eral techniques for identifying high-precision nor-
malization rules are proposed, and we introduce a
character-based normalization model to account for
predictable character-level processes, like repetition
and substitution (?4). We then describe our decod-
ing procedure (?5) and show that our normaliza-
tion model improve translation quality for English?
Chinese microblog translation (?6).1
2 Why Normalize?
Consider the English tweet shown in the first row of
Table 1 which contains several elements that NLP
1The datasets used in this paper are available from http:
//www.cs.cmu.edu/?lingwang/microtopia.
73
Table 1: Translations of an English microblog message
into Mandarin, using three web translation services.
orig. To DanielVeuleman yea iknw imma work on that
MT1 ?iknw DanielVeuleman?????
MT2 DanielVeuleman?iknw???????
MT3 ?DanielVeuleman??iknw imma??????
systems trained on edited domains may not handle
well. First, it contains several nonstandard abbre-
viations, such as, yea, iknw and imma (abbrevia-
tions of yes, I know and I am going to). Second,
there is no punctuation in the text although stan-
dard convention would dictate that it should be used.
To illustrate the effect this can have, consider now
the translations produced by Google Translate,2 Mi-
crosoft Bing,3 and Youdao,4 shown in rows 2?4.
Even with no knowledge of Chinese, it is not hard
to see that all engines have produced poor transla-
tions: the abbreviation iknw is left translated by all
engines, and imma is variously deleted, left untrans-
lated, or transliterated into the meaningless sequence
?? (pronounced y?? ma?).
While normalization to a form like To Daniel
Veuleman: Yes, I know. I am going to work on that.
does indeed lose some information (information im-
portant for an analysis of sociolinguistic or phono-
logical variation clearly goes missing), it expresses
the propositional content of the original in a form
that is more amenable to processing by traditional
tools. Translating the normalized form with Google
Translate produces ????Veuleman?????
???????????, which is a substantial
improvement over all translations in Table 1.
3 Obtaining Normalization Examples
We want to treat normalization as a supervised learn-
ing problem akin to machine translation, and to do
so, we need to obtain pairs of microblog posts and
their normalized forms. While it would be possible
to ask annotators to create such a corpus, it would
be quite expensive to obtain large numbers of ex-
amples. In this section, we propose a method for
creating normalization examples without any human
2http://translate.google.com/
3http://www.bing.com/translator
4http://fanyi.youdao.com/
Table 2: Translations of Chinese original post to English
using web-based service.
orig. To DanielVeuleman yea iknw imma work on that
orig. ?DanielVeuleman?????????
?????????
MT1 Right DanielVeuleman say, yes, I know, I?m
Xiangna efforts
MT2 DanielVeuleman said, Yes, I know, I?m that hard
MT3 Said to DanielVeuleman, yes, I know, I?m to
that effort
annotation, by leveraging existing tools and data re-
sources.
The English example sentence in Table 1 was se-
lected from the ?topia parallel corpus (Ling et
al., 2013), which consists of self-translated mes-
sages from Twitter and Sina Weibo (i.e., each mes-
sage contains a translation of itself). Row 2 of
Table 2 shows the Mandarin self-translation from
the corpus. The key observation is what happens
when we automatically translate the Mandarin ver-
sion back into English. Rows 3?5 shows automatic
translations from three standard web MT engines.
While not perfect, the translations contain several
correctly normalized subphrases. We will use such
re-translations as a source of (noisy) normalization
examples. Since such self-translations are relatively
numerous on microblogs, this technique can provide
a large amount of data.
Of course, to motivate this paper, we argued that
NLP tools ? like the very translation systems we
propose to use ? often fail on unnormalized input.
Is this a problem? We argue that it is not for the
following two reasons.
Normalization in translation. Work in transla-
tion studies has observed that translation tends to
be a generalizing process that ?smooths out? author-
and work-specific idiosyncrasies (Laviosa, 1998;
Volansky et al, 2013). Assuming this observa-
tion is robust, we expect that dialectal variant forms
found in microblogs to be normalized in translation.
Therefore, if the parallel segments in our microblog
parallel corpus did indeed originate through a trans-
lation process (rather than, e.g., being generated as
two independent utterances from a bilingual), we
may then state the following assumption about the
distribution of variant forms in a parallel segment
74
?e, f?: if e contains nonstandard lexical variants,
then f is likely to be a normalized translation using
with fewer nonstandard lexical variants (and vice-
versa).
Uncorrelated orthographic variants. Any writ-
ten language has the potential to make creative use
of orthography: alphabetic scripts can render ap-
proximations of pronunciation variants; logographic
scripts can use homophonic substitutions. However,
the kinds of innovations used in particular languages
will be language specific (depending on details of
the phonology, lexicon, and orthography of the lan-
guage). However, for language pairs that differ sub-
stantially in these dimensions, it may not always
be possible (or at least easy) to preserve particular
kinds of nonstandard orthographic forms in trans-
lation. Consider the (relatively common) pronoun-
verb compounds like iknw and imma from our mo-
tivating example: since Chinese uses a logographic
script without spaces, there is no obvious equivalent.
3.1 Variant?Normalized Parallel Corpus
For the two reasons outlined above, we argue that
we will be able to translate back into English us-
ing MT, even when the underlying English part of
the parallel corpus has a great deal of nonstandard
content. We leverage this fact to build the normal-
ization corpus, where the original English tweet is
treated as the variant form, and the automatic trans-
lation obtained from another language is considered
a potential normalization.5
Our process is as follows. The microblog cor-
pus of Ling et al (2013) contains sentence pairs ex-
tracted from Twitter and Sina Weibo, for multiple
language pairs. We use all corpora that include En-
glish as one of the languages in the pair. The respec-
tive non-English side is translated into English using
different translation engines. The different sets we
used and the engines we used to translate are shown
in Table 3. Thus, for each original English post o,
we obtain n paraphrases {pi}
n
i=1, from n different
translation engines.
5We additionally assume that the translation engines are
trained to output more standardized data, so there will be addi-
tional normalizing effect from the machine translation system.
Table 3: Corpora Used for Paraphrasing.
Lang. Pair Source Segs. MT Engines
ZH-EN Weibo 800K Google, Bing, Youdao
ZH-EN Twitter 113K Google, Bing, Youdao
AR-EN Twitter 114K Google, Bing
RU-EN Twitter 119K Google, Bing
KO-EN Twitter 78K Google, Bing
JA-EN Twitter 75K Google, Bing
3.2 Alignment and Filtering
Our parallel microblog corpus was crawled automat-
ically and contains many misaligned sentences. To
improve precision, we attempt to find the similar-
ity between the (unnormalized) original and each
of the normalizations using an alignment based on
the one used in METEOR (Denkowski and Lavie,
2011), which computes the best alignment between
the original tweet and each of the normalizations
but modified to permit domain-specific approximate
matches. To address lexical variants, we allow fuzzy
word matching, that is, we allow lexically similar,
such as yea and yes to be aligned (similarity is de-
termined by the Levenshtein distance). We also per-
form phrasal matchings, such as ikwn to i know. To
do so, we extend the alignment algorithm from word
to phrasal alignments. More precisely, given the
original post o and a candidate normalization n, we
wish to find the optimal segmentation producing a
good alignment. A segmentation s = ?s1, . . . , s|s|?
is a sequence of segments that aligns as a block to a
source word. For instance, for the sentence yea iknw
imma work on that, one possible segmentation could
be s1 =yea ikwn, s2 =imma and s3 =work on that.
Model. We define the score of an alignment a and
segmentation s in using a model that makes semi-
Markov independence assumptions, similar to the
work in (Bansal et al, 2011), u(a, s | o,n) =
|s|?
i=1
[
ue(si, ai | n)? ut(ai | ai?1)? u`(|si|)
]
In this model, the maximal scoring segmentation
and alignment can be found using a polynomial time
dynamic programming algorithm. Each segment
can be aligned to any word or segment in o. The
aligned segment for sk is defined as ak. For the
75
score of a segment correspondence ue(s, a | n), we
assume that this can be estimated using the lexical
similarity between segments, which we define to be
1? L(sk,ak)max{|sk|,|ak|} , where L(x, y) denotes the Leven-
shtein distance between strings x and y, normalized
by the highest possible distance between those seg-
ments.
For the alignment score ut, we assume that the
relative order of the two sequences will be mostly
monotonous. Thus, we approximate ut with the fol-
lowing density poss(ak) ? pose(ak?1) ? N (1, 1),
where the poss is the index of the first word in the
segment and pose the one of the last word.
After finding the Viterbi alignments, we compute
the similarity measure ? = |A||A|+|U | , used in (Resnik
and Smith, 2003), where |A| and |U | are the number
of words that were aligned and unaligned, respec-
tively. In this work, we extract the pair if ? > 0.2.
4 Normalization Model
From the normalization corpus, we learn a nor-
malization model that generalizes the normalization
process. That is, from the data we observe that To
DanielVeuleman yea iknw imma work on that is nor-
malized to To Daniel Veuleman: yes, I know. I
am going to work on that. However, this is not
useful, since the chances of the exact sentence To
DanielVeuleman yea iknw imma work on that occur-
ring in the data is low. We wish to learn a process to
convert the original tweet into the normalized form.
There are two mechanisms that we use in our
model. The first (?4.1) learns word?word and
phrase?phrase mappings. That is, we wish to find
that DanielVeuleman is normalized to Daniel Veule-
man, that iknw is normalized to I know and that
imma is normalized to I am going. These mappings
are more useful, since whenever iknw occurs in the
data, we have the option to normalize it to I know.
The second (?4.2) learns character sequence map-
pings. If we look at the normalization DanielVeule-
man to Daniel Veuleman, we can see that it is only
applicable when the exact word DanielVeuleman oc-
curs. However, we wish to learn that it is uncom-
mon for the letters l and v to occur in the same word
sequentially, so that be can add missing spaces in
words that contain the lv character sequence, such as
normalizing phenomenalvoter to phenomenal voter.
I wanna go 4 pizza 2day
I want go for pizza todayto
Figure 1: Variant?normalized alignment with the variant
form above and the normalized form below; solid lines
show potential normalizations, while dashed lines repre-
sent identical translations.
However, there are also cases where this is not true,
for instance, in the word velvet, we do not wish to
separate the letters l and v. Thus, we shall describe
the process we use to decide when to apply these
transformations.
4.1 From Sentences To Phrases
The process to find phrases from sentences has been
throughly studied in Machine Translation. This is
generally done in two steps, Word Alignments and
Phrase Extraction.
Alignment. The first step is to find the word-level
alignments between the original post and its nor-
malization. This is a well studied problem in MT,
referred as Word Alignment (Brown et al, 1993).
Many alignment models have been proposed, such
as, the HMM-based word alignment models (Vo-
gel et al, 1996) and the IBM models (Och and
Ney, 2003). Generally, a symmetrization step is per-
formed, where the bidirectional alignments are com-
bined heuristically. In our work, we use the fast
aligner proposed in (Dyer et al, 2013) to obtain the
word alignments. Figure 1 shows an example of an
word aligned pair of a tweet and its normalization.
Phrase Extraction. The phrasal extraction
step (Ling et al, 2010), uses the word aligned
sentences and extracts phrasal mappings between
the original tweet and its normalization, named
phrase pairs. For instance, in Figure 1, we would
like to extract the phrasal mapping from go 4 to go
for, so that we learn that the word 4 in the context of
go is normalized to the proposition for. To do this,
the most common approach is to use the template
proposed in (Och and Ney, 2004), which allows
phrase pairs to be extracted, if there is at least one
word alignment within the pair, and there are no
76
Table 4: Fragment of the phrase normalization model
built, for each original phrase o, we present the top-3 nor-
malized forms ranked by f(n | o).
Original (o) Normalization (n) f(n | o)
wanna want to 0.4679
wanna will 0.0274
wanna going to 0.0114
4 4 0.5641
4 for 0.01795
go 4 go for 1.0000
words inside the pair that are aligned to words not
in the pair. For instance, in the example above, the
phrase pair that normalizes wanna to want to would
be extracted, but the phrase pair normalizing wanna
to want to go would not, because the word go in the
normalization is aligned to a word not in the pair.
Phrasal Features. After extracting the phrase
pairs, a model is produced with features derived
from phrase pair occurrences during extraction. This
model is equivalent to phrasal translation model in
MT, but we shall refer to it as the normalization
model. For a phrase pair ?o,n?, where o is the origi-
nal phrase, and n is the normalized phrase, we com-
pute the normalization relative frequency f(n | o) =
C(n,o)
C(o) , where C(n, o) denotes the number of times
o was normalized to n and C(o) denotes the number
of times o was seen in the extracted phrase pairs. Ta-
ble 4 gives a fragment of the normalization model.
The columns represent the original phrase, its nor-
malization and the probability, respectively.
In Table 4, we observe that the abbreviation
wanna is normalized to want to with a relatively
high probability, but it can also be normalized to
other equivalent expressions, such as will and go-
ing to. The word 4 by itself has a low probability
to be normalized to the preposition for. This is ex-
pected, since this decision cannot be made without
context. However, we see that the phrase go 4 is
normalized to go for with a high probability, which
specifies that within the context of go, 4 is generally
used as a preposition.
4.2 From Phrases to Characters
While we can learn lexical variants that are in the
corpora using the phrase model, we can only address
word forms that have been observed in the corpora.
Table 5: Fragment of the character normalization model
where examples representative of the lexical variant gen-
eration process are encoded in the model.
Original (o) Normalization (n) f(n | o)
o o o o o 0.0223
o o o o 0.0439
s c 0.0331
z s 0.0741
s h c h 0.019
2 t o 0.014
4 f o r 0.0013
0 o 0.0657
i n g f o r i n g <space> f o r 0.4545
g f g <space> f 0.01028
This is quite limited, since we cannot expect all the
word forms to be present, such as all the possible
orthographic errors for the word cat, such as catt,
kat and caaaat. Thus, we will build a character-
based model that learns the process lexical variants
are generated at the subword level.
Our character-based model is similar to the
phrase-based model, except that, rather than learn-
ing word-based mappings from the original tweet
and the normalization sentences, we learn character-
based mappings from the original phrases to the nor-
malizations of those phrases. Thus, we extract the
phrase pairs in the phrasal normalization model, and
use them as a training corpora. To do this, for each
phrase pair, we add a start token, <start>, and a
end token, <end>, at the beginning and ending of
the phrase pair. Afterwards, we separate all charac-
ters by space and add a space token <space> where
spaces were originally. For instance, the phrase
pair normalizing DanielVeuleman to Daniel Veule-
man would be converted to <start> d a n i e l v e u
l e m a n <end> and <start> d a n i e l <space> v
e u l e m a n <end>.
Character-based Normalization Model - To
build the character-based model, we proceed using
the same approach as in the phrasal normalization
model. We first align characters using Word Align-
ment Models, and then we perform phrase extrac-
tion to retrieve the phrasal character segments, and
build the character-based model by collecting statis-
tics. Once again, we provide examples of entries in
the model in Table 5.
77
We observe that many of the normalizations dealt
with in the previous model by memorizing phrases
are captured with string transformations. For in-
stance, from phrase pairs such as tooo to too and
sooo to so, we learn that sequences of o?s can be
reduced to 2 or 1 o. Other examples include or-
thographic substitutions, such as 2 for to and 4
for for (as found in 2gether, 2morrow, 4ever and
4get). Moreover, orthographic errors can be gener-
ated from mistaking characters with similar phonetic
properties, such as, s to c, z to s and sh to ch, gener-
ating lexical variants such as reprecenting. Finally,
we learn that the number 0 that resembles the letter
o, can be used as a replacement, as in g00d. Finally,
we can see that the rule ingfor to ing for attempts to
find segmentation errors, such as goingfor, where a
space between going and for was omitted.6
5 Normalization Decoder
In section 4, we built two models to learn the process
of normalization, the phrase-based model and the
character-based model. In this section, we describe
the decoder we used to normalize the sentences.
The advantage of the phrase-based model is that it
can make decisions for normalization based on con-
text. That is, it contains phrasal units, such as, go
4, that determine, when the word 4 should be nor-
malized to the preposition for and when to leave it
as a number. However, it cannot address words that
are unseen in the corpora. For instance, if the word
form 4ever is not seen in the training corpora, it is
not be able to normalize it, even if it has seen the
word 4get normalized to forget. On the other hand,
the character-based model learns subword normal-
izations, for instance, if we see the word nnnnno
normalized to no, we can learn that repetitions of
the letter n are generally shorted to n, which al-
lows it to generate new word forms. This model
has strong generalization potential, but the weak-
ness of the character-based model is that it fails to
6Note that this captures the context in which such transfor-
mations are likely to occur: there are not many words that con-
tain the sequence ingfor, so the probability that these should be
normalized by inserting a space is high. On the other hand, we
cannot assume that if we observe the sequence gf, we can safely
separate these with a space. This is because, there are many
words that contain this sequence, such as the abbreviation of
gf (girlfriend), dogfight, and bigfoot.
consider the context of the normalization that the
phrase-based model uses to make normalization de-
cisions. Thus, our goal in this section is describe a
decoder that uses both models to improve the quality
of the normalizations.
5.1 Phrasal Decoder
We use Moses, an off-the-shelf phrase-based MT
system (Koehn et al, 2007), to ?translate? the orig-
inal tweet its normalized form using the phrasal
model (?4.1). Aside form the normalization prob-
ability, we also use the common features used in
MT. These are the reverse normalization probabil-
ity, the lexical and reverse lexical probabilities and
the phrase penalty. We also use the MSD reorder-
ing model proposed in (Koehn et al, 2005), which
adds reordering features.7 The final score of each
phrase pair is given as a sum of weighted log fea-
tures. The weights for these features are optimized
using MERT (Och, 2003). In our work, we sampled
150 tweets randomly from Twitter and normalized
them manually, and used these samples as devel-
opment data for MERT. As for the character-based
model features, we simply rank the training phrase
pairs by their relative frequency the f(n | o), and use
the top-1000 phrase pairs as development set. Fi-
nally, a language model is required during decoding
as a prior, since it defines the type of language that
is produced by the output. We wish to normalized
to formal language, which is generally better pro-
cessed by NLP tools. Thus, for the phrase model,
we use the English NIST dataset composed of 8M
sentences in English from the news domain to build
a 5-gram Kneser-Ney smoothed language model.
5.2 Character and Phrasal Decoder
We now turn to how to apply the character-based
(?4.2), together with the phrasal model. For this
model, we again use Moses, treating each charac-
ter as a ?word?. The simplest way to combine both
methods is first to decode the input o sentence with
the character-based decoder, normalizing each word
independently and then normalizing the resulting
output using the phrase-based decoder, which en-
ables the phrase model to score the outputs of the
character model in context.
7Reordering helps find lexical variants that are generated by
transposing characters, such as, mabye to maybe.
78
0 1 2 3 4 5 6
I
wanna
want to meeeeet
meet
met
DanielVeuleman
Daniel Veuleman
Figure 2: Example output lattice of the character-based decoder, for the sentence I wanna meeeeet DanielVeuleman.
Our process is as follows. Given the input sen-
tence o, with the words o1, . . . , om, where m is
the number of words in the input, we generate for
each word oi a list of n-best normalization candi-
dates z1oi , . . . , z
n
oi . We further filter the candidates
using two criteria. We start by filtering each can-
didate zjoi that occurs less frequently than the orig-
inal word oi. This is motivated by our observation
that lexical variants occur far less than the respec-
tive standard form. Second, we build a corpus of
English language Twitter consisting of 70M tweets,
extract the unigram counts, and perform Brown clus-
tering (Brown et al, 1992) with k = 3000 clusters.
Next, we calculate the cluster similarity between oi
and each surviving candidate, zjoi . We filter the can-
didate if the similarity is less than 0.8. The similar-
ity between two clusters represented as bit strings,
S[c(oi), c(z
j
oi)], calculated as:
S(x, y) =
2 ? |lpm{x, y)}|
|x|+ |y|
,
where lpm computes the longest common prefix of
the contexts and |x| is the length of the bit string.8
If a candidate contains more than one word (because
a space was inserted), we set its count as the mini-
mum count among its words. To find the cluster for
multiple word units, we concatenate the words to-
gether, and find the cluster with the resulting word if
it exists. This is motivated by the fact that it is com-
mon for missing spaces to exist in microblog cor-
pora, generating new word forms, such as wantto,
goingfor, and given a large enough corpora as the
one we used, these errors occur frequently enough to
be placed in the correct cluster. In fact, the variants
such as wanna and tmi, occur in the same clusters as
the words wantto and toomuchinformation.
Remaining candidates are combined into a word
lattice, enabling us to perform lattice-based decod-
8Brown clusters are organized such that more words with
more similar distributions share common prefixes.
ing with the phrasal model (Dyer et al, 2008). Fig-
ure 2, provides an example of such a lattice for the
variant sentence I wanna meeeet DanielVeuleman.
5.3 Learning Variants from Monolingual Data
Until now, we learned normalizations from pairs of
original tweets and their normalizations. We shall
now describe a process to leverage monolingual doc-
uments to learn new normalizations, since the mono-
lingual data is far easier to obtain than parallel data.
This process is similar to the work in (Han et al,
2012), where confusion sets of contextually simi-
lar words are built initially as potential normaliza-
tion candidates. We again use the k = 3000 Brown
clusters,9 and this time consider the contents of each
cluster as a set of possible normalization variants.
For instance, we find that the cluster that includes the
word never, also includes the variant forms neverrrr,
neva and nevahhh. However, the cluster also con-
tains non-variant forms, such as gladly and glady.
Thus, we want to find that neverrrr maps to never,
while glady maps to gladly in the same cluster. Our
work differs from previous work in that, rather than
defining features manually, we use our character-
based decoder to find the mappings between lexical
variants and their normalizations.
For every word type wi in cluster c(wi) =
{w1, . . . , wn}, we generate a set of possible candi-
dates for each word w1i , . . . , w
m
i . Then, we build
a directed acyclic graph (DAG), where every word.
We add an edge between wi and wj , if wi can be
decoded into wj using the character model from the
previous section, and also if wi occurs less than wj ;
the second condition guarantees that the graph will
be acyclic. Sample graphs are shown in Figure 3.
Afterwards, we find the number of paths between
all nodes in the graph (this can be computed effi-
ciently in O(|V | + |E|) time). Then, for each word
9The Brown clustering algorithm groups words together
based on contextual similarity.
79
neverr
neva neve
nevar
never
glady
gladly
cladly
Figure 3: Example DAGs, built from the cluster contain-
ing the words never and gladly.
wi, we find the wj to which it has the highest num-
ber of paths to and extract the normalization of wi
to wj . In case of a tie, we choose the word wj that
occurs more often in the monolingual corpora. This
is motivated by the fact that normalizations are tran-
sitive. Thus, even if neva cannot be decoded directly
to never, we can use nevar as an intermediate step to
find the correct normalization. This is performed for
all the clusters, and the resulting dictionary of lexi-
cal variants mapped to their standard forms is added
to the training data of the character-based model.
6 Experiments
We evaluate our normalization model intrinsically
by testing whether our normalizations more closely
resemble standardized data, and then extrinsically
by testing whether we can improve the translation
quality of in-house as well as online Machine Trans-
lation systems by normalizing the input.
6.1 Setup
We use the gold standard by Ling et al (2013), com-
posed by 2581 English-Mandarin microblog sen-
tence pairs. From this set, we randomly select 1290
pairs for development and 1291 pairs for testing.
The normalizer model is trained on the corpora
extracted and filtered in section 3, in total, there
were 1.3M normalization pairs used during training.
The test sentences are normalized using four differ-
ent setups. The first setup leaves the input sentence
unchanged, which we call No Norm. The second
uses the phrase-based model to normalize the input
sentence, which we will denote Norm+phrase. The
third uses the character-based model to output lat-
tices, and then decodes with the phrase based model,
which we will denote Norm+phrase+char. Finally,
we test the same model after adding the training data
extracted using monolingual documents, which we
will refer as Norm+phrase+char+mono.
To test the normalizations themselves, we used
Google Translate to translate the Mandarin side of
the 1291 test sentence pairs back to English and use
the original English tweet. While, this is by itself
does not guarantee that the normalizations are cor-
rect, since the normalizations could be syntactically
and semantically incorrect, it will allow us to check
whether the normalizations are closer to those pro-
duced by systems trained on news data. This exper-
iment will be called Norm.
As an application and extrinsic evaluation for our
normalizer, we test if we can obtain gains on the
MT task on microblog data by using our normalizer
prior to translation. We build two MT systems us-
ing Moses. Firstly, we build a out-of-domain model
using the full 2012 NIST Chinese-English dataset
(approximately 8M sentence pairs), which is dataset
from the news domain, and we will denote this sys-
tem as Inhouse+News. Secondly, we build a in-
domain model using the 800K sentence pairs from
?topia corpora (Ling et al, 2013). We also add
the NIST dataset to improve coverage. We call this
system Inhouse+News+Weibo. To train these sys-
tems, we use the Moses phrase-based MT system
with standard features (Koehn et al, 2003). For re-
ordering, we use the MSD reordering model (Axel-
rod et al, 2005). As the language model, we train
a 5-gram model with Kneser-ney smoothing using a
10M tweets from twitter. Finally, the weights were
tuned using MERT (Och, 2003). As for online sys-
tems, we consider the systems used to generate the
paraphrase corpora in section 3, which we will de-
note as Online A, Online B and Online C10
The normalization and MT results are evaluated
with BLEU-4 (Papineni et al, 2002) comparing the
produced translations or normalizations with the ap-
propriate reference.
6.2 Results
Results are shown in Table 6. In terms of the normal-
izations, we observe a much better match between
10The names of the systems are hidden to not violate the pri-
vacy issues in the terms and conditions of these online systems.
80
Table 6: Normalization and MT Results. Rows denote different normalizations, and columns different translation
systems, except the first column (Norm), which denotes the normalization experiment. Cells display the BLEU score
of that experiment.
Moses Moses
Condition Norm (News) (News+Weibo) Online A Online B Online C
baseline 19.90 15.10 24.37 20.09 17.89 18.79
norm+phrase 21.96 15.69 24.29 20.50 18.13 18.93
norm+phrase+char 22.39 15.87 24.40 20.61 18.22 19.08
norm+phrase+char+mono 22.91 15.94 24.46 20.78 18.37 19.21
the normalized text with the reference, than the orig-
inal tweets. In most cases, adding character-based
models improves the quality of the normalizations.
We observe that better normalizations tend to lead
to better translations. The relative improvements
are most significant, when moving from No Norm
to norm+phrase normalization. This is because,
we are normalizing words that are not seen in gen-
eral MT system?s training data, but occur frequently
in microblog data, such as wanna to want to, u to
you and im to i?m. The only exception is in the In-
house+News+Weibo system, where the normaliza-
tion deteriorates the results. This is to be expected,
since this system is trained on the same microblog
data used to learn the normalizations. However, we
can observe on norm+phrase+char that if we add
the character-based model, we can observe improve-
ments for this system as well as for all other ones.
This is because the model is actually learning nor-
malizations that are unseen in the data. Some ex-
amples of these normalization include, normalizing
lookin to looking, nutz to nuts and maimi to miami
but also separating peaceof to peace of. The fact
that these improvements are obtained for all sys-
tems is strong evidence that we are actually produc-
ing good normalizations, and not overfitting to one
of the systems that we used to generate our data.
The gains are much smaller from norm+phrase
to norm+phrase+char, since the improvements we
obtain come from normalizing less frequent words.
Finally, we can obtain another small improvement
by adding monolingual data to the character-based
model in norm+phrase+char+mono.
7 Related Work
Most of the work in microblog normalization is fo-
cused on finding the standard forms of lexical vari-
ants (Yang and Eisenstein, 2013; Han et al, 2013;
Han et al, 2012; Kaufmann, 2010; Han and Bald-
win, 2011; Gouws et al, 2011; Aw et al, 2006). A
lexical variant is a variation of a standard word in
a different lexical form. This ranges from minor or
major spelling errors, such as jst, juxt and jus that
are lexical variants of just, to abbreviations, such as
tmi and wanna, which stand for too much informa-
tion and want to, respectively. Jargon can also be
treated as variants, for instance cday is a slang word
for birthday, in some groups.
There are many rules that govern the process lex-
ical variants are generated. Some variants are gener-
ated from orthographic errors, caused by some mis-
take from the user when writing. For instance, the
variants representin, representting, or reprecenting
can be generated by a spurious letter swap, insertion
or substitution by the user. One way to normalize
these types of errors is to attempt to insert, remove
and swap words in a lexical variant until a word in
a dictionary of standard words is found (Kaufmann,
2010). Contextual features are another way to find
lexical variants, since variants generally occur in the
same context as their standard form. This includes
orthographic errors, abbreviations and slang. How-
ever, this is generally not enough to detect lexical
variants, as many words share similar contexts, such
as already, recently and normally. Consequently,
contextual features are generally used to generate a
confusion set of possible normalizations of a lexical
variant, and then more features are used to find the
correct normalization (Han et al, 2012). One simple
approach is to compute the Levenshtein distance to
find lexical similarities between words, which would
effectively capture the mappings between represent-
ting, reprecenting and representin to representing.
However, a pronunciation model (Tang et al, 2012)
81
would be needed to find the mapping between g8,
2day and 4ever to great, today and forever, respec-
tively. Moreover, visual character similarity features
would be required to find the mapping between g00d
and? to good and i.
Clearly, learning this process is a challenging
task, and addressing each different case individually
would require vast amounts of resources. Further-
more, once we change the language to normalize
to another language, the types of rules that generate
lexical variants would radically change and a new set
of features would have to be engineered. We believe
that to be successful in normalizing microblogs,
the process to learn new lexical variants should be
learned from data, making as few assumptions as
possible. We learn our models without using any
type of predefined features, such as phonetic fea-
tures or lexical features. In fact, we will not assume
that most words and characters map to themselves,
as it is assumed in methods using the Levenshtein
distance (Kaufmann, 2010; Han et al, 2012; Wang
and Ng, 2013). All these mappings are learned from
our data. Furthermore, in the work above, the dictio-
naries built using these methods assume that lexical
variants are mapped to standard forms in a word-to-
word mapping. Thus, variants such as wanna, gonna
and imma are not normalizable, since they are nor-
malized to multiple words want to, going to and I
am gonna. Moreover, there are segmentation errors
that occur from missing spaces, such as sortof and
goingfor, which also map to more than one word to
sort of and going for. These cases shall also be ad-
dressed in our work.
Wang and Ng (2013) argue that microblog nor-
malization is not simply to map lexical variants into
standard forms, but that other tasks, such as punctua-
tion correction and missing word recovery should be
performed. Consider the example tweet you free?,
while there are no lexical variants in this message,
the authors consider that it is the normalizer should
recover the missing article are and normalize this
tweet to are you free?. To do this, the authors train a
series of models to detect and correct specific errors.
While effective for narrow domains, training models
to address each specific type of normalization is not
scalable over all types of normalizations that need to
be performed within the language, and the fact that a
set of new models must be implemented for another
language limits the applicability of this work.
Another strong point of the work above is that
a decoder is presented, while the work on build-
ing dictionaries only normalize out of vocabu-
lary (OOV) words. The work on (Han et al, 2012)
trains a classifier to decide whether to normalize a
word or not, but is still preconditioned on the fact
that the word in question is OOV. Thus, lexical vari-
ants, such as, 4 and u, with the standard forms for
and you, are left untreated, since they occur in other
contexts, such as u in u s a. Inspired by the work
above, we also propose a decoder based on the exist-
ing off-the-self decoder Moses (Koehn et al, 2007).
Finally, the work in (Xu et al, 2013) obtains para-
phrases from Twitter, by finding tweets that contain
common entities, such as Obama, that occur during
the same period by matching temporal expressions.
The resulting paraphrase corpora can also be used to
train a normalizer.
8 Conclusion
We introduced a data-driven approach to microblog
normalization based on paraphrasing. We build a
corpora of tweets and their normalizations using par-
allel corpora from microblogs using MT techniques.
Then, we build two models that learn generalizations
of the normalization process, one the phrase level
and on the character level. Then, we build a de-
coder that combines both models during decoding.
Improvements on multiple MT systems support the
validity of our method.
In future work, we shall attempt to build normal-
izations for other languages. We shall also attempt
to learn an unsupervised normalization model with
only monolingual data, similar to the work for MT
in (Ravi and Knight, 2011).
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT ?
Fundac?a?o para a Cie?ncia e a Tecnologia, under project
SFRH/BD/51157/2010. This work was supported by na-
tional funds through FCT ? Fundac?a?o para a Cie?ncia e a
Tecnologia, under project PEst-OE/EEI/LA0021/2013.
The authors also wish to express their gratitude to the
anonymous reviewers for their comments and insight.
82
References
[Aw et al2006] AiTi Aw, Min Zhang, Juan Xiao, and
Jian Su. 2006. A phrase-based statistical model for
SMS text normalization. In Proceedings of the ACL,
COLING-ACL ?06, pages 33?40, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David Tal-
bot. 2005. Edinburgh system description for the 2005
iwslt speech translation evaluation. In In Proc. Inter-
national Workshop on Spoken Language Translation
(IWSLT.
[Bannard and Callison-Burch2005] Colin Bannard and
Chris Callison-Burch. 2005. Paraphrasing with bilin-
gual parallel corpora. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 597?604, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
[Bansal et al2011] Mohit Bansal, Chris Quirk, and
Robert C. Moore. 2011. Gappy phrasal alignment by
agreement. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies - Volume 1, HLT ?11,
pages 1308?1317, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Brown et al1992] Peter F Brown, Peter V Desouza,
Robert L Mercer, Vincent J Della Pietra, and Jenifer C
Lai. 1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4):467?479.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Linguist.,
19:263?311, June.
[Denkowski and Lavie2011] Michael Denkowski and
Alon Lavie. 2011. Meteor 1.3: Automatic metric
for reliable optimization and evaluation of machine
translation systems. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
85?91, Edinburgh, Scotland, July. Association for
Computational Linguistics.
[Dyer et al2008] Chris Dyer, Smaranda Muresan, and
Philip Resnik. 2008. Generalizing word lattice trans-
lation. In Proceedings of HLT-ACL.
[Dyer et al2013] Chris Dyer, Victor Chahuneau, and
Noah A Smith. 2013. A simple, fast, and effective
reparameterization of ibm model 2. In Proceedings of
NAACL-HLT, pages 644?648.
[Eisenstein et al2011] Jacob Eisenstein, Noah A. Smith,
and Eric P. Xing. 2011. Discovering sociolinguis-
tic associations with structured sparsity. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 1365?1374,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
[Eisenstein2013] Jacob Eisenstein. 2013. What to do
about bad language on the internet. In Proceedings
of NAACL-HLT, pages 359?369.
[Gouws et al2011] Stephan Gouws, Dirk Hovy, and Don-
ald Metzler. 2011. Unsupervised mining of lexical
variants from noisy text. In Proceedings of the First
Workshop on Unsupervised Learning in NLP, EMNLP
?11, pages 82?90, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Han and Baldwin2011] Bo Han and Timothy Baldwin.
2011. Lexical normalisation of short text messages:
makn sens a #twitter. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 368?378, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Han et al2012] Bo Han, Paul Cook, and Timothy Bald-
win. 2012. Automatically constructing a normalisa-
tion dictionary for microblogs. In Proceedings of the
2012 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ?12, pages 421?
432, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
[Han et al2013] Bo Han, Paul Cook, and Timothy Bald-
win. 2013. Lexical normalization for social media
text. ACM Transactions on Intelligent Systems and
Technology (TIST), 4(1):5.
[Hawn2009] Carleen Hawn. 2009. Take two aspirin and
tweet me in the morning: how twitter, facebook, and
other social media are reshaping health care. Health
affairs, 28(2):361?368.
[Kaufmann2010] M. Kaufmann. 2010. Syntactic Nor-
malization of Twitter Messages. studies, 2.
[Koehn et al2003] Philipp Koehn, Franz Josef Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy - Volume 1, NAACL ?03, pages 48?54, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
[Koehn et al2005] Philipp Koehn, Amittai Axelrod,
Alexandra Birch Mayne, Chris Callison-Burch, Miles
Osborne, David Talbot, and Michael White. 2005.
Edinburgh system description for the 2005 nist mt
evaluation. In Proceedings of Machine Translation
Evaluation Workshop 2005.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-
dra Birch, Chris Callison-burch, Richard Zens, Rwth
83
Aachen, Alexandra Constantin, Marcello Federico,
Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade
Shen, Christine Moran, and Ondrej Bojar. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
[Laviosa1998] Sara Laviosa. 1998. Core patterns of
lexical use in a comparable corpus of English lexical
prose. Meta, 43(4):557?570.
[Ling et al2010] Wang Ling, Tiago Lu??s, Joa?o Grac?a,
Lu??sa Coheur, and Isabel Trancoso. 2010. Towards a
general and extensible phrase-extraction algorithm. In
IWSLT ?10: International Workshop on Spoken Lan-
guage Translation, pages 313?320, Paris, France.
[Ling et al2013] Wang Ling, Guang Xiang, Chris Dyer,
Alan Black, and Isabel Trancoso. 2013. Microblogs
as parallel corpora. In Proceedings of the 51st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?13. Association for Computational Lin-
guistics.
[Och and Ney2003] Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statis-
tical alignment models. Computational linguistics,
29(1):19?51.
[Och and Ney2004] Franz Josef Och and Hermann Ney.
2004. The alignment template approach to statistical
machine translation. Comput. Linguist., 30(4):417?
449, December.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine transla-
tion. In Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, ACL ?02,
pages 311?318, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Ravi and Knight2011] Sujith Ravi and Kevin Knight.
2011. Deciphering foreign language. In ACL, pages
12?21.
[Resnik and Smith2003] Philip Resnik and Noah A
Smith. 2003. The web as a parallel corpus. Com-
putational Linguistics, 29(3):349?380.
[Tang et al2012] Hao Tang, Joseph Keshet, and Karen
Livescu. 2012. Discriminative pronunciation mod-
eling: A large-margin, feature-rich approach. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 194?203. Association for Computational Lin-
guistics.
[Vogel et al1996] S. Vogel, H. Ney, and C. Tillmann.
1996. Hmm-based word alignment in statistical trans-
lation. In Proceedings of the 16th conference on Com-
putational linguistics-Volume 2, pages 836?841. Asso-
ciation for Computational Linguistics.
[Volansky et al2013] Vered Volansky, Noam Ordan, and
Shuly Wintner. 2013. On the features of transla-
tionese. Literary and Linguistic Computing.
[Wang and Ng2013] Pidong Wang and Hwee Ng. 2013.
A beam-search decoder for normalization of social
media text with application to machine translation. In
Proceedings of NAACL-HLT 2013, NAACL ?13. As-
sociation for Computational Linguistics.
[Xu et al2013] Wei Xu, Alan Ritter, and Ralph Grish-
man. 2013. Gathering and generating paraphrases
from twitter with application to normalization. In Pro-
ceedings of the Sixth Workshop on Building and Us-
ing Comparable Corpora, pages 121?128, Sofia, Bul-
garia, August. Association for Computational Linguis-
tics.
[Yang and Eisenstein2013] Yi Yang and Jacob Eisenstein.
2013. A log-linear model for unsupervised text nor-
malization. In Proc. of EMNLP.
84
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1100?1111,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Systematic Exploration of Diversity in Machine Translation
Kevin Gimpel? Dhruv Batra? Chris Dyer? Gregory Shakhnarovich?
?Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
?Virginia Tech, Blacksburg, VA 24061, USA
?Carnegie Mellon University, Pittsburgh, PA 15213, USA
Corresponding author: kgimpel@ttic.edu
Abstract
This paper addresses the problem of produc-
ing a diverse set of plausible translations. We
present a simple procedure that can be used
with any statistical machine translation (MT)
system. We explore three ways of using di-
verse translations: (1) system combination,
(2) discriminative reranking with rich features,
and (3) a novel post-editing scenario in which
multiple translations are presented to users.
We find that diversity can improve perfor-
mance on these tasks, especially for sentences
that are difficult for MT.
1 Introduction
From the perspective of user interaction, the ideal
machine translator is an agent that reads documents
in one language and produces accurate, high qual-
ity translations in another. This interaction ideal
has been implicit in machine translation (MT) re-
search since the field?s inception. It is the way
we interact with commercial MT services (such as
Google Translate and Microsoft Translator), and the
way MT systems are evaluated (Bojar et al, 2013).
Unfortunately, when a real, imperfect MT system
makes an error, the user is left trying to guess what
the original sentence means.
Multiple Hypotheses. In contrast, when we look
at the way other computer systems consume out-
put from MT systems (or similarly unreliable tools),
we see a different pattern. In a pipeline setting
it is commonplace to propagate not just a single-
best output but the M -best hypotheses (Venugopal
et al, 2008). Multiple solutions are also used for
reranking (Collins, 2000; Shen and Joshi, 2003;
Collins and Koo, 2005; Charniak and Johnson,
2005), tuning (Och, 2003), minimum Bayes risk de-
coding (Kumar and Byrne, 2004), and system com-
bination (Rosti et al, 2007). When dealing with
error-prone systems, knowing about alternatives has
benefits over relying on only a single output (Finkel
et al, 2006; Dyer, 2010).
Need for Diversity. Unfortunately, M -best lists are
a poor surrogate for structured output spaces (Finkel
et al, 2006; Huang, 2008). In MT, for exam-
ple, many translations on M -best lists are extremely
similar, often differing only by a single punctua-
tion mark or minor morphological variation. Re-
cent work has explored reasoning about sets using
packed representations such as lattices and hyper-
graphs (Macherey et al, 2008; Tromble et al, 2008;
Kumar et al, 2009), or sampling translations propor-
tional to their probability (Chatterjee and Cancedda,
2010). We argue that the implicit goal behind these
techniques is to better explore the output space by
introducing diversity into the surrogate set.
Overview and Contributions. In this work, we el-
evate diversity to a first-class status and directly ad-
dress the problem of generating a set of diverse,
plausible translations. We use the recently pro-
posed technique of Batra et al (2012), which pro-
duces diverse M -best solutions from a probabilistic
model using a generic dissimilarity function ?(?, ?)
that specifies how two solutions differ. Our first con-
tribution is a family of dissimilarity functions for
MT that admit simple algorithms for generating di-
verse translations. Other contributions are empiri-
cal: we show that diverse translations can lead to
improvements for system combination and discrim-
inative reranking. We also perform a novel human
1100
post-editing evaluation in order to measure whether
diverse translations can help users make sense of
noisy MT output. We find that diverse translations
can help post-editors produce better outputs for sen-
tences that are the most difficult for MT. While we
focus on machine translation in this paper, we note
that our approach is applicable to other structure pre-
diction problems in NLP.
2 Preliminaries and Notation
Let X denote the set of all strings in a source lan-
guage. For an x ? X, let Yx denote the set of its pos-
sible translations y in the target language. MT mod-
els typically include a latent variable that captures
the derivational structure of the translation process.
Regardless of its specific form, we refer to this vari-
able as a derivation h ? Hx, where Hx is the set of
possible values of h for x. Derivations are coupled
with translations and we define Tx ? Yx ? Hx as
the set of possible ?y,h? pairs for x.
We use a linear model with a parameter vector w
and a vector ?(x,y,h) of feature functions on x, y,
and h (Och and Ney, 2002). The translation of x is
selected using a simple decision rule:
?y?, h?? = argmax
?y,h??Tx
w??(x,y,h) (1)
where we also maximize over the latent variable h
for efficiency. Translation models differ in the form
of Tx and the choice of the feature functions ?. In
this paper we focus on phrase-based (Koehn et al,
2003) and hierarchical phrase-based (Chiang, 2007)
models, which include several bilingual and mono-
lingual features, including n-gram language models.
3 Diversity in Machine Translation
We now address the task of producing a set of di-
verse high-scoring translations.
3.1 Generating Diverse Translations
We use a recently proposed technique (Batra et al,
2012) that constructs diverse lists via a greedy itera-
tive procedure as follows. Let y1 be the model-best
translation (Eq. 1). On the m-th iteration, the m-th
best (diverse) translation is obtained as ?ym,hm? =
argmax
?y,h??Tx
w??(x,y,h) +
m?1?
j=1
?j?(yj ,y) (2)
where ? is a dissimilarity function and ?j is the
weight placed on dissimilarity to previous trans-
lation j relative to the model score. Intuitively,
we seek a translation that is highly-scoring under
the model while being different (as measured by
?) from all previous translations. The ? param-
eters determine the trade-off between model score
and diversity. We refer to Eq. (2) as dissimilarity-
augmented decoding.
The objective in Eq. (2) is a Lagrangian relax-
ation for an intractable constrained objective speci-
fying a minimum dissimilarity ?min between trans-
lations in the list, i.e., ?(yj ,y) ? ?min (Batra et
al., 2012). Instead of setting the dissimilarity thresh-
old ?min , we set the weights ?j . While the formu-
lation allows for a different ?j for each previous so-
lution j, we simply use a single ? = ?j for all j.
This was also done in the experiments in (Batra et
al., 2012).
Note that if the dissimilarity function factors
across the parts of the output variables ?y,h? in the
same way as the features ?, then the same decod-
ing algorithm can be used as for Eq. (1). We discuss
design choices for ? next.
3.2 Dissimilarity Functions for MT
When designing a dissimilarity function ?(?, ?) for
MT, we want to consider variation both in individ-
ual word choice and longer-range sentence structure.
We also want a function that can be easily incorpo-
rated into extant statistical MT systems. We propose
a dissimilarity function that simply counts the num-
ber of times any n-gram is present in both transla-
tions, then negates. Letting q = n? 1:
?n(y,y?) = ?
|y|?q?
i=1
|y?|?q?
j=1
[[yi:i+q = y?j:j+q]] (3)
where [[?]] is the Iverson bracket (1 if input condition
is true, 0 otherwise) and yi:j is the subsequence of y
from word i to word j (inclusive).
Importantly, Eq. (2) can be solved with no change
to the decoding algorithm. The dissimilarity terms
can simply be incorporated as an additional lan-
guage model in ARPA format that sets the log-
probability to the negated count for each n-gram
in previous diverse translations, and sets to zero
all other n-grams? log-probabilities and back-off
weights.
1101
The advantage of this dissimilarity function is its
simplicity. It can be easily used with any transla-
tion system that uses n-gram language models with-
out any change to the decoder. Indeed, we use both
phrase-based and hierarchical phrase-based models
in our experiments below.
4 Related Work
MT researchers have recently started to con-
sider diversity in the context of system combina-
tion (Macherey and Och, 2007). Most closely-
related is work by Devlin and Matsoukas (2012),
who proposed a way to generate diverse transla-
tions by varying particular ?traits,? such as transla-
tion length, number of rules applied, etc. Their ap-
proach can be viewed as solving Eq. (2) with a richer
dissimilarity function that requires a special-purpose
decoding algorithm. We chose our n-gram dissimi-
larity function due to its simplicity and applicability
to most MT systems without requiring any change
to decoders.
Among other work, Xiao et al (2013) used bag-
ging and boosting to get diverse system outputs for
system combination and Cer et al (2013) used mul-
tiple identical systems trained jointly with an objec-
tive function that encourages the systems to generate
complementary translations.
There is also similarity between our approach and
minimum Bayes risk decoding (Kumar and Byrne,
2004), variational decoding (Li et al, 2009), and
other ?consensus? decoding algorithms (DeNero et
al., 2009). These all seek a single translation that
is most similar on average to the model?s preferred
translations. In this way, they try to capture the
model?s range of beliefs in a single translation. We
instead seek a set of translations that, when consid-
ered as a whole, similarly express the full range of
the model?s beliefs about plausible translations for
the input.
Also related is work on determinantal point pro-
cesses (DPPs; Kulesza and Taskar, 2010), an ele-
gant probabilistic model over sets of items that nat-
urally prefers diverse sets. DPPs have been ap-
plied to summarization (Kulesza and Taskar, 2011)
and discovery of topical threads in document collec-
tions (Gillenwater et al, 2012). Unfortunately, in
the structured setting, DPPs make severely restric-
tive assumptions on the scoring function, while our
framework does not.
5 Experimental Setup
We now embark on an extensive empirical evalua-
tion of the framework presented above. We begin
by analyzing our diverse sets of translations, show-
ing how they differ from standard M -best lists (Sec-
tion 6), followed by three tasks that illustrate how di-
versity can be exploited to improve translation qual-
ity: system combination (Section 7), discrimina-
tive reranking (Section 8), and a novel human post-
editing task (Section 9). In the remainder of this sec-
tion, we describe details of our experimental setup.
5.1 Language Pairs and Datasets
We use three language pairs: Arabic-to-English
(AR?EN), Chinese-to-English (ZH?EN), and
German-to-English (DE?EN). For AR?EN and
DE?EN, we used a phrase-based model (Koehn et
al., 2003) and for ZH?EN we used a hierarchical
phrase-based model (Chiang, 2007).
Each language pair has two tuning and one test
set: TUNE1 is used for tuning the baseline sys-
tems with minimum error rate training (MERT; Och,
2003), TUNE2 is used for training system combin-
ers and rerankers, and TEST is used for evaluation.
There are four references for AR?EN and ZH?EN
and one for DE?EN.
For AR?EN, we used data provided by the LDC
for the NIST evaluations, which includes 3.3M sen-
tences of UN data and 982K sentences from other
(mostly news) sources. Arabic text was prepro-
cessed using an HMM segmenter that splits attached
prepositional phrases, personal pronouns, and the
future marker (Lee et al, 2003). The common stylis-
tic sentence-initial w+ (and) clitic was removed.
The resulting corpus contained 130M Arabic tokens
and 130M English tokens. We used the NIST MT06
test set as TUNE1, a 764-sentence subset of MT05 as
TUNE2, and MT08 as TEST.
For ZH?EN, we used 303k sentence pairs from
the FBIS corpus (LDC2003E14). We segmented
the Chinese data using the Stanford Chinese seg-
menter (Chang et al, 2008) in ?CTB? mode, giving
us 7.9M Chinese tokens and 9.4M English tokens.
We used the NIST MT02 test set as TUNE1, MT05
1102
as TUNE2, and MT03 as TEST.
For DE?EN, we used data released for the
WMT2011 shared task (Callison-Burch et al, 2011).
German compound words were split using a CRF
segmenter (Dyer, 2009). We used the WMT2010
test set as TUNE1, the 2009 test set as TUNE2, and
the 2011 test set as TEST.
5.2 Baseline Systems
We used the Moses MT toolkit (Koehn et al,
2007; Hoang et al, 2009) with default settings
and features for both phrase-based and hierarchi-
cal systems. Word alignment was done using
GIZA++ (Och and Ney, 2003) in both directions,
with the grow-diag-final-and heuristic used
to symmetrize the alignments and a max phrase
length of 7 used for phrase extraction.
Language models used the target side of the paral-
lel corpus in each case augmented with 24.8M lines
(601M tokens) of randomly-selected sentences from
the Gigaword v4 corpus (excluding the NY Times
and LA Times). We used 5-gram models, estimated
using the SRI Language Modeling toolkit (Stolcke,
2002) with modified Kneser-Ney smoothing (Chen
and Goodman, 1998). The minimum count cut-off
for unigrams, bigrams, and trigrams was 1 and the
cut-off for 4-grams and 5-grams was 3. Language
model inference used KenLM (Heafield, 2011).
Uncased IBM BLEU was used for evaluation (Pa-
pineni et al, 2002). MERT was used to train the fea-
ture weights for the baseline systems on TUNE1. We
used the learned parameters to generate M -best and
diverse lists for TUNE2 and TEST to use for subse-
quent experiments.
5.3 Diverse List Generation
Generating diverse translations depends on two hy-
perparameters: the n-gram order used by the dissim-
ilarity function ?n (?3.2) and the ?j weights on the
dissimilarity terms in Eq. (2). Though our frame-
work permits different ?j for each j, we use a sin-
gle ? value for simplicity, as was also done in (Ba-
tra et al, 2012). The values of n and ? were tuned
on a 200 sentence subset of TUNE1 separately for
each language pair (which we call TUNE200), so as
to maximize the oracle BLEU score of the diverse
AR?EN ZH?EN DE?EN
1 best 50.1 36.9 21.8
20 best 54.0 40.3 24.7
200 best 57.5 43.8 27.7
1000 best 59.8 46.4 29.8
unique 20 best 56.6 44.1 26.7
unique 200 best 59.6 46.4 29.5
20 diverse 58.5 46.4 28.6
20 div ? 10 best 61.3 48.7 30.3
20 div ? 50 best 63.2 50.6 31.6
Table 1: Oracle BLEU scores on TEST for various sizes
of M -best and diverse lists. Unique lists were obtained
from 1,000-best lists and therefore may not contain the
target number of unique translations for all sentences.
lists.1 We considered n values in {2, 3, . . . , 9} and
? values in {0.005, 0.01, 0.05, 0.1}. We give details
on optimal values for these hyperparameters when
discussing particular tasks below.
Though simple, our approach is computationally
expensive as M grows because it requires decoding
M times for each sentence. So, we assumeM ? 20.
But we also extract an N -best list for each of the M
diverse translations.2 Many MT decoders, including
the phrase-based and hierarchical implementations
in Moses, permit efficient extraction of N -best lists,
so we exploit this to obtain larger lists that still ex-
hibit diversity. But we note that these N -best lists
for each diverse solution are not in themselves di-
verse; with more computational power or more effi-
cient algorithms (Devlin and Matsoukas, 2012) we
could potentially generate larger, more diverse lists.
6 Analysis of Diverse Lists
We now characterize our diverse lists by compar-
ing them to M -best lists. Table 1 shows oracle
BLEU scores on TEST for M -best lists, unique M -
best lists, and diverse lists of several sizes. To get
unique lists, we first generated 1000-best lists, then
retained only the highest-scoring derivation for each
unique translation. When comparingM -best and di-
verse lists of comparable size, the diverse lists al-
1Since BLEU does not decompose additively across seg-
ments, we chose translations for individual sentences that max-
imized BLEU+1 (Lin and Och, 2004), then computed ?oracle?
corpus BLEU of these translations.
2We did not consider n-grams from previous N -best lists
when computing the dissimilarity function, but only those from
the previous diverse translations.
1103
0 
10 
20 
30 
40 
50 
60 
70 
0-25 25-36 36-47 47-94 
%B
LEU
 
1-best BLEU bin 
20 best 20 diverse 
Figure 1: Median, min, and max BLEU+1 of 20-best
and 20-diverse lists for the ZH?EN test set, divided into
quartiles according to the BLEU+1 score of the 1-best
translation, and averaged across sentences in each quar-
tile. Heights of the bars show median and ?error bars?
indicate max and min.
ways have higher oracle BLEU. The differences are
largest when comparing 20-best lists and 20-diverse
lists, where they range from 4 to 6 BLEU points.
When generating these diverse lists, we used the
n and ? values that were tuned for each language
pair to maximize oracle BLEU on TUNE200 for the
?20 div ? 50 best? configuration. The optimal val-
ues of n were 6 for ZH?EN and AR?EN and 7 for
DE?EN.3 When instead tuning to maximize oracle
BLEU for 20-diverse lists, the optimal n stayed at
7 for DE?EN, but increased to 7 for AR?EN and 9
for ZH?EN. These values are noticeably larger than
n-gram sizes typically used in language modeling
and evaluation. They suggest that for optimal ora-
cle BLEU, translations with long-spanning amounts
of repeated material should be avoided, while short
overlapping n-grams are permitted.
Figure 1 shows other statistics on TEST for
ZH?EN. Plots for AR?EN and DE?EN are quali-
tatively similar. We divided the TEST sentences into
quartiles based on BLEU+1 of the 1-best transla-
tions from the baseline system. We computed the
median, min, and max BLEU+1 on each list and av-
eraged over the sentences in each quartile. As shown
in the plot, the ranges of 20-diverse lists subsume
those of 20-best lists, though the medians of diverse
3The optimal values of ? were 0.005 for AR?EN and 0.01
for ZH?EN and DE?EN. Since these values depend on the
scale of the weights learned by MERT, they are difficult to in-
terpret in isolation.
lists drop when the baseline system has high BLEU
score. This matches intuition: when the baseline
system is performing well, forcing it to find different
translations is likely to result in worse translations.
So we may expect diverse lists to be most helpful for
more difficult sentences, a point we return to in our
experiments below.
7 System Combination Experiments
One way to evaluate the quality of our diverse lists
is to use them in system combination, as was sim-
ilarly done by Devlin and Matsoukas (2012) and
Cer et al (2013). We use the system combination
framework of Heafield and Lavie (2010b), which
has an open-source implementation (Heafield and
Lavie, 2010a).4
We use our baseline systems (trained on TUNE1)
to generate lists for system combination on TUNE2
and TEST. We compareM -best lists, uniqueM -best
lists, and M -diverse lists, with M ? {10, 15, 20}.5
For each choice of list type and M , we trained the
system combiner on TUNE2 and tested on TEST with
the learned parameters. System combination hyper-
parameters (whether to use feature length normal-
ization; the size of the k-best lists generated by the
system combiner during tuning, k ? {300, 600})
were chosen to maximize BLEU on TUNE200. Also,
we removed the individual features from the
default feature set because they correspond to in-
dividual systems in the combination; they did not
seem appropriate for us since our hypotheses all
come from the same system.
The results are shown in Table 2. Like Devlin and
Matsoukas (2012), we see no gain from system com-
bination using M -best lists. We see some improve-
ment with unique lists, particularly for AR?EN, al-
though it is not consistent across M values. But
we see larger improvements with diverse lists for
AR?EN and ZH?EN. For these language pairs, our
4The implementation uses MERT to tune parameters, but we
found this to be time-consuming and noisy for the larger feature
sets. So we used a structured support vector machine learning
framework instead (described in Section 8), using multiple it-
erations of learning interleaved with (system combiner) N -best
list generation, and accumulating N -best lists across iterations.
5Dissimilarity hyperparameters n and ? were again chosen
to maximize oracle BLEU on TUNE200, separately for each M
and for each language pair.
1104
AR?EN ZH?EN DE?EN
10 15 20 10 15 20 10 15 20
baseline (no system combination) 50.1 36.9 21.8
M -best 50.2 50.1 50.0 36.7 36.9 37.0 21.7 21.7 21.8
unique M -best (from 1000-best list) 50.6 50.0 50.8 37.1 36.9 37.1 21.8 21.9 21.9
M -diverse 51.4 51.2 51.2 37.6 37.6 37.5 22.0 21.8 21.6
Table 2: System combination results (%BLEU on TEST). Size of lists is M ? {10, 15, 20}. Highest score in each
column is bold.
AR?EN ZH?EN DE?EN
q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4
baseline 30.1 44.1 55.1 70.0 15.2 28.9 41.0 57.5 5.3 14.4 23.7 40.9
15-best 30.1 44.6 55.5 68.8 15.9 29.2 40.5 56.8 6.0 15.0 23.6 40.0
unique 15-best 30.4 44.7 55.2 68.4 16.7 29.0 41.2 56.6 5.9 14.9 23.8 40.6
15-diverse 31.3 45.3 57.8 69.1 17.7 30.6 41.7 56.9 7.6 15.2 23.4 39.6
Table 3: System combination results (%BLEU on quartiles of TEST, M = 15). Source sentences were divided into
quartiles (numbered ?qn?) according to BLEU+1 of the 1-best translations of the baseline system. Highest score in
each column is bold.
gains are similar to those seen by Devlin and Mat-
soukas, but use our simpler dissimilarity function.6
For DE?EN, results are similar for all settings and
do not show much improvement from system com-
bination.
In Table 3, we break down the scores according
to 1-best BLEU+1 quartiles, as done in Figure 1.7
In general, we find the largest gains for the low-
BLEU translations. For the two worst BLEU quar-
tiles, we see gains of 1.2 to 2.5 BLEU points, while
the gains shrink or disappear entirely for the best
quartile. This may be a worthwhile trade-off: a
large improvement in the worst translations may be
more significant to users than a smaller degredation
on sentences that are already being translated well.
In addition, quality estimation (Specia et al, 2011;
Bach et al, 2011) could be used to automatically de-
termine the BLEU quartile for each sentence. Then
system combination of diverse translations might be
used only when the 1-best translation is predicted to
be of low quality.
8 Reranking Experiments
We now turn to discriminative reranking, which has
frequently been used to easily add rich features to
a model. It has been used for MT with varying de-
6They reported +0.8 BLEU from system combination for
AR?EN, and saw a further +0.5?0.7 from their new features.
7Quartile points are: 39, 49, 61 for AR?EN; 25, 36, and 47
for ZH?EN; and 14.5, 21.1, and 30.3 for DE?EN.
gree of success (Och et al, 2004; Shen et al, 2004;
Hildebrand and Vogel, 2008); some have attributed
its mixed results to a lack of diversity in the M -best
lists traditionally used. We propose diverse lists as a
way to address this concern.
8.1 Learning Framework
Several learning formulations have been proposed
for M -best reranking. One commonly-used ap-
proach in MT is MERT, used in the reranking ex-
periments of Och et al (2004) and Hildebrand and
Vogel (2008), among others. We experimented with
MERT and other algorithms, including pairwise
ranking optimization (Hopkins and May, 2011), but
we found best results using the approach of Yadol-
lahpour et al (2013), who used a slack-rescaled
structured support vector machine (Tsochantaridis
et al, 2005) with L2 regularization. As a sentence-
level loss, we used negated BLEU+1. We used the
1-slack cutting-plane algorithm of Joachims et al
(2009) for optimization during learning.8 A more
detailed description of the reranker is provided in the
supplementary material.
We used 5-fold cross-validation on TUNE2 to
choose the regularization parameter C from the set
{0.01, 0.1, 1, 10}. We selected the value yielding
the highest average BLEU score across the held-out
8Our implementation uses OOQP (Gertz and Wright, 2003)
to solve the quadratic program in the inner loop, which uses
HSL, a collection of Fortran codes for large-scale scientific
computation (www.hsl.rl.ac.uk).
1105
folds. This value was then used for one final round
of training on the entirety of TUNE2. Additionally,
we tuned the decision to return the parameters at
convergence or those that produced the highest train-
ing corpus BLEU score. Since we use a sentence-
level metric during training (BLEU+1) and a corpus-
level metric for final evaluation (BLEU), we found
that it was often better to return parameters that pro-
duced the highest training BLEU score.
This tuning procedure was repeated for each fea-
ture set and for each list type (M -best or diverse).
The test set was not used for any of this tuning.
8.2 Features
In addition to the features from the baseline models
(14 for phrase-based, 8 for hierarchical), we add 36
more for reranking:
Inverse Model 1 (INVMOD1): We added the ?in-
verse? versions of the three IBM Model 1 features
described in Section 2.2 of Hildebrand and Vogel
(2008). The first is the probability of the source sen-
tence given the translation under IBM Model 1, the
second replaces the
?
with a max in the first fea-
ture, and the third computes the percentage of words
whose lexical translation probability falls below a
threshold. We also include versions of the first 2
features normalized by the translation length, for a
total of 5 INVMOD1 features.
Large LM (LLM): We created a large 4-gram LM
by interpolating LMs from the WMT news data, Gi-
gaword, Europarl, and the DE?EN news commen-
tary (NC) corpus to maximize likelihood of a held-
out development set (WMT08 test set). We used the
average per-word log-probability as the single fea-
ture function in this category.
Syntactic LM (SYN): We used the syntactic treelet
language model of Pauls and Klein (2012) to com-
pute two features: the translation log probability and
the length-normalized log probability.
Finite/Non-Finite Verbs (VERB): We ran the Stan-
ford part-of-speech (POS) tagger (Toutanova et al,
2003) on each translation and added four features:
the fraction of words tagged as finite/non-finite
verbs, and the fraction of verbs that are finite/non-
finite.9
9Words tagged as MD, VBP, VBZ, and VBD were counted
Reranking AR?EN ZH?EN DE?EN
features best div best div best div
N/A (baseline) 50.1 36.9 21.8
None 50.5 50.7 37.3 37.1 21.9 21.6
+ INVMOD1 50.3 50.8 37.6 37.1 22.0 21.8
+ LLM, SYN 50.5 51.1 37.4 37.3 21.7 21.7
+ VERB, DISC 50.4 51.3 37.3 37.3 21.9 22.2
+ GOOG 50.7 51.3 36.8 37.1 21.9 22.2
+ WCLM 51.2 51.8 37.3 37.4 22.2 22.3
Table 4: Reranking results (%BLEU on TEST).
Discriminative Word/Tag LMs (DISC): For each
language pair, we generated 10,000-best lists for
TUNE1 and computed BLEU+1 for each. From
these lists, we estimated 3- and 5-gram LMs,
weighting the n-gram counts by the BLEU+1
scores.10 We repeated this procedure except using
1 minus BLEU+1 as the weight (learning a language
model of ?bad? translations). This yielded 4 fea-
tures. The procedure was then repeated using POS
tags instead of words, for 8 features in total.
Google 5-Grams (GOOG): Translations were com-
pared to the Google 5-gram corpus (LDC2006T13)
to compute: the number of 5-grams that matched,
the number of 5-grams that missed, and a set of
indicator features that fire if the fraction of 5-
grams that matched in the sentence was greater than
{0.05, 0.1, 0.2, . . . , 0.9}, for a total of 12 features.
Word Cluster LMs (WCLM): Using an imple-
mentation provided by Liang (2005), we performed
Brown clustering (Brown et al, 1992) on 900k En-
glish sentences, including the NC corpus and ran-
dom sentences from Gigaword. We clustered words
that appeared at least twice, once with 300 clus-
ters and again with 1000. We then replaced words
with their clusters in a large corpus consisting of
the WMT news data, Gigaword, and the NC data.
An additional cluster label was used for unknown
words. For each of the clusterings (300 and 1000),
we estimated 5- and 7-gram LMs with Witten-Bell
smoothing (Witten and Bell, 1991). We added 4 fea-
tures to the reranker, one for the log-probability of
the translation under each of the word cluster LMs.
as finite verbs, and VB, VBG, and VBN were non-finite verbs.
10Before estimating LMs, we projected the sentence weights
so that the min and max per source sentence were 0 and 1.
1106
List type
Features
None All
20 best 50.3 50.6
100 best 50.6 50.8
200 best 50.4 51.2
1000 best 50.5 51.2
unique 20 best 50.5 51.2
unique 100 best 50.6 51.2
unique 200 best 50.4 51.3
20 diverse 50.5 51.1
20 div ? 5 best 50.6 51.4
20 div ? 10 best 50.7 51.3
20 div ? 50 best 50.7 51.8
Table 5: List comparison for AR?EN reranking.
8.3 Results
Our results are shown in Table 4. We report results
using the baseline system alone (labeled ?N/A (base-
line)?), and reranking standard M -best lists and our
diverse lists. For diverse lists, we use the ?20 div ?
50 best? lists described in Section 5.3, with the tuned
dissimilarity hyperparameters reported in Section 6.
In the reranking settings, we also report results with-
out adding any additional features (the row labeled
?None?).11
The remaining rows add features. For AR?EN,
we see the largest gains, both over the baseline as
well as differences betweenM -best lists and diverse
lists. When using all features, we achieve a gain
of 0.6 BLEU over M -best reranking and 1.7 BLEU
points over the baseline system. The difference of
0.6 BLEU is consistent across feature subsets. We
found the WCLM features to give the largest in-
dividual improvement, with the remaining feature
sets each contributing a small amount. For Chinese
and German, the gains and individual differences are
smaller. Nonetheless, diverse lists appear to be more
robust for these language pairs as features are added.
In Table 5, we compare several sizes and types of
lists for AR?EN reranking both with no additional
features and with the full set. We see that using 20-
diverse lists nearly matches the performance of 200-
best lists. Also, retaining 50-best lists for each di-
verse solution improves BLEU by 0.7.
11Though such results have not always been reported in prior
work on reranking, we generally found them to improve over
the baseline, presumably because seeing more data improves
generalization ability.
Train
best div
Te
st best 51.2 51.7
div 50.5 51.8
Table 6: Comparing M -best and diverse lists for train-
ing/testing (AR?EN, all features).
Thus far, when training the reranker on M -best
lists, we tested it on M -best lists, and similarly for
diverse lists. Table 6 shows what happens with the
other two pairings for AR?EN with the full feature
set. When training on diverse lists, we see very lit-
tle difference in BLEU whether testing on M -best
or diverse lists. This has a practical benefit: we can
use (computationally-expensive) diverse lists during
offline training and then use fast M -best lists at test
time. When training on M -best lists and testing
on diverse lists, we see a substantial drop (51.2 vs
50.5). The reranker may be overfitting to the limited
scope of translations present in typical M -best lists,
thereby hindering its ability to correctly rank diverse
lists at test time. These results suggest that part of
the benefit of using diverse lists comes from seeing
a larger portion of the output space during training.
9 Human Post-Editing Experiments
We wanted to determine whether diverse translations
could be helpful to users struggling to understand
the output of an imperfect MT system. We con-
sider a post-editing task in which users are presented
with translation output without the source sentence,
and are asked to improve it. This setting has been
studied; e.g., Koehn (2010) presented evidence that
monolingual speakers could often produce improved
translations for this task, occasionally reaching the
level of an expert translator.
Here, we use a novel variation of this task in
which multiple translations are shown to editors. We
compare the use of entries from an M -best list and
entries from a diverse list. Again, the original source
sentence is not provided. Our goal is to determine
whether multiple, diverse translations can help users
to more accurately guess the meaning of the original
sentence than entries from a standard M -best list. If
so, commercial MT systems might permit users to
request additional diverse translations for those sen-
tences whose model-best translations are difficult to
understand.
1107
9.1 Translation List Post-Editing
We use Amazon Mechanical Turk (MTurk) for this
experiment. Workers are shown 3 outputs from an
MT system. They are not shown the original sen-
tence, nor are they shown a reference. Based on
the 3 imperfect translations, they are asked to write
a single fluent English translation that best cap-
tures the understood meaning. Half of the time, the
worker is shown 3 entries from an M -best list, and
the other half of the time 3 entries from a diverse
list. We then compare the outputs produced under
the two conditions. The goal is to measure whether
workers are able to produce translations that are
closer in meaning to the (unseen) references when
shown diverse translations. We refer to this task as
the EDITING task.
To evaluate the outputs, we use a second task in
which users are shown a reference translation along
with two outputs from the first task: one created
from M -best lists and one from diverse lists. Work-
ers in this task are asked to choose which translation
is a better match to the reference in terms of mean-
ing, or they can indicate that the translations are of
the same quality. We refer to this second task as the
EVAL task.
9.2 Dissimilarity Functions
To generate diverse lists for the EDITING task, we
use the same dissimilarity function as in reranking,
but we tune the hyperparameters n and ? differently.
Since our expectation here is that workers may com-
bine information from multiple translations to pro-
duce a superior output, we are interested in the cov-
erage of the translations in the diverse list, rather
than the oracle BLEU score.
We designed a metric based on coverage of entire
lists of translations. It is similar to BLEU+1, except
(1) it uses n-gram recalls instead of n-gram preci-
sions, (2) there is no brevity penalty term, and (3) it
compares a list to a set of references and any trans-
lation in the list can contribute a match of an n-gram
in any reference. Like BLEU, counts are clipped
based on those in the references. We maximized
this metric over diverse lists of length 5, for n ?
{2, 3, . . . , 9} and ? ? {0.005, 0.01, 0.05, 0.1, 0.2}.
The optimal values for AR?EN were n = 4 and
? = 0.1, while for ZH?EN they were n = 4 and
? = 0.2. These n values are smaller than for rerank-
ing, and the ? values are larger. This suggests that,
when maximizing coverage of a small diverse list,
more dissimilarity is desired among the translations.
9.3 Detailed Procedure
We focused on AR?EN and ZH?EN for this study.
We sampled 200 sentences from their test sets, cho-
sen from among those whose reference translation
was between 5 and 25 words. We generated a unique
5-best list for each sentence using our baseline sys-
tem (described in Section 5.2) and also generated a
diverse list of length 5 using the dissimilarity func-
tion ? with hyperparameters tuned using the proce-
dure from the previous section. We untokenized and
truecased the translations. We dropped non-ASCII
characters because we feared they would confuse
our workers. As a result, workers must contend with
missing words in the output, often proper nouns.
Given the 2 lists for each sentence, we sampled
two integers i, j ? {2, 3, 4, 5} without replacement.
The indices i and j indicate two entries from the
lists. We took translations 1, i, and j from the 5-best
list and created an EDITING task from them. We did
the same using entries 1, i, and j from the diverse
list. We repeated this process 3 times for each sen-
tence, obtaining 3? 2 = 6 tasks for each, giving us
a total of 1,200 EDITING tasks per language pair.
The outputs of the EDITING tasks were evaluated
with EVAL tasks. For each sentence, we had 3 post-
edited outputs generated using entries in 5-best lists
and 3 post-edited outputs from diverse lists. We cre-
ated EVAL tasks for all 9 output pairs, for all 200
sentences per language pair. We additionally gave
each task to three MTurk workers. This gave us
10,800 evaluation judgments for the EVAL task.
9.4 Results
Figure 2 shows the quartile breakdown for judg-
ments collected from the EVAL task. The Y axis
represents the percentage of judgments for which
best/diverse outputs were preferred; the missing per-
centage for each bin is accounted for by ?same?
judgments.
We observe an interesting phenomenon. Overall,
there is a slight preference for the post-edited out-
puts of M -best entries (?best?) over those from di-
verse translations (?div?); this preference is clearest
1108
0?34 34?46 46?62 62?9420
25
30
35
40
45 Arabic?English
% C
hos
en
0?23 23?36 36?50 50?94
20
30
40
BLEU Bin
% C
hos
en
Chinese?English
 
 best
div
Figure 2: Percentages in which post-edited output given
M -best entries (?best?) was preferred by human eval-
uators as compared to post-edited output given diverse
translations (?div?), broken down by the BLEU+1 score
of the 1-best translation for the sentences. When the base-
line system is doing poorly, diversity helps post-editors to
produce better translations.
when the baseline system?s 1-best translation had a
high BLEU score. However, we see this trend re-
versed for sentences in which the baseline system?s
1-best translation had a low BLEU score. In general,
when the BLEU score of the baseline system is be-
low 35, it is preferable to give diverse translations to
users for post-editing. But when the baseline system
does very well, diverse translations do not contribute
anything, and in fact hurt because they may distract
users from the high-quality (and typically very sim-
ilar) translations from the 5-best lists.
Estimation of the quality of the output (?confi-
dence estimation?) has recently gained interest in
the MT community (Specia et al, 2011; Bach et
al., 2011; Callison-Burch et al, 2012; Bojar et al,
2013), including specifically for post-editing (Tat-
sumi, 2009; Specia, 2011; Koponen, 2012). Future
work could investigate whether such automatic con-
fidence estimation could be used to identify situa-
tions in which diverse translations can be helpful for
aiding user understanding.
10 Future Work
Our dissimilarity function captures diversity in the
particular phrases used by an MT system, but for
certain applications we may prefer other types of di-
versity. Defining the dissimilarity function on POS
tags or word clusters would help us to capture stylis-
tic patterns in sentence structure, as would targeting
syntactic structures in syntax-based translation.
A weakness of our approach is its computational
expense; by contrast, the method of Devlin and Mat-
soukas (2012) obtains diverse translations more ef-
ficiently by extracting them from a single decoding
of an input sentence (albeit with a wide beam). We
expect their ideas to be directly applicable to our set-
ting in order to get diverse solutions more cheaply.
We also plan to explore methods of explicitly target-
ing multiple, diverse solutions as part of the search
algorithm.
Finally, M -best lists are currently used to ap-
proximate structured spaces for many areas of MT,
including tuning (Och, 2003), minimum Bayes
risk decoding (Kumar and Byrne, 2004), and
pipelines (Venugopal et al, 2008). Future work
could replace M -best lists with diverse lists in these
and related tasks, whether for MT or other areas of
structured NLP.
Acknowledgments
We thank the anonymous reviewers as well as Colin
Cherry, Kenneth Heafield, Silja Hildebrand, Fei
Huang, Dan Klein, Adam Pauls, and Bing Xiang.
DB was partially supported by the National Science
Foundation under Grant No. 1353694.
References
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness:
A method for measuring machine translation confi-
dence. In Proc. of ACL.
D. Batra, P. Yadollahpour, A. Guzman-Rivera, and
G. Shakhnarovich. 2012. Diverse M-best solutions
in Markov random fields. In Proc. of ECCV.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proc. of WMT.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della
Pietra, and J. C. Lai. 1992. Class-based N-gram mod-
1109
els of natural language. Computational Linguistics,
18.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 Workshop on Statistical
Machine Translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 Work-
shop on Statistical Machine Translation. In Proc. of
WMT.
D. Cer, C. D. Manning, and D. Jurafsky. 2013. Positive
diversity tuning for machine translation system com-
bination. In Proc. of WMT.
P. Chang, M. Galley, and C. D. Manning. 2008. Opti-
mizing Chinese word segmentation for machine trans-
lation performance. In Proc. of WMT.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc. of ACL.
S. Chatterjee and N. Cancedda. 2010. Minimum error
rate training by sampling the translation lattice. In
Proc. of EMNLP.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1).
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
J. DeNero, D. Chiang, and K. Knight. 2009. Fast con-
sensus decoding over translation forests. In Proc. of
ACL.
J. Devlin and S. Matsoukas. 2012. Trait-based hypoth-
esis selection for machine translation. In Proc. of
NAACL.
C. Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proc. of HLT-
NAACL.
C. Dyer. 2010. A Formal Model of Ambiguity and its Ap-
plications in Machine Translation. Ph.D. thesis, Uni-
versity of Maryland.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
Bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
E. M. Gertz and S. J. Wright. 2003. Object-oriented soft-
ware for quadratic programming. ACM Transactions
on Mathematical Software, 29(1).
J. Gillenwater, A. Kulesza, and B. Taskar. 2012. Discov-
ering diverse and salient threads in document collec-
tions. In Proc. of EMNLP.
K. Heafield and A. Lavie. 2010a. Combining machine
translation output with open source: The Carnegie
Mellon multi-engine machine translation scheme. The
Prague Bulletin of Mathematical Linguistics, 93.
K. Heafield and A. Lavie. 2010b. Voting on n-grams for
machine translation system combination. In Proc. of
AMTA.
K. Heafield. 2011. Kenlm: Faster and smaller language
model queries. In Proc. of WMT.
A. Hildebrand and S. Vogel. 2008. Combination of
machine translation systems via hypothesis selection
from combined n-best lists. In Proc. of AMTA.
H. Hoang, P. Koehn, and A. Lopez. 2009. A Uni-
fied Framework for Phrase-Based, Hierarchical, and
Syntax-Based Statistical Machine Translation. In
Proc. of IWSLT.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
T. Joachims, T. Finley, and C. Yu. 2009. Cutting-
plane training of structural SVMs. Machine Learning,
77(1).
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL (demo
session).
P. Koehn. 2010. Enabling monolingual translators: Post-
editing vs. options. In Proc. of NAACL.
M. Koponen. 2012. Comparing human perceptions of
post-editing effort with post-editing operations. In
Proc. of WMT.
A. Kulesza and B. Taskar. 2010. Structured determinan-
tal point processes. In Proc. of NIPS.
A. Kulesza and B. Taskar. 2011. Learning determinantal
point processes. In Proc. of UAI.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
1110
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. of ACL-IJCNLP.
Y. Lee, K. Papineni, S. Roukos, O. Emam, and H. Hassan.
2003. Language model based Arabic word segmenta-
tion. In Proc. of ACL.
Z. Li, J. Eisner, and S. Khudanpur. 2009. Variational
decoding for statistical machine translation. In Proc.
of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
C. Lin and F. J. Och. 2004. Orange: a method for evalu-
ating automatic evaluation metrics for machine trans-
lation. In Proc. of COLING.
W. Macherey and F. J. Och. 2007. An empirical study
on computing consensus translations from multiple
machine translation systems. In Proc. of EMNLP-
CoNLL.
W. Macherey, F. J. Och, I. Thayer, and J. Uszkoreit. 2008.
Lattice-based minimum error rate training for statisti-
cal machine translation. In Proc. of EMNLP.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL.
F. J. Och. 2003. Minimum error rate training for statisti-
cal machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
A. Pauls and D. Klein. 2012. Large-scale syntactic lan-
guage modeling with treelets. In Proc. of ACL.
A.-V. Rosti, N. F. Ayan, B. Xiang, S. Matsoukas,
R. Schwartz, and B. Dorr. 2007. Combining outputs
from multiple machine translation systems. In HLT-
NAACL.
L. Shen and A. K. Joshi. 2003. An SVM-based voting
algorithm with application to parse reranking. In Proc.
of CoNLL.
L. Shen, A. Sarkar, and F. J. Och. 2004. Discriminative
reranking for machine translation. In Proc. of HLT-
NAACL.
L. Specia, N. Hajlaoui, C. Hallett, and W. Aziz. 2011.
Predicting machine translation adequacy. In Proc. of
MT Summit XIII.
L. Specia. 2011. Exploiting objective annotations for
measuring translation post-editing effort. In Proc. of
EAMT.
A. Stolcke. 2002. SRILM?an extensible language mod-
eling toolkit. In Proc. of ICSLP.
M. Tatsumi. 2009. Correlation between automatic evalu-
ation metric scores, post-editing speed, and some other
factors. In Proc. of MT Summit XII.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. of HLT-NAACL.
R. Tromble, S. Kumar, F. J. Och, and W. Macherey. 2008.
Lattice Minimum Bayes-Risk decoding for statistical
machine translation. In Proc. of EMNLP.
I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Al-
tun. 2005. Large margin methods for structured and
interdependent output variables. JMLR, 6.
A. Venugopal, A. Zollmann, N.A. Smith, and S. Vogel.
2008. Wider pipelines: N-best alignments and parses
in MT training. In Proc. of AMTA.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Transactions on
Information Theory, 37(4).
T. Xiao, J. Zhu, and T. Liu. 2013. Bagging and boosting
statistical machine translation systems. Artif. Intell.,
195.
P. Yadollahpour, D. Batra, and G. Shakhnarovich. 2013.
Discriminative re-ranking of diverse segmentations. In
Proc. of CVPR.
1111
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1677?1687,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Translating into Morphologically Rich Languages with Synthetic Phrases
Victor Chahuneau Eva Schlinger Noah A. Smith Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,eschling,nasmith,cdyer}@cs.cmu.edu
Abstract
Translation into morphologically rich lan-
guages is an important but recalcitrant prob-
lem in MT. We present a simple and effec-
tive approach that deals with the problem in
two phases. First, a discriminative model is
learned to predict inflections of target words
from rich source-side annotations. Then, this
model is used to create additional sentence-
specific word- and phrase-level translations
that are added to a standard translation model
as ?synthetic? phrases. Our approach re-
lies on morphological analysis of the target
language, but we show that an unsupervised
Bayesian model of morphology can success-
fully be used in place of a supervised analyzer.
We report significant improvements in transla-
tion quality when translating from English to
Russian, Hebrew and Swahili.
1 Introduction
Machine translation into morphologically rich lan-
guages is challenging, due to lexical sparsity and the
large variety of grammatical features expressed with
morphology. In this paper, we introduce a method
that uses target language morphological grammars
(either hand-crafted or learned unsupervisedly) to
address this challenge and demonstrate its effective-
ness at improving translation from English into sev-
eral morphologically rich target languages.
Our approach decomposes the process of produc-
ing a translation for a word (or phrase) into two
steps. First, a meaning-bearing stem is chosen and
then an appropriate inflection is selected using a
feature-rich discriminative model that conditions on
the source context of the word being translated.
Rather than attempting to directly produce full-
sentence translations using such an elementary pro-
cess, we use our model to generate translations of
individual words and short phrases that augment?
on a sentence-by-sentence basis?the inventory of
translation rules obtained using standard translation
rule extraction techniques (Chiang, 2007). We call
these synthetic phrases.
The major advantages of our approach are: (i)
synthesized forms are targeted to a specific transla-
tion context; (ii) multiple, alternative phrases may
be generated with the final choice among rules left
to the global translation model; (iii) virtually no
language-specific engineering is necessary; (iv) any
phrase- or syntax-based decoder can be used with-
out modification; and (v) we can generate forms that
were not attested in the bilingual training data.
The paper is structured as follows. We first
present our ?translate-and-inflect? model for pre-
dicting lexical translations into morphologically rich
languages given a source word and its context (?2).
Our approach requires a morphological grammar to
relate surface forms to underlying ?stem, inflection?
pairs; we discuss how either a standard morpholog-
ical analyzer or a simple Bayesian unsupervised an-
alyzer can be used (?3). After describing an ef-
ficient parameter estimation procedure for the in-
flection model (?4), we employ the translate-and-
inflect model in an MT system. We describe
how we use our model to synthesize translation
options (?5) and then evaluate translation quality
on English?Russian, English?Hebrew, and English?
1677
Swahili translation tasks, finding significant im-
provements in all language pairs (?6). We finally
review related work (?7) and conclude (?8).
2 Translate-and-Inflect Model
The task of the translate-and-inflect model is illus-
trated in Fig. 1 for an English?Russian sentence pair.
The input will be a sentence e in the source language
(in this paper, always English) and any available lin-
guistic analysis of e. The output f will be composed
of (i) a sequence of stems, each denoted ? and (ii)
one morphological inflection pattern for each stem,
denoted ?. When the information is available, a
stem ? is composed of a lemma and an inflectional
class. Throughout, we use ?? to denote the set
of possible morphological inflection patterns for a
given stem ?. ?? might be defined by a grammar;
our models restrict ?? to be the set of inflections
observed anywhere in our monolingual or bilingual
training data as a realization of ?.1
We assume the availability of a deterministic
function that maps a stem ? and morphological in-
flection ? to a target language surface form f . In
some cases, such as our unsupervised approach in
?3.2, this will be a concatenation operation, though
finite-state transducers are traditionally used to de-
fine such relations (?3.1). We abstractly denote this
operation by ?: f = ? ? ?.
Our approach consists in defining a probabilistic
model over target words f . The model assumes in-
dependence between each target word f conditioned
on the source sentence e and its aligned position i in
this sentence.2 This assumption is further relaxed
in ?5 when the model is integrated in the translation
system.
We decompose the probability of generating each
target word f in the following way:
p(f | e, i) =
?
???=f
p(? | ei)
? ?? ?
gen. stem
? p(? | ?, e, i)
? ?? ?
gen. inflection
Here, each stem is generated independently from a
single aligned source word ei, but in practice we
1This prevents the model from generating words that would
be difficult for the language model to reliably score.
2This is the same assumption that Brown et al (1993) make
in, for example, IBM Model 1.
use a standard phrase-based model to generate se-
quences of stems and only the inflection model op-
erates word-by-word. We turn next to the inflection
model.
2.1 Modeling Inflection
In morphologically rich languages, each stem may
be combined with one or more inflectional mor-
phemes to express many different grammatical fea-
tures (e.g., case, definiteness, mood, tense, etc.).
Since the inflectional morphology of a word gen-
erally expresses multiple grammatical features, we
would like a model that naturally incorporates rich,
possibly overlapping features in its representation of
both the input (i.e., conditioning context) and out-
put (i.e., the inflection pattern). We therefore use
the following parametric form to model inflectional
probabilities:
u(?, e, i) = exp
[
?(e, i)>W?(?)+
?(?)>V?(?)
]
,
p(? | ?, e, i) =
u(?, e, i)
?
????? u(?
?, e, i)
. (1)
Here, ? is an m-dimensional source context fea-
ture vector function, ? is an n-dimensional mor-
phology feature vector function, W ? Rm?n and
V ? Rn?n are parameter matrices. As with the
more familiar log-linear parametrization that is writ-
ten with a single feature vector, single weight vec-
tor and single bias vector, this model is linear in its
parameters (it can be understood as working with
a feature space that is the outer product of the two
feature spaces). However, using two feature vectors
allows to define overlapping features of both the in-
put and the output, which is important for modeling
morphology in which output variables are naturally
expressed as bundles of features. The second term
in the sum in u enables correlations among output
features to be modeled independently of input, and
as such can be understood as a generalization of the
bias terms in multi-class logistic regression (on the
diagonal Vii) and interaction terms between output
variables in a conditional random field (off the diag-
onalVij).
1678
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
?:????????_V,+,?:mis2sfm2e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
root
-1 +1
??? ??? ???? ? ? ???? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN         TO   VB      DT     NN  IN  PRP$   NN
nsubj
aux
xcomp
?:????????_V,+,?:mis sfm2e
C50   C473        C28         C8    C275   C37   C43  C82 C94   C331
root
-1 +1
Figure 1: The inflection model predicts a form for the target verb lemma ? =???????? (pytat?sya) based on its
source attempted and the linear and syntactic source context. The correct inflection string for the observed Russian
form in this particular training instance is ? = mis-sfm-e (equivalent to the more traditional morphological string:
+MAIN+IND+PAST+SING+FEM+MEDIAL+PERF).
?
???
???
source aligned word ei
parent word epii with its dependency pii ? i
all children ej | pij = i with their dependency i? j
source words ei?1 and ei+1
?
???
???
?
?
?
token
part-of-speech tag
word cluster
?
?
?
? are ei, epii at the root of the dependency tree?
? number of children, siblings of ei
Figure 2: Source features ?(e, i) extracted from e and its linguistic analysis. pii denotes the parent of the token in
position i in the dependency tree and pii ? i the typed dependency link.
2.2 Source Context Features: ?(e, i)
In order to select the best inflection of a target-
language word, given the source word it translates
and the context of that source word, we seek to ex-
ploit as many features of the context as are avail-
able. Consider the example shown in Fig. 1, where
most of the inflection features of the Russian word
(past tense, singular number, and feminine gender)
can be inferred from the context of the English word
it is aligned to. Indeed, many grammatical functions
expressed morphologically in Russian are expressed
syntactically in English. Fortunately, high-quality
parsers and other linguistic analyzers are available
for English.
On the source side, we apply the following pro-
cessing steps:
? Part-of-speech tagging with a CRF tagger
trained on sections 02?21 of the Penn Tree-
bank.
? Dependency parsing with TurboParser (Mar-
tins et al, 2010), a non-projective dependency
parser trained on the Penn Treebank to produce
basic Stanford dependencies.
? Assignment of tokens to one of 600 Brown
clusters, trained on 8G words of English text.3
We then extract binary features from e using this
information, by considering the aligned source word
ei, its preceding and following words, and its syn-
tactic neighbors. These are detailed in Figure 2.
3 Morphological Grammars and Features
We now describe how to obtain morphological anal-
yses and convert them into feature vectors (?) for
our target languages, Russian, Hebrew, and Swahili,
using supervised and unsupervised methods.
3.1 Supervised Morphology
The state-of-the-art in morphological analysis uses
unweighted morphological transduction rules (usu-
3The entire monolingual data available for the translation
task of the 8th ACL Workshop on Statistical Machine Transla-
tion was used.
1679
ally in the form of an FST) to produce candidate
analyses for each word in a sentence and then sta-
tistical models to disambiguate among the analy-
ses in context (Hakkani-Tu?r et al, 2000; Hajic? et
al., 2001; Smith et al, 2005; Habash and Rambow,
2005, inter alia). While this technique is capable
of producing high quality linguistic analyses, it is
expensive to develop, requiring hand-crafted rule-
based analyzers and annotated corpora to train the
disambiguation models. As a result, such analyzers
are only available for a small number of languages,
and, as a practical matter, each analyzer (which re-
sulted from different development efforts) operates
differently from the others.
We therefore focus on using supervised analysis
for a single target language, Russian. We use the
analysis tool of Sharoff et al (2008) which produces
for each word in context a lemma and a fixed-length
morphological tag encoding the grammatical fea-
tures. We process the target side of the parallel data
with this tool to obtain the information necessary
to extract ?lemma, inflection? pairs, from which we
compute ? and morphological feature vectors ?(?).
Supervised morphology features: ?(?). Since
a positional tag set is used, it is straightforward to
convert each fixed-length tag ? into a feature vector
by defining a binary feature for each key-value pair
(e.g., Tense=past) composing the tag.
3.2 Unsupervised Morphology
Since many languages into which we might want to
translate do not have supervised morphological an-
alyzers, we now turn to the question of how to gen-
erate morphological analyses and features using an
unsupervised analyzer. We hypothesize that perfect
decomposition into rich linguistic structures may not
be required for accurate generation of new inflected
forms. We will test this hypothesis by experimenting
with a simple, unsupervised model of morphology
that segments words into sequences of morphemes,
assuming a (na??ve) concatenative generation process
and a single analysis per type.
Unsupervised morphological segmentation. We
assume that each word can be decomposed into any
number of prefixes, a stem, and any number of suf-
fixes. Formally, we let M represent the set of all
possible morphemes and define a regular grammar
M?MM? (i.e., zero or more prefixes, a stem, and
zero or more suffixes). To infer the decomposition
structure for the words in the target language, we as-
sume that the vocabulary was generated by the fol-
lowing process:
1. Sample morpheme distributions from symmet-
ric Dirichlet distributions: ?p ? Dir|M |(?p)
for prefixes, ?? ? Dir|M |(??) for stems, and
?s ? Dir|M |(?s) for suffixes.
2. Sample length distribution parameters
?p ? Beta(?p, ?p) for prefix sequences
and ?s ? Beta(?s, ?s) for suffix sequences.
3. Sample a vocabulary by creating each word
type w using the following steps:
(a) Sample affix sequence lengths:
lp ? Geometric(?p);
ls ? Geometric(?s).
(b) Sample lp prefixes p1, . . . , plp indepen-
dently from ?p; ls suffixes s1, . . . , sls in-
dependently from ?s; and a stem ? ? ??.
(c) Concatenate prefixes, the stem, and suf-
fixes: w = p1+? ? ?+plp+?+s1+? ? ?+sls .
We use blocked Gibbs sampling to sample seg-
mentations for each word in the training vocabulary.
Because of our particular choice of priors, it possible
to approximately decompose the posterior over the
arcs of a compact finite-state machine. Sampling a
segmentation or obtaining the most likely segmenta-
tion a posteriori then reduces to familiar FST opera-
tions. This model is reminiscent of work on learning
morphology using adaptor grammars (Johnson et al,
2006; Johnson, 2008).
The inferred morphological grammar is very sen-
sitive to the Dirichlet hyperparameters (?p, ?s, ??)
and these are, in turn, sensitive to the number of
types in the vocabulary. Using ?p, ?s  ??  1
tended to recover useful segmentations, but we have
not yet been able to find reliable generic priors for
these values. Therefore, we selected them empiri-
cally to obtain a stem vocabulary size on the parallel
data that is one-to-one with English.4 Future work
4Our default starting point was to use ?p = ?s =
10?6, ?? = 10?4 and then to adjust all parameters by factors
of 10.
1680
Table 1: Corpus statistics.
Parallel Parallel+Monolingual
Sentences EN-tokens TRG-tokens EN-types TRG-types Sentences TRG-tokens TRG-types
Russian 150k 3.5M 3.3M 131k 254k 20M 360M 1,971k
Hebrew 134k 2.7M 2.0M 48k 120k 806k 15M 316k
Swahili 15k 0.3M 0.3M 23k 35k 596k 13M 334k
will involve a more direct method for specifying or
inferring these values.
Unsupervised morphology features: ?(?). For
the unsupervised analyzer, we do not have a map-
ping from morphemes to structured morphological
attributes; however, we can create features from the
affix sequences obtained after morphological seg-
mentation. We produce binary features correspond-
ing to the content of each potential affixation posi-
tion relative to the stem:
prefix      suffix
...-3 -2 -1 STEM +1 +2 +3...
For example, the unsupervised analysis ? =
wa+ki+wa+STEM of the Swahili word wakiwapiga
will produce the following features:
?prefix[?3][wa](?) = 1,
?prefix[?2][ki](?) = 1,
?prefix[?1][wa](?) = 1.
4 Inflection Model Parameter Estimation
To set the parametersW andV of the inflection pre-
diction model (Eq. 1), we use stochastic gradient de-
scent to maximize the conditional log-likelihood of
a training set consisting of pairs of source (English)
sentence contextual features (?) and target word in-
flectional features (?). The training instances are
extracted from the word-aligned parallel corpus with
the English side preprocessed as discussed in ?2.2
and the target side disambiguated as discussed in ?3.
When morphological category information is avail-
able, we train an independent model for each open-
class category (in Russian, nouns, verbs, adjectives,
numerals, adverbs); otherwise a single model is used
for all words (excluding words less than four char-
acters long, which are ignored).
Statistics of the parallel corpora used to train the
inflection model are summarized in Table 1. It is
important to note here that our richly parameterized
model is trained on the full parallel training cor-
pus, not just on a handful of development sentences
(which are typically used to tune MT system param-
eters). Despite this scale, training is simple: the in-
flection model is trained to discriminate among dif-
ferent inflectional paradigms, not over all possible
target language sentences (Blunsom et al, 2008) or
learning from all observable rules (Subotin, 2011).
This makes the training problem relatively tractable:
all experiments in this paper were trained on a sin-
gle processor using a Cython implementation of the
SGD optimizer. For our largest model, trained on
3.3M Russian words, n = 231K ? m = 336 fea-
tures were produced, and 10 SGD iterations were
performed in less than 16 hours.
4.1 Intrinsic Evaluation
Before considering the broader problem of integrat-
ing the inflection model in a machine translation
system, we perform an artificial evaluation to ver-
ify that the model learns sensible source sentence-
target inflection patterns. To do so, we create an
inflection test set as follows. We preprocess the
source (English) sentences exactly as during train-
ing (?2.2), and using the target language morpholog-
ical analyzer, we convert each aligned target word to
?stem, inflection? pairs. We perform word alignment
on the held-out MT development data for each lan-
guage pair (cf. Table 1), exactly as if it were going to
produce training instances, but instead we use them
for testing.
Although the resulting dataset is noisy (e.g., due
to alignment errors), this becomes our intrinsic eval-
uation test set. Using this data, we measure inflec-
tion quality using two measurements:5
5Note that we are not evaluating the stem translation model,
1681
acc. ppl. |??|
S
up
er
vi
se
d
Russian
N 64.1% 3.46 9.16
V 63.7% 3.41 20.12
A 51.5% 6.24 19.56
M 73.0% 2.81 9.14
average 63.1% 3.98 14.49
U
ns
up
. Russian all 71.2% 2.15 4.73
Hebrew all 85.5% 1.49 2.55
Swahili all 78.2% 2.09 11.46
Table 2: Intrinsic evaluation of inflection model (N:
nouns, V: verbs, A: adjectives, M: numerals).
? the accuracy of predicting the inflection given
the source, source context and target stem, and
? the inflection model perplexity on the same set
of test instances.
Additionally, we report the average number of pos-
sible inflections for each stem, an upper bound to the
perplexity that indicates the inherent difficulty of the
task. The results of this evaluation are presented in
Table 2 for the three language pairs considered. We
remark on two patterns in these results. First, per-
plexity is substantially lower than the perplexity of a
uniform model, indicating our model is overall quite
effective at predicting inflections using source con-
text only. Second, in the supervised Russian results,
we see that predicting the inflections of adjectives
is relatively more difficult than for other parts-of-
speech. Since adjectives agree with the nouns they
modify in gender and case, and gender is an idiosyn-
cratic feature of Russian nouns (and therefore not
directly predictable from the English source), this
difficulty is unsurprising.
We can also inspect the weights learned by the
model to assess the effectiveness of the features
in relating source-context structure with target-side
morphology. Such an analysis is presented in Fig. 3.
4.2 Feature Ablation
Our inflection model makes use of numerous fea-
ture types. Table 3 explores the effect of removing
different kinds of (source) features from the model,
evaluated on predicting Russian inflections using
supervised morphological grammars.6 Rows 2?3
just the inflection prediction model.
6The models used in the feature ablation experiment were
trained on fewer examples, resulting in overall lower accuracies
show the effect of removing either linear or depen-
dency context. We see that both are necessary for
good performance; however removing dependency
context substantially degrades performance of the
model (we interpret this result as evidence that Rus-
sian morphological inflection captures grammatical
relationships that would be expressed structurally in
English). The bottom four rows explore the effect
of source language word representation. The results
indicate that lexical features are important for accu-
rate prediction of inflection, and that POS tags and
Brown clusters are likewise important, but they seem
to capture similar information (removing one has lit-
tle impact, but removing both substantially degrades
performance).
Table 3: Feature ablation experiments using supervised
Russian classification experiments.
Features (?(e, i)) acc.
all 54.7%
?linear context 52.7%
?dependency context 44.4%
?POS tags 54.5%
?Brown clusters 54.5%
?POS tags, ?Brown cl. 50.9%
?lexical items 51.2%
5 Synthetic Phrases
We turn now to translation; recall that our translate-
and-inflect model is used to augment the set of rules
available to a conventional statistical machine trans-
lation decoder. We refer to the phrases it produces
as synthetic phrases.
Our baseline system is a standard hierarchical
phrase-based translation model (Chiang, 2007). Fol-
lowing Lopez (2007), the training data is compiled
into an efficient binary representation which allows
extraction of sentence-specific grammars just before
decoding. In our case, this also allows the creation
of synthetic inflected phrases that are produced con-
ditioning on the sentence to translate.
To generate these synthetic phrases with new in-
flections possibly unseen in the parallel training
than seen in Table 2, but the pattern of results is the relevant
datapoint here.
1682
Russian supervised
Verb: 1st Person
child(nsubj)=I child(nsubj)=we
Verb: Future tense
child(aux)=MD child(aux)=will
Noun: Animate
source=animals/victims/...
Noun: Feminine gender
source=obama/economy/...
Noun: Dative case
parent(iobj)
Adjective: Genitive case
grandparent(poss)
Hebrew
Suffix ?? (masculine plural)
parent=NNS after=NNS
Prefix ? (first person sing. + future)
child(nsubj)=I child(aux)='ll
Prefix ? (preposition like/as)
child(prep)=IN parent=as
Suffix ? (possesive mark)
before=my child(poss)=my
Suffix ? (feminine mark)
child(nsubj)=she before=she
Prefix ?? (when)
before=when before=WRB
Swahili
Prefix li (past)
source=VBD source=VBN
Prefix nita (1st person sing. + future)
child(aux) child(nsubj)=I
Prefix ana (3rd person sing. + present)
source=VBZ
Prefix wa (3rd person plural)
before=they child(nsubj)=NNS
Suffix tu (1st person plural)
child(nsubj)=she before=she
Prefix ha (negative tense)
source=no after=not
Figure 3: Examples of highly weighted features learned by the inflection model. We selected a few frequent morpho-
logical features and show their top corresponding source context features.
data, we first construct an additional phrase-based
translation model on the parallel corpus prepro-
cessed to replace inflected surface words with their
stems. We then extract a set of non-gappy phrases
for each sentence (e.g., X ? <attempted,
???????? V>). The target side of each such phrase
is re-inflected, conditioned on the source sentence,
using the inflection model from ?2. Each stem is
given its most likely inflection.7
The original features extracted for the stemmed
phrase are conserved, and the following features
are added to help the decoder select good synthetic
phrases:
? a binary feature indicating that the phrase is
synthetic,
? the log-probability of the inflected forms ac-
cording to our model,
? the count of words that have been inflected,
with a separate feature for each morphological
category in the supervised case.
Finally, these synthetic phrases are combined with
the original translation rules obtained for the base-
line system to produce an extended sentence-specific
grammar which is used as input to the decoder. If a
7Several reviewers asked about what happens when k-best
inflections are added. The results for k ? {2, 4, 8} range from
no effect to an improvement over k = 1 of about 0.2 BLEU
(absolute). We hypothesize that larger values of k could have a
greater impact, perhaps in a more ?global? model of the target
string; however, exploration of this question is beyond the scope
of this paper.
phrase already existing in the standard phrase table
happens to be recreated, both phrases are kept and
will compete with each other with different features
in the decoder.
For example, for the large EN?RU system, 6%
of all the rules used for translation are synthetic
phrases, with 65% of these phrases being entirely
new rules.
6 Translation Experiments
We evaluate our approach in the standard discrim-
inative MT framework. We use cdec (Dyer et al,
2010) as our decoder and perform MIRA training
to learn feature weights of the sentence translation
model (Chiang, 2012). We compare the following
configurations:
? A baseline system, using a 4-gram language
model trained on the entire monolingual and
bilingual data available.
? An enriched system with a class-based n-gram
language model8 trained on the monolingual
data mapped to 600 Brown clusters. Class-
based language modeling is a strong baseline
for scenarios with high out-of-vocabulary rates
but in which large amounts of monolingual
target-language data are available.
? The enriched system further augmented with
our inflected synthetic phrases. We expect the
class-based language model to be especially
8For Swahili and Hebrew, n = 6; for Russian, n = 7.
1683
helpful here and capture some basic agreement
patterns that can be learned more easily on
dense clusters than from plain word sequences.
Detailed corpus statistics are given in Table 1:
? The Russian data consist of the News Com-
mentary parallel corpus and additional mono-
lingual data crawled from news websites.9
? The Hebrew parallel corpus is composed of
transcribed TED talks (Cettolo et al, 2012).
Additional monolingual news data is also used.
? The Swahili parallel corpus was obtained by
crawling the Global Voices project website10
for parallel articles. Additional monolingual
data was taken from the Helsinki Corpus of
Swahili.11
We evaluate translation quality by translating and
measuring the BLEU score of a 2000?3000 sentence-
long evaluation corpus, averaging the results over 3
MIRA runs to control for optimizer instability (Clark
et al, 2011). Table 4 reports the results. For all lan-
guages, using class language models improves over
the baseline. When synthetic phrases are added, sig-
nificant additional improvements are obtained. For
the English?Russian language pair, where both su-
pervised and unsupervised analyses can be obtained,
we notice that expert-crafted morphological analyz-
ers are more efficient at improving translation qual-
ity. Globally, the amount of improvement observed
varies depending on the language; this is most likely
indicative of the quality of unsupervised morpholog-
ical segmentations produced and the kinds of gram-
matical relations expressed morphologically.
Finally, to confirm the effectiveness of our ap-
proach as corpus size increases, we use our tech-
nique on top of a state-of-the art English?Russian
system trained on data from the 8th ACL Work-
shop on Machine Translation (30M words of bilin-
gual text and 410M words of monolingual text). The
setup is identical except for the addition of sparse
9http://www.statmt.org/wmt13/
translation-task.html
10http://sw.globalvoicesonline.org
11http://www.aakkl.helsinki.fi/cameel/
corpus/intro.htm
Table 4: Translation quality (measured by BLEU) aver-
aged over 3 MIRA runs.
EN?RU EN?HE EN?SW
Baseline 14.7?0.1 15.8?0.3 18.3?0.1
+Class LM 15.7?0.1 16.8?0.4 18.7?0.2
+Synthetic
unsupervised 16.2?0.1 17.6?0.1 19.0?0.1
supervised 16.7?0.1 ? ?
rule shape indicator features and bigram cluster fea-
tures. In these large scale conditions, the BLEU score
improves from 18.8 to 19.6 with the addition of word
clusters and reaches 20.0 with synthetic phrases.
Details regarding this system are reported in Ammar
et al (2013).
7 Related Work
Translation into morphologically rich languages is
a widely studied problem and there is a tremen-
dous amount of related work. Our technique of syn-
thesizing translation options to improve generation
of inflected forms is closely related to the factored
translation approach proposed by Koehn and Hoang
(2007); however, an important difference to that
work is that we use a discriminative model that con-
ditions on source context to make ?local? decisions
about what inflections may be used before combin-
ing the phrases into a complete sentence translation.
Combination pre-/post-processing solutions are
also frequently proposed. In these, the tar-
get language is generally transformed from multi-
morphemic surface words into smaller units more
amenable to direct translation, and then a post-
processing step is applied independent of the trans-
lation model. For example, Oflazer and El-Kahlout
(2007) experiment with partial morpheme groupings
to produce novel inflected forms when translating
into Turkish; Al-Haj and Lavie (2010) compare dif-
ferent processing schemes for Arabic. A related but
different approach is to enrich the source language
items with grammatical features (e.g., a source sen-
tence like John saw Mary is preprocessed into, e.g.,
John+subj saw+msubj+fobj Mary+obj) so as
to make the source and target lexicons have simi-
lar morphological contrasts (Avramidis and Koehn,
2008; Yeniterzi and Oflazer, 2010; Chang et al,
1684
2009). In general, this work suffers from the prob-
lem that it is extremely difficult to know a priori
what the right preprocessing is for a given language
pair, data size, and domain.
Several post-processing approaches have relied
on supervised classifiers to predict the optimal com-
plete inflection for an incomplete or lemmatized
translation. Minkov et al (2007) present a method
for predicting the inflection of Russian and Arabic
sentences aligned to English sentences. They train a
sequence model to predict target morphological fea-
tures from the lemmas and the syntactic structures
of both aligned sentences and demonstrate its ability
to recover accurately inflections on reference trans-
lations. Toutanova et al (2008) apply this method
to generate inflections after translation in two differ-
ent ways: by rescoring inflected n-best outputs or by
translating lemmas and re-inflecting them a posteri-
ori. El Kholy and Habash (2012) follow a similar
method and compare different approaches for gen-
erating rich morphology in Arabic after a transla-
tion step. Fraser et al (2012) observe improvements
for translation into German with a similar method.
As in that work, we model morphological features
rather than directly inflected forms. However, that
work may be criticized for providing no mechanism
to translate surface forms directly, even when evi-
dence for a direct translation is available in the par-
allel data.
Unsupervised morphology has begun to play a
role in translation between morphologically com-
plex languages. Stallard et al (2012) show that an
unsupervised approach to Arabic segmentation per-
forms as well as a supervised segmenter for source-
side preprocessing (in terms of English translation
quality). For translation into morphological rich lan-
guages, Clifton and Sarkar (2011) use an unsuper-
vised morphological analyzer to produce morpho-
logical affixes in Finnish, injecting some linguistic
knowledge in the generation process.
Several authors have proposed using conditional
models to predict the probability of phrase transla-
tion in context (Gimpel and Smith, 2008; Chan et
al., 2007; Carpuat and Wu, 2007; Jeong et al, 2010).
Of particular note is the work of Subotin (2011),
who use a conditional model to predict morpholog-
ical features conditioned on rich linguistic features;
however, this latter work also conditions on target
context, which substantially complicates decoding.
Finally, synthetic phrases have been used for
different purposes than generating morphology.
Callison-Burch et al (2006) expanded the cov-
erage of a phrase table by adding synthesized
phrases by paraphrasing source language phrases,
Chen et al (2011) produced ?fabricated? phrases
by paraphrasing both source and target phrases, and
Habash (2009) created new rules to handle out-of-
vocabulary words. In related work, Tsvetkov et al
(2013) used synthetic phrases to improve generation
of (in)definite articles when translating into English
from Russian and Czech, two languages which do
not lexically mark definiteness.
8 Conclusion
We have presented an efficient technique that ex-
ploits morphologically analyzed corpora to produce
new inflections possibly unseen in the bilingual
training data. Our method decomposes into two
simple independent steps involving well-understood
discriminative models.
By relying on source-side context to generate ad-
ditional local translation options and by leaving the
choice of the full sentence translation to the decoder,
we sidestep the difficulty of computing features on
target translations hypotheses. However, many mor-
phological processes (most notably, agreement) are
most best modeled using target language context. To
capture target context effects, we depend on strong
target language models. Therefore, an important
extension of our work is to explore the interaction
of our approach with more sophisticated language
models that more directly model morphology, e.g.,
the models of Bilmes and Kirchhoff (2003), or, alter-
natively, ways to incorporate target language context
in the inflection model.
We also achieve language independence by
exploiting unsupervised morphological segmen-
tations in the absence of linguistically informed
morphological analyses.
Code for replicating the experiments is available from
https://github.com/eschling/morphogen;
further details are available in (Schlinger et al, 2013).
1685
Acknowledgments
This work was supported by the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533. We would
like to thank Kim Spasaro for curating the Swahili devel-
opment and test sets, Yulia Tsvetkov for assistance with
Russian, and the anonymous reviewers for their helpful
comments.
References
Hassan Al-Haj and Alon Lavie. 2010. The im-
pact of Arabic morphological segmentation on broad-
coverage English-to-Arabic statistical machine trans-
lation. In Proc. of AMTA.
Waleed Ammar, Victor Chahuneau, Michael Denkowski,
Greg Hanneman, Wang Ling, Austin Matthews, Ken-
ton Murray, Nicola Segall, Yulia Tsvetkov, Alon
Lavie, and Chris Dyer. 2013. The CMU machine
translation systems at WMT 2013: Syntax, synthetic
translation options, and pseudo-references. In Proc. of
WMT.
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proc. of ACL.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proc. of NAACL.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: parameter es-
timation. Computational Linguistics, 19(2):263?311.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Improved statistical machine transla-
tion using paraphrases. In Proc. of NAACL.
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proc. of EMNLP.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proc. of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL.
Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Man-
ning. 2009. Disambiguating ?DE? for Chinese?
English machine translation. In Proc. of WMT.
Boxing Chen, Roland Kuhn, and George Foster. 2011.
Semantic smoothing and fabrication of phrase pairs for
SMT. In Proc. of IWSLT.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2012. Hope and fear for discrimina-
tive training of statistical translation models. JMLR,
13:1159?1187.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In Proc. of ACL.
Ann Clifton and Anoop Sarkar. 2011. Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction. In Proc. of ACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc. of
ACL.
Ahmed El Kholy and Nizar Habash. 2012. Translate,
predict or generate: Modeling rich morphology in sta-
tistical machine translation. In Proc. of EAMT.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling inflection and word-
formation in SMT. In Proc. of EACL.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In
Proc. of WMT.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proc. of ACL.
Nizar Habash. 2009. REMOOV: A tool for online han-
dling of out-of-vocabulary words in machine transla-
tion. In Proceedings of the 2nd International Confer-
ence on Arabic Language Resources and Tools.
Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva, and
Vladim??r Petkevic?. 2001. Serial combination of rules
and statistics: A case study in Czech tagging. In Proc.
of ACL.
Dilek Z. Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.
2000. Statistical morphological disambiguation for
agglutinative languages. In Proc. of COLING.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and
Chris Quirk. 2010. A discriminative lexicon model
for complex morphology. In Proc. of AMTA.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2006. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
NIPS, pages 641?648.
Mark Johnson. 2008. Unsupervised word segmentation
for Sesotho using adaptor grammars. In Proc. SIG-
MORPHON.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proc. of EMNLP.
1686
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proc. of EMNLP.
Andre? F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and Ma?rio A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL.
Kemal Oflazer and I?lknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proc. of
WMT.
Eva Schlinger, Victor Chahuneau, and Chris Dyer. 2013.
morphogen: Translation into morphologically rich lan-
guages with synthetic phrases. Prague Bulletin of
Mathematical Linguistics, (100).
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a Russian tagset. In Proc. of LREC.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proc. of EMNLP.
David Stallard, Jacob Devlin, Michael Kayser,
Yoong Keok Lee, and Regina Barzilay. 2012.
Unsupervised morphology rivals supervised morphol-
ogy for Arabic MT. In Proc. of ACL.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. ACL.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proc. of ACL.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna Bha-
tia. 2013. Generating English determiners in phrase-
based translation with synthetic translation options. In
Proc. of WMT.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based statis-
tical machine translation from English to Turkish. In
Proc. of ACL.
1687
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1001?1012,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Dependency Parser for Tweets
Lingpeng Kong Nathan Schneider Swabha Swayamdipta
Archna Bhatia Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{lingpenk,nschneid,swabha,archna,cdyer,nasmith}@cs.cmu.edu
Abstract
We describe a new dependency parser for
English tweets, TWEEBOPARSER. The
parser builds on several contributions: new
syntactic annotations for a corpus of tweets
(TWEEBANK), with conventions informed
by the domain; adaptations to a statistical
parsing algorithm; and a new approach to
exploiting out-of-domain Penn Treebank
data. Our experiments show that the parser
achieves over 80% unlabeled attachment
accuracy on our new, high-quality test set
and measure the benefit of our contribu-
tions.
Our dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
1 Introduction
In contrast to the edited, standardized language of
traditional publications such as news reports, social
media text closely represents language as it is used
by people in their everyday lives. These informal
texts, which account for ever larger proportions of
written content, are of considerable interest to re-
searchers, with applications such as sentiment anal-
ysis (Greene and Resnik, 2009; Kouloumpis et al.,
2011). However, their often nonstandard content
makes them challenging for traditional NLP tools.
Among the tools currently available for tweets are
a POS tagger (Gimpel et al., 2011; Owoputi et al.,
2013) and a named entity recognizer (Ritter et al.,
2011)?but not a parser.
Important steps have been taken. The English
Web Treebank (Bies et al., 2012) represents an
annotation effort on web text?which likely lies
somewhere between newspaper text and social me-
dia messages in formality and care of editing?that
was sufficient to support a shared task (Petrov and
McDonald, 2012). Foster et al. (2011b) annotated
a small test set of tweets to evaluate parsers trained
on the Penn Treebank (Marcus et al., 1993), aug-
mented using semi-supervision and in-domain data.
Others, such as Soni et al. (2014), have used exist-
ing Penn Treebank?trained models on tweets.
In this work, we argue that the Penn Treebank
approach to annotation?while well-matched to
edited genres like newswire?is poorly suited to
more informal genres. Our starting point is that
rapid, small-scale annotation efforts performed
by imperfectly-trained annotators should provide
enough evidence to train an effective parser. We
see this starting point as a necessity, given observa-
tions about the rapidly changing nature of tweets
(Eisenstein, 2013), the attested difficulties of do-
main adaptation for parsing (Dredze et al., 2007),
and the expense of creating Penn Treebank?style
annotations (Marcus et al., 1993).
This paper presents TWEEBOPARSER, the first
syntactic dependency parser designed explicitly for
English tweets. We developed this parser follow-
ing current best practices in empirical NLP: we
annotate a corpus (TWEEBANK) and train the pa-
rameters of a statistical parsing algorithm. Our
research contributions include:
? a survey of key challenges posed by syntactic
analysis of tweets (by humans or machines) and
decisions motivated by those challenges and by
our limited annotation-resource scenario (?2);
? our annotation process and quantitative mea-
sures of the quality of the annotations (?3);
? adaptations to a statistical dependency parsing
algorithm to make it fully compatible with the
above, and also to exploit information from out-
of-domain data cheaply and without a strong
commitment (?4); and
? an experimental analysis of the parser?s unla-
beled attachment accuracy?which surpasses
80%?and contributions of various important
components (?5).
The dataset and parser can be found at http://www.
ark.cs.cmu.edu/TweetNLP.
1001
2 Annotation Challenges
Before describing our annotated corpus of tweets
(?3), we illustrate some of the challenges of syn-
tactic analysis they present. These challenges moti-
vate an approach to annotation that diverges signif-
icantly from conventional approaches to treebank-
ing. Figure 1 presents a single example illustrating
four of these: token selection, multiword expres-
sions, multiple roots, and structure within noun
phrases. We discuss each in turn.
2.1 Token Selection
Many elements in tweets have no syntactic function.
These include, in many cases, hashtags, URLs, and
emoticons. For example:
RT @justinbieber : now Hailee get a twitter
The retweet discourse marker, username, and colon
should not, we argue, be included in the syntactic
analysis. By contrast, consider:
Got #college admissions questions ? Ask them
tonight during #CampusChat I?m looking
forward to advice from @collegevisit
http://bit.ly/cchOTk
Here, both the hashtags and the at-mentioned user-
name are syntactically part of the utterances, while
the punctuation and the hyperlink are not. In the
example of Figure 1, the unselected tokens include
several punctuation tokens and the final token #be-
lieber, which marks the topic of the tweet.
Typically, dependency parsing evaluations ig-
nore punctuation token attachment (Buchholz and
Marsi, 2006), and we believe it is a waste of an-
notator (and parser) time to decide how to attach
punctuation and other non-syntactic tokens. Ma
et al. (2014) recently proposed to treat punctua-
tion as context features rather than dependents, and
found that this led to state-of-the-art performance
in a transition-based parser. A small adaptation
to our graph-based parsing approach, described in
?4.2, allows a similar treatment.
Our approach to annotation (?3) forces annota-
tors to explicitly select tokens that have a syntactic
function. 75.6% tokens were selected by the anno-
tators. Against the annotators? gold standard, we
found that a simple rule-based filter for usernames,
hashtags, punctuation, and retweet tokens achieves
95.2% (with gold-standard POS tags) and 95.1%
(with automatic POS tags) average accuracy in the
task of selecting tokens with a syntactic function
in a ten-fold cross-validation experiment. To take
context into account, we developed a first-order
sequence model and found that it achieves 97.4%
average accuracy (again, ten-fold cross-validated)
with either gold-standard or automatic POS tags.
Features include POS; shape features that recog-
nize the retweet marker, hashtags, usernames, and
hyperlinks; capitalization; and a binary feature
for tokens that include punctuation. We trained
the model using the structured perceptron (Collins,
2002).
2.2 Multiword Expressions
We consider multiword expressions (MWEs) of
two kinds. The first, proper names, have been
widely modeled for information extraction pur-
poses, and even incorporated into parsing (Finkel
and Manning, 2009). (An example found in Fig-
ure 1 is LA Times.) The second, lexical idioms,
have been a ?pain in the neck? for many years (Sag
et al., 2002) and have recently received shallow
treatment in NLP (Baldwin and Kim, 2010; Con-
stant and Sigogne, 2011; Schneider et al., 2014).
Constant et al. (2012), Green et al. (2012), Candito
and Constant (2014), and Le Roux et al. (2014)
considered MWEs in parsing. Figure 1 provides
LA Times and All the Rage as examples.
Penn Treebank?style syntactic analysis (and de-
pendency representations derived from it) does
not give first-class treatment to this phenomenon,
though there is precedent for marking multiword
lexical units and certain kinds of idiomatic relation-
ships (Haji
?
c et al., 2012; Abeill? et al., 2003).
1
We argue that internal analysis of MWEs is not
critical for many downstream applications, and
therefore annotators should not expend energy on
developing and respecting conventions (or mak-
ing arbitrary decisions) within syntactically opaque
or idiosyncratic units. We therefore allow annota-
tors to decide to group words as explicit MWEs,
including: proper names (Justin Bieber, World
Series), noncompositional or entrenched nominal
compounds (belly button, grilled cheese), connec-
tives (as well as), prepositions (out of), adverbials
(so far), and idioms (giving up, make sure).
From an annotator?s perspective, a MWE func-
tions as a single node in the dependency parse,
with no internal structure. For idioms whose in-
ternal syntax is easily characterized, the parse can
be used to capture compositional structure, an at-
1
The popular Stanford typed dependencies (de Marneffe
and Manning, 2008) scheme includes a special dependency
type for multiwords, though this is only applied to a small list.
1002
Helvetica font or similar (such as Arial). No italics. The current gray color for unselected tokens is fine.
#1385F0#17AD3F
#C82506
#FA9F1B
ROOT
COORD MWEmultiword expressionscoordination noun phrase internal structure
multiple roots
OMG I <3 the Biebs & want to have his babies ! ?> LATimes : Social Media ? #belieber
NOUN PHRASE INTERNAL STRUCTURE
NOUN PHRASE ?INTERNAL STRUCTUREOMG I ? the Biebs & want to have his babies ! ?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ? #belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWE
ROOT ROOT
? #belieber
Helvetica font or similar (such as Arial). No italics. The current gray color for unselected tokens is fine.
#1385F0#17AD3F
#C82506
#FA9F1B
ROOT
COORD MWEmultiword expressionscoordination noun phrase internal structure
multiple roots
OMG I <3 the Biebs & want to have his babies ! ?> LATimes : Social Media ? #belieber
NOUN PHRASE INTERNAL STRUCTURE
NOUN PHRASE ?INTERNAL STRUCTUREOMG I ? the Biebs & want to have his babies ! ?> LA Times : Teen Pop Star Heartthrob is All the Rage on Social Media ? #belieberROOT MULTIPLE ROOTSCOORD ROOT MWE MWE
ROOT ROOT
Figure 1: Parse tree for a (constructed) example illustrating annotation challenges discussed in ?2. Colors highlight token
selection (gray; ?2.1), multiword expressions (blue; ?2.2), multiple roots (red; ?2.3), coordination (dotted arcs, green; ?3.2), and
noun phrase internal structure (orange; ?2.4). The internal structure of multiword expressions (dashed arcs below the sentence)
was predicted automatically by a parser, as described in ?2.2.
tractive property from the perspective of semantic
processing.
To allow training a fairly conventional statisti-
cal dependency parser from these annotations, we
find it expedient to apply an automatic conversion
to the MWE annotations, in the spirit of Johnson
(1998). We apply an existing dependency parser,
the first-order TurboParser (Martins et al., 2009)
trained on the Penn Treebank, to parse each MWE
independently, assigning structures like those for
LA Times and All the Rage in Figure 1. Arcs
involving the MWE in the annotation are then re-
connected to the MWE-internal root, so that the re-
sulting tree respects the original tokenization. The
MWE-internal arcs are given a special label so that
the transformation can be reversed and MWEs re-
constructed from parser output.
2.3 Multiple Roots
For news text such as that found in the Penn Tree-
bank, sentence segmentation is generally consid-
ered a very easy task (Reynar and Ratnaparkhi,
1997). Tweets, however, often contain multiple
sentences or fragments, which we call ?utterances,?
each with its own syntactic root disconnected from
the others. The selected tokens in Figure 1 com-
prise four utterances.
Our approach to annotation allows multiple ut-
terances to emerge directly from the connectedness
properties of the graph implied by an annotator?s
decisions. Our parser allows multiple attachments
to the ?wall? symbol, so that multi-rooted analyses
can be predicted.
2.4 Noun Phrase Internal Structure
A potentially important drawback of deriving de-
pendency structures from phrase-structure annota-
tions, as is typically done using the Penn Treebank,
is that flat annotations lead to loss of information.
This is especially notable for noun phrases in the
Penn Treebank (Vadas and Curran, 2007). Consider
Teen Pop Star Heartthrob in Figure 1; Penn Tree-
bank conventions would label this as a single NP
with four NN children and no internal structure. De-
pendency conversion tools would likely attach the
first three words in the NP to Heartthrob. Direct de-
pendency annotation (rather than phrase-structure
annotation followed by automatic conversion) al-
lows a richer treatment of such structures, which is
potentially important for semantic analysis (Vecchi
et al., 2013).
3 A Twitter Dependency Corpus
In this section, we describe the TWEEBANK cor-
pus, highlighting data selection (?3.1), the annota-
tion process (?3.2), important convention choices
(?3.3), and measures of quality (?3.4).
3.1 Data Selection
We added manual dependency parses to 929 tweets
(12,318 tokens) drawn from the POS-tagged Twit-
ter corpus of Owoputi et al. (2013), which are tok-
enized and contain manually annotated POS tags.
Owoputi et al.?s data consists of two parts. The
first, originally annotated by Gimpel et al. (2011),
consists of tweets sampled from a particular day,
October 27, 2010?this is known as OCT27. Due
to concerns about overfitting to phenomena specific
to that day (e.g., tweets about a particular sports
game), Owoputi et al. (2013) created a new set of
547 tweets (DAILY547) consisting of one random
English tweet per day from January 2011 through
June 2012.
Our corpus is drawn roughly equally from
OCT27 and DAILY547.
2
Despite its obvious tem-
poral skew, there is no reason to believe this sample
is otherwise biased; our experiments in ?5 suggest
that this property is important.
3.2 Annotation
Unlike a typical treebanking project, which may
take years and involve thousands of person-hours
of work by linguists, most of TWEEBANK was built
in a day by two dozen annotators, most of whom
had only cursory training in the annotation scheme.
2
This results from a long-term goal to fully annotate both.
1003
(1) RT @FRIENDSHlP : Friendship is love without
kissing ...
Friendship > is < love < without < kissing
(2) bieber is an alien ! :O he went down to earth .
bieber > is** < alien < an
he > [went down]** < to < earth
(3) RT @YourFavWhiteGuy : Helppp meeeee . I?mmm
meltiiinngggg ? http://twitpic.com/316cjg
Helppp** < meeeee
I?mmm** < meltiiinngggg
Figure 2: Examples of GFL annotations from the corpus.
Our annotators used the Graph Fragment Lan-
guage (GFL), a text-based notation that facilitates
keyboard entry of parses (Schneider et al., 2013). A
Python Flask web application allows the annotator
to validate and visualize each parse (Mordowanec
et al., 2014). Some examples are shown in Fig-
ure 2. Note that all of the challenges in ?2 are
handled easily by GFL notation: ?retweet? infor-
mation, punctuation, and a URL are not selected by
virtue of their exclusion from the GFL expression;
in (2) went down is annotated as a MWE using
GFL?s square bracket notation; in (3) the tokens
are grouped into two utterances whose roots are
marked by the ** symbol.
Schneider et al.?s GFL offers some additional fea-
tures, only some of which we made use of in this
project. One important feature allows an annotator
to leave the parse underspecified in some ways. We
allowed our annotators to make use of this feature;
however, we excluded from our training and test-
ing data any parse that was incomplete (i.e., any
parse that contained multiple disconnected frag-
ments with no explicit root, excluding unselected
tokens). Learning to parse from incomplete anno-
tations is a fascinating topic explored in the past
(Hwa, 2001; Pereira and Schabes, 1992) and, in the
case of tweets, left for future work.
An important feature of GFL that we did use is
special notation for coordination structures. For
the coordination structure in Figure 1, for example,
the notation is:
$a :: {? want} :: {&}
where $a creates a new node in the parse tree as it is
visualized for the annotator, and this new node at-
taches to the syntactic parent of the conjoined struc-
ture, avoiding the classic forced choice between
coordinator and conjunct as parent. For learning to
parse, we transform GFL?s coordination structures
into specially-labeled dependency parses collaps-
ing nodes like $a with the coordinator and labeling
the attachments specially for postprocessing, fol-
lowing Schneider et al. (2013). In our evaluation
(?5), these are treated like other attachments.
3.3 Annotation Conventions
A wide range of dependency conventions are in use;
in many cases these are conversion conventions
specifying how dependency trees can be derived
from phrase-structure trees. For English, the most
popular are due to Yamada and Matsumoto (2003)
and de Marneffe and Manning (2008), known as
?Yamada-Matsumoto? (YM) and ?Stanford? depen-
dencies, respectively. The main differences be-
tween them are in whether the auxiliary is the par-
ent of the main verb (or vice versa) and whether the
preposition or its argument heads a prepositional
phrase (Elming et al., 2013).
A full discussion of our annotation conventions
is out of scope. We largely followed the conven-
tions suggested by Schneider et al. (2013), which in
turn are close to those of YM. Auxiliary verbs are
parents of main verbs, and prepositions are parents
of their arguments. The key differences from YM
are in coordination structures (discussed in ?3.2;
YM makes the first conjunct the head) and posses-
sive structures, in which the possessor is the child
of the clitic, which is the child of the semantic head,
e.g., the > king > ?s > horses.
3.4 Intrinsic Quality
Our approach to developing this initial corpus of
syntactically annotated tweets was informed by an
aversion to making the perfect the enemy of the
good; that is, we sought enough data of sufficient
quality to build a usable parser within a relatively
short amount of time. If our research goals had
been to develop a replicable process for annotation,
more training and more quality control would have
been called for. Under our budgeted time and anno-
tator resources, this overhead was simply too costly.
Nonetheless, we performed a few analyses that give
a general picture of the quality of the annotations.
Inter-annotator agreement. 170 of the tweets
were annotated by multiple users. By the softCom-
Prec measure (Schneider et al., 2013),
3
the agree-
ment rate on dependencies is above 90%.
Expert linguistic judgment. A linguist co-
author examined a stratified sample (balanced
3
softComPrec is a generalization of attachment accuracy
that handles unselected tokens and MWEs.
1004
across annotators) of 60 annotations and rated their
quality on a 5-point scale. 30 annotations were
deemed to have ?no obvious errors,? 15 only minor
errors, 3 a major error (i.e., clear violation of an-
notation guidelines),
4
4 a major error and at least
one minor error, and 8 as containing multiple major
errors. Thus, 75% are judged as having no major
errors. We found this encouraging, considering that
this sample is skewed in favor of people who anno-
tated less (including many of the less experienced
and/or lower-proficiency annotators).
Pairwise ranking. For 170 of the doubly anno-
tated tweets, an experienced annotator examined
whether one or the other was markedly better. In
100 cases the two annotations were of comparable
quality (neither was obviously better) and did not
contain any obvious major errors. In only 7 pairs
did both of the annotations contain a serious error.
Qualitatively, we found several unsurprising
sources of error or disagreement, including em-
bedded/subordinate clauses, subject-auxiliary in-
version, predeterminers, and adverbial modifiers
following a modal/auxiliary verb and a main verb.
Clarification of the conventions, or even explicit
rule-based checking in the validation step, might
lead to quality improvements in further annotation
efforts.
4 Parsing Algorithm
For parsing, we start with TurboParser, which is
open-source and has been found to perform well on
a range of parsing problems in different languages
(Martins et al., 2013; Kong and Smith, 2014). The
underlying model allows for flexible incorporation
of new features and changes to specification in the
output space. We briefly review the key ideas in
TurboParser (?4.1), then describe decoder modifi-
cations required for our problem (?4.2). We then
discuss features we added to TurboParser (?4.3).
4.1 TurboParser
Let an input sentence be denoted by x and the set
of possible dependency parses for x be denoted by
Y
x
. A generic linear scoring function based on a
4
What we deemed major errors included, for example,
an incorrect dependency relation between an auxiliary verb
and the main verb (like ima > [have to]). Minor errors
included an incorrect attachment between two modifiers of
the same head, as in the > only > [grocery store]?the
correct annotation would have two attachments to a single
head, i.e. the > [grocery store] < only (or equivalent).
feature vector representation g is used in parsing
algorithms that seek to find:
parse
?(x) = argmax
y?Y
x
w
?
g(x,y) (1)
The score is parameterized by a vector w of
weights, which are learned from data (most com-
monly using MIRA, McDonald et al., 2005a).
The decomposition of the features into local
?parts? is a critical choice affecting the computa-
tional difficulty of solving Eq. 1. The most aggres-
sive decomposition leads to an ?arc-factored? or
?first-order? model, which permits exact, efficient
solution of Eq. 1 using spanning tree algorithms
(McDonald et al., 2005b) or, with a projectivity
constraint, dynamic programming (Eisner, 1996).
Second- and third-order models have also been
introduced, typically relying on approximations,
since less-local features increase the computational
cost, sometimes to the point of NP-hardness (Mc-
Donald and Satta, 2007). TurboParser attacks the
parsing problem using a compact integer linear pro-
gramming (ILP) representation of Eq. 1 (Martins
et al., 2009), then employing alternating directions
dual decomposition (AD
3
; Martins et al., 2011).
This enables inclusion of second-order features
(e.g., on a word with its sibling or grandparent;
Carreras, 2007) and third-order features (e.g., a
word with its parent, grandparent, and a sibling, or
with its parent and two siblings; Koo and Collins,
2010).
For a collection of (possibly overlapping) parts
for input x, S
x
(which includes the union of all
parts of all trees in Y
x
), we will use the following
notation. Let
g(x,y) = ?
s?S
x
f
s
(x,y), (2)
where f
s
only considers part s and is nonzero only
if s is present in y. In the ILP framework, each s
has a corresponding binary variable z
s
indicating
whether part s is included in the output. A col-
lection of constraints relating z
s
define the set of
feasible vectors z that correspond to valid outputs
and enfore agreement between parts that overlap.
Many different versions of these constraints have
been studied (Riedel and Clarke, 2006; Smith and
Eisner, 2008; Martins et al., 2009, 2010).
A key attraction of TurboParser is that many
overlapping parts can be handled, making use of
separate combinatorial algorithms for efficiently
handling subsets of constraints. For example, the
constraints that force z to encode a valid tree can
be exploited within the framework by making calls
1005
to classic arborescence algorithms (Chu and Liu,
1965; Edmonds, 1967). As a result, when describ-
ing modifications to TurboParser, we need only to
explain additional constraints and features imposed
on parts.
4.2 Adapted Parse Parts
The first collection of parts we adapt are simple
arcs, each consisting of an ordered pair of indices
of words in x; arc(p,c) corresponds to the attach-
ment of x
c
as a child of x
p
(iff z
arc(p,c) = 1). Our rep-
resentation explicitly excludes some tokens from
being part of the syntactic analysis (?2.1); to han-
dle this, we constrain z
arc(i, j) = 0 whenever xi or x j
is excluded.
The implication is that excluded tokens are still
?visible? to feature functions that involve other
edges. For example, some conventional first-order
features consider the tokens occurring between a
parent and child. Even if a token plays no syntactic
role of its own, it might still be informative about
the syntactic relationships among other tokens. We
note three alternative methods:
1. We might remove all unselected tokens from
x before running the parser. In ?5.6 we find
this method to fare 1.7?2.3% worse than our
modified decoding algorithm.
2. We might remove unselected tokens but use
them to define new features, so that they still
serve as evidence. This is the approach taken
by Ma et al. (2014) for punctuation. We judge
our simple modification to the decoding algo-
rithm to be more expedient, and leave the trans-
lation of existing context-word features into that
framework for future exploration.
3. We might incorporate the token selection deci-
sions into the parser, performing joint inference
for selection and parsing. The AD
3
algorithm
within TurboParser is well-suited to this kind
of extension: z-variables for each token?s se-
lection could be added, and similar scores to
those of our token selection sequence model
(?2.1) could be integrated into parsing. Given,
however, that the sequence model achieves over
97% accuracy, and that perfect token selection
would gain only 0.1?1% in parsing accuracy (re-
ported in ?5.5), we leave this option for future
work as well.
For first-order models, the above change is all
that is necessary. For second- and third-order
models, TurboParser makes use of head automata,
in particular ?grand-sibling head automata? that
assign scores to word tuples of x
g
, its child x
p
,
and two of x
p
?s adjacent children, x
c
and x
?
c
(Koo
et al., 2010). The second-order models in our
experiments include parts for sibling(p,c,c?) and
grandparent(p,c,g) and use the grand-sibling head
automaton to reason about these together. Au-
tomata for an unselected x
p
or x
g
, and transitions
that consider unselected tokens as children, are
eliminated. In order to allow the scores to depend
on unselected tokens between x
c
and x
?
c
, we added
the binned counts of unselected tokens (mostly
punctuation) joint with the word form and POS
tag of x
p
and the POS tag of x
c
and x
?
c
as features
scored in the sibling(p,c,c?) part. The changes dis-
cussed above comprise the totality of adaptations
we made to the TurboParser algorithm; we refer to
them as ?parsing adaptations? in the experiments.
4.3 Additional Features
Brown clusters. Owoputi et al. (2013) found that
Brown et al. (1992) clusters served as excellent fea-
tures in Twitter POS tagging. Others have found
them useful in parsing (Koo et al., 2008) and other
tasks (Turian et al., 2010). We therefore follow
Koo et al. in incorporating Brown clusters as fea-
tures, making use of the publicly available Twitter
clusters from Owoputi et al.
5
We use 4 and 6 bit
cluster representations to create features wherever
POS tags are used, and full bit strings to create
features wherever words were used.
Penn Treebank features. A potential danger of
our choice to ?start from scratch? in developing
a dependency parser for Twitter is that the result-
ing annotation conventions, data, and desired out-
put are very different from dependency parses de-
rived from the Penn Treebank. Indeed, Foster et al.
(2011a) took a very different approach, applying
Penn Treebank conventions in annotation of a test
dataset for evaluation of a parser trained using Penn
Treebank trees. In ?5.4, we replicate, for depen-
dencies, their finding that a Penn Treebank?trained
parser is hard to beat on their dataset, which was
not designed to be topically representative of En-
glish Twitter. When we turn to a more realistic
dataset like ours, we find the performance of the
Penn Treebank?trained parser to be poor.
Nonetheless, it is hard to ignore such a large
amount of high-quality syntactic data. We there-
5http://www.ark.cs.cmu.edu/TweetNLP/clusters/
50mpaths2
1006
fore opted for a simple, stacking-inspired incor-
poration of Penn Treebank information into our
model.
6
We define a feature on every candidate arc
whose value is the (quantized) score of the same arc
under a first-order model trained on the Penn Tree-
bank converted using head rules that are as close
as possible to our conventions (discussed in more
detail in ?5.1). This lets a Penn Treebank model
literally ?weigh in? on the parse for a tweet, and
lets the learning algorithm determine how much
consideration it deserves.
5 Experiments
Our experiments quantify the contributions of vari-
ous components of our approach.
5.1 Setup
We consider two test sets. The first, TEST-NEW,
consists of 201 tweets from our corpus annotated
by the most experienced of our annotators (one
of whom is a co-author of this work). Given very
limited data, we believe using the highest quality
data for measuring performance, and lower-quality
data for training, is a sensibly realistic choice.
Our second test set, TEST-FOSTER, is the dataset
annotated by Foster et al. (2011b), which consists
of 250 sentences. Recall that their corpus was
annotated with phrase structures according to Penn
Treebank conventions. Conversion to match our
annotation conventions was carried out as follows:
1. We used the PennConverter tool with head rule
options selected to approximate our annotation
conventions as closely as possible.
7
2. An experienced annotator manually modified
the automatically converted trees by:
(a) Performing token selection (?2.1) to remove
the tokens which have no syntactic function.
(b) Grouping MWEs (?2.2). Here, most of the
MWEs are named entities such as Manch-
ester United.
(c) Attaching the roots of the utterance in tweets
to the ?wall? symbol (?2.3).
8
6
Stacking is a machine learning method where the predic-
tions of one model are used to create features for another. The
second model may be from a different family. Stacking has
been found successful for dependency parsing by Nivre and
McDonald (2008) and Martins et al. (2008). Johansson (2013)
describes further advances that use path features.
7http://nlp.cs.lth.se/software/treebank_
converter; run with -rightBranching=false
-coordStructure=prague -prepAsHead=true
-posAsHead=true -subAsHead=true -imAsHead=true
-whAsHead=false.
8
This was infrequent; their annotations split most multi-
TRAIN TEST-NEW TEST-FOSTER
tweets 717 201 < 250?
unique tweets 569 201 < 250?
tokens 9,310 2,839 2,841
selected tokens 7,015 2,158 2,366
types 3,566 1,461 1,230
utterances 1,473 429 337
multi-root tweets 398 123 60
MWEs 387 78 109
Table 1: Statistics of our datasets. (A tweet with k annotations
in the training set is counted k times for the totals of tokens,
utterances, etc.).
?
TEST-FOSTER contains 250 manually split
sentences. The number of tweets should be smaller but is not
recoverable from the data release.
(d) Recovering the internal structure of the noun
phrases.
(e) Fixing a difference in conventions with re-
spect to subject-auxiliary inversion.
9
We consider two training sets. TRAIN-NEW con-
sists of the remaining 717 tweets from our corpus
(?3) annotated by the rest of the annotators. Some
of these tweets have annotations from multiple an-
notators; 11 annotations for tweets that also oc-
curred in TEST-NEW were excluded. TRAIN-PTB
is the conventional training set from the Penn Tree-
bank (?2?21). The PennConverter tool was used
to extract dependencies, with head rule options se-
lected to approximate our annotation conventions
as closely as possible (see footnote 7). The result-
ing annotations lack the same attention to noun
phrase?internal structure (?2.4) and handle subject-
auxiliary inversions differently than our data. Part-
of-speech tags were coarsened to be compatible
with the Twitter POS tags, using the mappings spec-
ified by Gimpel et al. (2011).
Statistics for the in-domain datasets are given in
Table 1. As we can see in the table, more than half
of the tweets in our corpus have multiple utterances.
The out-of-vocabulary rate for our TRAIN/TEST-
NEW split is 33.7% by token and 62.5% by type;
for TRAIN/TEST-FOSTER it is 41.4% and 64.6%
respectively. These are much higher than the 2.5%
and 13.2% in the standard Penn Treebank split.
All evaluations here are on unlabeled attachment
F
1
scores.
10
Our parser provides labels for coordi-
nation structures and MWEs (?2), but we do not
present detailed evaluations of those due to space
constraints.
utterance tweets into separate sentence-instances.
9
For example, in the sentence Is he driving, we attached
he to driving while PennConverter attaches it to Is.
10
Because of token selection, precision and recall may not
be equal.
1007
5.2 Preprocessing
Because some of the tweets in our test set were
also in the training set of Owoputi et al. (2013),
we retrained their POS tagger on all the annotated
data they have minus the 201 tweets in our test
set. Its tagging accuracy was 92.8% and 88.7% on
TEST-NEW and TEST-FOSTER, respectively. The
token selection model (?2.1) achieves 97.4% on
TEST-NEW with gold or automatic POS tagging;
and on TEST-FOSTER, 99.0% and 99.5% with gold
and automatic POS tagging, respectively.
As noted in ?4.3, Penn Treebank features were
developed using a first-order TurboParser trained
on TRAIN-PTB; Brown clusters were included in
computing these Penn Treebank features if they
were available in the parser to which the features
(i.e. Brown clusters) were added.
5.3 Main Parser
The second-order TurboParser described in ?4,
trained on TRAIN-NEW (default hyperparameter
values), achieves 80.9% unlabeled attachment ac-
curacy on TEST-NEW and 76.1% on TEST-FOSTER.
The experiments consider variations on this main
approach, which is the version released as TWEE-
BOPARSER.
The discrepancy between the two test sets is
easily explained: as noted in ?3.1, the dataset
from which our tweets are drawn was designed
to be representative of English on Twitter. Fos-
ter et al. (2011b) selected tweets from Berming-
ham and Smeaton?s (2010) corpus, which uses fifty
predefined topics like politics, business, sports,
and entertainment?in short, topics not unlike
those found in the Penn Treebank. Relative to
the Penn Treebank training set, the by-type out-
of-vocabulary rates are 45.2% for TEST-NEW and
only 21.6% for TEST-FOSTER (cf. 13.2% for the
Penn Treebank test set).
Another mismatch is in the handling of utter-
ances. In our corpus, utterance segmentation
emerges from multi-rooted annotations (?2.3). Fos-
ter et al. (2011b) manually split each tweet into
utterances and treat those as separate instances in
their corpus, so that our model trained on often
multi-rooted tweets from TRAIN is being tested
only on single-rooted utterances.
5.4 Experiment: Which Training Set?
We consider the direct use of TRAIN-PTB instead
of TRAIN-NEW. Table 2 reports the results on both
Unlabeled Attachment F
1
(%)
mod. POS POS as-is
TEST-NEW
Baseline 73.0 73.5
+ Brown 73.7 73.3
+ Brown & PA 72.9 73.1
TEST-FOSTER
Baseline 76.3 75.2
+ Brown 75.5 76.7
+ Brown & PA 76.9 77.0
Table 2: Performance of second-order TurboParser trained on
TRAIN-PTB, with various preprocessing options. The main
parser (?5.3) achieves 80.9% and 76.1% on the two test sets,
respectively; see ?5.4 for discussion.
test sets, with various options. ?Baseline? is off-
the-shelf second-order TurboParser. We consider
augmenting it with Brown cluster features (?4.3;
?+ Brown?) and then also with the parsing adapta-
tions of ?4.2 (?+ Brown & PA?). Another choice
is whether to modify the POS tags at test time; the
modified version (?mod. POS?) maps at-mentions
to pronoun, and hashtags and URLs to noun.
We note that comparing these scores to our main
parser (?5.3) conflates three very important inde-
pendent variables: the amount of training data
(39,832 Penn Treebank sentences vs. 1,473 Twitter
utterances), the annotation method, and the source
of the data. However, we are encouraged that, on
what we believe is the superior test set (TEST-NEW),
our overall approach obtains a 7.8% gain with an
order of magnitude less annotated data.
5.5 Experiment: Effect of Preprocessing
Table 3 (second block, italicized) shows the per-
formance of the main parser on both test sets with
gold-standard and automatic POS tagging and to-
ken selection. On TEST-NEW, with either gold-
standard POS tags or gold-standard token selection,
performance increases by 1.1%; with both, it in-
creases by 2.3%. On TEST-FOSTER, token selec-
tion matters much less, but POS tagging accounts
for a drop of more than 6%. This is consistent with
Foster et al.?s finding: using a fine-grained Penn
Treebank?trained POS tagger (achieving around
84% accuracy on Twitter), they saw 5?8% improve-
ment in unlabeled dependency attachment accuracy
using gold-standard POS tags.
5.6 Experiment: Ablations
We ablated each key element of our main parser?
PTB features, Brown features, second order fea-
tures and decoding, and the parsing adaptations of
1008
0.76
0.78
0.8
0.82
Unla
beled
 Atta
chme
nt F 1
 
 
Baselin
e ?PA?PTB ?P
A
?Brow
n ?PTB?Brow
n ?PA ?PTB ?Brow
n Main
First?OrderSecond?Order
(a) TEST-NEW
0.7
0.72
0.74
0.76
Unla
beled
 Atta
chme
nt F 1
Baselin
e ?PA?PTB ?P
A
?Brow
n ?PTB?Brow
n ?PA ?PTB ?Brow
n Main
 
 First?OrderSecond?Order
(b) TEST-FOSTER
Figure 3: Feature ablations; these charts present the same scores shown in Table 3 and more variants of the first-order model.
Unlabeled Attachment F
1
(%)
TEST-NEW TEST-FOSTER
Main parser 80.9 76.1
Gold POS and TS 83.2 82.8
Gold POS, automatic TS 82.0 82.3
Automatic POS, gold TS 82.0 76.2
Single ablations:
? PTB 80.2 72.6
? Brown 81.2 75.4
? 2nd order 80.1 75.6
? PA 79.2 73.7
Double ablations:
? PTB, ? Brown 79.5 72.8
? PTB, ? 2nd order 78.5 72.2
? PTB, ? PA 77.4 69.6
? Brown, ? 2nd order 80.7 74.5
? Brown, ? PA 78.2 73.7
? 2nd order, ? PA 77.7 73.5
Baselines:
Second order 76.5 70.4
First order 76.1 70.4
Table 3: Effects of gold-standard POS tagging and token
selection (TS; ?5.5) and of feature ablation (?5.6). The ?base-
lines? are TurboParser without the parsing adaptations in ?4.2
and without Penn Treebank or Brown features. The best result
in each column is bolded. See also Figure 3.
?4.2?as well as each pair of these. These condi-
tions use automatic POS tags and token selection.
The ?? PA? condition, which ablates parsing adap-
tations, is accomplished by deleting punctuation
(in training and test data) and parsing using Turbo-
Parser?s existing algorithm.
Results are shown in Table 3. Further results
with first- and second-order TurboParsers are plot-
ted in Figure 3. Notably, a 2?3% gain is obtained by
modifying the parsing algorithm, and our stacking-
inspired use of Penn Treebank data contributes in
both cases, quite a lot on TEST-FOSTER (unsur-
prisingly given that test set?s similarity to the Penn
Treebank). More surprisingly, we find that Brown
cluster features do not consistently improve perfor-
mance, at least not as instantiated here, with our
small training set.
6 Conclusion
We described TWEEBOPARSER, a dependency
parser for English tweets that achieves over 80%
unlabeled attachment score on a new, high-quality
test set. This is on par with state-of-the-art re-
ported results for news text in Turkish (77.6%;
Koo et al., 2010) and Arabic (81.1%; Martins
et al., 2011). Our contributions include impor-
tant steps taken to build the parser: a considera-
tion of the challenges of parsing tweets that in-
formed our annotation process, the resulting new
TWEEBANK corpus, adaptations to a statistical
parsing algorithm, a new approach to exploiting
data in a better-resourced domain (the Penn Tree-
bank), and experimental analysis of the decisions
we made. The dataset and parser can be found at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgments
The authors thank the anonymous reviewers
and Andr? Martins, Yanchuan Sim, Wang Ling,
Michael Mordowanec, and Alexander Rush for
helpful feedback, as well as the annotators Waleed
Ammar, Jason Baldridge, David Bamman, Dallas
Card, Shay Cohen, Jesse Dodge, Jeffrey Flanigan,
Dan Garrette, Lori Levin, Wang Ling, Bill Mc-
Dowell, Michael Mordowanec, Brendan O?Connor,
Rohan Ramanath, Yanchuan Sim, Liang Sun, Sam
Thomson, and Dani Yogatama. This research was
supported in part by the U. S. Army Research Lab-
oratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533 and by
NSF grants IIS-1054319 and IIS-1352440.
1009
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Treebanks,
pages 165?187. Springer.
Timonthy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Handbook of Natural Lan-
guage Processing, Second Edition. CRC Press, Tay-
lor and Francis Group.
Adam Bermingham and Alan F. Smeaton. 2010. Clas-
sifying sentiment in microblogs: Is brevity an advan-
tage? In Proc. of CIKM.
Ann Bies, Justin Mott, Colin Warner, and Seth
Kulick. 2012. English Web Treebank. Techni-
cal Report LDC2012T13, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T13.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL.
Marie Candito and Matthieu Constant. 2014. Strate-
gies for contiguous multiword expression analysis
and dependency parsing. In Proc. of ACL.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proc. of
EMNLP-CoNLL.
Yoeng-Jin Chu and Tseng-Hong Liu. 1965. On shortest
arborescence of a directed graph. Scientia Sinica,
14(10):1396.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to
the Real World.
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate mul-
tiword expression recognition and parsing. In Proc.
of ACL.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Proc. of COLING Workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mark Dredze, John Blitzer, Partha Pratim Taluk-
dar, Kuzman Ganchev, Joao Graca, and Fernando
Pereira. 2007. Frustratingly hard domain adapta-
tion for dependency parsing. In Proc. of EMNLP-
CoNLL.
Jack Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards B,
71(233-240):160.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proc. of NAACL-HLT.
Jason Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proc. of
COLING.
Jakob Elming, Anders Johannsen, Sigrid Klerke,
Emanuele Lapponi, H?ctor Mart?nez Alonso, and
Anders S?gaard. 2013. Down-stream effects of tree-
to-dependency conversions. In Proc. of NAACL-
HLT.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Proc
of ACL-HLT.
Jennifer Foster, ?zlem ?etinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011a.
#hardtoparse: POS tagging and parsing the Twitter-
verse. In Proc. of AAAI Workshop on Analyzing Mi-
crotext.
Jennifer Foster, ?zlem ?etino?glu, Joachim Wagner,
Joseph Le Roux, Joakim Nivre, Deirdre Hogan, and
Josef van Genabith. 2011b. From news to comment:
resources and benchmarks for parsing the language
of Web 2.0. In Proc. of IJCNLP.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proc. of ACL-HLT.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D. Manning. 2012. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195?227.
Stephan Greene and Philip Resnik. 2009. Syntac-
tic packaging and implicit sentiment. In Proc. of
NAACL.
1010
Jan Haji?c, Eva Haji?cov?, Jarmila Panevov?, Petr Sgall,
Silvie Cinkov?, Eva Fu?c?kov?, Marie Mikulov?,
Petr Pajas, Jan Popelka, Ji?r? Semeck`y, Jana
?indlerov?, Jan ?t?ep?nek, Josef Toman, Zde?nka
Ure?ov?, and Zden?ek ?abokrtsk?. 2012. Prague
Czech-English Dependency Treebank 2.0. Techni-
cal Report LDC2012T08, Linguistic Data Consor-
tium. URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2012T08.
Rebecca Hwa. 2001. Learning Probabilistic Lexical-
ized Grammars for Natural Language Processing.
Ph.D. thesis, Harvard University.
Richard Johansson. 2013. Training parsers on incom-
patible treebanks. In Proc. of NAACL-HLT.
Mark Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613?632.
Lingpeng Kong and Noah A. Smith. 2014. An empiri-
cal comparison of parsing methods for Stanford de-
pendencies. ArXiv:1404.4314.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In
Proc. of ACL.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Terry Koo, Alexander M. Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual de-
composition for parsing with non-projective head au-
tomata. In Proc. of EMNLP.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proc. of ICWSM.
Joseph Le Roux, Matthieu Constant, and Antoine
Rozenknop. 2014. Syntactic parsing and compound
recognition via dual decomposition: application to
French. In Proc. of COLING.
Ji Ma, Yue Zhang, and Jingbo Zhu. 2014. Punctua-
tion processing for projective dependency parsing.
In Proc. of ACL.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional linguistics, 19(2):313?330.
Andr? F.T. Martins, Miguel Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proc. of ACL.
Andr? F.T. Martins, Dipanjan Das, Noah A. Smith, and
Eric P. Xing. 2008. Stacking dependency parsers. In
Proc. of EMNLP.
Andr? F.T. Martins, Noah A. Smith, Pedro M.Q.
Aguiar, and M?rio A.T. Figueiredo. 2011. Dual de-
composition with many overlapping components. In
Proc. of EMNLP.
Andr? F.T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proc. of ACL-
IJCNLP.
Andr? F.T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M.Q. Aguiar, and M?rio A.T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of EMNLP.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proc. of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji
?
c. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of HLT-
EMNLP.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency
parsing. In Proc. of IWPT.
Michael T. Mordowanec, Nathan Schneider, Chris.
Dyer, and Noah A. Smith. 2014. Simplified depen-
dency annotations with GFL-Web. In Proc. of ACL,
demonstration track.
Joakim Nivre and Ryan McDonald. 2008. Integrat-
ing graph-based and transition-based dependency
parsers. In Proc. of ACL-HLT.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proc. of NAACL-HLT.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proc. of ACL.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 Shared Task on Parsing the Web. In Notes
of the First Workshop on Syntactic Analysis of Non-
Canonical Language.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sentence
boundaries. In Proc. of ANLP.
1011
Sebastian Riedel and James Clarke. 2006. Incremen-
tal integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an ex-
perimental study. In Proc. of EMNLP.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Proc. of
CICLing.
Nathan Schneider, Emily Danchik, Chris Dyer, and
Noah A. Smith. 2014. Discriminative lexical se-
mantic segmentation with gaps: Running the MWE
gamut. Transactions of the Association for Compu-
tational Linguistics, 2:193?206.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse.
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proc. of EMNLP.
Sandeep Soni, Tanushree Mitra, Eric Gilbert, and Jacob
Eisenstein. 2014. Modeling factuality judgments in
social media text. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the Penn Treebank. In Proc. of
ACL.
Eva Maria Vecchi, Roberto Zamparelli, and Marco Ba-
roni. 2013. Studying the recursive behaviour of
adjectival modification with compositional distribu-
tional semantics. In Proc. of EMNLP.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proc. of IWPT.
1012
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1487?1498,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Language Modeling with Power Low Rank Ensembles
Ankur P. Parikh
School of Computer Science
Carnegie Mellon University
apparikh@cs.cmu.edu
Avneesh Saluja
Electrical & Computer Engineering
Carnegie Mellon University
avneesh@cs.cmu.edu
Chris Dyer
School of Computer Science
Carnegie Mellon University
cdyer@cs.cmu.edu
Eric P. Xing
School of Computer Science
Carnegie Mellon University
epxing@cs.cmu.edu
Abstract
We present power low rank ensembles
(PLRE), a flexible framework for n-gram
language modeling where ensembles of
low rank matrices and tensors are used
to obtain smoothed probability estimates
of words in context. Our method can
be understood as a generalization of n-
gram modeling to non-integer n, and in-
cludes standard techniques such as abso-
lute discounting and Kneser-Ney smooth-
ing as special cases. PLRE training is effi-
cient and our approach outperforms state-
of-the-art modified Kneser Ney baselines
in terms of perplexity on large corpora as
well as on BLEU score in a downstream
machine translation task.
1 Introduction
Language modeling is the task of estimating the
probability of sequences of words in a language
and is an important component in, among other
applications, automatic speech recognition (Ra-
biner and Juang, 1993) and machine translation
(Koehn, 2010). The predominant approach to lan-
guage modeling is the n-gram model, wherein
the probability of a word sequence P (w
1
, . . . , w
`
)
is decomposed using the chain rule, and then a
Markov assumption is made: P (w
1
, . . . , w
`
) ?
?
`
i=1
P (w
i
|w
i?1
i?n+1
). While this assumption sub-
stantially reduces the modeling complexity, pa-
rameter estimation remains a major challenge.
Due to the power-law nature of language (Zipf,
1949), the maximum likelihood estimator mas-
sively overestimates the probability of rare events
and assigns zero probability to legitimate word se-
quences that happen not to have been observed in
the training data (Manning and Sch?utze, 1999).
Many smoothing techniques have been pro-
posed to address the estimation challenge. These
reassign probability mass (generally from over-
estimated events) to unseen word sequences,
whose probabilities are estimated by interpolating
with or backing off to lower order n-gram models
(Chen and Goodman, 1999).
Somewhat surprisingly, these widely used
smoothing techniques differ substantially from
techniques for coping with data sparsity in other
domains, such as collaborative filtering (Koren et
al., 2009; Su and Khoshgoftaar, 2009) or matrix
completion (Cand`es and Recht, 2009; Cai et al.,
2010). In these areas, low rank approaches based
on matrix factorization play a central role (Lee
and Seung, 2001; Salakhutdinov and Mnih, 2008;
Mackey et al., 2011). For example, in recom-
mender systems, a key challenge is dealing with
the sparsity of ratings from a single user, since
typical users will have rated only a few items. By
projecting the low rank representation of a user?s
(sparse) preferences into the original space, an es-
timate of ratings for new items is obtained. These
methods are attractive due to their computational
efficiency and mathematical well-foundedness.
In this paper, we introduce power low rank en-
sembles (PLRE), in which low rank tensors are
used to produce smoothed estimates for n-gram
probabilities. Ideally, we would like the low rank
structures to discover semantic and syntactic relat-
edness among words and n-grams, which are used
to produce smoothed estimates for word sequence
probabilities. In contrast to the few previous low
rank language modeling approaches, PLRE is not
orthogonal to n-gram models, but rather a gen-
eral framework where existing n-gram smoothing
methods such as Kneser-Ney smoothing are spe-
cial cases. A key insight is that PLRE does not
compute low rank approximations of the original
1487
joint count matrices (in the case of bigrams) or ten-
sors i.e. multi-way arrays (in the case of 3-grams
and above), but instead altered quantities of these
counts based on an element-wise power operation,
similar to how some smoothing methods modify
their lower order distributions.
Moreover, PLRE has two key aspects that lead
to easy scalability for large corpora and vocabu-
laries. First, since it utilizes the original n-grams,
the ranks required for the low rank matrices and
tensors tend to be remain tractable (e.g. around
100 for a vocabulary size V ? 1 ? 10
6
) leading
to fast training times. This differentiates our ap-
proach over other methods that leverage an under-
lying latent space such as neural networks (Bengio
et al., 2003; Mnih and Hinton, 2007; Mikolov et
al., 2010) or soft-class models (Saul and Pereira,
1997) where the underlying dimension is required
to be quite large to obtain good performance.
Moreover, at test time, the probability of a se-
quence can be queried in time O(?
max
) where
?
max
is the maximum rank of the low rank matri-
ces/tensors used. While this is larger than Kneser
Ney?s virtually constant query time, it is substan-
tially faster than conditional exponential family
models (Chen and Rosenfeld, 2000; Chen, 2009;
Nelakanti et al., 2013) and neural networks which
require O(V ) for exact computation of the nor-
malization constant. See Section 7 for a more de-
tailed discussion of related work.
Outline: We first review existing n-gram
smoothing methods (?2) and then present the in-
tuition behind the key components of our tech-
nique: rank (?3.1) and power (?3.2). We then
show how these can be interpolated into an ensem-
ble (?4). In the experimental evaluation on English
and Russian corpora (?5), we find that PLRE out-
performs Kneser-Ney smoothing and all its vari-
ants, as well as class-based language models. We
also include a comparison to the log-bilinear neu-
ral language model (Mnih and Hinton, 2007) and
evaluate performance on a downstream machine
translation task (?6) where our method achieves
consistent improvements in BLEU.
2 Discount-based Smoothing
We first provide background on absolute discount-
ing (Ney et al., 1994) and Kneser-Ney smooth-
ing (Kneser and Ney, 1995), two common n-gram
smoothing methods. Both methods can be formu-
lated as back-off or interpolated models; we de-
scribe the latter here since that is the basis of our
low rank approach.
2.1 Notation
Let c(w) be the count of word w, and similarly
c(w,w
i?1
) for the joint count of words w and
w
i?1
. For shorthand we will define w
j
i
to denote
the word sequence {w
i
, w
i+1
, ..., w
j?1
, w
j
}. Let
?
P (w
i
) refer to the maximum likelihood estimate
(MLE) of the probability of word w
i
, and simi-
larly
?
P (w
i
|w
i?1
) for the probability conditioned
on a history, or more generally,
?
P (w
i
|w
i?1
i?n+1
).
Let N
?
(w
i
) := |{w : c(w
i
, w) > 0}| be
the number of distinct words that appear be-
fore w
i
. More generally, let N
?
(w
i
i?n+1
) =
|{w : c(w
i
i?n+1
, w) > 0}|. Similarly, let
N
+
(w
i?1
i?n+1
) = |{w : c(w,w
i?1
i?n+1
) > 0}|. V
denotes the vocabulary size.
2.2 Absolute Discounting
Absolute discounting works on the idea of inter-
polating higher order n-gram models with lower-
order n-gram models. However, first some prob-
ability mass must be ?subtracted? from the higher
order n-grams so that the leftover probability can
be allocated to the lower order n-grams. More
specifically, define the following discounted con-
ditional probability:
?
P
D
(w
i
|w
i?1
i?n+1
) =
max{c(w
i
, w
i?1
i?n+1
)?D, 0}
c(w
i?1
i?n+1
)
Then absolute discounting P
abs
(?) uses the follow-
ing (recursive) equation:
P
abs
(w
i
|w
i?1
i?n+1
) =
?
P
D
(w
i
|w
i?1
i?n+1
)
+ ?(w
i?1
i?n+1
)P
abs
(w
i
|w
i?1
i?n+2
)
where ?(w
i?1
i?n+1
) is the leftover weight (due to
the discounting) that is chosen so that the con-
ditional distribution sums to one: ?(w
i?1
i?n+1
) =
D
c(w
i?1
i?n+1
)
N
+
(w
i?1
i?n+1
). For the base case, we set
P
abs
(w
i
) =
?
P (w
i
).
Discontinuity: Note that if c(w
i?1
i?n+1
) = 0, then
?(w
i?1
i?n+1
) =
0
0
, in which case ?(w
i?1
i?n+1
) is set
to 1. We will see that this discontinuity appears in
PLRE as well.
1488
2.3 Kneser Ney Smoothing
Ideally, the smoothed probability should preserve
the observed unigram distribution:
?
P (w
i
) =
?
w
i?1
i?n+1
P
sm
(w
i
|w
i?1
i?n+1
)
?
P (w
i?1
i?n+1
) (1)
where P
sm
(w
i
|w
i?1
i?n+1
) is the smoothed condi-
tional probability that a model outputs. Unfortu-
nately, absolute discounting does not satisfy this
property, since it exclusively uses the unaltered
MLE unigram model as its lower order model. In
practice, the lower order distribution is only uti-
lized when we are unsure about the higher order
distribution (i.e., when ?(?) is large). Therefore,
the unigram model should be altered to condition
on this fact.
This is the inspiration behind Kneser-Ney (KN)
smoothing, an elegant algorithm with robust per-
formance in n-gram language modeling. KN
smoothing defines alternate probabilities P
alt
(?):
P
alt
D
(w
i
|w
i?1
i?n
?
+1
) =
?
?
?
?
?
?
?
?
P
D
(w
i
|w
i?1
i?n
?
+1
), if n
?
= n
max{N
?
(w
i
i?n
?
+1
)?D,0}
?
w
i
N
?
(w
i
i?n
?
+1
)
, if n
?
< n
The base case for unigrams reduces to
P
alt
(w
i
) =
N
?
(w
i
)?
w
i
N
?
(w
i
)
. Intuitively P
alt
(w
i
) is
proportional to the number of unique words that
precede w
i
. Thus, words that appear in many dif-
ferent contexts will be given higher weight than
words that consistently appear after only a few
contexts. These alternate distributions are then
used with absolute discounting:
P
kn
(w
i
|w
i?1
i?n+1
) = P
alt
D
(w
i
|w
i?1
i?n+1
)
+ ?(w
i?1
i?n+1
)P
kn
(w
i
|w
i?1
i?n+2
) (2)
where we set P
kn
(w
i
) = P
alt
(w
i
). By definition,
KN smoothing satisfies the marginal constraint in
Eq. 1 (Kneser and Ney, 1995).
3 Power Low Rank Ensembles
In n-gram smoothing methods, if a bigram count
c(w
i
, w
i?1
) is zero, the unigram probabilities are
used, which is equivalent to assuming that w
i
and
w
i?1
are independent ( and similarly for general
n). However, in this situation, instead of back-
ing off to a 1-gram, we may like to back off to a
?1.5-gram? or more generally an order between 1
and 2 that captures a coarser level of dependence
between w
i
and w
i?1
and does not assume full in-
dependence.
Inspired by this intuition, our strategy is to con-
struct an ensemble of matrices and tensors that
not only consists of MLE-based count informa-
tion, but also contains quantities that represent lev-
els of dependence in-between the various orders in
the model. We call these combinations power low
rank ensembles (PLRE), and they can be thought
of as n-gram models with non-integer n. Our ap-
proach can be recursively formulated as:
P
plre
(w
i
|w
i?1
i?n+1
) = P
alt
D
0
(w
i
|w
i?1
i?n+1
)
+ ?
0
(w
i?1
i?n+1
)
(
ZD
1
(w
i
|w
i?1
i?n+1
) + .....
+ ?
??1
(w
i?1
i?n+1
)
(
ZD
?
(w
i
|w
i?1
i?n+1
)
+ ?
?
(w
i?1
i?n+1
)
(
P
plre
(w
i
|w
i?1
i?n+2
)
))
...
)
(3)
where Z
1
, ...,Z
?
are conditional probability ma-
trices that represent the intermediate n-gram or-
ders
1
and D is a discount function (specified in
?4).
This formulation begs answers to a few crit-
ical questions. How to construct matrices that
represent conditional probabilities for intermedi-
ate n? How to transform them in a way that
generalizes the altered lower order distributions
in KN smoothing? How to combine these matri-
ces such that the marginal constraint in Eq. 1 still
holds? The following propose solutions to these
three queries:
1. Rank (Section 3.1): Rank gives us a concrete
measurement of the dependence between w
i
and w
i?1
. By constructing low rank ap-
proximations of the bigram count matrix and
higher-order count tensors, we obtain matri-
ces that represent coarser dependencies, with
a rank one approximation implying that the
variables are independent.
2. Power (Section 3.2): In KN smoothing, the
lower order distributions are not the original
counts but rather altered estimates. We pro-
pose a continuous generalization of this alter-
ation by taking the element-wise power of the
counts.
1
with a slight abuse of notation, let ZD
j
be shorthand
for Z
j,D
j
1489
3. Creating the Ensemble (Section 4): Lastly,
PLRE also defines a way to interpolate the
specifically constructed intermediate n-gram
matrices. Unfortunately a constant discount,
as presented in Section 2, will not in general
preserve the lower order marginal constraint
(Eq. 1). We propose a generalized discount-
ing scheme to ensure the constraint holds.
3.1 Rank
We first show how rank can be utilized to construct
quantities between an n-gram and an n? 1-gram.
In general, we think of an n-gram as an n
th
or-
der tensor i.e. a multi-way array with n indices
{i
1
, ..., i
n
}. (A vector is a tensor of order 1, a ma-
trix is a tensor of order 2 etc.) Computing a spe-
cial rank one approximation of slices of this tensor
produces the n? 1-gram. Thus, taking rank ? ap-
proximations in this fashion allows us to represent
dependencies between an n-gram and n?1-gram.
Consider the bigram count matrix B with
N counts which has rank V . Note that
?
P (w
i
|w
i?1
) =
B(w
i
,w
i?1
)?
w
B(w,w
i?1
)
. Additionally, B
can be considered a random variable that is the re-
sult of sampling N tuples of (w
i
, w
i?1
) and ag-
glomerating them into a count matrix. Assum-
ing w
i
and w
i?1
are independent, the expected
value (with respect to the empirical distribution)
E[B] = NP (w
i
)P (w
i?1
), which can be rewrit-
ten as being proportional to the outer product of
the unigram probability vector with itself, and is
thus rank one.
This observation extends to higher order
n-grams as well. Let C
n
be the n
th
order tensor
where C
n
(w
i
, ...., w
i?n+1
) = c(w
i
, ..., w
i?n+1
).
Furthermore denote C
n
(:, w?
i?1
i?n+2
, :) to
be the V ? V matrix slice of C
n
where
w
i?n+2
, ..., w
i?1
are held fixed to a particular
sequence w?
i?n+2
, ..., w?
i?1
. Then if w
i
is con-
ditionally independent of w
i?n+1
given w
i?1
i?n+2
,
then E[C
n
(:, w?
i?1
i?n+2
, :)] is rank one ?w?
i?1
i?n+2
.
However, it is rare that these matrices are ac-
tually rank one, either due to sampling vari-
ance or the fact that w
i
and w
i?1
are not in-
dependent. What we would really like to say
is that the best rank one approximation B
(1)
(under some norm) of B is ?
?
P (w
i
)
?
P (w
i?1
).
While this statement is not true under the `
2
norm, it is true under generalized KL diver-
gence (Lee and Seung, 2001): gKL(A||B) =
?
ij
(
A
ij
log(
A
ij
B
ij
)?A
ij
+B
ij
)
)
.
In particular, generalized KL divergence pre-
serves row and column sums: if M
(?)
is the best
rank ? approximation of M under gKL then the
row sums and column sums of M
(?)
and M are
equal (Ho and Van Dooren, 2008). Leveraging
this property, it is straightforward to prove the fol-
lowing lemma:
Lemma 1. Let B
(?)
be the best rank ? ap-
proximation of B under gKL. Then B
(1)
?
?
P (w
i
)
?
P (w
i?1
) and ?w
i?1
s.t. c(w
i?1
) 6= 0:
?
P (w
i
) =
B
(1)
(w
i
, w
i?1
)
?
w
B
(1)
(w,w
i?1
)
For more general n, let C
n,(?)
i?1,...,i?n+2
be the
best rank ? approximation of C
n
(:, w?
i?1
i?n+2
, :
) under gKL. Then similarly, ?w
i?1
i?n+1
s.t.
c(w
i?1
i?n+1
) > 0:
?
P (w
i
|w
i?1
, ..., w
i?n+2
)
=
C
n,(1)
i?1,...,i?n+2
(w
i
, w
i?1
i?n+1
)
?
w
C
n,(1)
i?1,...,i?n+2
(w,w
i?1
i?n+1
)
(4)
Thus, by selecting 1 < ? < V , we obtain count
matrices and tensors between n and n ? 1-grams.
The condition that c(w
i?1
i?n+1
) > 0 corresponds to
the discontinuity discussed in ?2.2.
3.2 Power
Since KN smoothing alters the lower order distri-
butions instead of simply using the MLE, vary-
ing the rank is not sufficient in order to generalize
this suite of techniques. Thus, PLRE computes
low rank approximations of altered count matri-
ces. Consider taking the elementwise power ? of
the bigram count matrix, which is denoted by B
??
.
For example, the observed bigram count matrix
and associated row sum:
B
?1
=
(
1.0 2.0 1.0
0 5.0 0
2.0 0 0
)
row sum
?
(
4.0
5.0
2.0
)
As expected the row sum is equal to the uni-
gram counts (which we denote as u). Now con-
sider B
?0.5
:
B
?0.5
=
(
1.0 1.4 1.0
0 2.2 0
1.4 0 0
)
row sum
?
(
3.4
2.2
1.4
)
Note how the row sum vector has been altered.
In particular since w
1
(corresponding to the first
1490
row) has a more diverse history than w
2
, it has
a higher row sum (compared to in u where w
2
has the higher row sum). Lastly, consider the case
when p = 0:
B
?0
=
(
1.0 1.0 1.0
0 1.0 0
1.0 0 0
)
row sum
?
(
3.0
1.0
1.0
)
The row sum is now the number of unique words
that precede w
i
(since B
0
is binary) and is thus
equal to the (unnormalized) Kneser Ney unigram.
This idea also generalizes to higher order n-grams
and leads us to the following lemma:
Lemma 2. Let B
(?,?)
be the best rank ? ap-
proximation of B
??
under gKL. Then ?w
i?1
s.t.
c(w
i?1
) 6= 0:
P
alt
(w
i
) =
B
(0,1)
(w
i
, w
i?1
)
?
w
B
(0,1)
(w,w
i?1
)
For more general n, let C
n,(?,?)
i?1,...,i?n+2
be the best
rank ? approximation of C
n,(?)
(:, w?
i?1
i?n+2
, :) un-
der gKL. Similarly, ?w
i?1
i?n+1
s.t. c(w
i?1
i?n+1
) > 0:
P
alt
(w
i
|w
i?1
, ..., w
i?n+2
)
=
C
n,(0,1)
i?1,...,i?n+2
(w
i
, w
i?1
i?n+1
)
?
w
C
n,(0,1)
i?1,...,i?n+2
(w,w
i?1
i?n+1
)
(5)
4 Creating the Ensemble
Recall our overall formulation in Eq. 3; a naive
solution would be to set Z
1
, ...,Z
?
to low rank
approximations of the count matrices/tensors un-
der varying powers, and then interpolate through
constant absolute discounting. Unfortunately, the
marginal constraint in Eq. 1 will generally not hold
if this strategy is used. Therefore, we propose a
generalized discounting scheme where each non-
zero n-gram count is associated with a different
discount D
j
(w
i
, w
i?1
i?n
?
+1
). The low rank approxi-
mations are then computed on the discounted ma-
trices, leaving the marginal constraint intact.
For clarity of exposition, we focus on the spe-
cial case where n = 2 with only one low rank
matrix before stating our general algorithm:
P
plre
(w
i
|w
i?1
) =
?
PD
0
(w
i
|w
i?1
)
+ ?
0
(w
i?1
)
(
ZD
1
(w
i
|w
i?1
) + ?
1
(w
i?1
)P
alt
(w
i
)
)
(6)
Our goal is to compute D
0
,D
1
and Z
1
so
that the following lower order marginal constraint
holds:
?
P (w
i
) =
?
w
i?1
P
plre
(w
i
|w
i?1
)
?
P (w
i?1
) (7)
Our solution can be thought of as a two-
step procedure where we compute the discounts
D
0
,D
1
(and the ?(w
i?1
) weights as a by-
product), followed by the low rank quantity Z
1
.
First, we construct the following intermediate en-
semble of powered, but full rank terms. Let
Y
?
j
be the matrix such that Y
?
j
(w
i
, w
i?1
) :=
c(w
i
, w
i?1
)
?
j
. Then define
P
pwr
(w
i
|w
i?1
) := Y
(?
0
=1)
D
0
(w
i
|w
i?1
)
+ ?
0
(w
i?1
)
(
Y
(?
1
)
D
1
(w
i
|w
i?1
)
+ ?
1
(w
i?1
)Y
(?
2
=0)
(w
i
|w
i?1
)
)
(8)
where with a little abuse of notation:
Y
?
j
D
j
(w
i
|w
i?1
) =
c(w
i
, w
i?1
)
?
j
?D
j
(w
i
, w
i?1
)
?
w
i
c(w
i
, w
i?1
)
?
j
Note that P
alt
(w
i
) has been replaced with
Y
(?
2
=0)
(w
i
|w
i?1
), based on Lemma 2, and will
equal P
alt
(w
i
) once the low rank approximation is
taken as discussed in ? 4.2).
Since we have only combined terms of differ-
ent power (but all full rank), it is natural choose
the discounts so that the result remains unchanged
i.e., P
pwr
(w
i
|w
i?1
) =
?
P (w
i
|w
i?1
), since the low
rank approximation (not the power) will imple-
ment smoothing. Enforcing this constraint gives
rise to a set of linear equations that can be solved
(in closed form) to obtain the discounts as we now
show below.
4.1 Step 1: Computing the Discounts
To ensure the constraint that P
pwr
(w
i
|w
i?1
) =
?
P (w
i
|w
i?1
), it is sufficient to enforce the follow-
ing two local constraints:
Y
(?
j
)
(w
i
|w
i?1
) = Y
(?
j
)
D
j
(w
i
|w
i?1
)
+ ?
j
(w
i?1
)Y
(?
j+1
)
(w
i
|w
i?1
) for j = 0, 1
(9)
This allows each D
j
to be solved for indepen-
dently of the other {D
j
?
}
j
?
6=j
. Let c
i,i?1
=
c(w
i
, w
i?1
), c
j
i,i?1
= c(w
i
, w
i?1
)
?
j
, and d
j
i,i?1
=
1491
Dj
(w
i
, w
i?1
). Expanding Eq. 9 yields that
?w
i
, w
i?1
:
c
j
i,i?1
?
i
c
j
i,i?1
=
c
j
i,i?1
? d
j
i,i?1
?
i
c
j
i,i?1
+
(
?
i
d
j
i,i?1
?
i
c
j
i,i?1
)
c
j+1
i,i?1
?
i
c
j+1
i,i?1
(10)
which can be rewritten as:
?d
j
i,i?1
+
(
?
i
d
j
i,i?1
)
c
j+1
i,i?1
?
i
c
j+1
i,i?1
= 0 (11)
Note that Eq. 11 decouples across w
i?1
since the
only d
j
i,i?1
terms that are dependent are the ones
that share the preceding context w
i?1
.
It is straightforward to see that setting d
j
i,i?1
proportional to c
j+1
i,i?1
satisfies Eq. 11. Furthermore
it can be shown that all solutions are of this form
(i.e., the linear system has a null space of exactly
one). Moreover, we are interested in a particular
subset of solutions where a single parameter d
?
(independent of w
i?1
) controls the scaling as in-
dicated by the following lemma:
Lemma 3. Assume that ?
j
? ?
j+1
. Choose any
0 ? d
?
? 1. Set d
j
i,i?1
= d
?
c
j+1
i,i?1
?i, j. The
resulting discounts satisfy Eq. 11 as well as the
inequality constraints 0 ? d
j
i,i?1
? c
j
i,i?1
. Fur-
thermore, the leftover weight ?
j
takes the form:
?
j
(w
i?1
) =
?
i
d
j
i,i?1
?
i
c
j
i,i?1
=
d
?
?
i
c
j+1
i,i?1
?
i
c
j
i,i?1
Proof. Clearly this choice of d
j
i,i?1
satisfies
Eq. 11. The largest possible value of d
j
i,i?1
is
c
j+1
i,i?1
. ?
j
? ?
j+1
, implies c
j
i,i?1
? c
j+1
i,i?1
. Thus
the inequality constraints are met. It is then easy
to verify that ? takes the above form.
The above lemma generalizes to longer contexts
(i.e. n > 2) as shown in Algorithm 1. Note that if
?
j
= ?
j+1
then Algorithm 1 is equivalent to scal-
ing the counts e.g. deleted-interpolation/Jelinek
Mercer smoothing (Jelinek and Mercer, 1980). On
the other hand, when ?
j+1
= 0, Algorithm 1
is equal to the absolute discounting that is used
in Kneser-Ney. Thus, depending on ?
j+1
, our
method generalizes different types of interpola-
tion schemes to construct an ensemble so that the
marginal constraint is satisfied.
Algorithm 1 Compute D
In: Count tensor C
n
, powers ?
j
, ?
j+1
such that
?
j
? ?
j+1
, and parameter d
?
.
Out: Discount D
j
for powered counts C
n,(?
j
)
and associated leftover weight ?
j
1: Set D
j
(w
i
, w
i?1
i?n+1
) = d
?
c(w
i
, w
i?1
i?n+1
)
?
j+1
.
2:
?
j
(w
i
, w
i?1
i?n+1
) =
d
?
?
w
i
c(w
i
, w
i?1
i?n+1
)
?
j+1
?
w
i
c(w
i
, w
i?1
i?n+1
)
?
j
Algorithm 2 Compute Z
In: Count tensor C
n
, power ?, discounts D, rank
?
Out: Discounted low rank conditional probability
table Z
(?,?)
D (wi|w
i?1
i?n+1
) (represented implicitly)
1: Compute powered counts C
n,(??)
.
2: Compute denominators
?
w
i
c(w
i
, w
i?1
i?n+1
)
?
?w
i?1
i?n+1
s.t. c(w
i?1
i?n+1
) > 0.
3: Compute discounted powered counts
C
n,(??)
D = C
n,(??)
?D.
4: For each slice M
w?
i?1
i?n+2
:= C
n,(??)
D (:
, w?
i?1
i?n+2
, :) compute
M
(?)
:= min
A?0:rank(A)=?
?M
w?
i?1
i?n+2
?A?
KL
(stored implicitly as M
(?)
= LR)
Set Z
(?,?)
D (:, w?
i?1
i?n+2
, :) = M
(?)
5: Note that
Z
(?,?)
D (wi|w
i?1
i?n+1
) =
Z
(?,?)
D (wi, w
i?1
i?n+1
)
?
w
i
c(w
i
, w
i?1
i?n+1
)
?
4.2 Step 2: Computing Low Rank Quantities
The next step is to compute low rank approxi-
mations ofY
(?
j
)
D
j
to obtainZD
j
such that the inter-
mediate marginal constraint in Eq. 7 is preserved.
This constraint trivially holds for the intermediate
ensemble P
pwr
(w
i
|w
i?1
) due to how the discounts
were derived in ? 4.1. For our running bigram ex-
ample, define Z
(?
j
,?
j
)
D
j
to be the best rank ?
j
ap-
proximation to Y
(?
j
,?
j
)
D
j
according to gKL and let
Z
?
j
,?
j
D
j
(w
i
|w
i?1
) =
Z
?
j
,?
j
D
j
(w
i
, w
i?1
)
?
w
i
c(w
i
, w
i?1
)
?
j
Note that Z
?
j
,?
j
D
j
(w
i
|w
i?1
) is a valid (discounted)
conditional probability since gKL preserves
row/column sums so the denominator remains un-
changed under the low rank approximation. Then
1492
using the fact that Z
(0,1)
(w
i
|w
i?1
) = P
alt
(w
i
)
(Lemma 2) we can embellish Eq. 6 as
P
plre
(w
i
|w
i?1
) = PD
0
(w
i
|w
i?1
)+
?
0
(w
i?1
)
(
Z
(?
1
,?
1
)
D
1
(w
i
|w
i?1
) + ?
1
(w
i?1
)P
alt
(w
i
)
)
Leveraging the form of the discounts and
row/column sum preserving property of gKL, we
then have the following lemma (the proof is in the
supplementary material):
Lemma 4. Let P
plre
(w
i
|w
i?1
) indicate the PLRE
smoothed conditional probability as computed by
Eq. 6 and Algorithms 1 and 2. Then, the marginal
constraint in Eq. 7 holds.
4.3 More general algorithm
In general, the principles outlined in the previ-
ous sections hold for higher order n-grams. As-
sume that the discounts are computed according
to Algorithm 1 with parameter d
?
and Z
(?
j
,?
j
)
D
j
is
computed according to Algorithm 2. Note that, as
shown in Algorithm 2, for higher order n-grams,
theZ
(?
j
,?
j
)
D
j
are created by taking low rank approx-
imations of slices of the (powered) count tensors
(see Lemma 2 for intuition). Eq. 3 can now be
embellished:
P
plre
(w
i
|w
i?1
i?n+1
) = P
alt
D
0
(w
i
|w
i?1
i?n+1
)
+ ?
0
(w
i?1
i?n+1
)
(
Z
(?
1
,?
1
)
D
1
(w
i
|w
i?1
i?n+1
) + .....
+ ?
??1
(w
i?1
i?n+1
)
(
Z
(?
?
,?
?
)
D
?
(w
i
|w
i?1
i?n+1
)
+ ?
?
(w
i?1
i?n+1
)
(
P
plre
(w
i
|w
i?1
i?n+2
)
))
...
)
(12)
Lemma 4 also applies in this case and is given in
Theorem 1 in the supplementary material.
4.4 Links with KN Smoothing
In this section, we explicitly show the relation-
ship between PLRE and KN smoothing. Rewrit-
ing Eq. 12 in the following form:
P
plre
(w
i
|w
i?1
i?n+1
) = P
terms
plre
(w
i
|w
i?1
i?n+1
)
+?
0:?
(w
i?1
i?n+1
)P
plre
(w
i
|w
i?1
i?n+2
) (13)
where P
terms
plre
(w
i
|w
i?1
i?n+1
) contains the terms in
Eq. 12 except the last, and ?
0:?
(w
i?1
i?n+1
) =
?
?
h=0
?
h
(w
i?1
i?n+1
), we can leverage the form of
the discount, and using the fact that ?
?+1
= 0
2
:
?
0:?
(w
i?1
i?n?1
) =
d
?
?+1
N
+
(w
i?1
i?n+1
)
c(w
i?1
i?n+1
)
With this form of ?(?), Eq. 13 is remarkably sim-
ilar to KN smoothing (Eq. 2) if KN?s discount pa-
rameter D is chosen to equal (d
?
)
?+1
.
The difference is that P
alt
(?) has been replaced
with the alternate estimate P
terms
plre
(w
i
|w
i?1
i?n+1
),
which have been enriched via the low rank struc-
ture. Since these alternate estimates were con-
structed via our ensemble strategy they contain
both very fine-grained dependencies (the origi-
nal n-grams) as well as coarser dependencies (the
lower rank n-grams) and is thus fundamentally
different than simply taking a single matrix/tensor
decomposition of the trigram/bigram matrices.
Moreover, it provides a natural way of setting
d
?
based on the Good-Turing (GT) estimates em-
ployed by KN smoothing. In particular, we can set
d
?
to be the (? + 1)
th
root of the KN discount D
that can be estimated via the GT estimates.
4.5 Computational Considerations
PLRE scales well even as the order n increases.
To compute a low rank bigram, one low rank ap-
proximation of a V ? V matrix is required. For
the low rank trigram, we need to compute a low
rank approximation of each slice C
n,(?p)
D (:, w?i?1, :
) ?w?
i?1
. While this may seem daunting at first, in
practice the size of each slice (number of non-zero
rows/columns) is usually much, much smaller than
V , keeping the computation tractable.
Similarly, PLRE also evaluates conditional
probabilities at evaluation time efficiently. As
shown in Algorithm 2, the normalizer can be pre-
computed on the sparse powered matrix/tensor. As
a result our test complexity is O(
?
?
total
i=1
?
i
) where
?
total
is the total number of matrices/tensors in
the ensemble. While this is larger than Kneser
Ney?s practically constant complexity of O(n),
it is much faster than other recent methods for
language modeling such as neural networks and
conditional exponential family models where ex-
act computation of the normalizing constant costs
O(V ).
5 Experiments
To evaluate PLRE, we compared its performance
on English and Russian corpora with several vari-
2
for derivation see proof of Lemma 4 in the supplemen-
tary material
1493
ants of KN smoothing, class-based models, and
the log-bilinear neural language model (Mnih and
Hinton, 2007). We evaluated with perplexity in
most of our experiments, but also provide results
evaluated with BLEU (Papineni et al., 2002) on a
downstream machine translation (MT) task. We
have made the code for our approach publicly
available
3
.
To build the hard class-based LMs, we utilized
mkcls
4
, a tool to train word classes that uses
the maximum likelihood criterion (Och, 1995) for
classing. We subsequently trained trigram class
language models on these classes (correspond-
ing to 2
nd
-order HMMs) using SRILM (Stolcke,
2002), with KN-smoothing for the class transition
probabilities. SRILM was also used for the base-
line KN-smoothed models.
For our MT evaluation, we built a hierarchi-
cal phrase translation (Chiang, 2007) system us-
ing cdec (Dyer et al., 2010). The KN-smoothed
models in the MT experiments were compiled us-
ing KenLM (Heafield, 2011).
5.1 Datasets
For the perplexity experiments, we evaluated our
proposed approach on 4 datasets, 2 in English and
2 in Russian. In all cases, the singletons were re-
placed with ?<unk>? tokens in the training cor-
pus, and any word not in the vocabulary was re-
placed with this token during evaluation. There is
a general dearth of evaluation on large-scale cor-
pora in morphologically rich languages such as
Russian, and thus we have made the processed
Large-Russian corpus available for comparison
3
.
? Small-English: APNews corpus (Bengio et al.,
2003): Train - 14 million words, Dev - 963,000,
Test - 963,000. Vocabulary- 18,000 types.
? Small-Russian: Subset of Russian news com-
mentary data from 2013 WMT translation task
5
:
Train- 3.5 million words, Dev - 400,000 Test -
400,000. Vocabulary - 77,000 types.
? Large-English: English Gigaword, Training -
837 million words, Dev - 8.7 million, Test - 8.7
million. Vocabulary- 836,980 types.
? Large-Russian: Monolingual data from WMT
2013 task. Training - 521 million words, Vali-
dation - 50,000, Test - 50,000. Vocabulary- 1.3
million types.
3
http://www.cs.cmu.edu/?apparikh/plre.html
4
http://code.google.com/p/giza-pp/
5
http://www.statmt.org/wmt13/training-monolingual-
nc-v8.tgz
For the MT evaluation, we used the parallel data
from the WMT 2013 shared task, excluding the
Common Crawl corpus data. The newstest2012
and newstest2013 evaluation sets were used as the
development and test sets respectively.
5.2 Small Corpora
For the class-based baseline LMs, the
number of classes was selected from
{32, 64, 128, 256, 512, 1024} (Small-English)
and {512, 1024} (Small-Russian). We could not
go higher due to the computationally laborious
process of hard clustering. For Kneser-Ney, we
explore four different variants: back-off (BO-KN)
interpolated (int-KN), modified back-off (BO-
MKN), and modified interpolated (int-MKN).
Good-Turing estimates were used for discounts.
All models trained on the small corpora are of
order 3 (trigrams).
For PLRE, we used one low rank bigram and
one low rank trigram in addition to the MLE n-
gram estimates. The powers of the intermediate
matrices/tensors were fixed to be 0.5 and the dis-
counts were set to be square roots of the Good Tur-
ing estimates (as explained in ? 4.4). The ranks
were tuned on the development set. For Small-
English, the ranges were {1e ? 3, 5e ? 3} (as a
fraction of the vocabulary size) for both the low
rank bigram and low rank trigram models. For
Small-Russian the ranges were {5e ? 4, 1e ? 3}
for both the low rank bigram and the low rank tri-
gram models.
The results are shown in Table 1. The best class-
based LM is reported, but is not competitive with
the KN baselines. PLRE outperforms all of the
baselines comfortably. Moreover, PLRE?s perfor-
mance over the baselines is highlighted in Russian.
With larger vocabulary sizes, the low rank ap-
proach is more effective as it can capture linguistic
similarities between rare and common words.
Next we discuss how the maximum n-gram or-
der affects performance. Figure 1 shows the rela-
tive percentage improvement of our approach over
int-MKN as the order is increased from 2 to 4 for
both methods. The Small-English dataset has a
rather small vocabulary compared to the number
of tokens, leading to lower data sparsity in the bi-
gram. Thus the PLRE improvement is small for
order = 2, but more substantial for order = 3. On
the other hand, for the Small-Russian dataset, the
vocabulary size is much larger and consequently
the bigram counts are sparser. This leads to sim-
1494
Dataset class-1024(3) BO-KN(3) int-KN(3) BO-MKN(3) int-MKN(3) PLRE(3)
Small-English Dev 115.64 99.20 99.73 99.95 95.63 91.18
Small-English Test 119.70 103.86 104.56 104.55 100.07 95.15
Small-Russian Dev 286.38 281.29 265.71 287.19 263.25 241.66
Small-Russian Test 284.09 277.74 262.02 283.70 260.19 238.96
Table 1: Perplexity results on small corpora for all methods.
Small-Russian
Small-English
Figure 1: Relative percentage improvement of
PLRE over int-MKN as the maximum n-gram or-
der for both methods is increased.
ilar improvements for all orders (which are larger
than that for Small-English).
On both these datasets, we also experimented
with tuning the discounts for int-MKN to see if
the baseline could be improved with more careful
choices of discounts. However, this achieved only
marginal gains (reducing the perplexity to 98.94
on the Small-English test set and 259.0 on the
Small-Russian test set).
Comparison to LBL (Mnih and Hinton,
2007): Mnih and Hinton (2007) evaluate on the
Small-English dataset (but remove end markers
and concatenate the sentences). They obtain per-
plexities 117.0 and 107.8 using contexts of size 5
and 10 respectively. With this preprocessing, a 4-
gram (context 3) PLRE achieves 108.4 perplexity.
5.3 Large Corpora
Results on the larger corpora for the top 2 per-
forming methods ?PLRE? and ?int-MKN? are pre-
sented in Table 2. Due to the larger training size,
we use 4-gram models in these experiments. How-
ever, including the low rank 4-gram tensor pro-
vided little gain and therefore, the 4-gram PLRE
only has additional low rank bigram and low rank
trigram matrices/tensors. As above, ranks were
tuned on the development set. For Large-English,
the ranges were {1e?4, 5e?4, 1e?3} (as a frac-
tion of the vocabulary size) for both the low rank
Dataset int-MKN(4) PLRE(4)
Large-English Dev 73.21 71.21
Large-English Test 77.90 ? 0.203 75.66 ? 0.189
Large-Russian Dev 326.9 297.11
Large-Russian Test 289.63 ? 6.82 264.59 ? 5.839
Table 2: Mean perplexity results on large corpora,
with standard deviation.
Dataset PLRE Training Time
Small-English 3.96 min ( order 3) / 8.3 min (order 4)
Small-Russian 4.0 min (order 3) / 4.75 min (order 4)
Large-English 3.2 hrs (order 4)
Large-Russian 8.3 hrs (order 4)
Table 3: PLRE training times for a fixed parameter
setting
6
. 8 Intel Xeon CPUs were used.
Method BLEU
int-MKN(4) 17.63 ? 0.11
PLRE(4) 17.79 ? 0.07
Smallest Diff PLRE+0.05
Largest Diff PLRE+0.29
Table 4: Results on English-Russian translation
task (mean ? stdev). See text for details.
bigram and low rank trigram models. For Small-
Russian the ranges were {1e?5, 5e?5, 1e?4} for
both the low rank bigram and the low rank trigram
models. For statistical validity, 10 test sets of size
equal to the original test set were generated by ran-
domly sampling sentences with replacement from
the original test set. Our method outperforms ?int-
MKN? with gains similar to that on the smaller
datasets. As shown in Table 3, our method obtains
fast training times even for large datasets.
6 Machine Translation Task
Table 4 presents results for the MT task, trans-
lating from English to Russian
7
. We used
MIRA (Chiang et al., 2008) to learn the feature
weights. To control for the randomness in MIRA,
we avoid retuning when switching LMs - the set
of feature weights obtained using int-MKN is the
same, only the language model changes. The
6
As described earlier, only the ranks need to be tuned, so
only 2-3 low rank bigrams and 2-3 low rank trigrams need to
be computed (and combined depending on the setting).
7
the best score at WMT 2013 was 19.9 (Bojar et al.,
2013)
1495
procedure is repeated 10 times to control for op-
timizer instability (Clark et al., 2011). Unlike
other recent approaches where an additional fea-
ture weight is tuned for the proposed model and
used in conjunction with KN smoothing (Vaswani
et al., 2013), our aim is to show the improvements
that PLRE provides as a substitute for KN. On av-
erage, PLRE outperforms the KN baseline by 0.16
BLEU, and this improvement is consistent in that
PLRE never gets a worse BLEU score.
7 Related Work
Recent attempts to revisit the language model-
ing problem have largely come from two direc-
tions: Bayesian nonparametrics and neural net-
works. Teh (2006) and Goldwater et al. (2006)
discovered the connection between interpolated
Kneser Ney and the hierarchical Pitman-Yor pro-
cess. These have led to generalizations that ac-
count for domain effects (Wood and Teh, 2009)
and unbounded contexts (Wood et al., 2009).
The idea of using neural networks for language
modeling is not new (Miikkulainen and Dyer,
1991), but recent efforts (Mnih and Hinton, 2007;
Mikolov et al., 2010) have achieved impressive
performance. These methods can be quite expen-
sive to train and query (especially as the vocab-
ulary size increases). Techniques such as noise
contrastive estimation (Gutmann and Hyv?arinen,
2012; Mnih and Teh, 2012; Vaswani et al., 2013),
subsampling (Xu et al., 2011), or careful engi-
neering approaches for maximum entropy LMs
(which can also be applied to neural networks)
(Wu and Khudanpur, 2000) have improved train-
ing of these models, but querying the probabil-
ity of the next word given still requires explicitly
normalizing over the vocabulary, which is expen-
sive for big corpora or in languages with a large
number of word types. Mnih and Teh (2012) and
Vaswani et al. (2013) propose setting the normal-
ization constant to 1, but this is approximate and
thus can only be used for downstream evaluation,
not for perplexity computation. An alternate tech-
nique is to use word-classing (Goodman, 2001;
Mikolov et al., 2011), which can reduce the cost
of exact normalization to O(
?
V ). In contrast, our
approach is much more scalable, since it is triv-
ially parallelized in training and does not require
explicit normalization during evaluation.
There are a few low rank approaches (Saul and
Pereira, 1997; Bellegarda, 2000; Hutchinson et al.,
2011), but they are only effective in restricted set-
tings (e.g. small training sets, or corpora divided
into documents) and do not generally perform
comparably to state-of-the-art models. Roark et
al. (2013) also use the idea of marginal constraints
for re-estimating back-off parameters for heavily-
pruned language models, whereas we use this con-
cept to estimate n-gram specific discounts.
8 Conclusion
We presented power low rank ensembles, a tech-
nique that generalizes existing n-gram smoothing
techniques to non-integer n. By using ensembles
of sparse as well as low rank matrices and ten-
sors, our method captures both the fine-grained
and coarse structures in word sequences. Our
discounting strategy preserves the marginal con-
straint and thus generalizes Kneser Ney, and un-
der slight changes can also extend other smooth-
ing methods such as deleted-interpolation/Jelinek-
Mercer smoothing. Experimentally, PLRE con-
vincingly outperforms Kneser-Ney smoothing as
well as class-based baselines.
Acknowledgements
This work was supported by NSF IIS1218282,
NSF IIS1218749, NSF IIS1111142, NIH
R01GM093156, the U. S. Army Research Labo-
ratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533, the
NSF Graduate Research Fellowship Program
under Grant No. 0946825 (NSF Fellowship to
APP), and a grant from Ebay Inc. (to AS).
References
Jerome R. Bellegarda. 2000. Large vocabulary speech
recognition with multispan statistical language mod-
els. IEEE Transactions on Speech and Audio Pro-
cessing, 8(1):76?84.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. J. Mach. Learn. Res., 3:1137?1155,
March.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen.
2010. A singular value thresholding algorithm for
1496
matrix completion. SIAM Journal on Optimization,
20(4):1956?1982.
Emmanuel J Cand`es and Benjamin Recht. 2009. Exact
matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717?
772.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for lan-
guage modeling. Computer Speech & Language,
13(4):359?393.
Stanley F Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for me models. Speech and
Audio Processing, IEEE Transactions on, 8(1):37?
50.
Stanley F. Chen. 2009. Shrinking exponential lan-
guage models. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
468?476, Stroudsburg, PA, USA. Association for
Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 224?233. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201?228, June.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: Short
Papers - Volume 2, HLT ?11, pages 176?181.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12. Association for Computational
Linguistics.
Sharon Goldwater, Thomas Griffiths, and Mark John-
son. 2006. Interpolating between types and tokens
by estimating power-law generators. In Advances in
Neural Information Processing Systems, volume 18.
Joshua Goodman. 2001. Classes for fast maximum
entropy training. In Acoustics, Speech, and Signal
Processing, 2001. Proceedings.(ICASSP?01). 2001
IEEE International Conference on, volume 1, pages
561?564. IEEE.
Michael Gutmann and Aapo Hyv?arinen. 2012. Noise-
contrastive estimation of unnormalized statistical
models, with applications to natural image statistics.
Journal of Machine Learning Research, 13:307?
361.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the
EMNLP 2011 Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
Ngoc-Diep Ho and Paul Van Dooren. 2008. Non-
negative matrix factorization with fixed row and col-
umn sums. Linear Algebra and its Applications,
429(5):1020?1025.
Brian Hutchinson, Mari Ostendorf, and Maryam Fazel.
2011. Low rank language models for small training
sets. Signal Processing Letters, IEEE, 18(9):489?
492.
Frederick Jelinek and Robert Mercer. 1980. Interpo-
lated estimation of markov source parameters from
sparse data. Pattern recognition in practice.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA,
1st edition.
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.
Matrix factorization techniques for recommender
systems. Computer, 42(8):30?37.
Daniel D. Lee and H. Sebastian Seung. 2001. Algo-
rithms for non-negative matrix factorization. Ad-
vances in Neural Information Processing Systems,
13:556?562.
Lester Mackey, Ameet Talwalkar, and Michael I Jor-
dan. 2011. Divide-and-conquer matrix factoriza-
tion. arXiv preprint arXiv:1107.0789.
Christopher D Manning and Hinrich Sch?utze. 1999.
Foundations of statistical natural language process-
ing, volume 999. MIT Press.
Risto Miikkulainen and Michael G. Dyer. 1991. Natu-
ral language processing with modular pdp networks
and distributed lexicon. Cognitive Science, 15:343?
399.
Tom Mikolov, Martin Karafit, Luk Burget, Jan ernock,
and Sanjeev Khudanpur. 2010. Recurrent neu-
ral network based language model. In Proceed-
ings of the 11th Annual Conference of the Interna-
tional Speech Communication Association (INTER-
SPEECH 2010), volume 2010, pages 1045?1048.
International Speech Communication Association.
1497
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
JH Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Acoustics, Speech and Signal Processing
(ICASSP), 2011 IEEE International Conference on,
pages 5528?5531. IEEE.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
Andriy Mnih and Yee Whye Teh. 2012. A fast and
simple algorithm for training neural probabilistic
language models. In Proceedings of the Interna-
tional Conference on Machine Learning.
Anil Kumar Nelakanti, Cedric Archambeau, Julien
Mairal, Francis Bach, and Guillaume Bouchard.
2013. Structured penalties for log-linear language
models. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 233?243, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On Structuring Probabilistic Dependencies in
Stochastic Language Modelling. Computer Speech
and Language, 8:1?38.
Franz Josef Och. 1995. Maximum-likelihood-
sch?atzung von wortkategorien mit verfahren der
kombinatorischen optimierung. Bachelor?s thesis
(Studienarbeit), University of Erlangen.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311?318.
Lawrence Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of speech recognition.
Brian Roark, Cyril Allauzen, and Michael Riley. 2013.
Smoothed marginal distribution constraints for lan-
guage modeling. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 43?52.
Ruslan Salakhutdinov and Andriy Mnih. 2008.
Bayesian probabilistic matrix factorization using
Markov chain Monte Carlo. In Proceedings of the
25th international conference on Machine learning,
pages 880?887. ACM.
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language processing. In Proceedings of the sec-
ond conference on empirical methods in natural lan-
guage processing, pages 81?89. Somerset, New Jer-
sey: Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference in Spoken Language Pro-
cessing.
Xiaoyuan Su and Taghi M Khoshgoftaar. 2009. A sur-
vey of collaborative filtering techniques. Advances
in artificial intelligence, 2009:4.
Yee Whye Teh. 2006. A hierarchical bayesian lan-
guage model based on pitman-yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 985?992. Association for Computa-
tional Linguistics.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1387?1392, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Frank Wood and Yee Whye Teh. 2009. A hierarchical
nonparametric Bayesian approach to statistical lan-
guage model domain adaptation. In Artificial Intel-
ligence and Statistics, pages 607?614.
Frank Wood, C?edric Archambeau, Jan Gasthaus,
Lancelot James, and Yee Whye Teh. 2009. A
stochastic memoizer for sequence data. In Proceed-
ings of the 26th Annual International Conference on
Machine Learning, pages 1129?1136. ACM.
Jun Wu and Sanjeev Khudanpur. 2000. Efficient train-
ing methods for maximum entropy language model-
ing. In Interspeech, pages 114?118.
Puyang Xu, Asela Gunawardana, and Sanjeev Khu-
danpur. 2011. Efficient subsampling for training
complex language models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 1128?1136,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
George Zipf. 1949. Human behaviour and the prin-
ciple of least-effort. Addison-Wesley, Cambridge,
MA.
1498
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953?1964,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Latent-Variable Synchronous CFGs for Hierarchical Translation
Avneesh Saluja and Chris Dyer
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{avneesh,cdyer}@cs.cmu.edu
Shay B. Cohen
University of Edinburgh
Edinburgh EH8 9AB, UK
scohen@inf.ed.ac.uk
Abstract
Data-driven refinement of non-terminal
categories has been demonstrated to be
a reliable technique for improving mono-
lingual parsing with PCFGs. In this pa-
per, we extend these techniques to learn
latent refinements of single-category syn-
chronous grammars, so as to improve
translation performance. We compare two
estimators for this latent-variable model:
one based on EM and the other is a spec-
tral algorithm based on the method of mo-
ments. We evaluate their performance on a
Chinese?English translation task. The re-
sults indicate that we can achieve signifi-
cant gains over the baseline with both ap-
proaches, but in particular the moments-
based estimator is both faster and performs
better than EM.
1 Introduction
Translation models based on synchronous context-
free grammars (SCFGs) treat the translation prob-
lem as a context-free parsing problem. A parser
constructs trees over the input sentence by pars-
ing with the source language projection of a syn-
chronous CFG, and each derivation induces trans-
lations in the target language (Chiang, 2007).
However, in contrast to syntactic parsing, where
linguistic intuitions can help elucidate the ?right?
tree structure for a grammatical sentence, no such
intuitions are available for synchronous deriva-
tions, and so learning the ?right? grammars is a
central challenge.
Of course, learning synchronous grammars
from parallel data is a widely studied problem
(Wu, 1997; Blunsom et al., 2008; Levenberg et
al., 2012, inter alia). However, there has been
less exploration of learning rich non-terminal cat-
egories, largely because previous efforts to learn
such categories have been coupled with efforts
to learn derivation structures?a computationally
formidable challenge. One popular approach has
been to derive categories from source and/or target
monolingual grammars (Galley et al., 2004; Zoll-
mann and Venugopal, 2006; Hanneman and Lavie,
2013). While often successful, accurate parsers
are not available in many languages: a more ap-
pealing approach is therefore to learn the category
structure from the data itself.
In this work, we take a different approach to
previous work in synchronous grammar induc-
tion by assuming that reasonable tree structures
for a parallel corpus can be chosen heuristically,
and then, fixing the trees (thereby enabling us to
sidestep the worst of the computational issues), we
learn non-terminal categories as latent variables to
explain the distribution of these synchronous trees.
This technique has a long history in monolingual
parsing (Petrov et al., 2006; Liang et al., 2007;
Cohen et al., 2014), where it reliably yields state-
of-the-art phrase structure parsers based on gen-
erative models, but we are the first to apply it to
translation.
We first generalize the concept of latent PCFGs
to latent-variable SCFGs (?2). We then follow
by a presentation of the tensor-based formulation
for our parameters, a representation that makes it
convenient to marginalize over latent states. Sub-
sequently, two methods for parameter estimation
are presented (?4): a spectral approach based on
the method of moments, and an EM-based likeli-
hood maximization. Results on a Chinese?English
evaluation set (?5) indicate significant gains over
baselines and point to the promise of using latent-
variable synchronous grammars in conjunction
with a smaller, simpler set of rules instead of un-
wieldy and bloated grammars extracted via exist-
ing heuristics, where a large number of context-
independent but un-generalizable rules are uti-
lized. Hence, the hope is that this work pro-
1953
motes the move towards translation models that
directly model the conditional likelihood of trans-
lation rules via (potentially feature-rich) latent-
variable models which leverage information con-
tained in the synchronous tree structure, instead
of relying on a heuristic set of features based on
empirical relative frequencies (Koehn et al., 2003)
from non-hierarchical phrase-based translation.
2 Latent-Variable SCFGs
Before discussing parameter learning, we in-
troduce latent-variable synchronous context-free
grammars (L-SCFGs) and discuss an inference al-
gorithm for marginalizing over latent states.
We extend the definition of L-PCFGs (Mat-
suzaki et al., 2005; Petrov et al., 2006) to syn-
chronous grammars as used in machine transla-
tion (Chiang, 2007). A latent-variable SCFG (L-
SCFG) is a 6-tuple (N ,m, n
s
, n
t
, pi, t) where:
? N is a set of non-terminal (NT) symbols in the
grammar. For hierarchical phrase-based transla-
tion (HPBT), the set consists of only two sym-
bols, X and a goal symbol S.
? [m] is the set of possible hidden states associ-
ated with NTs. Aligned pairs of NTs across the
source and target languages share the same hid-
den state.
? [n
s
] is the set of source side words, i.e., the
source-side vocabulary, with [n
s
] ?N = ?.
? [n
t
] is the set of target side words, i.e., the
target-side vocabulary, with [n
t
] ?N = ?.
? The synchronous production rules compose a
setR = R
0
?R
1
?R
2
:
? Arity 2 (binary) rules (R
2
):
a(h
1
)? ??
1
b(h
2
)?
2
c(h
3
)?
3
, ?
1
b(h
2
)?
2
c(h
3
)?
3
?
or
a(h
1
)? ??
1
b(h
2
)?
2
c(h
3
)?
3
, ?
1
c(h
2
)?
2
b(h
3
)?
3
?
where a, b, c ? N , h
1
, h
2
, h
3
? [m],
?
1
, ?
2
, ?
3
? [n
s
]
?
and ?
1
, ?
2
, ?
3
? [n
t
]
?
.
? Arity 1 (unary) rules (R
1
):
a(h
1
)? ??
1
b(h
2
)?
2
, ?
1
b(h
2
)?
2
?
where a, b ? N , h
1
, h
2
? [m], ?
1
, ?
2
? [n
s
]
?
and ?, ?
2
? [n
t
]
?
.
? Pre-terminal rules (R
0
): a(h
1
) ? ??, ??
where a ? N , ? ? [n
t
]
?
and ? ? [n
s
]
?
.
Each of these rules is associated with a proba-
bility t(a(h
1
) ? ?|a, h
1
) where ? is the right-
hand side (RHS) of the rule.
? For a ? N , h ? [m], pi(a, h) is a parameter
specifying the root probability of a(h).
A skeletal tree (s-tree) for a sentence is the set
of rules in the synchronous derivation of that sen-
tence, without any additional latent state informa-
tion or decoration. A full tree consists of an s-
tree r
1
, . . . , r
N
together with values h
1
, . . . , h
N
for every NT in the tree. An important point to
keep in mind in comparison to L-PCFGs is that
the right-hand side (RHS) non-terminals of syn-
chronous rules are aligned pairs across the source
and target languages.
In this work, we refine the one-category gram-
mar introduced by Chiang (2007) for HPBT in or-
der to learn additional latent NT categories. Thus,
the following discussion is restricted to these kinds
of grammars, although the method is equally ap-
plicable in other scenarios, e.g., the extended tree-
to-string transducer (xRs) formalism (Huang et
al., 2006; Graehl et al., 2008) commonly used in
syntax-directed translation, and phrase-based MT
(Koehn et al., 2003).
Marginal Inference with L-SCFGs. For a pa-
rameter t of rule r, the latent state h
1
attached to
the left-hand side (LHS) NT of r is associated with
the outside tree for the sub-tree rooted at the LHS,
and the states attached to the RHS NTs are asso-
ciated with the inside trees of that NT. Since we
do not assume conditional independence of these
states, we need to consider all possible interac-
tions, which can be compactly represented as a
3
rd
-order tensor in the case of a binary rule, a ma-
trix (i.e., a 2
nd
-order tensor) for unary rules, and
a vector for pre-terminal (lexical) rules. Prefer-
ences for certain outside-inside tree combinations
are reflected in the values contained in these tensor
structures. In this manner, we intend to capture in-
teractions between non-local context of a phrase,
which can typically be represented via features de-
fined over outside trees of the node spanning the
phrase, and the interior context, correspondingly
defined via features over the inside trees. We re-
fer to these tensor structures collectively as C
r
for
rules r ? R, which encompass the parameters t.
For r ? R
0
: C
r
? R
m?1
; similarly for
r ? R
1
: C
r
? R
m?m
and r ? R
2
: C
r
?
R
m?m?m
. We also maintain a vector C
S
? R
1?m
corresponding to the parameters pi(S, h) for the
1954
Inputs: Sentence f
1
. . . f
N
, L-SCFG (N , S,m, n), param-
eters C
r
? R
(m?m?m)
, ? R
(m?m)
, or ? R
(m?1)
for all
r ? R, C
S
? R
(1?m)
, hypergraphH.
Data structures:
For each node q ? H:
? ?(q) ? Rm?1 is a column vector of inside terms.
? ?(q) ? R1?m is a row vector of outside terms.
? For each incoming edge e ? B(q) to node q, ?(e) is a
marginal probability for edge (rule) e.
Algorithm:
. Inside Computation
For nodes q in topological order inH,
?(q) = 0
For each incoming edge e ? B(q),
tail = t(e), rule = r(e)
if |tail| = 0, then ?(q) = ?(q) + C rule
else if |tail| = 1, then ?(q) = ?(q) +
C
rule
?
1
?(tail
0
)
else if |tail| = 2, then ?(q) = ?(q) +
C
rule
?
2
?(tail
1
)?
1
?(tail
0
)
. Outside Computation
For q ? H,
?(q) = 0
?(goal) = CS
For q in reverse topological order inH,
For each incoming edge e ? B(q),
tail = t(e), rule = r(e)
if |tail| = 1, then
?(tail
0
) = ?(tail
0
) + ?(q)?
0
C
rule
else if |tail| = 2, then
?(tail
0
) = ?(tail
0
) +
?(q)?
0
C
rule
?
2
?(tail
1
)
?(tail
1
) = ?(tail
1
) +
?(q)?
0
C
rule
?
1
?(tail
0
)
.Edge Marginals
Sentence probability g = ?(goal)? ?(goal)
For edge e ? H,
head = h(e), tail = t(e), rule = r(e)
if |tail| = 0, then ?(e) = (?(head)?
0
C
rule
)/g
else if |tail| = 1, then ?(e) = (?(head) ?
0
C
rule
?
1
?(tail
0
))/g
else if |tail| = 2, then ?(e) = (?(head) ?
0
C
rule
?
2
?(tail
1
)?
1
?(tail
0
))/g
Figure 1: The tensor form of the hypergraph inside-
outside algorithm, for calculation of rule marginals ?(e). A
slight simplification in the marginal computation yields NT
marginals for spans ?(X, i, j). B(q) returns the incoming hy-
peredges for node q, and h(e), t(e), r(e) return the head node,
tail nodes, and rule for hyperedge e.
goal node (root). These parameters participate in
tensor-vector operations: a 3
rd
-order tensor C
r
2
can be multiplied along each of its three modes
(?
0
,?
1
,?
2
), and if multiplied by an m ? 1 vec-
tor, will produce an m?m matrix.
1
Note that ma-
trix multiplication can be represented by ?
1
when
multiplying on the right and ?
0
when multiplying
on the left of the matrix. The decoder computes
marginal probabilities for each skeletal rule in the
1
This operation is sometimes called a contraction.
parse forest of a source sentence by marginaliz-
ing over the latent states, which in practice corre-
sponds to simple tensor-vector products. This op-
eration is not dependent on the manner in which
the parameters were estimated.
Figure 1 presents the tensor version of the
inside-outside algorithm for decoding L-SCFGs.
The algorithm takes as input the parse forest of
the source sentence represented as a hypergraph
(Klein and Manning, 2001), which is computed
using a bottom-up parser with Earley-style rules
similar to the algorithm in Chiang (2007). Hyper-
graphs are a compact way to represent a forest of
multiple parse trees. Each node in the hypergraph
corresponds to an NT span, and can have multiple
incoming and outgoing hyperedges. Hyperedges,
which connect one or more tail nodes to a single
head node, correspond exactly to rules, and tail or
head nodes correspond to children (RHS NTs) or
parent (LHS NT). The function B(q) returns all in-
coming hyperedges to a node q, i.e., all rules such
that the LHS NT of the rule corresponds to the NT
span of the node q. The algorithm computes inside
and outside probabilities over the hypergraph us-
ing the tensor representations, and converts these
probabilities to marginal rule probabilities. It is
similar to the version presented in Cohen et al.
(2014), but adapted to hypergraph parse forests.
The complexity of this decoding algorithm is
O(n
3
m
3
|G|) where n is the length of the input
sentence, m is the number of latent states, and |G|
is the number of production rules in the grammar
without latent-variable annotations (i.e., m = 1).
2
The bulk of the computation is a series of tensor-
vector products of relatively small size (each di-
mension is of length m), which can be computed
very quickly and in parallel. The tensor computa-
tions can be significantly sped up using techniques
described by Cohen and Collins (2012), so that
they are linear in m and not cubic.
3 Derivation Trees for Parallel Sentences
To estimate the parameters t and pi of an L-
SCFG (discussed in detail in the next section),
we assume the existence of a dataset composed
of synchronous s-trees, which can be acquired
from word alignments. Normally in phrase-based
translation models, we consider all possible phrase
2
In practice, the term m
3
|G| can be replaced with a
smaller term, which separates the rules inG by the number of
NTs on the RHS. This idea relates to the notion of ?effective
grammar size? which we discuss in ?5.
1955
pairs consistent with the word alignments and es-
timate features based on surface statistics associ-
ated with the phrase pairs or rules. The weights of
these features are then learned using a discrimina-
tive training algorithm (Och, 2003; Chiang, 2012,
inter alia). In contrast, in this work we restrict
the number of possible synchronous derivations
for each sentence pair to just one; thus, derivation
forests do not have to be considered, making pa-
rameter estimation more tractable.
3
To achieve this objective, for each sentence in
the training data we extract the minimal set of
synchronous rules consistent with the word align-
ments, as opposed to the composed set of rules
(Galley et al., 2006). Composed rules are ones that
can be formed from smaller rules in the grammar;
with these rules, there are multiple synchronous
trees consistent with the alignments for a given
sentence pair, and thus the total number of applica-
ble rules can be combinatorially larger than if we
just consider the set of rules that cannot be formed
from other rules, namely the minimal rules. The
rule types across all sentence pairs are combined
to form a minimal grammar.
4
To extract a set of
minimal rules, we use the linear-time extraction
algorithm of Zhang et al. (2008). We give a rough
description of their method below, and refer the
reader to the original paper for additional details.
The algorithm returns a complete minimal
derivation tree for each word-aligned sentence
pair, and generalizes an approach for finding all
common intervals (pairs of phrases such that no
word pair in the alignment links a word inside
the phrase to a word outside the phrase) between
two permutations (Uno and Yagiura, 2000) to se-
quences with many-to-many alignment links be-
tween the two sides, as in word alignment. The
key idea is to encode all phrase pairs of a sen-
tence alignment in a tree of size proportional to
the source sentence length, which they call the
normalized decomposition tree. Each node cor-
responds to a phrase pair, with larger phrase spans
represented by higher nodes in the tree. Construct-
ing the tree is analogous to finding common in-
tervals in two permutations, a property that they
leverage to propose a linear-time algorithm for tree
3
For future work, we will consider efficient algorithms for
parameter estimation over derivation forests, since there may
be multiple valid ways to explain the sentence pair via a syn-
chronous tree structure.
4
Table 2 presents a comparison of grammar sizes for our
experiments (?5.1).
extraction. Converting the tree to a set of minimal
SCFG rules for the sentence pair is straightfor-
ward, by replacing nodes corresponding to spans
with lexical items or NTs in a bottom-up manner.
5
By using minimal rules as a starting point
instead of the traditional heuristically-extracted
rules (Chiang, 2007) or arbitrary compositions of
minimal rules (Galley et al., 2006), we are also
able to explore the transition from minimal rules
to composed ones in a principled manner by en-
coding contextual information through the latent
states. Thus, a beneficial side effect of our re-
finement process is the creation of more context-
specific rules without increasing the overall size
of the baseline grammar, instead holding this in-
formation in our parameters C
r
.
4 Parameter Estimation for L-SCFGs
We explore two methods for estimating the param-
eters C
r
of the model: a likelihood-maximization
approach based on EM (Dempster et al., 1977),
and a spectral approach based on the method of
moments (Hsu et al., 2009; Cohen et al., 2014),
where we identify a subspace using a singular
value decomposition (SVD) of the cross-product
feature space between inside and outside trees and
estimate parameters in this subspace.
Figure 2 presents a side-by-side comparison of
the two algorithms, which we discuss in this sec-
tion. In the spectral approach, we base our pa-
rameter estimates on low-rank representations of
moments of features, while EM explicitly maxi-
mizes a likelihood criterion. The parameter es-
timation algorithms are relatively similar, but in
lieu of sparse feature functions in the spectral case,
EM uses partial counts estimated with the current
set of parameters. The nature of EM allows it to
be susceptible to local optima, while the spectral
approach comes with guarantees on obtaining the
global optimum (Cohen et al., 2014). Lastly, com-
puting the SVD and estimating parameters in the
low-rank space is a one-shot operation, as opposed
to the iterative procedure of EM, and therefore is
much more computationally efficient.
4.1 Estimation with Spectral Method
We generalize the parameter estimation algorithm
presented in Cohen et al. (2013) to the syn-
5
We filtered rules with arity 3 and above (i.e., containing
more than 3 NTs on the RHS). While the L-SCFG formalism
is perfectly capable of handling such cases, it would have re-
sulted in higher order tensors for our parameter structures.
1956
Inputs:
Training examples (r
(i)
, t
(i,1)
, t
(i,2)
, t
(i,3)
, o
(i)
, b
(i)
)
for i ? {1 . . .M}, where r
(i)
is a context free rule;
t
(i,1)
, t
(i,2)
, and t
(i,3)
are inside trees; o
(i)
is an out-
side tree; and b
(i)
= 1 if the rule is at the root of tree,
0 otherwise. A function ? that maps inside trees t to
feature-vectors ?(t) ? R
d
. A function ? that maps
outside trees o to feature-vectors ?(o) ? R
d
?
.
Algorithm:
. Step 0: Singular Value Decomposition
? Compute the SVD of Eq. 1 to calculate matri-
ces
?
U ? R
(d?m)
and
?
V ? R
(d
?
?m)
.
. Step 1: Projection
Y (t) = U
>
?(t)
Z(o) = ?
?1
V
>
?(o)
. Step 2: Calculate Correlations
?
E
r
=
?
?
??
?
??
?
o?Q
r
Z(o)
|Q
r
|
if r ? R
0
?
(o,t)?Q
r
Z(o)?Y (t)
|Q
r
|
if r ? R
1
?
(
o,t
2
,t
3
)
?Q
r
Z(o)?Y (t
2
)?Y (t
3
)
|Q
r
|
if r ? R
2
Q
r
is the set of outside-inside tree triples for binary
rules, outside-inside tree pairs for unary rules, and
outside trees for pre-terminals.
. Step 3: Compute Final Parameters
? For all r ? R,
?
C
r
=
count(r)
M
?
?
E
r
? For all r
(i)
? {1, . . . ,M} such that b
(i)
is 1,
?
C
S
=
?
C
S
+
Y (t
(i,1)
)
|Q
S
|
Q
S
is the set of trees at the root.
(a) The spectral learning algorithm for estimating pa-
rameters of an L-SCFG.
Inputs:
Training examples (r
(i)
, t
(i,1)
, t
(i,2)
, t
(i,3)
, o
(i)
, b
(i)
) for i ?
{1 . . .M}, where r
(i)
is a context free rule; t
(i,1)
, t
(i,2)
, and
t
(i,3)
are inside trees; o
(i)
is an outside tree; b
(i)
= 1 if the rule
is at the root of tree, 0 otherwise; and MAX ITERATIONS.
Algorithm:
. Step 0: Parameter Initialization
For rule r ? R,
? if r ? R
0
: initialize
?
C
r
? R
m?1
? if r ? R
1
: initialize
?
C
r
R
m?m
? if r ? R
2
: initialize
?
C
r
R
m?m?m
Initialize
?
C
S
? R
m?1
?
C
r
0
=
?
C
r
,
?
C
S
0
=
?
C
S
For iteration t = 1, . . . ,MAX ITERATIONS,
? Expectation Step:
. Estimate Y and Z
Compute partial counts and total tree probabili-
ties g for all t and o using Fig. 1 and parameters
?
C
r
t?1
,
?
C
S
t?1
.
. Calculate Correlations
?
E
r
=
?
?
?
??
?
?
??
?
o,g?Q
r
Z(o)
g
if r ? R
0
?
(o,t,g)?Q
r
Z(o)?Y (t)
g
if r ? R
1
?
(
o,t
2
,t
3
,g
)
?Q
r
Z(o)?Y (t
2
)?Y (t
3
)
g
if r ? R
2
. Update Parameters
For all r ? R,
?
C
r
t
=
?
C
r
t?1

?
E
r
For all r
(i)
? {1, . . . ,M} such that b
(i)
is 1,
?
C
S
t
=
?
C
S
t
+ (
?
C
S
t?1
 Y (r
(i)
))/g
Q
S
is the set of trees at the root.
? Maximization Step
if r ? R
0
: ?h
1
:
?
C
r
(h
1
) =
?
C
r
(h
1
)
?
r
?
=r
?
h
1
?
C
r
?
(h
1
)
if r ? R
1
: ?h
1
, h
2
:
?
C
r
(h
1
, h
2
) =
?
C
r
(h
1
,h
2
)
?
r
?
=r
?
h
2
?
C
r
?
(h
1
,h
2
)
if r ? R
2
: ?h
1
, h
2
, h
3
:
?
C
r
(h
1
, h
2
, h
3
) =
?
C
r
(h
1
,h
2
,h
3
)
?
r
?
=r
?
h
2
,h
3
?
C
r
?
(h
1
,h
2
,h
3
)
if LHS(r) = S: ?h
1
:
?
C
r
(h
1
) =
?
C
r
(h
1
)
?
r
?
=r
?
h
1
?
C
r
?
(h
1
)
(b) The EM-based algorithm for estimating parameters of an L-
SCFG.
Figure 2: The two parameter estimation algorithms proposed for L-SCFGs; (a) method of moments; (b) expectation maxi-
mization.  is the element-wise multiplication operator.
chronous or bilingual case. The central concept
of the spectral parameter estimation algorithm is
to learn an m-dimensional representation of in-
side and outside trees by defining these trees in
terms of features, in combination with a projection
step (SVD), with the hope being that the lower-
dimensional space captures the syntactic and se-
mantic regularities among rules from the sparse
feature space. Every NT in an s-tree has an as-
sociated inside and outside tree; the inside tree
contains the entire sub-tree at and below the NT,
and the outside tree is everything else in the syn-
chronous s-tree except the inside tree. The inside
feature function ? maps the domain of inside tree
1957
fragments to a d-dimensional Euclidean space,
and the outside feature function ? maps the do-
main of outside tree fragments to a d
?
-dimensional
space. The specific features we used are discussed
in ?5.2.
Let O be the set of all tuples of inside-outside
trees in our training corpus, whose size is equiva-
lent to the number of rule tokens (occurrences in
the corpus)M , and let ?(t) ? R
d?1
, ?(o) ? R
d
?
?1
be the inside and outside feature functions for in-
side tree t and outside tree o. By computing the
outer product ? between the inside and outside
feature vectors for each pair and aggregating, we
obtain the empirical inside-outside feature covari-
ance matrix:
?
? =
1
|O|
?
(o,t)?O
?(t) (?(o))
>
(1)
If m is the desired latent space dimension, we
compute an m-rank truncated SVD of the empir-
ical covariance matrix
?
? ? U?V
>
, where U ?
R
d?m
and V ? R
d
?
?m
are the matrices containing
the left and right singular vectors, and ? ? R
m?m
is a diagonal matrix containing the m-largest sin-
gular values along its diagonal.
Figure 2a provides the remaining steps in the
algorithm. The M training examples are obtained
by considering all nodes in all of the synchronous
s-trees given as input. In step 1, for each inside
and outside tree, we project its high-dimensional
representation to the m-dimensional latent space.
Using the m-dimensional representations for in-
side and outside trees, in step 2 for each rule type r
we compute the covariance between the inside tree
vectors and the outside tree vector using the ten-
sor product, a generalized outer product to com-
pute covariances between more than two random
vectors. For binary rules, with two child inside
vectors and one outside vector, the result
?
E
r
is a
3-mode tensor; for unary rules, a regular matrix,
and for pre-terminal rules with no right-hand side
non-terminals, a vector. The final parameter es-
timate is then the associated tensor/matrix/vector,
scaled by the maximum likelihood estimate of the
rule r, as in step 3.
The corresponding theoretical guarantees from
Cohen et al. (2014) can also be generalized to
the synchronous case.
?
? is an empirical esti-
mate of the true covariance matrix ?, and if ?
has rank m, then the marginals computed using
the spectrally-estimated parameters will converge
to the true marginals, with the sample complexity
for convergence inversely proportional to a poly-
nomial function of the m
th
largest singular value
of ?.
4.2 Estimation with EM
A likelihood maximization approach can also be
used to learn the parameters of an L-SCFG. Pa-
rameters are initialized by sampling each param-
eter value
?
C
r
(h
1
, h
2
, h
3
) from the interval [0, 1]
uniformly at random.
6
We first decode the train-
ing corpus using an existing set of parameters to
compute the inside and outside probability vectors
associated with NTs for every rule in each s-tree,
constrained to the tree structure of the training ex-
ample. These probabilities can be computed us-
ing the decoding algorithm in Figure 1 (where ?
and ? correspond to the inside and outside proba-
bilities respectively), except the parse forest con-
sists of a single tree only. These vectors repre-
sent partial counts over latent states. We then de-
fine functions Y and Z (analogous to the spectral
case) which map inside and outside tree instances
to m-dimensional vectors containing these partial
counts. In the spectral case, Y and Z are estimated
just once, while in the case of EM they have to be
re-estimated at each iteration.
The expectation step thus consists of comput-
ing the partial counts of inside and outside trees t
and o, i.e., recovering the functions Y and Z, and
updating parameters C
r
by computing correla-
tions, which involves summing over partial counts
(across all occurrences of a rule in the corpus).
Each partial count?s contribution is divided by a
normalization factor g, which is the total probabil-
ity of the tree which t or o is part of. Note that
unlike the spectral case, there is a specific normal-
ization factor for each inside-outside tuple. Lastly,
the correlations are scaled by the existing parame-
ter estimates.
To obtain the next set of parameters, in the max-
imization step we normalize
?
C
r
for r ? R such
that for every h
1
,
?
r
?
=r,h
2
,h
3
?
C
r
?
(h
1
, h
2
, h
3
) = 1
for r ? R
2
,
?
r
?
=r,h
2
?
C
r
?
(h
1
, h
2
) = 1 for r ? R
1
,
and
?
r
?
=r,h
2
?
C
r
?
(h
2
) = 1 for r ? R
0
. We
also normalize the root rule parameters
?
C
r
where
LHS(r) = S. It is also possible to add sparse,
overlapping features to an EM-based estimation
6
In our experiments, we also tried the initialization
scheme described in Matsuzaki et al. (2005), but found that it
provided little benefit.
1958
procedure (Berg-Kirkpatrick et al., 2010) and we
leave this extension for future work.
5 Experiments
The goal of the experimental section is to evalu-
ate the performance of the latent-variable SCFG
in comparison to a baseline without any additional
NT annotations (MIN-GRAMMAR), and to com-
pare the performance of the two parameter esti-
mation algorithms. We also compare L-SCFGs to
a HIERO baseline (Chiang, 2007). The language
pair of evaluation is Chinese?English (ZH-EN).
We score translations using BLEU (Papineni
et al., 2002). The latent-variable model is inte-
grated into the standard MT pipeline by comput-
ing marginal probabilities for each rule in the parse
forest of a source sentence using the algorithm in
Figure 1 with the parameters estimated through
the algorithms in Figure 2, and is added as a fea-
ture for the rule during MERT (Och, 2003). These
probabilities are conditioned on the LHS (X), and
are thus joint probabilities for a source-target RHS
pair. We also write out as features the condi-
tional relative frequencies
?
P (e|f) and
?
P (f |e) as
estimated by our latent-variable model, i.e., con-
ditioned on the source and target RHS.
Overall, we find that both the spectral and
the EM-based estimators improve upon a mini-
mal grammar baseline with only a single cate-
gory, but the spectral approach does better. In fact,
it matches the performance of the standard HI-
ERO baseline, despite learning on top of a minimal
grammar.
5.1 Data and Baselines
The ZH-EN data is the BTEC parallel corpus
(Paul, 2009); we combine the first and second
development sets in one, and evaluate on the third
development set. The development and test sets
are evaluated with 16 references. Statistics for
the data are shown in Table 1. We used the CDEC
decoder (Dyer et al., 2010) to extract word align-
ments and the baseline hierarchical grammars,
MERT tuning, and decoding. We used a 4-gram
language model built from the target-side of the
parallel training data. The Python-based imple-
mentation of the tensor-based decoder, as well as
the parameter estimation algorithms is available at
github.com/asaluja/spectral-scfg/.
The baseline HIERO system uses a grammar ex-
tracted by applying the commonly used heuris-
ZH-EN
TRAIN (SRC) 334K
TRAIN (TGT) 366K
DEV (SRC) 7K
DEV (TGT) 7.6K
TEST (SRC) 3.8K
TEST (TGT) 3.9K
Table 1: Corpus statistics (in words). For the target DEV and
TEST statistics, we take the first reference.
tics (Chiang, 2007). Each rule is decorated with
two lexical and phrasal features corresponding to
the forward (e|f) and backward (f |e) conditional
log frequencies, along with the log joint frequency
(e, f), the log frequency of the source phrase (f),
and whether the phrase pair or the source phrase
is a singleton. Weights for the language model
(and language model OOV), glue rule, and word
penalty are also tuned. The MIN-GRAMMAR
baseline
7
maintains the same set of weights.
Grammar Number of Rules
HIERO 1.69M
MIN-GRAMMAR 59K
LV m = 1 27.56K
LV m = 8 3.18M
LV m = 16 22.22M
Table 2: Grammar sizes for the different systems; for the
latent-variable models, effective grammar sizes are provided.
Grammar sizes are presented in Table 2. For
the latent-variable models, we provide the effec-
tive grammar size, where the number of NTs on
the RHS of a rule is taken into account when com-
puting the grammar size, by assuming each possi-
ble latent variable configuration amongst the NTs
generates a different rule. Furthermore, all single-
tons are mapped to the OOV rule, while we in-
clude singletons in MIN-GRAMMAR.
8
Hence, ef-
fective grammar size can be computed as m(1 +
|R
>1
0
|) +m
2
|R
1
|+m
3
|R
2
|, whereR
>1
0
is the set
of pre-terminal rules that occur more than once.
5.2 Spectral Features
We use the following set of sparse, binary features
in the spectral learning process:
7
Code to extract the minimal derivation trees is available
at www.cs.rochester.edu/u/gildea/mt/.
8
This OOV mapping is done so that the latent-variable
model can handle unknown tokens.
1959
? Rule Indicator. For the inside features, we con-
sider the rule production containing the current
non-terminal on the left-hand side, as well as
the rules of the children (distinguishing between
left and right children for binary rules). For
the outside features, we consider the parent rule
production along with the rule production of the
sibling (if it exists).
? Lexical. for both the inside and outside fea-
tures, any lexical items that appear in the rule
productions are recorded. Furthermore, we con-
sider the first and last words of spans (left and
right child spans for inside features, distinguish-
ing between the two if both exist, and sibling
span for outside features). Source and target
words are treated separately.
? Length. the span length of the tree and each
of its children for inside features, and the span
length of the parent and sibling for outside fea-
tures.
In our experiments, we instantiated a total of
170,000 rule indicator features, 155,000 lexical
features, and 80 length features.
5.3 Chinese?English Experiments
Table 3 presents a comprehensive evaluation of the
ZH-EN experimental setup. The first section con-
sists of the various baselines we consider. In ad-
dition to the aforementioned baselines, we eval-
uated a setup where the spectral parameters sim-
ply consist of the joint maximum likelihood esti-
mates of the rules. This baseline should perform
en par with MIN-GRAMMAR, which we see is the
case on the development set. The performance
on the test set is better though, primarily because
we also include the reverse log relative frequency
(f |e) computed from the latent-variable model as
an additional feature in MERT. Furthermore, in
line with previous work (Galley et al., 2006) which
compares minimal and composed rules, we find
that minimal grammars take a hit of more than 2.5
BLEU points on the development set, compared to
composed (HIERO) grammars. The m = 1 spec-
tral baseline with only rule indicator features per-
forms slightly better than the minimal grammar
baseline, since it overtly takes into account inside-
outside tree combination preferences in the param-
eters, but improvement is minimal with one latent
state naturally and the performance on the test set
is in line with the MLE baseline.
On top of the baselines, we looked at a number
BLEU
Setup Dev Test
Baselines
HIERO 46.08 55.31
MIN-
GRAMMAR
43.38 51.78
MLE 43.24 52.80
Spectral
m = 1 RI 44.18 52.62
m = 8 RI 44.60 53.63
m = 16 RI 46.06 55.83
m=16 RI+Lex+Sm 46.08 55.22
m=16 RI+Lex+Len 45.70 55.29
m=24 RI+Lex 43.00 51.28
m=32 RI+Lex 43.06 52.16
EM
m = 8 40.53 (0.2) 49.78 (0.5)
m = 16 42.85 (0.2) 52.93 (0.9)
m = 32 41.07 (0.4) 49.95 (0.7)
Table 3: Results for the ZH-EN corpus, comparing across
the baselines and the two parameter estimation techniques.
RI, Lex, and Len correspond to the rule indicator, lexical,
and length features respectively, and Sm denotes smoothing.
For the EM experiments, we selected the best scoring iter-
ation by tuning weights for parameters obtained after 25 it-
erations and evaluating other parameters with these weights.
Results for EM are averaged over 5 starting points, with stan-
dard deviation given in parentheses. Spectral, EM, and MLE
performances compared to the MIN-GRAMMAR baseline are
statistically significant (p < 0.01).
of feature combinations and latent states for the
spectral and EM-estimated latent-variable models.
For the spectral models, we tuned MERT parame-
ters separately for each rank on a set of parameters
estimated from rule indicator features only; subse-
quent variations within a given rank, e.g., the ad-
dition of lexical or length features or smoothing,
were evaluated with the same set of rank-specific
weights from MERT. For EM, we ran parame-
ter estimation with 5 randomly initialized starting
points for 50 iterations; we tuned the MERT pa-
rameters with EM parameters obtained after 25
th
iterations. Similar to the spectral experiments,
we fixed the MERT weight values and evaluated
BLEU performance with parameters after every 5
iterations and chose the iteration with the highest
score on the development set. The results are av-
eraged over the 5 initializations, with standard de-
viation in parentheses.
Firstly, we can see a clear dependence on rank,
with peak performance for the spectral and EM
models occurring at m = 16. In this instance, the
spectral model roughly matches the performance
of the HIERO baseline, but it only uses rules ex-
tracted from a minimal grammar, whose size is a
fraction of the HIERO grammar. The gains seem
to level off at this rank; additional ranks seem to
add noise to the parameters. Feature-wise, addi-
tional lexical and length features add little, prob-
1960
ably because much of this information is encap-
sulated in the rule indicator features. For EM,
m = 16 outperforms the minimal grammar base-
line, but is not at the level of the spectral results.
All EM, spectral, and MLE results are statistically
significant (p < 0.01) with respect to the MIN-
GRAMMAR baseline (Zhang et al., 2004), and the
improvement over the HIERO baseline achieved by
them = 16 rule indicator configuration is also sta-
tistically significant.
The two estimation algorithms differ signifi-
cantly in their estimation time. Given a feature
covariance matrix, the spectral algorithm (SVD,
which was done with Matlab, and correlation com-
putation steps) for m = 16 took 7 minutes, while
the EM algorithm took 5 minutes for each iteration
with this rank.
5.4 Analysis
Figure 3 presents a comparison of the non-
terminal span marginals for two sentences in the
development set. We visualize these differences
through a heat map of the CKY parse chart, where
the starting word of the span is on the rows, and
the span end index is on the columns. Each cell is
shaded to represent the marginal of that particular
non-terminal span, with higher likelihoods in blue
and lower likelihoods in red.
For the most part, marginals at the leaves (i.e.,
pre-terminal marginals) tend to score relatively
similarly across different setups. Higher up in the
chart, the latent SCFG marginals look quite dif-
ferent than the MLE parameters. Most noticeably,
spans starting at the beginning of the sentence are
much more favored. It is these rules that allow
the right translation to be preferred since the MLE
chooses not to place the object of the sentence in
the subject?s span. However, the spectral param-
eters seem to discriminate between these higher-
level rules better than EM, which scores spans
starting with the first word uniformly highly. An-
other interesting point is that the range of likeli-
hoods is much larger in the EM case compared to
the MLE and spectral variants. For the second sen-
tence (row), the 1-best hypothesis produced by all
systems are the same, but the heat map accentuates
the previous observation.
6 Related Work
The goal of refining single-category HPBT gram-
mars or automatically learning the NT categories
in a grammar, instead of relying on noisy parser
outputs, has been explored from several different
angles in the MT literature. Blunsom et al. (2008)
present a Bayesian model for synchronous gram-
mar induction, and place an appropriate nonpara-
metric prior on the parameters. However, their
starting point is to estimate a synchronous gram-
mar with multiple categories from parallel data
(using the word alignments as a prior), while we
aim to refine a fixed grammar with additional la-
tent states. Furthermore, their estimation proce-
dure is extremely expensive and is restricted to
learning up to five NT categories, via a series of
mean-field approximations.
Another approach is to explicitly attach a real-
valued vector to each NT: Huang et al. (2010) use
an external source-language parser for this pur-
pose and score rules based on the similarity be-
tween a source sentence parse and the information
contained in this vector, which explicitly requires
the integration of a good-quality source-language
parser. The EM-based algorithm that we propose
here is similar to what they propose, except that we
need to handle tensor structures. Mylonakis and
Sima?an (2011) select among linguistically moti-
vated non-terminal labels with a cross-validated
version of EM. Although they consider a restricted
hypothesis space, they do marginalize over dif-
ferent derivations therefore their inside-outside al-
gorithm is O(n
6
). In the syntax-directed trans-
lation literature, there have been efforts to relax
or coarsen the hard labels provided by a syntactic
parser in an automatic manner to promote param-
eter sharing (Venugopal et al., 2009; Hanneman
and Lavie, 2013), which is the complement of our
aim in this paper.
The idea of automatically learned grammar re-
finements comes from the monolingual parsing lit-
erature, where phenomena like head lexicalization
can be modeled through latent variables. Mat-
suzaki et al. (2005) look at a likelihood-based
method to split the NT categories of a gram-
mar into a fixed number of sub-categories, while
Petrov et al. (2006) learn a variable number of
sub-categories per NT. The latter?s extension may
be useful for finding the optimal number of latent
states from the data in our case.
The question of whether we can incorporate ad-
ditional contextual information in minimal rule
grammars in MT via auxiliary models instead of
using longer, composed rules has been investi-
gated before as well. n-gram translation mod-
1961
0 1 2 3 4Span End
Span
 start
ing a
t wor
d: 
1.05
0.90
0.75
0.60
0.45
0.30
0.15
0.00
ln(su
m)
I go away .
(a) MLE
0 1 2 3 4Span End
Span
 start
ing a
t wor
d: 
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
ln(su
m)
I ?ll bring it .
(b) Spectral m = 16 RI
0 1 2 3 4Span End
Span
 start
ing a
t wor
d: 
9
8
7
6
5
4
3
2
1
0
ln(su
m)
I ?ll bring it .
(c) EM m = 16
0 1 2 3 4 5 6 7Span End
Span
 start
ing a
t wor
d: 
2.00
1.75
1.50
1.25
1.00
0.75
0.50
0.25
0.00
ln(su
m)
I ?d like a shampoo and style .
(d) MLE
0 1 2 3 4 5 6 7Span End
Span
 start
ing a
t wor
d: 
3.2
2.8
2.4
2.0
1.6
1.2
0.8
0.4
0.0
ln(su
m)
I ?d like a shampoo and style .
(e) Spectral m = 16 RI
0 1 2 3 4 5 6 7Span End
Span
 start
ing a
t wor
d: 
10.5
9.0
7.5
6.0
4.5
3.0
1.5
0.0
ln(su
m)
I ?d like a shampoo and style .
(f) EM m = 16
Figure 3: A comparison of the CKY charts containing marginal probabilities of non-terminal spans ?(X, i, j) for the MLE,
spectral m = 16 with rule indicator features, and EM m = 16, for the two Chinese sentences. Higher likelihoods are in blue,
lower likelihoods in red. The hypotheses produced by each setup are below the heat maps.
els (Mari?no et al., 2006; Durrani et al., 2011)
seek to model long-distance dependencies and re-
orderings through n-grams. Similarly, Vaswani
et al. (2011) use a Markov model in the context
of tree-to-string translation, where the parameters
are smoothed with absolute discounting (Ney et
al., 1994), while in our instance we capture this
smoothing effect through low rank or latent states.
Feng and Cohn (2013) also utilize a Markov model
for MT, but learn the parameters through a more
sophisticated estimation technique that makes use
of Pitman-Yor hierarchical priors.
Hsu et al. (2009) presented one of the initial
efforts at spectral-based parameter estimation (us-
ing SVD) of observed moments for latent-variable
models, in the case of Hidden Markov models.
This idea was extended to L-PCFGs (Cohen et al.,
2014), and our approach can be seen as a bilingual
or synchronous generalization.
7 Conclusion
In this work, we presented an approach to re-
fine synchronous grammars used in MT by in-
ferring the latent categories for the single non-
terminal in our grammar rules, and proposed two
algorithms to estimate parameters for our latent-
variable model. By fixing the synchronous deriva-
tions of each parallel sentence in the training data,
it is possible to avoid many of the computational
issues associated with synchronous grammar in-
duction. Improvements over a minimal grammar
baseline and equivalent performance to a hierar-
chical phrase-based baseline are achieved by the
spectral approach. For future work, we will seek
to relax this consideration and jointly reason about
non-terminal categories and derivation structures.
Acknowledgements
The authors would like to thank Daniel Gildea
for sharing his code to extract minimal derivation
trees, Stefan Riezler for useful discussions, Bren-
dan O?Connor for the CKY visualization advice,
and the anonymous reviewers for their feedback.
This work was supported by a grant from eBay
Inc. (Saluja), the U. S. Army Research Laboratory
and the U. S. Army Research Office under con-
tract/grant number W911NF-10-1-0533 (Dyer).
1962
References
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian Synchronous Grammar Induction. In Pro-
ceedings of NIPS.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
David Chiang. 2012. Hope and Fear for Dis-
criminative Training of Statistical Translation Mod-
els. Journal of Machine Learning Research, pages
1159?1187.
Shay B. Cohen and Michael Collins. 2012. Tensor
decomposition for fast parsing with latent-variable
PCFGs. In Proceedings of NIPS.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2013. Experiments with
spectral learning of latent-variable PCFGs. In Pro-
ceedings of NAACL.
Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.
Foster, and Lyle Ungar. 2014. Spectral learning
of latent-variable PCFGs: Algorithms and sample
complexity. Journal of Machine Learning Research.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society, Series B, 39(1):1?38.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with inte-
grated reordering. In Proceedings of ACL.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL.
Yang Feng and Trevor Cohn. 2013. A Markov
model of machine translation using non-parametric
bayesian inference. In Proceedings of ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3):391?427, September.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL.
Daniel Hsu, Sham M. Kakade, and Tong Zhang. 2009.
A Spectral Algorithm for Learning Hidden Markov
Models. In Proceedings of COLT.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Zhongqiang Huang, Martin
?
Cmejrek, and Bowen
Zhou. 2010. Soft syntactic constraints for hierar-
chical phrase-based translation using latent syntactic
distributions. In Proceedings of EMNLP.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of NAACL.
Abby Levenberg, Chris Dyer, and Phil Blunsom. 2012.
A Bayesian model for learning SCFGs with discon-
tiguous rules. In Proceedings of EMNLP-CoNLL.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The infinite PCFG using hierarchical
dirichlet processes. In Proceedings of EMNLP.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonol-
losa, and Marta R. Costa-juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549, December.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of ACL.
Markos Mylonakis and Khalil Sima?an. 2011. Learn-
ing hierarchical translation structure with linguistic
annotations. In Proceedings of ACL.
Hermann Ney, Ute Essen, and Reinhard Kneser.
1994. On Structuring Probabilistic Dependencies in
Stochastic Language Modelling. Computer Speech
and Language, 8:1?38.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Michael Paul. 2009. Overview of the IWSLT 2009
evaluation campaign. In Proceedings of IWSLT.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of ACL.
1963
Takeaki Uno and Mutsunori Yagiura. 2000. Fast al-
gorithms to enumerate all common intervals of two
permutations. Algorithmica, 26(2):290?309.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule Markov models for fast tree-to-
string translation. In Proceedings of ACL.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2009. Preference grammars:
Softening syntactic constraints to improve statistical
machine translation. In Proceedings of NAACL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403, Septem-
ber.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system. In
In Proceedings LREC.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings of
COLING.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, StatMT ?06, pages 138?141.
Association for Computational Linguistics.
1964
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 395?404,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Learning from Post-Editing:
Online Model Adaptation for Statistical Machine Translation
Michael Denkowski Chris Dyer Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{mdenkows,cdyer,alavie}@cs.cmu.edu
Abstract
Using machine translation output as a
starting point for human translation has
become an increasingly common applica-
tion of MT. We propose and evaluate three
computationally efficient online methods
for updating statistical MT systems in a
scenario where post-edited MT output is
constantly being returned to the system:
(1) adding new rules to the translation
model from the post-edited content, (2)
updating a Bayesian language model of
the target language that is used by the
MT system, and (3) updating the MT
system?s discriminative parameters with
a MIRA step. Individually, these tech-
niques can substantially improve MT qual-
ity, even over strong baselines. Moreover,
we see super-additive improvements when
all three techniques are used in tandem.
1 Introduction
Using machine translation outputs as a starting
point for human translators is becoming increas-
ingly common and is now arguably one of the most
commercially important applications of MT. Con-
siderable evidence has accumulated showing that
human translators are more productive and accu-
rate when post-editing MT output than when trans-
lating from scratch (Guerberof, 2009; Carl et al.,
2011; Koehn, 2012; Zhechev, 2012, inter alia).
An important (if unsurprising) insight from prior
research in this area is that translators become
more productive as MT quality improves (Tat-
sumi, 2009). While general improvements to MT
continue to lead to further productivity gains, we
explore how MT quality can be improved specifi-
cally in an online post-editing scenario in which
sentence-level MT outputs are constantly being
presented to human experts, edited, and then re-
turned to the system for immediate learning. This
task is challenging in two regards. First, from a
technical perspective, post-edited outputs must be
processed rapidly: a productive post-editor cannot
wait for a standard batch MT training pipeline to
be rerun after each sentence is corrected! Sec-
ond, from a methodological perspective, it is ex-
pensive to run many human subject experiments,
in particular when the human subjects must have
translation expertise. We therefore use a sim-
ulated post-editing paradigm in which either
non-post-edited reference translations or manually
post-edited translations from a similar MT system
are used in lieu of human post-editors (?2). This
paradigm allows us to efficiently develop and eval-
uate systems that can go on to function in real-time
post-editing scenarios without modification.
We present and evaluate three online methods
for improving translation models using feedback
from editors: adding new translations rules to
the translation grammar (?3), updating a Bayesian
language model with observations of the post-
edited output (?4), and using an online discrimi-
native parameter update to minimize model error
(?5). These techniques are computationally effi-
cient and make minimal use of approximation or
heuristics, handling initial and incremental data in
a uniform way. We evaluate these techniques in a
variety of language and data scenarios that mimic
the demands of real-world translation tasks. Com-
pared to a competitive baseline, we show substan-
tial improvement from updating the translation
grammar or language model independently and
super-additive gains from combining these tech-
niques with a MIRA update (?6). We then discuss
how our techniques relate to prior work (?7) and
conclude (?8).
2 Simulated Post-Editing Paradigm
In post-editing scenarios, humans continuously
edit machine translation outputs into production-
quality translations, providing an additional, con-
395
stant stream of data absent in batch translation.
This data consists of highly domain-relevant ref-
erence translations that are minimally different
from MT outputs, making them ideal for learn-
ing. However, true post-editing data is infeasi-
ble to collect during system development and in-
ternal testing as standard MT pipelines require
tens of thousands of sentences to be translated
with low latency. To address this problem, Hardt
and Elming (2010) formulate the task of sim-
ulated post-editing, wherein pre-generated refer-
ence translations are used as a stand-in for actual
post-editing. This approximation is equivalent to
the case where humans edit each translation hy-
pothesis to be identical to the reference rather than
simply correcting the MT output to be grammat-
ical and meaning-equivalent to the source. Our
work uses this approximation for tuning and eval-
uation. We also introduce a more accurate approx-
imation wherein MT output from the target sys-
tem (or a similar system) is post-edited in advance,
creating ?offline? post-edited data that is similar
to expected system outputs and should thus min-
imize unnecessary edits. An experiment in ?6.4
compares the two approximations.
In our simulated post-editing tasks, decoding
(for both the test corpus and each pass over the
development corpus during optimization) begins
with baseline models trained on standard bilin-
gual and monolingual data. After each sentence
is translated, the following take place in order:
First, MIRA uses the new source?reference pair
to update weights for the current models. Second,
the source is aligned to the reference and used to
update the translation grammar. Third, the refer-
ence is added to the Bayesian language model. As
sentences are translated, the models gain valuable
context information, allowing them to zero in on
the target document and translator. Context is re-
set at the start of each development or test corpus.
1
This setup, which allows a uniform approach to
tuning and decoding, is visualized in Figure 1.
3 Translation Grammar Adaptation
Translation models (either phrase tables or syn-
chronous grammars) are typically generated of-
fline from large bilingual text. This is reasonable
in scenarios where available training data is fixed
over long periods of time. However, this approach
1
Initial experiments show this to outperform resetting
models on more fine-grained document boundaries, although
further investigation is warranted.
Hola contestadora ...
Hello voicemail, ...
He llamado a servicio ... I?ve called for tech ...
Ignor?e la advertencia ... I ignored my boss? ...
Ahora anochece, ...
Now it?s evening, and ...
Todav??a sigo en espera ...
I?m still on hold ...
No creo que me hayas ... I don?t think you ...
Ya he presionado cada ... I punched every touch ...
Incremental training data
Source
Target (Reference)
Figure 1: Context when translating an input sen-
tence (bold) with simulated post-editing. Previ-
ous sentences and references (shaded) are added
to the training data. After the current sentence is
translated, it is aligned to the reference (italic) and
added to the context for the next sentence.
does not allow adding new data without repeating
model estimation in its entirety, which may take
hours or days. In this section, we describe a simple
technique for incorporating new bilingual training
data as soon as it is available. Our approach is
an extension of the on-demand grammar extractor
described by Lopez (2008a). We extend the work
initially designed for on-the-fly grammar extrac-
tion from static data (to mitigate the expense of
storing large translation grammars), to specifically
handle incremental data from post-editing.
3.1 Suffix Array Grammar Extraction
Lopez (2008a) introduces an alternative to tradi-
tional model estimation for hierarchical phrase-
based statistical machine translation (Chiang,
2007). Rather than estimating a single grammar
from all training data, the aligned bitext is indexed
using a source-side suffix array (Manber and My-
ers, 1993). When an input sentence is to be trans-
lated, a grammar extraction program samples in-
stances of aligned phrase pairs from the suffix ar-
ray that match the source side of the sentence.
Using statistics from these samples rather than
the entire bitext, a sentence-specific grammar is
rapidly generated. In addition to speed gains from
sampling, indexing the source side of the bitext fa-
cilitates a more powerful feature set. Rules in on-
demand grammars are generated using a sample S
for each source phrase f in the input sentence. The
sample, containing pairs ?f, e?, is used to calculate
the following statistics:
396
Feature Baseline Adaptive
coherent
p(e|f)
C
S
(f, e)
|S|
C
S
(f, e) + C
L
(f, e)
|S|+ |L|
sample size |S| |S|+ |L|
co-occur-
rence ?f, e?
C
S
(f, e) C
S
(f, e)+C
L
(f, e)
singleton f
C
S
(f)
= 1
C
S
(f) + C
L
(f) =
1
singleton
?f, e?
C
S
(f, e)
= 1
C
S
(f, e) + C
L
(f, e)
= 1
post-edit sup-
port ?f, e?
0 C
L
(f, e) > 0
Table 1: Phrase feature definitions for baseline and
adaptive translation models.
? C
S
(f, e): count of instances in S where f
aligns to e (phrase co-occurrence count).
? C
S
(f): count of instances in S where f aligns
to any target phrase.
? |S|: total number of instances in S, equal to
number of occurrences of f in training data,
capped by the sample size limit.
These statistics are used to instantiate translation
rules X??f, e? and calculate scores for the phrase
feature set shown in the ?Baseline? column of Ta-
ble 1. Notably, the coherent phrase translation
probability that conditions on f occurring in the
data (|S|) rather than f being extracted as part of a
phrase pair (C
S
(f)) is shown by Lopez (2008b) to
yield significant improvement over the traditional
translation probability.
3.2 Online Grammar Extraction
When a human translator post-edits MT output, a
new bilingual sentence pair is created. However,
in typical settings, it can be weeks or months be-
fore these training instances are incorporated into
bilingual data and models retrained. Our exten-
sion to on-demand grammar extraction incorpo-
rates these new training instances into the model
immediately. In addition to a static suffix array
that indexes initial data, our system maintains a
dynamic lookup table. Each new sentence pair is
word-aligned with the model estimated from the
initial data (a process often called forced align-
ment). This makes a generally insignificant ap-
proximation with respect to the original alignment
model. Extractable phrase pairs are stored in the
lookup table and phrase occurrences are counted
on the source side. When subsequent grammars
are extracted, the suffix array sample S for each
f is accompanied by an exhaustive lookup L from
the lookup table. Matching statistics are calculated
from L:
? C
L
(f, e): count of instances in L where f
aligns to e.
? C
L
(f): count of instances inLwhere f aligns
to any target phrase.
? |L|: total number of instances of f in post-
editing data (no size limit).
We use combined statistics from S and L to calcu-
late scores for the ?Adaptive? feature set defined in
Table 1. In addition to updating existing features,
we introduce a new indicator feature that identi-
fies rules supported by post-editor feedback. Fur-
ther, our approach allows us to extract rules that
encode translations (phrase mappings and reorder-
ings) only observed in the incremental post-editing
data. This process, which can be seen as influ-
encing the distribution from which grammars are
sampled over time, produces comparable results
to the infeasible process of rebuilding the transla-
tion model after every sentence is translated with
the added benefit of allowing an optimizer to learn
a weight for the post-edited data via the post-edit
support feature. The simple aggregation of statis-
tics allows our model to handle initial and incre-
mental data in a formally consistent way. Further,
any additional features that can be calculated on a
suffix array sample can be matched by an incre-
mental data lookup, making our translation model
a viable platform for further exploration in online
learning for MT.
4 Language Model Adaptation
Adapting language models in an online manner
based on the content they are generating has long
been seen as a promising technique for improving
automatic speech recognition and machine transla-
tion (Kuhn and de Mori, 1990; Zhao et al., 2004;
Sanchis-Trilles, 2012, inter alia). The post-editing
scenario we are considering simplifies this process
somewhat since rather than only having a poste-
rior distribution over machine-generated outputs
(any of which may be ungrammatical), the out-
puts, once edited by human translators, may be
presumed to be grammatical.
We thus take a novel approach to language
model adaptation, building on recent work show-
ing that state-of-the-art language models can be
397
inferred as the posterior predictive distribution
of a Bayesian language model with hierarchi-
cal Pitman-Yor process priors, conditioned on the
training corpus (Teh, 2006). The Bayesian formu-
lation provides a natural way to incorporate pro-
gressively more data: by updating the posterior
distribution given subsequent observations. Fur-
thermore, the nonparametric nature of the model
means that the model is well suited to poten-
tially unbounded growth of vocabulary. Unfortu-
nately, in general, Bayesian techniques are com-
putationally difficult to work with. However, hi-
erarchical Pitman-Yor process language models
(HPYPLMs) are convenient in this regard since
(1) inference can be carried out efficiently in a
convenient collapsed representation (the ?Chinese
restaurant franchise?) and (2) the posterior predic-
tive distribution from a single sample provides a
high quality language model.
We thus use the following procedure. Using
the target side of the bitext as observations, we
run the Gibbs sampling procedure described by
Teh (2006) for 100 iterations in a 3-gram HPY-
PLM. The inferred ?seating configuration? defines
a posterior predictive distribution over words in 2-
gram contexts (as with any 3-gram LM) as well
as a posterior distribution over how the model will
generate subsequent observations. We use the for-
mer as a language model component of a transla-
tion model. And, as post-edited sentences become
available, we add their n-grams to the model us-
ing the later. We do not run any Gibbs sampling.
Just updating the language model in this way, we
obtain the results shown in Table 2 for the experi-
mental conditions described in ?6.
5 Learning Feature Weights
MT system parameter optimization (learning fea-
ture weights for the decoder) is also typically con-
ducted as a batch process. Discriminative learn-
ing techniques such as minimum error rate train-
ing (Och, 2003) are used to find feature weights
that maximize automatic metric score on a small
development corpus. The resulting weight vector
is then used to decode given input sentences. Us-
ing this approach with post-editing tasks presents
two major issues. First, reference translation are
only considered after all sentences are translated,
a mismatch with post-editing where references are
available incrementally. Second, despite the fact
that adaptive feature sets become more powerful
as post-editing data increases, an optimizer must
Spanish?English WMT10 WMT11 TED1 TED2
HPYPLM 25.5 24.8 29.4 26.6
+data 25.8 25.2 29.5 27.0
English?Spanish WMT10 WMT11 TED1 TED2
HPYPLM 25.1 26.8 26.0 24.3
+data 25.4 27.2 26.2 25.0
Arabic?English MT08 MT09 TED1 TED2
HPYPLM 19.3 24.7 9.5 10.0
+data 19.6 24.9 9.8 10.5
Table 2: BLEU scores for systems with trigram
HPYPLM (no large language model), with and
without incremental updates from simulated post-
editing data. Scores are averages over 3 optimizer
runs. Bold scores indicate statistically significant
improvement. Tuning set scores are italicized.
learn a single corpus-level weight for each fea-
ture. This forces an averaging effect that can lead
to decoding individual sentences with suboptimal
weights. We address the first issue by using ref-
erence translations to simulate post-editing (Hardt
and Elming, 2010) at tuning time and the second
by using a version of the margin-infused relaxed
algorithm (Crammer et al., 2006; Eidelman, 2012)
to make online parameter updates during decod-
ing. The result is a consistent approach to tuning
and decoding that brings out the potential of adap-
tive models.
5.1 Parameter Optimization
In order to make our decoding process fully con-
sistent with tuning, we introduce an online dis-
criminative parameter update that allows our adap-
tive translation and language models be weighted
appropriately as more data is available. This re-
quires an optimization algorithm that can func-
tion as an online learner during decoding as well
as a batch optimizer during tuning. Popular opti-
mizers such as MERT (Och, 2003) and pairwise
rank optimization (Hopkins and May, 2011) can-
not be used due to their reliance on corpus-level
optimization. We select the cutting-plane variant
of the margin-infused relaxed algorithm (Chiang,
2012; Crammer et al., 2006) with additional exten-
sions described by Eidelman (2012). MIRA is an
online large-margin learner that makes a param-
eter update after each model prediction with the
objective of choosing the correct output over the
incorrect output by a margin at least as large as the
cost of predicting the incorrect output. Applied
398
to MT system optimization on a development cor-
pus, MIRA proceeds as follows. The MT system
generates a list of the k best translations for a sin-
gle input sentence. From the list, a ?hope? hy-
pothesis is selected as a translation with both high
model score and high automatic metric score. A
?fear? hypothesis is selected as a translation with
high model score but low metric score. Parameters
are updated away from the fear hypothesis, toward
the hope hypothesis, and the system processes the
next input sentence. This process continues for a
set number of passes over the development corpus.
All adaptive systems used in our work are opti-
mized with this variant of MIRA using the param-
eter settings described by Eidelman (2012). For
each pass over the data, translation and language
models have incremental access to reference trans-
lations (simulated post-editing data) as input sen-
tences are translated. Translation and language
models reset to using background data only at the
beginning of each MIRA iteration.
2
5.2 Online Parameter Updates
Our optimization strategy allows us to treat de-
coding as if it were simply the next iteration of
MIRA (or alternatively that MIRA makes a single
pass over an input corpus that consists of the de-
velopment data concatenated n times followed by
unseen input data). After each sentence is trans-
lated, a reference translation (resulting from ac-
tual human post-editing in production or simulated
post-editing for our experiments) is provided to
the models and MIRA makes a parameter update.
In the only departure from our optimization setup,
we decrease the maximum step size for MIRA (de-
scribed in ?6.2), effectively increasing regulariza-
tion strength. This allows us to prefer small ad-
justments to already optimized decoding parame-
ters over the large changes needed during tuning.
It is also important to note that by using MIRA
for updating weights during both tuning and de-
coding, we avoid scaling issues between multiple
optimizers (such as when tuning with MERT and
updating with a passive-aggressive algorithm).
6 Experiments
We evaluate our online extensions to standard
machine translation systems in a series of sim-
2
Resetting translation and language models prevents con-
tamination. If models retained state from previous passes
over the development set, they would include data for input
sentences before they were translated, rather than after as in
post-editing.
Spanish?English WMT10 WMT11 TED1 TED2
Base MERT 29.1 27.9 32.8 29.6
Base MIRA 29.2 28.0 32.7 29.7
G 29.8 28.3 34.2 30.7
L 29.2 28.1 33.0 29.8
M 29.2 28.1 33.1 29.8
G+L+M 30.0 28.8 35.2 31.3
English?Spanish WMT10 WMT11 TED1 TED2
Base MERT 27.8 29.4 26.5 25.7
Base MIRA 27.7 29.6 26.8 26.7
G 28.1 29.8 27.9 27.5
L 27.9 29.7 26.8 26.5
M 27.9 29.7 27.2 26.6
G+L+M 28.4 30.4 28.6 27.9
Arabic?English MT08 MT09 TED1 TED2
Base MERT 21.5 25.0 10.4 10.5
Base MIRA 21.2 25.9 10.6 10.9
G 21.8 26.2 11.0 11.7
L 20.6 25.7 10.6 10.9
M 21.3 25.7 10.8 11.0
G+L+M 21.8 26.5 11.4 11.8
Table 3: BLEU scores for baseline and adap-
tive systems. Scores are averages over three opti-
mizer runs. Highest scores are bold and tuning set
scores are italicized. All fully adaptive systems
(G+L+M) show statistically significant improve-
ment over both MERT and MIRA baselines.
ulated post-editing experiments that cover high-
traffic languages and challenging domains. We
show incremental improvement from our adaptive
models and significantly larger gains when pair-
ing our models with an online parameter update.
We finally validate our adaptive system on actual
post-edited data.
6.1 Data
We conduct a series of simulated post-editing
experiments in three full scale language sce-
narios: Spanish?English, English?Spanish, and
Arabic?English. Spanish?English and English?
Spanish systems are trained on the 2012 NAACL
WMT (Callison-Burch et al., 2012) constrained
resources (2 million bilingual sentences, 300 mil-
lion words of monolingual Spanish, and 1.1 billion
words of monolingual English). Arabic?English
systems are trained on the 2012 NIST OpenMT
(Przybocki, 2012) constrained bilingual resources
plus a selection from the English Gigaword cor-
pus (Parker et al., 2011) (5 million bilingual sen-
tences and 650 million words of monolingual En-
399
glish). We tune and evaluate on standard news
sets: WMT10 and WMT11 for Spanish?English
and English?Spanish, and MT08 and MT09 for
Arabic?English. To simulate real-world post edit-
ing where one translator works on a document at a
time, we use only one of the four available refer-
ence translation sets for MT08 and MT09.
We also evaluate on a blind domain adapta-
tion scenario that mimics the demands placed
on MT systems in real-world translation tasks.
The Web Inventory of Transcribed and Translated
Talks (WIT
3
) corpus (Cettolo et al., 2012) makes
transcriptions of TED talks
3
available in several
languages, including English, Spanish, and Ara-
bic. For each language pair, we select two sets of
10 talk transcripts each (2000-3000 sentences) as
blind evaluation sets. These sets consist of spoken
language covering a broad range of topics. Sys-
tems have no access to any training or develop-
ment data in this domain prior to translation.
6.2 Translation Systems
For each language scenario, we first construct a
competitive baseline system. Bilingual data is
word aligned using the model described by Dyer
et al. (2013) and suffix array-backed transla-
tion grammars are extracted using the method
described by Lopez (2008a). We add the stan-
dard lexical and derivation features
4
from Lopez
(2008b) and Dyer et al. (2010). An unpruned,
modified Kneser-Ney-smoothed 4-gram language
model is estimated using the KenLM toolkit
(Heafield et al., 2013). Feature weights are op-
timized using the lattice-based variant of MERT
(Macherey et al., 2008; Och, 2003) on either
WMT10 or MT08. Evaluation sets are translated
using the cdec decoder (Dyer et al., 2010) and
evaluated with the BLEU metric (Papineni et al.,
2002). These results are listed as ?Base MERT?
in Table 3. To establish a baseline for our adap-
tive systems, we tune the same baseline system
using cutting-plane MIRA with 500-best lists, the
pseudo-document approximation described by Ei-
delman (2012), and a maximum update size of
0.01. We begin with uniform weights and make
20 passes over the development corpus. Results
for this system are listed as ?Base MIRA?.
To evaluate the impact of each online model
adaptation technique, we report the results for the
3
http://www.ted.com/talks
4
Derivation features consist of word count, discretized
rule-level non-terminal count (0, 1, or 2), glue rule count,
and out-of-vocabulary pass-through count.
News TED Talks
New Supp New Supp
Spanish?English 15% 19% 14% 18%
English?Spanish 12% 16% 9% 13%
Arabic?English 9% 12% 23% 28%
Table 5: Percentages of new rules (only seen
in incremental data) and post-edit supported rules
(Rules from all data for which the ?post-edit sup-
port ?f, e?? feature fires) in grammars by domain.
following systems in Table 3:
? G: Baseline MIRA system with online gram-
mar extraction, including incrementally up-
dating existing phrase features plus an addi-
tional indicator feature for post-edit support.
? L: Baseline MIRA with a trigram hierarchi-
cal Pitman-Yor process language model that
is incrementally updated, including a sepa-
rate out-of-vocabulary feature.
? M: Baseline MIRA with online feature
weight updates from cutting-plane MIRA.
Finally, we report results for a fully adaptive
system that includes online grammar, language
model, and feature weight updates. This system
is reported as ?G+L+M?. To account for optimizer
instability, all systems are tuned (consisting of
running either MERT or MIRA) and evaluated 3
times. We report average scores over optimizer
runs and conduct statistical significance tests us-
ing the methods described by Clark et al. (2011).
6.3 Results
Our simulated translation post-editing experi-
ments are summarized in Table 3. Simply mov-
ing from MERT to cutting-plane MIRA for pa-
rameter optimization yields improvement in most
cases, corroborating existing work (Eidelman,
2012). Using incremental post-editing data to up-
date translation grammars (G) yields further im-
provement in all cases evaluated. Gains are signif-
icantly larger for TED talks where translator feed-
back can bridge the gap between domains. Table 5
shows the aggregate percentages of rules in online
grammars that are entirely new (extracted from
post-editing instances only) or post-edit supported
(superset of new rules). While percentages vary
by data set, the overall trend is a combination of
learning new vocabulary and reordering and dis-
ambiguating existing translation choices.
The introduction of a trigram Bayesian lan-
guage model (L) yields mixed results: in some
400
Base MERT and changing the definition of what the Zona Cero is .
G+L+M and the changing definition of what the Ground Zero is .
Reference and the changing definition of what Ground Zero is .
Base MERT was that when we side by side comparisons with coal , timber
G+L+M was that when we did side-by-side comparisons with wood charcoal ,
Reference was when we did side-by-side comparisons with wood charcoal ,
Base MERT There was a way ? there was one ?
G+L+M There was a way ? there had to be a way ?
Reference There was a way ? there had to be a way ?
Table 4: Translation examples from baseline and fully adaptive systems of Spanish TED talks into En-
glish. Examples illustrate (from top to bottom) learning translations for new vocabulary items, selecting
correct translation candidates for the domain, and learning domain-appropriate phrasing.
cases it leads to slight improvement and in oth-
ers, degradation. It appears that a static but large
4-gram language model often outperforms an in-
crementally updated but smaller trigram model.
Further, learning a single weight for the Bayesian
model can lead to a harmful mismatch. As a tun-
ing pass over the development corpus proceeds,
the model incorporates additional data and MIRA
learns a weight corresponding to its predictive
ability at the end of the corpus. During decod-
ing, all sentences are translated with this language
model weight, even before the model can ade-
quately adapt itself to the target domain. This
problem is alleviated in our fully adaptive system.
Using cutting-plane MIRA to incrementally up-
date weights during decoding (M) also leads to
mixed results, frequently resulting in both small
increases and decreases in score. This could be
due to the noise incurred when making small ad-
justments to static features after each sentence:
depending on the similarity between the previous
and current sentence and the limit of the step size
(regularization strength), a parameter update may
slightly improve or degrade translation.
Finally, we see significantly larger gains for
our fully adaptive system (G+L+M) that com-
bines adaptive translation grammars and language
models with online parameter updates. In many
cases, the difference between the baseline sys-
tems and our adaptive system is greater than the
sum of the differences from our individual tech-
niques, demonstrating the effectiveness of com-
bining online learning methods. Our final sys-
tem has two key advantages over any individual
extension. First, incremental updates from MIRA
can rescale weights for features that change over
time, keeping the model consistent. Second, the
Bayesian language model?s out-of-vocabulary fea-
ture can discriminate between true OOV items
and vocabulary items in the post-editing data not
present in the monolingual data. By contrast, the
only OOVs in the baseline system are untranslated
items, as the target side of the bitext is included in
the language model training data. This interplay
between the adaptive components in our transla-
tion system leads to significant gains over MERT
and MIRA baselines. Table 4 contains examples
from our system?s output that exemplify key im-
provements in translation quality. With respect to
performance, our fully adaptive system translates
an average of 1.5 sentences per second per CPU
core. The additional cost incurred updating trans-
lation grammars and language models is less than
one second per sentence (though the baseline cost
of on-demand grammar extraction can be up to a
few seconds). In total, the system is well within
the acceptable speed range needed to function in
real-time human translation scenarios.
6.4 Evaluation Using Post-Edited References
The 2012 ACL Workshop on Machine Translation
(Callison-Burch et al., 2012) makes available a set
of 1832 English?Spanish parallel news source sen-
tences, independent references, initial MT outputs,
and post-edited MT outputs. The employed MT
system is trained on largely the same resources as
our own English?Spanish system, granting the op-
portunity for a much closer approximation to an
actual post-editing task; our system configurations
score between 54 and 56 BLEU against the sam-
ple MT, indicating that humans post-edited trans-
lations similar but not identical to our own. We
split the data into development and test sets, each
916 sentences, and run 3 iterations of optimizing
on the development set and evaluating on the test
set with both the MERT baseline and our G+L+M
401
system on both types of references. Using inde-
pendent references for tuning and evaluation (as
before), our system yields an improvement of 0.6
BLEU (23.3 to 23.9). With post-edited references,
our system yields an improvement of 1.3 BLEU
(43.0 to 44.3). This provides strong evidence that
our adaptive systems would provide better trans-
lations (both in terms of absolute quality and im-
provement over a standard baseline) for real-world
post-editing scenarios.
7 Related Work
Prior work has led to the extension of standard
phrase-based translation systems to make use of
incrementally available data.
5
Approaches gen-
erally fall into categories of adding new data to
translation models and of using incremental data
to adjust model parameters (feature weights). In
the first case, Nepveu et al. (2004) use cache-based
translation and language models to incorporate
data from the current document into a computer-
aided translation scenario. Ortiz-Mart??nez et al.
(2010) augment a standard translation model by
storing sufficient statistics in addition to feature
scores for phrase pairs, allowing feature values to
be incrementally updated as new sentence pairs
are available for phrase extraction. Hardt and Elm-
ing (2010) demonstrate the benefit of maintain-
ing a distinction between background and post-
editing data in an adaptive model with simulated
post-editing. Though not targeted at post-editing
applications, the most similar work to our online
grammar adaptation is the stream-based transla-
tion model described by Levenberg et al. (2010).
The authors introduce a dynamic suffix array that
can incorporate new training text as it becomes
available. Sanchis-Trilles (2012) proposes a strat-
egy for online language model adaptation wherein
several smaller domain-specific models are built
and their scores interpolated for each sentence
translated based on the target domain.
Focusing on incrementally updating model pa-
rameters with post-editing data, Mart??nez-G?omez
et al. (2012) and L?opez-Salcedo et al. (2012)
show improvement under some conditions when
using techniques including passive-aggressive al-
gorithms, perceptron, and discriminative ridge re-
gression to adapt feature weights for systems ini-
tially tuned using MERT. This work also uses ref-
erence translations to simulate post-editing. Saluja
5
Prior to phrase-based systems, NISHIDA et al. (1988)
use post-editing data to correct errors in transfer-based MT.
et al. (2012) introduce a support vector machine-
based algorithm capable of learning from binary-
labeled examples. This learning algorithm is used
to incrementally adjust feature weights given user
feedback on whether a translation is ?good? or
?bad?. As with our work, this strategy can be used
during both optimization and decoding.
Finally, Simard and Foster (2013) apply a
pipeline solution to the post-editing task wherein
a second stage automatic post-editor (APE) sys-
tem learns to replicate the corrections made to ini-
tial MT output by human translators. As incre-
mental data accumulates, the APE (itself a statisti-
cal phrase-based system) attempts to ?correct? the
MT output before it is shown to humans.
8 Conclusion
Casting machine translation for post-editing as
an online learning task, we have presented three
methods for incremental model adaptation: adding
data to the indexed bitext from which gram-
mars are extracted, updating a Bayesian language
model with incremental data, and using an on-
line discriminative parameter update during de-
coding. These methods, which allow the sys-
tem to handle all data in a uniform way, are ap-
plied to a strong baseline system optimized using
MIRA in conjunction with simulated post-editing.
In addition to showing gains for individual meth-
ods under various circumstances, we report super-
additive improvement from combining our tech-
niques to produce a fully adaptive system. Im-
provements generalize over language and data sce-
narios, with the greatest gains realized in blind
out-of-domain tasks where the system must rely
heavily on post-editor feedback to improve qual-
ity. Gains are also more significant when using of-
fline post-edited references, showing promise for
applying our techniques to real-world post-editing
tasks. All software used for our online model
adaptation experiments is freely available under an
open source license as part of the cdec toolkit.
6
Acknowledgements
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
6
http://www.cs.cmu.edu/
?
mdenkows/
cdec-realtime.html
402
References
[Callison-Burch et al.2012] Chris Callison-Burch,
Philipp Koehn, Christof Monz, Matt Post, Radu
Soricut, and Lucia Specia. 2012. Findings of the
2012 workshop on statistical machine translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada, June. Association for Computational
Linguistics.
[Carl et al.2011] Michael Carl, Barbara Dragsted,
Jakob Elming, Daniel Hardt, and Arnt Lykke
Jakobsen. 2011. The process of post-editing: A
pilot study. Copenhagen Studies in Language,
41:131?142.
[Cettolo et al.2012] Mauro Cettolo, Christian Girardi,
and Marcello Federico. 2012. Wit
3
: Web inventory
of transcribed and translated talks. In Proceedings
of the Sixteenth Annual Conference of the European
Association for Machine Translation.
[Chiang2007] David Chiang. 2007. Hierarchical
phrase-based translation. Computational Linguis-
tics, 33.
[Chiang2012] David Chiang. 2012. Hope and fear for
discriminative training of statistical translation mod-
els. Journal of Machine Learning Research, pages
1159?1187, April.
[Clark et al.2011] Jonathan H. Clark, Chris Dyer, Alon
Lavie, and Noah A. Smith. 2011. Better hypothe-
sis testing for statistical machine translation: Con-
trolling for optimizer instability. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 176?181, Portland, Oregon, USA, June.
Association for Computational Linguistics.
[Crammer et al.2006] Koby Crammer, Ofer Dekel,
Joseph Keshet, Shai Shalev-Shwartz, and Yoram
Singer. 2006. Online passive-aggressive algo-
rithms. Journal of Machine Learning Research,
pages 551?558, March.
[Dyer et al.2010] Chris Dyer, Adam Lopez, Juri Gan-
itkevitch, Jonathan Weese, Ferhan Ture, Phil Blun-
som, Hendra Setiawan, Vladimir Eidelman, and
Philip Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proceedings of the ACL
2010 System Demonstrations, pages 7?12, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
[Dyer et al.2013] Chris Dyer, Victor Chahuneau, and
Noah A. Smith. 2013. A simple, fast, and effective
reparameterization of IBM model 2. In The 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies.
[Eidelman2012] Vladimir Eidelman. 2012. Optimiza-
tion strategies for online large-margin learning in
machine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
480?489, Montr?eal, Canada, June. Association for
Computational Linguistics.
[Guerberof2009] Ana Guerberof. 2009. Productivity
and quality in mt post-editing. In Proceedings of MT
Summit XII - Workshop: Beyond Translation Memo-
ries: New Tools for Translators MT.
[Hardt and Elming2010] Daniel Hardt and Jakob Elm-
ing. 2010. Incremental re-training for post-editing
smt. In Proceedings of the Ninth Conference of the
Association for Machine Translation in the Ameri-
cas.
[Heafield et al.2013] Kenneth Heafield, Ivan
Pouzyrevsky, Jonathan H. Clark, and Philipp
Koehn. 2013. Scalable modified Kneser-Ney
language model estimation. In Proceedings of
the 51st Annual Meeting of the Association for
Computational Linguistics, Sofia, Bulgaria, August.
[Hopkins and May2011] Mark Hopkins and Jonathan
May. 2011. Tuning as ranking. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1352?1362, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
[Koehn2012] Philipp Koehn. 2012. Computer-aided
translation. Machine Translation Marathon.
[Kuhn and de Mori1990] Roland Kuhn and Renato
de Mori. 1990. A cache-based natural language
model for speech recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
12(6).
[Levenberg et al.2010] Abby Levenberg, Chris
Callison-Burch, and Miles Osborne. 2010.
Stream-based translation models for statistical
machine translation. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 394?402, Los Angeles,
California, June. Association for Computational
Linguistics.
[Lopez2008a] Adam Lopez. 2008a. Machine transla-
tion by pattern matching. In Dissertation, Univer-
sity of Maryland, March.
[Lopez2008b] Adam Lopez. 2008b. Tera-scale transla-
tion models via pattern matching. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 505?512,
Manchester, UK, August. Coling 2008 Organizing
Committee.
[L?opez-Salcedo et al.2012] Francisco-Javier L?opez-
Salcedo, Germ?an Sanchis-Trilles, and Francisco
Casacuberta. 2012. Online learning of log-linear
weights in interactive machine translation. Ad-
vances in Speech and Language Technologies for
Iberian Languages, pages 277?286.
403
[Macherey et al.2008] Wolfgang Macherey, Franz Och,
Ignacio Thayer, and Jakob Uszkoreit. 2008. Lattice-
based minimum error rate training for statistical ma-
chine translation. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 725?734, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
[Manber and Myers1993] Udi Manber and Gene My-
ers. 1993. Suffix arrays: A new method for on-
line string searches. SIAM Journal of Computing,
22:935?948.
[Mart??nez-G?omez et al.2012] Pascual Mart??nez-
G?omez, Germ?an Sanchis-Trilles, and Francisco
Casacuberta. 2012. Online adaptation strategies
for statistical machine translation in post-editing
scenarios. Pattern Recognition, 45:3193?3203.
[Nepveu et al.2004] Laurent Nepveu, Guy Lapalme,
Philippe Langlais, and George Foster. 2004. Adap-
tive language and translation models for interactive
machine translation. In Dekang Lin and Dekai Wu,
editors, Proceedings of EMNLP 2004, pages 190?
197, Barcelona, Spain, July. Association for Com-
putational Linguistics.
[NISHIDA et al.1988] Fujio NISHIDA, Shinobu
TAKAMATSU, Tadaaki TANI, and Tsunehisa
DOI. 1988. Feedback of correcting information
in postediting to a machine translation system. In
Proc. of COLING.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 160?167,
Sapporo, Japan, July. Association for Computational
Linguistics.
[Ortiz-Mart??nez et al.2010] Daniel Ortiz-Mart??nez, Is-
mael Garc??a-Varea, and Francisco Casacuberta.
2010. Online learning for interactive statistical ma-
chine translation. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 546?554, Los Ange-
les, California, June. Association for Computational
Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 311?318, Philadelphia, Pennsylvania, USA,
July. Association for Computational Linguistics.
[Parker et al.2011] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2011. En-
glish Gigaword Fifth Edition, June. Linguistic Data
Consortium, LDC2011T07.
[Przybocki2012] Mark Przybocki. 2012. Nist open
machine translation 2012 evaluation (openmt12).
http://www.nist.gov/itl/iad/mig/openmt12.cfm.
[Saluja et al.2012] Avneesh Saluja, Ian Lane, and Ying
Zhang. 2012. Machine translation with binary feed-
back: a large-margin approach. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas.
[Sanchis-Trilles2012] Germ?an Sanchis-Trilles. 2012.
Building task-oriented machine translation systems.
In Ph.D. Thesis, Universitat Politcnica de Valncia.
[Simard and Foster2013] Michel Simard and George
Foster. 2013. PEPr: Post-edit propagation using
phrase-based statistical machine translation. In Pro-
ceedings of the XIV Machine Translation Summit,
pages 191?198,, September.
[Tatsumi2009] Midori Tatsumi. 2009. Correlation
between automatic evaluation metric scores, post-
editing speed, and some other factors. In Proceed-
ings of the Twelfth Machine Translation Summit.
[Teh2006] Yee Whye Teh. 2006. A hierarchical
Bayesian language model based on Pitman-Yor pro-
cesses. In Proc. of ACL.
[Zhao et al.2004] Bing Zhao, Matthias Eck, and
Stephan Vogel. 2004. Language model adaptation
for statistical machine translation with structured
query models. In Proc. of COLING.
[Zhechev2012] Ventsislav Zhechev. 2012. Machine
Translation Infrastructure and Post-editing Perfor-
mance at Autodesk. In AMTA 2012 Workshop
on Post-Editing Technology and Practice (WPTP
2012), pages 87?96, San Diego, USA, October. As-
sociation for Machine Translation in the Americas
(AMTA).
404
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 462?471,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Improving Vector Space Word Representations
Using Multilingual Correlation
Manaal Faruqui and Chris Dyer
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mfaruqui, cdyer}@cs.cmu.edu
Abstract
The distributional hypothesis of Harris
(1954), according to which the meaning
of words is evidenced by the contexts
they occur in, has motivated several effec-
tive techniques for obtaining vector space
semantic representations of words using
unannotated text corpora. This paper ar-
gues that lexico-semantic content should
additionally be invariant across languages
and proposes a simple technique based
on canonical correlation analysis (CCA)
for incorporating multilingual evidence
into vectors generated monolingually. We
evaluate the resulting word representations
on standard lexical semantic evaluation
tasks and show that our method produces
substantially better semantic representa-
tions than monolingual techniques.
1 Introduction
Data-driven learning of vector-space word embed-
dings that capture lexico-semantic properties is
a technique of central importance in natural lan-
guage processing. Using cooccurrence statistics
from a large corpus of text (Deerwester et al.,
1990; Turney and Pantel, 2010),
1
it is possible
to construct high-quality semantic vectors ? as
judged by both correlations with human judge-
ments of semantic relatedness (Turney, 2006;
Agirre et al., 2009) and as features for downstream
applications (Turian et al., 2010).
The observation that vectors representing cooc-
currence tendencies would capture meaning is ex-
pected according to the distributional hypothe-
sis (Harris, 1954), famously articulated by Firth
1
Related approaches use the internal representations from
neural network models of word sequences (Collobert and We-
ston, 2008) or continuous bags-of-context wordsels (Mikolov
et al., 2013a) to arrive at vector representations that likewise
capture cooccurence tendencies and meanings.
(1957) as You shall know a word by the company
it keeps. Although there is much evidence in fa-
vor of the distributional hypothesis, in this paper
we argue for incorporating translational context
when constructing vector space semantic models
(VSMs). Simply put: knowing how words trans-
late is a valuable source of lexico-semantic infor-
mation and should lead to better VSMs.
Parallel corpora have long been recognized as
valuable for lexical semantic applications, in-
cluding identifying word senses (Diab, 2003;
Resnik and Yarowsky, 1999) and paraphrase and
synonymy relationships (Bannard and Callison-
Burch, 2005). The latter work (which we build on)
shows that if different words or phrases in one lan-
guage often translate into a single word or phrase
type in a second language, this is good evidence
that they are synonymous. To illustrate: the En-
glish word forms aeroplane, airplane, and plane
are observed to translate into the same Hindi word:
vAy  yAn (vaayuyaan). Thus, even if we did not
know the relationship between the English words,
this translation fact is evidence that they all have
the same meaning.
How can we exploit information like this when
constructing VSMs? We propose a technique that
first constructs independent VSMs in two lan-
guages and then projects them onto a common
vector space such that translation pairs (as deter-
mined by automatic word alignments) should be
maximally correlated (?2). We review latent se-
mantic analysis (LSA), which serves as our mono-
lingual VSM baseline (?3), and a suite of stan-
dard evaluation tasks that we use to measure the
quality of the embeddings (?4). We then turn to
experiments. We first show that our technique
leads to substantial improvements over monolin-
gual LSA (?5), and then examine how our tech-
nique fares with vectors learned using two dif-
ferent neural networks, one that models word se-
quences and a second that models bags-of-context
462
Figure 1: Cross-lingual word vector projection us-
ing CCA.
words. We observe substantial improvements over
the sequential model using multilingual evidence
but more mixed results relative to using the bags-
of-contexts model (?6).
2 Multilingual Correlation with CCA
To gain information from the translation of a given
word in other languages the most basic thing to do
would be to just append the given word represen-
tation with the word representations of its transla-
tion in the other language. This has three draw-
backs: first, it increases the number of dimensions
in the vector; second, it can pull irrelevant infor-
mation from the other language that doesn?t gen-
eralize across languages and finally the given word
might be out of vocabulary of the parallel corpus
or dictionary.
To counter these problems we use CCA
2
which
is a way of measuring the linear relationship be-
tween two multidimensional variables. It finds two
projection vectors, one for each variable, that are
optimal with respect to correlations. The dimen-
sionality of these new projected vectors is equal to
or less than the smaller dimensionality of the two
variables.
Let ? ? R
n
1
?d
1
and ? ? R
n
2
?d
2
be vector
2
We use the MATLAB module for CCA: http://www.
mathworks.com/help/stats/canoncorr.html
space embeddings of two different vocabularies
where rows represent words. Since the two vo-
cabularies are of different sizes (n
1
and n
2
) and
there might not exist translation for every word
of ? in ?, let ?
?
? ? where every word in ?
?
is translated to one other word
3
in ?
?
? ? and
? ? R
n?d
1
and ? ? R
n?d
2
.
Let x and y be two corresponding vectors from
?
?
and ?
?
, and v and w be two projection direc-
tions. Then, the projected vectors are:
x
?
= xv y
?
= yw
(1)
and the correlation between the projected vectors
can be written as:
?(x
?
,y
?
) =
E[x
?
y
?
]
?
E[x
?
2
]E[y
?
2
]
(2)
CCA maximizes ? for the given set of vectors ?
?
and ?
?
and outputs two projection vectors v and
w:
v,w = CCA(x,y)
= arg max
v,w
?(xv,yw)
(3)
Using these two projection vectors we can project
the entire vocabulary of the two languages ? and
? using equation 1. Summarizing:
V ,W = CCA(?
?
,?
?
) (4)
?
?
= ?V ?
?
= ?W (5)
where, V ? R
d
1
?d
, W ? R
d
2
?d
con-
tain the projection vectors and d =
min{rank(V ), rank(W )}. Thus, the result-
ing vectors cannot be longer than the original
vectors. Since V and W can be used to project
the whole vocabulary, CCA also solves the
problem of not having translations of a particular
word in the dictionary. The schema of performing
CCA on the monolingual word representations of
two languages is shown in Figure 1.
Further Dimensionality Reduction: Since
CCA gives us correlations and corresponding
projection vectors across d dimensions which
can be large, we perform experiments by taking
projections of the original word vectors across
only the top k correlated dimensions. This is
trivial to implement as the projection vectors V ,
3
Further information on how these one-to-one translations
are obtained in ?5
463
W in equation 4 are already sorted in descending
order of correlation. Therefore in,
?
?
k
= ?V
k
?
?
k
= ?W
k
(6)
?
?
k
and?
?
k
are now word vector projections along
the top k correlated dimensions, where, V
k
and
W
k
are the column truncated matrices.
3 Latent Semantic Analysis
We perform latent semantic analysis (Deerwester
et al., 1990) on a word-word co-occurrence ma-
trix. We construct a word co-occurrence frequency
matrix F for a given training corpus where each
row w, represents one word in the corpus and ev-
ery column c, is the context feature in which the
word is observed. In our case, every column is
a word which occurs in a given window length
around the target word. For scalability reasons, we
only select words with frequency greater than 10
as features. We also remove the top 100 most fre-
quent words (mostly stop words) from the column
features.
We then replace every entry in the sparse fre-
quency matrix F by its pointwise mutual infor-
mation (PMI) (Church and Hanks, 1990; Turney,
2001) resulting in X . PMI is designed to give a
high value to x
ij
where there is a interesting rela-
tion between w
i
and c
j
, a small or negative value
of x
ij
indicates that the occurrence of w
i
in c
j
is
uninformative. Finally, we factorize the matrix X
using singular value decomposition (SVD). SVD
decomposes X into the product of three matrices:
X = U?V
>
(7)
where, U and V are in column orthonormal
form and ? is a diagonal matrix of singular val-
ues (Golub and Van Loan, 1996). We obtain a re-
duced dimensional representation of words from
size |V | to k:
A = U
k
?
k
(8)
where k can be controlled to trade off between re-
construction error and number of parameters, ?
k
is the diagonal matrix containing the top k singular
values, U
k
is the matrix produced by selecting the
corresponding columns from U and A represents
the new matrix containing word vector representa-
tions in the reduced dimensional space.
4 Word Representation Evaluation
We evaluate the quality of our word vector repre-
sentations on a number of tasks that test how well
they capture both semantic and syntactic aspects
of the representations.
4.1 Word Similarity
We evaluate our word representations on four dif-
ferent benchmarks that have been widely used to
measure word similarity. The first one is the WS-
353 dataset (Finkelstein et al., 2001) containing
353 pairs of English words that have been assigned
similarity ratings by humans. This data was fur-
ther divided into two fragments by Agirre et al.
(2009) who claimed that similarity (WS-SIM) and
relatedness (WS-REL) are two different kinds of
relations and should be dealt with separately. We
present results on the whole set and on the individ-
ual fragments as well.
The second and third benchmarks are the RG-
65 (Rubenstein and Goodenough, 1965) and the
MC-30 (Miller and Charles, 1991) datasets that
contain 65 and 30 pairs of nouns respectively and
have been given similarity rankings by humans.
These differ from WS-353 in that it contains only
nouns whereas the former contains all kinds of
words. The fourth benchmark is the MTurk-287
(Radinsky et al., 2011) dataset that constitutes of
287 pairs of words and is different from the above
two benchmarks in that it has been constructed by
crowdsourcing the human similarity ratings using
Amazon Mechanical Turk.
We calculate similarity between a given pair
of words by the cosine similarity between their
corresponding vector representation. We then re-
port Spearman?s rank correlation coefficient (My-
ers and Well, 1995) between the rankings pro-
duced by our model against the human rankings.
4.2 Semantic Relations (SEM-REL)
Mikolov et al. (2013a) present a new semantic re-
lation dataset composed of analogous word pairs.
It contains pairs of tuples of word relations that
follow a common semantic relation. For example,
in England : London :: France : Paris, the two
given pairs of words follow the country-capital re-
lation. There are three other such kinds of rela-
tions: country-currency, man-woman, city-in-state
and overall 8869 such pairs of words
4
.
The task here is to find a word d that best fits
the following relationship: a : b :: c : d given a, b
and c. We use the vector offset method described
4
107 pairs were out of vocabulary for our vectors and
were ignored.
464
in Mikolov et al. (2013a) that computes the vector
y = x
a
? x
b
+ x
c
where, x
a
,x
b
and x
c
are word
vectors of a, b and c respectively and returns the
vector x
w
from the whole vocabulary which has
the highest cosine similarity to y:
x
w
= arg max
x
w
x
w
? y
|x
w
| ? |y|
It is worth noting that this is a non-trivial |V |-way
classification task where V is the size of the vo-
cabulary.
4.3 Syntactic Relations (SYN-REL)
This dataset contains word pairs that are differ-
ent syntactic forms of a given word and was pre-
pared by Mikolov et al. (2013a). For exam-
ple, in walking and walked, the second word is
the past tense of the first word. There are nine
such different kinds of relations: adjective-adverb,
opposites, comaparative, superlative, present-
participle, nation-nationality, past tense, plural
nouns and plural verbs. Overall there are 10675
such syntactic pairs of word tuples. The task here
again is identifying a word d that best fits the fol-
lowing relationship: a : b :: c : d and we solve it
using the method described in ?4.2.
5 Experiments
5.1 Data
For English, German and Spanish we used the
WMT-2011
5
monolingual news corpora and for
French we combined the WMT-2011 and 2012
6
monolingual news corpora so that we have around
300 million tokens for each language to train the
word vectors.
For CCA, a one-to-one correspondence be-
tween the two sets of vectors is required. Obvi-
ously, the vocabulary of two languages are of dif-
ferent sizes and hence to obtain one-to-one map-
ping, for every English word we choose a word
from the other language to which it has been
aligned the maximum number of times
7
in a paral-
lel corpus. We got these word alignment counts
using cdec (Dyer et al., 2010) from the paral-
lel news commentary corpora (WMT 2006-10)
combined with the Europarl corpus for English-
{German, French, Spanish}.
5
http://www.statmt.org/wmt11/
6
http://www.statmt.org/wmt12/
7
We also tried weighted average of vectors across all
aligned words and did not observe any significant difference
in results.
5.2 Methodology
We construct LSA word vectors of length 640
8
for
English, German, French and Spanish. We project
the English word vectors using CCA by pairing
them with German, French and Spanish vectors.
For every language pair we take the top k cor-
related dimensions (cf. equation 6), where k ?
10%, 20%, . . . 100% and tune the performance on
WS-353 task. We then select the k that gives
us the best average performance across language
pairs, which is k = 80%, and evaluate the cor-
responding vectors on all other benchmarks. This
prevents us from over-fitting k for every individual
task.
5.3 Results
Table 1 shows the Spearman?s correlation ratio ob-
tained by using word vectors to compute the sim-
ilarity between two given words and compare the
ranked list against human rankings. The first row
in the table shows the baseline scores obtained
by using only the monolingual English vectors
whereas the other rows correspond to the multi-
lingual cases. The last row shows the average per-
formance of the three language pairs. For all the
tasks we get at least an absolute gain of 20 points
over the baseline. These results are highly assur-
ing of our hypothesis that multilingual context can
help in improving the semantic similarity between
similar words as described in the example in ?1.
Results across language pairs remain almost the
same and the differences are most of the times sta-
tistically insignificant.
Table 1 also shows the accuracy obtained on
predicting different kinds of relations between
word pairs. For the SEM-REL task the average
improvement in accuracy is an absolute 30 points
over the baseline which is highly statistically sig-
nificant (p < 0.01) according to the McNemar?s
test (Dietterich, 1998). The same holds true for
the SYN-REL task where we get an average im-
provement of absolute 8 points over the baseline
across the language pairs. Such an improvement
in scores across these relation prediction tasks fur-
ther enforces our claim that cross-lingual context
can be exploited using the method described in ?2
and it does help in encoding the meaning of a word
better in a word vector than monolingual informa-
tion alone.
8
See section 5.5 for further discussion on vector length.
465
Lang Dim WS-353 WS-SIM WS-REL RG-65 MC-30 MTurk-287 SEM-REL SYN-REL
En 640 46.7 56.2 36.5 50.7 42.3 51.2 14.5 36.8
De-En 512 68.0 74.4 64.6 75.5 81.9 53.6 43.9 45.5
Fr-En 512 68.4 73.3 65.7 73.5 81.3 55.5 43.9 44.3
Es-En 512 67.2 71.6 64.5 70.5 78.2 53.6 44.2 44.5
Average ? 56.6 64.5 51.0 62.0 65.5 60.8 44 44.7
Table 1: Spearman?s correlation (left) and accuracy (right) on different tasks.
Figure 2: Monolingual (top) and multilingual (bottom; marked with apostrophe) word projections of the
antonyms (shown in red) and synonyms of ?beautiful?.
5.4 Qualitative Example
To understand how multilingual evidence leads to
better results in semantic evaluation tasks, we plot
the word representations obtained in ?3 of sev-
eral synonyms and antonyms of the word ?beau-
tiful? by projecting both the transformed and un-
transformed vectors onto R
2
using the t-SNE
tool (van der Maaten and Hinton, 2008). The
untransformed LSA vectors are in the upper part
of Fig. 2, and the CCA-projected vectors are in
the lower part. By comparing the two regions,
we see that in the untransformed representations,
the antonyms are in two clusters separated by the
synonyms, whereas in the transformed representa-
tion, both the antonyms and synonyms are in their
own cluster. Furthermore, the average intra-class
distance between synonyms and antonyms is re-
duced.
Figure 3: Performance of monolingual and mul-
tilingual vectors on WS-353 for different vector
lengths.
5.5 Variation in Vector Length
In order to demonstrate that the gains in perfor-
mance by using multilingual correlation sustains
466
for different number of dimensions, we compared
the performance of the monolingual and (German-
English) multilingual vectors with k = 80% (cf.
?5.2). It can be see in figure 3 that the perfor-
mance improvement for multilingual vectors re-
mains almost the same for different vector lengths
strengthening the reliability of our approach.
6 Neural Network Word Representations
Other kinds of vectors shown to be useful in many
NLP tasks are word embeddings obtained from
neural networks. These word embeddings capture
more complex information than just co-occurrence
counts as explained in the next section. We test
our multilingual projection method on two types
of such vectors by keeping the experimental set-
ting exactly the same as in ?5.2.
6.1 RNN Vectors
The recurrent neural network language model
maximizes the log-likelihood of the training cor-
pus. The architecture (Mikolov et al., 2013b) con-
sists of an input layer, a hidden layer with recur-
rent connections to itself, an output layer and the
corresponding weight matrices. The input vector
w(t) represents input word at time t encoded us-
ing 1-of-N encoding and the output layer y(t) pro-
duces a probability distribution over words in the
vocabulary V . The hidden layer maintains a repre-
sentation of the sentence history in s(t). The val-
ues in the hidden and output layer are computed as
follows:
s(t) = f(Uw(t) + Ws(t? 1)) (9)
y(t) = g(V s(t)) (10)
where, f and g are the logistic and softmax func-
tions respectively. U and V are weight matri-
ces and the word representations are found in the
columns of U . The model is trained using back-
propagation. Training such a purely lexical model
will induce representations with syntactic and se-
mantic properties. We use the RNNLM toolkit
9
to
induce these word representations.
6.2 Skip Gram Vectors
In the RNN model (?6.1) most of the complexity
is caused by the non-linear hidden layer. This is
avoided in the new model proposed in Mikolov
9
http://www.fit.vutbr.cz/
?
imikolov/
rnnlm/
et al. (2013a) where they remove the non-linear
hidden layer and there is a single projection layer
for the input word. Precisely, each current word is
used as an input to a log-linear classifier with con-
tinuous projection layer and words within a cer-
tain range before and after the word are predicted.
These vectors are called the skip-gram (SG) vec-
tors. We used the tool
10
for obtaining these word
vectors with default settings.
6.3 Results
We compare the best results obtained by using dif-
ferent types of monolingual word representations
across all language pairs. For brevity we do not
show the results individually for all language pairs
as they follow the same pattern when compared to
the baseline for every vector type. We train word
vectors of length 80 because it was computation-
ally intractable to train the neural embeddings for
higher dimensions. For multilingual vectors, we
obtain k = 60% (cf. ?5.2).
Table 2 shows the correlation ratio and the accu-
racies for the respective evaluation tasks. For the
RNN vectors the performance improves upon in-
clusion of multilingual context for almost all tasks
except for SYN-REL where the loss is statistically
significant (p < 0.01). For MC-30 and SEM-
REL the small drop in performance is not statis-
tically significant. Interestingly, the performance
gain/loss for the SG vectors in most of the cases is
not statistically significant, which means that in-
clusion of multilingual context is not very helpful.
In fact, for SYN-REL the loss is statistically sig-
nificant (p < 0.05) which is similar to the perfor-
mance of RNN case. Overall, the best results are
obtained by the SG vectors in six out of eight eval-
uation tasks whereas SVD vectors give the best
performance in two tasks: RG-65, MC-30. This is
an encouraging result as SVD vectors are the eas-
iest and fastest to obtain as compared to the other
two vector types.
To further understand why multilingual context
is highly effective for SVD vectors and to a large
extent for RNN vectors as well, we plot (Figure 4)
the correlation ratio obtained by varying the length
of word representations by using equation 6 for the
three different vector types on two word similarity
tasks: WS-353 and RG-65.
SVD vectors improve performance upon the in-
crease of the number of dimensions and tend to
10
https://code.google.com/p/word2vec/
467
Vectors Dim Lang WS-353 WS-SIM WS-REL RG-65 MC-30 MTurk SEM-REL SYN-REL
SVD
80 Mono 34.8 45.5 23.4 30.8 21.0 46.6 13.5 24.4
48 Multi 58.1 65.3 52.7 62.7 67.7 62.1 23.4 33.2
RNN
80 Mono 23.6 35.6 17.5 26.2 47.7 32.9 4.7 18.2
48 Multi 35.4 47.3 29.8 36.6 46.5 43.8 4.1 12.2
SG
80 Mono 63.9 69.9 60.9 54.6 62.8 66.9 47.8 47.8
48 Multi 63.1 70.4 57.6 54.9 64.7 58.7 46.5 44.2
Table 2: Spearman?s correlation (left) and accuracy (right) on different tasks. Bold indicates best result
across all vector types. Mono: monolingual and Multi: multilingual.
	 ? WS-353 RG-65 
SVD 
RNN 
SG 
Number of dimensions 
Correlati
on ratio 
(%) 
Figure 4: Performance as a function of vector length on word similarity tasks. The monolingual vectors
always have a fixed length of 80, they are just shown in the plots for comparison.
468
saturate towards the end. For all the three lan-
guage pairs the SVD vectors show uniform pat-
tern of performance which gives us the liberty to
use any language pair at hand. This is not true
for the RNN vectors whose curves are signifi-
cantly different for every language pair. SG vec-
tors show a uniform pattern across different lan-
guage pairs and the performance with multilin-
gual context converges to the monolingual perfor-
mance when the vector length becomes equal to
the monolingual case (k = 80). The fact that both
SG and SVD vectors have similar behavior across
language pairs can be treated as evidence that se-
mantics or information at a conceptual level (since
both of them basically model word cooccurrence
counts) transfers well across languages (Dyvik,
2004) although syntax has been projected across
languages as well (Hwa et al., 2005; Yarowsky and
Ngai, 2001). The pattern of results in the case of
RNN vectors are indicative of the fact that these
vectors encode syntactic information as explained
in ?6 which might not generalize well as compared
to semantic information.
7 Related Work
Our method of learning multilingual word vectors
is most closely associated to Zou et al. (2013) who
learn bilingual word embeddings and show their
utility in machine translation. They optimize the
monolingual and the bilingual objective together
whereas we do it in two separate steps and project
to a common vector space to maximize correla-
tion between the two. Vuli?c and Moens (2013)
learn bilingual vector spaces from non parallel
data induced using a seed lexicon. Our method
can also be seen as an application of multi-view
learning (Chang et al., 2013; Collobert and We-
ston, 2008), where one of the views can be used
to capture cross-lingual information. Klementiev
et al. (2012) use a multitask learning framework
to encourage the word representations learned by
neural language models to agree cross-lingually.
CCA can be used for dimension reduction and
to draw correspondences between two sets of
data.Haghighi et al. (2008) use CCA to draw trans-
lation lexicons between words of two different lan-
guages using only monolingual corpora. CCA
has also been used for constructing monolingual
word representations by correlating word vectors
that capture aspects of word meaning and dif-
ferent types of distributional profile of the word
(Dhillon et al., 2011). Although our primary ex-
perimental emphasis was on LSA based monolin-
gual word representations, which we later gener-
alized to two different neural network based word
embeddings, these monolingual word vectors can
also be obtained using other continuous models of
language (Collobert and Weston, 2008; Mnih and
Hinton, 2008; Morin and Bengio, 2005; Huang et
al., 2012).
Bilingual representations have previously been
explored with manually designed vector space
models (Peirsman and Pad?o, 2010; Sumita, 2000)
and with unsupervised algorithms like LDA and
LSA (Boyd-Graber and Blei, 2012; Zhao and
Xing, 2006). Bilingual evidence has also been ex-
ploited for word clustering which is yet another
form of representation learning, using both spec-
tral methods (Zhao et al., 2005) and structured
prediction approaches (T?ackstr?om et al., 2012;
Faruqui and Dyer, 2013).
8 Conclusion
We have presented a canonical correlation anal-
ysis based method for incorporating multilingual
context into word representations generated using
only monolingual information and shown its ap-
plicability across three different ways of generat-
ing monolingual vectors on a variety of evalua-
tion benchmarks. These word representations ob-
tained after using multilingual evidence perform
significantly better on the evaluation tasks com-
pared to the monolingual vectors. We have also
shown that our method is more suitable for vec-
tors that encode semantic information than those
that encode syntactic information. Our work sug-
gests that multilingual evidence is an important
resource even for purely monolingual, semanti-
cally aware applications. The tool for projecting
word vectors can be found at http://cs.cmu.
edu/
?
mfaruqui/soft.html.
Acknowledgements
We thanks Kevin Gimpel, Noah Smith, and David
Bamman for helpful comments on earlier drafts
of this paper. This research was supported by the
NSF through grant IIS-1352440.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
469
A study on similarity and relatedness using distri-
butional and wordnet-based approaches. In Pro-
ceedings of North American Chapter of the Associ-
ation for Computational Linguistics, NAACL ?09,
pages 19?27, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc. of
ACL.
Jordan L. Boyd-Graber and David M. Blei. 2012. Mul-
tilingual topic models for unaligned text. CoRR,
abs/1205.2657.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602?1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16(1):22?29, March.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, ICML ?08, pages 160?167, New
York, NY, USA. ACM.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science.
Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Un-
gar. 2011. Multi-view learning of word embeddings
via cca. In NIPS, pages 199?207.
Mona Talat Diab. 2003. Word sense disambiguation
within a multilingual framework. Ph.D. thesis, Uni-
versity of Maryland at College Park, College Park,
MD, USA. AAI3115805.
Thomas G. Dietterich. 1998. Approximate statis-
tical tests for comparing supervised classification
learning algorithms. Neural Computation, 10:1895?
1923.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Hendra Setiawan, Ferhan Ture, Vladimir Ei-
delman, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In In Proceedings of ACL System Demonstrations.
Helge Dyvik. 2004. Translations as semantic mir-
rors: from parallel corpus to wordnet. Language
and Computers, 49(1):311?326.
Manaal Faruqui and Chris Dyer. 2013. An informa-
tion theoretic approach to bilingual word clustering.
In Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 777?783, Sofia, Bulgaria, Au-
gust.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In WWW ?01: Proceedings of the
10th international conference on World Wide Web,
pages 406?414, New York, NY, USA. ACM Press.
J.R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in linguistic analysis, pages 1?32.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
computations (3rd ed.). Johns Hopkins University
Press, Baltimore, MD, USA.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. of ACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873?882. Asso-
ciation for Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11:11?311.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed rep-
resentations of words. In Proceedings of COLING.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In In NIPS.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
AISTATS05, pages 246?252.
470
Jerome L. Myers and Arnold D. Well. 1995. Research
Design & Statistical Analysis. Routledge, 1 edition,
June.
Yves Peirsman and Sebastian Pad?o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Com-
putational Linguistics, HLT ?10, pages 921?929,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
WWW ?11, pages 337?346, New York, NY, USA.
ACM.
Philip Resnik and David Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: new evalua-
tion methods for word sense disambiguation. Nat.
Lang. Eng., 5(2):113?133, June.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, ACL ?00, pages 425?431, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In The 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, volume 1, page 11. Association
for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning : Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
pages 141?188.
Peter D. Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. In Proceedings of the 12th
European Conference on Machine Learning, EMCL
?01, pages 491?502, London, UK, UK. Springer-
Verlag.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Comput. Linguist., 32(3):379?416, Septem-
ber.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing Data using t-SNE. Journal of Machine
Learning Research, 9:2579?2605, November.
Ivan Vuli?c and Marie-Francine Moens. 2013. A study
on bootstrapping bilingual vector spaces from non-
parallel data (and nothing else). In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1613?1624, Seattle,
Washington, USA, October. Association for Compu-
tational Linguistics.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust
projection across aligned corpora. In Proceedings
of the second meeting of the North American Chap-
ter of the Association for Computational Linguistics
on Language technologies, NAACL ?01, pages 1?
8, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL06.
Bing Zhao, Eric P. Xing, and Alex Waibel. 2005.
Bilingual word spectral clustering for statistical ma-
chine translation. In Proceedings of the ACL Work-
shop on Building and Using Parallel Texts, ParaText
?05, pages 25?32, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
471
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 616?625,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Augmenting Translation Models with Simulated Acoustic Confusions for
Improved Spoken Language Translation
Yulia Tsvetkov Florian Metze Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213; U.S.A.
{ytsvetko, fmetze, cdyer}@cs.cmu.edu
Abstract
We propose a novel technique for adapting
text-based statistical machine translation
to deal with input from automatic speech
recognition in spoken language translation
tasks. We simulate likely misrecognition
errors using only a source language pro-
nunciation dictionary and language model
(i.e., without an acoustic model), and use
these to augment the phrase table of a stan-
dard MT system. The augmented sys-
tem can thus recover from recognition er-
rors during decoding using synthesized
phrases. Using the outputs of five differ-
ent English ASR systems as input, we find
consistent and significant improvements in
translation quality. Our proposed tech-
nique can also be used in conjunction with
lattices as ASR output, leading to further
improvements.
1 Introduction
Spoken language translation (SLT) systems gen-
erally consist of two components: (i) an auto-
matic speech recognition (ASR) system that tran-
scribes source language utterances and (ii) a ma-
chine translation (MT) system that translates the
transcriptions into the target language. These two
components are usually developed independently
and then combined and integrated (Ney, 1999;
Matusov et al., 2006; Casacuberta et al., 2008;
Zhou, 2013; He and Deng, 2013).
While this architecture is attractive since it re-
lies only on components that are independently
useful, such systems face several challenges. First,
spoken language tends to be quite different from
the highly edited parallel texts that are available to
train translation systems. For example, disfluen-
cies, such as repeated words or phrases, restarts,
and revisions of content, are frequent in spon-
taneous speech,
1
while these are usually absent
in written texts. In addition, ASR outputs typi-
cally lack explicit segmentation into sentences, as
well as reliable casing and punctuation informa-
tion, which are crucial for MT and other text-based
language processing applications (Ostendorf et al.,
2008). Second, ASR systems are imperfect and
make recognition errors. Even high quality sys-
tems make recognition errors, especially in acous-
tically similar words with similar language model
scores, for example morphological substitutions
like confusing bare stem and past tense forms, and
in high-frequency short words (function words)
which often lack both disambiguating context and
are subject to reduced pronunciations (Goldwater
et al., 2010).
One would expect that training an MT system
on ASR outputs (rather than the usual written-
style texts) would improve matters. Unfortunately,
there are few corpora of speech paired with text
translations into a second language that could be
used for this purpose. This has been an incentive
to various MT adaptation approaches and devel-
opment of speech-input MT systems. MT adapta-
tion has been done via input text pre-processing,
by transformation of spoken language (ASR out-
put) into written language (MT input) (Peitz et
al., 2012; Xu et al., 2012); via decoding ASR n-
best lists (Quan et al., 2005), or confusion net-
works (Bertoldi et al., 2007; Casacuberta et al.,
2008), or lattices (Dyer et al., 2008; Onishi et al.,
2010); via additional translation features captur-
ing acoustic information (Zhang et al., 2004); and
with methods that follow a paradigm of unified de-
coding (Zhou et al., 2007; Zhou, 2013). In line
with the previous research, we too adapt a standard
MT system to a speech-input MT, but by altering
the translation model itself so it is better able to
1
Disfluencies constitute about 6% of word tokens in spon-
taneous speech, not including silent pauses (Tree, 1995; Kasl
and Mahl, 1965)
616
deal with ASR output (Callison-Burch et al., 2006;
Tsvetkov et al., 2013a).
We address speech translation in a resource-
deficient scenario, specifically, adapting MT sys-
tems to SLT when ASR is unavailable. We aug-
ment a discriminative set that translation models
rescore with synthetic translation options. These
automatically generated translation rules (hence-
forth synthetic phrases) are noisy variants of ob-
served translation rules with simulated plausible
speech recognition errors (?2). To simulate ASR
errors we generate acoustically and distribution-
ally similar phrases to a source (English) phrase
with a phonologically-motivated algorithm (?4).
Likely phonetic substitutions are learned with an
unsupervised algorithm that produces clusters of
similar phones (?3). We show that MT systems
augmented with synthetic phrases increase the
coverage of input sequences that can be translated,
and yield significant improvement in the quality of
translated speech (?6).
This work makes several contributions. Primary
is our framework to adapt MT to SLT by popu-
lating translation models with synthetic phrases.
2
Second, we propose a novel method to generate
acoustic confusions that are likely to be encoun-
tered in ASR transcription hypotheses. Third, we
devise simple and effective phone clustering al-
gorithm. All aforementioned algorithms work in
a low-resource scenario, without recourse to au-
dio data, speech transcripts, or ASR outputs: our
method to predict likely recognition errors uses
phonological rather than acoustic information and
does not depend on a specific ASR system. Since
our source language is English, we operate on a
phone level and employ a pronunciation dictionary
and a language model, but the algorithm can in
principle be applied without pronunciation dictio-
nary for languages with a phonemic orthography.
2 Methodology
We adopt a standard ASR-MT cascading approach
and then augment translation models with syn-
thetic phrases. Our proposed system architecture
is depicted in Figure 1.
Synthetic phrases are generated from entries in
the original translation model?phrase translation
2
We augment phrase tables only with synthetic phrases
that capture simulated ASR errors, the methodology that we
advocate, however, is applicable to many problems in transla-
tion (Tsvetkov et al., 2013a; Ammar et al., 2013; Chahuneau
et al., 2013).
ASR
ASR LM
(source lang.)
Acoustic 
Model
cat ????
MT
MT LM
(target lang.)
TM+TM'
Translation Model 
augmented with 
simulated ASR errors
Figure 1: SLT architecture: ASR and MT are
trained independently and then cascaded. We im-
prove SLT by populating MT translation model
with synthetic phrases. Each synthetic phrase is
a variant of an original phrase pair with simulated
ASR errors on the source side.
pairs acquired from parallel data. From a source
side of an original phrase pair we generate list of
its plausible misrecognition variants (pseudo-ASR
outputs with recognition errors) and add them as
a source side of a synthetic phrase. For k-best
simulated ASR outputs we construct k synthetic
phrases: a simulated ASR output in the source
side is coupled with its translation?an original tar-
get phrase (identical for all k phrases). Synthetic
phrases are annotated with five standard phrasal
translation features (forward and reverse phrase
and lexical translation probabilities and phrase
penalty); these were found in the original phrase
and remain unchanged. In addition, we add three
new features to all phrase pairs, both synthetic and
original. First, we add a boolean feature indi-
cating the origin of a phrase: synthetic or origi-
nal. Two other features correspond to an ASR lan-
guage model score of the source side. One is LM
score of the synthetic phrase, another is a score
of a phrase from which the source side was gener-
ated. We then append synthetic phrases to a phrase
table: k synthetic phrases for each original phrase
pair, with eight features attached to each phrase.
We show synthetic phrases example in Figure 2.
3 Acoustically confusable phones
The phonetic context of a given phone affects its
acoustic realization, and a variability in a produc-
tion of the same phone is possible depending on
coarticulation with its neighboring phones.
3
In ad-
dition, there are phonotactic constraints that can
restrict allowed sequences of phones. English has
strong constraints on sequences of consonants; the
sequence [zdr], for example, cannot be a legal En-
3
These are the reasons why in context-dependent acous-
tic modeling different HMM models are trained for different
contexts.
617
Source
phrase
Target
phrase
Original phrase
translation features
Synthetic
indicator
Synthetic
LM score
Original
LM score
tells the story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
0 3.9?10
?3
3.9?10
?3
tell their story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 5.9?10
?3
3.9?10
?3
tells a story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 2.2?10
?3
3.9?10
?3
tell the story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 1.7?10
?3
3.9?10
?3
tell a story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 1.3?10
?3
3.9?10
?3
tell that story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 1.0?10
?3
3.9?10
?3
tell their stories raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 0.9?10
?3
3.9?10
?3
tells the stories raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 0.8?10
?3
3.9?10
?3
tells her story raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 0.7?10
?3
3.9?10
?3
chelsea star raconte l?histoire f
1
, f
2
, f
3
, f
4
, f
5
1 0.5?10
?3
3.9?10
?3
Figure 2: Example of acoustically confusable synthetic phrases. Phrases were synthesized from the
original phrase pair in Row 1 by generating acoustically similar phrases for the English phrase tells the
story. All phrases have the same (target) French translation me raconte l?histoire and the same five basic
phrase-based translation rule features. To these, three additional features are added: a synthetic phrase
indicator, the source language LM score of the source phrase, and the source language LM score of a
source phrase in the original phrase pair.
T
left
T
right
TW
left
WIH
right
. . .
T P (T |WIH) . . .
W P (W |T ) . . .
IH P (IH|T ) P (IH|TW ) . . .
ER P (ER|T ) . . .
. . . . . . . . . . . . . . . . . .
Figure 3: A fragment of the co-occurrence matrix
for phone sequence [T W IH T ER]. Rows corre-
spond to phones; columns correspond to left/right
context phones of lengths one and two.
glish syllable onset (Jurafsky and Martin, 2000).
Motivated by the constraining effect of context
on phonetic distribution, we cluster phones using a
distance-based measure. To do so, we build a vec-
tor space model representation of each phone by
creating a co-occurrence matrix from a corpus of
phonetic forms where each row represents a phone
and columns indicate the contextual phones. We
take into account left/right context windows of
lengths one and two. A cell r
p,c
in the vector space
dictionary matrix represents phone p and context c
using the empirical relative frequency f(p | c), as
estimated from a pronunciation dictionary. Fig-
ure 3 shows a fragment of the co-occurrence ma-
trix constructed from a dictionary containing just
the pronunciation of Twitter ? [T W IH T ER].
Under this representation, the similarity of
phones can be easily quantified by measuring their
distance in the vector space, the cosine of the angle
between them:
Sim(p
1
, p
2
) =
p
1
?p
2
||p
1
||?||p
2
||
Armed with this similarity function, we apply the
K-means algorithm
4
to partition the phones into
disjoint sets.
4 Plausible misrecognition variants
For an input English sequence we generate top-k
pseudo-ASR outputs, that are added as a source
side of a synthetic phrase. Every ASR output that
we simulate is a plausible misrecognition that has
two distinguishing characteristics: it is acousti-
cally and linguistically confusable with the input
sequence. Former corresponds to phonetic simi-
larity and latter to distributional similarity of these
two phrases in corpus.
Given a reference string?a word or sequence
of words w in the source language, we generate
k-best hypotheses v. This can be modeled as a
weighted finite state transducer:
{v} = G ?D
?1
? T ?D ? {w} (1)
where
? D maps from words to pronunciations
? T is a phone confusion transducer
? D
?1
maps from pronunciations to words
? G is an ASR language model
D maps words to their phonetic representation
5
,
or multiple representations for words with several
4
Value of K=12 was determined empirically.
5
Using the CMU pronounciation dictionary
http://www.speech.cs.cmu.edu/cgi-bin/cmudict
618
pronunciation variants. To create a phone con-
fusion transducer T maps source to target phone
sequences by performing a number of edit opera-
tions. Allowed edits are:
? Deletion of a consonant (mapping to ).
? Doubling of a vowel.
? Insertion of one or two phones in the end of a
sequence from the list of possible suffixes: S
(-s), IX NG (-ing), D (-ed).
? Substitution of a phone by an acousti-
cally similar phone. The clusters of the
similar phones are {Z,S}, {XL,L,R},
{AA,AO,EY,UH}, {AXR,AX}, {XN,XM},
{P,B,F}, {DH,CH,ZH,T,SH}, {OY,AE},
{IY,AY,OW}, {EH,AH,IH,AW,ER,UW}.
The phone clustering algorithm that pro-
duced these is detailed in the previous
section.
After a series of edit operations, D
?1
trans-
ducer maps new phonetic sequences from pronun-
ciations to n-grams of words. The k-best variants
resulting from the weighted composition are the
k-best plausible misrecognitions.
One important property of this method is that it
maps words in decoding vocabulary (41,487 types
are possible inputs to transducer D) into CMU
dictionary which is substantially larger (141,304
types are possible outputs of transducer D
?1
).
This allows to generate out-of-vocabulary (OOV)
words and phrases, which are not only recogni-
tion errors, but also plausible variants of different
source phrases that can be translated to one tar-
get phrase, e.g., verb past tense forms or function
words.
Consider a bigram tells the from our synthetic
phrase example in Figure 2. We first obtain
its phonetic representation [T EH L Z] [DH IY],
and then a sequence of possible edit operations
is Substitute(T, CH), Substitute(Z, S), Delete(DH)
and translation of phonetic sequence [CH EH L S
IY] back to words brings us to chelsea. See Fig-
ure 4 for visualization.
5 Experimental setups
To establish the effectiveness and ro-
bustness of our approach, we conducted
two sets of experiments?expASR and
expMultilingual?with transcribed and
tells the  T EH L Z DH IY
chelsea CH EH L S    IY
Figure 4: Pseudo-ASR output generation exam-
ple for a bigram tells the. Phonetic edits are
Substitute(T, CH), Substitute(Z, S), Delete(DH).
translated TED talks (Cettolo et al., 2012b).
6
En-
glish is the source language in all the experiments.
In expASR we used tst2011?the official test
set of the SLT track of the IWSLT 2011 evalu-
ation campaign on the English-French language
pair (Federico et al., 2011).
7
This test set com-
prises reference transcriptions of 8 talks (approx-
imately 1.1h of speech, segmented to 818 utter-
ances), 1-best hypotheses from five different ASR
systems, a ROVER combination of four systems
(Fiscus, 1997), and three sets of lattices produced
by the participants of the IWSLT 2011 ASR track.
In this set of experiments we compare baseline
systems performance to a performance of systems
augmented with synthetic phrases on (1) reference
transcriptions, (2) 1-best hypotheses from all re-
leased ASR systems, and (3) a set of ASR lattices
produced by FBK (Ruiz et al., 2011).
8
Experi-
ments with individual systems are aimed to val-
idate that MT augmented with synthetic phrases
can better translate ASR outputs with recogni-
tion errors and sequences that were not observed
in the MT training data. Consistency in perfor-
mance across different ASRs is expected if our ap-
proach to generate plausible misrecognition vari-
ants is universal, rather than biased to a specific
system. Comparison of 1-best system with syn-
thetic phrases to lattice decoding setup without
synthetic phrases should demonstrate whether n-
best plausible misrecognition variants that we gen-
erate assemble multiple paths through a lattice.
The purpose of expMultilingual is to
show that translation improvement is consistent
across different target languages. This multilin-
gual experiment is interesting because typologi-
cally different languages pose different challenges
to translation (degree and locality of reordering,
morphological richness, etc.). By showing that
we improve results across languages (even with
6
http://www.ted.com/
7
http://iwslt2011.org/doku.php?id=06_evaluation#slt_track_
english_to_french
8
Pruning threshold for lattices is 0.08.
619
the same underlying ASR system), we show that
our technique is robust to the different demands
that languages place on the translation model. We
could not find any publicly available multilingual
data sets of the translated speech,
9
therefore we
constructed a new test set.
We use our in-house speech recognizer and
evaluate on locally crawled and pre-processed
TED audio and text data. We build SLT systems
for five target languages: French, German, Rus-
sian, Hebrew, and Hindi. Consequently, our test
systems are diverse typologically and trained on
corpora of different sizes. We sample a test set of
seven talks, representing approximately two hours
of English speech, for which we have translations
to all five languages;
10
talks are listed in Table 1.
Due to segmentation differences in the released
TED (text) corpora and then several automatic
preprocessing stages, numbers of sentences for
the same talks are not identical across languages.
Therefore, we select English-French system as an
oracle (this is the largest dataset), and first align it
with the ASR output. Then, we filter out test sets
for non-French MT systems, to retain only sen-
tence pairs that are included in the English-French
test set. Thus, our test sets for non-French MT
systems are smaller, and source-side sentences in
the English-French MT is a superset of source-side
sentences in all five languages. Training, tuning,
and test corpora sizes are listed in Table 2. Same
training and development sets were used in both
expASR and expMultilingual experiments.
Training Dev Test
EN?FR 140,816 2,521 843
EN?DE 130,010 2,373 501
EN?RU 117,638 2,380 735
EN?HE 135,366 2,501 540
EN?HI 126,117 2,000 300
Table 2: Number of sentences in training, dev and
expMultilingual test corpora.
5.1 ASR
In the expMultilingual set of experiments,
we employ the JANUS Recognition Toolkit that
features the IBIS single pass decoder (Soltau et
9
After we conducted our experiments, a new multilingual
parallel corpus of translated speech was released for SLT
track of IWSLT 2013 Evaluation Campaign, however, this
data set does not include Russian, Hebrew and Hindi, which
are a subject of this research.
10
Since TED translation is a voluntary effort, not all talks
are available in all languages.
al., 2001). The acoustic model is maximum
likelihood system, no speaker adaptation or dis-
criminative training applied. The acoustic model
training data is 186h of Broadcast News-style
data. 5-gram language model with modified
Kneser-Ney smoothing is trained with the SRILM
toolkit (Stolcke, 2002) on the EPPS, TED, News-
Commentary, and the Gigaword corpora. The
Broadcast News test set contains 4h of audio; we
obtain 25.6% word error rate (WER) on this test
set.
We segment the TED test audio by the times-
tamps of transcripts appearance on the screen.
Then, we manually detect and discard noisy hy-
potheses around segmentation boundaries, and
manually align the remaining hypotheses with
the references which are the source side of the
English-French MT test set. The resulting test
set of 843 hypotheses, sentence aligned with tran-
scripts, yields 30.7% WER. Higher error rates (rel-
atively to the Broadcast News baseline) can be
explained by the idiosyncratic nature of the TED
genre, and the fact that our ASR system was not
trained on the TED data.
For the expASR set of experiments the ASR
outputs and lattices in standard lattice format
(SLF) were produces by the participants of IWSLT
2011 evaluation campaign.
5.2 MT
We train and test MT using the TED corpora in
all five languages. For French, German and Rus-
sian we use sentence-aligned training and develop-
ment sets (without our test talks) released for the
IWSLT 2012 evaluation campaign (Cettolo et al.,
2012a); we split Hebrew and Hindi to training and
development respectively.
11
We split Hebrew and
Hindi to sentences with simple heuristics, and then
sentence-align with the Microsoft Bilingual Sen-
tence Aligner (Moore, 2002). Punctuation marks
were removed, corpora were lowercased, and tok-
enized using the cdec scripts (Dyer et al., 2010).
In all MT experiments, both for sentence and
lattice translation, we employ the Moses toolkit
(Koehn et al., 2007), implementing the phrase-
based statistical MT model (Koehn et al., 2003)
and optimize parameters with MERT (Och, 2003).
Target language 3-gram Kneser-Ney smoothed
11
Since TED Hindi corpus is very small (only about 6K
sentences) we augment it with additional parallel data (Bojar
et al., 2010); however, this improved Hindi system quality
only marginally, probably owing to domain mismatch.
620
TED id TED talk
1 Al Gore, 15 Ways to Avert a Climate Crisis, 2006
39 Aubrey de Grey: A roadmap to end aging, 2005
142 Alan Russell: The potential of regenerative medicine, 2006
228 Alan Kay shares a powerful idea about ideas, 2007
248 Alisa Miller: The news about the news, 2008
451 Bill Gates: Mosquitos, malaria and education, 2009
535 Al Gore warns on latest climate trends, 2009
Table 1: Test set of TED talks.
language models are trained on the training part
of each corpus. Results are reported using case-
insensitive BLEU with a single reference and no
punctuation (Papineni et al., 2002). To verify
that our improvements are consistent and are not
just an effect of optimizer instability (Clark et al.,
2011), we train three systems for each MT setup.
Statistical significance is measured with the Mul-
tEval toolkit.
12
Reported BLEU scores are aver-
aged over three systems.
In MT adaptation experiments we augment
baseline phrase tables with synthetic phrases. For
each entry in the original phrase table we add (at
most) five
13
best acoustic confusions, detailed in
Section 4. Table 3 contains sizes of phrase tables,
original and augmented with synthetic phrases.
Original Synthetic
EN?FR 4,118,702 24,140,004
EN?DE 2,531,556 14,807,308
EN?RU 1,835,553 10,743,818
EN?HE 2,169,397 12,692,641
EN?HI 478,281 2,674,025
Table 3: Sizes of phrase tables from the baseline
systems, and phrase tables with synthetic phrases.
6 Experiments
6.1 expASR
We first measure the phrasal coverage of recog-
nition errors that our technique is able to predict.
We compute a number of 1- and 2-gram phrases
in ASR hypotheses from the tst2011 that are
not in the references: these are ASR errors. Then,
we compare their OOV rate in the English-French
phrase tables, original vs. synthetic. The pur-
pose of synthetic phrases is to capture misrecog-
nized sequences, ergo, reduction in OOV rate of
12
https://github.com/jhclark/multeval
13
This threshold is of course rather arbitrary. In future ex-
periments we are planning to conduct an in-depth investiga-
tion of the threshold value, based on ASR LM score and pho-
netic distance from the original phrase.
ASR errors in synthetic phrase tables corresponds
to the portion of errors that our method was able
to predict. Table 4 shows that the OOV rate of n-
grams in phrase tables augmented with synthetic
phrases drops dramatically, up to 54%. Consis-
tent reduction of recognized errors across outputs
from five different ASR systems confirms that our
error-prediction approach is ASR-independent.
tst2011 #1-grams #2-grams
system0 29 (50.9%) 230 (20.3%)
system1 27 (41.5%) 234 (21.3%)
system2 36 (36.0%) 230 (20.1%)
system3 34 (44.1%) 275 (20.1%)
system4 46 (52.9%) 182 (16.8%)
ROVER 30 (54.5%) 183 (18.7%)
Table 4: Phrasal coverage of recognition errors
that our technique is able to predict. These are
raw counts of 1-gram and 2-gram types that are
OOVs in the baseline system and are recovered
by our method when we augment the system with
plausible misrecognitions. Percentages in paren-
theses show OOV rate reduction due to recovered
n-grams.
Next, we explore the effect of synthetic phrases
on translation performance, across different (1-
best) ASR outputs. For references, ASR hypothe-
ses, and ROVERed hypotheses we compare trans-
lations produced by MT systems trained with and
without synthetic phrases. We detail our findings
in Table 5.
Improvements in translation are significant for
all systems with synthetic phrases. This experi-
ment corroborates the underlying assumption that
simulated ASR errors are paired with correct tar-
get phrases. Moreover, this experiment supports
the claim that incorporating noisier translations in
the translation model successfully adapts MT to
SLT scenario and has indeed a positive effect on
speech translation. Interestingly, improvement of
reference translations is also observed. We spec-
ulate that this stems from better lexical selection
due to a smoothing effect that our technique may
621
WER
BLEU
Baseline
BLEU
Synthetic
p
references - 30.8 31.2 0.05
system0 22.0 24.3 25.0 <0.01
system1 23.3 23.8 24.3 <0.01
system2 21.1 23.9 24.4 0.02
system3 32.4 20.8 21.3 <0.01
system4 19.5 24.5 25.0 0.01
ROVER 17.4 25.0 25.6 0.01
Table 5: Comparison of the baseline translation
systems with the systems augmented with syn-
thetic phrases. We measure EN?FR MT perfor-
mance on the tst2011 test set: reference tran-
scripts and ASR outputs on from five systems
and their ROVER combination. Improvements in
translation of all ASR outputs are statistically sig-
nificant. This confirms the claim that incorporat-
ing simulated ASR errors via synthetic phrases ef-
fectively adapts MT to SLT scenario.
have.
Finally, we contrast the proposed approach of
translation models adaptation to a conventional
method of lattice translation. We decode FBK lat-
tices produced for IWSLT 2011 Evaluation Cam-
paign, and compare results to FBK 1-best transla-
tion results, which correspond to system1 in Table
5. Table 6 summarizes our main finding: 1-best
system with synthetic phrases significantly outper-
forms lattice decoding setup with baseline trans-
lation table.
14
The additional small improvement
in lattice decoding with synthetic phrases suggests
that lattice decoding and phrase table adaptation
are two complementary strategies and their com-
bination is beneficial.
6.2 expMultilingual
In the multilingual experiment we train ten MT se-
tups: five baseline setups and five systems with
synthetic phrases, three systems per setup. For
each system we compare translations of the refer-
ence transcripts and ASR hypotheses on the multi-
lingual test set described in Section 6. We evaluate
translations produced by MT systems trained with
and without synthetic phrases. Table 7 summa-
rizes experimental results, along with the test set
WER for each language.
14
Automatic evaluation results (in terms of BLEU) pub-
lished during the IWSLT 2011 Evaluation Campaign (Fed-
erico et al., 2011) (p. 21) are 26.1 for FBK systems. Unsur-
prisingly, performance of our systems is lower, as we focus
only on translation table and do not optimize factors, such as
LMs and others.
BLEU
Baseline
BLEU
Synthetic
FBK 1-best 23.8 24.3
FBK lattices 24.0 24.4
Table 6: Comparison of the baseline EN?FR trans-
lation systems with the systems augmented with
synthetic phrases, in 1-best and lattice decoding
setups. 1-best synthetic system significantly out-
performs baseline lattice decoding setup. Addi-
tional improvement in lattice decoding with syn-
thetic phrases suggests that lattice decoding and
phrase table adaptation are two complementary
strategies.
WER Baseline Synthetic
Ref ASR Ref ASR
EN?FR 30.7
23.3 17.8 23.9 18.1
EN?DE 33.6
14.0 11.1 14.2 11.4
EN?RU 30.7
12.3 10.7 12.2 10.6
EN?HE 29.7
9.2 7.0 9.5 7.2
EN?HI 32.1
5.5 4.5 5.6 4.8
Table 7: Comparison of the baseline translation
systems with the systems augmented with syn-
thetic phrases. We measure MT performance on
the reference transcripts and ASR outputs. Con-
sistent improvements are observed in four out of
five languages.
Modest but consistent improvements are ob-
served in four out of five setups with synthetic
phrases. Only French setup yielded statistically
significant improvement (p < .01). However,
if we concatenate the outputs of all languages,
the improvement in translation of references with
BLEU score averaged over all systems becomes
statistically significant (p = .03), improving from
16.8 for the baseline system to 17.3 for the adapted
MT outputs. While more careful evaluation is re-
quired in order to estimate the effect of acous-
tic confusions, the accumulated result show that
synthetic phrases facilitate MT adaptation to SLT
across languages.
7 Analysis
We conducted careful manual analysis of actual
usages of synthetic phrases in translation. The pur-
pose of this qualitative analysis is to verify that
predicted ASR errors are paired with phrases that
contribute to better translation to a target language.
Table 8 shows some examples. In the first sentence
from the tst2011 test set (output from system 4)
the word area was erroneously recognized as airy,
622
English ref so what they do is they move into an area
ASR output so what they do is they move into an airy
Baseline MT donc ce qu?ils font c?est qu?ils se d?placer dans un airy
Synthetic MT donc ce qu?ils font c?est qu?ils se d?placer dans une zone
French ref donc ce qu?ils font c?est qu?ils emm?nagent dans une zone
English ref so i started thinking and listing what all it was that i thought would make a perfect biennial
ASR output so on i started a thinking and listing was all it was that i thought would make a pretty by neil
Baseline MT donc j?ai commenc? ? une pens?e et listing ?tait tout c??tait que je pensais ferait un assez par neil
Synthetic MT donc j?ai commenc? ? penser et une liste ?tait tout c??tait que je pensais ferait un assez par neil
French ref alors j?ai commenc? ? penser et ? lister tout ce qui selon moi ferait une biennale parfaite
Table 8: Examples of translations improved with synthetic phrases.
which is an OOV word for the baseline system.
Our confusion generation algorithm also produced
the word airy as a plausible misrecognition variant
for the word area and attached it to a correct tar-
get phrase zone, and this synthetic phrase was se-
lected during decoding, yielding to a correct trans-
lation for the ASR error. Second example shows a
similar behavior for an indefinite article a. Third
example is taken from the English-Russian system
in the multilingual test set. Gauge was produced
as a plausible misrecognition variant to age, and
therefore correctly translated (albeit incorrectly in-
flected) as ????????(age+sg+m+acc). Synthetic
phrases were also used in translations contain-
ing misrecognized function words, segmentation-
related examples, and longer n-grams.
8 Related work
Predicting ASR errors to improve speech recog-
nition quality has been explored in several previ-
ous studies. Jyothi and Fosler-Lussier (2009) de-
velop weighted finite-state transducer framework
for error prediction. They build a confusion ma-
trix FST between phones to model acoustic errors
made by the recognizer. Costs in the confusion
matrix combine acoustic variations in the HMM
representations of the phones (information from
the acoustic model) and word-based phone confu-
sions (information from the pronunciation model).
In their follow-up work, Jyothi and Fosler-Lussier
(2010) employ this error-predicting framework to
train the parameters of a global linear discrimina-
tive language model that improves ASR.
Sagae et al. (2012) examined three protocols
for ?hallucinating? ASR n-best lists. First ap-
proach generates confusions on the phone level,
with a phone-based finite-state transducer that em-
ploys real n-best lists produced by the ASR sys-
tem. Second is generating confusions at the word
level with a MT-based approach. Third is a phrasal
cohorts approach, in which acoustically confus-
able phrases are extracted from ASR n-best lists,
based on pivots?identical left and right contexts of
a phrase. All three methods were evaluated on the
task of ASR improvement through decoding with
discriminative language models. Discriminative
language models trained on simulated n-best lists
produced with phrasal cohorts method yielded the
largest WER reduction on the telephone speech
recognition task.
Our approach to generating plausible ASR mis-
recognitions is similar to previously explored FST-
based methods. The fundamental difference, how-
ever, is in speech-free phonetic confusion trans-
ducer that does not employ any data extracted
from acoustic models or ASR outputs. Simulated
ASR errors are typically used to improve ASR ap-
plications. To the best of our knowledge no prior
work has been done on integrating ASR errors di-
rectly in the translation models.
9 Conclusion
The idea behind the novel ASR error-prediction
algorithm that we devise is to identify phonolog-
ical neighbors with similar distributional proper-
ties, i.e. similar sounding words for which lan-
guage model probabilities are insufficient for their
disambiguation. These sequences have been iden-
tified as significant contributors to ASR errors
(Goldwater et al., 2010). Additional and even
more important factors that cause recognition er-
rors are disfluencies in speech (Tsvetkov et al.,
2013b). In the task of adapting MT to SLT these
and other irregularities can effectively be incor-
porated in a useful general framework: synthetic
phrases that augment phrase tables. Our exper-
iments show that simulated acoustic confusions
capture real ASR errors and that proposed frame-
work effectively exploits them to improve transla-
tion.
623
Acknowledgments
We are grateful to Jo?o Miranda and Alan Black for providing
us the TED audio with transcriptions, and to Zaid Sheikh for
his help with ASR decoding. This work was supported in part
by the U. S. Army Research Laboratory and the U. S. Army
Research Office under contract/grant number W911NF-10-1-
0533.
References
Waleed Ammar, Victor Chahuneau, Michael
Denkowski, Greg Hanneman, Wang Ling, Austin
Matthews, Kenton Murray, Nicola Segall, Yulia
Tsvetkov, Alon Lavie, and Chris Dyer. 2013.
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references.
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In Proc. ICASSP, pages 1297?1300. IEEE.
Ondrej Bojar, Pavel Stranak, and Daniel Zeman. 2010.
Data issues in English-to-Hindi machine translation.
In Proceedings of LREC.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine
translation using paraphrases. In Proceedings of
HLT/NAACL, pages 17?24. Association for Compu-
tational Linguistics.
Francisco Casacuberta, Marcello Federico, Hermann
Ney, and Enrique Vidal. 2008. Recent efforts
in spoken language translation. Signal Processing
Magazine, IEEE, 25(3):80?88.
Mauro Cettolo, Marcello Federico, Luisa Bentivogli,
Michael Paul, and Sebastian St?ker. 2012a.
Overview of the IWSLT 2012 evaluation campaign.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012b. WIT
3
: Web inventory of transcribed
and translated talks. In Proceedings of EAMT, pages
261?268, Trento, Italy.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of ACL, pages
176?181. Association for Computational Linguis-
tics.
Chris Dyer, Smaranda Muresan, and Philip Resnik.
2008. Generalizing word lattice translation. In Pro-
ceedings of ACL-08: HLT, pages 1012?1020. Asso-
ciation for Computational Linguistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL.
Marcello Federico, Luisa Bentivogli, Michael Paul,
and Sebastian St?ker. 2011. Overview of the
IWSLT 2011 evaluation campaign. In Proc. IWSLT,
pages 8?9.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recognizer output
voting error reduction (ROVER). In Proc. ASRU,
pages 347?352. IEEE.
Sharon Goldwater, Dan Jurafsky, and Christopher D
Manning. 2010. Which words are hard to rec-
ognize? prosodic, lexical, and disfluency factors
that increase speech recognition error rates. Speech
Communication, 52(3):181?200.
Xiaodong He and Li Deng. 2013. Speech-centric in-
formation processing: An optimization-oriented ap-
proach. IEEE, 101(5):1116?1135.
Dan Jurafsky and James H Martin. 2000. Speech &
Language Processing. Pearson Education India.
Preethi Jyothi and Eric Fosler-Lussier. 2009. A com-
parison of audio-free speech recognition error pre-
diction methods. In Proc. INTERSPEECH, pages
1211?1214.
Preethi Jyothi and Eric Fosler-Lussier. 2010. Discrimi-
native language modeling using simulated asr errors.
In Proc. INTERSPEECH, pages 1049?1052.
Stanislav V Kasl and George F Mahl. 1965. The re-
lationship of disturbances and hesitations in spon-
taneous speech to anxiety. In Journal of Personal-
ity and Social Psychology, volume 1(5), pages 425?
433.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL, pages 48?54. Association
for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL, pages 177?180. Asso-
ciation for Computational Linguistics.
Evgeny Matusov, Stephan Kanthak, and Hermann Ney.
2006. Integrating speech recognition and machine
translation: Where do we stand? In Proc. ICASSP,
pages V?1217?V?1220. IEEE.
624
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings
of AMTA, pages 135?144, London, UK. Springer-
Verlag.
Hermann Ney. 1999. Speech translation: Coupling of
recognition and translation. In Proc. ICASSP, vol-
ume 1, pages 517?520. IEEE.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Takashi Onishi, Masao Utiyama, and Eiichiro Sumita.
2010. Paraphrase lattice for statistical machine
translation. In Proceedings of ACL.
Mari Ostendorf, Beno?t Favre, Ralph Grishman, Dilek
Hakkani-Tur, Mary Harper, Dustin Hillard, Julia
Hirschberg, Heng Ji, Jeremy G Kahn, Yang Liu,
Sameer Maskey, Evgeny Matusov, Hermann Ney,
Andrew Rosenberg, Elizabeth Shriberg, Wen Wang,
and Chuck Wooters. 2008. Speech segmentation
and spoken document processing. Signal Process-
ing Magazine, IEEE, 25(3):59?69.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318. Association for Computa-
tional Linguistics.
Stephan Peitz, Simon Wiesler, Markus Nu?baum-
Thom, and Hermann Ney. 2012. Spoken language
translation using automatically transcribed text in
training. In Proc. IWSLT.
Vu H Quan, Marcello Federico, and Mauro Cettolo.
2005. Integrated n-best re-ranking for spoken lan-
guage translation. In Proc. INTERSPEECH, pages
3181?3184. IEEE.
Nick Ruiz, Arianna Bisazza, Fabio Brugnara, Daniele
Falavigna, Diego Giuliani, Suhel Jaber, Roberto
Gretter, and Marcello Federico. 2011. FBK@
IWSLT 2011. In Proc. IWSLT.
Kenji Sagae, M. Lehr, E. Prud?hommeaux, P. Xu,
N. Glenn, D. Karakos, S. Khudanpur, B. Roark,
M. Sara?lar, I. Shafran, D. Bikel, C. Callison-Burch,
Y. Cao, K. Hall, E. Hasler, P. Koehn, A. Lopez,
M. Post, and D. Riley. 2012. Hallucinated n-best
lists for discriminative language modeling. In Proc.
ICASSP. IEEE.
H. Soltau, F. Metze, C. F?gen, and A. Waibel. 2001.
A one-pass decoder based on polymorphic linguistic
context assignment. In Proc. ASRU.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Proc. ICSLP, pages 901?
904.
Jean E Fox Tree. 1995. The effects of false starts and
repetitions on the processing of subsequent words in
spontaneous speech. Journal of memory and lan-
guage, 34(6):709?738.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Bhatia. 2013a. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of WMT. Association for
Computational Linguistics.
Yulia Tsvetkov, Zaid Sheikh, and Florian Metze.
2013b. Identification and modeling of word frag-
ments in spontaneous speech. In Proc. ICASSP.
IEEE.
Ping Xu, Pascale Fung, and Ricky Chan. 2012.
Phrase-level transduction model with reordering for
spoken to written language transformation. In Proc.
ICASSP, pages 4965?4968. IEEE.
Ruiqiang Zhang, Genichiro Kikui, Hirofumi Ya-
mamoto, Taro Watanabe, Frank Soong, and Wai Kit
Lo. 2004. A unified approach in speech-to-speech
translation: integrating features of speech recog-
nition and machine translation. In Proceedings
of COLING, page 1168. Association for Computa-
tional Linguistics.
Bowen Zhou, Laurent Besacier, and Yuqing Gao.
2007. On efficient coupling of ASR and SMT for
speech translation. In Proc. ICASSP, volume 4,
pages IV?101. IEEE.
Bowen Zhou. 2013. Statistical machine translation for
speech: A perspective on structures, learning, and
decoding. IEEE, 101(5):1180?1202.
625
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 263?266,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Two monolingual parses are better than one (synchronous parse)
?
Chris Dyer
UMIACS Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland, College Park, MD 20742, USA
redpony AT umd.edu
Abstract
We describe a synchronous parsing algorithm
that is based on two successive monolingual
parses of an input sentence pair. Although
the worst-case complexity of this algorithm
is and must be O(n
6
) for binary SCFGs,
its average-case run-time is far better. We
demonstrate that for a number of common
synchronous parsing problems, the two-parse
algorithm substantially outperforms alterna-
tive synchronous parsing strategies, making it
efficient enough to be utilized without resort-
ing to a pruned search.
1 Introduction
Synchronous context free grammars (SCFGs) gener-
alize monolingual context-free grammars to gener-
ate strings concurrently in pairs of languages (Lewis
and Stearns, 1968) in much the same way that fi-
nite state transducers (FSTs) generalize finite state
automata (FSAs).
1
Synchronous parsing is the prob-
lem of finding the best derivation, or forest of deriva-
tions, of a source and target sentence pair ?f, e? under
an SCFG, G.
2
Solving this problem is necessary for
several applications, for example, optimizing how
well an SCFG translation model fits parallel train-
ing data. Wu (1997) describes a bottom-up O(n
6
)
synchronous parsing algorithm for ITGs, a binary
SCFG with a restricted form. For general grammars,
the situation is even worse: the problem has been
shown to be NP-hard (Satta and Peserico, 2005).
Even if we restrict ourselves to binary ITGs, the
?
This work was supported in part by the GALE program of
DARPA, Contract No. HR0011-06-2-001. The author wishes
to thank Philip Rensik, Adam Lopez, Phil Blunsom, and Jason
Eisner for helpful discussions.
1
SCFGs have enjoyed a resurgence in popularity as the for-
mal basis for a number of statistical translation systems, e.g.
Chiang (2007). However, translation requires only the manipu-
lation of SCFGs using monolingual parsing algorithms.
2
It is assumed that n = |f| ? |e|.
O(n
6
) run-time makes large-scale learning applica-
tions infeasible. The usual solution is to use a heuris-
tic search that avoids exploring edges that are likely
(but not guaranteed) to be low probability (Zhang et
al., 2008; Haghighi et al, 2009). In this paper, we
derive an alternative synchronous parsing algorithm
starting from a conception of parsing with SCFGs as
a composition of binary relations. This enables us
to factor the synchronous parsing problem into two
successive monolingual parses. Our algorithm runs
more efficiently than O(n
6
) with many grammars
(including those that required using heuristic search
with other parsers), making it possible to take ad-
vantage of synchronous parsing without developing
search heuristics; and the SCFGs are not required
to be in a normal form, making it possible to easily
parse with more complex SCFG types.
2 Synchronous parsing
Before presenting our algorithm, we review the
O(n
6
) synchronous parser for binary ITGs.
3
2.1 ITG synchronous parsing algorithm
Wu (1997) describes a bottom-up synchronous pars-
ing algorithm that can be understood as a generaliza-
tion of the CKY algorithm. CKY defines a table con-
sisting of n
2
cells, with each cell corresponding to a
span [i, j] in the input sentence; and the synchronous
variant defines a table in 4 dimensions, with cells
corresponding to a source span [s, t] and a target
span [u, v]. The bottom of the chart is initialized
first, and pairs of items are combined from bottom
to top. Since combining items from the n
4
cells in-
volves considering two split points (one source, one
target), it is not hard to see that this algorithm runs
in time O(n
6
).
3
Generalizing the algorithm to higher rank grammars is pos-
sible (Wu, 1997), as is converting a grammar to a weakly equiv-
alent binary form in some cases (Huang et al, 2009).
263
2.2 Parsing, intersection, and composition
We motivate an alternative conception of the syn-
chronous parsing problem as follows. It has long
been appreciated that monolingual parsing computes
the intersection of an FSA and a CFG (Bar-Hillel et
al., 1961; van Noord, 1995). That is, if S is an FSA
encoding some sentence s, intersection of S with a
CFG, G, results in a parse forest which contains all
and only derivations of s, that is L(S) ? L(G) ?
{{s}, ?}.
4
Crucially for our purposes, the resulting
parse forest is also itself a CFG.
5
Figure 1 illus-
trates, giving two equivalent representations of the
forest S?G, once as a directed hypergraph and once
as a CFG. While S ? G appears similar to G, the
non-terminals (NTs) of the resulting CFG are a cross
product of pairs of states from S and NTs from G.
6
Two parses are better than one (for synchronous parsing)
Chris Dyer
UMIACS Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland, College Park, MD 20742, USA
redpony AT umd.edu
Abstract
We describe an alternative to the well-known
O(n
6
) synchronous parsing algorithm given
in Wu (1997). Although this algorithm, which
is based on two successive monolingual parses
of the input sentence pair, does not (and prov-
ably can not) improve the worst-case run-
time, its best-case performance is O(n
3
). We
show that for a number of common syn-
chronous parsing problems, the two-parse al-
gorithm performs efficiently enough to be uti-
lized, without pruning, in iterative learning al-
gorithms that rely on inside-outside inference.
The algorithm has further advantages: prun-
ing strategies that would be difficult to realize
in the original algorithm become feasible, and
certain kinds of discriminative training require
the results of both parses, making this algo-
rithm a natural fit when those training regimes
are required.
1 Introduction
Synchronous context free grammars (SCFGs) gener-
alize traditional context-free grammars to generate
strings concurrently in a pair of languages (Lewis
and Stearns, 1968), in much the same way that fi-
nite state transducers (FSTs) generalize finite state
automata. In recent years, SCFGs have enjoyed
a resurgence in popularity as the formal basis for
several of the best-performing statistical machine
translation systems (Chiang, 2007; Zollmann et al,
2008). The translation task is a straightforward ma-
nipulation of SCFGs using standard monolingual al-
gorithms. To translate some f (a string of words in
the source language) into the target language, f is
parsed (with a monolingual parser), which, because
of the parallel structure of the rules, induces a forest
of translations in the target language.
Synchronous parsing, which is our focus for the
remainder of this paper, is the problem of finding
the best derivation, or the forest of derivations, of
a source and target sentence pair ?f, e?. This forest
is particularly useful in learning problems since it
can be used to compute and optimize statistics about
derivations of parallel training data. In the MT lit-
erature, this task is also known as ?const ained d -
coding?. Wu (1997) d cribes a bottom-up algo-
rithm for constructing this forest given a sentence
pair ?f, e? and grammar G that runs in O(|f|
3
|e|
3
)
since we will assume that n = |f| ? |e|, the run-
time is O(n
6
).
1.1 Parsing as composition
We motivate an alternative conception of the syn-
chronous parsing problem as follows. It has long
been appreciated that parsing computes the intersec-
tion of an FSA and a CFG (Bar-Hillel et al, 1961;
van Noord, 1995; Grune and Jacobs, 2008). That
is, parsing an FSA, S, with a CFG, G, results in a
parse forest which contains derivations of strings in
I = L(S) ? L(G),
1
and which may be ?. But, it is
helpful to keep in mind that the resulting parse for-
est is also itself a CFG (that exactly derives strings
in I). See Figure ?? for an example.
In the parallel parsing case, it?s helpful to think
in terms of an SCFG representing a context-free re-
lations and parallel parsing as being a composition
1
In the familiar case, S is a deterministic linear chain FSA
representing a sentence.
0 1 2 43
i saw the forest
Two parses are better than one (for synchronous parsing)
Chris Dyer
UMIACS Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland, College Park, MD 20742, USA
redpony AT umd.edu
Abstract
We describe an alternative to the well-known
O(n
6
) synchronous parsing algorithm given
in Wu (1997). Although this algorithm, which
is based on two successive monolingual parses
of the input sentence pair, does not (and prov-
ably can not) improve the worst-case run-
time, its best-case performance is O(n
3
). We
show that for a number of common sy -
chronous parsing problems, the two-parse al-
gorithm performs efficiently en ugh to be uti-
lized, without pruning, iterative learning al-
gorithms that rely on inside-outside infer nce.
The algorithm has further advantages: prun-
ing strategies that would be difficult to realize
in the original algorithm become feasible, and
certain kinds of discriminative training require
the results of both parses, making this algo-
rithm a natural fit when those training regimes
are required.
1 Introduction
Synchronous contex free grammars (SCFGs) gener-
alize traditional context-free grammars to generate
strings concurrently in a pair of languages (Lewis
and Stearns, 1968), in much the same way that fi-
nite state transducers (FSTs) generalize finite state
automata. In recent years, SCFGs have enjoyed
a resurgence in popularity as the formal basis for
several of the best-performing statistical machine
translation systems (Chia g, 2007; Zollmann et al,
2008). The translation task is a straightforward ma-
nipulation of SCFGs using standard monolingual al-
gorithms. To translate some f (a string of words in
the source language) into the target nguage, f is
parsed (with a monolingual parser), which, because
of the parallel structure of the rules, induces a forest
of translations in the target language.
Synchronous parsing, which is our focus for the
remainder of this paper, is the problem of finding
the best derivation, or the forest of derivations, of
a source and target sentence pair ?f, e?. This forest
is particularly useful in learning problems since it
can be used to compute and optimize sta stics bout
derivations of parallel training data. In the MT lit-
erature, this task is also known as ?constrained de-
coding?. Wu (1997) describes a bottom-up algo-
rithm for constructing this forest given a sentence
pair ?f, e? and grammar G that runs in O(|f|
3
|e|
3
)
since we will assume that n = |f| ? |e|, the run-
tim is O(n
6
).
1.1 Parsing as composition
We motivate an alternative conception of the syn-
chronous parsing problem as follows. It has long
been appreciated that parsing computes the intersec-
tion of an FSA and a CFG (Bar-Hillel et al, 1961;
van Noord, 1995; Grune and Jacobs, 2008). That
is, parsing an FSA, S, with a CFG, G, results in a
parse forest which contains derivations of strings in
I = L(S) ? L(G),
1
and which may be ?. But, it is
helpful to keep in mind that the resulting parse for-
est is also itself a CFG (that exactly derives strings
in I). See Figure ?? for an example.
In the parallel parsing case, it?s helpful to think
in terms of an SCFG repr senting a text-fre re-
lations and parallel parsing as being a composition
1
In the familiar case, S is a deterministic linear chain FSA
represent ng a sentence.
NP VPS
PRNNP
DT NNNP
V NPVP
theDT
aDT
forestNN
treeNN
iPRN
sawV
operation.
2
S ? G (1)
2 Experiments
Figure 1 plots the average runtime of the algorithm
as a function of the Arabic sentence length on an
Arabic-English phrasal ITG alignment task.
3 R lated work
Synchronous parsing has been widely used to com-
pute sufficient statistics for a variety of machine
learning models of synchronous trees; however,
since the naive algorithm is too slow to deal with
sentence sizes, most authors have proposed pruning
techniques. Zhang et al (2008) suggest tic-tac-toe
pruning, which uses Model 1 posteriors to exclude
ranges of cells from being computed. Blunsom et al
(2008) do a monolingual parse with of one language
but split the parser states by the string yielded by
the target derivations, pruning any nodes that yield
strings that do not exist in the target. Haghighi et al
(2009) also describe a pruning heuristic that results
in average case runtime of O(n
3
).
References
Y. Bar-Hillel, M. Perles, and E. Shamir. 1961. On for-
mal properties of simple phrase structure grammars.
Zeitschrift f?ur Phonetik, Sprachwissenschaft und Kom-
munikationsforschung, 14:143?172.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-HTL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
D. Grune and C.J. H. Jacobs. 2008. Parsing as intersec-
tion. Parsing Techniques, pages 425?442.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of the Joint Conference
of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 923?931, August.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15(3):465?488.
2
For a discussion of the equivalence of composition and
intersection in finite-state objects, refer to Mohri (2009). Al-
though necessity forces us to use different algorithms to realize
composition, the relationship still holds at the context-free level.
Figure 1: Synchronous parser runtime as a function of
(Arabic) sentence length on an Arabic-English corpus us-
ing a phrasal ITG.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, Mono-
graphs in Theoretical Computer Science, pages 213?
254. Springer.
Gertjan van Noord. 1995. The intersection of finite state
automata and definite clause grammars. In Proceed-
ings of the 33rd Annual Meeting of the Assocation
for Computational Linguistics, pages 159?165, Cam-
bridge, MA.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404, Sep.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings of ACL.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proc. of 22nd International Conference on
Computational Linguistics (Coling), Manchester, U.K.
2
DT
3 3
NN
4
2
NP
4
1
V
2
1
VP
4
0
S
4
0
NP
1
0
PRN
1
i saw the forest
0
NP
1
 
1
VP
40
S
4
0
PRN
10
NP
1
2
DT
3
 
3
NN
42
NP
4
1
V
2
 
2
NP
41
VP
4
the
2
DT
3
forest
3
NN
4
i
0
PRN
1
saw
1
V
2
(a) (b)
Figure 1: A CFG, G, an FSA, S, encoding a sentence, and
two equivalent representations of the parse forest S ? G,
(a) as a directed hypergraph and (b) as a CFG.
When dealing with SCFGs, rather than intersec-
4
L(x) denotes the set of strings generated by the gram-
mar/automaton x. In future mentions of intersection and com-
position operations, this will be implicit.
5
Th forest grammar derives only s, but using possibly many
derivations.
6
Each pair of states from the FSA corresponds to a span [i, j]
in a CKY table.
tion, parsing computes a related operation, composi-
tion.
7
The standard MT decoding-by-parsing task
can be understood as computing the composition
of an FST,
8
F , which encodes the source sentence
f with the SCFG, G, representing the translation
model. The result is the translation forest, F ? G,
which encodes all translations of f licensed by the
translation model. While G can generate a poten-
tially infinite set of strings in the source and target
languages, F ? G generates only f in the source lan-
guage (albeit with possibly infinitely many deriva-
tions), but any number of different strings in the tar-
get language. It is not hard to see that a second com-
position operation of an FST, E, encoding the target
string e with the e-side of F ?G (again using a mono-
lingual parsing algorithm), will result in a parse for-
est that exactly derives ?f, e?, which is the goal of
synchronous composition. Figure 2 shows an exam-
ple. In F ? G ? E the NTs (nodes) are the cross
product of pairs of states from E, the NTs from G,
and pairs of states in F .
Thus, synchronous parsing is the task of comput-
ing F ? G ?E. Since composition is associative, we
can compute this quantity either as (F ? G) ? E or
F ? (G ?E). Alternatively, we can use an algorithm
that performs 3-way composition directly.
2.3 The two-parse algorithm
9
The two-parse algorithm refers to performing a syn-
chronous parse by computing either (F ? G) ? E or
F ? (G ? E). Each composition operation is carried
out using a standard monolingual parsing algorithm,
such as Earley?s or CKY. In the experiments below,
since we use -free grammars, we use a variant of
CKY for unrestricted CFGs (Chiang, 2007).
Once the first composition is done, the resulting
parse forest must be converted into a CFG repre-
sentation that the second parser can utilize. This is
straightforward to do: each node becomes a unique
non-terminal symbol, with its incoming edges cor-
responding to different ways of rewriting it. Tails
of edges are non-terminal variables in the RHS of
these rewrites. A single bottom-up traversal of the
forest is sufficient to perform the conversion. Since
7
Intersection is a special case of composition where the in-
put and output labels on the transducers are identical (Mohri,
2009).
8
FSTs used to represent the source and target sentences have
identical input and output labels on every transition.
9
Satta (submitted) has independently derived this algorithm.
264
Two parses are better than one (for synchronous parsing)
Chris Dyer
UMIACS Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland, College Park, MD 20742, USA
redpony AT umd.edu
Abstract
We describe an alternative to the well-known
O(n
6
) synchronous parsing algorithm given
in Wu (1997). Although this algorithm, which
is based on two successive monolingual parses
of the input sentence pair, does not (and prov-
ably can not) improve the worst-case run-
time, its best-case performance is O(n
3
). We
show that for a number of common syn-
chronous parsing problems, the two-parse al-
gorithm performs efficiently enough to be uti-
lized, without pruning, in iterative learning al-
gorithms that rely on inside-outside inference.
The algorithm has further advantages: prun-
ing strategies that would be difficult to realize
in the original algorithm become feasible, and
certain kinds of discriminative training require
the results of both parses, making this algo-
rithm a natural fit when those training regimes
are required.
1 Introduction
Synchronous context free grammars (SCFGs) gener-
alize traditional context-free grammars to generate
strings concurrently in a pair of languages (Lewis
and Stearns, 1968), in much the same way that fi-
nite state transducers (FSTs) generalize finite state
automata. In recent years, SCFGs have enjoyed
a resurgence in popularity as the formal basis for
several of the best-performing statistical machine
translation systems (Chiang, 2007; Zollmann et al,
2008). The translation task is a straightforward ma-
nipulation of SCFGs using standard monolingual al-
gorithms. To translate some f (a string of words in
the source language) into the target language, f is
parsed (with a monolingual parser), which, because
of the parallel structure of the rules, induces a forest
of translations in the target language.
Synchronous parsing, which is our focus for the
remainder of this paper, is the problem of finding
the best derivation, or the forest of derivations, of
a source and target sentence pair ?f, e?. This forest
is particularly useful in learning problems since it
can be used to compute and optimize statistics about
derivations of parallel training data. In the MT lit-
erature, this task is also known as ?constrained de-
coding?. Wu (1997) describes a bottom-up algo-
rithm for constructing this forest given a sentence
pair ?f, e? and grammar G that runs in O(|f|
3
|e|
3
)
since we will assume that n = |f| ? |e|, the run-
time is O(n
6
).
1.1 Parsing as composition
We motivate an alternative conception of the syn-
chronous parsing problem as follows. It has long
been appreciated that parsing computes the intersec-
tion of an FSA and a CFG (Bar-Hillel et al, 1961;
van Noord, 1995; Grune and Jacobs, 2008). That
is, parsing an FSA, S, with a CFG, G, results in a
parse forest which contains derivations of strings in
I = L(S) ? L(G),
1
and which may be ?. But, it is
helpful to keep in mind that the resulting parse for-
est is also itself a CFG (that exactly derives strings
in I). See Figure ?? for an example.
In the parallel parsing case, it?s helpful to think
in terms of an SCFG representing a context-free re-
lations and parallel parsing as being a composition
1
In the familiar case, S is a deterministic linear chain FSA
representing a sentence.
< X , X >S
< X b , c X >X
< X b , X d >X
< a , c >X
< a , d >X
E
0 1 2
a b
F
0 1 2
c d
Figure 1: A CFG, G, an FSA, S, encoding a sentence,
and two reprsentations of the parse forest S ? G, (a) as
a directed hypergraph and (b) as a context-free rewrite
system.
(NTs) of the resulting CFG are a cross product of
pairs of states in the FSA and the NTs in the original
grammar.
When dealing with SCFGs, rather than intersec-
tion, parsing computes a related operation, compo-
sition.
2
The standard MT decoding-by-parsing task
can be understood as computing the composition of
an FST, F , encoding a source sentence f with the
SCFG, G, representing the translation model. The
result is the so-called translation forest, F ?G, which
encodes all translations of f licensed by the transla-
tion model. Now observe that while G can generate
a potentially infinite set of strings in both source lan-
guage and target language, F ?G (as an SCFG) gen-
erates only f, albeit possibly via several derivations,
but different translations e. It is not hard to see that
a second composition operation with an E encoding
2
Intersection is a special case of composition where the in-
put and output labels on the transducers are identical (Mohri,
2009).
a string e in the target will result in the will result in
a parse forest that exactly derives ?f, e?, which is the
goal of synchronous composition.
Thus, in synchronous parsing, we seek to com-
pute F ?G ?E. Since composition is associative, we
can compute this quantity either as (F ? G) ? E or
F ? (G ?E). Alternatively, we can use an algorithm
that performs 3-way composition directly, such as
Wu?s algorithm.
3
F ? G (1)
1.2 Analysis
Monolingual parsing is commonly thought of as a
worst-case O(n
3
) algorithm, even the known algo-
rithms do have a grammar term that can contribute
significantly. However, since the grammar that a
parser will employ is generally assumed to be fixed,
2 Experiments
Figure 2 plots the average runtime of the algorithm
as a function of the Arabic sentence length on an
Arabic-English phrasal ITG alignment task.
3 Related work
Synchronous parsing has been widely used to com-
pute sufficient statistics for a variety of machine
learning models of synchronous trees; however,
since the naive algorithm is too slow to deal with
sentence sizes, most authors have proposed pruning
techniques. Zhang et al (2008) suggest tic-tac-toe
pruning, which uses Model 1 posteriors to exclude
ranges of cells from being computed. Blunsom et al
(2008) do a monolingual parse with of one language
but split the parser states by the string yielded by
the target derivations, pruning any nodes that yield
strings that do not exist in the target. Haghighi et al
(2009) also describe a pruning heuristic that results
in average case runtime of O(n
3
).
References
Cyril Allauzen and Mehryar Mohri. 2008. 3-way
composition of weighted finite-state transducers. In
3
Three-way composition algorithms that operate only on
FSTs have also been developed (Allauzen and Mohri, 2008).
a : c a : d
0
X
1
0
S
2
0
X
1
 b : 
0
X
1
 d
0
X
1
 b : c 
0
X
1
< 
0
X
1
 b , c 
0
X
1
 >
0
S
2
< 
0
X
1
 b , 
0
X
1
 d >
0
S
2
< a , c >
0
X
1
< a , d >
0
X
1
Figure 1: A CFG, G, an FSA, S, encoding a sentence,
and two reprsentations of the parse forest S ? G, (a) as
a directed hypergraph and (b) as a context-free rewrite
system.
(NTs) of the resulting CFG are a cross product of
pairs of states in the FSA and the NTs in the original
grammar.
When dealing with SCFGs, rather than intersec-
tion, parsing computes a related operation, compo-
sition.
2
The standard MT decoding-by-parsing task
can be understood as computing the composition of
an FST, F , encoding a source sentence f with the
SCFG, G, representing the translation model. The
result is the so-called translation forest, F ?G, which
encod s all tr nslation of f licensed by th transla-
tion model. Now observe that while G can generate
a potentially infinite set of strings in both source lan-
guage and target language, F ?G (as an SCFG) gen-
erates only f, albeit possibly via several derivations,
but different translations e. It is not hard to see that
a second composition operation with an E encoding
2
Intersection is a special case of composition where the in-
put and output labels on the transducers are identical (Mohri,
2009).
a stri g e in t target will result in the will result in
parse forest that exactly derives ?f, e?, which is the
goal f synchronous composition.
Thus, in synchronous parsing, we seek to com-
pute F ?G ?E. Since composition is associative, we
can compute this quantity either as (F ? G) ? E or
F ? (G ?E). Alternatively, we can use an algorithm
that performs 3-way composition directly, such as
Wu?s algorithm.
3
F ? G ? E (1)
1.2 Analysis
Monolingual parsing is com only thought of as a
worst-case O(n
3
) algor thm, even the know lgo-
rithms do have a grammar ter that can contribute
significantly. However, since the grammar that a
parser will employ is generally assumed to be fixed,
2 Experiments
Figure 2 plot the average runtime of the algorithm
as a fu ction of the Arabic sentence length on an
Arabic-English phrasal ITG alignment task.
3 Related work
Synchronous parsing has been widely used to com-
pute sufficient statistics for a variety of machine
learning models of synchronous trees; however,
since the naive algorithm is too slow to deal with
sentence sizes, most authors have proposed pruning
techniques. Zhang et al (2008) suggest tic-tac-toe
pruning, which uses Model 1 posteriors to exclude
ranges of cells from being computed. Blunsom et al
(2008) do a monolingual parse with of one language
but split the parser states by the string yielded by
the target derivations, pruning any nodes that yield
strings that do not exist in the target. Haghighi et al
(2009) also describe a pruning heuristic that results
in average case runtime of O(n
3
).
References
Cyril Allauzen and Mehryar Mohri. 2008. 3-way
composition of weighted finite-state transducers. In
3
Three-way composition algorithms that operate only on
FSTs have also been developed (Allauzen and Mohri, 2008).
0
S
2
0 2
0
X
1 0
X
1
1 2
0 1
0
X
1
 b : c 
0
X
1 0
X
1
 b : 
0
X
1
 d
a : d a : c
0 1 0 11 2
1
2
Figure 2: An SCFG, G, two FSAs, E and F , and two
equivalent representations of F ? G. The synchronous
parse forest of the pair ?ab, cd? with G is given under F ?
G ? E.
our parser operates m re fficie tly with a deter-
minized grammar, w left-factor the grammar dur-
ing this traversal as well.
Analysis. Monolingual parsing runs in worst case
O(|G| ? n
3
) time, where n is the length of the in-
put being pa sed and |G| is a measure of the size
of the grammar (Graham et al, 1980). Since the
grammar term is constant for most typical parsing
applications, it is generally not considered carefully;
however, in the two-parse algorithm, the size of the
gra mar term for the second parse is not |G| but
|F ? G|, which clearly depends on the size of the in-
put F ; and so understanding the impact of this term
is key to understanding the algorithm?s run-time.
If G is an -free SCFG with non-terminals N and
maximally two NTs in a rule?s right hand side, and
is the number of states in F (corresponding to the
number of words in the f in a s ntence pair ?f, e?),
the the number of nodes in the parse forest F ? G
will be O(|N | ? n
2
). This can be shown easily since
by stipulation, we are able to use CKY+ to per-
form the parse, and there will be maximally as many
nodes in the forest as there are cells in the CKY chart
times the number of NTs. The number of edges will
be O(|N | ? n
3
), which occurs when every node can
be derived from all possible splits. This bound on
the number of edges implies that |F ?G| ? O(n
3
).
10
Therefore, the worst case run-time of the two-parse
algorithm isO(|N | ?n
3
?n
3
+ |G|?n
3
) = O(|N | ?n
6
),
the same as the bound on the ITG algorithm. We
note that while the ITG algorithm requires that the
SCFGs be rank-2 and in a normal form, the two-
parse algorithm analysis holds as long as the gram-
mars are rank-2 and -free.
11
3 Experiments
We now describe two different synchronous parsing
applications, with different classes of SCFGs, and
compare the performance of the two-parse algorithm
with that of previously used algorithms.
Phrasal ITGs. Here we compare performance of
the two-parse algorithm and the O(n
6
) ITG parsing
algorithm on an Arabic-English phrasal ITG align-
ment task. We used a variant of the phrasal ITG de-
scribed by Zhang et al (2008).
12
Figure 3 plots the
average run-time of the two algorithms as a function
of the Arabic sentence length. The two-parse ap-
proach is far more efficient. In total, aligning the 80k
sentence pairs in the corpus completed in less than
4 hours with the two-parse algorithm but required
more than 1 week with the baseline algorithm.
13
?Hiero? grammars. An alternative approach to
computing a synchronous parse forest is based on
cube pruning (Huang and Chiang, 2007). While
more commonly used to integrate a target m-gram
LM during decoding, Blunsom et al (2008), who re-
quired synchronous parses to discriminatively train
10
How tight these bounds are depends on the ambiguity in
the grammar w.r.t. the input: to generate n
3
edges, every item
in every cell must be derivable by every combination of its sub-
spans. Most grammars are substantially less ambiguous.
11
Since many widely used SCFGs meet these criteria, in-
cluding hierarchical phrase-based translation grammars (Chi-
ang, 2007), SAMT grammars (Zollmann and Venugopal, 2006),
and phrasal ITGs (Zhang et al, 2008), a detailed analysis of -
containing and higher rank grammars is left to future work.
12
The restriction that phrases contain exactly a single align-
ment point was relaxed, resulting in much larger and more am-
biguous grammars than those used in the original work.
13
A note on implementation: our ITG aligner was minimal; it
only computed the probability of the sentence pair using the in-
side algorithm. With the two-parse aligner, we stored the com-
plete forest during both the first and second parses.
265
10 20 30 40 50 60
0
20
40
60
Wu (1997)
this work
Figure 3: Average synchronous parser run-time (in sec-
onds) as a function of Arabic sentence length (in words).
an SCFG translation model, repurposed this algo-
rithm to discard partial derivations during transla-
tion of f if the derivation yielded a target m-gram
not found in e (p.c.). We replicated their BTEC
Chinese-English baseline system and compared the
speed of their ?cube-parsing? technique and our two-
parse algorithm.
14
The SCFG used here was ex-
tracted from a word-aligned corpus, as described in
Chiang (2007).
15
The following table compares the
average per sentence synchronous parse time.
Algorithm avg. run-time (sec)
Blunsom et al (2008) 7.31
this work 0.20
4 Discussion
Thinking of synchronous parsing as two composi-
tion operations has both conceptual and practical
benefits. The two-parse strategy can outperform
both the ITG parsing algorithm (Wu, 1997), as well
as the ?cube-parsing? technique (Blunsom et al,
2008). The latter result points to a connection with
recent work showing that determinization of edges
before LM integration leads to fewer search errors
during decoding (Iglesias et al, 2009).
Our results are somewhat surprising in light of
work showing that 3-way composition algorithms
for FSTs operate far more efficiently than perform-
ing successive pairwise compositions (Allauzen and
Mohri, 2009). This is certainly because the 3-way
algorithm used here (the ITG algorithm) does an ex-
14
To the extent possible, the two experiments were carried
out using the exact same code base, which was a C++ imple-
mentation of an SCFG-based decoder.
15
Because of the mix of terminal and non-terminal symbols,
such grammars cannot be used by the ITG synchronous parsing
algorithm.
haustive search over all n
4
span pairs without aware-
ness of any top-down constraints. This suggests that
faster composition algorithms that incorporate top-
down filtering may still be discovered.
References
C. Allauzen and M. Mohri. 2009. N-way composition of
weighted finite-state transducers. International Jour-
nal of Foundations of Comp. Sci., 20(4):613?627.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1961. On for-
mal properties of simple phrase structure grammars.
Zeitschrift f?ur Phonetik, Sprachwissenschaft und Kom-
munikationsforschung, 14:143?172.
P. Blunsom, T. Cohn, and M. Osborne. 2008. Probalistic
inference for machine translation. In EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
S. L. Graham, W. L. Ruzzo, and M. Harrison. 1980. An
improved context-free recognizer. ACM Trans. Pro-
gram. Lang. Syst., 2(3):415?462.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL/IJCNLP, pages 923?931.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL.
L. Huang, H. Zhang, D. Gildea, and K. Knight. 2009.
Binarization of synchronous context-free grammars.
Computational Linguistics, 35(4).
G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In Proc. NAACL.
P. M. Lewis, II and R. E. Stearns. 1968. Syntax-directed
transduction. J. ACM, 15(3):465?488.
M. Mohri. 2009. Weighted automata algorithms. In
M. Droste, W. Kuich, and H. Vogler, editors, Hand-
book of Weighted Automata, Monographs in Theoreti-
cal Computer Science, pages 213?254. Springer.
G. Satta and E. Peserico. 2005. Some computational
complexity results for synchronous context-free gram-
mars. In Proceedings of NAACL.
G. Satta. submitted. Translation algorithms by means of
language intersection.
G. van Noord. 1995. The intersection of finite state au-
tomata and definite clause grammars. In Proc. of ACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proceedings of ACL.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In Proc.
of the Workshop on SMT.
266
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 858?866,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Context-free reordering, finite-state translation
Chris Dyer and Philip Resnik
UMIACS Laboratory for Computational Linguistics and Information Processing
Department of Linguistics
University of Maryland, College Park, MD 20742, USA
redpony, resnik AT umd.edu
Abstract
We describe a class of translation model in
which a set of input variants encoded as a
context-free forest is translated using a finite-
state translation model. The forest structure of
the input is well-suited to representing word
order alternatives, making it straightforward to
model translation as a two step process: (1)
tree-based source reordering and (2) phrase
transduction. By treating the reordering pro-
cess as a latent variable in a probabilistic trans-
lation model, we can learn a long-range source
reordering model without example reordered
sentences, which are problematic to construct.
The resulting model has state-of-the-art trans-
lation performance, uses linguistically moti-
vated features to effectively model long range
reordering, and is significantly smaller than a
comparable hierarchical phrase-based transla-
tion model.
1 Introduction
Translation models based on synchronous context-
free grammars (SCFGs) have become widespread in
recent years (Wu, 1997; Chiang, 2007). Compared
to phrase-based models, which can be represented as
finite-state transducers (FSTs, Kumar et al (2006)),
one important benefit that SCFG models have is the
ability to process long range reordering patterns in
space and time that is polynomial in the length of
the displacement, whereas an FST must generally
explore a number of states that is exponential in
this length.1 As one would expect, for language
1Our interest here is the reordering made possible by varying
the arrangement of the translation units, not the local word order
differences captured inside memorized phrase pairs.
pairs with substantial structural differences (and thus
requiring long-range reordering during translation),
SCFG models have come to outperform the best FST
models (Zollmann et al, 2008).
In this paper, we explore a new way to take advan-
tage of the computational benefits of CFGs during
translation. Rather than using a single SCFG to both
reorder and translate a source sentence into the target
language, we break the translation process into a two
step pipeline where (1) the source language is re-
ordered into a target-like order, with alternatives en-
coded in a context-free forest, and (2) the reordered
source is transduced into the target language using
an FST that represents phrasal correspondences.
While multi-step decompositions of the transla-
tion problem have been proposed before (Kumar et
al., 2006), they are less practical with the rise of
SCFG models, since the context-free languages are
not closed under intersection (Hopcroft and Ullman,
1979). However, the CFLs are closed under intersec-
tion with regular languages. By restricting ourselves
to a finite-state phrase transducer and representing
reorderings of the source in a context-free forest, ex-
act inference over the composition of the two models
is possible.
The paper proceeds as follows. We first ex-
plore reordering forests and describe how to trans-
late them with an FST (?2). Since we would like our
reordering model to discriminate between good re-
orderings of the source and bad ones, we show how
to train our reordering component as a latent variable
in an end-to-end translation model (?3). We then
presents experimental results on language pairs re-
quiring small amounts and large amounts of reorder-
ing (?4). We conclude with a discussion of related
858
work (?6) and possible extensions (?7).
2 Reordering forests and translation
In this section, we describe source reordering
forests, a context-free representation of source lan-
guage word order alternatives.2 The basic idea is
that for the source sentence, f, that is to be trans-
lated, we want to create a (monolingual) context-free
grammarF that generates strings (f?) of words in the
source language that are permutations of the origi-
nal sentence. Specifically, this forest should contain
derivations that put the source words into an order
that approximates how they will be ordered in the
grammar of the target language.
For a concrete example, let us consider the task of
English-Japanese translation.3 Our input sentence
is John ate an apple. Japanese is a head-final lan-
guage, where the heads of phrases (such as the verb
in a verb phrase) typically come last, and English
is a head-initial language, where heads come first.
As a result, the usual order for a declarative sen-
tence in English is SVO (subject-verb-object), but
in Japanese, it is SOV, and the desired translation
is John-ga ringo-o [an apple] tabeta [ate]. In sum-
mary, when translating from English into Japanese,
it is usually necessary to move verbs from their po-
sition between the subject and object to the end of
the sentence.
This reordering can happen in two ways, which
we depict in Figure 1. In the derivation on the left,
a memorized phrase pair captures the movement of
the verb (Koehn et al, 2003). In the other deriva-
tion, the source is first reordered into target word
order and then translated, using smaller translation
units. In addition, we have assumed that the phrase
translations were learned from a parallel corpus that
is in the original ordering, so the reordering forest F
should include derivations of phrase-size units in the
source order as well as the target order.
2Note that forests are isomorphic to context-free grammars.
For example, what is referred to as the ?parse forest?, and un-
derstood to encode all derivations of a sentence s under some
grammar, can also be understood as being a context-free gram-
mar itself that exactly generates s. We therefore refer to a forest
as a grammar sometimes, or vice versa, depending on which
characterization is clearer in context.
3We use English as the source language since we expect the
parse structure of English sentences will be more familiar to
many readers.
0 1
an : ?
apple : ???? 
John : ???? 
ate : ??? 
[John-ga]
[ringo-o]
[tabeta]
23
ate : ?an : ?
apple : ????  ???  
[ringo-o   tabeta]
Figure 2: A fragment of a phrase-based English-Japanese
translation model, represented as an FST. Japanese ro-
manization is given in brackets.
A minimal reordering forest that supports the
derivations depicted needs to include both an SOV
and SVO version of the source. This could be ac-
complished trivially with the following grammar:
S ? John ate an apple
S ? John an apple ate
However, this grammar misses the opportunity to
take advantage of the regularities in the permuted
structure. A better alternative might be:
S ? John VP
VP ? ate NP
VP ? NP ate
NP ? an apple
In this grammar, the phrases John and an apple are
fixed and only the VP contains ordering ambiguity.
2.1 Reordering forests based on source parses
Many kinds of reordering forests are possible; in
general, the best one for a particular language pair
will be one that is easiest to create given the re-
sources available in the source language. It will
also be the one that most compactly expresses the
source reorderings that are most likely to be use-
ful for translation. In this paper, we consider a
particular kind of reordering forest that is inspired
by the reordering model of Yamada and Knight
(2001).4 These are generated by taking a source lan-
guage parse tree and ?expanding? each node so that it
4One important difference is that our translation model is not
restricted by the structure of the source parse tree; i.e., phrases
used in transduction need not correspond to constituents in the
source reordering forest. However, if a phrase does cross a con-
stituent boundary between constituents A and B, then transla-
tions that use that phrase will have A and B adjacent.
859
????    ????   ??? 
John-ga      ringo-o     tabeta
John an apple ate
????    ????   ??? 
John-ga      ringo-o     tabeta
John ate an apple
John ate an appleJohn ate an apple
f
f'
e
Figure 1: Two possible derivations of a Japanese translation of an English source sentence.
rewrites with different permutations of its children.5
For an illustration using our example sentence, re-
fer to Figure 3 for the forest representation and Fig-
ure 4 for its isomorphic CFG representation. It is
easy to see that this forest generates the two ?good?
order variants from Figure 1; however, the forest in-
cludes many other derivations that will probably not
lead to good translations. For this reason, it is help-
ful to associate the edges in the forest (that is, the
rules in the CFG) with weights reflecting how likely
that rule is to lead to a good translation. We discuss
how these weights can be learned automatically in
?3.
2.2 Translating reordering forests with FSTs
Having described how to construct a context-free re-
ordering forest for the source sentence, we now turn
to the problem of how to translate the source forest
into the target language using a phrase-based trans-
lation model encoded as an FST, e.g. Figure 2. The
process is quite similar to the one used when trans-
lating a source sentence with an SCFG, but with a
twist: rather than representing the translation model
as a grammar and parsing the source sentence, we
represent the source sentence as a grammar (i.e. its
reordering forest), and we use it to ?parse? the trans-
lation model (i.e. the FST representation of the
phrase-based model). The end result (either way!)
is a translation forest containing all possible target-
language translations of the source.
Parsing can be understood as a means of comput-
ing the intersection of an FSA and a CFG (Grune and
Jacobs, 2008). Since we are dealing with FSTs that
define binary relations over strings, not FSAs defin-
ing strings, this operation is more properly compo-
sition. However, since CFG/FSA intersection is less
5For computational tractability, we only consider all permu-
tations only when the number of children is less than 5, other-
wise we exclude permutations where a child moves more than
4 positions away from where it starts.
cumbersome to describe, we present the algorithm
in terms of intersection.
To compute the composition of a reordering for-
est, G, with an FSA, F , we will make use of a variant
of Earley?s algorithm (Earley, 1970). Let weighted
finite-state automaton F = ??, Q, q0, qfinal, ?, w?.
? is a finite alphabet; Q is a set of states; q0 and
qfinal ? Q are start and accept states, respectively,6 ?
is the transition function Q? ?? 2Q, and w is the
transition cost function Q ? Q ? R. We use vari-
ables that refer to states in the FSA with the letters
q, r, and s. We use x to represent a variable that is
an element of ?. Variables u and v represent costs.
X and Y are non-terminals. Lowercase Greek let-
ters are strings of terminals and non-terminals. The
function ?(q, x) returns the state(s) that are reach-
able from state q by taking a transition labeled with
x in the FSA.
Figure 5 provides the inference rules for a
top-down intersection algorithm in the form of a
weighted logic program; the three inference rules
correspond to Earley?s SCAN, PREDICT, and COM-
PLETE, respectively.
3 Reordering and translation model
As pointed out in ?2.1, our reordering forests may
contain many paths, some of which when translated
will lead to good translations and others that will be
bad. We would like a model to distinguish the two.
If we had a parallel corpus of source language
sentences paired with ?reference reorderings?, such
a model could be learned directly as a supervised
learning task. However, creating the optimal target-
language reordering f? for some f is a nontrivial
task.7 Instead of trying to solve this problem, we
opt to treat the reordered from of the source, f?, as a
6Other FSA definitions permit sets of start and final states.
We use the more restricted definition for simplicity and because
in our FSTs q0 = qfinal.
7For a discussion of methods for generating reference re-
860
Original parse:
Reordering forest:
S
V DT NN
VP
NP
subj
NP
obj
John ate
an apple
1 1
1
1
1
1
2
2
2
22
2
S
V DT NN
VP
NP
subj
NP
obj
John ate
an apple
1 1
1
2
2
2
Figure 3: Example of a reordering forest. Linearization
order of non-terminals is indicated by the index at the tail
of each edge. The isomorphic CFG is shown in Figure 4;
dashed edges correspond to reordering-specific rules.
latent variable in a probabilistic translation model.
By doing this, we only require a parallel corpus of
translations to learn the reordering model. Not only
does this make our lives easier, since ?reference re-
orderings? are not necessary, but it is also intuitively
satisfying because from a task perspective, we are
not concerned with values of f?, but only with pro-
ducing a good translation e.
3.1 A probabilistic translation model with a
latent reordering variable
The translation model we use is a two phase process.
First, source sentence f is reordered into a target-
like word order f? according to a reordering model
r(f?|f). The reordered source is then transduced into
the target language according to a translation model
t(e|f?). We require that r(f?|f) can be represented by
orderings from word aligned parallel corpora, refer to Tromble
and Eisner (2009).
Original parse grammar: S? NPsubj VP
VP? V NPobj NPobj ? DT NN
NPsubj ? John V? ate
DT? an NN? apple
Additional reordering grammar rules:
S? VP NPsubj
VP? NPobj V
NPobj ? NN DT
Figure 4: Context-free grammar representation of the for-
est in Figure 3. The reordering grammar contains the
parse grammar, plus the reordering-specific rules.
Initialization:
[S? ? ?S, q0, q0] : 1
Inference rules:
[X ? ? ? x?, q, r] : u
[X ? ?x ? ?, q, ?(r, x)] : u? w(?(r, x))
[X ? ? ? Y ?, q, r]
[Y ? ??, r, r] : u
Y
u
?? ? ? G
[X ? ? ? Y ?, q, s] : u [Y ? ??, s, r] : v
[X ? ?Y ? ?, q, r] : u? v
Goal state:
[S? ? S?, q0, qfinal]
Figure 5: Weighted logic program for computing the in-
tersection of a weighted FSA and a weighted CFG.
a recursion-free probabilistic context-free grammar,
i.e. a forest as in ?2.1, and that t(e|f?) is represented
by a (cyclic) finite-state transducer, as in Figure 2.
Since the reordering forest may define multiple
derivations a from f to a particular f?, and the trans-
ducer may define multiple derivations d from f? to
a particular translation e, we marginalize over these
nuisance variables as follows to define the probabil-
ity of a translation given the source:
p(e|f) =
?
d
?
f?
t(e,d|f?)
?
a
r(f?, a|f) (1)
Crucially, since we have restricted r(f?|f) to have
the form of a weighted CFG and t(e|f?) to be an
861
FST, the quantity (1), which sums over all reorder-
ings (and derivations), can be computed in polyno-
mial time with dynamic programming composition,
as described in ?2.2.
3.2 Conditional training
While it is straightforward to use expectation maxi-
mization to optimize the joint likelihood of the paral-
lel training data with a latent variable model, instead
we use a log-linear parameterization and maximize
conditional likelihood (Blunsom et al, 2008; Petrov
and Klein, 2008). This enables us to employ a rich
set of (possibly overlapping, non-independent) fea-
tures to discriminate among translations. The proba-
bility of a derivation from source to reordered source
to target is thus written in terms of model parameters
? = {?i} as:
p(e,d, f?, a|f; ?) =
exp
?
i ?i ?Hi(e,d, f
?, a, f)
Z(f; ?)
where Hi(e,d, f?, a, f) =
?
r?d
hi(f?, r) +
?
s?a
hi(f, s)
The derivation probability is globally normalized by
the partition Z(f; ?), which is just the sum of the
numerator for all derivations of f (corresponding to
any e). The Hi (written below without their argu-
ments) are real-valued feature functions that may
be overlapping and non-independent. For compu-
tational tractability, we assume that the feature func-
tions Hi decompose with the derivations of f? and e
in terms of local feature functions hi. We also de-
fineZ(e, f;?) to be the sum of the numerator over all
derivations that yield the sentence pair ?e, f?. Rather
than training purely to optimize conditional likeli-
hood, we also make use of a spherical Gaussian prior
on the value of ? with mean 0 and variance ?2,
which helps prevent overfitting of the model (Chen
and Rosenfeld, 1998). Our objective is thus to select
? minimizing:
L = ? log
?
?e,f?
p(e|f; ?)?
||?||2
2?2
= ?
?
?e,f?
[logZ(e, f; ?)? logZ(f; ?)]?
||?||2
2?2
The gradient of Lwith respect to the feature weights
has a parallel form; it is the difference in feature ex-
pectations under the reference distribution and the
translation distribution with a penalty term due to
the prior:
?L
??i
=
?
?e,f?
Ep(d,a|e,f;?)[hi]? Ep(e,d,a|f;?)[hi]?
?i
?2
The form of the objective and gradient are quite sim-
ilar to the traditional fully observed training scenario
for CRFs (Sha and Pereira, 2003). However, rather
than matching the feature expectations in the model
to an observable feature value, we have to sum over
the latent structure that remains after observing our
target e, which makes the form of the first summand
an expectation rather than just a feature function
value.
3.2.1 Computing the objective and gradient
The objective and gradient that were just introduced
can be computed in two steps. Given a training pair
?e, f?, we generate the forest of reorderings F from f
as described in ?2.1. We then compose this grammar
with T , the FST representing the translation model,
which yields F ?T , a translation forest that contains
all possible translations of f into the target language,
as described in ?2.2. Running the inside algorithm
on the translation forest computes Z(f; ?), the first
term in the objective, and the inside-outside algo-
rithm can be used to compute Ep(e,d,a|f)[hi]. Next,
to compute Z(e, f; ?) and the first expectation in the
gradient, we need to find the subset of the transla-
tion forest F ? T that exactly derives the reference
translation e. To do this, we again rely on the fact
that F ? T is a forest and therefore itself a context-
free grammar. So, we use this grammar to parse
the target reference string e. The resulting forest,
F ?T ?e, contains all and only derivations that yield
the pair ?e, f?. Here, the inside algorithm computes
Z(e, f; ?) and the inside-outside algorithm can be
used to compute Ep(e,d,a|f)[hi].
Once we have an objective and gradient, we can
apply any first-order numerical optimization tech-
nique.8 Although the conditional likelihood surface
of this model is non-convex (on account of the la-
tent variables), we did not find a significant initial-
ization effect. For the experiments below, we ini-
tialized ? = 0 and set ?2 = 1. Training generally
converged in fewer than 1500 function evaluations.
8For our experiments we used L-BFGS (Liu and Nocedal,
1989).
862
4 Experimental setup
We now turn to an experimental validation of the
models we have introduced. We define three con-
ditions: a small data scenario consisting of a trans-
lation task based on the BTEC Chinese-English cor-
pus (Takezawa et al, 2002), a large data Chinese-
English condition designed to be more comparable
to conditions in a NIST MT evaluation, and a large
data Arabic-English task.
For each condition, phrase tables were extracted
as described in Koehn et al (2003) with a maxi-
mum phrase size of 5. The parallel training data
was aligned using the Giza++ implementation of
IBM Model 4 (Och and Ney, 2003). The Chinese
text was segmented using a CRF-based word seg-
menter (Tseng et al, 2005). The Arabic text was
segmented using the technique described in Lee et
al. (2003). The Stanford parser was used to generate
source parses for all conditions, and these were then
used to generate the reordering forests as described
in ?2.1.
Table 1 summarizes statistics about the cor-
pora used. The reachability statistic indicates
what percentage of sentence pairs in the train-
ing data could be regenerated using our reorder-
ing/translation model.9 To train the reordering
model, we used all of the reachable sentence pairs
from BTEC, 20% of the reachable set in the
Chinese-English condition, and all reachable sen-
tence pairs under 40 words (source) in length in the
Arabic-English condition.
Error analysis indicates that a substantial portion
of unreachable sentence pairs are due to alignment
(word or sentence) or parse errors; however, in some
cases the reordering forests did not contain an ad-
equate source reordering to produce the necessary
target. For example, in Arabic, which is a VSO lan-
guage, the treebank annotation is to place the sub-
ject NP as the ?middle child? between the V and the
object constituent. This can be reordered into an En-
glish SVO order using our child-permutation rules;
however, if the source VP is modified by a modal
particle, the parser makes the particle the parent of
the VP, and it is no longer possible to move the sub-
ject to the first position in the sentence. Richer re-
ordering rules are needed to address this problem.
9Only sentences that can be generated by the model can be
used in training.
Other solutions to the reachability problem include
targeting reachable oracles instead of the reference
translation (Li and Khudanpur, 2009) or making use
of alternative training criteria, such as minimum risk
training (Li and Eisner, 2009).
4.1 Features
We briefly describe the feature functions we used
in our model. These include the typical dense fea-
tures used in translation: relative phrase translation
frequencies p(e|f) and p(f |e), ?lexically smoothed?
translation probabilities plex(e|f) and plex(f |e), and
a phrase count feature. For the reordering model, we
used a binary feature for each kind of rule used, for
example ?VP?V NP(a) would fire once for each time
the rule VP ? V NP was used in a derivation, a.
For the Arabic-English condition, we observed that
the parse trees tended to be quite flat, with many re-
peated non-terminal types in one rule, so we aug-
mented the non-terminal types with an index indi-
cating where they were located in the original parse
tree. This resulted in a total of 6.7k features for
IWSLT, 18k features for the large Chinese-English
condition, and 516k features for Arabic-English.10
A target language model was not used during the
training of the source reordering model, but it was
used during the translation experiments (see below).
4.2 Qualitative assessment of reordering model
Before looking at the translation results, we exam-
ine what the model learns during training. Figure 6
lists the 10 most highly weighted reordering features
learned by the BTEC model (above) and shows an
example reordering using this model (below), with
the most English-like reordering indicated with a
star.11 Keep in mind, we expect these features to
reflect what the best English-like order of the input
should be. All are almost surprisingly intuitive, but
this is not terribly surprising since Chinese and En-
glish have very similar large-scale structures (both
are head initial, both have adjectives and quanti-
fiers that precede nouns). However, we see two en-
tries in the list (starred) that correspond to an En-
10The large number of features in the Arabic system was due
to the relative flatness of the Arabic parse trees.
11The italicized symbols in the English gloss are functional
elements with no precise translation. Q is an interrogative parti-
cle, and DE marks a variety of attributive roles and is used here
as the head of a relative clause.
863
Table 1: Corpus statistics
Condition Sentences Source words Target words Reachability
BTEC 44k 0.33M 0.36M 81%
Chinese-English 400k 9.4M 10.9M 25%
Arabic-English 120k 3.3M 3.6M 66%
glish word order that is ungrammatical in Chinese:
PP modifiers in Chinese typically precede the VPs
they modify, and CPs (relative clauses) also typi-
cally precede the nouns they modify. In English, the
reverse is true, and we see that the model has indeed
learned to prefer this ordering. It was not necessary
that this be the case: since our model makes use
of phrases memorized from a non-reordered training
set, it could hav relied on those for all its reordering.
Yet these results provide evidence that it is learning
large-scale reordering successfully.
Feature ? note
VP? VE NP 0.995
VP? VV VP 0.939 modal + VP
VP? VV NP 0.895
VP? VP PP? 0.803 PP modifier of VP
VP? VV NP IP 0.763
PP? P NP 0.753
IP? NP VP PU 0.728 PU = punctuation
VP? VC NP 0.598
NP? DP NP 0.538
NP? NP CP? 0.537 rel. clauses follow
?   ?     ??           ?   ???  ??    ?   ?? ?  ?
 I    CAN  CATCH  [
NP
[
CP 
GO   HILTON  HOTEL  DE]   BUS]   Q   ? 
I  CAN  CATCH  [
NP
 BUS [
CP  
GO  HILTON  HOTEL  DE]]  Q  ? 
I  CAN  CATCH  [
NP
 BUS [
CP  
DE  GO  HILTON  HOTEL]]  Q  ? 
I  CAN  CATCH  [
NP
 BUS [
CP  
GO  HOTEL  HILTON  DE]]  Q  ? 
I  CAN  CATCH  [
NP
 BUS [
CP  
DE  GO  HOTEL  HILTON]]  Q  ? 
I  CATCH  [
NP
 BUS [
CP  
GO  HILTON  HOTEL  DE]]  CAN  Q  ? 
Input:
5-best reordering:
(Can I catch a bus that goes to the Hilton Hotel ?)
Figure 6: (Above) The 10 most highly-weighted features
in a Chinese-English reordering model. (Below) Exam-
ple reordering of a Chinese sentence (with English gloss,
translation, and partial syntactic information).
5 Translation experiments
We now consider how to apply this model to a trans-
lation task. The training we described in ?3.2 is
suboptimal for state-of-the-art translation systems,
since (1) it optimizes likelihood rather than an MT
metric and (2) it does not include a language model.
We describe how we addressed these problems here,
and then present our results in the three conditions
defined above.
5.1 Training for Viterbi decoding
A language model was incorporated using cube
pruning (Huang and Chiang, 2007), using a 200-
best limit at each node during LM integration. To
improve the ability of the phrase model to match
reordered phrases, we extracted the 1-best reorder-
ing of the training data under the learned reordering
model and generated the phrase translation model so
that it contained phrases from both the original order
and the 1-best reordering.
To be competitive with other state-of-the-art sys-
tems, we would like to use Och?s minimum error
training algorithm for training; however, we can-
not tune the model as described with it, since it has
far too many features. To address this, we con-
verted the coefficients on the reordering features into
a single reordering feature which then had a coef-
ficient assigned to it. This technique is similar to
what is done with logarithmic opinion pools, only
the learned model is not a probability distribution
(Smith et al, 2005). Once we collapsed the reorder-
ing weights into a single feature, we used the tech-
niques described by Kumar et al (2009) to optimize
the feature weights to maximize corpus BLEU on a
held-out development set.
5.2 Translation results
Scores on a held-out test set are reported in Table 2
using case-insensitive BLEU with 4 reference trans-
lations (16 for BTEC) using the original definition
of the brevity penalty. We report the results of our
864
model along with three baseline conditions, one with
no-reordering at all (mono), the performance of a
phrase-based translation model with distance-based
distortion, the performance of our implementation of
a hierarchical phrase-based translation model (Chi-
ang, 2007), and then our model.
Table 2: Translation results (BLEU)
Condition Mono PB Hiero Forest
BTEC 47.4 51.8 52.4 54.1
Chinese-Eng. 29.0 30.9 32.1 32.4
Arabic-Eng. 41.2 45.8 46.6 44.9
6 Related work
A variety of translation processes can be formalized
as the composition of a finite-state representation of
input (typically just a sentence, but often a more
complex structure, like a word lattice) with an SCFG
(Wu, 1997; Chiang, 2007; Zollmann and Venugopal,
2006). Like these, our work uses parsing algorithms
to perform the composition operation. But this is the
first time that the input to a finite-state transducer has
a context-free structure.12 Although not described
in terms of operations over formal languages, the
model of Yamada and Knight (2001) can be under-
stood as an instance of our class of models with a
specific input forest and phrases restricted to match
syntactic constituents.
In terms of formal similarity, Mi et al (2008) use
forests as input to a tree-to-string transducer pro-
cess, but the forests are used to recover from 1-
best parsing errors (as such, all derivations yield
the same source string). Iglesias et al (2009) use
a SCFG-based translation model, but implement it
using FSTs, although they use non-regular exten-
sions that make FSTs equivalent to recursive tran-
sition networks. Galley and Manning (2008) use
a context-free reordering model to score a phrase-
based (exponential) search space.
Syntax-based preprocessing approaches that have
relied on hand-written rules to restructure source
trees for particular translation tasks have been quite
widely used (Collins et al, 2005; Wang et al, 2007;
Xu et al, 2009; Chang et al, 2009). Discrimina-
tively trained reordering models have been exten-
sively explored. A widely used approach has been to
12Satta (submitted) discusses the theoretical possibility of
this sort of model but provides no experimental results.
use a classifier to predict the orientation of phrases
during decoding (Zens and Ney, 2006; Chang et al,
2009). These classifiers must be trained indepen-
dently from the translation model using training ex-
amples extracted from the training data. A more am-
bitious approach is described by Tromble and Eisner
(2009), who build a global reordering model that is
learned automatically from reordered training data.
The latent variable discriminative training ap-
proach we describe is similar to the one originally
proposed by Blunsom et al (2008).
7 Discussion and conclusion
We have described a new model of translation that
takes advantage of the strengths of context-free
modeling, but splits reordering and phrase transduc-
tion into two separate models. This lets the context-
free part handle what it does well, mid-to-long range
reordering, and lets the finite-state part handle lo-
cal phrasal correspondences. We have further shown
that the reordering component can be trained effec-
tively as a latent variable in a discriminative transla-
tion model using only conventional parallel training
data.
This model holds considerable promise for fu-
ture improvement. Not only does it already achieve
quite reasonable performance (performing particu-
larly well in Chinese-English, where mid-range re-
ordering is often required), but we have only begun
to scratch the surface in terms of the kinds of fea-
tures that can be included to predict reordering, as
well as the kinds of reordering forests used. Fur-
thermore, by reintroducing the concept of a cascade
of transducers into the context-free model space, it
should be possible to develop new and more effec-
tive rescoring mechanisms. Finally, unlike SCFG
and phrase-based models, our model does not im-
pose any distortion limits.
Acknowledgements
The authors gratefully acknowledge partial support from
the GALE program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001. Any
opinions, findings, conclusions or recommendations ex-
pressed in this paper are those of the authors and do not
necessarily reflect the views of the sponsors. Thanks
to Hendra Setiawan, Vlad Eidelman, Zhifei Li, Chris
Callison-Burch, Brian Dillon and the anonymous review-
ers for insightful comments.
865
References
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proceedings of ACL-HLT.
P.-C. Chang, D. Jurafsky, and C. D. Manning. 2009. Dis-
ambiguating ?DE? for Chinese-English machine trans-
lation,. In Proc. WMT.
S. F. Chen and R. Rosenfeld. 1998. A Gaussian prior
for smoothing maximum entropy models. Technical
Report TR-10-98, Computer Science Group, Harvard
University.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Pro-
ceedings of ACL 2005.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the Association for Com-
puting Machinery, 13(2):94?102.
M. Galley and C. D. Manning. 2008. A simple and ef-
fective hierarchical phrase reordering model. In Proc.
EMNLP.
D. Grune and C. J. H. Jacobs. 2008. Parsing as intersec-
tion. In D. Gries and F. B. Schneider, editors, Parsing
Techniques, pages 425?442. Springer, New York.
J. E. Hopcroft and J. D. Ullman. 1979. Introduc-
tion to Automata Theory, Languages and Computa-
tion. Addison-Wesley.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In ACL.
G. Iglesias, A. de Gispert, E. R. Banga, and W. Byrne.
2009. Hierarchical phrase-based translation with
weighted finite state transducers. In Proc. NAACL.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of NAACL, pages 48?54.
S. Kumar, Y. Deng, and W. Byrne. 2006. A weighted fi-
nite state transducer translation template model for sta-
tistical machine translation. Journal of Natural Lan-
guage Engineering, 12(1):35?75.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. ACL.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In Proc. ACL.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. EMNLP.
Z. Li and S. Khudanpur. 2009. Efficient extraction of
oracle-best translations from hypergraphs. In Proc.
NAACL.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical Programming B, 45(3):503?528.
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based transla-
tion. In Proceedings of ACL-08: HLT, pages 192?199,
Columbus, Ohio, June. Association for Computational
Linguistics.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
S. Petrov and D. Klein. 2008. Discriminative log-linear
grammars with latent variables. In Advances in Neu-
ral Information Processing Systems 20 (NIPS), pages
1153?1160.
G. Satta. submitted. Translation algorithms by means of
language intersection.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL,
pages 213?220.
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
opinion pools for conditional random fields. In Proc.
ACL.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proceedings of LREC 2002,
pages 147?152, Las Palmas, Spain.
R. Tromble and J. Eisner. 2009. Learning linear or-
der problems for better translation. In Proceedings of
EMNLP 2009.
H. Tseng, P. Chang, G. Andrew, D. Jurafsky, and C. Man-
ning. 2005. A conditional random field word seg-
menter. In Fourth SIGHAN Workshop on Chinese Lan-
guage Processing.
C. Wang, M. Collins, and P. Koehn. 2007. Chinese syn-
tactic reordering for statistical machine translation. In
Proc. EMNLP.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?404.
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
dependency parser to improve SMT for subject-object-
verb languages. In Proc. NAACL, pages 245?253.
K. Yamada and K. Knight. 2001. A syntax-based statis-
tical translation model. In Proc. ACL.
R. Zens and H. Ney. 2006. Discriminative reordering
models for statistical machine translation. In Proc. of
the Workshop on SMT.
A. Zollmann and A. Venugopal. 2006. Syntax aug-
mented machine translation via chart parsing. In Proc.
of the Workshop on SMT.
A. Zollmann, A. Venugopal, F. Och, and J. Ponte. 2008.
A systematic comparison of phrase-based, hierarchical
and syntax-augmented statistical MT. In Proc. Coling.
866
Proceedings of the NAACL HLT 2010: Tutorial Abstracts, pages 1?2,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Data-Intensive Text Processing with MapReduce
Jimmy Lin and Chris Dyer
University of Maryland, College Park
{jimmylin,redpony}@umd.edu
1. Overview
This half-day tutorial introduces participants to data-intensive text
processing with the MapReduce programming model [1], using the
open-source Hadoop implementation. The focus will be on scalability
and the tradeoffs associated with distributed processing of large
datasets. Content will include general discussions about algorithm
design, presentation of illustrative algorithms, case studies in HLT
applications, as well as practical advice in writing Hadoop programs
and running Hadoop clusters.
2. Intended Audience
The tutorial is targeted at any NLP researcher who is interested in
data-intensive processing and scalability issues in general. No
background in parallel or distributed computing is necessary, but a
prior knowledge of HLT is assumed.
3. Course Objectives
* Acquire understanding of the MapReduce programming model and how it
  relates to alternative approaches to concurrent programming.
* Acquire understanding of how data-intensive HLT problems (e.g., text
  retrieval, iterative optimization problems, and graph algorithms)
  can be solved using MapReduce.
* Acquire understanding of the tradeoffs involved in designing
  MapReduce algorithms and awareness of associated engineering issues.
4. Tutorial Topics
The following represents a tentative list of topics that will be covered:
* Introduction to parallel and distributed processing
* Introduction to MapReduce
* Tradeoffs and issues in algorithm design
* Simple counting applications (e.g., relative frequency estimation)
* Applications to inverted indexing and text retrieval
* Applications to graph algorithms
1
* Applications to iterative optimization algorithms (e.g., EM)
* Case study in machine translation
* Tips and tricks in writing Hadoop programs
* Practical issues in running Hadoop clusters
5. Instructor Bios
Jimmy Lin is an Associate Professor in the iSchool at the University
of Maryland, College Park. He joined the faculty in 2004 after
completing his Ph.D. in Electrical Engineering and Computer Science at
MIT. Dr Lin's research interests lie at the intersection of natural
language processing and information retrieval. He leads the University
of Maryland's effort in the Google/IBM Academic Cloud Computing
Initiative. Dr. Lin has taught two semester-long Hadoop courses and
has given numerous talks and tutorials about MapReduce to a wide
audience.
Chris Dyer is a Ph.D. student at the University of Maryland, College
Park, in the Department of Linguistics.  His current research
interests include statistical machine translation, machine learning,
and the relationship between artificial language processing systems
and the human linguistic processing system. He has served on program
committees for AMTA, ACL, COLING, EACL, EMNLP, NAACL, ISWLT, and the
ACL Workshops on Machine translation, and is one of the developers of
the Moses open source machine translation toolkit. He has practical
experience solving NLP problems with both the Hadoop MapReduce
framework and Google's MapReduce implementation, which was made
possible by an internship with Google Research in 2008.
References
[1] Dean, Jeffrey and Sanjay Ghemawat. MapReduce: Simplified Data
Processing on Large Clusters. Proceedings of the 6th Symposium on
Operating System Design and Implementation (OSDI 2004), p. 137-150,
2004, San Francisco, California.
[2] Jimmy Lin. Exploring Large-Data Issues in the Curriculum: A Case
Study with MapReduce. Proceedings of the Third Workshop on Issues in
Teaching Computational Linguistics (TeachCL-08) at ACL 2008, p. 54-61,
2008, Columbus, Ohio.
2
Proceedings of NAACL-HLT 2013, pages 248?258,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Large-Scale Discriminative Training for Statistical Machine Translation
Using Held-Out Line Search
Jeffrey Flanigan Chris Dyer Jaime Carbonell
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jflanigan,cdyer,jgc}@cs.cmu.edu
Abstract
We introduce a new large-scale discrimina-
tive learning algorithm for machine translation
that is capable of learning parameters in mod-
els with extremely sparse features. To ensure
their reliable estimation and to prevent over-
fitting, we use a two-phase learning algorithm.
First, the contribution of individual sparse fea-
tures is estimated using large amounts of par-
allel data. Second, a small development cor-
pus is used to determine the relative contri-
butions of the sparse features and standard
dense features. Not only does this two-phase
learning approach prevent overfitting, the sec-
ond pass optimizes corpus-level BLEU of the
Viterbi translation of the decoder. We demon-
strate significant improvements using sparse
rule indicator features in three different trans-
lation tasks. To our knowledge, this is the
first large-scale discriminative training algo-
rithm capable of showing improvements over
the MERT baseline with only rule indicator
features in addition to the standard MERT fea-
tures.
1 Introduction
This paper is about large scale discriminative
training of machine translation systems. Like
MERT (Och, 2003), our procedure directly optimizes
the cost of the Viterbi output on corpus-level met-
rics, but does so while scaling to millions of features.
The training procedure, which we call the Held-Out
Line Search algorithm (HOLS), is a two-phase iter-
ative batch optimization procedure consisting of (1)
a gradient calculation on a differentiable approxima-
tion to the loss on a large amount of parallel training
data and (2) a line search (using the standard MERT
algorithm) to search in a subspace defined by the
gradient for the weights that minimize the true cost.
While sparse features are successfully used in
many NLP systems, such parameterizations pose a
number of learning challenges. First, since any one
feature is likely to occur infrequently, a large amount
of training data is necessary to reliably estimate their
weights. Therefore, we use the full parallel train-
ing data (rather than a small development set) to
estimate the contribution of the sparse features in
phase 1. Second, sparse features can lead to overfit-
ting. To prevent this from hurting our model?s ability
to generalize to new data, we do two things. First,
we use ?grammar and language model folds? (trans-
lation grammars and language models built from
other portions of the training data than are being
used for discriminative training), and second, we
run the phase 2 line search on a held-out develop-
ment set. Finally, since our algorithm requires de-
coding the entire training corpus, it is desirable (on
computational grounds) to only require one or two
passes through the training data. To get the most out
of these passes, we rescale features by their inverse
frequency which improves the scaling of the opti-
mization problem. In addition to learning with few
passes through the training data, the HOLS algorithm
has the advantage that it is easily parallelizable.
After reviewing related work in the next section,
we analyze two obstacles to effective discriminative
learning for machine translation: overfitting (since
both rules and their weights must be learned, if they
are learned together degenerate solutions that fail to
generalize are possible) and poor scaling (since MT
248
decoding is so expensive, it is not feasible to make
many passes through large amounts of training data,
so optimization must be efficient). We then present
the details of our algorithm that addresses these is-
sues, give results on three language pairs, and con-
clude.
2 Related Work
Discriminative training of machine translation sys-
tems has been a widely studied problem for the
last ten years. The pattern of using small, high-
quality development sets to tune a relatively small
number of weights was established early (Och and
Ney, 2002; Och, 2003). More recently, standard
structured prediction algorithms that target linearly
decomposable approximations of translation qual-
ity metrics have been thoroughly explored (Liang et
al., 2006; Smith and Eisner, 2006; Watanabe et al,
2007; Rosti et al, 2010; Hopkins and May, 2011;
Chiang, 2012; Gimpel and Smith, 2012; Cherry and
Foster, 2012; Saluja et al, 2012). These have with-
out exception used sentence-level approximations of
BLEU to determine oracles and update weights using
a variety of criteria and with a variety of different
theoretical justifications.
Despite advancements in discriminative training
for machine translation, large-scale discriminative
training with rule indicator features has remained
notoriously difficult. Rule indicator features are an
extremely sparse and expressive parameterization of
the translation model: every rule has a feature, each
of which has its own separately tuned weight, which
count how often a specific rule is used in a trans-
lation. Early experiments (Liang et al, 2006) used
the structured perceptron to tune a phrase-based sys-
tem on a large subset of the training data, show-
ing improvements when using rule indicator fea-
tures, word alignment features, and POS tag fea-
tures. Another early attempt (Tillmann and Zhang,
2006) used phrase pair and word features in a block
SMT system trained using stochastic gradient de-
scent for a convex loss function, but did not compare
to MERT. Problems of overfitting and degenerate
derivations were tackled with a probabilistic latent
variable model (Blunsom et al, 2008) which used
rule indicator features yet failed to improve upon
the MERT baseline for the standard Hiero features.
Techniques for distributed learning and feature se-
lection for the perceptron loss using rule indicator,
rule shape, and source side-bigram features have re-
cently been proposed (Simianer et al, 2012), but no
comparison to MERT was made.
3 Difficulties in Large-Scale Training
Discriminative training for machine translation is
complicated by several factors. First, both transla-
tion rules and feature weights are learned from par-
allel data. If the same data is used for both tasks,
overfitting of the weights is very possible.1 Second,
the standard MT cost function, BLEU (Papineni et
al., 2002), does not decompose additively over train-
ing instances (because of the ?brevity penalty?) and
so approximations are used?these often have prob-
lems with the length (Nakov et al, 2012). Finally,
state-of-the-art MT systems make extensive good
use of ?dense? features, such as the log probabil-
ity of translation decisions under a simpler gener-
ative translation model. Our goal is to begin to
use much sparser features without abandoning the
proven dense features; however, extremely sparse
features leads to problems of scaling in the optimiza-
tion problem as we will show.
3.1 Training Data and Overfitting
One of the big questions in discriminative train-
ing of machine translation systems is why standard
machine learning techniques can perform so poorly
when applied to large-scale learning on the train-
ing data. Figure 1 shows a good example of this.
The structured SVM (Tsochantaridis et al, 2004;
Cherry and Foster, 2012) was used to learn the
weights for a Chinese-English Hiero system (Chi-
ang, 2005) with just eight features, using stochastic
gradient descent (SGD) for online learning (Bottou,
1998; Bottou, 2010). The weights were initialized
from MERT values tuned on a 2k-sentence dev set
(MT06), and the figure shows the progress of the on-
line method during a single pass through the 300k-
sentence Chinese-English FBIS training set.
As the training progresses in Figure 1, BLEU
scores on the training data go up, but scores on the
1Previous work has attempted to mitigate the risk of overfit-
ting through careful regularization (Blunsom et al, 2008; Simi-
aner et al, 2012).
249
0 50000 150000 2500002
6
30
34
BLE
U
Figure 1: Progress of the online SVM training
method after each training instance on FBIS dataset.
The solid line is BLEU on the test set, training set is
the dashed line, and the dev set is dotted.
dev and test sets go down. If we hope to apply dis-
criminative training techniques for not eight but mil-
lions of features on the training data, we must find a
way to prevent this overfitting.
We suggest that an important reason why overfit-
ting occurs is that the training data is used not only to
tune the system but also to extract the grammar, and
the target side is included in the data used to build
the language model. To test this hypothesis, we
compare tuning using three different dev sets: 1000
sentences from the standard 4-reference MT06 dev
set (Dev1000), a random selection of 1000 sentences
that overlap with the corpus used to extract transla-
tion rules (In1000), and 1000 sentences that came
from the training data but were then excluded from
rule extraction (Out1000). We run MERT on each of
these and evaluate. For evaluation we compare three
different sets: a random 1000 sentences from the
training corpus that was used to create the grammars
but which do not overlap with In1000 (Train1000),
the 1000 sentence dev set (Dev1000), and the stan-
dard 4-reference MT02-03 test set (Test). The en-
tire experiment (including selection of the 1000 sen-
tences) was replicated 5 times.
Table 1 shows the results, averaging over repli-
cations. Out1000 gives much higher scores on the
testing data, validating our hypothesis that tuning on
data used to build the LM and grammar can lead to
overfitting. However, the results also show that tun-
ing on the training data, even when it is held-out, can
still lead to a small reduction in translation quality.
One possible reason is that, unlike the training data
which may come from various domains, the dev data
is in the same domain as the test data and is typically
of higher quality (e.g., it has multiple references).
Table 1: MERT on Zh-En FBIS
Tuning Set Train1000 Dev1000 Test
Dev1000 32.2?1.1 30.2?.1 34.1?.3
In1000 37.0?1.2 25.7?.7 30.1?.6
Out1000 34.9?.8 29.0?.4 33.6?.5
3.2 Poor Scaling
When features occur with different frequencies,
changing the weights of more frequent features has
a larger effect than changing the weights of less fre-
quent features.2 An example of frequent features
that have a large impact on the translation quality are
the language model and translation model features.
These features are non-zero for every sentence, and
changing their weights slightly has a large impact on
translation output. In contrast, changing the weight
drastically for a feature that is non-zero for only one
out of a million sentences has very little effect on
translation metrics. The sensitivity of the translation
output to some feature weights over others was also
pointed out in a recent paper (Chiang, 2012).
When the objective function is more sensitive
in some dimensions than others, the optimization
problem is said to be poorly scaled (Nocedal and
Wright, 2000), and can slow down the convergence
rate for some optimizers. A typical fix is to rescale
the dimensions, as we will do in Section 5.2.
To verify that BLEU is poorly scaled with respect
to weights of rule indicator features, we look at the
effect of changing the weights for individual rules.
We vary the feature weights for four randomly cho-
sen frequent rules and four randomly chosen infre-
quent rules on our FBIS dev set (Figure 2). One
can think of this plot as a ?cross-section? of the
BLEU score in the direction of the feature weight.
The dense features are set to MERT-tuned values
which are normalized to one. All other rule indi-
cator features are set to zero, except the rule fea-
ture weight that is varied. The frequent features
2By the ?frequency of a feature? we mean this: given a set of
input instances, how many input instances the feature is nonzero
in the space of possible outputs for that input.
250
were selected randomly from the 20 most common
rule indictor features in the n-best lists on the dev
set, and the infrequent features were selected from
the features that only occurred once in these n-best
lists. The plots indicate that the BLEU score is
?2 ?1 0 1 22
7.0
28.0
29.0
30.0
Weight
BLE
U
(a) Four representative frequent sparse features.
?10 ?5 0 5 1030
.100
30.1
05
30.1
10
30.1
15
30.1
20
Weight
BLE
U
(b) Four representative infrequent sparse features
Figure 2: The effect of varying weights for rule indicator
features on the BLEU score. Note the difference of scale
on the y axis.
poorly scaled for rule feature weights. Changing the
weights for one of the common features changes the
BLEU score by almost 2.5 BLEU points, while for
the infrequent features the BLEU score changes by
at most .02 BLEU points. We take this as a sign that
gradient descent based optimizers for machine trans-
lation with rule features could be slow to converge
due to poor scaling, and that rescaling will improve
convergence.
3.3 Sentence Level Approximations to BLEU
Finally, we note that discriminative training methods
often use a sentence level approximation to BLEU. It
has been shown that optimizing corpus level BLEU
versus sentence level BLEU can lead to improve-
ments of up to nearly .4 BLEU points on the test
set (Nakov et al, 2012). Possible fixes to this prob-
lem include using a proper sentence level metric
such a METEOR (Denkowski and Lavie, 2011) or a
pseudo-corpus from the last few updates (Chiang et
al., 2008). However, in light of the result from sec-
tion 3.1 that tuning on the dev set is still better than
tuning on a held-out portion of the training data, we
observe that tuning a corpus level metric on a high-
quality dev set from the same domain as the test set
probably leads to the best translation quality. At-
tempts to improve upon this strong baseline lead us
to the development of the HOLS algorithm which we
describe next.
4 Held-Out Line Search Algorithm
In this section we give the details of the learning al-
gorithm that we developed for use in large-scale dis-
criminative training for machine translation, which
we call the Held-Out Line Search algorithm (abbre-
viated HOLS). It optimizes millions of features using
evidence from the full set of parallel training data
to obtain optimal predictive performance on a sec-
ondary development set.
The learning algorithm is a batch optimizer where
each iteration has two phases: a gradient calcula-
tion phase and a line search phase. In the gradient
calculation phase, a surrogate loss function is used
to compute a gradient for the feature weights. The
gradient is computed over a subset of the training
data. In the line search phase, a separate optimizer
(MERT) is used to search along this gradient to opti-
251
mize the evaluation score of the one-best prediction
of a translation system on a secondary development
set.3 The secondary dev set is a crucial aspect of
the algorithm that helps reduce overfitting (we will
demonstrate this in the experiments section).
During the line search phase we allow some of
the feature weights to be adjusted independently of
the line search. We will call the features we opti-
mize independently the dense features, and the fea-
tures we include in the line search the sparse fea-
tures.4 The feature vector space V is the direct sum
V = Vd ? Vs, where Vd is the vector space of
the dense features and Vs is the vector space of the
sparse features. The feature and weight vectors de-
compose as ~f = ~fd + ~fs and ~w = ~wd + ~ws. ~fd and
~wd are in the dense vector space, and the ~fs and ~ws
are in the sparse vector space.
In the gradient phase, we calculate a gradient of
the surrogate loss function and project it onto the
subspace of the sparse features. Let Ps be the pro-
jection operator onto Vs. Then the gradient projected
onto the sparse feature space is
~g = Ps?~wL?(~w,Dg)
where Dg is the subset of the training data used to
calculate this gradient, and L? is the surrogate loss
function. This just sets the dense components of the
gradient of L? to zero.
In the line search phase, we use a separate opti-
mizer to optimize the weights for the dense features
and the stepsize ?. Let L be the loss function we
wish to minimize, then
(~w?d, ?
?) = arg min
~wd,?
L(~wd + ~ws + ?~g,Dl)
Note ~ws is held fixed from the previous iteration. Dl
is the portion of the training data which is used in
the line search phase, and must not overlap with Dg
used in the gradient calculation phase.5
After the line search, the dense weights are up-
dated to ~w?d, and the sparse weights are updated with
~ws ? ~ws + ??~g. The process repeats for another
iteration as desired (or until convergence).
3While we use BLEU any loss function whose sufficient
statistics decompose over training instances could be used.
4The split over the features does not have to be done this
way in practice.
5L(~w?d, ?
?,Dl) can be thought of as unbiased or more accu-
rately less biased estimator of expected loss when Dl?Dg = ?.
5 Procedure for Large-Scale Training
Now that we have described the HOLS algorithm in
general, we next describe how to apply it to large-
scale training of machine translation systems with
millions of features. We find that it is necessary to
use disjoint sets of training instances for grammar
extraction and gradient estimation (?5.1) and to deal
with the poor scaling of the optimization problem
(?5.2).
5.1 Grammar and Language Model Folds
To address the problem of overfitting on the train-
ing data, we split the training data into n-folds, and
extract grammars for each fold using the data from
the other n? 1 folds. Similarly, we build a language
model for each fold using a target language mono-
lingual corpus and the target side of the training data
from the other n ? 1 folds. Whenever we decode a
sentence from the training data, we use the gram-
mar and language model for the appropriate fold.
This ensures that a sentence is never decoded using a
grammar or language model it helped build, thereby
reducing the overfitting effect demonstrated in ?3.1.
To perform the training, the HOLS algorithm is
used on the training data. In our experiments, only
1-2 passes over the training data are necessary for
significant gains. Data from one of the grammar
folds is used for the line search, and the rest of the
training data is used to calculate the gradient.
The procedure is iterative, first decoding training
data to obtain a gradient, and then performing a line
search with data from a held-out grammar fold. In-
stead of decoding the whole set of sentences used for
the gradient updates at once, one can also decode a
portion of the data, do a gradient update, and then
continue the next iteration of HOLS on the remain-
ing data before repeating.
The last line search of the HOLS algorithm is done
using dev data, rather than training data. This is be-
cause the dev data is higher quality, and from Table
1 we can see that tuning on dev data produces bet-
ter results than tuning on training data (even if the
training data has been held out from the grammar
process). The initial weights are obtained by run-
ning MERT on a subset of the one of the grammar
folds.
If one has an existing implementation of an op-
252
timizer for the loss function used during the line
search (in our case MERT), it can be used to perform
the line search. This is done simply by calling MERT
with two extra features in addition to the dense fea-
tures and omitting the sparse features.
To see how, notice that the feature weights
during the line search are decomposed as ~w =
~wdense + ~wsparse + ?~g where ~g is in the sparse
feature subspace, so the model score decomposes
as score(x, y) = ~wd ? ~fd(x, y) + ~ws ? ~fs(x, y) +
?~g ? ~fs(x, y) where x is the input translation, y is
the output translation and derivation. If we cre-
ate two new features f1(x, y) = ~ws ? ~fs(x, y) and
f2(x, y) = ~g ? ~fs(x, y) then the score can be written
score(x, y) = ~wd ? ~fd(x, y)
+f1(x, y) + ?f2(x, y)
= (~wd, 1, ?) ? (~fd, f1, f2)
Thus we can do the line search simply by calling
MERT with the features (~fd, f1, f2). 6
In summary our training algorithm is as follows:
1) split the training data into n-folds (we use n = 5),
2) initialize the dense weights to MERT values, 3)
decode some or all the data in 4 of the 5 folds to get
a gradient, 4) condition as in ?5.2 (see below), 5) run
MERT on a 10k subset of the remaining fold to do the
line search, 6) repeat steps 3-4 until convergence or
stop as desired, and 7) run MERT on the normal dev
set as a final step. We only run MERT on a 10k subset
of one of the folds so it does not require running
MERT on an entire fold.
In the special case where just one iteration of
HOLS is performed, the procedure is very simple:
decode the training data to get a gradient, include
the components of the gradient as an extra feature
f2 in addition to the dense features, and tune on a
dev set using MERT.
5.2 Conditioning
To address the problem of poor scaling, we use a
simple strategy of rescaling each component of the
gradient based on how frequent the feature is. We
call this process ?conditioning.? For each feature,
we simply divide the corresponding dimension of
6We could constrain the weight for f1 to be 1, but this is not
necessary since since MERT is invariant to the overall scale of
the weights.
the gradient by the number of n-best lists in which
the feature was non-zero in.
The necessity for conditioning is evident when we
run the HOLS algorithm as detailed so far on the
training data without conditioning. On subsequent
iterations, we observe that the features with the high-
est component of the gradient oscillate between iter-
ations, but the rest of the feature gradients stay the
same.
Based on our knowledge that the optimization
problem was poorly scaled, we divided by the fre-
quency of the feature. We can give the following
heuristic justification for our method of condition-
ing. For the ith feature weight, we will take a step
?wi. Assume that we want to take the step ?wi pro-
portional to the average gradient g?i calculated from
n-best lists in which the feature is non-zero. In other
words, we want ?wi = ?g?i. Let gi be the total
gradient calculated by adding the gradients over all
n-best lists (i.e. summing over training examples
in the corpus). For a feature that is nonzero in ex-
actly ni n-best lists, the gradient from each example
will have been added up ni times, so the total gra-
dient gi = nig?i. Therefore we should take the step
?wi = ?gi/ni. In other words, we rescale each
component gi of the gradient by 1/ni before taking
the gradient step.
We can relate this argument back to the oscillation
we observed of the rule feature weights. For rules
that are used a thousand times more often than the
average rule, the corresponding component of the
gradient is roughly a thousand times larger. But that
does not indicate that the adjustment ?wi to the rule
weight should be a thousand times larger in each it-
eration.
6 Experiments
We evaluate and analyze the performance of our
training method with three sets of experiments. The
first set of experiments compares HOLS to other
tuning algorithms used in machine translation in a
medium-scale discriminative setting. The second set
looks in detail at HOLS for large scale discriminative
training for a Chinese-English task. The third set
looks at two other languages.
All the experiments use a Hiero MT system with
rule indicator features for the sparse features and the
253
Table 2: Corpora
Language Corpus Sentences Tokens
Source Target
En Gigaword 24M 594M
Ar-En Train 1M 7M 31M
Dev (MT06) 1797 13K 236K
MT05 1,056 7K 144K
MT08nw 813 5K 116K
MT05wb 547 5K 89K
Mg-En Train 89K 2.1M 1.7M
Dev 1,359 34K 28K
Test 1,133 29K 24K
Zh-En Train (FBIS) 302K 1M 9.3M
Dev (MT06) 1,664 4K 192K
Test (MT02-03) 1,797 5K 223K
MT08 1,357 4K 167K
following 8 dense features: LM, phrasal and lexi-
cal p(e|f) and p(f |e), phrase and word penalties,
and glue rule. The total number of features is 2.2M
(Mg-En), 28.8M (Ar-En), and 10.8M (Zh-En). The
same features are used for all tuning methods, ex-
cept MERT baseline which uses only dense features.
Although we extract different grammars from vari-
ous subsets of the training corpus, word alignments
were done using the entire training corpus. We use
GIZA++ for word alignments (Och and Ney, 2003),
Thrax (Weese et al, 2011) to extract the grammars,
our decoder is cdec (Dyer et al, 2010) which uses
KenLM (Heafield, 2011), and we used a 4-gram LM
built using SRILM (Stolcke, 2002). Our optimizer
uses code implemented in the pycdec python inter-
face to cdec (Chahuneau et al, 2012). To speed up
decoding, for each source RHS we filtered the gram-
mars to the top 15 rules ranked by p(e | f). Statistics
about the datasets we used are listed in Table 2.
We use the ?soft ramp 3? loss function (Gimpel,
2012; Gimpel and Smith, 2012) as the surrogate loss
function for calculating the gradient in HOLS. It is
defined as
L? =
n?
i=1
[
? log
?
y?Gen(xi)
e~w?
~f(xi,y)?cost(yi,y)
+ log
?
y?Gen(xi)
e~w?
~f(xi,y)+cost(yi,y)
]
where the sum over i ranges over training exam-
ples, Gen(x) is the space of possible outputs and
derivations for the input x, and cost(yi, y) is add one
smoothing sentence level BLEU.7
Except where noted, all experiments are repeated
5 times and results are averaged, initial weights for
the dense features are drawn from a standard nor-
mal, and initial weights for the sparse features are
set to zero. We evaluate using MultEval (Clark et
al., 2011) and report standard deviations across opti-
mizer runs and significance at p = .05 using MultE-
val?s built-in permutation test. In the large-scale ex-
periments for HOLS, we only run the full optimizer
once, and report standard deviations using multiple
runs of the last MERT run (i.e. the last line search on
the dev data).
6.1 Comparison Experiments for ZH-EN
Our first set of experiments compares the perfor-
mance of the proposed HOLS algorithm to learn-
ing algorithms popularly used in machine transla-
tion on a Chinese-English task. We also compare to
a close relative of the HOLS algorithm: optimizing
the soft ramp 3 loss directly with online stochastic
gradient descent and with conditioning. As we will
see, SGD SOFTRAMP3 performs significantly worse
than HOLS, despite both algorithms optimizing sim-
ilar loss functions.
In the experiments in this section, we do not use
the full version of the training setup described in
?5 since we wish to compare to algorithms that do
not necessarily scale to large amounts of training
data. We therefore use only one fifth of the train-
ing data for learning the weights for both the dense
and sparse features.
In this section we refer to the subset of the train-
ing data used to learn the weights as the tuning set
(Tune). The grammar and LM are built using the
training data that is not in the tuning set (the LM also
includes the English monolingual corpus), and the
weights for the features are tuned using the tuning
set. This is similar to the typical train-dev-test split
commonly used to tune machine translation systems,
except that the tuning set is much larger (60k sen-
tence pairs versus the usual 1k-2k) and comes from
a random subset of the training data rather than a
7We found this loss function to work well, but other ?soft?
loss functions (Gimpel, 2012; Gimpel and Smith, 2012) also
work. Gen(x) is restricted to a k-best size of 1000. Following
(Gimpel, 2012) cost(yi, y) is multiplied by a factor of 20.
254
Table 3: Comparison Experiments for Zh-En
Algorithm Tune MT08 Runtime
MERT 22.1?.1 23.1?.1 6 hours
PRO 23.8?.05 23.6?.1 2 weeks
MIRA 21.7?.1 22.5?.1 19 hours
SOFTRAMP3 21.5?.3 22.3?.3 29 hours
HOLS 22.3?.1 23.4?.1 10 hours
HILS 24.3?.2 22.4?.1 10 hours
specialized development set.
We compare MERT, PRO (Hopkins and May,
2011), MIRA (Chiang, 2012), SOFTRAMP3, HOLS,
and a variant of HOLS which we call HILS (discussed
below). For HOLS, we used 10k of the 60k tun-
ing set for the line search, and the rest of the tun-
ing set was used for calculating the gradient. For
HILS (?Held-In? Line Search), the full 60k tuning
set was used to calculate the gradient, but the line
search was on a 10k subset of that set. For MERT,
we used a 10k subset of the tuning data because it
takes a long time to run on large datasets, and it only
has the eight dense features and so does not need the
entire 60k tuning set. All the subsets are drawn ran-
domly. Conditioning was performed only for HOLS,
HILS, and SOFTRAMP3 because conditioning would
affect the regularizer for PRO and require modifica-
tions to the MIRA algorithm. To do the condition-
ing for SOFTRAMP3 we used rule count during ex-
traction of the grammar and not the frequency in
the n-best lists because the online nature of SOFT-
RAMP3 prevents us from knowing how frequent a
rule will be (and the dense features are conditioned
using the corpus size). We chose MIRA?s best learn-
ing rate (? = .001) from {.1, .01, .001}, used de-
fault settings for PRO in cdec, and for SOFTRAMP3
we used the same loss function as HOLS but included
an L2 regularizer of strength .001 and used a step-
size of 1 (which was scaled because of condition-
ing). To remedy problems of length bias for sentence
level BLEU, we used brevity penalty smoothed and
grounded BLEU+1 for sentence level scores (Nakov
et al, 2012). Tuning was repeated four times with
different initial weights, except for PRO which we
only ran three times (due to training costs). The ini-
tial weights for MERT were drawn from a standard
normal distribution, and final MERT weights were
used as the initial weights for the dense features for
the other algorithms. Initial weights for the sparse
features were set to zero. For HOLS, and HILS, tun-
ing set BLEU scores were evaluated on the set that
the line search was run on. We also report run times
for 8 threads on an Opteron 6220 processor.8
The results are shown in Table 3. PRO and HOLS
are a statistically significant improvement upon the
MERT baseline on the MT08 test data, but MIRA,
SOFTRAMP3, and HILS are not.
HILS dramatically overfits the tuning set, while
HOLS does not, justifying the use of a held-out
dataset for the line search. SOFTRAMP3 performs
significantly worse than HOLS on the test set. PRO is
a promising training algorithm, but does not scale to
the full FBIS corpus because it requires many itera-
tions.
6.2 Full ZH-EN and Ablation Experiments
This set of experiments evaluates the performance
of the full HOLS algorithm described in ?5 for
large-scale discriminative training on the full FBIS
Chinese-English dataset. Since this is a relatively
small and widely studied dataset, we also investigate
what happens if different aspects of the procedure
are omitted.
Table 4 gives the results. The number of updates
is the number of times the HOLS line search opti-
mizer is run (gradient updates). For 2 passes, 4 up-
dates, a line search is performed after a half pass
through the training data, which is repeated four
times for a total of two passes.
Using just one pass through the training data and
8Standard MIRA and SGD SOFTRAMP3 are not paralleliz-
able and only use a single thread. All of these algorithms were
run for one iteration, except for MERT which ran for at least
seven iterations, and PRO which we stopped after 20 iterations.
Table 4: Full-scale Chinese-English and Ablation
Experiments
Configuration Dev Test
MERT Baseline 29.9?.3 34.0?.8
2 Pass, 4 updates 31.1?.2 35.1?.4
1 Pass, 1 update 30.7?.1 34.6?.5
?Folds 30.0?.2 34.0?.4
?Conditioning 30.1?.1 34.2?.2
255
Table 5: Arabic-English
System Dev (MT06) MT05 MT08(nw) MT08(wb)
MERT Baseline 39.2?.4 50.3?.4 45.2?.2 29.4?.14
HOLS 1 Pass, 2 updates 39.9?.9 51.2?.4 45.8?.4 30.0?.4
?BLEU +.7 +.9 +.6 +.6
Table 6: Malagasy-English
System Dev Test
MERT Baseline 19.8?.3 17.7?.2
HOLS 1 Pass, 1 update 20.5?.1 18.4?.2
?BLEU +.7 +.7
one gradient update, HOLS improves upon the MERT
baseline by .6 BLEU points, which is a statistically
significant improvement. With 2 passes through the
training data and 4 gradient updates, HOLS performs
even better, obtaining a 1.1 BLEU point improve-
ment over the baseline and is also statistically signif-
icant. With 16 threads, 1 pass, 1 update completed
in 9 hours, and 2 pass, 4 updates, completed in 40
hours. The medium-scale PRO setup in ?6.1 obtains
a result of 34.4? .1 on this test set, which is a statis-
tically significant improvement of .4 BLEU points
over the MERT baseline but does not beat the large-
scale HOLS results.
Is folding and conditioning necessary? We ex-
periment with what happens if grammar and LM
folds are not used and if conditioning is not done.
?Folds denotes 1 pass 1 update without folds, and
?Conditioning denotes 1 pass 1 update without con-
ditioning. We can see that both these steps are im-
portant for the training procedure to work well.
The decrease in performance of the training pro-
cedure without folds or conditioning is dramatic but
not too surprising. With just one gradient update,
one would expect conditioning to be very important.
And from the lessons learned in section 3.1, one
would also expect the procedure to perform poorly
or even worse than the MERT baseline without gram-
mar or LM folds. But because HOLS runs MERT on
the dev data for the last line search, it is almost im-
possible for HOLS to be worse than the MERT base-
line. (This, in fact, was part of our motivation when
we originally attempted the HOLS algorithm.)
6.3 Other Language Pairs
The last set of experiments looks at the performance
of the learning algorithm for two other languages
and data scenarios for one pass through the training
data. Using the same setup for large-scale discrimi-
native training as before, we apply the training pro-
cedure to a large data scenario Arabic-English task
and a small data scenario Malagasy-English task
(Tables 5 and 6). The training procedure gives statis-
tically significant improvements over the baseline by
.6 to .9 BLEU for Arabic, and a statistically signif-
icant improvement of .7 BLEU for Malagasy. With
16 threads, the runtime was 44 hours for Arabic and
5 hours for Malagasy.
7 Conclusion
We have explored the difficulties encountered
in large-scale discriminative training for machine
translation, and introduced a learning procedure de-
signed to overcome them and scale to large corpora.
We leave to future work to experiment with feature
sets designed for the large-scale discriminative set-
ting. In particular, we hope this framework will fa-
cilitate incorporation of richer linguistic knowledge
into machine translation.
Acknowledgments
This work was sponsored by the U. S. Army Research
Laboratory and the U. S. Army Research Office un-
der contract/grant number W911NF-10-1-0533. Jeffrey
Flanigan would like to thank his co-advisor Lori Levin
for support and encouragement during this work.
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL-HLT.
Le?on Bottou. 1998. Online algorithms and stochastic ap-
proximations. In David Saad, editor, Online Learning
256
and Neural Networks. Cambridge University Press,
Cambridge, UK. revised, oct 2012.
Le?on Bottou. 2010. Large-scale machine learning with
stochastic gradient descent. In Yves Lechevallier and
Gilbert Saporta, editors, Proceedings of the 19th In-
ternational Conference on Computational Statistics
(COMPSTAT?2010), pages 177?187, Paris, France,
August. Springer.
V. Chahuneau, N. A. Smith, and C. Dyer. 2012. pycdec:
A python interface to cdec. The Prague Bulletin of
Mathematical Linguistics, 98:51?61.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Proc.
of NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In In ACL, pages
263?270.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal of
Machine Learning Research, pages 1159?1187.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statisti-
cal machine translation: controlling for optimizer in-
stability. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume
2, HLT ?11, pages 176?181, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic Metric for Reliable Optimization and Eval-
uation of Machine Translation Systems. In Proceed-
ings of the EMNLP 2011 Workshop on Statistical Ma-
chine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. of NAACL.
K. Gimpel. 2012. Discriminative Feature-Rich Modeling
for Syntax-Based Machine Translation. Ph.D. thesis,
Carnegie Mellon University.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proc. of EMNLP.
Percy Liang, Alexandre Bouchard-co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In In Proceedings of
the Joint International Conference on Computational
Linguistics and Association of Computational Linguis-
tics (COLING/ACL, pages 761?768.
Preslav Nakov, Francisco Guzma?n, and Stephan Vogel.
2012. Optimizing for sentence-level bleu+1 yields
short translations. In Martin Kay and Christian Boitet,
editors, COLING, pages 1979?1994. Indian Institute
of Technology Bombay.
Jorge Nocedal and Stephen J. Wright. 2000. Numerical
Optimization. Springer.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Comput. Linguist., 29(1):19?51, March.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Antti-Veiko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2010. BBN system description for
WMT10 system combination task. In Proc. WMT.
Avneesh Saluja, Ian Lane, and Joy Zhang. 2012. Ma-
chine Translation with Binary Feedback: a large-
margin approach. In Proceedings of The Tenth Bien-
nial Conference of the Association for Machine Trans-
lation in the Americas, San Diego, CA, July.
Patrick Simianer, Chris Dyer, and Stefan Riezler. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT. In
Proc. ACL.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
ACL.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. pages 901?904.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of the 21st International Conference on
257
Computational Linguistics and the 44th annual meet-
ing of the Association for Computational Linguistics,
ACL-44, pages 721?728, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first inter-
national conference on Machine learning, ICML ?04,
pages 104?, New York, NY, USA. ACM.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. EMNLP-CoNLL.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: syntax-based machine translation with the thrax
grammar extractor. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, WMT ?11,
pages 478?484, Stroudsburg, PA, USA. Association
for Computational Linguistics.
258
Proceedings of NAACL-HLT 2013, pages 380?390,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Improved Part-of-Speech Tagging for Online Conversational Text
with Word Clusters
Olutobi Owoputi? Brendan O?Connor? Chris Dyer?
Kevin Gimpel? Nathan Schneider? Noah A. Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
?Toyota Technological Institute at Chicago, Chicago, IL 60637, USA
Corresponding author: brenocon@cs.cmu.edu
Abstract
We consider the problem of part-of-speech
tagging for informal, online conversational
text. We systematically evaluate the use of
large-scale unsupervised word clustering
and new lexical features to improve tagging
accuracy. With these features, our system
achieves state-of-the-art tagging results on
both Twitter and IRC POS tagging tasks;
Twitter tagging is improved from 90% to 93%
accuracy (more than 3% absolute). Quali-
tative analysis of these word clusters yields
insights about NLP and linguistic phenomena
in this genre. Additionally, we contribute the
first POS annotation guidelines for such text
and release a new dataset of English language
tweets annotated using these guidelines.
Tagging software, annotation guidelines, and
large-scale word clusters are available at:
http://www.ark.cs.cmu.edu/TweetNLP
This paper describes release 0.3 of the ?CMU
Twitter Part-of-Speech Tagger? and annotated
data.
1 Introduction
Online conversational text, typified by microblogs,
chat, and text messages,1 is a challenge for natu-
ral language processing. Unlike the highly edited
genres that conventional NLP tools have been de-
veloped for, conversational text contains many non-
standard lexical items and syntactic patterns. These
are the result of unintentional errors, dialectal varia-
tion, conversational ellipsis, topic diversity, and cre-
ative use of language and orthography (Eisenstein,
2013). An example is shown in Fig. 1. As a re-
sult of this widespread variation, standard model-
ing assumptions that depend on lexical, syntactic,
and orthographic regularity are inappropriate. There
1Also referred to as computer-mediated communication.
ikr
!
smh
G
he
O
asked
V
fir
P
yo
D
last
A
name
N
so
P
he
O
can
V
add
V
u
O
on
P
fb
?
lololol
!
Figure 1: Automatically tagged tweet showing nonstan-
dard orthography, capitalization, and abbreviation. Ignor-
ing the interjections and abbreviations, it glosses as He
asked for your last name so he can add you on Facebook.
The tagset is defined in Appendix A. Refer to Fig. 2 for
word clusters corresponding to some of these words.
is preliminary work on social media part-of-speech
(POS) tagging (Gimpel et al, 2011), named entity
recognition (Ritter et al, 2011; Liu et al, 2011), and
parsing (Foster et al, 2011), but accuracy rates are
still significantly lower than traditional well-edited
genres like newswire. Even web text parsing, which
is a comparatively easier genre than social media,
lags behind newspaper text (Petrov and McDonald,
2012), as does speech transcript parsing (McClosky
et al, 2010).
To tackle the challenge of novel words and con-
structions, we create a new Twitter part-of-speech
tagger?building on previous work by Gimpel et
al. (2011)?that includes new large-scale distribu-
tional features. This leads to state-of-the-art results
in POS tagging for both Twitter and Internet Relay
Chat (IRC) text. We also annotated a new dataset of
tweets with POS tags, improved the annotations in
the previous dataset from Gimpel et al, and devel-
oped annotation guidelines for manual POS tagging
of tweets. We release all of these resources to the
research community:
? an open-source part-of-speech tagger for online
conversational text (?2);
? unsupervised Twitter word clusters (?3);
? an improved emoticon detector for conversational
text (?4);
380
? POS annotation guidelines (?5.1); and
? a new dataset of 547 manually POS-annotated
tweets (?5).
2 MEMM Tagger
Our tagging model is a first-order maximum en-
tropy Markov model (MEMM), a discriminative se-
quence model for which training and decoding are
extremely efficient (Ratnaparkhi, 1996; McCallum
et al, 2000).2 The probability of a tag yt is condi-
tioned on the input sequence x and the tag to its left
yt?1, and is parameterized by a multiclass logistic
regression:
p(yt = k | yt?1,x, t;?) ?
exp
(
?(trans)yt?1,k +
?
j ?
(obs)
j,k fj(x, t)
)
We use transition features for every pair of labels,
and extract base observation features from token t
and neighboring tokens, and conjoin them against
all K = 25 possible outputs in our coarse tagset
(Appendix A). Our feature sets will be discussed
below in detail.
Decoding. For experiments reported in this paper,
we use the O(|x|K2) Viterbi algorithm for predic-
tion; K is the number of tags. This exactly max-
imizes p(y | x), but the MEMM also naturally al-
lows a fasterO(|x|K) left-to-right greedy decoding:
for t = 1 . . . |x|:
y?t ? argmaxk p(yt = k | y?t?1,x, t;?)
which we find is 3 times faster and yields similar ac-
curacy as Viterbi (an insignificant accuracy decrease
of less than 0.1% absolute on the DAILY547 test set
discussed below). Speed is paramount for social me-
dia analysis applications?which often require the
processing of millions to billions of messages?so
we make greedy decoding the default in the released
software.
2Although when compared to CRFs, MEMMs theoretically
suffer from the ?label bias? problem (Lafferty et al, 2001), our
system substantially outperforms the CRF-based taggers of pre-
vious work; and when comparing to Gimpel et al system with
similar feature sets, we observed little difference in accuracy.
This is consistent with conventional wisdom that the quality
of lexical features is much more important than the paramet-
ric form of the sequence model, at least in our setting: part-of-
speech tagging with a small labeled training set.
This greedy tagger runs at 800 tweets/sec. (10,000
tokens/sec.) on a single CPU core, about 40 times
faster than Gimpel et al?s system. The tokenizer by
itself (?4) runs at 3,500 tweets/sec.3
Training and regularization. During training,
the MEMM log-likelihood for a tagged tweet ?x,y?
is the sum over the observed token tags yt, each con-
ditional on the tweet being tagged and the observed
previous tag (with a start symbol before the first to-
ken in x),
`(x,y,?) =
?|x|
t=1 log p(yt | yt?1,x, t;?).
We optimize the parameters ? with OWL-QN, an
L1-capable variant of L-BFGS (Andrew and Gao,
2007; Liu and Nocedal, 1989) to minimize the regu-
larized objective
argmin
?
? 1N
?
?x,y? `(x,y,?) +R(?)
where N is the number of tokens in the corpus and
the sum ranges over all tagged tweets ?x,y? in the
training data. We use elastic net regularization (Zou
and Hastie, 2005), which is a linear combination of
L1 and L2 penalties; here j indexes over all features:
R(?) = ?1
?
j |?j |+
1
2?2
?
j ?
2
j
Using even a very small L1 penalty eliminates many
irrelevant or noisy features.
3 Unsupervised Word Clusters
Our POS tagger can make use of any number of pos-
sibly overlapping features. While we have only a
small amount of hand-labeled data for training, we
also have access to billions of tokens of unlabeled
conversational text from the web. Previous work has
shown that unlabeled text can be used to induce un-
supervised word clusters which can improve the per-
formance of many supervised NLP tasks (Koo et al,
2008; Turian et al, 2010; T?ckstr?m et al, 2012, in-
ter alia). We use a similar approach here to improve
tagging performance for online conversational text.
We also make our induced clusters publicly avail-
able in the hope that they will be useful for other
NLP tasks in this genre.
3Runtimes observed on an Intel Core i5 2.4 GHz laptop.
381
Binary path Top words (by frequency)
A1 111010100010 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol
A2 111010100011 haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa ahah
A3 111010100100 yes yep yup nope yess yesss yessss ofcourse yeap likewise yepp yesh yw yuup yus
A4 111010100101 yeah yea nah naw yeahh nooo yeh noo noooo yeaa ikr nvm yeahhh nahh nooooo
A5 11101011011100 smh jk #fail #random #fact smfh #smh #winning #realtalk smdh #dead #justsaying
B 011101011 u yu yuh yhu uu yuu yew y0u yuhh youh yhuu iget yoy yooh yuo yue juu dya youz yyou
C 11100101111001 w fo fa fr fro ov fer fir whit abou aft serie fore fah fuh w/her w/that fron isn agains
D 111101011000 facebook fb itunes myspace skype ebay tumblr bbm flickr aim msn netflix pandora
E1 0011001 tryna gon finna bouta trynna boutta gne fina gonn tryina fenna qone trynaa qon
E2 0011000 gonna gunna gona gna guna gnna ganna qonna gonnna gana qunna gonne goona
F 0110110111 soo sooo soooo sooooo soooooo sooooooo soooooooo sooooooooo soooooooooo
G1 11101011001010 ;) :p :-) xd ;-) ;d (; :3 ;p =p :-p =)) ;] xdd #gno xddd >:) ;-p >:d 8-) ;-d
G2 11101011001011 :) (: =) :)) :] :?) =] ^_^ :))) ^.^ [: ;)) ((: ^__^ (= ^-^ :))))
G3 1110101100111 :( :/ -_- -.- :-( :?( d: :| :s -__- =( =/ >.< -___- :-/ </3 :\ -____- ;( /: :(( >_< =[ :[ #fml
G4 111010110001 <3 xoxo <33 xo <333 #love s2 <URL-twitition.com> #neversaynever <3333
Figure 2: Example word clusters (HMM classes): we list the most probable words, starting with the most probable, in
descending order. Boldfaced words appear in the example tweet (Figure 1). The binary strings are root-to-leaf paths
through the binary cluster tree. For example usage, see e.g. search.twitter.com, bing.com/social and
urbandictionary.com.
3.1 Clustering Method
We obtained hierarchical word clusters via Brown
clustering (Brown et al, 1992) on a large set of
unlabeled tweets.4 The algorithm partitions words
into a base set of 1,000 clusters, and induces a hi-
erarchy among those 1,000 clusters with a series of
greedy agglomerative merges that heuristically opti-
mize the likelihood of a hidden Markov model with a
one-class-per-lexical-type constraint. Not only does
Brown clustering produce effective features for dis-
criminative models, but its variants are better unsu-
pervised POS taggers than some models developed
nearly 20 years later; see comparisons in Blunsom
and Cohn (2011). The algorithm is attractive for our
purposes since it scales to large amounts of data.
When training on tweets drawn from a single
day, we observed time-specific biases (e.g., nu-
merical dates appearing in the same cluster as the
word tonight), so we assembled our unlabeled data
from a random sample of 100,000 tweets per day
from September 10, 2008 to August 14, 2012,
and filtered out non-English tweets (about 60% of
the sample) using langid.py (Lui and Baldwin,
2012).5 Each tweet was processed with our to-
4As implemented by Liang (2005), v. 1.3: https://
github.com/percyliang/brown-cluster
5https://github.com/saffsd/langid.py
kenizer and lowercased. We normalized all at-
mentions to ?@MENTION? and URLs/email ad-
dresses to their domains (e.g. http://bit.ly/
dP8rR8 ? ?URL-bit.ly?). In an effort to reduce
spam, we removed duplicated tweet texts (this also
removes retweets) before word clustering. This
normalization and cleaning resulted in 56 million
unique tweets (847 million tokens). We set the
clustering software?s count threshold to only cluster
words appearing 40 or more times, yielding 216,856
word types, which took 42 hours to cluster on a sin-
gle CPU.
3.2 Cluster Examples
Fig. 2 shows example clusters. Some of the chal-
lenging words in the example tweet (Fig. 1) are high-
lighted. The term lololol (an extension of lol for
?laughing out loud?) is grouped with a large number
of laughter acronyms (A1: ?laughing my (fucking)
ass off,? ?cracking the fuck up?). Since expressions
of laughter are so prevalent on Twitter, the algorithm
creates another laughter cluster (A1?s sibling A2),
that tends to have onomatopoeic, non-acronym vari-
ants (e.g., haha). The acronym ikr (?I know, right??)
is grouped with expressive variations of ?yes? and
?no? (A4). Note that A1?A4 are grouped in a fairly
specific subtree; and indeed, in this message ikr and
382
lololol are both tagged as interjections.
smh (?shaking my head,? indicating disapproval)
seems related, though is always tagged in the an-
notated data as a miscellaneous abbreviation (G);
the difference between acronyms that are interjec-
tions versus other acronyms may be complicated.
Here, smh is in a related but distinct subtree from the
above expressions (A5); its usage in this example
is slightly different from its more common usage,
which it shares with the other words in its cluster:
message-ending expressions of commentary or emo-
tional reaction, sometimes as a metacomment on the
author?s message; e.g., Maybe you could get a guy
to date you if you actually respected yourself #smh
or There is really NO reason why other girls should
send my boyfriend a goodmorning text #justsaying.
We observe many variants of categories tradition-
ally considered closed-class, including pronouns (B:
u = ?you?) and prepositions (C: fir = ?for?).
There is also evidence of grammatical categories
specific to conversational genres of English; clusters
E1?E2 demonstrate variations of single-word con-
tractions for ?going to? and ?trying to,? some of
which have more complicated semantics.6
Finally, the HMM learns about orthographic vari-
ants, even though it treats all words as opaque sym-
bols; cluster F consists almost entirely of variants
of ?so,? their frequencies monotonically decreasing
in the number of vowel repetitions?a phenomenon
called ?expressive lengthening? or ?affective length-
ening? (Brody and Diakopoulos, 2011; Schnoebe-
len, 2012). This suggests a future direction to jointly
model class sequence and orthographic informa-
tion (Clark, 2003; Smith and Eisner, 2005; Blunsom
and Cohn, 2011).
We have built an HTML viewer to browse these
and numerous other interesting examples.7
3.3 Emoticons and Emoji
We use the term emoticon to mean a face or icon
constructed with traditional alphabetic or punctua-
6One coauthor, a native speaker of the Texan English dialect,
notes ?finna? (short for ?fixing to?, cluster E1) may be an im-
mediate future auxiliary, indicating an immediate future tense
that is present in many languages (though not in standard En-
glish). To illustrate: ?She finna go? approximately means ?She
will go,? but sooner, in the sense of ?She is about to go.?
7http://www.ark.cs.cmu.edu/TweetNLP/
cluster_viewer.html
tion symbols, and emoji to mean symbols rendered
in software as small pictures, in line with the text.
Since our tokenizer is careful to preserve emoti-
cons and other symbols (see ?4), they are clustered
just like other words. Similar emoticons are clus-
tered together (G1?G4), including separate clusters
of happy [[ :) =) ?_? ]], sad/disappointed [[ :/ :(
-_- </3 ]], love [[ ?xoxo ?.? ]] and winking [[
;) (?_-) ]] emoticons. The clusters are not per-
fectly aligned with our POS annotation guidelines;
for example, the ?sad? emoticon cluster included
emotion-bearing terms that our guidelines define as
non-emoticons, such as #ugh, #tear, and #fml (?fuck
my life?), though these seem potentially useful for
sentiment analysis.
One difficult task is classifying different types
of symbols in tweets: our annotation guidelines
differentiate between emoticons, punctuation, and
garbage (apparently non-meaningful symbols or to-
kenization errors). Several Unicode character ranges
are reserved for emoji-style symbols (including the
three Unicode hearts in G4); however, depending
on the user?s software, characters in these ranges
might be rendered differently or not at all. We
have found instances where the clustering algo-
rithm groups proprietary iOS emoji symbols along
with normal emoticons; for example, the character
U+E056, which is interpreted on iOS as a smiling
face, is in the same G2 cluster as smiley face emoti-
cons. The symbol U+E12F, which represents a pic-
ture of a bag of money, is grouped with the words
cash and money.
3.4 Cluster-Based Features
Since Brown clusters are hierarchical in a binary
tree, each word is associated with a tree path rep-
resented as a bitstring with length ? 16; we use pre-
fixes of the bitstring as features (for all prefix lengths
? {2, 4, 6, . . . , 16}). This allows sharing of statisti-
cal strength between similar clusters. Using prefix
features of hierarchical clusters in this way was sim-
ilarly found to be effective for named-entity recog-
nition (Turian et al, 2010) and Twitter POS tag-
ging (Ritter et al, 2011).
When checking to see if a word is associated with
a cluster, the tagger first normalizes the word using
the same techniques as described in ?3.1, then cre-
ates a priority list of fuzzy match transformations
383
of the word by removing repeated punctuation and
repeated characters. If the normalized word is not
in a cluster, the tagger considers the fuzzy matches.
Although only about 3% of the tokens in the devel-
opment set (?6) did not appear in a clustering, this
method resulted in a relative error decrease of 18%
among such word tokens.
3.5 Other Lexical Features
Besides unsupervised word clusters, there are two
other sets of features that contain generalized lexi-
cal class information. We use the tag dictionary fea-
ture from Gimpel et al, which adds a feature for
a word?s most frequent part-of-speech tag.8 This
can be viewed as a feature-based domain adaptation
method, since it gives lexical type-level information
for standard English words, which the model learns
to map between PTB tags to the desired output tags.
Second, since the lack of consistent capitalization
conventions on Twitter makes it especially difficult
to recognize names?Gimpel et al and Foster et
al. (2011) found relatively low accuracy on proper
nouns?we added a token-level name list feature,
which fires on (non-function) words from names
from several sources: Freebase lists of celebrities
and video games (Google, 2012), the Moby Words
list of US Locations,9 and lists of male, female, fam-
ily, and proper names from Mark Kantrowitz?s name
corpus.10
4 Tokenization and Emoticon Detection
Word segmentation on Twitter is challenging due
to the lack of orthographic conventions; in partic-
ular, punctuation, emoticons, URLs, and other sym-
bols may have no whitespace separation from textual
8Frequencies came from the Wall Street Journal and Brown
corpus sections of the Penn Treebank. If a word has multiple
PTB tags, each tag is a feature with value for the frequency rank;
e.g. for three different tags in the PTB, this feature gives a value
of 1 for the most frequent tag, 2/3 for the second, etc. Coarse
versions of the PTB tags are used (Petrov et al, 2011). While
88% of words in the dictionary have only one tag, using rank
information seemed to give a small but consistent gain over only
using the most common tag, or using binary features conjoined
with rank as in Gimpel et al
9http://icon.shef.ac.uk/Moby/mwords.html
10http://www.cs.cmu.edu/afs/cs/project/
ai-repository/ai/areas/nlp/corpora/names/
0.html
words (e.g. no:-d,yes should parse as four tokens),
and internally may contain alphanumeric symbols
that could be mistaken for words: a naive split(/[^a-
zA-Z0-9]+/) tokenizer thinks the words ?p? and ?d?
are among the top 100 most common words on Twit-
ter, due to misanalysis of :p and :d. Traditional Penn
Treebank?style tokenizers are hardly better, often
breaking a string of punctuation characters into a
single token per character.
We rewrote twokenize (O?Connor et al,
2010), a rule-based tokenizer, emoticon, and URL
detector, for use in the tagger. Emoticons are es-
pecially challenging, since they are open-class and
productive. We revise O?Connor et al?s regular ex-
pression grammar that describes possible emoticons,
adding a grammar of horizontal emoticons (e.g. -_-),
known as ?Eastern-style,?11 though we observe high
usage in English-speaking Twitter (Fig. 2, G2?G3).
We also add a number of other improvements to the
patterns. Because this system was used as prepro-
cessing for the word clustering experiment in ?3, we
were able to infer the emoticon clusters in Fig. 2.
Furthermore, whether a token matches the emoticon
pattern is also used as a feature in the tagger (?2).
URL recognition is also difficult, since the http://
is often dropped, resulting in protocol-less URLs
like about.me. We add recognition patterns for these
by using a list of top-level and country domains.
5 Annotated Dataset
Gimpel et al (2011) provided a dataset of POS-
tagged tweets consisting almost entirely of tweets
sampled from one particular day (October 27,
2010). We were concerned about overfitting to time-
specific phenomena; for example, a substantial frac-
tion of the messages are about a basketball game
happening that day.
We created a new test set of 547 tweets for eval-
uation. The test set consists of one random English
tweet from every day between January 1, 2011 and
June 30, 2012. In order for a tweet to be considered
English, it had to contain at least one English word
other than a URL, emoticon, or at-mention. We no-
ticed biases in the outputs of langid.py, so we
instead selected these messages completely manu-
11http://en.wikipedia.org/wiki/List_of_
emoticons
384
ally (going through a random sample of one day?s
messages until an English message was found).
5.1 Annotation Methodology
Gimpel et al provided a tagset for Twitter (shown in
Appendix A), which we used unmodified. The orig-
inal annotation guidelines were not published, but in
this work we recorded the rules governing tagging
decisions and made further revisions while annotat-
ing the new data.12 Some of our guidelines reiter-
ate or modify rules made by Penn Treebank annota-
tors, while others treat specific phenomena found on
Twitter (refer to the next section).
Our tweets were annotated by two annotators who
attempted to match the choices made in Gimpel et
al.?s dataset. The annotators also consulted the POS
annotations in the Penn Treebank (Marcus et al,
1993) as an additional reference. Differences were
reconciled by a third annotator in discussion with all
annotators.13 During this process, an inconsistency
was found in Gimpel et al?s data, which we cor-
rected (concerning the tagging of this/that, a change
to 100 labels, 0.4%). The new version of Gimpel et
al.?s data (called OCT27), as well as the newer mes-
sages (called DAILY547), are both included in our
data release.
5.2 Compounds in Penn Treebank vs. Twitter
Ritter et al (2011) annotated tweets using an aug-
mented version of the PTB tagset and presumably
followed the PTB annotation guidelines. We wrote
new guidelines because the PTB conventions are in-
appropriate for Twitter in several ways, as shown in
the design of Gimpel et al?s tagset. Importantly,
?compound? tags (e.g., nominal+verbal and nomi-
nal+possessive) are used because tokenization is dif-
ficult or seemingly impossible for the nonstandard
word forms that are commonplace in conversational
text.
For example, the PTB tokenization splits contrac-
tions containing apostrophes: I?m? I/PRP ?m/VBP.
But conversational text often contains variants that
resist a single PTB tag (like im), or even chal-
lenge traditional English grammatical categories
12The annotation guidelines are available online at
http://www.ark.cs.cmu.edu/TweetNLP/
13Annotators are coauthors of this paper.
(like imma or umma, which both mean ?I am go-
ing to?). One strategy would be to analyze these
forms into a PTB-style tokenization, as discussed in
Forsyth (2007), who proposes to analyze doncha as
do/VBP ncha/PRP, but notes it would be difficult.
We think this is impossible to handle in the rule-
based framework used by English tokenizers, given
the huge (and possibly growing) number of large
compounds like imma, gonna, w/that, etc. These
are not rare: the word clustering algorithm discov-
ers hundreds of such words as statistically coherent
classes (e.g. clusters E1 and E2 in Fig. 2); and the
word imma is the 962nd most common word in our
unlabeled corpus, more frequent than cat or near.
We do not attempt to do Twitter ?normalization?
into traditional written English (Han and Baldwin,
2011), which we view as a lossy translation task. In
fact, many of Twitter?s unique linguistic phenomena
are due not only to its informal nature, but also a set
of authors that heavily skews towards younger ages
and minorities, with heavy usage of dialects that are
different than the standard American English most
often seen in NLP datasets (Eisenstein, 2013; Eisen-
stein et al, 2011). For example, we suspect that
imma may implicate tense and aspect markers from
African-American Vernacular English.14 Trying to
impose PTB-style tokenization on Twitter is linguis-
tically inappropriate: should the lexico-syntactic be-
havior of casual conversational chatter by young mi-
norities be straightjacketed into the stylistic conven-
tions of the 1980s Wall Street Journal? Instead, we
would like to directly analyze the syntax of online
conversational text on its own terms.
Thus, we choose to leave these word forms un-
tokenized and use compound tags, viewing com-
positional multiword analysis as challenging fu-
ture work.15 We believe that our strategy is suf-
ficient for many applications, such as chunking or
named entity recognition; many applications such
as sentiment analysis (Turney, 2002; Pang and Lee,
2008, ?4.2.3), open information extraction (Carl-
son et al, 2010; Fader et al, 2011), and informa-
tion retrieval (Allan and Raghavan, 2002) use POS
14See ?Tense and aspect? examples in http:
//en.wikipedia.org/wiki/African_American_
Vernacular_English
15For example, wtf has compositional behavior in ?Wtf just
happened???, but only debatably so in ?Huh wtf?.
385
#Msg. #Tok. Tagset Dates
OCT27 1,827 26,594 App. A Oct 27-28, 2010
DAILY547 547 7,707 App. A Jan 2011?Jun 2012
NPSCHAT 10,578 44,997 PTB-like Oct?Nov 2006
(w/o sys. msg.) 7,935 37,081
RITTERTW 789 15,185 PTB-like unknown
Table 1: Annotated datasets: number of messages, to-
kens, tagset, and date range. More information in ?5,
?6.3, and ?6.2.
patterns that seem quite compatible with our ap-
proach. More complex downstream processing like
parsing is an interesting challenge, since contraction
parsing on traditional text is probably a benefit to
current parsers. We believe that any PTB-trained
tool requires substantial retraining and adaptation
for Twitter due to the huge genre and stylistic differ-
ences (Foster et al, 2011); thus tokenization conven-
tions are a relatively minor concern. Our simple-to-
annotate conventions make it easier to produce new
training data.
6 Experiments
We are primarily concerned with performance on
our annotated datasets described in ?5 (OCT27,
DAILY547), though for comparison to previous
work we also test on other corpora (RITTERTW in
?6.2, NPSCHAT in ?6.3). The annotated datasets
are listed in Table 1.
6.1 Main Experiments
We use OCT27 to refer to the entire dataset de-
scribed in Gimpel et al; it is split into train-
ing, development, and test portions (OCT27TRAIN,
OCT27DEV, OCT27TEST). We use DAILY547 as
an additional test set. Neither OCT27TEST nor
DAILY547 were extensively evaluated against until
final ablation testing when writing this paper.
The total number of features is 3.7 million, all
of which are used under pure L2 regularization; but
only 60,000 are selected by elastic net regularization
with (?1, ?2) = (0.25, 2), which achieves nearly the
same (but no better) accuracy as pure L2,16 and we
use it for all experiments. We observed that it was
16We conducted a grid search for the regularizer values on
part of DAILY547, and many regularizer values give the best or
nearly the best results. We suspect a different setup would have
yielded similar results.
l
l
l
l l l l
1e+03 1e+05 1e+07
75
80
85
90
Number of Unlabeled Tweets
Ta
gg
ing
 Ac
cu
rac
y
l
l
l l
l l l
1e+03 1e+05 1e+07
0.6
0
0.6
5
0.7
0
Number of Unlabeled Tweets
To
ke
n 
Co
ve
ra
ge
Figure 3: OCT27 development set accuracy using only
clusters as features.
Model In dict. Out of dict.
Full 93.4 85.0
No clusters 92.0 (?1.4) 79.3 (?5.7)
Total tokens 4,808 1,394
Table 3: DAILY547 accuracies (%) for tokens in and out
of a traditional dictionary, for models reported in rows 1
and 3 of Table 2.
possible to get radically smaller models with only
a slight degradation in performance: (4, 0.06) has
0.5% worse accuracy but uses only 1,632 features, a
small enough number to browse through manually.
First, we evaluate on the new test set, training on
all of OCT27. Due to DAILY547?s statistical repre-
sentativeness, we believe this gives the best view of
the tagger?s accuracy on English Twitter text. The
full tagger attains 93.2% accuracy (final row of Ta-
ble 2).
To facilitate comparisons with previous work, we
ran a series of experiments training only on OCT27?s
training and development sets, then report test re-
sults on both OCT27TEST and all of DAILY547,
shown in Table 2. Our tagger achieves substantially
higher accuracy than Gimpel et al (2011).17
Feature ablation. A number of ablation tests in-
dicate the word clusters are a very strong source of
lexical knowledge. When dropping the tag dictio-
naries and name lists, the word clusters maintain
most of the accuracy (row 2). If we drop the clus-
ters and rely only on tag dictionaries and namelists,
accuracy decreases significantly (row 3). In fact,
we can remove all observation features except for
word clusters?no word features, orthographic fea-
17These numbers differ slightly from those reported by Gim-
pel et al, due to the corrections we made to the OCT27 data,
noted in Section 5.1. We retrained and evaluated their tagger
(version 0.2) on our corrected dataset.
386
Feature set OCT27TEST DAILY547 NPSCHATTEST
All features 91.60 92.80 91.19 1
with clusters; without tagdicts, namelists 91.15 92.38 90.66 2
without clusters; with tagdicts, namelists 89.81 90.81 90.00 3
only clusters (and transitions) 89.50 90.54 89.55 4
without clusters, tagdicts, namelists 86.86 88.30 88.26 5
Gimpel et al (2011) version 0.2 88.89 89.17 6
Inter-annotator agreement (Gimpel et al, 2011) 92.2 7
Model trained on all OCT27 93.2 8
Table 2: Tagging accuracies (%) in ablation experiments. OCT27TEST and DAILY547 95% confidence intervals are
roughly ?0.7%. Our final tagger uses all features and also trains on OCT27TEST, achieving 93.2% on DAILY547.
tures, affix n-grams, capitalization, emoticon pat-
terns, etc.?and the accuracy is in fact still better
than the previous work (row 4).18
We also wanted to know whether to keep the tag
dictionary and name list features, but the splits re-
ported in Fig. 2 did not show statistically signifi-
cant differences; so to better discriminate between
ablations, we created a lopsided train/test split of
all data with a much larger test portion (26,974 to-
kens), having greater statistical power (tighter con-
fidence intervals of ? 0.3%).19 The full system got
90.8% while the no?tag dictionary, no-namelists ab-
lation had 90.0%, a statistically significant differ-
ence. Therefore we retain these features.
Compared to the tagger in Gimpel et al, most of
our feature changes are in the new lexical features
described in ?3.5.20 We do not reuse the other lex-
ical features from the previous work, including a
phonetic normalizer (Metaphone), a name list con-
sisting of words that are frequently capitalized, and
distributional features trained on a much smaller un-
labeled corpus; they are all worse than our new
lexical features described here. (We did include,
however, a variant of the tag dictionary feature that
uses phonetic normalization for lookup; it seemed to
yield a small improvement.)
18Furthermore, when evaluating the clusters as unsupervised
(hard) POS tags, we obtain a many-to-one accuracy of 89.2%
on DAILY547. Before computing this, we lowercased the text
to match the clusters and removed tokens tagged as URLs and
at-mentions.
19Reported confidence intervals in this paper are 95% bino-
mial normal approximation intervals for the proportion of cor-
rectly tagged tokens: ?1.96
?
p(1? p)/ntokens . 1/
?
n.
20Details on the exact feature set are available in a technical
report (Owoputi et al, 2012), also available on the website.
Non-traditional words. The word clusters are es-
pecially helpful with words that do not appear in tra-
ditional dictionaries. We constructed a dictionary
by lowercasing the union of the ispell ?American?,
?British?, and ?English? dictionaries, plus the stan-
dard Unix words file from Webster?s Second Inter-
national dictionary, totalling 260,985 word types.
After excluding tokens defined by the gold stan-
dard as punctuation, URLs, at-mentions, or emoti-
cons,21 22% of DAILY547?s tokens do not appear in
this dictionary. Without clusters, they are very dif-
ficult to classify (only 79.2% accuracy), but adding
clusters generates a 5.7 point improvement?much
larger than the effect on in-dictionary tokens (Ta-
ble 3).
Varying the amount of unlabeled data. A tagger
that only uses word clusters achieves an accuracy of
88.6% on the OCT27 development set.22 We created
several clusterings with different numbers of unla-
beled tweets, keeping the number of clusters con-
stant at 800. As shown in Fig. 3, there was initially
a logarithmic relationship between number of tweets
and accuracy, but accuracy (and lexical coverage)
levels out after 750,000 tweets. We use the largest
clustering (56 million tweets and 1,000 clusters) as
the default for the released tagger.
6.2 Evaluation on RITTERTW
Ritter et al (2011) annotated a corpus of 787
tweets23 with a single annotator, using the PTB
21We retain hashtags since by our guidelines a #-prefixed to-
ken is ambiguous between a hashtag and a normal word, e.g. #1
or going #home.
22The only observation features are the word clusters of a
token and its immediate neighbors.
23https://github.com/aritter/twitter_nlp/
blob/master/data/annotated/pos.txt
387
Tagger Accuracy
This work 90.0 ? 0.5
Ritter et al (2011), basic CRF tagger 85.3
Ritter et al (2011), trained on more data 88.3
Table 4: Accuracy comparison on Ritter et al?s Twitter
POS corpus (?6.2).
Tagger Accuracy
This work 93.4 ? 0.3
Forsyth (2007) 90.8
Table 5: Accuracy comparison on Forsyth?s NPSCHAT
IRC POS corpus (?6.3).
tagset plus several Twitter-specific tags, referred
to in Table 1 as RITTERTW. Linguistic concerns
notwithstanding (?5.2), for a controlled comparison,
we train and test our system on this data with the
same 4-fold cross-validation setup they used, attain-
ing 90.0% (?0.5%) accuracy. Ritter et al?s CRF-
based tagger had 85.3% accuracy, and their best tag-
ger, trained on a concatenation of PTB, IRC, and
Twitter, achieved 88.3% (Table 4).
6.3 IRC: Evaluation on NPSCHAT
IRC is another medium of online conversational
text, with similar emoticons, misspellings, abbrevi-
ations and acronyms as Twitter data. We evaluate
our tagger on the NPS Chat Corpus (Forsyth and
Martell, 2007),24 a PTB-part-of-speech annotated
dataset of Internet Relay Chat (IRC) room messages
from 2006.
First, we compare to a tagger in the same setup as
experiments on this data in Forsyth (2007), training
on 90% of the data and testing on 10%; we average
results across 10-fold cross-validation.25 The full
tagger model achieved 93.4% (?0.3%) accuracy,
significantly improving over the best result they re-
port, 90.8% accuracy with a tagger trained on a mix
of several POS-annotated corpora.
We also perform the ablation experiments on this
corpus, with a slightly different experimental setup:
we first filter out system messages then split data
24Release 1.0: http://faculty.nps.edu/
cmartell/NPSChat.htm
25Forsyth actually used 30 different 90/10 random splits; we
prefer cross-validation because the same test data is never re-
peated, thus allowing straightforward confidence estimation of
accuracy from the number of tokens (via binomial sample vari-
ance, footnote 19). In all cases, the models are trained on the
same amount of data (90%).
into 5,067 training and 2,868 test messages. Results
show a similar pattern as the Twitter data (see final
column of Table 2). Thus the Twitter word clusters
are also useful for language in the medium of text
chat rooms; we suspect these clusters will be appli-
cable for deeper syntactic and semantic analysis in
other online conversational text mediums, such as
text messages and instant messages.
7 Conclusion
We have constructed a state-of-the-art part-of-
speech tagger for the online conversational text
genres of Twitter and IRC, and have publicly re-
leased our new evaluation data, annotation guide-
lines, open-source tagger, and word clusters at
http://www.ark.cs.cmu.edu/TweetNLP.
Acknowledgements
This research was supported in part by the National Sci-
ence Foundation (IIS-0915187 and IIS-1054319).
A Part-of-Speech Tagset
N common noun
O pronoun (personal/WH; not possessive)
^ proper noun
S nominal + possessive
Z proper noun + possessive
V verb including copula, auxiliaries
L nominal + verbal (e.g. i?m), verbal + nominal (let?s)
M proper noun + verbal
A adjective
R adverb
! interjection
D determiner
P pre- or postposition, or subordinating conjunction
& coordinating conjunction
T verb particle
X existential there, predeterminers
Y X + verbal
# hashtag (indicates topic/category for tweet)
@ at-mention (indicates a user as a recipient of a tweet)
~ discourse marker, indications of continuation across
multiple tweets
U URL or email address
E emoticon
$ numeral
, punctuation
G other abbreviations, foreign words, possessive endings,
symbols, garbage
Table 6: POS tagset from Gimpel et al (2011) used in this
paper, and described further in the released annotation
guidelines.
388
References
J. Allan and H. Raghavan. 2002. Using part-of-speech
patterns to reduce query ambiguity. In Proc. of SIGIR.
G. Andrew and J. Gao. 2007. Scalable training of L1-
regularized log-linear models. In Proc. of ICML.
P. Blunsom and T. Cohn. 2011. A hierarchical Pitman-
Yor process HMM for unsupervised part of speech in-
duction. In Proc. of ACL.
S. Brody and N. Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!: using
word lengthening to detect sentiment in microblogs.
In Proc. of EMNLP.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18(4).
A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. R. Hr-
uschka Jr, and T. M. Mitchell. 2010. Toward an archi-
tecture for never-ending language learning. In Proc. of
AAAI.
A. Clark. 2003. Combining distributional and morpho-
logical information for part of speech induction. In
Proc. of EACL.
J. Eisenstein, N. A. Smith, and E. P. Xing. 2011. Discov-
ering sociolinguistic associations with structured spar-
sity. In Proc. of ACL.
J. Eisenstein. 2013. What to do about bad language on
the internet. In Proc. of NAACL.
A. Fader, S. Soderland, and O. Etzioni. 2011. Identifying
relations for open information extraction. In Proc. of
EMNLP.
E. N. Forsyth and C. H. Martell. 2007. Lexical and dis-
course analysis of online chat dialog. In Proc. of ICSC.
E. N. Forsyth. 2007. Improving automated lexical and
discourse analysis of online chat dialog. Master?s the-
sis, Naval Postgraduate School.
J. Foster, O. Cetinoglu, J. Wagner, J. L. Roux, S. Hogan,
J. Nivre, D. Hogan, and J. van Genabith. 2011. #hard-
toparse: POS tagging and parsing the Twitterverse. In
Proc. of AAAI-11 Workshop on Analysing Microtext.
K. Gimpel, N. Schneider, B. O?Connor, D. Das, D. Mills,
J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan,
and N. A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proc. of ACL.
Google. 2012. Freebase data dumps. http://
download.freebase.com/datadumps/.
B. Han and T. Baldwin. 2011. Lexical normalisation of
short text messages: Makn sens a #twitter. In Proc. of
ACL.
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
supervised dependency parsing. In Proc. of ACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
D. C. Liu and J. Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathemat-
ical programming, 45(1).
X. Liu, S. Zhang, F. Wei, and M. Zhou. 2011. Recogniz-
ing named entities in tweets. In Proc. of ACL.
M. Lui and T. Baldwin. 2012. langid.py: An off-the-
shelf language identification tool. In Proc. of ACL.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2).
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. of ICML.
D. McClosky, E. Charniak, and M. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Proc. of
NAACL.
B. O?Connor, M. Krieger, and D. Ahn. 2010.
TweetMotif: exploratory search and topic summariza-
tion for Twitter. In Proc. of AAAI Conference on We-
blogs and Social Media.
O. Owoputi, B. O?Connor, C. Dyer, K. Gimpel, and
N. Schneider. 2012. Part-of-speech tagging for Twit-
ter: Word clusters and other advances. Technical Re-
port CMU-ML-12-107, Carnegie Mellon University.
B. Pang and L. Lee. 2008. Opinion mining and sentiment
analysis. Now Publishers.
S. Petrov and R. McDonald. 2012. Overview of the 2012
shared task on parsing the web. Notes of the First
Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. of EMNLP.
A. Ritter, S. Clark, Mausam, and O. Etzioni. 2011.
Named entity recognition in tweets: An experimental
study. In Proc. of EMNLP.
T. Schnoebelen. 2012. Do you smile with your nose?
Stylistic variation in Twitter emoticons. University of
Pennsylvania Working Papers in Linguistics, 18(2):14.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
of ACL.
O. T?ckstr?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of lin-
guistic structure. In Proc. of NAACL.
389
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: A simple and general method for semi-
supervised learning. In Proc. of ACL.
P. D. Turney. 2002. Thumbs up or thumbs down?: se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proc. of ACL.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
67(2):301?320.
390
Proceedings of NAACL-HLT 2013, pages 644?648,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Simple, Fast, and Effective Reparameterization of IBM Model 2
Chris Dyer Victor Chahuneau Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{cdyer,vchahune,nasmith}@cs.cmu.edu
Abstract
We present a simple log-linear reparame-
terization of IBM Model 2 that overcomes
problems arising from Model 1?s strong
assumptions and Model 2?s overparame-
terization. Efficient inference, likelihood
evaluation, and parameter estimation algo-
rithms are provided. Training the model is
consistently ten times faster than Model 4.
On three large-scale translation tasks, systems
built using our alignment model outperform
IBM Model 4.
An open-source implementation of the align-
ment model described in this paper is available
from http://github.com/clab/fast align .
1 Introduction
Word alignment is a fundamental problem in statis-
tical machine translation. While the search for more
sophisticated models that provide more nuanced ex-
planations of parallel corpora is a key research activ-
ity, simple and effective models that scale well are
also important. These play a crucial role in many
scenarios such as parallel data mining and rapid
large scale experimentation, and as subcomponents
of other models or training and inference algorithms.
For these reasons, IBM Models 1 and 2, which sup-
port exact inference in time ?(|f| ? |e|), continue to
be widely used.
This paper argues that both of these models are
suboptimal, even in the space of models that per-
mit such computationally cheap inference. Model
1 assumes all alignment structures are uniformly
likely (a problematic assumption, particularly for
frequent word types), and Model 2 is vastly overpa-
rameterized, making it prone to degenerate behav-
ior on account of overfitting.1 We present a simple
log-linear reparameterization of Model 2 that avoids
both problems (?2). While inference in log-linear
models is generally computationally more expen-
sive than in their multinomial counterparts, we show
how the quantities needed for alignment inference,
likelihood evaluation, and parameter estimation us-
ing EM and related methods can be computed using
two simple algebraic identities (?3), thereby defus-
ing this objection. We provide results showing our
model is an order of magnitude faster to train than
Model 4, that it requires no staged initialization, and
that it produces alignments that lead to significantly
better translation quality on downstream translation
tasks (?4).
2 Model
Our model is a variation of the lexical translation
models proposed by Brown et al (1993). Lexical
translation works as follows. Given a source sen-
tence f with length n, first generate the length of
the target sentence, m. Next, generate an alignment,
a = ?a1, a2, . . . , am?, that indicates which source
word (or null token) each target word will be a trans-
lation of. Last, generate the m output words, where
each ei depends only on fai .
The model of alignment configurations we pro-
pose is a log-linear reparameterization of Model 2.
1Model 2 has independent parameters for every alignment
position, conditioned on the source length, target length, and
current target index.
644
Given : f, n = |f|, m = |e|, p0, ?, ?
h(i, j,m, n) = ?
?
?
?
?
i
m ?
j
n
?
?
?
?
?(ai = j | i,m, n) =
?
??
??
p0 j = 0
(1? p0)? e
?h(i,j,m,n)
Z?(i,m,n)
0 < j ? n
0 otherwise
ai | i,m, n ? ?(? | i,m, n) 1 ? i ? m
ei | ai, fai ? ?(? | fai) 1 ? i ? m
null
j? = 1
j? = 2
j? = 3
j? = 4
j? = 5
i =
3
}
n
=
5
}
m = 6
i =
1
i =
2
i =
4
i =
5
i =
6
j?
j?
Figure 1: Our proposed generative process yielding a translation e and its alignment a to a source sentence f, given the
source sentence f, alignment parameters p0 and ?, and lexical translation probabilities ? (left); an example visualization
of the distribution of alignment probability mass under this model (right).
Our formulation, which we write as ?(ai = j |
i,m, n), is shown in Fig. 1.2 The distribution over
alignments is parameterized by a null alignment
probability p0 and a precision ? ? 0 which con-
trols how strongly the model favors alignment points
close to the diagonal. In the limiting case as ?? 0,
the distribution approaches that of Model 1, and, as
it gets larger, the model is less and less likely to de-
viate from a perfectly diagonal alignment. The right
side of Fig. 1 shows a graphical illustration of the
alignment distribution in which darker squares indi-
cate higher probability.
3 Inference
We now discuss two inference problems and give ef-
ficient techniques for solving them. First, given a
sentence pair and parameters, compute the marginal
likelihood and the marginal alignment probabilities.
Second, given a corpus of training data, estimate
likelihood maximizing model parameters using EM.
3.1 Marginals
Under our model, the marginal likelihood of a sen-
tence pair ?f, e? can be computed exactly in time
2Vogel et al (1996) hint at a similar reparameterization of
Model 2; however, its likelihood and its gradient are not effi-
cient to evaluate, making it impractical to train and use. Och
and Ney (2003) likewise remark on the overparameterization
issue, removing a single variable of the original conditioning
context, which only slightly improves matters.
?(|f| ? |e|). This can be seen as follows. For
each position in the sentence being generated, i ?
[1, 2, . . . ,m], the alignment to the source and its
translation is independent of all other translation and
alignment decisions. Thus, the probability that the
ith word of e is ei can be computed as:
p(ei, ai | f,m, n) = ?(ai | i,m, n)? ?(ei | fai)
p(ei | f,m, n) =
n?
j=0
p(ei, ai = j | f,m, n).
We can also compute the posterior probability over
alignments using the above probabilities,
p(ai | ei, f,m, n) =
p(ei, ai | f,m, n)
p(ei | f,m, n)
. (1)
Finally, since all words in e (and their alignments)
are conditionally independent,3
p(e | f) =
m?
i=1
p(ei | f,m, n)
=
m?
i=1
n?
j=0
?(ai | i,m, n)? ?(ei | fai).
3We note here that Brown et al (1993) derive their variant
of this expression by starting with the joint probability of an
alignment and translation, marginalizing, and then reorganizing
common terms. While identical in implication, we find the di-
rect probabilistic argument far more intuitive.
645
3.2 Efficient Partition Function Evaluation
Evaluating and maximizing the data likelihood un-
der log-linear models can be computationally ex-
pensive since this requires evaluation of normalizing
partition functions. In our case,
Z?(i,m, n) =
n?
j?=1
exp?h(i, j?,m, n).
While computing this sum is obviously possible
in ?(|f|) operations, our formulation permits exact
computation in ?(1), meaning our model can be ap-
plied even in applications where computational ef-
ficiency is paramount (e.g., MCMC simulations).
The key insight is that the partition function is the
(partial) sum of two geometric series of unnormal-
ized probabilities that extend up and down from the
probability-maximizing diagonal. The closest point
on or above the diagonal j?, and the next point down
j? (see the right side of Fig. 1 for an illustration), is
computed as follows:
j? =
? i? n
m
?
, j? = j? + 1.
Starting at j? and moving up the alignment col-
umn, as well as starting at j? and moving down, the
unnormalized probabilities decrease by a factor of
r = exp ??n per step.
To compute the value of the partition, we only
need to evaluate the unnormalized probabilities at
j? and j? and then use the following identity, which
gives the sum of the first ` terms of a geometric se-
ries (Courant and Robbins, 1996):
s`(g1, r) =
?`
k=1
g1rk?1 = g1
1? r`
1? r .
Using this identity, Z?(i,m, n) can be computed as
sj?(e?h(i,j?,m,n), r) + sn?j?(e?h(i,j?,m,n), r).
3.3 Parameter Optimization
To optimize the likelihood of a sample of parallel
data under our model, one can use EM. In the E-step,
the posterior probabilities over alignments are com-
puted using Eq. 1. In the M-step, the lexical trans-
lation probabilities are updated by aggregating these
as counts and normalizing (Brown et al, 1993). In
the experiments reported in this paper, we make the
further assumption that ?f ? Dirichlet(?) where
?i = 0.01 and approximate the posterior distribu-
tion over the ?f ?s using a mean-field approximation
(Riley and Gildea, 2012).4
During the M-step, the ? parameter must also
be updated to make the E-step posterior distribu-
tion over alignment points maximally probable un-
der ?(? | i,m, n). This maximizing value cannot
be computed analytically, but a gradient-based op-
timization can be used, where the first derivative
(here, for a single target word) is:
??L = Ep(ai|ei,f,m,n) [h(i, ai,m, n)]
? E?(j?|i,m,n)
[
h(i, j?,m, n)
]
(2)
The first term in this expression (the expected value
of h under the E-step posterior) is fixed for the du-
ration of each M-step, but the second term?s value
(the derivative of the log-partition function) changes
many times as ? is optimized.
3.4 Efficient Gradient Evaluation
Fortunately, like the partition function, the deriva-
tive of the log-partition function (i.e., the second
term in Eq. 2) can be computed in constant time us-
ing an algebraic identity. To derive this, we observe
that the values of h(i, j?,m, n) form an arithmetic
sequence about the diagonal, with common differ-
ence d = ?1/n. Thus, the quantity we seek is the
sum of a series whose elements are the products of
terms from an arithmetic sequence and those of the
geometric sequence above, divided by the partition
function value. This construction is referred to as
an arithmetico-geometric series, and its sum may be
computed as follows (Fernandez et al, 2006):
t`(g1,a1, r, d) =
?`
k=1
[a1 + d(k ? 1)] g1rk?1
= a`g`+1 ? a1g11? r +
d (g`+1 ? g1r)
(1? r)2 .
In this expression r, the g1?s and the `?s have the
same values as above, d = ?1/n and the a1?s are
4The ?i value was fixed at the beginning of experimentation
by minimizing the AER on the 10k sentence French-English cor-
pus discussed below.
646
equal to the value of h evaluated at the starting in-
dices, j? and j?; thus, the derivative we seek at each
optimization iteration inside the M-step is:
??L =Ep(ai|ei,f,m,n) [h(i, ai,m, n)]
? 1Z?
(tj?(e?h(i,j?,m,n), h(i, j?,m, n), r, d)
+ tn?j?(e?h(i,j?,m,n), h(i, j?,m, n), r, d)).
4 Experiments
In this section we evaluate the performance of
our proposed model empirically. Experiments are
conducted on three datasets representing different
language typologies and dataset sizes: the FBIS
Chinese-English corpus (LDC2003E14); a French-
English corpus consisting of version 7 of the Eu-
roparl and news-commentary corpora;5 and a large
Arabic-English corpus consisting of all parallel data
made available for the NIST 2012 Open MT evalua-
tion. Table 1 gives token counts.
We begin with several preliminary results. First,
we quantify the benefit of using the geometric series
trick (?3.2) for computing the partition function rel-
ative to na??ve summation. Our method requires only
0.62 seconds to compute all partition function values
for 0 < i,m, n < 150, whereas the na??ve algorithm
requires 6.49 seconds for the same.6
Second, using a 10k sample of the French-English
data set (only 0.5% of the corpus), we determined
1) whether p0 should be optimized; 2) what the op-
timal Dirichlet parameters ?i are; and 3) whether
the commonly used ?staged initialization? procedure
(in which Model 1 parameters are used to initialize
Model 2, etc.) is necessary for our model. First,
like Och and Ney (2003) who explored this issue for
training Model 3, we found that EM tended to find
poor values for p0, producing alignments that were
overly sparse. By fixing the value at p0 = 0.08,
we obtained minimal AER. Second, like Riley and
Gildea (2012), we found that small values of ? im-
proved the alignment error rate, although the im-
pact was not particularly strong over large ranges of
5http://www.statmt.org/wmt12
6While this computational effort is a small relative to the
total cost in EM training, in algorithms where ? changes more
rapidly, for example in Bayesian posterior inference with Monte
Carlo methods (Chahuneau et al, 2013), this savings can have
substantial value.
Table 1: CPU time (hours) required to train alignment
models in one direction.
Language Pair Tokens Model 4 Log-linear
Chinese-English 17.6M 2.7 0.2
French-English 117M 17.2 1.7
Arabic-English 368M 63.2 6.0
Table 2: Alignment quality (AER) on the WMT 2012
French-English and FBIS Chinese-English. Rows with
EM use expectation maximization to estimate the ?f , and
?Dir use variational Bayes.
Model Estimator FR-EN ZH-EN
Model 1 EM 29.0 56.2
Model 1 ?Dir 26.6 53.6
Model 2 EM 21.4 53.3
Log-linear EM 18.5 46.5
Log-linear ?Dir 16.6 44.1
Model 4 EM 10.4 45.8
Table 3: Translation quality (BLEU) as a function of
alignment type.
Language Pair Model 4 Log-linear
Chinese-English 34.1 34.7
French-English 27.4 27.7
Arabic-English 54.5 55.7
?. Finally, we (perhaps surprisingly) found that the
standard staged initialization procedure was less ef-
fective in terms of AER than simply initializing our
model with uniform translation probabilities and a
small value of ? and running EM. Based on these
observations, we fixed p0 = 0.08, ?i = 0.01, and
set the initial value of ? to 4 for the remaining ex-
periments.7
We next compare the alignments produced by our
model to the Giza++ implementation of the standard
IBM models using the default training procedure
and parameters reported in Och and Ney (2003).
Our model is trained for 5 iterations using the pro-
cedure described above (?3.3). The algorithms are
7As an anonymous reviewer pointed out, it is a near certainty
that tuning of these hyperparameters for each alignment task
would improve results; however, optimizing hyperparameters of
alignment models is quite expensive. Our intention is to show
that it is possible to obtain reasonable (if not optimal) results
without careful tuning.
647
compared in terms of (1) time required for training;
(2) alignment error rate (AER, lower is better);8 and
(3) translation quality (BLEU, higher is better) of hi-
erarchical phrase-based translation system that used
the alignments (Chiang, 2007). Table 1 shows the
CPU time in hours required for training (one direc-
tion, English is generated). Our model is at least
10? faster to train than Model 4. Table 3 reports
the differences in BLEU on a held-out test set. Our
model?s alignments lead to consistently better scores
than Model 4?s do.9
5 Conclusion
We have presented a fast and effective reparameteri-
zation of IBM Model 2 that is a compelling replace-
ment for for the standard Model 4. Although the
alignment quality results measured in terms of AER
are mixed, the alignments were shown to work ex-
ceptionally well in downstream translation systems
on a variety of language pairs.
Acknowledgments
This work was sponsored by the U. S. Army Research
Laboratory and the U. S. Army Research Office under
contract/grant number W911NF-10-1-0533.
References
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
V. Chahuneau, N. A. Smith, and C. Dyer. 2013.
Knowledge-rich morphological priors for Bayesian
language models. In Proc. NAACL.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
R. Courant and H. Robbins. 1996. The geometric pro-
gression. In What Is Mathematics?: An Elementary
Approach to Ideas and Methods, pages 13?14. Oxford
University Press.
8Our Arabic training data was preprocessed using a seg-
mentation scheme optimized for translation (Habash and Sadat,
2006). Unfortunately the existing Arabic manual alignments
are preprocessed quite differently, so we did not evaluate AER.
9The alignments produced by our model were generally
sparser than the corresponding Model 4 alignments; however,
the extracted grammar sizes were sometimes smaller and some-
times larger, depending on the language pair.
P. A. Fernandez, T. Foregger, and J. Pahikkala. 2006.
Arithmetic-geometric series. PlanetMath.org.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
D. Riley and D. Gildea. 2012. Improving the IBM align-
ment models using Variational Bayes. In Proc. of ACL.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
648
Proceedings of NAACL-HLT 2013, pages 661?667,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supersense Tagging for Arabic: the MT-in-the-Middle Attack
Nathan Schneider? Behrang Mohit? Chris Dyer? Kemal Oflazer? Noah A. Smith?
School of Computer Science
Carnegie Mellon University
?Pittsburgh, PA 15213, USA
?Doha, Qatar
{nschneid@cs.,behrang@,cdyer@cs.,ko@cs.,nasmith@cs.}cmu.edu
Abstract
We consider the task of tagging Arabic nouns
with WordNet supersenses. Three approaches
are evaluated. The first uses an expert-
crafted but limited-coverage lexicon, Arabic
WordNet, and heuristics. The second uses un-
supervised sequence modeling. The third and
most successful approach uses machine trans-
lation to translate the Arabic into English,
which is automatically tagged with English
supersenses, the results of which are then pro-
jected back into Arabic. Analysis shows gains
and remaining obstacles in four Wikipedia
topical domains.
1 Introduction
A taxonomic view of lexical semantics groups word
senses/usages into categories of varying granulari-
ties. WordNet supersense tags denote coarse seman-
tic classes, including person and artifact (for nouns)
and motion and weather (for verbs); these categories
can be taken as the top level of a taxonomy. Nominal
supersense tagging (Ciaramita and Johnson, 2003)
is the task of identifying lexical chunks in the sen-
tence for common as well as proper nouns, and la-
beling each with one of the 25 nominal supersense
categories. Figure 1 illustrates two such labelings of
an Arabic sentence. Like the narrower problem of
named entity recognition, supersense tagging of text
holds attraction as a way of inferring representations
that move toward language independence. Here we
consider the problem of nominal supersense tagging
for Arabic, a language with ca. 300 million speak-
ers and moderate linguistic resources, including a
WordNet (Elkateb et al, 2006), annotated datasets
(Maamouri et al, 2004; Hovy et al, 2006), monolin-
gual corpora, and large amounts of Arabic-English
parallel data.
The supervised learning approach that is used
in state-of-the-art English supersense taggers (Cia-
. HA

?J
J.?

J? @
	
Y
	
? @?
	
K ?? ?? ?
	
?? ?


	
?
	
Y
	
? @?
	
J? @ QK
Y? ??j

JK

Ann-A Gloss Ann-B
controls
communication
manager
communicationthe-windows
in
attribute configuration relation
shape and-layout shape
communication
windows
communicationthe-applications
?The window manager controls the configuration and
layout of application windows.?
Figure 1: A sentence from the ?X Window System? ar-
ticle with supersense taggings from two annotators and
post hoc English glosses and translation.
ramita and Altun, 2006) is problematic for Ara-
bic, since there are supersense annotations for only
a small amount of Arabic text (65,000 words by
Schneider et al, 2012, versus the 360,000 words that
are annotated for English). Here, we reserve that
corpus for development and evaluation, not training.
We explore several approaches in this paper, the
most effective of which is to (1) translate the Arabic
sentence into English, returning the alignment struc-
ture between the source and target, (2) apply En-
glish supersense tagging to the target sentence, and
(3) heuristically project the tags back to the Arabic
sentence across these alignments. This ?MT-in-the-
middle? approach has also been successfully used
for mention detection (Zitouni and Florian, 2008)
and coreference resolution (Rahman and Ng, 2012).
We first discuss the task and relevant resources
(?2), then the approaches we explored (?3), and fi-
nally present experimental results and analysis in ?4.
2 Task and Resources
A gold standard corpus of sentences annotated
with nominal supersenses (as in figure 1) fa-
cilitates automatic evaluation of supersense tag-
gers. For development and evaluation we use
661
the AQMAR Arabic Wikipedia Supersense Corpus1
(Schneider et al, 2012), which augmented a named
entity corpus (Mohit et al, 2012) with nominal
supersense tags. The corpus consists of 28 ar-
ticles selected from four topical areas: history
(e.g., ?Islamic Golden Age?), science (?Atom?),
sports (?Real Madrid?), and technology (?Linux?).
Schneider et al (2012) found the distributions of
supersense categories in these four topical domains
to be markedly different; e.g., most instances of
communication (which includes kinds of software)
occurred in the technology domain, whereas most
substances were found in the science domain.
The 18 test articles have 1,393 sentences (39,916
tokens) annotated at least once.2 As the corpus
was released with two annotators? (partially overlap-
ping) taggings, rather than a single gold standard,
we treat the output of each annotator as a separate
test set. Both annotated some of every article; the
first (Ann-A) annotated 759 sentences, the second
(Ann-B) 811 sentences.
Lexicon. What became known as ?supersense
tags? arose from a high-level partitioning of synsets
in the original English WordNet (Fellbaum, 1998)
into lexicographer files. Arabic WordNet (AWN)
(Elkateb et al, 2006) allows us to recover super-
sense categories for some 10,500 Arabic nominal
types, since many of the synsets in AWN are cross-
referenced to English WordNet, and can therefore
be associated with supersense categories. Further,
OntoNotes contains named entity annotations for
Arabic (Hovy et al, 2006).
From these, we construct an Arabic supersense
lexicon, mapping Arabic noun lemmas to supersense
tags. This lexicon contains 23,000 types, of which
11,000 are multiword units. Token coverage of the
test set is 18% (see table 1). Lexical units encoun-
tered in the test data were up to 9-ways supersense-
ambiguous; the average ambiguity of in-vocabulary
tokens was 2.0 supersenses.
Unlabeled Arabic text. For unsupervised learn-
ing we collected 100,000 words of Arabic Wikipedia
text, not constrained by topic. The articles in this
sample were subject to a minimum length threshold
1http://www.ark.cs.cmu.edu/ArabicSST
2Our development/test split of the data follows Mohit et al
(2012), but we exclude two test set documents??Light? and
?Ibn Tolun Mosque??due to preprocessing issues.
and are all cross-linked to corresponding articles in
English, Chinese, and German.
Arabic?English machine translation. We used
two independently developed Arabic-English MT
systems. One (QCRI) is a phrase-based system
(Koehn et al, 2003), similar to Moses (Koehn et
al., 2007); the other (cdec) is a hierarchical phrase-
based system (Chiang, 2007), as implemented in
cdec (Dyer et al, 2010). Both were trained on
about 370M tokens of parallel data provided by the
LDC (by volume, mostly newswire and UN data).
Each system includes preprocessing for Arabic mor-
phological segmentation and orthographic normal-
ization.3 The QCRI system used a 5-gram modi-
fied Kneser-Ney language model that generated full-
cased forms (Chen and Goodman, 1999). cdec
used a 4-gram KN language model over lowercase
forms and was recased in a post-processing step.
Both language models were trained using the Giga-
word v. 4 corpus. Both systems were tuned to opti-
mize BLEU on a held-out development set (Papineni
et al, 2002).
English supersense tagger. For English super-
sense tagging, an open-source reimplementation of
the approach of Ciaramita and Altun (2006) was
released by Michael Heilman.4 This tagger was
trained on the SemCor corpus (Miller et al, 1993)
and achieves 77% F1 in-domain.
3 Methods
We explored 3 approaches to the supersense tagging
of Arabic: heuristic tagging with a lexicon, unsuper-
vised sequence tagging, and MT-in-the-middle.
3.1 Heuristic Tagging with a Lexicon
Using the lexicon built from AWN and OntoNotes
(see ?2), our heuristic approach works as follows:
1. Stem and vocalize; we used MADA (Habash
and Rambow, 2005; Roth et al, 2008).
2. Greedily detect word sequences matching lexi-
con entries from left to right.
3. If a lexicon entry has more than one associated
supersense, Arabic WordNet synsets are
3QCRI accomplishes this using MADA (Habash and Ram-
bow, 2005; Roth et al, 2008). cdec includes a custom CRF-
based segmenter and standard normalization rules.
4http://www.ark.cs.cmu.edu/mheilman/questions
662
E? person location artifact substance Automatic English supersense tagging
e? 1 2 3 4 5 6 7 8 9 English sentence
a 1 2 3 4 5 6 Arabic sentence (e.g., token 6 aligns to English tokens 7?9)
N P N A N N Arabic POS tagging
A? person location artifact Projected supersense tagging
Figure 2: A hypothetical aligned sentence pair of 9 English words (with their supersense tags) and 6 Ara-
bic words (with their POS tags). Step 4 of the projection procedure constructs the Arabic-to-English mapping
{1?person11, 4?location
4
3, {5, 6}?artifact
7
6}, resulting in the tagging shown in the bottom row.
weighted to favor earlier senses (presumed
by lexicographers to be more frequent) and
then the supersense with the greatest aggregate
weight is selected. Formally: Let senses(w) be
the ordered list of AWN senses of lemma w.
Let senses(w, s) ? senses(w) be those senses
that map to a given supersense s. We choose
arg maxs(|senses(w, s)|/ mini:senses(w)i?senses(w,s) i).
3.2 Unsupervised Sequence Models
Unsupervised sequence labeling is our second ap-
proach (Merialdo, 1994). Although it was largely
developed for part-of-speech tagging, the hope is
to use in-domain Arabic data (the unannotated
Wikipedia corpus discussed in ?2) to infer clus-
ters that correlate well with supersense groupings.
We applied the generative, feature-based model of
Berg-Kirkpatrick et al (2010), replicating a feature-
set used previously for NER (Mohit et al, 2012)?
including context tokens, character n-grams, and
POS?and adding the vocalized stem and several
stem shape features: 1) ContainsDigit?; 2) dig-
its replaced by #; 3) digit sequences replaced by
# (for stems mixing digits with other characters);
4) YearLike??true for 4-digit numerals starting with
19 or 20; 5) LatinWord?, per the morphological an-
alysis; 6) the shape feature of Ciaramita and Al-
tun (2006) (Latin words only). We used 50 itera-
tions of learning (tuned on dev data). For evaluation,
a many-to-one mapping from unsupervised clusters
to supersense tags is greedily induced to maximize
their correspondence on evaluation data.
3.3 MT-in-the-Middle
A standard approach to using supervised linguistic
resources in a second language is cross-lingual pro-
jection (Yarowsky and Ngai, 2001; Yarowsky et al,
2001; Smith and Smith, 2004; Hwa et al, 2005; Mi-
halcea et al, 2007; Burkett and Klein, 2008; Burkett
et al, 2010; Das and Petrov, 2011; Kim et al, 2012,
who use parallel sentences extracted from Wikipedia
for NER). The simplest such approach starts with an
aligned parallel corpus, applies supersense tagging
to the English side, and projects the labels through
the word alignments. A supervised monolingual tag-
ger is then trained on the projected labels. Prelimi-
nary experiments, however, showed that this under-
performed even the simple heuristic baseline above
(likely due to domain mismatch), so it was aban-
doned in favor of a technique that we call MT-in-
the-middle projection.
This approach does not depend on having par-
allel data in the training domain, but rather on an
Arabic?English machine translation system that
can be applied to the sentences we wish to tag. The
approach is inspired by token-level pseudo-parallel
data methods of previous work (Zitouni and Flo-
rian, 2008; Rahman and Ng, 2012). MT output for
this language pair is far from perfect?especially for
Wikipedia text, which is distant from the domain
of the translation system?s training data?but, in the
spirit of Church and Hovy (1993), we conjecture that
it may still be useful. The method is as follows:
1. Preprocess the input Arabic sentence a to
match the decoder?s model of Arabic.
2. Translate the sentence, recovering not just
the English output e? but also the deriva-
tion/alignment structure z relating words and/or
phrases of the English output to words and/or
phrases of the Arabic input.
3. Apply the English supersense tagger to the En-
glish translation, discarding any verbal super-
sense tags. Call the tagger output E?.
4. Project the supersense tags back to the Ara-
bic sentence, yielding A?: Each Arabic token
a ? a that is (a) a noun, or (b) an adjec-
tive following 0 or more adjectives following a
noun is mapped to the first English supersense
mention in E? containing some word aligned
to a. Then, for each English supersense men-
663
Coverage Ann-A Ann-B
Nouns All Tokens Mentions P R F1 P R F1
Lexicon heuristics (?3.1) 8,058 33% 8,465 18% 8,407 32 55 16 29 21.6 37.9 29 53 15 27 19.4 35.6
Unsupervised (?3.2) 20 59 16 48 17.5 52.6 14 56 10 39 11.6 45.9
MT-in-the-middle
(?3.3)
QCRI 14,401 59% 16,461 35% 12,861 34 65 27 50 29.9 56.4 36 64 28 51 31.6 56.6
cdec 14,270 58% 15,542 33% 13,704 37 69 31 57 33.8 62.4 38 67 32 56 34.6 61.0
MTitM + Lex. cdec 16,798 68% 18,461 40% 16,623 35 64 36 65 35.5 64.6 36 63 36 63 36.0 63.2
Table 1: Supersense tagging results on the test set: coverage measures5 and gold-standard evaluation?exact la-
beled/unlabeled6 mention precision, recall, and F-score against each annotator. The last row is a hybrid: MT-in-the-
middle followed by lexicon heuristics to improve recall. Best single-technique and best hybrid results are bolded.
tion, all its mapped Arabic words are grouped
into a single mention and the supersense cat-
egory for that mention is projected. Figure 2
illustrates this procedure. The cdec system
provides word alignments for its translations
derived from the training data; whereas QCRI
only produces phrase-level alignments, so for
every aligned phrase pair ?a?, e?? ? z, we con-
sider every word in a? as aligned to every word
in e? (introducing noise when English super-
sense mention boundaries do not line up with
phrase boundaries).
4 Experiments and Analysis
Table 1 compares the techniques (?3) for full Arabic
supersense tagging.7 The number of nouns, tokens,
and mentions covered by the automatic tagging is
reported, as is the mention-level evaluation against
human annotations. The evaluation is reported sep-
arately for the two annotators in the dataset.
With heuristic lexicon lookup, 18% of the tokens
are marked as part of a nominal supersense mention.
Both labeled and unlabeled mention recall with this
method are below 30%; labeled precision is about
30%, and unlabeled mention precision is above
50%. From this we conclude that the biggest prob-
lems are (a) out-of-vocabulary items and (b) poor
semantic disambiguation of in-vocabulary items.
The unsupervised sequence tagger does even
worse on the labeled evaluation. It has some success
at detecting supersense mentions?unlabeled recall
is substantially improved, and unlabeled precision is
5The unsupervised evaluation greedily maps clusters to tags,
separately for each version of the test set; coverage numbers
thus differ and are not shown here.
6Unlabeled tagging refers to noun chunk detection only.
7It was produced in part using the chunkeval.py script: see
https://github.com/nschneid/pyutil
slightly improved. But it seems to be much worse
at assigning semantic categories; the number of la-
beled true positive mentions is actually lower than
with the lexicon-based approach.
MT-in-the-middle is by far the most success-
ful single approach: both systems outperform the
lexicon-only baseline by about 10 F1 points, de-
spite many errors in the automatic translation, En-
glish tagging, and projection, as well as underlying
linguistic differences between English and Arabic.
The baseline?s unlabeled recall is doubled, indicat-
ing substantially more nominal expressions are de-
tected, in addition to the improved labeled scores.
We further tested simple hybrids combining the
lexicon-based and MT-based approaches. Applying
MT-in-the-middle first, then expanding token cover-
age with the lexicon improves recall at a small cost
to precision (table 1, last row). Combining the tech-
niques in the reverse order is slightly worse than MT-
based projection without consulting the lexicon.
MT-in-the middle improves upon the lexicon-only
baseline, yet performance is still dwarfed by the su-
pervised English tagger (at least in the SemCor eval-
uation; see ?2), and also well below the 70% inter-
annotator F1 reported by Schneider et al (2012). We
therefore examine the weaknesses of our approach
for Arabic.
4.1 MT for Projection
In analyzing our projection framework, we per-
formed a small-scale MT evaluation with the
Wikipedia data. Reference English translations for
140 Arabic Wikipedia sentences?5 per article in
the corpus?were elicited from a bilingual linguist.
Table 2 compares the two systems under three stan-
dard metrics of overall sentence translation quality.8
8BLEU (Papineni et al, 2002); METEOR (Banerjee and
Lavie, 2005; Lavie and Denkowski, 2009), with default options;
664
. ????@ ?


	
? @Yg.

?Q

	??

?
	
Jj ??@

?J.k. ??

?@?
	
K ??k ??mProceedings of NAACL-HLT 2013, pages 1206?1215,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Knowledge-Rich Morphological Priors for Bayesian Language Models
Victor Chahuneau Noah A. Smith Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{vchahune,nasmith,cdyer}@cs.cmu.edu
Abstract
We present a morphology-aware nonparamet-
ric Bayesian model of language whose prior
distribution uses manually constructed finite-
state transducers to capture the word forma-
tion processes of particular languages. This
relaxes the word independence assumption
and enables sharing of statistical strength
across, for example, stems or inflectional
paradigms in different contexts. Our model
can be used in virtually any scenario where
multinomial distributions over words would
be used. We obtain state-of-the-art results in
language modeling, word alignment, and un-
supervised morphological disambiguation for
a variety of morphologically rich languages.
1 Introduction
Despite morphological phenomena?s salience in
most human languages, many NLP systems treat
fully inflected forms as the atomic units of language.
By assuming independence of lexical stems? vari-
ous surface forms, this avoidance approach exacer-
bates the problem of data sparseness. If it is em-
ployed at all, morphological analysis of text tends
to be treated as a preprocessing step to other NLP
modules. While this latter disambiguation approach
helps address data sparsity concerns, it has substan-
tial drawbacks: it requires supervised learning from
expert-annotated corpora, and determining the op-
timal morphological granularity is labor-intensive
(Habash and Sadat, 2006).
Neither approach fully exploits the finite-state
transducer (FST) technology that has been so suc-
cessful for modeling the mapping between surface
forms and their morphological analyses (Karttunen
and Beesley, 2005), and the mature collections of
high quality transducers that already exist for many
languages (e.g., Turkish, Russian, Arabic). Much
linguistic knowledge is encoded in such FSTs.
In this paper, we develop morphology-aware non-
parametric Bayesian language models that bring to-
gether hand-written FSTs with statistical modeling
and require no token-level annotation. The sparsity
issue discussed above is addressed by hierarchical
priors that share statistical strength across different
inflections of the same stem by backing off to word
formation models that piece together morphemes us-
ing FSTs. Furthermore, because of the nonparamet-
ric formulation of our models, the regular morpho-
logical patterns found in the long tail of word types
will rely more heavily on deeper analysis, while fre-
quent and idiosyncratically behaved forms are mod-
eled opaquely.
Our prior can be used in virtually any generative
model of language as a replacement for multino-
mial distributions over words, bringing morphologi-
cal awareness to numerous applications. For various
morphologically rich languages, we show that:
? our model can provide rudimentary unsuper-
vised disambiguation for a highly ambiguous
analyzer;
? integrating morphology into n-gram language
models allows better generalization to unseen
words and can improve the performance of ap-
plications that are truly open vocabulary; and
? bilingual word alignment models also bene-
fit greatly from sharing translation information
1206
across stems.
We are particularly interested in low-resource sce-
narios, where one has to make the most of the
small quantity of available data, and overcoming
data sparseness is crucial. If analyzers exist in such
settings, they tend to be highly ambiguous, and an-
notated data for learning to disambiguate are also
likely to be scarce or non-existent. Therefore, in our
experiments with Russian, we compare two analyz-
ers: a rapidly-developed guesser, which models reg-
ular inflectional paradigms but contains no lexicon
or irregular forms, and a high-quality analyzer.
2 Word Models with Morphology
In this section, we describe a generative model of
word formation based on Pitman-Yor processes that
generates word types using a finite-state morpho-
logical generator. At a high level, the process first
produces lexicons of stems and inflectional patterns;
then it generates a lexicon of inflected forms us-
ing the finite-state generator. Finally, the inflected
forms are used to generate observed data. Different
independence assumptions can be made at each of
these levels to encode beliefs about where stems, in-
flections, and surface forms should share statistical
strength.
2.1 Pitman-Yor Processes
Our work relies extensively on Pitman-Yor pro-
cesses, which provide a flexible framework for ex-
pressing backoff and interpolation relationships and
extending standard models with richer word distri-
butions (Pitman and Yor, 1997). They have been
shown to match the performance of state-of-the-art
language models and to give estimates that follow
appropriate power laws (Teh, 2006).
A draw from a Pitman-Yor process (PYP), de-
noted G ? PY(d, ?,G0), is a discrete distribution
over a (possibly infinite) set of events, which we de-
note abstractly E . The process is parameterized by a
discount parameter 0 ? d < 1, a strength parameter
? > ?d, and a base distribution G0 over the event
space E .
In this work, our focus is on the base distribution
G0. We place vague priors on the hyperparameters
d ? U([0, 1]) and (? + d) ? Gamma(1, 1). Infer-
ence in PYPs is discussed below.
2.2 Unigram Morphology Model
The most basic expression of our model is a uni-
gram model of text. So far, we only assume that
each word can be analyzed into a stem and a se-
quence of morphemes forming an inflection pattern.
LetGs be a distribution over stems,Gp be a distribu-
tion over inflectional patterns, and let GENERATE be
a deterministic mapping from ?stem, pattern? pairs
to inflected word forms.1 An inflected word type is
generated with the following process, which we des-
ignate MP(Gs, Gd,GENERATE):
stem ? Gs
pattern ? Gp
word = GENERATE(stem, pattern)
For example, in Russian, we might sample stem
= ??????,2 pattern = STEM+Adj+Pl+Dat, and
obtain word = ??????.
This model could be used directly to generate ob-
served tokens. However, we have said nothing about
Gs and Gp, and the assumption that stems and pat-
terns are independent is clearly unsatisfying. We
therefore assume that both the stem and the pattern
distributions are generated from PY processes, and
that MP(Gs, Gp,GENERATE) is itself the base dis-
tribution of a PYP.
Gs ? PY(ds, ?s, G0s)
Gp ? PY(dp, ?p, G0p)
Gw ? PY(d, ?,MP(Gs, Gp,GENERATE))
A draw Gw from this PYP is a unigram distribu-
tion over tokens.
2.3 Base Stem Model G0s
In general there are an unbounded number of stems
possible in any language, so we set G0s to be charac-
ter trigram model, which we statically estimate, with
Kneser-Ney smoothing, from a large corpus of word
types in the language being modeled. While using
fixed parameters estimated to maximize likelihood is
1The assumption of determinism is only inappropriate in
cases of inflectional spelling variants (e.g., modeled vs. mod-
elled) or pronunciation variants (e.g., reduced forms in certain
environments).
2?????? (pronounced [pr5tCij]) = other
1207
questionable from the perspective of Bayesian learn-
ing, it is tremendously beneficial for computational
reasons. For some applications (e.g., word align-
ment), the set of possible stems for a corpus S can be
precomputed, so we will also experiment with using
a uniform stem distribution based on this set.
2.4 Base Pattern Model G0p
Several choices are possible for the base pattern dis-
tribution:
MP0 We can assume a uniformG0p when the num-
ber of patterns is small.
MP1 To be able to generalize to new patterns, we
can draw the length of the pattern from a Poisson
distribution and generate morphemes one by one
from a uniform distribution.
MP2 A more informative prior is a Markov chain
of morphemes, where each morpheme is generated
conditional on the preceding morpheme.
The choice of the base pattern distribution could
depend on the complexity of the inflectional patterns
produced by the morphological analyzer, reflecting
the type of morphological phenomena present in a
given language. For example, the number of possi-
ble patterns can practically be considered finite in
Russian, but this assumption is not valid for lan-
guages with more extensive derivational morphol-
ogy like Turkish.
2.5 Posterior Inference
For most applications, rather than directly gener-
ating from a model using the processes outlined
above, we seek to infer posterior distributions over
latent parameters and structures, given a sample of
data.
Although there is no known analytic form of
the PYP density, it is possible to marginalize the
draws from it and to work directly with observa-
tions. This marginalization produces the classi-
cal Chinese restaurant process representation (Teh,
2006). When working with the morphology mod-
els we are proposing, we also need to marginalize
the different latent forms (stems s and patterns p)
that may have given rise to a given word w. Thus,
we require that the inverse relation of GENERATE is
available to compute the marginal base word distri-
bution:
p(w | G0w) =
?
GENERATE(s,p)=w
p(s | Gs) p(p | Gp)
Since our approach encodes morphology using
FSTs, which are invertible, this poses no problem.
To illustrate, consider the Russian word ??????,
which may be analyzed in several ways:
?????? +Adj +Sg +Neut +Instr
?????? +Adj +Sg +Masc +Instr
?????? +Adj +Pl +Dat
??????? +Verb +Pl +1P
?????? +Pro +Sg +Ins
Because the set of possible analyses is in general
small, marginalization is fast and complex blocked
sampling is not necessary.
Finally, to infer hyperparameter values (d, ?, . . .),
a Metropolis-Hastings update is interleaved with
Gibbs sampling steps for the rest of the hidden vari-
ables.3
Having described a model for generating words,
we now show its usage in several contexts.
3 Unsupervised Morphological
Disambiguation
Given a rule-based morphological analyzer encoded
as an unweighted FST and a corpus on which the
analyzer has been run ? possibly generating multi-
ple analyses for each token ? we can use our un-
igram model to learn a probabilistic model of dis-
ambiguation in an unsupervised setting (i.e., with-
out annotated examples). The corpus is assumed to
be generated from the unigram distribution Gw, and
the base stem model is set to a fixed character tri-
gram model.4 After learning the parameters of the
model, we can find for each word in the vocabulary
its most likely analysis and use this as a crude dis-
ambiguation step.
3The proposal distribution for Metropolis-Hastings is a Beta
distribution (d) or a Gamma distribution (?+d) centered on the
previous parameter values.
4Experiments suggest that this is important to constrain the
model to realistic stems.
1208
3.1 Morphological Guessers
Finite-state morphological analyzers are usually
specified in three parts: a stem lexicon, which de-
fines the words in the language and classifies them
into several categories according to their grammat-
ical function and their morphological properties; a
set of prefixes and suffixes that can be applied to
each category to form surface words; and possibly
alternation rules that can encode exceptions and
spelling variations. The combination of these parts
provides a powerful framework for defining a gener-
ative model of words. Such models can be reversed
to obtain an analyzer. However, while the two latter
parts can be relatively easy to specify, enumerating
a comprehensive stem lexicon is a time consuming
and necessarily incomplete process, as some cate-
gories are truly open-class.
To allow unknown words to be analyzed, one
can use a guesser that attempts to analyze words
missing in the lexicon. Can we eliminate the stem
lexicon completely and use only the guesser? This
is what we try to do by designing a lexicon-free
analyzer for Russian. A guesser was developed
in three hours; it is prone to over-generation and
produces ambiguous analyses for most words
but covers a large number of morphological phe-
nomena (gender, case, tense, etc.). For example,
the word ??????5 can be correctly analyzed as
?????+Noun+Masc+Prep+Sg but also as the in-
correct forms: ??????+Verb+Pres+2P+Pl,
??????+Noun+Fem+Dat+Sg, ????-
??+Noun+Fem+Prep+Sg, and more.
3.2 Disambiguation Experiments
We train the unigram model on a 1.7M-word cor-
pus of TED talks transcriptions translated into Rus-
sian (Cettolo et al, 2012) and evaluate our ana-
lyzer against a test set consisting of 1,500 gold-
standard analyses obtained from the morphology
disambiguation task of the DIALOG 2010 confer-
ence (Lya?evskaya et al, 2010).6
Each analysis is composed of a lemma (?????),
a part of speech (Noun), and a sequence of ad-
ditional functional morphemes (Masc,Prep,Sg).
We consider only open-class categories: nouns, ad-
5?????? = Hebrew (masculine noun, prepositional case)
6http://ru-eval.ru
jectives, adverbs and verbs, and evaluate the output
of our model with three metrics: the lemma accu-
racy, the part-of-speech accuracy, and the morphol-
ogy F -measure.7
As a baseline, we consider picking a random anal-
ysis from output of the analyzer or choosing the
most frequent lemma and the most frequent morpho-
logical pattern.8 Then, we use our model with each
of the three versions of the pattern model described
in ?2.2. Finally, as an upper bound, we use the gold
standard to select one of the analyses produced by
the guesser.
Since our evaluation is not directly comparable
to the standard for this task, we use for reference
a high-quality analyzer from Xerox9 disambiguated
with the MP0 model (all of the models have very
close accuracy in this case).
Model Lemma POS Morph.
Random 29.8 70.9 50.2
Frequency 31.1 74.4 48.8
Guesser MP0 60.0 82.2 66.3
Guesser MP1 58.9 82.5 69.5
Guesser MP2 59.9 82.4 65.5
Guesser oracle 68.4 84.9 83.0
Xerox MP0 83.6 96.4 78.1
Table 1: Russian morphological disambiguation.
Considering the amount of effort put in develop-
ing the guesser, the baseline POS tagging accuracy
is relatively good. However, the disambiguation is
largely improved by using our unigram model with
respect to all the evaluation categories. We are still
far from the performance of a high-quality analyzer
but, in absence of such a resource, our technique
might be a sensible option. We also note that there is
no clear winner in terms of pattern model, and con-
clude that this choice is task-specific.
7F -measure computed for the set of additional morphemes
and averaged over the words in the corpus.
8We estimate these frequencies by assuming each analysis of
each token is uniformly likely, then summing fractional counts.
9http://open.xerox.com/Services/
fst-nlp-tools/Pages/morphology
1209
4 Open Vocabulary Language Models
We now integrate our unigram model in a hierar-
chical Pitman-Yor n-gram language model (Fig. 1).
The training corpus words are assumed to be
generated from a distribution Gnw drawn from
PY(dn, ?n, Gn?1w ), where Gn?1w is defined recur-
sively down to the base model G0w. Previous work
Teh (2006) simply used G0w = U(V ) where V is
the word vocabulary, but in our case G0w is the MP
defined in ?2.2.
G2wG
3
w G
1
w
d3, ?3 d2, ?2 d1, ?1
Gs
ds, ?s
Gp G0p
dp, ?p
G0sw
Figure 1: The trigram version of our language model rep-
resented as a graphical model. G1w is the unigram model
of ?2.2.
We are interested in evaluating our model in an
open vocabulary scenario where the ability to ex-
plain new unseen words matters. We expect our
model to be able to generalize better thanks to the
combination of a morphological analyzer and a stem
distribution which is less sparse than the word dis-
tribution (for example, for the 1.6M word Turkish
corpus, |V | ? 3.5|S| ? 140k).
To integrate out-of-vocabulary words in our eval-
uation, we use infinite base distributions: G0w (in the
baseline model) or G0s (in the MP) are character tri-
gram models. We define perplexity of a held-out test
corpus in the standard way:
ppl = exp
(
?
1
N
N?
i=1
log p (wi | wi?n+1 ? ? ?wi?1)
)
but compared to the common practice, we do not
need to discount OOVs from this sum since the
model vocabulary is infinite. Note that we also
marginalize by summing over all the possible analy-
ses for a given word when computing its base prob-
ability according to the MP.
4.1 Language Modeling Experiments
We train several trigram models on the Russian TED
talks corpus used in the previous section. Our base-
line is a hierarchical PY trigram model with a tri-
gram character model as the base word distribution.
We compare it with our model using the same char-
acter model for the base stem distribution. Both of
the morphological analyzers described in the previ-
ous section help obtaining perplexity reductions (Ta-
ble 2). We ran a similar experiment on the Turkish
version of this corpus (1.6M words) with a high-
quality analyzer (Oflazer, 1994) and obtain even
larger gains (Table 3).
Model ppl
PY-character LM 563
Guesser MP2 530
Xerox MP2 522
Table 2: Evaluation of the Russian n-gram model.
Model ppl
PY-character LM 1,640
Oflazer MP2 1,292
Table 3: Evaluation of the Turkish n-gram model.
These results can partly be attributed to the high
OOV rate in these conditions: 4% for the Russian
corpus and 6% for the Turkish corpus.
4.2 Predictive Text Input
It is difficult to know whether a decrease in perplex-
ity, as measured in the previous section, will result in
a performance improvement in downstream applica-
tions. As a confirmation that correctly modeling new
words matters, we consider a predictive task with a
truly open vocabulary and that requires only a lan-
guage model: predictive text input.
Given some text, we encode it using a lossy de-
terministic character mapping, and try to recover the
original content by computing the most likely word
sequence. This task is inspired by predictive text
input systems available on cellphones with a 9-key
keypad. For example, the string gave me a cup
is encoded as 4283 63 2 287, which could also
be decoded as: hate of a bus.
1210
Silfverberg et al (2012) describe a system de-
signed for this task in Finnish, which is composed
of a weighted finite-state morphological analyzer
trained on IRC logs. However, their system is re-
stricted to words that are encoded in the analyzer?s
lexicon and does not use context for disambiguation.
In our experiments, we use the same Turkish TED
talks corpus as the previous section. As a baseline,
we use a trigram character language model. We pro-
duce a character lattice which encodes all the pos-
sible interpretations for a word and compose it with
a finite-state representation of the character LM us-
ing OpenFST (Allauzen et al, 2007). Alternatively,
we can use a unigram word model to decode this lat-
tice, backing off to the character language model if
no solution is found. Finally, to be able to make use
of word context, we can extract the k most likely
paths according to the character LM and produce a
word lattice, which is in turn decoded with a lan-
guage model defined over the extracted vocabulary.
Model WER CER
Character LM 48.37 16.72
1-gram + character LM 8.50 3.28
1-gram MP2 6.46 2.37
3-gram + character LM 7.86 3.07
3-gram MP2 5.73 2.15
Table 4: Evaluation of Turkish predictive text input.
We measure word and character error rate (WER,
CER) on the predicted word sequence and observe
large improvements in both of these metrics by mod-
eling morphology, both at the unigram level and
when context is used (Table 4).
Preliminary experiments with a corpus of 1.6M
Turkish tweets, an arguably more appropriate do-
main this task, show smaller but consistent improv-
ing: the trigram word error rate is reduced from 26%
to 24% when our model is used.
4.3 Limitations
While our model is an important step forward in
practical modeling of OOVs using morphological
processes, we have made the linguistically naive as-
sumption that morphology applies inside the lan-
guage?s lexicon but has no effect on the process that
put inflected lexemes together into sentences. In this
regard, our model is a minor variant on traditional n-
gram models that work with ?opaque? word forms.
How to best relax this assumption in a computation-
ally tractable way is an important open question left
for future work.
5 Word Alignment Model
Monolingual models of language are not the only
models that can benefit from taking into account
morphology. In fact, alignment models are a good
candidate for using richer word distributions: they
assume a target word distribution conditioned on
each source word. When the target language is mor-
phologically rich, classic independence assumptions
produce very weak models unless some kind of pre-
processing is applied to one side of the corpus. An
alternative is to use our unigram model as a word
translation distribution for each source word in the
corpus.
Our alignment model is based on a simple variant
of IBM Model 2 where the alignment distribution is
only controlled by two parameters, ? and p0 (Dyer et
al., 2013). p0 is the probability of the null alignment.
For a source sentence f of length n, a target sentence
e of lengthm and a latent alignment a, we define the
following alignment link probabilities (j 6= 0):
p(ai = j | n,m) ? (1? p0) exp
(
??
?
?
?
?
i
m ?
j
n
?
?
?
?
)
? controls the flatness of this distribution: larger val-
ues make the probabilities more peaked around the
diagonal of the alignment matrix.
Each target word is then generated given a source
word and a latent alignment link from the word
translation distribution p(ei | fai , Gw). Note that
this is effectively a unigram distribution over tar-
get words, albeit conditioned on the source word
fj . Here is where our model differs from classic
alignment models: the unigram distribution Gw is
assumed be generated from a PY process. There are
two choices for the base word distribution:
? As a baseline, we use a uniform base distribu-
tion over the target vocabulary: G0w = U(V ).
? We define a stem distribution Gs[f ] for each
source word f , a shared pattern distributionGp,
and set G0w[f ] = MP(Gs[f ], Gp). In this case,
1211
we obtain the model depicted in Fig. 2. The
stem and the pattern models are also given PY
priors with uniform base distribution (G0s =
U(S)).
Finally, we put uninformative priors on the align-
ment distribution parameters: p0 ? Beta(?, ?) is
collapsed and ? ? Gamma(k, ?) is inferred using
Metropolis-Hastings.
f e
a
p0 
Gw
dw, ?w
Gp
G0sGs
G0p
dp, ?p
?, 
ds, ?s
Figure 2: Our alignment model, represented as a graphi-
cal model.
Experiments
We evaluate the alignment error rate of our models
for two language pairs with rich morphology on the
target side. We compare to alignments inferred us-
ing IBM Model 4 trained with EM (Brown et al,
1993),10 a version of our baseline model (described
above) without PY priors (learned using EM), and
the PY-based baseline. We consider two language
pairs.
English-Turkish We use a 2.8M word cleaned
version of the South-East European Times corpus
(Tyers and Alperen, 2010) and gold-standard align-
ments from ?akmak et al (2012). Our morphologi-
cal analyzer is identical to the one used in the previ-
ous sections.
English-Czech We use the 1.3M word News
Commentary corpus and gold-standard alignments
10We use the default GIZA++ stage training scheme:
Model 1 + HMM + Model 3 + Model 4.
from Bojar and Prokopov? (2006). The morpholog-
ical analyzer is provided by Xerox.
Results Results are shown in Table 5. Our lightly
parameterized model performs much better than
IBM Model 4 in these small-data conditions. With
an identical model, we find PY priors outperform
traditional multinomial distributions. Adding mor-
phology further reduced the alignment error rate, for
both languages.
AER
Model en-tr en-cs
Model 4 52.1 34.5
EM 43.8 28.9
PY-U(V ) 39.2 25.7
PY-U(S) 33.8 24.8
Table 5: Word alignment experiments on English-Turkish
(en-tr) and English-Czech (en-cs) data.
As an example of how our model generalizes bet-
ter, consider the sentence pair in Fig. 3, taken from
the evaluation data. The two words composing the
Turkish sentence are not found elsewhere in the cor-
pus, but several related inflections occur.11 It is
therefore trivial for the stem-base model to find the
correct alignment (marked in black), while all the
other models have no evidence for it and choose an
arbitrary alignment (gray points).
I w
a
s
n
o
t
a
b
l
e
t
o
f
i
n
i
s
h
m
y
h
o
m
e
w
o
r
k
?devimi
bitiremedim
Figure 3: A complex Turkish-English word alignment
(alignment points in gray: EM/PY-U(V ); black: PY-
U(S)).
6 Related Work
Computational morphology has received consider-
able attention in NLP since the early work on two-
level morphology (Koskenniemi, 1984; Kaplan and
11?devinin, ?devini, ?devleri; bitmez, bitirileceg?inden,
bitmesiyle, ...
1212
Kay, 1994). It is now widely accepted that finite-
state transducers have sufficient power to express
nearly all morphological phenomena, and the XFST
toolkit (Beesley and Karttunen, 2003) has con-
tributed to the practical adoption of this modeling
approach. Recently, open-source tools have been re-
leased: in this paper, we used Foma (Hulden, 2009)
to develop the Russian guesser.
Since some inflected forms have several possible
analyses, there has been a great deal of work on se-
lecting the intended one in context (Hakkani-T?r et
al., 2000; Hajic? et al, 2001; Habash and Rambow,
2005; Smith et al, 2005; Habash et al, 2009). Our
disambiguation model is closely related to genera-
tive models used for this purpose (Hakkani-T?r et
al., 2000).
Rule-based analysis is not the only approach
to modeling morphology, and many unsupervised
models have been proposed.12 Heuristic segmenta-
tion approaches based on the minimum description
length principle (Goldsmith, 2001; Creutz and La-
gus, 2007; de Marcken, 1996; Brent et al, 1995)
have been shown to be effective, and Bayesian
model-based versions have been proposed as well
(Goldwater et al, 2011; Snyder and Barzilay, 2008;
Snover and Brent, 2001). In ?3, we suggested a third
way between rule-based approaches and fully un-
supervised learning that combines the best of both
worlds.
Morphological analysis or segmentation is crucial
to the performance of several applications: machine
translation (Goldwater and McClosky, 2005; Al-
Haj and Lavie, 2010; Oflazer and El-Kahlout, 2007;
Minkov et al, 2007; Habash and Sadat, 2006, in-
ter alia), automatic speech recognition (Creutz et al,
2007), and syntactic parsing (Tsarfaty et al, 2010).
Several methods have been proposed to integrate
morphology into n-gram language models, includ-
ing factored language models (Bilmes and Kirch-
hoff, 2003), discriminative language modeling (Ar?-
soy et al, 2008), and more heuristic approaches
(Monz, 2011).
Despite the fundamentally open nature of the lex-
icon (Heaps, 1978), there has been distressingly lit-
12Developing a high-coverage analyzer can be a time-
consuming process even with the simplicity of modern toolkits,
and unsupervised morphology learning is an attractive problem
for computational cognitive science.
tle attention to the general problem of open vocabu-
lary language modeling problem (most applications
make a closed-vocabulary assumption). The classic
exploration of open vocabulary language modeling
is Brown et al (1992), which proposed the strategy
of interpolating between word- and character-based
models. Character-based language models are re-
viewed by Carpenter (2005). So-called hybrid mod-
els that model both words and sublexical units have
become popular in speech recognition (Shaik et al,
2012; Parada et al, 2011; Bazzi, 2002). Open-
vocabulary language language modeling has also re-
cently been explored in the context of assistive tech-
nologies (Roark, 2009).
Finally, Pitman-Yor processes (PYPs) have be-
come widespread in natural language processing
since they are natural power-law generators. It has
been shown that the widely used modified Kneser-
Ney estimator (Chen and Goodman, 1998) for n-
gram language models is an approximation of the
posterior predictive distribution of a language model
with hierarchical PYP priors (Goldwater et al, 2011;
Teh, 2006).
7 Conclusion
We described a generative model which makes use
of morphological analyzers to produce richer word
distributions through sharing of statistical strength
between stems. We have shown how it can be in-
tegrated into several models central to NLP appli-
cations and have empirically validated the effective-
ness of these changes. Although this paper mostly
focused on languages that are well studied and for
which high-quality analyzers are available, our mod-
els are especially relevant in low-resource scenarios
because they do not require disambiguated analyses.
In future work, we plan to apply these techniques to
languages such as Kinyarwanda, a resource-poor but
morphologically rich language spoken in Rwanda.
It is our belief that knowledge-rich models can help
bridge the gap between low- and high-resource lan-
guages.
Acknowledgments
We thank Kemal Oflazer for making his Turkish lan-
guage morphological analyzer available to us and Bren-
dan O?Connor for gathering the Turkish tweets used in
1213
the predictive text experiments. This work was spon-
sored by the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant number
W911NF-10-1-0533.
References
H. Al-Haj and A. Lavie. 2010. The impact
of Arabic morphological segmentation on broad-
coverage English-to-Arabic statistical machine trans-
lation. Proc. of AMTA.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Implementation and Application of Au-
tomata, pages 11?23.
Ebru Ar?soy, Brian Roark, Izhak Shafran, and Murat
Sara?lar. 2008. Discriminative n-gram language mod-
eling for Turkish. In Proc. of Interspeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, MIT.
K.R. Beesley and L. Karttunen. 2003. Finite-state mor-
phology: Xerox tools and techniques. CSLI, Stanford.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
Proc. of NAACL.
Ondr?ej Bojar and Magdalena Prokopov?. 2006. Czech-
English word alignment. In Proc. of LREC.
Michael R. Brent, Sreerama K. Murthy, and Andrew
Lundberg. 1995. Discovering morphemic suffixes: A
case study in MDL induction. In Proceedings of the
Fifth International Workshop on Artificial Intelligence
and Statistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, Robert L. Mercer, and Jennifer C. Lai.
1992. An estimate of an upper bound for the entropy
of English. Computational Linguistics, 18(1):31?40.
P. F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
Bob Carpenter. 2005. Scaling high-order character lan-
guage models to gigabytes. In Proceedings of the ACL
Workshop on Software.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proc. of EAMT.
Stanley F. Chen and Joshua Goodman. 1998. An empiri-
cal study of smoothing techniques for language model-
ing. Technical Report TR-10-98, Harvard University.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
models for morpheme segmentation and morphology
learning. ACM Transactions on Speech and Language
Processing, 4(1).
M. Creutz, T. Hirsim?ki, M. Kurimo, A. Puurula,
J. Pylkk?nen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Sara?lar, and A. Stolcke. 2007. Morph-based
speech recognition and modeling of out-of-vocabulary
words across languages. ACM Transactions on Speech
and Language Processing, 5(1):3.
Carl G. de Marcken. 1996. Unsupervised Language Ac-
quisition. Ph.D. thesis, MIT.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameterization
of IBM Model 2. In Proc. of NAACL.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153?198.
S. Goldwater and D. McClosky. 2005. Improving statis-
tical MT through morphological analysis. In Proc. of
EMNLP.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2011. Producing power-law distributions and
damping word frequencies with two-stage language
models. Journal of Machine Learning Research,
12:2335?2382.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging, and morphological
disambiguation in one fell swoop. In Proc. of ACL.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proc. of NAACL.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Proceedings
of the Second International Conference on Arabic Lan-
guage Resources and Tools.
Jan Hajic?, P. Krbec, P. Kve?ton?, K. Oliva, and V. Petrovic?.
2001. Serial combination of rules and statistics. In
Proc. of ACL.
D. Z. Hakkani-T?r, Kemal Oflazer, and G. T?r. 2000.
Statistical morphological disambiguation for aggluti-
native languages. In Proc. of COLING.
Harold Stanley Heaps. 1978. Information Retrieval:
Computational and Theoretical Aspects. Academic
Press.
M. Hulden. 2009. Foma: a finite-state compiler and li-
brary. In Proc. of EACL.
Ronald M. Kaplan and Martin Kay. 1994. Regular mod-
els of phonological rule systems. Computational Lin-
guistics, 20(3):331?378.
Lauri Karttunen and Kenneth R. Beesley. 2005. Twenty-
five years of finite-state morphology. In Inquiries into
Words, Constraints and Contexts, pages 71?83. CSLI.
Kimmo Koskenniemi. 1984. A general computational
model for word-form recognition and production. In
Proc. of ACL-COLING.
1214
O. Lya?evskaya, I. Astaf?yeva, A. Bonch-Osmolovskaya,
A. Garej?ina, Y. Gri?ina, V. D?yac?kov, M. Ionov,
A. Koroleva, M. Kudrinskij, A. Lityagina, Y. Luc?ina,
Y. Sidorova, S. Toldova, S. Savc?uk, and S. Ko-
val?. 2010. Ocenka metodov avtomatic?eskogo
analiza teksta: morfologic?eskie parseri russkogo
yazyka. Komp?juternaya lingvistika i intellektual?nye
texnologii (Computational linguistics and intellectual
technologies).
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In Proc. of ACL.
Christof Monz. 2011. Statistical machine translation
with local language models. In Proc. of EMNLP.
Kemal Oflazer and I?lknur Durgar El-Kahlout. 2007. Ex-
ploring different representational units in English-to-
Turkish statistical machine translation. In Proc. of
StatMT.
K. Oflazer. 1994. Two-level description of Turk-
ish morphology. Literary and Linguistic Computing,
9(2):137?148.
Carolina Parada, Mark Dredze, Abhinav Sethy, and Ariya
Rastrow. 2011. Learning sub-word units for open vo-
cabulary speech recognition. In Proc. of ACL.
Jim Pitman and Marc Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25(2):855?90.
Brian Roark. 2009. Open vocabulary language model-
ing for binary response typing interfaces. Technical
Report CSLU-09-001, Oregon Health & Science Uni-
versity.
M. Ali Basha Shaik, David Rybach, Stefan Hahn, Ralf
Schlu?ter, and Hermann Ney. 2012. Hierarchical hy-
brid language models for open vocabulary continuous
speech recognition using wfst. In Proc. of SAPA.
M. Silfverberg, K. Lind?n, and M. Hyv?rinen. 2012.
Predictive text entry for agglutinative languages using
unsupervised morphological segmentation. In Proc.
of Computational Linguistics and Intelligent Text Pro-
cessing.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proc. of EMNLP.
Matt G. Snover and Michael R. Brent. 2001. A Bayesian
model for morpheme and paradigm identification. In
Proc. of ACL.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proc. of ACL.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL.
Reut Tsarfaty, Djam? Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing of morphologically rich languages: What, how
and whither. In Proc. of Workshop on Statistical Pars-
ing of Morphologically Rich Languages.
F. Tyers and M.S. Alperen. 2010. South-east european
times: A parallel corpus of Balkan languages. In
Proceedings of the LREC workshop on Exploitation
of multilingual resources and tools for Central and
(South) Eastern European Languages.
M. Talha ?akmak, S?leyman Acar, and G?ls?en Eryig?it.
2012. Word alignment for English-Turkish language
pair. In Proc. of LREC.
1215
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 409?419,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Word Alignment with Arbitrary Features
Chris Dyer Jonathan Clark Alon Lavie Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{cdyer,jhclark,alavie,nasmith}@cs.cmu.edu
Abstract
We introduce a discriminatively trained, glob-
ally normalized, log-linear variant of the lex-
ical translation models proposed by Brown
et al (1993). In our model, arbitrary, non-
independent features may be freely incorpo-
rated, thereby overcoming the inherent limita-
tion of generative models, which require that
features be sensitive to the conditional inde-
pendencies of the generative process. How-
ever, unlike previous work on discriminative
modeling of word alignment (which also per-
mits the use of arbitrary features), the param-
eters in our models are learned from unanno-
tated parallel sentences, rather than from su-
pervised word alignments. Using a variety
of intrinsic and extrinsic measures, including
translation performance, we show our model
yields better alignments than generative base-
lines in a number of language pairs.
1 Introduction
Word alignment is an important subtask in statis-
tical machine translation which is typically solved
in one of two ways. The more common approach
uses a generative translation model that relates bilin-
gual string pairs using a latent alignment variable to
designate which source words (or phrases) generate
which target words. The parameters in these models
can be learned straightforwardly from parallel sen-
tences using EM, and standard inference techniques
can recover most probable alignments (Brown et al,
1993). This approach is attractive because it only
requires parallel training data. An alternative to the
generative approach uses a discriminatively trained
alignment model to predict word alignments in the
parallel corpus. Discriminative models are attractive
because they can incorporate arbitrary, overlapping
features, meaning that errors observed in the predic-
tions made by the model can be addressed by engi-
neering new and better features. Unfortunately, both
approaches are problematic, but in different ways.
In the case of discriminative alignment mod-
els, manual alignment data is required for train-
ing, which is problematic for at least three reasons.
Manual alignments are notoriously difficult to cre-
ate and are available only for a handful of language
pairs. Second, manual alignments impose a commit-
ment to a particular preprocessing regime; this can
be problematic since the optimal segmentation for
translation often depends on characteristics of the
test set or size of the available training data (Habash
and Sadat, 2006) or may be constrained by require-
ments of other processing components, such parsers.
Third, the ?correct? alignment annotation for differ-
ent tasks may vary: for example, relatively denser or
sparser alignments may be optimal for different ap-
proaches to (downstream) translation model induc-
tion (Lopez, 2008; Fraser, 2007).
Generative models have a different limitation: the
joint probability of a particular setting of the ran-
dom variables must factorize according to steps in a
process that successively ?generates? the values of
the variables. At each step, the probability of some
value being generated may depend only on the gen-
eration history (or a subset thereof), and the possible
values a variable will take must form a locally nor-
malized conditional probability distribution (CPD).
While these locally normalized CPDs may be pa-
409
rameterized so as to make use of multiple, overlap-
ping features (Berg-Kirkpatrick et al, 2010), the re-
quirement that models factorize according to a par-
ticular generative process imposes a considerable re-
striction on the kinds of features that can be incor-
porated. When Brown et al (1993) wanted to in-
corporate a fertility model to create their Models 3
through 5, the generative process used in Models 1
and 2 (where target words were generated one by
one from source words independently of each other)
had to be abandoned in favor of one in which each
source word had to first decide how many targets it
would generate.1
In this paper, we introduce a discriminatively
trained, globally normalized log-linear model of lex-
ical translation that can incorporate arbitrary, over-
lapping features, and use it to infer word alignments.
Our model enjoys the usual benefits of discrimina-
tive modeling (e.g., parameter regularization, well-
understood learning algorithms), but is trained en-
tirely from parallel sentences without gold-standard
word alignments. Thus, it addresses the two limita-
tions of current word alignment approaches.
This paper is structured as follows. We begin by
introducing our model (?2), and follow this with a
discussion of tractability, parameter estimation, and
inference using finite-state techniques (?3). We then
describe the specific features we used (?4) and pro-
vide experimental evaluation of the model, showing
substantial improvements in three diverse language
pairs (?5). We conclude with an analysis of related
prior work (?6) and a general discussion (?8).
2 Model
In this section, we develop a conditional model
p(t | s) that, given a source language sentence s with
length m = |s|, assigns probabilities to a target sen-
tence t with length n, where each word tj is an el-
ement in the finite target vocabulary ?. We begin
by using the chain rule to factor this probability into
two components, a translation model and a length
model.
p(t | s) = p(t, n | s) = p(t | s, n)
? ?? ?
translation model
? p(n | s)
? ?? ?
length model
1Moore (2005) likewise uses this example to motivate the
need for models that support arbitrary, overlapping features.
In the translation model, we then assume that each
word tj is a translation of one source word, or a
special null token. We therefore introduce a latent
alignment variable a = ?a1, a2, . . . , an? ? [0,m]n,
where aj = 0 represents a special null token.
p(t | s, n) =
?
a
p(t, a | s, n)
So far, our model is identical to that of (Brown et
al., 1993); however, we part ways here. Rather than
using the chain rule to further decompose this prob-
ability and motivate opportunities to make indepen-
dence assumptions, we use a log-linear model with
parameters ? ? Rk and feature vector function H
that maps each tuple ?a, s, t, n? into Rk to model
p(t, a | s, n) directly:
p?(t, a | s, n) =
exp?>H(t, a, s, n)
Z?(s, n)
, where
Z?(s, n) =
?
t???n
?
a?
exp?>H(t?, a?, s, n)
Under some reasonable assumptions (a finite target
vocabulary ? and that all ?k < ?), the partition
function Z?(s, n) will always take on finite values,
guaranteeing that p(t, a | s, n) is a proper probability
distribution.
So far, we have said little about the length model.
Since our intent here is to use the model for align-
ment, where both the target length and target string
are observed, it will not be necessary to commit to
any length model, even during training.
3 Tractability, Learning, and Inference
The model introduced in the previous section is
extremely general, and it can incorporate features
sensitive to any imaginable aspects of a sentence
pair and their alignment, from linguistically in-
spired (e.g., an indicator feature for whether both
the source and target sentences contain a verb), to
the mundane (e.g., the probability of the sentence
pair and alignment under Model 1), to the absurd
(e.g., an indicator if s and t are palindromes of each
other).
However, while our model can make use of arbi-
trary, overlapping features, when designing feature
functions it is necessary to balance expressiveness
and the computational complexity of the inference
410
algorithms used to reason under models that incor-
porate these features.2 To understand this tradeoff,
we assume that the random variables being modeled
(t, a) are arranged into an undirected graph G such
that the vertices represent the variables and the edges
are specified so that the feature function H decom-
poses linearly over all the cliques C in G,
H(t, a, s, n) =
?
C
h(tC , aC , s, n) ,
where tC and aC are the components associated with
subgraph C and h(?) is a local feature vector func-
tion. In general, exact inference is exponential in
the width of tree-decomposition of G, but, given a
fixed width, they can be solved in polynomial time
using dynamic programming. For example, when
the graph has a sequential structure, exact infer-
ence can be carried out using the familiar forward-
backward algorithm (Lafferty et al, 2001). Al-
though our features look at more structure than this,
they are designed to keep treewidth low, meaning
exact inference is still possible with dynamic pro-
gramming. Figure 1 gives a graphical representation
of our model as well as the more familiar genera-
tive (directed) variants. The edge set in the depicted
graph is determined by the features that we use (?4).
3.1 Parameter Learning
To learn the parameters of our model, we select the
?? that minimizes the `1 regularized conditional log-
likelihood of a set of training data T :
L(?) =?
?
?s,t??T
log
?
a
p?(t, a | s, n) + ?
?
k
|?k| .
Because of the `1 penalty, this objective is not every-
where differentiable, but the gradient with respect to
the parameters of the log-likelihood term is as fol-
lows.
?L
??
=
?
?s,t??T
Ep?(a|s,t,n)[H(?)]? Ep?(t,a|s,n)[H(?)]
(1)
To optimize L, we employ an online method that
approximates `1 regularization and only depends on
2One way to understand expressiveness is in terms of inde-
pendence assumptions, of course. Research in graphical models
has done much to relate independence assumptions to the com-
plexity of inference algorithms (Koller and Friedman, 2009).
the gradient of the unregularized objective (Tsu-
ruoka et al, 2009). This method is quite attrac-
tive since it is only necessary to represent the active
features, meaning impractically large feature spaces
can be searched provided the regularization strength
is sufficiently high. Additionally, not only has this
technique been shown to be very effective for opti-
mizing convex objectives, but evidence suggests that
the stochasticity of online algorithms often results
in better solutions than batch optimizers for non-
convex objectives (Liang and Klein, 2009). On ac-
count of the latent alignment variable in our model,
L is non-convex (as is the likelihood objective of the
generative variant).
To choose the regularization strength ? and the
initial learning rate ?0,3 we trained several mod-
els on a 10,000-sentence-pair subset of the French-
English Hansards, and chose values that minimized
the alignment error rate, as evaluated on a 447 sen-
tence set of manually created alignments (Mihalcea
and Pedersen, 2003). For the remainder of the ex-
periments, we use the values we obtained, ? = 0.4
and ?0 = 0.3.
3.2 Inference with WFSAs
We now describe how to use weighted finite-state
automata (WFSAs) to compute the quantities neces-
sary for training. We begin by describing the ideal
WFSA representing the full translation search space,
which we call the discriminative neighborhood, and
then discuss strategies for reducing its size in the
next section, since the full model is prohibitively
large, even with small data sets.
For each training instance ?s, t?, the contribution
to the gradient (Equation 1) is the difference in two
vectors of expectations. The first term is the ex-
pected value of H(?) when observing ?s, n, t? and
letting a range over all possible alignments. The
second is the expectation of the same function, but
observing only ?s, n? and letting t? and a take on
any possible values (i.e., all possible translations
of length n and all their possible alignments to s).
To compute these expectations, we can construct
a WFSA representing the discriminative neighbor-
hood, the set ?n?[0,m]n, such that every path from
the start state to goal yields a pair ?t?, a? with weight
3For the other free parameters of the algorithm, we use the
default values recommended by Tsuruoka et al (2009).
411
a1
a
2
a
3
a
n
t
1
t
2
t
3
t
n
s
n
Fully directed model (Brown et al, 1993;
Vogel et al, 1996; Berg-Kirkpatrick et al, 2010)
Our model
...
a
1
a
2
a
3
a
n
t
1
t
2
t
3
t
n
s
n
...
......
s
s s s s
ss s
Figure 1: A graphical representation of a conventional generative lexical translation model (left) and our model with
an undirected translation model. For clarity, the observed node s (representing the full source sentence) is drawn in
multiple locations. The dashed lines indicate a dependency on a deterministic mapping of tj (not its complete value).
H(t?, a, s, n). With our feature set (?4), number of
states in this WFSA isO(m?n) since at each target
index j, there is a different state for each possible in-
dex of the source word translated at position j ? 1.4
Once the WFSA representing the discriminative
neighborhood is built, we use the forward-backward
algorithm to compute the second expectation term.
We then intersect the WFSA with an unweighted
FSA representing the target sentence t (because of
the restricted structure of our WFSA, this amounts
to removing edges), and finally run the forward-
backward algorithm on the resulting WFSA to com-
pute the first expectation.
3.3 Shrinking the Discriminative
Neighborhood
The WFSA we constructed requires m? |?| transi-
tions between all adjacent states, which is impracti-
cally large. We can reduce the number of edges by
restricting the set of words that each source word can
translate into. Thus, the model will not discriminate
4States contain a bit more information than the index of the
previous source word, for example, there is some additional in-
formation about the previous translation decision that is passed
forward. However, the concept of splitting states to guarantee
distinct paths for different values of non-local features is well
understood by NLP and machine translation researchers, and
the necessary state structure should be obvious from the feature
description.
among all candidate target strings in ?n, but rather
in ?ns , where ?s =
?m
i=1 ?si , and where ?s is the
set of target words that s may translate into.5
We consider four different definitions of ?s: (1)
the baseline of the full target vocabulary, (2) the set
of all target words that co-occur in sentence pairs
containing s, (3) the most probable words under
IBM Model 1 that are above a threshold, and (4) the
same Model 1, except we add a sparse symmetric
Dirichlet prior (? = 0.01) on the translation distri-
butions and use the empirical Bayes (EB) method to
infer a point estimate, using variational inference.
Table 1: Comparison of alternative definitions ?s (arrows
indicate whether higher or lower is better).
?s time (s) ?
?
s |?s| ? AER ?
= ? 22.4 86.0M 0.0
co-occ. 8.9 0.68M 0.0
Model 1 0.2 0.38M 6.2
EB-Model 1 1.0 0.15M 2.9
Table 1 compares the average per-sentence time
required to run the inference algorithm described
5Future work will explore alternative formulations of the
discriminative neighborhood with the goal of further improving
inference efficiency. Smith and Eisner (2005) show that good
performance on unsupervised syntax learning is possible even
when learning from very small discriminative neighborhoods,
and we posit that the same holds here.
412
above under these four different definitions of ?s on
a 10,000 sentence subset of the Hansards French-
English corpus that includes manual word align-
ments. While our constructions guarantee that all
references are reachable even in the reduced neigh-
borhoods, not all alignments between source and tar-
get are possible. The last column is the oracle AER.
Although EB variant of Model 1 neighborhood is
slightly more expensive to do inference with than
regular Model 1, we use it because it has a lower
oracle AER.6
During alignment prediction (rather than during
training) for a sentence pair ?s, t?, it is possible to
further restrict ?s to be just the set of words occur-
ring in t, making extremely fast inference possible
(comparable to that of the generative HMM align-
ment model).
4 Features
Feature engineering lets us encode knowledge about
what aspects of a translation derivation are useful in
predicting whether it is good or not. In this section
we discuss the features we used in our model. Many
of these were taken from the discriminative align-
ment modeling literature, but we also note that our
features can be much more fine-grained than those
used in supervised alignment modeling, since we
learn our models from a large amount of parallel
data, rather than a small number of manual align-
ments.
Word association features. Word association fea-
tures are at the heart of all lexical translation models,
whether generative or discriminative. In addition to
fine-grained boolean indicator features ?saj , tj? for
pair types, we have several orthographic features:
identity, prefix identity, and an orthographic simi-
larity measure designed to be informative for pre-
dicting the translation of named entities in languages
that use similar alphabets.7 It has the property that
source-target pairs of long words that are similar are
given a higher score than word pairs that are short
and similar (dissimilar pairs have a score near zero,
6We included all translations whose probability was within
a factor of 10?4 of the highest probability translation.
7In experiments with Urdu, which uses an Arabic-derived
script, the orthographic feature was computed after first ap-
plying a heuristic Romanization, which made the orthographic
forms somewhat comparable.
regardless of length). We also include ?global? asso-
ciation scores that are precomputed by looking at the
full training data: Dice?s coefficient (discretized),
which we use to measure association strength be-
tween pairs of source and target word types across
sentence pairs (Dice, 1945), IBM Model 1 forward
and reverse probabilities, and the geometric mean of
the Model 1 forward and reverse probabilities. Fi-
nally, we also cluster the source and target vocab-
ularies (Och, 1999) and include class pair indicator
features, which can learn generalizations that, e.g.,
?nouns tend to translate into nouns but not modal
verbs.?
Positional features. Following Blunsom and
Cohn (2006), we include features indicating
closeness to the alignment matrix diagonal,
h(aj , j,m, n) =
?
?
?
aj
m ?
j
n
?
?
?. We also conjoin this
feature with the source word class type indicator to
enable the model to learn that certain word types
are more or less likely to favor a location on the
diagonal (e.g. Urdu?s sentence-final verbs).
Source features. Some words are functional el-
ements that fulfill purely grammatical roles and
should not be the ?source? of a translation. For ex-
ample, Romance languages require a preposition in
the formation of what could be a noun-noun com-
pound in English, thus, it may be useful to learn not
to translate certain words (i.e. they should not par-
ticipate in alignment links), or to have a bias to trans-
late others. To capture this intuition we include an
indicator feature that fires each time a source vocab-
ulary item (and source word class) participates in an
alignment link.
Source path features. One class of particularly
useful features assesses the goodness of the align-
ment ?path? through the source sentence (Vogel et
al., 1996). Although assessing the predicted path
requires using nonlocal features, since each aj ?
[0,m] and m is relatively small, features can be sen-
sitive to a wider context than is often practical.
We use many overlapping source path features,
some of which are sensitive to the distance and di-
rection of the jump between aj?1 and aj , and oth-
ers which are sensitive to the word pair these two
points define, and others that combine all three el-
ements. The features we use include a discretized
413
jump distance, the discretized jump conjoined with
an indicator feature for the target length n, the dis-
cretized jump feature conjoined with the class of saj ,
and the discretized jump feature conjoined with the
class of saj and saj?1 . To discretize the features we
take a log transform (base 1.3) of the jump width and
let an indicator feature fire for the closest integer.
In addition to these distance-dependent features, we
also include indicator features that fire on bigrams
?saj?1 , saj ? and their word classes. Thus, this fea-
ture can capture our intuition that, e.g., adjectives
are more likely to come before or after a noun in
different languages.
Target string features. Features sensitive to mul-
tiple values in the predicted target string or latent
alignment variable must be handled carefully for the
sake of computational tractability. While features
that look at multiple source words can be computed
linearly in the number of source words considered
(since the source string is always observable), fea-
tures that look at multiple target words require ex-
ponential time and space!8 However, by grouping
the tj?s into coarse equivalence classes and looking
at small numbers of variables, it is possible to incor-
porate such features. We include a feature that fires
when a word translates as itself (for example, a name
or a date, which occurs in languages that share the
same alphabet) in position j, but then is translated
again (as something else) in position j ? 1 or j + 1.
5 Experiments
We now turn to an empirical assessment of our
model. Using various datasets, we evaluate the
performance of the models? intrinsic quality and
theirtheir alignments? contribution to a standard ma-
chine translation system. We make use of parallel
corpora from languages with very different typolo-
gies: a small (0.8M words) Chinese-English corpus
from the tourism and travel domain (Takezawa et al,
2002), a corpus of Czech-English news commen-
tary (3.1M words),9 and an Urdu-English corpus
(2M words) provided by NIST for the 2009 Open
MT Evaluation. These pairs were selected since
each poses different alignment challenges (word or-
8This is of course what makes history-based language model
integration an inference challenge in translation.
9http://statmt.org/wmt10
der in Chinese and Urdu, morphological complex-
ity in Czech, and a non-alphabetic writing system in
Chinese), and confining ourselves to these relatively
small corpora reduced the engineering overhead of
getting an implementation up and running. Future
work will explore the scalability characteristics and
limits of the model.
5.1 Methodology
For each language pair, we train two log-linear
translation models as described above (?3), once
with English as the source and once with English
as the target language. For a baseline, we use
the Giza++ toolkit (Och and Ney, 2003) to learn
Model 4, again in both directions. We symmetrize
the alignments from both model types using the
grow-diag-final-and heuristic (Koehn et al,
2003) producing, in total, six alignment sets. We
evaluate them both intrinsically and in terms of their
performance in a translation system.
Since we only have gold alignments for Czech-
English (Bojar and Prokopova?, 2006), we can re-
port alignment error rate (AER; Och and Ney, 2003)
only for this pair. However, we offer two further
measures that we believe are suggestive and that
do not require gold alignments. One is the aver-
age alignment ?fertility? of source words that occur
only a single time in the training data (so-called ha-
pax legomena). This assesses the impact of a typical
alignment problem observed in generative models
trained to maximize likelihood: infrequent source
words act as ?garbage collectors?, with many target
words aligned to them (the word dislike in the Model
4 alignment in Figure 2 is an example). Thus, we ex-
pect lower values of this measure to correlate with
better alignments. The second measure is the num-
ber of rule types learned in the grammar induction
process used for translation that match the transla-
tion test sets.10 While neither a decrease in the aver-
age singleton fertility nor an increase in the number
of rules induced guarantees better alignment quality,
we believe it is reasonable to assume that they are
positively correlated.
For the translation experiments in each language
pair, we make use of the cdec decoder (Dyer et al,
10This measure does not assess whether the rule types are
good or bad, but it does suggest that the system?s coverage is
greater.
414
2010), inducing a hierarchical phrase based trans-
lation grammar from two sets of symmetrized align-
ments using the method described by Chiang (2007).
Additionally, recent work that has demonstrated that
extracting rules from n-best alignments has value
(Liu et al, 2009; Venugopal et al, 2008). We
therefore define a third condition where rules are
extracted from the corpus under both the Model 4
and discriminative alignments and merged to form
a single grammar. We incorporate a 3-gram lan-
guage model learned from the target side of the
training data as well as 50M supplemental words
of monolingual training data consisting of sentences
randomly sampled from the English Gigaword, ver-
sion 4. In the small Chinese-English travel domain
experiment, we just use the LM estimated from the
bitext. The parameters of the translation model were
tuned using ?hypergraph? minimum error rate train-
ing (MERT) to maximize BLEU on a held-out de-
velopment set (Kumar et al, 2009). Results are
reported using case-insensitive BLEU (Papineni et
al., 2002), METEOR11 (Lavie and Denkowski, 2009),
and TER (Snover et al, 2006), with the number of
references varying by task. Since MERT is a non-
deterministic optimization algorithm and results can
vary considerably between runs, we follow Clark et
al. (2011) and report the average score and stan-
dard deviation of 5 independent runs, 30 in the case
of Chinese-English, since observed variance was
higher.
5.2 Experimental Results
Czech-English. Czech-English poses problems
for word alignment models since, unlike English,
Czech words have a complex inflectional morphol-
ogy, and the syntax permits relatively free word or-
der. For this language pair, we evaluate alignment
error rate using the manual alignment corpus de-
scribed by Bojar and Prokopova? (2006). Table 2
summarizes the results.
Chinese-English. Chinese-English poses a differ-
ent set of problems for alignment. While Chinese
words have rather simple morphology, the Chinese
writing system renders our orthographic features
useless. Despite these challenges, the Chinese re-
11Meteor 1.0 with exact, stem, synonymy, and paraphrase
modules and HTER parameters.
Table 2: Czech-English experimental results. ??sing. is the
average fertility of singleton source words.
AER ? ??sing. ? # rules ?
Model 4 e | f 24.8 4.1
f | e 33.6 6.6
sym. 23.4 2.7 993,953
Our model e | f 21.9 2.3
f | e 29.3 3.8
sym. 20.5 1.6 1,146,677
Alignment BLEU ? METEOR ? TER ?
Model 4 16.3?0.2 46.1?0.1 67.4?0.3
Our model 16.5?0.1 46.8?0.1 67.0?0.2
Both 17.4?0.1 47.7?0.1 66.3?0.5
sults in Table 3 show the same pattern of results as
seen in Czech-English.
Table 3: Chinese-English experimental results.
??sing. ? # rules ?
Model 4 e | f 4.4
f | e 3.9
sym. 3.6 52,323
Our model e | f 3.5
f | e 2.6
sym. 3.1 54,077
Alignment BLEU ? METEOR ? TER ?
Model 4 56.5?0.3 73.0?0.4 29.1?0.3
Our model 57.2?0.8 73.8?0.4 29.3?1.1
Both 59.1?0.6 74.8?0.7 27.6?0.5
Urdu-English. Urdu-English is a more challeng-
ing language pair for word alignment than the pre-
vious two we have considered. The parallel data is
drawn from numerous genres, and much of it was ac-
quired automatically, making it quite noisy. So our
models must not only predict good translations, they
must cope with bad ones as well. Second, there has
been no previous work on discriminative modeling
of Urdu, since, to our knowledge, no manual align-
ments have been created. Finally, unlike English,
Urdu is a head-final language: not only does it have
SOV word order, but rather than prepositions, it has
post-positions, which follow the nouns they modify,
meaning its large scale word order is substantially
415
different from that of English. Table 4 demonstrates
the same pattern of improving results with our align-
ment model.
Table 4: Urdu-English experimental results.
??sing. ? # rules ?
Model 4 e | f 6.5
f | e 8.0
sym. 3.2 244,570
Our model e | f 4.8
f | e 8.3
sym. 2.3 260,953
Alignment BLEU ? METEOR ? TER ?
Model 4 23.3?0.2 49.3?0.2 68.8?0.8
Our model 23.4?0.2 49.7?0.1 67.7?0.2
Both 24.1?0.2 50.6?0.1 66.8?0.5
5.3 Analysis
The quantitative results presented in this section
strongly suggest that our modeling approach pro-
duces better alignments. In this section, we try to
characterize how the model is doing what it does
and what it has learned. Because of the `1 regular-
ization, the number of active (non-zero) features in
the inferred models is small, relative to the number
of features considered during training. The num-
ber of active features ranged from about 300k for
the small Chinese-English corpus to 800k for Urdu-
English, which is less than one tenth of the available
features in both cases. In all models, the coarse fea-
tures (Model 1 probabilities, Dice coefficient, coarse
positional features, etc.) typically received weights
with large magnitudes, but finer features also played
an important role.
Language pair differences manifested themselves
in many ways in the models that were learned.
For example, orthographic features were (unsurpris-
ingly) more valuable in Czech-English, with their
largely overlapping alphabets, than in Chinese or
Urdu. Examining the more fine-grained features is
also illuminating. Table 5 shows the most highly
weighted source path bigram features on the three
models where English was the source language, and
in each, we may observe some interesting character-
istics of the target language. Left-most is English-
Czech. At first it may be surprising that words like
since and that have a highly weighted feature for
transitioning to themselves. However, Czech punc-
tuation rules require that relative clauses and sub-
ordinating conjunctions be preceded by a comma
(which is only optional or outright forbidden in En-
glish), therefore our model translates these words
twice, once to produce the comma, and a second
time to produce the lexical item. The middle col-
umn is the English-Chinese model. In the training
data, many of the sentences are questions directed to
a second person, you. However, Chinese questions
do not invert and the subject remains in the canon-
ical first position, thus the transition from the start
of sentence to you is highly weighted. Finally, Fig-
ure 2 illustrates how Model 4 (left) and our discrimi-
native model (right) align an English-Urdu sentence
pair (the English side is being conditioned on in both
models). A reflex of Urdu?s head-final word order
is seen in the list of most highly weighted bigrams,
where a path through the English source where verbs
that transition to end-of-sentence periods are predic-
tive of good translations into Urdu.
Table 5: The most highly weighted source path bigram
features in the English-Czech, -Chinese, and -Urdu mod-
els.
Bigram ?k
. ?/s? 3.08
like like 1.19
one of 1.06
? . 0.95
that that 0.92
is but 0.92
since since 0.84
?s? when 0.83
, how 0.83
, not 0.83
Bigram ?k
. ?/s? 2.67
? ? 2.25
?s? please 2.01
much ? 1.61
?s? if 1.58
thank you 1.47
?s? sorry 1.46
?s? you 1.45
please like 1.24
?s? this 1.19
Bigram ?k
. ?/s? 1.87
?s? this 1.24
will . 1.17
are . 1.16
is . 1.09
is that 1.00
have . 0.97
has . 0.96
was . 0.91
will ?/s? 0.88
6 Related Work
The literature contains numerous descriptions of dis-
criminative approaches to word alignment motivated
by the desire to be able to incorporate multiple,
overlapping knowledge sources (Ayan et al, 2005;
Moore, 2005; Taskar et al, 2005; Blunsom and
Cohn, 2006; Haghighi et al, 2009; Liu et al, 2010;
DeNero and Klein, 2010; Setiawan et al, 2010).
This body of work has been an invaluable source
of useful features. Several authors have dealt with
the problem training log-linear models in an unsu-
416
IBM Model 4 alignment Our model's alignment
Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model
4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model
does not exhibit these problems, and in fact, makes no mistakes in the alignment.
pervised setting. The contrastive estimation tech-
nique proposed by Smith and Eisner (2005) is glob-
ally normalized (and thus capable of dealing with ar-
bitrary features), and closely related to the model we
developed; however, they do not discuss the problem
of word alignment. Berg-Kirkpatrick et al (2010)
learn locally normalized log-linear models in a gen-
erative setting. Globally normalized discriminative
models with latent variables (Quattoni et al, 2004)
have been used for a number of language processing
problems, including MT (Dyer and Resnik, 2010;
Blunsom et al, 2008a). However, this previous
work relied on translation grammars constructed us-
ing standard generative word alignment processes.
7 Future Work
While we have demonstrated that this model can be
substantially useful, it is limited in some important
ways which are being addressed in ongoing work.
First, training is expensive, and we are exploring al-
ternatives to the conditional likelihood objective that
is currently used, such as contrastive neighborhoods
advocated by (Smith and Eisner, 2005). Addition-
ally, there is much evidence that non-local features
like the source word fertility are (cf. IBM Model 3)
useful for translation and alignment modeling. To be
truly general, it must be possible to utilize such fea-
tures. Unfortunately, features like this that depend
on global properties of the alignment vector, a, make
the inference problem NP-hard, and approximations
are necessary. Fortunately, there is much recent
work on approximate inference techniques for incor-
porating nonlocal features (Blunsom et al, 2008b;
Gimpel and Smith, 2009; Cromie`res and Kurohashi,
2009; Weiss and Taskar, 2010), suggesting that this
problem too can be solved using established tech-
niques.
8 Conclusion
We have introduced a globally normalized, log-
linear lexical translation model that can be trained
discriminatively using only parallel sentences,
which we apply to the problem of word alignment.
Our approach addresses two important shortcomings
of previous work: (1) that local normalization of
generative models constrains the features that can be
used, and (2) that previous discriminatively trained
word alignment models required supervised align-
ments. According to a variety of measures in a vari-
ety of translation tasks, this model produces superior
alignments to generative approaches. Furthermore,
the features learned by our model reveal interesting
characteristics of the language pairs being modeled.
Acknowledgments
This work was supported in part by the DARPA GALE
program; the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
417
ber W911NF-10-1-0533; and the National Science Foun-
dation through grants IIS-0844507, IIS-0915187, IIS-
0713402, and IIS-0915327 and through TeraGrid re-
sources provided by the Pittsburgh Supercomputing Cen-
ter under grant number TG-DBS110003. We thank
Ondr?ej Bojar for providing the Czech-English alignment
data, and three anonymous reviewers for their detailed
suggestions and comments on an earlier draft of this pa-
per.
References
N. F. Ayan, B. J. Dorr, and C. Monz. 2005. NeurAlign:
combining word alignments using neural networks. In
Proc. of HLT-EMNLP.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2006. Discriminative word
alignment with conditional random fields. In Proc. of
ACL.
P. Blunsom, T. Cohn, and M. Osborne. 2008a. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2008b. Proba-
bilistic inference for machine translation. In Proc. of
EMNLP 2008.
O. Bojar and M. Prokopova?. 2006. Czech-English word
alignment. In Proc. of LREC.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Bet-
ter hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proc. of
ACL.
F. Cromie`res and S. Kurohashi. 2009. An alignment al-
gorithm using belief propagation and a structure-based
distortion model. In Proc. of EACL.
J. DeNero and D. Klein. 2010. Discriminative modeling
of extraction sets for machine translation. In Proc. of
ACL.
L. R. Dice. 1945. Measures of the amount of eco-
logic association between species. Journal of Ecology,
26:297?302.
C. Dyer and P. Resnik. 2010. Context-free reordering,
finite-state translation. In Proc. of NAACL.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL (demonstration session).
A. Fraser. 2007. Improved Word Alignments for Statis-
tical Machine Translation. Ph.D. thesis, University of
Southern California.
K. Gimpel and N. A. Smith. 2009. Cube summing, ap-
proximate inference with non-local features, and dy-
namic programming without semirings. In Proc. of
EACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL-IJCNLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL-IJCNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. Lavie and M. Denkowski. 2009. The METEOR metric
for automatic evaluation of machine translation. Ma-
chine Translation Journal, 23(2?3):105?115.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
Y. Liu, T. Xia, X. Xiao, and Q. Liu. 2009. Weighted
alignment matrices for statistical machine translation.
In Proc. of EMNLP.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3):303?339.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proc. of the Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proc. of HLT-EMNLP.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 1999. An efficient method for determining bilin-
gual word classes. In Proc. of EACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
418
A. Quattoni, M. Collins, and T. Darrell. 2004. Condi-
tional random fields for object recognition. In NIPS
17.
H. Setiawan, C. Dyer, and P. Resnik. 2010. Discrimina-
tive word alignment with a function word reordering
model. In Proc. of EMNLP.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
M. Snover, B. J. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proc. of LREC.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. of HLT-EMNLP.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL-IJCNLP.
A. Venugopal, A. Zollmann, N. A. Smith, and S. Vogel.
2008. Wider pipelines: n-best alignments and parses
in MT training. In Proc. of AMTA.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS.
419
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176?181,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Better Hypothesis Testing for Statistical Machine Translation:
Controlling for Optimizer Instability
Jonathan H. Clark Chris Dyer Alon Lavie Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jhclark,cdyer,alavie,nasmith}@cs.cmu.edu
Abstract
In statistical machine translation, a researcher
seeks to determine whether some innovation
(e.g., a new feature, model, or inference al-
gorithm) improves translation quality in com-
parison to a baseline system. To answer this
question, he runs an experiment to evaluate the
behavior of the two systems on held-out data.
In this paper, we consider how to make such
experiments more statistically reliable. We
provide a systematic analysis of the effects of
optimizer instability?an extraneous variable
that is seldom controlled for?on experimen-
tal outcomes, and make recommendations for
reporting results more accurately.
1 Introduction
The need for statistical hypothesis testing for ma-
chine translation (MT) has been acknowledged since
at least Och (2003). In that work, the proposed
method was based on bootstrap resampling and was
designed to improve the statistical reliability of re-
sults by controlling for randomness across test sets.
However, there is no consistently used strategy that
controls for the effects of unstable estimates of
model parameters.1 While the existence of opti-
mizer instability is an acknowledged problem, it is
only infrequently discussed in relation to the relia-
bility of experimental results, and, to our knowledge,
there has yet to be a systematic study of its effects on
1We hypothesize that the convention of ?trusting? BLEU
score improvements of, e.g., > 1, is not merely due to an ap-
preciation of what qualitative difference a particular quantita-
tive improvement will have, but also an implicit awareness that
current methodology leads to results that are not consistently
reproducible.
hypothesis testing. In this paper, we present a series
of experiments demonstrating that optimizer insta-
bility can account for substantial amount of variation
in translation quality,2 which, if not controlled for,
could lead to incorrect conclusions. We then show
that it is possible to control for this variable with a
high degree of confidence with only a few replica-
tions of the experiment and conclude by suggesting
new best practices for significance testing for ma-
chine translation.
2 Nondeterminism and Other
Optimization Pitfalls
Statistical machine translation systems consist of a
model whose parameters are estimated to maximize
some objective function on a set of development
data. Because the standard objectives (e.g., 1-best
BLEU, expected BLEU, marginal likelihood) are
not convex, only approximate solutions to the op-
timization problem are available, and the parame-
ters learned are typically only locally optimal and
may strongly depend on parameter initialization and
search hyperparameters. Additionally, stochastic
optimization and search techniques, such as mini-
mum error rate training (Och, 2003) and Markov
chain Monte Carlo methods (Arun et al, 2010),3
constitute a second, more obvious source of noise
in the optimization procedure.
This variation in the parameter vector affects the
quality of the model measured on both development
2This variation directly affects the output translations, and
so it will propagate to both automated metrics as well as human
evaluators.
3Online subgradient techniques such as MIRA (Crammer et
al., 2006; Chiang et al, 2008) have an implicit stochastic com-
ponent as well based on the order of the training examples.
176
data and held-out test data, independently of any ex-
perimental manipulation. Thus, when trying to de-
termine whether the difference between two mea-
surements is significant, it is necessary to control for
variance due to noisy parameter estimates. This can
be done by replication of the optimization procedure
with different starting conditions (e.g., by running
MERT many times).
Unfortunately, common practice in reporting ma-
chine translation results is to run the optimizer once
per system configuration and to draw conclusions
about the experimental manipulation from this sin-
gle sample. However, it could be that a particu-
lar sample is on the ?low? side of the distribution
over optimizer outcomes (i.e., it results in relatively
poorer scores on the test set) or on the ?high? side.
The danger here is obvious: a high baseline result
paired with a low experimental result could lead to a
useful experimental manipulation being incorrectly
identified as useless. We now turn to the question of
how to reduce the probability falling into this trap.
3 Related Work
The use of statistical hypothesis testing has grown
apace with the adoption of empirical methods in
natural language processing. Bootstrap techniques
(Efron, 1979; Wasserman, 2003) are widespread
in many problem areas, including for confidence
estimation in speech recognition (Bisani and Ney,
2004), and to determine the significance of MT re-
sults (Och, 2003; Koehn, 2004; Zhang et al, 2004;
Zhang and Vogel, 2010). Approximate randomiza-
tion (AR) has been proposed as a more reliable tech-
nique for MT significance testing, and evidence sug-
gests that it yields fewer type I errors (i.e., claiming
a significant difference where none exists; Riezler
and Maxwell, 2005). Other uses in NLP include
the MUC-6 evaluation (Chinchor, 1993) and pars-
ing (Cahill et al, 2008). However, these previous
methods assume model parameters are elements of
the system rather than extraneous variables.
Prior work on optimizer noise in MT has fo-
cused primarily on reducing optimizer instability
(whereas our concern is how to deal with optimizer
noise, when it exists). Foster and Kuhn (2009) mea-
sured the instability of held-out BLEU scores across
10 MERT runs to improve tune/test set correlation.
However, they only briefly mention the implications
of the instability on significance. Cer et al (2008)
explored regularization of MERT to improve gener-
alization on test sets. Moore and Quirk (2008) ex-
plored strategies for selecting better random ?restart
points? in optimization. Cer et al (2010) analyzed
the standard deviation over 5 MERT runs when each
of several metrics was used as the objective function.
4 Experiments
In our experiments, we ran the MERT optimizer to
optimize BLEU on a held-out development set many
times to obtain a set of optimizer samples on two dif-
ferent pairs of systems (4 configurations total). Each
pair consists of a baseline system (System A) and an
?experimental? system (System B), which previous
research has suggested will perform better.
The first system pair contrasts a baseline phrase-
based system (Moses) and experimental hierarchi-
cal phrase-based system (Hiero), which were con-
structed from the Chinese-English BTEC corpus
(0.7M words), the later of which was decoded with
the cdec decoder (Koehn et al, 2007; Chiang, 2007;
Dyer et al, 2010). The second system pair con-
trasts two German-English Hiero/cdec systems con-
structed from the WMT11 parallel training data
(98M words).4 The baseline system was trained on
unsegmented words, and the experimental system
was constructed using the most probable segmenta-
tion of the German text according to the CRF word
segmentation model of Dyer (2009). The Chinese-
English systems were optimized 300 times, and the
German-English systems were optimized 50 times.
Our experiments used the default implementation
of MERT that accompanies each of the two de-
coders. The Moses MERT implementation uses 20
random restart points per iteration, drawn uniformly
from the default ranges for each feature, and, at each
iteration, 200-best lists were extracted with the cur-
rent weight vector (Bertoldi et al, 2009). The cdec
MERT implementation performs inference over the
decoder search space which is structured as a hyper-
graph (Kumar et al, 2009). Rather than using restart
points, in addition to optimizing each feature inde-
pendently, it optimizes in 5 random directions per it-
eration by constructing a search vector by uniformly
sampling each element of the vector from (?1, 1)
and then renormalizing so it has length 1. For all
systems, the initial weight vector was manually ini-
tialized so as to yield reasonable translations.
4http://statmt.org/wmt11/
177
Metric System Avg ssel sdev stest
BTEC Chinese-English (n = 300)
BLEU ?
System A 48.4 1.6 0.2 0.5
System B 49.9 1.5 0.1 0.4
MET ?
System A 63.3 0.9 - 0.4
System B 63.8 0.9 - 0.5
TER ?
System A 30.2 1.1 - 0.6
System B 28.7 1.0 - 0.2
WMT German-English (n = 50)
BLEU ?
System A 18.5 0.3 0.0 0.1
System B 18.7 0.3 0.0 0.2
MET ?
System A 49.0 0.2 - 0.2
System B 50.0 0.2 - 0.1
TER ?
System A 65.5 0.4 - 0.3
System B 64.9 0.4 - 0.4
Table 1: Measured standard deviations of different au-
tomatic metrics due to test-set and optimizer variability.
sdev is reported only for the tuning objective function
BLEU.
Results are reported using BLEU (Papineni et
al., 2002), METEOR5 (Banerjee and Lavie, 2005;
Denkowski and Lavie, 2010), and TER (Snover et
al., 2006).
4.1 Extraneous variables in one system
In this section, we describe and measure (on the ex-
ample systems just described) three extraneous vari-
ables that should be considered when evaluating a
translation system. We quantify these variables in
terms of standard deviation s, since it is expressed
in the same units as the original metric. Refer to
Table 1 for the statistics.
Local optima effects sdev The first extraneous
variable we discuss is the stochasticity of the opti-
mizer. As discussed above, different optimization
runs find different local maxima. The noise due to
this variable can depend on many number of fac-
tors, including the number of random restarts used
(in MERT), the number of features in a model, the
number of references, the language pair, the portion
of the search space visible to the optimizer (e.g. 10-
best, 100-best, a lattice, a hypergraph), and the size
of the tuning set. Unfortunately, there is no proxy to
estimate this effect as with bootstrap resampling. To
control for this variable, we must run the optimizer
multiple times to estimate the spread it induces on
the development set. Using the n optimizer samples,
with mi as the translation quality measurement of
5METEOR version 1.2 with English ranking parameters and
all modules.
the development set for the ith optimization run, and
m is the average of all mis, we report the standard
deviation over the tuning set as sdev:
sdev =
?
?
?
?
n?
i=1
(mi ?m)
2
n? 1
A high sdev value may indicate that the optimizer is
struggling with local optima and changing hyperpa-
rameters (e.g. more random restarts in MERT) could
improve system performance.
Overfitting effects stest As with any optimizer,
there is a danger that the optimal weights for a tuning
set may not generalize well to unseen data (i.e., we
overfit). For a randomized optimizer, this means that
parameters can generalize to different degrees over
multiple optimizer runs. We measure the spread in-
duced by optimizer randomness on the test set met-
ric score stest, as opposed to the overfitting effect in
isolation. The computation of stest is identical to sdev
except that the mis are the translation metrics cal-
culated on the test set. In Table 1, we observe that
stest > sdev, indicating that optimized parameters are
likely not generalizing well.
Test set selection ssel The final extraneous vari-
able we consider is the selection of the test set it-
self. A good test set should be representative of
the domain or language for which experimental ev-
idence is being considered. However, with only a
single test corpus, we may have unreliable results
because of idiosyncrasies in the test set. This can
be mitigated in two ways. First, replication of ex-
periments by testing on multiple, non-overlapping
test sets can eliminate it directly. Since this is not
always practical (more test data may not be avail-
abile), the widely-used bootstrap resampling method
(?3) also controls for test set effects by resampling
multiple ?virtual? test sets from a single set, making
it possible to infer distributional parameters such as
the standard deviation of the translation metric over
(very similar) test sets.6 Furthermore, this can be
done for each of our optimizer samples. By averag-
ing the bootstrap-estimated standard deviations over
6Unlike actually using multiple test sets, bootstrap resam-
pling does not help to re-estimate the mean metric score due to
test set spread (unlike actually using multiple test sets) since the
mean over bootstrap replicates is approximately the aggregate
metric score.
178
optimizer samples, we have a statistic that jointly
quantifies the impact of test set effects and optimizer
instability on a test set. We call this statistic ssel.
Different values of this statistic can suggest method-
ological improvements. For example, a large ssel in-
dicates that more replications will be necessary to
draw reliable inferences from experiments on this
test set, so a larger test set may be helpful.
To compute ssel, assume we have n indepen-
dent optimization runs which produced weight vec-
tors that were used to translate a test set n times.
The test set has ` segments with references R =
?R1, R2, . . . , R`?. Let X = ?X1,X2, . . . ,Xn?
where each Xi = ?Xi1, Xi2, . . . , Xi`? is the list of
translated segments from the ith optimization run
list of the ` translated segments of the test set. For
each hypothesis output Xi, we construct k bootstrap
replicates by drawing ` segments uniformly, with re-
placement, from Xi, together with its corresponding
reference. This produces k virtual test sets for each
optimization run i. We designate the score of the jth
virtual test set of the ith optimization run with mij .
If mi = 1k
?k
j=1 mij , then we have:
si =
?
?
?
?
k?
j=1
(mij ?mi)
2
k ? 1
ssel =
1
n
n?
i=1
si
4.2 Comparing Two Systems
In the previous section, we gave statistics about
the distribution of evaluation metrics across a large
number of experimental samples (Table 1). Because
of the large number of trials we carried out, we can
be extremely confident in concluding that for both
pairs of systems, the experimental manipulation ac-
counts for the observed metric improvements, and
furthermore, that we have a good estimate of the
magnitude of that improvement. However, it is not
generally feasible to perform as many replications
as we did, so here we turn to the question of how
to compare two systems, accounting for optimizer
noise, but without running 300 replications.
We begin with a visual illustration how opti-
mizer instability affects test set scores when com-
paring two systems. Figure 1 plots the histogram
of the 300 optimizer samples each from the two
BTEC Chinese-English systems. The phrase-based
46 47 48 49 50 51
BLEU
0
5
10
15
20
25
30
35
40
Ob
se
rv
at
ion
 C
ou
nt
Figure 1: Histogram of test set BLEU scores for the
BTEC phrase-based system (left) and BTEC hierarchical
system (right). While the difference between the systems
is 1.5 BLEU in expectation, there is a non-trivial region
of overlap indicating that some random outcomes will re-
sult in little to no difference being observed.
0.6 0.3 0.0 0.3 0.6 0.9
BLEU difference
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Pr
ob
ab
ilit
y 
of
 o
bs
er
va
tio
n 1 sample
3 samples
5 samples
10 samples
50 samples
Figure 2: Relative frequencies of obtaining differences
in BLEU scores on the WMT system as a function of the
number of optimizer samples. The expected difference
is 0.2 BLEU. While there is a reasonably high chance of
observing a non-trivial improvement (or even a decline)
for 1 sample, the distribution quickly peaks around the
expected value given just a few more samples.
system?s distribution is centered at the sample
mean 48.4, and the hierarchical system is centered
at 49.9, a difference of 1.5 BLEU, correspond-
ing to the widely replicated result that hierarchi-
cal phrase-based systems outperform conventional
phrase-based systems in Chinese-English transla-
tion. Crucially, although the distributions are dis-
tinct, there is a non-trivial region of overlap, and
experimental samples from the overlapping region
could suggest the opposite conclusion!
To further underscore the risks posed by this over-
lap, Figure 2 plots the relative frequencies with
which different BLEU score deltas will occur, as a
function of the number of optimizer samples used.
When is a difference significant? To determine
whether an experimental manipulation results in a
179
statistically reliable difference for an evaluation met-
ric, we use a stratified approximate randomization
(AR) test. This is a nonparametric test that approxi-
mates a paired permutation test by sampling permu-
tations (Noreen, 1989). AR estimates the probability
(p-value) that a measured difference in metric scores
arose by chance by randomly exchanging sentences
between the two systems. If there is no significant
difference between the systems (i.e., the null hypoth-
esis is true), then this shuffling should not change
the computed metric score. Crucially, this assumes
that the samples being analyzed are representative
of all extraneous variables that could affect the out-
come of the experiment. Therefore, we must include
multiple optimizer replications. Also, since metric
scores (such as BLEU) are in general not compa-
rable across test sets, we stratify, exchanging only
hypotheses that correspond to the same sentence.
Table 2 shows the p-values computed by AR, test-
ing the significance of the differences between the
two systems in each pair. The first three rows illus-
trate ?single sample? testing practice. Depending on
luck with MERT, the results can vary widely from
insignificant (at p > .05) to highly significant.
The last two lines summarize the results of the test
when a small number of replications are performed,
as ought to be reasonable in a research setting. In
this simulation, we randomly selected n optimizer
outputs from our large pool and ran the AR test to
determine the significance; we repeated this proce-
dure 250 times. The p-values reported are the p-
values at the edges of the 95% confidence interval
(CI) according to AR seen in the 250 simulated com-
parison scenarios. These indicate that we are very
likely to observe a significant difference for BTEC
at n = 5, and a very significant difference by n = 50
(Table 2). Similarly, we see this trend in the WMT
system: more replications leads to more significant
results, which will be easier to reproduce. Based on
the average performance of the systems reported in
Table 1, we expect significance over a large enough
number of independent trials.
5 Discussion and Recommendations
No experiment can completely control for all pos-
sible confounding variables. Nor are metric scores
(even if they are statistically reliable) a substitute
for thorough human analysis. However, we believe
that the impact of optimizer instability has been ne-
p-value
n System A System B BTEC WMT
1 high low 0.25 0.95
1 median median 0.15 0.13
1 low high 0.0003 0.003
p-value (95% CI)
5 random random 0.001?0.034 0.001?0.38
50 random random 0.001?0.001 0.001?0.33
Table 2: Two-system analysis: AR p-values for three
different ?single sample? scenarios that illustrate differ-
ent pathological scenarios that can result when the sam-
pled weight vectors are ?low? or ?high.? For ?random,?
we simulate an experiments with n optimization replica-
tions by drawing n optimized system outputs from our
pool and performing AR; this simulation was repeated
250 times and the 95% CI of the AR p-values is reported.
glected by standard experimental methodology in
MT research, where single-sample measurements
are too often used to assess system differences. In
this paper, we have provided evidence that optimizer
instability can have a substantial impact on results.
However, we have also shown that it is possible to
control for it with very few replications (Table 2).
We therefore suggest:
? Replication be adopted as standard practice in
MT experimental methodology, especially in
reporting results;7
? Replication of optimization (MERT) and test
set evaluation be performed at least three times;
more replications may be necessary for experi-
mental manipulations with more subtle effects;
? Use of the median system according to a trusted
metric when manually analyzing system out-
put; preferably, the median should be deter-
mined based on one test set and a second test
set should be manually analyzed.
Acknowledgments
We thank Michael Denkowski, Kevin Gimpel, Kenneth
Heafield, Michael Heilman, and Brendan O?Connor for
insightful feedback. This research was supported in part
by the National Science Foundation through TeraGrid re-
sources provided by Pittsburgh Supercomputing Center
under TG-DBS110003; the National Science Foundation
under IIS-0713402, IIS-0844507, IIS-0915187, and IIS-
0915327; the DARPA GALE program, the U. S. Army
Research Laboratory, and the U. S. Army Research Of-
fice under contract/grant number W911NF-10-1-0533.
7Source code to carry out the AR test for multiple optimizer
samples on the three metrics in this paper is available from
http://github.com/jhclark/multeval.
180
References
A. Arun, B. Haddow, P. Koehn, A. Lopez, C. Dyer,
and P. Blunsom. 2010. Monte Carlo techniques
for phrase-based translation. Machine Translation,
24:103?121.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for mt evaluation with improved corre-
lation with human judgments. In Proc. of ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization.
N. Bertoldi, B. Haddow, and J.-B. Fouet. 2009. Im-
proved minimum error rate training in Moses. Prague
Bulletin of Mathematical Linguistics, No. 91:7?16.
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluation.
In Proc. of ICASSP.
A. Cahill, M. Burke, R. O?Donovan, S. Riezler, J. van
Genabith, and A. Way. 2008. Wide-coverage deep
statistical parsing using automatic dependency struc-
ture annotation. Computational Linguistics, 34(1):81?
124.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regular-
ization and search for minimum error rate training. In
Proc. of WMT.
D. Cer, C. D. Manning, and D. Jurafsky. 2010. The best
lexical metric for phrase-based statistical mt system
optimization. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 555?563. Proc. of ACL, June.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
N. Chinchor. 1993. The statistical significance of the
MUC-5 results. Proc. of MUC.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
M. Denkowski and A. Lavie. 2010. Extending the
METEOR machine translation evaluation metric to the
phrase level. In Proc. of NAACL.
C. Dyer, J. Weese, A. Lopez, V. Eidelman, P. Blunsom,
and P. Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proc. of ACL.
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
B. Efron. 1979. Bootstrap methods: Another look at the
jackknife. The Annals of Statistics, 7(1):1?26.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
P. Koehn, A. Birch, C. Callison-burch, M. Federico,
N. Bertoldi, B. Cowan, C. Moran, C. Dyer, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. of ACL-IJCNLP.
R. C. Moore and C. Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proc. of COLING, Manchester, UK.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-j. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
MT. In Proc. of the Workshop on Intrinsic and Extrin-
sic Evaluation Methods for Machine Translation and
Summarization.
M. Snover, B. Dorr, C. Park, R. Schwartz, L. Micciulla,
and J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
L. Wasserman. 2003. All of Statistics: A Concise Course
in Statistical Inference. Springer.
Y. Zhang and S. Vogel. 2010. Significance tests of auto-
matic machine translation metrics. Machine Transla-
tion, 24:51?65.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
BLEU/NIST scores: How much improvement do we
need to have a better system? In Proc. of LREC.
181
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 11?21,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Joint Feature Selection in Distributed Stochastic Learning
for Large-Scale Discriminative Training in SMT
Patrick Simianer and Stefan Riezler
Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany
{simianer,riezler}@cl.uni-heidelberg.de
Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cdyer@cs.cmu.edu
Abstract
With a few exceptions, discriminative train-
ing in statistical machine translation (SMT)
has been content with tuning weights for large
feature sets on small development data. Ev-
idence from machine learning indicates that
increasing the training sample size results in
better prediction. The goal of this paper is to
show that this common wisdom can also be
brought to bear upon SMT. We deploy local
features for SCFG-based SMT that can be read
off from rules at runtime, and present a learn-
ing algorithm that applies `1/`2 regulariza-
tion for joint feature selection over distributed
stochastic learning processes. We present ex-
periments on learning on 1.5 million training
sentences, and show significant improvements
over tuning discriminative models on small
development sets.
1 Introduction
The standard SMT training pipeline combines
scores from large count-based translation models
and language models with a few other features and
tunes these using the well-understood line-search
technique for error minimization of Och (2003). If
only a handful of dense features need to be tuned,
minimum error rate training can be done on small
tuning sets and is hard to beat in terms of accuracy
and efficiency. In contrast, the promise of large-
scale discriminative training for SMT is to scale to
arbitrary types and numbers of features and to pro-
vide sufficient statistical support by parameter esti-
mation on large sample sizes. Features may be lex-
icalized and sparse, non-local and overlapping, or
be designed to generalize beyond surface statistics
by incorporating part-of-speech or syntactic labels.
The modeler?s goals might be to identify complex
properties of translations, or to counter errors of pre-
trained translation models and language models by
explicitly down-weighting translations that exhibit
certain undesired properties. Various approaches to
feature engineering for discriminative models have
been presented (see Section 2), however, with a few
exceptions, discriminative learning in SMT has been
confined to training on small tuning sets of a few
thousand examples. This contradicts theoretical and
practical evidence from machine learning that sug-
gests that larger training samples should be benefi-
cial to improve prediction also in SMT. Why is this?
One possible reason why discriminative SMT has
mostly been content with small tuning sets lies in
the particular design of the features themselves. For
example, the features introduced by Chiang et al
(2008) and Chiang et al (2009) for an SCFG model
for Chinese/English translation are of two types:
The first type explicitly counters overestimates of
rule counts, or rules with bad overlap points, bad
rewrites, or with undesired insertions of target-side
terminals. These features are specified in hand-
crafted lists based on a thorough analysis of a tuning
set. Such finely hand-crafted features will find suf-
ficient statistical support on a few thousand exam-
ples and thus do not benefit from larger training sets.
The second type of features deploys external infor-
mation such as syntactic parses or word alignments
to penalize bad reorderings or undesired translations
of phrases that cross syntactic constraints. At large
scale, extraction of such features quickly becomes
11
(1) X ? X1 hat X2 versprochen, X1 promised X2
(2) X ? X1 hat mir X2 versprochen,
X1 promised me X2
(3) X ? X1 versprach X2, X1 promised X2
Figure 1: SCFG rules for translation.
infeasible because of costly generation and storage
of linguistic annotations. Another possible reason
why large training data did not yet show the ex-
pected improvements in discriminative SMT is a
special overfitting problem of current popular online
learning techniques. This is due to stochastic learn-
ing on a per-example basis where a weight update on
a misclassified example may apply only to a small
fraction of data that have been seen before. Thus
many features will not generalize well beyond the
training examples on which they were introduced.
The goal of this paper is to investigate if and
how it is possible to benefit from scaling discrimi-
native training for SMT to large training sets. We
deploy generic features for SCFG-based SMT that
can efficiently be read off from rules at runtime.
Such features include rule ids, rule-local n-grams,
or types of rule shapes. Another crucial ingredi-
ent of our approach is a combination of parallelized
stochastic learning with feature selection inspired
by multi-task learning. The simple but effective
idea is to randomly divide training data into evenly
sized shards, use stochastic learning on each shard
in parallel, while performing `1/`2 regularization
for joint feature selection on the shards after each
epoch, before starting a new epoch with a reduced
feature vector averaged across shards. Iterative fea-
ture selection procedure is the key to both efficiency
and improved prediction: Without interleaving par-
allelized stochastic learning with feature selection
our largest experiments would not be feasible. Se-
lecting features jointly across shards and averaging
does counter the overfitting effect that is inherent
to stochastic updating. Our resulting models are
learned on large data sets, but they are small and
outperform models that tune feature sets of various
sizes on small development sets. Our software is
freely available as a part of the cdec1 framework.
1https://github.com/redpony/cdec
2 Related Work
The great promise of discriminative training for
SMT is the possibility to design arbitrarily expres-
sive, complex, or overlapping features in great num-
bers. The focus of many approaches thus has been
on feature engineering and on adaptations of ma-
chine learning algorithms to the special case of SMT
(where gold standard rankings have to be created
automatically). Examples for adapted algorithms
include Maximum-Entropy Models (Och and Ney,
2002; Blunsom et al, 2008), Pairwise Ranking Per-
ceptrons (Shen et al, 2004; Watanabe et al, 2006;
Hopkins and May, 2011), Structured Perceptrons
(Liang et al, 2006a), Boosting (Duh and Kirchhoff,
2008; Wellington et al, 2009), Structured SVMs
(Tillmann and Zhang, 2006; Hayashi et al, 2009),
MIRA (Watanabe et al, 2007; Chiang et al, 2008;
Chiang et al, 2009), and others. Adaptations of the
loss functions underlying such algorithms to SMT
have recently been described as particular forms
of ramp loss optimization (McAllester and Keshet,
2011; Gimpel and Smith, 2012).
All approaches have been shown to scale to large
feature sets and all include some kind of regulariza-
tion method. However, most approaches have been
confined to training on small tuning sets. Exceptions
where discriminative SMT has been used on large
training data are Liang et al (2006a) who trained 1.5
million features on 67,000 sentences, Blunsom et
al. (2008) who trained 7.8 million rules on 100,000
sentences, or Tillmann and Zhang (2006) who used
230,000 sentences for training.
Our approach is inspired by Duh et al (2010)
who applied multi-task learning for improved gen-
eralization in n-best reranking. In contrast to our
work, Duh et al (2010) did not incorporate multi-
task learning into distributed learning, but defined
tasks as n-best lists, nor did they develop new algo-
rithms, but used off-the-shelf multi-task tools.
3 Local Features for Synchronous CFGs
The work described in this paper is based on the
SMT framework of hierarchical phrase-based trans-
lation (Chiang, 2005; Chiang, 2007). Transla-
tion rules are extracted from word-aligned paral-
lel sentences and can be seen as productions of a
synchronous CFG. Examples are rules like (1)-(3)
12
shown in Figure 1. Local features are designed to be
readable directly off the rule at decoding time. We
use three rule templates in our work:
Rule identifiers: These features identify each rule
by a unique identifier. Such features corre-
spond to the relative frequencies of rewrites
rules used in standard models.
Rule n-grams: These features identify n-grams of
consecutive items in a rule. We use bigrams
on source-sides of rules. Such features identify
possible source side phrases and thus can give
preference to rules including them.2
Rule shape: These features are indicators that ab-
stract away from lexical items to templates that
identify the location of sequences of terminal
symbols in relation to non-terminal symbols,
on both the source- and target-sides of each
rule used. For example, both rules (1) and (2)
map to the same indicator, namely that a rule
is being used that consists of a (NT, term*, NT,
term*) pattern on its source side, and an (NT,
term*, NT) pattern on its target side. Rule (3)
maps to a different template, that of (NT, term*,
NT) on source and target sides.
4 Joint Feature Selection in Distributed
Stochastic Learning
The following discussion of learning methods is
based on pairwise ranking in a Stochastic Gradi-
ent Descent (SGD) framework. The resulting al-
gorithms can be seen as variants of the perceptron
algorithm. Let each translation candidate be repre-
sented by a feature vector x ? IRD where preference
pairs for training are prepared by sorting translations
according to smoothed sentence-wise BLEU score
(Liang et al, 2006a) against the reference. For a
preference pair xj = (x(1)j ,x
(2)
j ) where x
(1)
j is pre-
ferred over x(2)j , and x?j = x
(1)
j ? x
(2)
j , we consider
the following hinge loss-type objective function:
lj(w) = (??w, x?j ?)+
where (a)+ = max(0, a) , w ? IRD is a weight vec-
tor, and ??, ?? denotes the standard vector dot prod-
uct. Instantiating SGD to the following stochastic
2Similar ?monolingual parse features? have been used in
Dyer et al (2011).
subgradient leads to the perceptron algorithm for
pairwise ranking3 (Shen and Joshi, 2005):
?lj(w) =
{
?x?j if ?w, x?j? ? 0,
0 else.
Our baseline algorithm 1 (SDG) scales pairwise
ranking to large scale scenarios. The algorithm takes
an average over the final weight updates of each
epoch instead of keeping a record of all weight up-
dates for final averaging (Collins, 2002) or for voting
(Freund and Schapire, 1999).
Algorithm 1 SGD: int I, T , float ?
Initialize w0,0,0 ? 0.
for epochs t? 0 . . . T ? 1: do
for all i ? {0 . . . I ? 1}: do
Decode ith input with wt,i,0.
for all pairs xj , j ? {0 . . . P ? 1}: do
wt,i,j+1 ? wt,i,j ? ??lj(wt,i,j)
end for
wt,i+1,0 ? wt,i,P
end for
wt+1,0,0 ? wt,I,0
end for
return 1T
T?
t=1
wt,0,0
While stochastic learning exhibits a runtime be-
havior that is linear in sample size (Bottou, 2004),
very large datasets can make sequential process-
ing infeasible. Algorithm 2 (MixSGD) addresses
this problem by parallelization in the framework of
MapReduce (Dean and Ghemawat, 2004).
Algorithm 2 MixSGD: int I, T, Z, float ?
Partition data into Z shards, each of size S ? I/Z;
distribute to machines.
for all shards z ? {1 . . . Z}: parallel do
Initialize wz,0,0,0 ? 0.
for epochs t? 0 . . . T ? 1: do
for all i ? {0 . . . S ? 1}: do
Decode ith input with wz,t,i,0.
for all pairs xj , j ? {0 . . . P ? 1}: do
wz,t,i,j+1 ? wz,t,i,j ? ??lj(wz,t,i,j)
end for
wz,t,i+1,0 ? wz,t,i,P
end for
wz,t+1,0,0 ? wz,t,S,0
end for
end for
Collect final weights from each machine,
return 1Z
Z?
z=1
(
1
T
T?
t=1
wz,t,0,0
)
.
3Other loss functions lead to stochastic versions of SVMs
(Collobert and Bengio, 2004; Shalev-Shwartz et al, 2007;
Chapelle and Keerthi, 2010).
13
Algorithm 2 is a variant of the SimuParallelSGD
algorithm of Zinkevich et al (2010) or equivalently
of the parameter mixing algorithm of McDonald et
al. (2010). The key idea of algorithm 2 is to parti-
tion the data into disjoint shards, then train SGD on
each shard in parallel, and after training mix the final
parameters from each shard by averaging. The algo-
rithm requires no communication between machines
until the end.
McDonald et al (2010) also present an iterative
mixing algorithm where weights are mixed from
each shard after training a single epoch of the per-
ceptron in parallel on each shard. The mixed weight
vector is re-sent to each shard to start another epoch
of training in parallel on each shard. This algorithm
corresponds to our algorithm 3 (IterMixSGD).
Algorithm 3 IterMixSGD: int I, T, Z, float ?
Partition data into Z shards, each of size S ? I/Z;
distribute to machines.
Initialize v? 0.
for epochs t? 0 . . . T ? 1: do
for all shards z ? {1 . . . Z}: parallel do
wz,t,0,0 ? v
for all i ? {0 . . . S ? 1}: do
Decode ith input with wz,t,i,0.
for all pairs xj , j ? {0 . . . P ? 1}: do
wz,t,i,j+1 ? wz,t,i,j ? ??lj(wz,t,i,j)
end for
wz,t,i+1,0 ? wz,t,i,P
end for
end for
Collect weights v? 1Z
Z?
z=1
wz,t,S,0.
end for
return v
Parameter mixing by averaging will help to ease
the feature sparsity problem, however, keeping fea-
ture vectors on the scale of several million features
in memory can be prohibitive. If network latency
is a bottleneck, the increased amount of information
sent across the network after each epoch may be a
further problem.
Our algorithm 4 (IterSelSGD) introduces feature
selection into distributed learning for increased effi-
ciency and as a more radical measure against over-
fitting. The key idea is to view shards as tasks, and
to apply methods for joint feature selection from
multi-task learning to achieve small sets of features
that are useful across all tasks or shards. Our algo-
rithm represents weights in a Z-by-D matrix W =
[wz1 | . . . |wzZ ]T of stacked D-dimensional weight
vectors across Z shards. We compute the `2 norm of
the weights in each feature column, sort features by
this value, and keep K features in the model. This
feature selection procedure is done after each epoch.
Reduced weight vectors are mixed and the result is
re-sent to each shard to start another epoch of paral-
lel training on each shard.
Algorithm 4 IterSelSGD: int I, T, Z,K, float ?
Partition data into Z shards, each of size S = I/Z;
distribute to machines.
Initialize v? 0.
for epochs t? 0 . . . T ? 1: do
for all shards z ? {1 . . . Z}: parallel do
wz,t,0,0 ? v
for all i ? {0 . . . S ? 1}: do
Decode ith input with wz,t,i,0.
for all pairs xj , j ? {0 . . . P ? 1}: do
wz,t,i,j+1 ? wz,t,i,j ? ??lj(wz,t,i,j)
end for
wz,t,i+1,0 ? wz,t,i,P
end for
end for
Collect/stack weights W? [w1,t,S,0| . . . |wZ,t,S,0]T
Select top K feature columns of W by `2 norm and
for k ? 1 . . .K do
v[k] = 1Z
Z?
z=1
W[z][k].
end for
end for
return v
This algorithm can be seen as an instance of `1/`2
regularization as follows: Let wd be the dth column
vector of W, representing the weights for the dth
feature across tasks/shards. `1/`2 regularization pe-
nalizes weights W by the weighted `1/`2 norm
?||W||1,2 = ?
D?
d=1
||wd||2.
Each `2 norm of a weight column represents
the relevance of the corresponding feature across
tasks/shards. The `1 sum of the `2 norms en-
forces a selection among features based on these
norms. Consider for example the two 5-feature, 3-
task weight matrices in Figure 2. Assuming the
same loss for both matrices, the right-hand side ma-
trix is preferred because of a smaller `1/`2 norm
(12 instead of 18). This matrix shares features
across tasks which leads to larger `2 norms for some
columns (here ||w1||2 and ||w2||2) and forces other
columns to zero. This results in shrinking the ma-
trix to those features that are useful across all tasks.
14
w1 w2 w3 w4 w5 w1 w2 w3 w4 w5
wz1 [ 6 4 0 0 0 ] [ 6 4 0 0 0 ]
wz2 [ 0 0 3 0 0 ] [ 3 0 0 0 0 ]
wz3 [ 0 0 0 2 3 ] [ 2 3 0 0 0 ]
column `2 norm: 6 4 3 2 3 7 5 0 0 0
`1 sum: ? 18 ? 12
Figure 2: `1/`2 regularization enforcing feature selection.
Our algorithm is related to Obozinski et al
(2010)?s approach to `1/`2 regularization where fea-
ture columns are incrementally selected based on the
`2 norms of the gradient vectors corresponding to
feature columns. Their algorithm is itself an exten-
sion of gradient-based feature selection based on the
`1 norm, e.g., Perkins et al (2003).4 In contrast to
these approaches we approximate the gradient by us-
ing the weights given by the ranking algorithm itself.
This relates our work to weight-based recursive fea-
ture elimination (RFE) (Lal et al, 2006). Further-
more, algorithm 4 performs feature selection based
on a choice of meta-parameter of K features instead
of by thresholding a regularization meta-parameter
?, however, these techniques are equivalent and can
be transformed into each other.
5 Experiments
5.1 Data, Systems, Experiment Settings
The datasets used in our experiments are versions
of the News Commentary (nc), News Crawl (crawl)
and Europarl (ep) corpora described in Table 1. The
translation direction is German-to-English.
The SMT framework used in our experiments
is hierarchical phrase-based translation (Chiang,
2007). We use the cdec decoder5 (Dyer et al,
2010) and induce SCFG grammars from two sets of
symmetrized alignments using the method described
by Chiang (2007). All data was tokenized and
lowercased; German compounds were split (Dyer,
2009). For word alignment of the news-commentary
data, we used GIZA++ (Och and Ney, 2000); for
aligning the Europarl data, we used the Berke-
ley aligner (Liang et al, 2006b). Before train-
ing, we collect all the grammar rules necessary to
4Note that by definition of ||W||1,2, standard `1 regulariza-
tion is a special case of `1/`2 regularization for a single task.
5cdec metaparameters were set to a non-terminal span limit
of 15 and standard cube pruning with a pop limit of 200.
translate each individual sentence into separate files
(so-called per-sentence grammars) (Lopez, 2007).
When decoding, cdec loads the appropriate file im-
mediately prior to translation of the sentence. The
computational overhead is minimal compared to the
expense of decoding. Also, deploying disk space
instead of memory fits perfectly into the MapRe-
duce framework we are working in. Furthermore,
the extraction of grammars for training is done in
a leave-one-out fashion (Zollmann and Sima?an,
2005) where rules are extracted for a parallel sen-
tence pair only if the same rules are found in other
sentences of the corpus as well.
3-gram (news-commentary) and 5-gram (Eu-
roparl) language models are trained on the data de-
scribed in Table 1, using the SRILM toolkit (Stol-
cke, 2002) and binarized for efficient querying using
kenlm (Heafield, 2011). For the 5-gram language
models, we replaced every word in the lm training
data with <unk> that did not appear in the English
part of the parallel training data to build an open vo-
cabulary language model.
HI
MID
LOW
Figure 3: Multipartite pairwise ranking.
Training data for discriminative learning are pre-
pared by comparing a 100-best list of transla-
tions against a single reference using smoothed per-
sentence BLEU (Liang et al, 2006a). From the
BLEU-reordered n-best list, translations were put
into sets for the top 10% level (HI), the middle
80% level (MID), and the bottom 10% level (LOW).
These level sets are used for multipartite ranking
15
News Commentary(nc)
train-nc lm-train-nc dev-nc devtest-nc test-nc
Sentences 132,753 180,657 1057 1064 2007
Tokens de 3,530,907 ? 27,782 28,415 53,989
Tokens en 3,293,363 4,394,428 26,098 26,219 50,443
Rule Count 14,350,552 (1G) ? 2,322,912 2,320,264 3,274,771
Europarl(ep)
train-ep lm-train-ep dev-ep devtest-ep test-ep
Sentences 1,655,238 2,015,440 2000 2000 2000
Tokens de 45,293,925 ? 57,723 56,783 59,297
Tokens en 45,374,649 54,728,786 58,825 58,100 60,240
Rule Count 203,552,525 (31.5G) ? 17,738,763 17,682,176 18,273,078
News Crawl(crawl)
dev-crawl test-crawl10 test-crawl11
Sentences 2051 2489 3003
Tokens de 49,848 64,301 76,193
Tokens en 49,767 61,925 74,753
Rule Count 9,404,339 11,307,304 12,561,636
Table 1: Overview of data used for train/dev/test. News Commentary (nc) and Europarl (ep) training data and
also News Crawl (crawl) dev/test data were taken from the WMT11 translation task (http://statmt.org/
wmt11/translation-task.html). The dev/test data of nc are the sets provided with the WMT07 shared
task (http://statmt.org/wmt07/shared-task.html). Ep dev/test data is from WMT08 shared task
(http://statmt.org/wmt08/shared-task.html). The numbers in brackets for the rule counts of ep/nc
training data are total counts of rules in the per-sentence grammars.
where translation pairs are built between the ele-
ments in HI-MID, HI-LOW, and MID-LOW, but not
between translations inside sets on the same level.
This idea is depicted graphically in Figure 3. The
intuition is to ensure that good translations are pre-
ferred over bad translations without teasing apart
small differences.
For evaluation, we used the mteval-v11b.pl
script to compute lowercased BLEU-4 scores (Pa-
pineni et al, 2001). Statistical significance was
measured using an Approximate Randomization test
(Noreen, 1989; Riezler and Maxwell, 2005).
All experiments for training on dev sets were car-
ried out on a single computer. For grammar extrac-
tion and training of the full data set we used a 30
node hadoop Map/Reduce cluster that can handle
300 jobs at once. We split the data into 2290 shards
for the ep runs and 141 shards for the nc runs, each
shard holding about 1,000 sentences, which corre-
sponds to the dev set size of the nc data set.
5.2 Experimental Results
The baseline learner in our experiments is a pairwise
ranking perceptron that is used on various features
and training data and plugged into various meta-
M
x?
BLEU[%] 23.0 25.0 27.0 29.0
Figure 4: Boxplot of BLEU-4 results for 100 runs of
MIRA on news commentary data, depicting median (M),
mean (x?), interquartile range (box), standard deviation
(whiskers), outliers (end points).
algorithms for distributed processing. The percep-
tron algorithm itself compares favorably to related
learning techniques such as the MIRA adaptation of
Chiang et al (2008). Figure 4 gives a boxplot depict-
ing BLEU-4 results for 100 runs of the MIRA imple-
mentation of the cdec package, tuned on dev-nc,
and evaluated on the respective test set test-nc.6 We
see a high variance (whiskers denote standard devi-
ations) around a median of 27.2 BLEU and a mean
of 27.1 BLEU. The fluctuation of results is due to
sampling training examples from the translation hy-
6MIRA was used with default meta parameters: 250 hypoth-
esis list to search for oracles, regularization strength C = 0.01
and using 15 passes over the input. It optimized IBM BLEU-4.
The initial weight vector was 0.
16
Algorithm Tuning set Features #Features devtest-nc test-nc
MIRA dev-nc default 12 ? 27.10
1
dev-nc default 12 25.88 28.0
dev-nc +id 137k 25.53 27.6?23
dev-nc +ng 29k 25.82 27.42?234
dev-nc +shape 51 25.91 28.1
dev-nc +id,ng,shape 180k 25.71 28.1534
2
train-nc default 12 25.73 27.86
train-nc +id 4.1M 25.13 27.19?134
train-nc +ng 354k 26.09 28.03134
train-nc +shape 51 26.07 27.913
train-nc +id,ng,shape 4.7M 26.08 27.8634
3
train-nc default 12 26.09 @2 27.94?
train-nc +id 3.4M 26.1 @4 27.97?12
train-nc +ng 330k 26.33 @4 28.3412
train-nc +shape 51 26.39 @9 28.312
train-nc +id,ng,shape 4.7M 26.42 @9 28.55124
4
train-nc +id 100k 25.91 @7 27.82?2
train-nc +ng 100k 26.42 @4 28.37?12
train-nc +id,ng,shape 100k 26.8 @8 28.81123
Table 2: BLEU-4 results for algorithms 1 (SGD), 2 (MixSGD), 3 (IterMixSDG), and 4 (IterSelSGD) on news-
commentary (nc) data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and
rule shape (shape). Statistical significance at p-level < 0.05 of a result difference on the test set to a different algo-
rithm applied to the same feature group is indicated by raised algorithm number. ? indicates statistically significant
differences to best result across features groups for same algorithm, indicated in bold face. @ indicates the optimal
number of epochs chosen on the devtest set.
pergraph as is done in the cdec implementation of
MIRA. We found similar fluctuations for the cdec
implementations of PRO (Hopkins and May, 2011)
or hypergraph-MERT (Kumar et al, 2009) both of
which depend on hypergraph sampling. In contrast,
the perceptron is deterministic when started from a
zero-vector of weights and achieves favorable 28.0
BLEU on the news-commentary test set. Since we
are interested in relative improvements over a stable
baseline, we restrict our attention in all following ex-
periments to the perceptron.7
Table 2 shows the results of the experimental
comparison of the 4 algorithms of Section 4. The
7Absolute improvements would be possible, e.g., by using
larger language models or by adding news data to the ep train-
ing set when evaluating on crawl test sets (see, e.g., Dyer et al
(2011)), however, this is not the focus of this paper.
default features include 12 dense models defined on
SCFG rules;8 The sparse features are the 3 templates
described in Section 3. All feature weights were
tuned together using algorithms 1-4. If not indicated
otherwise, the perceptron was run for 10 epochs with
learning rate ? = 0.0001, started at zero weight vec-
tor, using deduplicated 100-best lists.
The results on the news-commentary (nc) data
show that training on the development set does not
benefit from adding large feature sets ? BLEU re-
sult differences between tuning 12 default features
8negative log relative frequency p(e|f); log count(f ); log
count(e, f ); lexical translation probability p(f |e) and p(e|f)
(Koehn et al, 2003); indicator variable on singleton phrase e;
indicator variable on singleton phrase pair f, e; word penalty;
language model weight; OOV count of language model; num-
ber of untranslated words; Hiero glue rules (Chiang, 2007).
17
Alg. Tuning set Features #Feats devtest-ep test-ep Tuning set test-crawl10 test-crawl11
1
dev-ep default 12 25.62 26.42? dev-crawl 15.39? 14.43?
dev-ep +id,ng,shape 300k 27.84 28.37 dev-crawl 17.84 16.834
4 train-ep +id,ng,shape 100k 28.0 @9 28.62 train-ep 19.121 17.331
Table 3: BLEU-4 results for algorithms 1 (SGD) and 4 (IterSelSGD) on Europarl (ep) and news crawl (crawl) test
data. Feature groups are 12 dense features (default), rule identifiers (id), rule n-gram (ng), and rule shape (shape).
Statistical significance at p-level < 0.05 of a result difference on the test set to a different algorithm applied to the
same feature group is indicated by raised algorithm number. ? indicates statistically significant differences to best
result across features groups for same algorithm, indicated in bold face. @ indicates the optimal number of epochs
chosen on the devtest set.
and tuning the full set of 180,000 features are not
significant. However, scaling all features to the full
training set shows significant improvements for al-
gorithm 3, and especially for algorithm 4, which
gains 0.8 BLEU points over tuning 12 features on
the development set. The number of features rises
to 4.7 million without feature selection, which iter-
atively selects 100,000 features with best `2 norm
values across shards. Feature templates such as rule
n-grams and rule shapes only work if iterative mix-
ing (algorithm 3) or feature selection (algorithm 4)
are used. Adding rule id features works in combina-
tion with other sparse features.
Table 3 shows results for algorithms 1 and 4 on
the Europarl data (ep) for different devtest and test
sets. Europarl data were used in all runs for train-
ing and for setting the meta-parameter of number
of epochs. Testing was done on the Europarl test
set and news crawl test data from the years 2010
and 2011. Here tuning large feature sets on the
respective dev sets yields significant improvements
of around 2 BLEU points over tuning the 12 de-
fault features on the dev sets. Another 0.5 BLEU
points (test-crawl11) or even 1.3 BLEU points (test-
crawl10) are gained when scaling to the full training
set using iterative features selection. Result differ-
ences on the Europarl test set were not significant
for moving from dev to full train set. Algorithms 2
and 3 were infeasible to run on Europarl data beyond
one epoch because features vectors grew too large to
be kept in memory.
6 Discussion
We presented an approach to scaling discrimina-
tive learning for SMT not only to large feature
sets but also to large sets of parallel training data.
Since inference for SMT (unlike many other learn-
ing problems) is very expensive, especially on large
training sets, good parallelization is key. Our ap-
proach is made feasible and effective by applying
joint feature selection across distributed stochastic
learning processes. Furthermore, our local features
are efficiently computable at runtime. Our algo-
rithms and features are generic and can easily be re-
implemented and make our results relevant across
datasets and language pairs.
In future work, we would like to investigate more
sophisticated features, better learners, and in gen-
eral improve the components of our system that have
been neglected in the current investigation of rela-
tive improvements by scaling the size of data and
feature sets. Ultimately, since our algorithms are in-
spired by multi-task learning, we would like to apply
them to scenarios where a natural definition of tasks
is given. For example, patent data can be charac-
terized along the dimensions of patent classes and
patent text fields (Wa?schle and Riezler, 2012) and
thus are well suited for multi-task translation.
Acknowledgments
Stefan Riezler and Patrick Simianer were supported
in part by DFG grant ?Cross-language Learning-to-
Rank for Patent Retrieval?. Chris Dyer was sup-
ported in part by a MURI grant ?The linguistic-
core approach to structured translation and analysis
of low-resource languages? from the US Army Re-
search Office and a grant ?Unsupervised Induction
of Multi-Nonterminal Grammars for SMT? from
Google, Inc.
18
References
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable models for statistical
machine translation. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT?08), Columbus, OH.
Le?on Bottou. 2004. Stochastic learning. In Olivier
Bousquet, Ulrike von Luxburg, and Gunnar Ra?tsch,
editors, Advanced Lectures on Machine Learning,
pages 146?168. Springer, Berlin.
Olivier Chapelle and S. Sathiya Keerthi. 2010. Efficient
algorithms for ranking with SVMs. Information Re-
trieval Journal.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?08), Waikiki, Honolulu,
Hawaii.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proceedings of the 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL-HLT?09),
Boulder, CO.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), Ann Arbor, MI.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2).
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In Proceedings of the con-
ference on Empirical Methods in Natural Language
Processing (EMNLP?02), Philadelphia, PA.
Ronan Collobert and Samy Bengio. 2004. Links be-
tween perceptrons, MLPs, and SVMs. In Proceed-
ings of the 21st International Conference on Machine
Learning (ICML?04), Banff, Canada.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapre-
duce: Simplified data processing on large clusters. In
Proceedings of the 6th Symposium on Operating Sys-
tem Design and Implementation (OSDI?04), San Fran-
cisco, CA.
Kevin Duh and Katrin Kirchhoff. 2008. Beyond log-
linear models: Boosted minimum error rate training
for n-best ranking. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?08), Short Paper Track, Columbus, OH.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. N-best reranking
by multitask learning. In Proceedings of the 5th Joint
Workshop on Statistical Machine Translation and Met-
ricsMATR, Uppsala, Sweden.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of the ACL 2010 System Demonstrations, Upp-
sala, Sweden.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK german-
english translation system. In Proceedings of the 6th
Workshop on Machine Translation (WMT11), Edin-
burgh, UK.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings
of the Conference of the North American Chapter of
the Association for Computational Linguistics - Hu-
man Language Technologies (NAACL-HLT?09), Boul-
der, CO.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Journal of Machine Learning Research, 37:277?296.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL-
HLT 2012), Montreal, Canada.
Katsuhiko Hayashi, Taro Watanabe, Hajime Tsukada,
and Hideki Isozaki. 2009. Structural support vector
machines for log-linear approach in statistical machine
translation. In Proceedings of IWSLT, Tokyo, Japan.
Kenneth Heafield. 2011. KenLM: faster and smaller lan-
guage model queries. In Proceedings of the EMNLP
2011 Sixth Workshop on Statistical Machine Transla-
tion (WMT?11), Edinburgh, UK.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP?11), Edinburgh, Scotland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
and the 3rd Meeting of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?03), Edmonton, Cananda.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum Bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the 47th
Annual Meeting of the Association for Computational
19
Linguistics and the 4th IJCNLP of the AFNLP (ACL-
IJCNLP?09, Suntec, Singapore.
Thomas Navin Lal, Olivier Chapelle, Jason Weston, and
Andre? Elisseeff. 2006. Embedded methods. In I.M.
Guyon, S.R. Gunn, M. Nikravesh, and L. Zadeh, ed-
itors, Feature Extraction: Foundations and Applica-
tions. Springer.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006a. An end-to-end discriminative
approach to machine translation. In Proceedings of
the joint conference of the International Committee
on Computational Linguistics and the Association for
Computational Linguistics (COLING-ACL?06), Syd-
ney, Australia.
Percy Liang, Ben Taskar, and Dan Klein. 2006b. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference - North American
Chapter of the Association for Computational Linguis-
tics annual meeting (HLT-NAACL?06), New York, NY.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In Proceedings of EMNLP-
CoNLL, Prague, Czech Republic.
David McAllester and Joseph Keshet. 2011. Generaliza-
tion bounds and consistency for latent structural pro-
bit and ramp loss. In Proceedings of the 25th Annual
Conference on Neural Information Processing Sytems
(NIPS 2011), Granada, Spain.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of Human Language Tech-
nologies: The 11th Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT?10), Los Angeles,
CA.
Eric W. Noreen. 1989. Computer Intensive Methods
for Testing Hypotheses. An Introduction. Wiley, New
York.
Guillaume Obozinski, Ben Taskar, and Michael I. Jordan.
2010. Joint covariate selection and joint subspace se-
lection for multiple classification problems. Statistics
and Computing, 20:231?252.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hongkong, China.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL?02), Philadelphia, PA.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceedings
of the Human Language Technology Conference and
the 3rd Meeting of the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL?03), Edmonton, Cananda.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
IBM Research Division Technical Report, RC22176
(W0190-022), Yorktown Heights, N.Y.
Simon Perkins, Kevin Lacker, and James Theiler. 2003.
Grafting: Fast, incremental feature selection by gra-
dient descent in function space. Journal of Machine
Learning Research, 3:1333?1356.
Stefan Riezler and John Maxwell. 2005. On some pit-
falls in automatic evaluation and significance testing
for MT. In Proceedings of the ACL-05 Workshop on
Intrinsic and Extrinsic Evaluation Measures for MT
and/or Summarization, Ann Arbor, MI.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-
bro. 2007. Pegasos: Primal Estimated sub-GrAdient
SOlver for SVM. In Proceedings of the 24th Inter-
national Conference on Machine Learning (ICML?07),
Corvallis, OR.
Libin Shen and Aravind K. Joshi. 2005. Ranking and
reranking with perceptron. Journal of Machine Learn-
ing Research, 60(1-3):73?96.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proceedings of the Human Language Technology con-
ference / North American chapter of the Associa-
tion for Computational Linguistics annual meeting
(HLT/NAACL?04), Boston, MA.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing, Denver,
CO.
Christoph Tillmann and Tong Zhang. 2006. A dis-
criminatie global training algorithm for statistical MT.
In Proceedings of the joint conference of the In-
ternational Committee on Computational Linguistics
and the Association for Computational Linguistics
(COLING-ACL?06), Sydney, Australia.
Katharina Wa?schle and Stefan Riezler. 2012. Structural
and topical dimensions in multi-task patent translation.
In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, Avignon, France.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2006. NTT statistical machine translation
for IWSLT 2006. In Proceedings of the International
Workshop on Spoken Language Translation (IWSLT),
Kyoto, Japan.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proceedings of the 2007
20
Joint Conference on Empirical Mehtods in Natural
Language Processing and Computational Language
Learning (EMNLP?07), Prague, Czech Republic.
Benjamin Wellington, Joseph Turian, and Dan Melamed.
2009. Toward purely discriminative training for tree-
structured translation models. In Cyril Goutte, Nicola
Cancedda, and Marc Dymetman, editors, Learning
Machine Translation, pages 132?149, Cambridge,
MA. The MIT Press.
Martin A. Zinkevich, Markus Weimer, Alex Smola, and
Lihong Li. 2010. Parallelized stochastic gradient de-
scent. In Proceedings of the 24th Annual Conference
on Neural Information Processing Sytems (NIPS?10),
Vancouver, Canada.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata, Languages and Combinatorics,
10(2/3):367?388.
21
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 176?186,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Microblogs as Parallel Corpora
Wang Ling123 Guang Xiang2 Chris Dyer2 Alan Black2 Isabel Trancoso 13
(1)L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior Te?cnico, Lisbon, Portugal
{lingwang,guangx,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
In the ever-expanding sea of microblog data, there
is a surprising amount of naturally occurring par-
allel text: some users create post multilingual mes-
sages targeting international audiences while oth-
ers ?retweet? translations. We present an efficient
method for detecting these messages and extract-
ing parallel segments from them. We have been
able to extract over 1M Chinese-English parallel
segments from Sina Weibo (the Chinese counter-
part of Twitter) using only their public APIs. As a
supplement to existing parallel training data, our
automatically extracted parallel data yields sub-
stantial translation quality improvements in trans-
lating microblog text and modest improvements
in translating edited news commentary. The re-
sources in described in this paper are available at
http://www.cs.cmu.edu/?lingwang/utopia.
1 Introduction
Microblogs such as Twitter and Facebook have
gained tremendous popularity in the past 10 years.
In addition to being an important form of commu-
nication for many people, they often contain ex-
tremely current, even breaking, information about
world events. However, the writing style of mi-
croblogs tends to be quite colloquial, with fre-
quent orthographic innovation (R U still with me
or what?) and nonstandard abbreviations (idk!
shm)?quite unlike the style found in more tra-
ditional, edited genres. This poses considerable
problems for traditional NLP tools, which were
developed with other domains in mind, which of-
ten make strong assumptions about orthographic
uniformity (i.e., there is just one way to spell you).
One approach to cope with this problem is to an-
notate in-domain data (Gimpel et al, 2011).
Machine translation suffers acutely from the
domain-mismatch problem caused by microblog
text. On one hand, standard models are probably
suboptimal since they (like many models) assume
orthographic uniformity in the input. However,
more acutely, the data used to develop these sys-
tems and train their models is drawn from formal
and carefully edited domains, such as parallel web
pages and translated legal documents. MT training
data seldom looks anything like microblog text.
This paper introduces a method for finding nat-
urally occurring parallel microblog text, which
helps address the domain-mismatch problem.
Our method is inspired by the perhaps surpris-
ing observation that a reasonable number of mi-
croblog users tweet ?in parallel? in two or more
languages. For instance, the American entertainer
Snoop Dogg regularly posts parallel messages on
Sina Weibo (Mainland China?s equivalent of Twit-
ter), for example, watup Kenny Mayne!! - Kenny
Mayne?????????, where an English
message and its Chinese translation are in the
same post, separated by a dash. Our method is able
to identify and extract such translations. Briefly,
this requires determining if a tweet contains more
than one language, if these multilingual utterances
contain translated material (or are due to some-
thing else, such as code switching), and what the
translated spans are.
The paper is organized as follows. Section 2
describes the related work in parallel data extrac-
tion. Section 3 presents our model to extract par-
allel data within the same document. Section 4
describes our extraction pipeline. Section 5 de-
scribes the data we gathered from both Sina Weibo
(Chinese-English) and Twitter (Chinese-English
and Arabic-English). We then present experiments
showing that our harvested data not only substan-
tially improves translations of microblog text with
176
existing (and arguably inappropriate) translation
models, but that it improves the translation of
more traditional MT genres, like newswire. We
conclude in Section 6.
2 Related Work
Automatic collection of parallel data is a well-
studied problem. Approaches to finding par-
allel web documents automatically have been
particularly important (Resnik and Smith, 2003;
Fukushima et al, 2006; Li and Liu, 2008; Uszko-
reit et al, 2010; Ture and Lin, 2012). These
broadly work by identifying promising candidates
using simple features, such as URL similarity or
?gist translations? and then identifying truly par-
allel segments with more expensive classifiers.
More specialized resources were developed using
manual procedures to leverage special features of
very large collections, such as Europarl (Koehn,
2005).
Mining parallel or comparable messages from
microblogs has mainly relied on Cross-Lingual In-
formation Retrieval techniques (CLIR). Jelh et al
(2012) attempt to find pairs of tweets in Twitter us-
ing Arabic tweets as search queries in a CLIR sys-
tem. Afterwards, the model described in (Xu et al,
2001) is applied to retrieve a set of ranked trans-
lation candidates for each Arabic tweet, which are
then used as parallel candidates.
The work on mining parenthetical transla-
tions (Lin et al, 2008), which attempts to find
translations within the same document, has some
similarities with our work, since parenthetical
translations are within the same document. How-
ever, parenthetical translations are generally used
to translate names or terms, which is more lim-
ited than our work which extracts whole sentence
translations.
Finally, crowd-sourcing techniques to obtain
translations have been previously studied and ap-
plied to build datasets for casual domains (Zbib
et al, 2012; Post et al, 2012). These approaches
require remunerated workers to translate the mes-
sages, and the amount of messages translated per
day is limited. We aim to propose a method that
acquires large amounts of parallel data for free.
The drawback is that there is a margin of error in
the parallel segment identification and alignment.
However, our system can be tuned for precision or
for recall.
3 Parallel Segment Retrieval
We will first abstract from the domain of Mi-
croblogs and focus on the task of retrieving par-
allel segments from single documents. Prior work
on finding parallel data attempts to reason about
the probability that pairs of documents (x, y) are
parallel. In contrast, we only consider one doc-
ument at a time, defined by x = x1, x2, . . . , xn,
and consisting of n tokens, and need to deter-
mine whether there is parallel data in x, and if
so, where are the parallel segments and their lan-
guages. For simplicity, we assume that there are
at most 2 continuous segments that are parallel.
As representation for the parallel seg-
ments within the document, we use the tuple
([p, q], l, [u, v], r, a). The word indexes [p, q] and
[u, v] are used to identify the left segment (from
p to q) and right segment (from u to v), which
are parallel. We shall refer [p, q] and [u, v] as the
spans of the left and right segments. To avoid
overlaps, we set the constraint p ? q < u ? v.
Then, we use l and r to identify the language of
the left and right segments, respectively. Finally, a
represents the word alignment between the words
in the left and the right segments.
The main problem we address is to find the
parallel data when the boundaries of the parallel
segments are not defined explicitly. If we knew
the indexes [p, q] and [u, v], we could simply run
a language detector for these segments to find l
and r. Then, we would use an word alignment
model (Brown et al, 1993; Vogel et al, 1996),
with source s = xp, . . . , xq, target t = xu, . . . , xv
and lexical table ?l,r to calculate the Viterbi align-
ment a. Finally, from the probability of the word
alignments, we can determine whether the seg-
ments are parallel.
Thus, our model will attempt to find the opti-
mal values for the segments [p, q][u, v], languages
l, r and word alignments a jointly. However, there
are two problems with this approach. Firstly, word
alignment models generally attribute higher prob-
abilities to smaller segments, since these are the
result of a smaller product chain of probabilities.
In fact, because our model can freely choose the
segments to align, choosing only one word as the
left segment that is well aligned to a word in the
right segment would be the best choice. This
is obviously not our goal, since we would not
obtain any useful sentence pairs. Secondly, in-
ference must be performed over the combination
of all latent variables, which is intractable using
177
a brute force algorithm. We shall describe our
model to solve the first problem in 3.1 and our
dynamic programming approach to make the in-
ference tractable in 3.2.
3.1 Model
We propose a simple (non-probabilistic) three-
factor model that models the spans of the parallel
segments, their languages, and word alignments
jointly. This model is defined as follows:
S([u, v], r, [p, q],l, a | x) =
S?S ([p, q], [u, v] | x)?
S?L(l, r | [p, q], [u, v], x)?
S?T (a | [p, q], l, [u, v], r, x)
Each of the components is weighted by the pa-
rameters ?, ? and ?. We set these values empiri-
cally ? = 0.3, ? = 0.3 and ? = 0.4, and leave the
optimization of these parameters as future work.
We discuss the components of this model in turn.
Span score SS . We define the score of hypothe-
sized pair of spans [p, q], [u, v] as:
SS([p, q], [u, v] | x) =
(q ? p+ 1) + (v ? u+ 1)?
0<p??q?<u??v??n(q? ? p? + 1) + (v? ? u? + 1)
?
?([p, q], [u, v], x)
The first factor is a distribution over all spans that
assigns higher probability to segmentations that
cover more words in the document. It is highest
for segmentations that cover all the words in the
document (this is desirable since there are many
sentence pairs that can be extracted but we want
to find the largest sentence pair in the document).
The function ? takes on values of 0 or 1 depend-
ing on whether certain constraints are violated,
these include: parenthetical constraints that en-
force that spans must not break text within par-
enthetical characters and language constraints that
ensure that we do break a sequence of Mandarin
characters, Arabic words or Latin words.
Language score SL. The language score
SL(l, r | [p, q], [u, v], x) indicates whether the lan-
guage labels l, r are appropriate to the document
contents:
SL(l, r | [p, q], [u, v], x) =?q
i=p L(l, xi) +
?v
i=u L(r, xi)
n
where L(l, x) is a language detection function that
yields 1 if the word xi is in language l, and 0 oth-
erwise. We build the function simply by consid-
ering all words that are composed of Latin char-
acters as English, Arabic characters as Arabic and
Han characters as Mandarin. This approach is not
perfect, but it is simple and works reasonably well
for our purposes.
Translation score ST . The translation score
ST (a | [p, q], l, [u, v], r) indicates whether [p, q]
is a reasonable translation of [u, v] with the align-
ment a. We rely on IBM Model 1 probabilities for
this score:
ST (a | [p, q], l, [u, v], r, x) =
1
(q ? p+ 1)v?u+2
v?
i=u
PM1(xi | xai).
The lexical tables PM1 for the various language
pairs are trained a priori using available parallel
corpora. While IBM Model 1 produces worse
alignments than other models, in our problem, we
need to efficiently consider all possible spans, lan-
guage pairs and word alignments, which makes
the problem intractable. We will show that dy-
namic programing can be used to make this prob-
lem tractable, using Model 1. Furthermore, IBM
Model 1 has shown good performance for sen-
tence alignment systems previously (Xu et al,
2005; Braune and Fraser, 2010).
3.2 Inference
Our goal is to find the spans, language pair and
alignments such that:
argmax
[p,q],l,[u,v],r,a
S([p, q], l, [u, v], r, a | x) (1)
A high score indicates that the predicted bispan is
likely to correspond to a valid parallel span, so we
set a constant threshold ? to determine whether a
document has parallel data, i.e., the value of z:
z? = max
[u,v],r,[p,q],l,a
S([u, v], r, [p, q], l, a | x) > ?
Naively maximizing Eq. 1 would require
O(|x|6) operations, which is too inefficient to be
practical on large datasets. To process millions
of documents, this process would need to be op-
timized.
The main bottleneck of the naive algorithm is
finding new Viterbi Model 1 word alignments ev-
ery time we change the spans. Thus, we propose
178
an iterative approach to compute the Viterbi word
alignments for IBM Model 1 using dynamic pro-
gramming.
Dynamic programming search. The insight we
use to improve the runtime is that the Viterbi
word alignment of a bispan can be reused to cal-
culate the Viterbi word alignments of larger bis-
pans. The algorithm operates on a 4-dimensional
chart of bispans. It starts with the minimal valid
span (i.e., [0, 0], [1, 1]) and progressively builds
larger spans from smaller ones. Let Ap,q,u,v rep-
resent the Viterbi alignment (under ST ) of the bis-
pan [p, q], [u, v]. The algorithm uses the follow-
ing recursions defined in terms of four operations
?{+v,+u,+p,+q} that manipulate a single dimension
of the bispan to construct larger spans:
? Ap,q,u,v+1 = ?+v(Ap,q,u,v) adds one token to
the end of the right span with index v + 1 and
find the viterbi alignment for that token. This
requires iterating over all the tokens in the left
span, [p, q] and possibly updating their align-
ments. See Fig. 1 for an illustration.
? Ap,q,u+1,v = ?+u(Ap,q,u,v) removes the first to-
ken of the right span with index u, so we only
need to remove the alignment from u, which can
be done in time O(1).
? Ap,q+1,u,v = ?+q(Ap,q,u,v) adds one token to
the end of the left span with index q + 1, we
need to check for each word in the right span, if
aligning to the word in index q+1 yields a better
translation probability. This update requires n?
q + 1 operations.
? Ap+1,q,u,v = ?+p(Ap,q,u,v) removes the first
token of the left span with index p. After re-
moving the token, we need to find new align-
ments for all tokens that were aligned to p.
Thus, the number of operations for this update
is K ? (q ? p + 1), where K is the number of
words that were aligned to p. In the best case, no
words are aligned to the token in p, and we can
simply remove it. In the worst case, if all target
words were aligned to p, this update will result
in the recalculation of all Viterbi Alignments.
The algorithm proceeds until all valid cells have
been computed. One important aspect is that the
update functions differ in complexity, so the se-
quence of updates we apply will impact the per-
formance of the system. Most spans are reach-
able using any of the four update functions. For
instance, the span A2,3,4,5 can be reached us-
ing ?+v(A2,3,4,4), ?+u(A2,3,3,5), ?+q(A2,2,4,5) or
?+p(A1,3,4,5). However, we want to use ?+u
a b - A B
a
b
-
A
B
a b - A B
p
qu v
p
qu v?+v
Figure 1: Illustration of the ?+v operator. The
light gray boxes show the parallel span and the
dark boxes show the span?s Viterbi alignment.
In this example, the parallel message contains a
?translation? of a b to A B.
whenever possible, since it only requires one op-
eration, although that is not always possible. For
instance, the state A2,2,2,4 cannot be reached us-
ing ?+u, since the state A2,2,1,4 is not valid, be-
cause the spans overlap. If this happens, incre-
mentally more expensive updates need to be used,
such as ?+v, then ?+q, which are in the same order
of complexity. Finally, we want to minimize the
use of ?+p, which is quadratic in the worst case.
Thus, we use the following recursive formulation
that guarantees the optimal outcome:
Ap,q,u,v =
?
????
????
?+u(Ap,q,u?1,v) if u > q + 1
?+v(Ap,q,u,v?1) else if v > q + 1
?+p(Ap?1,q,u,v) else if q = p+ 1
?+q(Ap,q?1,u,v) otherwise
This transition function applies the cheapest
possible update to reach state Ap,q,u,v.
Complexity analysis. We can see that ?+u
is only needed in the following the cases
[0, 1][2, 2], [1, 2][3, 3], ? ? ? , [n ? 2, n ? 1][n, n].
Since, this update is quadratic in the worst
case, the complexity of this operations is
O(n3). The update ?+q, is applied to the cases
[?, 1][2, 2], [?, 2][3, 3], ? ? ? , [?, n?1], [n, n], where
? denotes any number within the span constraints
but not present in previous updates. Since, the
update is linear and we need to iterate through
all tokens twice, this update takes O(n3) opera-
tions. The update ?+v is applied for the cases
[?, 1][2, ?], [?, 2][3, ?], ? ? ? , [?, n? 1], [n, ?]. Thus,
with three degrees of freedom and a linear update,
it runs in O(n4) time. Finally, update ?+u runs in
constant time, but is run for all remaining cases,
which constitute O(n4) space. By summing the
179
executions of all updates, we observe that the or-
der of magnitude of our exact inference process is
O(n4). Note that for exact inference, it is not pos-
sible to get a lower order of magnitude, since we
need to at least iterate through all possible span
values once, which takes O(n4) time.
4 Parallel Data Extraction
We will now describe our method to extract par-
allel data from Microblogs. The target domains
in this work are Twitter and Sina Weibo, and
the main language pair is Chinese-English. Fur-
thermore, we also run the system for the Arabic-
English language pair using the Twitter data.
For the Twitter domain, we use a previously
crawled dataset from the years 2008 to 2013,
where one million tweets are crawled every day.
In total, we processed 1.6 billion tweets.
Regarding Sina Weibo, we built a crawler that
continuously collects tweets from Weibo. We start
from one seed user and collect his posts, and then
we find the users he follows that we have not con-
sidered, and repeat. Due to the rate limiting es-
tablished by the Weibo API1, we are restricted in
terms of number of requests every hour, which
greatly limits the amount of messages we can col-
lect. Furthermore, each request can only fetch up
to 100 posts from a user, and subsequent pages of
100 posts require additional API calls. Thus, to
optimize the number of parallel posts we can col-
lect per request, we only crawl all messages from
users that have at least 10 parallel tweets in their
first 100 posts. The number of parallel messages
is estimated by running our alignment model, and
checking if ? > ?, where ? was set empirically
initially, and optimized after obtaining annotated
data, which will be detailed in 5.1. Using this
process, we crawled 65 million tweets from Sina
Weibo within 4 months.
In both cases, we first filter the collection of
tweets for messages containing at least one trigram
in each language of the target language pair, deter-
mined by their Unicode ranges. This means that
for the Chinese-English language pair, we only
keep tweets with more than 3 Mandarin charac-
ters and 3 latin words. Furthermore, based on the
work in (Jelh et al, 2012), if a tweet A is iden-
tified as a retweet, meaning that it references an-
other tweetB, we also consider the hypothesis that
these tweets may be mutual translations. Thus, if
A and B contain trigrams in different languages,
1http://open.weibo.com/wiki/API??/en
these are also considered for the extraction of par-
allel data. This is done by concatenating tweets A
and B, and adding the constraint that [p, q] must
be within A and [u, v] must be within B. Finally,
identical duplicate tweets are removed.
After filtering, we obtained 1124k ZH-EN
tweets from Sina Weibo, 868k ZH-EN and 136k
AR-EN tweets from Twitter. These language pairs
are not definite, since we simply check if there is
a trigram in each language.
Finally, we run our alignment model described
in section 3, and obtain the parallel segments and
their scores, which measure how likely those seg-
ments are parallel. In this process, lexical tables
for EN-ZH language pair used by Model 1 were
built using the FBIS dataset (LDC2003E14) for
both directions, a corpus of 300K sentence pairs
from the news domain. Likewise, for the EN-
AR language pair, we use a fraction of the NIST
dataset, by removing the data originated from UN,
which leads to approximately 1M sentence pairs.
5 Experiments
We evaluate our method in two ways. First, intrin-
sically, by observing how well our method identi-
fies tweets containing parallel data, the language
pair and what their spans are. Second, extrinsi-
cally, by looking at how well the data improves
a translation task. This methodology is similar to
that of Smith et al (2010).
5.1 Parallel Data Extraction
Data. Our method needs to determine if a given
tweet contains parallel data, and if so, what is
the language pair of the data, and what segments
are parallel. Thus, we had a native Mandarin
speaker, also fluent in English, to annotate 2000
tweets sampled from crawled Weibo tweets. One
important question of answer is what portion of
the Microblogs contains parallel data. Thus, we
also use the random sample Twitter and annotated
1200 samples, identifying whether each sample
contains parallel data, for the EN-ZH and AR-EN
filtered tweets.
Metrics. To test the accuracy of the score S, we
ordered all 2000 samples by score. Then, we cal-
culate the precision, recall and accuracy at increas-
ing intervals of 10% of the top samples. We count
as a true positive (tp) if we correctly identify a par-
allel tweet, and as a false positive (fp) spuriously
detect a parallel tweet. Finally, a true negative (tn)
occurs when we correctly detect a non-parallel
180
tweet, and a false negative (fn) if we miss a par-
allel tweet. Then, we set the precision as tptp+fp ,
recall as tptp+fn and accuracy as tp+tntp+fp+tn+fn . Forlanguage identification, we calculate the accuracy
based on the number of instances that were iden-
tified with the correct language pair. Finally, to
evaluate the segment alignment, we use the Word
Error Rate (WER) metric, without substitutions,
where we compare the left and right spans of our
system and the respective spans of the reference.
We count an insertion error (I) for each word in
our system?s spans that is not present in the refer-
ence span and a deletion error (D) for each word
in the reference span that is not present in our sys-
tem?s spans. Thus, we set WER = D+IN , where
N is the number of tokens in the tweet. To com-
pute this score for the whole test set, we compute
the average of the WER for each sample.
Results. The precision, recall and accuracy
curves are shown in Figure 2. The quality of the
parallel sentence detection did not vary signifi-
cantly with different setups, so we will only show
the results for the best setup, which is the baseline
model with span constraints.
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
0.9	 ?
1	 ?
10%	 ? 20%	 ? 30%	 ? 40%	 ? 50%	 ? 60%	 ? 70%	 ? 80%	 ? 90%	 ? 100%	 ?
Precision	 ?
Recall	 ?
Accuracy	 ?
Figure 2: Precision, recall and accuracy curves
for parallel data detection. The y-axis denotes the
scores for each metric, and the x-axis denotes the
percentage of the highest scoring sentence pairs
that are kept.
From the precision and recall curves, we ob-
serve that most of the parallel data can be found
at the top 30% of the filtered tweets, where 5 in 6
tweets are detected correctly as parallel, and only
1 in every 6 parallel sentences is lost. We will de-
note the score threshold at this point as ?, which is
a good threshold to estimate on whether the tweet
is parallel. However, this parameter can be tuned
for precision or recall. We also see that in total,
30% of the filtered tweets are parallel. If we gen-
eralize this ratio for the complete set with 1124k
tweets, we can expect approximately 337k paral-
lel sentences. Finally, since 65 million tweets were
extracted to generate the 337k tweets, we estimate
that approximately 1 parallel tweet can be found
for every 200 tweets we process using our tar-
geted approach. On the other hand, from the 1200
tweets from Twitter, we found that 27 had parallel
data in the ZH-EN pair, if we extrapolate for the
whole 868k filtered tweets, we expect that we can
find 19530. 19530 parallel sentences from 1.6 bil-
lion tweets crawled randomly, represents 0.001%
of the total corpora. For AR-EN, a similar re-
sult was obtained where we expect 12407 tweets
out of the 1.6 billion to be parallel. This shows
that targeted approaches can substantially reduce
the crawling effort required to find parallel tweets.
Still, considering that billions of tweets are posted
daily, this is a substantial source of parallel data.
The remainder of the tests will be performed on
the Weibo dataset, which contains more parallel
data. Tests on the Twitter data will be conducted
as future work, when we process Twitter data on a
larger scale to obtain more parallel sentences.
For the language identification task, we had an
accuracy of 99.9%, since distinguishing English
and Mandarin is trivial. The small percentage of
errors originated from other latin languages (Ex:
French) due to our naive language detector.
As for the segment alignment task. Our base-
line system with no constraints obtains a WER of
12.86%, and this can be improved to 11.66% by
adding constraints to possible spans. This shows
that, on average, approximately 1 in 9 words on
the parallel segments is incorrect. However, trans-
lation models are generally robust to such kinds of
errors and can learn good translations even in the
presence of imperfect sentence pairs.
Among the 578 tweets that are parallel, 496
were extracted within the same tweet and 82 were
extracted from retweets. Thus, we see that the ma-
jority of the parallel data comes from within the
same tweet.
Topic analysis. To give an intuition about the
contents of the parallel data we found, we looked
at the distribution over topics of the parallel
dataset inferred by LDA (Blei et al, 2003). Thus,
we grouped the Weibo filtered tweets by users,
and ran LDA over the predicted English segments,
with 12 topics. The 7 most interpretable topics are
shown in Table 1. We see that the data contains a
181
# Topic Most probable words in topic
1 (Dating) love time girl live mv back word night rt wanna
2 (Entertainment) news video follow pong image text great day today fans
3 (Music) cr day tour cn url amazon music full concert alive
4 (Religion) man god good love life heart would give make lord
5 (Nightlife) cn url beijing shanqi party adj club dj beijiner vt
6 (Chinese News) china chinese year people world beijing years passion country government
7 (Fashion) street fashion fall style photo men model vogue spring magazine
Table 1: Most probable words inferred using LDA in several topics from the parallel data extracted from
Weibo. Topic labels (in parentheses) were assigned manually for illustration purposes.
variety of topics, both formal (Chinese news, reli-
gion) and informal (entertainment, music).
Example sentence pairs. To gain some perspec-
tive on the type of sentence pairs we are extract-
ing, we will illustrate some sentence pairs we
crawled and aligned automatically. Table 2 con-
tains 5 English-Mandarin and 4 English-Arabic
sentence pairs that were extracted automatically.
These were chosen, since they contain some as-
pects that are characteristic of the text present in
Microblogs and Social Media. These are:
? Abbreviations - In most sentence pairs exam-
ples, we can witness the use of abbreviated
forms of English words, such as wanna, TMI,
4 and imma. These can be normalized as want
to, too much information, for and I am going
to, respectively. In sentence 5, we observe that
this phenomena also occurs in Mandarin. We
find that TMD is a popular way to write???
whose Pinyin rendering is ta? ma? de. The mean-
ing of this expression depends on the context it
is used, and can convey a similar connotation
as adding the intensifier the hell to an English
sentence.
? Jargon - Another common phenomena is the
appearance of words that are only used in sub-
communities. For instance, in sentence pair 4,
we the jargon word cday is used, which is a col-
loquial variant for birthday.
? Emoticons - In sentence 8, we observe the pres-
ence of the emoticon :), which is frequently
used in this media. We found that emoticons are
either translated as they are or simply removed,
in most cases.
? Syntax errors - In the domain of microblogs, it
is also common that users do not write strictly
syntactic sentences, for instance, in sentence
pair 7, the sentence onni this gift only 4 u, is
clearly not syntactically correct. Firstly, onni
is a named entity, yet it is not capitalized. Sec-
ondly, a comma should follow onni. Thirdly, the
verb is should be used after gift. Having exam-
ples of these sentences in the training set, with
common mistakes (intentional or not), might
become a key factor in training MT systems that
can be robust to such errors.
? Dialects - We can observe a much broader range
of dialects in our data, since there are no di-
alect standards in microblogs. For instance, in
sentence pair 6, we observe an arabic word (in
bold) used in the spoken Arabic dialect used in
some countries along the shores of the Persian
Gulf, which means means the next. In standard
Arabic, a significantly different form is used.
We can also see in sentence pair 9 that our
aligner does not alway make the correct choice
when determining spans. In this case, the segment
RT @MARYAMALKHAWAJA: was included in the
English segment spuriously, since it does not cor-
respond to anything in the Arabic counterpart.
5.2 Machine Translation Experiments
We report on machine translation experiments us-
ing our harvested data in two domains: edited
news and microblogs.
News translation. For the news test, we cre-
ated a new test set from a crawl of the Chinese-
English documents on the Project Syndicate web-
site2, which contains news commentary articles.
We chose to use this data set, rather than more
standard NIST test sets to ensure that we had re-
cent documents in the test set (the most recent
NIST test sets contain documents published in
2007, well before our microblog data was created).
We extracted 1386 parallel sentences for tuning
and another 1386 sentences for testing, from the
manually aligned segments. For this test set, we
used 8 million sentences from the full NIST par-
allel dataset as the language model training data.
We shall call this test set Syndicate.
2http://www.project-syndicate.org/
182
ENGLISH MANDARIN
1 i wanna live in a wes anderson world ??????Wes Anderson????
2 Chicken soup, corn never truly digests. TMI. ??????????????????.??
3 To DanielVeuleman yea iknw imma work on that ?DanielVeuleman?????????????????
4 msg 4 Warren G his cday is today 1 yr older. ????Warren G????????????????
5 Where the hell have you been all these years? ????TMD????
ENGLISH ARABIC
6 It?s gonna be a warm week! Qk ?


AJ
? @ ? ?J.?B@
7 onni this gift only 4 u ?? ?? 	? ?K
Y?? @ ? 	Y? ?

	
G?

@
8 sunset in aqaba :) (: ?J. ???@ ?

	
? ?? ??@ H. ?Q
	
?
9 RT @MARYAMALKHAWAJA: there is a call @Y 	? ??A 	J? ?Y? ?


	
? H@Q?A 	??? Z @Y 	K ?A 	J?for widespread protests in #bahrain tmrw
Table 2: Examples of English-Mandarin and English-Arabic sentence pairs. The English-Mandarin
sentences were extracted from Sina Weibo and the English-Arabic sentences were extracted from Twitter.
Some messages have been shorted to fit into the table. Some interesting aspects of these sentence pairs
are marked in bold.
Microblog translation. To carry out the mi-
croblog translation experiments, we need a high
quality parallel test set. Since we are not aware
of such a test set, we created one by manually se-
lecting parallel messages from Weibo. Our proce-
dure was as follows. We selected 2000 candidate
Weibo posts from users who have a high num-
ber of parallel tweets according to our automatic
method (at least 2 in every 5 tweets). To these, we
added another 2000 messages from our targeted
Weibo crawl, but these had no requirement on the
proportion of parallel tweets they had produced.
We identified 2374 parallel segments, of which we
used 1187 for development and 1187 for testing.
We refer to this test set as Weibo.3
Obviously, we removed the development and
test sets from our training data. Furthermore, to
ensure that our training data was not too similar to
the test set in the Weibo translation task, we fil-
tered the training data to remove near duplicates
by computing edit distance between each paral-
lel sentence in the heldout set and each training
instance. If either the source or the target sides
of the a training instance had an edit distance of
less than 10%, we removed it.4 As for the lan-
guage models, we collected a further 10M tweets
from Twitter for the English language model and
another 10M tweets from Weibo for the Chinese
language model.
3We acknowledge that self-translated messages are prob-
ably not a typically representative sample of all microblog
messages. However, we do not have the resources to produce
a carefully curated test set with a more broadly representative
distribution. Still, we believe these results are informative as
long as this is kept in mind.
4Approximately 150,000 training instances removed.
Syndicate Weibo
ZH-EN EN-ZH ZH-EN EN-ZH
FBIS 9.4 18.6 10.4 12.3
NIST 11.5 21.2 11.4 13.9
Weibo 8.75 15.9 15.7 17.2
FBIS+Weibo 11.7 19.2 16.5 17.8
NIST+Weibo 13.3 21.5 16.9 17.9
Table 3: BLEU scores for different datasets in dif-
ferent translation directions (left to right), broken
with different training corpora (top to bottom).
Baselines. We report results on these test sets us-
ing different training data. First, we use the FBIS
dataset which contains 300K high quality sentence
pairs, mostly in the broadcast news domain. Sec-
ond, we use the full 2012 NIST Chinese-English
dataset (approximately 8M sentence pairs, includ-
ing FBIS). Finally, we use our crawled data (re-
ferred as Weibo) by itself and also combined with
the two previous training sets.
Setup. We use the Moses phrase-based MT sys-
tem with standard features (Koehn et al, 2003).
For reordering, we use the MSD reordering
model (Axelrod et al, 2005). As the language
model, we use a 5-gram model with Kneser-
Ney smoothing. The weights were tuned using
MERT (Och, 2003). Results are presented with
BLEU-4 (Papineni et al, 2002).
Results. The BLEU scores for the different par-
allel corpora are shown in Table 3 and the top 10
out-of-vocabulary (OOV) words for each dataset
are shown in Table 4. We observe that for the
Syndicate test set, the NIST and FBIS datasets
183
Syndicate (test) Weibo (test)
FBIS NIST Weibo FBIS NIST Weibo
obama (83) barack (59) democracies (15) 2012 (24) showstudio (9) submissions (4)
barack (59) namo (6) imbalances (13) alanis (13) crue (9) ivillage (4)
princeton (40) mitt (6) mahmoud (12) crue (9) overexposed (8) scola (3)
ecb (8) guant (6) millennium (9) showstudio (9) tweetmeian (5) rbst (3)
bernanke (8) fairtrade (6) regimes (8) overexposed (8) tvd (5) curitiba (3)
romney (7) hollande (5) wolfowitz (7) itunes (8) iheartradio (5) zeman (2)
gaddafi (7) wikileaks (4) revolutions (7) havoc (8) xoxo (4) @yaptv (2)
merkel (7) wilders (3) qaddafi (7) sammy (6) snoop (4) witnessing (2)
fats (7) rant (3) geopolitical (7) obama (6) shinoda (4) whoohooo (2)
dialogue (7) esm (3) genome (7) lol (6) scrapbook (4) wbr (2)
Table 4: The most frequent out-of-vocabulary (OOV) words and their counts for the two English-source
test sets with three different training sets.
perform better than our extracted parallel data.
This is to be expected, since our dataset was ex-
tracted from an extremely different domain. How-
ever, by combining the Weibo parallel data with
this standard data, improvements in BLEU are ob-
tained. Error analysis indicates that one major fac-
tor is that names from current events, such as Rom-
ney and Wikileaks do not occur in the older NIST
and FBIS datasets, but they are represented in the
Weibo dataset. Furthermore, we also note that the
system built on the Weibo dataset does not per-
form substantially worse than the one trained on
the FBIS dataset, a further indication that harvest-
ing parallel microblog data yields a diverse collec-
tion of translated material.
For the Weibo test set, a significant improve-
ment over the news datasets can be achieved us-
ing our crawled parallel data. Once again newer
terms, such as iTunes, are one of the reasons older
datasets perform less well. However, in this case,
the top OOV words of the news domain datasets
are not the most accurate representation of cov-
erage problems in this domain. This is because
many frequent words in microblogs, e.g., nonstan-
dard abbreviations, like u and 4 are found in the
news domain as words, albeit with different mean-
ings. Thus, the OOV table gives an incomplete
picture of the translation problems when using
the news domain corpora to translate microblogs.
Also, some structural errors occur when training
with the news domain datasets, one such example
is shown in table 5, where the character ? is in-
correctly translated to said. This occurs because
this type of constructions is infrequent in news
datasets. Furthermore, we can see that compound
expressions, such as the translation from ???
? to party time are also learned.
Finally, we observe that combining the datasets
Source ?sam farrar??????
Reference to sam farrar , party time
FBIS farrar to sam said , in time
NIST to sam farrar said , the moment
WEIBO to sam farrar , party time
Table 5: Translation Examples using different
training sets.
yields another gain over individual datasets, both
in the Syndicate and in the Weibo test sets.
6 Conclusion
We presented a framework to crawl parallel data
from microblogs. We find parallel data from sin-
gle posts, with translations of the same sentence
in two languages. We show that a considerable
amount of parallel sentence pairs can be crawled
from microblogs and these can be used to improve
Machine Translation by updating our translation
tables with translations of newer terms. Further-
more, the in-domain data can substantially im-
prove the translation quality on microblog data.
The resources described in this paper and fur-
ther developments are available to the general pub-
lic at http://www.cs.cmu.edu/?lingwang/utopia.
Acknowledgements
The PhD thesis of Wang Ling is supported by FCT
grant SFRH/BD/51157/2010. The authors wish
to express their gratitude to thank William Cohen,
Noah Smith, Waleed Ammar, and the anonymous
reviewers for their insight and comments. We are
also extremely grateful to Brendan O?Connor for
providing the Twitter data and to Philipp Koehn
and Barry Haddow for providing the Project Syn-
dicate data.
184
References
[Axelrod et al2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT.
[Blei et al2003] David M. Blei, Andrew Y. Ng, and
Michael I. Jordan. 2003. Latent dirichlet alocation.
J. Mach. Learn. Res., 3:993?1022, March.
[Braune and Fraser2010] Fabienne Braune and Alexan-
der Fraser. 2010. Improved unsupervised sentence
alignment for symmetrical and asymmetrical paral-
lel corpora. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 81?89, Stroudsburg, PA, USA.
Association for Computational Linguistics.
[Brown et al1993] Peter F. Brown, Vincent J. Della
Pietra, Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Comput. Lin-
guist., 19:263?311, June.
[Fukushima et al2006] Ken?ichi Fukushima, Kenjiro
Taura, and Takashi Chikayama. 2006. A fast and
accurate method for detecting English-Japanese par-
allel texts. In Proceedings of the Workshop on Mul-
tilingual Language Resources and Interoperability,
pages 60?67, Sydney, Australia, July. Association
for Computational Linguistics.
[Gimpel et al2011] Kevin Gimpel, Nathan Schneider,
Brendan O?Connor, Dipanjan Das, Daniel Mills, Ja-
cob Eisenstein, Michael Heilman, Dani Yogatama,
Jeffrey Flanigan, and Noah A. Smith. 2011. Part-
of-speech tagging for twitter: annotation, features,
and experiments. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies: short
papers - Volume 2, HLT ?11, pages 42?47, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
[Jelh et al2012] Laura Jelh, Felix Hiebel, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montre?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
[Koehn2005] Philipp Koehn. 2005. Europarl: A Par-
allel Corpus for Statistical Machine Translation. In
Proceedings of the tenth Machine Translation Sum-
mit, pages 79?86, Phuket, Thailand. AAMT, AAMT.
[Li and Liu2008] Bo Li and Juan Liu. 2008. Mining
Chinese-English parallel corpora from the web. In
Proceedings of the 3rd International Joint Confer-
ence on Natural Language Processing (IJCNLP).
[Lin et al2008] Dekang Lin, Shaojun Zhao, Benjamin
Van Durme, and Marius Pas?ca. 2008. Mining par-
enthetical translations from the web by word align-
ment. In Proceedings of ACL-08: HLT, pages 994?
1002, Columbus, Ohio, June. Association for Com-
putational Linguistics.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montre?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Smith et al2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Proceedings of the 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
[Ture and Lin2012] Ferhan Ture and Jimmy Lin. 2012.
Why not grab a free lunch? mining large corpora for
parallel sentences to improve translation modeling.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 626?630, Montre?al, Canada, June. Associa-
tion for Computational Linguistics.
[Uszkoreit et al2010] Jakob Uszkoreit, Jay Ponte,
Ashok C. Popat, and Moshe Dubiner. 2010. Large
scale parallel document mining for machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 1101?
1109.
[Vogel et al1996] Stephan Vogel, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based word align-
ment in statistical translation. In Proceedings of the
16th conference on Computational linguistics - Vol-
ume 2, COLING ?96, pages 836?841, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
[Xu et al2001] Jinxi Xu, Ralph Weischedel, and Chanh
Nguyen. 2001. Evaluating a probabilistic model
185
for cross-lingual information retrieval. In Proceed-
ings of the 24th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?01, pages 105?110, New
York, NY, USA. ACM.
[Xu et al2005] Jia Xu, Richard Zens, and Hermann
Ney. 2005. Sentence segmentation using ibm word
alignment model 1. In Proceedings of EAMT 2005
(10th Annual Conference of the European Associa-
tion for Machine Translation, pages 280?287.
[Zbib et al2012] Rabih Zbib, Erika Malchiodi, Jacob
Devlin, David Stallard, Spyros Matsoukas, Richard
Schwarz, John Makhoul, Omar F. Zaidan, and Chris
Callison-Burch. 2012. Machine translation of Ara-
bic dialects. In Proceedings of the 2012 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies.
186
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 777?783,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Information Theoretic Approach to Bilingual Word Clustering
Manaal Faruqui and Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mfaruqui, cdyer}@cs.cmu.edu
Abstract
We present an information theoretic objec-
tive for bilingual word clustering that in-
corporates both monolingual distributional
evidence as well as cross-lingual evidence
from parallel corpora to learn high qual-
ity word clusters jointly in any number of
languages. The monolingual component
of our objective is the average mutual in-
formation of clusters of adjacent words in
each language, while the bilingual com-
ponent is the average mutual information
of the aligned clusters. To evaluate our
method, we use the word clusters in an
NER system and demonstrate a statisti-
cally significant improvement in F1 score
when using bilingual word clusters instead
of monolingual clusters.
1 Introduction
A word cluster is a group of words which ideally
captures syntactic, semantic, and distributional
regularities among the words belonging to the
group. Word clustering is widely used to reduce
the number of parameters in statistical models
which leads to improved generalization (Brown et
al., 1992; Kneser and Ney, 1993; Clark, 2003; Koo
et al, 2008; Turian et al, 2010), and multilingual
clustering has been proposed as a means to im-
prove modeling of translational correspondences
and to facilitate projection of linguistic resource
across languages (Och, 1999; Ta?ckstro?m et al,
2012). In this paper, we argue that generally more
informative clusters can be learned when evidence
from multiple languages is considered while cre-
ating the clusters.
We propose a novel bilingual word clustering
objective (?2). The first term deals with each
language independently and ensures that the data
is well-explained by the clustering in a sequence
model (?2.1). The second term ensures that the
cluster alignments induced by a word alignment
have high mutual information across languages
(?2.2). Since the objective consists of terms rep-
resenting the entropy monolingual data (for each
language) and parallel bilingual data, it is partic-
ularly attractive for the usual situation in which
there is much more monolingual data available
than parallel data. Because of its similarity to the
variation of information metric (Meila?, 2003), we
call this bilingual term in the objective the aligned
variation of information.
2 Word Clustering
A word clustering C is a partition of a vocabulary
? = {x1, x2, . . . , x|?|} into K disjoint subsets,
C1, C2, . . . , CK . That is, C = {C1, C2, . . . , CK};
Ci ? Cj = ? for all i 6= j and ?Kk=1Ck = ?.
2.1 Monolingual objective
We use the average surprisal in a probabilistic se-
quence model to define the monolingual clustering
objective. Let ci denote the word class of word
wi. Our objective assumes that the probability of
a word sequence w = ?w1, w2, . . . , wM ? is
p(w) =
M?
i=1
p(ci | ci?1)? p(wi | ci), (2.1)
where c0 is a special start symbol. The term p(ci |
ci?1) is the probability of class ci following class
ci?1, and p(wi | ci) is the probability of class ci
emitting word wi. Using the MLE esitmates after
taking the negative logarithm, this term reduces to
777
the following as shown in (Brown et al, 1992):
H(C;w) = 2
K?
k=1
#(Ck)
M log
#(Ck)
M
?
?
i
?
j 6=i
#(Ci, Cj)
M log
#(Ci, Cj)
M
where #(Ck) is the count of Ck in the corpus w
under the clustering C, #(Ci, Cj) is the count of
the number of times that cluster Ci precedes Cj
and M is the size of the corpus. Using the mono-
lingual objective to cluster, we solve the following
search problem:
C? = arg min
C
H(C;w). (2.2)
2.2 Bilingual objective
Now let us suppose we have a second lan-
guage with vocabulary ? = {y1, y2, . . . , y|?|},
which is clustered into K disjoint subsets D =
{D1, D2, . . . , DK}, and a corpus of text in the
second language, v = ?v1, v2, . . . , vN ?. Obvi-
ously we can cluster both languages using the
monolingual objective above:
C?, D? = arg min
C,D
H(C;w) +H(D; v).
This joint minimization for the clusterings for both
languages clearly has no benefit since the two
terms of the objective are independent. We must
alter the object by further assuming that we have
a priori beliefs that some of the words in w and v
have the same meaning.
To encode this belief, we introduce the notion
of a weighted vocabulary alignment A, which is
a function on pairs of words in vocabularies ? and
? to a value greater than or equal to 0, i.e., A :
?? ? 7? R?0. For concreteness, A(x, y) will be
the number of times that x is aligned to y in a word
aligned parallel corpus. By abuse of notation, we
write marginal weights A(x) = ?y??A(x, y)
and A(y) = ?x??A(x, y). We also define the
set marginals A(C,D) = ?x?C
?
y?DA(x, y).Using this weighted vocabulary alignment, we
state an objective that encourages clusterings to
have high average mutual information when align-
ment links are followed; that is, on average how
much information does knowing the cluster of a
word x ? ? impart about the clustering of y ? ?,
and vice-versa?
C DC
Figure 1: Factor graphs of the monolingual (left)
& proposed bilingual clustering problem (right).
We call this quantity the aligned variation of
information (AVI).
AVI(C,D;A) =
EA(x,y) [? log p(cx | dy)? log p(dy | cx)]
Writing out the expectation and gathering terms,
we obtain
AVI(C,D;A) = ?
?
x??
?
y??
A(x, y)
A(?, ?) ?
[
2 log A(C,D)A(?, ?) ? log p(C)? log p(D)
]
,
where it is assumed that 0 log x = 0.
Our bilingual clustering objective can therefore
be stated as the following search problem over a
linear combination of the monolingual and bilin-
gual objectives:
arg min
C,D
monolingual? ?? ?
H(C;w) +H(D; v) +
??bilingual? ?? ?
?AVI(C,D) .
(2.3)
Understanding AVI. Intuitively, we can imag-
ine sampling a random alignment from the distri-
bution obtained by normalizing A(?, ?). AVI gives
us a measure of how much information do we ob-
tain, on average, from knowing the cluster in one
language about the clustering of a linked element
chosen at random proportional to A(x, ?) (or con-
ditioned the other way around). In the following
sections, we denote AVI(C,D;A) by AVI(C,D).
To further understand AVI, we remark that AVI re-
duces to the VI metric when the alignment maps
words to themselves in the same language. As a
proper metric, VI has a number of attractive prop-
erties, and these can be generalized to AVI (with-
out restriction on the alignment map), namely:
? Non-negativity: AVI(C,D) ? 0;
? Symmetry: AVI(C,D) = AVI(D,C);
? Triangle inequality:
AVI(C,D) + AVI(D,E) ? AVI(C,E);
778
? Identity of indiscernables:
AVI(C,D) = 0 iff C ? D.1
2.3 Example
Figure 2 provides an example illustrating the dif-
ference between the bilingual vs. monolingual
clustering objectives. We compare two different
clusterings of a two-sentence Arabic-English par-
allel corpus (the English half of the corpus con-
tains the same sentence, twice, while the Ara-
bic half has two variants with the same mean-
ing). While English has a relatively rigid SVO
word order, Arabic can alternate between the tradi-
tional VSO order and an more modern SVO order.
Since our monolingual clustering objective relies
exclusively on the distribution of clusters before
and after each token, flexible word order alterna-
tions like this can cause unintuitive results. To
further complicate matters, verbs can inflect dif-
ferently depending on whether their subject pre-
cedes or follows them (Haywood and Nahmad,
1999), so a monolingual model, which knows
nothing about morphology and may only rely
on distributional clues, has little chance of per-
forming well without help. This is indeed what
we observe in the monolingual objective opti-
mal solution (center), in which AwlAd (boys) and
yElbwn (play+PRES + 3PL) are grouped into a
single class, while yElb (play+PRES + 3SG) is in
its own class. However, the AVI term (which is of
course not included) has a value of 1.0, reflecting
the relatively disordered clustering relative to the
given alignment. On the right, we see the optimal
solution that includes the AVI term in the cluster-
ing objective. This has an AVI of 0, indicating that
knowing the clustering of any word is completely
informative about the words it is aligned to. By in-
cluding this term, a slightly worse monolingual so-
lution is chosen, but the clustering corresponds to
the reasonable intuition that words with the same
meaning (i.e., the two variants of to play) should
be clustered together.
2.4 Inference
Figure 1 shows the factor graph representation
of our clustering models. Finding the optimal
clustering under both the monolingual and bilin-
gual objectives is a computationally hard combi-
natorial optimization problem (Och, 1995). We
use a greedy hill-climbing word exchange algo-
rithm (Martin et al, 1995) to find a minimum
1C ? D iff ?i|{D(y)|?(x, y) ? A, C(x) = i}| = 1
value for our objective. We terminate the opti-
mization procedure when the number of words
exchanged at the end of one complete iteration
through both the languages is less than 0.1% of
the sum of vocabulary of the two languages and
at least five complete iterations have been com-
pleted.2 For every language the word clusters are
initialised in a round robin order according to the
token frequency.
3 Experiments
Evaluation of clustering is not a trivial problem.
One branch of work seeks to recast the problem
as the of part-of-speech (POS) induction and at-
tempts to match linguistic intuitions. However,
hard clusters are particularly useful for down-
stream tasks (Turian et al, 2010). We therefore
chose to focus our evaluation on the latter prob-
lem. For our evaluation, we use our word clusters
as an input to a named entity recognizer which
uses these clusters as a source of features. Our
evaluation task is the German corpus with NER
annotation that was created for the shared task
at CoNLL-2003 3. The training set contains ap-
proximately 220,000 tokens and the development
set and test set contains 55,000 tokens each. We
use Stanford?s Named Entity Recognition system4
which uses a linear-chain conditional random field
to predict the most likely sequence of NE la-
bels (Finkel and Manning, 2009).
Corpora for Clustering: We used parallel cor-
pora for {Arabic, English, French, Korean &
Turkish}-German pairs from WIT-3 corpus (Cet-
tolo et al, 2012) 5, which is a collection of trans-
lated transcriptions of TED talks. Each language
pair contained around 1.5 million German words.
The corpus was word aligned in two directions
using an unsupervised word aligner (Dyer et al,
2013), then the intersected alignment points were
taken.
Monolingual Clustering: For every language
pair, we train German word clusters on the mono-
lingual German data from the parallel data. Note
that the parallel corpora are of different sizes and
hence the monolingual German data from every
parallel corpus is different. We treat the F1 score
2In practice, the number of exchanged words drops of exponentially,
so this threshold is typically reached in not many iterations.
3http://www.cnts.ua.ac.be/conll2003/ner/
4http://nlp.stanford.edu/ner/index.shtml
5https://wit3.fbk.eu/mt.php?release=2012-03
779
The boys are playing
Al- AwlAd ylEbwn
Al- AwlAdylEb
) ?*()'& ?$?"? (
) &'(, ?$?"? (
ylEb
ylEbwn
Al-
AwlAd
are
playing
boys
The
Al-
ylEb
are
playing
boys
The
AwlAd
ylEbwn
H(D;v) = 4H(C;w) = 4.56
H(C;w) = 4H(D;v) = 3.88
H(C;w) + H(D;v)= 8.56H(C;w) + H(D;v)= 7.88
AVI(C,D)= 0AVI(C,D)= 1.0
Figure 2: A two-sentence English-Arabic parallel corpus (left); a 3-class clustering that maximizes the
monolingual objective (? = 0; center); and a 3-class clustering that maximizes the joint monolingual
and bilingual objective (any ? > 0.68; right).
obtained using monolingual word clusters (? = 0)
as the baseline. Table 1 shows the F1 score of
NER6 when trained on these monolingual German
word clusters.
Bilingual Clustering: While we have formu-
lated a joint objective that enables using both
monolingual and bilingual evidence, it is possible
to create word clusters using the bilingual signal
only by removing the first term in Eq. 2.3. Ta-
ble 1 shows the performance of NER when the
word clusters are obtained using only the bilingual
information for different language pairs. As can
be seen, these clusters are helpful for all the lan-
guage pairs. For Turkish the F1 score improves
by 1.0 point over when there are no distributional
clusters which clearly shows that the word align-
ment information improves the clustering quality.
We now need to supplement the bilingual infor-
mation with monolingual information to see if the
improvement sustains.
We varied the weight of the bilingual objec-
tive (?) from 0.05 to 0.9 and observed the ef-
fect in NER performance on English-German lan-
guage pair. The F1 score is maximum for ? =
0.1 and decreases monotonically when ? is ei-
ther increased or decreased. This indicates that
bilingual information is helpful, but less valuable
than monolingual information. Preliminary exper-
iments showed that the value of ? = 0.1 is fairly
robust across other language pairs and hence we
fix it to that for all the experiments.
We run our bilingual clustering model (? =
6Faruqui and Pado? (2010) show that for the size of our generalization
data in German-NER, K = 100 should give us the optimum value.
0.1) across all language pairs and note the F1
scores. Table 1 (unrefined) shows that except for
Arabic-German & French-German, all other lan-
guage pairs deliver a better F1 score than only us-
ing monolingual German data. In case of Arabic-
German there is a drop in score by 0.25 points.
Although, we have observed improvement in F1
score over the monolingual case, the gains do
not reach significance according to McNemar?s
test (Dietterich, 1998).
Thus we propose to further refine the quality of
word alignment links as follows: Let x be a word
in language ? and y be a word in language ? and
let there exists an alignment link between x and
y. Recall that A(x, y) is the count of the align-
ment links between x and y observed in the par-
allel data, and A(x) and A(y) are the respective
marginal counts. Then we define an edge associ-
ation weight e(x, y) = 2?A(x,y)A(x)+A(y) This quantityis an association of the strength of the relationship
between x and y, and we use it to remove all align-
ment links whose e(x, y) is below a given thresh-
old before running the bilingual clustering model.
We vary e from 0.1 to 0.7 and observe the new F1
scores on the development data. Table 1 (refined)
shows the results obtained by our refined model.
The values shown in bold are the highest improve-
ments over the monolingual model.
For English and Turkish we observe a statisti-
cally significant improvement over the monolin-
gual model (cf. Table 1) with p < 0.007 and
p < 0.001 according to McNemar?s test. Ara-
bic improves least with just an improvement of
0.02 F1 points over the monolingual baseline. We
780
Dev Test
Language Pair ? ? = 0 ? = 0.1 ? = 0.1 ? = 0 ? = 0.1(only bi) (only mono) (unrefined) (refined) (only mono) (refined)
No clusters 68.27 72.32
En-De 68.95 70.04 70.33 70.64? 72.30 72.98?
Fr-De 69.16 69.74 69.69 69.89 72.66 72.83
Ar-De 69.01 69.65 69.40 69.67 72.90 72.37
Tr-De 69.29 69.46 69.64 70.05? 72.41 72.54
Ko-De 68.95 69.70 69.78 69.95 72.71 72.54
Average 69.07 69.71 69.76 70.04? 72.59 72.65
Table 1: NER performance using different word clustering models. Bold indicates an improvement over
the monolingual (? = 0) baseline; ? indicates a significant improvement (McNemar?s test, p < 0.01).
see that the optimal value of e changes from one
language pair to another. For French and English
e = 0.1 gives the best results whereas for Turk-
ish and Arabic e = 0.5 and for Korean e = 0.7.
Are these thresholds correlated with anything? We
suggest that higher values of e correspond to more
intrinsically noisy alignments. Since alignment
models are parameterized based on the vocabu-
laries of the languages they are aligning, larger
vocabularies are more prone to degenerate solu-
tions resulting from overfitting. So we are not
surprised to see that sparser alignments (resulting
from higher values of e) are required by languages
like Korean, while languages like French and En-
glish make due with denser alignments.
Evaluation on Test Set: We now verify our re-
sults on the test set. We take the best bilin-
gual word clustering model obtained for every lan-
guage pair (e = 0.1 for En, Fr. e = 0.5 for Ar,
Tr. e = 0.7 for Ko) and train NER classifiers
using these. Table 1 shows the performance of
German NER classifiers on the test set. All the
values shown in bold are better than the mono-
lingual baselines. English again has a statistically
significant improvement over the baseline. French
and Turkish show the next best improvements.
The English-German cluster model performs bet-
ter than the mkcls7 tool (72.83%).
4 Related Work
Our monolingual clustering model is purely distri-
butional in nature. Other extensions to word clus-
tering have incorporated morphological and or-
thographic information (Clark, 2003). The work
of Snyder and Barzilay (2010), which focused on
POS induction is very closely related. The ear-
liest work on bilingual word clustering was pro-
posed by (Och, 1999) which, like us, uses a lan-
7http://www.statmt.org/moses/giza/mkcls.html
guage modeling approach (Brown et al, 1992;
Kneser and Ney, 1993) for monolingual optimiza-
tion and a similarity function for bilingual simi-
larity. Ta?ckstro?m et al (2012) use cross-lingual
word clusters to show transfer of linguistic struc-
ture. While their clustering method is superficially
similar, the objective function is more heuristic in
nature than our information-theoretic conception
of the problem. Multilingual learning has been
applied to a number of unsupervised and super-
vised learning problems, including word sense dis-
ambiguation (Diab, 2003; Guo and Diab, 2010),
topic modeling (Mimno et al, 2009; Boyd-Graber
and Blei, 2009), and morphological segmenta-
tion (Snyder and Barzilay, 2008).
Also closely related is the technique of cross-
lingual annotation projection. This has been
applied to bootstrapping syntactic parsers (Hwa
et al, 2005; Smith and Smith, 2007; Co-
hen et al, 2011), morphology (Fraser, 2009),
tense (Schiehlen, 1998) and T/V pronoun us-
age (Faruqui and Pado?, 2012).
5 Conclusions
We presented a novel information theoretic model
for bilingual word clustering which seeks a clus-
tering with high average mutual information be-
tween clusters of adjacent words, and also high
mutual information across observed word align-
ment links. We have shown that improvement in
clustering can be obtained across a range of lan-
guage pairs, evaluated in terms of their value as
features in an extrinsic NER task. Our model can
be extended for clustering any number of given
languages together in a joint framework, and in-
corporate both monolingual and parallel data.
Acknowledgement: We woud like to thank W.
Ammar, V. Chahuneau and W. Ling for valuable
discussions.
781
References
J. Boyd-Graber and D. M. Blei. 2009. Multilingual
topic models for unaligned text. In Proceedings of
the Twenty-Fifth Conference on Uncertainty in Arti-
ficial Intelligence, UAI ?09, pages 75?82, Arlington,
Virginia, United States. AUAI Press.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra,
and J. C. Lai. 1992. Class-based n-gram models
of natural language. Comput. Linguist., 18(4):467?
479, December.
M. Cettolo, C. Girardi, and M. Federico. 2012. Wit3:
Web inventory of transcribed and translated talks. In
Proceedings of the 16th Conference of the European
Association for Machine Translation (EAMT), pages
261?268, Trento, Italy, May.
A. Clark. 2003. Combining distributional and mor-
phological information for part of speech induction.
In Proceedings of the tenth conference on Euro-
pean chapter of the Association for Computational
Linguistics - Volume 1, EACL ?03, pages 59?66,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 50?61, Stroudsburg, PA,
USA. Association for Computational Linguistics.
M. T. Diab. 2003. Word sense disambiguation within a
multilingual framework. Ph.D. thesis, University of
Maryland at College Park, College Park, MD, USA.
AAI3115805.
T. G. Dietterich. 1998. Approximate statistical tests
for comparing supervised classification learning al-
gorithms. Neural Computation, 10:1895?1923.
C. Dyer, V. Chahuneau, and N. A. Smith. 2013.
A simple, fast, and effective reparameterization of
IBM Model 2. In Proc. NAACL.
M. Faruqui and S. Pado?. 2010. Training and Evalu-
ating a German Named Entity Recognizer with Se-
mantic Generalization. In Proceedings of KON-
VENS 2010, Saarbru?cken, Germany.
M. Faruqui and S. Pado?. 2012. Towards a model of for-
mal and informal address in english. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics.
J. R. Finkel and C. D. Manning. 2009. Nested named
entity recognition. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1 - Volume 1, EMNLP ?09,
pages 141?150, Stroudsburg, PA, USA. Association
for Computational Linguistics.
A. Fraser. 2009. Experiments in morphosyntactic pro-
cessing for translating to and from German. In Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 115?119, Athens, Greece,
March. Association for Computational Linguistics.
W. Guo and M. Diab. 2010. Combining orthogonal
monolingual and multilingual sources of evidence
for all words wsd. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ?10, pages 1542?1551, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
J. A. Haywood and H. M. Nahmad. 1999. A new
Arabic grammar of the written language. Lund
Humphries Publishers.
R. Hwa, P. Resnik, A. Weinberg, C. I. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural Language
Engineering, pages 311?325.
R. Kneser and H. Ney. 1993. Forming word classes
by statistical clustering for statistical language mod-
elling. In R. Khler and B. Rieger, editors, Contri-
butions to Quantitative Linguistics, pages 221?226.
Springer Netherlands.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proc. of
ACL.
S. Martin, J. Liermann, and H. Ney. 1995. Algorithms
for bigram and trigram word clustering. In Speech
Communication, pages 1253?1256.
M. Meila?. 2003. Comparing Clusterings by the Varia-
tion of Information. In Learning Theory and Kernel
Machines, pages 173?187.
D. Mimno, H. M. Wallach, J. Naradowsky, D. A.
Smith, and A. McCallum. 2009. Polylingual topic
models. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 2 - Volume 2, EMNLP ?09, pages 880?
889, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
F. J. Och. 1995. Maximum-Likelihood-Scha?tzung von
Wortkategorien mit Verfahren der kombinatorischen
Optimierung. Studienarbeit, University of Erlangen.
F. J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ?99,
pages 71?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
M. Schiehlen. 1998. Learning tense translation from
bilingual corpora.
D. A. Smith and N. A. Smith. 2007. Probabilis-
tic Models of Nonprojective Dependency Trees.
In Proceedings of the 2007 Joint Conference on
782
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 132?140, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
B. Snyder and R. Barzilay. 2008. Unsupervised mul-
tilingual learning for morphological segmentation.
In In The Annual Conference of the Association for
Computational Linguistics.
B. Snyder and R. Barzilay. 2010. Climbing the tower
of babel: Unsupervised multilingual learning. In
J. Frnkranz and T. Joachims, editors, Proceedings
of the 27th International Conference on Machine
Learning (ICML-10), June 21-24, 2010, Haifa, Is-
rael, pages 29?36. Omnipress.
O. Ta?ckstro?m, R. McDonald, and J. Uszkoreit. 2012.
Cross-lingual word clusters for direct transfer of
linguistic structure. In The 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, volume 1, page 11. Association for Com-
putational Linguistics.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 384?394, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
783
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 248?258,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Metaphor Detection with Cross-Lingual Model Transfer
Yulia Tsvetkov Leonid Boytsov Anatole Gershman Eric Nyberg Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ytsvetko, srchvrs, anatoleg, ehn, cdyer}@cs.cmu.edu
Abstract
We show that it is possible to reliably dis-
criminate whether a syntactic construction
is meant literally or metaphorically using
lexical semantic features of the words that
participate in the construction. Our model
is constructed using English resources,
and we obtain state-of-the-art performance
relative to previous work in this language.
Using a model transfer approach by piv-
oting through a bilingual dictionary, we
show our model can identify metaphoric
expressions in other languages. We pro-
vide results on three new test sets in Span-
ish, Farsi, and Russian. The results sup-
port the hypothesis that metaphors are
conceptual, rather than lexical, in nature.
1 Introduction
Lakoff and Johnson (1980) characterize metaphor
as reasoning about one thing in terms of another,
i.e., a metaphor is a type of conceptual mapping,
where words or phrases are applied to objects and
actions in ways that do not permit a literal inter-
pretation. They argue that metaphors play a fun-
damental communicative role in verbal and writ-
ten interactions, claiming that much of our every-
day language is delivered in metaphorical terms.
There is empirical evidence supporting the claim:
recent corpus studies have estimated that the pro-
portion of words used metaphorically ranges from
5% to 20% (Steen et al, 2010), and Thibodeau and
Boroditsky (2011) provide evidence that a choice
of metaphors affects decision making.
Given the prevalence and importance of
metaphoric language, effective automatic detec-
tion of metaphors would have a number of ben-
efits, both practical and scientific. Language pro-
cessing applications that need to understand lan-
guage or preserve meaning (information extrac-
tion, machine translation, dialog systems, senti-
ment analysis, and text analytics, etc.) would have
access to a potentially useful high-level bit of in-
formation about whether something is to be under-
stood literally or not. Second, scientific hypothe-
ses about metaphoric language could be tested
more easily at a larger scale with automation.
However, metaphor detection is a hard problem.
On one hand, there is a subjective component: hu-
mans may disagree whether a particular expres-
sion is used metaphorically or not, as there is no
clear-cut semantic distinction between figurative
and metaphorical language (Shutova, 2010). On
the other, metaphors can be domain- and context-
dependent.
1
Previous work has focused on metaphor identi-
fication in English, using both extensive manually-
created linguistic resources (Mason, 2004; Gedi-
gian et al, 2006; Krishnakumaran and Zhu, 2007;
Turney et al, 2011; Broadwell et al, 2013) and
corpus-based approaches (Birke and Sarkar, 2007;
Shutova et al, 2013; Neuman et al, 2013; Shutova
and Sun, 2013; Hovy et al, 2013). We build on
this foundation and also extend metaphor detec-
tion into other languages in which few resources
may exist. Our work makes the following con-
tributions: (1) we develop a new state-of-the-art
English metaphor detection system that uses con-
ceptual semantic features, such as a degree of ab-
stractness and semantic supersenses;
2
(2) we cre-
ate new metaphor-annotated corpora for Russian
and English;
3
(3) using a paradigm of model trans-
fer (McDonald et al, 2011; T?ackstr?om et al, 2013;
Kozhenikov and Titov, 2013), we provide sup-
port for the hypothesis that metaphors are concep-
1
For example, drowning students could be used metaphor-
ically to describe the situation where students are over-
whelmed with work, but in the sentence a lifeguard saved
drowning students, this phrase is used literally.
2
https://github.com/ytsvetko/metaphor
3
http://www.cs.cmu.edu/
?
ytsvetko/
metaphor/datasets.zip
248
tual (rather than lexical) in nature by showing that
our English-trained model can detect metaphors in
Spanish, Farsi, and Russian.
2 Methodology
Our task in this work is to define features that dis-
tinguish between metaphoric and literal uses of
two syntactic constructions: subject-verb-object
(SVO) and adjective-noun (AN) tuples.
4
We give
examples of a prototypical metaphoric usage of
each type:
? SVO metaphors. A sentence containing a
metaphoric SVO relation is my car drinks
gasoline. According to Wilks (1978), this
metaphor represents a violation of selectional
preferences for the verb drink, which is nor-
mally associated with animate subjects (the
car is inanimate and, hence, cannot drink in
the literal sense of the verb).
? AN metaphors. The phrase broken promise
is an AN metaphor, where attributes from
a concrete domain (associated with the con-
crete word broken) are transferred to a more
abstract domain, which is represented by the
relatively abstract word promise. That is, we
map an abstract concept promise to a concrete
domain of physical things, where things can
be literally broken to pieces.
Motivated by Lakoff?s (1980) argument that
metaphors are systematic conceptual mappings,
we will use coarse-grained conceptual, rather than
fine-grained lexical features, in our classifier. Con-
ceptual features pertain to concepts and ideas as
opposed to individual words or phrases expressed
in a particular language. In this sense, as long as
two words in two different languages refer to the
same concepts, their conceptual features should
be the same. Furthermore, we hypothesize that
our coarse semantic features give us a language-
invariant representation suitable for metaphor de-
tection. To test this hypothesis, we use a cross-
lingual model transfer approach: we use bilingual
dictionaries to project words from other syntactic
constructions found in other languages into En-
glish and then apply the English model on the de-
rived conceptual representations.
4
Our decision to focus on SVO and AN metaphors is jus-
tified by corpus studies that estimate that verb- and adjective-
based metaphors account for a substantial proportion of all
metaphoric expressions, approximately 60% and 24%, re-
spectively (Shutova and Teufel, 2010; Gandy et al, 2013).
Each SVO (or AN) instance will be represented
by a triple (duple) from which a feature vector
will be extracted.
5
The vector will consist of the
concatenation of the conceptual features (which
we discuss below) for all participating words, and
conjunction features for word pairs.
6
For example,
to generate the feature vector for the SVO triple
(car, drink, gasoline), we compute all the features
for the individual words car, drink, gasoline and
combine them with the conjunction features for
the pairs car drink and drink gasoline.
We define three main feature categories (1) ab-
stractness and imageability, (2) supersenses, (3)
unsupervised vector-space word representations;
each category corresponds to a group of features
with a common theme and representation.
? Abstractness and imageability. Abstract-
ness and imageability were shown to be use-
ful in detection of metaphors (it is easier to
invoke mental pictures of concrete and im-
ageable words) (Turney et al, 2011; Broad-
well et al, 2013). We expect that abstract-
ness, used in conjunction features (e.g., a
feature denoting that the subject is abstract
and the verb is concrete), is especially use-
ful: semantically, an abstract agent perform-
ing a concrete action is a strong signal of
metaphorical usage.
Although often correlated with abstractness,
imageability is not a redundant property.
While most abstract things are hard to visu-
alize, some call up images, e.g., vengeance
calls up an emotional image, torture calls up
emotions and even visual images. There are
concrete things that are hard to visualize too,
for example, abbey is harder to visualize than
banana (B. MacWhinney, personal commu-
nication).
? Supersenses. Supersenses
7
are coarse se-
mantic categories originating in WordNet.
For nouns and verbs there are 45 classes:
26 for nouns and 15 for verbs, for example,
5
Looking at components of the syntactic constructions in-
dependent of their context has its limitations, as discussed
above with the drowning students example; however, it sim-
plifies the representation challenges considerably.
6
If word one is represented by features u ? R
n
and word
two by features v ? R
m
then the conjunction feature vector
is the vectorization of the outer product uv
>
.
7
Supersenses are called ?lexicographer classes? in Word-
Net documentation (Fellbaum, 1998), http://wordnet.
princeton.edu/man/lexnames.5WN.html
249
noun.body, noun.animal, verb.consumption,
or verb.motion (Ciaramita and Altun, 2006).
English adjectives do not, as yet, have a sim-
ilar high-level semantic partitioning in Word-
Net, thus we use a 13-class taxonomy of ad-
jective supersenses constructed by Tsvetkov
et al (2014) (discussed in ?3.2).
Supersenses are particularly attractive fea-
tures for metaphor detection: coarse sense
taxonomies can be viewed as semantic con-
cepts, and since concept mapping is a pro-
cess in which metaphors are born, we
expect different supersense co-occurrences
in metaphoric and literal combinations.
In ?drinks gasoline?, for example, map-
ping to supersenses would yield a pair
<verb.consumption, noun.substance>, con-
trasted with <verb.consumption, noun.food>
for ?drinks juice?. In addition, this coarse
semantic categorization is preserved in trans-
lation (Schneider et al, 2013), which makes
supersense features suitable for cross-lingual
approaches such as ours.
? Vector space word representations. Vec-
tor space word representations learned us-
ing unsupervised algorithms are often effec-
tive features in supervised learning methods
(Turian et al, 2010). In particular, many such
representations are designed to capture lex-
ical semantic properties and are quite effec-
tive features in semantic processing, includ-
ing named entity recognition (Turian et al,
2009), word sense disambiguation (Huang et
al., 2012), and lexical entailment (Baroni et
al., 2012). In a recent study, Mikolov et
al. (2013) reveal an interesting cross-lingual
property of distributed word representations:
there is a strong similarity between the vec-
tor spaces across languages that can be eas-
ily captured by linear mapping. Thus, vector
space models can also be seen as vectors of
(latent) semantic concepts, that preserve their
?meaning? across languages.
3 Model and Feature Extraction
In this section we describe a classification model,
and provide details on mono- and cross-lingual
implementation of features.
3.1 Classification using Random Forests
To make classification decisions, we use a random
forest classifier (Breiman, 2001), an ensemble of
decision tree classifiers learned from many inde-
pendent subsamples of the training data. Given
an input, each tree classifier assigns a probabil-
ity to each label; those probabilities are averaged
to compute the probability distribution across the
ensemble. Random forest ensembles are partic-
ularly suitable for our resource-scarce scenario:
rather than overfitting, they produce a limiting
value of the generalization error as the number
of trees increases,
8
and no hyperparameter tuning
is required. In addition, decision-tree classifiers
learn non-linear responses to inputs and often out-
perform logistic regression (Perlich et al, 2003).
9
Our random forest classifier models the probabil-
ity that the input syntactic relation is metaphorical.
If this probability is above a threshold, the relation
is classified as metaphoric, otherwise it is literal.
We used the scikit-learn toolkit to train our
classifiers (Pedregosa et al, 2011).
3.2 Feature extraction
Abstractness and imageability. The MRC psy-
cholinguistic database is a large dictionary listing
linguistic and psycholinguistic attributes obtained
experimentally (Wilson, 1988).
10
It includes,
among other data, 4,295 words rated by the de-
grees of abstractness and 1,156 words rated by the
imageability. Similarly to Tsvetkov et al (2013),
we use a logistic regression classifier to propagate
abstractness and imageability scores from MRC
ratings to all words for which we have vector space
representations. More specifically, we calculate
the degree of abstractness and imageability of all
English items that have a vector space representa-
tion, using vector elements as features. We train
two separate classifiers for abstractness and im-
ageability on a seed set of words from the MRC
database. Degrees of abstractness and imageabil-
ity are posterior probabilities of classifier predic-
tions. We binarize these posteriors into abstract-
concrete (or imageable-unimageable) boolean in-
dicators using pre-defined thresholds.
11
Perfor-
8
See Theorem 1.2 in (Breiman, 2001) for details.
9
In our experiments, random forests model slightly out-
performed logistic regression and SVM classifiers.
10
http://ota.oucs.ox.ac.uk/headers/
1054.xml
11
Thresholds are equal to 0.8 for abstractness and to 0.9
for imageability. They were chosen empirically based on ac-
250
mance of these classifiers, tested on a sampled
held-out data, is 0.94 and 0.85 for the abstractness
and imageability classifiers, respectively.
Supersenses. In the case of SVO relations, we
incorporate supersense features for nouns and
verbs; noun and adjective supersenses are used in
the case of AN relations.
Supersenses of nouns and verbs. A lexical item
can belong to several synsets, which are associ-
ated with different supersenses. Degrees of mem-
bership in different supersenses are represented
by feature vectors, where each element corre-
sponds to one supersense. For example, the word
head (when used as a noun) participates in 33
synsets, three of which are related to the super-
sense noun.body. The value of the feature corre-
sponding to this supersense is 3/33 ? 0.09.
Supersenses of adjectives. WordNet lacks
coarse-grained semantic categories for adjectives.
To divide adjectives into groups, Tsvetkov et al
(2014) use 13 top-level classes from the adapted
taxonomy of Hundsnurscher and Splett (1982),
which is incorporated in GermaNet (Hamp and
Feldweg, 1997). For example, the top-level
classes in GermaNet include: adj.feeling (e.g.,
willing, pleasant, cheerful); adj.substance (e.g.,
dry, ripe, creamy); adj.spatial (e.g., adjacent, gi-
gantic).
12
For each adjective type in WordNet,
they produce a vector with a classifier posterior
probabilities corresponding to degrees of mem-
bership of this word in one of the 13 semantic
classes,
13
similar to the feature vectors we build
for nouns and verbs. For example, for a word
calm the top-2 categories (with the first and second
highest degrees of membership) are adj.behavior
and adj.feeling.
Vector space word representations. We em-
ploy 64-dimensional vector-space word represen-
tations constructed by Faruqui and Dyer (2014).
14
Vector construction algorithm is a variation on
traditional latent semantic analysis (Deerwester
et al, 1990) that uses multilingual information
to produce representations in which synonymous
words have similar vectors. The vectors were
curacy during cross-validation.
12
For the full taxonomy see http://www.sfs.
uni-tuebingen.de/lsd/adjectives.shtml
13
http://www.cs.cmu.edu/
?
ytsvetko/
adj-supersenses.tar.gz
14
http://www.cs.cmu.edu/
?
mfaruqui/soft.
html
trained on the news commentary corpus released
by WMT-2011,
15
comprising 180,834 types.
3.3 Cross-lingual feature projection
For languages other than English, feature vectors
are projected to English features using translation
dictionaries. We used the Babylon dictionary,
16
which is a proprietary resource, but any bilingual
dictionary can in principle be used. For a non-
English word in a source language, we first ob-
tain all translations into English. Then, we av-
erage all feature vectors related to these transla-
tions. Consider an example related to projection
of WordNet supersenses. A Russian word ??????
is translated as head and brain. Hence, we select
all the synsets of the nouns head and brain. There
are 38 such synsets (33 for head and 5 for brain).
Four of these synsets are associated with the su-
persense noun.body. Therefore, the value of the
feature noun.body is 4/38 ? 0.11.
4 Datasets
In this section we describe a training and testing
dataset as well a data collection procedure.
4.1 English training sets
To train an SVO metaphor classifier, we employ
the TroFi (Trope Finder) dataset.
17
TroFi includes
3,737 manually annotated English sentences from
the Wall Street Journal (Birke and Sarkar, 2007).
Each sentence contains either literal or metaphori-
cal use for one of 50 English verbs. First, we use a
dependency parser (Martins et al, 2010) to extract
subject-verb-object (SVO) relations. Then, we fil-
ter extracted relations to eliminate parsing-related
errors, and relations with verbs which are not in
the TroFi verb list. After filtering, there are 953
metaphorical and 656 literal SVO relations which
we use as a training set.
In the case of AN relations, we construct and
make publicly available a training set contain-
ing 884 metaphorical AN pairs and 884 pairs
with literal meaning. It was collected by two
annotators using public resources (collections of
metaphors on the web). At least one additional
person carefully examined and culled the col-
lected metaphors, by removing duplicates, weak
metaphors, and metaphorical phrases (such as
15
http://www.statmt.org/wmt11/
16
http://www.babylon.com
17
http://www.cs.sfu.ca/
?
anoop/students/
jbirke/
251
drowning students) whose interpretation depends
on the context.
4.2 Multilingual test sets
We collect and annotate metaphoric and literal test
sentences in four languages. Thus, we compile
eight test datasets, four for SVO relations, and
four for AN relations. Each dataset has an equal
number of metaphors and non-metaphors, i.e., the
datasets are balanced. English (EN) and Russian
(RU) datasets have been compiled by our team
and are publicly available. Spanish (ES) and Farsi
(FA) datasets are published elsewhere (Levin et al,
2014). Table 1 lists test set sizes.
SVO AN
EN 222 200
RU 240 200
ES 220 120
FA 44 320
Table 1: Sizes of the eight test sets. Each dataset is
balanced, i.e., it has an equal number of metaphors
and non-metaphors. For example, English SVO
dataset has 222 relations: 111 metaphoric and 111
literal.
We used the following procedure to compile the
EN and RU test sets. A moderator started with seed
lists of 1000 most common verbs and adjectives.
18
Then she used the SketchEngine, which pro-
vides searching capability for the TenTen Web cor-
pus,
19
to extract sentences with words that fre-
quently co-occurred with words from the seed
lists. From these sentences, she removed sen-
tences that contained more than one metaphor, and
sentences with non-SVO and non-AN metaphors.
Remaining sentences were annotated by several
native speakers (five for English and six for Rus-
sian), who judged AN and SVO phrases in con-
text. The annotation instructions were general:
?Please, mark in bold all words that, in your opin-
ion, are used non-literally in the following sen-
tences. In many sentences, all the words may be
used literally.? The Fleiss? Kappas for 5 English
and 6 Russian annotators are: EN-AN = .76, RU-
18
Selection of 1000 most common verbs and adjectives
achieves much broader lexical and domain coverage than
what can be realistically obtained from continuous text. Our
test sentence domains are, therefore, diverse: economic, po-
litical, sports, etc.
19
http://trac.sketchengine.co.uk/wiki/
Corpora/enTenTen
AN = .85, EN-SVO = .75, RU-SVO = .78. For the fi-
nal selection, we filtered out low-agreement (<.8)
sentences.
The test candidate sentences were selected by
a person who did not participate in the selection
of the training samples. No English annotators of
the test set, and only one Russian annotator out
of 6 participated in the selection of the training
samples. Thus, we trust that annotator judgments
were not biased towards the cases that the system
is trained to process.
5 Experiments
5.1 English experiments
Our task, as defined in Section 2, is to classify
SVO and AN relations as either metaphoric or lit-
eral. We first conduct a 10-fold cross-validation
experiment on the training set defined in Section
4.1. We represent each candidate relation using
the features described in Section 3.2, and evalu-
ate performance of the three feature categories and
their combinations. This is done by computing an
accuracy in the 10-fold cross validation. Experi-
mental results are given in Table 2, where we also
provide the number of features in each feature set.
SVO AN
# FEAT ACC # FEAT ACC
AbsImg 20 0.73
?
16 0.76
?
Supersense 67 0.77
?
116 0.79
?
AbsImg+Sup. 87 0.78
?
132 0.80
?
VSM 192 0.81 228 0.84
?
All 279 0.82 360 0.86
Table 2: 10-fold cross validation results for three
feature categories and their combination, for clas-
sifiers trained on English SVO and AN training
sets. # FEAT column shows a number of features.
ACC column reports an accuracy score in the 10-
fold cross validation. Statistically significant dif-
ferences (p < 0.01) from the all-feature combina-
tion are marked with a star.
These results show superior performance over
previous state-of-the-art results, confirming our
hypothesis that conceptual features are effective
in metaphor classification. For the SVO task, the
cross-validation accuracy is about 10% better than
that of Tsvetkov et al (2013). For the AN task,
the cross validation accuracy is better by 8% than
the result of Turney et al (2011) (two baseline
252
methods are described in Section 5.2). We can
see that all types of features have good perfor-
mance on their own (VSM is the strongest feature
type). Noun supersense features alone allows us to
achieve an accuracy of 75%, i.e., adjective super-
sense features contribute 4% to adjective-noun su-
persense feature combination. Experiments with
the pairs of features yield better results than in-
dividual features, implying that the feature cate-
gories are not redundant. Yet, combining all fea-
tures leads to even higher accuracy during cross-
validation. In the case of the AN task, a difference
between the All feature combination and any other
combination of features listed in Table 2 is statis-
tically significant (p < 0.01 for both the sign and
the permutation test).
Although the first experiment shows very high
scores, the 10-fold cross-validation cannot fully
reflect the generality of the model, because all
folds are parts of the same corpus. They are col-
lected by the same human judges and belong to the
same domain. Therefore, experiments on out-of-
domain data are crucial. We carry out such exper-
iments using held-out SVO and AN EN test sets,
described in Section 4.2 and Table 1. In this ex-
periment, we measure the f -score. We classify
SVO and AN relations using a classifier trained on
the All feature combination and balanced thresh-
olds. The values of the f -score are 0.76, both for
SVO and AN tasks. This out-of-domain experi-
ment suggests that our classifier is portable across
domains and genres.
However, (1) different application may have
different requirements for recall/precision, and (2)
classification results may be skewed towards hav-
ing high precision and low recall (or vice versa). It
is possible to trade precision for recall by choos-
ing a different threshold. Thus, in addition to
giving a single f -score value for balanced thresh-
olds, we present a Receiver Operator Characteris-
tic (ROC) curve, where we plot a fraction of true
positives against the fraction of false positives for
100 threshold values in the range from zero to one.
The area under the ROC curve (AUC) can be in-
terpreted as the probability that a classifier will as-
sign a higher score to a randomly chosen positive
example than to a randomly chosen negative ex-
ample.
20
For a randomly guessing classifier, the
ROC curve is a dashed diagonal line. A bad classi-
20
Assuming that positive examples are labeled by ones,
and negative examples are labeled by zeros.
fier has an ROC curve that goes close to the dashed
diagonal or even below it.
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
Supersenses (area = 0.77)
AbsImg (area = 0.73)
VSM (area = 0.8)
All (area = 0.79)
(a) SVO
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
AbsImg (area = 0.9)
Supersenses (area = 0.86)
VSM (area = 0.89)
All (area = 0.92)
(b) AN
Figure 1: ROC curves for classifiers trained using
different feature sets (English SVO and AN test
sets).
According to ROC plots in Figure 1, all three
feature sets are effective, both for SVO and for
AN tasks. Abstractness and Imageability features
work better for adjectives and nouns, which is in
line with previous findings (Turney et al, 2011;
Broadwell et al, 2013). It can be also seen that
VSM features are very effective. This is in line
with results of Hovy et al (2013), who found that
it is hard to improve over the classifier that uses
only VSM features.
5.2 Comparison to baselines
In this section, we compare our method to state-of-
the-art methods of Tsvetkov et al (2013) and of
Turney et al (2011), who focused on classifying
SVO and AN relations, respectively.
In the case of SVO relations, we use software
253
and datasets from Tsvetkov et al (2013). These
datasets, denoted as an SVO-baseline, consist of
98 English and 149 Russian sentences. We train
SVO metaphor detection tools on SVO relations
extracted from TroFi sentences and evaluate them
on the SVO-baseline dataset. We also use the same
thresholds for classifier posterior probabilities as
Tsvetkov et al (2013). Our approach is different
from that of Tsvetkov et al (2013) in that it uses
additional features (vector space word representa-
tions) and a different classification method (we use
random forests while Tsvetkov et al (2013) use
logistic regression). According to Table 3, we ob-
tain higher performance scores for both Russian
and English.
EN RU
SVO-baseline 0.78 0.76
This work 0.86 0.85
Table 3: Comparing f -scores of our SVO
metaphor detection method to the baselines.
In the case of AN relations, we use the dataset
(denoted as an AN-baseline) created by Turney
et al (2011) (see Section 4.1 in the referred pa-
per for details). Turney et al (2011) manu-
ally annotated 100 pairs where an adjective was
one of the following: dark, deep, hard, sweet,
and worm. The pairs were presented to five
human judges who rated each pair on a scale
from 1 (very literal/denotative) to 4 (very non-
literal/connotative). Turney et al (2011) train
logistic-regression employing only abstractness
ratings as features. Performance of the method
was evaluated using the 10-fold cross-validation
separately for each judge.
We replicate the above described evaluation
procedure of Turney et al (2011) using their
model and features. In our classifier, we use the
All feature combination and the balanced thresh-
old as described in Section 5.1.
According to results in Table 4, almost all of the
judge-specific f -scores are slightly higher for our
system, as well as the overall average f -score.
In both baseline comparisons, we obtain perfor-
mance at least as good as in previously published
studies.
5.3 Cross-lingual experiments
In the next experiment we corroborate the main
hypothesis of this paper: a model trained on En-
AN-baseline This work
Judge 1 0.73 0.75
Judge 2 0.81 0.84
Judge 3 0.84 0.88
Judge 4 0.79 0.81
Judge 5 0.78 0.77
average 0.79 0.81
Table 4: Comparing AN metaphor detection
method to the baselines: accuracy of the 10-
fold cross validation on annotations of five human
judges.
glish data can be successfully applied to other
languages. Namely, we use a trained English
model discussed in Section 5.1 to classify literal
and metaphoric SVO and AN relations in English,
Spanish, Farsi and Russian test sets, listed in Sec-
tion 4.2. This time we used all available features.
Experimental results for all four languages, are
given in Figure 2. The ROC curves for SVO and
AN tasks are plotted in Figure 2a and Figure 2b,
respectively. Each curve corresponds to a test set
described in Table 1. In addition, we perform an
oracle experiment, to obtain actual f -score values
for best thresholds. Detailed results are shown in
Table 5.
Consistent results with high f -scores are ob-
tained across all four languages. Note that higher
scores are obtained for the Russian test set. We hy-
pothesize that this happens due to a higher-quality
translation dictionary (which allows a more accu-
rate model transfer). Relatively lower (yet rea-
sonable) results for Farsi can be explained by a
smaller size of the bilingual dictionary (thus, fewer
feature projections can be obtained). Also note
that, in our experience, most of Farsi metaphors
are adjective-noun constructions. This is why the
AN FA dataset in Table 1 is significantly larger
than SVO FA. In that, for the AN Farsi task we
observe high performance scores.
Figure 2 and Table 5 confirm, that we ob-
tain similar, robust results on four very differ-
ent languages, using the same English classi-
fiers. We view this result as a strong evidence of
language-independent nature of our metaphor de-
tection method. In particular, this shows that pro-
posed conceptual features can be used to detect se-
lectional preferences violation across languages.
To summarize the experimental section, our
metaphor detection approach obtains state-of-the-
254
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
EN (area = 0.79)
ES (area = 0.71)
FA (area = 0.69)
RU (area = 0.89)
(a) SVO
0.0 0.2 0.4 0.6 0.8 1.0
False Positive Rate
0.0
0.2
0.4
0.6
0.8
1.0
T
r
u
e
 
P
o
s
i
t
i
v
e
 
R
a
t
e
EN (area = 0.92)
ES (area = 0.73)
FA (area = 0.83)
RU (area = 0.8)
(b) AN
Figure 2: Cross-lingual experiment: ROC curves
for classifiers trained on the English data using a
combination of all features, and applied to SVO
and AN metaphoric and literal relations in four test
languages: English, Russian, Spanish, and Farsi.
art performance in English, is effective when ap-
plied to out-of-domain English data, and works
cross-lingually.
5.4 Examples
Manual data analysis on adjective-noun pairs sup-
ports an abstractness-concreteness hypothesis for-
mulated by several independent research studies.
For example, in English we classify as metaphoric
dirty word and cloudy future. Word pairs dirty
diaper and cloudy weather have same adjectives.
Yet they are classified as literal. Indeed, diaper
is a more concrete term than word and weather
is more concrete than future. Same pattern is ob-
served in non-English datasets. In Russian, ????-
??? ???????? ?sick society? and ?????? ????
?empty sound? are classified as metaphoric, while
SVO AN
EN 0.79 0.85
RU 0.84 0.77
ES 0.76 0.72
FA 0.75 0.74
Table 5: Cross-lingual experiment: f -scores for
classifiers trained on the English data using a com-
bination of all features, and applied, with optimal
thresholds, to SVO and AN metaphoric and literal
relations in four test languages: English, Russian,
Spanish, and Farsi.
??????? ??????? ?sick grandmother? and ??-
???? ????? ?empty cup? are classified as literal.
Spanish example of an adjective-noun metaphor
is a well-known m?usculo econ?omico ?economic
muscle?. We also observe that non-metaphoric ad-
jective noun pairs tend to have more imageable ad-
jectives, such as literal derecho humano ?human
right?. In Spanish, human is more imageable than
economic.
Verb-based examples that are correctly clas-
sified by our model are: blunder escaped no-
tice (metaphoric) and prisoner escaped jail (lit-
eral). We hypothesize that supersense features are
instrumental in the correct classification of these
examples: <noun.person,verb.motion> is usually
used literally, while <noun.act,verb.motion> is
used metaphorically.
6 Related Work
For a historic overview and a survey of
common approaches to metaphor detection,
we refer the reader to recent reviews by
Shutova et al (Shutova, 2010; Shutova et al,
2013). Here we focus only on recent approaches.
Shutova et al (2010) proposed a bottom-up
method: one starts from a set of seed metaphors
and seeks phrases where verbs and/or nouns be-
long to the same cluster as verbs or nouns in seed
examples.
Turney et al (2011) show how abstractness
scores could be used to detect metaphorical AN
phrases. Neuman et al (2013) describe a Concrete
Category Overlap algorithm, where co-occurrence
statistics and Turney?s abstractness scores are used
to determine WordNet supersenses that corre-
spond to literal usage of a given adjective or verb.
For example, given an adjective, we can learn that
it modifies concrete nouns that usually have the
255
supersense noun.body. If this adjective modifies
a noun with the supersense noun.feeling, we con-
clude that a metaphor is found.
Broadwell et al (2013) argue that metaphors
are highly imageable words that do not belong
to a discussion topic. To implement this idea,
they extend MRC imageability scores to all dic-
tionary words using links among WordNet super-
senses (mostly hypernym and hyponym relations).
Strzalkowski et al (2013) carry out experiments
in a specific (government-related) domain for four
languages: English, Spanish, Farsi, and Russian.
Strzalkowski et al (2013) explain the algorithm
only for English and say that is the same for Span-
ish, Farsi, and Russian. Because they heavily
rely on WordNet and availability of imageability
scores, their approach may not be applicable to
low-resource languages.
Hovy et al (2013) applied tree kernels to
metaphor detection. Their method also employs
WordNet supersenses, but it is not clear from the
description whether WordNet is essential or can
be replaced with some other lexical resource. We
cannot compare directly our model with this work
because our classifier is restricted to detection of
only SVO and AN metaphors.
Tsvetkov et al (2013) propose a cross-lingual
detection method that uses only English lexical re-
sources and a dependency parser. Their study fo-
cuses only on the verb-based metaphors. Tsvetkov
et al (2013) employ only English and Russian
data. Current work builds on this study, and incor-
porates new syntactic relations as metaphor candi-
dates, adds several new feature sets and different,
more reliable datasets for evaluating results. We
demonstrate results on two new languages, Span-
ish and Farsi, to emphasize the generality of the
method.
A words sense disambiguation (WSD) is a re-
lated problem, where one identifies meanings of
polysemous words. The difference is that in the
WSD task, we need to select an already existing
sense, while for the metaphor detection, the goal
is to identify cases of sense borrowing. Studies
showed that cross-lingual evidence allows one to
achieve a state-of-the-art performance in the WSD
task, yet, most cross-lingual WSD methods em-
ploy parallel corpora (Navigli, 2009).
7 Conclusion
The key contribution of our work is that we show
how to identify metaphors across languages by
building a model in English and applying it?
without adaptation?to other languages: Spanish,
Farsi, and Russian. This model uses language-
independent (rather than lexical or language spe-
cific) conceptual features. Not only do we estab-
lish benchmarks for Spanish, Farsi, and Russian,
but we also achieve state-of-the-art performance
in English. In addition, we present a comparison
of relative contributions of several types of fea-
tures. We concentrate on metaphors in the con-
text of two kinds of syntactic relations: subject-
verb-object (SVO) relations and adjective-noun
(AN) relations, which account for a majority of all
metaphorical phrases.
Future work will expand the scope of metaphor
identification by including nominal metaphoric re-
lations as well as explore techniques for incor-
porating contextual features, which can play a
key role in identifying certain kinds of metaphors.
Second, cross-lingual model transfer can be im-
proved with more careful cross-lingual feature
projection.
Acknowledgments
We are extremely grateful to Shuly Wintner for a
thorough review that helped us improve this draft;
we also thank people who helped in creating the
datasets and/or provided valuable feedback on this
work: Ed Hovy, Vlad Niculae, Davida Fromm,
Brian MacWhinney, Carlos Ram??rez, and other
members of the CMU METAL team. This work
was supported by the U.S. Army Research Labo-
ratory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533.
References
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-chieh Shan. 2012. Entailment above the
word level in distributional semantics. In Proc. of
EACL, pages 23?32.
Julia Birke and Anoop Sarkar. 2007. Active learning
for the identification of nonliteral language. In Proc.
of the Workshop on Computational Approaches to
Figurative Language, FigLanguages ?07, pages 21?
28.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
256
George Aaron Broadwell, Umit Boz, Ignacio Cases,
Tomek Strzalkowski, Laurie Feldman, Sarah Taylor,
Samira Shaikh, Ting Liu, Kit Cho, and Nick Webb.
2013. Using imageability and topic chaining to lo-
cate metaphors in linguistic corpora. In Social Com-
puting, Behavioral-Cultural Modeling and Predic-
tion, pages 102?110. Springer.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger. In
Proc. of EMNLP, pages 594?602.
Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. JASIS,
41(6):391?407.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proc. of EACL. Association for Com-
putational Linguistics.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. Language, Speech and
Communication. MIT Press.
Lisa Gandy, Nadji Allan, Mark Atallah, Ophir Frieder,
Newton Howard, Sergey Kanareykin, Moshe Kop-
pel, Mark Last, Yair Neuman, and Shlomo Arga-
mon. 2013. Automatic identification of conceptual
metaphors with limited knowledge. In Proc. of the
Twenty-Seventh AAAI Conference on Artificial Intel-
ligence, pages 328?334.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41?48.
Birgit Hamp and Helmut Feldweg. 1997. Germanet-
a lexical-semantic net for German. In Proc. of
ACL workshop Automatic Information Extraction
and Building of Lexical Semantic Resources for NLP
Applications, pages 9?15.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Proc. of
the First Workshop on Metaphor in NLP, page 52.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proc. of ACL, pages 873?882.
Franz Hundsnurscher and Jochen Splett. 1982. Se-
mantik der Adjektive des Deutschen. Number 3137.
Westdeutscher Verlag.
Mikhail Kozhenikov and Ivan Titov. 2013. Cross-
lingual transfer of semantic role labeling models. In
Proc. of ACL, pages 1190?1200.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proc. of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20.
George Lakoff and Mark Johnson. 1980. Conceptual
metaphor in everyday language. The Journal of Phi-
losophy, pages 453?486.
Lori Levin, Teruko Mitamura, Davida Fromm, Brian
MacWhinney, Jaime Carbonell, Weston Feely,
Robert Frederking, Anatole Gershman, and Carlos
Ramirez. 2014. Resources for the detection of con-
ventionalized metaphors in four languages. In Proc.
of LREC.
Andr?e F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and M?ario A. T. Figueiredo. 2010.
Turbo parsers: dependency parsing by approximate
variational inference. In Proc. of ENMLP, pages 34?
44.
Zachary J Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proc. of EMNLP.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013.
Exploiting similarities among languages for Ma-
chine Translation. CoRR, abs/1309.4168.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1?10:69,
February.
Yair Neuman, Dan Assaf, Yohai Cohen, Mark Last,
Shlomo Argamon, Newton Howard, and Ophir
Frieder. 2013. Metaphor identification in large texts
corpora. PloS one, 8(4):e62343.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825?2830.
Claudia Perlich, Foster Provost, and Jeffrey S. Si-
monoff. 2003. Tree induction vs. logistic regres-
sion: a learning-curve analysis. Journal of Machine
Learning Research, 4:211?255.
Nathan Schneider, Behrang Mohit, Chris Dyer, Kemal
Oflazer, and Noah A Smith. 2013. Supersense tag-
ging for Arabic: the MT-in-the-middle attack. In
Proc. of NAACL-HLT, pages 661?667.
Ekaterina Shutova and Lin Sun. 2013. Unsupervised
metaphor identification using hierarchical graph fac-
torization clustering. In Proc. of NAACL-HLT,
pages 978?988.
257
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source-target domain
mappings. In Proc. of LREC, pages 3255?3261.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proc. of COLING, pages 1002?1010.
Ekaterina Shutova, Simone Teufel, and Anna Korho-
nen. 2013. Statistical metaphor processing. Com-
putational Linguistics, 39(2):301?353.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proc. of ACL, pages 688?697.
Gerard J Steen, Aletta G Dorst, J Berenike Her-
rmann, Anna A Kaal, and Tina Krennmayr.
2010. Metaphor in usage. Cognitive Linguistics,
21(4):765?796.
Tomek Strzalkowski, George Aaron Broadwell, Sarah
Taylor, Laurie Feldman, Boris Yamrom, Samira
Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases,
et al 2013. Robust extraction of metaphors from
novel data. In Proc. of the First Workshop on
Metaphor in NLP, page 67.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. TACL, 1:1?12.
Paul H Thibodeau and Lera Boroditsky. 2011.
Metaphors we think with: The role of metaphor in
reasoning. PLoS One, 6(2):e16782.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection using
common semantic features. In The 1st Workshop on
Metaphor in NLP 2013, page 45.
Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, Archna
Bhatia, Manaal Faruqui, and Chris Dyer. 2014.
Augmenting English adjective senses with super-
senses. In Proc. of LREC.
Joseph Turian, Lev Ratinov, Yoshua Bengio, and Dan
Roth. 2009. A preliminary evaluation of word rep-
resentations for named-entity recognition. In NIPS
Workshop on Grammar Induction, Representation of
Language and Language Learning, pages 1?8.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proc. of ACL, pages
384?394.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proc. of EMNL, pages 680?690.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197?223.
Michael Wilson. 1988. MRC Psycholinguistic
Database: Machine-usable dictionary, version 2.00.
Behavior Research Methods, Instruments, & Com-
puters, 20(1):6?10.
258
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1426?1436,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Discriminative Graph-Based Parser
for the Abstract Meaning Representation
Jeffrey Flanigan Sam Thomson Jaime Carbonell Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jflanigan,sthomson,jgc,cdyer,nasmith}@cs.cmu.edu
Abstract
Abstract Meaning Representation (AMR)
is a semantic formalism for which a grow-
ing set of annotated examples is avail-
able. We introduce the first approach
to parse sentences into this representa-
tion, providing a strong baseline for fu-
ture improvement. The method is based
on a novel algorithm for finding a maxi-
mum spanning, connected subgraph, em-
bedded within a Lagrangian relaxation of
an optimization problem that imposes lin-
guistically inspired constraints. Our ap-
proach is described in the general frame-
work of structured prediction, allowing fu-
ture incorporation of additional features
and constraints, and may extend to other
formalisms as well. Our open-source sys-
tem, JAMR, is available at:
http://github.com/jflanigan/jamr
1 Introduction
Semantic parsing is the problem of mapping nat-
ural language strings into meaning representa-
tions. Abstract Meaning Representation (AMR)
(Banarescu et al, 2013; Dorr et al, 1998) is a
semantic formalism in which the meaning of a
sentence is encoded as a rooted, directed, acyclic
graph. Nodes represent concepts, and labeled di-
rected edges represent the relationships between
them?see Figure 1 for an example AMR graph.
The formalism is based on propositional logic and
neo-Davidsonian event representations (Parsons,
1990; Davidson, 1967). Although it does not
encode quantifiers, tense, or modality, the set of
semantic phenomena included in AMR were se-
lected with natural language applications?in par-
ticular, machine translation?in mind.
In this paper we introduce JAMR, the first pub-
lished system for automatic AMR parsing. The
system is based on a statistical model whose pa-
rameters are trained discriminatively using anno-
tated sentences in the AMR Bank corpus (Ba-
narescu et al, 2013). We evaluate using the
Smatch score (Cai and Knight, 2013), establishing
a baseline for future work.
The core of JAMR is a two-part algorithm
that first identifies concepts using a semi-Markov
model and then identifies the relations that ob-
tain between these by searching for the maximum
spanning connected subgraph (MSCG) from an
edge-labeled, directed graph representing all pos-
sible relations between the identified concepts. To
solve the latter problem, we introduce an appar-
ently novel O(|V |
2
log |V |) algorithm that is sim-
ilar to the maximum spanning tree (MST) algo-
rithms that are widely used for dependency pars-
ing (McDonald et al, 2005). Our MSCG algo-
rithm returns the connected subgraph with maxi-
mal sum of its edge weights from among all con-
nected subgraphs of the input graph. Since AMR
imposes additional constraints to ensure seman-
tic well-formedness, we use Lagrangian relaxation
(Geoffrion, 1974; Fisher, 2004) to augment the
MSCG algorithm, yielding a tractable iterative al-
gorithm that finds the optimal solution subject to
these constraints. In our experiments, we have
found this algorithm to converge 100% of the time
for the constraint set we use.
The approach can be understood as an alterna-
tive to parsing approaches using graph transduc-
ers such as (synchronous) hyperedge replacement
grammars (Chiang et al, 2013; Jones et al, 2012;
Drewes et al, 1997), in much the same way that
spanning tree algorithms are an alternative to us-
ing shift-reduce and dynamic programming algo-
rithms for dependency parsing.
1
While a detailed
1
To date, a graph transducer-based semantic
parser has not been published, although the Bolinas
toolkit (http://www.isi.edu/publications/
licensed-sw/bolinas/) contains much of the neces-
sary infrastructure.
1426
want-01
boy
visit-01
city
name
?New? ?York?
?City?
ARG0
ARG1
ARG0
ARG1
name
op1
op2 op3
(a) Graph.
(w / want-01
:ARG0 (b / boy)
:ARG1 (g / visit-01
:ARG0 b
:ARG1 (c / city
:name (n / name
:op1 "New"
:op2 "York"
:op3 "City"))))
(b) AMR annotation.
Figure 1: Two equivalent ways of representing the AMR
parse for the sentence, ?The boy wants to visit New York
City.?
comparison of these two approaches is beyond the
scope of this paper, we emphasize that?as has
been observed with dependency parsing?a diver-
sity of approaches can shed light on complex prob-
lems such as semantic parsing.
2 Notation and Overview
Our approach to AMR parsing represents an AMR
parse as a graph G = ?V,E?; vertices and edges
are given labels from sets L
V
and L
E
, respec-
tively. G is constructed in two stages. The first
stage identifies the concepts evoked by words and
phrases in an input sentence w = ?w
1
, . . . , w
n
?,
each w
i
a member of vocabulary W . The second
stage connects the concepts by adding L
E
-labeled
edges capturing the relations between concepts,
and selects a root in G corresponding to the focus
of the sentence w.
Concept identification (?3) involves segmenting
w into contiguous spans and assigning to each
span a graph fragment corresponding to a concept
from a concept set denoted F (or to ? for words
that evoke no concept). In ?5 we describe how
F is constructed. In our formulation, spans are
contiguous subsequences of w. For example, the
words ?New York City? can evoke the fragment
represented by
(c / city
:name (n / name
:op1 "New"
:op2 "York"
:op3 "City"))))
We use a sequence labeling algorithm to identify
concepts.
The relation identification stage (?4) is similar
to a graph-based dependency parser. Instead of
finding the maximum-scoring tree over words, it
finds the maximum-scoring connected subgraph
that preserves concept fragments from the first
stage, links each pair of vertices by at most one
edge, and is deterministic
2
with respect to a spe-
cial set of edge labels L
?
E
? L
E
. The set L
?
E
consists of the labels ARG0?ARG5, and does not
include labels such as MOD or MANNER, for ex-
ample. Linguistically, the determinism constraint
enforces that predicates have at most one semantic
argument of each type; this is discussed in more
detail in ?4.
To train the parser, spans of words must be la-
beled with the concept fragments they evoke. Al-
though AMR Bank does not label concepts with
the words that evoke them, it is possible to build
an automatic aligner (?5). The alignments are
used to construct the concept lexicon and to train
the concept identification and relation identifica-
tion stages of the parser (?6). Each stage is a
discriminatively-trained linear structured predic-
tor with rich features that make use of part-of-
speech tagging, named entity tagging, and depen-
dency parsing.
In ?7, we evaluate the parser against gold-
standard annotated sentences from the AMR Bank
corpus (Banarescu et al, 2013) under the Smatch
score (Cai and Knight, 2013), presenting the first
published results on automatic AMR parsing.
3 Concept Identification
The concept identification stage maps spans of
words in the input sentence w to concept graph
fragments from F , or to the empty graph fragment
?. These graph fragments often consist of just
one labeled concept node, but in some cases they
are larger graphs with multiple nodes and edges.
3
2
By this we mean that, at each node, there is at most one
outgoing edge with that label type.
3
About 20% of invoked concept fragments are multi-
concept fragments.
1427
Concept identification is illustrated in Figure 2 us-
ing our running example, ?The boy wants to visit
New York City.?
Let the concept lexicon be a mapping clex :
W
?
? 2
F
that provides candidate graph frag-
ments for sequences of words. (The construc-
tion of F and clex is discussed below.) Formally,
a concept labeling is (i) a segmentation of w
into contiguous spans represented by boundaries
b, giving spans ?w
b
0
:b
1
,w
b
1
:b
2
, . . .w
b
k?1
:b
k
?, with
b
0
= 0 and b
k
= n, and (ii) an assignment of
each phrase w
b
i?1
:b
i
to a concept graph fragment
c
i
? clex (w
b
i?1
:b
i
) ? ?.
Our approach scores a sequence of spans b and
a sequence of concept graph fragments c, both of
arbitrary length k, using the following locally de-
composed, linearly parameterized function:
score(b, c;?) =
?
k
i=1
?
>
f(w
b
i?1
:b
i
, b
i?1
, b
i
, c
i
)
(1)
where f is a feature vector representation of a span
and one of its concept graph fragments in context.
The features are:
? Fragment given words: Relative frequency es-
timates of the probability of a concept graph
fragment given the sequence of words in the
span. This is calculated from the concept-word
alignments in the training corpus (?5).
? Length of the matching span (number of to-
kens).
? NER: 1 if the named entity tagger marked the
span as an entity, 0 otherwise.
? Bias: 1 for any concept graph fragment from F
and 0 for ?.
Our approach finds the highest-scoring b and
c using a dynamic programming algorithm: the
zeroth-order case of inference under a semi-
Markov model (Janssen and Limnios, 1999). Let
S(i) denote the score of the best labeling of the
first i words of the sentence, w
0:i
; it can be calcu-
lated using the recurrence:
S(0) = 0
S(i) = max
j:0?j<i,
c?clex(w
j:i
)??
{
S(j) + ?
>
f(w
j:i
, j, i, c)
}
The best score will be S(n), and the best scor-
ing concept labeling can be recovered using back-
pointers, as in typical implementations of the
Viterbi algorithm. Runtime is O(n
2
).
clex is implemented as follows. When clex is
called with a sequence of words, it looks up the
sequence in a table that contains, for every word
sequence that was labeled with a concept fragment
in the training data, the set of concept fragments it
was labeled with. clex also has a set of rules for
generating concept fragments for named entities
and time expressions. It generates a concept frag-
ment for any entity recognized by the named entity
tagger, as well as for any word sequence matching
a regular expression for a time expression. clex
returns the union of all these concept fragments.
4 Relation Identification
The relation identification stage adds edges among
the concept subgraph fragments identified in the
first stage (?3), creating a graph. We frame the
task as a constrained combinatorial optimization
problem.
Consider the fully dense labeled multigraph
D = ?V
D
, E
D
? that includes the union of all la-
beled vertices and labeled edges in the concept
graph fragments, as well as every possible labeled
edge u
`
?? v, for all u, v ? V
D
and every ` ? L
E
.
4
We require a subgraph G = ?V
G
, E
G
? that re-
spects the following constraints:
1. Preserving: all graph fragments (including la-
bels) from the concept identification phase are
subgraphs of G.
2. Simple: for any two vertices u and v ? V
G
,E
G
includes at most one edge between u and v. This
constraint forbids a small number of perfectly
valid graphs, for example for sentences such as
?John hurt himself?; however, we see that< 1%
of training instances violate the constraint. We
found in preliminary experiments that including
the constraint increases overall performance.
5
3. Connected: G must be weakly connected (ev-
ery vertex reachable from every other vertex, ig-
noring the direction of edges). This constraint
follows from the formal definition of AMR and
is never violated in the training data.
4. Deterministic: For each node u ? V
G
, and for
each label ` ? L
?
E
, there is at most one outgoing
edge in E
G
from u with label `. As discussed in
?2, this constraint is linguistically motivated.
4
To handle numbered OP labels, we pre-process the train-
ing data to convert OPN to OP, and post-process the output by
numbering the OP labels sequentially.
5
In future work it might be treated as a soft constraint, or
the constraint might be refined to specific cases.
1428
The
boy
wants to
visit New York
City
? ?
boy
want-01
visit-01
city
name
?New?
?York?
?City?
name
op1
op2
op3
Figure 2: A concept labeling for the sentence ?The boy wants to visit New York City.?
One constraint we do not include is acyclicity,
which follows from the definition of AMR. In
practice, graphs with cycles are rarely produced
by JAMR. In fact, none of the graphs produced on
the test set violate acyclicity.
Given the constraints, we seek the maximum-
scoring subgraph. We define the score to decom-
pose by edges, and with a linear parameterization:
score(E
G
;?) =
?
e?E
G
?
>
g(e) (2)
The features are shown in Table 1.
Our solution to maximizing the score in Eq. 2,
subject to the constraints, makes use of (i) an al-
gorithm that ignores constraint 4 but respects the
others (?4.1); and (ii) a Lagrangian relaxation that
iteratively adjusts the edge scores supplied to (i)
so as to enforce constraint 4 (?4.2).
4.1 Maximum Preserving, Simple, Spanning,
Connected Subgraph Algorithm
The steps for constructing a maximum preserving,
simple, spanning, connected (but not necessar-
ily deterministic) subgraph are as follows. These
steps ensure the resulting graph G satisfies the
constraints: the initialization step ensures the pre-
serving constraint is satisfied, the pre-processing
step ensures the graph is simple, and the core al-
gorithm ensures the graph is connected.
1. (Initialization) Let E
(0)
be the union of the
concept graph fragments? weighted, labeled, di-
rected edges. Let V denote its set of vertices.
Note that ?V,E
(0)
? is preserving (constraint 4),
as is any graph that contains it. It is also sim-
ple (constraint 4), assuming each concept graph
fragment is simple.
2. (Pre-processing) We form the edge set E by in-
cluding just one edge from E
D
between each
pair of nodes:
? For any edge e = u
`
?? v in E
(0)
, include e in
E, omitting all other edges between u and v.
? For any two nodes u and v, include only the
highest scoring edge between u and v.
Note that without the deterministic constraint,
we have no constraints that depend on the label
of an edge, nor its direction. So it is clear that
the edges omitted in this step could not be part
of the maximum-scoring solution, as they could
be replaced by a higher scoring edge without vi-
olating any constraints.
Note also that because we have kept exactly one
edge between every pair of nodes, ?V,E? is sim-
ple and connected.
3. (Core algorithm) Run Algorithm 1, MSCG, on
?V,E? and E
(0)
. This algorithm is a (to our
knowledge novel) modification of the minimum
spanning tree algorithm of Kruskal (1956).
Note that the directions of edges do not matter
for MSCG.
Steps 1?2 can be accomplished in one pass
through the edges, with runtime O(|V |
2
). MSCG
can be implemented efficiently in O(|V |
2
log |V |)
time, similarly to Kruskal?s algorithm, using a
disjoint-set data structure to keep track of con-
nected components.
6
The total asymptotic runtime
complexity is O(|V |
2
log |V |).
The details of MSCG are given in Algorithm 1.
In a nutshell, MSCG first adds all positive edges to
the graph, and then connects the graph by greedily
adding the least negative edge that connects two
previously unconnected components.
Theorem 1. MSCG finds a maximum spanning,
connected subgraph of ?V,E?
Proof. We closely follow the original proof of cor-
rectness of Kruskal?s algorithm. We first show by
induction that, at every iteration of MSCG, there
exists some maximum spanning, connected sub-
graph that contains G
(i)
= ?V,E
(i)
?:
6
For dense graphs, Prim?s algorithm (Prim, 1957) is
asymptotically faster (O(|V |
2
)). We conjecture that using
Prim?s algorithm instead of Kruskall?s to connect the graph
could improve the runtime of MSCG.
1429
Name Description
Label For each ` ? L
E
, 1 if the edge has that label
Self edge 1 if the edge is between two nodes in the same fragment
Tail fragment root 1 if the edge?s tail is the root of its graph fragment
Head fragment root 1 if the edge?s head is the root of its graph fragment
Path Dependency edge labels and parts of speech on the shortest syntactic path between any two
words in the two spans
Distance Number of tokens (plus one) between the two concepts? spans (zero if the same)
Distance indicators A feature for each distance value, that is 1 if the spans are of that distance
Log distance Logarithm of the distance feature plus one.
Bias 1 for any edge.
Table 1: Features used in relation identification. In addition to the features above, the following conjunctions are used (Tail and
Head concepts are elements of L
V
): Tail concept ? Label, Head concept ? Label, Path ? Label, Path ? Head concept, Path ?
Tail concept, Path ? Head concept ? Label, Path ? Tail concept ? Label, Path ? Head word, Path ? Tail word, Path ? Head
word ? Label, Path ? Tail word ? Label, Distance ? Label, Distance ? Path, and Distance ? Path ? Label. To conjoin the
distance feature with anything else, we multiply by the distance.
input : weighted, connected graph ?V,E?
and set of edges E
(0)
? E to be
preserved
output: maximum spanning, connected
subgraph of ?V,E? that preserves
E
(0)
let E
(1)
= E
(0)
? {e ? E | ?
>
g(e) > 0};
create a priority queue Q containing
{e ? E | ?
>
g(e) ? 0} prioritized by scores;
i = 1;
while Q nonempty and ?V,E
(i)
? is not yet
spanning and connected do
i = i+ 1;
E
(i)
= E
(i?1)
;
e = argmax
e
?
?Q
?
>
g(e
?
);
remove e from Q;
if e connects two previously unconnected
components of ?V,E
(i)
? then
add e to E
(i)
end
end
return G = ?V,E
(i)
?;
Algorithm 1: MSCG algorithm.
Base case: ConsiderG
(1)
, the subgraph contain-
ing E
(0)
and every positive edge. Take any maxi-
mum preserving spanning connected subgraph M
of ?V,E?. We know that such an M exists be-
cause ?V,E? itself is a preserving spanning con-
nected subgraph. Adding a positive edge to M
would strictly increase M ?s score without discon-
necting M , which would contradict the fact that
M is maximal. Thus M must contain G
(1)
.
Induction step: By the inductive hypothesis,
there exists some maximum spanning connected
subgraph M = ?V,E
M
? that contains G
(i)
.
Let e be the next edge added to E
(i)
by MSCG.
If e is in E
M
, then E
(i+1)
= E
(i)
? {e} ? E
M
,
and the hypothesis still holds.
Otherwise, since M is connected and does not
contain e, E
M
? {e} must have a cycle containing
e. In addition, that cycle must have some edge e
?
that is not in E
(i)
. Otherwise, E
(i)
? {e} would
contain a cycle, and e would not connect two un-
connected components of G
(i)
, contradicting the
fact that e was chosen by MSCG.
Since e
?
is in a cycle in E
M
? {e}, removing it
will not disconnect the subgraph, i.e. (E
M
?{e})\
{e
?
} is still connected and spanning. The score of
e is greater than or equal to the score of e
?
, oth-
erwise MSCG would have chosen e
?
instead of e.
Thus, ?V, (E
M
?{e}) \ {e
?
}? is a maximum span-
ning connected subgraph that containsE
(i+1)
, and
the hypothesis still holds.
When the algorithm completes, G = ?V,E
(i)
?
is a spanning connected subgraph. The maximum
spanning connected subgraph M that contains it
cannot have a higher score, because G contains
every positive edge. Hence G is maximal.
4.2 Lagrangian Relaxation
If the subgraph resulting from MSCG satisfies con-
straint 4 (deterministic) then we are done. Oth-
erwise we resort to Lagrangian relaxation (LR).
Here we describe the technique as it applies to our
task, referring the interested reader to Rush and
Collins (2012) for a more general introduction to
Lagrangian relaxation in the context of structured
prediction problems.
In our case, we begin by encoding a graph G =
?V
G
, E
G
? as a binary vector. For each edge e in
the fully dense multigraph D, we associate a bi-
1430
nary variable z
e
= 1{e ? E
G
}, where 1{P} is
the indicator function, taking value 1 if the propo-
sition P is true, 0 otherwise. The collection of z
e
form a vector z ? {0, 1}
|E
D
|
.
Determinism constraints can be encoded as a
set of linear inequalities. For example, the con-
straint that vertex u has no more than one outgoing
ARG0 can be encoded with the inequality:
?
v?V
1{u
ARG0
???? v ? E
G
} =
?
v?V
z
u
ARG0
????v
? 1.
All of the determinism constraints can collectively
be encoded as one system of inequalities:
Az ? b,
with each row A
i
inA and its corresponding entry
b
i
in b together encoding one constraint. For the
previous example we have a row A
i
that has 1s
in the columns corresponding to edges outgoing
from u with label ARG0 and 0?s elsewhere, and a
corresponding element b
i
= 1 in b.
The score of graph G (encoded as z) can be
written as the objective function ?
>
z, where ?
e
=
?
>
g(e). To handle the constraint Az ? b, we in-
troduce multipliers ? ? 0 to get the Lagrangian
relaxation of the objective function:
L?(z) = maxz (?
>
z+ ?
>
(b?Az)),
z
?
? = argmaxz L?(z).
And the dual objective:
L(z) = min
??0
L?(z),
z
?
= argmax
z
L(z).
Conveniently, L?(z) decomposes over edges:
L?(z) = maxz (?
>
z+ ?
>
(b?Az))
= max
z
(?
>
z? ?
>
Az)
= max
z
((??A
>
?)
>
z).
So for any ?, we can find z
?
?
by assigning edges
the new Lagrangian adjusted weights ? ? A
>
?
and reapplying the algorithm described in ?4.1.
We can find z
?
by projected subgradient descent,
by starting with ? = 0, and taking steps in the
direction:
?
?L?
??
(z
?
?) = Az
?
?.
If any components of ? are negative after taking a
step, they are set to zero.
L(z) is an upper bound on the unrelaxed ob-
jective function ?
>
z, and is equal to it if and
only if the constraints Az ? b are satisfied. If
L(z
?
) = ?
>
z
?
, then z
?
is also the optimal solu-
tion to the constrained solution. Otherwise, there
exists a duality gap, and Lagrangian relaxation
has failed. In that case we still return the sub-
graph encoded by z
?
, even though it might vio-
late one or more constraints. Techniques from in-
teger programming such as branch-and-bound or
cutting-planes methods could be used to find an
optimal solution when LR fails (Das et al, 2012),
but we do not use these techniques here. In our
experiments, with a stepsize of 1 and max number
of steps as 500, Lagrangian relaxation succeeds
100% of the time in our data.
4.3 Focus Identification
In AMR, one node must be marked as the focus of
the sentence. We notice this can be accomplished
within the relation identification step: we add a
special concept node root to the dense graph D,
and add an edge from root to every other node,
giving each of these edges the label FOCUS. We
require that root have at most one outgoing FO-
CUS edge. Our system has two feature types for
this edge: the concept it points to, and the shortest
dependency path from a word in the span to the
root of the dependency tree.
5 Automatic Alignments
In order to train the parser, we need alignments be-
tween sentences in the training data and their an-
notated AMR graphs. More specifically, we need
to know which spans of words invoke which con-
cept fragments in the graph. To do this, we built
an automatic aligner and tested its performance on
a small set of alignments we annotated by hand.
The automatic aligner uses a set of rules to
greedily align concepts to spans. The list of rules
is given in Table 2. The aligner proceeds down
the list, first aligning named-entities exactly, then
fuzzy matching named-entities, then date-entities,
etc. For each rule, an entire pass through the AMR
graph is done. The pass considers every concept in
the graph and attempts to align a concept fragment
rooted at that concept if the rule can apply. Some
rules only apply to a particular type of concept
fragment, while others can apply to any concept.
For example, rule 1 can apply to any NAME con-
cept and its OP children. It searches the sentence
1431
for a sequence of words that exactly matches its
OP children and aligns them to the NAME and OP
children fragment.
Concepts are considered for alignment in the or-
der they are listed in the AMR annotation (left to
right, top to bottom). Concepts that are not aligned
in a particular pass may be aligned in subsequent
passes. Concepts are aligned to the first match-
ing span, and alignments are mutually exclusive.
Once aligned, a concept in a fragment is never re-
aligned.
7
However, more concepts can be attached
to the fragment by rules 8?14.
We use WordNet to generate candidate lemmas,
and we also use a fuzzy match of a concept, de-
fined to be a word in the sentence that has the
longest string prefix match with that concept?s la-
bel, if the match length is ? 4. If the match length
is < 4, then the concept has no fuzzy match. For
example the fuzzy match for ACCUSE-01 could be
?accusations? if it is the best match in the sen-
tence. WordNet lemmas and fuzzy matches are
only used if the rule explicitly uses them. All to-
kens and concepts are lowercased before matches
or fuzzy matches are done.
On the 200 sentences of training data we
aligned by hand, the aligner achieves 92% preci-
sion, 89% recall, and 90% F
1
for the alignments.
6 Training
We now describe how to train the two stages of the
parser. The training data for the concept identifi-
cation stage consists of (X,Y ) pairs:
? Input: X , a sentence annotated with named
entities (person, organization, location, mis-
ciscellaneous) from the Illinois Named Entity
Tagger (Ratinov and Roth, 2009), and part-of-
speech tags and basic dependencies from the
Stanford Parser (Klein and Manning, 2003; de
Marneffe et al, 2006).
? Output: Y , the sentence labeled with concept
subgraph fragments.
The training data for the relation identification
stage consists of (X,Y ) pairs:
7
As an example, if ?North Korea? shows up twice in
the AMR graph and twice in the input sentence, then the
first ?North Korea? concept fragment listed in the AMR gets
aligned to the first ?North Korea? mention in the sentence,
and the second fragment to the second mention (because the
first span is already aligned when the second ?North Korea?
concept fragment is considered, so it is aligned to the second
matching span).
1. (Named Entity) Applies to name concepts and their
opn children. Matches a span that exactly matches its
opn children in numerical order.
2. (Fuzzy Named Entity) Applies to name concepts and
their opn children. Matches a span that matches the
fuzzy match of each child in numerical order.
3. (Date Entity) Applies to date-entity concepts
and their day, month, year children (if exist).
Matches any permutation of day, month, year, (two digit
or four digit years), with or without spaces.
4. (Minus Polarity Tokens) Applies to - concepts, and
matches ?no?, ?not?, ?non.?
5. (Single Concept) Applies to any concept. Strips
off trailing ?-[0-9]+? from the concept (for example
run-01 ? run), and matches any exact matching
word or WordNet lemma.
6. (Fuzzy Single Concept) Applies to any concept.
Strips off trailing ?-[0-9]+?, and matches the fuzzy match
of the concept.
7. (U.S.) Applies to name if its op1 child is united
and its op2 child is states. Matches a word that
matches ?us?, ?u.s.? (no space), or ?u. s.? (with space).
8. (Entity Type) Applies to concepts with an outgoing
name edge whose head is an aligned fragment. Up-
dates the fragment to include the unaligned concept.
Ex: continent in (continent :name (name
:op1 "Asia")) aligned to ?asia.?
9. (Quantity) Applies to .
*
-quantity concepts with
an outgoing unit edge whose head is aligned. Up-
dates the fragment to include the unaligned concept. Ex:
distance-quantity in (distance-quantity
:unit kilometer) aligned to ?kilometres.?
10. (Person-Of, Thing-Of) Applies to person and
thing concepts with an outgoing .
*
-of edge whose
head is aligned. Updates the fragment to include
the unaligned concept. Ex: person in (person
:ARG0-of strike-02) aligned to ?strikers.?
11. (Person) Applies to person concepts with a sin-
gle outgoing edge whose head is aligned. Updates
the fragment to include the unaligned concept. Ex:
person in (person :poss (country :name
(name :op1 "Korea")))
12. (Goverment Organization) Applies to concepts
with an incoming ARG.
*
-of edge whose tail is an
aligned government-organization concept. Up-
dates the fragment to include the unaligned concept. Ex:
govern-01 in (government-organization
:ARG0-of govern-01) aligned to ?government.?
13. (Minus Polarity Prefixes) Applies to - concepts
with an incoming polarity edge whose tail is aligned
to a word beginning with ?un?, ?in?, or ?il.? Up-
dates the fragment to include the unaligned concept.
Ex: - in (employ-01 :polarity -) aligned to
?unemployment.?
14. (Degree) Applies to concepts with an incoming
degree edge whose tail is aligned to a word ending
is ?est.? Updates the fragment to include the unaligned
concept. Ex: most in (large :degree most)
aligned to ?largest.?
Table 2: Rules used in the automatic aligner.
1432
? Input: X , the sentence labeled with graph frag-
ments, as well as named enties, POS tags, and
basic dependencies as in concept identification.
? Output: Y , the sentence with a full AMR
parse.
8
Alignments are used to induce the concept label-
ing for the sentences, so no annotation beyond the
automatic alignments is necessary.
We train the parameters of the stages separately
using AdaGrad (Duchi et al, 2011) with the per-
ceptron loss function (Rosenblatt, 1957; Collins,
2002). We give equations for concept identifica-
tion parameters ? and features f(X,Y ). For a
sentence of length k, and spans b labeled with a
sequence of concept fragments c, the features are:
f(X,Y ) =
?
k
i=1
f(w
b
i?1
:b
i
, b
i?1
, b
i
, c
i
)
To train with AdaGrad, we process examples in
the training data ((X
1
, Y
1
), . . . , (X
N
, Y
N
)) one
at a time. At time t, we decode (?3) to get
?
Y
t
and
compute the subgradient:
s
t
= f(X
t
,
?
Y
t
)? f(X
t
, Y
t
)
We then update the parameters and go to the next
example. Each component i of the parameter vec-
tor gets updated like so:
?
t+1
i
= ?
t
i
?
?
?
?
t
t
?
=1
s
t
?
i
s
t
i
? is the learning rate which we set to 1. For
relation identification training, we replace ? and
f(X,Y ) in the above equations with ? and
g(X,Y ) =
?
e?E
G
g(e).
We ran AdaGrad for ten iterations for concept
identification, and five iterations for relation iden-
tification. The number of iterations was chosen by
early stopping on the development set.
7 Experiments
We evaluate our parser on the newswire section
of LDC2013E117 (deft-amr-release-r3-proxy.txt).
Statistics about this corpus and our train/dev./test
splits are given in Table 3.
8
Because the alignments are automatic, some concepts
may not be aligned, so we cannot compute their features. We
remove the unaligned concepts and their edges from the full
AMR graph for training. Thus some graphs used for training
may in fact be disconnected.
Split Document Years Sentences Tokens
Train 1995-2006 4.0k 79k
Dev. 2007 2.1k 40k
Test 2008 2.1k 42k
Table 3: Train/dev./test split.
Train Test
P R F
1
P R F
1
.92 .90 .91 .90 .79 .84
Table 4: Concept identification performance.
For the performance of concept identification,
we report precision, recall, and F
1
of labeled spans
using the induced labels on the training and test
data as a gold standard (Table 4). Our concept
identifier achieves 84% F
1
on the test data. Pre-
cision is roughly the same between train and test,
but recall is worse on test, implicating unseen con-
cepts as a significant source of errors on test data.
We evaluate the performance of the full parser
using Smatch v1.0 (Cai and Knight, 2013), which
counts the precision, recall and F
1
of the concepts
and relations together. Using the full pipeline
(concept identification and relation identification
stages), our parser achieves 58% F
1
on the test
data (Table 5). Using gold concepts with the re-
lation identification stage yields a much higher
Smatch score of 80% F
1
. As a comparison, AMR
Bank annotators have a consensus inter-annotator
agreement Smatch score of 83% F
1
. The runtime
of our system is given in Figure 3.
The large drop in performance of 22% F
1
when
moving from gold concepts to system concepts
suggests that joint inference and training for the
two stages might be helpful.
8 Related Work
Our approach to relation identification is inspired
by graph-based techniques for non-projective syn-
tactic dependency parsing. Minimum span-
ning tree algorithms?specifically, the optimum
branching algorithm of Chu and Liu (1965) and
Edmonds (1967)?were first used for dependency
parsing by McDonald et al (2005). Later ex-
Train Test
concepts P R F
1
P R F
1
gold .85 .95 .90 .76 .84 .80
automatic .69 .78 .73 .52 .66 .58
Table 5: Parser performance.
1433
0 10 20 30 400.0
0.1
0.2
0.3
0.4
0.5
sentence length (words)
aver
age r
untim
e (seco
nds)
Figure 3: Runtime of JAMR (all stages).
tensions allow for higher-order (non?edge-local)
features, often making use of relaxations to solve
the NP-hard optimization problem. Mcdonald and
Pereira (2006) incorporated second-order features,
but resorted to an approximate algorithm. Oth-
ers have formulated the problem as an integer lin-
ear program (Riedel and Clarke, 2006; Martins et
al., 2009). TurboParser (Martins et al, 2013) uses
AD
3
(Martins et al, 2011), a type of augmented
Lagrangian relaxation, to integrate third-order fea-
tures into a CLE backbone. Future work might ex-
tend JAMR to incorporate additional linguistically
motivated constraints and higher-order features.
The task of concept identification is similar in
form to the problem of Chinese word segmenta-
tion, for which semi-Markov models have success-
fully been used to incorporate features based on
entire spans (Andrew, 2006).
While all semantic parsers aim to transform nat-
ural language text to a formal representation of
its meaning, there is wide variation in the mean-
ing representations and parsing techniques used.
Space does not permit a complete survey, but we
note some connections on both fronts.
Interlinguas (Carbonell et al, 1992) are an im-
portant precursor to AMR. Both formalisms are
intended for use in machine translation, but AMR
has an admitted bias toward the English language.
First-order logic representations (and exten-
sions using, e.g., the ?-calculus) allow variable
quantification, and are therefore more power-
ful. In recent research, they are often associ-
ated with combinatory categorial grammar (Steed-
man, 1996). There has been much work on sta-
tistical models for CCG parsing (Zettlemoyer and
Collins, 2005; Zettlemoyer and Collins, 2007;
Kwiatkowski et al, 2010, inter alia), usually using
chart-based dynamic programming for inference.
Natural language interfaces for querying
databases have served as another driving applica-
tion (Zelle and Mooney, 1996; Kate et al, 2005;
Liang et al, 2011, inter alia). The formalisms
used here are richer in logical expressiveness than
AMR, but typically use a smaller set of concept
types?only those found in the database.
In contrast, semantic dependency parsing?in
which the vertices in the graph correspond to the
words in the sentence?is meant to make semantic
parsing feasible for broader textual domains. Al-
shawi et al (2011), for example, use shift-reduce
parsing to map sentences to natural logical form.
AMR parsing also shares much in common
with tasks like semantic role labeling and frame-
semantic parsing (Gildea and Jurafsky, 2002; Pun-
yakanok et al, 2008; Das et al, 2014, inter alia).
In these tasks, predicates are often disambiguated
to a canonical word sense, and roles are filled
by spans (usually syntactic constituents). They
consider each predicate separately, and produce
a disconnected set of shallow predicate-argument
structures. AMR, on the other hand, canonical-
izes both predicates and arguments to a common
concept label space. JAMR reasons about all con-
cepts jointly to produce a unified representation of
the meaning of an entire sentence.
9 Conclusion
We have presented the first published system for
automatic AMR parsing, and shown that it pro-
vides a strong baseline based on the Smatch eval-
uation metric. We also present an algorithm for
finding the maximum, spanning, connected sub-
graph and show how to incorporate extra con-
straints with Lagrangian relaxation. Our feature-
based learning setup allows the system to be easily
extended by incorporating new feature sources.
Acknowledgments
The authors gratefully acknowledge helpful cor-
respondence from Kevin Knight, Ulf Hermjakob,
and Andr?e Martins, and helpful feedback from
Nathan Schneider, Brendan O?Connor, Waleed
Ammar, and the anonymous reviewers. This
work was sponsored by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533
and DARPA grant FA8750-12-2-0342 funded un-
der the DEFT program.
1434
References
Hiyan Alshawi, Pi-Chuan Chang, and Michael Ring-
gaard. 2011. Deterministic statistical mapping of
sentences to underspecified semantics. In Proc. of
ICWS.
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In Proc. of EMNLP.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract meaning representation
for sembanking. In Proc. of the Linguistic Annota-
tion Workshop and Interoperability with Discourse.
Shu Cai and Kevin Knight. 2013. Smatch: an eval-
uation metric for semantic feature structures. In
Proc. of ACL.
Jaime G. Carbonell, Teruko Mitamura, and Eric H. Ny-
berg. 1992. The KANT perspective: A critique
of pure transfer (and pure interlingua, pure trans-
fer, . . . ). In Proc. of the Fourth International Con-
ference on Theoretical and Methodological Issues
in Machine Translation: Empiricist vs. Rationalist
Methods in MT.
David Chiang, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, Bevan Jones, and Kevin
Knight. 2013. Parsing graphs with hyperedge
replacement grammars. In Proc. of ACL.
Y. J. Chu and T. H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In Proc. of
EMNLP.
Dipanjan Das, Andr?e F. T. Martins, and Noah A. Smith.
2012. An exact dual decomposition algorithm for
shallow semantic parsing with constraints. In Proc.
of the Joint Conference on Lexical and Computa-
tional Semantics.
Dipanjan Das, Desai Chen, Andr?e F. T. Martins,
Nathan Schneider, and Noah A. Smith. 2014.
Frame-semantic parsing. Computational Linguis-
tics, 40(1):9?56.
Donald Davidson. 1967. The logical form of action
sentences. In Nicholas Rescher, editor, The Logic of
Decision and Action, pages 81?120. Univ. of Pitts-
burgh Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. of LREC.
Bonnie Dorr, Nizar Habash, and David Traum. 1998.
A thematic hierarchy for efficient generation from
lexical-conceptual structure. In David Farwell, Lau-
rie Gerber, and Eduard Hovy, editors, Machine
Translation and the Information Soup: Proc. of
AMTA.
Frank Drewes, Hans-J?org Kreowski, and Annegret Ha-
bel. 1997. Hyperedge replacement graph gram-
mars. In Handbook of Graph Grammars, pages 95?
162. World Scientific.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine
Learning Research, 12:2121?2159, July.
Jack Edmonds. 1967. Optimum branchings. National
Bureau of Standards.
Marshall L. Fisher. 2004. The Lagrangian relaxation
method for solving integer programming problems.
Management Science, 50(12):1861?1871.
Arthur M Geoffrion. 1974. Lagrangean relaxation for
integer programming. Springer.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
Jacques Janssen and Nikolaos Limnios. 1999. Semi-
Markov Models and Applications. Springer, Octo-
ber.
Bevan Jones, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
Semantics-based machine translation with hyper-
edge replacement grammars. In Proc. of COLING.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to
formal languages. In Proc. of AAAI.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proc. of ACL.
Joseph B. Kruskal. 1956. On the shortest spanning
subtree of a graph and the traveling salesman prob-
lem. Proc. of the American Mathematical Society,
7(1):48.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proc. of EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proc. of ACL.
Andr?e F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proc. of ACL.
1435
Andr?e F. T. Martins, Noah A. Smith, Pedro M. Q.
Aguiar, and M?ario A. T. Figueiredo. 2011. Dual
decomposition with many overlapping components.
In Proc. of EMNLP.
Andr?e F. T. Martins, Miguel Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective Turbo parsers. In Proc. of ACL.
Ryan Mcdonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proc. of EACL, page 81?88.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Haji?c. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proc. of
EMNLP.
Terence Parsons. 1990. Events in the Semantics of En-
glish: A study in subatomic semantics. MIT Press.
Robert C. Prim. 1957. Shortest connection networks
and some generalizations. Bell System Technology
Journal, 36:1389?1401.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. of CoNLL.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective de-
pendency parsing. In Proc. of EMNLP.
Frank Rosenblatt. 1957. The perceptron?a perceiving
and recognizing automaton. Technical Report 85-
460-1, Cornell Aeronautical Laboratory.
Alexander M. Rush and Michael Collins. 2012. A
tutorial on dual decomposition and Lagrangian re-
laxation for inference in natural language process-
ing. Journal of Artificial Intelligence Research,
45(1):305?-362.
Mark Steedman. 1996. Surface structure and interpre-
tation. Linguistic inquiry monographs. MIT Press.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proc. of AAAI.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proc. of UAI.
Luke S. Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In In Proc. of EMNLP-CoNLL.
1436
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828?834,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Distributed Representations of Geographically Situated Language
David Bamman Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{dbamman,cdyer,nasmith}@cs.cmu.edu
Abstract
We introduce a model for incorporating
contextual information (such as geogra-
phy) in learning vector-space representa-
tions of situated language. In contrast to
approaches to multimodal representation
learning that have used properties of the
object being described (such as its color),
our model includes information about the
subject (i.e., the speaker), allowing us to
learn the contours of a word?s meaning
that are shaped by the context in which
it is uttered. In a quantitative evaluation
on the task of judging geographically in-
formed semantic similarity between repre-
sentations learned from 1.1 billion words
of geo-located tweets, our joint model out-
performs comparable independent models
that learn meaning in isolation.
1 Introduction
The vast textual resources used in NLP ?
newswire, web text, parliamentary proceedings ?
can encourage a view of language as a disembod-
ied phenomenon. The rise of social media, how-
ever, with its large volume of text paired with in-
formation about its author and social context, re-
minds us that each word is uttered by a particular
person at a particular place and time. In short: lan-
guage is situated.
The coupling of text with demographic infor-
mation has enabled computational modeling of
linguistic variation, including uncovering words
and topics that are characteristic of geographical
regions (Eisenstein et al, 2010; O?Connor et al,
2010; Hong et al, 2012; Doyle, 2014), learning
correlations between words and socioeconomic
variables (Rao et al, 2010; Eisenstein et al, 2011;
Pennacchiotti and Popescu, 2011; Bamman et al,
2014); and charting how new terms spread geo-
graphically (Eisenstein et al, 2012). These models
can tell us that hella was (at one time) used most
often by a particular demographic group in north-
ern California, echoing earlier linguistic studies
(Bucholtz, 2006), and that wicked is used most
often in New England (Ravindranath, 2011); and
they have practical applications, facilitating tasks
like text-based geolocation (Wing and Baldridge,
2011; Roller et al, 2012; Ikawa et al, 2012).
One desideratum that remains, however, is how the
meaning of these terms is shaped by geographical
influences ? while wicked is used throughout the
United States to mean bad or evil (?he is a wicked
man?), in New England it is used as an adverbial
intensifier (?my boy?s wicked smart?). In lever-
aging grounded social media to uncover linguistic
variation, what we want to learn is how a word?s
meaning is shaped by its geography.
In this paper, we introduce a method that ex-
tends vector-space lexical semantic models to
learn representations of geographically situated
language. Vector-space models of lexical seman-
tics have been a popular and effective approach
to learning representations of word meaning (Lin,
1998; Turney and Pantel, 2010; Reisinger and
Mooney, 2010; Socher et al, 2013; Mikolov et al,
2013, inter alia). In bringing in extra-linguistic in-
formation to learn word representations, our work
falls into the general domain of multimodal learn-
ing; while other work has used visual informa-
tion to improve distributed representations (An-
drews et al, 2009; Feng and Lapata, 2010; Bruni
et al, 2011; Bruni et al, 2012a; Bruni et al,
2012b; Roller and im Walde, 2013), this work
generally exploits information about the object be-
ing described (e.g., strawberry and a picture of a
strawberry); in contrast, we use information about
the speaker to learn representations that vary ac-
cording to contextual variables from the speaker?s
perspective. Unlike classic multimodal systems
that incorporate multiple active modalities (such
as gesture) from a user (Oviatt, 2003; Yu and
828
...
W
X
Main
Alabama Alaska
Arizona
Arkansas
h
o
Figure 1: Model. Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item
#2) spoken in Alaska, along with a single output. Parameter matrixW consists of the learned low-dimensional embeddings.
Ballard, 2004), our primary input is textual data,
supplemented with metadata about the author and
the moment of authorship. This information en-
ables learning models of word meaning that are
sensitive to such factors, allowing us to distin-
guish, for example, between the usage of wicked
in Massachusetts from the usage of that word else-
where, and letting us better associate geographi-
cally grounded named entities (e.g, Boston) with
their hypernyms (city) in their respective regions.
2 Model
The model we introduce is grounded in the distri-
butional hypothesis (Harris, 1954), that two words
are similar by appearing in the same kinds of con-
texts (where ?context? itself can be variously de-
fined as the bag or sequence of tokens around a tar-
get word, either by linear distance or dependency
path). We can invoke the distributional hypothe-
sis for many instances of regional variation by ob-
serving that such variants often appear in similar
contexts. For example:
? my boy?s wicked smart
? my boy?s hella smart
? my boy?s very smart
Here, all three variants can often be seen in an im-
mediately pre-adjectival position (as is common
with intensifying adverbs).
Given the empirical success of vector-space rep-
resentations in capturing semantic properties and
their success at a variety of NLP tasks (Turian et
al., 2010; Socher et al, 2011; Collobert et al,
2011; Socher et al, 2013), we use a simple, but
state-of-the-art neural architecture (Mikolov et al,
2013) to learn low-dimensional real-valued repre-
sentations of words. The graphical form of this
model is illustrated in figure 1.
This model corresponds to an extension of
the ?skip-gram? language model (Mikolov et al,
2013) (hereafter SGLM). Given an input sentence
s and a context window of size t, each word s
i
is
conditioned on in turn to predict the identities of
all of the tokens within t words around it. For a
vocabulary V , each input word s
i
is represented
as a one-hot vector w
i
of length |V |. The SGLM
has two sets of parameters. The first is the rep-
resentation matrix W ? R
|V |?k
, which encodes
the real-valued embeddings for each word in the
vocabulary. A matrix multiply h = w
>
W,? R
k
serves to index the particular embedding for word
w, which constitutes the model?s hidden layer. To
predict the value of the context word y (again, a
one-hot vector of dimensionality |V |), this hidden
representation h is then multiplied by a second pa-
rameter matrix X ? R
|V |?k
. The final prediction
over the output vocabulary is then found by pass-
ing this resulting vector through the softmax func-
tion o = softmax(Xh), giving a vector in the |V |-
dimensional unit simplex. Backpropagation using
(input x, output y) word tuples learns the values
of W (the embeddings) and X (the output param-
eter matrix) that maximize the likelihood of y (i.e.,
the context words) conditioned on x (i.e., the s
i
?s).
During backpropagation, the errors propagated are
the difference between o (a probability distribu-
tion with k outcomes) and the true (one-hot) out-
put y.
Let us define a set of contextual variables
C; in the experiments that follow, C is com-
prised solely of geographical state C
state
=
{AK,AL, . . . ,WY}) but could in principle in-
clude any number of features, such as calendar
829
month, day of week, or other demographic vari-
ables of the speaker. Let |C| denote the sum of the
cardinalities of all variables in C (i.e., 51 states,
including the District of Columbia). Rather than
using a single embedding matrix W that contains
low-dimensional representations for every word in
the vocabulary, we define a global embedding ma-
trix W
main
? R
|V |?k
and an additional |C| such
matrices (each again of size |V | ? k, which cap-
ture the effect that each variable value has on each
word in the vocabulary. Given an input word w
and set of active variable values A (e.g., A =
{state = MA}), we calculate the hidden layer
h as the sum of these independent embeddings:
h = w
>
W
main
+
?
a?A
w
>
W
a
. While the word
wicked has a common low-dimensional represen-
tation in W
main,wicked
that is invoked for every
instance of its use (regardless of the place), the
corresponding vector W
MA,wicked
indicates how
that common representation should shift in k-
dimensional space when used in Massachusetts.
Backpropagation functions as in standard SGLM,
with gradient updates for each training example
{x, y} touching not onlyW
main
(as in SGLM), but
all active W
A
as well.
The additional W embeddings we add lead to
an increase in the number of total parameters by
a factor of |C|. To control for the extra degrees
of freedom this entails, we add squared `
2
regu-
larization to all parameters, using stochastic gra-
dient descent for backpropagation with minibatch
updates for the regularization term. As in Mikolov
et al (2013), we speed up computation using the
hierarchical softmax (Morin and Bengio, 2005) on
the output matrix X .
This model defines a joint parameterization over
all variable values in the data, where information
from data originating in California, for instance,
can influence the representations learned for Wis-
consin; a naive alternative would be to simply train
individual models on each variable value (a ?Cal-
ifornia? model using data only from California,
etc.). A joint model has three a priori advantages
over independent models: (i) sharing data across
variable values encourages representations across
those values to be similar; e.g., while city may be
closer to Boston in Massachusetts and Chicago in
Illinois, in both places it still generally connotes
a municipality; (ii) such sharing can mitigate data
sparseness for less-witnessed areas; and (iii) with
a joint model, all representations are guaranteed to
be in the same vector space and can therefore be
compared to each other; with individual models
(each with different initializations), word vectors
across different states may not be directly com-
pared.
3 Evaluation
We evaluate our model by confirming its face
validity in a qualitative analysis and estimating
its accuracy at the quantitative task of judging
geographically-informed semantic similarity. We
use 1.1 billion tokens from 93 million geolocated
tweets gathered between September 1, 2011 and
August 30, 2013 (approximately 127,000 tweets
per day evenly sampled over those two years).
This data only includes tweets that have been ge-
olocated to state-level granularity in the United
States using high-precision pattern matching on
the user-specified location field (e.g., ?new york
ny? ? NY, ?chicago? ? IL, etc.). As a pre-
processing step, we identify a set of target mul-
tiword expressions in this corpus as the maximal
sequence of adjectives + nouns with the highest
pointwise mutual information; in all experiments
described below, we define the vocabulary V as
the most frequent 100,000 terms (either unigrams
or multiword expressions) in the total data, and set
the dimensionality of the embedding k = 100. In
all experiments, the contextual variable is the ob-
served US state (including DC), so that |C| = 51;
the vector space representation of word w in state
s is w
>
W
main
+ w
>
W
s
.
3.1 Qualitative Evaluation
To illustrate how the model described above can
learn geographically-informed semantic represen-
tations of words, table 1 displays the terms with
the highest cosine similarity to wicked in Kansas
and Massachusetts after running our joint model
on the full 1.1 billion words of Twitter data; while
wicked in Kansas is close to other evaluative terms
like evil and pure and religious terms like gods and
spirit, in Massachusetts it is most similar to other
intensifiers like super, ridiculously and insanely.
Table 2 likewise presents the terms with the
highest cosine similarity to city in both Califor-
nia and New York; while the terms most evoked
by city in California include regional locations
like Chinatown, Los Angeles? South Bay and San
Francisco?s East Bay, in New York the most sim-
ilar terms include hamptons, upstate and borough
830
Kansas Massachusetts
term cosine term cosine
wicked 1.000 wicked 1.000
evil 0.884 super 0.855
pure 0.841 ridiculously 0.851
gods 0.841 insanely 0.820
mystery 0.830 extremely 0.793
spirit 0.830 goddamn 0.781
king 0.828 surprisingly 0.774
above 0.825 kinda 0.772
righteous 0.823 #sarcasm 0.772
magic 0.822 sooooooo 0.770
Table 1: Terms with the highest cosine similarity to wicked
in Kansas and Massachusetts.
California New York
term cosine term cosine
city 1.000 city 1.000
valley 0.880 suburbs 0.866
bay 0.874 town 0.855
downtown 0.873 hamptons 0.852
chinatown 0.854 big city 0.842
south bay 0.854 borough 0.837
area 0.851 neighborhood 0.835
east bay 0.845 downtown 0.827
neighborhood 0.843 upstate 0.826
peninsula 0.840 big apple 0.825
Table 2: Terms with the highest cosine similarity to city in
California and New York.
(New York City?s term of administrative division).
3.2 Quantitative Evaluation
As a quantitative measure of our model?s perfor-
mance, we consider the task of judging semantic
similarity among words whose meanings are likely
to evoke strong geographical correlations. In the
absence of a sizable number of linguistically in-
teresting terms (like wicked) that are known to be
geographically variable, we consider the proxy of
estimating the named entities evoked by specific
terms in different geographical regions. As noted
above, geographic terms like city provide one such
example: in Massachusetts we expect the term city
to be more strongly connected to grounded named
entities like Boston than to other US cities. We
consider seven categories for which we can rea-
sonably expect the connotations of each term to
vary by geography; in each case, we calculate the
distance between two terms x and y using repre-
sentations learned for a given state (?
state
(x, y)).
1. city. For each state, we measure the distance
between the word city and the state?s most
populous city; e.g., ?
AZ
(city , phoenix ).
2. state. For each state, the distance between
the word state and the state?s name; e.g.,
?
WI
(state,wisconsin).
3. football. For all NFL teams, the distance be-
tween the word football and the team name;
e.g., ?
IL
(football , bears).
4. basketball. For all NBA teams from
a US state, the distance between the
word basketball and the team name; e.g.,
?
FL
(basketball , heat).
5. baseball. For all MLB teams from a US
state, the distance between the word baseball
and the team name; e.g., ?
IL
(baseball , cubs),
?
IL
(baseball ,white sox ).
6. hockey. For all NHL teams from a US state,
the distance between the word hockey and the
team name; e.g., ?
PA
(hockey , penguins).
7. park. For all US national parks, the distance
between the word park and the park name;
e.g., ?
AK
(park , denali).
Each of these questions asks the following:
what words are evoked for a given target word
(like football)? While football may everywhere
evoke similar sports like baseball or soccer or
more specific football-related terms like touch-
down or field goal, we expect that particular sports
teams will be evoked more strongly by the word
football in their particular geographical region: in
Wisconsin, football should evoke packers, while
in Pennsylvania, football evokes steelers. Note
that this is not the same as simply asking which
sports team is most frequently (or most character-
istically) mentioned in a given area; by measuring
the distance to a target word (football), we are at-
tempting to estimate the varying strengths of asso-
ciation between concepts in different regions.
For each category, we measure similarity as the
average cosine similarity between the vector for
the target word for that category (e.g., city) and the
corresponding vector for each state-specific an-
swer (e.g., chicago for IL; boston for MA). We
compare three different models:
1. JOINT. The full model described in section
2, in which we learn a global representation
for each word along with deviations from that
common representation for each state.
2. INDIVIDUAL. For comparison, we also parti-
tion the data among all 51 states, and train a
single model for each state using only data
from that state. In this model, there is no
sharing among states; California has the most
831
0.00
0.25
0.50
0.75
city state baseball basketball football hockey park
sim
ila
rity
Model
Joint
Individual
?Geo
Figure 2: Average cosine similarity for all models across all categories, with 95% confidence intervals on the mean.
data with 11,604,637 tweets; Wyoming has
the least with 47,503 tweets.
3. ?GEO. We also train a single model on all of
the training data, but ignore any state meta-
data. In this case the distance ? between two
terms is their overall distance within the en-
tire United States.
As one concrete example of these differences
between individual data points, the cosine similar-
ity between city and seattle in the ?GEO model
is 0.728 (seattle is ranked as the 188th most sim-
ilar term to city overall); in the INDIVIDUAL
model using only tweets from Washington state,
?
WA
(city, seattle) = 0.780 (rank #32); and in
the JOINT model, using information from the en-
tire United States with deviations for Washington,
?
WA
(city, seattle) = 0.858 (rank #6). The over-
all similarity for the city category of each model is
the average of 51 such tests (one for each city).
Figure 2 present the results of the full evalua-
tion, including 95% confidence intervals for each
mean. While the two models that include ge-
ographical information naturally outperform the
model that does not, the JOINT model generally
far outperforms the INDIVIDUAL models trained
on state-specific subsets of the data.
1
A model that
can exploit all of the information in the data, learn-
ing core vector-space representations for all words
along with deviations for each contextual variable,
is able to learn more geographically-informed rep-
resentations for this task than strict geographical
models alone.
1
This result is robust to the choice of distance metric; an
evaluation measuring the Euclidean distance between vectors
shows the JOINT model to outperform the INDIVIDUAL and
?GEO models across all seven categories.
4 Conclusion
We introduced a model for leveraging situational
information in learning vector-space representa-
tions of words that are sensitive to the speaker?s
social context. While our results use geographical
information in learning low-dimensional represen-
tations, other contextual variables are straightfor-
ward to include as well; incorporating effects for
time ? such as time of day, month of year and ab-
solute year ? may be a powerful tool for reveal-
ing periodic and historical influences on lexical se-
mantics.
Our approach explores the degree to which ge-
ography, and other contextual factors, influence
word meaning in addition to frequency of usage.
By allowing all words in different regions (or more
generally, with different metadata factors) to ex-
ist in the same vector space, we are able com-
pare different points in that space ? for example,
to ask what terms used in Chicago are most simi-
lar to hot dog in New York, or what word groups
shift together in the same region in comparison
to the background (indicating the shift of an en-
tire semantic field). All datasets and software to
support these geographically-informed represen-
tations can be found at: http://www.ark.
cs.cmu.edu/geoSGLM.
5 Acknowledgments
The research reported in this article was supported
by US NSF grants IIS-1251131 and CAREER IIS-
1054319, and by an ARCS scholarship to D.B.
This work was made possible through the use of
computing resources made available by the Open
Cloud Consortium, Yahoo and the Pittsburgh Su-
percomputing Center.
832
References
Mark Andrews, Gabriella Vigliocco, and David Vin-
son. 2009. Integrating experiential and distribu-
tional data to learn semantic representations. Psy-
chological Review, 116(3):463?498.
David Bamman, Jacob Eisenstein, and Tyler Schnoe-
belen. 2014. Gender identity and lexical variation
in social media. Journal of Sociolinguistics, 18(2).
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In
Proc. of the Workshop on Geometrical Models of
Natural Language Semantics.
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-
Khanh Tran. 2012a. Distributional semantics in
technicolor. In Proc. of ACL.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes:
Using image analysis to improve computational rep-
resentations of word meaning. In Proc. of the ACM
International Conference on Multimedia.
Mary Bucholtz. 2006. Word up: Social meanings of
slang in California youth culture. In Jane Goodman
and Leila Monaghan, editors, A Cultural Approach
to Interpersonal Communication: Essential Read-
ings, Malden, MA. Blackwell.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Gabriel Doyle. 2014. Mapping dialectal variation by
querying social media. In Proc. of EACL.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2010. A latent variable model for
geographic lexical variation. In Proc. of EMNLP.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. of ACL.
Jacob Eisenstein, Brendan O?Connor, Noah A. Smith,
and Eric P. Xing. 2012. Mapping the geographical
diffusion of new words. arXiv, abs/1210.5268.
Yansong Feng and Mirella Lapata. 2010. Visual in-
formation in semantic representation. In Proc. of
NAACL.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Liangjie Hong, Amr Ahmed, Siva Gurumurthy,
Alexander J. Smola, and Kostas Tsioutsiouliklis.
2012. Discovering geographical topics in the Twit-
ter stream. In Proc. of WWW.
Yohei Ikawa, Miki Enoki, and Michiaki Tatsubori.
2012. Location inference using microblog mes-
sages. In Proc. of WWW.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proc. of COLING-ACL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Robert G. Cowell and Zoubin Ghahramani, editors,
Proc. of AISTATS.
Brendan O?Connor, Jacob Eisenstein, Eric P. Xing, and
Noah A. Smith. 2010. Discovering demographic
language variation. In NIPS Workshop on Machine
Learning and Social Computing.
Sharon Oviatt. 2003. Multimodal interfaces.
In Julie A. Jacko and Andrew Sears, editors,
The Human-computer Interaction Handbook, pages
286?304, Hillsdale, NJ, USA. L. Erlbaum Asso-
ciates Inc.
Marco Pennacchiotti and Ana-Maria Popescu. 2011.
Democrats, Republicans and Starbucks afficionados:
User classification in Twitter. In Proc. of KDD.
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in Twitter. In Proc. of the Workshop on
Search and Mining User-generated Contents.
Maya Ravindranath. 2011. A wicked good reason to
study intensifiers in New Hampshire. In NWAV 40.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proc. of NAACL.
Stephen Roller and Sabine Schulte im Walde. 2013. A
multimodal LDA model integrating textual, cogni-
tive and visual modalities. In Proc. of EMNLP.
Stephen Roller, Michael Speriosu, Sarat Rallapalli,
Benjamin Wing, and Jason Baldridge. 2012. Super-
vised text-based geolocation using language models
on an adaptive grid. In Proc. of EMNLP-CoNLL.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proc. of EMNLP.
Richard Socher, John Bauer, Christopher D. Manning,
and Ng Andrew Y. 2013. Parsing with composi-
tional vector grammars. In Proc. of ACL.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188, January.
833
Benjamin P. Wing and Jason Baldridge. 2011. Sim-
ple supervised document geolocation with geodesic
grids. In Proc. of ACL.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perception, 1(1):57?80.
834
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 19?24,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Community Evaluation and Exchange of Word Vectors
at wordvectors.org
Manaal Faruqui and Chris Dyer
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{mfaruqui, cdyer}@cs.cmu.edu
Abstract
Vector space word representations are use-
ful for many natural language process-
ing applications. The diversity of tech-
niques for computing vector representa-
tions and the large number of evaluation
benchmarks makes reliable comparison a
tedious task both for researchers devel-
oping new vector space models and for
those wishing to use them. We present
a website and suite of offline tools that
that facilitate evaluation of word vectors
on standard lexical semantics benchmarks
and permit exchange and archival by users
who wish to find good vectors for their
applications. The system is accessible at:
www.wordvectors.org.
1 Introduction
Data-driven learning of vector-space word embed-
dings that capture lexico-semantic properties is
a technique of central importance in natural lan-
guage processing. Using co-occurrence statistics
from a large corpus of text (Deerwester et al.,
1990; Turney and Pantel, 2010), it is possible
to construct high-quality semantic vectors ? as
judged by both correlations with human judge-
ments of semantic relatedness (Turney, 2006;
Agirre et al., 2009) and as features for down-
stream applications (Turian et al., 2010). A num-
ber of approaches that use the internal representa-
tions from models of word sequences (Collobert
and Weston, 2008) or continuous bags-of-context
wordsets (Mikolov et al., 2013) to arrive at vector
representations have also been shown to likewise
capture co-occurrence tendencies and meanings.
With an overwhelming number of techniques
to obtain word vector representations the task of
comparison and choosing the vectors best suitable
for a particular task becomes difficult. This is
further aggravated by the large number of exist-
ing lexical semantics evaluation benchmarks be-
ing constructed by the research community. For
example, to the best of our knowledge, for evaluat-
ing word similarity between a given pair of words,
there are currently at least 10 existing bench-
marks
1
that are being used by researchers to prove
the effectiveness of their word vectors.
In this paper we describe an online application
that provides the following utilities:
? Access to a suite of word similarity evalua-
tion benchmarks
? Evaluation of user computed word vectors
? Visualizing word vectors in R
2
? Evaluation and comparison of the available
open-source vectors on the suite
? Submission of user vectors for exhaustive of-
fline evaluation and leader board ranking
? Publicly available repository of word vectors
with performance details
Availability of such an evaluation system will
help in enabling better consistency and uniformity
in evaluation of word vector representations as
well as provide an easy to use interface for end-
users in a similar spirit to Socher et al. (2013a),
a website for text classification.
2
Apart from the
online demo version, we also provide a software
that can be run in an offline mode on the command
line. Both the online and offline tools will be kept
updated with continuous addition of new relevant
tasks and vectors.
1
www.wordvectors.org/suite.php
2
www.etcml.com
19
2 Word Similarity Benchmarks
We evaluate our word representations on 10 dif-
ferent benchmarks that have been widely used to
measure word similarity. The first one is the WS-
353
3
dataset (Finkelstein et al., 2001) containing
353 pairs of English words that have been assigned
similarity ratings by humans. This data was fur-
ther divided into two fragments by Agirre et al.
(2009) who claimed that similarity (WS-SIM) and
relatedness (WS-REL)
4
are two different kinds
of relations and should be dealt with separately.
The fourth and fifth benchmarks are the RG-65
(Rubenstein and Goodenough, 1965) and the MC-
30 (Miller and Charles, 1991) datasets that contain
65 and 30 pairs of nouns respectively and have
been given similarity rankings by humans. These
differ from WS-353 in that it contains only nouns
whereas the former contains all kinds of words.
The sixth benchmark is the MTurk-287
5
(Radinsky et al., 2011) dataset that constitutes 287
pairs of words and is different from the previ-
ous benchmarks in that it has been constructed
by crowdsourcing the human similarity ratings
using Amazon Mechanical Turk (AMT). Simi-
lar in spirit is the MTruk-771
6
(Halawi et al.,
2012) dataset that contains 771 word pairs whose
similarity was crowdsourced from AMT. An-
other, AMT created dataset is the MEN
7
bench-
mark (Bruni et al., 2012) that consists of 3000
word pairs, randomly selected from words that
occur at least 700 times in the freely available
ukWaC and Wackypedia
8
corpora combined.
The next two benchmarks were created to put
emphasis on different kinds of word types. To
specifically emphasize on verbs, Yang and Pow-
ers (2006) created a new benchmark YP-130 of
130 verb pairs with human similarity judgements.
Since, most of the earlier discussed datasets con-
tain word pairs that are relatively more frequent in
a corpus, Luong et al. (2013) create a new bench-
3
http://www.cs.technion.ac.il/
?
gabr/
resources/data/wordsim353/
4
http://alfonseca.org/eng/research/
wordsim353.html
5
http://tx.technion.ac.il/
?
kirar/
Datasets.html
6
http://www2.mta.ac.il/
?
gideon/
mturk771.html
7
http://clic.cimec.unitn.it/
?
elia.
bruni/MEN.html
8
http://wacky.sslmit.unibo.it/doku.
php?id=corpora
mark (Rare-Word)
9
that contains rare-words by
sampling words from different frequency bins to a
total of 2034 word pairs.
We calculate similarity between a given pair
of words by the cosine similarity between their
corresponding vector representation. We then re-
port Spearman?s rank correlation coefficient (My-
ers and Well, 1995) between the rankings pro-
duced by our model against the human rankings.
Multilingual Benchmarks. As is the case with
most NLP problems, the lexical semantics evalua-
tion benchmarks for languages other than English
have been limited. Currently, we provide a link
to some of these evaluation benchmarks from our
website and in future will expand the website to
encompass vector evaluation for other languages.
3 Visualization
The existing benchmarks provide ways of vector
evaluation in a quantitative setting. To get an idea
of what kind of information the vectors encode it is
important to see how these vectors represent words
in n-dimensional space, where n is the length
of the vector. Visualization of high-dimensional
data is an important problem in many different do-
mains, and deals with data of widely varying di-
mensionality. Over the last few decades, a variety
of techniques for the visualization of such high-
dimensional data have been proposed (de Oliveira
and Levkowitz, 2003).
Since visualization in n dimensions is hard
when n >= 3, we use the t-SNE (van der Maaten
and Hinton, 2008) tool
10
to project our vectors into
R
2
. t-SNE converts high dimensional data set into
a matrix of pairwise similarities between individ-
ual elements and then provides a way to visual-
ize these distances in a way which is capable of
capturing much of the local structure of the high-
dimensional data very well, while also revealing
global structure such as the presence of clusters at
several scales.
In the demo system, we give the user an option
to input words that they need to visualize which
are fed to the t-SNE tool and the produced images
are shown to the user on the webpage. These im-
ages can then be downloaded and used. We have
9
http://www-nlp.stanford.edu/
?
lmthang/
morphoNLM/
10
http://homepage.tudelft.nl/19j49/
t-SNE_files/tsne_python.zip
20
Figure 1: Antonyms (red) and synonyms (green) of beautiful represented by Faruqui and Dyer (2014)
(left) and Huang et al. (2012) (right).
included two datasets by default which exhibit dif-
ferent properties of the language:
? Antonyms and synonyms of beautiful
? Common male-female nouns and pronouns
In the first plot, ideally the antonyms (ugly,
hideous, . . . ) and synonyms (pretty, gorgeous,
. . . ) of beautiful should form two separate clus-
ters in the plot. Figure 1 shows the plots of the
antonyms and synonyms of the word beautiful for
two available embeddings. The second default
word plot is the gender data set, every word in
which has a male and a female counterpart (ex.
grandmother and grandfather), this data set ex-
hibits both local and global properties. Locally,
the male and female counterparts should occur in
pairs together and globally there should be two
separate clusters of male and female.
4 Word Vector Representations
4.1 Pre-trained Vectors
We haves collected several standard pre-trained
word vector representations freely available for re-
search purposes and provide a utility for the user
to test them on the suite of benchmarks, as well
as try out the visualization functionality. The user
can also choose the option to choose two different
types of word vectors and compare their perfor-
mance on the benchmarks. We will keep adding
word vectors on the website as and when they are
released. The following word vectors have been
included in our collection:
Metaoptimize. These word embeddings
11
have
been trained in (Turian et al., 2010) using a neu-
ral network language model and were shown to
be useful for named entity recognition (NER) and
phrase chunking.
SENNA. It is a software
12
which outputs a host
of predictions: part-of-speech (POS) tags, chunk-
ing, NER etc (Collobert et al., 2011). The soft-
ware uses neural word embeddings trained over
Wikipedia data for over 2 months.
RNNLM. The recurrent neural network lan-
guage modeling toolkit
13
comes with some
pre-trained embeddings on broadcast news
data (Mikolov et al., 2011).
Global Context. Huang et al. (2012) present a
model to incorporate document level information
into embeddings to generate semantically more in-
formed word vector representations. These em-
beddings
14
capture both local and global context
of the words.
Skip-Gram. This model is a neural network lan-
guage model except for that it does not have a
hidden layer and instead of predicting the target
word, it predicts the context given the target word
(Mikolov et al., 2013). These embeddings are
much faster to train
15
than the other neural em-
beddings.
11
http://metaoptimize.com/projects/
wordreprs/
12
http://ronan.collobert.com/senna/
13
http://rnnlm.org/
14
http://nlp.stanford.edu/
?
socherr/
ACL2012_wordVectorsTextFile.zip
15
https://code.google.com/p/word2vec/
21
Figure 2: Vector selection interface (right) of the demo system.
Multilingual. Faruqui and Dyer (2014) propose
a method based on canonical correlation analy-
sis to produce more informed monolingual vec-
tors using multilingual knowledge. Their method
is shown to perform well for both neural embed-
dings and LSA (Deerwester et al., 1990) based
vectors.
16
4.2 User-created Vectors
Our demo system provides the user an option to
upload their word vectors to perform evaluation
and visualization. However, since the size of the
word vector file will be huge due to a lot of in-
frequent words that are not useful for evaluation,
we give an option to filter the word vectors file
to only include the words required for evaluation.
The script and the vocabulary file can be found on
the website online.
5 Offline Evaluation & Public Access
We provide an online portal where researchers can
upload their vectors which are then be evaluated
on a variety of NLP tasks and then placed on the
leader board.
17
The motivation behind creating
such a portal is to make it easier for a user to se-
lect the kind of vector representation that is most
suitable for their task. In this scenario, instead of
asking the uploader to filter their word vectors for
a small vocabulary, they will be requested to up-
load their vectors for the entire vocabulary.
16
http://cs.cmu.edu/
?
mfaruqui/soft.html
17
We provide an initial list of some such tasks to which we
will later add more tasks as they are developed.
5.1 Offline Evaluation
Syntactic & semantic relations. Mikolov et al.
(2013) present a new semantic and syntactic re-
lation dataset composed of analogous word pairs
of size 8869 and 10675 pairs resp.. It contains
pairs of tuples of word relations that follow a com-
mon relation. For example, in England : Lon-
don :: France : Paris, the two given pairs of words
follow the country-capital relation. We use the
vector offset method (Mikolov et al., 2013) to
compute the missing word in these relations. This
is non-trivial |V |-way classification task where V
is the size of the vocabulary.
Sentence Completion. The Microsoft Research
sentence completion dataset contains 1040 sen-
tences from each of which one word has been re-
moved. The task is to correctly predict the miss-
ing word from a given list of 5 other words per
sentence. We average the word vectors of a given
sentence q
sent
=
?
N
i=1,i 6=j
q
w
i
/N , where w
j
is
the missing word and compute the cosine similar-
ity of q
sent
vector with each of the options. The
word with the highest similarity is chosen as the
missing word placeholder.
Sentiment Analysis Socher et al. (2013b) have
created a treebank which contains sentences an-
notated with fine-grained sentiment labels on both
the phrase and sentence level. They show that
compositional vector space models can be used
to predict sentiment at these levels with high ac-
curacy. The coarse-grained treebank, containing
only positive and negative classes has been split
into training, development and test datasets con-
22
Figure 3: Screenshot of the command line version showing word similarity evaluation.
taining 6920, 872 and 1821 sentences respectively.
We train a logistic regression classifier with L2
regularization on the average of the word vectors
of a given sentence to predict the coarse-grained
sentiment tag at the sentence level.
TOEFL Synonyms. These are a set of 80 ques-
tions compiled by Landauer and Dutnais (1997),
where a given word needs to be matched to its
closest synonym from 4 given options. A num-
ber of systems have reported their results on this
dataset.
18
We use cosine similarity to identify the
closest synonym.
5.2 Offline Software
Along with the web demo system we are making
available a software which can be downloaded and
be used for evaluation of vector representations of-
fline on all the benchmarks listed above. Since, we
cannot distribute the evaluation benchmarks along
with the software because of licensing issues, we
would give links to the resources which should be
downloaded prior to using the software. This soft-
ware can be run on a command line interface. Fig-
ure 3 shows a screenshot of word similarity evalu-
ation using the software.
5.3 Public Access
Usually corpora that the vectors are trained upon
are not available freely because of licensing issues
but it is easier to release the vectors that have been
trained on them. In the system that we have devel-
oped, we give the user an option to either make the
vectors freely available for everyone to use under a
GNU General Public License
19
or a Creative Com-
mons License.
20
If the user chooses not to make
the word vectors available, we would evaluate the
18
http://aclweb.org/aclwiki/index.php?
title=TOEFL_Synonym_Questions_(State_of_
the_art)
19
https://www.gnu.org/copyleft/gpl.html
20
https://creativecommons.org/licenses/
by-nc-sa/4.0/legalcode
vectors and give it a position in the leader board
with proper citation to the publications/softwares.
6 Conclusion
In this paper we have presented a demo system that
supports rapid and consistent evaluation of word
vector representations on a variety of tasks, visual-
ization with an easy-to-use web interface and ex-
change and comparison of different word vector
representations. The system also provides access
to a suite of evaluation benchmarks both for En-
glish and other languages. The functionalities of
the system are aimed at: (1) Being a portal for
systematic evaluation of lexical semantics tasks
that heavily rely on word vector representation, (2)
Making it easier for an end-user to choose the most
suitable vector representation schema.
Acknowledgements
We thank members of Noah?s Ark and c-lab for
their helpful comments about the demo system.
Thanks to Devashish Thakur for his help in set-
ting up the website. This work was supported by
the NSF through grant IIS-1352440.
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pas?ca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and wordnet-based approaches. In Proceed-
ings of NAACL, NAACL ?09, pages 19?27, Strouds-
burg, PA, USA.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in technicolor. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 136?145,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
23
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, ICML ?08, pages 160?167, New
York, NY, USA. ACM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Maria Cristina Ferreira de Oliveira and Haim Lev-
kowitz. 2003. From visual data exploration to vi-
sual data mining: A survey. IEEE Trans. Vis. Com-
put. Graph., 9(3):378?394.
S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. 1990. Indexing by
latent semantic analysis. Journal of the American
Society for Information Science.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proceedings of the 14th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Gothenburg, Sweden, April.
Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: the
concept revisited. In WWW ?01: Proceedings of the
10th international conference on World Wide Web,
pages 406?414, New York, NY, USA. ACM Press.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of word
relatedness with constraints. In KDD, pages 1406?
1414.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th ACL: Long
Papers-Volume 1, pages 873?882.
Thomas K Landauer and Susan T. Dutnais. 1997. A
solution to platos problem: The latent semantic anal-
ysis theory of acquisition, induction, and represen-
tation of knowledge. Psychological review, pages
211?240.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In CoNLL, Sofia, Bulgaria.
Tomas Mikolov, Stefan Kombrink, Anoop Deoras,
Lukar Burget, and J Cernock`y. 2011. Rnnlm?
recurrent neural network language modeling toolkit.
Proc. of the 2011 ASRU Workshop, pages 196?201.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Jerome L. Myers and Arnold D. Well. 1995. Research
Design & Statistical Analysis. Routledge, 1 edition,
June.
Kira Radinsky, Eugene Agichtein, Evgeniy
Gabrilovich, and Shaul Markovitch. 2011. A
word at a time: computing word relatedness using
temporal semantic analysis. In Proceedings of the
20th international conference on World wide web,
WWW ?11, pages 337?346, New York, NY, USA.
ACM.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
Richard Socher, Romain Paulus, Bryan McCann,
Kai Sheng Tai, and Andrew Y. Hu, JiaJi Ng. 2013a.
etcml.com - easy text classification with machine
learning. In Advances in Neural Information Pro-
cessing Systems (NIPS 2013).
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631?1642, Stroudsburg, PA, October.
Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th ACL, ACL ?10, pages 384?394, Stroudsburg,
PA, USA.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning : Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
pages 141?188.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Comput. Linguist., 32(3):379?416, Septem-
ber.
Laurens van der Maaten and Geoffrey Hinton. 2008.
Visualizing Data using t-SNE. Journal of Machine
Learning Research, 9:2579?2605, November.
Dongqiang Yang and David M. W. Powers. 2006. Verb
similarity on the taxonomy of wordnet. In In the 3rd
International WordNet Conference (GWC-06), Jeju
Island, Korea.
24
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121?126,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Simplified Dependency Annotations with GFL-Web
Michael T. Mordowanec Nathan Schneider Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
michael.mordowanec@gmail.com, {nschneid,cdyer,nasmith}@cs.cmu.edu
Abstract
We present GFL-Web, a web-based in-
terface for syntactic dependency annota-
tion with the lightweight FUDG/GFL for-
malism. Syntactic attachments are spec-
ified in GFL notation and visualized as
a graph. A one-day pilot of this work-
flow with 26 annotators established that
even novices were, with a bit of training,
able to rapidly annotate the syntax of En-
glish Twitter messages. The open-source
tool is easily installed and configured; it
is available at: https://github.com/
Mordeaux/gfl_web
1 Introduction
High-quality syntactic annotation of natural lan-
guage is expensive to produce. Well-known large-
scale syntactic annotation projects, such as the
Penn Treebank (Marcus et al., 1993), the En-
glish Web Treebank (Bies et al., 2012), the Penn
Arabic Treebank (Maamouri et al., 2004), and
the Prague dependency treebanks (Haji
?
c, 1998;
?
Cmejrek et al., 2005), have relied on expert lin-
guists to produce carefully-controlled annotated
data. Because this process is costly, such anno-
tation projects have been undertaken for only a
handful of important languages. Therefore, devel-
oping syntactic resources for less-studied, lower-
resource, or politically less important languages
and genres will require alternative methods. To
address this, simplified annotation schemes that
trade cost for detail have been proposed (Habash
and Roth, 2009).
1
1
These can be especially effective when some details of
the syntax can be predicted automatically with high accuracy
(Alkuhlani et al., 2013).
The Fragmentary Unlabeled Dependency
Grammar (FUDG) formalism (Schneider et al.,
2013) was proposed as a simplified framework for
annotating dependency syntax. Annotation effort
is reduced by relaxing a number of constraints
placed on traditional annotators: partial fragments
can be specified where the annotator is uncertain
of part of the structure or wishes to focus only
on certain phenomena (such as verbal argument
structure). FUDG also offers mechanisms for
excluding extraneous tokens from the annotation,
for marking multiword units, and for describing
coordinate structures. FUDG is written in an
ASCII-based notation for annotations called
Graph Fragment Language (GFL), and text-based
tools for verifying, converting, and rendering GFL
annotations are provided.
Although GFL offers a number of conveniences
to annotators, the text-based UI is limiting: the
existing tools require constant switching between
a text editor and executing commands, and there
are no tools for managing a large-scale annotation
effort. Additionally, user interface research has
found marked preferences for and better perfor-
mance with graphical tools relative to text-based
interfaces?particularly for less computer-savvy
users (Staggers and Kobus, 2000). In this paper,
we present the GFL-Web tool, a web-based inter-
face for FUDG/GFL annotation. The simple inter-
face provides instantaneous feedback on the well-
formedness of a GFL annotation, and by wrapping
Schneider et al.?s notation parsing and rendering
software, gives a user-friendly visualization of the
annotated sentence. The tool itself is lightweight,
multi-user, and easily deployed with few software
dependencies. Sentences are assigned to anno-
tators via an administrative interface, which also
records progress and provides for a text dump of
121
(a) @Bryan_wright11 i lost all my contacts , smh .
(b) Texas Rangers are in the World Series ! Go
Rangers !!!!!!!!! http://fb.me/D2LsXBJx
Figure 1: FUDG annotation graphs for two tweets.
all annotations. The interface for annotators is de-
signed to be as simple as possible.
We provide an overview of the FUDG/GFL
framework (?2), detail how the tool is set up and
utilized (?3), and discuss a pilot exercise in which
26 users provided nearly 1,000 annotations of En-
glish Twitter messages (?4). Finally, we note some
of the technical aspects of the tool (?5) and related
syntactic annotation software (?6).
2 Background
GFL-Web is designed to simplify the creation
of dependency treebanks from noisy or under-
resourced data; to that end, it exploits the
lightweight FUDG/GFL framework of Schneider
et al. (2013). Here we outline how FUDG differs
from traditional Dependency Grammar (?2.1) and
detail major aspects of GFL (?2.2).
2.1 FUDG
Figure 1 displays two FUDG graphs of annota-
tions of Twitter messages (?tweets?, shown below
in tokenized form). Arrows point upwards from
dependents to their heads. These tweets illustrate
several characteristics of the formalism, including:
? The input may contain multiple independent
syntactic units, or ?utterances?; the annotation
indicates these by attaching their heads to a spe-
cial root node called **.
? Some input tokens are omitted if deemed ex-
trinsic to the syntax; by convention, these in-
clude most punctuation, hashtags, usernames,
and URLs.
? Multiword units may be joined to form com-
posite lexical nodes (e.g., World_Series in fig-
ure 1b). These nodes are not annotated with any
internal syntactic structure.
? Tokens that are used in the FUDG parse must be
unambiguous. If a word appears multiple times
in the input, it is disambiguated with ~ and an
index (e.g., Rangers~2 in figure 1b).
(Some of the other mechanisms in FUDG, such as
coordinate structures and underspecification, are
not shown here; they are not important for pur-
poses of this paper.)
2.2 GFL
The Graph Fragment Language is a simple ASCII-
based language for FUDG annotations. Its norms
are designed to be familiar to users with basic pro-
gramming language proficiency, and they are intu-
itive and easy to learn even for those without. The
annotation in figure 1a may be expressed in GFL
as:
2
i > lost** < ({all my} > contacts)
smh**
In GFL, angle brackets point from a dependent
(child) to its head (parent). Parentheses group
nodes together; the head of this group is then at-
tached to another node. The double asterisk (**)
marks a root node in an annotations containing
multiple utterances. Curly braces group nodes that
modify the same head.
GFL corresponding to Figure 1b is:
2
The abbreviation smh stands for shaking my head.
122
Sentence: Texas Rangers are in the World Series ! Go Rangers !!!!!!!!! http://fb.me/D2LsXBJx
Input format:
---
% ID data_set_name:417
% TEXT
Texas Rangers~1 are in the World Series ! Go Rangers~2 !!!!!!!!!
http://fb.me/D2LsXBJx
% ANNO
Texas Rangers~1 are in the World Series Go Rangers~2
http://fb.me/D2LsXBJx
Figure 2: Illustration of the GFL-Web input format for a tweet. The ANNO section will be shown to the user as the default
annotation; punctuation has been stripped out automatically to save time.
Figure 3: User home screen showing assigned batches for annotation, with links to the training set and blank annotation form.
[Texas Rangers~1] > are** < in
in < (the > [World Series])
Go** < Rangers~2
This uses square brackets for multiword expres-
sions. Similar to a programming language, there
are often many equivalent GFL annotation options
for a given sentence. The annotation can be split
across multiple lines so that annotators can ap-
proach smaller units first and then link them to-
gether.
3 Using GFL-Web
The GFL-Web tool uses the Python programming
language?s Flask microframework for server-side
scripting. This allows it to be deployed on a web
server, locally or via the Internet. This also en-
ables the interface to rely on scripts previously
created for analyzing GFL. Once installed, the re-
searcher need only configure a few settings and be-
gin entering data to be annotated.
3.1 Setup
There are a few simple configuration options. The
most useful of these options specify how many
sentences should be in each batch that is assigned
to an annotator, and how many sentences in each
batch should be doubly annotated, for the purpose
of assessing inter-annotator agreement. By de-
fault, the batch size is 10, and the first 2 sentences
of each batch overlap with the previous batch, so
4/10 of the sentences in the batch will be annotated
by someone else (assuming no two consecutive
batches are assigned to the same user). The pro-
gram requires tokenized input, with indices added
to distinguish between words that appear twice
(easily automated). The input format, figure 2, al-
lows for easy editing with a text editor if so de-
sired.
Once the input files have been placed in a des-
ignated directory, an admin interface can be used
to assign batches of data to specific users (annota-
tors).
3.2 Annotation
Annotators log in with their username and see a
home screen, figure 3. The home screen always
offers links to a training set to get them up to
speed, as well as a blank annotation form into
which they can enter and annotate any sentence.
Beneath these is a table of batches of sentences
which have been assigned to the user. Clicking
123
Figure 4: A well-formed GFL annotation is indicated by a
green background and visualization of the analysis graph.
any of these will take the annotator to an annota-
tion page, which displays the text to be annotated,
an input field, and a comments box.
The annotation interface is simple and intuitive
and provides instant feedback, preventing the an-
notator from submitting ill-formed annotations.
Annotators press the Analyze button and receive
feedback before submitting annotations (figure 4).
Common GFL errors such as unbalanced paren-
theses are caught by the program and brought to
the attention of the annotator with an informative
error message (figure 5). The annotator can then
fix the error, and will be able to submit once all
errors are resolved.
The training set consists of 15 sentences se-
lected from Rossman and Mills (1922), shown in
the same annotation interface. Examples become
increasingly more complicated in order to famil-
iarize the user with different syntactic phenomena
and the entry-analyze-review workflow. A button
displays the FUDG graph from an expert annota-
tion so the novice can compare it to her own and
consult the guidelines (or ask for help) where the
two graphs deviate.
4 Pilot User Study
We conducted a pilot annotation project in which
26 annotators were trained on GFL-Web and asked
to annotate English Twitter messages from the
daily547 and oct27 Twitter datasets of Gimpel
et al. (2011). The overwhelming majority were all
trained on the same day, having no prior knowl-
edge of GFL. Most, but not all, were native speak-
ers of English. Those who had no prior knowl-
edge of dependency grammar in general received
a short tutorial on the fundamentals before being
introduced to the annotation workflow. All par-
ticipants who were new to FUDG/GFL worked
through the training set before moving on to the
Twitter data. Annotators were furnished with the
English annotation guidelines of Schneider et al.
(2013).
3
4.1 Results
In the one-day event, 906 annotations were gen-
erated. Inter-annotator agreement was high?.9
according to the softComPrec measure (Schnei-
der et al., 2013)?and an expert?s examination of a
sample of the annotations found that 75% of con-
tained no major error.
Annotators used the analysis feature of the
interface?which displays either a visual represen-
tation of the tree or an error message?an aver-
age of 3.06 times per annotation. The interface re-
quires they analyze each annotation at least once.
Annotators have the ability to resubmit annota-
tions if they later realize they made an error, and
each annotation was submitted an average of 1.16
times. Disregarding instances that took over 1,000
seconds (under the assumption that these repre-
sent annotators taking breaks), the median time
between the first analysis and the first submission
of an annotation was 30 seconds. We take this
as evidence that annotators found the instant feed-
back features useful in refining their annotations.
4.2 Post-Pilot Improvements
Annotator feedback prompted some changes to the
interface. The annotation input box was changed
to incorporate bracket-matching. The graph visu-
alization for a correct annotation was added for
each example in the training set so new annota-
tors could compare their tree to an example. Pre-
sumably these changes would further reduce anno-
tators? training time and improve their efficiency.
Progress bars were added to the user home screen
to show per-batch completion information.
4.3 Other Languages
In addition to English, guidelines for Swahili,
Zulu, and Mandarin are currently in development.
3https://github.com/brendano/gfl_syntax/
blob/master/guidelines/guidelines.md
124
Figure 5: An example error notification. The red background indicates an error, and the cause of the error is displayed at the
bottom of the screen.
5 Technical Architecture
GFL-Web and its software dependencies for ana-
lyzing and visualizing GFL are largely written in
Python. The tool is built with Flask, a Python
framework for web applications. Data is stored
and transmitted to and from the browser in the
Javascript Object Notation (JSON) format, which
is supported by libraries in most programming lan-
guages. The browser interface uses AJAX tech-
niques to interact dynamically with the server.
GFL-Web is written for Python version 2.7.
It wraps scripts previously written for the analy-
sis and visualization of GFL (Schneider et al.,
2013). These in turn require Graphviz (Ellson
et al., 2002), which is freely available.
Flask provides a built-in server, but can also be
deployed in Apache, via WSGI or CGI, etc.
6 Other Tools
In treebanking, a good user interface is essen-
tial for annotator productivity and accuracy. Sev-
eral existing tools support dependency annota-
tion; GFL-Web is the first designed specifi-
cally for the FUDG/GFL framework. Some,
including WebAnno (Yimam et al., 2013) and
brat (Stenetorp et al., 2012), are browser-based,
while WordFreak (Morton and LaCivita, 2003),
Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pa-
jas and Fabian, 2000?2011) are client-side appli-
cations. All offer tree visualizations; to the best
of our knowledge, ours is the only dependency
annotation interface that has text as the exclu-
sive mode of entry. Some, such as WebAnno
and brat, aim to be fairly general-purpose, sup-
porting a wide range of annotation schemes; by
contrast, GFL-Web supports a single annotation
scheme, which keeps the configuration (and code-
base) simple. In the future, GFL-Web might in-
corporate elements of monitoring progress, such
as display of evaluation measures computed with
existing FUDG/GFL scripts.
Certain elements of the FUDG/GFL framework
can be found in other annotation systems, such
as the PASSAGE syntactic representation (Vilnat
et al., 2010), which allows for grouping of words
into units, but still requires dependency relations
to be labeled.
Finally, we note that new approaches to corpus
annotation of semantic dependencies also come
with rich browser-based annotation interfaces (Ba-
narescu et al., 2013; Abend and Rappoport, 2013).
7 Conclusion
While the creation of high-quality, highly speci-
fied, syntactically annotated corpora is a goal that
is out of reach for most languages and genres,
GFL-Web facilitates a rapid annotation workflow
within a simple framework for dependency syn-
tax. More information on FUDG/GFL is avail-
able at http://www.ark.cs.cmu.edu/FUDG/,
and the source code for GFL-Web is available at
https://github.com/Mordeaux/gfl_web.
125
Acknowledgments
The authors thank Archna Bhatia, Lori Levin, Ja-
son Baldridge, Dan Garrette, Jason Mielens, Liang
Sun, Shay Cohen, Spencer Onuffer, Nora Ka-
zour, Manaal Faruqui, Wang Ling, Waleed Am-
mar, David Bamman, Dallas Card, Jeff Flani-
gan, Lingpeng Kong, Bill McDowell, Brendan
O?Connor, Tobi Owoputi, Yanchuan Sim, Swabha
Swayamdipta, and Dani Yogatama for annotating
data, and anonymous reviewers for helpful feed-
back. This research was supported by NSF grant
IIS-1352440.
References
Omri Abend and Ari Rappoport. 2013. Universal Con-
ceptual Cognitive Annotation (UCCA). In Proc. of
ACL, pages 228?238. Sofia, Bulgaria.
Sarah Alkuhlani, Nizar Habash, and Ryan Roth. 2013.
Automatic morphological enrichment of a mor-
phologically underspecified treebank. In Proc. of
NAACL-HLT, pages 460?470. Atlanta, Georgia.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina
Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin
Knight, Philipp Koehn, Martha Palmer, and Nathan
Schneider. 2013. Abstract Meaning Representation
for sembanking. In Proc. of the 7th Linguistic An-
notation Workshop and Interoperability with Dis-
course, pages 178?186. Sofia, Bulgaria.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Martin
?
Cmejrek, Jan Cu
?
r?n, Jan Haji
?
c, and Ji
?
r? Havelka.
2005. Prague Czech-English Dependency Treebank:
resource for structure-based MT. In Proc. of EAMT,
pages 73?78. Budapest, Hungary.
John Ellson, Emden Gansner, Lefteris Koutsofios,
Stephen C. North, and Gordon Woodhull. 2002.
Graphviz?open source graph drawing tools. In Pe-
tra Mutzel, Michael J?nger, and Sebastian Leipert,
editors, Graph Drawing, pages 483?484. Springer,
Berlin.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: annotation, features, and experiments.
In Proc. of ACL-HLT, pages 42?47. Portland, Ore-
gon.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proc. of ACL-
IJCNLP, pages 221?224. Suntec, Singapore.
Jan Haji
?
c. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Haji?cov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Arantza D?az De Ilarraza, Aitzpea Garmendia, and
Maite Oronoz. 2004. Abar-Hitz: An annotation tool
for the Basque dependency treebank. In Proc. of
LREC, pages 251?254. Lisbon, Portugal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Tree-
bank: building a large-scale annotated Arabic cor-
pus. In NEMLAR Conference on Arabic Language
Resources and Tools, pages 102?109. Cairo.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Thomas Morton and Jeremy LaCivita. 2003.
WordFreak: An open tool for linguistic anno-
tation. In Proc. of HLT-NAACL: Demonstrations,
pages 17?18. Edmonton, Canada.
Petr Pajas and Peter Fabian. 2000?2011. Tree Editor
TrEd 2.0. http://ufal.mff.cuni.cz/tred/.
Mary Blanche Rossman and Mary Wilda Mills. 1922.
Graded Sentences for Analysis, Selected from the
Best Literature and Systematically Graded for Class
Use. L. A. Noble.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. In Proc. of the 7th Lin-
guistic Annotation Workshop and Interoperability
with Discourse, pages 51?60. Sofia, Bulgaria.
Nancy Staggers and David Kobus. 2000. Comparing
response time, errors, and satisfaction between text-
based and graphical user interfaces during nursing
order tasks. Journal of the American Medical Infor-
matics Association, 7(2):164?176.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi
?
c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proc. of EACL: Demonstrations,
pages 102?107. Avignon, France.
Anne Vilnat, Patrick Paroubek, Eric Villemonte
de la Clergerie, Gil Francopoulo, and Marie-Laure
Gu?not. 2010. PASSAGE syntactic representation:
a minimal common ground for evaluation. In Proc.
of LREC, pages 2478?2485. Valletta, Malta.
Seid Muhie Yimam, Iryna Gurevych, Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno: A flexible, web-based and visually
supported system for distributed annotations. In
Proc. of ACL: Demonstrations, pages 1?6. Sofia,
Bulgaria.
126
Discriminative Lexical Semantic Segmentation with Gaps:
Running the MWE Gamut
Nathan Schneider Emily Danchik Chris Dyer Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nschneid,emilydan,cdyer,nasmith}@cs.cmu.edu
Abstract
We present a novel representation, evaluation
measure, and supervised models for the task of
identifying the multiword expressions (MWEs)
in a sentence, resulting in a lexical seman-
tic segmentation. Our approach generalizes
a standard chunking representation to encode
MWEs containing gaps, thereby enabling effi-
cient sequence tagging algorithms for feature-
rich discriminative models. Experiments on a
new dataset of English web text offer the first
linguistically-driven evaluation of MWE iden-
tification with truly heterogeneous expression
types. Our statistical sequence model greatly
outperforms a lookup-based segmentation pro-
cedure, achieving nearly 60% F1 for MWE
identification.
1 Introduction
Language has a knack for defying expectations when
put under the microscope. For example, there is the
notion?sometimes referred to as compositionality?
that words will behave in predictable ways, with indi-
vidual meanings that combine to form complex mean-
ings according to general grammatical principles. Yet
language is awash with examples to the contrary:
in particular, idiomatic expressions such as awash
with NP, have a knack for VP-ing, to the contrary, and
defy expectations. Thanks to processes like metaphor
and grammaticalization, these are (to various degrees)
semantically opaque, structurally fossilized, and/or
statistically idiosyncratic. In other words, idiomatic
expressions may be exceptional in form, function,
or distribution. They are so diverse, so unruly, so
1. MW named entities: Prime Minister Tony Blair
2. MW compounds: hot air balloon, skinny dip
3. conventionally SW compounds: somewhere
4. verb-particle: pick up, dry out, take over, cut short
5. verb-preposition: refer to, depend on, look for
6. verb-noun(-preposition): pay attention (to)
7. support verb: make decisions, take pictures
8. other phrasal verb: put up with, get rid of
9. PP modifier: above board, at all, from time to time
10. coordinated phrase: cut and dry, more or less
11. connective: as well as, let alone, in spite of
12. semi-fixed VP: pick up where <one> left off
13. fixed phrase: scared to death, leave of absence
14. phatic: You?re welcome. Me neither!
15. proverb: Beggars can?t be choosers.
Figure 1: Some of the classes of idioms in English.
The examples included here contain multiple lexicalized
words?with the exception of those in (3), if the conven-
tional single-word (SW) spelling is used.
difficult to circumscribe, that entire theories of syn-
tax are predicated on the notion that constructions
with idiosyncratic form-meaning mappings (Fillmore
et al., 1988; Goldberg, 1995) or statistical properties
(Goldberg, 2006) offer crucial evidence about the
grammatical organization of language.
Here we focus on multiword expressions
(MWEs): lexicalized combinations of two or more
words that are exceptional enough to be considered
as single units in the lexicon. As figure 1 illus-
trates, MWEs occupy diverse syntactic and semantic
functions. Within MWEs, we distinguish (a) proper
names and (b) lexical idioms. The latter have proved
themselves a ?pain in the neck for NLP? (Sag et al.,
2002). Automatic and efficient detection of MWEs,
though far from solved, would have diverse appli-
193
Transactions of the Association for Computational Linguistics, 2 (2014) 193?206. Action Editor: Joakim Nivre.
Submitted 12/2013; Revised 1/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
cations including machine translation (Carpuat and
Diab, 2010), information retrieval (Newman et al.,
2012), opinion mining (Berend, 2011), and second
language learning (Ellis et al., 2008).
It is difficult to establish any comprehensive tax-
onomy of multiword idioms, let alone develop lin-
guistic criteria and corpus resources that cut across
these types. Consequently, the voluminous litera-
ture on MWEs in computational linguistics?see ?7,
Baldwin and Kim (2010), and Ramisch (2012) for
surveys?has been fragmented, looking (for exam-
ple) at subclasses of phrasal verbs or nominal com-
pounds in isolation. To the extent that MWEs have
been annotated in existing corpora, it has usually
been as a secondary aspect of some other scheme.
Traditionally, such resources have prioritized certain
kinds of MWEs to the exclusion of others, so they
are not appropriate for evaluating general-purpose
identification systems.
In this article, we briefly review a shallow form
of analysis for MWEs that is neutral to expression
type, and that facilitates free text annotation with-
out requiring a prespecified MWE lexicon (?2). The
scheme applies to gappy (discontinuous) as well as
contiguous expressions, and allows for a qualitative
distinction of association strengths. In Schneider
et al. (2014) we have applied this scheme to fully an-
notate a 55,000-word corpus of English web reviews
(Bies et al., 2012a), a conversational genre in which
colloquial idioms are highly salient. This article?s
main contribution is to show that the representation?
constrained according to linguistically motivated as-
sumptions (?3)?can be transformed into a sequence
tagging scheme that resembles standard approaches
in named entity recognition and other text chunking
tasks (?4). Along these lines, we develop a discrim-
inative, structured model of MWEs in context (?5)
and train, evaluate, and examine it on the annotated
corpus (?6). Finally, in ?7 and ?8 we comment on
related work and future directions.
2 Annotated Corpus
To build and evaluate a multiword expression ana-
lyzer, we use the MWE-annotated corpus of Schnei-
der et al. (2014). It consists of informal English web
text that has been specifically and completely anno-
tated for MWEs, without reference to any particular
lexicon. To the best of our knowledge, this corpus
is the first to be freely annotated for many kinds of
MWEs (without reference to a lexicon), and is also
the first dataset of social media text with MWE an-
notations beyond named entities. This section gives
a synopsis of the annotation conventions used to de-
velop that resource, as they are important to under-
standing our models and evaluation.
Rationale. The multiword expressions community
has lacked a canonical corpus resource comparable
to benchmark datasets used for problems such as
NER and parsing. Consequently, the MWE litera-
ture has been driven by lexicography: typically, the
goal is to acquire an MWE lexicon with little or no
supervision, or to apply such a lexicon to corpus
data. Studies of MWEs in context have focused on
various subclasses of constructions in isolation, ne-
cessitating special-purpose datasets and evaluation
schemes. By contrast, Schneider et al.?s (2014) cor-
pus creates an opportunity to tackle general-purpose
MWE identification, such as would be desirable for
use by high-coverage downstream NLP systems. It is
used to train and evaluate our models below. The cor-
pus is publicly available as a benchmark for further
research.1
Data. The documents in the corpus are online user
reviews of restaurants, medical providers, retailers,
automotive services, pet care services, etc. Marked
by conversational and opinionated language, this
genre is fertile ground for colloquial idioms (Nunberg
et al., 1994; Moon, 1998). The 723 reviews (55,000
words, 3,800 sentences) in the English Web Tree-
bank (WTB; Bies et al., 2012b) were collected by
Google, tokenized, and annotated with phrase struc-
ture trees in the style of the Penn Treebank (Marcus
et al., 1993). MWE annotators used the sentence and
word tokenizations supplied by the treebank.2
Annotation scheme. The annotation scheme itself
was designed to be as simple as possible. It consists
of grouping together the tokens in each sentence that
belong to the same MWE instance. While annotation
guidelines provide examples of MWE groupings in
a wide range of constructions, the annotator is not
1http://www.ark.cs.cmu.edu/LexSem/
2Because we use treebank data, syntactic parses are available
to assist in post hoc analysis. Syntactic information was not
shown to annotators.
194
# of constituent tokens
2 3 ?4 total
strong 2257 595 172 3024
weak 269 121 69 459
2526 716 241 3483
# of gaps
0 1 2
2626 394 4
322 135 2
2948 529 6
Table 1: Counts in the MWE corpus.
tied to any particular taxonomy or syntactic structure.
This simplifies the number of decisions that have to
be made for each sentence, even if some are difficult.
Further instructions to annotators included:
? Groups should include only the lexically fixed parts
of an expression (modulo inflectional morphology);
this generally excludes determiners and pronouns:
made the mistake, pride themselves on.
? Multiword proper names count as MWEs.
? Misspelled or unconventionally spelled tokens are
interpreted according to the intended word if clear.
? Overtokenized words (spelled as two tokens, but
conventionally one word) are joined as multiwords.
Clitics separated by the tokenization in the corpus?
negative n?t, possessive ?s, etc.?are joined if func-
tioning as a fixed part of a multiword (e.g., T ?s
Cafe), but not if used productively.
Gaps. There are, broadly speaking, three reasons
to group together tokens that are not fully contigu-
ous. Most commonly, gaps contain internal modifiers,
such as good in make good decisions. Syntactic con-
structions such as the passive can result in gaps that
might not otherwise be present: in good decisions
were made, there is instead a gap filled by the pas-
sive auxiliary. Finally, some MWEs may take internal
arguments: they gave me a break. Figure 1 has addi-
tional examples. Multiple gaps can occur even within
the same expression, though it is rare: they agreed to
give Bob a well-deserved break.
Strength. The annotation scheme has two
?strength? levels for MWEs. Clearly idiomatic ex-
pressions are marked as strong MWEs, while mostly
compositional but especially frequent collocations/
phrases (e.g., abundantly clear and patently obvious)
are marked as weak MWEs. Weak multiword groups
are allowed to include strong MWEs as constituents
(but not vice versa). Strong groups are required to
cohere when used inside weak groups: that is, a weak
group cannot include only part of a strong group.
For purposes of annotation, there were no constraints
hinging on the ordering of tokens in the sentence.
Process. MWE annotation proceeded one sentence
at a time. The 6 annotators referred to and improved
the guidelines document on an ongoing basis. Every
sentence was seen independently by at least 2 an-
notators, and differences of opinion were discussed
and resolved (often by marking a weak MWE as a
compromise). See Schneider et al. (2014) for details.
Statistics. The annotated corpus consists of 723
documents (3,812 sentences). MWEs are frequent
in this domain: 57% of sentences (72% of sentences
over 10 words long) and 88% of documents contain
at least one MWE. 8,060/55,579=15% of tokens
belong to an MWE; in total, there are 3,483 MWE
instances. 544 (16%) are strong MWEs containing a
gold-tagged proper noun?most are proper names. A
breakdown appears in table 1.
3 Representation and Task Definition
We define a lexical segmentation of a sentence as a
partitioning of its tokens into segments such that each
segment represents a single unit of lexical meaning.
A multiword lexical expression may contain gaps,
i.e. interruptions by other segments. We impose two
restrictions on gaps that appear to be well-motivated
linguistically:
? Projectivity: Every expression filling a gap must
be completely contained within that gap; gappy
expressions may not interleave.
? No nested gaps: A gap in an expression may be
filled by other single- or multiword expressions, so
long as those do not themselves contain gaps.
Formal grammar. Our scheme corresponds to the
following extended CFG (Thatcher, 1967), where S
is the full sentence and terminals w are word tokens:
S ? X+
X ? w+ (Y+ w+)?
Y ? w+
Each expression X or Y is lexicalized by the words in
one or more underlined variables on the right-hand
side. An X constituent may optionally contain one or
more gaps filled by Y constituents, which must not
contain gaps themselves.3
3MWEs with multiple gaps are rare but attested in data: e.g.,
putting me at my ease. We encountered one violation of the gap
nesting constraint in the reviews data: I have21 nothing21 but21
fantastic things2 to21 say21 . Additionally, the interrupted phrase
195
Denoting multiword groupings with subscripts, My
wife had taken1 her ?072 Ford2 Fusion2 in1 for a
routine oil3 change3 contains 3 multiword groups?{taken, in}, {?07, Ford, Fusion}, {oil, change}?and
7 single-word groups. The first MWE is gappy (ac-
centuated by the box); a single word and a contiguous
multiword group fall within the gap. The projectivity
constraint forbids an analysis like taken1 her ?072
Ford1 Fusion2, while the gap nesting constraint for-
bids taken1 her2 ?07 Ford2 Fusion2 in1.
3.1 Two-level Scheme: Strong vs. Weak MWEs
Our annotated data distinguish two strengths of
MWEs as discussed in ?2. Augmenting the gram-
mar of the previous section, we therefore designate
nonterminals as strong (X , Y ) or weak (X? , Y? ):
S ? X?+
X? ? X+ (Y?+ X+)?
X ? w+ (Y?+ w+)?
Y? ? Y+
Y ? w+
A weak MWE may be lexicalized by single words
and/or strong multiwords. Strong multiwords cannot
contain weak multiwords except in gaps. Further, the
contents of a gap cannot be part of any multiword
that extends outside the gap.4
For example, consider the segmentation: he was
willing to budge1 a2 little2 on1 the price which
means4 a43 lot43 to4 me4. Subscripts denote strong
MW groups and superscripts weak MW groups; un-
marked tokens serve as single-word expressions. The
MW groups are thus {budge, on}, {a, little}, {a, lot},
and {means, {a, lot}, to, me}. As should be evident
from the grammar, the projectivity and gap-nesting
constraints apply here just as in the 1-level scheme.
3.2 Evaluation
Matching criteria. Given that most tokens do not
belong to an MWE, to evaluate MWE identification
we adopt a precision/recall-based measure from the
coreference resolution literature. The MUC criterion
(Vilain et al., 1995) measures precision and recall
great gateways never1 before1 , so23 far23 as23 Hudson knew2 ,
seen1 by Europeans was annotated in another corpus.
4This was violated 6 times in our annotated data: modifiers
within gaps are sometimes collocated with the gappy expression,
as in on12 a12 tight1 budget12 and have12 little1 doubt12.
of links in terms of groups (units) implied by the
transitive closure over those links.5 It can be defined
as follows:
Let a ? b denote a link between two elements
in the gold standard, and a??b denote a link in the
system prediction. Let the ? operator denote the tran-
sitive closure over all links, such that ?a??b? is 1 if
a and b belong to the same (gold) set, and 0 other-
wise. Assuming there are no redundant6 links within
any annotation (which in our case is guaranteed by
linking consecutive words in each MWE), we can
write the MUC precision and recall measures as:
P = ?a,b?a??b ?a??b??a,b?a??b 1 R =
?a,b?a?b ?a???b??a,b?a?b 1
This awards partial credit when predicted and gold
expressions overlap in part. Requiring full MWEs to
match exactly would arguably be too stringent, over-
penalizing larger MWEs for minor disagreements.
We combine precision and recall using the standard
F1 measure of their harmonic mean. This is the link-
based evaluation used for most of our experiments.
For comparison, we also report some results with
a more stringent exact match evaluation where the
span of the predicted MWE must be identical to the
span of the gold MWE for it to count as correct.
Strength averaging. Recall that the 2-level
scheme (?3.1) distinguishes strong vs. weak links/
groups, where the latter category applies to reason-
ably compositional collocations as well as ambigu-
ous or difficult cases. If where one annotation uses
a weak link the other has a strong link or no link at
all, we want to penalize the disagreement less than
if one had a strong link and the other had no link.
To accommodate the 2-level scheme, we therefore
average F?1 , in which all weak links have been con-
verted to strong links, and F?1 , in which they have
been removed: F1 = 12(F?1 +F?1 ).7 If neither annota-tion contains any weak links, this equals the MUC
5As a criterion for coreference resolution, the MUC measure
has perceived shortcomings which have prompted several other
measures (see Recasens and Hovy, 2011 for a review). It is not
clear, however, whether any of these criticisms are relevant to
MWE identification.
6A link between a and b is redundant if the other links already
imply that a and b belong to the same set. A set of N elements is
expressed non-redundantly with exactly N ?1 links.
7Overall precision and recall are likewise computed by aver-
aging ?strengthened? and ?weakened? measurements.
196
no gaps,
1-level
he
O
was
O
willing
O
to
O
budge
O
a
B
little
I
on
O
the
O
price
O
which
O
means
B
a
I
lot
I
to
I
me
I
.
O
(O?BI+)+
no gaps,
2-level
he
O
was
O
willing
O
to
O
budge
O
a
B
little
I?
on
O
the
O
price
O
which
O
means
B
a
I?
lot
I?
to
I?
me
I?
.
O
(O?B[I?I?]+)+
gappy,
1-level
he
O
was
O
willing
O
to
O
budge
B
a
b
little
i
on
I
the
O
price
O
which
O
means
B
a
I
lot
I
to
I
me
I
.
O
(O?B(o?bi+?I)?I+)+
gappy,
2-level
he
O
was
O
willing
O
to
O
budge
B
a
b
little
??
on
I?
the
O
price
O
which
O
means
B
a
I?
lot
I?
to
I?
me
I?
.
O
(O?B(o?b[????]+?[I?I?])?[I?I?]+)+
Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and
weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications.
score because F1 = F?1 = F?1 . This method appliesto both the link-based and exact match evaluation
criteria.
4 Tagging Schemes
Following (Ramshaw and Marcus, 1995), shallow an-
alysis is often modeled as a sequence-chunking task,
with tags containing chunk-positional information.
The BIO scheme and variants (e.g., BILOU; Ratinov
and Roth, 2009) are standard for tasks like named
entity recognition, supersense tagging, and shallow
parsing.
The language of derivations licensed by the gram-
mars in ?3 allows for a tag-based encoding of MWE
analyses with only bigram constraints. We describe
4 tagging schemes for MWE identification, starting
with BIO and working up to more expressive variants.
They are depicted in figure 2.
No gaps, 1-level (3 tags). This is the standard con-
tiguous chunking representation from Ramshaw and
Marcus (1995) using the tags {O B I}. O is for to-
kens outside any chunk; B marks tokens beginning
a chunk; and I marks other tokens inside a chunk.
Multiword chunks will thus start with B and then I.
B must always be followed by I; I is not allowed at
the beginning of the sentence or following O.
No gaps, 2-level (4 tags). We can distinguish
strength levels by splitting I into two tags: I? for
strong expressions and I? for weak expressions. To
express strong and weak contiguous chunks requires
4 tags: {O B I? I?}. (Marking B with a strength as well
would be redundant because MWEs are never length-
one chunks.) The constraints on I? and I? are the same
as the constraints on I in previous schemes. If I? and
I? occur next to each other, the strong attachment will
receive higher precedence, resulting in analysis of
strong MWEs as nested within weak MWEs.
Gappy, 1-level (6 tags). Because gaps cannot
themselves contain gappy expressions (we do not
support full recursivity), a finite number of additional
tags are sufficient to encode gappy chunks. We there-
fore add lowercase tag variants representing tokens
within a gap: {O o B b I i}. In addition to the con-
straints stated above, no within-gap tag may occur at
the beginning or end of the sentence or immediately
following or preceding O. Within a gap, b, i, and o
behave like their out-of-gap counterparts.
Gappy, 2-level (8 tags). 8 tags are required to en-
code the 2-level scheme with gaps: {O o B b I? ?? I? ??}.
Variants of the inside tag are marked for strength of
the incoming link?this applies gap-externally (capi-
talized tags) and gap-internally (lowercase tags). If I?
or I? immediately follows a gap, its diacritic reflects
the strength of the gappy expression, not the gap?s
contents.
5 Model
With the above representations we model MWE iden-
tification as sequence tagging, one of the paradigms
that has been used previously for identifying con-
tiguous MWEs (Constant and Sigogne, 2011, see
?7).8 Constraints on legal tag bigrams are sufficient
to ensure the full tagging is well-formed subject to
the regular expressions in figure 2; we enforce these
8Hierarchical modeling based on our representations is left
to future work.
197
constraints in our experiments.9
In NLP, conditional random fields (Lafferty et al.,
2001) and the structured perceptron (Collins, 2002)
are popular techniques for discriminative sequence
modeling with a convex loss function. We choose
the second approach for its speed: learning and in-
ference depend mainly on the runtime of the Viterbi
algorithm, whose asymptotic complexity is linear in
the length of the input and (with a first-order Markov
assumption) quadratic in the number of tags. Below,
we review the structured perceptron and discuss our
cost function, features, and experimental setup.
5.1 Cost-Augmented Structured Perceptron
The structured perceptron?s (Collins, 2002) learn-
ing procedure, algorithm 1, generalizes the classic
perceptron algorithm (Freund and Schapire, 1999) to
incorporate a structured decoding step (for sequences,
the Viterbi algorithm) in the inner loop. Thus, train-
ing requires only max inference, which is fast with a
first-order Markov assumption. In training, features
are adjusted where a tagging error is made; the pro-
cedure can be viewed as optimizing the structured
hinge loss. The output of learning is a weight vector
that parametrizes a feature-rich scoring function over
candidate labelings of a sequence.
To better align the learning algorithm with our
F-score?based MWE evaluation (?3.2), we use a
cost-augmented version of the structured perceptron
that is sensitive to different kinds of errors during
training. When recall is the bigger obstacle, we can
adopt the following cost function: given a sentence
x, its gold labeling y?, and a candidate labeling y?,
cost(y?,y?,x) = ?y???
j=1c(y?j ,y?j) where
c(y?,y?) = ?y? ? y??+??y? ? {B,b}?y? ? {O,o}?
A single nonnegative hyperparameter, ? , controls
the tradeoff between recall and accuracy; higher ?
biases the model in favor of recall (possibly hurt-
ing accuracy and precision). This is a slight variant
of the recall-oriented cost function of Mohit et al.
(2012). The difference is that we only penalize
beginning-of-expression recall errors. Preliminary
9The 8-tag scheme licenses 42 tag bigrams: sequences such
as B O and o ?? are prohibited. There are also constraints on the
allowed tags at the beginning and end of the sequence.
Input: data ??x(n),y(n)??Nn=1; number of iterations Mw? 0
w? 0
t ? 1
for m = 1 to M do
for n = 1 to N do?x,y?? ?x(n),y(n)?
y?? argmaxy? (w?g(x,y?)+cost(y,y?,x))
if y? ? y then
w?w+g(x,y)?g(x, y?)
w?w+ tg(x,y)? tg(x, y?)
end
t ? t +1end
end
Output: w?(w/t)
Algorithm 1: Training with the averaged perceptron.
(Adapted from Daum?, 2006, p. 19.)
experiments showed that a cost function penalizing
all recall errors?i.e., with ??y? ? O?y? = O? as the
second term, as in Mohit et al.?tended to append
additional tokens to high-confidence MWEs (such
as proper names) rather than encourage new MWEs,
which would require positing at least two new non-
outside tags.
5.2 Features
Basic features. These are largely based on those
of Constant et al. (2012): they look at word unigrams
and bigrams, character prefixes and suffixes, and POS
tags, as well as lexicon entries that match lemmas10
of multiple words in the sentence. Appendix A lists
the basic features in detail.
Some of the basic features make use of lexicons.
We use or construct 10 lists of English MWEs: all
multiword entries in WordNet (Fellbaum, 1998); all
multiword chunks in SemCor (Miller et al., 1993);
all multiword entries in English Wiktionary;11 the
WikiMwe dataset mined from English Wikipedia
(Hartmann et al., 2012); the SAID database of
phrasal lexical idioms (Kuiper et al., 2003); the
named entities and other MWEs in the WSJ corpus
on the English side of the CEDT (Hajic? et al., 2012);
10The WordNet API in NLTK (Bird et al., 2009) was used for
lemmatization.
11http://en.wiktionary.org; data obtained from
https://toolserver.org/~enwikt/definitions/
enwikt-defs-20130814-en.tsv.gz
198
LOOKUP SUPERVISED MODEL
preexising lexicons entries max gap
length
P R F1 ? P R F1 ?
none 0 74.39 44.43 55.57 2.19
WordNet + SemCor 71k 0 46.15 28.41 35.10 2.44 74.51 45.79 56.64 1.90
6 lexicons 420k 0 35.05 46.76 40.00 2.88 76.08 52.39 61.95 1.67
10 lexicons 437k 0 33.98 47.29 39.48 2.88 75.95 51.39 61.17 2.30
best configuration with
in-domain lexicon
1 46.66 47.90 47.18 2.31 76.64 51.91 61.84 1.65
2 lexicons + MWtypes(train)?1 6 lexicons + MWtypes(train)?2
Table 2: Use of lexicons for lookup-based vs. statistical segmentation. Supervised learning used only basic features
and the structured perceptron, with the 8-tag scheme. Results are with the link-based matching criterion for evaluation.
Top: Comparison of preexisting lexicons. ?6 lexicons? refers to WordNet and SemCor plus SAID, WikiMwe, Phrases.net,
and English Wiktionary; ?10 lexicons? adds MWEs from CEDT, VNC, LVC, and Oyz. (In these lookup-based
configurations, allowing gappy MWEs never helps performance.)
Bottom: Combining preexisting lexicons with a lexicon derived from MWEs annotated in the training portion of each
cross-validation fold at least once (lookup) or twice (model).
All precision, recall, and F1 percentages are averaged across 8 folds of cross-validation on train; standard deviations
are shown for the F1 score. In each column, the highest value using only preexisting lexicons is underlined, and the
highest overall value is bolded. The boxed row indicates the configuration used as the basis for subsequent experiments.
the verb-particle constructions (VPCs) dataset of
(Baldwin, 2008); a list of light verb constructions
(LVCs) provided by Claire Bonial; and two idioms
websites.12 After preprocessing, each lexical entry
consists of an ordered sequence of word lemmas,
some of which may be variables like <something>.
Given a sentence and one or more of the lexicons,
lookup proceeds as follows: we enumerate entries
whose lemma sequences match a sequence of lemma-
tized tokens, and build a lattice of possible analyses
over the sentence. We find the shortest path (i.e.,
using as few expressions as possible) with dynamic
programming, allowing gaps of up to length 2.13
Unsupervised word clusters. Distributional clus-
tering on large (unlabeled) corpora can produce lexi-
cal generalizations that are useful for syntactic and
semantic analysis tasks (e.g.: Miller et al., 2004; Koo
et al., 2008; Turian et al., 2010; Owoputi et al., 2013;
Grave et al., 2013). We were interested to see whether
a similar pattern would hold for MWE identification,
given that MWEs are concerned with what is lexi-
cally idiosyncratic?i.e., backing off from specific
lexemes to word classes may lose the MWE-relevant
information. Brown clustering14 (Brown et al., 1992)
12http://www.phrases.net/ and http://home.
postech.ac.kr/~oyz/doc/idiom.html
13Each top-level lexical expression (single- or multiword)
incurs a cost of 1; each expression within a gap has cost 1.25.
14With Liang?s (2005) implementation: https://github.
com/percyliang/brown-cluster. We obtain 1,000 clusters
on the 21-million-word Yelp Academic Dataset15
(which is similar in genre to the annotated web re-
views data) gives us a hard clustering of word types.
To our tagger, we add features mapping the previ-
ous, current, and next token to Brown cluster IDs.
The feature for the current token conjoins the word
lemma with the cluster ID.
Part-of-speech tags. We compared three PTB-
style POS taggers on the full REVIEWS subcor-
pus (train+test). The Stanford CoreNLP tagger16
(Toutanova et al., 2003) yields an accuracy of 90.4%.
The ARK TweetNLP tagger v. 0.3.2 (Owoputi et al.,
2013) achieves 90.1% with the model17 trained on the
Twitter corpus of Ritter et al. (2011), and 94.9% when
trained on the ANSWERS, EMAIL, NEWSGROUP, and
WEBLOG subcorpora of WTB. We use this third con-
figuration to produce automatic POS tags for training
and testing our MWE tagger. (A comparison condi-
tion in ?6.3 uses oracle POS tags.)
5.3 Experimental Setup
The corpus of web reviews described in ?2 is used
for training and evaluation. 101 arbitrarily chosen
documents (500 sentences, 7,171 words) were held
from words appearing at least 25 times.
15https://www.yelp.com/academic_dataset
16v. 3.2.0, with english-bidirectional-distsim
17http://www.ark.cs.cmu.edu/TweetNLP/model.
ritter_ptb_alldata_fixed.20130723
199
LINK-BASED EXACT MATCH
configuration M ? ?w? P R F1 P R F1
base model 5 ? 1,765k 69.27 50.49 58.35 60.99 48.27 53.85+ recall cost 4 150 1,765k 61.09 57.94 59.41 53.09 55.38 54.17+ clusters 3 100 2,146k 63.98 55.51 59.39 56.34 53.24 54.70+ oracle POS 4 100 2,145k 66.19 59.35 62.53 58.51 57.00 57.71
Table 3: Comparison of supervised models on test (using the 8-tag scheme). The base model corresponds to the boxed
result in table table 2, but here evaluated on test. For each configuration, the number of training iterations M and (except
for the base model) the recall-oriented hyperparameter ? were tuned by cross-validation on train.
out as a final test set. This left 3,312 sentences/
48,408 words for training/development (train). Fea-
ture engineering and hyperparameter tuning were
conducted with 8-fold cross-validation on train. The
8-tag scheme is used except where otherwise noted.
In learning with the structured perceptron (algo-
rithm 1), we employ two well-known techniques that
can both be viewed as regularization. First, we use
the average of parameters over all timesteps of learn-
ing. Second, within each cross-validation fold, we de-
termine the number of training iterations (epochs) M
by early stopping?that is, after each iteration, we use
the model to decode the held-out data, and when that
accuracy ceases to improve, use the previous model.
The two hyperparameters are the number of iterations
and the value of the recall cost hyperparameter (?).
Both are tuned via cross-validation on train; we use
the multiple of 50 that maximizes average link-based
F1. The chosen values are shown in table 3. Experi-
ments were managed with the ducttape tool.18
6 Results
We experimentally address the following questions
to probe and justify our modeling approach.
6.1 Is supervised learning necessary?
Previous MWE identification studies have found
benefit to statistical learning over heuristic lexicon
lookup (Constant and Sigogne, 2011; Green et al.,
2012). Our first experiment tests whether this holds
for comprehensive MWE identification: it compares
our supervised tagging approach with baselines of
heuristic lookup on preexisting lexicons. The base-
lines construct a lattice for each sentence using the
same method as lexicon-based model features (?5.2).
If multiple lexicons are used, the union of their en-
18https://github.com/jhclark/ducttape/
tries is used to construct the lattice. The resulting
segmentation?which does not encode a strength
distinction?is evaluated against the gold standard.
Table 2 shows the results. Even with just the la-
beled training set as input, the supervised approach
beats the strongest heuristic baseline (that incorpo-
rates in-domain lexicon entries extracted from the
training data) by 30 precision points, while achieving
comparable recall. For example, the baseline (but not
the statistical model) incorrectly predicts an MWE in
places to eat in Baltimore (because eat in, meaning
?eat at home,? is listed in WordNet). The supervised
approach has learned not to trust WordNet too much
due to this sort of ambiguity. Downstream applica-
tions that currently use lexicon matching for MWE
identification (e.g., Ghoneim and Diab, 2013) likely
stand to benefit from our statistical approach.
6.2 How best to exploit MWE lexicons
(type-level information)?
For statistical tagging (right portion of table 2), using
more preexisting (out-of-domain) lexicons generally
improves recall; precision also improves a bit.
A lexicon of MWEs occurring in the non-held-out
training data at least twice19 (table 2, bottom right) is
marginally worse (better precision/worse recall) than
the best result using only preexisting lexicons.
6.3 Variations on the base model
We experiment with some of the modeling alterna-
tives discussed in ?5. Results appear in table 3 under
both the link-based and exact match evaluation cri-
teria. We note that the exact match scores are (as
expected) several points lower.
19If we train with access to the full lexicon of training
set MWEs, the learner credulously overfits to relying on that
lexicon?after all, it has perfect coverage of the training data!?
which proves fatal for the model at test time.
200
Recall-oriented cost. The recall-oriented cost
adds about 1 link-based F1 point, sacrificing precision
in favor of recall.
Unsupervised word clusters. When combined
with the recall-oriented cost, these produce a slight
improvement to precision/degradation to recall, im-
proving exact match F1 but not affecting link-based
F1. Only a few clusters receive high positive weight;
one of these consists of matter, joke, biggie, pun,
avail, clue, corkage, frills, worries, etc. These words
are diverse semantically, but all occur in collocations
with no, which is what makes the cluster coherent
and useful to the MWE model.
Oracle part-of-speech tags. Using human-
annotated rather than automatic POS tags improves
MWE identification by about 3 F1 points on test
(similar differences were observed in development).
6.4 What are the highest-weighted features?
An advantage of the linear modeling framework is
that we can examine learned feature weights to gain
some insight into the model?s behavior.
In general, the highest-weighted features are the
lexicon matching features and features indicative of
proper names (POS tag of proper noun, capitalized
word not at the beginning of the sentence, etc.).
Despite the occasional cluster capturing colloca-
tional or idiomatic groupings, as described in the
previous section, the clusters appear to be mostly
useful for identifying words that tend to belong (or
not) to proper names. For example, the cluster with
street, road, freeway, highway, airport, etc., as well
as words outside of the cluster vocabulary, weigh
in favor of an MWE. A cluster with everyday desti-
nations (neighborhood, doctor, hotel, bank, dentist)
prefers non-MWEs, presumably because these words
are not typically part of proper names in this corpus.
This was from the best model using non-oracle POS
tags, so the clusters are perhaps useful in correct-
ing for proper nouns that were mistakenly tagged
as common nouns. One caveat, though, is that it is
hard to discern the impact of these specific features
where others may be capturing essentially the same
information.
6.5 How heterogeneous are learned MWEs?
On test, the final model (with automatic POS tags)
predicts 365 MWE instances (31 are gappy; 23 are
POS pattern # examples (lowercased lemmas)
NOUN NOUN 53 customer service, oil change
VERB PREP 36 work with, deal with, yell at
PROPN PROPN 29 eagle transmission, comfort zone
ADJ NOUN 21 major award, top notch, mental health
VERB PART 20 move out, end up, pick up, pass up
VERB ADV 17 come back, come in, come by, stay away
PREP NOUN 12 on time, in fact, in cash, for instance
VERB NOUN 10 take care, make money, give crap
VERB PRON 10 thank you, get it
PREP PREP 8 out of, due to, out ta, in between
ADV ADV 6 no matter, up front, at all, early on
DET NOUN 6 a lot, a little, a bit, a deal
VERB DET NOUN 6 answer the phone, take a chance
NOUN PREP 5 kind of, care for, tip on, answer to
Table 4: Top predicted POS patterns and frequencies.
weak). There are 298 unique MWE types.
Organizing the predicted MWEs by their coarse
POS sequence reveals that the model is not too preju-
diced in the kinds of expressions it recognizes: the
298 types fall under 89 unique POS+strength patterns.
Table 4 shows the 14 POS sequences predicted 5 or
more times as strong MWEs. Some of the examples
(major award, a deal, tip on) are false positives, but
most are correct. Singleton patterns include PROPN
VERB (god forbid), PREP DET (at that), ADJ PRON
(worth it), and PREP VERB PREP (to die for).
True positive MWEs mostly consist of (a) named
entities, and (b) lexical idioms seen in training and/or
listed in one of the lexicons. Occasionally the sys-
tem correctly guesses an unseen and OOV idiom
based on features such as hyphenation (walk - in) and
capitalization/OOV words (Chili Relleno, BIG MIS-
TAKE). On test, 244 gold MWE types were unseen
in training; the system found 93 true positives (where
the type was predicted at least once), 109 false posi-
tives, and 151 false negatives?an unseen type recall
rate of 38%. Removing types that occurred in lexi-
cons leaves 35 true positives, 61 false positives, and
111 false negatives?a unseen and OOV type recall
rate of 24%.
6.6 What kinds of mismatches occur?
Inspection of the output turns up false positives due
to ambiguity (e.g., Spongy and sweet bread); false
negatives (top to bottom); and overlap (get high qual-
ity service, gold get high quality service; live up to,
gold live up to). A number of the mismatches turn
201
scheme ?Y ? ? M ?w? P R F1
no gaps, 1-level 3 100 2.1 733k 73.33 55.72 63.20
no gaps, 2-level 4 150 3.3 977k 72.60 59.11 65.09
gappy, 1-level 6 200 1.6 1,466k 66.48 61.26 63.65
gappy, 2-level 8 100 3.5 1,954k 73.27 60.44 66.15
Table 5: Training with different tagging schemes. Results
are cross-validation averages on train. All schemes are
evaluated against the full gold standard (8 tags).
out to be problems with the gold standard, like hav-
ing our water shut off (gold having our water shut
off ). This suggests that even noisy automatic taggers
might help identify annotation inconsistencies and
errors for manual correction.
6.7 Are gappiness and the strength distinction
learned in practice?
Three quarters of MWEs are strong and contain no
gaps. To see whether our model is actually sensi-
tive to the phenomena of gappiness and strength,
we train on data simplified to remove one or both
distinctions?as in the first 3 labelings in figure 2?
and evaluate against the full 8-tag scheme. For the
model with the recall cost, clusters, and oracle POS
tags, we evaluate each of these simplifications of
the training data in table 5. The gold standard for
evaluation remains the same across all conditions.
If the model was unable to recover gappy expres-
sions or the strong/weak distinction, we would expect
it to do no better when trained with the full tagset than
with the simplified tagset. However, there is some
loss in performance as the tagset for learning is sim-
plified, which suggests that gappiness and strength
are being learned to an extent.
7 Related Work
Our annotated corpus (Schneider et al., 2014) joins
several resources that indicate certain varieties of
MWEs: lexicons such as WordNet (Fellbaum, 1998),
SAID (Kuiper et al., 2003), and WikiMwe (Hartmann
et al., 2012); targeted lists (Baldwin, 2005, 2008;
Cook et al., 2008; Tu and Roth, 2011, 2012); web-
sites like Wiktionary and Phrases.net; and large-scale
corpora such as SemCor (Miller et al., 1993), the
French Treebank (Abeill? et al., 2003), the Szeged-
ParalellFX corpus (Vincze, 2012), and the Prague
Czech-English Dependency Treebank (C?mejrek et al.,
2005). The difference is that Schneider et al. (2014)
pursued a comprehensive annotation approach rather
than targeting specific varieties of MWEs or relying
on a preexisting lexical resource. The annotations
are shallow, not relying explicitly on syntax (though
in principle they could be mapped onto the parses in
the Web Treebank).
In terms of modeling, the use of machine learn-
ing classification (Hashimoto and Kawahara, 2008;
Shigeto et al., 2013) and specifically BIO sequence
tagging (Diab and Bhutada, 2009; Constant and Si-
gogne, 2011; Constant et al., 2012; Vincze et al.,
2013) for contextual recognition of MWEs is not
new. Lexical semantic classification tasks like named
entity recognition (e.g., Ratinov and Roth, 2009), su-
persense tagging (Ciaramita and Altun, 2006; Paa?
and Reichartz, 2009), and index term identification
(Newman et al., 2012) also involve chunking of cer-
tain MWEs. But our discriminative models, facili-
tated by the new corpus, broaden the scope of the
MWE identification task to include many varieties of
MWEs at once, including explicit marking of gaps
and a strength distinction. By contrast, the afore-
mentioned identification systems, as well as some
MWE-enhanced syntactic parsers (e.g., Green et al.,
2012), have been restricted to contiguous MWEs.
However, Green et al. (2011) allow gaps to be de-
scribed as constituents in a syntax tree. Gimpel and
Smith?s (2011) shallow, gappy language model al-
lows arbitrary token groupings within a sentence,
whereas our model imposes projectivity and nest-
ing constraints (?3). Blunsom and Baldwin (2006)
present a sequence model for HPSG supertagging,
and evaluate performance on discontinuous MWEs,
though the sequence model treats the non-adjacent
component supertags like other labels?it cannot en-
force that they mutually require one another, as we
do via the gappy tagging scheme (?3.1). The lexicon
lookup procedures of Bejc?ek et al. (2013) can match
gappy MWEs, but are nonstatistical and extremely
error-prone when tuned for high oracle recall.
Another major thread of research has pursued un-
supervised discovery of multiword types from raw
corpora, such as with statistical association measures
(Church et al., 1991; Pecina, 2010; Ramisch et al.,
2012, inter alia), parallel corpora (Melamed, 1997;
Moir?n and Tiedemann, 2006; Tsvetkov and Wint-
ner, 2010), or a combination thereof (Tsvetkov and
202
Wintner, 2011); this may be followed by a lookup-
and-classify approach to contextual identification
(Ramisch et al., 2010). Though preliminary experi-
ments with our models did not show benefit to incor-
porating such automatically constructed lexicons, we
hope these two perspectives can be brought together
in future work.
8 Conclusion
This article has presented the first supervised model
for identifying heterogeneous multiword expressions
in English text. Our feature-rich discriminative se-
quence tagger performs shallow chunking with a
novel scheme that allows for MWEs containing gaps,
and includes a strength distinction to separate highly
idiomatic expressions from collocations. It is trained
and evaluated on a corpus of English web reviews
that are comprehensively annotated for multiword
expressions. Beyond the training data, its features in-
corporate evidence from external resources?several
lexicons as well as unsupervised word clusters; we
show experimentally that this statistical approach is
far superior to identifying MWEs by heuristic lexicon
lookup alone. Future extensions might integrate addi-
tional features (e.g., exploiting statistical association
measures computed over large corpora), enhance the
lexical representation (e.g., by adding semantic tags),
improve the expressiveness of the model (e.g., with
higher-order features and inference), or integrate the
model with other tasks (such as parsing and transla-
tion).
Our data and open source software are released at
http://www.ark.cs.cmu.edu/LexSem/.
Acknowledgments
This research was supported in part by NSF CA-
REER grant IIS-1054319, Google through the Read-
ing is Believing project at CMU, and DARPA grant
FA8750-12-2-0342 funded under the DEFT program.
We are grateful to Kevin Knight, Martha Palmer,
Claire Bonial, Lori Levin, Ed Hovy, Tim Baldwin,
Omri Abend, members of JHU CLSP, the NLP group
at Berkeley, and the Noah?s ARK group at CMU, and
anonymous reviewers for valuable feedback.
A Basic Features
All are conjoined with the current label, yi.
Label Features
1. previous label (the only first-order feature)
Token Features
Original token
2. i = {1,2}
3. i = ?w??{0,1}
4. capitalized ? ?i = 0?
5. word shape
Lowercased token
6. prefix: [wi]k1 ?4k=1
7. suffix: [wi]?w?j ??w?j=?w??3
8. has digit
9. has non-alphanumeric c
10. context word: w j ?i+2j=i?2
11. context word bigram: w j+1j ?i+1j=i?2
Lemma Features
12. lemma + context lemma if one of them is a verb and the other
is a noun, verb, adjective, adverb, preposition, or particle: ?i ?
? j ?i+2j=i?2
Part-of-speech Features
13. context POS: pos j ?i+2j=i?2
14. context POS bigram: pos j+1j ?i+1j=i?2
15. word + context POS: wi?posi?1
16. context word + POS: wi?1?posi
Lexicon Features (unlexicalized)
WordNet only
17. OOV: ?i is not in WordNet as a unigram lemma ? posi
18. compound: non-punctuation lemma ?i and the {previous,
next} lemma in the sentence (if it is non-punctuation; an inter-
vening hyphen is allowed) form an entry in WordNet, possibly
separated by a hyphen or space
19. compound-hyphen: posi = HYPH ? previous and next tokens
form an entry in WordNet, possibly separated by a hyphen or
space
20. ambiguity class: if content word unigram ?i is in WordNet,
the set of POS categories it can belong to; else posi if not a
content POS ? the POS of the longest MW match to which ?i
belongs (if any) ? the position in that match (B or I)
For each multiword lexicon
21. lexicon name ? status of token i in the shortest path segmen-
tation (O, B, or I) ? subcategory of lexical entry whose match
includes token i, if matched ? whether the match is gappy
22. the above ? POS tags of the first and last matched tokens in
the expression
Over all multiword lexicons
23. at least k lexicons contain a match that includes this token (if
n ? 1 matches, n active features)
24. at least k lexicons contain a match that includes this token,
starts with a given POS, and ends with a given POS
203
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?
and Nancy Ide, editors, Treebanks, volume 20 of Text,
Speech and Language Technology, pages 165?187.
Kluwer Academic Publishers, Dordrecht, The Nether-
lands.
Timothy Baldwin. 2005. Looking for prepositional verbs
in corpus data. In Proc. of the Second ACL-SIGSEM
Workshop on the Linguistic Dimensions of Prepositions
and their Use in Computational Linguistics Formalisms
and Applications, pages 115?126. Colchester, UK.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of English verb-particle con-
structions. In Proc. of MWE, pages 1?2. Marrakech,
Morocco.
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. In Nitin Indurkhya and Fred J. Damerau,
editors, Handbook of Natural Language Processing,
Second Edition. CRC Press, Taylor and Francis Group,
Boca Raton, Florida, USA.
Eduard Bejc?ek, Pavel Stran??k, and Pavel Pecina. 2013.
Syntactic identification of occurrences of multiword
expressions in text using a lexicon with dependency
structures. In Proc. of the 9th Workshop on Multiword
Expressions, pages 106?115. Atlanta, Georgia, USA.
G?bor Berend. 2011. Opinion expression mining by ex-
ploiting keyphrase extraction. In Proc. of 5th Interna-
tional Joint Conference on Natural Language Process-
ing, pages 1162?1170. Chiang Mai, Thailand.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012a. English Web Treebank. Technical Report
LDC2012T13, Linguistic Data Consortium, Philadel-
phia, Pennsylvania, USA.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012b. English Web Treebank. Technical Report
LDC2012T13, Linguistic Data Consortium, Philadel-
phia, Pennsylvania, USA.
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natu-
ral Language Processing with Python: Analyzing Text
with the Natural Language Toolkit. O?Reilly Media,
Inc., Sebastopol, California, USA.
Phil Blunsom and Timothy Baldwin. 2006. Multilingual
deep lexical acquisition for HPSGs via supertagging.
In Proc. of EMNLP, pages 164?171. Sydney, Australia.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Marine Carpuat and Mona Diab. 2010. Task-based eval-
uation of multiword expressions: a pilot study in sta-
tistical machine translation. In Proc. of NAACL-HLT,
pages 242?245. Los Angeles, California, USA.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Hindle. 1991. Using statistics in lexical analysis.
In Uri Zernik, editor, Lexical acquisition: exploiting
on-line resources to build a lexicon, pages 115?164.
Lawrence Erlbaum Associates, Hillsdale, New Jersey,
USA.
Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-
coverage sense disambiguation and information extrac-
tion with a supersense sequence tagger. In Proc. of
EMNLP, pages 594?602. Sydney, Australia.
Michael Collins. 2002. Discriminative training methods
for Hidden Markov Models: theory and experiments
with perceptron algorithms. In Proc. of EMNLP, pages
1?8. Philadelphia, Pennsylvania, USA.
Matthieu Constant and Anthony Sigogne. 2011. MWU-
aware part-of-speech tagging with a CRF model and
lexical resources. In Proc. of the Workshop on Multi-
word Expressions: from Parsing and Generation to the
Real World, pages 49?56. Portland, Oregon, USA.
Matthieu Constant, Anthony Sigogne, and Patrick Watrin.
2012. Discriminative strategies to integrate multiword
expression recognition and parsing. In Proc. of ACL,
pages 204?212. Jeju Island, Korea.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2008.
The VNC-Tokens dataset. In Proc. of MWE, pages
19?22. Marrakech, Morocco.
Hal Daum?, III. 2006. Practical structured learning tech-
niques for natural language processing. Ph.D. disserta-
tion, University of Southern California, Los Angeles,
California, USA. URL http://hal3.name/docs/
daume06thesis.pdf.
Mona Diab and Pravin Bhutada. 2009. Verb noun con-
struction MWE token classification. In Proc. of MWE,
pages 17?22. Suntec, Singapore.
Nick C. Ellis, Rita Simpson-Vlach, and Carson Maynard.
2008. Formulaic language in native and second lan-
guage speakers: psycholinguistics, corpus linguistics,
and TESOL. TESOL Quarterly, 42(3):375?396.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge, Mas-
sachusetts, USA.
Charles J. Fillmore, Paul Kay, and Mary Catherine
O?Connor. 1988. Regularity and idiomaticity in gram-
matical constructions: the case of ?let alone?. Language,
64(3):501?538.
Yoav Freund and Robert E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
Mahmoud Ghoneim and Mona Diab. 2013. Multiword
expressions in the context of statistical machine trans-
204
lation. In Proc. of IJCNLP, pages 1181?1187. Nagoya,
Japan.
Kevin Gimpel and Noah A. Smith. 2011. Generative
models of monolingual and bilingual gappy patterns.
In Proc. of WMT, pages 512?522. Edinburgh, Scotland,
UK.
Adele E. Goldberg. 1995. Constructions: a construction
grammar approach to argument structure. University
of Chicago Press, Chicago, Illinois, USA.
Adele E. Goldberg. 2006. Constructions at work: the
nature of generalization in language. Oxford University
Press, Oxford, UK.
Edouard Grave, Guillaume Obozinski, and Francis Bach.
2013. Hidden Markov tree models for semantic class
induction. In Proc. of CoNLL, pages 94?103. Sofia,
Bulgaria.
Spence Green, Marie-Catherine de Marneffe, John Bauer,
and Christopher D. Manning. 2011. Multiword expres-
sion identification with tree substitution grammars: a
parsing tour de force with French. In Proc. of EMNLP,
pages 725?735. Edinburgh, Scotland, UK.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2012. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Jan Hajic?, Eva Hajic?ov?, Jarmila Panevov?, Petr Sgall,
Silvie Cinkov?, Eva Fuc??kov?, Marie Mikulov?, Petr
Pajas, Jan Popelka, Jir?? Semeck?, Jana ?indlerov?, Jan
?te?p?nek, Josef Toman, Zden?ka Ure?ov?, and Zdene?k
?abokrtsk?. 2012. Prague Czech-English Dependency
Treebank 2.0. Technical Report LDC2012T08, Linguis-
tic Data Consortium, Philadelphia, Pennsylvania, USA.
URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T10.
Silvana Hartmann, Gy?rgy Szarvas, and Iryna Gurevych.
2012. Mining multiword terms from Wikipedia. In
Maria Teresa Pazienza and Armando Stellato, editors,
Semi-Automatic Ontology Development. IGI Global,
Hershey, Pennsylvania, USA.
Chikara Hashimoto and Daisuke Kawahara. 2008. Con-
struction of an idiom corpus and its application to id-
iom identification based on WSD incorporating idiom-
specific features. In Proc. of EMNLP, pages 992?1001.
Honolulu, Hawaii, USA.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL-08: HLT, pages 595?603. Columbus, Ohio.
Koenraad Kuiper, Heather McCann, Heidi Quinn,
Therese Aitchison, and Kees van der Veer. 2003.
SAID. Technical Report LDC2003T10, Linguistic
Data Consortium, Philadelphia, Pennsylvania, USA.
URL http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2003T10.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: probabilistic
models for segmenting and labeling sequence data. In
Proc. of ICML, pages 282?289.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master?s thesis, Massachusetts In-
stitute of Technology, Cambridge, Massachusetts,
USA. URL http://people.csail.mit.edu/
pliang/papers/meng-thesis.pdf.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Proc.
of EMNLP, pages 97?108. Providence, Rhode Island,
USA.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance. In
Proc. of HLT, pages 303?308. Plainsboro, New Jersey,
USA.
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative
training. In Proc. of HLT-NAACL, pages 337?342.
Boston, Massachusetts, USA.
Behrang Mohit, Nathan Schneider, Rishav Bhowmick, Ke-
mal Oflazer, and Noah A. Smith. 2012. Recall-oriented
learning of named entities in Arabic Wikipedia. In
Proc. of EACL, pages 162?173. Avignon, France.
Begona Villada Moir?n and J?rg Tiedemann. 2006. Iden-
tifying idiomatic expressions using automatic word-
alignment. In Proc. of the EACL 2006 Workshop
on Multi-word Expressions in a Multilingual Context,
pages 33?40. Trento, Italy.
Rosamund Moon. 1998. Fixed expressions and idioms
in English: a corpus-based approach. Oxford Stud-
ies in Lexicography and Lexicology. Clarendon Press,
Oxford, UK.
David Newman, Nagendra Koilada, Jey Han Lau, and
Timothy Baldwin. 2012. Bayesian text segmentation
for index term identification and keyphrase extraction.
In Proc. of COLING 2012, pages 2077?2092. Mumbai,
India.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow. 1994.
Idioms. Language, 70(3):491?538.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL-HLT,
pages 380?390. Atlanta, Georgia, USA.
Gerhard Paa? and Frank Reichartz. 2009. Exploiting
205
semantic constraints for estimating supersenses with
CRFs. In Proc. of the Ninth SIAM International Confer-
ence on Data Mining, pages 485?496. Sparks, Nevada,
USA.
Pavel Pecina. 2010. Lexical association measures and
collocation extraction. Language Resources and Evalu-
ation, 44(1):137?158.
Carlos Ramisch. 2012. A generic and open
framework for multiword expressions treatment:
from acquisition to applications. Ph.D. disser-
tation, University of Grenoble and Federal Uni-
versity of Rio Grande do Sul, Grenoble, France.
URL http://www.inf.ufrgs.br/~ceramisch/
download_files/thesis-getalp.pdf.
Carlos Ramisch, Vitor De Araujo, and Aline Villavicencio.
2012. A broad evaluation of techniques for automatic
acquisition of multiword expressions. In Proc. of ACL
2012 Student Research Workshop, pages 1?6. Jeju Is-
land, Korea.
Carlos Ramisch, Aline Villavicencio, and Christian Boitet.
2010. mwetoolkit: a framework for multiword expres-
sion identification. In Proc. of LREC, pages 662?669.
Valletta, Malta.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using transformation-based learning. In Proc.
of the Third ACL Workshop on Very Large Corpora,
pages 82?94. Cambridge, Massachusetts, USA.
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of CoNLL, pages 147?155. Boulder, Colorado, USA.
Marta Recasens and Eduard Hovy. 2011. BLANC: Im-
plementing the Rand index for coreference evaluation.
Natural Language Engineering, 17(04):485?510.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011.
Named entity recognition in tweets: an experimental
study. In Proc. of EMNLP, pages 1524?1534. Edin-
burgh, Scotland, UK.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expressions:
a pain in the neck for NLP. In Alexander Gelbukh,
editor, Computational Linguistics and Intelligent Text
Processing, volume 2276 of Lecture Notes in Computer
Science, pages 189?206. Springer, Berlin, Germany.
Nathan Schneider, Spencer Onuffer, Nora Kazour, Emily
Danchik, Michael T. Mordowanec, Henrietta Conrad,
and Noah A. Smith. 2014. Comprehensive annotation
of multiword expressions in a social web corpus. In
Proc. of LREC. Reykjav?k, Iceland.
Yutaro Shigeto, Ai Azuma, Sorami Hisamoto, Shuhei
Kondo, Tomoya Kouse, Keisuke Sakaguchi, Akifumi
Yoshimoto, Frances Yung, and Yuji Matsumoto. 2013.
Construction of English MWE dictionary and its appli-
cation to POS tagging. In Proc. of the 9th Workshop
on Multiword Expressions, pages 139?144. Atlanta,
Georgia, USA.
James W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization of
finite automata theory. Journal of Computer and System
Sciences, 1(4):317?322.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173?180. Edmonton, Alberta,
Canada.
Yulia Tsvetkov and Shuly Wintner. 2010. Extraction of
multi-word expressions from small parallel corpora.
In Coling 2010: Posters, pages 1256?1264. Beijing,
China.
Yulia Tsvetkov and Shuly Wintner. 2011. Identification
of multi-word expressions by combining multiple lin-
guistic information sources. In Proc. of EMNLP, pages
836?845. Edinburgh, Scotland, UK.
Yuancheng Tu and Dan Roth. 2011. Learning English
light verb constructions: contextual or statistical. In
Proc. of the Workshop on Multiword Expressions: from
Parsing and Generation to the Real World, pages 31?39.
Portland, Oregon, USA.
Yuancheng Tu and Dan Roth. 2012. Sorting out the most
confusing English phrasal verbs. In Proc. of *SEM,
pages 65?69. Montr?al, Quebec, Canada.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: a simple and general
method for semi-supervised learning. In Proc. of ACL,
pages 384?394. Uppsala, Sweden.
Martin C?mejrek, Jan Cur??n, Jan Hajic?, and Jir?? Havelka.
2005. Prague Czech-English Dependency Treebank:
resource for structure-based MT. In Proc. of EAMT,
pages 73?78. Budapest, Hungary.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-theoretic
coreference scoring scheme. In Proc. of MUC-6, pages
45?52. Columbia, Maryland, USA.
Veronika Vincze. 2012. Light verb constructions in the
SzegedParalellFX English-Hungarian parallel corpus.
In Proc. of LREC. Istanbul, Turkey.
Veronika Vincze, Istv?n Nagy T., and J?nos Zsibrita. 2013.
Learning to detect English and Hungarian light verb
constructions. ACM Transactions on Speech and Lan-
guage Processing, 10(2):6:1?6:25.
206
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 70?74,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Learning Semantics and Selectional Preference of Adjective-Noun Pairs
Karl Moritz Hermann
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
karl.moritz.hermann@cs.ox.ac.uk
Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
cdyer@cs.cmu.edu
Phil Blunsom
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
phil.blunsom@cs.ox.ac.uk
Stephen Pulman
Department of Computer Science
University of Oxford
Oxford OX1 3QD, UK
stephen.pulman@cs.ox.ac.uk
Abstract
We investigate the semantic relationship be-
tween a noun and its adjectival modifiers.
We introduce a class of probabilistic mod-
els that enable us to to simultaneously cap-
ture both the semantic similarity of nouns
and modifiers, and adjective-noun selectional
preference. Through a combination of novel
and existing evaluations we test the degree to
which adjective-noun relationships can be cat-
egorised. We analyse the effect of lexical con-
text on these relationships, and the efficacy of
the latent semantic representation for disam-
biguating word meaning.
1 Introduction
Developing models of the meanings of words and
phrases is a key challenge for computational linguis-
tics. Distributed representations are useful in captur-
ing such meaning for individual words (Sato et al,
2008; Maas and Ng, 2010; Curran, 2005). How-
ever, finding a compelling account of semantic com-
positionality that utilises such representations has
proven more difficult and is an active research topic
(Mitchell and Lapata, 2008; Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). It is in
this area that our paper makes its contribution.
The dominant approaches to distributional se-
mantics have relied on relatively simple frequency
counting techniques. However, such approaches fail
to generalise to the much sparser distributions en-
countered when modeling compositional processes
and provide no account of selectional preference.
We propose a probabilistic model of the semantic
representations for nouns and modifiers. The foun-
dation of this model is a latent variable representa-
tion of noun and adjective semantics together with
their compositional probabilities. We employ this
formulation to give a dual view of noun-modifier
semantics: the induced latent variables provide an
explicit account of selectional preference while the
marginal distributions of the latent variables for each
word implicitly produce a distributed representation.
Most related work on selectional preference uses
class-based probabilities to approximate (sparse)
individual probabilities. Relevant papers include
O? Se?aghdha (2010), who evaluates several topic
models adapted to learning selectional preference
using co-occurence and Baroni and Zamparelli
(2010), who represent nouns as vectors and adjec-
tives as matrices, thus treating them as functions
over noun meaning. Again, inference is achieved
using co-occurrence and dimensionality reduction.
2 Adjective-Noun Model
We hypothesize that semantic classes determine the
semantic characteristics of nouns and adjectives, and
that the distribution of either with respect to other
components of the sentences they occur in is also
mediated by these classes (i.e., not by the words
themselves). We assume that in general nouns select
for adjectives,1 and that this selection is dependent
on both their latent semantic classes. In the next sec-
tion, we describe a model encoding our hypotheses.
2.1 Generative Process
We model a corpus D of tuples of the form
(n,m, c1 . . . ck) consisting of a noun n, an adjective
m (modifier), and k words of context. The context
variables (c1 . . . ck) are treated as a bag of words and
1We evaluate this hypothesis as well as its inverse.
70
|N| |M|
|N|
N M
n m
c
k
|D|
?
N
?
N
?
M
?
M
?
c
?
c
|N|
?
n
?
m
?
n
?
m
Figure 1: Plate diagram illustrating our model of noun
and modifier semantic classes (designated N and M , re-
spectively), a modifier-noun pair (m,n), and its context.
include the words to the left and right of the noun,
its siblings and governing verbs. We designate the
vocabulary Vn for nouns, Vm for modifiers and Vc
for context. We use zi to refer to the ith tuple in D
and refer to variables within that tuple by subscript-
ing them with i, e.g., ni and c3,i are the noun and
the third context variable of zi. The latent noun and
adjective class variables are designated Ni and Mi.
The corpus D is generated according to the plate
diagram in figure 1. First, a set of parameters is
drawn. A multinomial ?N representing the distribu-
tion of noun semantic classes in the corpus is drawn
from a Dirichlet distribution with parameter ?N. For
each noun class i we have distributions ?Mi over
adjective classes, ?ni over Vn and ?
c
i over Vc, also
drawn from Dirichlet distributions. Finally, for each
adjective class j, we have distributions ?mj over Vm.
Next, the contents of the corpus are generated by
first drawing the length of the corpus (we do not
parametrise this since we never generate from this
model). Then, for each i, we generate noun class
Ni, adjective class Mi, and the tuple zi as follows:
Ni | ?
N ? Multi(?N)
Mi | ?
M
Ni? Multi(?
M
Ni)
ni | ?
n
Ni? Multi(?
n
Ni)
mi | ?
m
Mi? Multi(?
m
Mi)
?k: ck,i | ?
c
Ni? Multi(?
c
Ni)
2.2 Parameterization and Inference
We use Gibbs sampling to estimate the distributions
ofN andM , integrating out the multinomial param-
eters ?x (Griffiths and Steyvers, 2004). The Dirich-
let parameters ? are drawn independently from a
?(1, 1) distribution, and are resampled using slice
sampling at frequent intervals throughout the sam-
pling process (Johnson and Goldwater, 2009). This
?vague? prior encourages sparse draws from the
Dirichlet distribution. The number of noun and ad-
jective classes N and M was set to 50 each; other
sizes (100,150) did not significantly alter results.
3 Experiments
As our model was developed on the basis of several
hypotheses, we design the experiments and evalu-
ation so that these hypotheses can be examined on
their individual merit. We test the first hypothesis,
that nouns and adjectives can be represented by se-
mantic classes, recoverable using co-occurence, us-
ing a sense clustering evaluation by Ciaramita and
Johnson (2003). The second hypothesis, that the dis-
tribution with respect to context and to each other is
governed by these semantic classes is evaluated us-
ing pseudo-disambiguation (Clark and Weir, 2002;
Pereira et al, 1993; Rooth et al, 1999) and bigram
plausibility (Keller and Lapata, 2003) tests.
To test whether noun classes indeed select for ad-
jective classes, we also evaluate an inverse model
(Modi), where the adjective class is drawn first, in
turn generating both context and the noun class. In
addition, we evaluate copies of both models ignoring
context (Modnc and Modinc).
We use the British National Corpus (BNC), train-
ing on 90 percent and testing on 10 percent of the
corpus. Results are reported after 2,000 iterations
including a burn-in period of 200 iterations. Classes
are marginalised over every 10th iteration.
4 Evaluation
4.1 Supersense Tagging
Supersense tagging (Ciaramita and Johnson, 2003;
Curran, 2005) evaluates a model?s ability to clus-
ter words by their semantics. The task of this eval-
uation is to determine the WORDNET supersenses
of a given list of nouns. We report results on the
WN1.6 test set as defined by Ciaramita and John-
son (2003), who used 755 randomly selected nouns
with a unique supersense from the WORDNET 1.6
71
corpus. As their test set was random, results weren?t
exactly replicable. For a fair comparison, we select
all suitable nouns from the corpus that also appeared
in the training corpus. We report results on type and
token level (52314 tokens with 1119 types). The
baseline2 chooses the most common supersense.
k Token Type
Baseline .241 .210
Ciaramita & Johnson .523 .534
Curran - .680
Mod 10 .592 .517
Modnc 10 .473 .410
Table 1: Supersense evaluation results. Values are the
percentage of correctly assigned supersenses. k indicates
the number of nearest neighbours considered.
We use cosine-similarity on the marginal noun
class vectors to measure distance between nouns.
Each noun in the test set is then assigned a su-
persense by performing a distance-weighted voting
among its k nearest neighbours. Results of this eval-
uation are shown in Table 1, with Figure 2 showing
scores for model Mod across different values for k.
Figure 2: Scores of Mod on the supersense task. The up-
per line denotes token-, the lower type-level scores. The
y-axis is the percentage of correct assignments, the x-axis
denotes the number of neighbours included in the vote.
The results demonstrate that nouns can semanti-
cally be represented as members of latent classes,
while the superiority of Mod over Modnc supports
our hypothesis that context co-occurence is a key
feature for learning these classes.
4.2 Pseudo-Disambiguation
Pseudo-disambiguation was introduced by Clark
and Weir (2002) to evaluate models of selectional
preference. The task is to select the more probable
of two candidate arguments to associate with a given
2The baseline results are from Ciaramita and Johnson
(2003). Using the majority baseline on the full test set, we only
get .176 and .160 for token and type respectively.
predicate. For us, this is to decide which adjective,
a1 or a2, is more likely to modify a noun n.
We follow the approach by Clark and Weir (2002)
to create the test data. To improve the quality of
the data, we filtered using bigram counts from the
Web1T corpus, setting a lower bound on the proba-
ble bigram (a1, n) and chosing a2 from five candi-
dates, picking the lowest count for bigram (a2, n).
We report results for all variants of our model in
Table 2. As baseline we use unigram counts in our
training data, chosing the more frequent adjective.
L-bound 0 100 500 1000
Size 5714 5253 3741 2789
Baseline .543 .543 .539 .550
Mod .783 .792 .810 .816
Modi .781 .787 .800 .810
Modnc .720 .728 .746 .750
Modinc .722 .730 .747 .752
Table 2: Pseudo-disambiguation: Percentage of correct
choices made. L-bound denotes the Web1T lower bound
on the (a1, n) bigram, size the number of decisions made.
While all models decisively beat the baseline, the
models using context strongly outperform those that
do not. This supports our hypothesis regarding the
importance of context in semantic clustering.
The similarity between the normal and inverse
models implies that the direction of the noun-
adjective relationship has negligible impact for this
evaluation.
4.3 Bigram Plausibility
Bigram plausibility (Keller and Lapata, 2003) is a
second evaluation for selectional preference. Unlike
the frequency-based pseudo-disambiguation task, it
evaluates how well a model matches human judge-
ment of the plausibility of adjective-noun pairs.
Keller and Lapata (2003) demonstrated a correlation
between frequencies and plausibility, but this does
not sufficiently explain human judgement. An ex-
ample taken from their unseen data set illustrates the
dissociation between frequency and plausibility:
? Frequent, implausible: ?educational water?
? Infrequent, plausible: ?difficult foreigner?3
The plausibility evaluation has two data sets of 90
adjective-noun pairs each. The first set (seen) con-
tains random bigrams from the BNC. The second set
(unseen) are bigrams not contained in the BNC.
3At the time of writing, Google estimates 56,900 hits for
?educational water? and 575 hits for ?difficult foreigner?. ?Ed-
ucational water? ranks bottom in the gold standard of the unseen
set, ?difficult foreigner? ranks in the top ten.
72
Recent work (O? Se?aghdha, 2010; Erk et al,
2010) approximated plausibility with joint probabil-
ity (JP). We believe that for semantic plausibility
(not probability!) mutual information (MI), which
factors out acutal frequencies, is a better metric.4 We
report results using JP, MI and MI?2.
Seen Unseen
r ? r ?
AltaVista .650 ? .480 ?
BNC (Rasp) .543 .622 .135 .102
Pado? et al .479 .570 .120 .138
LDA .594 .558 .468 .459
ROOTH-LDA .575 .599 .501 .469
DUAL-LDA .460 .400 .334 .278
Mod (JP) .495 .413 .286 .276
Mod (MI) .394 .425 .471 .457
Mod (MI?2) .575 .501 .430 .408
Modnc (JP) .626 .505 .357 .369
Modnc (MI) .628 .574 .427 .385
Modnc (MI?2) .701 .623 .423 .394
Table 3: Results (Pearson r and Spearman ? correlations)
on the Keller and Lapata (2003) plausibility data. Bold
indicates best scores, underlining our best scores. High
values indicate high correlation with the gold standard.
Table 3 shows the performance of our models
compared to results reported in O? Se?aghdha (2010).
As before, results between the normal and the in-
verse model (omitted due to space) are very simi-
lar. Surprisingly, the no-context models consistently
outperform the models using context on the seen
data set. This suggests that the seen data set can
quite precisely be ranked using frequency estimates,
which the no-context models might be better at cap-
turing without the ?noise? introduced by context.
Standard Inverse (i)
r ? r ?
Mod (JP) .286 .276 .243 .245
Mod (MI) .471 .457 .409 .383
Mod (MI?2) .430 .408 .362 .347
Modnc (JP) .357 .369 .181 .161
Modnc (MI) .427 .385 .220 .209
Modnc (MI?2) .423 .394 .218 .185
Table 4: Results on the unseen plausibility dataset.
The results on the unseen data set (Table 4)
prove interesting as well. The inverse no-context
model is performing significantly poorer than any
of the other models. To understand this result we
must investigate the differences between the unseen
data set and the seen data set and to the pseudo-
disambiguation evaluation. The key difference to
pseudo-disambiguation is that we measure a human
4See (Evert, 2005) for a discussion of these metrics.
plausibility judgement, which ? as we have demon-
strated ? only partially correlates with bigram fre-
quencies. Our models were trained on the BNC,
hence they could only learn frequency estimates for
the seen data set, but not for the unseen data.
Based on our hypothesis about the role of con-
text, we expect Mod and Modi to learn semantic
classes based on the distribution of context. Without
the access to that context, we argued thatModnc and
Modinc would instead learn frequency estimates.5
The hypothesis that nouns generally select for ad-
jectives rather than vice versa further suggests that
Mod and Modnc would learn semantic properties
that Modi and Modinc could not learn so well.
In summary, we hence expected Mod to perform
best on the unseen data, learning semantics from
both context and noun-adjective selection. Also, as
supported by the results, we expected Modinc to
performs poorly, as it is the model least capable of
learning semantics according to our hypotheses.
5 Conclusion
We have presented a class of probabilistic mod-
els which successfully learn semantic clusterings of
nouns and a representation of adjective-noun selec-
tional preference. These models encoded our beliefs
about how adjective-noun pairs relate to each other
and to the other words in the sentence. The perfor-
mance of our models on estimating selectional pref-
erence strongly supported these initial hypotheses.
We discussed plausibility judgements from a the-
oretical perspective and argued that frequency esti-
mates and JP are imperfect approximations for plau-
sibility. While models can perform well on some
evaluations by using either frequency estimates or
semantic knowledge, we explained why this does
not apply to the unseen plausibility test. The perfor-
mance on that task demonstrates both the success of
our model and the shortcomings of frequency-based
approaches to human plausibility judgements.
Finally, this paper demonstrated that it is feasi-
ble to learn semantic representations of words while
concurrently learning how they relate to one another.
Future work will explore learning words from
broader classes of semantic relations and the role of
context in greater detail. Also, we will evaluate the
system applied to higher level tasks.
5This could also explain their weaker performance on
pseudo-disambiguation in the previous section, where the neg-
ative examples had zero frequency in the training corpus.
73
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense tagging of unknown nouns in wordnet. In
Proceedings of the 2003 conference on Empirical
methods in natural language processing, EMNLP ?03,
pages 168?175, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Comput.
Linguist., 28:187?206, June.
James R. Curran. 2005. Supersense tagging of unknown
nouns using semantic similarity. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?05, pages 26?33, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Katrin Erk, Sebastian Pado?, and Ulrike Pado?. 2010. A
flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics,
36:723?763.
Stefan Evert. 2005. The statistics of word cooccur-
rences: word pairs and collocations. Ph.D. the-
sis, Universita?t Stuttgart, Holzgartenstr. 16, 70174
Stuttgart.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?11, pages 1394?1404,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101:5228?5235.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 317?325, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Frank Keller and Mirella Lapata. 2003. Using the web to
obtain frequencies for unseen bigrams. Computational
Linguistics, pages 459?484.
Andrew L. Maas and Andrew Y. Ng. 2010. A probabilis-
tic model for semantic word vectors. In Workshop on
Deep Learning and Unsupervised Feature Learning,
NIPS ?10.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL-HLT?08,
pages 236 ? 244.
Diarmuid O? Se?aghdha. 2010. Latent variable models
of selectional preference. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 435?444, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In Pro-
ceedings of the 31st annual meeting on Association for
Computational Linguistics, ACL ?93, pages 183?190,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing a semantically
annotated lexicon via EM-based clustering. In Pro-
ceedings of the 37th annual meeting of the Association
for Computational Linguistics on Computational Lin-
guistics, ACL ?99, pages 104?111, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Issei Sato, Minoru Yoshida, and Hiroshi Nakagawa.
2008. Knowledge discovery of semantic relationships
between words using nonparametric bayesian graph
model. In Proceeding of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, KDD ?08, pages 587?595, New York,
NY, USA. ACM.
74
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 176?180,
Dublin, Ireland, August 23-24, 2014.
CMU: Arc-Factored, Discriminative Semantic Dependency Parsing
Sam Thomson Brendan O?Connor Jeffrey Flanigan David Bamman
Jesse Dodge Swabha Swayamdipta Nathan Schneider Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{sthomson,brenocon,jflanigan,dbamman,jessed,
swabha,nschneid,cdyer,nasmith}@cs.cmu.edu
Abstract
We present an arc-factored statistical model
for semantic dependency parsing, as de-
fined by the SemEval 2014 Shared Task 8
on Broad-Coverage Semantic Dependency
Parsing. Our entry in the open track placed
second in the competition.
1 Introduction
The task of broad coverage semantic dependency
parsing aims to provide a shallow semantic analysis
of text not limited to a specific domain. As distinct
from deeper semantic analysis (e.g., parsing to a
full lambda-calculus logical form), shallow seman-
tic parsing captures relationships between pairs
of words or concepts in a sentence, and has wide
application for information extraction, knowledge
base population, and question answering (among
others).
We present here two systems that produce seman-
tic dependency parses in the three formalisms of the
SemEval 2014 Shared Task 8 on Broad-Coverage
Semantic Dependency Parsing (Oepen et al., 2014).
These systems generate parses by extracting fea-
tures for each potential dependency arc and learn-
ing a statistical model to discriminate between good
arcs and bad; the first treats each labeled edge de-
cision as an independent multiclass logistic regres-
sion (?3.2.1), while the second predicts arcs as part
of a graph-based structured support vector machine
(?3.2.2). Common to both models is a rich set of
features on arcs, described in ?3.2.3. We include a
discussion of features found to have no discernable
effect, or negative effect, during development (?4).
Our system placed second in the open track of
the Broad-Coverage Semantic Dependency Parsing
This work is licensed under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
Figure 1: Example annotations for DM (top), PAS (middle),
and PCEDT (bottom).
task (in which output from syntactic parsers and
other outside resources can be used). We present
our results in ?5.
2 Formalisms
The Shared Task 8 dataset consists of annota-
tions of the WSJ Corpus in three different se-
mantic dependency formalisms. DM is derived
from LinGO English Resource Grammar (ERG)
annotations in DeepBank (Flickinger et al., 2012).
PAS is derived from the Enju HPSG treebank us-
ing the conversion rules of Miyao et al. (2004).
PCEDT is derived from the tectogrammatical layer
of the Prague Czech-English Dependency Treebank
(Haji?c, 1998). See Figure 1 for an example.
The three formalisms come from very different
linguistic theories, but all are represented as labeled
directed graphs, with words as vertices, and all
have ?top? annotations, corresponding roughly to
the semantic focus of the sentence. (A ?top? need
not be a root of the graph.) This allows us to use
the same machinery (?3) for training and testing
statistical models for the three formalisms.
3 Models
We treat the problem as a three-stage pipeline. The
first stage prunes words by predicting whether they
have any incoming or outgoing edges at all (?3.1);
if a word does not, then it is not considered for
any attachments in later stages. The second stage
176
predicts where edges are present, and their labels
(?3.2). The third stage predicts whether a predicate
word is a top or not (?3.3). Formalisms sometimes
annotate more than one ?top? per sentence, but we
found that we achieve the best performance on all
formalisms by predicting only the one best-scoring
?top? under the model.
3.1 Singleton Classification
For each formalism, we train a classifier to rec-
ognize singletons, nodes that have no parents or
children. (For example, punctuation tokens are of-
ten singletons.) This makes the system faster with-
out affecting accuracy. For singleton prediction,
we use a token-level logistic regression classifier,
with features including the word, its lemma, and
its part-of-speech tag. If the classifier predicts a
probability of 99% or higher the token is pruned;
this removes around 10% of tokens. (The classi-
fier performs differently on different formalisms;
on PAS it has perfect accuracy, while on DM and
PCEDT accuracy is in the mid-90?s.)
3.2 Edge Prediction
In the second stage of the pipeline, we predict the
set of labeled directed edges in the graph. We use
the same set of edge-factored features (?3.2.3) in
two alternative models: an edge-independent mul-
ticlass logistic regression model (LOGISTICEDGE,
?3.2.1); and a structured SVM (Taskar et al., 2003;
Tsochantaridis et al., 2004) that enforces a deter-
minism constraint for certain labels, which allows
each word to have at most one outgoing edge with
that label (SVMEDGE, ?3.2.2). For each formalism,
we trained both models with varying features en-
abled and hyperparameter settings and submitted
the configuration that produced the best labeled F
1
on the development set. For DM and PCEDT, this
was LOGISTICEDGE; for PAS, this was SVMEDGE.
We report results only for the submitted configu-
rations, with different features enabled. Due to
time constraints, full hyperparameter sweeps and
comparable feature sweeps were not possible.
3.2.1 LOGISTICEDGE Parser
The LOGISTICEDGE model considers only token
index pairs (i, j) where |i ? j| ? 10, i 6= j,
and both t
i
and t
j
have been predicted to be non-
singletons by the first stage. Although this prunes
some gold edges, among the formalisms, 95%?97%
of all gold edges are between tokens of distance
10 or less. Both directions i ? j and j ? i are
considered between every pair.
Let L be the set of K + 1 possible output labels:
the formalism?s original K edge labels, plus the
additional label NOEDGE, which indicates that no
edge exists from i to j. The model treats every pair
of token indices (i, j) as an independent multiclass
logistic regression over output space L. Let x be
an input sentence. For candidate parent index i,
child index j, and edge label `, we extract a feature
vector f(x, i, j, `), where ` is conjoined with every
feature described in ?3.2.3. The multiclass logis-
tic regression model defines a distribution over L,
parametrized by weights ?:
P (` | ?, x, i, j) =
exp{? ? f(x, i, j, `)}
?
`
?
?L
exp{? ? f(x, i, j, `
?
)}
.
? is learned by minimizing total negative log-
likelihood of the above (with weighting; see be-
low), plus `
2
regularization. AdaGrad (Duchi et al.,
2011) is used for optimization. This seemed to opti-
mize faster than L-BFGS (Liu and Nocedal, 1989),
at least for earlier iterations, though we did no sys-
tematic comparison. Stochastic gradient steps are
applied one at a time from individual examples,
and a gradient step for the regularizer is applied
once per epoch.
The output labels have a class imbalance; in all
three formalisms, there are many more NOEDGE
examples than true edge examples. We improved
F
1
performance by downweighting NOEDGE
examples through a weighted log-likelihood
objective,
?
i,j
?
`
w
`
logP (` |?, x, i, j), with
w
NOEDGE
= 0.3 (selected on development set) and
w
`
= 1 otherwise.
Decoding: To predict a graph structure at test-time
for a new sentence, the most likely edge label is pre-
dicted for every candidate (i, j) pair of unpruned
tokens. If an edge is predicted for both directions
for a single (i, j) pair, only the edge with the higher
score is chosen. (There are no such bidirectional
edges in the training data.) This post-processing ac-
tually did not improve accuracy on DM or PCEDT;
it did improve PAS by ?0.2% absolute F
1
, but we
did not submit LOGISTICEDGE for PAS.
3.2.2 SVMEDGE Parser
In the SVMEDGE model, we use a structured SVM
with a determinism constraint. This constraint en-
sures that each word token has at most one outgoing
edge for each label in a set of deterministic labels
L
d
. For example, in DM a predicate never has more
177
than one child with edge label ?ARG1.? L
d
was
chosen to be the set of edges that were > 99.9%
deterministic in the training data.
1
Consider the fully dense graph of all edges be-
tween all words predicted as not singletons by the
singleton classifier ?3.1 (in all directions with all
possible labels). Unlike LOGISTICEDGE, the la-
bel set L does not include an explicit NOEDGE
label. If ? denotes the model weights, and f de-
notes the features, then an edge from i to j with
label ` in the dense graph has a weight c(i, j, `)
assigned to it using the linear scoring function
c(i, j, `) = ? ? f(x, i, j, `).
Decoding: For each node and each label `, if ` ?
L
d
, the decoder adds the highest scoring outgoing
edge, if its weight is positive. For ` 6? L
d
, every
outgoing edge with positive weight is added. This
procedure is guaranteed to find the highest scoring
subgraph (largest sum of edge weights) of the dense
graph subject to the determinism constraints. Its
runtime is O(n
2
).
The model weights are trained using the struc-
tured SVM loss. If x is a sentence and y is a
graph over that sentence, let the features be de-
noted f(x, y) =
?
(i,j,`)?y
f(x, i, j, `). The SVM
loss for each training example (x
i
, y
i
) is:
??
>
f(x
i
, y
i
)+max
y
?
>
f(x
i
, y)+cost(y, y
i
)
where cost(y, y
i
) = ?|y \ y
i
| + ?|y
i
\ y|. ? and
? trade off between precision and recall for the
edges (Gimpel and Smith, 2010). The loss is min-
imized with AdaGrad using early-stopping on a
development set.
3.2.3 Edge Features
Table 1 describes the features we used for predict-
ing edges. These features were computed over an
edge e with parent token s at index i and child
token t at index j. Unless otherwise stated, each
feature template listed has an indicator feature that
fires for each value it can take on. For the sub-
mitted results, LOGISTICEDGE uses all features
except Dependency Path v2, POS Path, and Dis-
tance Thresholds, and SVMEDGE uses all features
except Dependency Path v1. This was due to
SVMEDGE being faster to train than LOGISTIC-
EDGE when including POS Path features, and due
1
By this we mean that of the nodes that have at least
one outgoing ` edge, 99.9% of them have only one outgo-
ing ` edge. For DM, L
d
= L\{? and c,? ? or c,? ? then c,?
?loc,? ?mwe,? ?subord?}; for PAS, L
d
= L; and for PCEDT,
L
d
={?DPHR,? ?INTF,? ?VOCAT?}.
Tokens: The tokens s and t themselves.
Lemmas: Lemmas of s and t.
POS tags: Part of speech tags of s and t.
Linear Order: Fires if i < j.
Linear Distance: i? j.
Dependency Path v1 (LOGISTICEDGE only): The
concatenation of all POS tags, arc labels and up/down
directions on the path in the syntactic dependency tree
from s to t. Conjoined with s, with t, and without either.
Dependency Path v2 (SVMEDGE only): Same as De-
pendency Path v1, but with the lemma of s or t instead
of the word, and substituting the token for any ?IN? POS
tag.
Up/Down Dependency Path: The sequence of upward
and downward moves needed to get from s to t in the
syntactic dependency tree.
Up/Down/Left/Right Dependency Path: The unla-
beled path through the syntactic dependency tree from s
to t, annotated with whether each step through the tree
was up or down, and whether it was to the right or left in
the sentence.
Is Parent: Fires if s is the parent of t in the syntactic
dependency parse.
Dependency Path Length: Distance between s and t in
the syntactic dependency parse.
POS Context: Concatenated POS tags of tokens at i?1,
i, i+ 1, j ? 1, j, and j + 1. Concatenated POS tags of
tokens at i? 1, i, j ? 1, and j. Concatenated POS tags
of tokens at i, i+ 1, j, and j + 1.
Subcategorization Sequence: The sequence of depen-
dency arc labels out of s, ordered by the index of the
child. Distinguish left children from right children. If t
is a direct child of s, distinguish its arc label with a ?+?.
Conjoin this sequence with the POS tag of s.
Subcategorization Sequence with POS: As above, but
add the POS tag of each child to its arc label.
POS Path (SVMEDGE only): Concatenated POS tags
between and including i and j. Conjoined with head
lemma, with dependent lemma, and without either.
Distance Thresholds (SVMEDGE only): Fires for ev-
ery integer between 1 and blog(|i? j|+1)/ log(1.39)c
inclusive.
Table 1: Features used in edge prediction
to time constraints for the submission we were un-
able to retrain LOGISTICEDGE with these features.
3.2.4 Feature Hashing
The biggest memory usage was in the map from
feature names to integer indices during feature
extraction. For experimental expedience, we im-
plemented multitask feature hashing (Weinberger
et al., 2009), which hashes feature names to indices,
under the theory that errors due to collisions tend
to cancel. No drop in accuracy was observed.
3.3 Top Prediction
We trained a separate token-level binary logistic
regression model to classify whether a token?s node
had the ?top? attribute or not. At decoding time, all
predicted predicates (i.e., nodes where there is at
178
least one outbound edge) are possible candidates
to be ?top?; the classifier probabilities are evalu-
ated, and the highest-scoring node is chosen to be
?top.? This is suboptimal, since some graphs have
multiple tops (in PCEDT this is more common);
but selection rules based on probability thresholds
gave worse F
1
performance on the dev set. For a
given token t at index i, the top classifier?s features
included t?s POS tag, i, those two conjoined, and
the depth of t in the syntactic dependency tree.
4 Negative Results
We followed a forward-selection process during
feature engineering. For each potential feature,
we tested the current feature set versus the current
feature set plus the new potential feature. If the
new feature did not improve performance, we did
not add it. We list in table 2 some of the features
which we tested but did not improve performance.
In order to save time, we ran these feature se-
lection experiments on a subsample of the training
data, for a reduced number of iterations. These re-
sults thus have a strong caveat that the experiments
were not exhaustive. It may be that some of these
features could help under more careful study.
5 Experimental Setup
We participated in the Open Track, and used the
syntactic dependency parses supplied by the orga-
nizers. Feature engineering was performed on a
development set (?20), training on ??00?19. We
evaluate labeled precision (LP), labeled recall (LR),
labeled F
1
(LF), and labeled whole-sentence match
(LM) on the held-out test data using the evaluation
script provided by the organizers. LF was aver-
aged over the formalisms to determine the winning
system. Table 3 shows our scores.
6 Conclusion and Future Work
We found that feature-rich discriminative models
perform well at the task of mapping from sentences
to semantic dependency parses. While our final
approach is fairly standard for work in parsing,
we note here additional features and constraints
which did not appear to help (contrary to expecta-
tion). There are a number of clear extensions to
this work that could improve performance. While
an edge-factored model allows for efficient infer-
ence, there is much to be gained from higher-order
features (McDonald and Pereira, 2006; Martins
et al., 2013). The amount of information shared
Word vectors: Features derived from 64-dimensional
vectors from (Faruqui and Dyer, 2014), including the
concatenation, difference, inner product, and element-
wise multiplication of the two vectors associated with
a parent-child edge. We also trained a Random Forest
on the word vectors using Liaw and Wiener?s (2002) R
implementation. The predicted labels were then used as
features in LOGISTICEDGE.
Brown clusters Features derived from Brown clusters
(Brown et al., 1992) trained on a large corpus of web data.
Parent, child, and conjoined parent-child edge features
from cluster prefixes of length 2, 4, 6, 8, 10, and 12.
Conjunctions of those features with the POS tags of the
parent and child tokens.
Active/passive: Active/passive voice feature (as in Jo-
hansson and Nugues (2008)) conjoined with both the
Linear Distance features and the Subcategorization Se-
quence features. Voice information may already be cap-
tured by features from the Stanford dependency?style
parses, which include passivization information in arc
labels such as nsubjpass and auxpass (de Marneffe and
Manning, 2008).
Connectivity constraint: Enforcing that the graph is
connected (ignoring singletons), similar to Flanigan et al.
(2014). Almost all semantic dependency graphs in the
training data are connected (ignoring singletons), but
we found that enforcing this constraint significantly hurt
precision.
Tree constraint: Enforces that the graph is a tree. Un-
surprisingly, we found that enforcing a tree constraint
hurt performance.
Table 2: Features and constraints giving negative results.
LP LR LF LM
DM 0.8446 0.8348 0.8397 0.0875
PAS 0.9078 0.8851 0.8963 0.2604
PCEDT 0.7681 0.7072 0.7364 0.0712
Average 0.8402 0.8090 0.8241 0.1397
Table 3: Labeled precision (LP), recall (LR), F
1
(LF), and
whole-sentence match (LM) on the held-out test data.
between the three formalisms suggests that a multi-
task learning (Evgeniou and Pontil, 2004) frame-
work could lead to gains. And finally, there is
additional structure in the formalisms which could
be exploited (such as the deterministic processes
by which an original PCEDT tree annotation was
converted into a graph); formulating more subtle
graph constraints to capture this a priori knowl-
edge could lead to improved performance. We
leave such explorations to future work.
Acknowledgements
We are grateful to Manaal Faruqui for his help in word vector
experiments, and to reviewers for helpful comments. The re-
search reported in this paper was sponsored by the U.S. Army
Research Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533, DARPA
grant FA8750-12-2-0342 funded under the DEFT program,
U.S. NSF grants IIS-1251131 and IIS-1054319, and Google?s
support of the Reading is Believing project at CMU.
179
References
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Lin-
guistics, 18(4):467?479.
Marie-Catherine de Marneffe and Christopher D. Manning.
2008. The Stanford typed dependencies representation. In
Coling 2008: Proc. of the Workshop on Cross-Framework
and Cross-Domain Parser Evaluation, pages 1?8. Manch-
ester, UK.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adap-
tive subgradient methods for online learning and stochas-
tic optimization. Journal of Machine Learning Research,
12:2121?2159.
Theodoros Evgeniou and Massimiliano Pontil. 2004. Regular-
ized multitask learning. In Proc. of KDD, pages 109?117.
Seattle, WA, USA.
Manaal Faruqui and Chris Dyer. 2014. Improving vector
space word representations using multilingual correlation.
In Proc. of EACL, pages 462?471. Gothenburg, Sweden.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer,
and Noah A. Smith. 2014. A discriminative graph-based
parser for the Abstract Meaning Representation. In Proc.
of ACL, pages 1426?1436. Baltimore, MD, USA.
Dan Flickinger, Yi Zhang, and Valia Kordoni. 2012. Deep-
Bank: a dynamically annotated treebank of the Wall Street
Journal. In Proc. of the Eleventh International Workshop on
Treebanks and Linguistic Theories, pages 85?96. Lisbon,
Portugal.
Kevin Gimpel and Noah A. Smith. 2010. Softmax-margin
training for structured log-linear models. Technical
Report CMU-LTI-10-008, Carnegie Mellon Univer-
sity. URL http://lti.cs.cmu.edu/sites/
default/files/research/reports/2010/
cmulti10008.pdf.
Jan Haji?c. 1998. Building a syntactically annotated corpus:
the Prague Dependency Treebank. In Eva Haji?cov?a, ed-
itor, Issues of Valency and Meaning. Studies in Honour
of Jarmila Panevov?a, pages 106?132. Prague Karolinum,
Charles University Press, Prague.
Richard Johansson and Pierre Nugues. 2008. Dependency-
based semantic role labeling of PropBank. In Proc. of
EMNLP, pages 69?78. Honolulu, HI, USA.
Andy Liaw and Matthew Wiener. 2002. Classification
and regression by randomForest. R News, 2(3):18?
22. URL http://cran.r-project.org/web/
packages/randomForest/.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45(3):503?528.
Andr?e F. T. Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL, pages 617?622. Sofia,
Bulgaria.
Ryan McDonald and Fernando Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In Proc. of
EACL, pages 81?88. Trento, Italy.
Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii. 2004.
Corpus-oriented grammar development for acquiring a
head-driven phrase structure grammar from the Penn Tree-
bank. In Proc. of IJCNLP, pages 684?693. Hainan Island,
China.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel
Zeman, Dan Flickinger, Jan Haji?c, Angelina Ivanova, and
Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage
semantic dependency parsing. In Proc. of SemEval. Dublin,
Ireland.
Ben Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-
margin Markov networks. In Proc. of NIPS, pages 25?32.
Vancouver, British Columbia, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims,
and Yasemin Altun. 2004. Support vector machine learning
for interdependent and structured output spaces. In Proc.
of ICML, pages 104?111. Banff, Alberta, Canada.
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In Proc. of ICML, pages
1113?1120. Montreal, Quebec, Canada.
180
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 72?76,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The University of Maryland Statistical Machine Translation System for
the Fifth Workshop on Machine Translation
Vladimir Eidelman?, Chris Dyer??, and Philip Resnik??
?UMIACS Laboratory for Computational Linguistics and Information Processing
?Department of Linguistics
University of Maryland, College Park
{vlad,redpony,resnik}@umiacs.umd.edu
Abstract
This paper describes the system we devel-
oped to improve German-English transla-
tion of News text for the shared task of
the Fifth Workshop on Statistical Machine
Translation. Working within cdec, an
open source modular framework for ma-
chine translation, we explore the benefits
of several modifications to our hierarchical
phrase-based model, including segmenta-
tion lattices, minimum Bayes Risk de-
coding, grammar extraction methods, and
varying language models. Furthermore,
we analyze decoder speed and memory
performance across our set of models and
show there is an important trade-off that
needs to be made.
1 Introduction
For the shared translation task of the Fifth Work-
shop on Machine Translation (WMT10), we par-
ticipated in German to English translation under
the constraint setting. We were especially inter-
ested in translating from German due to set of
challenges it poses for translation. Namely, Ger-
man possesses a rich inflectional morphology, pro-
ductive compounding, and significant word re-
ordering with respect to English. Therefore, we
directed our system design and experimentation
toward addressing these complications and mini-
mizing their negative impact on translation qual-
ity.
The rest of this paper is structured as follows.
After a brief description of the baseline system
in Section 2, we detail the steps taken to improve
upon it in Section 3, followed by experimental re-
sults and analysis of decoder performance metrics.
2 Baseline system
As our baseline system, we employ a hierarchical
phrase-based translation model, which is formally
based on the notion of a synchronous context-free
grammar (SCFG) (Chiang, 2007). These gram-
mars contain pairs of CFG rules with aligned non-
terminals, and by introducing these nonterminals
into the grammar, such a system is able to uti-
lize both word and phrase level reordering to cap-
ture the hierarchical structure of language. SCFG
translation models have been shown to be well
suited for German-English translation, as they are
able to both exploit lexical information for and ef-
ficiently compute all possible reorderings using a
CKY-based decoder (Dyer et al, 2009).
Our system is implemented within cdec, an ef-
ficient and modular open source framework for
aligning, training, and decoding with a num-
ber of different translation models, including
SCFGs (Dyer et al, 2010).1 cdec?s modular
framework facilitates seamless integration of a
translation model with different language models,
pruning strategies and inference algorithms. As
input, cdec expects a string, lattice, or context-free
forest, and uses it to generate a hypergraph repre-
sentation, which represents the full translation for-
est without any pruning. The forest can now be
rescored, by intersecting it with a language model
for instance, to obtain output translations. The
above capabilities of cdec allow us to perform the
experiments described below, which would other-
wise be quite cumbersome to carry out in another
system.
The set of features used in our model were the
rule translation relative frequency P (e|f), a target
n-gram language model P (e), a ?pass-through?
penalty when passing a source language word
to the target side without translating it, lexical
translation probabilities Plex(e|f) and Plex(f |e),
1http://cdec-decoder.org
72
a count of the number of times that arity-0,1, or 2
SCFG rules were used, a count of the total num-
ber of rules used, a source word penalty, a target
word penalty, the segmentation model cost, and a
count of the number of times the glue rule is used.
The number of non-terminals allowed in a syn-
chronous grammar rule was restricted to two, and
the non-terminal span limit was 12 for non-glue
grammars. The hierarchical phrase-base transla-
tion grammar was extracted using a suffix array
rule extractor (Lopez, 2007).
2.1 Data preparation
In order to extract the translation grammar nec-
essary for our model, we used the provided Eu-
roparl and News Commentary parallel training
data. The lowercased and tokenized training data
was then filtered for length and aligned using the
GIZA++ implementation of IBM Model 4 (Och
and Ney, 2003) to obtain one-to-many alignments
in both directions and symmetrized by combining
both into a single alignment using the grow-diag-
final-and method (Koehn et al, 2003). We con-
structed a 5-gram language model using the SRI
language modeling toolkit (Stolcke, 2002) from
the provided English monolingual training data
and the non-Europarl portions of the parallel data
with modified Kneser-Ney smoothing (Chen and
Goodman, 1996). Since the beginnings and ends
of sentences often display unique characteristics
that are not easily captured within the context of
the model, and have previously been demonstrated
to significantly improve performance (Dyer et al,
2009), we explicitly annotate beginning and end
of sentence markers as part of our translation
process. We used the 2525 sentences in news-
test2009 as our dev set on which we tuned the fea-
ture weights, and report results on the 2489 sen-
tences of the news-test2010 test set.
2.2 Viterbi envelope semiring training
To optimize the feature weights for our model,
we use Viterbi envelope semiring training (VEST),
which is an implementation of the minimum er-
ror rate training (MERT) algorithm (Dyer et al,
2010; Och, 2003) for training with an arbitrary
loss function. VEST reinterprets MERT within
a semiring framework, which is a useful mathe-
matical abstraction for defining two general oper-
ations, addition (?) and multiplication (?) over
a set of values. Formally, a semiring is a 5-tuple
(K,?,?, 0, 1), where addition must be commu-
nicative and associative, multiplication must be as-
sociative and must distribute over addition, and an
identity element exists for both. For VEST, hav-
ing K be the set of line segments, ? be the union
of them, and? be Minkowski addition of the lines
represented as points in the dual plane, allows us
to compute the necessary MERT line search with
the INSIDE algorithm.2 The error function we use
is BLEU (Papineni et al, 2002), and the decoder is
configured to use cube pruning (Huang and Chi-
ang, 2007) with a limit of 100 candidates at each
node. During decoding of the test set, we raise
the cube pruning limit to 1000 candidates at each
node.
2.3 Compound segmentation lattices
To deal with the aforementioned problem in Ger-
man of productive compounding, where words
are formed by the concatenation of several mor-
phemes and the orthography does not delineate the
morpheme boundaries, we utilize word segmen-
tation lattices. These lattices serve to encode al-
ternative ways of segmenting compound words,
and as such, when presented as the input to the
system allow the decoder to automatically choose
which segmentation is best for translation, leading
to markedly improved results (Dyer, 2009).
In order to construct diverse and accurate seg-
mentation lattices, we built a maximum entropy
model of compound word splitting which makes
use of a small number of dense features, such
as frequency of hypothesized morphemes as sep-
arate units in a monolingual corpus, number of
predicted morphemes, and number of letters in
a predicted morpheme. The feature weights are
tuned to maximize conditional log-likelihood us-
ing a small amount of manually created reference
lattices which encode linguistically plausible seg-
mentations for a selected set of compound words.3
To create lattices for the dev and test sets, a lat-
tice consisting of all possible segmentations for
every word consisting of more than 6 letters was
created, and the paths were weighted by the pos-
terior probability assigned by the segmentation
model. Then, max-marginals were computed us-
ing the forward-backward algorithm and used to
prune out paths that were greater than a factor of
2.3 from the best path, as recommended by Dyer
2This algorithm is equivalent to the hypergraph MERT al-
gorithm described by Kumar et al (2009).
3The reference segmentation lattices used for training are
available in the cdec distribution.
73
(2009).4 To create the translation model for lattice
input, we segmented the training data using the
1-best segmentation predicted by the segmenta-
tion model, and word aligned this with the English
side. This version of the parallel corpus was con-
catenated with the original training parallel cor-
pus.
3 Experimental variation
This section describes the experiments we per-
formed in attempting to assess the challenges
posed by current methods and our exploration of
new ones.
3.1 Bloom filter language model
Language models play a crucial role in transla-
tion performance, both in terms of quality, and in
terms of practical aspects such as decoder memory
usage and speed. Unfortunately, these two con-
cerns tend to trade-off one another, as increasing
to a higher-order more complex language model
improves performance, but comes at the cost of
increased size and difficulty in deployment. Ide-
ally, the language model will be loaded into mem-
ory locally by the decoder, but given memory con-
straints, it is entirely possible that the only option
is to resort to a remote language model server that
needs to be queried, thus introducing significant
decoding speed delays.
One possible alternative is a randomized lan-
guage model (RandLM) (Talbot and Osborne,
2007). Using Bloom filters, which are a ran-
domized data structure for set representation, we
can construct language models which signifi-
cantly decrease space requirements, thus becom-
ing amenable to being stored locally in memory,
while only introducing a quantifiable number of
false positives. In order to assess what the im-
pact on translation quality would be, we trained
a system identical to the one described above, ex-
cept using a RandLM. Conveniently, it is possi-
ble to construct a RandLM directly from an exist-
ing SRILM, which is the route we followed in us-
ing the SRILM described in Section 2.1 to create
our RandLM.5 Table 1 shows the comparison of
SRILM and RandLM with respect to performance
on BLEU and TER (Snover et al, 2006) on the test
set.
4While normally the forward-backward algorithm com-
putes sum-marginals, by changing the addition operator to
max, we can obtain max-marginals.
5Default settings were used for constructing the RandLM.
Language Model BLEU TER
RandLM 22.4 69.1
SRILM 23.1 68.0
Table 1: Impact of language model on translation
3.2 Minimum Bayes risk decoding
During minimum error rate training, the decoder
employs a maximum derivation decision rule.
However, upon exploration of alternative strate-
gies, we have found benefits to using a mini-
mum risk decision rule (Kumar and Byrne, 2004),
wherein we want the translation E of the input F
that has the least expected loss, again as measured
by some loss function L:
E? = argmin
E?
EP (E|F )[L(E,E
?)]
= argmin
E?
?
E
P (E|F )L(E,E?)
Using our system, we generate a unique 500-
best list of translations to approximate the poste-
rior distribution P (E|F ) and the set of possible
translations. Assuming H(E,F ) is the weight of
the decoder?s current path, this can be written as:
P (E|F ) ? exp?H(E,F )
where ? is a free parameter which depends on
the models feature functions and weights as well
as pruning method employed, and thus needs to
be separately empirically optimized on a held out
development set. For this submission, we used
? = 0.5 and BLEU as the loss function. Table 2
shows the results on the test set for MBR decod-
ing.
Language Model Decoder BLEU TER
RandLM
Max-D 22.4 69.1
MBR 22.7 68.8
SRILM
Max-D 23.1 68.0
MBR 23.4 67.7
Table 2: Comparison of maximum derivation ver-
sus MBR decoding
3.3 Grammar extraction
Although the grammars employed in a SCFG
model allow increased expressivity and translation
quality, they do so at the cost of having a large
74
Language Model Grammar Decoder Memory (GB) Decoder time (Sec/Sentence)
Local SRILM corpus 14.293 ? 1.228 5.254 ? 3.768
Local SRILM sentence 10.964 ? .964 5.517 ? 3.884
Remote SRILM corpus 3.771 ? .235 15.252 ? 10.878
Remote SRILM sentence .443 ? .235 14.751 ? 10.370
RandLM corpus 7.901 ? .721 9.398 ? 6.965
RandLM sentence 4.612 ? .699 9.561 ? 7.149
Table 3: Decoding memory and speed requirements for language model and grammar extraction varia-
tions
number of rules, thus efficiently storing and ac-
cessing grammar rules can become a major prob-
lem. Since a grammar consists of the set of rules
extracted from a parallel corpus containing tens of
millions of words, the resulting number of rules
can be in the millions. Besides storing the whole
grammar locally in memory, other approaches
have been developed, such as suffix arrays, which
lookup and extract rules on the fly from the phrase
table (Lopez, 2007). Thus, the memory require-
ments for decoding have either been for the gram-
mar, when extracted beforehand, or the corpus, for
suffix arrays. In cdec, however, loading grammars
for single sentences from a disk is very fast relative
to decoding time, thus we explore the additional
possibility of having sentence-specific grammars
extracted and loaded on an as-needed basis by the
decoder. This strategy is shown to massively re-
duce the memory footprint of the decoder, while
having no observable impact on decoding speed,
introducing the possibility of more computational
resources for translation. Thus, in addition to the
large corpus grammar extracted in Section 2.1,
we extract sentence-specific grammars for each of
the test sentences. We measure the performance
across using both grammar extraction mechanisms
and the three different language model configu-
rations: local SRILM, remote SRILM, and Ran-
dLM.
As Table 3 shows, there is a marked trade-
off between memory usage and decoding speed.
Using a local SRILM regardless of grammar in-
creases decoding speed by a factor of 3 compared
to the remote SRILM, and approximately a fac-
tor of 2 against the RandLM. However, this speed
comes at the cost of its memory footprint. With a
corpus grammar, the memory footprint of the lo-
cal SRILM is twice as large as the RandLM, and
almost 4 times as large as the remote SRILM. Us-
ing sentence-specific grammars, the difference be-
comes increasingly glaring, as the remote SRILM
memory footprint drops to ?450MB, a factor of
nearly 24 compared to the local SRILM and a fac-
tor of 10 compared to the process size with the
RandLM. Thus, using the remote SRILM reduces
the memory footprint substantially but at the cost
of significantly slower decoding speed, and con-
versely, using the local SRILM produces increased
decoder speed but introduces a substantial mem-
ory overhead. The RandLM provides a median
between the two extremes: reduced memory and
(relatively) fast decoding at the price of somewhat
decreased translation quality. Since we are using
a relatively large beam of 1000 candidates for de-
coding, the time presented in Table 3 does not rep-
resent an accurate basis for comparison of cdec to
other decoders, which should be done using the
results presented in Dyer et al (2010).
We also tried one other grammar extraction
configuration, which was with so-called ?loose?
phrase extraction heuristics, which permit un-
aligned words at the edges of phrases (Ayan and
Dorr, 2006). When decoded using the SRILM and
MBR, this achieved the best performance for our
system, with a BLEU score of 23.6 and TER of
67.7.
4 Conclusion
We presented the University of Maryland hier-
archical phrase-based system for the WMT2010
shared translation task. Using cdec, we experi-
mented with a number of methods that are shown
above to lead to improved German-to-English
translation quality over our baseline according to
BLEU and TER evaluation. These include methods
to directly address German morphological com-
plexity, such as appropriate feature functions, seg-
mentation lattices, and a model for automatically
constructing the lattices, as well as alternative de-
coding strategies, such asMBR.We also presented
75
several language model configuration alternatives,
as well as grammar extraction methods, and em-
phasized the trade-off that must be made between
decoding time, memory overhead, and translation
quality in current statistical machine translation
systems.
5 Acknowledgments
The authors gratefully acknowledge partial sup-
port from the GALE program of the Defense Ad-
vanced Research Projects Agency, Contract No.
HR0011-06-2-001 and NSF award IIS0838801.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the view of the
sponsors.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings
of the Joint Conference of the International Com-
mittee on Computational Linguistics and the As-
sociation for Computational Linguistics (COLING-
ACL?2006), pages 9?16, Sydney.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. In Computational Linguistics, volume 33(2),
pages 201?228.
Chris Dyer, Hendra Setiawan, Yuval Marton, and
P. Resnik. 2009. The University of Maryland sta-
tistical machine translation system for the Fourth
Workshop on Machine Translation. In Proceedings
of the EACL-2009 Workshop on Statistical Machine
Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of ACL System Demonstrations.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for mt. In Proceedings
of NAACL-HLT.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
144?151.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In HLT-NAACL 2004: Main Proceedings.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 163?171.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP,
pages 976?985.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
In Computational Linguistics, volume 29(21), pages
19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Intl. Conf. on Spoken
Language Processing.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, June.
76
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 337?343,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The CMU-ARK German-English Translation System
Chris Dyer Kevin Gimpel Jonathan H. Clark Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{cdyer,kgimpel,jhclark,nasmith}@cs.cmu.edu
Abstract
This paper describes the German-English
translation system developed by the ARK re-
search group at Carnegie Mellon University
for the Sixth Workshop on Machine Trans-
lation (WMT11). We present the results of
several modeling and training improvements
to our core hierarchical phrase-based trans-
lation system, including: feature engineering
to improve modeling of the derivation struc-
ture of translations; better handing of OOVs;
and using development set translations into
other languages to create additional pseudo-
references for training.
1 Introduction
We describe the German-English translation system
submitted to the shared translation task in the Sixth
Workshop on Machine Translation (WMT11) by the
ARK research group at Carnegie Mellon Univer-
sity.1 The core translation system is a hierarchical
phrase-based machine translation system (Chiang,
2007) that has been extended in several ways de-
scribed in this paper.
Some of our innovations focus on modeling.
Since German and English word orders can diverge
considerably, particularly in non-matrix clauses,
we focused on feature engineering to improve the
modeling of long-distance relationships, which are
poorly captured in standard hierarchical phrase-
based translation models. To do so, we devel-
oped features that assess the goodness of the source
1http://www.ark.cs.cmu.edu
language parse tree under the translation grammar
(rather than of a ?linguistic? grammar). To train the
feature weights, we made use of a novel two-phase
training algorithm that incorporates a probabilistic
training objective and standard minimum error train-
ing (Och, 2003). These segmentation features were
supplemented with a 7-gram class-based language
model, which more directly models long-distance
relationships. Together, these features provide a
modest improvement over the baseline and suggest
interesting directions for future work. While our
work on parse modeling was involved and required
substantial changes to the training pipeline, some
other modeling enhancements were quite simple: for
example, improving how out-of-vocabulary words
are handled. We propose a very simple change, and
show that it provides a small, consistent gain.
On the training side, we had two improvements
over our baseline system. First, we were inspired
by the work of Madnani (2010), who showed that
when training to optimize BLEU (Papineni et al,
2002), overfitting is reduced by supplementing a sin-
gle human-generated reference translation with ad-
ditional computer-generated references. We gener-
ated supplementary pseudo-references for our de-
velopment set (which is translated into many lan-
guages, but once) by using MT output from a sec-
ondary Spanish-English translation system. Second,
following Foster and Kuhn (2009), we used a sec-
ondary development set to select from among many
optimization runs, which further improved general-
ization.
We largely sought techniques that did not require
language-specific resources (e.g., treebanks, POS
337
annotations, morphological analyzers). An excep-
tion is a compound segmentation model used for
preprocessing that was trained on a corpus of man-
ually segmented German. Aside from this, no fur-
ther manually annotated data was used, and we sus-
pect many of the improvements described here can
be had in other language pairs. Despite avoiding
language-specific resources and using only the train-
ing data provided by the workshop, an extensive
manual evaluation determined that the outputs pro-
duced were of significantly higher quality than both
statistical and rule-based systems that made use of
language-specific resources (Callison-Burch et al,
2011).
2 Baseline system and data
Our translation system is based on a hierarchical
phrase-based translation model (Chiang, 2007), as
implemented in the cdec decoder (Dyer et al,
2010). Since German is a language that makes
productive use of ?closed? compounds (compound
words written as a single orthographic token), we
use a CRF segmentation model of to evaluate the
probability of all possible segmentations, encoding
the most probable ones compactly in a lattice (Dyer,
2009). For the purposes of grammar induction, the
single most probable segmentation of each word in
the source side of the parallel training data under the
model was inferred.
The parallel data were aligned using the
Giza++ implementation of IBM Model 4 run
in both directions and then symmetrized using
the grow-diag-final-and heuristic (Och and
Ney, 2002; Brown et al, 1993; Koehn et al, 2003).
The aligned corpus was encoded as a suffix array
(Lopez, 2008) and lattice-specific grammars (con-
taining just the rules that are capable of matching
spans in the input lattice) were extracted for each
sentence in the test and development sets, using the
heuristics recommended by Chiang (2007).
A 4-gram modified Kneser-Ney language model
(Chen and Goodman, 1996) was constructed using
the SRI language modeling toolkit (Stolcke, 2002)
from the English side of the parallel text, the mono-
lingual English data, and the English version 4 Giga-
word corpus (Parker et al, 2009). Since there were
many duplicate segments in the training data (much
of which was crawled from the web), duplicate seg-
ments and segments longer than 100 words were re-
moved. Inference was carried out using the language
modeling library described by Heafield (2011).
The newstest-2009 set (with the 500 longest
segments removed) was used for development,2 and
newstest-2010 was used as a development test
set. Results in this paper are reported on the dev-
test set using uncased BLEU4 with a single refer-
ence translation. Minimum error rate training (Och,
2003) was used to optimize the parameters of the
system to maximize BLEU on the development data,
and inference was performed over a pruned hyper-
graph representation of the translation hypothesis
space (Kumar et al, 2009).
For the experiments reported in this paper, Viterbi
(max-derivation) decoding was used. The system
submitted for manual evaluation used segment-level
MBR decoding with 1 ? BLEU as the loss function,
approximated over a 500-best list for each sentence.
This reliably results in a small but consistent im-
provement in translation quality, but is much more
time consuming to compute (Kumar and Byrne,
2004).
3 Source parse structure modeling
Improving phrase-based translation systems is chal-
lenging in part because our intuitions about what
makes a ?good? phrase or translation derivation are
often poor. For example, restricting phrases and
rules to be consistent with syntactic constituents
consistently harms performance (Chiang, 2007; Gal-
ley et al, 2006; Koehn et al, 2003), although our
intuitions might suggest this is a reasonable thing
to do. On the other hand, it has been shown that
incorporating syntactic information in the form of
features can lead to improved performance (Chiang,
2010; Gimpel and Smith, 2009; Marton and Resnik,
2008). Syntactic features that are computed by as-
sessing the overlap of the translation parse with a
linguistic parse can be understood to improve trans-
lation because they lead to a better model of what a
?correct? parse of the source sentence is under the
translation grammar.
Like the ?soft syntactic features? used in pre-
2Removing long segments substantially reduces training
time and does not appear to negatively affect performance.
338
vious work (Marton and Resnik, 2008; Chiang et
al., 2008), we propose features to assess the tree
structure induced during translation. However, un-
like that work, we do not rely on linguistic source
parses, but instead only make use of features that
are directly computable from the source sentence
and the parse structure being considered in the de-
coder. In particular, we take inspiration from the
model of Klein and Manning (2002), which mod-
els constituency in terms of the contexts that rule
productions occur in. Additionally, we make use of
salient aspects of the spans being dominated by a
nonterminal, such as the words at the beginning and
end of the span, and the length of the span. Impor-
tantly, the features do not rely on the target words
being predicted, but only look at the structure of the
translation derivation. As such, they can be under-
stood as monolingual parse features.3
Table 1 lists the feature templates that were used.
Template Description
CTX:fi?1, fj context bigram
CTX:fi?1, fj , x context bigram + NT
CTX:fi?1, fj , x, (j ? i) context bigram + NT + len
LU:fi?1 left unigram
LB:fi?1, fi left bigram (overlapping)
RU:fj right unigram
RB:fj?1, fj right bigram (overlapping)
Table 1: Context feature templates for features extracted
from every translation rule used; i and j indicate hypothe-
sized constituent span, x is its nonterminal category label
(in our grammar, X or S), and fk is the kth word of the
source sentence, with f<1 = ?s? and f>|f| = ?/s?. If a
word fk is not among the 1000 most frequent words in
the training corpus, it is replaced by a special unknown
token. The SMALLCAPS prefixes prevent accidental fea-
ture collisions.
3.1 Two-phase discriminative learning
The parse features just introduced are numerous and
sparse, which means that MERT can not be used
to infer their weights. Instead, we require a learn-
ing algorithm that can cope with millions of fea-
tures and avoid overfitting, perhaps by eliminating
most of the features and keeping only the most valu-
able (which would also keep the model compact).
3Similar features have been proposed for use in discrimina-
tive monolingual parsing models (Taskar et al, 2004).
Furthermore, we would like to be able to still tar-
get the BLEU measure of translation quality during
learning. While large-scale discriminative training
for machine translation is a widely studied problem
(Hopkins and May, 2011; Li and Eisner, 2009; De-
vlin, 2009; Blunsom et al, 2008; Watanabe et al,
2007; Arun and Koehn, 2007; Liang et al, 2006), no
tractable algorithm exists for learning a large num-
ber of feature weights while directly optimizing a
corpus-level metric like BLEU. Rather than resorting
to a decomposable approximation, we have explored
a new two-phase training algorithm in development
of this system.
The two-phase algorithm works as follows. In
phase 1, we use a non-BLEU objective to train a
translation model that includes the large feature set.
Then, we use this model to compute a small num-
ber of coarse ?summary features,? which summa-
rize the ?opinion? of the first model about a trans-
lation hypothesis in a low dimensional space. Then,
in the second training pass, MERT is used to deter-
mine how much weight to give these summary fea-
tures together with the other standard coarse trans-
lation features. At test time, translation becomes a
multi-step process as well. The hypothesis space is
first scored using the phase-1 model, then summary
features are computed, then the hypothesis space is
rescored with the phase-2 model. As long as the fea-
tures used factor with the edges in the translation
space (which ours do), this can be carried out in lin-
ear time in the size of the translation forest.
3.1.1 Phase 1 training
For the first model, which includes the sparse parse
features, we learn weights in order to optimize pe-
nalized conditional log likelihood (Blunsom et al,
2008). We are specifically interested in modeling
an unobserved variable (i.e., the parse tree underly-
ing a translation derivation), this objective is quite
natural, since probabilistic models offer a principled
account of unobserved data. Furthermore, because
our features factor according to edges in the trans-
lation forest (they are ?stateless? in standard MT
terminology), there are efficient dynamic program-
ming algorithms that can be used to exactly compute
the expected values of the features (Lari and Young,
1990), which are necessary for computing the gradi-
ents used in optimization.
339
We are therefore optimizing the following objec-
tive, given a set T of parallel training sentences:
L = ?R(?)?
?
?f,e??T
log
?
d
p?(e,d | f)
where p?(e,d | f) =
exp ?>h(f, e,d)
Z(f)
,
where d is a variable representing the unobserved
synchronous parses giving rise to the pair of sen-
tences ?f, e?, and where R(?) is a penalty that favors
less complex models. Since we not only want to pre-
vent over fitting but also want a small model, we use
R(?) =
?
k |?k|, the `1 norm, which forces many
parameters to be exactly 0.
Although L is not convex in ? (on account of the
latent derivation variable), we make use of an on-
line stochastic gradient descent algorithm that im-
poses an `1 penalty on the objective (Tsuruoka et
al., 2009). Online algorithms are often effective for
non-convex objectives (Liang and Klein, 2009).
We selected 12,500 sentences randomly from the
news-commentary portion of the training data to use
to train the latent variable model. Using the stan-
dard rule extraction heuristics (Chiang, 2007), 9,967
of the sentence pairs could be derived.4 In addition
to the parse features describe above, the standard
phrase features (relative frequency and lexical trans-
lation probabilities), and a rule count feature were
included. Training was run for 48 hours on a sin-
gle machine, which resulted in 8 passes through the
training data, instantiating over 8M unique features.
The regularization strength ? was chosen so that ap-
proximately 10, 000 (of the 8M) features would be
non-zero.5
3.1.2 Summary features
As outlined above, the phase 1 model will be incor-
porated into the final translation model using a low
dimensional ?summary? of its opinion. Because we
are using a probabilistic model, posterior probabili-
ties (given the source sentence f) under the parsing
4When optimizing conditional log likeligood, it is necessary
to be able to exactly derive the training pair. See Blunsom et al
(2008) for more information.
5Ideally, ? would have been tuned to optimize held-out like-
lihood or BLEU; however, the evaluation deadline prevented us
from doing this.
model are easily defined and straightforward to com-
pute with dynamic programming. We made use of
four summary features: the posterior log probability
log p?(e,d|f); for every rule r ? d, the probability of
its span being a constituent under the parse model;
the probabilities that some span starts at the r?s start-
ing index, or that some rule ends at r?s ending index.
Once these summary features have been com-
puted, the sparse features are discarded, and the
summary features are reweighted using coefficients
learned by MERT, together with the standard MT
features (language model, word penalty, etc.). This
provides a small improvement over our already very
strong baseline, as the first two rows in Table 2 show.
Condition BLEU
baseline 25.0
+ parse features 25.2
+ parse features + 7-gram LM 25.4
Table 2: Additional features designed to improve model
of long-range reordering.
3.2 7-gram class-based LM
The parsing features above were intended to im-
prove long range reordering quality. To further sup-
port the modeling of larger spans, we incorporated
a 7-gram class-based language model. Automatic
word clusters are attractive because they can be
learned for any language without supervised data,
and, unlike part-of-speech annotations, each word
is in only a single class, which simplifies inference.
We performed Brown clustering (Brown et al, 1992)
on 900k sentences from our language modeling data
(including the news commentary corpus and a sub-
set of Gigaword). We obtained 1,000 clusters us-
ing an implementation provided by Liang (2005),6
as Turian et al (2010) found that relatively large
numbers clusters gave better performance for infor-
mation extraction tasks. We then replaced words
with their clusters in our language modeling data
and built a 7-gram LM with Witten-Bell smoothing
(Witten and Bell, 1991).7 The last two rows of Ta-
6http://www.cs.berkeley.edu/?pliang/
software
7The distributional assumptions made by the more com-
monly used Kneser-Ney estimator do not hold in the word-
340
ble 2 shows that in conjunction with the source parse
features, a slight improvement comes from includ-
ing the 7-gram LM.
4 Non-translating tokens
When two languages share a common alphabet (as
German and English largely do), it is often appro-
priate to leave some tokens untranslated when trans-
lating. Named entities, numbers, and graphical el-
ements such as emoticons are a few common ex-
amples of such ?non-translating? elements. To en-
sure that such elements are well-modeled, we aug-
ment our translation grammar so that every token
in the input can translate as itself and add a feature
that counts the number of times such self-translation
rules are used in a translation hypothesis. This is in
contrast to the behavior of most other decoders, such
as Moses, which only permit a token to translate as
itself if it is learned from the training data, or if there
is no translation in the phrase table at all.
Since many non-translating tokens are out-of-
vocabulary (OOV) in the target LM, we also add
a feature that fires each time the LM encounters a
word that is OOV.8 This behavior be understood as
discriminatively learning the unknown word penalty
that is part of the LM. Again, this is in contrast to
the behavior of other decoders, which typically add
a fixed (and very large) cost to the LM feature for
every OOV. Our multi-feature parameterization per-
mits the training algorithm to decide that, e.g., some
OOVs are acceptable if they occur in a ?good? con-
text rather than forcing the decoder to avoid them
at all costs. Table 3 shows that always providing
a non-translating translation option together with a
discriminative learned OOV feature improves the
quality of German-English translation.9
Condition BLEU
?OOV (baseline) 24.6
+OOV and non-translating rules 25.0
Table 3: Effect of discriminatively learned penalties for
OOV words.
classified corpus.
8When multiple LMs are used, there is an extra OOV feature
for each LM.
9Both systems were trained using the human+ES-EN refer-
ence set described below (?5).
5 Computer-generated references
Madnani (2010) shows that models learned by op-
timizing BLEU are liable to overfit if only a sin-
gle reference is used, but that this overfitting can
be mitigated by supplementing the single reference
with supplemental computer-generated references
produced by paraphrasing the human reference us-
ing a whole-sentence statistical paraphrase system.
These computer-generated paraphrases are just used
to compute ?better? BLEU scores, but not directly as
examples of target translations.
Although we did not have access to a paraphrase
generator, we took advantage of the fact that our de-
velopment set (newstest-2009) was translated
into several languages other than English. By trans-
lating these back into English, we hypothesized we
would get suitable pseudo-references that could be
used in place of computer-generated paraphrases.
Table 4 shows the results obtained on our held-out
test set simply by altering the reference translations
used to score the development data. These systems
all contain the OOV features described above.
Condition BLEU
1 human 24.7
1 human + ES-EN 25.0
1 human + FR-EN 24.0
1 human + ES-EN + FR-EN 24.2
Table 4: Effect of different sets of reference translations
used during tuning.
While the effect is somewhat smaller than Mad-
nani (2010) reports using a sentential paraphraser,
the extremely simple technique of adding the output
of a Spanish-English (ES-EN) system was found to
consistently improve the quality of the translations
of the held-out data. However, a comparable effect
was not found when using references generated from
a French-English (FR-EN) translation system, indi-
cating that the utility of this technique must be as-
sessed empirically and depends on several factors.
6 Case restoration
Our translation system generates lowercased out-
put, so we must restore case as a post-processing
step. We do so using a probabilistic transducer as
implemented in SRILM?s disambig tool. Each
341
lowercase token in the input can be mapped to a
cased variant that was observed in the target lan-
guage training data. Ambiguities are resolved us-
ing a language model that predicts true-cased sen-
tences.10 We used the same data sources to con-
struct this model as were used above. During devel-
opment, it was observed that many named entities
that did not require translation required some case
change, from simple uppercasing of the first letter,
to more idiosyncratic casings (e.g., iPod). To ensure
that these were properly restored, even when they
did not occur in the target language training data, we
supplement the true-cased LM training data and case
transducer training data with the German source test
set.
Condition BLEU (Cased)
English-only 24.1
English+test-set 24.3
Table 5: Effect of supplementing recasing model training
data with the test set source.
7 Model selection
Minimum error rate training (Och, 2003) is a
stochastic optimization algorithm that typically finds
a different weight vector each time it is run. Foster
and Kuhn (2009) showed that while the variance on
the development set objective may be narrow, the
held-out test set variance is typically much greater,
but that a secondary development set can be used to
select a system that will have better generalization.
We therefore replicated MERT 6 times and selected
the output that performed best on NEWSTEST-2010.
Since we had no additional blind test set, we can-
not measure what the impact is. However, the BLEU
scores we selected on varied from 25.4 to 26.1.
8 Summary
We have presented a summary of the enhancements
made to a hierarchical phrase-based translation sys-
tem for the WMT11 shared translation task. Some
of our results are still preliminary (the source parse
10The model used is p(y | x)p(y). While this model is some-
what unusual (the conditional probability is backwards from a
noisy channel model), it is a standard and effective technique
for case restoration.
model), but a number of changes we made were
quite simple (OOV handling, using MT output to
provide additional references for training) and also
led to improved results.
Acknowledgments
This research was supported in part by the NSF through
grant IIS-0844507, the U. S. Army Research Laboratory
and the U. S. Army Research Office under contract/grant
number W911NF-10-1-0533, and Sandia National Labo-
ratories (fellowship to K. Gimpel). We thank the anony-
mous reviewers for their thorough feedback.
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18:467?479.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of the Sixth Workshop
on Statistical Machine Translation.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of ACL, pages 310?318.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. EMNLP, pages 224?233.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
D. Chiang. 2010. Learning to translate with source and
target syntax. In Proc. of ACL, pages 1443?1452.
J. Devlin. 2009. Lexical features for statistical machine
translation. Master?s thesis, University of Maryland.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL (demonstration session).
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
342
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proc. of ACL, pages 961?968.
K. Gimpel and N. A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP, pages 219?228.
K. Heafield. 2011. KenLM: Faster and smaller language
model queries. In Proc. of the Sixth Workshop on Sta-
tistical Machine Translation.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of ACL, pages 128?135.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL-IJCNLP.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. of EMNLP,
pages 40?51.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
N. Madnani. 2010. The Circle of Meaning: From Trans-
lation to Paraphrasing and Back. Ph.D. thesis, De-
partment of Computer Science, University of Mary-
land College Park.
Y. Marton and P. Resnik. 2008. Soft syntactic constraints
for hierarchical phrased-based translation. In Proc. of
ACL, pages 1003?1011, Columbus, Ohio.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL, pages 295?302.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2009. English gigaword fourth edition.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Intl. Conf. on Spoken Language Pro-
cessing.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL-IJCNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proc. of ACL, pages 384?394.
T. Watanabe, J. Suzuki, H. Tsukuda, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Trans. Informa-
tion Theory, 37(4).
343
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 64?71,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Bilingual POS Tagging with Markov Random Fields
Desai Chen Chris Dyer Shay B. Cohen Noah A. Smith
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
desaic@andrew.cmu.edu, {cdyer,scohen,nasmith}@cs.cmu.edu
Abstract
In this paper, we give a treatment to the prob-
lem of bilingual part-of-speech induction with
parallel data. We demonstrate that na??ve op-
timization of log-likelihood with joint MRFs
suffers from a severe problem of local max-
ima, and suggest an alternative ? using con-
trastive estimation for estimation of the pa-
rameters. Our experiments show that estimat-
ing the parameters this way, using overlapping
features with joint MRFs performs better than
previous work on the 1984 dataset.
1 Introduction
This paper considers unsupervised learning of lin-
guistic structure?specifically, parts of speech?in
parallel text data. This setting, and more gener-
ally the multilingual learning scenario, has been
found advantageous for a variety of unsupervised
NLP tasks (Snyder et al, 2008; Cohen and Smith,
2010; Berg-Kirkpatrick et al, 2010; Das and Petrov,
2011).
We consider globally normalized Markov random
fields (MRFs) as an alternative to directed models
based on multinomial distributions or locally nor-
malized log-linear distributions. This alternate pa-
rameterization allows us to introduce correlated fea-
tures that, at least in principle, depend on any parts
of the hidden structure. Such models, sometimes
called ?undirected,? are widespread in supervised
NLP; the most notable instances are conditional ran-
dom fields (Lafferty et al, 2001), which have en-
abled rich feature engineering to incorporate knowl-
edge and improve performance. We conjecture that
the ?features view? of NLP problems is also more
appropriate in unsupervised settings than the con-
trived, acyclic causal stories required by directed
models. Indeed, as we will discuss below, previous
work on multilingual POS induction has had to re-
sort to objectionable independence assumptions to
avoid introducing cyclic dependencies in the causal
network.
While undirected models are formally attractive,
they are computationally demanding, particularly
when they are used generatively, i.e., as joint dis-
tributions over input and output spaces. Inference
and learning algorithms for these models are usually
intractable on realistic datasets, so we must resort to
approximations. Our emphasis here is primarily on
the machinery required to support overlapping fea-
tures, not on weakening independence assumptions,
although we weaken them slightly. Specifically, our
parameterization permits us to model the relation-
ship between aligned words in any configuration,
rather than just those that conform to an acyclic gen-
erative process, as previous work in this area has
done (?2). We incorporate word prefix and suffix
features (up to four characters) in an undirected ver-
sion of a model designed by Snyder et al (2008).
Our experiments suggest that feature-based MRFs
offer advantages over the previous approach.
2 Related Work
The task of unsupervised bilingual POS induction
was originally suggested and explored by Snyder et
al. (2008). Their work proposes a joint model over
pairs of tag sequences and words that can be under-
stood as a pair of hidden Markov models (HMMs)
64
in which aligned words share states (a fixed and
observable word alignment is assumed). Figure 1
gives an example for a French-English sentence pair.
Following Goldwater and Griffiths (2007), the tran-
sition, emission and coupling parameters are gov-
erned by Dirichlet priors, and a token-level col-
lapsed Gibbs sampler is used for inference. The hy-
perparameters of the prior distributions are inferred
from data in an empirical Bayesian fashion.
  
Why repeat that catastrophe ?
Pourquoi r?p?ter la m?me ?catastrophe
x1/y1 X2/y2 y3 y4 x5/y6x4/y5
x3
Figure 1: Bilingual Directed POS induction model
When word alignments are monotonic (i.e., there
are no crossing links in the alignment graph), the
model of Snyder et al is straightforward to con-
struct. However, crossing alignment links pose a
problem: they induce cycles in the tag sequence
graph, which corresponds to an ill-defined probabil-
ity model. Their solution is to eliminate such align-
ment pairs (their algorithm for doing so is discussed
below). Unfortunately, this is a potentially a seri-
ous loss of information. Crossing alignments often
correspond to systematic word order differences be-
tween languages (e.g., SVO vs. SOV languages). As
such, leaving them out prevents useful information
about entire subsets of POS types from exploiting of
bilingual context.
In the monolingual setting, Smith and Eisner
(2005) showed similarly that a POS induction model
can be improved with spelling features (prefixes and
suffixes of words), and Haghighi and Klein (2006)
describe an MRF-based monolingual POS induction
model that uses features. An example of such a
monolingual model is shown in Figure 2. Both pa-
pers developed different approximations of the com-
putationally expensive partition function. Haghighi
and Klein (2006) approximated by ignoring all sen-
tences of length greater than some maximum, and
the ?contrastive estimation? of Smith and Eisner
(2005) approximates the partition function with a set
Eco
nom
ic
dis
cre
pan
cie
s
A
N
areV
gro
win
g
V
Figure 2: Monolingual MRF tag model (Haghighi
and Klein, 2006)
of automatically distorted training examples which
are compactly represented in WFSTs.
Das and Petrov (2011) also consider the prob-
lem of unsupervised bilingual POS induction. They
make use of independent conventional HMM mono-
lingual tagging models that are parameterized with
feature-rich log-linear models (Berg-Kirkpatrick et
al., 2010). However, training is constrained with tag
dictionaries inferred using bilingual contexts derived
from aligned parallel data. In this way, the complex
inference and modeling challenges associated with a
bilingual tagging model are avoided.
Finally, multilingual POS induction has also been
considered without using parallel data. Cohen et al
(2011) present a multilingual estimation technique
for part-of-speech tagging (and grammar induction),
where the lack of parallel data is compensated by
the use of labeled data for some languages and unla-
beled data for other languages.
3 Model
Our model is a Markov random field whose ran-
dom variables correspond to words in two parallel
sentences and POS tags for those words. Let s =
?s1, . . . , sNs? and t = ?t1, . . . , tNt? denote the two
word sequences; these correspond to Ns + Nt ob-
served random variables.1 Let x and y denote the se-
quences of POS tags for s and t, respectively. These
are the hidden variables whose values we seek to in-
fer. We assume that a word alignment is provided for
the sentences. Let A ? {1, . . . , Ns} ? {1, . . . Nt}
denote the word correspondences specified by the
alignment. The MRF?s unnormalized probability S
1We use ?source? and ?target? but the two are completely
symmetric in our undirected framework.
65
assigns:
S(s, t,x,y | A,w) =
expw>
(
Ns?
i=1
fs-emit(si, xi) +
Ns?
i=2
fs-tran(xi?1, xi)
+
Nt?
i=1
ft-emit(ti, yi) +
Nt?
i=2
ft-tran(yi?1, yi)
+
?
(i,j)?A
falign-POS(xi, yj)
?
?
where w is a numerical vector of feature weights
that parameterizes the model. Each f? corre-
sponds to features on pairs of random variables;
a source POS tag and word, two adjacent source
POS tags, similarly for the target side, and aligned
source/target POS pairs. For simplicity, we let f de-
note the sum of these five feature vectors. (In most
settings, each feature/coordinate will be specific to
one of the five addends.) In this paper, the features
are indicators for each possible value of the pair of
random variables, plus prefix and suffix features for
words (up to four characters). These features encode
information similar to the Bayesian bilingual HMM
discussed in ?2. Future work might explore exten-
sions to this basic feature set.
The marginal probability of the words is given by:
p(s, t | A,w) =
?
x,y S(x,y, s, t | A,w)
?
s?,t?
?
x,y S(s
?, t?,x,y | A,w)
.
Maximum likelihood estimation would choose
weights w to optimize a product of quantities like
the above, across the training data.
A key advantage of this representation is that any
alignments may be present. In directed models,
crossing links create forbidden cycles in the graph-
ical model. For example, Figure 3 shows a cross-
ing link between ?Economic discrepancies? and ?di-
vergences economiques.? Snyder et al (2008) dealt
with this problem by deleting word correspondences
that created cycles. The authors deleted crossing
links by considering each alignment link in the order
of the source sentence, deleting it if it crossed pre-
vious links. Deleting crossing links removes some
information about word correspondence.
divergences
?conomiques
Eco
nom
ic
disc
rep
anc
ies
N
A
A
N
Les ART
vont areV V
croissant gro
win
g
V V
Figure 3: Bilingual tag model.
4 Inference and Parameter Learning
When using traditional generative models, such as
hidden Markov models, the unsupervised setting
lends itself well to maximizing joint log-likelihood,
leading to a model that performs well (Snyder et
al., 2008). However, as we show in the following
analysis, maximizing joint log-likelihood for a joint
Markov random field with arbitrary features suffers
from serious issues which are related to the com-
plexity of the optimized objective surface.
4.1 MLE with Gradient Descent
For notational simplicity, we assume a single pair of
sentences s and t; generalizing to multiple training
instances is straightforward. The marginalized log-
likelihood of the data given w is
L(w) = log p(s, t | w)
= log
?
x,y S(x,y, s, t | w)
?
s?,t?
?
x,y S(x,y, s
?, t? | w)
.
In general, maximizing marginalized log-
likelihood is a non-concave optimization problem.
Iterative hill-climbing methods (e.g., expectation-
maximization and gradient-based optimization) will
lead only to local maxima, and these may be quite
shallow. Our analysis suggests that the problem
is exacerbated when we move from directed to
undirected models. We next describe a simple
experiment that gives insight into the problem.
We created a small synthetic monolingual data set
for sequence labeling. Our synthetic data consists of
the following five sequences of observations: {(0 1 2
3) , (1 2 3 0) , (2 3 0 1) , (3 0 1 2) , (0 1 2 3)}. We then
66
maximized the marginalized log-likelihood for two
models: a hidden Markov model and an MRF. Both
use the same set features, only the MRF is globally
normalized. The number of hidden states in both
models is 4.
The global maximium in both cases would be
achieved when the emission probabilities (or feature
weights, in the case of MRF) map each observation
symbol to a single state. When we tested whether
this happens in practice, we noticed that it indeed
happens for hidden Markov models. The MRF, how-
ever, tended to use fewer than four tags in the emis-
sion feature weights, i.e., for half of the tags, all
emission feature weights were close to 0. This ef-
fect also appeared in our real data experiments.
The reason for this problem with the MRF, we be-
lieve, is that the parameter space of the MRF is un-
derconstrained. HMMs locally normalize the emis-
sion probabilities, which implies that a tag cannot
?disappear??a total probability mass of 1 must al-
ways be allocated to the observation symbols. With
MRFs, however, there is no such constraint. Fur-
ther, effective deletion of a state y requires zeroing
out transition probabilities from all other states to
y, a large number of parameters that are completely
decoupled within the model.
  Wh Wy rh ry yh yy eh ey ph py ah ayhthrheh
ahchhcthcrh
ceh p e y r W
(a) likelihood
  WhyyWhyr WhyeWhypWhyaWhyt WhycWhysWhyo e p a tWrWpWtW
sWyWWyrWypW
ytWysW a p?c p?ae?a
(b) contrastive objective
Figure 4: Histograms of local optima found by opti-
mizing the length neighborhood objective (a) and the
contrastive objective (b) on a synthetic dataset with
8 sentences of length 7. The weights are initialized
uniformly at random in the interval [?1, 1]. We plot
frequency versus negated log-likelihood (lower hor-
izontal values are better). An HMM always finds a
solution that uses all available tags. The numbers at
the top are numbers of tags used by each local opti-
mum.
Our bilingual model is more complex than the
above example, and we found in preliminary exper-
iments that the effect persists there, as well. In the
following section, we propose a remedy to this prob-
lem based on contrastive estimation (Smith and Eis-
ner, 2005).
4.2 Contrastive Estimation
Contrastive estimation maximizes a modified ver-
sion of the log-likelihood. In the modified version,
it is the normalization constant of the log-likelihood
that changes: it is limited to a sum over possible ele-
ments in a neighborhood of the observed instances.
More specifically, in our bilingual tagging model,
we would define a neighborhood function for sen-
tences, N(s, t) which maps a pair of sentences to
a set of pairs of sentences. Using this neighborhood
function, we maximize the following objective func-
tion:
Lce(w)
= log p(S = s,T = t | S ? N1(s),T ? N2(t),w)
= log
?
x,y S(s, t,x,y | w)
?
s?,t??N(s,t)
?
x,y
S(s?, t?,x,y | w).
(1)
We define the neighborhood function using
a cross-product of monolingual neighborhoods:
N(s, t) = N1(s) ? N1(t). N1 is the ?dynasearch?
neighborhood function (Potts and van de Velde,
1995; Congram et al, 2002), used for contrastive
estimation previously by Smith (2006). This neigh-
borhood defines a subset of permutations of a se-
quence s, based on local transpositions. Specifically,
a permutation of s is in N1(s) if it can be derived
from s through swaps of any adjacent pairs of words,
with the constraint that each word only be moved
once. This neighborhood can be compactly repre-
sented with a finite-state machine of size O(Ns) but
encodes a number of sequences equal to the Nsth
Fibonacci number.
Monolingual Analysis To show that contrastive
estimation indeed gives a remedy to the local max-
imum problem, we return to the monolingual syn-
thetic data example from ?4.1 and apply contrastive
estimation on this problem. The neighborhood we
use is the dynasearch neighborhood. In Figure 4b
67
we compare the maxima identified using MLE with
the monolingual MRF model to the maxima identi-
fied by contrastive estimation. The results are con-
clusive: MLE tends to get stuck much more often in
local maxima than contrastive estimation.
Following an analysis of the feature weights
found by contrastive estimation, we found that con-
trastive estimation puts more weight on the transi-
tion features than emission features, i.e., the tran-
sition features weights have larger absolute values
than emission feature weights. We believe that this
could explain why contrastive estimation finds better
local maximum that plain MLE, but we leave explo-
ration of this effect for future work.
It is interesting to note that even though the con-
trastive objective tends to use more tags available in
the dictionary than the likelihood objective does, the
maximum objective that we were able to find does
not correspond to the tagging that uses all available
tags, unlike with HMM, where the maximum that
achieved highest likelihood also uses all available
tags.
4.3 Optimizing the Contrastive Objective
To optimize the objective in Eq. 1 we use a generic
optimization technique based on the gradient. Using
the chain rule for derivatives, we can derive the par-
tial derivative of the log-likelihood with respect to a
weight wi:
?Lce(w)
?wi
= Ep(X,Y|s,t,w)[fi]
? Ep(S,T,X,Y|S?N1(s),T?N1(t),w)[fi]
The second term corresponds to a computationally
expensive inference problem, because of the loops
in the graphical model. This situation is differ-
ent from previous work on linear chain-structured
MRFs (Smith and Eisner, 2005; Haghighi and Klein,
2006), where exact inference is possible. To over-
come this problem, we use Gibbs sampling to obtain
the two expectations needed by the gradient. This
technique is closely related to methods like stochas-
tic expectation-maximization (Andrieu et al, 2003)
and to contrastive divergence (Hinton, 2000).
The training algorithm iterates between sam-
pling part-of-speech tags and sampling permutations
of words to compute the expected value of fea-
tures. To sample permutations, the sampler iterates
through the sentences and decides, for each sen-
tence, whether to swap a pair of adjacent tags and
words or not. The Markov blanket for computing
the probability of swapping a pair of tags and words
is shown in Figure 5. We run the algorithm for a
fixed number (50) of iterations. By testing on a de-
velopment set, we observed that the accuracy may
increase after 50 iterations, but we chose this small
number of iterations for speed.
  
N A
divergences ?conomiques
A N
Economic discrepancies
V
vont 
  
N A
divergences ?conomiques
A N
Economic discrepancies
AVt
?s 
?
von? 
are
?
Figure 5: Markov blanket of a tag (left) and of a pair
of adjacent tags and words (right).
In preliminary experiments we considered
stochastic gradient descent, with online updating.
We found this led to low-accuracy local optima,
and opted for gradient descent with batch updates
in our implementation. The step size was chosen to
limit the maximum absolute value of the update in
any weight to 0.1. Preliminary experiments showed
only harmful effects from regularization, so we did
not use it. These issues deserve further analysis and
experimentation in future research.
5 Experiments
We next describe experiments using our undirected
model to unsupervisedly learn POS tags.
With unsupervised part-of-speech tagging, it is
common practice to use a full or partial dictionary
that maps words to possible part-of-speech tags. The
goal of the learner is then to discern which tag a
word should take among the tags available for that
word. Indeed, in all of our experiments we make
use of a tag dictionary. We consider both a com-
plete tag dictionary, where all of the POS tags for all
words in the data are known,2 and a smaller tag dic-
tionary that only provides possible tags for the 100
2Of course, additional POS tags may be possible for a given
word that were not in evidence in our finite dataset.
68
most frequent words in each language, leaving the
other words completely ambiguous. The former dic-
tionary makes the problem easier by reducing ambi-
guity; it also speeds up inference.
Our experiments focus on the Orwell novel 1984
dataset for our experiments, the same data used by
Snyder et al (2008). It consists of parallel text of
the 1984 novel in English, Bulgarian, Slovene and
Serbian (Erjavec, 2004), totalling 5,969 sentences in
each language. The 1984 datset uses fourteen part-
of-speech tags, two of which denote punctuation.
The tag sets for English and other languages have
minor differences in determiners and particles.
We use the last 25% of sentences in the dataset
as a test set, following previous work. The dataset
is manually annotated with part-of-speech tags. We
use automatically induced word alignments using
Giza++ (Och and Ney, 2003). The data show very
regular patterns of tags that are aligned together:
words with the same tag in two languages tend to
be aligned with each other.
When a complete tag dictionary derived from the
Slavic language data is available, the level of ambi-
guity is very low. The baseline of choosing random
tags for each word gives an accuracy in the low 80s.
For English, we use an extended tag dictionary built
from the Wall Street Journal and the 1984 data. The
English tag dictionary is much more ambiguous be-
cause it is obtained from a much larger dataset. The
random baseline gives an accuracy of around 56%.
(See Table 1.)
In our first set of experiments (?5.1), we perform
a ?sanity check? with a monolingual version of the
MRF that we described in earlier sections. We com-
pare it against plain HMM to assure that the MRFs
behave well in the unsupervised setting.
In our second set of experiments (?5.2), we com-
pare the bilingual HMM model from Snyder et al
(2008) to the joint MRF model. We show that using
an MRF has an advantage over an HMM model in
the partial tag dictionary setting.
5.1 Monolingual Experiments
We turn now to two monolingual experiments that
verify our model?s suitability for the tagging prob-
lem.
Language Random HMM MRF
Bulgarian 82.7 88.9 93.5
English 56.2 90.7 87.0
Serbian 83.4 85.1 89.3
Slovene 84.7 87.4 94.5
Table 1: Unsupervised monolingual tagging accura-
cies with complete tag dictionary on 1984 data.
Supervised Learning As a very primitive com-
parison, we trained a monolingual supervised MRF
model to compare to the results of supervised
HMMs. The training procedure is based on sam-
pling, just like the unsupervised estimation method
described in ?4.3. The only difference is that there is
no need to sample the words because the tags are the
only random variables to be marginalized over. Our
model and HMM give very close performance with
difference in accuracy less than 0.1%. This shows
that the MRF is capable of representing an equiva-
lent model represented by the HMM. It also shows
that gradient descent with MCMC approximate in-
ference is capable of finding a good model with the
weights initialized to all 0s.
Unsupervised Learning We trained our model
under the monolingual setting as a sanity check for
our approximate training algorithm. Our model un-
der monolingual mode is exactly the same as the
models introduced in ?2. We ran our model on the
1984 data with the complete tag dictionary. A com-
parison between our result and monolingual directed
model is shown in Table 1. ?Random? is obtained by
choosing a random tag for each word according to
the tag dictionary. ?HMM? is a Bayesian HMM im-
plemented by (Snyder et al, 2008). We also imple-
mented a basic (non-Bayesian) HMM. We trained
the HMM with EM and obtained rsults similar to the
Bayesian HMM (not shown).
5.2 Billingual Results
Table 2 gives the full results in the bilingual setting
for the 1984 dataset with a partial tag dictionary. In
general, MRFs do better than their directed counter-
parts, the HMMs. Interestingly enough, removing
crossing links from the data has only a slight adverse
effect. It appears like the prefix and suffix features
are more important than having crossing links. Re-
69
Language pair HMM MRF MRF w/o cross. MRF w/o spell.
English 71.3 73.3? 0.6 73.4? 0.6 67.4? 0.9
Bulgarian 62.6 62.3? 0.3 63.8? 0.4 55.2? 0.5
Serbian 54.1 55.7? 0.2 54.6? 0.3 47.7? 0.5
Slovene 59.7 61.4? 0.3 60.4? 0.3 56.7? 0.4
English 66.5 73.3? 0.3 73.4? 0.2 62.3? 0.5
Slovene 53.8 59.7? 2.5 57.6? 2.0 52.1? 1.3
Bulgarian 54.2 58.1? 0.1 56.3? 1.3 58.0? 0.2
Serbian 56.9 58.6? 0.3 59.0? 1.2 55.1? 0.3
English 68.2 72.8? 0.6 72.7? 0.6 65.7? 0.4
Serbian 54.7 58.5? 0.6 57.7? 0.3 54.2? 0.3
Bulgarian 55.9 59.8? 0.1 60.3? 0.5 55.0? 0.4
Slovene 58.5 61.4? 0.3 61.6? 0.4 58.1? 0.6
Average 59.7 62.9 62.5 56.5
Table 2: Unsupervised bilingual tagging accuracies with tag dictionary only for the top 100 frequent words.
?HMM? is the result reported by (Snyder et al, 2008). ?MRF? is our contrastive model averaged over ten
runs. ?MRF w/o cross.? is our model trained without crossing links, like Snyder et al?s HMM. ?MRF
w/o spell.? is our model without prefix and suffix features. Numbers appearing next to results are standard
deviations over the ten runs.
Language w/ cross. w/o cross.
French 73.8 70.3
English 56.0 59.2
Table 3: Effect of removing crossing links when
learning French and English in a bilingual setting.
moving the prefix and suffix features gives substan-
tially lower results on average, results even below
plain HMMs.
The reason that crossing links do not change the
results much could be related to fact that most of
the sentence pairs in the 1984 dataset do not contain
many crossing links (only 5% of links cross another
link). To see whether crossing links do have an ef-
fect when they come in larger number, we tested our
model on French-English data. We aligned 10,000
sentences from the Europarl corpus (Koehn, 2005),
resulting in 87K crossing links out of a total of 673K
links. Using the Penn treebank (Marcus et al, 1993)
and the French treebank (Abeille? et al, 2003) to
evaluate the model, results are given in Table 3. It is
evident that crossing links have a larger effect here,
but it is mixed: crossing links improve performance
for French while harming it for English.
6 Conclusion
In this paper, we explored the capabilities of joint
MRFs for modeling bilingual part-of-speech mod-
els. Exact inference with dynamic programming is
not applicable, forcing us to experiment with ap-
proximate inference techniques. We demonstrated
that using contrastive estimation together with Gibbs
sampling for the calculation of the gradient of the
objective function leads to better results in unsuper-
vised bilingual POS induction.
Our experiments also show that the advantage of
using MRFs does not necessarily come from the fact
that we can use non-monotonic alignments in our
model, but instead from the ability to use overlap-
ping features such as prefix and suffix features for
the vocabulary in the data.
Acknowledgments
We thank the reviewers and members of the ARK
group for helpful comments on this work. This re-
search was supported in part by the NSF through
grant IIS-0915187 and the U. S. Army Research
Laboratory and the U. S. Army Research Office un-
der contract/grant number W911NF-10-1-0533.
70
References
A. Abeille?, L. Cle?ment, and F. Toussenel. 2003. Building
a treebank for French. In A. Abeille?, editor, Treebanks.
Kluwer, Dordrecht.
C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan.
2003. An introduction to MCMC for machine learn-
ing. Machine Learning, 50:5?43.
T. Berg-Kirkpatrick, A. Bouchard-Cote, J. DeNero, and
D. Klein. 2010. Unsupervised learning with features.
In Proceedings of NAACL.
S. B. Cohen and N. A. Smith. 2010. Covariance in unsu-
pervised learning of probabilistic grammars. Journal
of Machine Learning Research, 11:3017?3051.
S. B. Cohen, D. Das, and N. A. Smith. 2011. Unsuper-
vised structure prediction with non-parallel multilin-
gual guidance. In Proceedings of EMNLP.
R. K. Congram, C. N. Potts, and S. L. van de Velde.
2002. An iterated Dynasearch algorithm for the
single-machine total weighted tardiness scheduling
problem. Informs Journal On Computing, 14(1):52?
67.
D. Das and S. Petrov. 2011. Unsupervised part-of-
speech tagging with bilingual graph-based projections.
In Procedings of ACL.
T. Erjavec. 2004. MULTEXT-East version 3: Multilin-
gual morphosyntactic specifications, lexicons and cor-
pora. In Proceedings of LREC.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proceedings of HLT-
NAACL.
G. E. Hinton. 2000. Training products of experts by
minimizing contrastive divergence. Technical Report
GCNU TR 2000-004, University College London.
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit 2005.
J. D. Lafferty, A. McCallum, and F. C. N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of ICML.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19:313?330.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
C. N. Potts and S. L. van de Velde. 1995. Dynasearch?
iterative local improvement by dynamic programming.
Part I: The traveling salesman problem. Technical re-
port.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
N. A. Smith. 2006. Novel Estimation Methods for Unsu-
pervised Discovery of Latent Structure in Natural Lan-
guage Text. Ph.D. thesis, Johns Hopkins University.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of EMNLP.
71
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 66?70,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Transliteration by Sequence Labeling with Lattice Encodings and Reranking
Waleed Ammar Chris Dyer Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{wammar,cdyer,nasmith}@cs.cmu.edu
Abstract
We consider the task of generating transliter-
ated word forms. To allow for a wide range of
interacting features, we use a conditional ran-
dom field (CRF) sequence labeling model. We
then present two innovations: a training objec-
tive that optimizes toward any of a set of possi-
ble correct labels (since more than one translit-
eration is often possible for a particular in-
put), and a k-best reranking stage to incorpo-
rate nonlocal features. This paper presents re-
sults on the Arabic-English transliteration task
of the NEWS 2012 workshop.
1 Introduction
Transliteration is the transformation of a piece of
text from one language?s writing system into an-
other. Since the transformation is mostly explained
as local substitutions, deletions, and insertions, we
treat word transliteration as a sequence labeling
problem (Ganesh et al, 2008; Reddy and Waxmon-
sky, 2009), using linear-chain conditional random
fields as our model (Lafferty et al, 2001; Sha and
Pereira, 2003). We tailor this model to the transliter-
ation task in several ways.
First, for the Arabic-English task, each Arabic in-
put is paired with multiple valid English transliter-
ation outputs, any of which is judged to be correct.
To effectively exploit these multiple references dur-
ing learning, we use a training objective in which
the model may favor some correct transliterations
over the others. Computationally efficient inference
is achieved by encoding the references in a lattice.
Second, inference for our first-order sequence la-
beling model requires a runtime that is quadratic in
the number of labels. Since our labels are character
n-grams in the target language, we must cope with
thousands of labels. To make the most of each in-
ference call during training, we apply a mini-batch
training algorithm which converges quickly.
Finally, we wish to consider some global features
that would render exact inference intractable. We
therefore use a reranking model (Collins, 2000).
We demonstrate the performance benefits of these
modifications on the Arabic-English transliteration
task, using the open-source library cdec (Dyer et
al., 2010)1 for learning and prediction.
2 Problem Description
In the NEWS 2012 workshop, the task is to gener-
ate a list of ten transliterations in a specified target
language for each named entity (in a known source
language) in the test set. A training set is provided
for each language pair. An entry in the training set
comprises a named entity in the source language and
one or more transliterations in the target language.
Zhang et al (2012) provides a detailed description
of the shared task.
3 Approach
3.1 Character Alignment
In order to extract source-target character map-
pings, we use m2m-aligner (Jiampojamarn et al,
2007),2 which implements a forward-backward al-
gorithm to sum over probabilities of possible charac-
ter sequence mappings, and uses Expectation Max-
imization to learn mapping probabilities. We allow
source characters to be deleted, but not target char-
acters. Parameters -maxX and -maxY are tuned on
a devevelopment set.
Our running example is the Arabic name EAdl
(in Buckwalter?s ASCII-based encoding of Arabic)
with two English transliterations: ADEL and ?ADIL.
The character alignment for the two pairs is shown
in Fig. 1.
1http://www.cdec-decoder.org
2http://code.google.com/p/m2m-aligner
66
AD
E
L
E
A
d
l
?
?
?
?
A
D
I
L
E
A
d
l
?
?
?
?
'
Arabic English Arabic English
Figure 1: Character alignment for transliterating EAdl to
ADEL and ?ADIL.
3.2 Sequence Labeling Scheme and Notation
We frame transliteration as a sequence labeling
problem. However, transliteration is not a one-to-
one process, meaning that a na??ve application of
one-label-per-token sequence models would be un-
likely to perform well. Previous work has taken
two different approaches. Reddy and Waxmonsky
(2009) first segment the input character sequence,
then use the segments to construct a transliteration
in the target language. Since segmentation errors
will compound to produce transliteration errors, we
avoid this. Ganesh et al (2008) do not require a seg-
mentation step, but their model does not allow for
many-to-one and many-to-many character mappings
which are often necessary.
Our approach overcomes both these shortcom-
ings: we have neither an explicit segmentation step,
nor do we forbid many-to-many mappings. In our
model, each character xi in the source-language in-
put x = ?x1, x2, . . . , xn? is assigned a label yi.
However, a label yi is a sequence of one or more
target-language characters, a special marker indi-
cating a deletion (), or a special marker indicat-
ing involvement in a many-to-one mapping (?), that
is, yi ? ?+ ? {, ?}, where ? is the target lan-
guage alphabet.3 When an input x has multiple al-
ternative reference transliterations, we denote the set
Y?(x) = {y1,y2, . . . ,yK}.
We map the many-to-many alignments produced
by m2m-aligner to one label for each input char-
acter, using the scheme in Table 1. Note that zero-
to-one alignments are not allowed.
The two reference label sequences for our running
example, which are constructed from the alignments
in Fig. 1 are:
3For an input type x, we only consider labels that were ac-
tually observed in the training data, which means the label set
is finite.
Type Alignment Labels
1:0 xi :  yi = 
1:1 xi : tj yi = tj
1:many xi : tj . . . tk yi = tj . . . tk
many:1 xi . . . xp : tj yp = tj
yi = ? ? ? = yp?1 = ?
many:many xi . . . xp : tj . . . tk yp = tj . . . tk
yi = ? ? ? = yp?1 = ?
Table 1: Transforming alignments to sequence labels.
x y1 y2
E ? ?
A A A
d DE DI
l L L
Of key importance in our model is defining, for
each source character, the set of labels that can be
considered for it. For each source character, we add
all labels consistent with character alignments to the
lexicon.
3.3 Model
Our model for mapping from inputs to outputs is
a conditional random field (Lafferty et al, 2001),
which defines the conditional probability of every
possible sequence labeling y of a sequence x with
the parametric form:
p?(y | x) ? exp
?|x|
i=1 ? ? f(x, yi, yi?1) (1)
where f is a vector of real-valued feature functions.
3.4 Features
The feature functions used are instantiated by apply-
ing templates shown in Table 2 to each position i in
the input string x.
3.5 Parameter Learning
Given a training dataset of pairs {?xj ,yj?}
`
j=1 (note
that each y is derived from the max-scoring char-
acter alignment), a CRF is trained to maximize the
regularized conditional log-likelihood:
max
?
L{1,...,`}(?) ,
?`
j=1 log p?(yj | xj) ? C||?||
2
2
(2)
The regularization strength hyperparameter is tuned
on development data. On account of the large data
sizes and large label sets in several language pairs
67
Feature Template Description
U1:yi-xi,
U2:yi-xi?1-xi,
U3:yi-xi-xi+1, moving window of unigram,
U4:yi-xi?2-xi?1-xi, bigram and trigram context
U5:yi-xi?1-xi-xi+1,
U6:yi-xi-xi+1-xi+2
U7:yi, B1:yi-yi?1 label unigrams and bigrams
U8:|yi| label size (in characters)
Table 2: Feature templates for features extracted from
transliteration hypotheses. The SMALLCAPS prefixes
prevent accidental feature collisions.
(Table 3), batch optimization with L-BFGS is in-
feasible. Therefore, we use a variant of the mini-
batch L-BFGS learning approach proposed by Le
et al (2011). This algorithm uses a series of ran-
domly chosen mini-batches B(1),B(2), . . ., each a
subset of {1, . . . , `}, to produce a series of weights
?(1),?(2), . . . by running N iterations of L-BFGS
on each mini-batch to compute the following:
max?(i) LB(i)(?
(i)) ? T??(i) ? ?(i?1)?22 (3)
The T parameter controls how far from the previ-
ous weights the optimizer can move in any particu-
lar mini-batch4. We use mini-batch sizes of 5, and
start training with a small value of T and increase it
as we process more iterations. This is equivalent to
reducing the step-size with the number of iterations
in conventional stochastic learning algorithms.
Language Pair Unique Labels
Arabic-English 1,240
Chinese-English 2,985
Thai-English 1,771
English-Chinese 1,321
English-Japanese Kanji 4,572
Table 3: Size of the label set in some language pairs.
3.6 Using Multiple Reference Transliterations
In some language pairs, NEWS-2012 provides mul-
tiple reference transliterations in the training set. In
this section, we discuss two possibilities for using
these multiple references to train our transliteration
4When T = 0, our learning algorithm is identical to the L-
BFGS mini-batch algorithm of Le et al (2011); however, we
find that more rapid convergence is possible when T > 0.
'
A
DI
L
DE
A
?
Figure 2: Lattice encoding two transliterations of EAdl:
ADEL and ?ADIL.
model. The first possibility is to create multiple in-
dependent training inputs for each input x, one for
each correct transliteration in Y?(x). Using this ap-
proach, with K different transliterations, the CRF
training objective will attempt to assign probability
1
K to each correct transliteration, and 0 to all others
(modulo regularization).
Alternatively, we can train the model to maximize
the marginal probability assigned by the model to
the set of correct labels Y? = {y1, . . . ,yK}. That
is, we assume a set of training data {(xj ,Y?j )}
`
j=1
and replace the standard CRF objective with the fol-
lowing (Dyer, 2009):5
max?
?`
j=1 log
?
y?Y?j
p?(y | xj) ? C||?||22 (4)
This learning objective has more flexibility. It can
maximize the likelihood of the training data by giv-
ing uniform probability to each reference transliter-
ation for a given x, but it does not have to. In effect,
we do not care how probability mass is distributed
among the correct labels. Our hope is that if some
transliterations are difficult to model?perhaps be-
cause they are incorrect?the model will be able to
disregard them.
To calculate the marginal probability for each xj ,
we represent Y?(x) as a label lattice, which is sup-
ported as label reference format in cdec. A fur-
ther computational advantage is that each x in the
training data is now only a single training instance
meaning that fewer forward-backward evaluations
are necessary. The lattice encoding of both translit-
erations of our running example is shown in Fig. 2.
3.7 Reranking
CRFs require feature functions to be ?local? to
cliques in the underlying graphical model. One way
to incorporate global features is to first decode the
5Unlike the standard CRF objective in eq. 2, the marginal
probability objective is non-convex, meaning that we are only
guaranteed to converge to a local optimum in training.
68
k-best transliterations using the CRF, then rerank
based on global features combined with the CRF?s
conditional probability of each candidate. We ex-
periment with three non-local features:
Character language model: an estimate of
pcharLM (y) according to a trigram character lan-
guage model (LM). While a bigram LM can be fac-
tored into local features in a first order CRF, higher
n-gram orders require a higher-order CRF.
Class language model: an estimate of pclassLM (y),
similar to the character LM, but collapses characters
which have a similar phonetic function into one class
(vowels, consonants, and hyphens/spaces). Due to
the reduced number of types in this model, we can
train a 5-gram LM.
Transliteration length: an estimate of plen(|y| |
|x|) assuming a multinomial distribution with pa-
rameters estimated using transliteration pairs of the
training set.
The probabilistic model for each of the global
features is trained using training data provided for
the shared task. The reranking score is a linear
combination of log pcrf (y | x), log pcharLM (y),
log pclassLM (y) and log plen(|y| | |x|). Linear co-
efficients are optimized using simulated annealing,
optimizing accuracy of the 1-best transliteration in a
development set. k-best lists are extracted from the
CRF trellis using the lazy enumeration algorithm of
Huang and Chiang (2005).
4 Experiments
We tested on the NEWS 2012 Arabic-English
dataset. The train, development, and test sets con-
sist of 27,177, 1,292, and 1,296 source named enti-
ties, respectively, with an average 9.6 references per
name in each case.
Table 4 summarizes our results using the ACC
score (Zhang et al, 2012) (i.e., word accuracy in
top-1). ?Basic CRF? is the model with mini-batch
learning and represents multiple reference translit-
erations as independent training examples. We man-
ually tuned the number of training examples and
LBFGS iterations per mini-batch to five and eight,
respectively. ?CRF w/lattice? compactly represents
the multiple references in a lattice, as detailed in
?3.6. We consider reranking using each of the three
global features along with the CRF, as well as the
Model Ar-En
Basic CRF 23.5
CRF w/lattice 37.0
CRF w/lattice; rerank pcrf , pcharLM 40.7
CRF w/lattice; rerank pcrf , pclassLM 38.4
CRF w/lattice; rerank pcrf , plen 37.3
CRF w/lattice, rerank all four 42.8
Table 4: Model performance, measured in word accuracy
in top-1 (ACC, %).
full set of four features.
Maximizing the marginal conditional likelihood
of the set of alternative transliterations (rather than
maximizing each alternative independently) shows
a dramatic improvement in transliteration accuracy
for Arabic-English. Moreover, in Arabic-English
the basic CRF model converges in 120K mini-batch
iterations, which is, approximately, seven times the
number of iterations needed for convergence with
lattice-encoded labels. A model converges when its
ACC score on the development set ceases to improve
in 800 mini-batch iterations. Results also show that
reranking a k-best list of only five transliterations
with any of the global features improves accuracy.
Using all the features together to rerank the k-best
list gives further improvements.
5 Conclusion
We built a CRF transliteration model that allows
for many-to-many character mappings. We address
limitations of CRFs using mini-batch learning and
reranking techniques. We also show how to relax
the learning objective when the training set contains
multiple references, resulting in faster convergence
and improved transliteration accuracy.
We suspect that including features of higher-order
n-gram labels would help improve transliteration ac-
curacy further, but it makes inference intractable due
to the large set of labels. In future work, coarse
transformations of label n-grams might address this
problem.
Acknowledgments
This research was supported in part by the U.S. Army
Research Laboratory and the U.S. Army Research Office
under contract/grant number W911NF-10-1-0533. We
thank anonymous reviewers for the valuable comments.
69
References
M. Collins. 2000. Discriminative reranking for natural
language parsing. In Proc. of ICML.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
S. Ganesh, S. Harsha, P. Pingali, and V. Varma. 2008.
Statistical transliteration for cross language informa-
tion retrieval using HMM alignment and CRF. In
Proc. of the 2nd Workshop On Cross Lingual Infor-
mation Access.
L. Huang and D. Chiang. 2005. Better k-best parsing. In
In Proc. of the 9th International Workshop on Parsing
Technologies.
S. Jiampojamarn, G. Kondrak, and T. Sherif. 2007. Ap-
plying many-to-many alignments and hidden Markov
models to letter-to-phoneme conversion. In Proc. of
NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
Q. V. Le, J. Ngiam, A. Coates, A. Lahiri, B. Prochnow,
and A. Y. Ng. 2011. On optimization methods for
deep learning. In Proc. of ICML.
S. Reddy and S. Waxmonsky. 2009. Substring-based
transliteration with conditional random fields. In Proc.
of the Named Entities Workshop.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. of NAACL-HLT.
M. Zhang, H. Li, M. Liu, and A. Kumaran. 2012.
Whitepaper of NEWS 2012 shared task on machine
transliteration.
70
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 279?287,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Identifying the L1 of non-native writers: the CMU-Haifa system
Yulia Tsvetkov? Naama Twitto? Nathan Schneider? Noam Ordan?
Manaal Faruqui? Victor Chahuneau? Shuly Wintner? Chris Dyer?
?Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA
cdyer@cs.cmu.edu
?Department of Computer Science
University of Haifa
Haifa, Israel
shuly@cs.haifa.ac.il
Abstract
We show that it is possible to learn to identify, with
high accuracy, the native language of English test
takers from the content of the essays they write.
Our method uses standard text classification tech-
niques based on multiclass logistic regression, com-
bining individually weak indicators to predict the
most probable native language from a set of 11 pos-
sibilities. We describe the various features used for
classification, as well as the settings of the classifier
that yielded the highest accuracy.
1 Introduction
The task we address in this work is identifying the
native language (L1) of non-native English (L2) au-
thors. More specifically, given a dataset of short
English essays (Blanchard et al, 2013), composed
as part of the Test of English as a Foreign Lan-
guage (TOEFL) by authors whose native language is
one out of 11 possible languages?Arabic, Chinese,
French, German, Hindi, Italian, Japanese, Korean,
Spanish, Telugu, or Turkish?our task is to identify
that language.
This task has a clear empirical motivation. Non-
native speakers make different errors when they
write English, depending on their native language
(Lado, 1957; Swan and Smith, 2001); understand-
ing the different types of errors is a prerequisite for
correcting them (Leacock et al, 2010), and systems
such as the one we describe here can shed interest-
ing light on such errors. Tutoring applications can
use our system to identify the native language of
students and offer better-targeted advice. Forensic
linguistic applications are sometimes required to de-
termine the L1 of authors (Estival et al, 2007b; Es-
tival et al, 2007a). Additionally, we believe that the
task is interesting in and of itself, providing a bet-
ter understanding of non-native language. We are
thus equally interested in defining meaningful fea-
tures whose contribution to the task can be linguis-
tically interpreted. Briefly, our features draw heav-
ily on prior work in general text classification and
authorship identification, those used in identifying
so-called translationese (Volansky et al, forthcom-
ing), and a class of features that involves determin-
ing what minimal changes would be necessary to
transform the essays into ?standard? English (as de-
termined by an n-gram language model).
We address the task as a multiway text-
classification task; we describe our data in ?3 and
classification model in ?4. As in other author attri-
bution tasks (Juola, 2006), the choice of features for
the classifier is crucial; we discuss the features we
define in ?5. We report our results in ?6 and con-
clude with suggestions for future research.
2 Related work
The task of L1 identification was introduced by Kop-
pel et al (2005a; 2005b), who work on the Inter-
national Corpus of Learner English (Granger et al,
2009), which includes texts written by students from
5 countries, Russia, the Czech Republic, Bulgaria,
France, and Spain. The texts range from 500 to
850 words in length. Their classification method
is a linear SVM, and features include 400 standard
function words, 200 letter n-grams, 185 error types
and 250 rare part-of-speech (POS) bigrams. Ten-
279
fold cross-validation results on this dataset are 80%
accuracy.
The same experimental setup is assumed by Tsur
and Rappoport (2007), who are mostly interested
in testing the hypothesis that an author?s choice of
words in a second language is influenced by the
phonology of his or her L1. They confirm this hy-
pothesis by carefully analyzing the features used by
Koppel et al, controlling for potential biases.
Wong and Dras (2009; 2011) are also motivated
by a linguistic hypothesis, namely that syntactic er-
rors in a text are influenced by the author?s L1.
Wong and Dras (2009) analyze three error types sta-
tistically, and then add them as features in the same
experimental setup as above (using LIBSVM with a
radial kernel for classification). The error types are
subject-verb disagreement, noun-number disagree-
ment and misuse of determiners. Addition of these
features does not improve on the results of Kop-
pel et al. Wong and Dras (2011) further extend
this work by adding as features horizontal slices of
parse trees, thereby capturing more syntactic struc-
ture. This improves the results significantly, yielding
78% accuracy compared with less than 65% using
only lexical features.
Kochmar (2011) uses a different corpus, the Cam-
bridge Learner Corpus, in which texts are 200-400
word long, and are authored by native speakers of
five Germanic languages (German, Swiss German,
Dutch, Swedish and Danish) and five Romance lan-
guages (French, Italian, Catalan, Spanish and Por-
tuguese). Again, SVMs are used as the classification
device. Features include POS n-grams, character n-
grams, phrase-structure rules (extracted from parse
trees), and two measures of error rate. The classi-
fier is evaluated on its ability to distinguish between
pairs of closely-related L1s, and the results are usu-
ally excellent.
A completely different approach is offered by
Brooke and Hirst (2011). Since training corpora for
this task are rare, they use mainly L1 (blog) cor-
pora. Given English word bigrams ?e1, e2?, they try
to assess, for each L1, how likely it is that an L1 bi-
gram was translated literally by the author, resulting
in ?e1, e2?. Working with four L1s (French, Span-
ish, Chinese, and Japanese), and evaluating on the
International Corpus of Learner English, accuracy is
below 50%.
3 Data
Our dataset in this work consists of TOEFL essays
written by speakers of eleven different L1s (Blan-
chard et al, 2013), distributed as part of the Na-
tive Language Identification Shared Task (Tetreault
et al, 2013). The training data consists of 1000
essays from each native language. The essays are
short, consisting of 10 to 20 sentences each. We
used the provided splits of 900 documents for train-
ing and 100 for development. Each document is an-
notated with the author?s English proficiency level
(low, medium, high) and an identification (1 to 8) of
the essay prompt. All essays are tokenized and split
into sentences. In table 1 we provide some statistics
on the training corpora, listed by the authors? profi-
ciency level. All essays were tagged with the Stan-
ford part-of-speech tagger (Toutanova et al, 2003).
We did not parse the dataset.
Low Medium High
# Documents 1,069 5,366 3,456
# Tokens 245,130 1,819,407 1,388,260
# Types 13,110 37,393 28,329
Table 1: Training set statistics.
4 Model
For our classification model we used the creg re-
gression modeling framework to train a 11-class lo-
gistic regression classifier.1 We parameterize the
classifier as a multiclass logistic regression:
p?(y | x) =
exp
?
j ? jh j(x, y)
Z?(x)
,
where x are documents, h j(?) are real-valued feature
functions of the document being classified, ? j are the
corresponding weights, and y is one of the eleven L1
class labels. To train the parameters of our model,
we minimized the following objective,
L = ?
`2 reg.
?????
j
?2j ?
?
{(xi,yi)}
|D|
i=1
(
log likelihood
?          ??          ?
log p?(yi | xi) +
?Ep?(y? |xi) log p?(y
? | xi)
?                      ??                      ?
?conditional entropy
)
,
1https://github.com/redpony/creg
280
which combines the negative log likelihood of the
training dataset D, an `2 (quadratic) penalty on the
magnitude of ? (weighted by ?), and the negative en-
tropy of the predictive model (weighted by ?). While
an `2 weight penalty is standard in regression prob-
lems like this, we found that the the additional en-
tropy term gave more reliable results. Intuitively,
the entropic regularizer encourages the model to re-
main maximally uncertain about its predictions. In
the metaphor of ?maximum entropy?, the entropic
prior finds a solution that has more entropy than the
?maximum? model that is compatible with the con-
straints.
The objective cannot be minimized in closed
form, but it does have a unique minimum and
is straightforwardly differentiable, so we used L-
BFGS to find the optimal weight settings (Liu et al,
1989).
5 Feature Overview
We define a large arsenal of features, our motivation
being both to improve the accuracy of classification
and to be able to interpret the characteristics of the
language produced by speakers of different L1s.
While some of the features were used in prior
work (?2), we focus on two broad novel categories
of features: those inspired by the features used
to identify translationese by Volansky et al (forth-
coming) and those extracted by automatic statisti-
cal ?correction? of the essays. Refer to figure 1 to
see the set of features and their values that were ex-
tracted from an example sentence.
POS n-grams Part-of-speech n-grams were used in
various text-classification tasks.
Prompt Since the prompt contributes information
on the domain, it is likely that some words (and,
hence, character sequences) will occur more fre-
quently with some prompts than with others. We
therefore use the prompt ID in conjunction with
other features.
Document length The number of tokens in the text
is highly correlated with the author?s level of flu-
ency, which in turn is correlated with the author?s
L1.
Pronouns The use of pronouns varies greatly
among different authors. We use the same list
of 25 English pronouns that Volansky et al (forth-
coming) use for identifying translationese.
Punctuation Similarly, different languages use
punctuation differently, and we expect this to taint
the use of punctuation in non-native texts. Of
course, character n-grams subsume this feature.
Passives English uses passive voice more fre-
quently than other languages. Again, the use of
passives in L2 can be correlated with the author?s
L1.
Positional token frequency The choice of the first
and last few words in a sentence is highly con-
strained, and may be significantly influenced by
the author?s L1.
Cohesive markers These are 40 function words
(and short phrases) that have a strong discourse
function in texts (however, because, in fact,
etc.). Translators tend to spell out implicit utter-
ances and render them explicitly in the target text
(Blum-Kulka, 1986). We use the list of Volansky
et al (forthcoming).
Cohesive verbs This is a list of manually compiled
verbs that are used, like cohesive markers, to spell
out implicit utterances (indicate, imply, contain,
etc.).
Function words Frequent tokens, which are mostly
function words, have been used successfully for
various text classification tasks. Koppel and Or-
dan (2011) define a list of 400 such words, of
which we only use 100 (using the entire list was
not significantly different). Note that pronouns
are included in this list.
Contextual function words To further capitalize
on the ability of function words to discriminate,
we define pairs consisting of a function word from
the list mentioned above, along with the POS tag
of its adjacent word. This feature captures pat-
terns such as verbs and the preposition or particle
immediately to their right, or nouns and the deter-
miner that precedes them. We also define 3-grams
consisting of one or two function words and the
POS tag of the third word in the 3-gram.
Lemmas The content of the text is not considered a
good indication of the author?s L1, but many text
categorization tasks use lemmas (more precisely,
the stems produced by the tagger) as features ap-
proximating the content.
Misspelling features Learning to perceive, pro-
duce, and encode non-native phonemic contrasts
281
Firstly the employers live more savely because they are going to have more money to spend for luxury .
Presence Considered alternatives/edits
Characters
"CHAR_l_y_ ": log 2 + 1
"CharPrompt_P5_g_o_i": log 1 + 1
"MFChar_e_ ": log 1 + 1
"Punc_period": log 1 + 1
"DeleteP_p_.": 1.0
"InsertP_p_,": 1.0
"MID:SUBST:v:f": log 1 + 1
"SUBST:v:f": log 1 + 1
Words
"DocLen_": log 19 + 1
"MeanWordRank": 422.6
"CohMarker_because": log 1 + 1
"MostFreq_have": log 1 + 1
"PosToken_last_luxury": log 1 + 1
"Pronouns_they": log 1 + 1
"MSP:safely": log 1 + 1
"Match_p_to": 0.5
"Delete_p_to": 0.5
"Delete_p_are": 1.0
"Delete_p_because": 1.0
"Delete_p_for": 1.0
POS "POS
_VBP_VBG_TO": log 1 + 1
"POS_p_VBP_VBG_TO": 0.059
Words + POS "VBP
_VBG_to": log 1 + 1
"FW__more RB": log 1 + 1
Figure 1: Some of the features extracted for an L1 German sentence.
is extremely difficult for L2 learners (Hayes-Harb
and Masuda, 2008). Since English?s orthogra-
phy is largely phonemic?even if it is irregular
in many places, we expect leaners whose na-
tive phoneme contrasts are different from those
of English to make characteristic spelling errors.
For example, since Japanese and Korean lack a
phonemic /l/-/r/ contrast, we expect native speak-
ers of those languages to be more likely to make
spelling errors that confuse l and r relative to
native speakers of languages such as Spanish in
which that pair is contrastive. To make this in-
formation available to our model, we use a noisy
channel spelling corrector (Kernighan, 1990) to
identify and correct misspelled words in the train-
ing and test data. From these corrections, we ex-
tract minimal edit features that show what inser-
tions, deletions, substitutions and joinings (where
two separate words are written merged into a sin-
gle orthographic token) were made by the author
of the essay.
Restored tags We focus on three important token
classes defined above: punctuation marks, func-
tion words and cohesive verbs. We first remove
words in these classes from the texts, and then
recover the most likely hidden tokens in a se-
quence of words, according to an n-gram lan-
guage model trained on all essays in the training
corpus corrected with a spell checker and con-
taining both words and hidden tokens. This fea-
ture should capture specific words or punctuation
marks that are consistently omitted (deletions),
or misused (insertions, substitutions). To restore
hidden tokens we use the hidden-ngram util-
ity provided in SRI?s language modeling toolkit
(Stolcke, 2002).
Brown clusters (Brown et al, 1992) describe an al-
gorithm that induces a hierarchical clustering of
a language?s vocabulary based on each vocabu-
lary item?s tendency to appear in similar left and
right contexts in a training corpus. While origi-
nally developed to reduce the number of parame-
ters required in n-gram language models, Brown
clusters have been found to be extremely effective
as lexical representations in a variety of regres-
sion problems that condition on text (Koo et al,
2008; Turian et al, 2010; Owoputi et al, 2013).
Using an open-source implementation of the al-
gorithm,2 we clustered 8 billion words of English
into 600 classes.3 We included log counts of all
4-grams of Brown clusters that occurred at least
100 times in the NLI training data.
5.1 Main Features
We use the following four feature types as the base-
line features in our model. For features that are sen-
sitive to frequency, we use the log of the (frequency-
plus-one) as the feature?s value. Table 2 reports the
accuracy of using each feature type in isolation (with
2https://github.com/percyliang/brown-cluster
3http://www.ark.cs.cmu.edu/cdyer/en-600/
cluster_viewer.html
282
Feature Accuracy (%)
POS 55.18
FreqChar 74.12
CharPrompt 65.09
Brown 72.26
DocLen 11.81
Punct 27.41
Pron 22.81
Position 53.03
PsvRatio 12.26
CxtFxn (bigram) 62.79
CxtFxn (trigram) 62.32
Misspell 37.29
Restore 47.67
CohMark 25.71
CohVerb 22.85
FxnWord 42.47
Table 2: Independent performance of feature types de-
tailed in ?5.1, ?5.2 and ?5.3. Accuracy is averaged over
10 folds of cross-validation on the training set.
10-fold cross-validation on the training set).
POS Part-of-speech n-grams. Features were ex-
tracted to count every POS 1-, 2-, 3- and 4-gram
in each document.
FreqChar Frequent character n-grams. We exper-
imented with character n-grams: To reduce the
number of parameters, we removed features only
those character n-grams that are observed more
than 5 times in the training corpus, and n ranges
from 1 to 4. High-weight features include:
TUR:<Turk>; ITA:<Ital>; JPN:<Japa>.
CharPrompt Conjunction of the character n-gram
features defined above with the prompt ID.
Brown Substitutions, deletions and insertions
counts of Brown cluster unigrams and bigrams in
each document.
The accuracy of the classifier on the development set
using these four feature types is reported in table 3.4
5.2 Additional Features
To the basic set of features we now add more spe-
cific, linguistically-motivated features, each adding
a small number of parameters to the model. As
above, we indicate the accuracy of each feature type
in isolation.
4For experiments in this paper combining multiple types of
features, we used Jonathan Clark?s workflow management tool,
ducttape (https://github.com/jhclark/ducttape).
Feature Group # Params Accuracy (%) `2
POS 540,947 55.18 1.0
+ FreqChar 1,036,871 79.55 1.0
+ CharPrompt 2,111,175 79.82 1.0
+ Brown 5,664,461 81.09 1.0
Table 3: Dev set accuracy with main feature groups,
added cumulatively. The number of parameters is always
a multiple of 11 (the number of classes). Only `2 regular-
ization was used for these experiments; the penalty was
tuned on the dev set as well.
DocLen Document length in tokens.
Punct Counts of each punctuation mark.
Pron Counts of each pronoun.
Position Positional token frequency. We use the
counts for the first two and last three words be-
fore the period in each sentence as features. High-
weight features for the second word include:
ARA:2<,>; CHI:2<is>; HIN:2<can>.
PsvRatio The proportion of passive verbs out of all
verbs.
CxtFxn Contextual function words. High-weight
features include: CHI:<some JJ>;
HIN:<as VBN>.
Misspell Spelling correction edits. Features
included substitutions, deletions, insertions,
doubling of letters and missing doublings of
letters, and splittings (alot?a lot), as well as the
word position where the error occurred.
High-weight features include: ARA:DEL<e>,
ARA:INS<e>, ARA:SUBST<e>/<i>;
GER:SUBST<z>/<y>; JPN:SUBST<l>/<r>,
JPN:SUBST<r>/<l>; SPA:DOUBLE<s>,
SPA:MID_INS<s>, SPA:INS<s>.
Restore Counts of substitutions, deletions and
insertions of predefined tokens that we restored
in the texts. High-weight features include:
CHI:DELWORD<do>; GER:DELWORD<on>;
ITA:DELWORD<be>
Table 4 reports the empirical improvement that each
of these brings independently when added to the
main features (?5.1).
5.3 Discarded Features
We also tried several other feature types that did not
improve the accuracy of the classifier on the devel-
opment set.
CohMark Counts of each cohesive marker.
283
Feature Group # Params Accuracy (%) `2
main + Position 6,153,015 81.00 1.0
main + PsvRatio 5,664,472 81.00 1.0
main 5,664,461 81.09 1.0
main + DocLen 5,664,472 81.09 1.0
main + Pron 5,664,736 81.09 1.0
main + Punct 5,664,604 81.09 1.0
main + Misspell 5,799,860 81.27 5.0
main + Restore 5,682,589 81.36 5.0
main + CxtFxn 7,669,684 81.73 1.0
Table 4: Dev set accuracy with main features plus addi-
tional feature groups, added independently. `2 regulariza-
tion was tuned as in table 3 (two values, 1.0 and 5.0, were
tried for each configuration; more careful tuning might
produce slightly better accuracy). Results are sorted by
accuracy; only three groups exhibited independent im-
provements over the main feature set.
CohVerb Counts of each cohesive verb.
FxnWord Counts of function words. These features
are subsumed by the highly discriminative CxtFxn
features.
6 Results
The full model that we used to classify the test set
combines all features listed in table 4. Using all
these features, the accuracy on the development set
is 84.55%, and on the test set it is 81.5%. The values
for ? and ? were tuned to optimize development set
performance, and found to be ? = 5, ? = 2.
Table 5 lists the confusion matrix on the test set,
as well as precision, recall and F1-score for each L1.
The largest error type involved predicting Telugu
when the true label was Hindi, which happened 18
times. This error is unsurprising since many Hindi
and Telugu speakers are arguably native speakers of
Indian English.
Production of L2 texts, not unlike translating from
L1 to L2, involves a tension between the impos-
ing models of L1 (and the source text), on the one
hand, and a set of cognitive constraints resulting
from the efforts to generate the target text, on the
other. The former is called interference in Trans-
lation Studies (Toury, 1995) and transfer in second
language acquisition (Selinker, 1972). Volansky et
al. (forthcoming) designed 32 classifiers to test the
validity of the forces acting on translated texts, and
found that features sensitive to interference consis-
tently yielded the best performing classifiers. And
indeed, in this work too, we find fingerprints of the
source language are dominant in the makeup of L2
texts. The main difference, however, between texts
translated by professionals and the texts we address
here, is that more often than not professional trans-
lators translate into their mother tongue, whereas L2
writers write out of their mother tongue by defini-
tion. So interference is ever more exaggerated in
this case, for example, also phonologically (Tsur and
Rappoport, 2007).
We explore the effects of interference by analyz-
ing several patterns we observe in the features. Our
classifier finds that the character sequence alot is
overrepresented in Arabic L2 texts. Arabic has no
indefinite article and we speculate that Arabic speak-
ers conceive a lot as a single word; the Arabic equiv-
alent for a lot is used adverbially like an -ly suffix
in English. For the same reason, another promi-
nent feature is a missing definite article before nouns
and adjectives. Additionally, Arabic, being an Ab-
jad language, rarely indicates vowels, and indeed we
find many missing e?s and i?s in the texts of Arabic
speakers. Phonologically, because Arabic conflates
/I/ and /@/ into /i/ (at least in Modern Standard Ara-
bic), we see that many e?s are indeed substituted for
i?s in these texts.
We find that essays that contain hyphens are more
likely to be from German authors. We again find
evidence of interference from the native language
here. First, relative clauses are widely used in Ger-
man, and we see this pattern in L2 English of L1
German speakers. For example, any given rational
being ? let us say Immanual Kant ? we find that.
Another source of extra hyphens stems from com-
pounding convention. So, for example, we find well-
known, community-help, spare-time, football-club,
etc. Many of these reflect an effort to both connect
and separate connected forms in the original (e.g.,
Fussballklub, which in English would be more natu-
rally rendered as football club). Another unexpected
feature of essays by native Germans is a frequent
substitution of the letter y for z and vice versa. We
suspect this owes to their switched positions on Ger-
man keyboards.
Lexical item frequency also provides clues to the
L1 of the essay writers. The word that occurs more
frequently in the texts of German L1 speakers. We
284
true? ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision (%) Recall (%) F1 (%)
ARA 80 0 2 1 3 4 1 0 4 2 3 80.8 80.0 80.4
CHI 3 80 0 1 1 0 6 7 1 0 1 88.9 80.0 84.2
FRE 2 2 81 5 1 2 1 0 3 0 3 86.2 81.0 83.5
GER 1 1 1 93 0 0 0 1 1 0 2 87.7 93.0 90.3
HIN 2 0 0 1 77 1 0 1 5 9 4 74.8 77.0 75.9
ITA 2 0 3 1 1 87 1 0 3 0 2 82.1 87.0 84.5
JPN 2 1 1 2 0 1 87 5 0 0 1 78.4 87.0 82.5
KOR 1 5 2 0 1 0 9 81 1 0 0 80.2 81.0 80.6
SPA 2 0 2 0 1 8 2 1 78 1 5 77.2 78.0 77.6
TEL 0 1 0 0 18 1 2 1 1 73 3 85.9 73.0 78.9
TUR 4 0 2 2 0 2 2 4 4 0 80 76.9 80.0 78.4
Table 5: Official test set confusion matrix with the full model. Accuracy is 81.5%.
hypothesize that in English it is optional in rela-
tive clauses whereas in German it is not, so Ger-
man speakers are less comfortable using the non-
obligatory form. Also, often is over represented. We
hypothesize that since it is cognate of German oft, it
is not cognitively expensive to retrieve it. We find
many times?a literal translation of muchas veces?
in Spanish essays.
Other informative features that reflect L1 features
include frequent misspellings involving confusions
of l and r in Japanese essays. More mysteriously,
the characters r and s are misused in Chinese and
Spanish, respectively. The word then is dominant
in the texts of Hindi speakers. Finally, it is clear
that authors refer to their native cultures (and, conse-
quently, native languages and countries); the strings
Turkish, Korea, and Ita were dominant in the texts of
Turkish, Korean and Italian native speakers, respec-
tively.
7 Discussion
We experimented with different classifiers and a
large set of features to solve an 11-way classifica-
tion problem. We hope that studying this problem
will improve to facilitate human assessment, grad-
ing, and teaching of English as a second language.
While the core features used are sparse and sensitive
to lexical and even orthographic features of the writ-
ing, many of them are linguistically informed and
provide insight into how L1 and L2 interact.
Our point of departure was the analogy between
translated texts as a genre in its own and L2 writ-
ers as pseudo translators, relying heavily on their
mother tongue and transferring their native models
to a second language. In formulating our features,
we assumed that like translators, L2 writers will
write in a simplified manner and overuse explicit
markers. Although this should be studied vis-?-vis
comparable outputs of mother tongue writers in En-
glish, we observe that the best features of our clas-
sifiers are of the ?interference? type, i.e. phonolog-
ical, morphological and syntactic in nature, mostly
in the form of misspelling features, restoration tags,
punctuation and lexical and syntactic modeling.
We would like to stress that certain features indi-
cating a particular L1 have no bearing on the quality
of the English produced. This has been discussed
extensively in Translation Studies (Toury, 1995),
where interference is observed by the overuse or un-
deruse of certain features reflecting the typological
differences between a specific pair of languages, but
which is still within grammatical limits. For exam-
ple, the fact that Italian native speakers favor the
syntactic sequence of determiner + adjective + noun
(e.g., a big risk or this new business) has little pre-
scriptive value for teachers.
A further example of how L2 quality and the
ability to predict L1 are uncorrelated, we noted
that certain L2 writers often repeat words appear-
ing in their essay prompts, and including informa-
tion about whether the writer was reusing prompt
words improved classification accuracy. We suggest
this reflects different educational backgrounds. This
feature says nothing about the quality of the text, just
as the tendency of Korean and Italian writers to men-
tion their home country more often does not.
285
Acknowledgments
This research was supported by a grant from the Is-
raeli Ministry of Science and Technology.
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. TOEFL11: A
corpus of non-native English. Technical report, Edu-
cational Testing Service.
Shoshana Blum-Kulka. 1986. Shifts of cohesion and co-
herence in translation. In Juliane House and Shoshana
Blum-Kulka, editors, Interlingual and intercultural
communication Discourse and cognition in translation
and second language acquisition studies, volume 35,
pages 17?35. Gunter Narr Verlag.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Conference
of Learner Corpus Research (LCR2011), Louvain-la-
Neuve, Belgium. Presses universitaires de Louvain.
Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4).
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007a. Author profil-
ing for English emails. In Proc. of PACLING, pages
263?272, Melbourne, Australia.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007b. TAT: An author
profiling tool with application to Arabic emails. In
Proc. of the Australasian Language Technology Work-
shop, pages 21?30, Melbourne, Australia, December.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier,
and Magali Paquot. 2009. International Corpus of
Learner English. Presses universitaires de Louvain,
Louvain-la-Neuve.
Rachel Hayes-Harb and Kyoko Masuda. 2008. Devel-
opment of the ability to lexically encode novel second
language phonemic contrasts. Second Language Re-
search, 24(1):5?33.
Patrick Juola. 2006. Authorship attribution. Founda-
tions and Trends in Information Retrieval, 1(3):233?
334.
Mark D. Kernighan. 1990. A spelling correction pro-
gram based on a noisy channel model. In Proc. of
COLING.
Ekaterina Kochmar. 2011. Identification of a writer?s na-
tive language by error analysis. Master?s thesis, Uni-
versity of Cambridge.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Proc.
of ACL.
Moshe Koppel and Noam Ordan. 2011. Translationese
and its dialects. In Proc. of ACL-HLT, pages 1318?
1326, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005a.
Automatically determining an anonymous author?s na-
tive language. Intelligence and Security Informatics,
pages 41?76.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005b.
Determining an author?s native language by mining
a text for errors. In Proc. of KDD, pages 624?628,
Chicago, IL. ACM.
Robert Lado. 1957. Linguistics across cultures: applied
linguistics for language teachers. University of Michi-
gan Press, Ann Arbor, Michigan, June.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel Tetreault. 2010. Automated Grammatical
Error Detection for Language Learners. Morgan and
Claypool.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory BFGS method
for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Proc. of NAACL.
Larry Selinker. 1972. Interlanguage. International
Review of Applied Linguistics in Language Teaching,
10(1?4):209?232.
Andreas Stolcke. 2002. SRILM?an extensible lan-
guage modeling toolkit. In Procedings of Interna-
tional Conference on Spoken Language Processing,
pages 901?904.
Michael Swan and Bernard Smith. 2001. Learner En-
glish: A Teacher?s Guide to Interference and Other
Problems. Cambridge Handbooks for Language
Teachers. Cambridge University Press.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proc. of the Eighth Workshop on Inno-
vative Use of NLP for Building Educational Applica-
tions, Atlanta, GA, USA, June. Association for Com-
putational Linguistics.
Gideon Toury. 1995. Descriptive Translation Studies
and beyond. John Benjamins, Amsterdam / Philadel-
phia.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Proc.
of HLT-NAACL, pages 173?180, Edmonton, Canada,
June. Association for Computational Linguistics.
286
Oren Tsur and Ari Rappoport. 2007. Using classifier fea-
tures for studying the effect of native language on the
choice of written second language words. In Proc. of
the Workshop on Cognitive Aspects of Computational
Language Acquisition, pages 9?16, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proc. of ACL.
Vered Volansky, Noam Ordan, and Shuly Wintner. forth-
coming. On the features of translationese. Literary
and Linguistic Computing.
Sze-Meng Jojo Wong and Mark Dras. 2009. Contrastive
analysis and native language identification. In Proc.
of the Australasian Language Technology Association
Workshop, pages 53?61, Sydney, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. of EMNLP, pages 1600?1610, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
287
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 70?77,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2013:
Syntax, Synthetic Translation Options, and Pseudo-References
Waleed Ammar Victor Chahuneau Michael Denkowski Greg Hanneman
Wang Ling Austin Matthews Kenton Murray Nicola Segall Yulia Tsvetkov
Alon Lavie Chris Dyer?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submit-
ted to the 2013 WMT shared task in ma-
chine translation. We participated in three
language pairs, French?English, Russian?
English, and English?Russian. Our
particular innovations include: a label-
coarsening scheme for syntactic tree-to-
tree translation and the use of specialized
modules to create ?synthetic translation
options? that can both generalize beyond
what is directly observed in the parallel
training data and use rich source language
context to decide how a phrase should
translate in context.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute par-
ticipated in three language pairs for the 2013
Workshop on Machine Translation shared trans-
lation task: French?English, Russian?English,
and English?Russian. Our French?English sys-
tem (?3) showcased our group?s syntactic sys-
tem with coarsened nonterminal types (Hanne-
man and Lavie, 2011). Our Russian?English and
English?Russian system demonstrate a new multi-
phase approach to translation that our group is us-
ing, in which synthetic translation options (?4)
to supplement the default translation rule inven-
tory that is extracted from word-aligned training
data. In the Russian-English system (?5), we used
a CRF-based transliterator (Ammar et al, 2012)
to propose transliteration candidates for out-of-
vocabulary words, and used a language model
to insert or remove common function words in
phrases according to an n-gram English language
model probability. In the English?Russian system
(?6), we used a conditional logit model to predict
the most likely inflectional morphology of Rus-
sian lemmas, conditioning on rich source syntac-
tic features (?6.1). In addition to being able to
generate inflected forms that were otherwise unob-
served in the parallel training data, the translations
options generated in this matter had features re-
flecting their appropriateness given much broader
source language context than usually would have
been incorporated in current statistical MT sys-
tems.
For our Russian?English system, we addition-
ally used a secondary ?pseudo-reference? transla-
tion when tuning the parameters of our Russian?
English system. This was created by automatically
translating the Spanish translation of the provided
development data into English. While the output
of an MT system is not always perfectly gram-
matical, previous work has shown that secondary
machine-generated references improve translation
quality when only a single human reference is
available when BLEU is used as an optimization
criterion (Madnani, 2010; Dyer et al, 2011).
2 Common System Components
The decoder infrastructure we used was cdec
(Dyer et al, 2010). Only the constrained data
resources provided for the shared task were used
for training both the translation and language
models. Word alignments were generated us-
ing the Model 2 variant described in Dyer et al
(2013). Language models used modified Kneser-
Ney smoothing estimated using KenLM (Heafield,
2011). Translation model parameters were dis-
criminatively set to optimize BLEU on a held-out
development set using an online passive aggres-
sive algorithm (Eidelman, 2012) or, in the case of
70
the French?English system, using the hypergraph
MERT algorithm and optimizing towards BLEU
(Kumar et al, 2009). The remainder of the paper
will focus on our primary innovations in the vari-
ous system pairs.
3 French-English Syntax System
Our submission for French?English is a tree-to-
tree translation system that demonstrates several
innovations from group?s research on SCFG-based
translation.
3.1 Data Selection
We divided the French?English training data into
two categories: clean data (Europarl, News Com-
mentary, UN Documents) totaling 14.8 million
sentence pairs, and web data (Common Crawl,
Giga-FrEn) totaling 25.2 million sentence pairs.
To reduce the volume of data used, we filtered
non-parallel and other unhelpful segments accord-
ing to the technique described by Denkowski et al
(2012). This procedure uses a lexical translation
model learned from just the clean data, as well as
source and target n-gram language models to com-
pute the following feature scores:
? French and English 4-gram log likelihood (nor-
malized by length);
? French?English and English?French lexical
translation log likelihood (normalized by
length); and,
? Fractions of aligned words under the French?
English and English?French models.
We pooled previous years? WMT news test sets
to form a reference data set. We computed the
same features. To filter the web data, we retained
only sentence for which each feature score was
no lower than two standard deviations below the
mean on the reference data. This reduced the web
data from 25.2 million to 16.6 million sentence
pairs. Parallel segments from all parts of the data
that were blank on either side, were longer than 99
tokens, contained a token of more than 30 charac-
ters, or had particularly unbalanced length ratios
were also removed. After filtering, 30.9 million
sentence pairs remained for rule extraction: 14.4
million from the clean data, and 16.5 million from
the web data.
3.2 Preprocessing and Grammar Extraction
Our French?English system uses parse trees in
both the source and target languages, so tokeniza-
tion in this language pair was carried out to match
the tokenizations expected by the parsers we used
(English data was tokenized with the Stanford to-
kenizer for English and an in-house tokenizer for
French that targets the tokenization used by the
Berkeley French parser). Both sides of the par-
allel training data were parsed using the Berkeley
latent variable parser.
Synchronous context-free grammar rules were
extracted from the corpus following the method of
Hanneman et al (2011). This decomposes each
tree pair into a collection of SCFG rules by ex-
haustively identifying aligned subtrees to serve as
rule left-hand sides and smaller aligned subtrees
to be abstracted as right-hand-side nonterminals.
Basic subtree alignment heuristics are similar to
those by Galley et al (2006), and composed rules
are allowed. The computational complexity is held
in check by a limit on the number of RHS elements
(nodes and terminals), rather than a GHKM-style
maximum composition depth or Hiero-style max-
imum rule span. Our rule extractor also allows
?virtual nodes,? or the insertion of new nodes in
the parse tree to subdivide regions of flat struc-
ture. Virtual nodes are similar to the A+B ex-
tended categories of SAMT (Zollmann and Venu-
gopal, 2006), but with the added constraint that
they may not conflict with the surrounding tree
structure.
Because the SCFG rules are labeled with non-
terminals composed from both the source and tar-
get trees, the nonterminal inventory is quite large,
leading to estimation difficulties. To deal with
this, we automatically coarsening the nonterminal
labels (Hanneman and Lavie, 2011). Labels are
agglomeratively clustered based on a histogram-
based similarity function that looks at what tar-
get labels correspond to a particular source label
and vice versa. The number of clusters used is de-
termined based on spikes in the distance between
successive clustering iterations, or by the number
of source, target, or joint labels remaining. Start-
ing from a default grammar of 877 French, 2580
English, and 131,331 joint labels, we collapsed
the label space for our WMT system down to 50
French, 54 English, and 1814 joint categories.1
1Selecting the stopping point still requires a measure of
intuition. The label set size of 1814 chosen here roughly cor-
responds to the number of joint labels that would exist in the
grammar if virtual nodes were not included. This equivalence
has worked well in practice in both internal and published ex-
periments on other data sets (Hanneman and Lavie, 2013).
71
Extracted rules each have 10 features associated
with them. For an SCFG rule with source left-
hand side `s, target left-hand side `t, source right-
hand side rs, and target right-hand side rt, they
are:
? phrasal translation log relative frequencies
log f(rs | rt) and log f(rt | rs);
? labeling relative frequency log f(`s, `t | rs, rt)
and generation relative frequency
log f(rs, rt | `s, `t);
? lexical translation log probabilities log plex(rs |
rt) and log plex(rt | rs), defined similarly to
Moses?s definition;
? a rarity score exp( 1c )?1exp(1)?1 for a rule with frequency
c (this score is monotonically decreasing in the
rule frequency); and,
? three binary indicator features that mark
whether a rule is fully lexicalized, fully abstract,
or a glue rule.
Grammar filtering. Even after collapsing la-
bels, the extracted SCFGs contain an enormous
number of rules ? 660 million rule types from just
under 4 billion extracted instances. To reduce the
size of the grammar, we employ a combination of
lossless filtering and lossy pruning. We first prune
all rules to select no more than the 60 most fre-
quent target-side alternatives for any source RHS,
then do further filtering to produce grammars for
each test sentence:
? Lexical rules are filtered to the sentence level.
Only phrase pairs whose source sides match the
test sentence are retained.
? Abstract rules (whose RHS are all nontermi-
nals) are globally pruned. Only the 4000 most
frequently observed rules are retained.
? Mixed rules (whose RHS are a mix of terminals
and nonterminals) must match the test sentence,
and there is an additional frequency cutoff.
After this filtering, the number of completely lex-
ical rules that match a given sentence is typically
low, up to a few thousand rules. Each fully ab-
stract rule can potentially apply to every sentence;
the strict pruning cutoff in use for these rules is
meant to focus the grammar to the most important
general syntactic divergences between French and
English. Most of the latitude in grammar pruning
comes from adjusting the frequency cutoff on the
mixed rules since this category of rule is by far the
most common type. We conducted experiments
with three different frequency cutoffs: 100, 200,
and 500, with each increase decreasing the gram-
mar size by 70?80 percent.
3.3 French?English Experiments
We tuned our system to the newstest2008 set of
2051 segments. Aside from the official new-
stest2013 test set (3000 segments), we also col-
lected test-set scores from last year?s newstest2012
set (3003 segments). Automatic metric scores
are computed according to BLEU (Papineni et al,
2002), METEOR (Denkowski and Lavie, 2011),
and TER (Snover et al, 2006), all computed ac-
cording to MultEval v. 0.5 (Clark et al, 2011).
Each system variant is run with two independent
MERT steps in order to control for optimizer in-
stability.
Table 1 presents the results, with the metric
scores averaged over both MERT runs. Quite in-
terestingly, we find only minor differences in both
tune and test scores despite the large differences in
filtered/pruned grammar size as the cutoff for par-
tially abstract rules increases. No system is fully
statistically separable (at p < 0.05) from the oth-
ers according to MultEval?s approximate random-
ization algorithm. The closest is the variant with
cutoff 200, which is generally judged to be slightly
worse than the other two. METEOR claims full
distinction on the 2013 test set, ranking the sys-
tem with the strictest grammar cutoff (500) best.
This is the version that we ultimately submitted to
the shared translation task.
4 Synthetic Translation Options
Before discussing our Russian?English and
English?Russian systems, we introduce the
concept of synthetic translation options, which
we use in these systems. We provide a brief
overview here; for more detail, we refer the reader
to Tsvetkov et al (2013).
In language pairs that are typologically similar,
words and phrases map relatively directly from
source to target languages, and the standard ap-
proach to learning phrase pairs by extraction from
parallel data can be very effective. However, in
language pairs in which individual source lan-
guage words have many different possible transla-
tions (e.g., when the target language word could
have many different inflections or could be sur-
rounded by different function words that have no
72
Dev (2008) Test (2012) Test (2013)
System BLEU METR TER BLEU METR TER BLEU METR TER
Cutoff 100 22.52 31.44 59.22 27.73 33.30 53.25 28.34 * 33.19 53.07
Cutoff 200 22.34 31.40 59.21 * 27.33 33.26 53.23 * 28.05 * 33.07 53.16
Cutoff 500 22.80 31.64 59.10 27.88 * 33.58 53.09 28.27 * 33.31 53.13
Table 1: French?English automatic metric scores for three grammar pruning cutoffs, averaged over two
MERT runs each. Scores that are statistically separable (p < 0.05) from both others in the same column
are marked with an asterisk (*).
direct correspondence in the source language), we
can expect the standard phrasal inventory to be
incomplete, except when very large quantities of
parallel data are available or for very frequent
words. There simply will not be enough exam-
ples from which to learn the ideal set of transla-
tion options. Therefore, since phrase based trans-
lation can only generate input/output word pairs
that were directly observed in the training corpus,
the decoder?s only hope for producing a good out-
put is to find a fluent, meaning-preserving transla-
tion using incomplete translation lexicons. Syn-
thetic translation option generation seeks to fill
these gaps using secondary generation processes
that produce possible phrase translation alterna-
tives that are not directly extractable from the
training data. By filling in gaps in the transla-
tion options used to construct the sentential trans-
lation search space, global discriminative transla-
tion models and language models can be more ef-
fective than they would otherwise be.
From a practical perspective, synthetic transla-
tion options are attractive relative to trying to build
more powerful models of translation since they
enable focus on more targeted translation prob-
lems (for example, transliteration, or generating
proper inflectional morphology for a single word
or phrase). Since they are translation options and
not complete translations, many of them may be
generated.
In the following system pairs, we use syn-
thetic translation options to augment hiero gram-
mar rules learned in the usual way. The synthetic
phrases we include augment draw from several
sources:
? transliterations of OOV Russian words (?5.3);
? English target sides with varied function words
(for example, given a phrase that translates into
cat we procedure variants like the cat, a cat and
of the cat); and,
? when translating into Russian, we generate
phrases by first predicting the most likely Rus-
sian lemma for a source word or phrase, and
then, conditioned on the English source context
(including syntactic and lexical features), we
predict the most likely inflection of the lemma
(?6.1).
5 Russian?English System
5.1 Data
We used the same parallel data for both the
Russian?English and English Russian systems.
Except for filtering to remove sentence pairs
whose log length ratios were statistical outliers,
we only filtered the Common Crawl corpus to re-
move sentence pairs with less than 50% concentra-
tion of Cyrillic characters on the Russian side. The
remaining data was tokenized and lower-cased.
For language models, we trained 4-gram Markov
models using the target side of the bitext and any
available monolingual data (including Gigaword
for English). Additionally, we trained 7-gram lan-
guage models using 600-class Brown clusters with
Witten-Bell smoothing.2
5.2 Baseline System
Our baseline Russian?English system is a hierar-
chical phrase-based translation model as imple-
mented in cdec (Chiang, 2007; Dyer et al, 2010).
SCFG translation rules that plausibly match each
sentence in the development and deftest sets were
extracted from the aligned parallel data using the
suffix array indexing technique of Lopez (2008).
A Russian morphological analyzer was used to
lemmatize the training, development, and test
data, and the ?noisier channel? translation ap-
proach of Dyer (2007) was used in the Russian?
English system to let unusually inflected surface
forms back off to per-lemma translations.
2http://www.ark.cs.cmu.edu/cdyer/ru-600/.
73
5.3 Synthetic Translations: Transliteration
Analysis revealed that about one third of the un-
seen Russian tokens in the development set con-
sisted of named entities which should be translit-
erated. We used individual Russian-English word
pairs in Wikipedia parallel headlines 3 to train a
linear-chained CRF tagger which labels each char-
acter in the Russian token with a sequence of zero
or more English characters (Ammar et al, 2012).
Since Russian names in the training set were in
nominative case, we used a simple rule-based mor-
phological generator to produce possible inflec-
tions and filtered out the ones not present in the
Russian monolingual corpus. At decoding, un-
seen Russian tokens are fed to the transliterator
which produces the most probable 20 translitera-
tions. We add a synthetic translation option for
each of the transliterations with four features: an
indicator feature for transliterations, the CRF un-
normalized score, the trigram character-LM log-
probability, and the divergence from the average
length-ratio between an English name and its Rus-
sian transliteration.
5.4 Synthetic Translations: Function Words
Slavic languages like Russian have a large number
of different inflected forms for each lemma, repre-
senting different cases, tenses, and aspects. Since
our training data is rather limited relative to the
number of inflected forms that are possible, we use
an English language model to generate a variety
of common function word contexts for each con-
tent word phrase. These are added to the phrase
table with a feature indicating that they were not
actually observed in the training data, but rather
hallucinated using SRILM?s disambig tool.
5.5 Summary
Table 5.5 summarizes our Russian-English trans-
lation results. In the submitted system, we addi-
tionally used MBR reranking to combine the 500-
best outputs of our system, with the 500-best out-
puts of a syntactic system constructed similarly to
the French?English system.
6 English?Russian System
The bilingual training data was identical to the
filtered data used in the previous section. Word
alignments was performed after lemmatizing the
3We contributed the data set to the shared task participants
at http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz
Table 2: Russian-English summary.
Condition BLEU
Baseline 30.8
Function words 30.9
Transliterations 31.1
Russian side of the training corpus. An unpruned,
modified Kneser-Ney smoothed 4-gram language
model (Chen and Goodman, 1996) was estimated
from all available Russian text (410 million words)
using the KenLM toolkit (Heafield et al, 2013).
A standard hierarchical phrase-based system
was trained with rule shape indicator features, ob-
tained by replacing terminals in translation rules
by a generic symbol. MIRA training was per-
formed to learn feature weights.
Additionally, word clusters (Brown et al, 1992)
were obtained for the complete monolingual Rus-
sian data. Then, an unsmoothed 7-gram language
model was trained on these clusters and added as
a feature to the translation system. Indicator fea-
tures were also added for each cluster and bigram
cluster occurence. These changes resulted in an
improvement of more than a BLEU point on our
held-out development set.
6.1 Predicting Target Morphology
We train a classifier to predict the inflection of
each Russian word independently given the cor-
responding English sentence and its word align-
ment. To do this, we first process the Russian
side of the parallel training data using a statisti-
cal morphological tagger (Sharoff et al, 2008) to
obtain lemmas and inflection tags for each word
in context. Then, we obtain part-of-speech tags
and dependency parses of the English side of the
parallel data (Martins et al, 2010), as well as
Brown clusters (Brown et al, 1992). We extract
features capturing lexical and syntactical relation-
ships in the source sentence and train structured
linear logistic regression models to predict the tag
of each English word independently given its part-
of-speech.4 In practice, due to the large size of
the corpora and of the feature space dimension,
we were only able to use about 10% of the avail-
able bilingual data, sampled randomly from the
Common Crawl corpus. We also restricted the
4We restrict ourselves to verbs, nouns, adjectives, adverbs
and cardinals since these open-class words carry most inflec-
tion in Russian.
74
??? ???????? ???????? ???? ?? ?? ?????????
she had attempted to cross the road on her bike
PRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NN
nsubj
aux
xcomp
aux
????????_V*+*mis/sfm/e
C50   C473        C28          C8    C275   C37   C43  C82 C94   C331
Figure 1: The classifier is trained to predict the verbal inflection mis-sfm-e based on the linear and
syntactic context of the words aligned to the Russian word; given the stem ???????? (pytat?sya), this
inflection paradigm produces the observed surface form ???????? (pytalas?).
set of possible inflections for each word to the set
of tags that were observed with its lemma in the
full monolingual training data. This was neces-
sary because of our choice to use a tagger, which
is not able to synthesize surface forms for a given
lemma-tag pair.
We then augment the standard hierarchical
phrase-base grammars extracted for the baseline
systems with new rules containing inflections not
necessarily observed in the parallel training data.
We start by training a non-gappy phrase transla-
tion model on the bilingual data where the Russian
has been lemmatized.5 Then, before translating an
English sentence, we extract translation phrases
corresponding to this specific sentence and re-
inflect each word in the target side of these phrases
using the classifier with features extracted from
the source sentence words and annotations. We
keep the original phrase-based translation features
and add the inflection score predicted by the clas-
sifier as well as indicator features for the part-of-
speech categories of the re-inflected words.
On a held-out development set, these synthetic
phrases produce a 0.3 BLEU point improvement.
Interestingly, the feature weight learned for using
these phrases is positive, indicating that useful in-
flections might be produced by this process.
7 Conclusion
The CMU systems draws on a large number of
different research directions. Techniques such as
MBR reranking and synthetic phrases allow dif-
ferent contributors to focus on different transla-
5We keep intact words belonging to non-predicted cate-
gories.
tion problems that are ultimately recombined into
a single system. Our performance, in particular,
on English?Russian machine translation was quite
satisfying, we attribute our biggest gains in this
language pair to the following:
? Our inflection model that predicted how an En-
glish word ought best be translated, given its
context. This enabled us to generate forms that
were not observed in the parallel data or would
have been rare independent of context with pre-
cision.
? Brown cluster language models seem to be quite
effective at modeling long-range morphological
agreement patterns quite reliably.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
75
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computional Linguistics, 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California, USA,
June. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2007. The ?noiser channel?: Translation
from morphologically complex languages. In Pro-
ceedings of WMT.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, Sofia, Bulgaria,
August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In Proc. of
ACL-IJCNLP.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. of COLING.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Andre? F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and Ma?rio A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
76
Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, Anna
Feldman, and Dagmar Divjak. 2008. Designing and
evaluating a russian tagset. In Proc. of LREC.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, Massachusetts, USA,
August.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
77
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271?280,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Generating English Determiners in Phrase-Based Translation with
Synthetic Translation Options
Yulia Tsvetkov Chris Dyer Lori Levin Archna Bhatia
Language Technologies Institute
Carnegie Mellon University
Pittspurgh, PA, 15213, USA
{ytsvetko, cdyer, lsl, archna}@cs.cmu.edu
Abstract
We propose a technique for improving
the quality of phrase-based translation
systems by creating synthetic translation
options?phrasal translations that are gen-
erated by auxiliary translation and post-
editing processes?to augment the de-
fault phrase inventory learned from par-
allel data. We apply our technique to
the problem of producing English deter-
miners when translating from Russian and
Czech, languages that lack definiteness
morphemes. Our approach augments the
English side of the phrase table using a
classifier to predict where English arti-
cles might plausibly be added or removed,
and then we decode as usual. Doing
so, we obtain significant improvements in
quality relative to a standard phrase-based
baseline and to a to post-editing complete
translations with the classifier.
1 Introduction
Phrase-based translation works as follows. A set
of candidate translations for an input sentence is
created by matching contiguous spans of the in-
put against an inventory of phrasal translations,
reordering them into a target-language appropri-
ate order, and choosing the best one according to a
discriminative model that combines features of the
phrases used, reordering patterns, and target lan-
guage model (Koehn et al, 2003). This relatively
simple approach to translation can be remarkably
effective, and, since its introduction, it has been
the basis for further innovations, including devel-
oping better models for distinguishing the good
translations from bad ones (Chiang, 2012; Gim-
pel and Smith, 2012; Cherry and Foster, 2012;
Eidelman et al, 2013), improving the identifica-
tion of phrase pairs in parallel data (DeNero et al,
2008; DeNero and Klein, 2010), and formal gen-
eralizations to gapped rules and rich nonterminal
types (Chiang, 2007; Galley et al, 2006). This
paper proposes a different mechanism for improv-
ing phrase-based translation: the use of synthetic
translation options to supplement the standard
phrasal inventory used in phrase-based translation
systems.
In the following, we argue that phrase tables ac-
quired in usual way will be expected to have gaps
in their coverage in certain language pairs and
that supplementing these with synthetic translation
options is a priori preferable to alternative tech-
niques, such as post processing, for generalizing
beyond the translation pairs observable in training
data (?2). As a case study, we consider the prob-
lem of producing English definite/indefinite arti-
cles (the, a, and an) when translating from Russian
and Czech, two languages that lack overt definite-
ness morphemes (?3). We develop a classifier that
predicts the presence and absence of English arti-
cles (?4). This classifier is used to generate syn-
thetic translation options that are used to augment
phrase tables used the usual way (?5). We eval-
uate their performance relative to post-processing
approach and to a baseline phrase-based system,
finding that synthetic translation options reliably
outperform the other approaches (?6). We then
discuss how our approach relates to previous work
(?7) and conclude by discussing further applica-
tions of our technique (?8).
2 Why Synthetic Translation Options?
Before turning to the problem of generating En-
glish articles, we give arguments for why syn-
thetic translation options are a useful extension of
271
standard phrase-based translation approaches, and
why this technique might be better than some al-
ternative proposals that been made for generaliz-
ing beyond translation examples directly observ-
able in the training data.
In language pairs that are typologically sim-
ilar (i.e., when both languages lexicalize the
same kinds of semantic and syntactic informa-
tion), words and phrases map relatively directly
from source to target languages, and the standard
approach to learning phrase pairs is quite effec-
tive.1 However, in language pairs in which in-
dividual source language words have many dif-
ferent possible translations (e.g., when the target
language word could have many different inflec-
tions or could be surrounded by different func-
tion words that have no direct correspondence in
the source language), we can expect the standard
phrasal inventory to be incomplete, except when
very large quantities of parallel data are available
or for very frequent words. There simply will not
be enough examples from which to learn the ideal
set of translation options. Therefore, since phrase
based translation can only generate input/output
word pairs that were directly observed in the train-
ing corpus, the decoder?s only hope for produc-
ing a good output is to find a fluent, meaning-
preserving translation using incomplete transla-
tion lexicons. Synthetic translation option genera-
tion seeks to fill these gaps using secondary gener-
ation processes that produce possible phrase trans-
lation alternatives that are not directly extractable
from the training data. We hypothesize that by
filling in gaps in the translation options, discrim-
inative translation models will be more effective
(leading to better translation quality).
The creation of synthetic translation options can
be understood as a kind of translation or post-
editing of phrasal units/translations. This raises
a question: if we have the ability to post-edit a
phrasal translation or retranslate a source phrase
so as to fill in gaps in the phrasal inventory, we
should be able to use the same technique to trans-
late the sentence; why not do this? While the ef-
fectiveness of this approach will ultimately be as-
sessed empirically, translation option generation is
appealing because the translation option synthe-
sizer need not produce only single-best guesses?
1When translating from a language with a richer lexical
inventory to a simpler one, approximate matching or backing
off to (e.g.) morphologically simpler forms likewise reliably
produces good translations.
??????
?
?????
saw +1SG +PST cat+ACC1SG+NOM
I
saw
saw a
saw the
cat
a
the cat
cat
saw the cat
saw a cat
I saw
I saw a
I saw the
Figure 1: Russian-English phrase-based transla-
tion example. Since Russian lacks a definiteness
morpheme the determiners a, the must be part of
a translation option containing ?????? or ?????
in order to be present in the right place in the En-
glish output. Translation options that are in dashed
boxes should exist but were not observed in the
training data. This work seeks to produce such
missing translation options synthetically.
if multiple possibilities appear to be equally good
(say, multiple inflections of a translated lemma),
then multiple translation options may be synthe-
sized. Ultimately, of course, the global translation
model must select one translation for every phrase
it uses, but the decoder will have access to global
information that it can use to pick better transla-
tion options.
3 Case Study: English Definite Articles
We now turn to a translation problem that we will
use to assess the value of synthetic translation op-
tions: generating English in/definite articles when
translating from Russian.
Definiteness is a semantic property of noun
phrases that expresses information such as iden-
tifiability, specificity, familiarity and unique-
ness (Lyons, 1999). In English, it is expressed
through the use of article determiners and non-
article determiners. Although languages may ex-
press definiteness through such morphemes, many
languages use alternative mechanisms. For exam-
ple they may use noncanonical word orders (Mo-
hanan, 1994)2 or different constructions such as
existentials, differential object marking (Aissen,
2003), and the ba (?) construction in Chinese
2See pp. 11?12 for an example in Hindi, a language with-
out articles.
272
(Chen, 2004). While these languages lack arti-
cles, they may use demonstratives and the quan-
tifier one to emphasize definiteness and indefinite-
ness, respectively.
Russian and Czech are examples of languages
that use non-lexical means to express definiteness.
As such, in Russian to English translation systems,
we expect that most Russian nouns should have at
least three translation options?the bare noun, the
noun preceded by the, and the noun preceded a/an.
Fig. 1 illustrates how the definiteness mismatch
between Russian and English can result in ?gaps?
in the phrasal inventory learned from a relatively
large parallel corpus. The Russian input should
translate (depending on context) as either I saw a
cat or I saw the cat; however, the phrase table we
learned is only able to generate the former.3
4 Predicting English Definite Articles
Although English articles express semantic con-
tent, their use is largely predictable in context,
both for native English speakers and for automated
systems (Knight and Chander, 1994).4 In this sec-
tion we describe a classifier that uses local contex-
tual features to predict whether an article belongs
in a particular position in a sequence of words, and
if so, whether it is definite or indefinite (the form
of the indefinite article is deterministic given the
pronunciation of the following word).
4.1 Model
The classifier takes an English word sequence w =
?w1, w2, . . . , w|w|?with missing articles and an in-
dex i and predicts whether no article, a definite ar-
ticle, or an indefinite article should appear before
wi. We parameterize the classifier as a multiclass
3The phrase table for this example was extracted from the
WMT 2013 shared task training data consisting of 1.2M sen-
tence pairs.
4An interesting contribution of this work is a discussion
on lower and upper bounds that can be achieved by native
English speakers in predicting determiners. 67% is a lower
bound, obtained by guessing the for every instance. The up-
per bound was obtained experimentally, and was measured on
noun phrases (NP) without context, in a context of 4 words
(2 before and 2 after NP), and given full context. Human
subjects achieved an accuracy of 94-96% given full context,
83-88% for NPs in a context of 4 words, and 79-80% for NPs
without context. Since in the current state-of-the-art building
an automated determiners prediction in a full context (repre-
senting meaning computationally) is not a feasible task, we
view 83-88% accuracy as our goal, and 88% as an upper
bound for our method.
logistic regression:
p(y | w, i) ? exp?
j
?jhj(y,w, i),
where hj(?) are feature functions, ?j are the corre-
sponding weights, and y ? {D,I,N} refer, respec-
tively, to the outputs: definite article, indefinite ar-
ticle, and no article.5
4.2 Features
The English article system is extremely com-
plex (as non-native English speakers will surely
know!): in addition to a general placement rule
that articles must precede a noun or its modifiers
in an NP, multiple other factors can also affect ar-
ticle selection, including countability of the head
noun, syntactic properties of an adjective modi-
fying a noun (superlative, ordinal), discourse fac-
tors, general knowledge, etc. In this section, we
define morphosyntactic features aimed at reflect-
ing basic grammatical rules, we define statistical,
semantic and shallow lexical features to capture
additional regular and idiosyncratic usages of def-
inite and indefinite articles in English. Below we
provide brief details of the features and their mo-
tivation.
Lexical. Because training data can be con-
structed inexpensively (from any unannotated En-
glish corpus), n-gram indicator features, such as
[[wi?1ywiwi+1 = with y lot of]], can be es-
timated reliably and capture construction-specific
article use.
Morphosyntactic. We used part-of-speech
(POS) tags produced by the Stanford POS tagger
(Toutanova and Manning, 2000) to capture gen-
eral article patterns. These are relevant features
in the prediction of articles as we observe certain
constraints regarding the use of articles in the
neighborhood of certain POS tags. For example,
we do not expect to predict an article following an
adjective (JJ).
Semantic. We extract further information indi-
cating whether a named entity, as identified by the
Stanford NE Recognizer (Finkel et al, 2005) be-
gins at wi. These features are relevant as there
5Realization of the classes D and N as lexical items is
straightforward. To convert I into a or an, we use the
CMU pronouncing dictionary (http://www.speech.
cs.cmu.edu/cgi-bin/cmudict) and select an if wi
starts with a phonetic vowel.
273
is, in general, a constraint on the co-occurrence
of articles with named entities which can help us
predict the use of articles in such constructions.
For example, proper nouns do not tend to co-
occur with articles in English. Although there are
some proper nouns that have an article included in
them, such as the Netherlands, the United States
of America, but these are fixed expressions and the
model is easily able to capture such cases with lex-
ical features.
Statistical. Statistical features capture probabil-
ity of co-occurrences of a sample with each of
the determiner classes, e.g., for wi?1ywi we
collect probabilities of wi?1Iwi, wi?1Dwi, and
wi?1Nwi.6
4.3 Training and evaluation
We employ the creg regression modeling frame-
work to train a ternary logistic regression classi-
fier.7 All features were computed for the target-
side of the Russian-English TED corpus (Cettolo
et al, 2012); from 117,527 sentences we removed
5K sentences used as tuning and test sets in the
MT system. We extract statistical features from
monolingual English corpora released for WMT-
11 (Callison-Burch et al, 2011).
In the training corpus there are 65,075 I in-
stances, 114,571 D instances, and 2,435,287 N in-
stances. To create a balanced training set we
randomly sample 65K instances from each set of
collected instances.8 This training set of feature
vectors has 142,604 features and 285,210 param-
eters. To minimize the number of free parame-
ters in our model we use `1 regularization. We
perform 10-fold cross validation experiments with
various feature combinations, evaluating the clas-
sifier accuracy for all classes and for each class
independently. The performance of the classifier
on individual classes and consolidated results for
all classes are listed in Table 1.
We observe that morphosyntactic and lexical
features are highly significant, reducing the er-
ror rate of statistical features by 25%. A combi-
6Although statistical features are de rigeur in NLP, they
are arguably justified for this problem on linguistic grounds
since human subjects use frequency-based in addition to their
grammatical knowledge. For example, we say He is at school
rather than He is at the school, but Americans say He is in
the hospital while UK English speakers might prefer He is in
hospital.
7https://github.com/redpony/creg
8Preliminary experiments indicated that the excess of N
labels resulted in poor performance.
Feature combination All I D N
Statistical 0.80 0.76 0.79 0.87
Lexical 0.82 0.79 0.80 0.87
Morphosyntactic 0.75 0.71 0.64 0.86
Semantic 0.35 0.99 0.02 0.04
Statistical+Lexical 0.85 0.83 0.82 0.89
+ Morphosyntactic 0.87 0.86 0.83 0.92
+ Semantic 0.87 0.86 0.83 0.92
Table 1: 10-fold cross validation accuracy of the
classifier over all and by class.
nation of morphosyntactic, lexical, and statistical
features is also helpful, reducing 13% more errors.
Semantic features do not contribute to the classi-
fier accuracy (we believe, mainly due to the feature
sparsity).
5 Experimental Setup
Our experimental workflow includes the follow-
ing steps. First, we select a phrase table PTsource
from which we generate synthetic phrases. For
each phrase pair ?f, e? in PTsource we generate n
synthetic variants of the target side phrase e which
we then append to PTbaseline. We annotate both
the original and synthetic phrases with additional
translation features in PTbaseline.
For this language pair, we have several options
for how to construct PTsource. The most straight-
forward way is to extract the phrasal inventory as
usual; a second option is to extract phrases from
training data from which definite articles have
been removed (since we will rely on the classifier
to reinsert them where they belong).
To synthesize phrases, we employ two differ-
ent techniques: LM-based and classifier-based.
We use a LM for one- or two-word phrases or
an auxiliary classifier for longer phrases and cre-
ate a new phrase in which we insert, remove or
substitute an article between each adjacent pair of
words in the original phrase. Such distinction be-
tween short and longer phrases has clear motiva-
tion: phrases without context may allow alterna-
tive, equally plausible options for article selection,
therefore we can just rely on a LM, trained on
large monolingual corpora, to identify phrases un-
observed in MT training corpus. Longer context
restricts determiners usage and statistical model
decisions are less prone to generating ungrammat-
ical synthetic phrases.
LM-based method is applied to phrases shorter
than three words. These phrases are numerous,
roughly 20% of a phrase table, and extracted from
274
many sites in the training data. For each short (tar-
get) phrase we add all possible alternative entries
observed in the LM and not observed in the orig-
inal translation model. For example, for a short
target phrase a cat we extract the cat.
We apply an auxiliary classifier to longer
phrases, containing three or more words. Based
on the classifier prediction, we use the maximally
probable class to insert, remove or substitute an
article between each adjacent pair of words in
the original phrase. Synthetic phrases are gener-
ated by linguistically-informed features and can
introduce alternative grammatically-correct trans-
lations of source phrases by adding or removing
existing articles (since the English article selection
in a local context is often ambiguous and not cat-
egorical). We add a synthetic phrase only if the
phrase pair not observed in the original model.
We compare two possible applications of a clas-
sifier: one-pass and iterative prediction. With
one-pass prediction we decide on the prediction
for each position independently of other deci-
sions. With iterative update we adopt the best
first (greedy) strategy, selecting in each iteration
the update-location in which the classifier obtains
highest confidence score. In each iteration we in-
corporate a prediction in a target phrase, and in the
next iteration the best first decision is made on an
updated phrase. Iterative prediction stops when no
updates are introduced.
Synthetic phrases are added to a phrase table
with the five standard phrasal translation features
that were found in the source phrase, and with sev-
eral new features. First, we add a boolean fea-
ture indicating the origin of a phrase: synthetic or
original. Second, we experiment with a posterior
probability of a classifier averaged over all loca-
tions where it could be extracted from the training
data. The next feature is derived from this score:
it is a boolean feature indicating a confidence of
the classifier: the feature value is 1 iff the average
classifier score is higher than some threshold.
Consider again a phrase I saw a cat discussed
in Section 1. Synthetic entry generation from the
original phrase table entry is illustrated in Fig-
ure 2.
6 Translation Results
We now review the results of experiments using
synthetic translation options in a machine trans-
lation system. We use the Moses toolkit (Koehn
et al, 2007) to train a baseline phrase-based SMT
system. Each configuration we compare has a dif-
ferent phrase table, with synthetic phrases gen-
erated with best-first or iterative strategies, from
a phrase table with- or without-determiners, with
variable number of translation features. To verify
that system improvement is consistent, and is not a
result of optimizer instability (Clark et al, 2011),
we replicate each experimental setup three times,
and then estimate the translation quality of the me-
dian MT system using the MultEval toolkit.9
The corpus is the same as in Section 4.3:
the training part contains 112,527 sentences from
Russian-English TED corpus, randomly sampled
3K sentences are used for tuning and a disjoint set
of 2K sentences is used for test. We lowercase
both sides, and use Stanford CoreNLP10 tools to
tokenize the corpora. We employ SRILM toolkit
(Stolcke, 2002) to linearly interpolate the target
side of the training corpus with the WMT En-
glish corpus, optimizing towards the MT tuning
set. This LM is used in all experiments.
The rest of this section is organized as follows.
First, we compare two approaches to the deter-
miners classifier application. Then, we provide
detailed description of experiments with synthetic
phrases. We evaluate various aspects of synthetic
phrases generation and summarize all the results
in Table 3. In Table 5 we show examples of im-
proved translations.
Classifier application: one-pass vs. iterative.
First, as an intrinsic evaluation of the prediction
strategy we remove definite and indefinite articles
from the reference translations (2K test sentences)
and then employ the determiners classifier to re-
produce the original sentences. In Table 2 we re-
port on the word error rate (WER) derived from
the Levenshtein distance between the original sen-
tences and the sentences (1) without articles, (2)
with articles recovered using one-pass prediction,
and (3) articles recovered using iterative predic-
tion. The WER is averaged over all test sentences.
Both one-pass and iterative approaches are effec-
tive in the task of determiners prediction, reducing
the number of errors by 44%. The iterative ap-
proach yields slightly lower WER, hence we em-
ploy the iterative prediction in the future experi-
ments with synthetic phrases.
9https://github.com/jhclark/multeval
10http://nlp.stanford.edu/software/corenlp.shtml
275
the
? ?????? ????? ||| i saw the cat ||| f0 f1 f2 f3 f4 exp(1) exp(0) |||
<s>      I      saw   a   cat </s>
None
None
? ?????? ????? ||| i saw a cat ||| f0 f1 f2 f3 f4 exp(0) exp(0) |||
original phrase
post-processing
synthetic phrase
is synthetic
is no-context
Figure 2: Synthetic entry generation example. The original parallel phrase has two additional boolean
features (set to false) indicating that this is not a synthetic phrase and not a short phrase. We apply
our determiners classifier to predict an article at each location marked with a dashed box. Based on a
classifier prediction we derive a new phrase I saw the cat. Since corresponding parallel entry is not in
the original phrase table, we set the synthetic indicator feature to 1.
Post-processing WER
None 5.6%
One-pass 3.2%
Iterative 3.1%
Table 2: WER (lower is better) of reference trans-
lations without articles and of post-processed ref-
erence translations. Both one-pass and iterative
approaches are effective in the task of determin-
ers prediction.
MT output post-processing. We then evaluate
the post-processing strategy directly on the MT
output. We experiment with one-pass and itera-
tive post-processing of two variants of the base-
line system outputs: original output and the out-
put without articles (we remove the articles prior
to post-processing). The results are listed in Ta-
ble 3. Interestingly, we do not obtain any improve-
ments applying the determiners classifier in a con-
ventional way of a MT output post-processing. It
is the combination of linguistically-motivated fea-
tures with synthetic phrases that contribute to the
best performance.
LM-based synthetic phrases. As discussed
above, LM-based (short) phrases are shorter than
3 tokens and their synthetic variants contain same
words with articles inserted or deleted between
each adjacent pair of words. The phrase table
of the baseline system contains 2,441,678 phrase
pairs. There are 518,453 original short phrases,
and our technique yields 842,252 new synthetic
entries which we append to the baseline phrase ta-
ble. Table 3 shows the evaluation of the median
SMT system (derived from three systems) with
short phrases. In these systems the five phrasal
translation features are the same as in the base-
line systems. Improvement in the BLEU score
(Papineni et al, 2002) is statistically significant
(p < .05), compared to the baseline system
Classifier-generated synthetic phrases We ap-
ply classifier with the iterative prediction directly
on the baseline phrase table entries and synthe-
size 944,145 new parallel phrases, increasing the
phrase table size by 38%. The phrasal transla-
tion features in each synthetic phrase are the same
as in the phrase it was derived from. The BLEU
score of the median SMT system with synthetic
phrases is 22.9 ? .1, the improvement is statisti-
cally significant (p < .01). Post-processing of a
phrase table created from corpora without articles
and adding synthetic phrases to the baseline phrase
table yielded similar results.
Translation features for synthetic phrases In
the following experiments we aim to establish the
optimal set of translation features that should be
used with synthetic phrases. We train several SMT
systems, each containing synthetic phrases derived
from the original phrase table by iterative classifi-
cation, and with LM-based short phrases. Each
synthetic phrase has five translation features as an
original phrase it was derived from. The additional
features that we evaluate are:
1. Boolean feature for LM-based synthetic
phrases
276
MT System BLEU
Baseline 22.6? .1
MT output post-processing
one-pass, MT output with articles 20.8
one-pass, MT output without articles 19.7
iterative, MT output with articles 22.6
iterative, MT output without articles 21.8
With synthetic phrases
LM-based phrases 22.9? .1
+ classifier-generated phrases 22.9? .1
+ features 1,2 23.0 ? .1
+ features 1,2,3 22.8? .1
+ features 1,2,3,4 22.8? .1
+ feature 5 22.9? .1
Table 3: Summary of experiments with MT out-
put post-processing and with synthetic translation
options in a phrase table. Post-processing of the
MT output do not improve translations. Best per-
forming system with synthetic phrases has five
original phrase translation features and two addi-
tional boolean features indicating if the phrase is
LM-based or not, is classifier-generated or not. All
the synthetic systems are significantly better than
the baseline system.
2. Boolean feature for classifier-generated syn-
thetic phrases
3. Classifier confidence: posterior probability of
the classifier averaged over all samples in a tar-
get phrase.
4. Boolean feature indicating a confidence of the
classifier: the feature value is 1 iff the Fea-
ture 3 scores higher than some threshold. The
threshold was set to 0.8, we did not experiment
with other values.
5. Boolean feature for a synthetic phrase of any
type: LM-based or classifier-generated
Table 3 details the change in the BLEU score
of each experimental setup. The best perform-
ing system has five original phrase translation fea-
tures and two additional boolean features indicat-
ing if the phrase is LM-based or not, is classifier-
generated or not. Note that all the synthetic sys-
tems are significantly better than the baseline.
Czech-English. Our technique was developed
using Russian-English system in the TED domain,
so we want to see how our method generalizes to a
different domain when translating from a different
language. We therefore applied our most success-
ful configuration to a Czech-English news transla-
tion task.11 For training, we use the WMT Czech-
English parallel corpus CzEng0.7; we tune using
the WMT2011 test set and test on the WMT2012
test set. The LM is trained on the target side of the
training corpus. Determiners classifier, re-trained
on the English side of this corpus, with statistical,
lexical, morphosyntactic and dependency features
obtained an accuracy of 88%.
In Table 4, we report the results of evaluat-
ing the performance of the Russian-to-English
and Czech-to-English MT systems with synthetic
phrases. The results of both systems show a statis-
tically significant (p < .01) improvement in terms
of BLEU score.
Russian Czech
Baseline 22.6? .1 16.0? .05
Synthetic 23.0? .1 16.2? .03
Table 4: BLEU score of Russian-to-English
and Czech-to-English MT systems with synthetic
phrases and features 1 and 2 show a significant im-
provement.
Qualitative analysis. Table 5 shows some ex-
amples from the output of our Russian-to-English
systems. Although both systems produce compre-
hensible translations, the system augmented with
determiner classifier is more fluent. The first ex-
ample represents a case where a singular count
noun (piece) is present which requires an article.
The baseline is not able to identify this require-
ment and hence does not insert the article an be-
fore the phrase extraordinary engineering piece.
Our system, however, correctly identifies the con-
struction requiring an article and thus provides an
appropriate form of the article (an- Indefinite arti-
cle for lexical items beginning with a vowel). Thus
we see that our system is able to capture the lin-
guistic requirement of the singular count nouns to
co-occur with an article. In the second row, the
lexical item poor is used as an adjective. The base-
line has inserted an article in front of it, chang-
ing it to a noun. Our system, however, is able to
maintain the status of poor as an adjective since
it has the option not to insert an article. Thus we
see that besides fluency, our system also does bet-
ter in maintaining the grammatical category of a
lexical item. In the third row, the phrase three
11Like Russian, Czech is a Slavic language that does not
have definite or indefinite articles.
277
Source: ?? ??? ?? ????? , ??? ?????????? ???????????? ??????????? ????????? .
Reference: but nonetheless , it ?s an extraordinary piece of engineering .
Baseline: but nevertheless , it ?s extraordinary engineering piece of art .
Ours: but nevertheless , it ?s an extraordinary piece of engineering art .
Source: ? ?? ?????? ?????????? ??? ??? ?? ?????? .
Reference: and by many definitions she is no longer poor .
Baseline: and in a lot definitions , it ?s not a poor .
Ours: and in a lot definitions she ?s not poor .
Source: ??? ????? ????????? ??? ????????? ????????? ??????? .
Reference: we must feed three billion people in cities .
Baseline: we need to feed the three billion urban hundreds of them .
Ours: we need to feed three billion people in the city .
Table 5: Examples of translations with improved articles handling.
billion people refers to a nonidentifiable referent.
The baseline inserts the definite article the. If a
human subject reads this translation, it would mis-
lead him/her to interpret the object three billion
people as referring to a specific identifiable set.
Our system, on the other hand, correctly selects
the determiner class N and hence does not insert an
article. Thus we see that our system does not just
add fluency but it also captures a semantic distinc-
tion, namely identifiability, that a human subject
makes when producing or interpreting a phrase.
7 Related Work
Automated determiner prediction has been found
beneficial in a variety of applications, including
postediting of MT output (Knight and Chander,
1994), text generation (Elhadad, 1993; Minnen
et al, 2000), and more recently identification and
correction of ESL errors (Han et al, 2006; De Fe-
lice and Pulman, 2008; Gamon et al, 2009; Ro-
zovskaya and Roth, 2010). Our work on determin-
ers extends previous studies in several dimensions.
While all previous approaches were tested only on
NP constructions, we evaluate our classifier on any
sequence of tokens.
To the best of our knowledge, the only stud-
ies that directly address generation of synthetic
phrase table entries was conducted by Chen et al
(2011) and Koehn and Hoang (2007). The former
find semantically similar source phrases and pro-
duce ?fabricated? translations by combining these
source phrases with a set of their target phrases;
however, they do not observe improvements. The
later work integrates the synthesis of translation
options into the decoder. While related in spirit,
their method only supports a limited set of gen-
erative processes for producing the candidate set
(lacking, for instance, the simple and effective
phrase post-editing process we have used), and
their implementation has been plagued by compu-
tational challenges.
Post-processing techniques have been ex-
tremely popular. These can be understood as using
a translation model to generate a translation skele-
ton (or k-best skeletons) and then post-editing
these in various ways. These have been applied
to translation into morphologically rich languages,
such as Japanese, German, Turkish, and Finnish
(de Gispert et al, 2005; Suzuki and Toutanova,
2006; Suzuki and Toutanova, 2007; Fraser et al,
2012; Clifton and Sarkar, 2011; Oflazer and Dur-
gar El-Kahlout, 2007).
8 Conclusions and future work
The contribution of this work is twofold. First, we
propose a new supervised method to predict defi-
nite and indefinite articles. Our log-linear model
trained on a linguistically-motivated set of fea-
tures outperforms previously reported results, and
obtains an upper bound of an accuracy achieved
by human subjects given a context of four words.
However, more important result of this work is the
experimentally verified idea of improving phrase-
based SMT via synthetic phrases. While we have
focused on a limited problem in this paper, there
are numerous alternative applications including
translation into morphologically rich languages, as
a method for incorporating (source) contextual in-
formation in making local translation decisions,
enriching the target language lexicon using lexical
translation resources, and many others.
Acknowledgments
We are grateful to Shuly Wintner for insightful sugges-
tions and support. This work was supported in part by the
U. S. Army Research Laboratory and the U. S. Army Re-
search Office under contract/grant number W911NF-10-1-
0533.
278
References
J. Aissen. 2003. Differential object marking: Iconic-
ity vs. economy. Natural Language and Linguistic
Theory, 21(3):435?483.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statisti-
cal machine translation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
22?64, Edinburgh, Scotland, July. Association for
Computational Linguistics.
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
Web inventory of transcribed and translated talks. In
Proceedings of the 16th Conference of the European
Association for Machine Translation (EAMT), pages
261?268, Trento, Italy, May.
B. Chen, R. Kuhn, and G. Foster. 2011. Semantic
smoothing and fabrication of phrase pairs for SMT.
In Proceedings of the International Workshop on
Spoken Lanuage Translation (IWSLT-2011).
P. Chen. 2004. Identifiability and definiteness in chi-
nese. Linguistics, 42(6):1129?1184.
C. Cherry and G. Foster. 2012. Batch tuning strategies
for statistical machine translation. In Proceedings of
HLT-NAACL 2012, volume 12, pages 34?35.
D. Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
D. Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. The Jour-
nal of Machine Learning Research, 98888:1159?
1187.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011. Better hypothesis testing for statistical ma-
chine translation: Controlling for optimizer instabil-
ity. In In Proc. of ACL.
A. Clifton and A. Sarkar. 2011. Combining
morpheme-based machine translation with post-
processing morpheme prediction. In Proceedings of
ACL.
R. De Felice and S. G. Pulman. 2008. A classifier-
based approach to preposition and determiner error
correction in L2 English. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 169?176. Association
for Computational Linguistics.
A. de Gispert, J. B. Marin?o, and J. M. Crego. 2005.
Improving statistical machine translation by classi-
fying and generalizing inflected verb forms. In Pro-
ceedings of InterSpeech.
J. DeNero and D. Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1453?
1463. Association for Computational Linguistics.
J. DeNero, A. Bouchard-Co?te?, and D. Klein. 2008.
Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 314?323. Association for Com-
putational Linguistics.
V. Eidelman, Y. Marton, and P. Resnik. 2013. Online
relative margin maximization for statistical machine
translation. In Proceedings of ACL.
M. Elhadad. 1993. Generating argumentative judg-
ment determiners. In AAAI, pages 344?349.
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In ACL ?05:
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 363?
370, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
A. Fraser, M. Weller, A. Cahill, and F. Cap. 2012.
Modeling inflection and word-formation in SMT. In
Proceedings of EACL.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable infer-
ence and training of context-rich syntactic transla-
tion models. In ACL-44: Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Associa-
tion for Computational Linguistics, pages 961?968,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. B.
Dolan, D. Belenko, and L. Vanderwende. 2009. Us-
ing contextual speller techniques and language mod-
eling for ESL error correction. Urbana, 51:61801.
K. Gimpel and N. A. Smith. 2012. Structured ramp
loss minimization for machine translation. In Pro-
ceedings of 2012 Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: Human Language Technologies HLT-
NAACL 2012, Montreal, Canada.
N.-R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers.
K. Knight and I. Chander. 1994. Automated poste-
diting of documents. In Proceedings of the Na-
tional Conference on Artificial Intelligence, pages
779?779, Seattle, WA.
P. Koehn and H. Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
279
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54. Association for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
C. Lyons. 1999. Definiteness. Cambridge University
Press.
G. Minnen, F. Bond, and A. Copestake. 2000.
Memory-based learning for article generation. In
Proceedings of the 2nd workshop on Learning lan-
guage in logic and the 4th conference on Compu-
tational natural language learning-Volume 7, pages
43?48. Association for Computational Linguistics.
T. Mohanan. 1994. Argument Structure in Hindi.
CSLI Publications.
K. Oflazer and I. Durgar El-Kahlout. 2007. Explor-
ing different representational units in English-to-
Turkish statistical machine translation. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 25?32, Prague, Czech Republic,
June. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In ACL ?02: Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318, Morristown, NJ,
USA. Association for Computational Linguistics.
A. Rozovskaya and D. Roth. 2010. Training
paradigms for correcting errors in grammar and us-
age. Urbana, 51:61801.
A. Stolcke. 2002. SRILM?an extensible language
modeling toolkit. In Procedings of International
Conference on Spoken Language Processing, pages
901?904.
H. Suzuki and K. Toutanova. 2006. Learning to pre-
dict case markers in Japanese. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 1049?
1056. Association for Computational Linguistics.
H. Suzuki and K. Toutanova. 2007. Generating case
markers in machine translation. In Proceedings of
HLT-NAACL 2007, pages 49?56.
K. Toutanova and C. D. Manning. 2000. Enriching
the knowledge sources used in a maximum entropy
part-of-speech tagger. In Proceedings of the 2000
Joint SIGDAT conference on Empirical methods in
natural language processing and very large corpora,
pages 63?70, Morristown, NJ, USA. Association for
Computational Linguistics.
280
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 51?60,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
A Framework for (Under)specifying Dependency Syntax
without Overloading Annotators
Nathan Schneider?? Brendan O?Connor? Naomi Saphra? David Bamman?
Manaal Faruqui? Noah A. Smith? Chris Dyer? Jason Baldridge?
?School of Computer Science, Carnegie Mellon University
?Department of Linguistics, The University of Texas at Austin
Abstract
We introduce a framework for lightweight
dependency syntax annotation. Our for-
malism builds upon the typical represen-
tation for unlabeled dependencies, per-
mitting a simple notation and annotation
workflow. Moreover, the formalism en-
courages annotators to underspecify parts
of the syntax if doing so would streamline
the annotation process. We demonstrate
the efficacy of this annotation on three lan-
guages and develop algorithms to evaluate
and compare underspecified annotations.
1 Introduction
Computational representations for natural lan-
guage syntax are borne of competing design con-
siderations. When designing such representations,
there may be a tradeoff between parsimony and
expressiveness. A range of linguistic theories at-
tract support due to differing purposes and aes-
thetic principles (Chomsky, 1957; Tesni?re, 1959;
Hudson, 1984; Sgall et al, 1986; Mel?c?uk, 1988,
inter alia). Formalisms concerned with tractable
computation may care chiefly about learnabil-
ity or parsing efficiency (Shieber, 1992; Sleator
and Temperly, 1993; Kuhlmann and Nivre, 2006).
Further considerations may include psychologi-
cal and evolutionary plausibility (Croft, 2001;
Tomasello, 2003; Steels et al, 2011; Fossum and
Levy, 2012), integration with other representa-
tions such as semantics (Steedman, 2000; Bergen
and Chang, 2005), or suitability for particular ap-
plications (e.g., translation).
Here we elevate ease of annotation as a pri-
mary design concern for a syntactic annotation
formalism. Currently, a lack of annotated data
is a huge bottleneck for robust NLP, standing in
the way of parsers for social media text (Foster
et al, 2011) and many low-resourced languages
(to name two examples). Traditional syntactic an-
notation projects like the Penn Treebank (Marcus
?Corresponding author: nschneid@cs.cmu.edu
et al, 1993) or Prague Dependency Treebank (Ha-
jic?, 1998) require highly trained annotators and
huge amounts of effort. Lowering the cost of an-
notation, by making it easier and more accessi-
ble, could greatly facilitate robust NLP in new lan-
guages and genres.
To that end, we design and test new, lightweight
methodologies for syntactic annotation. We pro-
pose a formalism, Fragmentary Unlabeled De-
pendency Grammar (FUDG) for unlabeled de-
pendency syntax that addresses some of the most
glaring deficiencies of basic unlabeled dependen-
cies (?2), with little added burden on annotators.
FUDG requires minimal theoretical commitments,
and can be supplemented with a project-specific
style guide (we provide a brief one for English).
We contribute a simple ASCII markup language?
Graph Fragment Language (GFL; ?3)?that al-
lows annotations to be authored using any text ed-
itor, along with tools for validating, normalizing,
and visualizing GFL annotations.1
An important characteristic of our framework is
annotator flexibility. The formalism supports this
by allowing underspecification of structural por-
tions that are unclear or unnecessary for the pur-
poses of a project. Fully leveraging this power re-
quires new algorithms for evaluation, e.g., of inter-
annotator agreement, where annotations are par-
tial; such algorithms are presented in ?4.2
Finally, small-scale case studies (?5) apply our
framework (formalism, notation, and evaluations)
to syntactically annotate web text in English, news
in Malagasy, and dialogues in Kinyarwanda.
2 A Dependency Grammar for
Annotation
Although dependency-based approaches to syntax
play a major role in computational linguistics, the
nature of dependency representations is far from
uniform. Exemplifying one end of the spectrum
is the Prague Dependency Treebank, which articu-
lates an elaborate dependency-based syntactic the-
1https://github.com/brendano/gfl_syntax/
2Parsing algorithms are left for future work.
51
Found the scarriest mystery door in my school . I?M SO CURIOUS D:
Found** < (the scarriest mystery door*)
Found < in < (my > school)
I?M** < (SO > CURIOUS)
D:**
my = I?M
thers still like 1 1/2 hours till Biebs bday here :P
thers** < still
thers < ((1 1/2) > hours < till < (Biebs > bday))
(thers like 1 1/2 hours)
thers < here
:P**
Figure 1: Two tweets with example GFL annotations. (The formalism and notation are described in ?3.)
ory in a rich, multi-tiered formalism (Hajic?, 1998;
B?hmov? et al, 2003). On the opposite end of
the spectrum are the structures used in dependency
parsing research which organize all the tokens of
a sentence into a tree, sometimes with category la-
bels on the edges (K?bler et al, 2009). Insofar as
they reflect a theory of syntax, these vanilla de-
pendency grammars provide a highly reduction-
ist view of structure?indeed, parses used to train
and evaluate dependency parses are often simpli-
fications of Prague-style parses, or else converted
from constituent treebanks.
In addition to the binary dependency links of
vanilla dependency representations, we offer three
devices to capture certain linguistic phenomena
more straightforwardly:3
1. We make explicit the meaningful lexical units
over which syntactic structure is represented. Our
approach (a) allows punctuation and other extrane-
ous tokens to be excluded so as not to distract from
the essential structure; and (b) permits tokens to be
grouped into shallow multiword lexical units.4
2. Coordination is problematic to represent with
unlabeled dependencies due to its non-binary na-
ture. A coordinating conjunction typically joins
multiple expressions (conjuncts) with equal sta-
tus, and other expressions may relate to the com-
pound structure as a unit. There are several differ-
ent conventions for forcing coordinate structures
into a head-modifier straightjacket (Nivre, 2005;
de Marneffe and Manning, 2008; Marec?ek et al,
2013). Conjuncts, coordinators, and shared de-
pendents can be distinguished with edge labels;
we equivalently use a special notation, permitting
the coordinate structure to be automatically trans-
formed with any of the existing conventions.5
3Some of this is inspired by the conventions of Reed-
Kellogg sentence diagramming, a graphical dependency an-
notation system for English pedagogy (Reed and Kellogg,
1877; Kolln and Funk, 1994; Florey, 2006).
4The Stanford representation supports a limited notion of
multiword expressions (de Marneffe and Manning, 2008).
For simplicity, our formalism treats multiwords as unana-
lyzed (syntactically opaque) wholes, though some multiword
expressions may have syntactic descriptions (Baldwin and
Kim, 2010).
5Tesni?re (1959) and Hudson (1984) similarly use
special structures for coordination (Schneider, 1998;
3. Following Tesni?re (1959), our formalism
offers a simple facility to express anaphora-
antecedent relations (a subset of semantic relation-
ships) that are salient in particular syntactic phe-
nomena such as relative clauses, appositives, and
wh-expressions.
Underspecification. Our desire to facilitate
lightweight annotation scenarios requires us to
abandon the expectation that syntactic informants
provide a complete parse for every sentence. On
one hand, an annotator may be uncertain about the
appropriate parse due to lack of expertise, insuf-
ficiently mature annotation conventions, or actual
ambiguity in the sentence. On the other hand, an-
notators may be indifferent to certain phenomena.
This can happen for a variety of reasons:
? Some projects may only need annotations of
specific constructions. For example, building a
semantic resource for events may require anno-
tation of syntactic verb-argument relations, but
not internal noun phrase structure.
? As a project matures, it may be more useful to
annotate only infrequent lexical items.
? Semisupervised learning from partial annota-
tions may be sufficient to learn complete parsers
(Hwa, 1999; Clark and Curran, 2006).
? Beginning annotators may wish to focus on eas-
ily understood syntactic phenomena.
? Different members of a project may wish to spe-
cialize in different syntactic phenomena, reduc-
ing training cost and cognitive load.
Rather than treating annotations as invalid unless
and until they are complete trees, we formally rep-
resent and reason about partial parse structures.
Annotators produce annotations, which encode
constraints on the (inferred) analysis, the parse
structure, of a sentence. We say that a valid anno-
tation supports (is compatible with) one or more
analyses. Both annotations and analyses are rep-
resented as graphs (the graph representation is de-
scribed below in ?3.2). We require that the di-
rected edges in an analysis graph must form a tree
over all the lexical items in the sentence.6 Less
Sangati and Mazza, 2009).
6While some linguistic phenomena (e.g., relative clauses,
control constructions) can be represented using non-tree
52
stringent well-formedness constraints on the an-
notation graph leave room for underspecification.
Briefly, an annotation can be underspecified in
two ways: (a) an expression may not be attached to
any parent, indicating it might depend on any non-
descendant in a full analysis?this is useful for an-
notating sentences piece by piece; and (b) multiple
expressions may be grouped together in a fudge
expression (?3.3), a constraint that the elements
form a connected subgraph in the full analysis
while leaving the precise nature of that subgraph
indeterminate?this is useful for marking relation-
ships between chunks (possibly constituents).
A formalism, not a theory. Our framework for
dependency grammar annotation is a syntactic
formalism, but it is not sufficiently comprehen-
sive to constitute a theory of syntax. Though
it standardizes the basic treatment of a few ba-
sic phenomena, simplicity of the formalism re-
quires us to be conservative about making such
extensions. Therefore, just as with simpler for-
malisms, language- and project-specific conven-
tions will have to be developed for specific linguis-
tic phenomena. By embracing underspecified an-
notation, however, our formalism aims to encour-
age efficient corpus coverage in a nascent anno-
tation project, without forcing annotators to make
premature decisions.
3 Syntactic Formalism and GFL
In our framework, a syntactic annotation of a sen-
tence follows an extended dependency formalism
based on the desiderata enumerated in the previ-
ous section. We call our formalism Fragmentary
Unlabeled Dependency Grammar (FUDG).
To make it simple to create FUDG annotations
with a text editor, we provide a plain-text de-
pendency notation called Graph Fragment Lan-
guage (GFL). Fragments of the FUDG graph?
nodes and dependencies linking them?are en-
coded in this language; taken together, these frag-
ments describe the annotation in its entirety. The
ordering of GFL fragments, and of tokens within
each fragment, is of no formal consequence. Since
the underlying FUDG representation is transpar-
ently related to GFL constructions, GFL notation
will be introduced alongside the discussion of each
kind of FUDG node.7
structures, we find that being able to alert annotators when
they inadvertently violate the tree constraint is more useful
than the expressive flexibility.
7In principle, FUDG annotations could be created with
3.1 Tokens
We expect a tokenized string, such as a sentence
or short message. The provided tokenization is re-
spected in the annotation. For human readability,
GFL fragments refer to tokens as strings (rather
than offsets), so all tokens that participate in an
annotation must be unambiguous in the input.8 A
token may be referenced multiple times in the an-
notation.
3.2 Graph Encoding
Directed arcs. As in other dependency
formalisms, dependency arcs are directed
links indicating the syntactic headedness
relationship between pairs of nodes. In
GFL, directed arcs are indicated with an-
gle brackets pointing from the dependent to
its head, as in black > cat or (equivalently)
cat < black. Multiple arcs can be chained to-
gether: the > cat < black < jet describes three
arcs. Parentheses help group portions of a chain:
(the > cat < black < jet) > likes < fish (the
structure black < jet > likes, in which jet
appears to have two heads, is disallowed). Note
that another encoding for this structure would be
to place the contents of the parentheses and the
chain cat > likes < fish on separate lines. Curly
braces can be used to list multiple dependents of
the same head: {cat fish} > likes.
Anaphoric links. These undirected links join
coreferent anaphora to each other and to their an-
tecedent(s). In English this includes personal pro-
nouns, relative pronouns (who, which, that), and
anaphoric do and so (Leo loves Ulla and so does
Max). This introduces a bit of semantics into our
annotation, though at present we do not attempt to
mark non-anaphoric coreference. It also allows a
more satisfying treatment of appositives and rel-
ative clauses than would be possible from just the
directed tree (the third example in figures 2 and 3).
Lexical nodes. Whereas in vanilla dependency
grammar syntactic links are between pairs of to-
ken nodes, FUDG abstracts away from the indi-
vidual tokens in the input. The lowest level of a
FUDG annotation consists of lexical nodes, i.e.,
an alternative mechanism such as a GUI, as in Hajic? et al
(2001).
8If a word is repeated within the sentence, it must be in-
dexed in the input string in order to be referred to from a
fragment. In our notation, successive instances of the same
word are suffixed with ~1, ~2, ~3, etc. Punctuation and other
tokens omitted from an annotation do not need to be indexed.
53
'll
If
's
I wake_up
restin' it~1
it~2
weapons
Our three
are
$a
fear surprise efficiency
ruthless
and~1 and~2
are
We knights
the
who
say
Ni
Figure 2: FUDG graphs corresponding to the examples in figure 3. The two special kinds of directed edges are for attaching
conjuncts (bolded) and their coordinators (dotted) in a coordinate structure. Anaphoric links are undirected. The root node of
each sentence is omitted.
If it~1 's restin' I 'll wake it~2 up .
If < (it~1 > 's < restin')
I > 'll < [wake up] < it~2
If > 'll**
it~1 = it~2
Our three weapons are fear and~1 surprise and~2
ruthless efficiency ...
{Our three} > weapons > are < $a
$a :: {fear surprise efficiency} :: {and~1 and~2}
ruthless > efficiency
We are the knights who say ... Ni !
We > are < knights < the
knights < (who > say < Ni)
who = knights
Figure 3: GFL for the FUDG graphs in figure 2.
lexical item occurrences. Every token node maps
to 0 or 1 lexical nodes (punctuation, for instance,
can be ignored).
A multiword is a lexical node incorporating
more than one input token and is atomic (does
not contain internal structure). A multiword node
may group any subset of input tokens; this allows
for multiword expressions which are not neces-
sarily contiguous in the sentence (e.g., the verb-
particle construction make up in make the story
up). GFL notates multiwords with square brack-
ets, e.g., [break a leg].
Coordination nodes. Coordinate structures re-
quire at least two kinds of dependents: co-
ordinators (i.e., lexical nodes for coordinat-
ing conjunctions?at least one per coordina-
tion node) and conjuncts (heads of the con-
joined subgraphs?at least one per coordination
node). The GFL annotation has three parts:
a variable representing the node, a set of con-
juncts, and a set of coordinator nodes. For in-
stance, $a :: {[peanut butter] honey} :: {and}
(peanut butter and honey) can be embedded
within a phrase via the coordination node
variable $a; a [fresh [[peanut butter] and
honey] sandwich] snack would be formed with
{fresh $a} > sandwich > snack < a. A graphical
example of coordination can be seen in figure 2?
note the bolded conjunct edges and the dotted co-
ordinator edges. If the conjoined phrase as a whole
takes modifiers, these are attached to the coordina-
tion node with regular directed arcs. For example,
in Sam really adores kittens and abhors puppies.,
the shared subject Sam and adverb really attach to
the entire conjoined phrase. In GFL:
$a :: {adores abhors} :: {and}
Sam > $a < really
adores < kittens abhors < puppies
Root node. This is a special top-level node used
to indicate that a graph fragment constitutes a stan-
dalone utterance or a discourse connective. For an
input with multiple utterances, the head of each
should be designated with ** to indicate that it at-
taches to the root.
3.3 Means of Underspecification
As discussed in ?2, our framework distinguishes
annotations from full syntactic analyses. With re-
spect to dependency structure (directed edges), the
former may underspecify the latter, allowing the
annotator to commit only to a partial analysis.
For an annotationA, we define support(A) to be
the set of full analyses compatible with that anno-
tation. A full analysis is required to be a directed
rooted tree over all lexical nodes in the annotation.
An annotation is valid if its support is non-empty.
The 2 mechanisms for dependency underspeci-
fication are unattached nodes and fudge nodes.
Unattached nodes. For any node in an annota-
tion, the annotator is free to simply leave it not
attached to any head. This is interpreted as al-
lowing its head to be any other node (including
the root node), subject to the tree constraint. We
call a node?s possible heads its supported par-
ents. Formally, for an unattached node v in an-
notation A, suppParentsA(v) = nodes(A) \ ({v} ?
descendants(v)).
Fudge nodes. Sometimes, however, it is desir-
able to represent a sort of skeletal structure with-
out filling in all the details. A fudge expres-
sion (FE) asserts that a group of nodes (the ex-
pression?s members) belong together in a con-
nected subgraph, while leaving the internal struc-
ture of that subgraph unspecified.9 The notation
9This underspecification semantics is, to the best of our
knowledge, novel, though it has been proposed that con-
nected dependency subgraphs (known as catenae) are of the-
oretical importance in syntax (Osborne et al, 2012).
54
FN2
a
b
f
FN1
c d e
f
b
f
b
b c
b
b a
a
d
a
c a
d
a
d
c db e fe c e fe
a
d d
cf
e e f
c
Figure 4: Left: An annotation graph with 2 fudge nodes and 6 lexical nodes; it can be encoded with GFL fragments
((a b)* c d) < e and b < f. Right: All of its supported analyses: prom(A) = 6. com(A) = 1 ?
log 6
log 75
= .816.
for this is a list of two or more nodes within
parentheses: an annotation for Few if any witches
are friends with Maria. might contain the FE
(Few if any) so as to be compatible with the
structures Few < if < any, Few > if > any, etc.?
but not, for instance, Few > witches < any. In
the FUDG graph, this is represented with a fudge
node to which members are attached by special
member arcs. Fudge nodes may be linked to other
nodes: the GFL fragment (Few if any) > witches
is compatible with (Few < if < any) > witches,
(Few < (if > any)) > witches, and so forth.
Properties. Let f be a fudge expression. From
the connected subgraph definition and the tree
constraint on analyses, it follows that:
? Exactly 1 member of f must, in any compatible
analysis, have a parent that is not a member of f.
Call this node the top of the fudge expression,
denoted f ?. f ? dominates all other members of
f; it can be considered f?s ?internal head.?
? f does not necessarily form a full subtree. Any
of its members may have dependents that are
not themselves members of the fudge expres-
sion. (Such dependencies can be specified in
additional GFL fragments.)
Top designation. A single member of a fudge
expression may optionally be designated as its top
(internal head). This is specified with an asterisk:
(Few* if any) > witches indicates that Few must
attach to witches and also dominate both if and
any. In the FUDG graph, this is represented with
a special top arc as depicted in bold in figure 4.
Nesting. One fudge expression may nest
within another, e.g. (Few (if any)) > witches;
the word analyzed as attaching to witches might
be Few or whichever of (if any) heads the other.
A nested fudge expression can be designated as
top: (Vanishingly few (if any)*).
Modifiers. An arc attaching a node to a
fudge expression as a whole asserts that the
external node should modify the top of the fudge
expression (whether or not that top is designated
in the annotation). For instance, two of the
interpretations of British left waffles on Falklands
would be preserved by specifying British > left
and (left waffles) < on < Falklands. Analyses
British > left < waffles < on < Falklands and
(British > left < on < Falklands) > waffles
would be excluded because the preposition does
not attach to the head of (left waffles).10
Multiple membership. A node may be a mem-
ber of multiple fudge expressions, or a member
of an FE while attached to some other node via
an explicit arc. Each connected component of
the FUDG graph is therefore a polytree (not nec-
essarily a tree). The annotation graph minus all
member edges of fudge nodes and all (undirected)
anaphoric links must be a directed tree or forest.
Enumerating supported parents. Fudge ex-
pressions complicate the procedure for listing a
node?s supported parents (see above). Consider an
FE f having some member v. v might be the top
of f (unless some other node is so designated), in
which case anything the fudge node can attach to
is a potential parent of v. If some node other than
v might be the top of f, then v?s head could be any
member of f. Below (?4.1) we develop an algo-
rithm for enumerating supported parents for any
annotation graph node.
4 Annotation Evaluation Measures
For an annotation task which allows for a great
deal of latitude?as in our case, where a syntac-
tic annotation may be full or partial?quantitative
evaluation of data quality becomes a challenge. In
the context of our formalism, we propose mea-
sures that address:
? Annotation efficiency, quantified in terms of
annotator productivity (tokens per hour).
? The amount of information in an underspeci-
fied annotation. Intuitively, an annotation that
flirts with many full analyses conveys less syn-
tactic information than one which supports few
analyses. We define an annotation?s promiscu-
ity to be the number of full analyses it supports,
and develop an algorithm to compute it (?4.1).
10Not all attachment ambiguities can be precisely encoded
in FUDG. For instance, there is no way to forbid an attach-
ment to a word that lies along the path between the pos-
sible heads. The best that can be done given a sentence
like They conspired to defenestrate themselves on Tuesday. is
They > conspired < to < defenestrate < themselves and
(conspired* to defenestrate (on < Tuesday)).
55
? Inter-annotator agreement between two par-
tial annotations. Our measures for dependency
structure agreement (?4.2) incorporate the no-
tion of promiscuity.
We test these evaluations on our pilot annotation
data in the case studies (?5).
4.1 Promiscuity vs. Commitment
Given a FUDG annotation of a sentence, we quan-
tify the extent to which it underspecifies the full
structure by counting the number of analyses that
are compatible with the constraints in the annota-
tion. We call this number the promiscuity of the
annotation. Each analysis tree is rooted with the
root node and must span all lexical nodes.11
A na?ve algorithm for computing promiscuity
would be to enumerate all directed spanning trees
over the lexical nodes, and then check each of
them for compatibility with the annotation. But
this quickly becomes intractable: for n nodes,
one of which is designated as the root, there are
nn?2 spanning trees. However, we can filter out
edges that are known to be incompatible with
the annotation before searching for spanning
trees. Our ?upward-downward? method for
constructing a graph of supported edges first
enumerates a set of candidate top nodes for every
fudge expression, then uses that information
to infer a set of supported parents for every
node.12 The supported edge graph then consists
of vertices lexnodes(A) ? {root} and edges
?
v?lexnodes(A) {(v? v?) ? v? ? suppParentsA(v)}.
From this graph we can count all directed span-
ning trees in cubic time using Kirchhoff?s matrix
tree theorem (Chaiken and Kleitman, 1978; Smith
and Smith, 2007; Margoliash, 2010).13 If some
lexical node has no supported parents, this reflects
conflicting constraints in the annotation, and no
spanning tree will be found.
Promiscuity will tend to be higher for longer
sentences. To control for this, we define a second
quantity, the annotation?s commitment quotient
(commitment being the opposite of promiscuity),
11This measure assumes a fixed lexical analysis (set of lex-
ical nodes) and does not consider anaphoric links. Coordinate
structures are simplified into ordinary dependencies, with co-
ordinate phrases headed by the coordinator?s lexical node. If
a coordination node has multiple coordinators, one is arbi-
trarily chosen as the head and the others as its dependents.
12Python code for these algorithms appears in Schneider
et al (2013) and the accompanying software release.
13Due to a technicality with non-member attachments to
fudge nodes, for some annotations this is only an upper bound
on promiscuity; see Schneider et al (2013).
which normalizes for the number of possible span-
ning trees given the sentence length. The commit-
ment quotient for an annotation of a sentence with
n?1 lexical nodes and one root node is given by:
com(A) = 1 ?
log prom(A)
log nn?2
(the logs are to attenuate the dominance of the ex-
ponential term). This will be 1 if only a single
tree is supported by the annotation, and 0 if the
annotation does not constrain the structure at all.
(If the constraints in the annotation are internally
inconsistent, then promiscuity will be 0 and com-
mitment undefined.) In practice, there is a trade-
off between efficiency and commitment: more de-
tailed annotations require more time. The value of
minimizing promiscuity will therefore depend on
the resources and goals of the annotation project.
4.2 Inter-Annotator Agreement
FUDG can encode flat groupings and coreference
at the lexical level, as well as syntactic structure
over lexical items. Inter-annotator agreement can
be measured separately for each of these facets.
Pilot annotator feedback indicated that our initial
lexical-level guidelines were inadequate, so we fo-
cus here on measuring structural agreement pend-
ing further clarification of the lexical conventions.
Attachment accuracy, a standard measure for
evaluating dependency parsers, cannot be com-
puted between two FUDG annotations if either of
them underspecifies any part of the dependency
structure. One solution is to consider the inter-
section of supported full trees, in the spirit of
our promiscuity measure. For annotations A1 and
A2 of sentence s, one annotation?s supported an-
alyses can be enumerated and then filtered sub-
ject to the constraints of the other annotation.
The tradeoff between inter-annotator compatibil-
ity and commitment can be accounted for by tak-
ing their product, i.e. comPrec(A1 | A2) =
com(A1)
|supp(A1)?supp(A2)|
|supp(A1)|
.
A limitation of this support-intersection ap-
proach is that if the two annotations are not
compatible, the intersection will be empty. A
more fine-grained approach is to decompose
the comparison by lexical node: we general-
ize attachment accuracy with softComPrec(A1 |
A2) = com(A1)
?
`?s
?
i?{1,2} suppParentsAi (`)?
`?s suppParentsA1 (`)
, comput-
ing com(?) and suppParents(?) as in the previous
section. As lexical nodes may differ between the
two annotations, a reconciliation step is required
56
Language Tokens Rate (tokens/hr)
English Tweets (partial) 667 430
English Tweets (full) 388 250
Malagasy 4,184 47
Kinyarwanda 8,036 80
Table 1: Productivity estimates from pilot annotation project.
All annotators were native speakers of English.
to compare the structures: multiwords proposed in
only one of the two annotations are converted to
fudge expressions. Tokens annotated by neither
annotator are ignored. Like with the promiscuity
measure, we simplify coordinate structures to or-
dinary dependencies (see footnote 11).
5 Case Studies
5.1 Annotation Time
To estimate annotation efficiency, we performed
a pilot annotation project consisting of annotating
several hundred English tweets, about 1,000 sen-
tences in Malagasy, and a further 1,000 sentences
in Kinyarwanda.14 Table 1 summarizes the num-
ber of tokens annotated and the effort required. For
the two Twitter cases, the same annotator was first
permitted to do partial annotation of 100 tweets,
and then spend the same amount of time doing a
complete annotation of all tokens. Although this is
a very small study, the results clearly suggest she
was able to make much more rapid progress when
partial annotation was an option.15
This pilot study helped us to identify linguistic
phenomena warranting specific conventions: these
include wh-expressions, comparatives, vocatives,
discourse connectives, null copula constructions,
and many others. We documented these cases in a
20-page style guide for English,16 which informed
the subsequent pilot studies discussed below.
5.2 Underspecification and Agreement
We annotated 2 small English data samples in
order to study annotators? use of underspecifica-
tion. The first is drawn from Owoputi et al?s 2013
Twitter part-of-speech corpus; the second is from
the Reviews portion of the English Web Treebank
14Malagasy is a VOS Austronesian language spoken by 15
million people, mostly in Madagascar. Kinyarwanda is an
SVO Bantu language spoken by 12 million people mostly in
Rwanda. All annotations were done by native speakers of En-
glish. The Kinyarwanda and Malagasy annotators had basic
proficiency in these languages.
15As a point of comparison, during the Penn Treebank
project, annotators corrected the syntactic bracketings pro-
duced by a high-quality hand-written parser (Fidditch) and
achieved a rate of only 375 tokens/hour using a specialized
GUI interface (Marcus et al, 1993).
16Included with the data and software release (footnote 1).
Omit. prom Hist. Mean
1Ws MWs Tkns FEs 1 >1 ?10 ?102 com
Tweets 60 messages, 957 tokens
A 597 56 304 23 43 17 11 5 .96
B 644 47 266 28 37 23 12 6 .95
Reviews 55 sentences, 778 tokens
A 609 33 136 2 53 2 2 1 1.00
C ? D 643 19 116 114 11 44 38 21 .82
T 704 ? 74 ? 55 0 0 0 1
Table 2: Measures of our annotation samples. Note that
annotator ?D? specialized in noun phrase?internal structure,
while annotator ?C? specialized in verb phrase/clausal phe-
nomena; C ? D denotes the combination of their annotation
fragments. ?T? denotes our dependency conversion of the
English Web Treebank parses. (The value 1.00 was rounded
up from .9994.)
(EWTB) (Bies et al, 2012). (Our annotators only
saw the tokenized text.) Both datasets are infor-
mal and conversational in nature, and are dom-
inated by short messages/sentences. In spite of
their brevity, many of the items were deemed to
contain multiple ?utterances,? which we define to
include discourse connectives and emoticons (at
best marginal parts of the syntax); utterance heads
are marked with ** in figure 1.
Table 2 indicates the sizes of the two data sam-
ples, and gives statistics over the output of each
annotator: total counts of single-word and mul-
tiword lexical nodes, tokens not represented by
any lexical node, and fudge nodes; as well as
a histogram of promiscuity counts and the aver-
age of commitment quotients (see ?4.1). For in-
stance, the two sets of annotations obtained for the
Tweets sample used underspecification in 17/60
and 23/60 tweets, respectively, though the promis-
cuity rarely exceeded 100 compatible trees per an-
notation. Examples can be seen in figure 1, where
annotator ?A? marked only the noun phrase head
for the scarriest mystery door, opted not to choose
a head within the quantity 1 1/2, and left ambigu-
ous the attachment of the hedge like. The strong
but not utter commitment to the dependency struc-
ture is reflected in the mean commitment quotients
for this dataset, both of which exceed 0.95.
Inter-annotator agreement (IAA) is quantified in
table 3. The row marked A ? B, for instance,
considers the agreement between annotator ?A?
and annotator ?B?. Measuring IAA on the depen-
dency structure requires a common set of lexical
nodes, so a lexical reconciliation step ensures that
(a) any token used by either annotation is present
in both, and (b) no multiword node is present
in only one annotation?solved by relaxing in-
compatible multiwords to FEs (which increases
promiscuity). For Tweets, lexical reconciliation
57
thus reduces the commitment averages for each
annotation?to a greater extent for annotator ?A?
(.96 in table 2 vs. .82 in table 3) because ?A?
marked more multiwords. An analysis fully com-
patible with both annotations exists for only 27/60
sentences; the finer-grained softComPrec measure
(?4.2), however, offers insight into the balance be-
tween commitment and agreement.
Qualitatively, we observe three leading causes
of incompatibilities (disagreements): obvious an-
notator mistakes (such as the marked as a head);
inconsistent handling of verbal auxiliaries; and un-
certainty whether to attach expressions to a verb
or the root node, as with here in figure 1.17 An-
notators noticed occasional ambiguous cases and
attempted to encode the ambiguity with fudge ex-
pressions: again in the tweet maybe put it off un-
til you feel like ~ talking again ? is one example.
More often, fudge expressions proved useful for
syntactically difficult constructions, such as those
shown in figure 1 as well as: 2 shy of breaking it,
asked what tribe I was from, a $ 13 / day charge,
you two, and the most awkward thing ever.
5.3 Annotator Specialization
As an experiment in using underspecification for
labor division, two of the annotators of Reviews
data were assigned specific linguistic phenomena
to focus on. Annotator ?D? was tasked with the in-
ternal structure of base noun phrases, including re-
solving the antecedents of personal pronouns. ?C?
was asked to mark the remaining phenomena?
i.e., utterance/clause/verb phrase structure?but to
mark base noun phrases as fudge expressions,
leaving their internal structure unspecified. Both
annotators provided a full lexical analysis. For
comparison, a third individual, ?A,? annotated the
same data in full. The three annotators worked
completely independently.
Of the results in tables 2 and 3, the most notable
difference between full and specialized annotation
is that the combination of independent specialized
annotations (C ? D) produces somewhat higher
promiscuity/lower commitment. This is unsurpris-
ing because annotators sometimes overlook rela-
tionships that fall under their specialty.18 Still, an-
notators reported that specialization made the task
17Another example: Some uses of conjunctions like and
and so can be interpreted as either phrasal coordinators or dis-
course connectives (cf. The PDTB Research Group, 2007).
18A more practical and less error-prone approach might be
for specialists to work sequentially or collaboratively (rather
than independently) on each sentence.
com softComPrec
IAA 1 2 N|?|>0 1|2 2|1 F1
Tweets (N=60)
A ? B .82 .91 27 .57 .72 .63
Reviews (N=55)
A ? (C ? D) .95 .76 30 .64 .40 .50
A ? T .92 1 26 .48 .91 .63
(C ? D) ? T .73 1.00 28 .33 .93 .49
Table 3: Measures of inter-annotator agreement. Annotator
labels are as in table 2. Per-annotator com (with lexical rec-
onciliation) and inter-annotator softComPrec are aggregated
over sentences by arithmetic mean.
less burdensome, and the specialized annotations
did prove complementary to each other.19
5.4 Treebank Comparison
Though the annotators in our study were native
speakers well acquainted with representations of
English syntax, we sought to quantify their agree-
ment with the expert treebankers who created the
EWTB (the source of the Reviews sentences). We
converted the EWTB?s constituent parses to de-
pendencies via the PennConverter tool (Johansson
and Nugues, 2007),20 then removed punctuation.
Agreement with the converted treebank parses
appears in the bottom two rows of table 3. Be-
cause the EWTB commits to a single analysis,
precision scores are quite lopsided. Most of its
attachments are consistent with our annotations
(softComPrec > 0.9), but these allow many ad-
ditional analyses (hence the scores below 0.5).
6 Conclusion
We have presented a framework for simple depen-
dency annotation that overcomes some of the rep-
resentational limitations of unlabeled dependency
grammar and embraces the practical realities of
resource-building efforts. Pilot studies (in multiple
languages and domains, supported by a human-
readable notation and a suite of open-source tools)
showed this approach lends itself to rapid annota-
tion with minimal training.
The next step will be to develop algorithms ex-
ploiting these representations for learning parsers.
Other future extensions might include additional
expressive mechanisms (e.g., multi-headedness,
labels), crowdsourcing of FUDG annotations
(Snow et al, 2008), or even a semantic counter-
part to the syntactic representation.
19In fact, for only 2 sentences did ?C? and ?D? have in-
compatible annotations, and both were due to simple mis-
takes that were then fixed in the combination.
20We ran PennConverter with options chosen to emulate
our annotation conventions; see Schneider et al (2013).
58
Acknowledgments
We thank Lukas Biewald, Yoav Goldberg, Kyle Jerro, Vi-
jay John, Lori Levin, Andr? Martins, and several anony-
mous reviewers for their insights. This research was sup-
ported in part by the U. S. Army Research Laboratory and
the U. S. Army Research Office under contract/grant number
W911NF-10-1-0533 and by NSF grant IIS-1054319.
References
Timothy Baldwin and Su Nam Kim. 2010. Multi-
word expressions. In Nitin Indurkhya and Fred J.
Damerau, editors, Handbook of Natural Language
Processing, Second Edition. CRC Press, Taylor and
Francis Group, Boca Raton, FL.
Benjamin K. Bergen and Nancy Chang. 2005. Embod-
ied Construction Grammar in simulation-based lan-
guage understanding. In Jan-Ola ?stman and Mir-
jam Fried, editors, Construction grammars: cog-
nitive grounding and theoretical extensions, pages
147?190. John Benjamins, Amsterdam.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. Technical Re-
port LDC2012T13, Linguistic Data Consortium,
Philadelphia, PA.
Alena B?hmov?, Jan Hajic?, Eva Hajic?ov?, Barbora
Hladk?, and Anne Abeill?. 2003. The Prague De-
pendency Treebank: a three-level annotation sce-
nario. In Treebanks: building and using parsed cor-
pora, pages 103?127. Springer.
Seth Chaiken and Daniel J. Kleitman. 1978. Matrix
Tree Theorems. Journal of Combinatorial Theory,
Series A, 24(3):377?381.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
La Haye.
Stephen Clark and James Curran. 2006. Partial training
for a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL (HLT-NAACL 2006), pages 144?151. As-
sociation for Computational Linguistics, New York
City, USA.
William Croft. 2001. Radical Construction Grammar:
Syntactic Theory in Typological Perspective. Oxford
University Press, Oxford.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed dependencies man-
ual. http://nlp.stanford.edu/downloads/
dependencies_manual.pdf.
Kitty Burns Florey. 2006. Sister Bernadette?s Barking
Dog: The quirky history and lost art of diagramming
sentences. Melville House, New York.
Victoria Fossum and Roger Levy. 2012. Sequential
vs. hierarchical syntactic models of human incre-
mental sentence processing. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics (CMCL 2012), pages 61?69. As-
sociation for Computational Linguistics, Montr?al,
Canada.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS Tagging and Parsing the Twitter-
verse. In Proceedings of the 2011 AAAI Workshop
on Analyzing Microtext, pages 20?25. AAAI Press,
San Francisco, CA.
The PDTB Research Group. 2007. The Penn Discourse
Treebank 2.0 annotation manual. Technical Report
IRCS-08-01, Institute for Research in Cognitive Sci-
ence, University of Pennsylvania, Philadelphia, PA.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: the Prague Dependency Treebank. In Eva
Hajic?ov?, editor, Issues of Valency and Meaning.
Studies in Honor of Jarmila Panevov?, pages 12?
19. Prague Karolinum, Charles University Press,
Prague.
Jan Hajic?, Barbora Vidov? Hladk?, and Petr Pajas.
2001. The Prague Dependency Treebank: anno-
tation structure and support. In Proceedings of
the IRCS Workshop on Linguistic Databases, pages
105?114. University of Pennsylvania, Philadelphia,
USA.
Richard A. Hudson. 1984. Word Grammar. Blackwell,
Oxford.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-99), pages 73?79. Association for Computa-
tional Linguistics, College Park, Maryland, USA.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English.
In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muis-
chnek, and Mare Koit, editors, Proceedings of the
16th Nordic Conference of Computational Linguis-
tics (NODALIDA-2007), pages 105?112. Tartu, Es-
tonia.
Martha Kolln and Robert Funk. 1994. Understanding
English Grammar. Macmillan, New York.
Sandra K?bler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Number 2 in Synthesis
Lectures on Human Language Technologies. Mor-
gan & Claypool, San Rafael, CA.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, pages 507?514. Association for Computa-
tional Linguistics, Sydney, Australia.
59
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
David Marec?ek, Martin Popel, Loganathan Ramasamy,
Jan ?te?p?nek, Daniel Zeman, Zdene?k ?abokrtsk?,
and Jan Hajic?. 2013. Cross-language study on in-
fluence of coordination style on dependency parsing
performance. Technial Report 49, ?FAL MFF UK.
Jonathan Margoliash. 2010. Matrix-Tree Theorem for
directed graphs. http://www.math.uchicago.
edu/~may/VIGRE/VIGRE2010/REUPapers/
Margoliash.pdf.
Igor Aleksandrovic? Mel?c?uk. 1988. Dependency Syn-
tax: Theory and Practice. SUNY Press, Albany,
NY.
Joakim Nivre. 2005. Dependency grammar and depen-
dency parsing. Technical Report MSI report 05133,
V?xj? University School of Mathematics and Sys-
tems Engineering, V?xj?, Sweden.
Timothy Osborne, Michael Putnam, and Thomas Gro?.
2012. Catenae: introducing a novel unit of syntactic
analysis. Syntax, 15(4):354?396.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 380?390. Association for Computational Lin-
guistics, Atlanta, Georgia, USA.
Alonzo Reed and Brainerd Kellogg. 1877. Work on
English grammar & composition. Clark & Maynard.
Federico Sangati and Chiara Mazza. 2009. An English
dependency treebank ? la Tesni?re. In Marco Pas-
sarotti, Adam Przepi?rkowski, Savina Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eigth
International Workshop on Treebanks and Linguistic
Theories, pages 173?184. EDUCatt, Milan, Italy.
Gerold Schneider. 1998. A linguistic comparison of
constituency, dependency and link grammar. Mas-
ter?s thesis, University of Zurich.
Nathan Schneider, Brendan O?Connor, Naomi Saphra,
David Bamman, Manaal Faruqui, Noah A. Smith,
Chris Dyer, and Jason Baldridge. 2013. A frame-
work for (under)specifying dependency syntax with-
out overloading annotators. arXiv:1306.2091
[cs.CL]. arxiv.org/pdf/1306.2091.
Petr Sgall, Eva Hajic?ov?, and Jarmila Panevov?.
1986. The Meaning of the Sentence in its Seman-
tic and Pragmatic Aspects. Reidel, Dordrecht and
Academia, Prague.
Stuart M. Shieber. 1992. Constraint-Based Grammar
Formalisms. MIT Press, Cambridge, MA.
Daniel Sleator and Davy Temperly. 1993. Parsing En-
glish with a link grammar. In Proceedings of the
Third International Workshop on Parsing Technol-
ogy (IWPT?93), pages 277?292. Tilburg, Nether-
lands.
David A. Smith and Noah A. Smith. 2007. Proba-
bilistic models of nonprojective dependency trees.
In Proceedings of the 2007 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 132?140. Associa-
tion for Computational Linguistics, Prague, Czech
Republic.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
Evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP 2008), pages 254?263. As-
sociation for Computational Linguistics, Honolulu,
Hawaii.
Mark Steedman. 2000. The Syntatic Process. MIT
Press, Cambridge, MA.
Luc Steels, Jan-Ola ?stman, and Kyoko Ohara, editors.
2011. Design patterns in Fluid Construction Gram-
mar. Number 11 in Constructional Approaches to
Language. John Benjamins, Amsterdam.
Lucien Tesni?re. 1959. El?ments de Syntaxe Struc-
turale. Klincksieck, Paris.
Michael Tomasello. 2003. Constructing a Language: A
Usage-Based Theory of Language Acquisition. Har-
vard University Press, Cambridge, MA.
60
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 1?11,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Distributions on Minimalist Grammar Derivations
Tim Hunter
Department of Linguistics
Cornell University
159 Central Ave., Ithaca, NY, 14853
tim.hunter@cornell.edu
Chris Dyer
School of Computer Science
Carnegie Mellon University
5000 Forbes Ave., Pittsburgh, PA, 15213
cdyer@cs.cmu.edu
Abstract
We present three ways of inducing proba-
bility distributions on derivation trees pro-
duced by Minimalist Grammars, and give
their maximum likelihood estimators. We
argue that a parameterization based on lo-
cally normalized log-linear models bal-
ances competing requirements for mod-
eling expressiveness and computational
tractability.
1 Introduction
Grammars that define not just sets of trees or
strings but probability distributions over these ob-
jects have many uses both in natural language pro-
cessing and in psycholinguistic models of such
tasks as sentence processing and grammar ac-
quisition. Minimalist Grammars (MGs) (Stabler,
1997) provide a computationally explicit formal-
ism that incorporates the basic elements of one
of the most common modern frameworks adopted
by theoretical syntacticians, but these grammars
have not often been put to use in probabilistic set-
tings. In the few cases where they have (e.g. Hale
(2006)), distributions over MG derivations have
been over-parametrized in a manner that follows
straightforwardly from a conceptualization of the
derivation trees as those generated by a particu-
lar context-free grammar, but which does not re-
spect the characteristic perspective of the under-
lying MG derivation. We propose an alternative
approach with a smaller number of parameters that
are straightforwardly interpretable in terms that re-
late to the theoretical primitives of the MG formal-
ism. This improved parametrization opens up new
possibilities for probabilistically-based empirical
evaluation of MGs as a cognitive hypothesis about
the discrete primitives of natural language gram-
mars, and for the use of MGs in applied natural
language processing.
In Section 2 we present MGs and their equiv-
alence to MCFGs, which provides a context-
free characterization of MG derivation trees. We
demonstrate the problems with the straightforward
method of supplementing a MG with probabili-
ties that this equivalence permits in Section 3, and
then introduce our proposed reparametrization that
solves these problems in Section 4. Section 5 con-
cludes and outlines some suggestions for future re-
lated work.
2 Minimalist Grammars and Multiple
Context-Free Grammars
2.1 Minimalist Grammars
A Minimalist Grammar (MG) (Sta-
bler and Keenan, 2003)1 is a five-tuple
G = ??,Sel ,Lic,Lex ,c? where:
? ? is a finite alphabet
? Sel (?selecting types?) and Lic (?licensing
types?) are disjoint finite sets which together
determine the set Syn (?syntactic features?),
which is the union of the following four sets:
selectors = {=f | f ? Sel}
selectees = { f | f ? Sel}
licensors = {+f | f ? Lic}
licensees = {-f | f ? Lic}
? Lex (?the lexicon?) is a finite subset of
?? ? (selectors ? licensors)? ? selectees ?
licensees?
? c ? Sel is a designated type of completed ex-
pressions
(A sample lexicon is shown in Fig. 3 below.)
1We restrict attention here to MGs without head move-
ment as presented by Stabler and Keenan (2003). Weak gen-
erative capacity is unaffected by this choice (Stabler, 2001).
1
Given an MG G, an expression is an ordered
binary tree with non-leaf nodes labeled by an ele-
ment of {<,>}, and with leaf nodes labeled by an
element of ?? ? Syn?. We take elements of Lex
to be one-node trees, hence expressions. We often
write elements of ?? ? Syn? with the two com-
ponents separated by a colon (e.g. arrive : +d v).
Each application of one of the derivational opera-
tions MERGE and MOVE, defined below, ?checks?
or deletes syntactic features on the expression(s)
to which it applies.
The head of a one-node expression is the ex-
pression?s single node; the head of an expression
[< e1 e2] is the head of e1; the head of an expres-
sion [> e1 e2] is the head of e2. An expression is
complete iff the only syntactic feature on its head
is a selectee feature c and there are no syntactic
features on any of its other nodes. Given an ex-
pression e, yield(e) ? ?? is result of concatenat-
ing the leaves of e in order, discarding all syntactic
features.
CL(G) is the set of expressions generated
by taking the closure of Lex under the func-
tions MERGE and MOVE, defined in Fig. 1;
intuitive graphical illustrations are given in
Fig 2. The language generated by G is {s | ?e ?
CL(G) such that e is complete and yield(e) = s}.
An example derivation, using the grammar in
Fig. 3, is shown in Fig. 4. This shows both the
?history? of derivational operations ? although
operations are not shown explicitly, all binary-
branching nodes correspond to applications of
MERGE and all unary-branching nodes to MOVE
? and the expression that results from each op-
eration. Writing instead only MERGE or MOVE
at each internal node would suffice to determine
the eventual derived expression, since these op-
erations are functions. A derivation tree is a
tree that uses this less redundant labeling: more
precisely, a derivation tree is either (i) a lexi-
cal item, or (ii) a tree [MERGE ?1 ?2] such that
MERGE(eval(?1), eval(?2)) is defined, or (iii) a
tree [MOVE ? ] such that MOVE(eval(?)) is defined;
where eval is the ?interpretation? function that
maps a derivation tree to an expression in the ob-
vious way. We define ?(G) to be the set of all
derivation trees using the MG G.
An important property of the definition of
MOVE is that it is only defined on ? [+f?] if there
is a unique subtree of this tree whose (head?s) first
feature is -f . From this it follows that in any
pierre : d who : d -wh
marie : d will : =v =d t
praise : =d v  : =t c
often : =v v  : =t +wh c
Figure 3: A Minimalist Grammar lexicon. The
type of completed expressions is c.
>
who : <
 : c >
marie : <
will : <
praise :  :
<
 : +wh c >
marie : <
will : <
praise : who : -wh
 : =t +wh c
>
marie : <
will : t <
praise : who : -wh
<
will : =d t <
praise : who : -wh
will : =v =d t
<
praise : v who : -wh
praise : =d v who : d -wh
marie : d
Figure 4: An MG derivation of an embedded ques-
tion
2
MERGE
(
e1[=f ?], e2[f ?]
)
=
{
[< e1[?] e2[?]] if e1[=f ?] ? Lex
[> e2[?] e1[?]] otherwise
MOVE
(
e1[+f ?]
)
= [> e2[?] e
?
1[?]]
where e2[-f ?] is a unique subtree of e1[+f ?]
and e?1 is like e1 but with e2[-f ?] replaced by an empty leaf node  : 
Figure 1: Definitions of MG operations MERGE and MOVE. The first case of MERGE creates comple-
ments, the second specifiers. f ranges over Sel ? Lic; ? and ? range over Syn?; and e[?] is an MG
expression whose head bears the feature-sequence ?.
=f ?
f ?
MERGE
?
?
<
=f ? f ?
MERGE
? ?
>
+f ?
-f ?
MOVE ??
>
Figure 2: Graphical illustrations of definitions of MERGE and MOVE. Rectangles represent single-node
trees. Triangles represent either single-node trees or complex trees, but the second case of MERGE applies
only when the first case does not (i.e. when the =f ? tree is complex).
3
derivation of a complete expression, every inter-
mediate derived expression will have at most |Lic|
subtrees whose (head?s) first feature is of the form
-g for any g ? Lic.
2.2 Multiple Context-Free Grammars
Multiple Context-Free Grammars (MCFGs) (Seki
et al, 1991; Kallmeyer, 2010) are a mildly
context-sensitive grammar formalism in the sense
of Joshi (1985).2 They bring additional expressive
capacity over context-free grammars (CFGs) by
generalizing to allow nonterminals to categorize
not just single strings, but tuples of strings. For
example, while a CFG might categorize eats cake
as a VP and the boy as an NP, an MCFG could
categorize the tuple ?says is tall,which girl? as a
VPWH (intuitively, a VP containing a WH which
will move out of it). Correspondingly, MCFG
production rules (construed as recipes for build-
ing expressions bottom-up) can specify not only,
for example, how to combine a string which is an
NP and a string which is a VP, but also how to
combine a string which is an NP with a tuple of
strings which is a VPWH. The CFG rule which
would usually be written ?S ? NP VP? is shown
in (1) in a format that makes explicit the string-
concatenation operation; (2) uses this notation to
express an MCFG rule that combines an NP with
a VPWH to form a string of category Q, an em-
bedded question. (We often omit angle brackets
around one-tuples.) An example application of
this rule is shown in (3).
st :: S ? s :: NP t :: VP (1)
t2st1 :: Q ? s :: NP ?t1, t2? :: VPWH (2)
which girl the boy says is tall :: Q ?
the boy :: NP ?says is tall,which girl? :: VPWH
(3)
Every nonterminal in an MCFG derives (only) n-
tuples of strings, for some n known as the non-
terminal?s rank. In the examples above NP, VP,
S and Q are of rank 1, and VPWH is of rank 2.
A CFG is an MCFG where every nonterminal has
rank 1.
Michaelis (2001) showed that it is possible to
reformulate MGs in a way that uses categorized
2MCFGs are almost identical to Linear Context-Free
Rewrite Systems (Vijay-Shanker et al, 1987). Seki et al
(1991) show that the two formalisms are weakly equivalent.
string-tuples, of the sort that MCFGs manipulate,
as derived structures (or expressions) instead of
trees. The ?purpose? of the internal tree structure
that we assign to derived objects is, in effect, to
allow a future application of MOVE to break them
apart and rearrange their pieces, as illustrated in
Fig. 2. But since the placement of the syntactic
features on a tree determines the parts that will be
rearranged by a future application of MOVE (in any
derivation of a complete expression), we lose no
relevant information by splitting up a tree?s yield
into the components that will be rearranged and
then ignoring all other internal structure. Thus the
following tree:
+f ?
-f ? -g ? (4)
becomes a tuple of categorized strings (we will ex-
plain the 0 subscript shortly):
?
s : +f ? , t : -f ? , u : -g ?
?
0
or, equivalently, a tuple of strings, categorized by
a tuple-of-categories:
?s, t, u? :: ?+f ?,-f ?,-g ??0 (5)
The order of the components is irrelevant except
for the first component, which contains the entire
structure?s head node; intuitively, this is the com-
ponent out of which the others move.
Based on this idea, Michaelis (2001) shows
how to construct, for any MG, a correspond-
ing MCFG whose nonterminals are tuples like
?+f ?,-f ?,-g ??0 from above. The uniqueness
requirement in the definition of MOVE ensures that
we need only a finite number of such nontermi-
nals. The feature sequences that comprise the
MCFG nonterminals, in combination with the MG
operations, determine the MCFG production rules
in which each MCFG nonterminal appears. For
example, the arrangement of features on the tree
in (4) dictates that MOVE is the only MG opera-
tion that can apply to it; thus the internals of the
complex category in (5) correspondingly dictate
that the only MCFG production that takes (5) as
?input? (again, thinking right-to-left or bottom-up
as in (1) and (2)) is one that transforms it in ac-
cord with the effects of MOVE. If ? = , then this
4
effect will be to transform the three-tuple into a
two-tuple as shown in (6), since the t-component
now has no remaining features and has therefore
reached its final position:
?ts, u? :: ??,-g ??0 ?
?s, t, u? :: ?+f ?,-f,-g ??0 (6)
This is analogous ? modulo the presence of the
additional u : -g ? component ? to the rule that
is used in the final step of the derivation in Fig. 5,
which is the MCFG equivalent of Fig. 4.
If, on the other hand, ? 6= , then the t-
component will need to move again later in the
derivation, and so we keep it as a separated com-
ponent:
?s, t, u? :: ??, ?,-g ??0
? ?s, t, u? :: ?+f ?,-f ?,-g ??0 (7)
The subscript 0 on the tuples above indi-
cates that the corresponding expressions are non-
lexical; for lexical expressions, the subscript is 1.
This information is not relevant to MOVE oper-
ations, but is crucial for distinguishing between
the complement and specifier cases of MERGE.
For example, in the simplest cases where no to-
be-moved subconstituents are present, the con-
structed MCFG must contain two rules corre-
sponding to MERGE as follows. (n matches either
1 or 0.)
st :: ???0 ? s :: ?=f ??1 t :: ?f?n (8)
ts :: ???0 ? s :: ?=f ??0 t :: ?f?n (9)
By similar logic, it is possible to construct
all the necessary MCFG rules corresponding to
MERGE and MOVE; see, for example, Stabler and
Keenan (2003, p.347) for (a presentation of the
MG operations that can also be straightforwardly
be read as) the general schemas that generate these
rules. One straightforward lexical/preterminal rule
is added for each lexical item in the MG, and
the MCFG?s start symbol is ?c?0.3 The resulting
MCFG is weakly equivalent to the original MG,
and strongly equivalent in the sense that one can
straightforwardly convert back and forth between
the two grammars? derivation trees. The MCFG
equivalent of the MG in Fig. 3 is shown in Fig. 6
(ignoring the weights for now, which we come to
below).4
3We exclude ?c?1 on the simplifying assumption that the
who marie will praise :: ?c?0
?marie will praise,who? :: ?+wh c,-wh?0
 :: ?=t +wh c?1 ?marie will praise,who? :: ?t,-wh?0
?will praise,who? :: ?=d t,-wh?0
will :: ?=v =d t?1 ?praise,who? :: ?v,-wh?0
praise :: ?=d v?1 who :: ?d -wh?1
marie :: ?d?1
Figure 5: The MG derivation from Fig. 4 illus-
trated with tuples of strings instead of trees as the
derived structures.
Notation. We define the above conversion pro-
cess to be an (invertible) function pi from MGs to
MCFGs. That is, for an valid MG, G it holds that
pi(G) is an equivalent MCFG and pi?1(pi(G)) =
G. By abuse of notation, we will use pi as the func-
tion for converting from MG derivation trees to
equivalent MCFG derivation trees. By an MCFG
derivation tree we mean a tree like Fig. 5 but with
non-leaf nodes labelled only by nonterminals (not
tuples of strings). The derivation tree language of
an MCFG is thus a local tree language, just as for
a CFG; that of an MG is non-local but regular (Ko-
bele et al, 2007).
3 Distributions on Derivations
Assume a Minimalist Grammar, G. In this sec-
tion and the next, we will consider various ways of
defining probability distributions on the derivation
trees in ?(G).5 The first approach, introduced in
Section 3.2, is conceptually straightforward but is
problematic in certain respects that we discuss in
Section 3.3. We present a different approach that
resolves these problems in Section 4.
We also consider the problem of estimating the
parameters of these distributions from a finite sam-
ple of training data, specified by a function f? :
?(G) ? N, where f?(?) is the number of times
derivation ? occurs in the sample. To this end, it
MG has no lexical item whose only feature is the selectee c.
4This MCFG includes only the rules that are ?reachable?
from the lexical items. For example, we leave aside rules
involving the nonterminal ?=c v -wh?0, even though the
schemas in Stabler and Keenan (2003) generate them.
5We use the terms derivation tree and derivation inter-
changeably.
5
?ERF
2/2  :: ?=t +wh c?1
95/95  :: ?=t c?1
97/97 will :: ?=v =d t?1
6/6 often :: ?=v v?1
97/97 praise :: ?=d v?1
95/192 marie :: ?d?1
97/192 pierre :: ?d?1
2/2 who :: ?d -wh?1
?ERF
2/2 ?st, u? :: ?+wh c,-wh?0 ? s :: ?=t +wh c?1 ?t, u? :: ?t,-wh?0
95/95 st :: ?=d t?0 ? s :: ?=v =d t?1 t :: ?v?0
2/2 ?st, u? :: ?=d t,-wh?0 ? s :: ?=v =d t?1 ?t, u? :: ?v,-wh?0
2/97 ts :: ?c?0 ? ?s, t? :: ?+wh c,-wh?0
95/97 st :: ?c?0 ? s :: ?=t c?1 t :: ?t?0
95/95 ts :: ?t?0 ? s :: ?=d t?0 t :: ?d?1
2/2 ?ts, u? :: ?t,-wh?0 ? ?s, u? :: ?=d t,-wh?0 t :: ?d?1
95/100 st :: ?v?0 ? s :: ?=d v?1 t :: ?d?1
5/100 st :: ?v?0 ? s :: ?=v v?1 t :: ?v?0
2/3 ?s, t? :: ?v,-wh?0 ? s :: ?=d v?1 t :: ?d -wh?1
1/3 ?st, u? :: ?v,-wh?0 ? s :: ?=v v?1 ?t, u? :: ?v,-wh?0
Figure 6: The MCFG produced from the MG in Fig. 3, as described in Section 2.2; with weights com-
puted by relative frequency estimation based on the naive parametrization, as described in Section 3.
will be useful to define the empirical distribution
on derivations to be p?(?) = f?(?)/
?
? ? f?(?
?).
3.1 Stochastic MCFGs
As with CFGs, it is straightforward to imbue
an MCFG, H , with production probabilities and
thereby create a stochastic MCFG.6 In stochas-
tic MCFGs (as in CFGs) the probability of a non-
terminal rewrite in a derivation is conditionally in-
dependent of all other rewrite decisions, given the
non-terminal type. This formulation defines a dis-
tribution over MCFG derivations in terms of a ran-
dom branching process that begins with probabil-
ity 1 at the start symbol and recursively expands
frontier nodes N , drawing branching decisions
from the the conditional distribution p(? | N); the
process terminates when lexical items have been
produced on all frontiers.
If p(? | N) is the probability that N rewrites as
? and f? (N ? ?) is the number of times N ? ?
occurs in derivation tree ? , then
p(?) =
?
(N??)?H
p(? | N)f? (N??). (10)
With mild assumptions to ensure consistency (Chi,
1999), the p(?)?s form a proper probability distri-
bution over all derivations in H .7
Because the derivation trees of the MG G stand
in a bijection with the derivation trees of the
MCFG pi(G), stochastic MCFGs can be used to
define a distribution on MG derivations.
6Although MCFGs have a greater generative capacity
than CFGs, the statistical properties do not change at all, un-
less otherwise noted.
7The estimators that are based on empirical frequencies
in a derivation bank which we use in this paper will always
yield consistent estimates. Refer Chi (1999) for more detail.
3.2 The naive parametrization
The most straightforward way to parameterize a
stochastic MCFG uses individual parameters ??|N
to represent each production probability, i.e., p(? |
N)
.
= ??|N . When applied to an MCFG that is
derived from an MG, we will refer to this as the
naive parametrization.
This is the parametrization used by Hale (2006)
to define a probability distribution over the deriva-
tions of MGs in order to explore the predictions
of an information-theoretic hypothesis concerning
sentence comprehension difficulty.
MLE. The arguably most standard technique for
setting the parameters of a probability distribution
is so that they maximize the likelihood of a sam-
ple of training data. In the naive parameterization,
the maximum likelihood estimate (MLE) for each
parameter ??ERF?|N is the empirical relative frequency
of the rewrite N ? ? in the training data (Abney,
1997):
??ERF?|N =
?
? f?(?)fpi(?)(N ? ?)
?
? f?(?)
?
(N???)?pi(G) fpi(?)(N ? ?
?)
.
3.3 Unfaithfulness to MGs
While the naive parameterization with MLE esti-
mation is simple, it is arguably a poor choice for
parameterizing distributions on MGs. The prob-
lem is that, relative to the independence assump-
tions encoded in the MG formalism, each step of
the MCFG derivation both conditions on and pre-
dicts ?too much? structure. As a result, common-
alities across different applications of the same
MG operation are modeled independently and do
not share statistical strength. This arises because
6
90 pierre will praise marie
5 pierre will often praise marie
1 who pierre will praise
1 who pierre will often praise
Figure 7: An artificial corpus of sentences deriv-
able from the grammars in Figures 3 and 6.
of the way the MCFG?s nonterminals multiply out
all relevant arrangements of features.8 We illus-
trate the problem with an example.
Consider the corpus in Fig. 7, where each sen-
tence is preceded by its frequency. Since each sen-
tence is assigned a unique derivation by our exam-
ple MG, this is equivalent to a treebank.
One reasonable statistical interpretation of the
first two lines is that a verb phrase comprises a
verb and an object 95% of the time, and comprises
the adverb often and another verb phrase 5% of
the time (since pierre will often praise marie has
two nested verb phrase constituents). The last two
lines provide an analogous pair of sentences in-
volving wh-movement of the object. A priori, one
would expect that the 95:5 relative frequency that
describes the presence of the adverb also applies
here; however, the ERF estimator will use 2:1 in-
stead. Why is this? The VP category in the MCFG
is ?split? into two to indicate whether it has a wh-
feature inside it, and each has its own parameters.
We criticize this on the grounds that it is not in line
with our main goal of defining a distribution over
the derivations of the MG: from the perspective of
the MG, there is a sense in which it is ?the same
instance? of MERGE that combines often with a
verb phrase, whether or not the verb phrase?s ob-
ject bears a -wh feature. In other words, the differ-
ences between the following two trees seem unre-
lated to the way in which they are both candidates
to be merged with often : =v v.
<
praise : v who : -wh
<
praise : v marie :
From the perspective of the MCFG, however, the
introduction of the adverb is mediated by expan-
sions of the nonterminal ?v?0 in cases without
object wh-movement, but by expansions of the
distinct nonterminal ?v,-wh?0 in cases with it.
Therefore the information about adverb inclusion
that is conveyed by the movement-free entries in
8Stabler (forthcoming) also discusses the sense in which
MCFG rules ?miss generalizations? found in MGs.
the corpus is interpreted as only relevant to simi-
larly movement-free derivations. This can be seen
in the weights of the last four rules in Fig. 6, which
were computed by relative frequency estimation
on the basis of the corpus.
Relative to the underlying MG, the naive
parametrization has too many degrees of freedom:
the model is overparameterized and is capable of
capturing statistical distinctions that we have the-
oretical reasons to dislike. Of course, it is possi-
ble that VPs have meaningfully different distribu-
tions depending on whether or not they contain a
wh-feature; however, we would like a parameter-
ization that provides the flexibility to treat these
two different contexts as identical, as different, or
to share statistical strength between them in some
other way. In the next section we propose two
alternative parametrizations that provide this con-
trol.
4 Log-linear MCFGs
4.1 Globally normalized log-linear models
An alternative mechanism for inducing a distribu-
tion on ?(G) that provides more control over in-
dependence assumptions is the globally normal-
ized log-linear model (also called a Markov ran-
dom field, undirected model, or Gibbs distribu-
tion). Unlike the model in the previous section,
log-linear models are not stochastic in nature?
they assign probabilities to structured objects, but
they do not rely on a random branching process
to do so. Rather, they use a d-dimensional vector
of feature functions? = ??1,?2, . . . ,?d?, where
?i : ?(G) ? R, to extract features of the deriva-
tion, and a real-valued weight vector ? ? Rd.9
Together, ? and ? define the score of a derivation
? as a monotonic function of the weighted sum of
the feature values ?1(?), . . . ,?d(?):
s?(?) = exp(? ??(?)).
Using this function, a Gibbs distribution on the
derivations in ?(G) is
p?(?) =
s?(?)
?
? ???(G) s?(?
?)
, (11)
9The term feature here refers to functions of a derivation;
it should not be confused with the syntactic features dis-
cussed immediately above. However, in as much as syntactic
features characterize the steps in a derivation, it is natural that
they would play a central role in defining distributions over
derivations, and indeed, our proposed feature functions ex-
amine syntactic features almost exclusively.
7
provided that the sum in the denominator is fi-
nite.10
Notice that (11) is similar to the formula for
a relative frequency, the difference being that we
use a derivation?s score s?(?) rather than its em-
pirical count. This use of scores provides a way
to express the kind of ?missed similarities? we
discussed in Section 3.3 via the choice of feature
functions. Returning to the example from above,
in order to express the similarity between the two
adverb-introducing rules ? one involving the non-
terminal ?v?0, the other involving ?v,-wh?0 ?
we could define a particular feature function ?i
that maps a derivation to 1 if it contains either one
of these rules and 0 otherwise. Then, all else be-
ing equal, setting the corresponding parameter ?i
to a higher value will increase the score s?(?),
and hence the probability p?(?), of any derivation
? that introduces an adverb, with or without wh-
movement of the object.
MLE. As with the naive parameterization, the
the parameters ?may be set to maximize the (log)
likelihood of the training data, i.e.,
?? = arg max
?
n?
i=1
p?(?i)
f?(?i)
= arg max
?
n?
i=1
f?(?i) log p?(?i)
? ?? ?
=L [log likelihood]
. (12)
We remark that maximizing the log likelihood
of data in this parameterization is equivalent to
finding the distribution p?(?) in which the ex-
pected value of ?(?) is equal to the expected
value of the same under the empirical distribution
(i.e., under p?(?)) and whose entropy is maximized
(Della Pietra et al, 1997). This equivalence is par-
ticularly clear when the gradient of L (see (12))
with respect to ? is examined:
??L = Ep?(?)[?(?)]? Ep?(?)[?(?)]. (13)
This form makes clear thatL achieves an optimum
when the expectations of ? match under the two
distributions.11
10There are several conditions under which this is true. It
is trivially true if |?(G)| < ?. When ? is infinite, the de-
nominator may still be finite if features functions grow (su-
per) linearly with the derivation size in the limiting case as
the size tends to infinity. Then, if feature weights are nega-
tive, the denominator will either be equal to or bounded from
above by an infinite geometric series with a finite sum. Refer
to Goodman (1999) and references therein.
11While the maximizing point cannot generally be solved
4.2 Feature locality
Notice that the approach just outlined is extremely
general: the feature functions ? can examine the
derivation trees as a whole. It is possible to define
features that pay attention to arbitrary or global
properties of a derivation. While such features
might in fact generalize well to new data ? for ex-
ample, one could mimic a bigram language model
by including features counting bigrams in the
string that is generated by the derivation ? these
are intuitively ?bad? since they ignore the deriva-
tion?s structure. Furthermore, there is a substantial
practical downside to allowing unrestricted feature
definitions: features that do not ?agree? with the
derivation structure make inference computation-
ally intractable. Specifically, finding the best most
probable derivation of a sentence with ?global?
features is NP-hard (Koller and Friedman, 2009).
For these reasons, it is advantageous to require
that ? decompose additively in terms of local fea-
ture functions, ? over the steps that make up a
derivation. For defining distributions under an MG
G, we will assume that feature functions decom-
pose over the productions in a derivation under the
MCFG projection pi(G), i.e.,
?(?) =
?
(N??)?pi(?)
? (N ? ?) .
Under the locality assumption, we may rewrite the
score s?(?) as
?
(N??)?pi(G)
(exp(? ??(N ? ?)))fpi(?)(N??) .
This (partially) addresses the issue of computa-
tional tractability, enforces our intuition that the
score of a derivation tree should be a function of
scores of its component steps, and still gives us the
ability to avoid the overconditioning that we iden-
tified in Section 3.3.12
4.3 Locally normalized log-linear models
Even with our assumption of feature locality, find-
ing ?? remains challenging since the second term
for analytically, gradient based optimization techniques may
be effectively used to find it (and it is both guaranteed to exist
and guaranteed to be unique).
12We say that the issue of computational tractability is only
partially resolved because only certain operations ? identi-
fying the most probable derivation of a string ? are truly ef-
ficient. Computing the model?s normalization function, while
no longer NP-hard, still not practical.
8
in (13) is difficult to compute.13 In this section we
suggest a parameterization that admits both effi-
cient ML estimation and retains the ability to use
feature functions to control the distribution.
To do so, we revisit the approach of defining
distributions on derivations in terms of a stochas-
tic process from Section 3.1, but rather than defin-
ing the branching distributions with independent
parameters for each MCFG nonterminal rewrite
type, we parameterize it in terms of locally nor-
malized log-linear models, also called a condi-
tional logit model (Murphy, 2012). Given an MG
G, a weight vector w ? Rd, and rule-local feature
functions ? as defined above,14 let the branching
probability
pw(? | N)
.
=
exp(w ??(N ? ?))
?
(N???)?pi(G) exp(w ??(N ? ?
?))
.
Like the parametrization in Section 4.1, this
new parametrization is based on log-linear mod-
els and therefore allows us to express similarities
among derivational operations via choices of fea-
ture functions. However, rather than defining fea-
ture functions ?i on entire derivations, these fea-
tures can only ?see? individual MCFG rules. Put
differently, the same technique we used in Sec-
tion 4.1 to define a probability distribution over the
entire set of derivations, is used here to define each
of the local conditional probability distributions
over the expansions of a single MCFG nontermi-
nal. Via the perspective familiar from stochastic
MCFGs, these individual conditional probability
distributions together define a distribution on the
entire set of derivations.
MLE. As with the previous two models, we can
set parametersw to maximize the likelihood of the
training data. Here, the global likelihood is ex-
pressed in terms of the probabilities of condition-
ally independent rewrite events, each defined in a
log-linear model:
Lc =
?
?
f?(?)
?
(N??)?pi(?)
fpi(?)(N ? ?) log pw(? | N).
13Specifically, it requires computing expectations under all
possible derivations in ?(pi(G)) during each step of gradient
ascent, which requires polynomial space/time in the size of
the lexicon to compute exactly.
14The notational shift from? tow to emphasizes that these
two parameter vectors have very different semantics. The
former parameterizes potential functions in a globally nor-
malized random field while the later is used to determine a
family of conditional probability distributions used to define
a stochastic process.
Its gradient with respect to w is therefore
?wL
c =
?
?
f?(?)
?
(N??)?pi(?)
fpi(?)(N ? ?)
[
?(N ? ?)? Epw(??|N)?(N ? ?
?)
]
.
As with the globally normalized model, ?wLc =
0 has no closed form solution; however, gradient-
based optimization is likewise effective. How-
ever, unlike (13), this gradient is straightforward to
compute since it requires summing only over the
different rewrites of each non-terminal category
during each iteration of gradient ascent, rather
than over all possible derivations in ?(G)!
4.4 Example parameter estimation
In this section we compare the probability esti-
mates for productions in a stochastic MCFGs ob-
tained using the naive parameterization discussed
in Section 3.2 that conditions on ?too much? infor-
mation and those obtained using locally normal-
ized log-linear models with grammar-appropriate
feature functions. Our very simple feature set con-
sists just of binary-valued feature functions that in-
dicate:
? whether a MERGE step, MOVE step, or a termi-
nating lexical-insertion step is being generated;
? what selector feature (in the case of MERGE
steps) or licensor feature (in the case of MOVE
steps) is being checked (e.g., +wh or =d or =v);
and
? what lexical item is used (e.g., marie : d or
 : =t c), in the case of terminating lexical-
insertion steps.
Table 1 shows the values of some of these features
for a sample of the MCFG rules in Fig. 6.
Table 2 compares the production probabilities
estimated for last four rules in Fig. 6 using the
naive empirical frequency method and our recom-
mended log-linear approach with the features de-
fined as above.15 The presence or absence of a
-wh feature does not affect the log-linear model?s
probability of adding an adverb to a verb phrase,
in keeping with the perspective suggested by the
derivational operations of MGs.
15The log-linear parameters were optimized using a stan-
dard quasi-Newtonian method (Liu and Nocedal, 1989).
9
Table 1: Selected feature values for a sample of MCFG rules. The first four rules are the ones that
illustrated the problems with the naive parametrization in Section 3.3.
MCFG Rule ?MERGE ?=d ?=v ?=t ?MOVE ?+wh
st :: ?v?0 ? s :: ?=d v?1 t :: ?d?1 1 1 0 0 0 0
st :: ?v?0 ? s :: ?=v v?1 t :: ?v?0 1 0 1 0 0 0
?s, t? :: ?v,-wh?0 ? s :: ?=d v?1 t :: ?d -wh?1 1 1 0 0 0 0
?st, u? :: ?v,-wh?0 ? s :: ?=v v?1 ?t, u? :: ?v,-wh?0 1 0 1 0 0 0
st :: ?c?0 ? s :: ?=t c?1 t :: ?t?0 1 0 0 1 0 0
ts :: ?c?0 ? ?s, t? :: ?+wh c,-wh?0 0 0 0 0 1 1
Table 2: Comparison of probability estimators.
MCFG Rule Naive p? Log-linear p?
st :: ?v?0 ? s :: ?=d v?1 t :: ?d?1 0.95 0.94
st :: ?v?0 ? s :: ?=v v?1 t :: ?v?0 0.05 0.06
?s, t? :: ?v,-wh?0 ? s :: ?=d v?1 t :: ?d -wh?1 0.67 0.94
?st, u? :: ?v,-wh?0 ? s :: ?=v v?1 ?t, u? :: ?v,-wh?0 0.33 0.06
5 Conclusion and Future Work
We have presented a method for inducing a prob-
ability distribution on the derivations of a Min-
imalist Grammar in a way that remains faithful
to the way the derivations are conceived of in
this formalism, and for obtaining the maximum
likelihood estimate of its parameters. Our pro-
posal takes advantage of the MG-MCFG equiva-
lence in the sense that it uses the underlying prob-
abilistic branching process of a stochastic MCFG,
but avoids the problems of overparametrization
that come with the naive approach that reifies the
MCFG itself.
Our parameterization has several applications
worth noting. It provides a new way to compare
variants of the MG formalism that propose slightly
different sets of primitives (operations, types of
features, etc.) but are equivalent once transformed
into MCFGs. Examples of such variants include
the addition of an ADJOIN operation (Frey and
Ga?rtner, 2002), or replacing MERGE and MOVE
with a single feature-checking operation (Stabler,
2006; Hunter, 2011). Derivations using these dif-
ferent versions of the formalism often boil down
to the same string-concatenation operations and
will therefore be expressible using equivalent sets
of MCFG rules. The naive parametrization will
therefore not distinguish them, but in the same
way that our proposal above ?respects? standard
MGs? classification of MCFG rules according to
one set of derivational primitives, one could de-
fine feature vectors that respect different classifi-
cations.
Outside of MGs, the strategy is applicable to
any other formalisms whose derivations can be re-
cast as those of MCFGs, such as TAGs and CCGs.
More generally still, it could be applied to any
formalism whose derivation tree languages can be
characterized by a local tree grammar; in our case,
the relevant local tree language is obtained via a
projection from the regular tree language of MG
derivation trees.
Acknowledgments
Thanks to John Hale for helpful discussion and to
the anonymous reviewers for their insightful com-
ments. This work was sponsored by NSF award
number 0741666, and by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533.
References
Steven P. Abney. 1997. Stochastic attribute-value
grammars. Computational Linguistics.
Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 19(4).
10
Werner Frey and Hans-Martin Ga?rtner. 2002. On the
treatment of scrambling and adjunction in minimal-
ist grammars. In Gerhard Ja?ger, Paola Monachesi,
Gerald Penn, and Shuly Wintner, editors, Proceed-
ings of Formal Grammar 2002, pages 41?52.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4).
John Hale. 2006. Uncertainty about the rest of the
sentence. Cognitive Science, 30:643?672.
Tim Hunter. 2011. Insertion Minimalist Gram-
mars: Eliminating redundancies between merge and
move. In Makoto Kanazawa, Andra?s Kornai, Mar-
cus Kracht, and Hiroyuki Seki, editors, The Mathe-
matics of Language (MOL 12 Proceedings), volume
6878 of LNCS, pages 90?107. Springer, Berlin Hei-
delberg.
Aravind Joshi. 1985. How much context-sensitivity
is necessary for characterizing structural descrip-
tions? In David Dowty, Lauri Karttunen, and
Arnold Zwicky, editors, Natural Language Process-
ing: Theoretical, Computational and Psychological
Perspectives, pages 206?250. Cambridge University
Press, New York.
Laura Kallmeyer. 2010. Parsing Beyond Context-Free
Grammars. Springer-Verlag, Berlin Heidelberg.
Gregory M. Kobele, Christian Retore?, and Sylvain Sal-
vati. 2007. An automata theoretic approach to
minimalism. In James Rogers and Stephan Kepser,
editors, Proceedings of the Workshop on Model-
Theoretic Syntax at 10; ESSLLI ?07.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
Jens Michaelis. 2001. Derivational minimalism is
mildly context-sensitive. In Michael Moortgat, ed-
itor, Logical Aspects of Computational Linguistics,
LACL 1998, volume 2014 of LNCS, pages 179?198.
Springer, Berlin Heidelberg.
Kevin P. Murphy. 2012. Machine Learning: A Proba-
bilistic Perspective. MIT Press.
Hiroyuki Seki, Takashi Matsumara, Mamoru Fujii, and
Tadao Kasami. 1991. On multiple context-free
grammars. Theoretical Computer Science, 88:191?
229.
Edward P. Stabler and Edward L. Keenan. 2003. Struc-
tural similarity within and among languages. Theo-
retical Computer Science, 293:345?363.
Edward P. Stabler. 1997. Derivational minimalism. In
Christian Retore?, editor, Logical Aspects of Compu-
tational Linguistics, volume 1328 of LNCS, pages
68?95. Springer, Berlin Heidelberg.
Edward P. Stabler. 2001. Recognizing head move-
ment. In Philippe de Groote, Glyn Morrill, and
Christian Retore?, editors, Logical Aspects of Com-
putational Linguistics, volume 2099 of LNCS, pages
254?260. Springer, Berlin Heidelberg.
Edward P. Stabler. 2006. Sidewards without copying.
In Shuly Wintner, editor, Proceedings of The 11th
Conference on Formal Grammar, pages 157?170.
CSLI Publications, Stanford, CA.
Edward Stabler. forthcoming. Two models of min-
imalist, incremental syntactic analysis. Topics in
Cognitive Science.
K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. 1987. Characterizing structural descriptions
produced by various grammatical formalisms. In
Proc. 25th Meeting of Assoc. Computational Lin-
guistics, pages 104?111.
11
Workshop on Humans and Computer-assisted Translation, pages 72?77,
Gothenburg, Sweden, 26 April 2014.
c?2014 Association for Computational Linguistics
Real Time Adaptive Machine Translation for Post-Editing with
cdec and TransCenter
Michael Denkowski Alon Lavie Isabel Lacruz
?
Chris Dyer
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA 15213 USA
?
Institute for Applied Linguistics, Kent State University, Kent, OH 44242 USA
{mdenkows,alavie,cdyer}@cs.cmu.edu ilacruz@kent.edu
Abstract
Using machine translation output as a
starting point for human translation has
recently gained traction in the transla-
tion community. This paper describes
cdec Realtime, a framework for build-
ing adaptive MT systems that learn from
post-editor feedback, and TransCenter, a
web-based translation interface that con-
nects users to Realtime systems and logs
post-editing activity. This combination
allows the straightforward deployment of
MT systems specifically for post-editing
and analysis of human translator produc-
tivity when working with these systems.
All tools, as well as actual post-editing
data collected as part of a validation exper-
iment, are freely available under an open
source license.
1 Introduction
This paper describes the end-to-end machine
translation post-editing setup provided by cdec
Realtime and TransCenter. As the quality of MT
systems continues to improve, the idea of using
automatic translation as a primary technology in
assisting human translators has become increas-
ingly attractive. Recent work has explored the
possibilities of integrating MT into human transla-
tion workflows by providing MT-generated trans-
lations as a starting point for translators to cor-
rect, as opposed to translating source sentences
from scratch. The motivation for this process is
to dramatically reduce human translation effort
while improving translator productivity and con-
sistency. This computer-aided approach is directly
applicable to the wealth of scenarios that still re-
quire precise human-quality translation that MT
is currently unable to deliver, including an ever-
increasing number of government, commercial,
and community-driven projects.
The software described in the following sec-
tions enables users to translate documents with
the assistance of an adaptive MT system using
a web-based interface. The system learns from
user feedback, improving translation quality as
users work. All user interaction is logged, al-
lowing post-editing sessions to be replayed and
analyzed. All software is freely available under
an open source license, allowing anyone to eas-
ily build, deploy, and evaluate MT systems specif-
ically for post-editing. We first describe the under-
lying adaptive MT paradigm (?2) and the Realtime
implementation (?3). We then describe Trans-
Center (?4) and the results of an end-to-end post-
editing experiment with human translators (?5).
All data collected as part of this validation experi-
ment is also publicly available.
2 Adaptive Machine Translation
Traditional machine translation systems operate in
batch mode: statistical translation models are es-
timated from large volumes of sentence-parallel
bilingual text and then used to translate new text.
Incorporating new data requires a full system re-
build, an expensive operation taking up to days of
time. As such, MT systems in production scenar-
ios typically remain static for large periods of time
(months or even indefinitely). Recently, an adap-
tive MT paradigm has been introduced specifi-
cally for post-editing (Denkowski et al., 2014).
Three major MT system components are extended
to support online updates, allowing human post-
editor feedback to be immediately incorporated:
? An online translation model is updated to in-
clude new translations extracted from post-
editing data.
? A dynamic language model is updated to in-
clude post-edited target language text.
? An online update is made to the system?s
feature weights after each sentence is post-
edited.
72
These extensions allow the MT system to gener-
ate improved translations that require significantly
less effort to correct for later sentences in the doc-
ument. This paradigm is now implemented in
the freely available cdec (Dyer et al., 2010) ma-
chine translation toolkit as Realtime, part of the
pycdec (Chahuneau et al., 2012) Python API.
Standard MT systems use aggregate statistics
from all training text to learn a single large
translation grammar (in the case of cdec?s hi-
erarchical phrase-based model (Chiang, 2007), a
synchronous context-free grammar) consisting of
rules annotated with feature scores. As an alter-
native, the bitext can be indexed using a suffix ar-
ray (Lopez, 2008), a data structure allowing fast
source-side lookups. When a new sentence is to be
translated, training sentences that share spans of
text with the input sentence are sampled from the
suffix array. Statistics from the sample are used to
learn a small, sentence-specific grammar on-the-
fly. The adaptive paradigm extends this approach
to support online updates by also indexing the
new bilingual sentences generated as a post-editor
works. When a new sentence is translated, match-
ing sentences are sampled from the post-editing
data as well as the suffix array. All feature scores
that can be computed on a suffix array sample can
be identically computed on the combined sample,
allowing uniform handling of all data. An addi-
tional ?post-edit support? feature is included that
indicates whether a grammar rule was extracted
from the post-editing data. This allows an opti-
mizer to learn to prefer translations that originate
from human feedback. This adaptation approach
also serves as a platform for exploring expanded
post-editing-aware feature sets; any feature that
can be computed from standard text can be added
to the model and will automatically include post-
editing data. Implementationally, feature scoring
is broken out into a single Python source file con-
taining a single function for each feature score.
New feature functions can be added easily.
The adaptive paradigm uses two language mod-
els. A standard (static) n-gram language model es-
timated on large monolingual text allows the sys-
tem to prefer translations more similar to human-
generated text in the target language. A (dy-
namic) Bayesian n-gram language model (Teh,
2006) can be updated with observations of the
post-edited output in a straightforward way. This
smaller model exactly covers the training bitext
and all post-editing data, letting the system up-
weight translations with newly learned vocabu-
lary and phrasing absent in the large monolingual
text. Finally, the margin-infused relaxed algorithm
(MIRA) (Crammer et al., 2006; Eidelman, 2012)
is used to make an online parameter update after
each sentence is post-edited, minimizing model er-
ror. This allows the system to continuously rescale
weights for translation and language model fea-
tures that adapt over time.
Since true post-editing data is infeasible to col-
lect during system development and internal test-
ing, as standard MT pipelines require tens of thou-
sands of sentences to be translated with low la-
tency, a simulated post-editing paradigm (Hardt
and Elming, 2010) can be used, wherein pre-
generated reference translations act as a stand-in
for actual post-editing. This approximation is ef-
fective for tuning and internal evaluation when
real post-editing data is unavailable. In simulated
post-editing tasks, decoding (for both the test cor-
pus and each pass over the development corpus
during optimization) begins with baseline mod-
els trained on standard bilingual and monolingual
text. After each sentence is translated, the fol-
lowing take place in order: First, MIRA uses the
new source?reference pair to update weights for
the current models. Second, the source is aligned
to the reference using word-alignment models
learned from the initial data and used to update the
translation grammar. Third, the reference is added
to the Bayesian language model. As sentences are
translated, the models gain valuable context infor-
mation, allowing them to adapt to the specific tar-
get document and translator. Context is reset at the
start of each development or test corpus. Systems
optimized with simulated post-editing can then be
deployed to serve real human translators without
further modification.
3 cdec Realtime
Now included as part of the free, open source
cdec machine translation toolkit (Dyer et al.,
2010), Realtime
1
provides an efficient implemen-
tation of the adaptive MT paradigm that can serve
an arbitrary number of unique post-editors concur-
rently. A full Realtime tutorial, including step-
by-step instructions for installing required soft-
ware and building full adaptive systems, is avail-
1
https://github.com/redpony/cdec/tree/
master/realtime
73
import rt
# Start new Realtime translator using a Spanish--English
# system and automatic, language-independent text normalization
# (pre-tokenization and post-detokenization)
translator = rt.RealtimeTranslator(?es-en.d?, tmpdir=?/tmp?, cache_size=5,
norm=True)
# Translate a sentence for user1
translation = translator.translate(?Muchas gracias Chris.?, ctx_name=?user1?)
# Learn from user1?s post-edited transaltion
translator.learn(?Muchas gracias Chris.?, ?Thank you so much, Chris.?,
ctx_name=?user1?)
# Save, free, and reload state for user1
translator.save_state(file_or_stringio=?user1.state?, ctx_name=?user1?)
translator.drop_ctx(ctx_name=?user1?)
translator.load_state(file_or_stringio=?user1.state?, ctx_name=?user1?)
Figure 1: Sample code using the Realtime Python API to translate and learn from post-editing.
able online.
2
Building an adaptive system begins
with the usual MT pipeline steps: word alignment,
bitext indexing (for suffix array grammar extrac-
tion), and standard n-gram language model esti-
mation. Additionally, the cpyp
3
package, also
freely available, is used to estimate a Bayesian
n-gram language model on the target side of the
bitext. The cdec grammar extractor and dy-
namic language model implementations both in-
clude support for efficient inclusion of incremental
data, allowing optimization with simulated post-
editing to be parallelized. The resulting system,
optimized for post-editing, is then ready for de-
ployment with Realtime.
At runtime, a Realtime system operates as fol-
lows. A single instance of the indexed bitext is
loaded into memory for grammar extraction. Sin-
gle instances of the directional word alignment
models are loaded into memory for force-aligning
post-edited data. When a new user requests a
translation, a new context is started. The follow-
ing are loaded into memory: a table of all post-
edited data from the user, a user-specific dynamic
language model, and a user-specific decoder (in
this case an instance of MIRA that has a user-
specific decoder and set of weights). Each user
also requires an instance of the large static lan-
guage model, though all users effectively share a
single instance through the memory mapped im-
plementation of KenLM (Heafield, 2011). When a
2
http://www.cs.cmu.edu/
?
mdenkows/
cdec-realtime.html
3
https://github.com/redpony/cpyp
new sentence is to be translated, the grammar ex-
tractor samples from the shared background data
plus the user-specific post-editing data to generate
a sentence-specific grammar incorporating data
from all prior sentences translated by the same
user. The sentence is then decoded using the user
and time-specific grammar, current weights, and
current dynamic language model. When a post-
edited sentence is available as feedback, the fol-
lowing happen in order: (1) the source-reference
pair is used to update feature weights with MIRA,
(2) the source-reference pair is force-aligned and
added to the indexed post-editing data, and (3) the
dynamic language model is updated with the ref-
erence. User state (current weights and indexed
post-edited data for grammars and the language
model) can be saved and loaded, allowing mod-
els to be loaded and freed from memory as trans-
lators start and stop their work. Figure 1 shows
a minimal example of the above using the Real-
time package. While this paper describes integra-
tion with TransCenter, a tool primarily targeting
data collection and analysis, the Realtime Python
API allows straightforward integration with other
computer-assisted translation tools such as full-
featured translation workbench environments.
4 TransCenter: Web-Based Translation
Research Suite
The TransCenter software (Denkowski and Lavie,
2012) dramatically lowers barriers in post-editing
data collection and increases the accuracy and de-
scriptiveness of the collected data. TransCenter
74
Figure 2: Example of editing and rating machine translations with the TransCenter web interface.
Figure 3: Example TransCenter summary report for a single user on a document.
provides a web-based translation editing interface
that remotely monitors and records user activity.
The ?live? version
4
now uses cdec Realtime to
provide on-demand MT that automatically learns
from post-editor feedback. Translators use a web
browser to access a familiar two-column editing
environment (shown in Figure 2) from any com-
puter with an Internet connection. The left column
displays the source sentences, while the right col-
umn, initially empty, is incrementally populated
with translations from the Realtime system as the
user works. For each sentence, the translator ed-
its the MT output to be grammatically correct and
convey the same information as the source sen-
tence. During editing, all user actions (key presses
and mouse clicks) are logged so that the full edit-
ing process can be replayed and analyzed. After
editing, the final translation is reported to the Re-
altime system for learning and the next transla-
tion is generated. The user is additionally asked
to rate the amount of work required to post-edit
each sentence immediately after completing it,
yielding maximally accurate feedback. The rating
scale ranges from 5 (no post-editing required) to
1 (requires total re-translation). TransCenter also
records the number of seconds each sentence is
focused, allowing for exact timing measurements.
A pause button is available if the translator needs
to take breaks. TransCenter can generate reports
4
https://github.com/mjdenkowski/
transcenter-live
of translator effort as measured by (1) keystroke,
(2) exact timing, and (3) actual translator post-
assessment. Final translations are also available
for calculating edit distance. Millisecond-level
timing of all user actions further facilitates time
sequence analysis of user actions and pauses. Fig-
ure 3 shows an example summary report gener-
ated by TransCenter showing a user?s activity on
each sentence in a document. This information
is also output in a simple comma-separated value
format for maximum interoperability with other
standards-compliant tools.
TransCenter automatically handles resource
management with Realtime. When a TransCenter
server is started, it loads a Realtime system with
zero contexts into memory. As users log in to work
on documents, new contexts are created to deliver
on-demand translations. As users finish work-
ing or take extended breaks, contexts automati-
cally time out and resources are freed. Translator
and document-specific state is automatically saved
when contexts time out and reloaded when transla-
tors resume work with built-in safeguards against
missing or duplicating any post-editing data due
to timeouts or Internet connectivity issues. This
allows any number of translators to work on trans-
lation tasks at their convenience.
5 Experiments
In a preliminary experiment to evaluate the impact
of adaptive MT in real-world post-editing scenar-
75
HTER Rating
Baseline 19.26 4.19
Adaptive 17.01 4.31
Table 1: Aggregate HTER scores and average
translator self-ratings (5 point scale) of post-
editing effort for translations of TED talks from
Spanish into English.
ios, we compare a static Spanish?English MT sys-
tem to a comparable adaptive system on a blind
out-of-domain test. Competitive with the current
state-of-the-art, both systems are trained on the
2012 NAACL WMT (Callison-Burch et al., 2012)
constrained resources (2 million bilingual sen-
tences) using the cdec toolkit (Dyer et al., 2010).
Blind post-editing evaluation sets are drawn from
the Web Inventory of Transcribed and Translated
Talks (WIT
3
) corpus (Cettolo et al., 2012) that
makes transcriptions of TED talks
5
available in
several languages, including English and Spanish.
We select 4 excerpts from Spanish talk transcripts
(totaling 100 sentences) to be translated into En-
glish. Five students training to be professional
translators post-edit machine translations of these
excerpts using TransCenter. Translations are pro-
vided by either the static or fully adaptive system.
Tasks are divided such that each user translates
2 excerpts with the static system and 2 with the
adaptive system and each excerpt is post-edited ei-
ther 2 or 3 times with each system. Users do not
know which system is providing the translations.
Using the data collected by TransCenter, we
evaluate post-editing effort with the established
human-targeted translation edit rate (HTER) met-
ric (Snover et al., 2006). HTER computes an
edit distance score between initial MT outputs and
the ?targeted? references created by human post-
editing, with lower scores being better. Results
for the two systems are aggregated over all users
and documents. Shown in Table 1, introducing
an adaptive MT system results in a significant re-
duction in editing effort. We additionally aver-
age the user post-ratings for each translation by
system to evaluate user perception of the adap-
tive system compared to the static baseline. Also
shown in Table 1, we see a slight preference for
the adaptive system. This data, as well as precise
keystroke, mouse click, and timing information is
5
http://www.ted.com/talks
made freely available for further analysis.
6
Trans-
Center records all data necessary for more sophis-
ticated editing time analysis (Koehn, 2012) as well
as analysis of translator behavior, including pauses
(used as an indicator of cognitive effort) (Lacruz et
al., 2012).
6 Related Work
There has been a recent push for new computer-
aided translation (CAT) tools that leverage adap-
tive machine translation. The CASMACAT
7
project (Alabau et al., 2013) focuses on building
state-of-the-art tools for computer-aided transla-
tion. This includes translation predictions backed
by machine translation systems that incrementally
update model parameters as users edit translations
(Mart??nez-G?omez et al., 2012; L?opez-Salcedo et
al., 2012). The MateCat
8
project (Cattelan, 2013)
specifically aims to integrate machine translation
(including online model adaptation and translation
quality estimation) into a web-based CAT tool.
Bertoldi et al. (2013) show improvements in trans-
lator productivity when using the MateCat tool
with an adaptive MT system that uses cache-based
translation and language models.
7 Conclusion
This paper describes the free, open source MT
post-editing setup provided by cdec Realtime
and TransCenter. All software and the data col-
lected for a preliminary post-editing experiment
are all freely available online. A live demon-
stration of adaptive MT post-editing powered by
Realtime and TransCenter is scheduled for the
2014 EACL Workshop on Humans and Computer-
assisted Translation (HaCaT 2014).
Acknowledgements
This work is supported in part by the National Sci-
ence Foundation under grant IIS-0915327, by the
Qatar National Research Fund (a member of the
Qatar Foundation) under grant NPRP 09-1140-1-
177, and by the NSF-sponsored XSEDE program
under grant TG-CCR110017.
6
www.cs.cmu.edu/
?
mdenkows/
transcenter-round1.tar.gz
7
http://casmacat.eu/
8
http://www.matecat.com/
76
References
Vicent Alabau, Ragnar Bonk, Christian Buck, Michael
Carl, Francisco Casacuberta, Mercedes Garc??a-
Mart??nez, Jes?us Gonz?alez-Rubio, Philipp Koehn,
Luis A. Leiva, Bartolom?e Mesa-Lao, Daniel Ortiz-
Mart??nez, Herv?e Saint-Amand, Germ?an Sanchis-
Trilles, and Chara Tsoukala. 2013. Casmacat:
An open source workbench for advanced computer
aided translation. In The Prague Bulletin of Mathe-
matical Linguistics, pages 101?112.
Nicola Bertoldi, Mauro Cettolo, and Marcello Fed-
erico. 2013. Cache-based online adaptation for ma-
chine translation enhanced computer assisted trans-
lation. In Proceedings of the XIV Machine Transla-
tion Summit, pages 35?42.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Alessandro Cattelan. 2013. Second version of Mate-
Cat tool. Deliverable 4.2.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit
3
: Web inventory of transcribed
and translated talks. In Proceedings of the Sixteenth
Annual Conference of the European Association for
Machine Translation.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2012. pycdec: A python interface to cdec. The
Prague Bulletin of Mathematical Linguistics, 98:51?
61.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, pages 551?558, March.
Michael Denkowski and Alon Lavie. 2012. Trans-
Center: Web-based translation research suite. In
AMTA 2012 Workshop on Post-Editing Technology
and Practice Demo Session.
Michael Denkowski, Chris Dyer, and Alon Lavie.
2014. Learning from post-editing: Online model
adaptation for statistical machine translation. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 480?489, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Daniel Hardt and Jakob Elming. 2010. Incremental
re-training for post-editing smt. In Proceedings of
the Ninth Conference of the Association for Machine
Translation in the Americas.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, United Kingdom,
July.
Philipp Koehn. 2012. Computer-aided translation.
Machine Translation Marathon.
Isabel Lacruz, Gregory M. Shreve, and Erik Angelone.
2012. Average Pause Ratio as an Indicator of Cogni-
tive Effort in Post-Editing: A Case Study. In AMTA
2012 Workshop on Post-Editing Technology and
Practice (WPTP 2012), pages 21?30, San Diego,
USA, October. Association for Machine Translation
in the Americas (AMTA).
Adam Lopez. 2008. Machine translation by pattern
matching. In Dissertation, University of Maryland,
March.
Francisco-Javier L?opez-Salcedo, Germ?an Sanchis-
Trilles, and Francisco Casacuberta. 2012. On-
line learning of log-linear weights in interactive ma-
chine translation. Advances in Speech and Lan-
guage Technologies for Iberian Languages, pages
277?286.
Pascual Mart??nez-G?omez, Germ?an Sanchis-Trilles, and
Francisco Casacuberta. 2012. Online adaptation
strategies for statistical machine translation in post-
editing scenarios. Pattern Recognition, 45:3193?
3203.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In Proceedings of the 7th Conference of the.
Association for Machine Translation of the Ameri-
cas, pages 223?231.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proc. of ACL.
77
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141?150,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Weakly-Supervised Bayesian Learning of a CCG Supertagger
Dan Garrette
?
Chris Dyer
?
Jason Baldridge
?
Noah A. Smith
?
?
Department of Computer Science, The University of Texas at Austin
?
School of Computer Science, Carnegie Mellon University
?
Department of Linguistics, The University of Texas at Austin
?
Corresponding author: dhg@cs.utexas.edu
Abstract
We present a Bayesian formulation for
weakly-supervised learning of a Combina-
tory Categorial Grammar (CCG) supertag-
ger with an HMM. We assume supervi-
sion in the form of a tag dictionary, and
our prior encourages the use of cross-
linguistically common category structures
as well as transitions between tags that
can combine locally according to CCG?s
combinators. Our prior is theoretically ap-
pealing since it is motivated by language-
independent, universal properties of the
CCG formalism. Empirically, we show
that it yields substantial improvements
over previous work that used similar bi-
ases to initialize an EM-based learner. Ad-
ditional gains are obtained by further shap-
ing the prior with corpus-specific informa-
tion that is extracted automatically from
raw text and a tag dictionary.
1 Introduction
Unsupervised part-of-speech (POS) induction is a
classic problem in NLP. Many proposed solutions
are based on Hidden Markov models (HMMs), with
various improvements obtainable through: induc-
tive bias in the form of tag dictionaries (Kupiec,
1992; Merialdo, 1994), sparsity constraints (Lee
et al., 2010), careful initialization of parameters
(Goldberg et al., 2008), feature based represen-
tations (Berg-Kirkpatrick et al., 2010; Smith and
Eisner, 2005), and priors on model parameters
(Johnson, 2007; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011, inter alia).
When tag dictionaries are available, a situa-
tion we will call type-supervision, POS induc-
tion from unlabeled corpora can be relatively suc-
cessful; however, as the number of possible tags
increases, performance drops (Ravi and Knight,
2009). In such cases, there are a large number
of possible labels for each token, so picking the
right one simply by chance is unlikely; the pa-
rameter space tends to be large; and devising good
initial parameters is difficult. Therefore, it is un-
surprising that the unsupervised (or even weakly-
supervised) learning of a Combinatory Categorial
Grammar (CCG) supertagger, which labels each
word with one of a large (possibly unbounded)
number of structured categories called supertags,
is a considerable challenge.
Despite the apparent complexity of the task, su-
pertag sequences have regularities due to univer-
sal properties of the CCG formalism (?2) that can
be used to reduce the complexity of the problem;
previous work showed promising results by using
these regularities to initialize an HMM that is then
refined with EM (Baldridge, 2008). Here, we ex-
ploit CCG?s category structure to motivate a novel
prior over HMM parameters for use in Bayesian
learning (?3). This prior encourages (i) cross-
linguistically common tag types, (ii) tag bigrams
that can combine using CCG?s combinators, and
(iii) sparse transition distributions. We also go be-
yond the use of these universals to show how ad-
ditional, corpus-specific information can be auto-
matically extracted from a combination of the tag
dictionary and raw data, and how that information
can be combined with the universal knowledge for
integration into the model to improve the prior.
We use a blocked sampling algorithm to sam-
ple supertag sequences for the sentences in the
training data, proportional to their posterior prob-
ability (?4). We experimentally verify that
our Bayesian formulation is effective and sub-
stantially outperforms the state-of-the-art base-
line initialization/EM strategy in several languages
(?5). We also evaluate using tag dictionaries that
are unpruned and have only partial word coverage,
finding even greater improvements in these more
realistic scenarios.
141
2 CCG and Supertagging
CCG (Steedman, 2000; Steedman and Baldridge,
2011) is a grammar formalism in which each lex-
ical token is associated with a structured category,
often referred to as a supertag. CCG categories are
defined by the following recursive definition:
C ? {S, N, NP, PP, ...}
C ? {C/C,C\C}
A CCG category can either be an atomic cate-
gory indicating a particular type of basic gram-
matical phrase (S for a sentence, N for a noun,
NP for a noun phrase, etc), or a complex category
formed from the combination of two categories
by one of two slash operators. In CCG, complex
categories indicate a grammatical relationship be-
tween the two operands. For example, the cate-
gory (S\NP)/NP might describe a transitive verb,
looking first to its right (indicated by /) for an ob-
ject, then to its left (\) for a subject, to produce a
sentence. Further, atomic categories may be aug-
mented with features, such as S
dcl
, to restrict the
set of atoms with which they may unify. The task
of assigning a category to each word in a text is
called supertagging (Bangalore and Joshi, 1999).
Because they are recursively defined, there is
an infinite number of potential CCG categories
(though in practice it is limited by the number
of actual grammatical contexts). As a result, the
number of supertags appearing in a corpus far ex-
ceeds the number of POS tags (see Table 1). Since
supertags specify the grammatical context of a to-
ken, and high frequency words appear in many
contexts, CCG grammars tend to have very high
lexical ambiguity, with frequent word types asso-
ciating with a large number of categories. This
ambiguity has made type-supervised supertagger
learning very difficult because the typical ap-
proaches to initializing parameters for EM become
much less effective.
Grammar-informed supertagger learning.
Baldridge (2008) was successful in extending the
standard type-supervised tagger learning to the
task of CCG supertagging by setting the initial
parameters for EM training of an HMM using
two intrinsic properties of the CCG formalism:
the tendency for adjacent tags to combine, and
the tendency to use less complex tags. These
properties are explained in detail in the original
work, but we restate the ideas briefly throughout
this paper for completeness.
X/Y Y ? X (>)
Y X\Y ? X (<)
X/Y Y/Z ? X/Z (>B)
Y \Z X\Y ? X\Z (<B)
Y/Z X\Y ? X/Z (<B
?
)
Figure 1: Combination rules used by CCGBank.
S
NP
NP/N
N
S\NP
(S\NP)/NP
NP
NP/N
N
The
man
walks
a
dog
Figure 2: CCG parse for ?The man walks a dog.?
Tag combinability. A CCG parse of a sentence is
derived by recursively combining the categories of
sub-phrases. Category combination is performed
using only a small set of generic rules (see Fig-
ure 1). In the tree in Figure 2, we can see that
a and dog can combine via Forward Application
(>), with NP/N and N combining to produce NP.
The associativity engendered by CCG?s compo-
sition rules means that most adjacent lexical cate-
gories may be combined. In the Figure 2 tree, we
can see that instead of combining (walks?(a?dog)),
we could have combined ((walks?a)?dog) since
(S\NP)/NP and NP/N can combine using >B.
3 Model
In this section we define the generative process
we use to model a corpus of sentences. We begin
by generating the model parameters: for each
supertag type t in the tag set T , the transition
probabilities to the next state (pi
t
) and the emis-
sion probabilities (?
t
) are generated by draws
from Dirichlet distributions parameterized with
per-tag mean distributions (pi
0
t
and ?
0
t
, respec-
tively) and concentration parameters (?
pi
and
?
?
). By setting ?
pi
close to zero, we can encode
our prior expectation that transition distributions
should be relatively peaked (i.e., that each tag
type should be followed by relatively few tag
types). The prior means, discussed below, encode
both linguistic intuitions about expected tag-tag
transition behavior and automatically-extracted
corpus information. Given these parameters, we
next generate the sentences of the corpus. This
process is summarized as follows:
142
Parameters:
?
t
? Dirichlet(?
?
, ?
0
t
) ?t ? T
pi
t
? Dirichlet(?
pi
, pi
0
t
) ?t ? T
Sentence:
y
1
? Categorical(pi
?S?
)
for i ? {1, 2, . . .}, until y
i
= ?E?
x
i
| y
i
? Categorical(?
y
i
)
y
i+1
| y
i
? Categorical(pi
y
i
)
This model can be understood as a Bayesian
HMM (Goldwater and Griffiths, 2007). We next
discuss how the prior distributions are constructed
to build in additional inductive bias.
3.1 Transition Prior Means (pi
0
t
)
We use the prior mean for each tag?s transition dis-
tribution to build in two kinds of bias. First, we
want to favor linguistically probable tags. Second,
we want to favor transitions that result in a tag
pair that combines according to CCG?s combina-
tors. For simplicity, we will define pi
0
t
as a mixture
of two components, the first, P
pi
(u) is an (uncon-
ditional) distribution over category types u that fa-
vors cross-linguistically probable categories. The
second component, P
pi
(u | t), conditions on the
previous tag type, t, and assigns higher probabil-
ity to pairs of tags that can be combined. That is,
the probability of transitioning from t to u in the
Dirichlet mean distribution is given by
1
pi
0
t
(u) = ? ? P
pi
(u) + (1? ?) ? P
pi
(u | t).
We discuss the two mixture components in turn.
3.1.1 Unigram Category Generator (P
pi
(u))
In this section, we define a CCG category gener-
ator that generates cross-linguistically likely cat-
egory types. Baldridge?s approach estimated the
likelihood of a category using the inverse number
of sub-categories: P
CPLX
(u) ? 1/complexity(u).
We propose an improvement, P
G
, expressed as a
probabilistic grammar:
2
C ? a p
term
?p
atom
(a)
C ? A/A p
term
?p
fw
?p
mod
?P
G
(A)
C ? A/B, A 6=B p
term
?p
fw
?p
mod
?P
G
(A) ?P
G
(B)
C ? A\A p
term
?p
fw
?p
mod
?P
G
(A)
C ? A\B, A 6=B p
term
?p
fw
?p
mod
?P
G
(A) ?P
G
(B)
1
Following Baldridge (2008), we fix ? = 0.5 for our ex-
periments.
2
For readability, we use the notation p = (1? p).
where A,B,C are categories and a is an atomic
category (and terminal): a ? {S, N, NP, ...}.
3
We have designed this grammar to capture sev-
eral important CCG characteristics. In particular
we encode four main ideas, each captured through
a different parameter of the grammar and dis-
cussed in greater detail below:
1. Simpler categories are more likely: e.g. N/N is
a priori more likely than (N/N)/(N/N).
2. Some atoms are more likely than others: e.g.
NP is more likely than S, much more than NP
nb
.
3. Modifiers are more likely: e.g. (S\NP)/(S\NP)
is more likely than (S\NP)/(NP\NP).
4. Operators occur with different frequencies.
The first idea subsumes the complexity measure
used by Baldridge, but accomplishes the goal nat-
urally by letting the probabilities decrease as the
category grows. The rate of decay is governed
by the p
term
parameter: the marginal probability
of generating a terminal (atomic) category in each
expansion. A higher p
term
means a stronger em-
phasis on simplicity. The probability distribution
over categories is guaranteed to be proper so long
as p
term
>
1
2
since the probability of the depth of a
tree will decrease geometrically (Chi, 1999).
The second idea is a natural extension of the
complexity concept and is particularly relevant
when features are used. The original complex-
ity measure treated all atoms uniformly, but e.g.
we would expect NP
expl
/N to be less likely than
NP/N since it contains the more specialized, and
thus rarer, atom NP
expl
. We define the distribution
p
atom
(a) as the prior over atomic categories.
Due to our weak, type-only supervision, we
have to estimate p
atom
from just the tag dictionary
and raw corpus, without frequency data. Our goal
is to estimate the number of each atom in the su-
pertags that should appear on the raw corpus to-
kens. Since we don?t know what the correct su-
pertags are, we first estimate counts of supertags,
from which we can extract estimated atom counts.
Our strategy is to uniformly distribute each raw
corpus token?s counts over all of its possible su-
pertags, as specified in the tag dictionary. Word
types not appearing in the tag dictionary are ig-
3
While very similar to standard probabilistic context-free
grammars seen in NLP work, this grammar is not context-free
because modifier categories must have matching operands.
However, this is not a problem for our approach since the
grammar is unambiguous, defines a proper probability distri-
bution, and is only used for modeling the relative likelihoods
of categories (not parsing categories).
143
nored for the purposes of these estimates. Assum-
ing that C(w) is the number of times that word
type w is seen in the raw corpus, atoms(a, t) is the
number of times atom a appears in t, TD(w) is the
set of tags associated with w, and TD(t) is the set
of word types associated with t:
C
supertag
(t) =
?
w?TD(t)
(C(w)+?)/|TD(w)|
C
atom
(a) =
?
t?T
atoms(a, t) ? C
supertag
(t)
p
atom
(a) ? C
atom
(a) + ?
Adding ? smooths the estimates.
Using the raw corpus and tag dictionary data to
set p
atom
allows us to move beyond Baldridge?s
work in another direction: it provides us with a
natural way to combine CCG?s universal assump-
tions with corpus-specific data.
The third and fourth ideas pertain only to com-
plex categories. If the category is complex, then
we consider two additional parameters. The pa-
rameter p
fw
is the marginal probability that the
complex category?s operator specifies a forward
argument. The parameter p
mod
gives the amount
of marginal probability mass that is allocated for
modifier categories. Note that it is not necessary
for p
mod
to be greater than
1
2
to achieve the de-
sired result of making modifier categories more
likely than non-modifier categories: the number
of potential modifiers make up only a tiny fraction
of the space of possible categories, so allocating
more than that mass as p
mod
will result in a cate-
gory grammar that gives disproportionate weight
to modifiers, increasing the likelihood of any par-
ticular modifier from what it would otherwise be.
3.1.2 Bigram Category Generator (P
pi
(u | t))
While the above processes encode important prop-
erties of the distribution over categories, the in-
ternal structure of categories is not the full story:
cross-linguistically, the categories of adjacent to-
kens are much more likely to be combinable via
some CCG rule. This is the second component of
our mixture model.
Baldridge derives this bias by allocating the ma-
jority of the transition probability mass from each
tag t to tags that can follow t according to some
combination rule. Let ?(t,u) be an indicator of
whether t connects to u; for ? ? [0, 1]:
4
P
?
(u | t) =
{
? ? uniform(u) if ?(t,u)
(1? ?) ? uniform(u) otherwise
4
Again, following Baldridge (2008), we fix ? = 0.95 for
our experiments.
There are a few additional considerations that
must be made in defining ?, however. In assum-
ing the special tags ?S? and ?E? for the start and
end of the sentence, respectively, we can define
?(?S?,u) = 1 when u seeks no left-side argu-
ments (since there are no tags to the left with
which to combine) and ?(t, ?E?) = 1 when t seeks
no right-side arguments. So ?(?S?, NP/N) = 1, but
?(?S?, S\NP) = 0. If atoms have features asso-
ciated, then the atoms are allowed to unify if the
features match, or if at least one of them does
not have a feature. So ?(NP
nb
, S\NP) = 1, but
?(NP
nb
, S\NP
conj
) = 0. In defining ?, it is also im-
portant to ignore possible arguments on the wrong
side of the combination since they can be con-
sumed without affecting the connection between
the two. To achieve this for ?(t,u), it is assumed
that it is possible to consume all preceding argu-
ments of t and all following arguments of u. So
?(NP, (S\NP)/NP) = 1. This helps to ensure the
associativity discussed earlier. Finally, the atom
NP is allowed to unify with N if N is the argument.
So ?(N, S\NP) = 1, but ?(NP/N, NP) = 0. This is
due to the fact that CCGBank assumes that N can
be rewritten as NP.
Type-supervised initialization. As above, we
want to improve upon Baldridge?s ideas by en-
coding not just universal CCG knowledge, but
also automatically-induced corpus-specific infor-
mation where possible. To that end, we can de-
fine a conditional distribution P
tr
(u | t) based on
statistics from the raw corpus and tag dictionary.
We use the same approach as we did above for set-
ting p
atom
(and the definition of ?
0
t
below): we esti-
mate by evenly distributing raw corpus counts over
the tag dictionary entries. Assume that C(w
1
, w
2
)
is the (?-smoothed) count of times word type w
1
was directly followed byw
2
in the raw corpus, and
ignoring any words not found in the tag dictionary:
C(t,u) = ?+
?
w
1
?TD(t), w
2
?TD(u)
C(w
1
, w
2
)
|TD(w
1
)| ? |TD(w
2
)|
P
tr
(u | t) = C(t,u)/
?
u
?
C(t,u
?
)
Then the alternative definition of the compatibility
distribution is as follows:
P
tr
?
(u | t) =
{
? ? P
tr
(u | t) if ?(t,u)
(1??) ? P
tr
(u | t) otherwise
144
Our experiments compare performance when
pi
0
t
is set using P
pi
(u)=P
CPLX
(experiment 3) ver-
sus our category grammar P
G
(4?6), and using
P
pi
(u | t) = P
?
as the compatibility distribution
(3?4) versus P
tr
?
(5?6).
3.2 Emission Prior Means (?
0
t
)
For each supertag type t, ?
0
t
is the mean distri-
bution over words it emits. While Baldridge?s
approach used a uniform emission initialization,
treating all words as equally likely, we can,
again, induce token-level corpus-specific informa-
tion:
5
To set ?
0
t
, we use a variant and simplifica-
tion of the procedure introduced by Garrette and
Baldridge (2012) that takes advantage of our prior
over categories P
G
.
Assuming that C(w) is the count of word type
w in the raw corpus, TD(w) is the set of supertags
associated with word type w in the tag dictionary,
and TD(t) is the set of known word types associ-
ated with supertag t, the count of word/tag pairs
for known words (words appearing in the tag dic-
tionary) is estimated by uniformly distributing a
word?s (?-smoothed) raw counts over its tag dic-
tionary entries:
C
known
(t, w) =
{
C(w)+?
|TD(w)|
if t ? TD(w)
0 otherwise
For unknown words, we first use the idea of tag
?openness? to estimate the likelihood of a partic-
ular tag t applying to an unknown word: if a tag
applies to many word types, it is likely to apply to
some new word type.
P (unk | t) ? |known words w s.t. t ? TD(w)|
Then, we apply Bayes? rule to get P (t | unk), and
use that to estimate word/tag counts for unknown
words:
P (t | unk) ? P (unk | t) ? P
G
(t)
C
unk
(t, w) = C(w) ? P (t | unk)
Thus, with the estimated counts for all words:
P
em
(w | t) =
C
known
(t, w) + C
unk
(t, w)
?
w
?
C
known
(t, w
?
) + C
unk
(t, w
?
)
We perform experiments comparing perfor-
mance when ?
0
t
is uniform (3?5) and when
?
0
t
(w) = P
em
(w | t) (6).
5
Again, without gold tag frequencies.
4 Posterior Inference
We wish to find the most likely supertag of each
word, given the model we just described and a cor-
pus of training data. Since there is exact inference
with these models is intractable, we resort to Gibbs
sampling to find an approximate solution. At a
high level, we alternate between resampling model
parameters (?
t
, pi
t
) given the current tag sequence
and resampling tag sequences given the current
model parameters and observed word sequences.
It is possible to sample a new tagging from the
posterior distribution over tag sequences for a sen-
tence, given the sentence and the HMM parameters
using the forward-filter backward-sample (FFBS)
algorithm (Carter and Kohn, 1996). To effi-
ciently sample new HMM parameters, we exploit
Dirichlet-multinomial conjugacy. By repeating
these alternating steps and accumulating the num-
ber of times each supertag is used in each position,
we obtain an approximation of the required poste-
rior quantities.
Our inference procedure takes as input the tran-
sition prior means pi
0
t
, the emission prior means
?
0
t
, and concentration parameters ?
pi
and ?
?
,
along with the raw corpus and tag dictionary. The
set of supertags associated with a word w will be
known as TD(w). We will refer to the set of word
types included in the tag dictionary as ?known?
words and others as ?unknown? words. For sim-
plicity, we will assume that TD(w), for any un-
known word w, is the full set of CCG categories.
During sampling, we always restrict the possible
tag choices for a word w to the categories found in
TD(w). We refer to the sequence of word tokens
as x and tags as y.
We initialize the sampler by setting pi
t
= pi
0
t
and ?
t
= ?
0
t
and then sampling tagging sequences
using FFBS.
To sample a tagging for a sentence x, the strat-
egy is to inductively compute, for each token x
i
starting with i = 0 and going ?forward?, the prob-
ability of generating x
0
, x
1
, . . . , x
i
via any tag se-
quence that ends with y
i
= u:
p(y
i
= u | x
0:i
) =
?
u
(x
i
) ?
?
t?T
pi
t
(u) ? p(y
i?1
= t | x
0:i?1
)
We then pass through the sequence again, this time
?backward? starting at i = |x| ? 1 and sampling
y
i
| y
i+1
? p(y
i
= t | x
0:i
) ? pi
t
(y
i+1
).
145
num. raw TD TD ambiguity dev test
Corpus tags tokens tokens entries type token tokens tokens
English
CCGBank POS 50
158k 735k
45k 3.75 13.11 ? ?
CCGBank 1,171 65k 56.98 296.18 128k 127k
Chinese CTB-CCG 829 99k 439k 60k 96.58 323.37 59k 85k
Italian CCG-TUT 955 6k 27k 9k 178.88 426.13 5k 5k
Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT
is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates
are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English
POS statistics are shown only for comparison; only CCG experiments were run.
The block-sampling approach of choosing new
tags for a sentence all at once is particularly ben-
eficial given the sequential nature of the model of
the HMM. In an HMM, a token?s adjacent tags tend
to hold onto its current tag due to the relation-
ships between the three. Resampling all tags at
once allows for more drastic changes at each it-
eration, providing better opportunities for mixing
during inference. The FFBS approach has the ad-
ditional advantage that, by resampling the distri-
butions only once per iteration, we are able to re-
sample all sentences in parallel. This is not strictly
true of all HMM problems with FFBS, but because
our data is divided by sentence, and each sentence
has a known start and end tag, the tags chosen dur-
ing the sampling of one sentence cannot affect the
sampling of another sentence in the same iteration.
Once we have sampled tags for the entire cor-
pus, we resample pi and ?. The newly-sampled
tags y are used to compute C(w, t), the count of
tokens with word type w and tag t, and C(t,u),
the number of times tag t is directly followed by
tag u. We then sample, for each t ? T where T is
the full set of valid CCG categories:
pi
t
? Dir
(
??
pi
? pi
0
t
(u) + C(t,u)?
u?T
)
?
t
? Dir
(
??
?
? ?
0
t
(w) + C(w, t)?
w?V
)
It is important to note that this method of re-
sampling allows the draws to incorporate both the
data, in the form of counts, and the prior mean,
which includes all of our carefully-constructed bi-
ases derived from both the intrinsic, universal CCG
properties as well as the information we induced
from the raw corpus and tag dictionary.
With the distributions resampled, we can con-
tinue the procedure by resampling tags as above,
and then resampling distributions again, until a
maximum number of iterations is reached.
5 Experiments
6
To evaluate our approach, we used CCGBank
(Hockenmaier and Steedman, 2007), which is
a transformation of the English Penn Treebank
(Marcus et al., 1993); the CTB-CCG (Tse and
Curran, 2010) transformation of the Penn Chinese
Treebank (Xue et al., 2005); and the CCG-TUT
corpus (Bos et al., 2009), built from the TUT cor-
pus of Italian text (Bosco et al., 2000). Statistics
on the size and ambiguity of these datasets are
shown in Table 1.
For CCGBank, sections 00?15 were used for
extracting the tag dictionary, 16?18 for the raw
corpus, 19?21 for development data, and 22?24
for test data. For TUT, the first 150 sentences of
each of the CIVIL LAW and NEWSPAPER sections
were used for raw data, the next sentences 150?
249 of each was used for development, and the
sentences 250?349 were used for test; the remain-
ing data, 457 sentences from CIVIL LAW and 548
from NEWSPAPER, plus the much smaller 132-
sentence JRC ACQUIS data, was used for the tag
dictionary. For CTB-CCG, sections 00?11 were
used for the tag dictionary, 20?24 for raw, 25?27
for dev, and 28?31 for test.
Because we are interested in showing the rel-
ative gains that our ideas provide over Baldridge
(2008), we reimplemented the initialization pro-
cedure from that paper, allowing us to evaluate
all approaches consistently. For each dataset, we
ran a series of experiments in which we made fur-
ther changes from the original work. We first ran
a baseline experiment with uniform transition and
emission initialization of EM (indicated as ?1.? in
Table 2) followed by our reimplementation of the
initialization procedure by Baldridge (2). We then
6
All code and experimental scripts are available
at http://www.github.com/dhgarrette/
2014-ccg-supertagging
146
Corpus English Chinese Italian
TD cutoff 0.1 0.01 0.001 no 0.1 0.01 0.001 no 0.1 0.01 0.001 no
1. uniform EM 77 62 47 38 64 39 30 26 51 32 30 30
2. init (Baldridge) EM 78 67 55 41 66 43 33 28 54 36 33 32
3. init Bayes 74 68 56 42 65 56 47 37 52 46 40 40
4. P
G
Bayes 74 70 59 42 64 57 47 36 52 40 39 40
5. P
G
, P
tr
?
Bayes 75 72 61 50 66 58 49 44 52 44 41 43
6. P
G
, P
tr
?
, P
em
Bayes 80 80 73 51 69 62 56 49 53 47 45 46
Table 2: Experimental results: test-set per-token supertag accuracies. ?TD cutoff? indicates the level of
tag dictionary pruning; see text. (1) is uniform EM initialization. (2) is a reimplementation of (Baldridge,
2008). (3) is Bayesian formulation using only the ideas from Baldridge: P
CPLX
, P
?
, and uniform emis-
sions. (4?6) are our enhancements to the prior: using our category grammar in P
G
instead of P
CPLX
, using
P
tr
?
instead of P
?
, and using P
em
instead of uniform.
experimented with the Bayesian formulation, first
using the same information used by Baldridge, and
then adding our enhancements: using our category
grammar in P
G
, using P
tr
?
as the transition com-
patability distribution, and using P
em
as ?
0
t
(w).
For each dataset, we ran experiments using four
different levels of tag dictionary pruning. Prun-
ing is the process of artificially removing noise
from the tag dictionary by using token-level anno-
tation counts to discard low-probability tags; for
each word, for cutoff x, any tag with probability
less than x is excluded. Tag dictionary pruning
is a standard procedure in type-supervised train-
ing, but because it requires information that does
not truly conform to the type-supervised scenario,
we felt that it was critical to demonstrate the per-
formance of our approach under situations of less
pruning, including no artificial pruning at all.
We emphasize that unlike in most previous
work, we use incomplete tag dictionaries. Most
previous work makes the unrealistic assumption
that the tag dictionary contains an entry for ev-
ery word that appears in either the training or test-
ing data. This is a poor approximation of a real
tagging system, which will never have complete
lexical knowledge about the test data. Even work
that only assumes complete knowledge of the tag-
ging possibilities for the lexical items in the train-
ing corpus is problematic (Baldridge, 2008; Ravi
et al., 2010). This still makes learning unrealisti-
cally easy since it dramatically reduces the ambi-
guity of words that would have been unseen, and,
in the case of CCG, introduces additional tags that
would not have otherwise been known. To ensure
that our experiments are more realistic, we draw
our tag dictionary entries from data that is totally
disjoint from both the raw and test corpora. Dur-
ing learning, any unknown words (words not ap-
pearing in the tag dictionary) are unconstrained so
that they may take any tag, and are, thus, maxi-
mally ambiguous.
We only performed minimal parameter tuning,
choosing instead to stay consistent with Baldridge
(2008) and simply pick reasonable-seeming val-
ues for any additional parameters. Any tuning that
was performed was done with simple hill-climbing
on the development data of English CCGBank.
All parameters were held consistent across exper-
iments, including across languages. For EM, we
used 50 iterations; for FFBS we used 100 burn-
in iterations and 200 sampling iterations.
7
For
all experiments, we used ? = 0.95 for P
(tr)
?
and
? = 0.5 for pi
0
t
to be consistent with previous
work, ?
pi
= 3000, ?
?
= 7000, p
term
= 0.6,
p
fw
= 0.5, p
mod
= 0.8, and ? = 1000 for p
atom
.
Test data was run only once, for the final figures.
The final results reported were achieved by us-
ing the following training sequence: initialize pa-
rameters according to the scenario, train an HMM
using EM or FFBS starting with that set of parame-
ters, tag the raw corpus with the trained HMM, add-
0.1 smooth counts from the now-tagged raw cor-
pus, and train a maximum entropy Markov model
(MEMM) from this ?auto-supervised? data.
8
Results are shown in Table 2. Most notably, the
contributions described in this paper improve re-
sults in nearly every experimental scenario. We
can see immediate, often sizable, gains in most
7
Final counts are averaged across the sampling iterations.
8
Auto-supervised training of an MEMM increases accu-
racy by 1?3% on average (Garrette and Baldridge, 2013). We
use the OpenNLP MEMM implementation with its standard
set of features: http://opennlp.apache.org
147
cases simply by using the Bayesian formulation.
Further gains are seen from adding each of the
other various contributions of this paper. Perhaps
most interestingly, the gains are only minimal with
maximum pruning, but the gains increase as the
pruning becomes less aggressive ? as the scenar-
ios become more realistic. This indicates that our
improvements make the overall procedure more
robust.
Error Analysis Like POS-taggers, the learned
supertagger frequently confuses nouns (N) and
their modifiers (N/N), but the most frequent er-
ror made by the English (6) experiment was
(((S\NP)\(S\NP))/N) instead of (NP
nb
/N). How-
ever, these are both determiner types, indicating an
interesting problem for the supertagger: it often
predicts an object type-raised determiner instead
of the vanilla NP/N, but in many contexts, both cat-
egories are equally valid. (In fact, for parsers that
use type-raising as a rule, this distinction in lexical
categories does not exist.)
6 Related Work
Ravi et al. (2010) also improved upon the work by
Baldridge (2008) by using integer linear program-
ming to find a minimal model of supertag transi-
tions, thereby generating a better starting point for
EM than the grammatical constraints alone could
provide. This approach is complementary to the
work presented here, and because we have shown
that our work yields gains under tag dictionaries
of various levels of cleanliness, it is probable that
employing minimization to set the base distribu-
tion for sampling could lead to still higher gains.
On the Bayesian side, Van Gael et al. (2009)
used a non-parametric, infinite HMM for truly un-
supervised POS-tagger learning (Van Gael et al.,
2008; Beal et al., 2001). While their model is not
restricted to the standard set of POS tags, and may
learn a more fine-grained set of labels, the induced
labels are arbitrary and not grounded in any gram-
matical formalism.
Bisk and Hockenmaier (2013) developed an ap-
proach to CCG grammar induction that does not
use a tag dictionary. Like ours, their procedure
learns from general properties of the CCG formal-
ism. However, while our work is intended to pro-
duce categories that match those used in a partic-
ular training corpus, however complex they might
be, their work produces categories in a simplified
form of CCG in which N and S are the only atoms
and no atoms have features. Additionally, they as-
sume that their training corpus is annotated with
POS tags, whereas we assume truly raw text.
Finally, we find the task of weakly-supervised
supertagger learning to be particularly relevant
given the recent surge in popularity of CCG.
An array of NLP applications have begun using
CCG, including semantic parsing (Zettlemoyer and
Collins, 2005) and machine translation (Weese et
al., 2012). As CCG finds more applications, and
as these applications move to lower-resource do-
mains and languages, there will be increased need
for the ability to learn without full supervision.
7 Conclusion and Future Work
Standard strategies for type-supervised HMM es-
timation are less effective as the number of cat-
egories increases. In contrast to POS tag sets,
CCG supertags, while quite numerous, have struc-
tural clues that can simplify the learning prob-
lem. Baldridge (2008) used this formalism-
specific structure to inform an initialization pro-
cedure for EM. In this work, we have shown that
CCG structure can instead be used to motivate an
effective prior distribution over the parameters of
an HMM supertagging model, allowing our work
to outperform Baldridge?s previously state-of-the-
art approach, and to do so in a principled manner
that lends itself better to future extensions such as
incorporation in more complex models.
This work also improves on Baldridge?s simple
?complexity? measure, developing instead a prob-
abilistic category grammar over supertags that al-
lows our prior to capture a wider variety of inter-
esting and useful properties of the CCG formalism.
Finally, we were able to achieve further gains
by augmenting the universal CCG knowledge with
corpus-specific information that could be automat-
ically extracted from the weak supervision that is
available: the raw corpus and the tag dictionary.
This allows us to combine the cross-linguistic
properties of the CCG formalism with corpus- or
language-specific information in the data into a
single, unified Bayesian prior.
Our model uses a relatively large number of pa-
rameters, e.g., p
term
, p
fw
, p
mod
, p
atom
, in the prior.
Here, we fixed each to a single value (i.e., a ?fully
Bayesian? approach). Future work might explore
sensitivity to these choices, or empirical Bayesian
or maximum a posteriori inference for their values
(Johnson and Goldwater, 2009).
148
In this work, as in most type-supervised work,
the tag dictionary was automatically extracted
from an existing tagged corpus. However, a tag
dictionary could instead be automatically induced
via multi-lingual transfer (Das and Petrov, 2011)
or generalized from human-provided information
(Garrette and Baldridge, 2013; Garrette et al.,
2013). Again, since the approach presented here
has been shown to be somewhat robust to tag dic-
tionary noise, it is likely that the model would
perform well even when using an automatically-
induced tag dictionary.
Acknowledgements
This work was supported by the U.S. Department
of Defense through the U.S. Army Research Of-
fice (grant number W911NF-10-1-0533). Exper-
iments were run on the UTCS Mastodon Cluster,
provided by NSF grant EIA-0303609.
References
Jason Baldridge. 2008. Weakly supervised supertag-
ging with grammar-informed initialization. In Pro-
ceedings of COLING.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2).
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The innite hidden Markov
model. In NIPS.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL.
Yonatan Bisk and Julia Hockenmaier. 2013. An HDP
model for inducing combinatory categorial gram-
mars. Transactions of the Association for Compu-
tational Linguistics, 1.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
speech induction. In Proceedings of ACL.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cat-
egorial grammar treebank for Italian. In M. Pas-
sarotti, Adam Przepi?orkowski, S. Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eighth
International Workshop on Treebanks and Linguistic
Theories (TLT8).
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a treebank
for Italian: a data-driven annotation schema. In Pro-
ceedings of LREC.
Christopher K. Carter and Robert Kohn. 1996. On
Gibbs sampling for state space models. Biometrika,
81(3):341?553.
Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1).
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of POS-
taggers for low-resource languages. In Proceedings
of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: Ex-
periments on unsupervised word segmentation with
adaptor grammars. In Proceedings of NAACL.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of EMNLP-
CoNLL.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &
Language, 6(3).
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
149
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010.
Minimized models and grammar-informed initial-
ization for supertagging with highly ambiguous lex-
icons. In Proceedings of ACL, pages 495?503.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL.
Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti Borjars, editors, Non-Transformational Syntax:
Formal and Explicit Models of Grammar. Wiley-
Blackwell.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: Extracting CCG derivations from the Penn
Chinese treebank. In Proceedings of COLING.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite hidden Markov model. In Proceedings of
ICML.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of EMNLP.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proceedings of WMT.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
150
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142?149,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The CMU Machine Translation Systems at WMT 2014
Austin Matthews Waleed Ammar Archna Bhatia Weston Feely
Greg Hanneman Eva Schlinger Swabha Swayamdipta Yulia Tsvetkov
Alon Lavie Chris Dyer
?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
?
Corresponding author: cdyer@cs.cmu.edu
Abstract
We describe the CMU systems submitted
to the 2014 WMT shared translation task.
We participated in two language pairs,
German?English and Hindi?English. Our
innovations include: a label coarsening
scheme for syntactic tree-to-tree transla-
tion, a host of new discriminative features,
several modules to create ?synthetic trans-
lation options? that can generalize beyond
what is directly observed in the training
data, and a method of combining the out-
put of multiple word aligners to uncover
extra phrase pairs and grammar rules.
1 Introduction
The MT research group at Carnegie Mellon Uni-
versity?s Language Technologies Institute partici-
pated in two language pairs for the 2014 Workshop
on Machine Translation shared translation task:
German?English and Hindi?English. Our systems
showcase our multi-phase approach to translation,
in which synthetic translation options supple-
ment the default translation rule inventory that is
extracted from word-aligned training data.
In the German?English system, we used our
compound splitter (Dyer, 2009) to reduce data
sparsity, and we allowed the translator to back
off to translating lemmas when it detected case-
inflected OOVs. We also demonstrate our group?s
syntactic system with coarsened nonterminal types
(Hanneman and Lavie, 2011) as a contrastive
German?English submission.
In both the German?English and Hindi?English
systems, we used an array of supplemental ideas to
enhance translation quality, ranging from lemma-
tization and synthesis of inflected phrase pairs to
novel reordering and rule preference features.
2 Core System Components
The decoder infrastructure we used was cdec
(Dyer et al., 2010). For our primary systems,
all data was tokenized using cdec?s tokenization
tool. Only the constrained data resources pro-
vided for the shared task were used for training
both the translation and language models. Word
alignments were generated using both FastAlign
(Dyer et al., 2013) and GIZA++ (Och and Ney,
2003). All our language models were estimated
using KenLM (Heafield, 2011). Translation model
parameters were chosen using MIRA (Eidelman,
2012) to optimize BLEU (Papineni et al., 2002)
on a held-out development set.
Our data was filtered using qe-clean
(Denkowski et al., 2012), with a cutoff of
two standard deviations from the mean. All
data was left in fully cased form, save the first
letter of each segment, which was changed to
whichever form the first token more commonly
used throughout the data. As such, words like The
were lowercased at the beginning of segments,
while words like Obama remained capitalized.
Our primary German?English and Hindi?
English systems were Hiero-based (Chiang,
2007), while our contrastive German?English sys-
tem used cdec?s tree-to-tree SCFG formalism.
Before submitting, we ran cdec?s implementa-
tion of MBR on 500-best lists from each of our
systems. For both language pairs, we used the
Nelder?Mead method to optimize the MBR pa-
rameters. In the German?English system, we ran
MBR on 500 hypotheses, combining the output of
the Hiero and tree-to-tree systems.
The remainder of the paper will focus on our
primary innovations in the two language pairs.
142
3 Common System Improvements
A number of our techniques were used for both our
German?English and Hindi?English primary sub-
missions. These techniques each fall into one of
three categories: those that create translation rules,
those involving language models, or those that add
translation features. A comparison of these tech-
niques and their performance across the two lan-
guage pairs can be found in Section 6.
3.1 Rule-Centric Enhancements
While many of our methods of enhancing the
translation model with extra rules are language-
specific, three were shared between language
pairs.
First, we added sentence-boundary tokens <s>
and </s> to the beginning and end of each line in
the data, on both the source and target sides.
Second, we aligned all of our training data us-
ing both FastAlign and GIZA++ and simply con-
catenated two copies of the training corpus, one
aligned with each aligner, and extracted rules from
the resulting double corpus.
Third, we hand-wrote a list of rules that trans-
form numbers, dates, times, and currencies into
well-formed English equivalents, handling differ-
ences such as the month and day reversal in dates
or conversion from 24-hour time to 12-hour time.
3.2 Employed Language Models
Each of our primary systems uses a total of three
language models.
The first is a traditional 4-gram model gen-
erated by interoplating LMs built from each of
the available monolingual corpora. Interpolation
weights were calculated used the SRILM toolkit
(Stolcke, 2002) and 1000 dev sentences from the
Hindi?English system.
The second is a model trained on word clus-
ters instead of surface forms. For this we mapped
the LM vocabulary into 600 clusters based on the
algorithm of Brown et al. (1992) and then con-
structed a 7-gram LM over the resulting clusters,
allowing us to capture more context than our tra-
ditional surface-form language model.
The third is a bigram model over the source side
of each language?s respective bitext. However, at
run time this LM operates on the target-side out-
put of the translator, just like the other two. The
intuition here is that if a source-side LM likes our
output, then we are probably passing through more
than we ought to.
Both source and target surface-form LM used
modified Kneser-Ney smoothing (Kneser and Ney,
1995), while the model over Brown clusters
(Brown et al., 1992) used subtract-0.5 smoothing.
3.3 New Translation Features
In addition to the standard array of features, we
added four new indicator feature templates, lead-
ing to a total of nearly 150,000 total features.
The first set consists of target-side n-gram fea-
tures. For each bigram of Brown clusters in the
output string generated by our translator, we fire
an indicator feature. For example, if we have the
sentence, Nato will ihren Einfluss im Osten st?arken
translating as NATO intends to strengthen its influ-
ence in the East, we will fire an indicator features
NGF C367 C128=1, NGF C128 C31=1, etc.
The second set is source-language n-gram fea-
tures. Similar to the previous feature set, we fire
an indicator feature for each ngram of Brown clus-
ters in the output. Here, however, we use n = 1,
and we use the map of source language words to
Brown clusters, rather than the target language?s,
despite the fact that this is examining target lan-
guage output. The intuition here is to allow this
feature to penalize passthroughs differently de-
pending on their source language Brown cluster.
For example, passing through the German word
zeitung (?newspaper?) is probably a bad idea, but
passing through the German word Obama proba-
bly should not be punished as severely.
The third type of feature is source path features.
We can imagine translation as a two-step process
in which we first permute the source words into
some order, then translate them phrase by phrase.
This set of features examines that intermediate
string in which the source words have been per-
muted. Again, we fire an indicator feature for each
bigram in this intermediate string, this time using
surface lexical forms directly instead of first map-
ping them to Brown clusters.
Lastly, we create a new type of rule shape fea-
ture. Traditionally, rule shape features have indi-
cated, for each rule, the sequence of terminal and
non-terminal items on the right-hand side. For ex-
ample, the rule [X] ? der [X] :: the [X] might
have an indicator feature Shape TN TN, where
T represents a terminal and N represents a non-
terminal. One can also imagine lexicalizing such
rules by replacing each T with its surface form.
We believe such features would be too sparse, so
instead of replacing each terminal by its surface
form, we instead replace it with its Brown cluster,
143
creating a feature like Shape C37 N C271 N.
4 Hindi?English Specific Improvements
In addition to the enhancements common to the
two primary systems, our Hindi?English system
includes improved data cleaning of development
data, a sophisticated linguistically-informed tok-
enization scheme, a transliteration module, a syn-
thetic phrase generator that improves handling of
function words, and a synthetic phrase generator
that leverages source-side paraphrases. We will
discuss each of these five in turn.
4.1 Development Data Cleaning
Due to a scarcity of clean development data, we
augmented the 520 segments provided with 480
segments randomly drawn from the training data
to form our development set, and drew another
random 1000 segments to serve as a dev test set.
After observing large discrepencies between the
types of segments in our development data and the
well-formed news domain sentences we expected
to be tested on, we made the decision to prune our
tuning set by removing any segment that did not
appear to be a full sentence on both the Hindi and
English sides. While this reduced our tuning set
from 1000 segments back down to 572 segments,
we believe it to be the single largest contributor to
our success on the Hindi?English translation task.
4.2 Nominal Normalization
Another facet of our system was normalization of
Hindi nominals. The Hindi nominal system shows
much more morphological variation than English.
There are two genders (masculine and feminine)
and at least six noun stem endings in pronuncia-
tion and 10 in writing.
The pronominal system also is much richer than
English with many variants depending on whether
pronouns appear with case markers or other post-
positions.
Before normalizing the nouns and pronouns, we
first split these case markers / postpositions from
the nouns / pronouns to result in two words in-
stead of the original combined form. If the case
marker was n (ne), the ergative case marker in
Hindi, we deleted it as it did not have any trans-
lation in English. All the other postpositions were
left intact while splitting from and normalizing the
nouns and pronouns.
These changes in stem forms contribute to the
sparsity in data; hence, to reduce this sparsity, we
construct for each input segment an input lattice
that allows the decoder to use the split or original
forms of all nouns or pronouns, as well as allowing
it to keep or delete the case marker ne.
4.3 Transliteration
We used the 12,000 Hindi?English transliteration
pairs from the ACL 2012 NEWS workshop on
transliteration to train a linear-chained CRF tag-
ger
1
that labels each character in the Hindi token
with a sequence of zero or more English characters
(Ammar et al., 2012). At decoding, unseen Hindi
tokens are fed to the transliterator, which produces
the 100 most probable transliterations. We add
a synthetic translation option for each candidate
transliteration.
In addition to this sophisticated transliteration
scheme, we also employ a rule-based translitera-
tor that specifically targets acronyms. In Hindi,
many acronyms are spelled out phonetically, such
as NSA being rendered as enese (en.es.e). We
detected such words in the input segments and
generated synthetic translation options both with
and without periods (e.g. N.S.A. and NSA).
4.4 Synthetic Handling of Function Words
In different language pairs, individual source
words may have many different possible trans-
lations, e.g., when the target language word has
many different morphological inflections or is sur-
rounded by different function words that have no
direct counterpart in the source language. There-
fore, when very large quantities of parallel data
are not available, we can expect our phrasal inven-
tory to be incomplete. Synthetic translation option
generation seeks to fill these gaps using secondary
generation processes that exploit existing phrase
pairs to produce plausible phrase translation alter-
natives that are not directly extractable from the
training data (Tsvetkov et al., 2013; Chahuneau et
al., 2013).
To generate synthetic phrases, we first remove
function words from the source and target sides
of existing non-gappy phrase pairs. We manually
constructed English and Hindi lists of common
function words, including articles, auxiliaries, pro-
nouns, and adpositions. We then employ the
SRILM hidden-ngram utility (Stolcke, 2002) to re-
store missing function words according to an n-
gram language model probability, and add the re-
sulting synthetic phrases to our phrase table.
1
https://github.com/wammar/transliterator
144
4.5 Paraphrase-Based Synthetic Phrases
We used a graph-based method to obtain transla-
tion distributions for source phrases that are not
present in the phrase table extracted from the par-
allel corpus. Monolingual data is used to construct
separate similarity graphs over phrases (word se-
quences or n-grams), using distributional features
extracted from the corpora. The source similar-
ity graph consists of phrase nodes representing se-
quences of words in the source language. In our
instance, we restricted the phrases to bigrams, and
the bigrams come from both the phrase table (the
labeled phrases) and from the evaluation set but
not present in the phrase table (unlabeled phrases).
The labels for these source phrases, namely the
target phrasal inventory, can also be represented
in a graph form, where the distributional features
can also be computed from the target monolingual
data. Translation information is then propagated
from the labeled phrases to the unlabeled phrases
in the source graph, proportional to how similar
the phrases are to each other on the source side,
as well as how similar the translation candidates
are to each other on the target side. The newly
acquired translation distributions for the unlabeled
phrases are written out to a secondary phrase table.
For more information, see Saluja et al. (2014).
5 German?English Specific
Improvements
Our German?English system also had its own
suite of tricks, including the use of ?pseudo-
references? and special handling of morphologi-
cally inflected OOVs.
5.1 Pseudo-References
The development sets provided have only a sin-
gle reference, which is known to be sub-optimal
for tuning of discriminative models. As such,
we use the output of one or more of last year?s
top performing systems as pseudo-references dur-
ing tuning. We experimented with using just one
pseudo-reference, taken from last year?s Spanish?
English winner (Durrani et al., 2013), and with
using four pseudo-references, including the out-
put of last year?s winning Czech?English, French?
English, and Russian?English systems (Pino et al.,
2013).
5.2 Morphological OOVs
Examination of the output of our baseline sys-
tems lead us to conclude that the majority of our
system?s OOVs were due to morphologically in-
flected nouns in the input data, particularly those
in genitive case. As such, for each OOV in the
input, we attempt to remove the German genitive
case marker -s or -es. We then run the resulting
form f through our baseline translator to obtain a
translation e of the lemma. Finally, we add two
translation rules to our translation table: f ? e,
and f ? e?s.
6 Results
As we added each feature to our systems, we
first ran a one-off experiment comparing our base-
line system with and without each individual fea-
ture. The results of that set of experiments are
shown in Table 1 for Hindi?English and Table 2
for German?English. Features marked with a *
were not included in our final system submission.
The most surprising result is the strength of
our Hindi?English baseline system. With no extra
bells or whistles, it is already half a BLEU point
ahead of the second best system submitted to this
shared task. We believe this is due to our filter-
ing of the tuning set, which allowed our system to
generate translations more similar in length to the
final test set.
Another interesting result is that only one fea-
ture set, namely our rule shape features based on
Brown clusters, helped on the test set in both lan-
guage pairs. No feature hurt the BLEU score on
the test set in both language pairs, meaning the
majority of features helped in one language and
hurt in the other.
If we compare results on the tuning sets, how-
ever, some clearer patterns arise. Brown cluster
language models, n-gram features, and our new
rule shape features all helped.
Furthermore, there were a few features, such as
the Brown cluster language model and tuning to
Meteor (Denkowski and Lavie, 2011), that helped
substantially in one language pair while just barely
hurting the other. In particular, the fact that tuning
to Meteor instead of BLEU can actually help both
BLEU and Meteor scores was rather unexpected.
7 German?English Syntax System
In addition to our primary German?English sys-
tem, we also submitted a contrastive German?
English system showcasing our group?s tree-to-
tree syntax-based translation formalism.
145
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 15.7 25.3 68.0 11.4 22.9 70.3
*Meteor Tuning 15.2 25.8 71.3 12.8 23.7 71.3
Sentence Boundaries 15.2 25.4 69.1 12.1 23.4 70.0
Double Aligners 16.1 25.5 66.6 11.9 23.1 69.2
Manual Number Rules 15.7 25.4 68.5 11.6 23.0 70.3
Brown Cluster LM 15.6 25.1 67.3 11.5 22.7 69.8
*Source LM 14.2 25.1 72.1 11.3 23.0 72.3
N-Gram Features 15.6 25.2 67.9 12.2 23.2 69.2
Src N-Gram Features 15.3 25.2 68.9 12.0 23.4 69.5
Src Path Features 15.8 25.6 68.8 11.9 23.3 70.4
Brown Rule Shape 15.9 25.4 67.2 11.8 22.9 69.6
Lattice Input 15.2 25.8 71.3 11.4 22.9 70.3
CRF Transliterator 15.7 25.7 69.4 12.1 23.5 70.1
Acronym Translit. 15.8 25.8 68.8 12.4 23.4 70.2
Synth. Func. Words 15.7 25.3 67.8 11.4 22.8 70.4
Source Paraphrases 15.6 25.2 67.7 11.5 22.7 69.9
Final Submission 16.7
Table 1: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero Hindi?
English system. Each line is the baseline plus that one feature, non-cumulatively. Lines marked with a *
were not included in our final WMT submission.
Test (2014) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 25.3 30.4 52.6 26.2 31.3 53.6
*Meteor Tuning 26.2 31.3 53.1 26.9 32.2 54.4
Sentence Boundaries 25.4 30.5 52.2 26.1 31.4 53.3
Double Aligners 25.2 30.4 52.5 26.0 31.3 53.6
Manual Number Rules 25.3 30.3 52.5 26.1 31.4 53.4
Brown Cluster LM 26.4 31.0 51.9 27.0 31.8 53.2
*Source LM 25.8 30.6 52.4 26.4 31.5 53.4
N-Gram Features 25.4 30.4 52.6 26.7 31.6 53.0
Src N-Gram Features 25.3 30.5 52.5 26.2 31.5 53.4
Src Path Features 25.0 30.1 52.6 26.0 31.2 53.3
Brown Rule Shape 25.5 30.5 52.4 26.3 31.5 53.2
One Pseudo Ref 25.5 30.4 52.6 34.4 32.7 49.3
*Four Psuedo Refs 22.6 29.2 52.6 49.8 35.0 46.1
OOV Morphology 25.5 30.5 52.4 26.3 31.5 53.3
Final Submission 27.1
Table 2: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero
German?English system. Each line is the baseline plus that one feature, non-cumulatively.
Dev (2013) Dev Test (2012)
System BLEU Met TER BLEU Met TER
Baseline 20.98 29.81 58.47 18.65 28.72 61.80
+ Label coarsening 23.07 30.71 56.46 20.43 29.34 60.16
+ Meteor tuning 23.48 30.90 56.18 20.96 29.60 59.87
+ Brown LM + Lattice + Synthetic 24.46 31.41 56.66 21.50 30.28 60.51
+ Span limit 15 24.20 31.25 55.48 21.75 29.97 59.18
+ Pseudo-references 24.55 31.30 56.22 22.10 30.12 59.73
Table 3: BLEU, Meteor, and TER results for experiments conducted in the tree-to-tree German?English
system. The system in the bottom line was submitted to WMT as a contrastive entry.
7.1 Basic System Construction
Since all training data for the tree-to-tree system
must be parsed in addition to being word-aligned,
we prepared separate copies of the training, tun-
ing, and testing data that are more suitable for in-
put into constituency parsing. Importantly, we left
the data in its original mixed-case format. We used
the Stanford tokenizer to replicate Penn Treebank
tokenization on the English side. On the German
side, we developed new in-house normalization
and tokenization script.
We filtered tokenized training sentences by sen-
146
tence length, token length, and sentence length ra-
tio. The final corpus for parsing and word align-
ment contained 3,897,805 lines, or approximately
86 percent of the total training resources released
under the WMT constrained track. Word align-
ment was carried out using FastAlign (Dyer et
al., 2013), while for parsing we used the Berke-
ley parser (Petrov et al., 2006).
Given the parsed and aligned corpus, we ex-
tracted synchronous context-free grammar rules
using the method of Hanneman et al. (2011).
In addition to aligning subtrees that natively ex-
ist in the input trees, our grammar extractor also
introduces ?virtual nodes.? These are new and
possibly overlapping constituents that subdivide
regions of flat structure by combining two adja-
cent sibling nodes into a single nonterminal for
the purposes of rule extraction. Virtual nodes
are similar in spirit to the ?A+B? extended cate-
gories of SAMT (Zollmann and Venugopal, 2006),
and their nonterminal labels are constructed in the
same way, but with the added restriction that they
do not violate any existing syntactic structure in
the parse tree.
7.2 Improvements
Nonterminals in our tree-to-tree grammar are
made up of pairs of symbols: one from the source
side and one from the target side. With virtual
nodes included, this led to an initial German?
English grammar containing 153,219 distinct non-
terminals ? a far larger set than is used in SAMT,
tree-to-string, string-to-tree, or Hiero systems. To
combat the sparsity introduce by this large nonter-
minal set, we coarsened the label set with an ag-
glomerative label-clustering technique(Hanneman
and Lavie, 2011; Hanneman and Lavie, 2013).
The stopping point was somewhat arbitrarily cho-
sen to be a grammar of 916 labels.
Table 3 shows a significant improvement in
translation quality due to coarsening the label set:
approximately +1.8 BLEU, +0.6 Meteor, and ?1.6
TER on our dev test set, newtest2012.
2
In the MERT runs, however, we noticed that the
length of the MT output can be highly variable,
ranging on the tuning set from a low of 92.8% of
the reference length to a high of 99.1% in another.
We were able to limit this instability by tuning to
Meteor instead of BLEU. Aside from a modest
2
We follow the advice of Clark et al. (2011) and eval-
uate our tree-to-tree experiments over multiple independent
MERT runs. All scores in Table 3 are averages of two or
three runs, depending on the row.
score improvement, we note that the variability in
length ratio is reduced from 6.3% to 2.8%.
Specific difficulties of the German?English lan-
guage pair led to three additional system compo-
nents to try to combat them.
First, we introduced a second language model
trained on Brown clusters instead of surface forms.
Next we attempted to overcome the sparsity
of German input by making use of cdec?s lattice
input functionality introduce compound-split ver-
sions of dev and test sentences.
Finally, we attempted to improve our grammar?s
coverage of new German words by introducing
synthetic rules for otherwise out-of-vocabulary
items. Each token in a test sentence that the gram-
mar cannot translate generates a synthetic rule al-
lowing the token to be translated as itself. The left-
hand-side label is decided heuristically: a (coars-
ened) ?noun? label if the German OOV starts with
a capital letter, a ?number? label if the OOV con-
tains only digits and select punctuation characters,
an ?adjective? label if the OOV otherwise starts
with a lowercase letter or a number, or a ?symbol?
label for anything left over.
The effect of all three of these improvements
combined is shown in the fourth row of Table 3.
By default our previous experiments were per-
formed with a span limit of 12 tokens. Increasing
this limit to 15 has a mixed effect on metric scores,
as shown in the fifth row of Table 3. Since two out
of three metrics report improvement, we left the
longer span limit in effect in our final system.
Our final improvement was to augment our tun-
ing set with the same set of pseudo-references
as our Hiero systems. We found that using one
pseudo-reference versus four pseudo-references
had negligible effect on the (single-reference) tun-
ing scores, but four produced a better improve-
ment on the test set.
The best MERT run of this final system (bottom
line of Table 3) was submitted to the WMT 2014
evaluation as a contrastive entry.
Acknowledgments
We sincerely thank the organizers of the work-
shop for their hard work, year after year, and the
reviewers for their careful reading of the submit-
ted draft of this paper. This research work was
supported in part by the U. S. Army Research
Laboratory and the U. S. Army Research Office
under contract/grant number W911NF-10-1-0533,
by the National Science Foundation under grant
147
IIS-0915327, by a NPRP grant (NPRP 09-1140-
1-177) from the Qatar National Research Fund (a
member of the Qatar Foundation), and by com-
puting resources provided by the NSF-sponsored
XSEDE program under grant TG-CCR110017.
The statements made herein are solely the respon-
sibility of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In NEWS workshop at ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467?479.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proceed-
ings of EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing
for statistical machine translation: Crontrolling for
optimizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Short Papers, pages 176?181, Portland,
Oregon, USA, June.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85?91, Edinburgh, Scot-
land, UK, July.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The cmu-avenue french-english translation
system. In Proceedings of the NAACL 2012 Work-
shop on Statistical Machine Translation.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine transla-
tion systems for european language pairs.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM Model 2. In Proc. of NAACL.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 406?414. Association for Computational Lin-
guistics.
Vladimir Eidelman. 2012. Optimization strategies for
online large-margin learning in machine translation.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation.
Greg Hanneman and Alon Lavie. 2011. Automatic
category label coarsening for syntax-based machine
translation. In Proceedings of SSST-5: Fifth Work-
shop on Syntax, Semantics, and Structure in Statis-
tical Translation, pages 98?106, Portland, Oregon,
USA, June.
Greg Hanneman and Alon Lavie. 2013. Improving
syntax-augmented machine translation by coarsen-
ing the label set. In Proceedings of NAACL-HLT
2013, pages 288?297, Atlanta, Georgia, USA, June.
Greg Hanneman, Michelle Burroughs, and Alon Lavie.
2011. A general-purpose rule extractor for SCFG-
based machine translation. In Proceedings of SSST-
5: Fifth Workshop on Syntax, Semantics, and Struc-
ture in Statistical Translation, pages 135?144, Port-
land, Oregon, USA, June.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, Scotland, UK, July.
R. Kneser and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
IEEE Internation Conference on Acoustics, Speech,
and Signal Processing, pages 181?184.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gis-
pert, Federico Flego, and William Byrne. 2013.
The university of cambridge russian-english system
at wmt13.
148
Avneesh Saluja, Hany Hassan, Kristina Toutanova, and
Chris Quirk. 2014. Graph-based semi-supervised
learning of translation models from monolingual
data. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Baltimore, Maryland, June.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In INTERSPEECH.
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Batia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proceedings of the Eighth Workshop on
Statistical Machine Translation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, New
York, USA, June.
149
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 426?436,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Crowdsourcing High-Quality Parallel Data Extraction from Twitter
?
Wang Ling
123
Lu?s Marujo
123
Chris Dyer
2
Alan Black
2
Isabel Trancoso
13
(1)L
2
F Spoken Systems Lab, INESC-ID, Lisbon, Portugal
(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA
(3)Instituto Superior T?cnico, Lisbon, Portugal
{lingwang,lmarujo,cdyer,awb}@cs.cmu.edu
isabel.trancoso@inesc-id.pt
Abstract
High-quality parallel data is crucial for a
range of multilingual applications, from
tuning and evaluating machine translation
systems to cross-lingual annotation pro-
jection. Unfortunately, automatically ob-
tained parallel data (which is available
in relative abundance) tends to be quite
noisy. To obtain high-quality parallel data,
we introduce a crowdsourcing paradigm
in which workers with only basic bilin-
gual proficiency identify translations from
an automatically extracted corpus of par-
allel microblog messages. For less than
$350, we obtained over 5000 parallel seg-
ments in five language pairs. Evaluated
against expert annotations, the quality of
the crowdsourced corpus is significantly
better than existing automatic methods:
it obtains an performance comparable to
expert annotations when used in MERT
tuning of a microblog MT system; and
training a parallel sentence classifier with
it leads also to improved results. The
crowdsourced corpora will be made avail-
able in http://www.cs.cmu.edu/
~lingwang/microtopia/.
1 Introduction
High-quality parallel data is essential for tun-
ing and evaluating statistical MT systems, and
it plays a role in a wide range of multilingual
NLP applications, such as word sense disambigua-
tion (Gale et al., 1992; Ng et al., 2003; Specia
et al., 2005), paraphrasing (Bannard and Callison-
burch, 2005; Ganitkevitch et al., 2012), annota-
tion projection (Das and Petrov, 2011), and other
language-specific applications (Schwarck et al.,
?
A sample of the crowdsourced corpora and the inter-
faces used are available as supplementary material.
2010; Liu et al., 2011). While large amounts
of parallel data can be easily obtained by mining
the web (Resnik and Smith, 2003), comparable
corpora (Munteanu and Marcu, 2005), and even
social media sites (Ling et al., 2013), automati-
cally extracted parallel tends to be noisy, and, as a
result, ?evaluation-quality? parallel corpora have
generally been produced at considerable expense
by targeted translation efforts (Bojar et al., 2013,
inter alia). Unfortunately, in some domains such
as microblogs, the only corpora that are available
are automatically extracted and noisy.
While phrase-based translation models can ef-
fectively learn translation rules from noisy parallel
data (Goutte et al., 2012), having a subset of high-
quality parallel segments is nevertheless crucial.
Firstly, the automatic parallel data extraction sys-
tem?s parameters can be tuned by optimizing on
the gold standard data. Secondly, even though the
parallel data used to train MT systems can contain
a considerable amount of noise, it is conventional
to use human annotated parallel data to tune and
evaluate the system. Finally, other NLP applica-
tions may not be as noise-robust as MT.
We introduce a new crowdsourcing protocol for
obtaining high-quality parallel data from noisy,
automatically extracted parallel data (?3), focus-
ing on the challenging case of identifying par-
allel data in microblog messages (Ling et al.,
2013). In contrast to previous attempts to use
crowdsourcing to obtain parallel data, in which
workers performed translation (Ambati and Vo-
gel, 2010; Zaidan and Callison-Burch, 2011; Post
et al., 2012; Ambati et al., 2012), our approach
only requires that they identify whether a candi-
date message contains a translation, and if so, what
the spans of the translated segments are. This is
a much simpler task than translation, and one that
can often be completed by workers with only a ba-
sic proficiency in the source and target languages.
For evaluation (?4), we use our protocol to build
426
parallel datasets on a Chinese-English corpus orig-
inally extracted from Sina Weibo and for which we
have expert annotations. This lets us quantify the
effectiveness of our method under different task
variations. We also show that the crowdsourced
corpus performs as well as expert annotation (and
better than the automatically extracted corpus) for
tuning an MT system with MERT. We next apply
our method on a corpus of five language pairs (en-
ar, en-ja, en-ko, en-ru, en-zh) extracted from Twit-
ter (?5), for which we have no gold-standard data.
Using this data in a cross-validation setup, we train
and evaluate a maxent classifier for detecting par-
allel data (?6), and then we conclude (?7).
2 Related Work
Our work crosses crowdsourcing techniques and
automatic parallel data extraction from mi-
croblogs. In this section, we shall provide back-
ground information and analysis of the work per-
formed in these two fields.
2.1 Parallel Data Extraction from Microblogs
Many sources of parallel data exist on the
web. The most popular choice are parallel web
pages (Resnik and Smith, 2003), while other
work have looked at specific domains with large
amounts of data, such as Wikipedia (Smith et
al., 2010). Microblogs, such as Twitter and Sina
Weibo, represent a subdomain of the Web. Some
of its characteristics is the informal language used
and the short nature of the messages that are
posted. Due to its large size and growing pop-
ularity, work has been done on parallel data ex-
traction from this domain. Ling et al. (2013) at-
tempt to find naturally occurring parallel data from
Sina Weibo and Twitter. Some examples of what
is found are illustrated in Figure 1. The extrac-
tion process starts by finding the parallel segments
within the same message and the word alignments
between those segments that maximize a hand-
tuned model score.
Another method (Jehl et al., 2012) leverages
CLIR (Cross Lingual Information Retrieval) tech-
niques to find pairs of tweets that are translations.
The main challenge in this approach is the large
amount of pairs of tweets that must be considered,
which raises some scalability issues when process-
ing billions of tweets.
Our crowdsourcing method can be applied to
annotate data from any naturally occurring source.
In this paper, we will use the corpus developed
by Ling et al. (2013), since it is publicly available
and has parallel data for 6 languages from Twitter,
and for 10 languages from Sina Weibo.
2.2 Parallel Data using Crowdsourcing
Most of the work done in building parallel data
using crowdsourcing (Ambati and Vogel, 2010;
Zaidan and Callison-Burch, 2011; Post et al.,
2012; Ambati et al., 2012) relies on using crowd-
sourcing workers to translate. These methods
must address the fact that workers may produce
poor and sometimes incorrect translations. Thus,
in order to find good translations, subsequent
postediting and/or ranking is generally necessary.
In contrast, in our work, crowdsourcing is used
for data extraction rather than translation, a sub-
stantially simpler task than translation (in particu-
lar, translation of informal text) that requires less
expertise in the language pair (basic proficiency in
the two languages is generally sufficient to suc-
cessfully complete the task). Furthermore, assess-
ing whether a worker performed the task correctly
and combining the outputs of different workers is
simpler. The time spent per item is also reduced:
our annotation interface only requires the worker
to make a few clicks on the tweet to complete
each annotation, meaning that tasks are completed
faster and with less effort, allowing us to obtain
translations at lower cost. On the other hand,
the main drawback of our method is that it can
only obtain parallel data from translations that ex-
ist, which corresponds to the amount of posts that
have been translated and posted. This limits the
potential coverage of our method. Furthermore,
the resulting datasets may not be fully representa-
tive of the Twitter domain, since not all types of
content are translated and follow the same distri-
bution as the data in Twitter.
3 Proposed Crowdsourcing Protocol
As discussed above, automatically extracted par-
allel is often noisy. The sources of error range
from language detection errors, to errors determin-
ing if material is actually translation, and errors in
extracting the appropriate spans of the translated
material. Consider the fragment of the microblog
parallel corpus mined by Ling et al. (2013), which
is shown in Figure 1. In the Korean-English mes-
sage, the system may incorrectly added the un-
translated word Hahah in the English segment,
427
and missed the translated word Weather. At a high
level, the task faced by annotators will be to iden-
tify and resolve such errors.
3.1 Overview
We separate the tasks of identifying the parallel
posts, which we shall denote by identification,
and of locating the parallel segments, which we
will call location. The justification for this is that
the majority of the tweets are not parallel, as re-
ported by Ling et al. (2013), and the location of
the parallel data is only applicable if the tweet
actually contains parallel data. This is also de-
sirable because the identification task is simpler
than the location task. Firstly, identifying whether
a tweet contains translations requires much less
proficiency in the respective languages than locat-
ing the parallel segments, since it only requires
the worker to understand parts of the message.
This means we can have more potential workers
capable of performing this task. Secondly, the
first task is a binary decision, and each annota-
tion can be completed with only one action, which
means that the average required time for this task
is much lower than the second task and the pay-
ment required for each hit will naturally be lower
as well. Finally, combining worker results for a
binary decision is simpler than combining transla-
tions, since the space of possible answers is sev-
eral orders of magnitude lower.
As crowdsourcing platform, we use Amazon?s
Mechanical Turk. In this platform, the requesters
can submit tasks, where one can define the num-
ber of workers n that will complete each task and
what is the payment p for each task submission,
henceforth denoted as job. In our work, we had to
consider the following components:
? Interface - To submit a task, an interface
must be provided, which workers will be us-
ing to complete the job.
? Worker Quality Prediction - After submit-
ting a job, the requester can accept and pay
the agreed fee or reject the task. It is cru-
cial to have a method to automatically pre-
dict whether workers have performed the job
properly, and reject them otherwise.
? Result Combination - It is common for mul-
tiple workers to complete the same task with
different results. Thus, a method must be im-
plemented to combine multiple responses for
correctly predicting the desired response.
We structured each of our tasks as a series of q
questions, which include a small number of refer-
ences r, for which we know the answers. Thus,
the amount of answers we obtain for each dollar is
given by
q?r
np
, where n is the number of workers
per task and p is the payment for each task. In or-
der to maximize this quotient, we can either reduce
the number of reference question r, the number of
workers per task n, or the payment p. However,
reducing r will also limit our capability of esti-
mating the quality of the worker results, since we
will have less data to make such prediction. For
the same reason, reducing n will limit our abil-
ity to combine results properly. As for the pay-
ment p, while there is no direct effect on our task,
it has been noted that workers will perform the
task faster for higher payments (Post et al., 2012).
In our work, we will propose methods to predict
quality and combine results that will minimize the
requirements for n and r, while maximizing the
quality of the final results.
3.2 Parallel Post Identification
In the identification task, for each question, we
will show a post, and solicit the worker to detect if
it contains translations in a given language pair.
Interface The interface for this task is straight-
forward. We present to the worker each tweet in-
dividually, together with a checkbox to be checked
in case the tweet contains parallel data. The navi-
gation between tweets is done by adding next and
previous buttons, allowing the user to go back and
review previous answers. Finally, the worker can
only submit the HIT after traversing all 25 ques-
tions. Unlike the work in crowdsourcing transla-
tion (Zaidan and Callison-Burch, 2011), where au-
tomatic translation systems are discouraged, since
it produces poor output, we allow its usage as long
as this leads to correct annotations. In fact, we add
a button to automatically translate the tweet into
English from the non-English language.
Worker Quality Prediction We accept the job
if it answers enough reference questions correctly.
We consider two different approaches to select ref-
erences. A random sampler that selects tweets
randomly and a balanced sampler that selects
the same number of positive and negative sam-
ples. As notation, we will denote as acceptor
428
Figure 1: Parallel microblog posts in 5 language pairs. Shaded backgrounds mark the parallel segments
(annotated manually), non shaded parts do not have translations.
accept(rand, c, r) a setup where the worker?s job
is accepted if c out of r randomly sampled refer-
ences are correctly answered. Likewise, acceptor
accept(bal, c, r) denotes the same setup using bal-
anced reference questions.
Result Combination Given n jobs with answers
for a question that can be either positive or nega-
tive, we calculate the weighted ratio of positive an-
swers, given by
?
i=1..n
?
p
(i)w(i)
?
i=1..n
w(i)
, where ?
p
is one if
answer i is positive and 0 otherwise, and w(i) is
the weight of the worker. w(i) is defined as the
ratio of correct answers from job i in the reference
set. If the weighted ratio is higher than 0.5, we la-
bel the tweet as positive and otherwise as negative.
3.3 Parallel Data Location
In the location task, we also present one tweet per
question, where the worker will be asked to iden-
tify the parallel segments. The worker can also
define that there are no translations in the tweet.
Interface The interface for this task presents the
user with one tweet at a time, and allows the user
to break the tweet into segments, by clicking be-
tween characters. Each segment can then be clas-
sified as English, the non-English language (Ex:
Mandarin), or non-parallel, which is the default
option. To understand the concept of non-parallel
segments, notice that when we are locating par-
allel data in tweets, we are essentially breaking
the tweet into the structure ?N
left
P
left
N
middle
P
right
N
right
", where P
left
and P
right
are the par-
allel segments and N
left
, N
middle
and N
right
are
textual segments that are non-parallel. These may
not exist, for instance, the Arabic tweet in Fig-
ure 1 (line 1) does not contain any non-parallel text
and does not require any non-parallel segments
to delineate the parallel data. The Korean tweet
(line 2), on the other hand, has an N
middle
corre-
sponding to????????????????????????????* and an
N
right
corresponding to Hahah and requires two
non-parallel segments to locate the parallel data.
Thus, if the worker does not commit any errors,
each question can be answered with at most four
clicks, when all five segments exist, and two op-
tion choices for identifying the parallel segments.
In the easiest case, when only the parallel seg-
ments exist, only one click and two option choices
are needed. If there are no translations, the button
no translations can be clicked.
For instance, to annotate the Korean tweet in
Figure 1, the worker must click immediately be-
fore????, then before Weather and finally before
Hahah. Then on the drop-down box of the first
and and third segments, the worker must choose
Korean and English, respectively. The interface
after these operations is show in Figure 2.
Work Quality Prediction To score the worker?s
jobs, we use the scoring function devised in (Ling
et al., 2013), which measures the word overlap
between the reference parallel segments segments
and the predicted segments. However, setting the
score threshold to accept a job is a challenge, since
scores are bound to change for different language-
pairs and domains. Moreover, some tweets are
harder to annotate than others. Learning this
threshold automatically requires annotated data,
which we do not have for all language pairs and
domains. Thus, we propose a method to generate
thresholds specifically for each sample.
We consider a ?smart but lazy" pseudo worker,
who will complete the same jobs automatically
and generate scores that the real worker?s jobs
must beat to be accepted. We say he is ?smart",
429
Figure 2: Location Interface (After the annotation is performed)
since he knows the reference annotation, and
?lazy" because he will only define a new non-
parallel segment if it is significant, otherwise it
will just be left in the parallel segments. By sig-
nificant, we will define whether it is at least 20%
larger (in number of characters) than the parallel
segments. For instance, in the Korean example in
Figure 1, Hahah would be left in the English par-
allel segment, while ???????????????????????
??? ??* would not be in the Korean segment. We
will accept a job if the average of the scores in the
reference set is higher or equal than the pseudo
worker?s scores. This acceptor shall be denoted as
accept(lazy, a), where a is the number of refer-
ences used.
Another option is to use the automatic system?s
output as a baseline that workers must improve to
be accepted. We will also test this option and call
this acceptor accept(auto, a).
Result Combination Unlike the identification
task, where the result is binary and combining
multiple decisions is straightforward, the range of
results from this task is larger and combining them
is a challenge. Thus, we score each job based on
the WER on the reference set and use annotations
of the highest scoring job.
4 Experiments
To obtain results on the effectiveness of the meth-
ods described in Section 3, we will first perform
experiments using pre-annotated data. We use the
annotated dataset with tweets in Mandarin-English
from Sina Weibo created in (Ling et al., 2013).
It consists of approximately 4000 tweets crawled
from Sina Weibo that were annotated on whether
they contained parallel data and the location of the
parallel segments. In our experiment, we sample
1000 tweets from this dataset, where 602 tweets
were parallel and 398 were not.
1
We will not submit the same tasks using differ-
ent setups, since we would have to pay the cost of
the tasks multiple times. Furthermore, we know
the answers for all the questions in this controlled
experiment, the quality of a job can be evalu-
ated precisely by using all questions as references.
Thus, we will perform the task once, with a larger
number of workers and accepting and rejecting
jobs based on their real quality. Then, we will use
the resulting datasets and simulate the conditions
using different setups.
430
Acceptor avg(a) avg(r) d
accept(rand, 2, 2) 0.44 0.00 0.44
accept(rand, 3, 4) 0.44 0.00 0.44
accept(rand, 4, 4) 0.55 0.04 0.51
accept(bal, 2, 2) 0.69 0.09 0.60
accept(bal, 3, 4) 0.64 0.03 0.61
accept(bal, 4, 4) 0.76 0.15 0.61
Table 1: Agreement with the expert annotations
for different acceptors.
4.1 Identification Task
The 1000 tweets were distributed into 40 tasks
with 25 questions each (q = 25). Each task is
to be performed by 5 workers (n = 5) and upon
acceptance, a worker would be rewarded with 6
cents (p = 0.06). As we know the answers for
all the questions in this case, we will calculate the
Cohen?s Kappa between the responses of each job
and the expert annotator, and accept a job if it is
higher than 0.5. We decided to use Cohen?s kappa
to evaluate a job, rather than accuracy, since each
set of 25 questions does not contain the same num-
ber of positive and negative samples. For instance,
in a set of 20 negative samples, a worker would
achieve an accuracy of 80% if he simply answers
negatively to all questions, which is not an ade-
quate assessment of the job?s quality. On the other
hand, the Cohen?s Kappa balances the positive and
negative question in each task by using their prior
probabilities. In total, there were 566 jobs, where
200 where accepted and 366 were rejected.
Next, we pretended that we only have access to
4 references, which will be used for quality es-
timation and simulate the acceptances and rejec-
tions for each strategy. Table 1 shows the aver-
ages of the real Kappa values of accepted (col-
umn avg(a)) and rejected jobs (column avg(r))
using different acceptors. Our goal is to maximize
the number of acceptances with high Kappa val-
ues and minimize those that have low Kappa val-
ues. Thus, we define d as the difference between
avg(a) and avg(r). From the results, we observe
that using a balanced reference yields a much bet-
ter estimation of the jobs quality using our metric
d. Similar conclusions can be reached by compar-
ing accept(rand, 3, 4) with accept(bal, 3, 4) and
accept(rand, 4, 4) with accept(bal, 4, 4). Quality
predictors that use balanced reference sets achieve
1
We wished to annotate a sample where the number of
parallel posts is high, so that we would have enough samples
to perform the location task.
Acceptor prec recall F1 acc ?
Automatic 0.87 0.69 0.77 0.75 0.51
All jobs 0.75 0.84 0.8 0.74 0.44
accept(rand, 2, 2) 0.85 0.92 0.88 0.86 0.69
accept(rand, 3, 4) 0.84 0.93 0.88 0.85 0.68
accept(rand, 4, 4) 0.91 0.95 0.93 0.92 0.82
accept(bal, 2, 2) 0.94 0.94 0.94 0.92 0.84
accept(bal, 3, 4) 0.93 0.95 0.94 0.93 0.85
accept(bal, 4, 4) 0.94 0.93 0.93 0.92 0.84
Table 2: Parallel post prediction scores using dif-
ferent acceptors.
approximately the same results for d. However,
the setup accept(bal, 3, 4) has a lower Kappas for
both avg(a) and avg(r), which means that it is
less likely to reject good jobs at the cost of accept-
ing more bad jobs. This is desirable from an ethi-
cal perspective, since workers are not responsible
for errors in our quality prediction. Furthermore,
rejecting good jobs has a negative impact on the
progress of the task, since good workers may be
discouraged to perform more tasks.
Results on the identification task, obtained for
n = 3, are shown in Table 2. Naturally, us-
ing a balanced reference set yields better results,
since these have a higher d value. We can also
see the importance of quality prediction, since not
performing quality estimation (row All jobs) will
yield worse results than the automatic system.
Next, we will compare results using different
numbers of workers. We fix the quality predic-
tion methodology to accept(bal, 3, 4) and results
are shown in Table 3. We observe that in gen-
eral, using more workers will generate better re-
sults, but score gains from adding another worker
becomes lower as n increases. One problem for
n = 2 is the fact that there are many cases where
two workers with the same weight chose a posi-
tive and a negative answer, in which case, no de-
cision can be made, and we simply choose false
by default. This explains the high recall and low
precision values. However, this problem seems to
occur much less with higher values of n.
4.2 Location Task
For the location task, we used the predicted par-
allel posts the identification task with the setup
accept(bal, 3, 4) and n = 5. We preferred to use
this rather than using the expert annotations, since
it would not contain false positives, which does not
simulate a real situation. Then, we used 500 out of
431
# workers prec recall F1 acc ?
Automatic 0.87 0.69 0.77 0.75 0.51
1 0.86 0.85 0.85 0.82 0.64
2 0.85 0.95 0.90 0.87 0.72
3 0.93 0.95 0.94 0.93 0.85
4 0.94 0.96 0.95 0.94 0.87
5 0.96 0.96 0.96 0.95 0.90
Table 3: Identification scores for different n.
the 607 identified positive samples. This makes
20 tasks in total, with 25 questions (q = 25), and
each task would be run until 5 jobs are accepted
(n = 5). For this task, we set a payment of 30
cents (p = 0.3), since it is a more complex task.
Again, since we have the expert annotations for all
questions, we calculated the average WER on all
answers and rejected jobs scoring less than 0.6
2
.
This task is mainly focused on the quality pre-
diction of the workers, as the result combination
is done by finding the job with the highest score
in the reference set. This means, for an arbitrary
large n, all quality estimation methods will pro-
duce the same result, since we will find the best
job on the references eventually. However, bet-
ter quality estimation will allow us to find the best
jobs with lower n, which makes the task less ex-
pensive. Table 4 shows results using different se-
tups. In these results, we set aside 4 questions to
be used as references. We can see that for low n
(1 or 2), if we simply accept all jobs, the quality
of the results will be lower than the automatic sys-
tem. For n = 4, this approach can achieve a WER
score of 0.06. However, if we use the automatic
system as a baseline that jobs must surpass, we can
achieve this WER score with only two jobs, which
reduces the cost of this task by half. Yet, this is
strongly dependent on the automatic system, as a
worse system will be easier to match for the work-
ers. On the other hand, using the smart but lazy
pseudo worker, where we degrade the reference
annotations slightly, we can see that we can obtain
the 0.06 WER score using only the first worker. At
n = 2, we can see that the WER improves to 0.05,
which is lost for n = 3. This is because the pre-
diction of the quality of the job using the workers
is not always precise.
4.3 Machine Translation Results
Finally, we will perform an extrinsic test to see
how the improvements obtained by using crowd-
2
Determined empirically
Number of jobs 1 2 3 4 5
Automatic 0.16 0.16 0.16 0.16 0.16
All Jobs 0.23 0.21 0.07 0.06 0.06
accept(auto, 4) 0.09 0.06 0.06 0.06 0.06
accept(lazy, 4) 0.06 0.05 0.06 0.06 0.06
Table 4: Parallel data location scores for different
acceptors (rows) and different numbers of work-
ers. Each cell denotes the WER for that setup.
Auto (Pos) Crowd Expert Auto (All)
Size 483 479 483 908
EN-ZH 10.21 10.49 10.51 10.71
ZH-EN 7.59 7.87 7.82 8.02
Table 5: BLEU score comparison using different
corpora for MERT tuning. The Size row denotes
the number of sentences of each corpus, and the
EN-ZH and ZH-EN rows denote the BLEU scores
of the respective language pair and tuning dataset.
sourcing map to Machine Translations. We will
build an out of domain MT system using the FBIS
dataset (LDC2003E14), a corpus of 300K sen-
tence pairs from the news domain in the Chinese-
English pair using the Moses (Koehn et al., 2007)
pipeline. Due to the small size of our crowd-
sourced corpus, we will use it in the MERT tun-
ing (Och, 2003), and test its effects compared to
automatically extracted parallel data and the ex-
perts judgements. As the test set, we will use
1,500 sentence pairs from the Weibo gold standard
from Ling et al. (2013), that were not used in our
crowdsourcing experiment to prevent data over-
lap. For reordering, we use the MSD reordering
model (Axelrod et al., 2005) and as the language
model, we use a 5-gram model with Kneser-Ney
smoothing (Heafield, 2011). Finally, results are
presented with BLEU-4 (Papineni et al., 2002).
We build 3 tuning corpora, the automatically ex-
tracted corpus (denoted Auto), the crowdsourced
corpus (denoted Crowd) and the corpus annotated
by the expert (denoted Expert). This is done by
taking the 1000 tweets used in this experiment, se-
lect those that were identified as parallel accord-
ing to each criteria. For the automatic extraction,
the authors in (Ling et al., 2013) simply use all
tweets as parallel, which may influence the tun-
ing results. Thus, we test two versions of this cor-
pus, one where we take all samples as parallel (de-
noted Auto (All)), and one where we use the ex-
pert?s decision for the identification task only (de-
432
Pair Parallel Avg(en) cost(I) cost(L) total
en-ar 1512 8.3 $35.7 $43.2 $76.2
en-zh 1302 8.7 $35.7 $37.2 $70.2
en-ja 1155 7.9 $35.7 $33.0 $68.7
en-ko 1008 7.1 $35.7 $28.8 $64.5
en-ru 798 6.3 $35.7 $22.8 $58.5
all 5775 ? $178.5 $165.0 $343.5
Table 7: AMT costs for crowdsourced corpora
from Twitter.
noted Auto (Pos)). In the crowdsourcing case, we
use the accept(bal, 3, 4) setup, with n = 5, for the
identification task and the accept(lazy, 4) setup,
with n = 2, for the location task. From the re-
sulting parallel tweets, we also remove all tweets
that were used as reference in the accept(lazy, 4)
quality estimator, as this would give an unfair ad-
vantage to the crowdsourced corpora.
Results are shown in Table 5, where each cell
contains the average BLEU score in 5 MERT runs,
using a different tuning dataset. Surprisingly, us-
ing the whole set of automatically extracted cor-
pora actually achieves better results than using
carefully selected data that are parallel. We be-
lieve that is because many non-parallel segments
actually contain comparable information that can
be used to improve the weights during MERT tun-
ing. However, this does not mean that the qual-
ity of the automatically crawled corpus is better
than the crowdsourced and expert annotated cor-
pus. When using a similar number of parallel sen-
tences, we observe that using the crowdsourced
corpus yields better scores than the automatically
extracted corpora, comparable to experts annota-
tions. While results are not significantly better
than automatically extracted corpora, this suggests
that the crowdsourced corpora has a better overall
quality than automatically extracted corpora.
5 Five Language Twitter Parallel Corpus
Now that we have established the effectiveness of
our technique for extracting high-quality parallel
data in a scenario where we have gold standard
annotations, we apply it to creating parallel cor-
pora in five languages on Twitter, for which we
have no gold-standard parallel data: Arabic, Man-
darin, Japanese, Korean and Russian. Once again,
we use the extracted automatically Twitter cor-
pus from Ling et al. (2013) and deploy the task
in Mechanical Turk. We use the setup that ob-
tained the best results in Section 4. For the identi-
fication task, we used the accept(bal, 3, 4) setup,
with n = 5. The payment for each task was
0.06 dollars. Thus, for this task, each dollar spent
yields 70 annotated tweets. For the location task,
we used the accept(lazy, 4) setup, with n = 2
and each task was rewarded with 0.3 dollars. To
obtain the tweet sample, we filtered the corpora
in Ling et al. (2013) for tweets with alignment
scores higher than 0.1. Then, we uniformly ex-
tracted 2500 tweets for each language. To gener-
ate gold standard references, the authors manually
annotated 40 samples for each pair.
Table 7 contains information about the result-
ing corpora. The number of parallel sentences ex-
tracted from the 2500 tweets in each language pair
is shown in column Parallel and we can see that
this differs given the language pair. We can also
see in column Avg(en) that the average number of
English words is much smaller than what is seen
in more formal domains. Finally, Arabic parallel
data seems more predominant from our samples
followed by Mandarin, while Russian parallel data
seem scarcer.
6 Discriminative Parallel Data Detection
While the work in (Ling et al., 2013) used a linear
combination of three models, the alignment, lan-
guage and segment features, these weights were
determined manually. However, using the crowd-
sourced corpus (in Section 5), we will apply previ-
ously proposed methods that learn a classifier with
machine learning techniques as in related work
on finding parallel data (Resnik and Smith, 2003;
Munteanu and Marcu, 2005). In our work, we use
a max entropy classifier model, similar to that pre-
sented by Munteanu and Marcu (2005) to detect
parallel data in tweets. Our features are:
? Alignment feature - The baseline feature is
the alignment score from the work in (Ling et
al., 2013), and measures how well the paral-
lel segments align, which is derived from the
content-based matching methods for detect-
ing parallel data (Resnik and Smith, 2003).
? User features - An observation in (Ling et
al., 2013) is that a user that frequently posts
in parallel is likely to post more parallel mes-
sages. Based on this, we added the aver-
age alignment score from all messages of the
same user and the ratio of messages that are
predicted to be parallel as features.
433
Weibo (en-zh) Twitter (en-zh) Twitter (en-ar) Twitter (en-ru) Twitter (en-ko) Twitter (en-ja)
Alignment 0.781 0.599 0.721 0.692 0.635 0.570
+User 0.814 0.598 0.721 0.705 0.650 0.566
+Length 0.839 0.603 0.725 0.706 0.650 0.569
+Repetition 0.849 0.652 0.763 0.729 0.655 0.579
+Language 0.849 0.668 0.782 0.737 0.747 0.584
Table 6: Classification Results using a 10-fold cross validation over different datasets. Each cell contains
the F-measure using a given dataset and an incremental set of features.
? Repetition features - There are many words
that are not translated, such as hashtags, at
mentions, numbers and named entities. So, if
we see these repeated twice in the same post,
it can be used as a strong cue that this was
the result of a translation. Hence, we define
features for each of these cases, that trigger if
either of these occur in multiples of two times
in the same post. Named Entities were iden-
tified using a naive approach by considering
words with capital letters.
? Length feature - It is known that the length
differences between parallel sentences can
be modelled by a normal distribution (Gale
and Church, 1991). Hence, we used parallel
data in the respective language to determine
(??, ??
2
), which lets us calculate the likelihood
of two hypothesized segments being parallel.
Since we did not have annotated parallel data
for this domain, we used the top 2000 scoring
parallel sentences from the respective Twitter
dataset in (Ling et al., 2013).
? Language feature - It is common for non-
English words to be found in English seg-
ments, such as names of foreign celebri-
ties, numbers and hashtags. However, when
this happens to the majority of the words in
a segment that is supposed to be English,
it may indicated that there was an error in
the language detection. The same happens
with non-English segments. We used the
same naive approach to detect languages as
in (Ling et al., 2013), where we calculate the
ratio of number of words in the English seg-
ment and the total number of words from the
segment detected as English and the ratio of
the number of Foreign words and the total
number of words in the Foreign segment ,de-
tected by their unicode ranges. This was also
included in the work in (Ling et al., 2013).
Results using a 10 fold cross-validation are
shown in Table 6. In general, we can see that the
classifier performs worse in Twitter datasets com-
pared to the Weibo dataset. We believe that this is
because parallel sentences extracted from Twitter
are smaller, due to the 140 character limit, which
does not hold in Sina Weibo. Each parallel En-
glish segment from the Sina Weibo parallel data
contains 15.4 words on average. On other hand,
we see in Table 7 that this number is smaller in
the parallel data from Twitter. This means that the
aligner will have a much smaller range of words to
align when detecting parallel data, which makes it
more difficult to find parallel segments.
As for the features, we observe that by defin-
ing these simple features, we can get a signifi-
cant improvement over previous baselines. For
the User feature, we see that the improvements
in the Weibo dataset are much larger than in
the Twitter datasets. This is because the Twitter
dataset was crawled uniformly, whereas the Weibo
dataset was focused on users that post parallel
data frequently. Thus, in the Weibo dataset there
more posts that were posted by the same user,
which does not happen as frequently in the Twitter
dataset. As for the Length feature, we can see that
it yields a small but consistent improvement over
all datasets. Repetition based features also lead to
improvements across all datasets, and produces a
5% improvement in the English-Mandarin Twitter
dataset. Finally, language based features also add
another improvement over previous results.
7 Conclusions
We presented a crowdsourcing approach to extract
parallel data from tweets. As opposed to meth-
ods to crowdsource translations, our tasks do not
require workers to translate sentences, but to find
them in tweets. Our method is divided into two
tasks. First, we identify which tweets contain
translations, and we show that multiple worker?s
jobs can be combined to obtain results compara-
434
ble to those of expert annotators. Secondly, tweets
that are found to contain translations are given
to other workers to locate the parallel segments,
where we can also obtain high quality results.
Then, we use our method to extract high quality
parallel data from Twitter in 5 language pairs. Fi-
nally, we improve the automatic identification of
tweets with translations by using a max entropy
classifier trained on the crowdsourced data.
We are currently extracting more data and the
crowdsourced parallel data from Twitter will made
be available to the public.
References
[Ambati and Vogel2010] Vamshi Ambati and Stephan
Vogel. 2010. Can crowds build parallel corpora
for machine translation systems? In Proceedings
of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon?s Mechan-
ical Turk, Stroudsburg, PA, USA. Association for
Computational Linguistics.
[Ambati et al.2012] Vamshi Ambati, Stephan Vogel,
and Jaime Carbonell. 2012. Collaborative workflow
for crowdsourcing translation. In Proceedings of the
ACM 2012 Conference on Computer Supported Co-
operative Work, CSCW ?12, pages 1191?1194, New
York, NY, USA. ACM.
[Axelrod et al.2005] Amittai Axelrod, Ra Birch Mayne,
Chris Callison-burch, Miles Osborne, and David
Talbot. 2005. Edinburgh system description for the
2005 iwslt speech translation evaluation. In Pro-
ceedings International Workshop on Spoken Lan-
guage Translation (IWSLT.
[Bannard and Callison-burch2005] Colin Bannard and
Chris Callison-burch. 2005. Paraphrasing with
bilingual parallel corpora. In In ACL-2005, pages
597?604.
[Bojar et al.2013] Ond
?
rej Bojar, Christian Buck, Chris
Callison-Burch, Christian Federmann, Barry Had-
dow, Philipp Koehn, Christof Monz, Matt Post,
Radu Soricut, and Lucia Specia. 2013. Find-
ings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 1?
44, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
[Das and Petrov2011] Dipanjan Das and Slav Petrov.
2011. Unsupervised part-of-speech tagging with
bilingual graph-based projections. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies - Volume 1, HLT ?11, pages 600?609,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Gale and Church1991] William A. Gale and Ken-
neth W. Church. 1991. A program for aligning
sentences in bilingual corpora. In Proceedings of
the 29th Annual Meeting on Association for Com-
putational Linguistics, ACL ?91, pages 177?184,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Gale et al.1992] William A. Gale, Kenneth W. Church,
and David Yarowsky. 1992. Using bilingual materi-
als to develop word sense disambiguation methods.
[Ganitkevitch et al.2012] Juri Ganitkevitch, Yuan Cao,
Jonathan Weese, Matt Post, and Chris Callison-
Burch. 2012. Joshua 4.0: Packing, PRO, and para-
phrases. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 283?291,
Montr?al, Canada, June. Association for Computa-
tional Linguistics.
[Goutte et al.2012] Cyril Goutte, Marine Carpuat, and
George Foster. 2012. The impact of sentence
alignment errors on phrase-based machine transla-
tion performance. In Proc. of AMTA.
[Heafield2011] Kenneth Heafield. 2011. KenLM:
faster and smaller language model queries. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 187?197, Edinburgh, Scot-
land, United Kingdom, July.
[Jehl et al.2012] Laura Jehl, Felix Hieber, and Stefan
Riezler. 2012. Twitter translation using translation-
based cross-lingual retrieval. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion, pages 410?421, Montr?al, Canada, June. Asso-
ciation for Computational Linguistics.
[Koehn et al.2007] Philipp Koehn, Hieu Hoang,
Alexandra Birch, Chris Callison-burch, Richard
Zens, Rwth Aachen, Alexandra Constantin, Mar-
cello Federico, Nicola Bertoldi, Chris Dyer, Brooke
Cowan, Wade Shen, Christine Moran, and Ondrej
Bojar. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June.
Association for Computational Linguistics.
[Ling et al.2013] Wang Ling, Guang Xiang, Chris Dyer,
Alan Black, and Isabel Trancoso. 2013. Microblogs
as parallel corpora. In Proceedings of the 51st An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?13. Association for Computational
Linguistics.
[Liu et al.2011] Feifan Liu, Fei Liu, and Yang Liu.
2011. Learning from chinese-english parallel data
for chinese tense prediction. In IJCNLP, pages
1116?1124.
[Munteanu and Marcu2005] Dragos Munteanu and
Daniel Marcu. 2005. Improving machine transla-
tion performance by exploiting comparable corpora.
Computational Linguistics, 31(4):477?504.
435
[Ng et al.2003] Hwee Tou Ng, Bin Wang, and Yee Seng
Chan. 2003. Exploiting parallel texts for word sense
disambiguation: An empirical study. In Proceedings
of ACL03, pages 455?462.
[Och2003] Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 41st Annual Meeting on Association
for Computational Linguistics - Volume 1, ACL ?03,
pages 160?167, Stroudsburg, PA, USA. Association
for Computational Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,
Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a
method for automatic evaluation of machine trans-
lation. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 311?318, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Post et al.2012] Matt Post, Chris Callison-Burch, and
Miles Osborne. 2012. Constructing parallel cor-
pora for six indian languages via crowdsourcing. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 401?409, Montr?al,
Canada, June. Association for Computational Lin-
guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.
Smith. 2003. The web as a parallel corpus. Compu-
tational Linguistics, 29:349?380.
[Schwarck et al.2010] Florian Schwarck, Alexander
Fraser, and Hinrich Sch?tze. 2010. Bitext-based
resolution of german subject-object ambiguities. In
Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter of
the Association for Computational Linguistics, HLT
?10, pages 737?740, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
[Smith et al.2010] Jason R. Smith, Chris Quirk, and
Kristina Toutanova. 2010. Extracting parallel sen-
tences from comparable corpora using document
level alignment. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 403?411, Stroudsburg, PA,
USA. Association for Computational Linguistics.
[Specia et al.2005] Lucia Specia, Maria Das Gra?as,
Volpe Nunes, and Mark Stevenson. 2005. Exploit-
ing parallel texts to produce a multilingual sense
tagged corpus for word sense disambiguation. In
Proceedings of RANLP-05, Borovets, pages 525?
531.
[Zaidan and Callison-Burch2011] Omar F. Zaidan and
Chris Callison-Burch. 2011. Crowdsourcing trans-
lation: professional quality from non-professionals.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, HLT ?11, pages
1220?1229, Stroudsburg, PA, USA. Association for
Computational Linguistics.
436
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 80?86,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
The CMU Submission for the Shared Task on Language Identification in
Code-Switched Data
Chu-Cheng Lin Waleed Ammar Lori Levin Chris Dyer
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{chuchenl,wammar,lsl,cdyer}@cs.cmu.edu
Abstract
We describe the CMU submission for
the 2014 shared task on language iden-
tification in code-switched data. We
participated in all four language pairs:
Spanish?English, Mandarin?English,
Nepali?English, and Modern Standard
Arabic?Arabic dialects. After describing
our CRF-based baseline system, we
discuss three extensions for learning from
unlabeled data: semi-supervised learning,
word embeddings, and word lists.
1 Introduction
Code switching (CS) occurs when a multilingual
speaker uses more than one language in the same
conversation or discourse. Automatic idenefica-
tion of the points at which code switching occurs
is important for two reasons: (1) to help sociolin-
guists analyze the frequency, circumstances and
motivations related to code switching (Gumperz,
1982), and (2) to automatically determine which
language-specific NLP models to use for analyz-
ing segments of text or speech.
CS is pervasive in social media due to its in-
formal nature (Lui and Baldwin, 2014). The first
workshop on computational approaches to code
switching in EMNLP 2014 organized a shared task
(Solorio et al., 2014) on identifying code switch-
ing, providing training data of multilingual tweets
with token-level language-ID annotations. See
?2 for a detailed description of the shared task.
This short paper documents our submission in the
shared task.
We note that constructing a CS data set that is
annotated at the token level requires remarkable
manual effort. However, collecting raw tweets is
easy and fast. We propose leveraging both labeled
and unlabeled data in a unified framework; condi-
tional random field autoencoders (Ammar et al.,
2014). The CRF autoencoder framework consists
of an encoding model and a reconstruction model.
The encoding model is a linear-chain conditional
random field (CRF) (Lafferty et al., 2001) which
generates a sequence of labels, conditional on a
token sequence. Importantly, the parameters of
the encoding model can be interpreted in the same
way a CRF model would. This is in contrary to
generative model parameters which explain both
the observation sequence and the label sequence.
The reconstruction model, on the other hand, inde-
pendently generates the tokens conditional on the
corresponding labels. Both labeled and unlabeled
data can be efficiently used to fit parameters of this
model, minimizing regularized log loss. See ?4.1
for more details.
After modeling unlabeled token sequences, we
explore two other ways of leveraging unlabeled
data: word embeddings and word lists. The word
embeddings we use capture monolingual distribu-
tional similarities and therefore may be indicative
of a language (see ?4.2). A word list, on the other
hand, is a collection of words which have been
manually or automatically constructed and share
some property (see ?4.3). For example, we extract
the set of surface forms in monolingual corpora.
In ?5, we describe the experiments and discuss
results. According to the results, modeling unla-
beled data using CRF autoencoders did not im-
prove prediction accuracy. Nevertheless, more ex-
periments need to be run before we can conclude
this setting. On the positive side, word embed-
dings and word lists have been shown to improve
CS prediction accuracy, provided they have decent
coverage of tokens in the test set.
2 Task Description
The shared task training data consists of code?
switched tweets with token-level annotations.
The data is organized in four language pairs:
English?Spanish (En-Es), English?Nepali (En-
80
Ne), Mandarin?English (Zh-En) and Modern
Standard Arabic?Arabic dialects (MSA-ARZ).
Table 1 shows the size of the data sets provided
for the shared task in each language pair.
For each tweet in the data set, the user ID, tweet
ID, and a list of tokens? start offset and end offset
are provided. Each token is annotated with one
of the following labels: lang1, lang2, ne (i.e.,
named entities), mixed (i.e., mixed parts of lang1
and lang2), ambiguous (i.e., cannot be identified
given context), and other.
Two test sets were used to evaluate each sub-
mission for the shared task in each language pair.
The first test set consists of Tweets, similar to the
training set. The second test set consists of token
sequences from a surprise genre. Since partici-
pants were not given the test sets, we only report
results on a Twitter test set (a subset of the data
provided for shared task participants). Statistics
of our train/test data splits are given in Table 5.
lang. pair split tweets tokens users
En?Ne all 9, 993 146, 053 18
train 7, 504 109, 040 12
test 2, 489 37, 013 6
En?Es all 11, 400 140, 738 9
train 7, 399 101, 451 6
test 4, 001 39, 287 3
Zh?En all 994 17, 408 995
train 662 11, 677 663
test 332 5, 731 332
MSA?ARZ all 5, 862 119, 775 7
train 4, 800 95, 352 6
test 1, 062 24, 423 1
Table 1: Total number of tweets, tokens, and Twit-
ter user IDs for each language pair. For each lan-
guage pair, the first line represents all data pro-
vided to shared task participants. The second and
third lines represent our train/test data split for the
experiments reported in this paper. Since Twit-
ter users are allowed to delete their tweets, the
number of tweets and tokens reported in the third
and fourth columns may be less than the number
of tweets and tokens originally annotated by the
shared task organizers.
3 Baseline System
We model token-level language ID as a sequence
of labels using a linear-chain conditional ran-
dom field (CRF) (Lafferty et al., 2001) described
in ?3.1 with the features in ?3.2.
3.1 Model
A linear-chain CRF models the conditional proba-
bility of a label sequence y given a token sequence
x and given extra context ?, as follows:
p(y | x,?) =
exp?
>
?
|x|
i=1
f(x, y
i
, y
i?1
,?)
?
y
?
exp?
>
?
|x|
i=1
f(x, y
?
i
, y
?
i?1
,?)
where ? is a vector of feature weights, and f is
a vector of local feature functions. We use ? to
explicitly represent context information necessary
to compute the feature functions described below.
In a linear-chain structure, y
i
only depends on
observed variables x,? and the neighboring labels
y
i?1
and y
i+1
. Therefore, we can use dynamic
programming to do inference in run time that is
quadratic in the number of unique labels and lin-
ear in the sequence length. We use L-BFGS to
learn the feature weights ?, maximizing the L
2
-
regularized log-likelihood of labeled examples L:
``
supervised
(?) =
c
L
2
||?||
2
2
+
?
?x,y??L
log p(y | x,?)
After training the model, we use again use dy-
namic programming to find the most likely label
sequence, for each token sequence in the test set.
3.2 Features
We use the following features in the baseline sys-
tem:
? character n-grams (loweredcased tri- and quad-
grams)
? prefixes and suffixes of lengths 1, 2, 3 and 4
? unicode page of the first character
1
? case (first-character-uppercased vs. all-
characters-uppercased vs. all-characters-
alphanumeric)
? tweet-level language ID predictions from two
off-the-shelf language identifiers: cld2
2
and
ldig
3
1
http://www.unicode.org/charts/
2
https://code.google.com/p/cld2/
3
https://github.com/shuyo/ldig
81
encod
ing 
recon
struc
tion 
x 
yi-1 yi yi+1 
xi-1 xi xi+1 ? ? ? 
? 
Figure 1: A diagram of the CRF autoencoder
4 Using Unlabeled Data
In ?3, we learn the parameters of the CRF model
parameters in a standard fully supervised fashion,
using labeled examples in the training set. Here,
we attempt to use unlabeled examples to improve
our system?s performance in three ways: model-
ing unlabeled token sequences in the CRF autoen-
coder framework, word embeddings, and word
lists.
4.1 CRF Autoencoders
A CRF autoencoder (Ammar et al., 2014) consists
of an input layer, an output layer, and a hidden
layer. Both input and output layer represent the
observed token sequence. The hidden layer rep-
resents the label sequence. Fig. 1 illustrates the
model dependencies for sequence labeling prob-
lems with a first-order Markov assumption. Con-
ditional on an observation sequence x and side in-
formation ?, a traditional linear-chain CRF model
is used to generate the label sequence y. The
model then generates x? which represents a recon-
struction of the original observation sequence. El-
ements of this reconstruction (i.e., x?
i
) are then in-
dependently generated conditional on the corre-
sponding label y
i
using simple categorical distri-
butions.
The parametric form of the model is given by:
p(y, x? | x,?) =
|x|
?
i=1
?
x?
i
|y
i
?
exp?
>
?
|x|
i=1
f(x, y
i?1
, y
i
, i,?)
?
y
?
exp?
>
?
|x|
i=1
f(x, y
?
i?1
, y
?
i
, i,?)
where ? is a vector of CRF feature weights, f is a
vector of local feature functions (we use the same
features described in ?3.2), and ?
x?
i
|y
i
are categor-
ical distribution parameters of the reconstruction
model representing p(x?
i
| y
i
).
We can think of a label sequence as a low-
cardinality lossy compression of the correspond-
ing token sequence. CRF autoencoders explic-
itly model this intuition by creating an information
bottleneck where label sequences are required to
regenerate the same token sequence despite their
limited capacity. Therefore, when only unlabeled
examples U are available, we train CRF autoen-
coders by maximizing the regularized likelihood
of generating reconstructions x?, conditional on x,
marginalizing values of label sequences y:
``
unsupervised
(?,?) = c
L
2
||?||
2
2
+R
Dirichlet
(?, ?)+
?
?x,x???U
log
?
y:|y|=|x|
p(y, x? | x)
where R
Dirichlet
is a regularizer based on a vari-
ational approximation of a symmetric Dirichlet
prior with concentration parameter ? for the re-
construction parameters ?.
Having access to labeled examples, it is easy to
modify this objective to learn from both labeled
and unlabeled examples as follows:
``
semi
(?,?) = c
L
2
||?||
2
2
+R
Dirichlet
(?, ?)+
c
unlabeled
?
?
?x,x???U
log
?
y:|y|=|x|
p(y, x? | x)+
c
labeled
?
?
?x,y??L
log p(y | x)
We use block coordinate descent to optimize
this objective. First, we use c
em
iterations of
the expectation maximization algorithm to opti-
mize the ?-block while the ?-block is fixed, then
we optimize the ?-block with c
lbfgs
iterations of
L-BFGS (Liu et al., 1989) while the ?-block is
fixed.
4
4.2 Unsupervised Word Embeddings
For many NLP tasks, using unsupervised
word representations as features improves
accuracy (Turian et al., 2010). We use
word2vec (Mikolov et al., 2013) to train
100?dimensional word embeddings from a
large Twitter corpus of about 20 million tweets
extracted from the live stream, in multiple lan-
guages. We define an additional feature function
4
An open source efficient c++ imple-
mentation of our method can be found at
https://github.com/ldmt-muri/alignment-with-openfst
82
in the CRF autoencoder model ?4.1 for each of
the 100 dimensions, conjoined with the label y
i
.
The feature value is the corresponding dimension
for x
i
. A binary feature indicating the absence of
word embeddings is fired for out-of-vocabulary
words (i.e., words for which we do not have word
embeddings). The token-level coverage of the
word embeddings for each of the languages or
dialects used in the training data is reported in
Table 2.
4.3 Word List Features
While some words are ambiguous, many words
frequently occur in only one of the two lan-
guages being considered. An easy way to iden-
tify the label of such unambiguous words is to
check whether they belong to the vocabulary of
either language. Moreover, named entity recog-
nizers typically rely on gazetteers of named enti-
ties to improve their performance. We generalize
the notion of using monolingual vocabularies and
gazetteers of named entities to general word lists.
Using K word lists {l
1
, . . . , l
K
}, when a token x
i
is labeled with y
i
, we fire a binary feature that con-
joins ?y
i
, ?(x
i
? l
1
), . . . , ?(x
i
? l
K
)?, where ? is
an indicator boolean function. We use the follow-
ing word lists:
? Hindi and Nepali Wikipedia article titles
? multilingual named entities from the JRC
dataset
5
and CoNLL 2003 shared task
? word types in monolingual corpora in MSA,
ARZ, En and Es.
? set difference between the following pairs of
word lists: MSA-ARZ, ARZ-MSA, En-Es, Es-
En.
Transliteration from Devanagari The Nepali?
English tweets in the dataset are romanized. This
renders our Nepali word lists, which are based
on the Devanagari script, useless. Therefore, we
transliterate the Hindi and Nepali named entities
lists using a deterministic phonetic mapping. We
romanize the Devanagari words using the IAST
scheme.
6
We then drop all accent marks on the
characters to make them fit into the 7?bit ASCII
range.
5
http://datahub.io/dataset/jrc-names
6
http://en.wikipedia.org/wiki/
International_Alphabet_of_Sanskrit_
Transliteration
embeddings word lists
language coverage coverage
ARZ 30.7% 68.8%
En 73.5% 55.7%
MSA 26.6% 76.8%
Ne 14.5% 77.0%
Es 62.9% 78.0%
Zh 16.0% 0.7%
Table 2: The type-level coverage of annotated data
according to word embeddings (second column)
and according to word lists (third column), per lan-
guage.
5 Experiments
We compare the performance of five models for
each language pair, which correspond to the five
lines in Table 3. The first model, ?CRF? is the
baseline model described in ?3. The second ?CRF
+ U
test
? and the third ?CRF + U
all
? are CRF au-
toencoder models (see ?4.1) with two sets of un-
labeled data: (1) U
test
which only includes the test
set,
7
and (2) U
all
which includes the test set as well
as all tweets by the set of users who contributed
any tweets in L. The fourth model ?CRF + U
all
+
emb.? is a CRF autoencoder which uses word em-
bedding features (see ?4.2), as well as the features
described in ?3.2. Finally, the fifth model ?CRF +
U
all
+ emb. + lists? further adds word list features
(see ?4.3). In all but the ?CRF? model, we adopt a
transductive learning setup.
Since the CRF baseline is used as the encoding
part of the CRF autoencoder model, we use the
supervisedly-trained CRF parameters to initialize
the CRF autoencoder models. The categorical dis-
tributions of the reconstruction model are initial-
ized with discrete uniforms. We set the weight
of the labeled data log-likelihood c
labeled
= 0.5,
the weight of the unlabeled data log-likelihood
c
unlabeled
= 0.5, the L
2
regularization strength
c
L
2
= 0.3, the concentration parameter of the
Dirichlet prior ? = 0.1, the number of L-BFGS
iterations c
LBFGS
= 4, and the number of EM iter-
ations c
EM
= 4.
8
We stop training after 50 itera-
tions of block coordinate descent.
7
U
test
is potentially useful when the test set belongs to a
different domain than the labeled examples, which is often
referred to as ?domain adaptation?. However we were unable
to test this hypothesis since all the CS annotations we had
access to are from Twitter.
8
Hyper-parameters c
L
2
and ? were tuned using cross-
validation. The remaining hyper-parameters were not tuned.
83
config En?Ne MSA?ARZ En?Es Zh?En
CRF 95.2% 80.5% 94.6% 94.9%
+T
test
95.2% 80.6% 94.6% 94.9%
+T
all
95.2% 80.7% 94.6% 94.9%
+emb. 95.3% 81.3% 95.1% 95.0%
+lists 97.0% 81.2% 96.7% 95.3%
Table 3: Token level accuracy results for each of
the four language pairs.
label predicted predicted
MSA ARZ
true MSA 93.9% 5.3%
true ARZ 32.1% 65.2%
Table 4: Confusion between MSA and ARZ in the
Baseline configuration.
Results. The CRF baseline results are reported
in the first line in Table 3. For three language
pairs, the overall token-level accuracy ranges be-
tween 94.6% and 95.2%. In the fourth language
pair, MSA-ARZ, the baseline accuracy is 80.5%
which indicates the relative difficulty of this task.
The second and third lines in Table 3 show the
results when we use CRF autoencoders with the
unlabeled test set (U
test
), and with all unlabeled
tweets (U
all
), respectively. While semi-supervised
learning did not hurt accuracy on any of the lan-
guages, it only resulted in a tiny increase in accu-
racy for the Arabic dialects task.
The fourth line in Table 3 extends the CRF au-
toencoder model (third line) by adding unsuper-
vised word embedding features. This results in
an improvement of 0.6% for MSA-ARZ, 0.5% for
En-Es, 0.1% for En-Ne and Zh-En.
The fifth line builds on the fourth line by adding
word list features. This results in an improvement
of 1.7% in En-Ne, 1.6% in En-Es, 0.4% in Zh-En,
and degradation of 0.1% in MSA-ARZ.
Analysis and Discussion The baseline perfor-
mance in the MSA-ARZ task is considerably
lower than those of the other tasks. Table 4 illus-
trates how the baseline model confuses lang1 and
lang2 in the MSA-ARZ task. While the baseline
system correctly labels 93.9% of MSA tokens, it
only correctly labels 65.2% of ARZ tokens.
Although the reported semi-supervised results
did not significantly improve on the CRF baseline,
more work needs to be done in order to conclude
these results:
lang. pair |U
test
| |U
all
| |L|
En?Ne 2489 6230 7504
MSA?ARZ 1062 2520 4800
Zh?En 332 332 663
En?Es 4001 7177 7399
Table 5: Number of tweets inL, U
test
andU
all
used
for semi-supervised learning of CRF autoencoders
models.
? Use an out-of-domain test set where some adap-
tation to the test set is more promising.
? Vary the number of labeled examples |L| and
the number of unlabeled examples |U|. Table 5
gives the number of labeled and unlabeled ex-
amples used for training the model. It is pos-
sible that semi-supervised learning would have
been more useful with a smaller |L| and a larger
|U|.
? Tune c
labeled
and c
unlabeled
.
? Split the parameters ? into two subsets: ?
labeled
and ?
unlabeled
; where ?
labeled
are the parameters
which have a non-zero value for any input x in
L and ?
unlabeled
are the remaining parameters in
? which only have non-zero values with unla-
beled examples but not with the labeled exam-
ples.
? Use a richer reconstruction model.
? Reconstruct a transformation of the token se-
quences instead of their surface forms.
? Train a token-level language ID model trained
on a large number of languages, as opposed to
disambiguating only two languages at a time.
Word embeddings improve the results for all
language pairs, but the largest improvement is in
MSA-ARZ and En-Es. Looking into the word em-
beddings coverage of those languages (i.e., MSA,
ARZ, Es, En in Table 2), we find that they are bet-
ter covered than the other languages (Ne, Zh). We
conclude that further improvements on En-Ne and
Zh-En may be expected if they are better repre-
sented in the corpus used to learn word embed-
dings.
As for the word lists, the largest improvement
we get is the romanized word lists of Nepali,
which have a 77.0% coverage and improve the
accuracy by 1.7%. This shows that our translit-
erated word lists not only cover a lot of tokens,
and are also useful for language ID. The Spanish
84
Config lang1 lang2 ne
+lists 84.1% 76.5% 73.7%
-lists 84.2% 77.1% 71.5%
Table 6: F?Measures of two Arabic configura-
tions. lang1 is MSA. lang2 is ARZ.
word lists also have a wide coverage, improving
the overall accuracy by 1.6%. The overall accu-
racy of the Arabic dialects slightly degrades with
the addition of the word lists. Closer inspection
in table 6 reveals that it improves the F?Measure
of the named entities at the expense of both MSA
(lang1) and ARZ (lang2).
6 Related Work
Previous work on identifying languages in a mul-
tilingual document includes (Singh and Gorla,
2007; King and Abney, 2013; Lui et al., 2014).
Their goal is generally more about identifying the
languages that appear in the document than intra?
sentential CS points.
Previous work on computational models of
code?switching include formalism (Joshi, 1982)
and language models that encode syntactic con-
straints from theories of code?switching, such as
(Li and Fung, 2013; Li and Fung, 2014). These
require the existence of a parser for the languages
under consideration. Other work on prediction
of code?switching points, such as (Elfardy et al.,
2013; Nguyen and Dogruoz, 2013) and ours, do
not depend upon such NLP infrastructure. Both of
the aforementioned use basic character?level fea-
tures and dictionaries on sequence models.
7 Conclusion
We have shown that a simple CRF baseline with
a handful of feature templates obtains strong re-
sults for this task. We discussed three methods
to improve over the supervised baseline using un-
labeled data: (1) modeling unlabeleld data using
CRF autoencoders, (2) using pre-trained word em-
beddings, and (3) using word list features.
We show that adding word embedding features
and word lists features is useful when they have
good coverage of words in a data set. While mod-
est improvements are observed due to modeling
unlabeled data with CRF autoenocders, we iden-
tified possible directions to gain further improve-
ments.
While bilingual disambiguation was a good first
step for identifying code switching, we suggest a
reformulation of the task such that each label can
take on one of many languages.
Acknowledgments
We thank Brendan O?Connor who helped assem-
ble the Twitter dataset. We also thank the work-
shop organizers for their hard work, and the re-
viewers for their comments. This work was
sponsored by the U.S. Army Research Labora-
tory and the U.S. Army Research Office under
contract/grant number W911NF-10-1-0533. The
statements made herein are solely the responsibil-
ity of the authors.
References
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2014.
Conditional random field autoencoders for unsuper-
vised structured prediction. In Proc. of NIPS.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412?416. Springer.
John J. Gumperz. 1982. Discourse Strategies. Studies
in Interactional Sociolinguistics. Cambridge Univer-
sity Press.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In Proceedings of
the 9th Conference on Computational Linguistics -
Volume 1, COLING ?82, pages 145?150, Czechoslo-
vakia. Academia Praha.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. Proceedings of the
2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1110?1119. As-
sociation for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proc. of ICML.
Ying Li and Pascale Fung. 2013. Improved mixed lan-
guage speech recognition using asymmetric acous-
tic model and language model with code-switch in-
version constraints. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2013 IEEE International
Conference on, pages 7368?7372, May.
Ying Li and Pascale Fung. 2014. Code switch lan-
guage modeling with functional head constraint. In
Acoustics, Speech and Signal Processing (ICASSP),
2014 IEEE International Conference on, pages
4913?4917, May.
85
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the lim-
ited memory bfgs method for large scale optimiza-
tion. Mathematical Programming.
Marco Lui and Timothy Baldwin. 2014. Accurate
language identification of twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17?25, Gothenburg,
Sweden, April. Association for Computational Lin-
guistics.
Marco Lui, Han Jey Lau, and Timothy Baldwin. 2014.
Automatic detection and language identification of
multilingual documents. Transactions of the Asso-
ciation of Computational Linguistics, 2:27?40.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proc. of ICLR.
Dong Nguyen and Seza A. Dogruoz. 2013. Word level
language identification in online multilingual com-
munication. Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Processing,
pages 857?862. Association for Computational Lin-
guistics.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identi-
fication of languages and encodings in a multilingual
document. In Building and Exploring Web Corpora
(WAC3-2007): Proceedings of the 3rd Web as Cor-
pus Workshop, Incorporating Cleaneval, volume 4,
page 95.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 384?394,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
86
