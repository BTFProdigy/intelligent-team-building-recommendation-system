Sample Selection for Statistical Grammar Induction 
Rebecca  Hwa 
Division of Engineering and Applied Sciences 
? Harvard University 
Cambridge, MA 02138 USA 
rebecca@eecs.harvard.edu 
Abst rac t  
Corpus-based grz.mmar induction relies on us- 
ing many hand-parsed sentences as training 
examples. However, the construction of a 
training corpus with detailed syntactic analy- 
sis for every sentence is a labor-intensive task. 
We propose to use sample selection methods 
to minimize the amount of annotation eeded 
in the training data, thereby reducing the 
workload of the human annotators. This pa- 
per shows that the amount of annotated train- 
ing data can be reduced by 36% without de- 
grading the quality of the induced grammars. 
1 In t roduct ion :  
Many learning problems in the domain of 
natural anguage processing need supervised 
training. For instance, it is difficult to induce 
a grammar from a corpus of raw text; but the 
task becomes much easier when the training 
sentences are supplemented with their parse 
trees. However, appropriate supervised train- 
ing data may be difficult to obtain. Existing 
corpora might not contain the relevant ype 
of supervision, and the data might not be 
in the domain of interest. For example, one 
might need morphological nalyses of the lex- 
icon in addition to the parse trees for inducing 
a grammar, or one might be interested in pro- 
cessing non-English languages for which there 
is no annotated corpus. Because supervised 
training typically demands ignificant human 
involvement (e.g., annotating the parse trees 
of sentences by hand), building a new corpus is 
a labor-intensive task. Therefore, it is worth- 
while to consider ways of minimizing the size 
of the corpus to reduce the effort spent by an- 
notators. 
* This material is based upon work supported by the 
National Science Foundation under Grant No. IRI 
9712068. We thank Wheeler Rural for his plotting 
tool; and Stuart Shieber, Lillian Lee, Ric Crabbe, and 
the anonymous reviewers for their comments on the 
paper. 
There are two possible directions: one 
might attempt o reduce the amount of anno- 
tations in each sentence, as was explored by 
Hwa (1999); alternatively, one might attempt 
to reduce the number of training sentences. 
In this paper, we consider the latter approach 
using sample selection, an interactive l arning 
method in which the machine takes the initia- 
tive of selecting potentially beneficial train- 
ing examples for the humans to annotate. If 
the system could accurately identify a subset 
of examples with high Training Utility Values 
(TUV) out of a pool of unlabeled ata, the 
annotators would not need to waste time on 
processing uninformative examples. 
We show that sample selection can be 
applied to grammar induction to produce 
high quality grammars with fewer annotated 
training sentences. Our approach is to use 
uncertainty-based evaluation functions that 
estimate the TUV of a sentence by quantify- 
ing the grammar's uncertainty about assign- 
ing a parse tree to this sentence. We have 
considered two functions. The first is a sire- 
? ple heuristic that approximates the grammar's 
uncertainty in terms of sentence lengths. The 
second computes uncertainty in terms of the 
tree entropy of the sentence. This metric is 
described in detail later. 
This paper presents an empirical study 
measuring the effectiveness of our evaluation 
functions at selecting training sentences from 
the Wall Street Journal (WSJ) corpus (Mar- 
cuset al, 1993) for inducing rammars. Con- 
ducting the experiments with training pools 
of different sizes, we have found that sample 
selection based on tree entropy reduces a large 
training pool by 36% and a small training pool 
by 27%. These results u res t  that sample se- 
lection can significantly reduce \]~uman effort 
exerted in building training corpora. 
45 
2 Sample  Se lec t ion  
Unlike traditional learning systems that re- 
ceive training examples indiscriminately, a 
learning system that uses sample selection 
actively influences its progress by choosing 
new examples to incorporate into its training 
set. Sample selection works with two types 
of learning systems: a committee of learners 
or a single learner. The committee-based se-
lection algorithm works with multiple learn- 
ers, each maintaining a different hypothesis 
(perhaps pertaining to different aspects of the 
problem). The candidate xamples that led 
to the most disagreements among the differ- 
ent learners are considered to have the high- 
est TUV (Cohn et al, 1994; Freund et al, 
1997). For computationally intensive prob- 
lems such as grammar induction, maintaining 
multiple learners may be an impracticality. In 
this work, we explore sample selection with a 
single learner that keeps just one working hy- 
pothesis at all times. 
Figure 1 outlines the single-learner sample 
selection training loop in pseudo-code. Ini- 
tially, the training set, L ,  consists of a small 
number of labeled examples, based on which 
the learner proposes its first hypothesis of 
the target concept, C. Also available to the 
learner is a large pool of uulabeled training 
candidates, U. In each training iteration, the 
selection algorithm, Select(n, U, C, f) ,  ranks 
the  candidates of U according to their ex- 
pected TUVs and returns the n candidates 
with the highest values. The algorithm com- 
putes the expected TUV of each candidate, 
u E U, with an evaluation function, f(u, C). 
This function may possibly rely on the hy- 
pothesis concept C to estimate the utility of 
a candidate u. The set of the n chosen candi- 
dates are then labeled by human and added 
to the existing training set. Rnnning the 
learning algorithm~ Train(L), on the updated 
training set, the system proposes a new hy- 
pothesis consistent with all the examples een 
thus far. The loop continues until one of three 
stopping conditions is met: the hypothesis i  
considered close enough to the target concept, 
all candidates are labeled, or all human re- 
sources are exhausted. 
Sample selection may be beneficial for many 
learning tasks in natural language process- 
ing. Although there exist abundant collec- 
tions of raw text, the high expense of man- 
ually annotating the text sets a severe lim- 
itation for many learning algorithms in nat- 
U is a Set of unlabeled candidates. 
L is a set of labeled training examples. 
C is the current hypothesis. 
Initialize: 
C +-- Train(L). 
Repeat  
N ~-- Select(n, U, C, f).  
U~-U-N.  
L ~-- L t2 Label(N). 
C ~ Train(L). 
Unti l  (C ---- Ctrue)Or (U  = O) or (human stops) 
Figure 1: The pseudo-code for the sample se- 
lection learning algorithm 
ural language processing. Sample selection 
presents an attractive solution to offset this 
labeled data sparsity problem. Thus far, it 
has been successfully applied to several classi- 
fication applications. Some examples include 
text categorization (Lewis and Gale, 1994), 
part-of-speech tagging (Engelson and Dagan, 
1996), word-sense disambiguation (Fujii et al, 
1998), and prepositional-phrase attachment 
(Hwa, 2000). 
More difficult are learning problems whose 
objective is not classification, but generation 
of complex structures. One example in this di- 
rection is applying sample selection to seman- 
tic parsing (Thompson et al, 1999), in which 
sentences are paired with their semantic rep- 
resentation using a deterministic shift-reduce 
parser. Our work focuses on another complex 
natural anguage learning problem: inducing 
a stochastic ontext-free grammar that can 
generate syntactic parse trees for novel test 
sentences. 
Although abstractly, parsing with a gram- 
mar can be seen as a classification task of de- 
termining the structure of a sentence by se- 
lecting one tree out of a set of possible parse 
trees, there are two major distinctions that 
differentiate it from typical classification prob- 
lems. First, a classifier usually chooses from 
a fixed set of categories, but in our domain, 
every sentence has a different set of possible 
parse trees. Second, for most classification 
problems, the the number of the possible cate- 
gories is relatively small, whereas the number 
of potential parse trees for a sentence is expo- 
nential with respect o the sentence length. 
46 
3 Grammar  Induct ion  
The degree of difficulty of the task of learning 
a grammar from data depends on the quantity 
and quality of the training supervision. When 
the training corpus consists of a larg e reservoir 
of fully annotated parse trees, it is possible 
to directly extract a grammar based on these 
parse trees. The success of recent high-quality 
parsers (Charniak, 1997; Collins, 1997) relies 
on the availability of such treebank corpora. 
To work with smaller training corpora, the 
learning system would require even more in- 
formation about the examples than their syn- 
tactic parse trees. For instance, Hermjakob 
and Mooney (1997) have described a learning 
system that can build a deterministic shift- 
reduce parser from a small set of training 
examples with the aid of detailed morpho- 
logical, syntactical, and semantic knowledge 
databases and step-by-step guidance from hu- 
man experts. 
The induction task becomes more chal- 
lenging as the amount of  supervision in the 
training data and background knowledge de- 
creases. To compensate for the missing infor- 
mation, the learning process requires heuristic 
search to find locally optimal grammars. One 
form of partially supervised ata might spec- 
ify the phrasal boundaries without specify- 
ing their labels by bracketing each constituent 
unit with a pair of parentheses (McNaughton, 
1967). For example, the parse tree for the sen- 
tence '~Several fund managers expect a rough 
market this morning before prices stablize." 
is labeled as "((Several fund managers) (ex- 
pect ((a rough market) (this morning)) (be- 
fore (prices tabilize))).)" As shown in Pereira 
and Schabes (1992), an essentially unsuper- 
vised learning algorithm such as the Inside- 
Outside re-estimation process (Baker, 1979; 
Lari and Young, 1990) can be modified to take 
advantage of these bracketing constraints. 
For our sample selection experiment, we 
chose to work under the more stringent con- 
dition of partially supervised training data, as 
described above, because our ultimate goal is 
to minimize the amount of annotation done 
by humans in terms of both the number of 
sentences and the number of brackets within 
the sentences. Thus, the quality of our in- 
duced grammars hould not be compared to 
those extracted from a fully annotated train- 
ing corpus. The learning algorithm we use is 
a variant of the Inside-Outside algorithm that 
induces grammars expressed in the Probabilis- 
tic Lexicalized Tree Insertion Grammar ep- 
resentation (Schabes and Waters, 1993; Hwa, 
1998). This formalism's Context-free quiva- 
lence and its lexicalized representation make 
the training process efficient and computa- 
tionally plausible. 
4 Se lec t ive  Sampl ing  Eva luat ion  
Funct ions  
In this paper, we propose two uncertainty- 
based evaluation functions for estimating the 
training utilities of the candidate sentences. 
The first is a simple heuristic that uses the 
length of a sentence to estimate uncertain- 
ties. The second function computes uncer- 
tainty in terms of the entropy of the parse 
trees that the hypothesis-grammar generated 
for the sentence. 
4.1 Sentence Length  
Let us first consider a simple evaluation 
function that estimates the training utility 
of a candidate without consulting the cur- 
rent hypothesis-grammar, G. The function 
ften(s,G) coarsely approximates the uncer- 
tainty of a candidate sentence s with its 
length: 
flen(S, G) = length(s). 
The intuition behind this function is based 
on the general observation that longer sen- 
tences tend to have complex structures and 
introduce more opportunities for ambiguous 
parses. Since the scoring only depends on 
sentence lengths, this naive evaluation func- 
tion orders the training pool deterministically 
regardless of either the current state of the 
grammar or the annotation of previous train- 
ing sentences. This approach as one major 
advantage: it is easy to compute and takes 
negligible processing time. 
4.2 Tree Ent ropy  
Sentence length is not a very reliable indi- 
cator of uncertainty. To measure the un- 
certainty of a sentence more accurately, the 
evaluation function must base its estimation 
on the outcome of testing the sentence on 
the hypothesis-grammar. When a stochastic 
grammar parses a sentence, it generates a set 
of possible trees and associates a likelihood 
value with each. Typically, the most likely 
tree is taken to be the best parse for the sen- 
tence. 
We propose an evaluation function that 
considers the probabilities of all parses. The 
47 
set of probabilities of the possible parse trees 
for a sentence defines a distribution that in- 
dicates the grammar's uncertainty about the 
structure of the sentence. For example, a uni- 
form distribution signifies that the grammar 
is at its highest uncertainty because all the 
parses are equally likely; whereas a distribu- 
tion resembling an impulse function suggests 
that the grammar is very certain because it 
finds one parse much more likely than all oth- 
ers. To quantitatively characterize a distribu- 
tion, we compute its entropy. 
Entropy measures the uncertainty ofassign- 
ing a value to a random variable over a dis- 
tribution. Informally speaking, it is the ex- 
pected number of bits needed to encode the 
assignment. A higher entropy value signifies 
a higher degree of uncertainty. At the highest 
uncertainty, the random variable is assigned 
one of n values over a uniform distribution, 
and the outcome would require log2 (n) bits to 
encode. 
More formally, let V be a discrete random 
variable that can take any possible outcome 
in set V. Let p(v) be the density function 
p(v) = Pr(Y = v), v E l). The entropy H(V) 
is the expected negative log likelihood of ran- 
dom variable V: 
H (V) = -EX  ( logdv(V ) ) ). 
= - 
vEY 
Further details about the properties of en- 
tropy can be found in textbooks on informa- 
tion theory (Cover and Thomas, 1991). 
Determining the parse tree for a sentence 
from a set of possible parses can be viewed as 
assigning a value to a random variable. Thus, 
a direct application of the entropy definition 
to the probability distribution of the parses for 
sentence s in grammar G computes its tree en- 
tropy, TE(s, G), the expected number of bits 
needed to encode the distribution of possible 
parses for s. Note that we cannot compare 
sentences ofdifferent lengths by their entropy. 
For two sentences of unequal engths, both 
with uniform distributions, the entropy of the 
longer one is higher. To normalize for sen- 
tence length, we define an evaluation function 
that computes the similarity between the ac- 
tual probability distribution and the uniform 
distribution for a sentence of that length. For 
a sentence s of length l, there can be at most 
0(2 l) equally likely parse trees and its maxi- 
real entropy is 0(l) bits (Cover and Thomas, 
1991). Therefore, we define the evaluation 
function, fte(s, G) to be the tree entropy di- 
vided by the sentence l ngth. 
TE(s, G) 
Ire(s, G) = length(s)" 
We now derive the expression for TE(s, G). 
Suppose that a sentence s can be generated by 
a grammar G with some non-zero probability, 
Pr(s \[ G). Let V be the set of possible parses 
that G generated for s. Then the probability 
that sentence s is generated by G is the sum 
of the probabilities of its parses. That is: 
Pr(s \[G) = ~Pr (v lG) .  
vEY 
Note that Pr(v \[ G) reflects the probability of 
one particular parse tree, v, in the grammar 
out of all possible parse trees for all possible 
sentences that G accepts. But in order to ap- 
ply the entropy definition from above, we need 
to specify adistribution of probabilities for the 
parses of sentence s such that 
vr(v Is, o )= 1. 
vEV 
Pr(v \[ s, G) indicates the likelihood that v is 
the correct parse tree out of a set of possible 
parses for s according to grammar G. It is 
also the density function, p(v), for the distri- 
bution (i.e., the probability of assigning v to 
a random variable V). Using Bayes Rule and 
noting that Pr(v, s \[ G) = Pr(v \[ G) (because 
the existence of tree v implies the existence of 
sentence s), we get: 
v(v) = vr (v  I s, G) = Vr( . ,  s I G) = Vr(v I G) 
Pr(s I G) Pr(s I G)" 
Replacing the generic density function term 
in the entropy definition, we derive the expres- 
sion for TE(s, G), the tree entropy of s: 
TE(s,G) = H(V) 
- - - -  -- Z PCv) Iog2P(V ) 
vEV 
= - P (s I a) log2(? (s I c )  ) 
vEY 
Pr(v l C) 
= - ~ Pr(s \[G) l?g2Pr(v \[ G) 
vEY 
+ ~ Pr(v \[ G) log hPr (s lG)  
vev Pr(s \[ G) 
48 
~,cv Pr(v l G) log2 Pr(v l G) 
Pr(s I G) 
E sv P (v I a) 
+ logs P (s I - ,  i b) 
~vev Pr(v \] G) l?g 2Pr (v IG  ) 
Pr(s 1 a) 
+ log 2 Pr(s I G) 
Using the bottom-up, dynamic program- 
ming technique of computing Inside Proba- 
bilities (Lari and Young, 1990), we can ef- 
ficiently compute the probability of the sen- 
tence, Pr(s I G). Similarly, the algorithm 
can be modified to compute the quantity 
~\]v~vPr( v I G)log2(Pr(v I G)) (see Ap- 
pendix A). 
5 Exper imenta l  Setup  
To determine the effectiveness of selecting 
training examples with the two proposed eval- 
uation functions, we compare them against 
a baseline of random selection (frand(S, G) = 
rand()). The task is to induce grammars from 
selected sentences in the Wall Street Journal 
(WSJ) corpus, and to parse unseen test sen- 
tences with the trained gr~.mmars. Because 
the vocabulary size (and the grammar size 
by extension) is very large, we have substi- 
tuted the words with their part-of-speech tags 
to avoid additional computational complexity 
in training the grammar. After replacing the 
words with part-of-speech tags, the vocabu- 
lary size of the corpus is reduced to 47 tags. 
We repeat the study for two different 
candidate-pool sizes. For the first experiment, 
we assume that there exists an abundant sup-- 
ply of unlabeled ata. Based on empirical ob- 
servations (as will be shown in Section 6), for 
the task we are considering, the induction al- 
gorithm typically reaches its asymptotic limit 
after training with 2600 sentences; therefore, 
it is sufficient to allow for a candidate-pool size 
of U = 3500 unlabeled WSJ sentences. In the 
second experiment, we restrict the size of the 
candidate-pool such that U contains only 900 
unlabeled sentences. This experiment studies 
how the paucity of training data affects the 
evaluation functions. 
For both experiments, each of the three 
evaluation functions: frand, ften, and fte, is 
applied to the sample selection learning algo- 
rithm shown in Figure 1, where concept C is 
the current hypothesis-grammar G, and L, the 
set of labeled training data; initially consists 
of 100 sentences. In every iteration, n = 100 
new sentences are picked from U to be added 
to L, and a new C is induced from the updated 
L. After the hypothesis-grammar is updated, 
it is tested. The quality of the induced gram- 
max is judged by its ability to generate cor- 
rect parses for unseen test sentences. We use 
the consistent bracketing metric (i.e., the per- 
centage of brackets in the proposed parse not 
crossing brackets of the true parse) to mea- 
sure parsing accuracy 1. To ensure the staffs- 
tical significance of the results, we report the 
average of ten trials for each experiment 2. 
6 Results 
The results of the two experiments are graph- 
ically depicted in Figure 2. We plot learning 
rates of the induction processes using train- 
ing sentences selected by the three evaluation 
functions. The learning rate relates the qual- 
ity of the induced grammars to the amount of 
supervised training data available. In order 
for the induced grammar to parse test sen- 
tences with higher accuracy (x-axis), more su- 
pervision (y-axis) is needed. The amount of 
supervision is measured in terms of the num- 
ber of brackets rather than sentences because 
it more accurately quantifies the effort spent 
by the human annotator. Longer sentences 
tend to require more brackets than short ones, 
and thus take more time to analyze. We deem 
one evaluation function more effective than 
another if the smallest set of sentences it se- 
lected can train a grammar that performs at 
least as well as the grammar trained under the 
other function and if the selected data con- 
tains considerably fewer brackets than that of 
the other function. 
Figure 2(a) presents the outcomes of the 
first experiment, in which the evaluation func- 
tions select training examples out of a large 
candidate-pool. We see that overall, sample 
selection has a positive effect on the learning 
IThe unsupervised induction algorithm induces 
grammars that generate binary branching trees so that 
the number  of proposed brackets in a sentence is al- 
ways one fewer than the length of the sentence. The 
WSJ  corpus, on the other hand, favors a more fiat- 
tened tree structure with considerably fewer brackets 
per sentence. The consistent bracketing metric does 
not unfairly penalize a proposed parse tree for being 
binary branching. 
2We generate different candidate-pools by moving 
a fixed-size window across WSJ  sections 02 through 
05, advancing 400 sentences for each trial. Sec~n 23 
is always used for testing. 
49 
E 
/ o 
a ~  . . . .  
Pa.~ing accura,.~ on the ~ 
(a) 
i t f 
s : 
? J 
. / J  
...o?..o-- ~ 
Farsir~ accuracy on the tt~t 
(b) 
Figure 2: The learning rates of the induction processes using examples elected by the three 
evaluation functions for (a) when the candidate-pool is large, and (b) when the candidate-pool 
is small. 
grammar set 
baseline-26 
length-17 
tree entropy-!4 
11 avg. training brackets t-test on bracket.avg. \[ avg. score 
33355 N/A 80.3 
30288 better 80.3 
21236 better 80.4 
t-test on score avg 
N/A 
not sig. worse 
not sig. worse 
Table 1: Summary of pair-wise t-test with 95% confidence comparing the best set of grammars 
induced with the baseline (after 26 selection iterations) to the sets of grammars induced under 
the proposed evaluation functions (ften after 17 iterations, fte after 14 iterations). 
rate of the induction process. For the base- 
line case, the induction process uses frand, 
in which training sentences are randomly se- 
lected. The resulting grammars achieves an 
average parsing accuracy of 80.3% on the test 
sentences after seeing an average of 33355 
brackets in the training data. The learning 
rate of the tree entropy evaluation function, 
fte, progresses much faster than the baseline. 
To induce a grammar that reaches the same 
80.3% parsing accuracy with the examples e- 
lected by fte, the learner equires, on average, 
21236 training brackets, reducing the amount 
of annotation by 36% comparing to the base- 
line. While the simplistic sentence length 
evaluation function, f~en, is less helpful, its 
learning rate still improves lightly faster than 
the baseline. A grammar of comparable qual- 
ity can be induced from a set of training exam- 
ples selected by fzen containing an average of 
30288 brackets. This provides a small reduc- 
tion of 9% from the baseline 3. We consider a
set of grammars to be comparable to the base- 
3In terms of the number of sentences, the baseline 
f~d used 2600 randomly chosen training sentences; 
.fze,~ selected the 1700 longest sentences as training 
data; and fte selected 1400 sentences. 
line if its mean test score is at least as high 
as that of the baseline and if the difference of 
the means is not statistically significant (us- 
ing pair-wise t-test at 95% confidence). Ta- 
ble 1 summarizes the statistical significance of 
comparing the best set of baseline grammars 
with those of of f~en and ffte. 
Figure 2(b) presents the results of the sec- 
ond experiment, in which the evaluation func- 
tions only have access to a small candidate 
pool. Similar to the previous experiment, 
grammars induced from training examples e- 
lected by fte require significantly less annota-  
tions than the baseline. Under the baseline, 
frand, to train grammars with 78.5% parsing 
accuracy on test data, an average of 11699 
brackets (in 900 sentences) is required. In con- 
trast, fte can induce a comparable grammar 
with an average of 8559 brackets (in 600 sen- 
tences), providing a saving of 27% in the num- 
ber of training brackets. The simpler evalua- 
tion function f~n out:performs the baseline 
as well; the 600 sentences it selected have an 
average of 9935 brackets. Table 2 shows the 
statistical significance of these comParisons. 
A somewhat surprising outcome of the sec- 
ond study is that the grammars induced from 
50 
grammar set 
baseline-9 
length-6 
tree entropy-6 
tree entropy-8 
II avg. training brackets t-test on bracket avg. avg. score t-~est on score a~ 
11699 N/A 78.5 N/A 
9936 better 78.5 not sig. worse 
8559 better 78.5 not sig. worse 
11242 better 79.1 better 
test vg. 
Table 2: Summary of pair-wise t-test with 95% confidence comparing the best set of grammars 
induced with the baseline (after 9 selection iterations) to the sets of grammars induced under 
the proposed evaluation functions (ften after 6 iterations, fte after 6 and 8 iterations). 
the three methods did not parse with the same 
accuracy when all the sentences from the un- 
labeled pool have been added to the training 
set. Presenting the training examples in dif- 
ferent orders changes the search path of the 
induction process. Trained on data selected 
by fte, the induced grammar parses the test 
sentences with 79.1% accuracy, a small but 
statistically significant improvement over the 
baseline. This suggests that, when faced with 
a dearth of training candidates, fte can make 
good use of the available data to induce gram- 
mars that are comparable to those directly in- 
duced from more data. 
7 Conc lus ion  and  Future  Work  
This empirical study indicates that sample se- 
lection can significantly reduce the human ef- 
fort in parsing sentences for inducing gram- 
mars. Our proposed evaluation function using 
tree entropy selects helpful training examples. 
Choosing from a large pool of unlabeled can- 
didates, it significantly reduces the amount of 
training annotations needed (by 36% in the 
experiment). Although the reduction is less 
dramatic when the pool of candidates i  small 
(by 27% in the experiment), the training ex- 
amples it selected helped to induce slightly 
better grammars. 
The current work suggests many potential 
research directions on selective sampling for 
grammar induction. First, since the ideas be- 
hind the proposed evaluation fimctions are 
general and independent of formalisms, we 
would like to empirically determine their ef- 
fect on other parsers. Next, we shall explore 
alternative formulations of evaluation func- 
tions for the single-learner system. The cur- 
rent approach uses uncertainty-based evalua- 
tion functions; we hope to consider other fac- 
tors such as confidence about the parameters 
of the grammars and domain knowledge. We 
also plan to focus on the constituent units 
within a sentence as training examples. Thus, 
the evaluation functions could estimate the 
training utilities of constituent units rather 
than full sentences. Another area of interest 
is to experiment with committee-based sam- 
ple selection using multiple learners. Finally, 
we are interested in applying sample selection 
to other natural anguage learning algorithms 
that have been limited by the sparsity of an- 
notated ata. 
Re ferences  
James K. Baker. 1979. Trainable grammars for 
speech recognition. In Proceedings of the Spring 
Conference of the Acoustical Society of Amer- 
ica, pages 547-550, Boston, MA, June. 
Eugene Charniak. 1997. Statistical parsing with 
a context-free grammar and word statistics. In 
Proceedings of the AAAI, pages 598-603, Prov- 
idence, RI. AAAI Press/MIT Press. 
David Cohn, Les Atlas, and Richard Ladner. 1994. 
Improving generalization with active learning. 
Machine Learning, 15(2):201-221. 
Michael Collins. 1997. Three generative, lexi- 
calised models for statistical parsing. In Pro- 
ceedings of the 35th Annual Meeting of the A CL, 
pages 16-23, Madrid, Spain. 
Thomas M. Cover and Joy A. Thomas. 1991. El- 
ements of Information Theory. John Wiley. 
Sean P. Engelson and Ido Dagan. 1996. Mhaimiz- 
ing manual annotation cost in supervised train- 
ing from copora. In Proceedings ofthe 34th An- 
nual Meeting of the ACL, pages 319-326. 
Yoav Freund, H. Sebastian Seung, Eli Shamir, and 
Naftali Tishby. 1997. Selective sampling using 
the query by committee algorithm. Machine 
Learning, 28(2-3):133-168. 
Atsushi Fujii, Kentaro Inui, Takenobu Tokunaga, 
and Hozumi Tanaka. 1998. Selective sampling 
for example-based word sense disambiguation. 
Computational Linguistics, 24(4):573-598, De- 
cember. 
Ulf Hermjakob and Raymond J. Mooney. 1997. 
Learning parse and translation decisions from 
examples with rich context. In Proceedings o/ 
the Association for Computational Linguistics, 
pages 482-489. 
Rebecca Hwa. 1998. An empiric~al evaluation 
of probabilistic lexicaiized tree insertion gram- 
51 
mars. In Proceedings off COLING-ACL, vol- 
ume 1, pages 557-563. 
Rebecca Hwa. 1999. Supervised grammar in- 
duction using training data with limited con- 
stituent information. In Proceedings of37th An- 
nual Meeting of the ACL, pages 73-79, June. 
Rebecca Hwa. 2000. Learning Probabilistic Lex- 
icalized Grammars for Natural Language Pro- 
cessing. Ph.D. thesis, ttarvard University. 
Forthcoming. 
K. Lari and S.J. Young. 19!70. The estimation 
of stochastic ontext-free grammars using the 
inside-outside algorithm. Computer Speech and 
Language, 4:35-56. 
David D. Lewis and William A. Gale. 1994. A se- 
quential algorithm for training text classifiers. 
In Proceedings ofthe 17th Annual International 
ACM SIGIR Conference on Research and De- 
velopment inInformation Retrieval, pages 3-12. 
Mitchell Marcus, Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building 
a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313--330. 
Robert McNaughton. 1967. Parenthesis gram- 
mars. Journal off the ACM, 2(3):490--500. 
Fernando Pereira nd Yves Schabes. 1992. Inside- 
Outside reestimation from partially bracketed 
corpora. In Proceedings of the 30th Annual 
Meeting o\] the ACL, pages 128-135, Newark, 
Delaware. 
Yves Schabes and Richard Waters. 1993. Stochas- 
tic lexicalized context-free grammar. In Pro- 
ceedings of the Third International Workshop 
on Parsing Technologies, pages 257-266. 
Cynthia A. Thompson, Mary Elaine Califf, and 
Raymond J. Mooney. 1999. Active learning 
for natural anguage parsing and information 
extraction. In Proceedings of 1CML-99, pages 
406-414, Bled, Slovenia. 
A Ef f ic ient  Computat ion  o f  T ree  
Ent ropy  
The tree entropy of a sentence depends on the 
quantity ~vevPr (v  \[G)log~(Pr(v \] G)) de- 
scribed in Section 4.2, a snm of an exponential 
number of parses. Fortunately, through a dy- 
namic programming algorithm similar to the 
computation of the Inside Probabilities, this 
quantity can be efficiently computed. The ba- 
sic idea is to compose the tree entropy of the 
entire sentence from the tree entropy of the 
subtrees. 
For illustrative purposes, we describe the 
computation process using a PCFG grammar 
expressed in Chomsky Normal Form, in which 
each rule can have two forms: X ~ YZ 
or X ---r a, where X, Y, Z are variables over 
non-terminal symbols and a is a variable over 
terminal symbols. Moreover, let the sym- 
bol S be the start symbol of the grammar 
G. Following the notation of Lari and Young, 
we denote the inside probability as e(X, i,j), 
which represents the probability that a non- 
terminal X :~ wi . . .wj .  Similarly, we define 
a new function h(X, i, j) to represent the cor- 
responding entropy for the set of subtrees. 
h(X, i , j )  =-  P r ( .  I a)log (Pr(, IV)). 
vEX~wi...w~ 
Therefore, ~vev Pr(v \[G)log 2 Pr(v l G ) can 
be expressed as h(S, 1, n). 
We compute all possible h(X, i , j )  re- 
cursively. The base case is h(X,i , i )  = 
-e(X ,  i, i) log2 (e(X, i, i)) since a non-terminal 
X can generate the symbol wi in exactly one 
way. For the more general case, h(X, i,j), we 
consider all the possible rules with X on the 
left hand side that might have contributed to 
build X =~ wi . . . wj. 
j -1  
hr, 
k=i (x~YZ) 
The function hy, z,k(X, i , j )  is a portion of 
h(X, i , j )  where Y =~ wi . . .wk and Z ~ 
Wk+l... wj. The non-terminals Yand Z may, 
in turn, generate their substrings with mul- 
tiple parses. Let there be a parses for Y 
wi. . .  Wk and f~ parses for Z ~ Wk+l.. .w i.
Let x denote the event of X --r YZ; y E 
Yl,. . . ,Ya; and z E z l , . . . , zz .  The proba- 
bility of one of the a x fl possible parses is 
Pr(x)Pr(y)Pr(z), and hY, z,k is computed by 
summing over all possible parses: 
hy, z,k(X, i, j) 
= -- ~ ,z  Pr(x)Pr(y)Pr(z)x 
log 2 (Pr (x)Pr (y)Pr (z) ) 
= - Z~,~Pr(x)Pr(y)Pr(z)? 
\[log 2Pr(x) + log 2 Pr(y) + log 2 Pr(z)\] 
= -Pr (x )  log 2 Pr(x)e(Y, i, k)e(Z, k+l, j) 
+Pr(x)h(Y,i, k)e(Z,k + 1,j) 
+Pr(x)e(Y, i, k)h(Z, k + 1,j). 
These equations can be modified to compute 
the tree entropy of sentences using a Prob- 
abilistic Lexicalized Tree Insertion Grammar 
(Hwa, 2000). 
52 
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 60?68,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Correcting Automatic Translations through Collaborations between MT
and Monolingual Target-Language Users
Joshua S. Albrecht and Rebecca Hwa and G. Elisabeta Marai
Department of Computer Science
University of Pittsburgh
{jsa8,hwa,marai}@cs.pitt.edu
Abstract
Machine translation (MT) systems have
improved significantly; however, their out-
puts often contain too many errors to com-
municate the intended meaning to their
users. This paper describes a collabora-
tive approach for mediating between an
MT system and users who do not under-
stand the source language and thus cannot
easily detect translation mistakes on their
own. Through a visualization of multi-
ple linguistic resources, this approach en-
ables the users to correct difficult transla-
tion errors and understand translated pas-
sages that were otherwise baffling.
1 Introduction
Recent advances in machine translation (MT) have
given us some very good translation systems.
They can automatically translate between many
languages for a variety of texts; and they are
widely accessible to the public via the web. The
quality of the MT outputs, however, is not reliably
high. People who do not understand the source
language may be especially baffled by the MT out-
puts because they have little means to recover from
translation mistakes.
The goal of this work is to help monolingual
target-language users to obtain better translations
by enabling them to identify and overcome er-
rors produced by the MT system. We argue for a
human-computer collaborative approach because
both the users and the MT system have gaps in
their abilities that the other could compensate. To
facilitate this collaboration, we propose an inter-
face that mediates between the user and the MT
system. It manages additional NLP tools for the
source language and translation resources so that
the user can explore this extra information to gain
enough understanding of the source text to correct
MT errors. The interactions between the users and
the MT system may, in turn, offer researchers in-
sights into the translation process and inspirations
for better translation models.
We have conducted an experiment in which we
asked non-Chinese speakers to correct the outputs
of a Chinese-English MT system for several short
passages of different genres. They performed the
correction task both with the help of the visual-
ization interface and without. Our experiment ad-
dresses the following questions:
? To what extent can the visual interface help
the user to understand the source text?
? In what way do factors such as the user?s
backgrounds, the properties of source text,
and the quality of the MT system and other
NLP resources impact that understanding?
? What resources or strategies are more help-
ful to the users? What research directions
do these observations suggest in terms of im-
proving the translation models?
Through qualitative and quantitative analysis of
the user actions and timing statistics, we have
found that users of the interface achieved a more
accurate understanding of the source texts and
corrected more difficult translation mistakes than
those who were given the MT outputs alone. Fur-
thermore, we observed that some users made bet-
ter use of the interface for certain genres, such
as sports news, suggesting that the translation
model may be improved by a better integration of
document-level contexts.
60
2 Collaborative Translation
The idea of leveraging human-computer collab-
orations to improve MT is not new; computer-
aided translation, for instance, was proposed by
Kay (1980). The focus of these efforts has been on
improving the performance of professional trans-
lators. In contrast, our intended users cannot read
the source text.
These users do, however, have the world knowl-
edge and the language model to put together co-
herent sentences in the target-language. From the
MT research perspective, this raises an interesting
question: given that they are missing a transla-
tion model, what would it take to make these users
into effective ?decoders?? While some transla-
tion mistakes are recoverable from a strong lan-
guage model alone, and some might become read-
ily apparent if one can choose from some possi-
ble phrasal translations; the most difficult mistakes
may require greater contextual knowledge about
the source. Consider the range of translation re-
sources available to an MT decoder?which ones
might the users find informative, handicapped as
they are for not knowing the source language?
Studying the users? interactions with these re-
sources may provide insights into how we might
build a better translation model and a better de-
coder.
In exploring the collaborative approach, the de-
sign considerations for facilitating human com-
puter interaction are crucial. We chose to make
available relatively few resources to prevent the
users from becoming overwhelmed by the options.
We also need to determine how to present the in-
formation from the resources so that the users can
easily interpret them. This is a challenge because
the Chinese processing tools and the translation
resources are imperfect themselves. The informa-
tion should be displayed in such a way that con-
flicting analyses between different resources are
highlighted.
3 Prototype Design
We present an overview of our prototype for a col-
laborative translation interface, named The Chi-
nese Room1. A screen-shot is shown in Figure 1. It
1The inspiration for the name of our system came from
Searle?s thought experiment(Searle, 1980). We realize that
there are major differences between our system and Searle?s
description. Importantly, our users get to insert their knowl-
edge rather than purely operate based on instructions. We felt
Figure 1: A screen-shot of the visual interface. It
consists of two main regions. The left pane is a
workspace for users to explore the sentence; the
right pane provides multiple tabs that offer addi-
tional functionalities.
is a graphical environment that supports five main
sources of information and functionalities. The
space separates into two regions. On the left pane
is a large workspace for the user to explore the
source text one sentence at a time. On the right
pane are tabbed panels that provide the users with
access to a document view of the MT outputs as
well as additional functionalities for interpreting
the source. In our prototype, the MT output is ob-
tained by querying Google?s Translation API2. In
the interest of exploiting user interactions as a di-
agnostic tool for improving MT, we chose infor-
mation sources that are commonly used by mod-
ern MT systems.
First, we display the word alignments between
MT output and segmented Chinese3. Even with-
out knowing the Chinese characters, the users
can visually detect potential misalignments and
poor word reordering. For instance, the automatic
translation shown in Figure 1 begins: Two years
ago this month... It is fluent but incorrect. The
crossed alignments offer users a clue that ?two?
and ?months? should not have been split up. Users
can also explore alternative orderings by dragging
the English tokens around.
Second, we make available the glosses for
words and characters from a bilingual dictionary4.
the name was nonetheless evocative in that the user requires
additional resources to process the input ?squiggles.?
2http://code.google.com/apis/translate/
research
3The Chinese segmentation is obtained as a by-product of
Google?s translation process.
4We used the Chinese-English Translation Lexi-
61
The placement of the word gloss presents a chal-
lenge because there are often alternative Chi-
nese segmentations. We place glosses for multi-
character words in the column closer to the source.
When the user mouses over each definition, the
corresponding characters are highlighted, helping
the user to notice potential mis-segmentation in
the Chinese.
Third, the Chinese sentence is annotated with
its parse structure5. Constituents are displayed
as brackets around the source sentence. They
have been color-coded into four major types (noun
phrase, verb phrases, prepositional phrases, and
other). Users can collapse and expand the brack-
ets to keep the workspace uncluttered as they work
through the Chinese sentence. This also indicates
to us which fragments held the user?s focus.
Fourth, based on previous studies reporting
that automatic translations may improve when
given decomposed source inputs (Mellebeek et al,
2005), we allow the users to select a substring
from the source text for the MT system to trans-
late. We display the N -best alternatives in the
Translation Tab. The list is kept short; its purpose
is less for reranking but more to give the users a
sense of the kinds of hypotheses that the MT sys-
tem is considering.
Fifth, users can select a substring from the
source text and search for source sentences from
a bilingual corpus and a monolingual corpus that
contain phrases similar to the query6. The re-
trieved sentences are displayed in the Example
Tab. For sentences from the bilingual corpus, hu-
man translations for the queried phrase are high-
lighted. For sentences retrieved from the monolin-
gual corpus, their automatic translations are pro-
vided. If the users wished to examine any of the
retrieved translation pairs in detail, they can push
it onto the sentence workspace.
4 Experimental Methodology
We asked eight non-Chinese speakers to correct
the machine translations of four short Chinese pas-
con released by the LDC; for a handful of char-
acters that serve as function words, we added the
functional definitions using an online dictionary
http://www.mandarintools.com/worddict.html.
5It is automatically generated by the Stanford Parser for
Chinese (Klein and Manning, 2003).
6We used Lemur (2006) for the information retrieval
back-end; the parallel corpus is from the Federal Broadcast
Information Service corpus; the monolingual corpus is from
the Chinese Gigaword corpus.
Figure 2: The interface for users who are correct-
ing translations without help; they have access to
the document view, but they do not have access to
any of the other resources.
sages, with an average length of 11.5 sentences.
Two passages are news articles and two are ex-
cerpts of a fictional work. Each participant was
instructed to correct the translations for one news
article and one fictional passage using all the re-
sources made available by The Chinese Room and
the other two passages without. To keep the ex-
perimental conditions as similar as possible, we
provided them with a restricted version of the in-
terface (see Figure 2 for a screen-shot) in which all
additional functionalities except for the Document
View Tab are disabled. We assigned each person
to alternate between working with the full and the
restricted versions of the system; half began with-
out, and the others began with. Thus, every pas-
sage received four sets of corrections made collab-
oratively with the system and four sets of correc-
tions made based solely on the participants? inter-
nal language models. All together, there are 184
participant corrected sentences (11.5 sentences ?
4 passages ? 4 participants) for each condition.
The participants were asked to complete each
passage in one sitting. Within a passage, they
could work on the sentences in any arbitrary order.
They could also elect to ?pass? any part of a sen-
tence if they found it too difficult to correct. Tim-
ing statistics were automatically collected while
they made their corrections. We interviewed each
participant for qualitative feedbacks after all four
passages were corrected.
Next, we asked two bilingual speakers to eval-
uate all the corrected translations. The outcomes
between different groups of users are compared,
62
and the significance of the difference is deter-
mined using the two-sample t-test assuming un-
equal variances. We require 90% confidence (al-
pha=0.1) as the cut-off for a difference to be con-
sidered statistically significant; when the differ-
ence can be established with higher confidence,
we report that value. In the following subsections,
we describe the conditions of this study in more
details.
Participants? Background For this study, we
strove to maintain a relatively heterogeneous pop-
ulation; participants were selected to be varied in
their exposures to NLP, experiences with foreign
languages, as well as their age and gender. A sum-
mary of their backgrounds is shown in Table 1.
Prior to the start of the study, the participants
received a 20 minute long presentational tutorial
about the basic functionalities supported by our
system, but they did not have an opportunity to ex-
plore the system on their own. This helps us to de-
termine whether our interface is intuitive enough
for new users to pick up quickly.
Data The four passages used for this study were
chosen to span a range of difficulties and genre
types. The easiest of the four is a news arti-
cle about a new Tamagotchi-like product from
Bandai. It was taken from a webpage that offers
bilingual news to help Chinese students to learn
English. A harder news article is taken from a
past NIST Chinese-English MT Evaluation; it is
about Michael Jordan?s knee injury. For a dif-
ferent genre, we considered two fictional excerpts
from the first chapter of Martin Eden, a novel by
Jack London that has been professionally trans-
lated into Chinese7. One excerpt featured a short
dialog, while the other one was purely descriptive.
Evaluation of Translations Bilingual human
judges are presented with the source text as well as
the parallel English text for reference. Each judge
is then shown a set of candidate translations (the
original MT output, an alternative translation by
a bilingual speaker, and corrected translations by
the participants) in a randomized order. Since the
human corrected translations are likely to be flu-
ent, we have instructed the judges to concentrate
more on the adequacy of the meaning conveyed.
They are asked to rate each sentence on an abso-
7We chose an American story so as to not rely on a
user?s knowledge about Chinese culture. The participants
confirmed that they were not familiar with the chosen story.
Table 2: The guideline used by bilingual judges
for evaluating the translation quality of the MT
outputs and the participants? corrections.
9-10 The meaning of the Chinese sentence
is fully conveyed in the translation.
7-8 Most of the meaning is conveyed.
5-6 Misunderstands the sentence in a
major way; or has many small mistakes.
3-4 Very little meaning is conveyed.
1-2 The translation makes no sense at all.
lute scale of 1-10 using the guideline in Table 2.
To reduce the biases in the rating scales of differ-
ent judges, we normalized the judges? scores, fol-
lowing standard practices in MT evaluation (Blatz
et al, 2003). Post normalization, the correlation
coefficient between the judges is 0.64. The final
assessment score for each translated sentence is
the average of judges? scores, on a scale of 0-1.
5 Results
The results of human evaluations for the user ex-
periment are summarized in Table 3, and the corre-
sponding timing statistics (average minutes spent
editing a sentence) is shown in Table 4. We ob-
served that typical MT outputs contain a range of
errors. Some are primarily problems in fluency
such that the participants who used the restricted
interface, which provided no additional resources
other than the Document View Tab, were still able
to improve the MT quality from 0.35 to 0.42. On
the other hand, there are also a number of more
serious errors that require the participants to gain
some level of understanding of the source in order
to correct them. The participants who had access
to the full collaborative interface were able to im-
prove the quality from 0.35 to 0.53, closing the
gap between the MT and the bilingual translations
by 36.9%. These differences are all statistically
significant (with >98% confidence).
The higher quality of corrections does require
the participants to put in more time. Overall, the
participants took 2.5 times as long when they have
the interface than when they do not. This may be
partly because the participants have more sources
of information to explore and partly because the
participants tended to ?pass? on fewer sentences.
The average Levenshtein edit distance (with words
as the atomic unit, and with the score normalized
to the interval [0,1]) between the original MT out-
63
Table 1: A summary of participants? background. ?User5 recognizes some simple Kanji characters, but
does not have enough knowledge to gain any additional information beyond what the MT system and the
dictionary already provided.
User1 User2 User3 User4 User5? User6 User7 User8
NLP background intro grad none none intro grad intro none
Native English yes no yes yes yes yes yes yes
Other Languages French multiple none none Japanese none none Greek
(beginner) (fluent) (beginner) (beginner)
Gender M F F M M M F M
Education Ugrad PhD PhD Ugrad Ugrad PhD Ugrad Ugrad
puts and the corrected sentences made by partic-
ipants using The Chinese Room is 0.59; in con-
trast, the edit distance is shorter, at 0.40, when par-
ticipants correct MT outputs directly. The timing
statistics are informative, but they reflect the inter-
actions of many factors (e.g., the difficulty of the
source text, the quality of the machine translation,
the background and motivation of the user). Thus,
in the next few subsections, we examine how these
factors correlate with the quality of the participant
corrections.
5.1 Impact of Document Variation
Since the quality of MT varies depending on the
difficulty and genre of the source text, we inves-
tigate how these factors impact our participants?
performances. Columns 3-6 of Table 3 (and Ta-
ble 4) compare the corrected translations on a per-
document basis.
Of the four documents, the baseline MT sys-
tem performed the best on the product announce-
ment. Because the article is straight-forward, par-
ticipants found it relatively easy to guess the in-
tended translation. The major obstacle is in de-
tecting and translating Chinese transliteration of
Japanese names, which stumped everyone. The
quality difference between the two groups of par-
ticipants on this document was not statistically sig-
nificant. Relatedly, the difference in the amount of
time spent is the smallest for this document; par-
ticipants using The Chinese Room took about 1.5
times longer.
The other news article was much more difficult.
The baseline MT made many mistakes, and both
groups of participants spent longer on sentences
from this article than the others. Although sports
news is fairly formulaic, participants who only
read MT outputs were baffled, whereas those who
had access to additional resources were able to re-
cover from MT errors and produced good quality
translations.
Finally, as expected, the two fictional excerpts
were the most challenging. Since the participants
were not given any information about the story,
they also have little context to go on. In both cases,
participants who collaborated with The Chinese
Room made higher quality corrections than those
who did not. The difference is statistically signif-
icant at 97% confidence for the first excerpt, and
93% confidence for the second. The differences in
time spent between the two groups are greater for
these passages because the participants who had
to make corrections without help tended to give
up more often.
5.2 Impact of Participants? Background
We further analyze the results by separating the
participants into two groups according to four
factors: whether they were familiar with NLP,
whether they studied another language, their gen-
der, and their education level.
Exposure to NLP One of our design objectives
for The Chinese Room is accessibility by a diverse
population of end-users, many of whom may not
be familiar with human language technologies. To
determine how prior knowledge of NLP may im-
pact a user?s experience, we analyze the exper-
imental results with respect to the participants?
background. In columns 2 and 3 of Table 5, we
compare the quality of the corrections made by
the two groups. When making corrections on their
own, participants who had been exposed to NLP
held a significant edge (0.35 vs. 0.47). When both
groups of participants used The Chinese Room, the
difference is reduced (0.51 vs. 0.54) and is not sta-
tistically significant. Because all the participants
were given the same short tutorial prior to the start
of the study, we are optimistic that the interface is
intuitive for many users.
None of the other factors distinguished one
64
Table 3: Averaged human judgments of the translation quality of the four different approaches: automatic
MT, corrections by participants without help, corrections by participants using The Chinese Room, and
translation produced by a bilingual speaker. The second column reports score for all documents; columns
3-6 show the per-document scores.
Overall News (product) News (sports) Story1 Story2
Machine translation 0.35 0.45 0.30 0.25 0.26
Corrections without The Chinese Room 0.42 0.56 0.35 0.33 0.41
Corrections with The Chinese Room 0.53 0.55 0.62 0.42 0.49
Bilingual translation 0.83 0.83 0.73 0.92 0.88
Table 4: The average amount of time (minutes) participants spent on correcting a sentence.
Overall News (product) News (sports) Story1 Story2
Corrections without The Chinese Room 2.5 1.9 3.2 2.9 2.3
Corrections with The Chinese Room 6.3 2.9 8.7 6.5 8.5
Table 6: The quality of the corrections produced
by four participants using The Chinese Room for
the sports news article.
User1 0.57
User2 0.46
User5 0.70
User6 0.73
bilingual translator 0.73
group of participants from the others. The results
are summarized in columns 4-9 of Table 5. In each
case, the two groups had similar levels of perfor-
mance, and the differences between their correc-
tions were not statistically significant. This trend
holds for both when they were collaborating with
the system and when editing on their own.
Prior Knowledge Another factor that may im-
pact the success of the outcome is the user?s
knowledge about the domain of the source text.
An example from our study is the sports news ar-
ticle. Table 6 lists the scores that the four partic-
ipants who used The Chinese Room received for
their corrected translations for that passage (aver-
aged over sentences). User5 and User6 were more
familiar with the basketball domain; with the help
of the system, they produced translations that were
comparable to those from the bilingual translator
(the differences are not statistically significant).
5.3 Impact of Available Resources
Post-experiment, we asked the participants to de-
scribe the strategies they developed for collaborat-
ing with the system. Their responses fall into three
main categories:
Figure 3: This graph shows the average counts of
access per sentence for different resources.
Divide and Conquer Some users found the syn-
tactic trees helpful in identifying phrasal units for
N -best re-translations or example searches. For
longer sentences, they used the constituent col-
lapse feature to help them reduce clutter and focus
on a portion of the sentence.
Example Retrieval Using the search interface,
users examined the highlighted query terms to de-
termine whether the MT system made any seg-
mentation errors. Sometimes, they used the exam-
ples to arbitrate whether they should trust any of
the dictionary glosses or the MT?s lexical choices.
Typically, though, they did not attempt to inspect
the example translations in detail.
Document Coherence and Word Glosses
Users often referred to the document view to
determine the context for the sentence they are
editing. Together with the word glosses and other
65
Table 5: A comparison of translation quality, grouped by four characteristics of participant backgrounds:
their level of exposure to NLP, exposure to another language, their gender, and education level.
No NLP NLP No 2nd Lang. 2nd Lang. Female Male Ugrad PhD
without The Chinese Room 0.35 0.47 0.41 0.43 0.41 0.43 0.41 0.45
with The Chinese Room 0.51 0.54 0.56 0.51 0.50 0.55 0.52 0.54
resources, the discourse level clues helped to
guide users to make better lexical choices than
when they made corrections without the full
system, relying on sentence coherence alone.
Figure 3 compares the average access counts
(per sentence) of different resources (aggregated
over all participants and documents). The option
of inspect retrieved examples in detail (i.e., bring
them up on the sentence workspace) was rarely
used. The inspiration for this feature was from
work on translation memory (Macklovitch et al,
2000); however, it was not as informative for our
participants because they experienced a greater de-
gree of uncertainty than professional translators.
6 Discussion
The results suggest that collaborative translation
is a promising approach. Participant experiences
were generally positive. Because they felt like
they understood the translations better, they did
not mind putting in the time to collaborate with
the system. Table 7 shows some of the partici-
pants? outputs. Although there are some transla-
tion errors that cannot be overcome with our cur-
rent system (e.g., transliterated names), the partic-
ipants taken as a collective performed surprisingly
well. For many mistakes, even when the users can-
not correct them, they recognized a problem; and
often, one or two managed to intuit the intended
meaning with the help of the available resources.
As an upper-bound for the effectiveness of the sys-
tem, we construct a combined ?oracle? user out of
all 4 users that used the interface for each sentence.
The oracle user?s average score is 0.70; in contrast,
an oracle of users who did not use the system is
0.54 (cf. the MT?s overall of 0.35 and the bilin-
gual translator?s overall of 0.83). This suggests
The Chinese Room affords a potential for human-
human collaboration as well.
The experiment also made clear some limita-
tions of the current resources. One is domain de-
pendency. Because NLP technologies are typi-
cally trained on news corpora, their bias toward
the news domain may mislead our users. For ex-
ample, there is a Chinese character (pronounced
mei3) that could mean either ?beautiful? or ?the
United States.? In one of the passages, the in-
tended translation should have been: He was re-
sponsive to beauty... but the corresponding MT
output was He was sensitive to the United States...
Although many participants suspected that it was
wrong, they were unable to recover from this mis-
take because the resources (the searchable exam-
ples, the part-of-speech tags, and the MT system)
did not offer a viable alternative. This suggests
that collaborative translation may serve as a useful
diagnostic tool to help MT researchers verify ideas
about what types of models and data are useful in
translation. It may also provide a means of data
collection for MT training. To be sure, there are
important challenges to be addressed, such as par-
ticipation incentive and quality assurance, but sim-
ilar types of collaborative efforts have been shown
fruitful in other domains (Cosley et al, 2007). Fi-
nally, the statistics of user actions may be useful
for translation evaluation. They may be informa-
tive features for developing automatic metrics for
sentence-level evaluations (Kulesza and Shieber,
2004).
7 Related Work
While there have been many successful computer-
aided translation systems both for research and as
commercial products (Bowker, 2002; Langlais et
al., 2000), collaborative translation has not been
as widely explored. Previous efforts such as
DerivTool (DeNeefe et al, 2005) and Linear B
(Callison-Burch, 2005) placed stronger emphasis
on improving MT. They elicited more in-depth in-
teractions between the users and the MT system?s
phrase tables. These approaches may be more ap-
propriate for users who are MT researchers them-
selves. In contrast, our approach focuses on pro-
viding intuitive visualization of a variety of in-
formation sources for users who may not be MT-
savvy. By tracking the types of information they
consulted, the portions of translations they se-
lected to modify, and the portions of the source
66
Table 7: Some examples of translations corrected by the participants and their scores.
Score Translation
MT 0.34 He is being discovered almost hit an arm in the pile of books on the desktop, just
like frightened horse as a Lieju Wangbangbian almost Pengfan the piano stool.
without The Chinese Room 0.26 Startled, he almost knocked over a pile of book on his desk, just like a frightened
horse as a Lieju Wangbangbian almost Pengfan the piano stool.
with The Chinese Room 0.78 He was nervous, and when one of his arms nearly hit a stack of books on the
desktop, he startled like a horse, falling back and almost knocking over the piano
stool.
Bilingual Translator 0.93 Feeling nervous, he discovered that one of his arms almost hit the pile of books
on the table. Like a frightened horse, he stumbled aside, almost turning over a
piano stool.
MT 0.50 Bandai Group, a spokeswoman for the U.S. to be SIN-West said: ?We want to
bring women of all ages that ?the flavor of life?.?
without The Chinese Room 0.67 SIN-West, a spokeswoman for the U.S. Bandai Group declared: ?We want to
bring to women of all ages that ?flavor of life?.?
with The Chinese Room 0.68 West, a spokeswoman for the U.S. Toy Manufacturing Group, and soon to be
Vice President-said: ?We want to bring women of all ages that ?flavor of life?.?
Bilingual Translator 0.75 ?We wanted to let women of all ages taste the ?flavor of life?,? said Bandai?s
spokeswoman Kasumi Nakanishi.
text they attempted to understand, we may alter
the design of our translation model. Our objective
is also related to that of cross-language informa-
tion retrieval (Resnik et al, 2001). This work can
be seen as providing the next step in helping users
to gain some understanding of the information in
the documents once they are retrieved.
By facilitating better collaborations between
MT and target-language readers, we can naturally
increase human annotated data for exploring al-
ternative MT models. This form of symbiosis is
akin to the paradigm proposed by von Ahn and
Dabbish (2004). They designed interactive games
in which the player generated data could be used
to improve image tagging and other classification
tasks (von Ahn, 2006). While our interface does
not have the entertainment value of a game, its
application serves a purpose. Because users are
motivated to understand the documents, they may
willingly spend time to collaborate and make de-
tailed corrections to MT outputs.
8 Conclusion
We have presented a collaborative approach for
mediating between an MT system and monolin-
gual target-language users. The approach encour-
ages users to combine evidences from comple-
mentary information sources to infer alternative
hypotheses based on their world knowledge. Ex-
perimental evidences suggest that the collabora-
tive effort results in better translations than ei-
ther the original MT or uninformed human ed-
its. Moreover, users who are knowledgeable in the
document domain were enabled to correct transla-
tions with a quality approaching that of a bilin-
gual speaker. From the participants? feedbacks,
we learned that the factors that contributed to their
understanding include: document coherence, syn-
tactic constraints, and re-translation at the phrasal
level. We believe that the collaborative translation
approach can provide insights about the transla-
tion process and help to gather training examples
for future MT development.
Acknowledgments
This work has been supported by NSF Grants IIS-
0710695 and IIS-0745914. We would like to thank
Jarrett Billingsley, Ric Crabbe, Joanna Drum-
mund, Nick Farnan, Matt Kaniaris Brian Mad-
den, Karen Thickman, Julia Hockenmaier, Pauline
Hwa, and Dorothea Wei for their help with the ex-
periment. We are also grateful to Chris Callison-
Burch for discussions about collaborative trans-
lations and to Adam Lopez and the anonymous
reviewers for their comments and suggestions on
this paper.
67
References
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2003. Confidence es-
timation for machine translation. Technical Report
Natural Language Engineering Workshop Final Re-
port, Johns Hopkins University.
Lynne Bowker. 2002. Computer-Aided Translation
Technology. University of Ottawa Press, Ottawa,
Canada.
Chris Callison-Burch. 2005. Linear B System descrip-
tion for the 2005 NIST MT Evaluation. In The Pro-
ceedings of Machine Translation Evaluation Work-
shop.
Dan Cosley, Dan Frankowski, Loren Terveen, and John
Riedl. 2007. Suggestbot: using intelligent task rout-
ing to help people find work in wikipedia. In IUI
?07: Proceedings of the 12th international confer-
ence on Intelligent user interfaces, pages 32?41.
Steve DeNeefe, Kevin Knight, and Hayward H. Chan.
2005. Interactively exploring a machine transla-
tion model. In Proceedings of the ACL Interactive
Poster and Demonstration Sessions, pages 97?100,
Ann Arbor, Michigan, June.
Martin Kay. 1980. The proper place of men and
machines in language translation. Technical Re-
port CSL-80-11, Xerox. Later reprinted in Machine
Translation, vol. 12 no.(1-2), 1997.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems, 15.
Alex Kulesza and Stuart M. Shieber. 2004. A learn-
ing approach to improving sentence-level MT evalu-
ation. In Proceedings of the 10th International Con-
ference on Theoretical and Methodological Issues in
Machine Translation (TMI), Baltimore, MD, Octo-
ber.
Philippe Langlais, George Foster, and Guy Lapalme.
2000. Transtype: a computer-aided translation typ-
ing system. In Workshop on Embedded Machine
Translation Systems, pages 46?51, May.
Lemur. 2006. Lemur toolkit for language modeling
and information retrieval. The Lemur Project is a
collaborative project between CMU and UMASS.
Elliott Macklovitch, Michel Simard, and Philippe
Langlais. 2000. Transsearch: A free translation
memory on the world wide web. In Proceedings of
the Second International Conference on Language
Resources & Evaluation (LREC).
Bart Mellebeek, Anna Khasin, Josef van Genabith, and
Andy Way. 2005. Transbooster: Boosting the per-
formance of wide-coverage machine translation sys-
tems. In Proceedings of the 10th Annual Conference
of the European Association for Machine Transla-
tion (EAMT), pages 189?197.
Philip S. Resnik, Douglas W. Oard, and Gina-Anne
Levow. 2001. Improved cross-language retrieval us-
ing backoff translation. In Human Language Tech-
nology Conference (HLT-2001), San Diego, CA,
March.
John R. Searle. 1980. Minds, brains, and programs.
Behavioral and Brain Sciences, 3:417?457.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In CHI ?04: Proceed-
ings of the SIGCHI conference on Human factors in
computing systems, pages 319?326, New York, NY,
USA. ACM.
Luis von Ahn. 2006. Games with a purpose. Com-
puter, 39(6):92?94.
68
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 851?858, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Backoff Model for Bootstrapping Resources
for Non-English Languages?
Chenhai Xi and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{chenhai,hwa}@cs.pitt.edu
Abstract
The lack of annotated data is an ob-
stacle to the development of many
natural language processing applica-
tions; the problem is especially severe
when the data is non-English. Pre-
vious studies suggested the possibility
of acquiring resources for non-English
languages by bootstrapping from high
quality English NLP tools and paral-
lel corpora; however, the success of
these approaches seems limited for dis-
similar language pairs. In this paper,
we propose a novel approach of com-
bining a bootstrapped resource with a
small amount of manually annotated
data. We compare the proposed ap-
proach with other bootstrapping meth-
ods in the context of training a Chinese
Part-of-Speech tagger. Experimental
results show that our proposed ap-
proach achieves a significant improve-
ment over EM and self-training and
systems that are only trained on man-
ual annotations.
1 Introduction
Natural language applications that use super-
vised learning methods require annotated train-
ing data, but annotated data is scarce for many
?We thank Stephen Clark, Roger Levy, Carol Nichols,
and the three anonymous reviewers for their helpful com-
ments.
non-English languages. It has been suggested
that annotated data for these languages might
be automatically created by leveraging paral-
lel corpora and high-accuracy English systems
(Yarowsky and Ngai, 2001; Diab and Resnik,
2002). The studies are centered around the
assumption that linguistic analyses for English
(e.g., Part-of-Speech tags, Word sense disam-
biguation, grammatical dependency relation-
ships) are also valid analyses in the translation
of the English. For example, in the English
noun phrase the red apples, red modifies ap-
ples; the same modifier relationship also exists in
its French translations les pommes rouges, even
though the word orders differ. To the extent
that the assumption is true, annotated data in
the non-English language can be created by pro-
jecting English analyses across a word aligned
parallel corpus. The resulting projected data
can then serve as (albeit noisy) training exam-
ples to develop applications in the non-English
language.
The projection approach faces both a theo-
retical and a practical challenge. Theoretically,
it is well-known that two languages often do
not express the same meaning in the same way
(Dorr, 1994). Practically, the projection frame-
work is sensitive to component errors. In partic-
ular, poor word alignments significantly degrade
the accuracy of the projected annotations. Pre-
vious research on resource projection attempts
to address these problems by redistributing the
parameter values (Yarowsky and Ngai, 2001) or
by applying transformation rules (Hwa et al,
851
2002). Their experimental results suggest that
while these techniques can overcome some er-
rors, they are not sufficient for projected data
that are very noisy.
In this work, we tackle the same problems by
relaxing the zero manual annotation constraint.
The main question we address is: how can we
make the most out of a small set of manually la-
beled data (on the non-English side). Following
the work of Yarowsky and Ngai (2001) we focus
on the task of training a Part-of-Speech (POS)
tagger, but we conduct our experiments with
the more dissimilar language pair of English-
Chinese instead of English-French. Through
empirical studies, we show that when the word
alignment quality is sufficiently poor, the er-
ror correction techniques proposed by Yarowsky
and Ngai are unable to remove enough mistakes
in the projected data. We propose an alternative
approach that is inspired by backoff language
modeling techniques in which the parameters of
two tagging models (one trained on manually la-
beled data; the other trained on projected data)
are combined to achieve a more accurate final
model.
2 Background
The idea of trying to squeeze more out of an-
notated training examples has been explored in
a number of ways in the past. Most popular
is the family of bootstrapping algorithms, in
which a model is seeded with a small amount of
labeled data and iteratively improved as more
unlabeled data are folded into the training set,
typically, through unsupervised learning. An-
other approach is active learning (Cohn et al,
1996), in which the model is also iteratively im-
proved but the training examples are chosen by
the learning model, and the learning process is
supervised. Finally, the work that is the closest
to ours in spirit is the idea of joint estimation
(Smith and Smith, 2004).
Of the bootstrapping methods, perhaps the
most well-known is the Expectation Maximiza-
tion (EM) algorithm. This approach has been
explored in the context of many NLP applica-
tions; one example is text classification (Nigam
et al, 1999). Another bootstrapping approach
reminiscent of EM is self-training. Yarowsky
(1995) used this method for word sense disam-
biguation. In self-training, annotated examples
are used as seeds to train an initial classifier
with any supervised learning method. This ini-
tial classifier is then used to automatically an-
notate data from a large pool of unlabeled ex-
amples. Of these newly labeled data, the ones
labeled with the highest confidence are used as
examples to train a new classifier. Yarowsky
showed that repeated application of this pro-
cess resulted in a series of word sense classi-
fiers with improved accuracy and coverage. Also
related is the co-training algorithm (Blum and
Mitchell, 1998) in which the bootstrapping pro-
cess requires multiple learners that have differ-
ent views of the problem. The key to co-training
is that the views should be conditionally inde-
pendent given the label. The strong indepen-
dence requirement on the views is difficult to
satisfy. For practical applications, different fea-
tures sets or models (that are not conditionally
independent) have been used as an approxima-
tion for different views. Co-training has been ap-
plied to a number of NLP applications, includ-
ing POS-tagging (Clark et al, 2003), parsing
(Sarkar, 2001), word sense disambiguation (Mi-
halcea, 2004), and base noun phrase detection
(Pierce and Cardie, 2001). Due to the relaxation
of the view independence assumption, most em-
pirical studies suggest a marginal improvement.
The common thread between EM, self-training,
and co-training is that they all bootstrap off
of unannotated data. In this work, we explore
an alternative to ?pure? unannotated data; our
data have been automatically annotated with
projected labels from English. Although the
projected labels are error-prone, they provide us
with more information than automatically pre-
dicted labels used in bootstrapping methods.
With a somewhat different goal in mind, ac-
tive learning addresses the problem of choosing
the most informative data for annotators to la-
bel so that the model would achieve the greatest
improvement. Active learning also has been ap-
plied to many NLP applications, including POS
tagging (Engelson and Dagan, 1996) and pars-
852
ing (Baldridge and Osborne, 2003). The draw-
back of an active learning approach is that it
assumes that a staff of annotators is waiting on
call, ready to label the examples chosen by the
system at every iteration. In practice, it is more
likely that one could only afford to staff anno-
tators for a limited period of time. Although
active learning is not a focus in this paper, we
owe some ideas to active learning in choosing a
small initial set of training examples; we discuss
these ideas in section 3.2.
More recently, Smith and Smith (2004) pro-
posed to merge an English parser, a word align-
ment model, and a Korean PCFG parser trained
from a small number of Korean parse trees un-
der a unified log linear model. Their results sug-
gest that a joint model produces somewhat more
accurate Korean parses than a PCFG Korean
parser trained on a small amount of annotated
Korean parse trees alone. Their motivation is
similar to the starting point of our work: that a
word aligned parallel corpus and a small amount
of annotated data in the foreign language side
offer information that might be exploited. Our
approach differs from theirs in that we do not
optimize the three models jointly. One concern
is that joint optimization might not result in op-
timal parameter settings for the individual com-
ponents. Because our focus is primarily on ac-
quiring non-English language resources, we only
use the parallel corpus as a means of projecting
resources from English.
3 Our Approach
This work explores developing a Chinese POS
tagger without a large manually annotated cor-
pus. Our approach is to train two separate
models from two different data sources: a large
corpus of automatically tagged data (projected
from English) and a small corpus of manually
tagged data; the two models are then combined
into one via the Whitten-Bell backoff language
model.
3.1 Projected Data
One method of acquiring a large corpus of au-
tomatically POS tagged Chinese data is by
projection (Yarowsky and Ngai, 2001). This
approach requires a sentence-aligned English-
Chinese corpus, a high-quality English tagger,
and a method of aligning English and Chinese
words that share the same meaning. Given the
parallel corpus, we tagged the English words
with a publicly available maximum entropy tag-
ger (Ratnaparkhi, 1996), and we used an im-
plementation of the IBM translation model (Al-
Onaizan et al, 1999) to align the words. The
Chinese words in the parallel corpus would then
receive the same POS tags as the English words
to which they are aligned. Next, the basic pro-
jection algorithm is modified to accommodate
two complicating factors. First, word align-
ments are not always one-to-one. To compen-
sate, we assign a default tag to unaligned Chi-
nese words; in the case of one-Chinese-to-many-
English, the Chinese word would receive the tag
of the final English word. Second, English and
Chinese do not share the same tag set. Fol-
lowing Yarowsky and Ngai (2001), we define 12
equivalence classes over the 47 Penn-English-
Treebank POS tags. We refer to them as Core
Tags. With the help of 15 hand-coded rules and
a Naive Bayes model trained on a small amount
of manually annotated data, the Core Tags can
be expanded to the granularity of the 33 Penn-
Chinese-Treebank POS tags (which we refer to
as Full Tags).
3.2 Manually Annotated Data
Since the amount of manual annotation is lim-
ited, we must decide what type of data to anno-
tate. In the spirit of active learning, we aim to
select sentences that may bring about the great-
est improvements in the accuracy of our model.
Because it is well known that handling unknown
words is a serious problem for POS taggers, our
strategy for selecting sentences for manual anno-
tation is to maximize the word coverage of the
in ital model. That is, we wish to find a small
set of sentences that would lead to the greatest
reduction of currently unknown words Finding
these sentences is a NP-hard problem because
the 0/1 knapsack problem could be reduced to
this problem in polynomial-time (Gurari, 1989).
Thus, we developed an approximation algorithm
for finding sentences with the maximum word
853
M : number of tokens will be annotated.
S={s1, s2, . . . , sn}: the unannotated corpus.
Ssel : set of selected sentences in S.
Sunsel : set of unselected sentences in S.
|Ssel| : number of tokens in Ssel.
TY PE(Ssel) : number of types in Ssel.
MWC:
randomly choose Ssel ? S
such that|Ssel| ? M .
For each sentence si in Ssel
find a sentence rj in Sunsel
which maximizes swap score(si, rj).
if swap score(si, rj) > 0
{
Ssel = (Ssel ? si) ? rj ;
Sunsel = (Sunsel ? rj) ? si;
}
swap score(si, rj)
{
Ssel new = (Ssel ? si) ? rj ;
if ( |Ssel new| > M ) return -1;
else return TY PE(Ssel new)? TY PE(Ssel);
}
Figure 1: The pseudo-code for MWC algorithm.
The input is M and S and the output is Ssel
coverage of unknown words (MWC). This algo-
rithm is described in Figure 1,
3.3 Basic POS Tagging Model
It is well known that a POS tagger can be
trained with an HMM (Weischedel et al, 1993).
Given a trained model, the most likely tag se-
quence T? = {t1, t2, . . . tn} is computed for the
input word sentence: W? = {w1, w2, . . . wn}:
T? = arg max
T
P (T |W ) = arg max
T
P (T |W )P (T )
The transition probability P (T ) is approxi-
mated by a trigram model:
P (T ) ? p(t1)p(t2|t1)
n?
i=3
p(ti|ti?1, ti?2),
and the observation probability P (W |T ) is com-
puted by
P (W |T ) ?
n?
i=1
p(wi|ti).
3.4 Combined Models
From the two data sources, two separate trigram
taggers have been trained (Tanno from manually
annotated data and Tproj from projected data).
This section considers ways of combining them
into a single tagger. The key insight that drives
our approach is based on reducing the effect of
unknown words. We see the two data sources as
complementary in that the large projected data
source has better word coverage while the man-
ually labeled one is good at providing tag-to-tag
transitions. Based on this principle, one way of
merging these two taggers into a single HMM
(denoted as Tinterp) is to use interpolation:
pinterp(w|t) = ?? panno(w|t)
+(1? ?)? pproj(w|t)
pinterp(ti|ti?1, ti?2) = panno(ti|ti?1, ti?2)
where ? is a tunable weighting parameter1 of
the merged tagger. This approach may be prob-
lematic because it forces the model to always
include some fraction of poor parameter values.
Therefore, we propose to estimate the observa-
tion probabilities using backoff. The parameters
of Tback are estimated as follows:
pback(w|t) =
{
?(t)? panno(w|t) if panno(w|t) > 0
?(t) ? pproj(w|t) if panno(w|t) = 0
pback(ti|ti?1, ti?2) = panno(ti|ti?1, ti?2)
where ?(t) is a discounting coefficient and ?
is set to satisfy that
?
all words P (w|t) = 1.
The discounting coefficient is computed using
the Witten-Bell discounting method:
?(t) =
Canno(t)
Canno(t) + Sanno(t)
,
where Canno(t) is the count of tokens whose
tag is t in the manually annotated corpus and
1In our experiments, the value of ? is set to 0.8 based
on held-out data.
854
Sanno(t) is the seen types of words with tag t.
In other words, we trust the parameter estimates
from the model trained on manual annotation by
default unless it is based on unreliable statistics.
4 Experiments
We conducted a suite of experiments to inves-
tigate the effect of allowing a small amount of
manually annotated data in conjunction with
using annotations projected from English. We
first establish a baseline of training on projected
data alone in Section 4.1. It is an adaptation of
the approach described by Yarowsky and Ngai
(2001). Next, we consider the case of using
manually annotated data alone in Section 4.2.
We show that there is an increase in accuracy
when the MWC active learning strategy is used.
In Section 4.3, we show that with an appro-
priate merging strategy, a tagger trained from
both data sources achieves higher accuracy. Fi-
nally, in Section 4.4, we evaluate our approach
against other semi-supervised methods to ver-
ify that the projected annotations, though noisy,
contain useful information.
We use an English-Chinese Federal Broadcast
Information Service (FBIS) corpus as the data
source for the projected annotation. We sim-
ulated the manual annotation process by using
the POS tags provided by the Chinese Treebank
version 4 (CHTB). We used about a thousand
sentences from CHTB as held-out data. The re-
maining sentences are split into ten-fold cross
validation sets. Each test set contains 1400 sen-
tences. Training data are selected (using MWC)
from the remaining 12600 sentences. The re-
ported results are the average of the ten trials.
One tagger is considered to be better than an-
other if, according to the paired t-test, we are
at least 95% confident that their difference in
accuracy is non-zero. Performance is measured
in terms of the percentage of correctly tagged
tokens in the test data. For comparability with
Tproj (which assumes no availability of manu-
ally annotated data), most experimental results
are reported with respect to the reduced Core
Tag gold standard; evaluation against the full
33 CHTB tag gold standard is reported in Sec-
tion 4.4.
4.1 Tagger Trained from Projected
Data
To determine the quality of Tproj for Chinese,
we replicate the POS-tagging experiment in
Yarowsky and Ngai (2001). Trained on all pro-
jected data, the tagger has an accuracy of 58.2%
on test sentences. The low accuracy rate sug-
gests that the projected data is indeed very
noisy. To reduce the noise in the projected data,
Yarowsky and Ngai developed a re-estimation
technique based on the observation that words
in French, English and Czech have a strong ten-
dency to exhibit only a single core POS tag
and very rarely have more than two. Apply-
ing the same re-estimation technique that favors
this bias to the projected Chinese data raises
the final tagger accuracy to 59.1%. That re-
estimation did not help English-Chinese projec-
tion suggests that the dissimilarity between the
two languages is an important factor. A related
reason for the lower accuracy rate is due to poor
word alignments in the English-Chinese corpus.
As a further noise reduction step, we automat-
ically filter out sentence pairs that were poorly
aligned (i.e., the sentence pairs had too many
unaligned words or too many one-to-many align-
ments). This results in a corpus of about 9000
FBIS sentences. A tagger trained on the filtered
data has an improved accuracy of 64.5%. We
take this to be Tproj used in later experiments.
4.2 Taggers Trained from Manually
Labeled Data
This experiment verifies that the Maximum
Word Coverage (MWC) selection scheme pre-
sented in Section 3.2 is helpful in selecting data
for training Tanno. We compare it against ran-
dom selection. Figure 2 plots the taggers? per-
formances on test sentences as the number of
manually annotated tokens increase from 100 to
30,000. We see that the taggers trained on data
selected by MWC outperform those trained on
randomly selected data. Thus, in the main ex-
periments, we always use MWC to select the set
of manually tagged data for training Tanno.
855
 
0.35 0.4
 
0.45 0.5
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9  10
0 
3000
 
5000
 
10000
 
15000
 
20000
 
25000
 
30000
accuracy
numbe
r of an
notate
d toke
ns
Rando
m MWC
Figure 2: A comparison between MWC and ran-
dom selection.
4.3 Evaluation of the Combined
Taggers
 
0.4
 
0.45 0.5
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9  10
0 
3000
 
5000
 
10000
 
15000
 
20000
 
25000
 
30000
accuracy
numbe
r of an
notate
d toke
ns
proj anno concat Interp back
Figure 3: A comparison of the proposed backoff
approach against alternative methods of com-
bining Tproj and Tanno
To investigate how Tanno and Tproj might be
merged to form a higher quality tagger, we con-
duct an experiment to evaluate the different
alternatives described in section 3.4: Tinterp,
and Tback. They are compared against three
baselines: Tanno, Tproj , and Tconcat, a tagger
trained from the concatenation of the two data
sources. To determine the effect of manual an-
notation, we vary the size of the training set for
Tanno from 100 tokens (fewer than 10 sentences)
to 30,000 tokens (about 1000 sentences). The
learning curves are plotted in Figure 3. The re-
sult suggests that Tback successfully incorporates
information from both the manually annotated
data and the projected data. The improvement
over training on manually annotated data alone
(Tanno) is especially high when fewer than 10,000
manually annotated tokens are available. As ex-
pected, Tinterp, and Tconcat perform worse than
Tanno because they are not as effective at dis-
counting the erroneous projected annotations.
4.4 Comparisons with Other
Semi-Supervised Approaches
This experiment evaluates the proposed back-
off approach against two other semi-supervised
approaches: self-training (denoted as Tself ) and
EM (denoted as Tem). Both start with a fully su-
pervised model (Tanno) and iteratively improve
it by seeing more unannotated data.2 As dis-
cussed earlier, a major difference between our
proposed approach and the bootstrapping meth-
ods is that our approach makes use of anno-
tations projected from English while the boot-
strapping methods rely on unannotated data
alone. To investigate the effect of leveraging
from English resources, we use the Chinese por-
tion of the FBIS parallel corpus (the same 9000
sentences as the training corpus of Tproj but
without the projected tags) as the unannotated
data source for the bootstrapping methods.
Figure 4 compares the four learning curves.
We have evaluated them both in terms of the
Core Tag gold standard and in terms of Full
Tag gold standard. Although all three ap-
proaches produce taggers with higher accuracies
than that of Tanno, our backoff approach outper-
forms both self-training and EM. The difference
is especially prominent when manual annota-
tion is severely limited. When more manual an-
notations are made available, the gap narrows;
however, the differences are still statistically sig-
nificant at 30,000 manually annotated tokens.
These results suggest that projected data have
more useful information than unannotated data.
2In our implementation of self-training, the top 10%
of the unannoated sentences with the highest confidence
scores is selected. The confidence score is computed as:
logP (T |W )
length of the sentence .
856
 
0.4
 
0.45 0.5
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9  10
0 
3000
 
5000
 
10000
 
15000
 
20000
 
25000
 
30000
accuracy
numbe
r of an
notate
d toke
ns
anno self em back
 
0.4
 
0.45 0.5
 
0.55 0.6
 
0.65 0.7
 
0.75 0.8
 
0.85 0.9  10
0 
3000
 
5000
 
10000
 
15000
 
20000
 
25000
 
30000
accuracy
numbe
r of an
notate
d toke
ns
anno self em back
(a) (b)
Figure 4: A comparison of Backoff against self-training and EM. (a) Evaluation against the Core
Tag gold standard. (b) Evaluation against the Full Tag gold standard.
5 Discussion
While the experimental results support our intu-
ition that Tback is effective in making use of both
data sources, there are still two questions worth
addressing. First, there may be other ways of
estimating the parameters of a merged HMM
from the parameters of Tanno and Tproj . For ex-
ample, a natural way of merging the two taggers
into a single HMM (denoted as Tmerge) is to use
the values of the observation probabilities from
Tproj and the values of the transition probabili-
ties from Tanno:
pmerge(w|t) = pproj(w|t),
pmerge(ti|ti?1, ti?2) = panno(ti|ti?1, ti?2).
Another is the reverse of Tmerge:
prev merge(w|t) = panno(w|t)
prev merge(ti|ti?1, ti?2) = pproj(ti|ti?1, ti?2)
Tmerge is problematic because it ignores all man-
ual word-tag annotations; however, Trev merge?s
learning curve is nearly identical to that of Tanno
(graph not shown). Its models do not take ad-
vantage of the broader word coverage of the
projected data, so it does not perform as well
as Tback. Trev merge outperforms Tmerge when
trained from more than 2000 manually anno-
tated tokens. We make two observations from
this finding. One is that the differences between
pproj(ti|ti?1, ti?2) and panno(ti|ti?1, ti?2) are not
large. Another is that the success of the merged
HMM tagger hinges on the goodness of the ob-
servation probabilities, p(w|t). This is in accord
with our motivation in improving the reliability
of p(w|t) through backoff.
Second, while our experimental results sug-
gest that Tback outperforms self-training and
EM, these approaches are not incompatible with
one another. Because Tback is partially esti-
mated from the noisy corpus of projected an-
notations, it might be further improved by
applying a bootstrapping algorithm over the
noisy corpus (with the projected tags removed).
To test our hypothesis, we initialized the self-
training algorithm with a backoff tagger that
used 3000 manually annotated tokens. This led
to a slight but statistically significant improve-
ment, from 74.3% to 74.9%.
6 Conclusion and Future Work
In summary, we have shown that backoff is an ef-
fective technique for combining manually anno-
tated data with a large but noisy set of automat-
ically annotated data (from projection). Our ap-
857
proach is the most useful when a small amount
of annotated tokens is available. In our exper-
iments, the best results were achieved when we
used 3000 manually annotated tokens (approxi-
mately 100 sentences).
The current study points us to several direc-
tions for future work. One is to explore ways of
applying the proposed approach to other learn-
ing models. Another is to compare against other
methods of combining evidences from multiple
learners. Finally, we will investigate whether
the proposed approach can be adapted to more
complex tasks in which the output is not a class
label but a structure (e.g. parsing).
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, I. Dan Melamed, Franz-
Josef Och, David Purdy, Noah A. Smith, and
David Yarowsky. 1999. Statistical machine transla-
tion. Technical report, JHU. citeseer.nj.nec.com/al-
onaizan99statistical.html.
Jason Baldridge and Miles Osborne. 2003. Active learn-
ing for HPSG parse selection. In Proceedings of the 7th
Conference on Natural Language Learning, Edmonton,
Canada, June.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of
the 1998 Conference on Computational Learning The-
ory, pages 92?100, Madison, WI.
Stephen Clark, James Curran, and Miles Osborne. 2003.
Bootstrapping pos-taggers using unlabelled data. In
Proc. of the Computational Natural Language Learn-
ing Conference, pages 164?167, Edmonton, Canada,
June.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadelphia,
PA.
Bonnie J. Dorr. 1994. Machine translation divergences:
A formal description and proposed solution. Compu-
tational Linguistics, 20(4):597?635.
Sean P. Engelson and Ido Dagan. 1996. Minimizing man-
ual annotation cost in supervised training from copora.
In Proceedings of the 34th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 319?326,
Santa Cruz, CA.
Eitan Gurari. 1989. An Introduction to the Theory of
Computation. Ohio State University Computer Sci-
ence Press.
Rebecca Hwa, Philip S. Resnik, Amy Weinberg, and
Okan Kolak. 2002. Evaluating translational corre-
spondence using annotation projection. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics.
Rada Mihalcea. 2004. Co-training and self-training for
word sense disambiguation. In Proceedings of the Con-
ference on Computational Natural Language Learning
(CoNLL-2004).
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and
Tom Mitchell. 1999. Text Classification from Labeled
and Unlabeled Documents using EM. Machine Learn-
ing, 1(34).
David Pierce and Claire Cardie. 2001. Limitations of
co-training for natural language learning from large
datasets. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-01), pages 1?9, Pittsburgh, PA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Eric Brill and Kenneth
Church, editors, Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing, pages
133?142. Association for Computational Linguistics,
Somerset, New Jersey.
Anoop Sarkar. 2001. Applying co-training methods to
statistical parsing. In Proceedings of the Second Meet-
ing of the North American Association for Compu-
tational Linguistics, pages 175?182, Pittsburgh, PA,
June.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of the 2005 Conference
on Empirical Methods in Natural Language Processing
(EMNLP-05).
Ralph Weischedel, Richard Schwartz, Jeff Palmucci,
Marie Meteer, and Lance Ramshaw. 1993. Coping
with ambiguity and unknown words through proba-
bilistic models. Comput. Linguist., 19(2):361?382.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual POS taggers and NP bracketers via robust
projection across aligned corpora. In Proceedings of
the Second Meeting of the North American Associa-
tion for Computational Linguistics, pages 200?207.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 35th Annual Meeting of the Association
for Computational Linguistics, pages 189?196, Cam-
bridge, MA.
858
c? 2004 Association for Computational Linguistics
Sample Selection for Statistical Parsing
Rebecca Hwa?
University of Pittsburgh
Corpus-based statistical parsing relies on using large quantities of annotated text as training
examples. Building this kind of resource is expensive and labor-intensive. This work proposes to
use sample selection to find helpful training examples and reduce human effort spent on annotating
less informative ones. We consider several criteria for predicting whether unlabeled data might
be a helpful training example. Experiments are performed across two syntactic learning tasks
and within the single task of parsing across two learning models to compare the effect of different
predictive criteria. We find that sample selection can significantly reduce the size of annotated
training corpora and that uncertainty is a robust predictive criterion that can be easily applied to
different learning models.
1. Introduction
Many learning tasks for natural language processing require supervised training; that
is, the system successfully learns a concept only if it has been given annotated train-
ing data. For example, while it is difficult to induce a grammar with raw text alone,
the task is tractable when the syntactic analysis for each sentence is provided as a
part of the training data (Pereira and Schabes 1992). Current state-of-the-art statisti-
cal parsers (Collins 1999; Charniak 2000) are all trained on large annotated corpora
such as the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). However,
supervised training data are difficult to obtain; existing corpora might not contain the
relevant type of annotation, and the data might not be in the domain of interest. For
example, one might need lexical-semantic analyses in addition to the syntactic anal-
yses in the treebank, or one might be interested in processing languages, domains,
or genres for which there are no annotated corpora. Because supervised training de-
mands significant human involvement (e.g., annotating the syntactic structure of each
sentence by hand), creating a new corpus is a labor-intensive and time-consuming en-
deavor. The goal of this work is to minimize a system?s reliance on annotated training
data.
One promising approach to mitigating the annotation bottleneck problem is to
use sample selection, a variant of active learning. Sample selection is an interactive
learning method in which the machine takes the initiative in selecting unlabeled data
for the human to annotate. Under this framework, the system has access to a large pool
of unlabeled data, and it has to predict how much it can learn from each candidate in
the pool if that candidate is labeled. More quantitatively, we associate each candidate
in the pool with a training utility value (TUV). If the system can accurately identify
the subset of examples with the highest TUV, it will have located the most beneficial
? Computer Science Department, Pittsburgh, PA 15260. E-mail: hwa@cs.pitt.edu.
Submission received: 14 October 2002; Revised submission received: 30 September 2003; Accepted for
publication: 22 December 2003
254
Computational Linguistics Volume 30, Number 3
training examples, thus freeing the annotators from having to label less informative
examples.
In this article, we apply sample selection to two syntactic learning tasks: training
a prepositional-phrase attachment (PP-attachment) model and training a statistical
parsing model. We are interested in addressing two main questions. First, what are
good predictors of a candidate?s training utility? We propose several predictive criteria
and define evaluation functions based on them to rank the candidates? utility. We
have performed experiments comparing the effect of these evaluation functions on
the size of the training corpus. We find that, with a judiciously chosen evaluation
function, sample selection can significantly reduce the size of the training corpus. The
second main question is: Are the predictors consistently effective for different types of
learners? We compare the predictive criteria both across tasks (between PP-attachment
and parsing) and within a single task (applying the criteria to two parsing models:
an expectation-maximization-trained parser and a count-based parser). We find that
the learner?s uncertainty is a robust predictive criterion that can be easily applied to
different learning models.
2. Learning with Sample Selection
Unlike traditional learning systems that receive training examples indiscriminately,
a sample selection learning system actively influences its own progress by choosing
new examples to incorporate into its training set. There are two types of selection algo-
rithms: committee-based and single learner. A committee-based selection algorithm
works with multiple learners, each maintaining a different hypothesis (perhaps per-
taining to different aspects of the problem). The candidate examples that lead to the
most disagreements among the different learners are considered to have the highest
TUV (Cohn, Atlas, and Ladner 1994; Freund et al 1997). For computationally intensive
problems, such as parsing, keeping multiple learners may be impractical.
In this work, we focus on sample selection using a single learner that keeps one
working hypothesis. Without access to multiple hypotheses, the selection algorithm
can nonetheless estimate the TUV of a candidate. We identify the following three
classes of predictive criteria:
1. Problem-space: Knowledge about the problem space may provide
information about the type of candidates that are particularly plentiful or
difficult to learn. This criterion focuses on the general attributes of the
learning problem, such as the distribution of the input data and
properties of the learning algorithm, but it ignores the current state of
the hypothesis.
2. Performance of the hypothesis: Testing the candidates on the current
working hypothesis shows the type of input data on which the
hypothesis may perform weakly. That is, if the current hypothesis is
unable to label a candidate or is uncertain about it, then the candidate
might be a good training example (Lewis and Catlett 1994). The
underlying assumption is that an uncertain output is likely to be wrong.
3. Parameters of the hypothesis: Estimating the potential impact that the
candidates will have on the parameters of the current working
hypothesis locates those examples that will change the current
hypothesis the most.
255
Hwa Sample Selection for Statistical Parsing
U is a set of unlabeled candidates.
L is a set of labeled training examples.
C is the current hypothesis.
Initialize:
C ? Train(L).
Repeat
N ? Select(n, U, C, f ).
U ? U ? N.
L ? L ? Label(N).
C ? Train(L).
Until (C is good enough) or (U = ?) or (cutoff).
Figure 1
Pseudo code for the sample selection learning algorithm.
Figure 1 outlines the single-learner sample selection training loop in pseudocode.
Initially, the training set, L, consists of a small number of labeled examples, based on
which the learner proposes its first hypothesis of the target concept, C. Also available
to the learner is a large pool of unlabeled training candidates, U. In each training
iteration, the selection algorithm, Select(n, U, C, f ), ranks the candidates of U according
to their expected TUVs and returns the n candidates with the highest values. The
algorithm predicts the TUV of each candidate, u ? U, with an evaluation function,
f (u, C). This function may rely on the hypothesis concept C to estimate the utility of a
candidate u. The n chosen candidates are then labeled by human experts and added
to the existing training set. Running the learning algorithm, Train(L), on the updated
training set, the system proposes a new hypothesis regarding the target concept that
is the most compatible with the examples seen thus far. The loop continues until one
of three stopping conditions is met: The hypothesis is considered to perform well
enough, all candidates are labeled, or an absolute cutoff point is reached (e.g., no
more resources).
3. Sample Selection for Prepositional-Phrase Attachment
One common source of structural ambiguities arises from syntactic constructs in which
a prepositional phrase might be equally likely to modify the verb or the noun pre-
ceding it. Researchers have proposed many computational models for resolving PP-
attachment ambiguities. Some well-known approaches include rule-based models (Brill
and Resnik 1994), backed-off models (Collins and Brooks 1995), and a maximum-
entropy model (Ratnaparkhi 1998). Following the tradition of using learning PP-
attachment as a way to gain insight into the parsing problem, we first apply sample
selection to reduce the amount of annotation used in training a PP-attachment model.
We use the Collins-Brooks model as the basic learning algorithm and experiment with
several evaluation functions based on the types of predictive criteria described earlier.
Our experiments show that the best evaluation function can reduce the number of
labeled examples by nearly half without loss of accuracy.
3.1 A Summary of the Collins-Brooks Model
The Collins-Brooks model takes prepositional phrases and their attachment classifica-
tions as training examples: each is represented as a quintuple of the form (v, n, p, n2, a),
where v, n, p, and n2 are the head words of the verb phrase, the object noun phrase, the
256
Computational Linguistics Volume 30, Number 3
subroutine Train(L)
foreach ex ? L do
extract (v, n, p, n2, a) from ex
foreach tuple ? {(v, n, p, n2), (v, p, n2), (n, p, n2), (v, n, p), (v, p), (n, p), (p, n2), (p)} do
Count(tuple) ? Count(tuple) + 1
if a = noun then
CountNP(tuple) ? CountNP(tuple) + 1
subroutine Test(U)
foreach u ? U do
extract (v, n, p, n2) from u
if Count(v, n, p, n2) > 0 then
prob ? CountNP(v,n,p,n2)Count(v,n,p,n2)
elsif Count(v, p, n2) + Count(n, p, n2) + Count(v, n, p) > 0 then
prob ? CountNP(v,p,n2)+CountNP(n,p,n2)+CountNP(v,n,p)Count(v,p,n2)+Count(n,p,n2)+Count(v,n,p)
elsif Count(v, p) + Count(n, p) + Count(p, n2) > 0 then
prob ? CountNP(v,p)+CountNP(n,p)+CountNP(p,n2)Count(v,p)+Count(n,p)+Count(p,n2)
elsif Count(p) > 0 then
prob ? CountNP(p)Count(p)
else prob ? 1
if prob ? .5 then
output noun
else output verb
Figure 2
The Collins-Brooks PP-attachment classification algorithm.
preposition, and the prepositional noun phrase, respectively, and a specifies the attach-
ment classification. For example, (wrote a book in three days, attach-verb) would be anno-
tated as (wrote, book, in, days, verb). The head words can be automatically extracted using
a heuristic table lookup in the manner described by Magerman (1994). For this learning
problem, the supervision is the one-bit information of whether p should attach to v or
to n. In order to learn the attachment preferences of prepositional phrases, the system
builds attachment statistics for each the characteristic tuple of all training examples. A
characteristic tuple is some subset of the four head words in the example, with the con-
dition that one of the elements must be the preposition. Each training example forms
eight characteristic tuples: (v, n, p, n2), (v, n, p), (v, p, n2), (n, p, n2), (v, p), (n, p), (p, n2), (p).
The attachment statistics are a collection of the occurrence frequencies for all the char-
acteristic tuples in the training set and the occurrence frequencies for the characteristic
tuples of those examples determined to attach to nouns. For some characteristic tuple
t, Count(t) denotes the former and CountNP(t) denotes the latter. In terms of the sample
selection algorithm, the collection of counts represents the learner?s current hypothesis
(C in Figure 1). Figure 2 provides the pseudocode for the Train routine.
Once trained, the system can be used to classify test cases based on the statistics of
the most similar training examples and back off as necessary. For instance, to determine
the PP-attachment for a test case, the classifier would first consider the ratio of the
two frequency counts for the four-word characteristic tuple of the test case. If the tuple
257
Hwa Sample Selection for Statistical Parsing
Figure 3
In this example, the classification of the test case preposition is backed off to the
two-word-tuple level. In the diagram, each circle represents a characteristic tuple. A filled
circle denotes that the tuple has occurred in the training set. The dashed rectangular box
indicates the back-off level on which the classification is made.
never occurred in the training example, the classifier would then back off to look at the
test case?s three three-word characteristic tuples. It would continue to back off further,
if necessary. In the case that the model has no information on any of the characteristic
tuples of the test case, it would, by default, classify the test case as an instance of noun
attachment. Figure 3 shows using the back-off scheme on a test case. We describe in
the Test pseudocode routine in Figure 2 the model?s classification procedure for each
back-off level.
3.2 Evaluation Functions
Based on the three classes of predictive criteria discussed in Section 2, we propose
several evaluation functions for the Collins-Brooks model.
3.2.1 The Problem Space. One source of knowledge to exploit is our understanding of
the PP-attachment model and properties of English prepositional phrases. For instance,
we know that the most problematic test cases for the PP-attachment model are those
for which it has no statistics at all. Therefore, those data that the system has not
yet encountered might be good candidates. The first evaluation function we define,
fnovel(u, C), equates the TUV of a candidate u with its degree of novelty, the number
of its characteristic tuples that currently have zero counts:1
fnovel(u, C) =
?
t?Tuples(u)
{
1 : Count(t) = 0
0 : otherwise
This evaluation function has some blatant defects. It may distort the data distribution
so much that the system will not be able to build up a reliable collection of statistics.
The function does not take into account the intuition that those data that rarely occur,
no matter how novel, probably have overall low training utility. Moreover, the scoring
scheme does not make any distinction between the characteristic tuples of a candidate.
1 Note that the current hypothesis C is ignored in evaluation functions of this class because they depend
only on the knowledge about the problem space.
258
Computational Linguistics Volume 30, Number 3
book,
on, shelf
book,
on,
put,
book,
on,
shelf
put,
on,
shelf
put,
on, on,
book,
shelf
on,
put,
noun
on,
on,
shelf
put,
on, shelf
on,
put,
idea
on,
noun
put,
on,
shelf
put,
on, shelf
on,
on,
idea
idea
idea
book,
shelf
on,
on,
wrote
on,
noun
book,
on,
shelf
book,
on,
on,
shelf shelf
book,
on,
on,
wrote
wrote wrote
idea
on,
noun
on,
on,
on,
on,
on,
topic
had
idea
on,
topictopic
ideahad
on,
had
idea
had
topic
u1, u2 u3 u4 u5
Figure 4
If candidate u1 is selected, a total of 22 tuples can be ignored. The dashed rectangles show the
classification level before training, and the solid rectangles show the classification level after
the statistics of u1 have been taken. The obviated tuples are represented by the filled black
circles.
We know, however, that the PP-attachment classifier is a back-off model that makes
its decision based first on statistics of the characteristic tuple with the most words. A
more sophisticated sampling of the data domain should consider not only the novelty
of the data, but also the frequency of its occurrence, as well as the quality of its charac-
teristic tuples. We define a back-off-model-based evaluation function, fbackoff(u, C), that
scores a candidate u by counting the number of characteristic tuples that would be
obviated in all candidates if u were included in the training set. For example, suppose
we have a small pool of five candidates, and we are about to pick the first training
example:
u1 = (put, book, on, shelf)
u2 = (put, book, on, shelf)
u3 = (put, idea, on, shelf)
u4 = (wrote, book, on, shelf)
u5 = (had, idea, on, topic)
According to fbackoff, either u1 or u2 would be the best choice. By selecting either as
the first training example, we could ignore all but the four-word characteristic tuple
for both u1 and u2 (a saving of seven tuples each); since u3 and u4 each have three
words in common with the first two candidates, they would no longer depend on
their lower four tuples; and although we would also improve the statistics for one
of u5?s tuples (on), nothing could be pruned from u5?s characteristic tuples. Thus,
fbackoff(u1, C) = fbackoff(u2, C) = 7 + 7 + 4 + 4 = 22 (see Figure 4).
Under fbackoff, if u1 were chosen as the first example, u2 would lose all its utility,
because we could not prune any extra characteristic tuples by using u2. That is, in
the next round of selection, fbackoff(u2, C) = 0. Candidate u5 would be the best second
example because it would now have the most tuples to prune (7 tuples).
The evaluation function fbackoff improves upon fnovel in two ways. First, novel can-
didates that occur frequently are favored over those that rarely come up. As we have
seen in the above example, a candidate that is similar to other candidates can elimi-
nate more characteristic tuples all at once. Second, the evaluation strategy follows the
working principle of the back-off model and discounts lower-level characteristic tuples
that do not affect the classification process, even if they were ?novel.? For instance,
259
Hwa Sample Selection for Statistical Parsing
after selecting u1 as the first training example, we would no longer care about the
two-word tuples of u4 such as (wrote, on), even though we have no statistics for them.
A potential problem with fbackoff is that after all the obvious candidates have been
selected, the function is not very good at differentiating between the remaining can-
didates that have about the same level of novelty and occur infrequently.
3.2.2 The Performance of the Hypothesis. The evaluation functions discussed in the
previous section score candidates based on prior knowledge alone, independent of the
current state of the learner?s hypothesis and the annotation of the selected training
examples. To attune the selection of training examples to the learner?s progress, an
evaluation function might factor in its current hypothesis in predicting a candidate?s
TUV.
One way to incorporate the current hypothesis into the evaluation function is
to score each candidate using the current model, assuming its hypothesis is right.
An error-driven evaluation function, ferr, equates the TUV of a candidate with the
hypothesis? estimate of its likelihood to misclassify that candidate (i.e., one minus the
probability of the most-likely class). If the hypothesis predicts that the likelihood of a
prepositional phrase to attach to the noun is 80%, and if the hypothesis is accurate,
then there is a 20% chance that it has misclassified.
A related evaluation function is one that measures the hypothesis?s uncertainty
across all classes, rather than focusing on only the most likely class. Intuitively, if the
hypothesis classifies a candidate as equally likely to attach to the verb as to the noun, it
is the most uncertain of its answer. If the hypothesis assigns a candidate to a class with
a probability of one, then it is the most certain of its answer. For the binary-class case,
the uncertainty-based evaluation function, func, can be expressed in the same way as
the error-driven function, as a function that is symmetric about 0.5 and monotonically
decreases if the hypothesis prefers one class over another:2
func(u, C) = ferr(u, C)
=
{
1 ? P(noun | u, C) : P(noun | u, C) ? 0.5
P(noun | u, C) : otherwise
= 0.5 ? abs(0.5 ? P(noun | u, C)) (1)
In the general case of choosing between multiple classes, ferr and func are different from
one another. We shall return to this point in Section 4.1.2 when we consider training
parsers.
The potential drawback of the performance-based evaluation functions is that they
assume that the hypothesis is correct. Selecting training examples based on a poor hy-
pothesis is prone to pitfalls. On the one hand, the hypothesis may be overly confident
about the certainty of its decisions. For example, the hypothesis may assign noun to
a candidate with a probability of one based on parameter estimates computed from a
single previous observation in which a similar example was labeled as noun. Despite
the unreliable statistics, this candidate would not be selected, since the hypothesis
considers this a known case. Conversely, the hypothesis may also direct the selec-
tion algorithm to chase after undecidable cases. For example, consider prepositional
phrases (PPs) with in as the head. These PPs occur frequently, and about half of them
should attach to the object noun. Even though training on more labeled in examples
2 As long as it adheres to these criteria, the specific form of the function is irrelevant, since the selection
is not determined by the absolute scores of the candidates, but by their scores relative to each other.
260
Computational Linguistics Volume 30, Number 3
does not improve the model?s performance on future in PPs, the selection algorithm
will keep on requesting more in training examples because the hypothesis remains un-
certain about this preposition.3 With an unlucky starting hypothesis, these evaluation
functions may select uninformative candidates initially.
3.2.3 The Parameters of the Hypothesis. The potential problems with performance-
based evaluation function stems from their trust in the model?s diagnosis of its own
progress. Another way to incorporate the current hypothesis is to determine how good
it is and what type of examples will improve it the most. In this section we propose
an evaluation function that scores candidates based on their utilities in increasing the
confidence about the parameters of the hypothesis (i.e., the collection of statistics over
the characteristic tuples of the training examples).
Training the parameters of the PP-attachment model is similar to empirically de-
termining the bias of a coin. We measure the coin?s bias by repeatedly tossing it and
keeping track of the percentage of times it lands on heads. The more trials we perform,
the more confident we become about our estimation of the bias. Similarly, in estimating
p, the likelihood of a PP?s attaching to its object noun, we are more confident about the
classification decision based on statistics with higher counts than based on statistics
with lower counts. A quantitative measurement of our confidence in a statistic is the
confidence interval. This is a region around the measured statistic, bounding the area
within which the true statistic may lie. More specifically, the confidence interval for p,
a binomial parameter, is defined as
conf int(p?, n) =
1
1 + t
2
n
(
p? +
t2
2n
? t
?
p?(1 ? p?)
n
+
t2
4n2
)
where p? is the expected value of p based on n trials, and t is a threshold value that
depends on the number of trials and the level of confidence we desire. For instance,
if we want to be 90% confident that the true statistic p lies within the interval, and p?
is based on n = 30 trials, then we set t to be 1.697.4 Applying the confidence interval
concept to evaluating candidates for the back-off PP-attachment model, we define
a function fconf that scores a candidate by taking the average of the lengths of the
confidence interval of each back-off level. That is,
fconf(u, C) =
4
?
l=1
|conf int(p?l(u, C), nl(u, C))|
4
where p?l(u, C) is the probability that model C will attach u to noun at back-off level l,
and nl(u, C) is the number of training examples upon which this classification is based.
The confidence-based evaluation function has several potential problems. One of
its flaws is similar to that of fnovel. In the early stage, fconf picks the same examples
3 This phenomenon is particularly acute in the early stages of refining the hypothesis because most
decisions are based on statistics of the head preposition alone; in the later stages, the hypothesis can
usually rely on higher-ordered characteristic tuples that tend to be better classifiers.
4 For n ? 120, the values of t can be found in standard statistic textbooks; for n ? 120, t = 1.6576.
Because the derivation for the confidence interval equation makes a normality assumption, the
equation does not hold for small values of n (cf Larsen and Marx [1986], pp. 277?278). When n is large,
the contributions from the terms in t
2
n are negligible. Dropping these terms, we have the t statistic for
large n, p? ? t
?
p?(1 ? p?)/n.
261
Hwa Sample Selection for Statistical Parsing
as fnovel, because we have no confidence in the statistics of novel examples. Therefore,
fconf is also prone to chase after examples that rarely occur to build up the confidence
of some unimportant parameters. A second problem is that fconf ignores the output of
the model. Thus, if candidate A has a confidence interval around [0.6, 1] and candidate
B has a confidence interval around [0.4, 0.7], then fconf will prefer candidate A, even
though training on A will not change the hypothesis?s performance, since the entire
confidence interval is already in the noun zone.
3.2.4 Hybrid Function. The three categories of predictive criteria discussed above are
complementary, each focusing on a different aspect of the learner?s weakness. There-
fore, it may be beneficial to combine these criteria into one evaluation function. For
instance, the deficiency of the confidence-based evaluation function described in the
previous section can be avoided if the confidence interval covering the region around
the uncertainty boundary (candidate B in the example just discussed) is weighed more
heavily than one around the end points (candidate A).
In this section, we introduce a new function that tries to factor in both the uncer-
tainty of the model performance and the confidence of the model parameters. First, we
define a function, called area(p?, n), that computes the area under a Gaussian function
N(x,?,?) with a mean of 0.5 and a standard deviation of 0.1 that is bounded by the
confidence interval as computed by conf int(p?, n) (see Figure 5).5 That is, suppose p?
has a confidence interval of [a, b]; then
area(p?, n) =
? b
a
N(x, 0.5, 0.1)dx
Computing area for each back-off level, we define an evaluation function, farea(u, C),
as their average. This function can be viewed as a product of fconf and func.6
3.3 Experimental Comparison
To determine the relative merits of the proposed evaluation functions, we compare the
learning curve of training with sample selection according to each function against a
baseline of random selection in an empirical study. The corpus for this comparison is
a collection of phrases extracted from the Wall Street Journal (WSJ) Treebank. We use
Section 00 as the development set and Sections 2-23 as the training and test sets. We
perform 10-fold cross-validation to ensure the statistical significance of the results. For
each fold, the training candidate pool contains about 21,000 phrases, and the test set
contained about 2,000 phrases.
As shown in Figure 1, the learner generates an initial hypothesis based on a small
set of training examples, L. These examples are randomly selected from the pool of
unlabeled candidates and annotated by a human. Random sampling ensures that the
initial trained set reflects the distribution of the candidate pool and thus that the
initial hypothesis is unbiased. Starting with an unbiased hypothesis is important for
those evaluation functions whose scoring metrics are affected by the accuracy of the
hypothesis. In these experiments, L initially contains 500 randomly selected examples.
In each selection iteration, all the candidates are scored by the evaluation function,
and n examples with the highest TUVs are picked out from U to be labeled and added
5 The standard deviation value for the Gaussian is chosen so that more than 98% of the mass of the
distribution is between 0.25 and 0.75.
6 Note that we can replace the function in equation (1) with the N(x, 0.5, ?) without affecting func,
because it is also symmetric about 0.5 and monotonically decreasing as the input value moves further.
262
Computational Linguistics Volume 30, Number 3
0.4
Likelihood of Attach NP
1.00
Figure 5
An example: Suppose that the candidate has a likelihood of 0.4 for noun attachment and a
confidence interval of width 0.1. Then area computes the area bounded by the confidence
interval and the Gaussian curve.
to L. Ideally, we would like to have n = 1 for each iteration. In practice, however, it is
often more convenient for the human annotator to label data in larger batches rather
than one at a time. In these experiments, we use a batch size of n = 500 examples.
We make note of one caveat to this kind of n-best batch selection. Under a
hypothesis-dependent evaluation function, identical examples will receive identical
scores. Because identical (or very similar) examples tend to address the same defi-
ciency in the hypothesis, adding n very similar examples to the training set is unlikely
to lead to big improvements in the hypothesis. To diversify the examples in each batch,
we simulate single-example selection (whenever possible) by reestimating the scores
of the candidates after each selection. Suppose we have just chosen to add candidate
x to the batch. Then, before selecting the next candidate, we estimate the potential
decrease in scores of candidates similar to x once it belongs to the annotated training
set. The estimation is based entirely on the knowledge that x is chosen, but not on
the classification of x. Thus, only certain types of evaluation functions are amenable
to the reestimation process. For example, if scores have been assigned by fconf, then
we know that the confidence intervals of the candidates similar to x must decrease
slightly after learning x. On the other hand, if scores have been assigned by func, then
we cannot perceive any changes in the scores of similar candidates without knowing
the true classification of x.
3.3.1 Results and Discussion. This section presents the empirical measurements of
the model?s performances using training examples selected by different evaluation
functions. We compare each proposed function with the baseline of random selection
(frand). The results are graphically depicted from two perspectives. One (e.g., Figure
6(a)?6(c)) plots the learning curves of the functions, showing the relationship between
the number of training examples (x-axis) and the performance of the model on test
data (y-axis). We deem one evaluation function to be better than another if its learning
curve envelopes the other?s. An alternative way to interpret the results is to focus on
the reduction in training size offered by one evaluation function over another for some
particular performance level. Figure 6(d) is a bar graph comparing all the evaluation
263
Hwa Sample Selection for Statistical Parsing
74
76
78
80
82
84
86
0 5000 10000 15000 20000
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 o
n 
th
e 
te
st
 s
et
 (%
)
Number of examples in the training set
baseline
novelty
backoff
74
76
78
80
82
84
86
0 5000 10000 15000 20000
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 o
n 
th
e 
te
st
 s
et
 (%
)
Number of examples in the training set
baseline
uncertainty
confidence
(a) (b)
74
76
78
80
82
84
86
0 5000 10000 15000 20000
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 o
n 
th
e 
te
st
 s
et
 (%
)
Number of examples in the training set
baseline
area
0
5,000
10,000
15,000
20,000
25,000
ba
se
line
no
ve
lty
ba
cko
ff
un
ce
rta
int
y
co
nfi
de
nc
e
are
a
Evaluation Functions
N
um
be
r 
of
 L
ab
el
ed
 T
ra
in
in
g 
E
xa
m
pl
es
(c) (d)
Figure 6
A comparison of the performance of different evaluation functions: (a) compares the learning
curves of the functions that use knowledge about the problem space (fnovel and fbackoff) with
that of the baseline; (b) compares the learning curves of performance-based function (func and
fconf ) with the baseline; (c) compares the learning curve of farea, which combines uncertainty
and confidence, with func, fconf, and the baseline; (d) compares all the functions for the number
of training examples selected at the final performance level (83.8%).
functions at the highest performance level. The graph shows that in order to train a
model that attaches PPs with an accuracy rate of 83.8%, sample selection with fnovel
requires 2,500 fewer examples than the baseline.
Compared to fnovel, fbackoff selects more helpful training examples in the early stage.
As shown in Figure 6(a), the improvement rate of the model under fbackoff is always
at least as fast that for as fnovel. However, the differences between these two functions
become smaller for higher performance levels. This outcome validates our predictions.
Scoring candidates by a combination of their novelty, occurrence frequencies, and the
qualities of their characteristic tuples, fbackoff selects helpful early (the first 4,000 or so)
training examples. Then, just as in fnovel, the learning rate remains stagnant for the
next 2,000 poorly selected examples. Finally, when the remaining candidates all have
similar novelty values and contain mostly characteristic tuples that occur infrequently,
the selection becomes random.
Figure 6(b) compares the two evaluation functions that score candidates based on
the current state of the hypothesis. Although both functions suffer a slow start, they
are more effective than fbackoff at reducing the training set when learning high-quality
models. Initially, because all the unknown statistics are initialized to 0.5, selection based
on func is essentially random sampling. Only after the hypothesis becomes sufficiently
accurate (after training on about 5,000 annotated examples) does it begin to make
264
Computational Linguistics Volume 30, Number 3
informed selections. Following a similar but more exaggerated pattern, the confidence-
based function, fconf, also improves slowly at the beginning before finally overtaking
the baseline. As we noted earlier, because the hypothesis is not confident about novel
candidates, fconf and fnovel tend to select the same early examples. Therefore, the early
learning rate of fconf is as poor as that of fnovel. In the later stage, while fnovel continues
to flounder, fconf can select better candidates based on a more reliable hypothesis.
Finally, the best-performing evaluation function is the hybrid approach. Figure
6(c) shows that the learning curve of farea combines the earlier success of func and the
later success of fconf to always outperform the other functions. As shown in Figure
6(d), it requires the least number of examples to achieve the highest performance level
of 83.8%. Compared to the baseline, farea requires 47% fewer examples to achieve this
performance level. From these comparison studies, we conclude that involving the
hypothesis in the selection process is a key factor in reducing the size of the training
set.
4. Sample Selecting for Statistical Parsing
In applying sample selection to training a PP-attachment model, we have observed
that all effective evaluation functions make use of the model?s current hypothesis in
estimating the training utility of the candidates. Although knowledge about the prob-
lem space seems to help sharpening the learning curve initially, overall, it is not a good
predictor. In this section, we investigate whether these observations hold true for train-
ing statistical parsing models as well. Moreover, in order to determine whether the
performances of the predictive criteria are consistent across different learning models
within the same domain, we have performed the study on two parsing models: one
based on a context-free variant of tree-adjoining grammars (Joshi, Levy, and Taka-
hashi 1975), the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism
(Schabes and Waters 1993; Hwa 1998), and Collins?s Model 2 parser (1997). Although
both models are lexicalized, statistical parsers, their learning algorithms are different.
The Collins Parser is a fully supervised, history-based learner that models the pa-
rameters of the parser by taking statistics directly from the training data. In contrast,
PLTIG?s expectation-maximization-based induction algorithm is partially supervised;
the model?s parameters are estimated indirectly from the training data.
As a superset of the PP-attachment task, parsing is a more challenging learning
problem. Whereas a trained PP-attachment model is a binary classifier, a parser must
identify the correct syntactic analysis out of all possible parses for a sentence. This
classification task is more difficult than PP-attachment, since the number of possible
parses for a sentence grows exponentially with respect to its length. Consequently,
the annotator?s task is more complex. Whereas the person labeling the training data
for PP-attachment reveals one unit of information (always choosing between noun or
verb), the annotation needed for parser training is usually greater than one unit,7 and
the type of labels varies from sentence to sentence. Because the annotation complexity
differs from sentence to sentence, the evaluation functions must strike a balance be-
tween maximizing potential informational gain and minimizing the expected amount
7 We consider each pair of brackets in the training sentence to be one unit of supervised information,
assuming that the number of brackets correlates linearly with the amount of effort spent by the human
annotator. This correlation is an approximation, however; in real life, adding one pair of brackets to a
longer sentence may require more effort than adding a pair of brackets to a shorter one. To capture
bracketing interdependencies at this level, we would need to develop a model of the annotation
decision process and incorporate it as an additional factor in the evaluation functions.
265
Hwa Sample Selection for Statistical Parsing
of annotation exerted. We propose a set of evaluation functions similar in spirit to those
for the PP-attachment learner, but extended to accommodate the parsing domain.
4.1 Evaluation Functions
4.1.1 Problem Space. Similarly to scoring a PP candidate based on the novelty and
frequencies of its characteristic tuples, we define an evaluation function, flex(w, G) that
scores a sentence candidate, w, based on the novelty and frequencies of word pair
co-occurrences:
flex(w, G) =
?
wi,wj?w new(wi, wj)? coocc(wi, wj)
length(w)
where w is the unlabeled sentence candidate, G is the current parsing model (which
is ignored by problem-space-based evaluation functions), new(wi, wj) is an indicator
function that returns one if we have not yet selected any sentence in which wi and wj
co-occurred, and coocc(wi, wj) is a function that returns the number of times that wi co-
occurs8 with wj in the candidate pool. We expect these evaluation functions to be less
relevant for the parsing domain than for the PP-attachment domain for two reasons.
First, because we do not have the actual parses, the extraction of lexical relationships
is based on co-occurrence statistics, not syntactic relationships. Second, because the
distribution of words that form lexical relationships is wider and more uniform than
that of words that form PP characteristic tuples, most word pairs will be novel and
appear only once.
Another simple evaluation function based on the problem space is one that esti-
mates the TUV of a candidate from its sentence length:
flen(w, G) = length(w)
The intuition behind this function is based on the general observation that longer
sentences tend to have complex structures and introduce more opportunities for am-
biguous parses. Although these evaluation functions may seem simplistic, they have
one major advantage: They are easy to compute and require little processing time.
Because inducing parsing models demands significantly more time than inducing PP-
attachment models, it becomes more important that the evaluation functions for pars-
ing models be as efficient as possible.
4.1.2 The Performance of the Hypothesis. We previously defined two performance-
based evaluation functions: ferr, the model?s estimate of the likelihood that is has
made a classification error, and func, the model?s estimate of its uncertainty in making
the classification. We have shown the two functions to have similar performance for
the PP-attachment task. This is not the case for statistical parsing, however, because
the number of possible classes (parse trees) differs from sentence to sentence. For
example, suppose we wish to compare one candidate for which the current parsing
model generated four equally likely parses with another candidate for which the model
generated 1 parse with probability of 0.2 and 99 other parses with a probability of
0.01 (such that they sum to 0.98). The error-driven function, ferr, would score the latter
candidate higher because its most likely parse has a lower probability than that of the
most likely parse of the former candidate; the uncertainty-based function, func, would
score the former candidate higher because the model does not have a strong preference
8 We consider two words to be co-occuring if their log-likelihood ratio is greater than some threshold
value determined with held-out data.
266
Computational Linguistics Volume 30, Number 3
for one parse over any other. In this section, we provide a formal definition for both
functions.
Suppose that a parsing model G generates a candidate sentence w with probability
P(w | G), and that the set V contains all possible parses that G generated for w. Then,
we denote the probability of G?s generating a single parse, v ? V , as P(v | G) such that
?
v?V
P(v | G) = P(w | G). The parse chosen for w is the most likely parse in V , denoted
as vmax, where
vmax = argmaxv?VP(v | G)
Note that P(v | G) reflects the probability of one particular parse tree, v, out of all
possible parse trees for all possible sentences that G can generate. To compute the
likelihood of a parse?s being the correct parse out of the possible parses of w according
to G, denoted as P(v | w, G), we need to normalize the tree probability by the sentence
probability. So according to G, the likelihood that vmax is the correct parse for w is9
P(vmax | w, G) =
P(vmax | G)
P(w | G)
=
P(vmax | G)
?
v?V P(v | G)
. (2)
Therefore, the error-driven evaluation function is defined as
ferr(w, G) = 1 ? P(vmax | w, G)
Unlike the error-driven function, which focuses on the most likely parse, the
uncertainty-based function takes the probability distribution of all parses into account.
To quantitatively characterize its distribution, we compute the entropy of the distri-
bution. That is,
H(V) = ?
?
v?V
p(v) lg(p(v)) (3)
where V is a random variable that can take any possible outcome in set V , and p(v) =
Pr(V = v) is the density function. Further details about the properties of entropy can
be found in textbooks on information theory (e.g., Cover and Thomas 1991).
Determining the parse tree for a sentence from a set of possible parses can be
viewed as assigning a value to a random variable. Thus, a direct application of the
entropy definition to the probability distribution of the parses for sentence w in G
computes its tree entropy, TE(w, G), the expected number of bits needed to encode
the distribution of possible parses for w. However, we may not wish to compare
two sentences with different numbers of parses by their entropy directly. If the parse
probability distributions for both sentences are uniform, the sentence with more parses
will have a higher entropy. Because longer sentences typically have more parses, using
entropy directly would result in a bias toward selecting long sentences. To normalize
for the number of parses, the uncertainty-based evaluation function, func, is defined as
a measurement of similarity between the actual probability distribution of the parses
and a hypothetical uniform distribution for that set of parses. In particular, we divide
9 Note that P(w|v, G) = 1 for any v ? V , where V is the set of all possible parses for w, because v exists
only when w is observed.
267
Hwa Sample Selection for Statistical Parsing
the tree entropy by the log of the number of parses:10
func(w, G) =
TE(w, G)
lg(?V?)
We now derive the expression for TE(w, G). Recall from equation (2) that if G
produces a set of parses, V , for sentence w, the set of probabilities P(v | w, G) (for all
v ? V) defines the distribution of parsing likelihoods for sentence w:
?
v?V
P(v | w, G) = 1
Note that P(v | w, G) can be viewed as a density function p(v) (i.e., the probability of
assigning v to a random variable V). Mapping it back into the entropy definition from
equation (3), we derive the tree entropy of w as follows:
TE(w, G) = H(V)
= ?
?
v?V
p(v) lg(p(v))
= ?
?
v?V
P(v | G)
P(w | G) lg(
P(v | G)
P(w | G) )
= ?
?
v?V
P(v | G)
P(w | G) lg(P(v | G)) +
?
v?V
P(v | G)
P(w | G) lg(P(w | G))
= ? 1
P(w | G)
?
v?V
P(v | G) lg(P(v | G)) + lg(P(w | G))
P(w | G)
?
v?V
P(v | G)
= ? 1
P(w | G)
?
v?V
P(v | G) lg(P(v | G)) + lg(P(w | G))
Using the bottom-up, dynamic programming technique (see the appendix for de-
tails) of computing inside probabilities (Lari and Young 1990), we can efficiently com-
pute the probability of the sentence, P(w | G). Similarly, the algorithm can be modified
to compute the quantity
?
v?V
P(v | G) lg(P(v | G)).
4.1.3 The Parameters of the Hypothesis. Although the confidence-based function
gives good TUV estimates to candidates for training PP-attachment models, it is not
clear how a similar technique can be applied to training parsers. Whereas binary
classification tasks can be described by binomial distributions, for which the confi-
dence interval is well defined, a parsing model is made up of many multinomial
classification decisions. We therefore need a way to characterize the confidence for
each decision as well as a way to combine them into an overall confidence. Another
difficulty is that the complexity of the induction algorithm deters us from reestimat-
ing the TUVs of the remaining candidates after selecting each new candidate. As we
10 When func(w, G) = 1, the parser is considered to be the most uncertain about a particular sentence.
Instead of dividing tree entropies, one could have computed the Kullback-Leibler distance between the
two distributions (in which case a score of zero would indicate the highest level of uncertainty).
Because the selection is based on relative scores, as long as the function is monotonic, the exact form of
the function should not have much impact on the outcome.
268
Computational Linguistics Volume 30, Number 3
discussed in Section 3.3, reestimation is important for batched annotation. Without
some means of updating the TUVs after each selection, the learner will not realize that
it has already selected a candidate to train some parameter with low confidence until
the retraining phase, which occurs only at the end of the batch selection; therefore, it
may continue to select very similar candidates to train the same parameter. Even if we
assume that the statistics can be updated, reestimating the TUVs is a computationally
expensive operation. Essentially, all the remaining candidates that share some param-
eters with the selected candidate will need to be re-parsed. For these practical rea-
sons, we do not include an evaluation function measuring confidence for the parsing
experiment.
4.2 Experiments and Results
We compare the effectiveness of sample selection using the proposed evaluation func-
tions against a baseline of random selection (frand(w, G) = rand()). Similarly to previous
experimental designs, the learner is given a small set of annotated seed data from the
WSJ Treebank and a large set of unlabeled data (also from the WSJ Treebank but with
the labels removed) from which to select new training examples. All training data are
from Sections 2?21 of the treebank. We monitor the learning progress of the parser by
testing it on unseen test sentences. We use Section 00 for development and Section
23 for testing. This study is repeated for two different models, the PLTIG parser and
Collins?s Model 2 parser.
4.2.1 An Expectation-Maximization-Based Learner. In the first experiment, we use
an induction algorithm (Hwa 2001a) based on the expectation-maximization (EM)
principle that induces parsers for PLTIGs. The algorithm performs heuristic search
through an iterative reestimation procedure to find local optima: sets of values for
the grammar parameters that maximizes the grammar?s likelihood of generating the
training data. In principle, the algorithm supports unsupervised learning; however,
because the search space has too many local optima, the algorithm tends to converge
on a model that is unsuitable for parsing. Here, we consider a partially supervised
variant in which we assume that the learner is given the phrasal boundaries of the
training sentences but not the label of the constituent units. For example, the sentence
Several fund managers expect a rough market this morning before prices stabilize. would be
labeled as ?((Several fund managers) (expect ((a rough market) (this morning)) (before
(prices stabilize))).)? Our algorithm is similar to the approach taken by Pereira and
Schabes (1992) for inducing PCFG parsers.
Because the EM algorithm itself is an iterative procedure, performing sample se-
lection on top of an EM-based learner is an extremely computational-intensive process.
Here, we restrict the experiments for the PLTIG parsers to a smaller-scale study in the
following two aspects. First, the lexical anchors of the grammar rules are backed off to
part-of-speech tags; this restricts the size of the grammar vocabulary to 48. Second, the
unlabeled candidate pool is set to contain 3,600 sentences, which is sufficiently large
for inducing a grammar of this size. The initial model is trained on 500 labeled seed
sentences. For each selection iteration, an additional 100 sentences are moved from
the unlabeled pool to be labeled and added to the training set. After training, the
updated parser is then tested on unseen sentences (backed off to their part-of-speech
tags) and compared to the gold standard. Because the induced PLTIG produces binary-
branching parse trees, which have more layers than the gold standard, we measure
parsing accuracy in terms of the crossing-bracket metric. The study is repeated for
10 trials, each using a different portion of the full training set, to ensure statistical
significance (using pairwise t-test at 95% confidence).
269
Hwa Sample Selection for Statistical Parsing
76
77
78
79
80
81
5000 10000 15000 20000 25000 30000 35000 40000 45000
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 o
n 
th
e 
te
st
 s
et
 (%
)
Number of labeled brackets in the training set
baseline
length
error driven
tree entropy
(a)
0
5,000
10,000
15,000
20,000
25,000
30,000
35,000
40,000
ba
se
line
len
gth
err
or 
dri
ve
n
tre
e e
ntr
op
y
Evaluation functions
N
um
be
r 
of
 L
ab
el
ed
 B
ra
ck
et
s 
in
 th
e 
Tr
ai
ni
ng
 D
at
a
(b)
Figure 7
PLTIG parser: (a) A comparison of the evaluation functions? learning curves. (b) A comparison
of the evaluation functions for a test performance score of 80%.
The results of the experiment are graphically shown in Figure 7. As with the
PP-attachment studies, Figure 7(a) compares the learning curves of the proposed eval-
uation functions to that of the baseline. Note that even though these functions select
examples in terms of entire sentences, the amount of annotation is measured in the
graphs (x-axis) in terms of the number of brackets rather than sentences. Unlike in
the PP-attachment case, the amount of effort from the annotators varies significantly
from example to example. A short and simple sentence takes much less time to an-
notate than a long and complex sentence. We address this effect by approximating
the amount of effort as the number of brackets the annotator needs to label. Thus,
we deem one evaluation function more effective than another if, for the desired level
of performance, the smallest set of sentences selected by the function contains fewer
brackets than that of the other function. Figure 7(b) compares the evaluation functions
at the final test performance level of 80%.
270
Computational Linguistics Volume 30, Number 3
Qualitatively comparing the learning curves in the figure, we see that with the ap-
propriate evaluation function, sample selection does reduce the amount of annotation.
Similarly to our findings in the PP-attachment study, the simple problem-space-based
evaluation function, flen, offers only little savings; its performance is nearly indistin-
guishable from that of the baseline, for the most part.11 The evaluation functions based
on hypothesis performances, on the other hand, do reduce the amount of annotation in
the training data. Of the two that we proposed for this category, the tree entropy eval-
uation function, func, has a slight edge over the error-driven evaluation function, ferr.
For a quantitative comparison, let us consider the set of grammars that achieve
an average parsing accuracy of 80% on the test sentences. We consider a grammar to
be comparable to that of the baseline if its mean test score is at least as high as that
of the baseline and if the difference between the means is not statistically significant.
The baseline case requires an average of about 38,000 brackets in the training data. In
contrast, to induce a grammar that reaches the same 80% parsing accuracy with the
examples selected by func, the learner requires, on average, 19,000 training brackets.
Although the learning rate of ferr is slower than that of func overall, it seems to have
caught up in the end; it needs 21,000 training brackets, slightly more than func. While
the simplistic sentence length evaluation function, flen, is less helpful, its learning rate
still improves slightly faster than that of the baseline. A grammar of comparable quality
can be induced from a set of training examples selected by flen containing an average
of 28,000 brackets.12
4.2.2 A History-Based Learner. In the second experiment, the basic learning model
is Collins?s (1997) Model 2 parser, which uses a history-based learning algorithm that
takes statistics directly over the treebank. As a fully supervised algorithm, it does not
have to iteratively reestimate its parameters and is computationally efficient enough
for us to carry out a large-scale experiment. For this set of studies, the unlabeled
candidate pool consists of around 39,000 sentences. The initial model is trained on
500 labeled seed sentences, and at each selection iteration, an additional 100 sentences
are moved from the unlabeled pool into the training set. The parsing performance on
the test sentences is measured in terms of the parser?s F-score, the harmonic average
of the labeled precision and labeled recall rates over the constituents (Van Rijsbergen
1979).13
We plot the comparisons between different evaluation functions and the baseline
for the history-based parser in Figure 8. The examples selected by the problem-space-
based functions do not seem to be helpful. Their learning curves are, for the most part,
slightly worse than the baseline. In contrast, the parsers trained on data selected by
the error-driven and uncertainty-based functions learn faster than the baseline; and as
before, func performs slightly better than ferr.
For the final parsing performance of 88%, the parser requires a baseline training set
of 30,500 sentences annotated with about 695,000 constituents. The same performance
can be achieved with a training set of 20,500 sentences selected by ferr, which contains
about 577,000 annotated constituents; or with a training set of 17,500 sentences selected
by func, which contains about 505,000 annotated constituents, reducing the number of
annotated constituents by 27%. Comparing the outcome of this experiment with that of
11 In this experiment, we have omitted the evaluation function for selecting novel lexical relationships,
flex, because the grammar does not use actual lexical anchors.
12 In terms of the number of sentences, the baseline frand selected 2,600 sentences; flen selected 1,300
sentences; and ferr and func each selected 900 sentences.
13 F = 2?LR?LPLR+LP , where LR is the labeled recall score and LP is the labeled precision score.
271
Hwa Sample Selection for Statistical Parsing
78
80
82
84
86
88
100000 200000 300000 400000 500000 600000 700000 800000 900000
Cl
as
sif
ica
tio
n 
ac
cu
ra
cy
 o
n 
th
e 
te
st
 s
et
 (%
)
Number of labeled constituents in the training set
baseline
length
novel lex
error driven
tree entropy
(a)
0
100,000
200,000
300,000
400,000
500,000
600,000
700,000
800,000
ba
se
line no
ve
l
len
gth
err
or 
dri
ve
n
tre
e e
ntr
op
y
Evaluation Functions
N
um
be
r 
of
 L
ab
el
ed
 C
on
st
itu
en
ts
 in
 th
e 
Tr
ai
ni
ng
 D
at
a
(b)
Figure 8
Model 2 parser: (a) A comparison of the learning curves of the evaluation functions. (b) A
comparison of all the evaluation functions at the test performance level of 88%.
the experiment involving the EM-based learner, we see that the training data reduction
rates are less dramatic than before. This may be because both func and ferr ignore lexical
items and chase after sentences containing words that rarely occur. Recent work by
Tang, Luo, and Roukos (2002) suggests that a hybrid approach that combines features
of the problem space and the uncertainty of the parser may result in better performance
for lexicalized parsers.
5. Related Work
Sample selection benefits problems in which the cost of acquiring raw data is cheap but
the cost of annotating them is high, as is certainly the case for many supervised learn-
ing tasks in natural language processing. In addition to PP-attachment, as discussed
in this article, sample selection has been successfully applied to other classification
272
Computational Linguistics Volume 30, Number 3
applications. Some examples include text categorization (Lewis and Catlett 1994), base
noun phrase chunking (Ngai and Yarowsky 2000), part-of-speech tagging (Engelson
Dagan 1996), spelling confusion set disambiguation (Banko and Brill 2001), and word
sense disambiguation (Fujii et al 1998).
More challenging are learning problems whose objective is not classification, but
generation of complex structures. One example in this direction is applying sample
selection to semantic parsing (Thompson, Califf, and Mooney 1999), in which sentences
are paired with their semantic representation using a deterministic shift-reduce parser.
A recent effort that focuses on statistical syntactic parsing is the work by Tang, Lou,
and Roukos (2002). Their results suggest that the number of training examples can be
further reduced by using a hybrid evaluation function that combines a hypothesis-
performance-based metric such as tree entropy (?word entropy? in their terminology)
with a problem-space-based metric such as sentence clusters.
Aside from active learning, researchers have applied other learning techniques
to combat the annotation bottleneck problem in parsing. For example, Henderson
and Brill (2002) consider the case in which acquiring additional human-annotated
training data is not possible. They show that parser performance can be improved by
using boosting and bagging techniques with multiple parsers. This approach assumes
that there are enough existing labeled data to train the individual parsers. Another
technique for making better use of unlabeled data is cotraining (Blum and Mitchell
1998), in which two sufficiently different learners help each other learn by labeling
training data for one another. The work of Sarkar (2001) and Steedman, Osborne, et
al. (2003) suggests that co-training can be helpful for statistical parsing. Pierce and
Cardie (2001) have shown, in the context of base noun identification, that combining
sample selection and cotraining can be an effective learning framework for large-scale
training. Similar approaches are being explored for parsing (Steedman, Hwa, et al
2003; Hwa et al 2003).
6. Conclusion
In this article, we have argued that sample selection is a powerful learning technique
for reducing the amount of human-labeled training data. Our empirical studies suggest
that sample selection is helpful not only for binary classification tasks such as PP-
attachment, but also for applications that generate complex outputs such as syntactic
parsing.
We have proposed several criteria for predicting the training utility of the unla-
beled candidates and developed evaluation functions to rank them. We have conducted
experiments to compare the functions? ability to select the most helpful training exam-
ples. We have found that the uncertainty criterion is a good predictor that consistently
finds helpful examples. In our experiments, evaluation functions that factor in the
uncertainty criterion consistently outperform the baseline of random selection across
different tasks and learning algorithms. For learning a PP-attachment model, the most
helpful evaluation function is a hybrid that factors in the prediction performance of the
hypothesis and the confidence for the values of the parameters of the hypothesis. For
training a parser, we found that uncertainty-based evaluation functions that use tree
entropy were the most helpful for both the EM-based learner and the history-based
learner.
The current work points us in several future directions. First, we shall continue
to develop alternative formulations of evaluation functions to improve the learn-
ing rates of parsers. Under the current framework, we did not experiment with any
hypothesis-parameter-based evaluation functions for the parser induction task; how-
273
Hwa Sample Selection for Statistical Parsing
ever, hypothesis-parameter-based functions may be feasible under a multilearner set-
ting, using parallel machines. Second, while in this work we focused on selecting
entire sentences as training examples, we believe that further reduction in the amount
of annotated training data might be possible if the system could ask the annotators
more-specific questions. For example, if the learner is unsure only of a local decision
within a sentence (such as a PP-attachment ambiguity), the annotator should not have
to label the entire sentence.
In order to allow for finer-grained interactions between the system and the an-
notators, we have to address some new challenges. To begin with, we must weigh
in other factors in addition to the amount of annotations. For instance, the learner
may ask about multiple substrings in one sentence. Even if the total number of la-
bels were fewer, the same sentence would still need to be mentally processed by the
annotators multiple times. This situation is particularly problematic when there are
very few annotators, as it becomes much more likely that a person will encounter the
same sentence many times. Moreover, we must ensure that the questions asked by the
learner are well-formed. If the learner were simply to present the annotator with some
substring that it could not process, the substring might not form a proper linguistic
constituent for the annotator to label. Additionally, we are interested in exploring the
interaction between sample selection and other semisupervised approaches such as
boosting, reranking, and cotraining. Finally, based on our experience with parsing, we
believe that active-learning techniques may be applicable to other tasks that produce
complex outputs such as machine translation.
Appendix: Efficient Computation of Tree Entropy
As discussed in Section 4.1.2, for learning tasks such as parsing, the number of possi-
ble classifications is so large that it may not be computationally efficient to calculate
the degree of uncertainty using the tree entropy definition. In the equation for the tree
entropy of w (TE(w, G)) presented in Section 4.1.2, the computation requires summing
over all possible parses, but the number of possible parses for a sentence grows ex-
ponentially with respect to the sentence length. In this appendix, we show that tree
entropy can be efficiently computed using dynamic programming.
For illustrative purposes, we describe the computation process using a PCFG ex-
pressed in Chomsky normal form.14 The basic idea is to compose the tree entropy of
the entire sentence from the tree entropy of the subtrees. The process is similar to
that for computing the probability of the entire sentence from the probabilities of sub-
strings (called Inside Probabilities). We follow the notation convention of Lari and
Young (1990).
The inside probability of a nonterminal X generating the substring wi . . .wj is
denoted as e(X, i, j); it is the sum of the probabilities of all possible subtrees that have
X as the root and wi . . .wj as the leaf nodes. We define a new function h(X, i, j) to
represent the corresponding entropy for the substring:
h(X, i, j) = ?
?
x?X ??wi...wj
P(x | G) lg(P(x | G))
where G is the current model. Under this notation, the tree entropy of a sentence,
?
v?V
P(v | G) lg P(v | G), is denoted as h(S, 1, n).
14 That is, every production rule must be in one of two forms: a nonterminal expands into two more
nonterminals, or a nonterminal expands into a terminal.
274
Computational Linguistics Volume 30, Number 3
Analogously to the computation of inside probabilities, we compute h(X, i, j) re-
cursively. The base case is when the nonterminal X generates a single token substring
wi. The only possible tree has X at the root, immediately dominating the leaf node wi.
Therefore, the tree entropy is
h(X, i, i) = e(X, i, i) lg(e(X, i, i))
For the general case, h(X, i, j), we must find all rules of the form X ? YZ, where Y and
Z are nonterminals, that have contributed toward X ?? wi . . .wj. To do so, we consider
all possible ways dividing up wi . . .wj into two pieces such that Y
?? wi . . .wk and
Z ?? wk+1 . . .wj:
h(X, i, j) =
j?1
?
k=i
?
(X?YZ)
hY,Z,k(X, i, j)
The function hY,Z,k(X, i, j) is a portion of h(X, i, j) that accounts for those parses in which
the rule X ? YZ is used and the division point is at word wk. The nonterminals Y and
Z may, in turn, generate their substrings with multiple parses. Let Y represent the set
of parses for Y ?? wi . . .wk; let Z represent the set of parses for Z
?? wk+1 . . .wj; and
let x represent the parse step of X ? YZ. Then, there are a total of ?Y? ? ?Z? parses,
and the probability of each parse is P(x)P(y)P(z), where y ? Y and z ? Z . To compute
hY,Z,k, we need to sum over all possible parses:
hY,Z,k(X, i, j) = ?
?
y?Y ,z?Z
P(x)P(y)P(z) lg(P(x)P(y)P(z))
= ?
?
y?Y ,z?Z
P(x)P(y)P(z)(lg P(x) + lg P(y) + lg P(z))
= ?P(x) lg(P(x))e(Y, i, k)e(Z, k + 1, j) + P(x)h(Y, i, k)e(Z, k + 1, j)
+P(x)e(Y, i, k)h(Z, k + 1, j)
Thus, the tree entropy of the entire sentence can be recursively computed from the
entropy values of the substrings.
Acknowledgments
We thank Joshua Goodman, Lillian Lee,
Wheeler Ruml, and Stuart Shieber for
helpful discussions, and Ric Crabbe, Philip
Resnik, and the reviewers for their
constructive comments on this article.
Portions of this work have appeared
previously (Hwa 2000, 2001b); we thank the
reviewers of those papers for their helpful
comments. Parts of this work was carried
out while the author was a graduate student
at Harvard University, supported by the
National Science Foundation under Grant
No. IRI 9712068. The work is also supported
by the Department of Defense contract
RD-02-5700, and ONR MURI Contract
FCPO.810548265.
References
Banko, Michele and Eric Brill. 2001. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics, Toulouse,
France, pages 26?33.
Blum, Avrim and Tom Mitchell. 1998.
Combining labeled and unlabeled data
with co-training. In Proceedings of the 1998
Conference on Computational Learning
Theory, pages 92?100, Madison, WI.
Brill, Eric and Philip S. Resnik. 1994. A rule
based approach to PP attachment
disambiguation. In Proceedings of the 15th
International Conference on Computational
Linguistics (COLING), Kyoto, Japan, pages
1198?1204.
275
Hwa Sample Selection for Statistical Parsing
Charniak, Eugene. 2000. A
maximum-entropy-inspired parser. In
Proceedings of the First Meeting of the North
American Association for Computational
Linguistics, Seattle.
Cohn, David, Les Atlas, and Richard
Ladner. 1994. Improving generalization
with active learning. Machine Learning,
15(2):201?221.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania,
Philadelphia.
Collins, Michael and James Brooks. 1995.
Prepositional phrase attachment through
a backed-off model. In Proceedings of the
Third Workshop on Very Large Corpora,
Cambridge, MA, pages 27?38.
Cover, Thomas M. and Joy A. Thomas.
1991. Elements of Information Theory. John
Wiley, New York.
Engelson, Sean P. and Ido Dagan. 1996.
Minimizing manual annotation cost in
supervised training from corpora. In
Proceedings of the 34th Annual Meeting of the
Association for Computational Linguistics,
Santa Cruz, CA, pages 319?326.
Freund, Yoav, H. Sebastian Seung, Eli
Shamir, and Naftali Tishby. 1997. Selective
sampling using the query by committee
algorithm. Machine Learning,
28(2?3):133?168.
Fujii, Atsushi, Kentaro Inui, Takenobu
Tokunaga, and Hozumi Tanaka. 1998.
Selective sampling for example-based
word sense disambiguation. Computational
Linguistics, 24(4):573?598.
Henderson, John C. and Eric Brill. 2000.
Bagging and boosting a treebank parser.
In Proceedings of the First Meeting of the
North American Association for Computational
Linguistics, Seattle, pages 34?41.
Hwa, Rebecca. 1998. An empirical
evaluation of probabilistic lexicalized tree
insertion grammars. In Proceedings of the
36th Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, Montreal, volume 1, pages
557?563.
Hwa, Rebecca. 2000. Sample selection for
statistical grammar induction. In
Proceedings of 2000 Joint SIGDAT Conference
on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages
45?52, Hong Kong, October.
Hwa, Rebecca. 2001a. Learning Probabilistic
Lexicalized Grammars for Natural Language
Processing. Ph.D. thesis, Harvard
University, Cambridge, MA.
Hwa, Rebecca. 2001b. On minimizing
training corpus for parser acquisition. In
Proceedings of the ACL 2001 Workshop on
Computational Natural Language Learning
(ConLL-2001), Toulouse, France, pages
84?89.
Hwa, Rebecca, Miles Osborne, Anoop
Sarkar, and Mark Steedman. 2003.
Corrected co-training for statistical
parsers. In Proceedings of the ICML
Workshop on the Continuum from Labeled to
Unlabeled Data in Machine Learning and Data
Mining at the 20th International Conference of
Machine Learning (ICML-2003),
Washington, DC, pages 95?102, August.
Joshi, Aravind K., Leon S. Levy, and
Masako Takahashi. 1975. Tree adjunction
grammars. Journal of Computer and System
Sciences, 10(1): 136?163.
Lari, Karim A. and Steve J. Young. 1990.
The estimation of stochastic context-free
grammars using the inside-outside
algorithm. Computer Speech and Language,
4:35?56.
Larsen, Richard J. and Morris L. Marx. 1986.
An Introduction to Mathematical Statistics
and Its Applications. Prentice-Hall,
Englewood Cliffs, NJ.
Lewis, David D. and Jason Catlett. 1994.
Heterogeneous uncertainty sampling for
supervised learning. In Proceedings of the
Eleventh International Conference on Machine
Learning, San Francisco, pages 148?156.
Magerman, David. 1994. Natural Language
Parsing as Statistical Pattern Recognition.
Ph.D. thesis, Stanford University,
Stanford, CA.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn Treebank. Computational Linguistics,
19(2):313?330.
Ngai, Grace and David Yarowsky. 2000.
Rule writing or annotation: Cost-efficient
resource usage for base noun phrase
chunking. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics, pages 117?125, Hong Kong,
October.
Pereira, Fernando C. N. and Yves Schabes.
1992. Inside-outside reestimation from
partially bracketed corpora. In Proceedings
of the 30th Annual Meeting of the Association
for Computational Linguistics, pages
128?135, Newark, DE.
276
Computational Linguistics Volume 30, Number 3
Pierce, David and Claire Cardie. 2001.
Limitations of co-training for natural
language learning from large datasets. In
Proceedings of the 2001 Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2001), pages 1?9,
Pittsburgh, PA.
Ratnaparkhi, Adwait. 1998. Statistical
models for unsupervised prepositional
phrase attachment. In Proceedings of the
36th Annual Meeting of the Association for
Computational Linguistics and 17th
International Conference on Computational
Linguistics, Montreal, volume 2, pages
1079?1085.
Sarkar, Anoop. 2001. Applying co-training
methods to statistical parsing. In
Proceedings of the Second Meeting of the
North American Association for
Computational Linguistics, Pittsburgh,
pages 175?182, June.
Schabes, Yves and Richard Waters. 1993.
Stochastic lexicalized context-free
grammar. In Proceedings of the Third
International Workshop on Parsing
Technologies, Tilburg, The Netherlands,
and Durbuy, Belgium, pages 257?266.
Steedman, Mark, Rebecca Hwa, Stephen
Clark, Miles Osborne, Anoop Sarkar, Julia
Hockenmaier, Paul Ruhlen, Steven Baker,
and Jeremiah Crim. 2003. Example
selection for bootstrapping statistical
parsers. In Proceedings of the Joint
Conference of Human Language Technologies
and the Annual Meeting of the North
American Chapter of the Association for
Computational Linguistics, Edmonton,
Alberta, Canada, pages 236?243.
Steedman, Mark, Miles Osborne, Anoop
Sarkar, Stephen Clark, Rebecca Hwa, Julia
Hockenmaier, Paul Ruhlen, Steven Baker,
and Jeremiah Crim. 2003. Bootstrapping
statistical parsers from small datasets. In
Proceedings of the Tenth Conference of the
European Chapter of the Association for
Computational Linguistics, Budapest, pages
331?338.
Tang, Min, Xiaoqiang Luo, and Salim
Roukos. 2002. Active learning for
statistical natural language parsing. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics,
Philadelphia, pages 120?127, July.
Thompson, Cynthia A., Mary Elaine Califf,
and Raymond J. Mooney. 1999. Active
learning for natural language parsing and
information extraction. In Proceedings of
the Sixteenth International Conference on
Machine Learning (ICML-99), pages
406?414, Bled, Slovenia.
Van Rijsbergen, Cornelis J. 1979. Information
Retrieval. Butterworth, London.
Co-training for Predicting Emotions with Spoken Dialogue Data 
Beatriz Maeireizo and Diane Litman and Rebecca Hwa 
Department of Computer Science 
University of Pittsburgh 
Pittsburgh, PA 15260, U.S.A. 
beamt@cs.pitt.edu, litman@cs.pitt.edu, hwa@cs.pitt.edu 
 
Abstract 
Natural Language Processing applications 
often require large amounts of annotated 
training data, which are expensive to obtain.  
In this paper we investigate the applicability of 
Co-training to train classifiers that predict 
emotions in spoken dialogues.  In order to do 
so, we have first applied the wrapper approach 
with Forward Selection and Na?ve Bayes, to 
reduce the dimensionality of our feature set. 
Our results show that Co-training can be 
highly effective when a good set of features 
are chosen.  
1 Introduction 
In this paper we investigate the automatic 
labeling of spoken dialogue data, in order to train a 
classifier that predicts students? emotional states in 
a human-human speech-based tutoring corpus.  
Supervised training of classifiers requires 
annotated data, which demands costly efforts from 
human annotators.  One approach to minimize this 
effort is to use Co-training (Blum and Mitchell, 
1998), a semi-supervised algorithm in which two 
learners are iteratively combining their outputs to 
increase the training set used to re-train each other 
and generate more labeled data automatically.  The 
main focus of this paper is to explore how Co-
training can be applied to annotate spoken 
dialogues.  A major challenge to address is in 
reducing the dimensionality of the many features 
available to the learners. 
The motivation for our research arises from the 
need to annotate a human-human speech corpus for 
the ITSPOKE (Intelligent Tutoring SPOKEn 
dialogue System) project (Litman and Silliman, 
2004). Ongoing research in ITSPOKE aims to 
recognize emotional states of students in order to 
build a spoken dialogue tutoring system that 
automatically predicts and adapts to the student?s 
emotions.  ITSPOKE uses supervised learning to 
predict emotions with spoken dialogue data.  Al-
though a large set of dialogues have been 
collected, only 8% of them have been annotated 
(10 dialogues with a total of 350 utterances), due to 
the laborious annotation process.  We believe that 
increasing the size of the training set with more 
annotated examples will increase the accuracy of 
the system?s predictions.  Therefore, we are 
looking for a less labour-intensive approach to data 
annotation.  
2 Data 
Our data consists of the student turns in a set of 
10 spoken dialogues randomly selected from a 
corpus of 128 qualitative physics tutoring 
dialogues between a human tutor and University of 
Pittsburgh undergraduates.  Prior to our study, the 
453 student turns in these 10 dialogues were 
manually labeled by two annotators as either 
"Emotional" or "Non-Emotional" (Litman and 
Forbes-Riley, 2004).  Perceived student emotions 
(e.g. confidence, confusion, boredom, irritation, 
etc.) were coded based on both what the student 
said and how he or she said it. For this study, we 
use only the 350 turns where both annotators 
agreed on the emotion label. 51.71% of these turns 
were labeled as Non-Emotional and the rest as 
Emotional. 
Also prior to our study, each annotated turn was 
represented as a vector of 449 features 
hypothesized to be relevant for emotion prediction 
(Forbes-Riley and Litman, 2004).  The features 
represent acoustic-prosodic (pitch, amplitude, 
temporal), lexical, and other linguistic 
characteristics of both the turn and its local and 
global dialogue context.   
3 Machine Learning Techniques 
In this section, we will briefly describe the ma-
chine learning techniques used by our system. 
3.1 Co-training 
To address the challenge of training classifiers 
when only a small set of labeled examples is 
available, Blum and Mitchell (1998) proposed Co-
training as a way to bootstrap classifiers from a 
large set of unlabeled data.  Under this framework, 
two (or more) learners are trained iteratively in 
tandem.  In each iteration, the learners classify 
more unlabeled data to increase the training data 
for each other.  In theory, the learners must have 
distinct views of the data (i.e., their features are 
conditionally independent given the label 
example), but some studies suggest that Co-
training can still be helpful even when the 
independence assumption does not hold (Goldman, 
2000). 
To apply Co-training to our task, we develop 
two high-precision learners: Emotional and Non-
Emotional.  The learners use different features 
because each is maximizing the precision of its 
label (possibly with low recall).  While we have 
not proved these two learners are conditionally 
independent, this division of expertise ensures that 
the learners are different.  The algorithm for our 
Co-training system is shown in Figure 1. Each 
learner selects the examples whose predicted 
labeled corresponds to its expertise class with the 
highest confidence.  The maximum number of 
iterations and the number of examples added per 
iteration are parameters of the system. 
While iteration < MAXITERATION 
   Emo_Learner.Train(train) 
   NE_Learner.Train(train) 
 
   emo_Predictions = Emo_Learner.Predict(predict) 
   ne_Predictions = NE_Learner.Predict(predict) 
 
   emo_sorted_Predictions = Sort_by_confidence( 
                             emo_Predictions) 
   ne_sorted_Predictions = Sort_by_confidence( 
                             ne_Predictions) 
 
   best_emo = Emo_Learner.select_best( 
                             emo_sorted_Predictions, 
                             NUM_SAMPLES_TO_ADD) 
   best_ne = NE_Learner.select_best( 
                             ne_sorted_Predictions,  
                             NUM_SAMPLES_TO_ADD) 
    
   train = train ? best_emo ? best_ne 
   predict = predict ? best_emo ? best_ne 
end 
 
Figure 1. Algorithm for Co-training System 
3.2 Wrapper Approach with Forward 
Selection 
As described in Section 2, 449 features have 
been currently extracted from each utterance of the 
ITSPOKE corpus (where an utterance is a 
student?s turn in a dialogue).  Unfortunately, high 
dimensionality, i.e. large amount of input features, 
may lead to a large variance of estimates, noise, 
overfitting, and in general, higher complexity and 
inefficiencies in the learners.  Different approaches 
have been proposed to address this problem.  In 
this work, we have used the Wrapper Approach 
with Forward Selection. 
The Wrapper Approach, introduced by John et 
al. (1994) and refined later by Kohavi and John 
(1997), is a method that searches for a good subset 
of relevant features using an induction algorithm as 
part of the evaluation function.  We can apply 
different search algorithms to find this set of 
features. 
Forward Selection is a greedy search algorithm 
that begins with an empty set of features, and 
greedily adds features to the set.  Figure 2 shows 
our algorithm implemented for the forward 
wrapper approach. 
bestFeatures = [] 
while dim(bestFeatures) < MINFEATURES 
  for iterations = 1: MAXITERATIONS 
   split train into training/development 
   parameters = computeParameters(training) 
   for feature = 1:MAXFEATURES 
 
  evaluate(parameters,development, 
                      [bestFeatures + feature]) 
 
  keep validation performance 
   end 
 
 end 
 average_performance and keep average_performance 
   end 
   B = best average_performance  
   bestFeatures  B ? bestFeatures 
end 
 
Figure 2. Implemented algorithm for forward 
wrapper approach.  The variables underlined are 
the ones whose parameters we have changed in 
order to test and improve the performance. 
We can use different criteria to select the feature 
to add, depending on the object of optimization. 
Earlier, we have explained the basis of the Co-
training system.  When developing an expert 
learner in one class, we want it to be correct most 
of the time when it guesses that class.  That is, we 
want the classifier to have high precision (possibly 
at the cost of lower overall accuracy).  Therefore, 
we are interested in finding the best set of features 
for precision in each class.  In this case, we are 
focusing on Emotional and Non-Emotional 
classifiers. 
Figure 3 shows the formulas used for the 
optimization criterion on each class.  For the 
Emotional Class, our optimization criterion was to 
maximize the PPV (Positive Predictive Value), and 
for the Non-Emotional Class our optimization 
criterion was to maximize the NPV (Negative 
Predictive Value). 
 
Figure 3. Confusion Matrix, Positive Predictive 
Value (Precision for Emotional) and Negative 
Predictive Value (Precision for Non-Emotional)  
4 Experiments 
For the following experiments, we fixed the size 
of our training set to 175 examples (50%), and the 
size of our test set to 140 examples (40%).  The 
remaining 10% has been saved for later 
experiments. 
4.1 Selecting the features 
The first task was to reduce the dimensionality 
and find the best set of features for maximizing the 
PPV for Emotional class and NPV for Non-
Emotional class.  We applied the Wrapper 
Approach with Forward Selection as described in 
section 3.2, using Na?ve Bayes to evaluate each 
subset of features. 
We have used 175 examples for the training set 
(used to select the best features) and 140 for the 
test set (used to measure the performance).  The 
training set is randomly divided into two sets in 
each iteration of the algorithm: One for training 
and the other for development (65% and 35% 
respectively).  We train the learners with the 
training set and we evaluate the performance to 
pick the best feature with the development set.   
Number of 
Features 
Na?ve 
Bayes 
AdaBoost-j48 
Decision Trees 
All Features 74.5 % 83.1 % 
3 best for PPV 92.9 % 92.9 % 
Table 1. Precision of Emotional with all features 
and 3 best features for PPV using Na?ve Bayes  
(used for Feature Selection) and AdaBoost-j48 
Decision Trees (used for Co-training) 
The selected features that gave the best PPV for 
Emotional Class are 2 lexical features and one 
acoustic-prosodic feature.  By using them we 
increased the precision of Na?ve Bayes from 74.5% 
(using all 449 features) to 92.9%, and of 
AdaBoost-j48 Decision Trees from 83.1% to 
92.9% (see Table 1). 
Number of 
Features 
Na?ve 
Bayes 
AdaBoost-j48 
Decision Trees 
All Features 74.2  % 90.7 % 
1 best for NPV 100.0  % 100.0 % 
Table 2. Precision of Non-Emotional with all 
features and best feature for NPV using Na?ve 
Bayes  (used for Feature Selection) and AdaBoost-
j48 Decision Trees (used for Co-training) 
For the Non-Emotional Class, we increased the 
NPV of Na?ve Bayes from 74.2% (with all 
features) to 100% just by using one lexical feature, 
and the NPV of AdaBoost-j48 Decision Trees from 
90.7% to 100%.  This precision remained the same 
with the set of 3 best features, one lexical and two 
non-acoustic prosodic features (see Table 2). 
These two set of features for each learner are 
disjoint. 
4.2 Co-training experiments 
The two learners are initialized with only 6 
labeled examples in the training set.  The Co-
training system added examples from the 140 
?pseudo-labeled? examples1 in the Prediction Set. 
The size of the training set increased in each 
iteration by adding the 2 best examples (those with 
the highest confidence scores) labeled by the two 
learners. The Emotional learner and the Non-
Emotional learner were set to work with the set of 
features selected by the wrapper approach to 
optimize the precision (PPV and NPV) as 
described in section 4.1. 
We have applied Weka?s (Witten and Frank, 
2000) AdaBoost?s version of j48 decision trees (as 
used in Forbes-Riley and Litman, 2004) to the 140 
unseen examples of the test set for generating the 
learning curve shown in figure 4.   
Figure 4 illustrates the learning curve of the 
accuracy on the test set, taking the union of the set 
of features selected to label the examples.  We 
used the 3 best features for PPV for the Emotional 
Learner and the best feature for NPV for the Non-
Emotional Learner (see Section 4.1).  The x-axis 
shows the number of training examples added; the 
y-axis shows the accuracy of the classifier on test 
instances.  We compare the learning curve from 
Co-training with a baseline of majority class and 
an upper-bound, in which the classifiers are trained 
on human-annotated data.  Post-hoc analyses 
reveal that four incorrectly labeled examples were 
added to the training set: example numbers 21, 22, 
45, and 51 (see the x-axis).  Shortly after the 
inclusion of example 21, the Co-training learning 
curve diverges from the upper-bound.  All of them 
correspond to Non-Emotional examples that were 
labeled as Emotional by the Emotional learner with 
the highest confidence. 
The Co-training system stopped after adding 58 
examples to the initial 6 in the training set because 
the remaining data cannot be labeled by the 
learners with high precision.  However, as we can 
see, the training set generated by the Co-training 
technique can perform almost as well as the upper-
bound, even if incorrectly labeled examples are 
included in the training set. 
                                                     
1
 This means that although the example has been 
labeled, the label remains unseen to the learners. 
Learning Curve - Accuracy (features for Emotional/Non-Emotional Precision)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97 103 109 115 121 127 133 139 145 151 157 163 169 175
Majority Class Cotrain Upper-bound
 
Figure 4. Learning Curve of Accuracy using best features for Precision of Emotional/Non-Emotional 
5 Conclusion 
We have shown Co-training to be a promising 
approach for predicting emotions with spoken 
dialogue data. We have given an algorithm that 
increased the size of the training set producing 
even better accuracy than the manually labeled 
training set, until it fell behind due to its inability 
to add more than 58 examples. 
We have shown the positive effect of selecting 
a good set of features optimizing precision for 
each learner and we have shown that the features 
can be identified with the Wrapper Approach.     
In the future, we will verify the generalization 
of our results to other partitions of our data.  We 
will also try to address the limitation of noise in 
our Co-training System, and generalize our 
solution to a corresponding corpus of human-
computer data (Litman and Forbes-Riley, 2004).  
We will also conduct experiments comparing Co-
training with other semi-supervised approaches 
such as self-training and Active learning.  
6 Acknowledgements 
Thanks to R. Pelikan, T. Singliar and M. 
Hauskrecht for their contribution with Feature 
Selection, and to the NLP group at University of 
Pittsburgh for their helpful comments. This 
research is partially supported by NSF Grant No. 
0328431. 
References  
A. Blum and T. Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-training.  
Proceedings of the 11th Annual Conference on 
Computational Learning Theory: 92-100. 
K. Forbes-Riley and D. Litman. 2004.  Predicting 
Emotion in Spoken Dialogue from Multiple 
Knowledge Sources. Proceedings of Human 
Language Technology Conference of the North 
American Chapter of the Association for 
Computational Linguistics (HLT/NAACL).  
S. Goldman and Y. Zhou. 2000.  Enhancing 
Supervised Learning with Unlabeled Data. 
International Joint Conference on Machine 
Learning, 2000. 
G. H. John, R. Kohavi and K. Pleger. 1994.  
Irrelevant Features and the Subset Selection 
Problem. Machine Learning: Proceedings of 
11th International Conference:121-129, Morgan 
Kaufmann Publishers, San Francisco, CA. 
R. Kohavi and G. H. John. 1997. Wrappers for 
Feature Subset Selection. Artificial 
Intelligence, Volume 97, Issue 1-2. 
D. J. Litman and K. Forbes-Riley, 2004. 
Annotating Student Emotional States in Spoken 
Tutoring Dialogues.  Proc. 5th Special Interest 
Group on Discourse and Dialogue Workshop 
on Discourse and Dialogue (SIGdial). 
D. J. Litman and S. Silliman, 2004. ITSPOKE: An 
Intelligent Tutoring Spoken Dialogue System. 
Companion Proceedings of Human Language 
Technology conf. of the North American 
Chapter of the Association for Computational 
Linguistics (HLT/NAACL). 
I. H. Witten and E. Frank. 2000.  Data Mining: 
Practical Machine Learning Tools and 
Techniques with Java implementations. Morgan 
Kaufmann, San Francisco. 
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 57?60, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Syntax-based Semi-Supervised Named Entity Tagging 
 
 
Behrang Mohit Rebecca Hwa 
Intelligent Systems Program Computer Science Department 
University of Pittsburgh University of Pittsburgh 
Pittsburgh, PA 15260 USA Pittsburgh, PA 15260, USA 
behrang@cs.pitt.edu hwa@cs.pitt.edu 
 
 
 
 
Abstract 
We report an empirical study on the role 
of syntactic features in building a semi-
supervised named entity (NE) tagger.  
Our study addresses two questions: What 
types of syntactic features are suitable for 
extracting potential NEs to train a classi-
fier in a semi-supervised setting? How 
good is the resulting NE classifier on test-
ing instances dissimilar from its training 
data? Our study shows that constituency 
and dependency parsing constraints are 
both suitable features to extract NEs and 
train the classifier.  Moreover, the classi-
fier showed significant accuracy im-
provement when constituency features are 
combined with new dependency feature.  
Furthermore, the degradation in accuracy 
on unfamiliar test cases is low, suggesting 
that the trained classifier generalizes well. 
1 Introduction 
Named entity (NE) tagging is the task of recogniz-
ing and classifying phrases into one of many se-
mantic classes such as persons, organizations and 
locations. Many successful NE tagging systems 
rely on a supervised learning framework where 
systems use large annotated training resources 
(Bikel et. al. 1999). These resources may not al-
ways be available for non-English domains.  This 
paper examines the practicality of developing a 
syntax-based semi-supervised NE tagger.  In our 
study we compared the effects of two types of syn-
tactic rules (constituency and dependency) in ex-
tracting and classifying potential named entities.  
We train a Naive Bayes classification model on a 
combination of labeled and unlabeled examples 
with the Expectation Maximization (EM) algo-
rithm.  We find that a significant improvement in 
classification accuracy can be achieved when we 
combine both dependency and constituency extrac-
tion methods.  In our experiments, we evaluate the 
generalization (coverage) of this bootstrapping ap-
proach under three testing schemas.  Each of these 
schemas represented a certain level of test data 
coverage (recall).  Although the system performs 
best on (unseen) test data that is extracted by the 
syntactic rules (i.e., similar syntactic structures as 
the training examples), the performance degrada-
tion is not high when the system is tested on more 
general test cases. Our experimental results suggest 
that a semi-supervised NE tagger can be success-
fully developed using syntax-rich features.  
2 Previous Works and Our Approach 
Supervised NE Tagging has been studied exten-
sively over the past decade (Bikel et al 1999, 
Baluja et. al. 1999, Tjong Kim Sang and De 
Meulder 2003).  Recently, there were increasing 
interests in semi-supervised learning approaches. 
Most relevant to our study, Collins and Singer 
(1999) showed that a NE Classifier can be devel-
oped by bootstrapping from a small amount of la-
beled examples.  To extract potentially useful 
training examples, they first parsed the sentences 
and looked for expressions that satisfy two con-
stituency patterns (appositives and prepositional 
phrases).  A small subset of these expressions was 
then manually labeled with their correct NE tags.  
The training examples were a combination of the 
labeled and unlabeled data.  In their studies, 
57
Collins and Singer compared several learning 
models using this style of semi-supervised training.  
Their results were encouraging, and their studies 
raised additional questions.  First, are there other 
appropriate syntactic extraction patterns in addition 
to appositives and prepositional phrases?  Second, 
because the test data were extracted in the same 
manner as the training data in their experiments, 
the characteristics of the test cases were biased.  In 
this paper we examine the question of how well a 
semi-supervised system can classify arbitrary 
named entities.  In our empirical study, in addition 
to the constituency features proposed by Collins 
and Singer, we introduce a new set of dependency 
parse features to recognize and classify NEs.  We 
evaluated the effects of these two sets of syntactic 
features on the accuracy of the classification both 
separately and in a combined form (union of the 
two sets). 
Figure 1 represents a general overview of our sys-
tem?s architecture which includes the following 
two levels: NE Recognizer and NE Classifier. 
Section 3 and 4 describes these two levels in de-
tails and section 5 covers the results of the evalua-
tion of our system. 
 
Figure 1: System's architecture 
3 Named Entity Recognition  
In this level, the system used a group of syntax-
based rules to recognize and extract potential 
named entities from constituency and dependency 
parse trees.  The rules are used to produce our 
training data; therefore they needed to have a nar-
row and precise coverage of each type of named 
entities to minimize the level of training noise. 
The processing starts from construction of con-
stituency and dependency parse trees from the in-
put text. Potential NEs are detected and extracted 
based on these syntactic rules. 
3.1 Constituency Parse Features 
Replicating the study performed by Collins-Singer 
(1999), we used two constituency parse rules to 
extract a set of proper nouns (along with their as-
sociated contextual information). These two con-
stituency rules extracted proper nouns within a 
noun phrase that contained an appositive phrase 
and a proper noun within a prepositional phrase. 
3.2 Dependency Parse Features 
We observed that a proper noun acting as the sub-
ject or the object of a sentence has a high probabil-
ity of being a particular type of named entity. 
Thus, we expanded our syntactic analysis of the 
data into dependency parse of the text and ex-
tracted a set of proper nouns that act as the subjects 
or objects of the main verb.  For each of the sub-
jects and objects, we considered the maximum 
span noun phrase that included the modifiers of the 
subjects and objects in the dependency parse tree. 
4 Named Entity Classification 
In this level, the system assigns one of the 4 class 
labels (<PER>, <ORG>, <LOC>, <NONE>) to a 
given test NE.  The NONE class is used for the 
expressions mistakenly extracted by syntactic fea-
tures that were not a NE.  We will discuss the form 
of the test NE in more details in section 5.  The 
underlying model we consider is a Na?ve Bayes 
classifier; we train it with the Expectation-
Maximization algorithm, an iterative parameter 
estimation procedure. 
4.1 Features 
We used the following syntactic and spelling fea-
tures for the classification: 
Full NE Phrase.  
Individual word: This binary feature indicates the 
presence of a certain word in the NE. 
58
Punctuation pattern: The feature helps to distin-
guish those NEs that hold certain patterns of punc-
tuations like (?) for U.S.A. or (&.) for A&M.  
All Capitalization:  This binary feature is mainly 
useful for some of the NEs that have all capital 
letters.  such as AP, AFP, CNN, etc. 
Constituency Parse Rule:  The feature indicates 
which of the two constituency rule is used for ex-
tract the NE. 
Dependency Parse Rule:  The feature indicates if 
the NE is the subject or object of the sentence. 
Except for the last two features, all features are 
spelling features which are extracted from the ac-
tual NE phrase.  The constituency and dependency 
features are extracted from the NE recognition 
phase (section 3).  Depending on the type of testing 
and training schema, the NEs might have 0 value 
for the dependency or constituency features which 
indicate the absence of the feature in the recogni-
tion step.  
4.2 Na?ve Bayes Classifier 
We used a Na?ve Bayes classifier where each NE 
is represented by a set of syntactic and word-level 
features (with various distributions) as described 
above.  The individual words within the noun 
phrase are binary features.  These, along with other 
features with multinomial distributions, fit well 
into Na?ve Bayes assumption where each feature is 
dealt independently (given the class value).  In or-
der to balance the effects of the large binary fea-
tures on the final class probabilities, we used some 
numerical methods techniques to transform some 
of the probabilities to the log-space. 
4.3 Semi-supervised learning 
Similar to the work of Nigam et al (1999) on 
document classification, we used Expectation 
Maximization (EM) algorithm along with our Na-
?ve Bayes classifier to form a semi supervised 
learning framework.  In this framework, the small 
labeled dataset is used to do the initial assignments 
of the parameters for the Na?ve Bayes classifier.  
After this initialization step, in each iteration the 
Na?ve Bayes classifier classifies all of the unla-
beled examples and updates its parameters based 
on the class probability of the unlabeled and la-
beled NE instances.  This iterative procedure con-
tinues until the parameters reach a stable point.  
Subsequently the updated Na?ve Bayes classifies 
the test instances for evaluation.   
5 Empirical Study 
Our study consists of a 9-way comparison that in-
cludes the usage of three types of training features 
and three types of testing schema. 
5.1 Data  
We used the data from the Automatic Content Ex-
traction (ACE)?s entity detection track as our la-
beled (gold standard) data.1 
For every NE that the syntactic rules extract from 
the input sentence, we had to find a matching NE 
from the gold standard data and label the extracted 
NE with the correct NE class label.  If the ex-
tracted NE did not match any of the gold standard 
NEs (for the sentence), we labeled it with the 
<NONE> class label. 
We also used the WSJ portion of the Penn Tree 
Bank as our unlabeled dataset and ran constituency 
and dependency analyses2 to extract a set of unla-
beled named entities for the semi-supervised clas-
sification. 
5.2 Evaluation 
In order to evaluate the effects of each group of 
syntactic features, we experimented with three dif-
ferent training strategies (using constituency rules, 
dependency rules or combinations of both). We 
conducted the comparison study with three types 
of test data that represent three levels of coverage 
(recall) for the system: 
1. Gold Standard NEs:  This test set contains in-
stances taken directly from the ACE data, and are 
therefore independent of the syntactic rules. 
2. Any single or series of proper nouns in the text:  
This is a heuristic for locating potential NEs so as 
to have the broadest coverage. 
3. NEs extracted from text by the syntactic rules.  
This evaluation approach is similar to that of Col-
lins and Singer.  The main difference is that we 
have to match the extracted expressions to a pre-
                                                           
1 We only used the NE portion of the data and removed the 
information for other tracking and extraction tasks. 
2 We used the Collins parser (1997) to generate the constitu-
ency parse and a dependency converter (Hwa and Lopez, 
2004) to obtain the dependency parse of English sentences. 
59
labeled gold standard from ACE rather than per-
forming manual annotations ourselves.   
All tests have been performed under a 5-fold cross 
validation training-testing setup.  Table 1 presents 
the accuracy of the NE classification and the size 
of labeled data in the different training-testing con-
figurations.  The second line of each cell shows the 
size of labeled training data and the third line 
shows the size of testing data.  Each column pre-
sents the result for one type of the syntactic fea-
tures that were used to extract NEs.  Each row of 
the table presents one of the three testing schema.  
We tested the statistical significance of each of the 
cross-row accuracy improvements against an alpha 
value of 0.1 and observed significant improvement 
in all of the testing schemas.   
 
Training Features Testing Data Const. Dep. Union 
Gold Standard NEs 
(ACE Data) 
76.7% 
668 
579 
78.5% 
884 
579 
82.4% 
1427 
579 
All Proper Nouns 
70.2% 
668 
872 
71.4% 
884 
872 
76.1% 
1427 
872 
NEs Extracted by 
Training Rules 
78.2% 
668 
169 
80.3% 
884 
217 
85.1% 
1427 
354 
Table 1: Classification Accuracy, labeled training & 
testing data size  
 
Our results suggest that dependency parsing fea-
tures are reasonable extraction patterns, as their 
accuracy rates are competitive against the model 
based solely on constituency rules.  Moreover, they 
make a good complement to the constituency rules 
proposed by Collins and Singer, since the accuracy 
rates of the union is higher than either model alone. 
As expected, all methods perform the best when 
the test data are extracted in the same manner as 
the training examples.  However, if the systems 
were given a well-formed named entity, the per-
formance degradation is reasonably small, about 
2% absolute difference for all training methods.  
The performance is somewhat lower when classi-
fying very general test cases of all proper nouns. 
6 Conclusion and Future Work 
In this paper, we experimented with different syn-
tactic extraction patterns and different NE recogni-
tion constraints.  We find that semi-supervised 
methods are compatible with both constituency and 
dependency extraction rules.  We also find that the 
resulting classifier is reasonably robust on test 
cases that are different from its training examples. 
An area that might benefit from a semi-supervised 
NE tagger is machine translation. The semi-
supervised approach is suitable for non-English 
languages that do not have very much annotated 
NE data.  We are currently applying our system to 
Arabic.  The robustness of the syntactic-based ap-
proach has allowed us to port the system to the 
new language with minor changes in our syntactic 
rules and classification features. 
Acknowledgement  
We would like to thank the NLP group at Pitt and 
the anonymous reviewers for their valuable com-
ments and suggestions. 
References 
Shumeet Baluja, Vibhu Mittal and Rahul Sukthankar, 
1999. Applying machine learning for high perform-
ance named-entity extraction. In Proceedings of Pa-
cific Association for Computational Linguistics. 
Daniel Bikel, Robert Schwartz & Ralph Weischedel, 
1999. An algorithm that learns what?s in a name. 
Machine Learning 34. 
Michael Collins, 1997.  Three generative lexicalized 
models for statistical parsing. In Proceedings of the 
35th Annual Meeting of the ACL. 
Michael Collins, and Yoram Singer, 1999. Unsuper-
vised Classification of Named Entities. In Proceed-
ings of SIGDAT. 
A. P. Dempster, N. M. Laird and D. B. Rubin, 1977. 
Maximum Likelihood from incomplete data via the 
EM algorithm. Journal of Royal Statistical Society, 
Series B, 39(1), 1-38. 
Rebecca Hwa and Adam Lopez, 2004.  On the Conver-
sion of Constituent Parsers to Dependency Parsers.  
Technical Report TR-04-118, Department of Com-
puter Science, University of Pittsburgh. 
Kamal Nigam, Andrew McCallum, Sebastian Thrun and 
Tom Mitchell, 2000. Text Classification from La-
beled and Unlabeled Documents using EM.  Machine 
Learning 39(2/3). 
Erik F. Tjong Kim Sang and Fien De Meulder, 2003. 
Introduction to the CoNLL-2003 Shared Task: Lan-
guage-Independent Named Entity Recognition. In 
Proceedings of CoNLL-2003. 
60
Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 69?72, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Word Alignment and Cross-Lingual Resource Acquisition ?
Carol Nichols and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260
{cln23,hwa}@cs.pitt.edu
Abstract
Annotated corpora are valuable resources
for developing Natural Language Process-
ing applications. This work focuses on
acquiring annotated data for multilingual
processing applications. We present an
annotation environment that supports a
web-based user-interface for acquiring word
alignments between English and Chinese as
well as a visualization tool for researchers
to explore the annotated data.
1 Introduction
The performance of many Natural Language Pro-
cessing (NLP) applications can be improved through
supervised machine learning techniques that train
systems with annotated training examples. For ex-
ample, a part-of-speech (POS) tagger might be in-
duced from words that have been annotated with
the correct POS tags. A limitation to the super-
vised approach is that the annotation is typically
performed manually. This poses as a challenge in
three ways. First, researchers must develop a com-
prehensive annotation guideline for the annotators
to follow. Guideline development is difficult because
researchers must be specific enough so that different
annotators? work will be comparable, but also gen-
eral enough to allow the annotators to make their
own linguistic judgments. Reported experiences of
previous annotation projects suggest that guideline
development is both an art and a science and is itself
?This work has been supported, in part, by CRA-
W Distributed Mentor Program. We thank Karina Iva-
netich, David Chiang, and the NLP group at Pitt for
helpful feedbacks on the user interfaces; Wanwan Zhang
and Ying-Ju Suen for testing the system; and the anony-
mous reviewers for their comments on the paper.
a time-consuming process (Litman and Pan, 2002;
Marcus et al, 1993; Xia et al, 2000; Wiebe, 2002).
Second, it is common for the annotators to make
mistakes, so some form of consistency check is nec-
essary. Third, the entire process (guideline develop-
ment, annotation, and error corrections) may have
to be repeated with new domains.
This work focuses on the first two challenges: help-
ing researchers to design better guidelines and to col-
lect a large set of consistently labeled data from hu-
man annotators. Our annotation environment con-
sists of two pieces of software: a user interface for
the annotators and a visualization tool for the re-
searchers to examine the data. The data-collection
interface asks the users to make lexical and phrasal
mappings (word alignments) between the two lan-
guages. Some studies suggest that supervised word
aligned data may improve machine translation per-
formance (Callison-Burch et al, 2004). The inter-
face can also be configured to ask the annotators
to correct projected annotated resources. The idea
of projecting English annotation resources across
word alignments has been explored in several studies
(Yarowsky and Ngai, 2001; Hwa et al, 2005; Smith
and Smith, 2004). Currently, our annotation inter-
face is configured for correcting projected POS tag-
ging for Chinese. The visualization tool aggregates
the annotators? work, takes various statistics, and vi-
sually displays the aggregate information. Our goal
is to aid the researchers conducting the experiment
to identify noise in the annotations as well as prob-
lematic constructs for which the guidelines should
provide further clarifications.
Our longer-term plan is to use this framework to
support active learning (Cohn et al, 1996), a ma-
chine learning approach that aims to reduce the num-
ber of training examples needed by the system when
it is provided with more informative training exam-
69
ples. We believe that through a combination of an in-
tuitive annotation interface, a visualization tool that
checks for style and quality consistency, and appro-
priate active learning techniques, we can make su-
pervised training more effective for developing mul-
tilingual applications.
2 Annotation Interface
One way to acquire annotations quickly is to appeal
to users across the Internet. First, we are more likely
to find annotators with the necessary qualifications.
Second, many more users can work simultaneously
than would be feasible to physically host in a lab.
Third, having many users annotate the same data
allows us to easily identify systematic problems as
well as spurious mistakes. The OpenMind Initiative
(Stork, 2001) has had success collecting information
that could not be obtained from data mining tools
or with a local small group of annotators.
Collecting data from users over the Internet in-
troduces complications. Since we cannot ascertain
the computer skills of the annotators, the interface
must be easy to use. Our interface is a JAVA ap-
plet on a webpage so that it is platform indepen-
dent. An online tutorial is also provided (and re-
quired for first-time users). Another problem of so-
liciting unknown users for data is the possibility of
receiving garbage data created by users who do not
have sufficient knowledge or are maliciously entering
random input. Our system minimizes this risk in
several ways. First, new users are required to work
through the tutorial, which also serves as a short
guide to reduce stylistic differences between the an-
notators. Second, we require the same data to be
labeled by multiple people to ensure reliability, and
researchers can use the visualization tool (see Section
3) to compare the agreement rates between annota-
tors. Finally, our program is designed with a filter for
malicious users. After completing the tutorial, the
user is given a randomly selected sample sentence
(for which we already have verified alignments) to
annotate. The user must obtain an F-measure agree-
ment of 60% with the ?correct? alignments in order
to be allowed to annotate sentences.1
Because word alignment annotation is a useful re-
source for both training and testing, quite a few in-
terfaces have already been developed. The earliest
1The correct alignments were performed by two
trained annotators who had an average agreement rate
of about 85%. We chose 60% to be the figure of merit
because this level is nearly impossible to obtain through
random guessing but is lenient enough to allow for the in-
experience of first time users. Automatic computer align-
ments average around 50%.
is the Blinker Project (Melamed, 1998); more re-
cent systems have been released to support more lan-
guages and visualization features (Ahrenberg et al,
2003; Lambert and Castell, 2004). 2 Our interface
does share some similarities with these systems, but
it is designed with additional features to support our
experimental goals of guideline development, active
learning and resource projection. Following the ex-
perimental design proposed by Och and Ney (2000),
we instruct the annotators to indicate their level of
confidence by choosing sure or unsure for each align-
ment they made. This allows researchers to identify
areas where the translation may be unclear or diffi-
cult. We provide a text area for comments on each
sentence so that the annotator may explain any as-
sumptions or problems. A hidden timer records how
long each user spends on each sentence in order to
gauge the difficulty of the sentence; this information
will be a useful measurement of the effectiveness of
different active learning algorithms. Finally, our in-
terface supports cross projection annotation. As an
initial study, we have focused on POS tagging, but
the framework can be extended for other types of
resources such as syntactic and semantic trees and
can be configured for languages other than English
and Chinese. When words are aligned, the known
and displayed English POS tag of the last English
word involved in the alignment group is automati-
cally projected onto all Chinese words involved, but
a drop-down menu allows the user to correct this if
the projection is erroneous. A screenshot of the in-
terface is provided in Figure 1a.
3 Tools for Researchers
Good training examples for NLP learning systems
should have a high level of consistency and accuracy.
We have developed a set of tools for researchers to
visualize, compare, and analyze the work of the an-
notators. The main interface is a JAVA applet that
provides a visual representation of all the alignments
superimposed onto each other in a grid.
For the purposes of error detection, our system
provides statistics for researchers to determine the
agreement rates between the annotators. The metric
we use is Cohen?s K (1960), which is computed for ev-
ery sentence across all users? alignments. Cohen?s K
is a measure of agreement that takes the total prob-
ability of agreement, subtracts the probability the
agreement is due to chance, and divides by the max-
imum agreement possible. We use a variant of the
2Rada Mihalcea maintains an alignment resource
repository (http://www.cs.unt.edu/~rada/wa) that
contains other downloadable interface packages that do
not have companion papers.
70
 
 
(a) (b)
Figure 1: (a) A screenshot of the word alignment user-interface. (b) A screenshot of the visualization tool
for analyzing multiple annotators? alignments.
equation that allows for having three or more judges
(Davies and Fleiss, 1982). The measurement ranges
from 0 (chance agreement) to 1 (perfect agreement).
For any selected sentence, we also compute for each
annotator an average pair-wise Cohen?s K against all
other users who aligned this sentence.3 This statistic
may be useful in several ways. First, someone with a
consistently low score may not have enough knowl-
edge to perform the task (or is malicious). Second,
if an annotator received an unusually low score for
a particular sentence, it might indicate that the per-
son made mistakes in that sentence. Third, if there is
too much disagreement among all users, the sentence
might be a poor example to be included.
In addition to catching individual annotation er-
rors, it is also important to minimize stylistic incon-
sistencies. These are differences in the ways different
annotators (consistently) handle the same phenom-
ena. A common scenario is that some function words
in one language do not have an equivalent counter-
part in the other language. Without a precise guide-
line ruling, some annotators always leave the func-
tion words unaligned while others always group the
function words together with nearby content words.
Our tool can be useful in developing and improving
style guides. It highlights the potential areas that
need further clarifications in the guidelines with an
at-a-glance visual summary of where and how the an-
notators differed in their work. Each cell in the grid
represents an alignment between one particular word
in the English sentence and one particular word in
the Chinese sentence. A white cell means no one pro-
posed an alignment between the words. Each colored
cell has two components: an upper green portion in-
3not shown in the screenshot here.
dicating a sure alignment and a lower yellow portion
indicating an unsure alignment. The proportion of
these components indicates the ratio of the number
of people who marked this alignment as sure to those
who were unsure (thus, an all-green cell means that
everyone who aligned these words together is sure).
Moreover, we use different saturation in the cells to
indicate the percentage of people who aligned the
two words together. A cell with faint colors means
that most people did not chose to align these words
together. Furthermore, researchers can elect to view
the annotation decisions of a particular user by click-
ing on the radio buttons below. Only the selected
user?s annotation decisions would be highlighted by
red outlines (i.e., only around the green portions of
those cells that the person chose sure and around the
yellow portions of this person?s unsure alignments).
Figure 1b displays the result of three annotators?
alignments of a sample sentence pair. This sentence
seems reasonably easy to annotate. Most of the col-
ored cells have a high saturation, showing that the
annotators agree on the words to be aligned. Most
of the cells are only green, showing that the anno-
tators are sure of their decisions. Three out of the
four unsure alignments coincide with the other an-
notators? sure alignments, and even in those cases,
more annotators are sure than unsure (the green ar-
eas are 2/3 of the cells while the yellow areas are
1/3). The colored cells with low saturation indicate
potential outliers. Comparing individual annotator?s
alignments against the composite, we find that one
annotator, rh, may be a potential outlier annota-
tor since this person generated the most number of
lightly saturated cells. The person does not appear
to be malicious since the three people?s overall agree-
ments are high. To determine whether the conflict
71
arises from stylistic differences or from careless mis-
takes, researchers can click on the disputed cell (a
cross will appear) to see the corresponding English
and Chinese words in the text boxes in the top and
left margin.
Different patterns in the visualization will indicate
different problems. If the visualization patterns re-
veal a great deal of disagreement and unsure align-
ments overall, we might conclude that the sentence
pair is a bad translation; if the disagreement is local-
ized, this may indicate the presence of an idiom or
a structure that does not translate word-for-word.
Repeated occurrences of a pattern may suggest a
stylistic inconsistency that should be addressed in
the guidelines. Ultimately, each area of wide dis-
agreement will require further analysis in order to
determine which of these problems is occurring.
4 Conclusion and Future Work
In summary, we have presented an annotation envi-
ronment for acquiring word alignments between En-
glish and Chinese as well as Part-Of-Speech tags for
Chinese. The system is in place and the annotation
process is underway.4
Once we have collected a medium-sized corpus, we
will begin exploring different active learning tech-
niques. Our goal is to find the best way to assign
utility scores to the as-of-yet unlabeled sentences in
order to obtain the greatest improvement in word
alignment accuracy. Potential information useful for
this task includes various measurements of the com-
plexity of the sentence such as the rate of (auto-
matic) alignments that are not one-to-one, the num-
ber of low-frequency words, and the number of po-
tential language divergences (for example, many En-
glish verbs are nominalized in Chinese), and the co-
occurrence of word pairs deemed to be unsure by the
annotators in other contexts. Furthermore, we be-
lieve that the aggregate visualization tool will also
help us uncover additional characteristics of poten-
tially informative training examples.
References
Lars Ahrenberg, Magnus Merkel, and Michael Petterst-
edt. 2003. Interactive word alignment for language en-
gineering. In Proceedings from EACL 2003, Budapest.
Christopher Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with
4The annotation interface is
open to public. Please visit
http://flan.cs.pitt.edu/~hwa/align/align.html
word- and sentence-aligned parallel corpora. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics, July.
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Meas., 20:37?46.
David A. Cohn, Zoubin Ghahramani, and Michael I. Jor-
dan. 1996. Active learning with statistical models.
Journal of Artificial Intelligence Research, 4:129?145.
M. Davies and J. Fleiss. 1982. Measuring agreement for
multinomial data. Biometrics, 38:1047?1051.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Journal of Natural Language Engineering. To appear.
Patrik Lambert and Nuria Castell. 2004. Alignment
of parallel corpora exploiting asymmetrically aligned
phrases. In Proc. of the LREC 2004 Workshop on the
Amazing Utility of Parallel and Comparable Corpora,
May.
Diane Litman and S. Pan. 2002. Desiging and evaluating
an adaptive spoken dialogue system. User Modeling
and User-adapted Interaction, 12(2/3):111?137.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
I. Dan Melamed. 1998. Annotation style guide for the
blinker project. Technical Report IRCS 98-06, Univer-
sity of Pennsylvania.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using english to
parse korean. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
David G Stork. 2001. Toward a computational theory
of data acquisition and truthing. In Proceedings of
Computational Learning Theory (COLT 01).
J. Wiebe. 2002. Instructions for annotating opinions
in newspaper articles. Technical Report TR-02-101,
University of Pittsburgh, Pittsburgh, PA.
Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen
Ocurowski, John Kovarik, Fu-Dong Chiou, Shizhe
Huang, Tony Kroch, and Mitch Marcus. 2000. Devel-
oping guidelines and ensuring consistency for chinese
text annotation. In Proceedings of the Second Lan-
guage Resources and Evaluation Conference, Athens,
Greece, June.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
Second Meeting of the North American Association for
Computational Linguistics, pages 200?207.
72
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 296?303,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Regression for Sentence-Level MT Evaluation with Pseudo References
Joshua S. Albrecht and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{jsa8,hwa}@cs.pitt.edu
Abstract
Many automatic evaluation metrics for ma-
chine translation (MT) rely on making com-
parisons to human translations, a resource
that may not always be available. We present
a method for developing sentence-level MT
evaluation metrics that do not directly rely
on human reference translations. Our met-
rics are developed using regression learn-
ing and are based on a set of weaker indi-
cators of fluency and adequacy (pseudo ref-
erences). Experimental results suggest that
they rival standard reference-based metrics
in terms of correlations with human judg-
ments on new test instances.
1 Introduction
Automatic assessment of translation quality is a
challenging problem because the evaluation task, at
its core, is based on subjective human judgments.
Reference-based metrics such as BLEU (Papineni
et al, 2002) have rephrased this subjective task as
a somewhat more objective question: how closely
does the translation resemble sentences that are
known to be good translations for the same source?
This approach requires the participation of human
translators, who provide the ?gold standard? refer-
ence sentences. However, keeping humans in the
evaluation loop represents a significant expenditure
both in terms of time and resources; therefore it is
worthwhile to explore ways of reducing the degree
of human involvement.
To this end, Gamon et al (2005) proposed a
learning-based evaluation metric that does not com-
pare against reference translations. Under a learn-
ing framework, the input (i.e., the sentence to be
evaluated) is represented as a set of features. These
are measurements that can be extracted from the in-
put sentence (and may be individual metrics them-
selves). The learning algorithm combines the fea-
tures to form a model (a composite evaluation met-
ric) that produces the final score for the input. With-
out human references, the features in the model pro-
posed by Gamon et al were primarily language
model features and linguistic indicators that could be
directly derived from the input sentence alone. Al-
though their initial results were not competitive with
standard reference-based metrics, their studies sug-
gested that a referenceless metric may still provide
useful information about translation fluency. How-
ever, a potential pitfall is that systems might ?game
the metric? by producing fluent outputs that are not
adequate translations of the source.
This paper proposes an alternative approach to
evaluate MT outputs without comparing against hu-
man references. While our metrics are also trained,
our model consists of different features and is
trained under a different learning regime. Crucially,
our model includes features that capture some no-
tions of adequacy by comparing the input against
pseudo references: sentences from other MT sys-
tems (such as commercial off-the-shelf systems or
open sourced research systems). To improve flu-
ency judgments, the model also includes features
that compare the input against target-language ?ref-
erences? such as large text corpora and treebanks.
Unlike human translations used by standard
reference-based metrics, pseudo references are not
296
?gold standards? and can be worse than the sen-
tences being evaluated; therefore, these ?references?
in-and-of themselves are not necessarily informative
enough for MT evaluation. The main insight of our
approach is that through regression, the trained met-
rics can make more nuanced comparisons between
the input and pseudo references. More specifically,
our regression objective is to infer a function that
maps a feature vector (which measures an input?s
similarity to the pseudo references) to a score that
indicates the quality of the input. This is achieved by
optimizing the model?s output to correlate against a
set of training examples, which are translation sen-
tences labeled with quantitative assessments of their
quality by human judges. Although this approach
does incur some human effort, it is primarily for the
development of training data, which, ideally, can be
amortized over a long period of time.
To determine the feasibility of the proposed ap-
proach, we conducted empirical studies that com-
pare our trained metrics against standard reference-
based metrics. We report three main findings.
First, pseudo references are informative compar-
ison points. Experimental results suggest that
a regression-trained metric that compares against
pseudo references can have higher correlations with
human judgments than applying standard metrics
with multiple human references. Second, the learn-
ing model that uses both adequacy and fluency fea-
tures performed the best, with adequacy being the
more important factor. Third, when the pseudo ref-
erences are multiple MT systems, the regression-
trained metric is predictive even when the input is
from a better MT system than those providing the
references. We conjecture that comparing MT out-
puts against other imperfect translations allows for a
more nuanced discrimination of quality.
2 Background and Related Work
For a formally organized event, such as the annual
MT Evaluation sponsored by National Institute of
Standard and Technology (NIST MT Eval), it may
be worthwhile to recruit multiple human translators
to translate a few hundred sentences for evaluation
references. However, there are situations in which
multiple human references are not practically avail-
able (e.g., the source may be of a large quantity, and
no human translation exists). One such instance is
translation quality assurance, in which one wishes
to identify poor outputs in a large body of machine
translated text automatically for human to post-edit.
Another instance is in day-to-day MT research and
development, where new test set with multiple ref-
erences are also hard to come by. One could work
with previous datasets from events such as the NIST
MT Evals, but there is a danger of over-fitting. One
also could extract a single reference from parallel
corpora, although it is known that automatic metrics
are more reliable when comparing against multiple
references.
The aim of this work is to develop a trainable au-
tomatic metric for evaluation without human refer-
ences. This can be seen as a form of confidence esti-
mation on MT outputs (Blatz et al, 2003; Ueffing et
al., 2003; Quirk, 2004). The main distinction is that
confidence estimation is typically performed with a
particular system in mind, and may rely on system-
internal information in estimation. In this study, we
draw on only system-independent indicators so that
the resulting metric may be more generally applied.
This allows us to have a clearer picture of the con-
tributing factors as they interact with different types
of MT systems.
Also relevant is previous work that applied ma-
chine learning approaches to MT evaluation, both
with human references (Corston-Oliver et al, 2001;
Kulesza and Shieber, 2004; Albrecht and Hwa,
2007; Liu and Gildea, 2007) and without (Gamon et
al., 2005). One motivation for the learning approach
is the ease of combining multiple criteria. Literature
in translation evaluation reports a myriad of criteria
that people use in their judgments, but it is not clear
how these factors should be combined mathemati-
cally. Machine learning offers a principled and uni-
fied framework to induce a computational model of
human?s decision process. Disparate indicators can
be encoded as one or more input features, and the
learning algorithm tries to find a mapping from input
features to a score that quantifies the input?s quality
by optimizing the model to match human judgments
on training examples. The framework is attractive
because its objective directly captures the goal of
MT evaluation: how would a user rate the quality
of these translations?
This work differs from previous approaches in
297
two aspects. One is the representation of the model;
our model treats the metric as a distance measure
even though there are no human references. An-
other is the training of the model. More so than
when human references are available, regression is
central to the success of the approach, as it deter-
mines how much we can trust the distance measures
against each pseudo reference system.
While our model does not use human references
directly, its features are adapted from the following
distance-based metrics. The well-known BLEU (Pa-
pineni et al, 2002) is based on the number of com-
mon n-grams between the translation hypothesis and
human reference translations of the same sentence.
Metrics such as ROUGE, Head Word Chain (HWC),
METEOR, and other recently proposed methods all
offer different ways of comparing machine and hu-
man translations. ROUGE utilizes ?skip n-grams?,
which allow for matches of sequences of words that
are not necessarily adjacent (Lin and Och, 2004a).
METEOR uses the Porter stemmer and synonym-
matching via WordNet to calculate recall and pre-
cision more accurately (Banerjee and Lavie, 2005).
The HWC metrics compare dependency and con-
stituency trees for both reference and machine trans-
lations (Liu and Gildea, 2005).
3 MT Evaluation with Pseudo References
using Regression
Reference-based metrics are typically thought of as
measurements of ?similarity to good translations?
because human translations are used as references,
but in more general terms, they are distance mea-
surements between two sentences. The distance be-
tween a translation hypothesis and an imperfect ref-
erence is still somewhat informative. As a toy ex-
ample, consider a one-dimensional line segment. A
distance from the end-point uniquely determines the
position of a point. When the reference location is
anywhere else on the line segment, a relative dis-
tance to the reference does not uniquely specify a
location on the line segment. However, the position
of a point can be uniquely determined if we are given
its relative distances to two reference locations.
The problem space for MT evaluation, though
more complex, is not dissimilar to the toy scenario.
There are two main differences. First, we do not
know the actual distance function ? this is what we
are trying to learn. The distance functions we have
at our disposal are all heuristic approximations to the
true translational distance function. Second, unlike
human references, whose quality value is assumed to
be maximum, the quality of a pseudo reference sen-
tence is not known. In fact, prior to training, we do
not even know the quality of the reference systems.
Although the direct way to calibrate a reference sys-
tem is to evaluate its outputs, this is not practically
ideal, since human judgments would be needed each
time we wish to incorporate a new reference system.
Our proposed alternative is to calibrate the reference
systems against an existing set of human judgments
for a range of outputs from different MT systems.
That is, if many of the reference system?s outputs
are similar to those MT outputs that received low
assessments, we conclude this reference system may
not be of high quality. Thus, if a new translation is
found to be similar with this reference system?s out-
put, it is more likely for the new translation to also
be bad.
Both issues of combining evidences from heuris-
tic distances and calibrating the quality of pseudo
reference systems can be addressed by a probabilis-
tic learning model. In particular, we use regression
because its problem formulation fits naturally with
the objective of MT evaluations. In regression learn-
ing, we are interested in approximating a function f
that maps a multi-dimensional input vector, x, to a
continuous real value, y, such that the error over a set
of m training examples, {(x1, y1), . . . , (xm, ym)},
is minimized according to a loss function.
In the context of MT evaluation, y is the ?true?
quantitative measure of translation quality for an in-
put sentence1. The function f represents a mathe-
matical model of human judgments of translations;
an input sentence is represented as a feature vector,
x, which contains the information that can be ex-
tracted from the input sentence (possibly including
comparisons against some reference sentences) that
are relevant to computing y. Determining the set of
relevant features for this modeling is on-going re-
1Perhaps even more so than grammaticality judgments, there
is variability in people?s judgments of translation quality. How-
ever, like grammaticality judgments, people do share some sim-
ilarities in their judgments at a coarse-grained level. Ideally,
what we refer to as the true value of translational quality should
reflect the consensus judgments of all people.
298
search. In this work, we consider some of the more
widely used metrics as features. Our full feature
vector consists of r ? 18 adequacy features, where
r is the number of reference systems used, and 26
fluency features:
Adequacy features: These include features de-
rived from BLEU (e.g., n-gram precision, where
1 ? n ? 5, length ratios), PER, WER, fea-
tures derived from METEOR (precision, recall,
fragmentation), and ROUGE-related features (non-
consecutive bigrams with a gap size of g, where
1 ? g ? 5 and longest common subsequence).
Fluency features: We consider both string-level
features such as computing n-gram precision against
a target-language corpus as well as several syntax-
based features. We parse each input sentence into a
dependency tree and compared aspects of it against a
large target-language dependency treebank. In addi-
tion to adapting the idea of Head Word Chains (Liu
and Gildea, 2005), we also compared the input sen-
tence?s argument structures against the treebank for
certain syntactic categories.
Due to the large feature space to explore, we
chose to work with support vector regression as the
learning algorithm. As its loss function, support vec-
tor regression uses an ?-insensitive error function,
which allows for errors within a margin of a small
positive value, ?, to be considered as having zero er-
ror (cf. Bishop (2006), pp.339-344). Like its classi-
fication counterpart, this is a kernel-based algorithm
that finds sparse solutions so that scores for new test
instances are efficiently computed based on a subset
of the most informative training examples. In this
work, Gaussian kernels are used.
The cost of regression learning is that it requires
training examples that are manually assessed by hu-
man judges. However, compared to the cost of cre-
ating new references whenever new (test) sentences
are evaluated, the effort of creating human assess-
ment training data is a limited (ideally, one-time)
cost. Moreover, there is already a sizable collection
of human assessed data for a range of MT systems
through multiple years of the NIST MT Eval efforts.
Our experiments suggest that there is enough as-
sessed data to train the proposed regression model.
Aside from reducing the cost of developing hu-
man reference translations, the proposed metric also
provides an alternative perspective on automatic MT
evaluation that may be informative in its own right.
We conjecture that a metric that compares inputs
against a diverse population of differently imperfect
sentences may be more discriminative in judging
translation systems than solely comparing against
gold standards. That is, two sentences may be
considered equally bad from the perspective of a
gold standard, but subtle differences between them
may become more prominent if they are compared
against sentences in their peer group.
4 Experiments
We conducted experiments to determine the feasibil-
ity of the proposed approach and to address the fol-
lowing questions: (1) How informative are pseudo
references in-and-of themselves? Does varying the
number and/or the quality of the references have an
impact on the metrics? (2) What are the contribu-
tions of the adequacy features versus the fluency fea-
tures to the learning-based metric? (3) How do the
quality and distribution of the training examples, to-
gether with the quality of the pseudo references, im-
pact the metric training? (4) Do these factors impact
the metric?s ability in assessing sentences produced
within a single MT system? How does that system?s
quality affect metric performance?
4.1 Data preparation and Experimental Setup
The implementation of support vector regression
used for these experiments is SVM-Light (Joachims,
1999). We performed all experiments using the 2004
NIST Chinese MT Eval dataset. It consists of 447
source sentences that were translated by four hu-
man translators as well as ten MT systems. Each
machine translated sentence was evaluated by two
human judges for their fluency and adequacy on a
5-point scale2. To remove the bias in the distribu-
tions of scores between different judges, we follow
the normalization procedure described by Blatz et
al. (2003). The two judge?s total scores (i.e., sum
of the normalized fluency and adequacy scores) are
then averaged.
2The NIST human judges use human reference translations
when making assessments; however, our approach is generally
applicable when the judges are bilingual speakers who compare
source sentences with translation outputs.
299
We chose to work with this NIST dataset because
it contains numerous systems that span over a range
of performance levels (see Table 1 for a ranking of
the systems and their averaged human assessment
scores). This allows us to have control over the vari-
ability of the experiments while answering the ques-
tions we posed above (such as the quality of the sys-
tems providing the pseudo references, the quality of
MT systems being evaluated, and the diversity over
the distribution of training examples).
Specifically, we reserved four systems (MT2,
MT5, MT6, and MT9) for the role of pseudo ref-
erences. Sentences produced by the remaining six
systems are used as evaluative data. This set in-
cludes the best and worst systems so that we can see
how well the metrics performs on sentences that are
better (or worse) than the pseudo references. Met-
rics that require no learning are directly applied onto
all sentences of the evaluative set. For the learning-
based metrics, we perform six-fold cross validation
on the evaluative dataset. Each fold consists of sen-
tences from one MT system. In a round robin fash-
ion, each fold serves as the test set while the other
five are used for training and heldout. Thus, the
trained models have seen neither the test instances
nor other instances from the MT system that pro-
duced them.
A metric is evaluated based on its Spearman rank
correlation coefficient between the scores it gave to
the evaluative dataset and human assessments for
the same data. The correlation coefficient is a real
number between -1, indicating perfect negative cor-
relations, and +1, indicating perfect positive cor-
relations. To compare the relative quality of dif-
ferent metrics, we apply bootstrapping re-sampling
on the data, and then use paired t-test to deter-
mine the statistical significance of the correlation
differences (Koehn, 2004). For the results we re-
port, unless explicitly mentioned, all stated compar-
isons are statistically significant with 99.8% con-
fidence. We include two standard reference-based
metrics, BLEU and METEOR, as baseline compar-
isons. BLEU is smoothed (Lin and Och, 2004b), and
it considers only matching up to bigrams because
this has higher correlations with human judgments
than when higher-ordered n-grams are included.
SysID Human-assessment score
MT1 0.661
MT2 0.626
MT3 0.586
MT4 0.578
MT5 0.537
MT6 0.530
MT7 0.530
MT8 0.375
MT9 0.332
MT10 0.243
Table 1: The human-judged quality of ten partici-
pating systems in the NIST 2004 Chinese MT Eval-
uation. We used four systems as references (high-
lighted in boldface) and the data from the remaining
six for training and evaluation.
4.2 Pseudo Reference Variations vs. Metrics
We first compare different metrics? performance
on the six-system evaluative dataset under different
configurations of human and/or pseudo references.
For the case when only one human reference is used,
the reference was chosen at random from the 2004
NIST Eval dataset3. The correlation results on the
evaluative dataset are summarized in Table 2.
Some trends are as expected: comparing within a
metric, having four references is better than having
just one; having human references is better than an
equal number of system references; having a high
quality system as reference is better than one with
low quality. Perhaps more surprising is the consis-
tent trend that metrics do significantly better with
four MT references than with one human reference,
and they do almost as well as using four human ref-
erences. The results show that pseudo references are
informative, as standard metrics were able to make
use of the pseudo references and achieve higher cor-
relations than judging from fluency alone. How-
ever, higher correlations are achieved when learning
with regression, suggesting that the trained metrics
are better at interpreting comparisons against pseudo
references.
Comparing within each reference configuration,
the regression-trained metric that includes both ad-
3One reviewer asked about the quality this human?s trans-
lations. Although we were not given official rankings of the
human references, we compared each person against the other
three using MT evaluation metrics and found this particular
translator to rank third, though the quality of all four are sig-
nificantly higher than even the best MT systems.
300
equacy and fluency features always has the highest
correlations. If the metric consists of only adequacy
features, its performance degrades with the decreas-
ing quality of the references. At another extreme, a
metric based only on fluency features has an over-
all correlation rate of 0.459, which is lower than
most correlations reported in Table 2. This confirms
the importance of modeling adequacy; even a sin-
gle mid-quality MT system may be an informative
pseudo reference. Finally, we note that a regression-
trained metric with the full features set that com-
pares against 4 pseudo references has a higher cor-
relation than BLEU with four human references.
These results suggest that the feedback from the hu-
man assessed training examples was able to help the
learning algorithm to combine different features to
form a better composite metric.
4.3 Sentence-Level Evaluation on Single
Systems
To explore the interaction between the quality of
the reference MT systems and that of the test MT
systems, we further study the following pseudo ref-
erence configurations: all four systems, a high-
quality system with a medium quality system, two
systems of medium-quality, one medium with one
poor system, and only the high-quality system. For
each pseudo reference configuration, we consider
three metrics: BLEU, METEOR, and the regression-
trained metric (using the full feature set). Each
metric evaluates sentences from four test systems
of varying quality: the best system in the dataset
(MT1), the worst in the set (MT10), and two mid-
ranged systems (MT4 and MT7). The correlation
coefficients are summarized in Table 3. Each row
specifies a metric/reference-type combination; each
column specifies an MT system being evaluated (us-
ing sentences from all other systems as training ex-
amples). The fluency-only metric and standard met-
rics using four human references are baselines.
The overall trends at the dataset level generally
also hold for the per-system comparisons. With the
exception of the evaluation of MT10, regression-
based metrics always has higher correlations than
standard metrics that use the same reference con-
figuration (comparing correlation coefficients within
each cell). When the best MT reference system
(MT2) is included as pseudo references, regression-
based metrics are typically better than or not statisti-
cally different from standard applications of BLEU
and METEOR with 4 human references. Using the
two mid-quality MT systems as references (MT5
and MT6), regression metrics yield correlations that
are only slightly lower than standard metrics with
human references. These results support our con-
jecture that comparing against multiple systems is
informative.
The poorer performances of the regression-based
metrics on MT10 point out an asymmetry in the
learning approach. The regression model aims to
learn a function that approximates human judgments
of translated sentences through training examples.
In the space of all possible MT outputs, the neigh-
borhood of good translations is much smaller than
that of bad translations. Thus, as long as the regres-
sion models sees some examples of sentences with
high assessment scores during training, it should
have a much better estimation of the characteristics
of good translations. This idea is supported by the
experimental data. Consider the scenario of eval-
uating MT1 while using two mid-quality MT sys-
tems as references. Although the reference systems
are not as high quality as the system under evalu-
ation, and although the training examples shown to
the regression model were also generated by systems
whose overall quality was rated lower, the trained
metric was reasonably good at ranking sentences
produced by MT1. In contrast, the task of evaluating
sentences from MT10 is more difficult for the learn-
ing approach, perhaps because it is sufficiently dif-
ferent from all training and reference systems. Cor-
relations might be improved with additional refer-
ence systems.
4.4 Discussions
The design of these experiments aims to simulate
practical situations to use our proposed metrics. For
the more frequently encountered language pairs, it
should be possible to find at least two mid-quality
(or better) MT systems to serve as pseudo refer-
ences. For example, one might use commercial off-
the-shelf systems, some of which are free over the
web. For less commonly used languages, one might
use open source research systems (Al-Onaizan et al,
1999; Burbank et al, 2005).
Datasets from formal evaluation events such as
301
Ref type and # Ref Sys. BLEU-S(2) METEOR Regr (adq. only) Regr (full)
4 Humans all humans 0.628 0.591 0.588 0.644
1 Human HRef #3 0.536 0.512 0.487 0.597
4 Systems all MTRefs 0.614 0.583 0.584 0.632
2 Systems Best 2 MTRefs 0.603 0.577 0.573 0.620
Mid 2 MTRefs 0.579 0.555 0.528 0.608
Worst 2 MTRefs 0.541 0.508 0.467 0.581
1 System Best MTRef 0.576 0.559 0.534 0.596
Mid MTRef (MT5) 0.538 0.528 0.474 0.577
Worst MTRef 0.371 0.329 0.151 0.495
Table 2: Comparisons of metrics (columns) using different types of references (rows). The full regression-
trained metric has the highest correlation (shown in boldface) when four human references are used; it has
the second highest correlation rate (shown in italic) when four MT system references are used instead. A
regression-trained metric with only fluency features has a correlation coefficient of 0.459.
Ref Type Metric MT-1 MT-4 MT-7 MT-10
No ref Regr. 0.367 0.316 0.301 -0.045
4 human refs Regr. 0.538* 0.473* 0.459* 0.247
BLEU-S(2) 0.466 0.419 0.397 0.321*
METEOR 0.464 0.418 0.410 0.312
4 MTRefs Regr. 0.498 0.429 0.421 0.243
BLEU-S(2) 0.386 0.349 0.404 0.240
METEOR 0.445 0.354 0.333 0.243
Best 2 MTRefs Regr. 0.492 0.418 0.403 0.201
BLEU-S(2) 0.391 0.330 0.394 0.268
METEOR 0.430 0.333 0.327 0.267
Mid 2 MTRefs Regr. 0.450 0.413 0.388 0.219
BLEU-S(2) 0.362 0.314 0.310 0.282
METEOR 0.391 0.315 0.284 0.274
Worst 2 MTRefs Regr. 0.430 0.386 0.365 0.158
BLEU-S(2) 0.320 0.298 0.316 0.223
METEOR 0.351 0.306 0.302 0.228
Best MTRef Regr. 0.461 0.401 0.414 0.122
BLEU-S(2) 0.371 0.330 0.380 0.242
METEOR 0.375 0.318 0.392 0.283
Table 3: Correlation comparisons of metrics by test systems. For each test system (columns) the overall
highest correlations is distinguished by an asterisk (*); correlations higher than standard metrics using
human-references are highlighted in boldface; those that are statistically comparable to them are italicized.
NIST MT Evals, which contains human assessed
MT outputs for a variety of systems, can be used
for training examples. Alternatively, one might di-
rectly recruit human judges to assess sample sen-
tences from the system(s) to be evaluated. This
should result in better correlations than what we re-
ported here, since the human assessed training ex-
amples will be more similar to the test instances than
the setup in our experiments.
In developing new MT systems, pseudo refer-
ences may supplement the single human reference
translations that could be extracted from a parallel
text. Using the same setup as Exp. 1 (see Table 2),
adding pseudo references does improve correlations.
Adding four pseudo references to the single human
reference raises the correlation coefficient to 0.650
(from 0.597) for the regression metric. Adding them
to four human references results in a correlation co-
efficient of 0.660 (from 0.644)4.
5 Conclusion
In this paper, we have presented a method for de-
veloping sentence-level MT evaluation metrics with-
out using human references. We showed that by
learning from human assessed training examples,
4BLEU with four human references has a correlation of
0.628. Adding four pseudo references increases BLEU to 0.650.
302
the regression-trained metric can evaluate an input
sentence by comparing it against multiple machine-
generated pseudo references and other target lan-
guage resources. Our experimental results suggest
that the resulting metrics are robust even when the
sentences under evaluation are from a system of
higher quality than the systems serving as refer-
ences. We observe that regression metrics that use
multiple pseudo references often have comparable
or higher correlation rates with human judgments
than standard reference-based metrics. Our study
suggests that in conjunction with regression training,
multiple imperfect references may be as informative
as gold-standard references.
Acknowledgments
This work has been supported by NSF Grants IIS-0612791 and
IIS-0710695. We would like to thank Ric Crabbe, Dan Gildea,
Alon Lavie, Stuart Shieber, and Noah Smith and the anonymous
reviewers for their suggestions. We are also grateful to NIST for
making their assessment data available to us.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin Knight,
John Lafferty, I. Dan Melamed, Franz-Josef Och, David
Purdy, Noah A. Smith, and David Yarowsky. 1999.
Statistical machine translation. Technical report, JHU.
citeseer.nj.nec.com/al-onaizan99statistical.html.
Joshua S. Albrecht and Rebecca Hwa. 2007. A re-examination
of machine learning approaches for sentence-level MT eval-
uation. In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics (ACL-2007).
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An auto-
matic metric for MT evaluation with improved correlation
with human judgments. In ACL 2005 Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, June.
Christopher M. Bishop. 2006. Pattern Recognition and Ma-
chine Learning. Springer Verlag.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence estimation for machine trans-
lation. Technical Report Natural Language Engineering
Workshop Final Report, Johns Hopkins University.
Andrea Burbank, Marine Carpuat, Stephen Clark, Markus
Dreyer, Declan Groves Pamela. Fox, Keith Hall, Mary
Hearne, I. Dan Melamed, Yihai Shen, Andy Way, Ben
Wellington, and Dekai Wu. 2005. Final report of the 2005
language engineering workshop on statistical machine trans-
lation by parsing. Technical Report Natural Language Engi-
neering Workshop Final Report, ?JHU?.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proceedings of the 39th
Annual Meeting of the Association for Computational Lin-
guistics, July.
Michael Gamon, Anthony Aue, and Martine Smets. 2005.
Sentence-level MT evaluation without reference translations:
Beyond language modeling. In European Association for
Machine Translation (EAMT), May.
Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Bernhard Scho?elkopf, Christopher Burges, and
Alexander Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Philipp Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP-04).
Alex Kulesza and Stuart M. Shieber. 2004. A learning ap-
proach to improving sentence-level MT evaluation. In Pro-
ceedings of the 10th International Conference on Theoretical
and Methodological Issues in Machine Translation (TMI),
Baltimore, MD, October.
Chin-Yew Lin and Franz Josef Och. 2004a. Automatic evalu-
ation of machine translation quality using longest common
subsequence and skip-bigram statistics. In Proceedings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics, July.
Chin-Yew Lin and Franz Josef Och. 2004b. Orange: a
method for evaluating automatic evaluation metrics for ma-
chine translation. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING 2004),
August.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In ACL 2005 Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, June.
Ding Liu and Daniel Gildea. 2007. Source-language features
and maximum correlation training for machine translation
evaluation. In Proceedings of the HLT/NAACL-2007, April.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, Philadel-
phia, PA.
Christopher Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of LREC
2004.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation. In
Machine Translation Summit IX, pages 394?401, September.
303
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 880?887,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Re-examination of Machine Learning Approaches
for Sentence-Level MT Evaluation
Joshua S. Albrecht and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{jsa8,hwa}@cs.pitt.edu
Abstract
Recent studies suggest that machine learn-
ing can be applied to develop good auto-
matic evaluation metrics for machine trans-
lated sentences. This paper further ana-
lyzes aspects of learning that impact per-
formance. We argue that previously pro-
posed approaches of training a Human-
Likeness classifier is not as well correlated
with human judgments of translation qual-
ity, but that regression-based learning pro-
duces more reliable metrics. We demon-
strate the feasibility of regression-based
metrics through empirical analysis of learn-
ing curves and generalization studies and
show that they can achieve higher correla-
tions with human judgments than standard
automatic metrics.
1 Introduction
As machine translation (MT) research advances, the
importance of its evaluation also grows. Efficient
evaluation methodologies are needed both for facili-
tating the system development cycle and for provid-
ing an unbiased comparison between systems. To
this end, a number of automatic evaluation metrics
have been proposed to approximate human judg-
ments of MT output quality. Although studies have
shown them to correlate with human judgments at
the document level, they are not sensitive enough
to provide reliable evaluations at the sentence level
(Blatz et al, 2003). This suggests that current met-
rics do not fully reflect the set of criteria that people
use in judging sentential translation quality.
A recent direction in the development of met-
rics for sentence-level evaluation is to apply ma-
chine learning to create an improved composite met-
ric out of less indicative ones (Corston-Oliver et al,
2001; Kulesza and Shieber, 2004). Under the as-
sumption that good machine translation will pro-
duce ?human-like? sentences, classifiers are trained
to predict whether a sentence is authored by a human
or by a machine based on features of that sentence,
which may be the sentence?s scores from individ-
ual automatic evaluation metrics. The confidence of
the classifier?s prediction can then be interpreted as a
judgment on the translation quality of the sentence.
Thus, the composite metric is encoded in the confi-
dence scores of the classification labels.
While the learning approach to metric design of-
fers the promise of ease of combining multiple met-
rics and the potential for improved performance,
several salient questions should be addressed more
fully. First, is learning a ?Human Likeness? classi-
fier the most suitable approach for framing the MT-
evaluation question? An alternative is regression, in
which the composite metric is explicitly learned as
a function that approximates humans? quantitative
judgments, based on a set of human evaluated train-
ing sentences. Although regression has been con-
sidered on a small scale for a single system as con-
fidence estimation (Quirk, 2004), this approach has
not been studied as extensively due to scalability and
generalization concerns. Second, how does the di-
versity of the model features impact the learned met-
ric? Third, how well do learning-based metrics gen-
eralize beyond their training examples? In particu-
lar, how well can a metric that was developed based
880
on one group of MT systems evaluate the translation
qualities of new systems?
In this paper, we argue for the viability of a
regression-based framework for sentence-level MT-
evaluation. Through empirical studies, we first
show that having an accurate Human-Likeness clas-
sifier does not necessarily imply having a good MT-
evaluation metric. Second, we analyze the resource
requirement for regression models for different sizes
of feature sets through learning curves. Finally, we
show that SVM-regression metrics generalize better
than SVM-classification metrics in their evaluation
of systems that are different from those in the train-
ing set (by languages and by years), and their corre-
lations with human assessment are higher than stan-
dard automatic evaluation metrics.
2 MT Evaluation
Recent automatic evaluation metrics typically frame
the evaluation problem as a comparison task: how
similar is the machine-produced output to a set of
human-produced reference translations for the same
source text? However, as the notion of similar-
ity is itself underspecified, several different fami-
lies of metrics have been developed. First, simi-
larity can be expressed in terms of string edit dis-
tances. In addition to the well-known word error
rate (WER), more sophisticated modifications have
been proposed (Tillmann et al, 1997; Snover et
al., 2006; Leusch et al, 2006). Second, similar-
ity can be expressed in terms of common word se-
quences. Since the introduction of BLEU (Papineni
et al, 2002) the basic n-gram precision idea has
been augmented in a number of ways. Metrics in the
Rouge family allow for skip n-grams (Lin and Och,
2004a); Kauchak and Barzilay (2006) take para-
phrasing into account; metrics such as METEOR
(Banerjee and Lavie, 2005) and GTM (Melamed et
al., 2003) calculate both recall and precision; ME-
TEOR is also similar to SIA (Liu and Gildea, 2006)
in that word class information is used. Finally, re-
searchers have begun to look for similarities at a
deeper structural level. For example, Liu and Gildea
(2005) developed the Sub-Tree Metric (STM) over
constituent parse trees and the Head-Word Chain
Metric (HWCM) over dependency parse trees.
With this wide array of metrics to choose from,
MT developers need a way to evaluate them. One
possibility is to examine whether the automatic met-
ric ranks the human reference translations highly
with respect to machine translations (Lin and Och,
2004b; Amigo? et al, 2006). The reliability of a
metric can also be more directly assessed by de-
termining how well it correlates with human judg-
ments of the same data. For instance, as a part of the
recent NIST sponsored MT Evaluation, each trans-
lated sentence by participating systems is evaluated
by two (non-reference) human judges on a five point
scale for its adequacy (does the translation retain the
meaning of the original source text?) and fluency
(does the translation sound natural in the target lan-
guage?). These human assessment data are an in-
valuable resource for measuring the reliability of au-
tomatic evaluation metrics. In this paper, we show
that they are also informative in developing better
metrics.
3 MT Evaluation with Machine Learning
A good automatic evaluation metric can be seen as
a computational model that captures a human?s de-
cision process in making judgments about the ade-
quacy and fluency of translation outputs. Inferring a
cognitive model of human judgments is a challeng-
ing problem because the ultimate judgment encom-
passes a multitude of fine-grained decisions, and the
decision process may differ slightly from person to
person. The metrics cited in the previous section
aim to capture certain aspects of human judgments.
One way to combine these metrics in a uniform and
principled manner is through a learning framework.
The individual metrics participate as input features,
from which the learning algorithm infers a compos-
ite metric that is optimized on training examples.
Reframing sentence-level translation evaluation
as a classification task was first proposed by
Corston-Oliver et al (2001). Interestingly, instead
of recasting the classification problem as a ?Hu-
man Acceptability? test (distinguishing good trans-
lations outputs from bad one), they chose to develop
a Human-Likeness classifier (distinguishing out-
puts seem human-produced from machine-produced
ones) to avoid the necessity of obtaining manu-
ally labeled training examples. Later, Kulesza and
Shieber (2004) noted that if a classifier provides a
881
confidence score for its output, that value can be
interpreted as a quantitative estimate of the input
instance?s translation quality. In particular, they
trained an SVM classifier that makes its decisions
based on a set of input features computed from the
sentence to be evaluated; the distance between input
feature vector and the separating hyperplane then
serves as the evaluation score. The underlying as-
sumption for both is that improving the accuracy of
the classifier on the Human-Likeness test will also
improve the implicit MT evaluation metric.
A more direct alternative to the classification ap-
proach is to learn via regression and explicitly op-
timize for a function (i.e. MT evaluation metric)
that approximates human judgments in training ex-
amples. Kulesza and Shieber (2004) raised two
main objections against regression for MT evalua-
tions. One is that regression requires a large set of
labeled training examples. Another is that regression
may not generalize well over time, and re-training
may become necessary, which would require col-
lecting additional human assessment data. While
these are legitimate concerns, we show through em-
pirical studies (in Section 4.2) that the additional re-
source requirement is not impractically high, and
that a regression-based metric has higher correla-
tions with human judgments and generalizes better
than a metric derived from a Human-Likeness clas-
sifier.
3.1 Relationship between Classification and
Regression
Classification and regression are both processes of
function approximation; they use training examples
as sample instances to learn the mapping from in-
puts to the desired outputs. The major difference be-
tween classification and regression is that the func-
tion learned by a classifier is a set of decision bound-
aries by which to classify its inputs; thus its outputs
are discrete. In contrast, a regression model learns
a continuous function that directly maps an input
to a continuous value. An MT evaluation metric is
inherently a continuous function. Casting the task
as a 2-way classification may be too coarse-grained.
The Human-Likeness formulation of the problem in-
troduces another layer of approximation by assum-
ing equivalence between ?Like Human-Produced?
and ?Well-formed? sentences. In Section 4.1, we
show empirically that high accuracy in the Human-
Likeness test does not necessarily entail good MT
evaluation judgments.
3.2 Feature Representation
To ascertain the resource requirements for different
model sizes, we considered two feature models. The
smaller one uses the same nine features as Kulesza
and Shieber, which were derived from BLEU and
WER. The full model consists of 53 features: some
are adapted from recently developed metrics; others
are new features of our own. They fall into the fol-
lowing major categories1:
String-based metrics over references These in-
clude the nine Kulesza and Shieber features as well
as precision, recall, and fragmentation, as calcu-
lated in METEOR; ROUGE-inspired features that
are non-consecutive bigrams with a gap size of m,
where 1 ? m ? 5 (skip-m-bigram), and ROUGE-L
(longest common subsequence).
Syntax-based metrics over references We un-
rolled HWCM into their individual chains of length
c (where 2 ? c ? 4); we modified STM so that it is
computed over unlexicalized constituent parse trees
as well as over dependency parse trees.
String-based metrics over corpus Features in
this category are similar to those in String-based
metric over reference except that a large English cor-
pus is used as ?reference? instead.
Syntax-based metrics over corpus A large de-
pendency treebank is used as the ?reference? instead
of parsed human translations. In addition to adap-
tations of the Syntax-based metrics over references,
we have also created features to verify the argument
structures for certain syntactic categories.
4 Empirical Studies
In these studies, the learning models used for both
classification and regression are support vector ma-
chines (SVM) with Gaussian kernels. All models
are trained with SVM-Light (Joachims, 1999). Our
primary experimental dataset is from NIST?s 2003
1As feature engineering is not the primary focus of this pa-
per, the features are briefly described here, but implementa-
tional details will be made available in a technical report.
882
Chinese MT Evaluations, in which the fluency and
adequacy of 919 sentences produced by six MT sys-
tems are scored by two human judges on a 5-point
scale2. Because the judges evaluate sentences ac-
cording to their individual standards, the resulting
scores may exhibit a biased distribution. We normal-
ize human judges? scores following the process de-
scribed by Blatz et al (2003). The overall human as-
sessment score for a translation output is the average
of the sum of two judges? normalized fluency and
adequacy scores. The full dataset (6 ? 919 = 5514
instances) is split into sets of training, heldout and
test data. Heldout data is used for parameter tuning
(i.e., the slack variable and the width of the Gaus-
sian). When training classifiers, assessment scores
are not used, and the training set is augmented with
all available human reference translation sentences
(4 ? 919 = 3676 instances) to serve as positive ex-
amples.
To judge the quality of a metric, we compute
Spearman rank-correlation coefficient, which is a
real number ranging from -1 (indicating perfect neg-
ative correlations) to +1 (indicating perfect posi-
tive correlations), between the metric?s scores and
the averaged human assessments on test sentences.
We use Spearman instead of Pearson because it
is a distribution-free test. To evaluate the rela-
tive reliability of different metrics, we use boot-
strapping re-sampling and paired t-test to determine
whether the difference between the metrics? correla-
tion scores has statistical significance (at 99.8% con-
fidence level)(Koehn, 2004). Each reported correla-
tion rate is the average of 1000 trials; each trial con-
sists of n sampled points, where n is the size of the
test set. Unless explicitly noted, the qualitative dif-
ferences between metrics we report are statistically
significant. As a baseline comparison, we report the
correlation rates of three standard automatic metrics:
BLEU, METEOR, which incorporates recall and
stemming, and HWCM, which uses syntax. BLEU
is smoothed to be more appropriate for sentence-
level evaluation (Lin and Och, 2004b), and the bi-
gram versions of BLEU and HWCM are reported
because they have higher correlations than when
longer n-grams are included. This phenomenon has
2This corpus is available from the Linguistic Data Consor-
tium as Multiple Translation Chinese Part 4.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 45  50  55  60  65  70  75  80  85
Co
rrel
atio
n C
oef
ficie
nt w
ith 
Hu
ma
n J
udg
em
ent
 (R)
Human-Likeness Classifier Accuracy (%)
Figure 1: This scatter plot compares classifiers? ac-
curacy with their corresponding metrics? correla-
tions with human assessments
been previously observed by Liu and Gildea (2005).
4.1 Relationship between Classification
Accuracy and Quality of Evaluation Metric
A concern in using a metric derived from a Human-
Likeness classifier is whether it would be predic-
tive for MT evaluation. Kulesza and Shieber (2004)
tried to demonstrate a positive correlation between
the Human-Likeness classification task and the MT
evaluation task empirically. They plotted the clas-
sification accuracy and evaluation reliability for a
number of classifiers, which were generated as a
part of a greedy search for kernel parameters and
found some linear correlation between the two. This
proof of concept is a little misleading, however, be-
cause the population of the sampled classifiers was
biased toward those from the same neighborhood as
the local optimal classifier (so accuracy and corre-
lation may only exhibit linear relationship locally).
Here, we perform a similar study except that we
sampled the kernel parameter more uniformly (on
a log scale). As Figure 1 confirms, having an ac-
curate Human-Likeness classifier does not necessar-
ily entail having a good MT evaluation metric. Al-
though the two tasks do seem to be positively re-
lated, and in the limit there may be a system that is
good at both tasks, one may improve classification
without improving MT evaluation. For this set of
heldout data, at the near 80% accuracy range, a de-
rived metric might have an MT evaluation correla-
tion coefficient anywhere between 0.25 (on par with
883
unsmoothed BLEU, which is known to be unsuitable
for sentence-level evaluation) and 0.35 (competitive
with standard metrics).
4.2 Learning Curves
To investigate the feasibility of training regression
models from assessment data that are currently
available, we consider both a small and a large
regression model. The smaller model consists of
nine features (same as the set used by Kulesza and
Shieber); the other uses the full set of 53 features
as described in Section 3.2. The reliability of the
trained metrics are compared with those developed
from Human-Likeness classifiers. We follow a sim-
ilar training and testing methodology as previous
studies: we held out 1/6 of the assessment dataset for
SVM parameter tuning; five-fold cross validation is
performed with the remaining sentences. Although
the metrics are evaluated on unseen test sentences,
the sentences are produced by the same MT systems
that produced the training sentences. In later exper-
iments, we investigate generalizing to more distant
MT systems.
Figure 2(a) shows the learning curves for the two
regression models. As the graph indicates, even
with a limited amount of human assessment data,
regression models can be trained to be comparable
to standard metrics (represented by METEOR in the
graph). The small feature model is close to conver-
gence after 1000 training examples3. The model
with a more complex feature set does require more
training data, but its correlation began to overtake
METEOR after 2000 training examples. This study
suggests that the start-up cost of building even a
moderately complex regression model is not impos-
sibly high.
Although we cannot directly compare the learning
curves of the Human-Likeness classifiers to those of
the regression models (since the classifier?s training
examples are automatically labeled), training exam-
ples for classifiers are not entirely free: human ref-
erence translations still must be developed for the
source sentences. Figure 2(c) shows the learning
curves for training Human-Likeness classifiers (in
terms of improving a classifier?s accuracy) using the
same two feature sets, and Figure 2(b) shows the
3The total number of labeled examples required is closer to
2000, since the heldout set uses 919 labeled examples.
correlations of the metrics derived from the corre-
sponding classifiers. The pair of graphs show, es-
pecially in the case of the larger feature set, that a
large improvement in classification accuracy does
not bring proportional improvement in its corre-
sponding metrics?s correlation; with an accuracy of
near 90%, its correlation coefficient is 0.362, well
below METEOR.
This experiment further confirms that judging
Human-Likeness and judging Human-Acceptability
are not tightly coupled. Earlier, we have shown in
Figure 1 that different SVM parameterizations may
result in classifiers with the same accuracy rate but
different correlations rates. As a way to incorpo-
rate some assessment information into classification
training, we modify the parameter tuning process so
that SVM parameters are chosen to optimize for as-
sessment correlations in the heldout data. By incur-
ring this small amount of human assessed data, this
parameter search improves the classifier?s correla-
tions: the metric using the smaller feature set in-
creased from 0.423 to 0.431, and that of the larger
set increased from 0.361 to 0.422.
4.3 Generalization
We conducted two generalization studies. The first
investigates how well the trained metrics evaluate
systems from other years and systems developed
for a different source language. The second study
delves more deeply into how variations in the train-
ing examples affect a learned metric?s ability to gen-
eralize to distant systems. The learning models for
both experiments use the full feature set.
Cross-Year Generalization To test how well the
learning-based metrics generalize to systems from
different years, we trained both a regression-based
metric (R03) and a classifier-based metric (C03)
with the entire NIST 2003 Chinese dataset (using
20% of the data as heldout4). All metrics are then
applied to three new datasets: NIST 2002 Chinese
MT Evaluation (3 systems, 2634 sentences total),
NIST 2003 Arabic MT Evaluation (2 systems, 1326
sentences total), and NIST 2004 Chinese MT Evalu-
ation (10 systems, 4470 sentences total). The results
4Here, too, we allowed the classifier?s parameters to be
tuned for correlation with human assessment on the heldout data
rather than accuracy.
884
(a) (b) (c)
Figure 2: Learning curves: (a) correlations with human assessment using regression models; (b) correlations
with human assessment using classifiers; (c) classifier accuracy on determining Human-Likeness.
Dataset R03 C03 BLEU MET. HWCM
2002 Ara 0.466 0.384 0.423 0.431 0.424
2002 Chn 0.309 0.250 0.269 0.290 0.260
2004 Chn 0.602 0.566 0.588 0.563 0.546
Table 1: Correlations for cross-year generalization.
Learning-based metrics are developed from NIST
2003 Chinese data. All metrics are tested on datasets
from 2003 Arabic, 2002 Chinese and 2004 Chinese.
are summarized in Table 1. We see that R03 con-
sistently has a better correlation rate than the other
metrics.
At first, it may seem as if the difference between
R03 and BLEU is not as pronounced for the 2004
dataset, calling to question whether a learned met-
ric might become quickly out-dated, we argue that
this is not the case. The 2004 dataset has many
more participating systems, and they span a wider
range of qualities. Thus, it is easier to achieve a
high rank correlation on this dataset than previous
years because most metrics can qualitatively discern
that sentences from one MT system are better than
those from another. In the next experiment, we ex-
amine the performance of R03 with respect to each
MT system in the 2004 dataset and show that its cor-
relation rate is higher for better MT systems.
Relationship between Training Examples and
Generalization Table 2 shows the result of a gen-
eralization study similar to before, except that cor-
relations are performed on each system. The rows
order the test systems by their translation quali-
ties from the best performing system (2004-Chn1,
whose average human assessment score is 0.655 out
of 1.0) to the worst (2004-Chn10, whose score is
0.255). In addition to the regression metric from
the previous experiment (R03-all), we consider two
more regression metrics trained from subsets of the
2003 dataset: R03-Bottom5 is trained from the sub-
set that excludes the best 2003 MT system, and R03-
Top5 is trained from the subset that excludes the
worst 2003 MT system.
We first observe that on a per test-system basis,
the regression-based metrics generally have better
correlation rates than BLEU, and that the gap is as
wide as what we have observed in the earlier cross-
years studies. The one exception is when evaluating
2004-Chn8. None of the metrics seems to correlate
very well with human judges on this system. Be-
cause the regression-based metric uses these individ-
ual metrics as features, its correlation also suffers.
During regression training, the metric is opti-
mized to minimize the difference between its pre-
diction and the human assessments of the training
data. If the input feature vector of a test instance
is in a very distant space from training examples,
the chance for error is higher. As seen from the
results, the learned metrics typically perform better
when the training examples include sentences from
higher-quality systems. Consider, for example, the
differences between R03-all and R03-Top5 versus
the differences between R03-all and R03-Bottom5.
Both R03-Top5 and R03-Bottom5 differ from R03-
all by one subset of training examples. Since R03-
all?s correlation rates are generally closer to R03-
Top5 than to R03-Bottom5, we see that having seen
extra training examples from a bad system is not as
harmful as having not seen training examples from a
good system. This is expected, since there are many
ways to create bad translations, so seeing a partic-
885
R03-all R03-Bottom5 R03-Top5 BLEU METEOR HWCM
2004-Chn1 0.495 0.460 0.518 0.456 0.457 0.444
2004-Chn2 0.398 0.330 0.440 0.352 0.347 0.344
2004-Chn3 0.425 0.389 0.459 0.369 0.402 0.369
2004-Chn4 0.432 0.392 0.434 0.400 0.400 0.362
2004-Chn5 0.452 0.441 0.443 0.370 0.426 0.326
2004-Chn6 0.405 0.392 0.406 0.390 0.357 0.380
2004-Chn7 0.443 0.432 0.448 0.390 0.408 0.392
2004-Chn8 0.237 0.256 0.256 0.265 0.259 0.179
2004-Chn9 0.581 0.569 0.591 0.527 0.537 0.535
2004-Chn10 0.314 0.313 0.354 0.321 0.303 0.358
2004-all 0.602 0.567 0.617 0.588 0.563 0.546
Table 2: Metric correlations within each system. The columns specify which metric is used. The rows
specify which MT system is under evaluation; they are ordered by human-judged system quality, from best
to worst. For each evaluated MT system (row), the highest coefficient in bold font, and those that are
statistically comparable to the highest are shown in italics.
ular type of bad translations from one system may
not be very informative. In contrast, the neighbor-
hood of good translations is much smaller, and is
where all the systems are aiming for; thus, assess-
ments of sentences from a good system can be much
more informative.
4.4 Discussion
Experimental results confirm that learning from
training examples that have been doubly approx-
imated (class labels instead of ordinals, human-
likeness instead of human-acceptability) does nega-
tively impact the performance of the derived metrics.
In particular, we showed that they do not generalize
as well to new data as metrics trained from direct
regression.
We see two lingering potential objections toward
developing metrics with regression-learning. One
is the concern that a system under evaluation might
try to explicitly ?game the metric5.? This is a con-
cern shared by all automatic evaluation metrics, and
potential problems in stand-alone metrics have been
analyzed (Callison-Burch et al, 2006). In a learning
framework, potential pitfalls for individual metrics
are ameliorated through a combination of evidences.
That said, it is still prudent to defend against the po-
tential of a system gaming a subset of the features.
For example, our fluency-predictor features are not
strong indicators of translation qualities by them-
selves. We want to avoid training a metric that as-
5Or, in a less adversarial setting, a system may be perform-
ing minimum error-rate training (Och, 2003)
signs a higher than deserving score to a sentence that
just happens to have many n-gram matches against
the target-language reference corpus. This can be
achieved by supplementing the current set of hu-
man assessed training examples with automatically
assessed training examples, similar to the labeling
process used in the Human-Likeness classification
framework. For instance, as negative training ex-
amples, we can incorporate fluent sentences that are
not adequate translations and assign them low over-
all assessment scores.
A second, related concern is that because the met-
ric is trained on examples from current systems us-
ing currently relevant features, even though it gener-
alizes well in the near term, it may not continue to
be a good predictor in the distant future. While pe-
riodic retraining may be necessary, we see value in
the flexibility of the learning framework, which al-
lows for new features to be added. Moreover, adap-
tive learning methods may be applicable if a small
sample of outputs of some representative translation
systems is manually assessed periodically.
5 Conclusion
Human judgment of sentence-level translation qual-
ity depends on many criteria. Machine learning af-
fords a unified framework to compose these crite-
ria into a single metric. In this paper, we have
demonstrated the viability of a regression approach
to learning the composite metric. Our experimental
results show that by training from some human as-
886
sessments, regression methods result in metrics that
have better correlations with human judgments even
as the distribution of the tested population changes.
Acknowledgments
This work has been supported by NSF Grants IIS-0612791 and
IIS-0710695. We would like to thank Regina Barzilay, Ric
Crabbe, Dan Gildea, Alex Kulesza, Alon Lavie, and Matthew
Stone as well as the anonymous reviewers for helpful comments
and suggestions. We are also grateful to NIST for making their
assessment data available to us.
References
Enrique Amigo?, Jesu?s Gime?nez, Julio Gonzalo, and Llu??s
Ma`rquez. 2006. MT evaluation: Human-like vs. human ac-
ceptable. In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, Sydney, Australia, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An auto-
matic metric for MT evaluation with improved correlation
with human judgments. In ACL 2005 Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,
Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola
Ueffing. 2003. Confidence estimation for machine trans-
lation. Technical Report Natural Language Engineering
Workshop Final Report, Johns Hopkins University.
Christopher Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in machine
translation research. In The Proceedings of the Thirteenth
Conference of the European Chapter of the Association for
Computational Linguistics.
Simon Corston-Oliver, Michael Gamon, and Chris Brockett.
2001. A machine learning approach to the automatic eval-
uation of machine translation. In Proceedings of the 39th
Annual Meeting of the Association for Computational Lin-
guistics, July.
Thorsten Joachims. 1999. Making large-scale SVM learning
practical. In Bernhard Scho?elkopf, Christopher Burges, and
Alexander Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
David Kauchak and Regina Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Main Confer-
ence, New York City, USA, June.
Philipp Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of the 2004 Confer-
ence on Empirical Methods in Natural Language Processing
(EMNLP-04).
Alex Kulesza and Stuart M. Shieber. 2004. A learning ap-
proach to improving sentence-level MT evaluation. In Pro-
ceedings of the 10th International Conference on Theoretical
and Methodological Issues in Machine Translation (TMI),
Baltimore, MD, October.
Gregor Leusch, Nicola Ueffing, and Hermann Ney. 2006.
CDER: Efficient MT evaluation using block movements. In
The Proceedings of the Thirteenth Conference of the Euro-
pean Chapter of the Association for Computational Linguis-
tics.
Chin-Yew Lin and Franz Josef Och. 2004a. Automatic evalu-
ation of machine translation quality using longest common
subsequence and skip-bigram statistics. In Proceedings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics, July.
Chin-Yew Lin and Franz Josef Och. 2004b. Orange: a
method for evaluating automatic evaluation metrics for ma-
chine translation. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING 2004),
August.
Ding Liu and Daniel Gildea. 2005. Syntactic features for
evaluation of machine translation. In ACL 2005 Workshop
on Intrinsic and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, June.
Ding Liu and Daniel Gildea. 2006. Stochastic iterative align-
ment for machine translation evaluation. In Proceedings
of the Joint Conference of the International Conference on
Computational Linguistics and the Association for Com-
putational Linguistics (COLING-ACL?2006) Poster Session,
July.
I. Dan Melamed, Ryan Green, and Joseph Turian. 2003. Preci-
sion and recall of machine translation. In In Proceedings of
the HLT-NAACL 2003: Short Papers, pages 61?63, Edmon-
ton, Alberta.
Franz Josef Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2002. Bleu: a method for automatic evaluation of ma-
chine translation. In Proceedings of the 40th Annual Meeting
of the Association for Computational Linguistics, Philadel-
phia, PA.
Christopher Quirk. 2004. Training a sentence-level machine
translation confidence measure. In Proceedings of LREC
2004.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Mic-
ciulla, and John Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In Proceedings of the
8th Conference of the Association for Machine Translation
in the Americas (AMTA-2006).
Christoph Tillmann, Stephan Vogel, Hermann Ney, Hassan
Sawaf, and Alex Zubiaga. 1997. Accelerated DP-based
search for statistical translation. In Proceedings of the 5th
European Conference on Speech Communication and Tech-
nology (EuroSpeech ?97).
887
Proceedings of the Second Workshop on Statistical Machine Translation, pages 248?255,
Prague, June 2007. c?2007 Association for Computational Linguistics
 
Localization of Difficult-to-Translate Phrases 
 
 
Behrang Mohit1 and Rebecca Hwa1,2 
Intelligent Systems Program1 
Department of Computer Science2 
University of Pittsburgh 
Pittsburgh, PA 15260 U.S.A. 
{behrang, hwa}@cs.pitt.edu 
 
 
Abstract 
This paper studies the impact that difficult-to-
translate source-language phrases might have 
on the machine translation process. We formu-
late the notion of difficulty as a measurable 
quantity; we show that a classifier can be 
trained to predict whether a phrase might be 
difficult to translate; and we develop a frame-
work that makes use of the classifier and ex-
ternal resources (such as human translators) to 
improve the overall translation quality. 
Through experimental work, we verify that by 
isolating difficult-to-translate phrases and 
processing them as special cases, their nega-
tive impact on the translation of the rest of the 
sentences can be reduced. 
1 Introduction 
For translators, not all source sentences are created 
equal. Some are straight-forward enough to be 
automatically translated by a machine, while others 
may stump even professional human translators. 
Similarly, within a single sentence there may be 
some phrases that are more difficult to translate 
than others. The focus of this paper is on identify-
ing Difficult-to-Translate Phrases (DTPs) within a 
source sentence and determining their impact on 
the translation process. We investigate three ques-
tions: (1) how should we formalize the notion of 
difficulty as a measurable quantity over an appro-
priately defined phrasal unit? (2) To what level of 
accuracy can we automatically identify DTPs? (3) 
To what extent do DTPs affect an MT system's 
performance on other (not-as-difficult) parts of the 
sentence? Conversely, would knowing the correct 
translation for the DTPs improve the system?s 
translation for the rest of the sentence?  
In this work, we model difficulty as a meas-
urement with respect to a particular MT system.  
We further assume that the degree of difficulty of a 
phrase is directly correlated with the quality of the 
translation produced by the MT system, which can 
be approximated using an automatic evaluation 
metric, such as BLEU (Papineni et al, 2002).  Us-
ing this formulation of difficulty, we build a 
framework that augments an off-the-shelf phrase-
based MT system with a DTP classifier that we 
developed.  We explore the three questions in a set 
of experiments, using the framework as a testbed.  
In the first experiment, we verify that our pro-
posed difficulty measurement is sensible.  The sec-
ond experiment evaluates the classifier's accuracy 
in predicting whether a source phrase is a DTP.  
For that, we train a binary SVM classifier via a 
series of lexical and system dependent features. 
The third is an oracle study in which the DTPs are 
perfectly identified and human translations are ob-
tained. These human-translated phrases are then 
used to constrain the MT system as it translates the 
rest of the sentence. We evaluate the translation 
quality of the entire sentence and also the parts that 
are not translated by humans.  Finally, the frame-
work is evaluated as a whole. Results from our 
experiments suggest that improved handling of 
DTPs will have a positive impact the overall MT 
output quality.  Moreover, we find the SVM-
trained DTP classifier to have a promising rate of 
accuracy, and that the incorporation of DTP infor-
mation can improve the outputs of the underlying 
MT system. Specifically, we achieve an improve-
ment of translation quality for non-difficult seg-
248
ments of a sentence when the DTPs are translated 
by humans. 
2 Motivation 
There are several reasons for investigating ways to 
identify DTPs.  For instance, it can help to find 
better training examples in an active learning 
framework; it can be used to coordinate outputs of 
multiple translation systems; or it can be used as 
means of error analysis for MT system 
development.  It can also be used as a pre-
processing step, an alternative to post-editing.  For 
many languages, MT output requires post-
translation editing that can be cumbersome task for 
low quality outputs, long sentences, complicated 
structures and idioms.  Pre-translation might be 
viewed as a kind of preventive medicine; that is, a 
system might produce an overall better output if it 
were not thwarted by some small portion of the 
input. By identifying DTPs and passing those cases 
off to an expensive translation resource (e.g. 
humans) first, we might avoid problems further 
down the MT pipeline. Moreover, pre-translation 
might not always have to be performed by humans.  
What is considered difficult for one system might 
not be difficult for another system; thus, pre-
translation might also be conducted using multiple 
MT systems. 
3 Our Approach 
Figure 1 presents the overall dataflow of our 
system.  The input is a source sentence (a1 ... an), 
from which DTP candidates are proposed. Because 
the DTPs will have to be translated by humans as 
independent units, we limit the set of possible 
phrases to be syntactically meaningful units. 
Therefore, the framework requires a source-
language syntactic parser or chunker. In this paper, 
we parse the source sentence with an off-the-shelf 
syntactic parser (Bikel, 2002). From the parse tree 
produced for the source sentence, every constituent 
whose string span is between 25% and 75% of the 
full sentence length is considered a DTP candidate.  
Additionally we have a tree node depth constraint 
that requires the constituent to be at least two 
levels above the tree?s yield and two levels below 
the root.  These two constraints ensure that the 
extracted phrases have balanced lengths. 
We apply the classifier on each candidate and 
select the one labeled as difficult with the highest 
classification score.  Depending on the underlying 
classifier, the score can be in various formats such 
as class probablity, confidence measure, etc.  In 
our SVM based classifier, the score is the distance 
from the margin. 
 
 
Figure 1: An overview of our translation frame-
work.  
 
The chosen phrase (aj ... ak) is translated by a 
human (ei ... em). We constrain the underlying 
phrase-based MT system (Koehn, 2003) so that its 
decoding of the source sentence must contain the 
human translation for the DTP. In the following 
subsections, we describe how we develop the DTP 
classifier with machine learning techniques and 
how we constrain the underlying MT system with 
human translated DTPs. 
3.1 Training the DTP Classifier 
Given a phrase in the source language, the DTP 
classifier extracts a set of features from it and pre-
dicts whether it is difficult or not based on its fea-
ture values. We use an SVM classifier in this work.  
We train the SVM-Light implementation of the 
249
algorithm (Joachims 1999).  To train the classifier, 
we need to tackle two challenges.  First, we need to 
develop some appropriate training data because 
there is no corpus with annotated DTPs. Second, 
we need to determine a set of predictive features 
for the classifier. 
Development of the Gold Standard 
Unlike the typical SVM training scenario, labeled 
training examples of DTPs do not exist. Manual 
creation of such data requires deep understanding 
of the linguistics differences of source and target 
languages and also deep knowledge about the MT 
system and its training data.  Such resources are 
not accessible to us.  Instead, we construct the gold 
standard automatically.  We make the strong as-
sumption that difficulty is directly correlated to 
translation quality and that translation quality can 
be approximately measured by automatic metrics 
such as BLEU.  We have two resource require-
ments ? a sentence-aligned parallel corpus (differ-
ent from the data used to train the underlying MT 
system), and a syntactic parser for the source lan-
guage. The procedure for creating the gold stan-
dard data is as follows:  
1. Each source sentence is parsed. 
2. Phrase translations are extracted from the par-
allel corpus. Specifically, we generate word-
alignments using GIZA++ (Och 2001) in both 
directions and combine them using the refined 
methodology (Och and Ney 2003), and then 
we applied Koehn?s toolkit (2004) to extract 
parallel phrases. We have relaxed the length 
constraints of the toolkit to ensure the extrac-
tion of long phrases (as long as 16 words).  
3. Parallel phrases whose source parts are not 
well-formed constituents are filtered out.   
4. The source phrases are translated by the under-
lying MT system, and a baseline BLEU score 
is computed over this set of MT outputs. 
5. To label each source phrase, we remove that 
phrase and its translation from the MT output 
and calculate the set?s new BLEU score. If 
new-score is greater than the baseline score by 
some threshold value (a tunable parameter), we 
label the phrase as difficult, otherwise we label 
it as not difficult.   
Rather than directly calculating the BLEU score 
for each phrase, we performed the round-robin 
procedure described in steps 4 and 5 because 
BLEU is not reliable for short phrases. BLEU is 
calculated as a geometric mean over n-gram 
matches with references, assigning a score of zero 
to an entire phrase if no higher-ordered n-gram 
matches were found against the references. How-
ever, some phrases with a score of 0 might have 
more matches in the lower-ordered n-grams than 
other phrases (and thus ought to be considered 
?easier?). A comparison of the relative changes in 
BLEU scores while holding out a phrase from the 
corpus gives us a more sensitive measurement than 
directly computing BLEU for each phrase. 
Features 
By analyzing the training corpus, we have found 
18 features that are indicative of DTPs. Some 
phrase-level feature values are computed as an av-
erage of the feature values of the individual words.  
The following first four features use some prob-
abilities that are collected from a parallel data and 
word alignments.  Such a resource does not exist at 
the time of testing.  Instead we use the history of 
the source words (estimated from the large parallel 
corpus) to predict the feature value. 
  (I) Average probability of word alignment 
crossings: word alignment crossings are indicative 
of word order differences and generally structural 
difference across two languages.  We collect word 
alignment crossing statistics from the training cor-
pus to estimate the crossing probability for each 
word in a new source phrase.  For example the 
Arabic word rhl has 67% probability of alignment 
crossing (word movement across English).  These 
probabilities are then averaged into one value for 
the entire phrase.  
(II) Average probability of translation ambi-
guity: words that have multiple equally-likely 
translations contribute to translation ambiguity.  
For example a word that has 4 different transla-
tions with similar frequencies tends to be more 
ambiguous than a word that has one dominant 
translation. We collect statistics about the lexical 
translational ambiguities from the training corpus 
and lexical translation tables and use them to pre-
dict the ambiguity of each word in a new source 
phrase. The score for the phrase is the average of 
the scores for the individual words. 
(III) Average probability of POS tag changes:  
Change of a word?s POS tagging is an indication 
of deep structural differences between the source 
phrase and the target phrase.  Using the POS tag-
ging information for both sides of the training cor-
pus, we learn the probability that each source 
word?s POS gets changed after the translation.  To 
250
overcome data sparseness, we only look at the col-
lapsed version of POS tags on both sides of the 
corpus.  The phrase?s score is the average the indi-
vidual word probabilities.  
(IV) Average probability of null alignments: 
In many cases null alignments of the source words 
are indicative of the weakness of information about 
the word.  This feature is similar to average ambi-
guity probability.  The difference is that we use the 
probability of null alignments instead of lexical 
probabilities. 
(V-IX) Normalized number of unknown 
words, content words, numbers, punctuations: 
For each of these features we normalize the count 
(e.g.: unknown words) with the length of the 
phrase.  The normalization of the features helps the 
classifier to not have length preference for the 
phrases.  
(X) Number of proper nouns: Named entities 
tend to create translation difficulty, due to their 
diversity of spellings and also domain differences.  
We use the number of proper nouns to estimate the 
occurrence of the named entities in the phrase. 
(XI Depth of the subtree: The feature is used as 
a measure of syntactic complexity of the phrase.  
For example continuous right branching of the 
parse tree which adds to the depth of the subtree 
can be indicative of a complex or ambiguous struc-
ture that might be difficult to translate. 
(XII) Constituency type of the phrase:  We 
observe that the different types of constituents 
have varied effects on the translations of the 
phrase.  For example prepositional phrases tend to 
belong to difficult phrases.  
(XIII) Constituency type of the parent phrase 
(XIV) Constituency types of the children 
nodes of the phrase: We form a set from the chil-
dren nodes of the phrase (on the parse tree).   
(XV) Length of the phrase: The feature is 
based on the number of the words in the phrase. 
(XVI) Proportional length of the phrase: The 
proportion of the length of the phrase to the length 
of the sentence.  As this proportion gets larger, the 
contextual effect on the translation of the phrase 
becomes less. 
    (XVII) Distance from the start of the sentence 
and: Phrases that are further away from the start of 
the sentence tend to not be translated as well due to 
compounding translational errors.   
(XVIII) Distance from a learned translation 
phrase: The feature measure the number of words 
before reaching a learned phrase.  In other words it 
s an indication of the level of error that is intro-
duced in the early parts of the phrase translation. 
3.2 Constraining the MT System 
Once human translations have been obtained for 
the DTPs, we want the MT system to only consider 
output candidates that contain the human transla-
tions. The additional knowledge can be used by the 
phrase-based system without any code modifica-
tion. Figure 2 shows the data-flow for this process. 
First, we append the pre-trained phrase-translation 
table with the DTPs and their human translations 
with a probability of 1.0. We also include the hu-
man translations for the DTPs as training data for 
the language model to ensure that the phrase vo-
cabulary is familiar to the decoder and relax the 
phrase distortion parameter that the decoder can 
include all phrase translations with any length in 
the decoding.  Thus, candidates that contain the 
human translations for the DTPs will score higher 
and be chosen by the decoder. 
 
 
Figure 2: Human translations for the DTPs can be 
incorporated into the MT system?s phrase table and 
language model. 
4 Experiments 
The goal of these four experiments is to gain a bet-
ter understanding of the DTPs and their impact on 
the translation process. All our studies are con-
ducted for Arabic-to-English MT.  We formed a 
one-million word parallel text out of two corpora 
released by the Linguistic Data Consortium: Ara-
251
bic News Translation Text Part 1 and Arabic Eng-
lish Parallel News Part 1.  The majority of the data 
was used to train the underlying phrase-based MT 
system. We reserve 2000 sentences for develop-
ment and experimentation.  Half of these are used 
for the training and evaluation of the DTP classi-
fier (Sections 4.1 and 4.2); the other half is used 
for translation experiments on the rest of the 
framework (Sections 4.3 and 4.4).  
In both cases, translation phrases are extracted 
from the sentences and assigned ?gold standard? 
labels according to the procedure described in Sec-
tion 3.1. It is necessary to keep two separate data-
sets because the later experiments make use of the 
trained DTP classifier.   
For the two translation experiments, we also face 
a practical obstacle: we do not have an army of 
human translators at our disposal to translate the 
identified phrases. To make the studies possible, 
we rely on a pre-translated parallel corpus to simu-
late the process of asking a human to translate a 
phrase. That is, we use the phrase extraction toolkit 
to find translation phrases corresponding to each 
DTP candidate (note that the data used for this ex-
periment is separate from the main parallel corpus 
used to train the MT system, so the system has no 
knowledge about these translations).  
4.1 Automatic Labeling of DTP  
In this first experiment, we verify whether our 
method for creating positive and negative labeled 
examples of DTPs (as described in Section 3.1) is 
sound. Out of 2013 extracted phrases, we found  
949 positive instances (DTPs) and 1064 negative 
instances. The difficult phrases have an average 
length of 8.8 words while the other phrases have an 
average length of 7.8 words1.  We measured the 
BLEU scores for the MT outputs for both groups 
of phrases (Table 1).  
 
Experiment BLEU Score 
DTPs 14.34 
Non-DTPs 61.22 
Table 1: Isolated Translation of the selected training 
phrases 
 
The large gap between the translation qualities 
of the two phrase groups suggests that the DTPs 
are indeed much more ?difficult? than the other 
phrases. 
                                                          
1
 Arabic words are tokenized and lemmatized by Diab?s Ara-
bic Toolset (Diab 2004). 
4.2 Evaluation of the DTP Classifier 
We now perform a local evaluation of the trained 
DTP classifier for its classification accuracy.  The 
classifier is trained as an SVM using a linear ker-
nel.  The ?gold standard? phrases from the section 
4.1 are split into three groups: 2013 instances are 
used as training data for the classifier; 100 in-
stances are used for development (e.g., parameter 
tuning and feature engineering); and 200 instances 
are used as test instances.  The test set has an equal 
number of difficult and non-difficult phrases (50% 
baseline accuracy).  
In order to optimize the accuracy of classifica-
tion, we used a development set for feature engi-
neering and trying various SVM kernels and asso-
ciated parameters.  For the feature engineering 
part, we used the all-but-one heuristic to test the 
contribution of each individual feature.  Table 2 
presents the most and least contributing four fea-
tures that we used in our classification.  Among 
various features, we observed that the syntactic 
features are the most contributing sources of in-
formation for our classification. 
 
Least Useful Features Most Useful Features 
Ft1: Align Crossing Ft 2: Lexical Ambiguity 
Ft 8: Count of Nums Ft 11: Depth of subtree 
Ft:9: Count of Puncs Ft 12: Const type of Phr 
Ft 10: Count of NNPs Ft 13: Const type of Par 
Table 2: The most and least useful features 
 
The DTP classifier achieves an average accu-
racy of 71.5%, using 10 fold cross validation on 
the test set. 
4.3 Study on the effect of DTPs 
This experiment concentrates on the second half of 
the framework: that of constraining the MT system 
to use human-translations for the DTPs. Our objec-
tive is to assess to what degree do the DTPs nega-
tively impact the MT process. We compare the MT 
outputs of two groups of sentences.  Group I is 
made up of 242 sentences that contain the most 
difficult to translate phrases in the 1000 sentences 
we reserved for this study. Group II is a control 
group made up of 242 sentences with the least dif-
ficult to translate phrases.  The DTPs make up 
about 9% of word counts in the above 484 sen-
tences.  We follow the procedure described in Sec-
tion 3.1 to identify and score all the phrases; thus, 
252
this experiment can be considered an oracle study. 
We compare four scenarios: 
1. Adding phrase translations for Group I: MT 
system is constrained using the method de-
scribed in Section 3.2 to incorporate human 
translations of the pre-identified DTPs in 
Group I.2 
2. Adding phrase translations for Group II: 
MT system is constrained to use human trans-
lations for the identified (non-difficult) phrases 
in Group II.  
3. Adding translations for random phrases: 
randomly replace 242 phrases from either 
Group I or Group II. 
4. Adding translations for classifier labeled  
DTPs: human translations for phrases that our 
trained classifier has identified as DTPs from 
both Group I and Group II. 
 
  All of the above scenarios are evaluated on a 
combined set of 484 sentences (group 1 + group 2).  
This set up normalizes the relative difficulty of 
each grouping. 
If the DTPs negatively impact the MT process, 
we would expect to see a greater improvement 
when Group I phrases are translated by humans 
than when Group II phrases are translated by 
humans.  
The baseline for the comparisons is to evaluate 
the outputs of the MT system without using any 
human translations. This results in a BLEU score 
of 24.0. When human translations are used, the 
BLEU score of the dataset increases, as shown in 
Table 3. 
 
Experiment BLEU  
Baseline (no human trans) 24.0 
w/ translated DTPs (Group I) 39.6 
w/ translated non-DTPs (Group II) 33.7 
w/ translated phrases (random) 35.1 
w/ translated phrases (classifier) 37.0 
Table 3: A comparison of BLEU scores for the entire set 
of sentences under the constraints of using human trans-
lations for different types of phrases. 
 
While it is unsurprising that the inclusion of 
human translations increases the overall BLEU 
score, this comparison shows that the boost is 
sharper when more DTPs are translated. This is 
                                                          
2
 In this study, because the sentences are from the training 
parallel corpus, we can extract human translations directly 
from the corpus. 
consistent with our conjecture that pre-translating 
difficult phrases may be helpful. 
A more interesting question is whether the hu-
man translations still provide any benefit once we 
factor out their direct contributions to the increase 
in BLEU scores. To answer this question, we com-
pute the BLEU scores for the outputs again, this 
time filtering out all 484 identified phrases from 
the evaluation.  In other words in this experiment 
we focus on the part of the sentence that is not la-
beled and does include any human translations.  
Table 4 presents the results.   
 
Experiment BLEU 
Baseline (no human trans) 23.0 
w/ translated DTPs (Group I) 25.4 
w/ translated non-DTPs (Group II) 23.9 
w/ translated phrases (random) 24.5 
w/ translated phrases (classifier) 25.1 
Table 4: BLEU scores for the translation outputs ex-
cluding the 484 (DTP and non-DTP) phrases. 
 
The largest gain (2.4 BLEU increment from 
baseline) occurs when all and only the DTPs were 
translated. In contrast, replacing phrases from 
Group II did not improve the BLEU score very 
much. These results suggest that better handling of 
DTPs will have a positive effect on the overall MT 
process. We also note that using our SVM-trained 
classifier to identify the DTPs, the constrained MT 
system?s outputs obtained a BLEU score that is 
nearly as high as if a perfect classifier was used.  
4.4 Full evaluation of the framework       
This final experiment evaluates the complete 
framework as described in Section 3. The setup of 
this study is similar to that of the previous section.   
The main difference is that now, we rely on the 
classifier to predict which phrase would be the 
most difficult to translate and use human transla-
tions for those phrases. 
Out of 1000 sentences, 356 have been identified 
to contain DTPs (that are in the phrase extraction 
list). In other words, only 356 sentences hold DTPs 
that we can find their human translations through 
phrase projection.  For the remaining sentences, we 
do not use any human translation. 
 
 
 
 
 
253
Table 5 presents the increase in BLEU scores 
when human translations for the 356 DTPs are 
used. As expected the BLEU score increases, but 
the improvement is less dramatic than in the previ-
ous experiment because most sentences are un-
changed. 
 
Experiment BLEU  
Baseline (no human trans) 24.9 
w/ human translations  29.0 
Table 5: Entire Corpus level evaluation (1000 sen-
tences) when replacing DTPs in the hit list 
 
Table 6 summarizes the experimental results on 
the subset of the 356 sentences.  The first two rows 
compare the translation quality at the sentence 
level (similar to Table 3); the next two rows com-
pare the translation quality of the non-DTP parts 
(similar to Table 4).  Rows 1 and 3 are conditions 
when we do not use human translation; and rows 2 
and 4 are conditions when we replace DTPs with 
their associated human translations.  The im-
provements of the BLEU score for the hit list are 
similar to the results we have previously seen. 
 
Experiment on 356 sentences BLEU 
Baseline: full sent. 25.1 
w/ human translation: full sent.  37.6 
Baseline: discount DTPs 26.0 
w/ human translation: discount 
DTPs 
27.8 
Table 6: Evaluation of the subset of 356 sentences: both 
for the full sentence and for non-DTP parts, with and 
without human translation replacement of DTPs.  
5 Related Work 
Our work is related to the problem of confidence 
estimation for MT (Blatz et. al. 2004; Zen and Ney 
2006).  The confidence measure is a score for n-
grams generated by a decoder3. The measure is 
based on the features like lexical probabilities 
(word posterior), phrase translation probabilities, 
N-best translation hypothesis, etc.  Our DTP classi-
fication differs from the confidence measuring in 
several aspects: one of the main purposes of our 
classification of DTPs is to optimize the usage of 
outside resources.  To do so, we focus on classifi-
cation of phrases which are syntactically meaning-
ful, because those syntactic constituent units have 
                                                          
3
 Most of the confidence estimation measures are for unigrams 
(word level measures). 
less dependency to the whole sentence structure 
and can be translated independently.  Our classifi-
cation relies on syntactic features that are impor-
tant source of information about the MT difficulty 
and also are useful for further error tracking (rea-
sons behind the difficulty).  Our classification is 
performed as a pre-translation step, so it does not 
rely on the output of the MT system for a test sen-
tence; instead, it uses a parallel training corpus and 
the characteristics of the underlying MT system 
(e.g.: phrase translations, lexical probabilities).   
Confidence measures have been used for error 
correction and interactive MT systems. Ueffing 
and Ney (2005) employed confidence measures 
within a trans-type-style interactive MT system.  In 
their system, the MT system iteratively generates 
the translation and the human translator accepts a 
part of the proposed translation by typing one or 
more prefix characters.  The system regenerates a 
new translation based on the human prefix input 
and word level confidence measures.  In contrast, 
our proposed usage of human knowledge is for 
translation at the phrase level.  We use syntactic 
restrictions to make the extracted phrases meaning-
ful and easy to translate in isolation.  In other 
words, by the usage of our framework trans-type 
systems can use human knowledge at the phrase 
level for the most difficult segments of a sentence.  
Additionally by the usage of our framework, the 
MT system performs the decoding task only once.   
The idea of isolated phrase translation has been 
explored successfully in MT community.  Koehn 
and Knight (2003) used isolated translation of NP 
and PP phrases and merge them with the phrase 
based MT system to translate the complete sen-
tence.  In our work, instead of focusing on specific 
type of phrases (NP or PP), we focus on isolated 
translation of difficult phrases with an aim to im-
prove the translation quality of non-difficult seg-
ments too. 
6 Conclusion and Future Work  
We have presented an MT framework that makes 
use of additional information about difficult-to-
translate source phrases.  Our framework includes 
an SVM-based phrase classifier that finds the seg-
ment of a sentence that is most difficult to trans-
late.  Our classifier achieves a promising 71.5% 
accuracy. By asking external sources (such as hu-
man translators) to pre-translate these DTPs and 
using them to constrain the MT process, we im-
254
prove the system outputs for the other parts of the 
sentences.  
We plan to extend this work in several direc-
tions.  First, our framework can be augmented to 
include multiple MT systems. We expect different 
systems will have difficulties with different con-
structs, and thus they may support each other, and 
thus reducing the need to ask human translators for 
help with the difficult phrases. Second, our current 
metric for phrasal difficulty depends on BLEU.  
Considering the recent debates about the shortcom-
ings of the BLEU score (Callison-Burch et. al. 
2006), we are interested in applying alternative 
metrics such a Meteor (Banerjee and Lavie 2005).  
Third, we believe that there is more room for im-
provement and extension of our classification fea-
tures.  Specifically, we believe that our syntactic 
analysis of source sentences can be improved by 
including richer parsing features.  Finally, the 
framework can also be used to diagnose recurring 
problems in the MT system.  We are currently de-
veloping methods for improving the translation of 
the difficult phrases for the phrase-based MT sys-
tem used in our experiments. 
Acknowledgements  
This work is supported by NSF Grant IIS-0612791. 
We would like to thank Alon Lavie, Mihai Rotaru 
and the NLP group at Pitt as well as the anony-
mous reviewers for their valuable comments. 
References  
Satanjeev Banerjee, Alon Lavie. 2005. METEOR: An 
automatic metric for MT evaluation with improved 
correlation with human judgments. In Proceedings of 
the ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for MT and/or Summarization, pages 
65?72. 
Daniel M. Bikel. 2002. Design of a multi-lingual, paral-
lel-processing statistical parsing engine. In Proceed-
ings of ARPA Workshop on Human Language Tech-
nology 
John Blatz, Erin Fitzgerald, George Foster, Simona Gan 
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, 
and Nicola Ueffing. 2003. Confidence estimation for 
machine translation. Technical report, Center for 
Language and Speech Processing, Johns Hopkins 
University, Baltimore. Summer Workshop Final Re-
port. 
Chris Callison-Burch, Miles Osborne, and Philip 
Koehn. 2006. Re-evaluating the Role of Bleu in Ma-
chine Translation Research. In Proc. of the European 
Chapter of the Association for Computational Lin-
guistics (EACL), Trento, Italy. 
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004. 
Automatic tagging of Arabic text: From raw text to 
base phrase chunks. In Proceeding of NAACL-HLT 
2004. Boston, MA. 
Thorsten Joachims, Making large-Scale SVM Learning 
Practical. Advances in Kernel Methods - Support 
Vector Learning, B. Sch?lkopf and C. Burges and A. 
Smola (ed.), MIT-Press, 1999. 
Philipp Koehn. 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the As-
sociation for Machine Translation in the Americas, 
pages 115?124 
Philipp Koehn and Kevin Knight. 2003. Feature-rich 
statistical translation of noun phrases. In Proceedings 
of 41st  the Annual Meeting on Association for Com-
putational Linguistics (ACL-2003), pages 311?318. 
Franz Och, 2001, ?Giza++: Training of statistical trans-
lation model?:  http://www.fjoch.com/GIZA++.html 
Franz. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1):19?51. 
Kishore Papineni and Salim Roukos and Todd Ward 
and Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation.  In Proceed-
ings of the 40th Annual Meeting on Association for 
Computational Linguistics (ACL-2002), Pages 311-
318, Philadelphia, PA 
Nicola Ueffing and Hermann Ney. 2005. Application of 
word-level confidence measures in translation. In 
Proceedings of the conference of the European Asso-
ciation of Machine Translation (EAMT 2005) , pages 
262?270, Budapest, Hungary 
Richard Zens and Hermann Ney, 2006. N -Gram Poste-
rior Probabilities for Statistical Machine Translation. 
In Proceedings of ACL Workshop on Statistical Ma-
chine Translation. 2006 
 
255
Proceedings of the Third Workshop on Statistical Machine Translation, pages 187?190,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The Role of Pseudo References in MT Evaluation
Joshua S. Albrecht and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{jsa8,hwa}@cs.pitt.edu
Abstract
Previous studies have shown automatic evalu-
ation metrics to be more reliable when com-
pared against many human translations. How-
ever, multiple human references may not al-
ways be available. It is more common to have
only a single human reference (extracted from
parallel texts) or no reference at all. Our ear-
lier work suggested that one way to address
this problem is to train a metric to evaluate a
sentence by comparing it against pseudo refer-
ences, or imperfect ?references? produced by
off-the-shelf MT systems. In this paper, we
further examine the approach both in terms of
the training methodology and in terms of the
role of the human and pseudo references. Our
expanded experiments show that the approach
generalizes well across multiple years and dif-
ferent source languages.
1 Introduction
Standard automatic metrics are reference-based;
that is, they compare system-produced translations
against human-translated references produced for
the same source. Since there is usually no single
best way to translate a sentence, each MT output
should be compared against many references. On
the other hand, creating multiple human references
is itself a costly process. For many naturally occur-
ring datasets (e.g., parallel corpora) only a single ref-
erence is readily available.
The focus of this work is on developing auto-
matic metrics for sentence-level evaluation with at
most one human reference. One way to supple-
ment the single human reference is to use pseudo
references, or sentences produced by off-the-shelf
MT systems, as stand-ins for human references.
However, since pseudo references may be imperfect
translations themselves, the comparisons cannot be
fully trusted. Previously, we have taken a learning-
based approach to develop a composite metric that
combines measurements taken from multiple pseudo
references (Albrecht and Hwa, 2007). Experimental
results suggested the approach to be promising; but
those studies did not consider how well the metric
might generalize across multiple years and different
languages. In this paper, we investigate the appli-
cability of the pseudo-reference metrics under these
more general conditions.
Using the WMT06 Workshop shared-task re-
sults (Koehn and Monz, 2006) as training exam-
ples, we train a metric that evaluates new sentences
by comparing them against pseudo references pro-
duced by three off-the-shelf MT systems. We ap-
ply the learned metric to sentences from the WMT07
shared-task (Callison-Burch et al, 2007b) and com-
pare the metric?s predictions against human judg-
ments. We find that additional pseudo references
improve correlations for automatic metrics.
2 Background
The ideal evaluation metric reports an accurate dis-
tance between an input instance and its gold stan-
dard, but even when comparing against imperfect
standards, the measured distances may still convey
some useful information ? they may help to trian-
gulate the input?s position relative to the true gold
standard.
In the context of sentence-level MT evaluations,
187
the challenges are two-fold. First, the ideal quantita-
tive distance function between a translation hypoth-
esis and the proper translations is not known; cur-
rent automatic evaluation metrics produce approxi-
mations to the true translational distance. Second,
although we may know the qualitative goodness of
the MT systems that generate the pseudo references,
we do not know how imperfect the pseudo refer-
ences are. These uncertainties make it harder to es-
tablish the true distance between the input hypoth-
esis and the (unobserved) acceptable gold standard
translations.
In order to combine evidence from these uncertain
observations, we take a learning-based approach.
Each hypothesis sentence is compared with multi-
ple pseudo references using multiple metrics. Rep-
resenting the measurements as a set of input features
and using human-assessed MT sentences as training
examples, we train a function that is optimized to
correlate the features with the human assessments in
the training examples. Specifically, for each input
sentence, we compute a set of 18 kinds of reference-
based measurements for each pseudo reference as
well as 26 monolingual fluency measurements. The
full set of measurements then serves as the input fea-
ture vector into the function, which is trained via
support vector regression. The learned function can
then be used as an evaluation metric itself: it takes
the measurements of a new sentence as input and re-
turns a composite score for that sentence.
The approach is considered successful if the met-
ric?s predictions on new test sentences correlate well
with quantitative human assessments. Like other
learned models, the metric is expected to perform
better on data that are more similar to the training
instances. Therefore, a natural question that arises
with a metric developed in this manner is: how well
does it generalize?
3 Research Questions
To better understand the capability of metrics that
compare against pseudo-references, we consider the
following aspects:
The role of learning Standard reference-based
metrics can also use pseudo references; however,
they would treat the imperfect references as gold
standard. In contrast, the learning process aims
to determine how much each comparison with a
pseudo reference might be trusted. To observe the
role of learning, we compare trained metrics against
standard reference-based metrics, all using pseudo
references.
The amount vs. types of training data The suc-
cess of any learned model depends on its training ex-
periences. We study the trade-off between the size
of the training set and the specificity of the train-
ing data. We perform experiments comparing a met-
ric trained from a large pool of heterogeneous train-
ing examples that include translated sentences from
multiple languages and individual metrics trained
from particular source languages.
The role of a single human reference Previous
studies have shown the importance of comparing
against multiple references. The approach in this
paper attempts to approximate multiple human ref-
erences with machine-produced sentences. Is a sin-
gle trust-worthy translation more useful than multi-
ple imperfect translations? To answer this question,
we compare three different reference settings: using
just a single human reference, using just the three
pseudo references, and using all four references.
4 Experimental Setup
For the experiments reported in this paper, we used
human-evaluated MT sentences from past shared-
tasks of the WMT 2006 and WMT 2007. The data
consists of outputs from German-English, Spanish-
English, and French-English MT systems. The out-
puts are translations from two corpora: Europarl and
news commentary. System outputs have been evalu-
ated by human judges on a 5-point scale (Callison-
Burch et al, 2007a). We have normalized scores
to reduce biases from different judges (Blatz et al,
2003).
We experimented with using four different sub-
sets of the WMT2006 data as training examples:
only German-English, only Spanish-English, only
French-English, all 06 data. The metrics are trained
using support vector regression with a Gaussian
kernel as implemented in the SVM-Light package
(Joachims, 1999). The SVM parameters are tuned
via grid-search on development data, 20% of the full
training set that has been reserved for this purpose.
188
We used three MT systems to generate pseudo ref-
erences: Systran1, GoogleMT 2, and Moses (Koehn
et al, 2007). We chose these three systems because
they are widely accessible and because they take
relatively different approaches. Moreover, although
they have not all been human-evaluated in the past
WMT shared tasks, they are well-known for produc-
ing good translations.
A metric is evaluated based on its Spearman rank
correlation coefficient between the scores it gave to
the evaluative dataset and human assessments for
the same data. The correlation coefficient is a real
number between -1, indicating perfect negative cor-
relations, and +1, indicating perfect positive correla-
tions.
Two standard reference-based metrics, BLEU
(Papineni et al, 2002) and METEOR (Banerjee and
Lavie, 2005), are used for comparisons. BLEU is
smoothed (Lin and Och, 2004), and it considers only
matching up to bigrams because this has higher cor-
relations with human judgments than when higher-
ordered n-grams are included.
5 Results
The full experimental comparisons are summarized
in Table 1. Each cell shows the correlation coef-
ficient between the human judgments and a metric
(column) that uses a particular kind of references
(row) for some evaluation data set (block row).
The role of learning With the exception of the
German-English data, the learned metrics had higher
correlations with human judges than the baselines,
which used standard metrics with a single human
reference. On the other hand, results suggest that
pseudo references often also improve correlations
for standard metrics. This may seem counter-
intuitive because we can easily think of cases in
which pseudo references hurt standard metrics (e.g.,
use poor outputs as pseudo references). We hypoth-
1Available from http://www.systransoft.com/.
We note that Systran is also a participating system under eval-
uation. Although Sys-Test will be deemed to be identical to
Sys-Ref, it will not automatically receive a high score because
the measurement is weighted by whether Sys-Ref was reliable
during training. Furthermore, measurements between Sys-Test
and other pseudo-references will provide alternative evidences
for the metric to consider.
2http://www.google.com/language tools/
esize that because the pseudo references came from
high-quality MT systems and because standard met-
rics are based on simple word matches, the chances
for bad judgments (input words matched against
pseudo reference, but both are wrong) are relatively
small compared to chances for good judgments. We
further hypothesize that the learned metrics would
be robust against the qualities of the pseudo refer-
ence MT systems.
The amount vs. types of training data Com-
paring the three metrics trained from single lan-
guage datasets against the metric trained from all
of WMT06 dataset, we see that the learning process
benefitted from the larger quantity of training exam-
ples. It may be the case that the MT systems for the
three language pairs are at a similar stage of maturity
such that the training instances are mutually helpful.
The role of a single human reference Our results
reinforce previous findings that metrics are more re-
liable when they have access to more than a sin-
gle human reference. Our experimental data sug-
gests that a single human reference often may not be
as reliable as using three pseudo references alone.
Finally, the best correlations are achieved by using
both human and pseudo references.
6 Conclusion
We have presented an empirical study on automatic
metrics for sentence-level MT evaluation with at
most one human reference. We show that pseudo
references from off-the-shelf MT systems can be
used to augment the single human reference. Be-
cause they are imperfect, it is important to weigh the
trustworthiness of these references through a train-
ing phase. The metric seems robust even when the
applied to sentences from different systems of a later
year. These results suggest that multiple imperfect
translations make informative comparison points in
supplement to human references.
Acknowledgments
This work has been supported by NSF Grants IIS-
0612791.
189
Eval. Data Ref Type METEOR BLEU SVM(de06) SVM(es06) SVM(fr06) SVM(wmt06)
de 1HR 0.458 0.471
europarl 3PR 0.521* 0.527* 0.422 0.403 0.480* 0.467
07 1HR+3PR 0.535* 0.547* 0.471 0.480* 0.477* 0.523*
de 1HR 0.290 0.333
news 3PR 0.400* 0.400* 0.262 0.279 0.261 0.261
07 1HR+3PR 0.432* 0.417* 0.298 0.321 0.269 0.330
es 1HR 0.377 0.412
europarl 3PR 0.453* 0.483* 0.336 0.453* 0.432* 0.456*
07 1HR+3PR 0.491* 0.503* 0.405 0.513* 0.483* 0.510*
es 1HR 0.317 0.332
news 3PR 0.320 0.317 0.393* 0.381* 0.426* 0.426*
07 1HR+3PR 0.353* 0.325 0.429* 0.427* 0.380* 0.486*
fr 1HR 0.265 0.246
europarl 3PR 0.196 0.285* 0.270* 0.284* 0.355* 0.366*
07 1HR+3PR 0.221 0.290* 0.277* 0.324* 0.304* 0.381*
fr 1HR 0.226 0.280
news 3PR 0.356* 0.383* 0.237 0.252 0.355* 0.373*
07 1HR+3PR 0.374* 0.394* 0.272 0.339* 0.319* 0.388*
Table 1: Correlation comparisons of metrics (columns) using different references (row): a single human reference
(1HR), 3 pseudo references (3PR), or all (1HR+3PR). The type of training used for the regression-trained metrics
are specified in parentheses. For each evaluated corpus, correlations higher than standard metric using one human
reference are marked by an asterisk(*).
References
Joshua S. Albrecht and Rebecca Hwa. 2007. Regression
for sentence-level MT evaluation with pseudo refer-
ences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007).
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summarization,
June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation
for machine translation. Technical Report Natural
Language Engineering Workshop Final Report, Johns
Hopkins University.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007a. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Cameron Shaw
Fordyce, and Christof Monz, editors. 2007b. Proceed-
ings of the Second Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics, Prague, Czech Republic, June.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?elkopf, Christo-
pher Burges, and Alexander Smola, editors, Advances
in Kernel Methods - Support Vector Learning. MIT
Press.
Philipp Koehn and Christof Monz, editors. 2006. Pro-
ceedings on the Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics, New York City, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. Proceedings
of ACL, Demonstration Session.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING 2004), August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, Philadelphia, PA.
190
331
332
333
334
335
336
337
338
Example Selection for Bootstrapping Statistical Parsers
Mark Steedman?, Rebecca Hwa?, Stephen Clark?, Miles Osborne?, Anoop Sarkar?
Julia Hockenmaier?, Paul Ruhlen? Steven Baker?, Jeremiah Crim?
?School of Informatics, University of Edinburgh
{steedman,stephenc,julia,osborne}@cogsci.ed.ac.uk
?Institute for Advanced Computer Studies, University of Maryland
hwa@umiacs.umd.edu
?School of Computing Science, Simon Fraser University
anoop@cs.sfu.ca
?Center for Language and Speech Processing, Johns Hopkins University
jcrim@jhu.edu,ruhlen@cs.jhu.edu
?Department of Computer Science, Cornell University
sdb22@cornell.edu
Abstract
This paper investigates bootstrapping for statis-
tical parsers to reduce their reliance on manu-
ally annotated training data. We consider both
a mostly-unsupervised approach, co-training,
in which two parsers are iteratively re-trained
on each other?s output; and a semi-supervised
approach, corrected co-training, in which a
human corrects each parser?s output before
adding it to the training data. The selection of
labeled training examples is an integral part of
both frameworks. We propose several selection
methods based on the criteria of minimizing er-
rors in the data and maximizing training util-
ity. We show that incorporating the utility cri-
terion into the selection method results in better
parsers for both frameworks.
1 Introduction
Current state-of-the-art statistical parsers (Collins, 1999;
Charniak, 2000) are trained on large annotated corpora
such as the Penn Treebank (Marcus et al, 1993). How-
ever, the production of such corpora is expensive and
labor-intensive. Given this bottleneck, there is consider-
able interest in (partially) automating the annotation pro-
cess.
To overcome this bottleneck, two approaches from ma-
chine learning have been applied to training parsers. One
is sample selection (Thompson et al, 1999; Hwa, 2000;
Tang et al, 2002), a variant of active learning (Cohn et al,
1994), which tries to identify a small set of unlabeled sen-
tences with high training utility for the human to label1.
Sentences with high training utility are those most likely
to improve the parser. The other approach, and the fo-
cus of this paper, is co-training (Sarkar, 2001), a mostly-
unsupervised algorithm that replaces the human by hav-
ing two (or more) parsers label training examples for each
other. The goal is for both parsers to improve by boot-
strapping off each other?s strengths. Because the parsers
may label examples incorrectly, only a subset of their out-
put, chosen by some selection mechanism, is used in or-
der to minimize errors. The choice of selection method
significantly affects the quality of the resulting parsers.
We investigate a novel approach of selecting training
examples for co-training parsers by incorporating the idea
of maximizing training utility from sample selection. The
selection mechanism is integral to both sample selection
and co-training; however, because co-training and sam-
ple selection have different goals, their selection methods
focus on different criteria: co-training typically favors se-
lecting accurately labeled examples, while sample selec-
tion typically favors selecting examples with high train-
ing utility, which often are not sentences that the parsers
already label accurately. In this work, we investigate se-
lection methods for co-training that explore the trade-off
between maximizing training utility and minimizing er-
rors.
Empirical studies were conducted to compare selection
methods under both co-training and a semi-supervised
framework called corrected co-training (Pierce and
Cardie, 2001), in which the selected examples are man-
ually checked and corrected before being added to the
1In the context of training parsers, a labeled example is a
sentence with its parse tree. Throughout this paper, we use the
term ?label? and ?parse? interchangeably.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 157-164
                                                         Proceedings of HLT-NAACL 2003
training data. For co-training, we show that the benefit of
selecting examples with high training utility can offset the
additional errors they contain. For corrected co-training,
we show that selecting examples with high training util-
ity reduces the number of sentences the human annotator
has to check. For both frameworks, we show that selec-
tion methods that maximize training utility find labeled
examples that result in better trained parsers than those
that only minimize error.
2 Co-training
Blum and Mitchell (1998) introduced co-training to
bootstrap two classifiers with different views of the data.
The two classifiers are initially trained on a small amount
of annotated seed data; then they label unannotated data
for each other in an iterative training process. Blum and
Mitchell prove that, when the two views are conditionally
independent given the label, and each view is sufficient
for learning the task, co-training can boost an initial
weak learner using unlabeled data.
The theory underlying co-training has been extended
by Dasgupta et al (2002) to prove that, by maximizing
their agreement over the unlabeled data, the two learn-
ers make few generalization errors (under the same in-
dependence assumption adopted by Blum and Mitchell).
Abney (2002) argues that this assumption is extremely
strong and typically violated in the data, and he proposes
a weaker independence assumption.
Goldman and Zhou (2000) show that, through care-
ful selection of newly labeled examples, co-training can
work even when the classifiers? views do not satisfy
the independence assumption. In this paper we investi-
gate methods for selecting labeled examples produced by
two statistical parsers. We do not explicitly maximize
agreement (along the lines of Abney?s algorithm (2002))
because it is too computationally intensive for training
parsers.
The pseudocode for our co-training framework is given
in Figure 1. It consists of two different parsers and a cen-
tral control that interfaces between the two parsers and
the data. At each co-training iteration, a small set of sen-
tences is drawn from a large pool of unlabeled sentences
and stored in a cache. Both parsers then attempt to label
every sentence in the cache. Next, a subset of the newly
labeled sentences is selected to be added to the train-
ing data. The examples added to the training set of one
parser (referred to as the student) are only those produced
by the other parser (referred to as the teacher), although
the methods we use generalize to the case in which the
parsers share a single training set. During selection, one
parser first acts as the teacher and the other as the student,
and then the roles are reversed.
A and B are two different parsers.
M iA and M iB are the models of A and B at step i.
U is a large pool of unlabeled sentences.
U i is a small cache holding a subset of U at step i.
L is the manually labeled seed data.
LiA and LiB are the labeled training examples for A and B
at step i.
Initialize:
L0A ? L
0
B ? L.
M0A ? Train(A,L
0
A)
M0B ? Train(B,L
0
B)
Loop:
U i ? Add unlabeled sentences from U .
M iA and M iB parse the sentences in U i and
assign scores to them according to their scoring
functions fA and fB .
Select new parses {PA} and {PB} according to some
selection method S, which uses the scores
from fA and fB .
Li+1A is L
i
A augmented with {PB}
Li+1B is L
i
B augmented with {PA}
M i+1A ? Train(A,L
i+1
A )
M i+1B ? Train(B,L
i+1
B )
Figure 1: The pseudo-code for the co-training algorithm
3 Selecting Training Examples
In each iteration, selection is performed in two steps.
First, each parser uses some scoring function, f , to assess
the parses it generated for the sentences in the cache.2
Second, the central control uses some selection method,
S, to choose a subset of these labeled sentences (based on
the scores assigned by f ) to add to the parsers? training
data. The focus of this paper is on the selection phase, but
to more fully investigate the effect of different selection
methods we also consider two possible scoring functions.
3.1 Scoring functions
The scoring function attempts to quantify the correctness
of the parses produced by each parser. An ideal scor-
ing function would give the true accuracy rates (e.g., F-
score, the combined labeled precision and recall rates).
In practice, accuracy is approximated by some notion
of confidence. For example, one easy-to-compute scor-
ing function measures the conditional probability of the
(most likely) parse. If a high probability is assigned, the
parser is said to be confident in the label it produced.
In our experimental studies, we considered the selec-
tion methods? interaction with two scoring functions: an
oracle scoring function fF-score that returns the F-score
of the parse as measured against a gold standard, and a
2In our experiments, both parsers use the same scoring func-
tion.
practical scoring function fprob that returns the condi-
tional probability of the parse.3
3.2 Selection methods
Based on the scores assigned by the scoring function,
the selection method chooses a subset of the parser la-
beled sentences that best satisfy some selection criteria.
One such criterion is the accuracy of the labeled exam-
ples, which may be estimated by the teacher parser?s con-
fidence in its labels. However, the examples that the
teacher correctly labeled may not be those that the stu-
dent needs. We hypothesize that the training utility of
the examples for the student parser is another important
criterion.
Training utility measures the improvement a parser
would make if that sentence were correctly labeled and
added to the training set. Like accuracy, the utility of
an unlabeled sentence is difficult to quantify; therefore,
we approximate it with values that can be computed from
features of the sentence. For example, sentences contain-
ing many unknown words may have high training util-
ity; so might sentences that a parser has trouble parsing.
Under the co-training framework, we estimate the train-
ing utility of a sentence for the student by comparing the
score the student assigned to its parse (according to its
scoring function) against the score the teacher assigned
to its own parse.
To investigate how the selection criteria of utility and
accuracy affect the co-training process, we considered a
number of selection methods that satisfy the requirements
of accuracy and training utility to varying degrees. The
different selection methods are shown below. For each
method, a sentence (as labeled by the teacher parser) is
selected if:
? above-n (Sabove-n): the score of the teacher?s parse
(using its scoring function) ? n.
? difference (Sdiff-n): the score of the teacher?s parse
is greater than the score of the student?s parse by
some threshold n.
? intersection (Sint-n): the score of the teacher?s parse
is in the set of the teacher?s n percent highest-
scoring labeled sentences, and the score of the stu-
dent?s parse for the same sentence is in the set of
the student?s n percent lowest-scoring labeled sen-
tences.
Each selection method has a control parameter, n, that
determines the number of labeled sentences to add at each
co-training iteration. It also serves as an indirect control
3A nice property of using conditional probability,
Pr(parse|sentence), as the scoring function is that it
normalizes for sentence length.
of the number of errors added to the training set. For ex-
ample, the Sabove-n method would allow more sentences
to be selected if n was set to a low value (with respect to
the scoring function); however, this is likely to reduce the
accuracy rate of the training set.
The above-n method attempts to maximize the accu-
racy of the data (assuming that parses with higher scores
are more accurate). The difference method attempts to
maximize training utility: as long as the teacher?s label-
ing is more accurate than that of the student, it is cho-
sen, even if its absolute accuracy rate is low. The inter-
section method attempts to maximize both: the selected
sentences are accurately labeled by the teacher and incor-
rectly labeled by the student.
4 Experiments
Experiments were performed to compare the effect of
the selection methods on co-training and corrected co-
training. We consider a selection method, S1, superior
to another, S2, if, when a large unlabeled pool of sen-
tences has been exhausted, the examples selected by S1
(as labeled by the machine, and possibly corrected by the
human) improve the parser more than those selected by
S2. All experiments shared the same general setup, as
described below.
4.1 Experimental Setup
For two parsers to co-train, they should generate com-
parable output but use independent statistical models.
In our experiments, we used a lexicalized context free
grammar parser developed by Collins (1999), and a lex-
icalized Tree Adjoining Grammar parser developed by
Sarkar (2002). Both parsers were initialized with some
seed data. Since the goal is to minimize human annotated
data, the size of the seed data should be small. In this pa-
per we used a seed set size of 1, 000 sentences, taken from
section 2 of the Wall Street Journal (WSJ) Penn Tree-
bank. The total pool of unlabeled sentences was the re-
mainder of sections 2-21 (stripped of their annotations),
consisting of about 38,000 sentences. The cache size is
set at 500 sentences. We have explored using different
settings for the seed set size (Steedman et al, 2003).
The parsers were evaluated on unseen test sentences
(section 23 of the WSJ corpus). Section 0 was used as
a development set for determining parameters. The eval-
uation metric is the Parseval F-score over labeled con-
stituents: F-score = 2?LR?LPLR+LP , where LP and LR
are labeled precision and recall rate, respectively. Both
parsers were evaluated, but for brevity, all results reported
here are for the Collins parser, which received higher Par-
seval scores.
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-10%int-60%No selection(Human annotated)
80
80.5
81
81.5
82
82.5
83
83.5
84
0 2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection(Human annotated)
(a) (b)
Figure 2: A comparison of selection methods using the oracle scoring function, fF-score, controlling for the label
quality of the training data. (a) The average accuracy rates are about 85%. (b) The average accuracy rates (except for
those selected by Sdiff-10%) are about 95%.
4.2 Experiment 1: Selection Methods and
Co-Training
We first examine the effect of the three selection meth-
ods on co-training without correction (i.e., the chosen
machine-labeled training examples may contain errors).
Because the selection decisions are based on the scores
that the parsers assign to their outputs, the reliability of
the scoring function has a significant impact on the per-
formance of the selection methods. We evaluate the ef-
fectiveness of the selection methods using two scoring
functions. In Section 4.2.1, each parser assesses its out-
put with an oracle scoring function that returns the Par-
seval F-score of the output (as compared to the human
annotated gold-standard). This is an idealized condition
that gives us direct control over the error rate of the la-
beled training data. By keeping the error rates constant,
our goal is to determine which selection method is more
successful in finding sentences with high training utility.
In Section 4.2.2 we replace the oracle scoring function
with fprob, which returns the conditional probability of
the best parse as the score. We compare how the selection
methods? performances degrade under the realistic con-
dition of basing selection decisions on unreliable parser
output assessment scores.
4.2.1 Using the oracle scoring function, fF-score
The goal of this experiment is to evaluate the selection
methods using a reliable scoring function. We therefore
use an oracle scoring function, fF-score, which guaran-
tees a perfect assessment of the parser?s output. This,
however, may be too powerful. In practice, we expect
even a reliable scoring function to sometimes assign high
scores to inaccurate parses. We account for this effect by
adjusting the selection method?s control parameter to af-
fect two factors: the accuracy rate of the newly labeled
training data, and the number of labeled sentences added
at each training iteration. A relaxed parameter setting
adds more parses to the training data, but also reduces
the accuracy of the training data.
Figure 2 compares the effect of the three selection
methods on co-training for the relaxed (left graph) and
the strict (right graph) parameter settings. Each curve in
the two graphs charts the improvement in the parser?s ac-
curacy in parsing the test sentences (y-axis) as it is trained
on more data chosen by its selection method (x-axis).
The curves have different endpoints because the selection
methods chose a different number of sentences from the
same 38K unlabeled pool. For reference, we also plotted
the improvement of a fully-supervised parser (i.e., trained
on human-annotated data, with no selection).
For the more relaxed setting, the parameters are chosen
so that the newly labeled training data have an average
accuracy rate of about 85%:
? Sabove-70% requires the labels to have an F-score ?
70%. It adds about 330 labeled sentences (out of the
500 sentence cache) with an average accuracy rate
of 85% to the training data per iteration.
? Sdiff-10% requires the score difference between the
teacher?s labeling and the student?s labeling to be at
least 10%. It adds about 50 labeled sentences with
an average accuracy rate of 80%.
? Sint-60% requires the teacher?s parse to be in the
top 60% of its output and the student?s parse for the
same sentence to be in its bottom 60%. It adds about
150 labeled sentences with an average accuracy rate
of 85%.
Although none rivals the parser trained on human an-
notated data, the selection method that improves the
parser the most is Sdiff-10%. One interpretation is that
the training utility of the examples chosen by Sdiff-10%
outweighs the cost of errors introduced into the training
data. Another interpretation is that the other two selection
methods let in too many sentences containing errors. In
the right graph, we compare the same Sdiff-10% with the
other two selection methods using stricter control, such
that the average accuracy rate for these methods is now
about 95%:
? Sabove-90% now requires the parses to be at least
90% correct. It adds about 150 labeled sentences
per iteration.
? Sint-30% now requires the teacher?s parse to be in
the top 30% of its output and the student?s parse for
the same sentence in its bottom 30%. It adds about
15 labeled sentences.
The stricter control on Sabove-90% improved the
parser?s performance, but not enough to overtake
Sdiff-10% after all the sentences in the unlabeled pool
had been considered, even though the training data of
Sdiff-10% contained many more errors. Sint-30% has a
faster initial improvement4, closely tracking the progress
of the fully-supervised parser. However, the stringent re-
quirement exhausted the unlabeled data pool before train-
ing the parser to convergence. Sint-30% might continue
to help the parser to improve if it had access to more un-
labeled data, which is easier to acquire than annotated
data5.
Comparing the three selection methods under both
strict and relaxed control settings, the results suggest that
training utility is an important criterion in selecting train-
ing examples, even at the cost of reduced accuracy.
4.2.2 Using the fprob scoring function
To determine the effect of unreliable scores on the se-
lection methods, we replace the oracle scoring function,
fF-score, with fprob, which approximates the accuracy
of a parse with its conditional probability. Although this
is a poor estimate of accuracy (especially when computed
from a partially trained parser), it is very easy to compute.
The unreliable scores also reduce the correlation between
the selection control parameters and the level of errors in
the training data. In this experiment, we set the parame-
ters for all three selection methods so that approximately
4A fast improvement rate is not a central concern here, but
it will be more relevant for corrected co-training.
5This oracle experiment is bounded by the size of the anno-
tated portion of the WSJ corpus.
79.8
80
80.2
80.4
80.6
80.8
81
81.2
1000 1500 2000 2500 3000 3500 4000 4500 5000
Par
sin
g A
ccu
rac
y o
n T
est
 Da
ta (F
scor
e)
Number of Training Sentences
above-70%diff-30%int-30%
Figure 3: A comparison of selection methods using the
conditional probability scoring function, fprob.
30-50 sentences were added to the training data per iter-
ation. The average accuracy rate of the training data for
Sabove-70% was about 85%, and the rate for Sdiff-30%
and Sint-30% was about 75%.
As expected, the parser performances of all three selec-
tion methods using fprob (shown in Figure 3) are lower
than using fF-score (see Figure 2). However, Sdiff-30%
and Sint-30% helped the co-training parsers to improve
with a 5% error reduction (1% absolute difference) over
the parser trained only on the initial seed data. In con-
trast, despite an initial improvement, using Sabove-70%
did not help to improve the parser. In their experiments on
NP identifiers, Pierce and Cardie (2001) observed a sim-
ilar effect. They hypothesize that co-training does not
scale well for natural language learning tasks that require
a huge amount of training data because too many errors
are accrued over time. Our experimental results suggest
that the use of training utility in the selection process can
make co-training parsers more tolerant to these accumu-
lated errors.
4.3 Experiment 2: Selection Methods and
Corrected Co-training
To address the problem of the training data accumulating
too many errors over time, Pierce and Cardie proposed
a semi-supervised variant of co-training called corrected
co-training, which allows a human annotator to review
and correct the output of the parsers before adding it to
the training data. The main selection criterion in their
co-training system is accuracy (approximated by confi-
dence). They argue that selecting examples with nearly
correct labels would require few manual interventions
from the annotator.
We hypothesize that it may be beneficial to consider
the training utility criterion in this framework as well.
We perform experiments to determine whether select-
ing fewer (and possibly less accurately labeled) exam-
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-90%diff-10%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-90%diff-10%int-30%No selection
(a) (b)
Figure 4: A comparison of selection methods for corrected co-training using fF-score (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
ples with higher training utility would require less effort
from the annotator. In our experiments, we simulated
the interactive sample selection process by revealing the
gold standard. As before, we compare the three selection
methods using both fF-score and fprob as scoring func-
tions.6
4.3.1 Using the oracle scoring function, fF-score
Figure 4 shows the effect of the three selection meth-
ods (using the strict parameter setting) on corrected co-
training. As a point of reference, we plot the improve-
ment rate for a fully supervised parser (same as the one
in Figure 2). In addition to charting the parser?s perfor-
mance in terms of the number of labeled training sen-
tences (left graph), we also chart the parser?s performance
in terms of the the number of constituents the machine
mislabeled (right graph). The pair of graphs indicates the
amount of human effort required: the left graph shows
the number of sentences the human has to check, and the
right graph shows the number of constituents the human
has to correct.
Comparing Sabove-90% and Sdiff-10%, we see that
Sdiff-10% trains a better parser than Sabove-90% when all
the unlabeled sentences have been considered. It also im-
proves the parser using a smaller set of training exam-
ples. Thus, for the same parsing performance, it requires
the human to check fewer sentences than Sabove-90% and
the reference case of no selection (Figure 4(a)). On the
other hand, because the labeled sentences selected by
Sdiff-10% contain more mistakes than those selected by
Sabove-90%, Sdiff-10% requires slightly more corrections
6The selection control parameters are the same as the previ-
ous set of experiments, using the strict setting (i.e., Figure 2(b))
for fF-score.
than Sabove-90% for the same level of parsing perfor-
mance; though both require fewer corrections than the
reference case of no selection (Figure 4(b)). Because
the amount of effort spent by the annotator depends on
the number of sentences checked as well as the amount
of corrections made, whether Sdiff-10% or Sabove-90% is
more effort reducing may be a matter of the annotator?s
preference.
The selection method that improves the parser at the
fastest rate is Sint-30%. For the same parser performance
level, it selects the fewest number of sentences for a hu-
man to check and requires the human to make the least
number of corrections. However, as we have seen in the
earlier experiment, very few sentences in the unlabeled
pool satisfy its stringent criteria, so it ran out of data be-
fore the parser was trained to convergence. At this point
we cannot determine whether Sint-30% might continue to
improve the parser if we used a larger set of unlabeled
data.
4.3.2 Using the fprob scoring function
We also consider the effect of unreliable scores in the
corrected co-training framework. A comparison between
the selection methods using fprob is reported in Figure
5. The left graph charts parser performance in terms of
the number of sentences the human must check; the right
charts parser performance in terms of the number of con-
stituents the human must correct. As expected, the unreli-
able scoring function degrades the effectiveness of the se-
lection methods; however, compared to its unsupervised
counterpart (Figure 3), the degradation is not as severe.
In fact, Sdiff-30% and Sint-30% still require fewer train-
ing data than the reference parser. Moreover, consistent
with the other experiments, the selection methods that at-
tempt to maximize training utility achieve better parsing
performance than Sabove-70%. Finally, in terms of reduc-
ing human effort, the three selection methods require the
human to correct comparable amount of parser errors for
the same level of parsing performance, but for Sdiff-30%
and Sint-30%, fewer sentences need to be checked.
4.3.3 Discussion
Corrected co-training can be seen as a form of active
learning, whose goal is to identify the smallest set of un-
labeled data with high training utility for the human to
label. Active learning can be applied to a single learner
(Lewis and Catlett, 1994) and to multiple learners (Fre-
und et al, 1997; Engelson and Dagan, 1996; Ngai and
Yarowsky, 2000). In the context of parsing, all previ-
ous work (Thompson et al, 1999; Hwa, 2000; Tang et
al., 2002) has focussed on single learners. Corrected co-
training is the first application of active learning for mul-
tiple parsers. We are currently investigating comparisons
to the single learner approaches.
Our approach is similar to co-testing (Muslea et al,
2002), an active learning technique that uses two classi-
fiers to find contentious examples (i.e., data for which the
classifiers? labels disagree) for a human to label. There is
a subtle but significant difference, however, in that their
goal is to reduce the total number of labeled training ex-
amples whereas we also wish to reduce the number of
corrections made by the human. Therefore, our selection
methods must take into account the quality of the parse
produced by the teacher in addition to how different its
parse is from the one produced by the student. The inter-
section method precisely aims at selecting sentences that
satisfy both requirements. Exploring different selection
methods is part of our on-going research effort.
5 Conclusion
We have considered three selection methods that have dif-
ferent priorities in balancing the two (often competing)
criteria of accuracy and training utility. We have em-
pirically compared their effect on co-training, in which
two parsers label data for each other, as well as corrected
co-training, in which a human corrects the parser labeled
data before adding it to the training set. Our results sug-
gest that training utility is an important selection criterion
to consider, even at the cost of potentially reducing the ac-
curacy of the training data. In our empirical studies, the
selection method that aims to maximize training utility,
Sdiff-n, consistently finds better examples than the one
that aims to maximize accuracy, Sabove-n. Our results
also suggest that the selection method that aims to maxi-
mize both accuracy and utility, Sint-n, shows promise in
improving co-training parsers and in reducing human ef-
fort for corrected co-training; however, a much larger un-
labeled data set is needed to verify the benefit of Sint-n.
The results of this study indicate the need for scor-
ing functions that are better estimates of the accuracy of
the parser?s output than conditional probabilities. Our
oracle experiments show that, by using effective selec-
tion methods, the co-training process can improve parser
peformance even when the newly labeled parses are
not completely accurate. This suggests that co-training
may still be beneficial when using a practical scoring
function that might only coarsely distinguish accurate
parses from inaccurate parses. Further avenues to ex-
plore include the development of selection methods to
efficiently approximate maximizing the objective func-
tion of parser agreement on unlabeled data, following the
work of Dasgupta et al (2002) and Abney (2002). Also,
co-training might be made more effective if partial parses
were used as training data. Finally, we are conducting ex-
periments to compare corrected co-training with other ac-
tive learning methods. We hope these studies will reveal
ways to combine the strengths of co-training and active
learning to make better use of unlabeled data.
Acknowledgments
This work has been supported, in part, by NSF/DARPA
funded 2002 Human Language Engineering Workshop
at JHU, EPSRC grant GR/M96889, the Department of
Defense contract RD-02-5700, and ONR MURI Con-
tract FCPO.810548265. We would like to thank Chris
Callison-Burch, Michael Collins, John Henderson, Lil-
lian Lee, Andrew McCallum, and Fernando Pereira for
helpful discussions; to Ric Crabbe, Adam Lopez, the par-
ticipants of CS775 at Cornell University, and the review-
ers for their comments on this paper.
References
Steven Abney. 2002. Bootstrapping. In Proceedings of the
40th Annual Meeting of the Association for Computational
Linguistics, pages 360?367, Philadelphia, PA.
Avrim Blum and Tom Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Proceedings of the
11th Annual Conference on Computational Learning Theory,
pages 92?100, Madison, WI.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Annual Meeting of the NAACL.
David Cohn, Les Atlas, and Richard Ladner. 1994. Improv-
ing generalization with active learning. Machine Learning,
15(2):201?221.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of Penn-
sylvania.
Sanjoy Dasgupta, Michael Littman, and David McAllester.
2002. PAC generalization bounds for co-training. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Advances
80
81
82
83
84
85
86
87
2000 4000 6000 8000 10000 12000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Training Sentences
above-70%diff-30%int-30%No selection
80
81
82
83
84
85
86
87
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Pa
rsin
g A
ccu
rac
y o
n T
est
 Da
ta (F
sco
re)
Number of Constituents to Correct in the Training Data
above-70%diff-30%int-30%No selection
(a) (b)
Figure 5: A comparison of selection methods for corrected co-training using fprob (a) in terms of the number of
sentences added to the training data; (b) in terms of the number of manually corrected constituents.
in Neural Information Processing Systems 14, Cambridge,
MA. MIT Press.
Sean P. Engelson and Ido Dagan. 1996. Minimizing manual
annotation cost in supervised training from copora. In Pro-
ceedings of the 34th Annual Meeting of the ACL, pages 319?
326.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and Naftali
Tishby. 1997. Selective sampling using the query by com-
mittee algorithm. Machine Learning, 28(2-3):133?168.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised
learning with unlabeled data. In Proceedings of the 17th In-
ternational Conference on Machine Learning, Stanford, CA.
Rebecca Hwa. 2000. Sample selection for statistical grammar
induction. In Proceedings of the 2000 Joint SIGDAT Confer-
ence on EMNLP and VLC, pages 45?52, Hong Kong, China,
October.
David D. Lewis and Jason Catlett. 1994. Heterogeneous un-
certainty sampling for supervised learning. In Proceedings
of the Eleventh International Conference on Machine Learn-
ing, pages 148?156.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Ion Muslea, Steve Minton, and Craig Knoblock. 2002. Selec-
tive sampling with redundant views. In Proceedings of the
Seventeenth National Conference on Artificial Intelligence,
pages 621?626.
Grace Ngai and David Yarowsky. 2000. Rule writing or an-
notation: Cost-efficient resource usage for base noun phrase
chunking. In Proceedings of the 38th Annual Meeting of the
ACL, pages 117?125, Hong Kong, China, October.
David Pierce and Claire Cardie. 2001. Limitations of co-
training for natural language learning from large datasets. In
Proceedings of the Empirical Methods in NLP Conference,
Pittsburgh, PA.
Anoop Sarkar. 2001. Applying co-training methods to statisti-
cal parsing. In Proceedings of the 2nd Annual Meeting of the
NAACL, pages 95?102, Pittsburgh, PA.
Anoop Sarkar. 2002. Statistical Parsing Algorithms for Lexi-
calized Tree Adjoining Grammars. Ph.D. thesis, University
of Pennsylvania.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen Clark,
Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen, Steven
Baker, and Jeremiah Crim. 2003. Bootstrapping statistical
parsers from small datasets. In The Proceedings of the An-
nual Meeting of the European Chapter of the ACL. To ap-
pear.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active
learning for statistical natural language parsing. In Proceed-
ings of the 40th Annual Meeting of the ACL, pages 120?127,
July.
Cynthia A. Thompson, Mary Elaine Califf, and Raymond J.
Mooney. 1999. Active learning for natural language pars-
ing and information extraction. In Proceedings of ICML-99,
pages 406?414, Bled, Slovenia.
 	
	
  	
Coling 2010: Poster Volume, pages 1373?1381,
Beijing, August 2010
Syntax-Driven Machine Translation as a Model of ESL
Revision
Huichao Xue and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
{hux10,hwa}@cs.pitt.edu
Abstract
In this work, we model the writing re-
vision process of English as a Second
Language (ESL) students with syntax-
driven machine translation methods.
We compare two approaches: tree-to-
string transformations (Yamada and
Knight, 2001) and tree-to-tree trans-
formations (Smith and Eisner, 2006).
Results suggest that while the tree-to-
tree model provides a greater cover-
age, the tree-to-string approach offers
a more plausible model of ESL learn-
ers? revision writing process.
1 Introduction
When learning a second language, students
make mistakes along the way. While some
mistakes are idiosyncratic and individual,
many are systematic and common to people
who share the same primary language. There
has been extensive research on grammar error
detection. Most previous efforts focus on iden-
tifying specific types of problems commonly
encountered by English as a Second Language
(ESL) learners. Some examples include the
proper usage of determiners (Yi et al, 2008;
Gamon et al, 2008), prepositions (Chodorow
et al, 2007; Gamon et al, 2008; Hermet et al,
2008), and mass versus count nouns (Nagata
et al, 2006). However, previous work suggests
that grammar error correction is considerably
more challenging than detection (Han et al,
2010). Furthermore, an ESL learner?s writing
may contain multiple interacting errors that
are difficult to detect and correct in isolation.
A promising research direction is to tackle
automatic grammar error correction as a ma-
chine translation (MT) problem. The dis-
fluent sentences produced by an ESL learner
can be seen as the input source language,
and the corrected revision is the result of the
translation. Brockett et al (2006) showed
that phrase-based statistical MT can help to
correct mistakes made on mass nouns. To
our knowledge, phrase-based MT techniques
have not been applied for rewriting entire sen-
tences. One major challenge is the lack of ap-
propriate training data such as a sizable par-
allel corpus. Another concern is that phrase-
based MT may not be similar enough to the
problem of correcting ESL learner mistakes.
While MT rewrites an entire source sentence
into the target language, not every word writ-
ten by an ESL learner needs to be modified.
Another alternative that may afford a more
general model of ESL error corrections is to
consider syntax-driven MT approaches. We
argue that syntax-based approaches can over-
come the expected challenges in applying MT
to this domain. First, it can be less data-
intensive because the mapping is formed at a
structural level rather than the surface word
level. While it does require a robust parser,
a syntax-driven MT model may not need to
train on a very large parallel corpus. Second,
syntactic transformations provide an intuitive
description of how second language learners
revise their writings: they are transforming
structures in their primary language to those
in the new language.
In this paper, we conduct a first inquiry into
the applicability of syntax-driven MT meth-
ods to automatic grammar error correction.
In particular, we investigate whether a syntax-
driven model can capture ESL students? pro-
cess of writing revisions. We compare two ap-
proaches: a tree-to-string mapping proposed
by Yamada & Knight (2001) and a tree-to-
tree mapping using the Quasi-Synchronous
1373
Grammar (QG) formalism (Smith and Eisner,
2006). We train both models on a parallel cor-
pus consisting of multiple drafts of essays by
ESL students. The approaches are evaluated
on how well they model the revision pairs in an
unseen test corpus. Experimental results sug-
gest that 1) the QG model has more flexibility
and is able to describe more types of transfor-
mations; but 2) the YK model is better at cap-
turing the incremental improvements in the
ESL learners? revision writing process.
2 Problem Description
This paper explores the research question: can
ESL learners? process of revising their writ-
ings be described by a computational model?
A successful model of the revision process has
several potential applications. In addition to
automatic grammar error detection and cor-
rection, it may also be useful as an auto-
matic metric in an intelligent tutoring system
to evaluate how well the students are learning
to make their own revisions.
Revising an ESL student?s writing bears
some resemblance to translating. The stu-
dent?s first draft is likely to contain disfluent
expressions that arose from translation diver-
gences between English and the student?s pri-
mary language. In the revised draft, the diver-
gences should be resolved so that the text be-
comes fluent English. We investigate to what
extent are formalisms used for machine trans-
lation applicable to model writing revision.
We hypothesize that ESL students typically
modify sentences to make them sound more
fluent rather than to drastically change the
meanings of what they are trying to convey.
Thus, our work focuses on syntax-driven MT
models.
One challenge of applying MT methods to
model grammar error correction is the lack of
appropriate training data. The equivalence
to the bilingual parallel corpus used for de-
veloping MT systems would be a corpus in
which each student sentence is paired with a
fluent version re-written by an instructor. Un-
like bilingual text, however, there is not much
data of this type in practice because there
are typically too many students for the teach-
ers to provide detailed manual inspection and
correction at a large scale. More commonly,
students are asked to revise their previously
written essays as they learn more about the
English language. Here is an example of a
student sentence from a first-draft essay:
The problem here is that they come
to the US like illegal.
In a later draft, it has been revised into:
The problem here is that they come
to the US illegally.
Although the students are not able to cre-
ate ?gold standard revisions? due to their still
imperfect understanding of English, a corpus
that pairs the students? earlier and later drafts
still offers us an opportunity to model how
ESL speakers make mistakes.
More formally, the corpus C consists of a
set of sentence pairs (O,R), where O repre-
sents the student?s original draft and R rep-
resents the revised draft. Note that while R
is assumed to be an improvement upon O,
its quality may fall short of the gold stan-
dard revision, G. To train the syntax-driven
MT models, we optimize the joint probabil-
ity of observing the sentence pair, Pr(O,R),
through some form of mapping between their
parse trees, ?O and ?R.
An added wrinkle to our problem is that it
might not always be possible to assign a sen-
sible syntactic structure to an ungrammati-
cal sentence. It is well-known that an English
parser trained on the Penn Treebank is bad
at handling disfluent sentences (Charniak et
al., 2003; Foster et al, 2008). In our domain,
since O (and perhaps also R) might be disflu-
ent, an important question that a translation
model must address is: how should the map-
ping between the trees ?O and ?R be handled?
3 Syntax-Driven Models for Essay
Revisions
There is extensive literature on syntax-driven
approaches to MT (cf. a recent survey by
1374
Lopez (2008)); we focus on two particular for-
malisms that reflects different perspectives on
the role of syntax. Our goal is to assess which
formalism is a better fit with the domain of
essay revision modeling, in which the data
largely consist of imperfect sentences that may
not support a plausible syntactic interpreta-
tion.
3.1 Tree-to-String Model
The Yamada & Knight (henceforth, YK) tree-
to-string model is an instance of noisy channel
translation systems, which assumes that the
observed source sentence is the result of trans-
formation performed on the parse tree of the
intended target sentence due to a noisy com-
munication channel. Given a parallel corpus,
and a parser for the the target side, the pa-
rameters of this model can be estimated using
EM(Expectation Maximization). The trained
model?s job is to recover the target sentence
(and tree) through decoding.
While the noisy channel generation story
may sound somewhat counter-intuitive for
translation, it gives a plausible account of ESL
learner?s writing process. The student really
wants to convey a fluent English sentence with
a well-formed structure, but due to an im-
perfect understanding of the language, writes
down an ungrammatical sentence, O, as a first
draft. The student serves as the noisy channel.
The YK model describes this as a stochastic
process that performs three operations on ?G,
the parse of the intended sentence, G:
1. Each node in ?G may have its children
reordered with some probability.
2. Each node in ?G may have a child node
inserted to its left or right with some
probability.
3. Each leaf node (i.e., surface word) in ?G
is replaced by some (possibly empty)
string according to its lexical translation
distribution.
The resulting sentence, O, is the concatena-
tion of the leaf nodes of the transformed ?G.
Common mistakes made by ESL learners,
such as misuses of determiners and preposi-
tions, word choice errors, and incorrect con-
stituency orderings, can be modeled by a com-
bination of the insert, replace, and reorder
operators. The YK model allows us to per-
form transformations on a higher syntactic
level. Another potential benefit is that the
model does not attempt to assign syntactic
interpretations over the source sentences (i.e.,
the less fluent original draft).
3.2 Tree-to-Tree Model
The Quasi-Synchronous Grammar formalism
(Smith and Eisner, 2006) is a generative model
that aims to produce the most likely target
tree for a given source tree. It differs from the
more strict synchronous grammar formalisms
(Wu, 1995; Melamed et al, 2004) because it
does not try to perform simultaneous pars-
ing on parallel grammars; instead, the model
learns an augmented target-language gram-
mar whose rules make ?soft alignments? with
a given source tree.
QG has been applied to some NLP tasks
other than MT, including answer selection for
question-answering (Wang et al, 2007), para-
phrase identification (Das and Smith, 2009),
and parser adaptation and projection (Smith
and Eisner, 2009). In this work we use
an instantiation of QG that largely follows
the model described by Smith and Eisner
(2006). The model is trained on a parallel
corpus in which both the first-draft and re-
vised sentences have been parsed. Using EM
to estimate its parameters, it learns an aug-
mented target PCFG grammar1 whose pro-
duction rules form associations with the given
source trees.
Consider the scenario in Figure 1. Given a
source tree ?O, the trained model generates a
target tree by expanding the production rules
in the augmented target PCFG. To apply a
1For expository purposes, we illustrate the model
using a PCFG production rule. In the experiment, a
statistical English dependency parser (Klein and Man-
ning, 2004) was used.
1375
Figure 1: An example of QG?s soft alignments
between a given source tree and a possible tar-
get rule expansion.
target-side production rule such as
A? BC,
the model considers which source tree nodes
might be associated with each target-side non-
terminals:
(?,A)? (?,B)(?,C)
where ?, ?, ? are nodes in ?O. Thus, as-
suming that the target symbol A has already
been aligned to source node ? from an ear-
lier derivation step, the likelihood of expand-
ing (?,A) with the above production rule de-
pends on three factors:
1. the likelihood of the monolingual tar-
get rule, Pr(A? BC)
2. the likelihood of alignments between B
and ? as well as C and ?.
3. the likelihood that the source nodes form
some expected configuration (i.e., be-
tween ? and ? as well as between ? and
?). In this work, we distinguish between
two configuration types: parent-child and
other. This restriction doesn?t reduce the
explanatory power of the resulting QG
model, though it may not be as fine-tuned
as some models in (Smith and Eisner,
2006).
Under QG, the ESL students? first drafts
are seen as text in a different language that
has its own syntactic constructions. QG ex-
plains the grammar rules that govern the re-
vised text in terms of how different compo-
nents map to structures in the original draft.
It makes explicit the representation of diver-
gences between the students? original mental
model and the expected structure.
3.3 Method of Model Comparison
Cross entropy can be used as a metric that
measures the distance between the learned
probabilistic model and the real data. It can
be interpreted as measuring the amount of in-
formation that is needed in addition to the
model to accurately recover the observed data.
In language modeling, cross entropy is widely
used in showing a given model?s prediction
power.
To determine how well the two syntax-
driven MT models capture the ESL student
revision generation process, we measure the
cross entropy of each trained model on an un-
seen test corpus. This quantity measures how
surprised a model is about relating an initial
sentence, O, to its corresponding revision, R.
Specifically, the cross entropy for some model
M on a test corpus C of original and revised
sentence pairs (O,R) is:
? 1|C|
?
(O,R)?C
log Pr
M
(O,R)
Because neither model computes the joint
probability of the sentence pair, we need to
make additional computations so that the
models can be compared directly.
The YK model computes the likelihood
of the first-draft sentence O given an as-
sumed gold parse ?R of the revised sentence:
PrY K(O | ?R). To determine the joint proba-
bility, we would need to compute:
Pr
Y K
(O,R) =
?
?R??R
Pr
Y K
(O, ?R)
=
?
?R??R
Pr
Y K
(O | ?R) Pr(?R)
where ?R represents the set of possible parse
trees for sentence R. Practically, perform-
ing tree-to-string mapping over the entire set
of trees in ?R is computationally intractable.
Moreover, the motivation behind the YK
1376
mean stdev
percentage of O = R 54.11% N/A
O?s length 12.95 4.87
R?s length 12.74 4.20
edit distance 1.88 3.58
Table 1: This table summarizes some statis-
tics of the dataset.
model is to trust the given ?R. Thus, we made
a Viterbi approximation:
Pr
Y K
(O,R) =
?
?R??R
Pr
Y K
(O | ?R) Pr(?R)
? Pr
Y K
(O | ??R) Pr(??R)
where Pr(??R) is the probability of the single
best parse tree according to a standard En-
glish parser.
Similarly, to compute the joint sentence pair
probability under the QG model would require
summing over both sets of trees because the
model computes PrQG(?R | ?O). Here, we
make the Viterbi approximation on both trees.
Pr
QG
(O,R) =
?
?R??R
?
?O??O
Pr
QG
(?O, ?R)
=
?
?R??R
?
?O??O
Pr
QG
(?R | ?O) Pr(?O)
? Pr
QG
(??R | ??O) Pr(??O)
where ??O and ??R are the best parses for sen-
tences O and R according to the underlying
English dependency parser, respectively.
4 Experiments
4.1 Data
Our experiments are conducted using a collec-
tion of ESL students? writing samples2. These
are short essays of approximately 30 sentences
on topics such as ?a letter to your parents.?
The students are asked to revise their essays
at least once. From the dataset, we extracted
358 article pairs.
2The dataset is made available by the Pittsburgh
Science of Learning Center English as a Second Lan-
guage Course Committee, supported by NSF Award
SBE-0354420.
Typically, the changes between the drafts
are incremental. Approximately half of the
sentences are not changed at all. These sen-
tences are considered useful because this phe-
nomenon strongly implies that the original
version is good enough to the best of the au-
thor?s knowledge. In a few rare cases, stu-
dents may write an entirely different essay.
We applied TF-IDF to automatically align the
sentences between essay drafts. Any sentence
pair with a cosine similarity score of less than
0.3 is filtered. This resulted in a parallel cor-
pus of 7580 sentence pairs.
Because both models are computational in-
tensive, we further restricted our experiments
to sentence pairs for which the revised sen-
tence has no more than 20 words. This re-
duces our corpus to 4666 sentence pairs. Some
statistics of the sentence pairs are shown in
Table 1.
4.2 Experimental Setup
We randomly split the resulting dataset into
a training corpus of 4566 sentence pairs and a
test corpus of 100 pairs.
The training of both models involve an EM
algorithm. We initialize the model parameters
with some reasonable values. Then, in each it-
eration of training, the model parameters are
re-estimated by collecting the expected counts
across possible alignments between each sen-
tence pair in the training corpus. In out ex-
periments, both models had two iterations of
training. Below, we highlight our initializa-
tion procedure for each model.
In the YK model, the initial reordering
probability distribution is set to prefer no
change 50% of the time. The remaining prob-
ability mass is distributed evenly over all of
the other permutations. For the insertion
operation, for each node, the YK model first
chooses whether to insert a new string to its
left, to its right, or not at all, conditioned on
the node?s label and its parent?s label. These
distributions are initialized uniformly (13). If
a new string should be inserted, the model
then makes that choice with some probability.
The insertion probability of each string in the
1377
dictionary is assigned evenly with 1N , where
N is the number of words in the dictionary.
Finally, the replace probability distribution
is initialized uniformly with the same value
( 1N+1) across all words in the dictionary, in-
cluding the empty string.
For the QG model, the initial parameters
are determined as follows: For the monolin-
gual target parsing model parameters,
we first parse the target side of the corpus
(i.e., the revised sentences) with the Stanford
parser; we then use the maximum likelihood
estimates based on these parse trees to ini-
tialize the parameters of the target parser,
Dependency Model with Valence (DMV). We
uniformly initialized the configuration pa-
rameters; the parent-child configuration and
other configuration each has 0.5 probability.
For the alignment parameters, we ran the
GIZA++ implementation of the IBM word
alignment model (Och and Ney, 2003) on the
sentence pairs, and used the resulting transla-
tion table as our initial estimation. There may
be better initialization setups, but the differ-
ence between those setups will become small
after a few rounds of EM.
Once trained, the two models compute the
joint probability of every sentence pair in the
test corpus as described in Section 3.3.
4.3 Experiment I
To evaluate how well the models describe the
ESL revision domain, we want to see which
model is less ?surprised? by the test data. We
expected that the better model should be able
to transform more sentence pair in the test
corpus; we also expect that the better model
should have a lower cross entropy with respect
to the test corpus.
Applying both YK and QG to the test cor-
pus, we find that neither model is able to
transform all the test sentence pairs. Of the
two, QG had the better coverage; it success-
fully modeled 59 pairs out of 100 (we denote
this subset as DQG). In contrast, YK modeled
36 pairs (this subset is denoted as DY K).
To determine whether there were some
characteristics of the data that made one
model better at performing transformations
for certain sentence pairs, we compare corpus
statistics for different test subsets. Based on
the results summarized in Table 2, we make a
few observations.
First, the sentence pairs that neither model
could transform seem, as a whole, more diffi-
cult. Their average lengths are longer, and the
average per word Levenshtein edit distance is
bigger. The differences between Neither and
the other subsets are statistically significant
with 90% confidence. For the length differ-
ence, we applied standard two-sample t-test.
For the edit distance difference, we applied hy-
pothesis testing with the null-hypothesis that
?longer sentence pairs are as likely to be cov-
ered by our model as shorter ones.?
Second, both models sometimes have trou-
ble with sentence pairs that require no change.
This may be due to out-of-vocabulary words
in the test corpus. A more aggressive smooth-
ing strategy could improve the coverage for
both models.
Third, comparing the subset of sentence
pairs that only QG could transform (DQG ?
DY K) against the subset of sentences that
both models could transform (DQG ? DY K),
the former has slightly higher average edit dis-
tance and length, but the difference is not
statistically significant. Although QG could
transform more sentence pairs, the cross en-
tropy of DQG ?DY K is higher than QG?s es-
timate for the DQG ?DY K subset. QG?s soft
alignment property allows it to model more
complex transformations with greater flexibil-
ity.
Finally, while the YK model has a more lim-
ited coverage, it models those transformations
with a greater certainty. For the common sub-
set of sentence pairs that both models could
transform, YK has a much lower cross entropy
than QG. Table 3 further breaks down the
common subset. It is not surprising that both
models have low entropy for identical sentence
pairs. For modeling sentence pairs that con-
tain revisions, YK is more efficient than QG.
1378
Neither DQG ?DY K DQG ?DY K DY K ?DQG
number of instances 38 33 26 3
average edit distance 2.42 1.88 2.08 1
% of identical pairs 53% 48% 58% 67%
average O length 14.63 12.36 12.58 6.67
average R length 13.87 12.06 12.62 6.67
QG cross entropy N/A 127.95 138.9 N/A
YK cross entropy N/A 78.76 N/A 43.84
Table 2: A comparison of the two models based on their coverage of the test corpus. Some
relevant statistics on the sentence subsets are also summarized in the table.
YK QG
overall entropy 78.76 127.95
on identical pairs 52.59 85.40
on non-identical pairs 103.99 168.00
Table 3: A further comparison of the two mod-
els on DQG ?DY K , the sentence pairs in the
test corpus that both could transform.
4.4 Experiment II
The results of the previous experiment raises
the possibility that QG might have a greater
coverage because it is too flexible. However,
an appropriate model should not only assign
large probability mass to positive examples,
but it should also have a low chance of choos-
ing negative examples. In this next experi-
ment, we construct a ?negative? test corpus
to see how it affects the models.
To construct a negative scenario, we still
use the same test corpus as before, but we re-
verse the sentence pairs. That is, we use the
revised sentences as ?originals? and the origi-
nal sentences as ?revisions.? We would expect
a good model to have a raised cross entropy
values along with a drop in coverage on the
new dataset because the ?revisions? should be
more disfluent than the ?original? sentences.
Table 4 summarizes the results. We ob-
serve that the number of instances that can
be transformed has dropped for both models:
from 59 to 49 pairs for QG, and from 36 to
20 pairs for YK; also, the proportion of iden-
tical instances in each set has raised. This
means that both models are more surprised
by the reverse test corpus, suggesting that
both models have, to some extent, succeeded
in modeling the ESL revision domain. How-
ever, QG still allows for many more transfor-
mations. Moreover, 16 out of the 49 instances
are non-identical pairs. In contrast, YK mod-
eled only 1 non-identical sentence pair. The
results from these two experiments suggest
that YK is more suited for modeling the ESL
revision domain than QG. One possible expla-
nation is that QG allows more flexibility and
would require more training. Another possi-
ble explanation is that because YK assumes
well-formed syntax structure for only the tar-
get side, the philosophy behind its design is a
better fit with the ESL revision problem.
5 Related Work
There are many research directions in the field
of ESL error correction. A great deal of the
work focuses on the lexical or shallow syn-
tactic level. Typically, local features such
as word identity and POS tagging informa-
tion are combined to deal with some specific
kind of error. Among them, (Burstein et al,
2004) developed a tool called Critique that
detects collocation errors and word choice er-
rors. Nagata et al (2006) uses a rule-based
approach in distinguishing mass and count
nouns. Knight and Chander (1994) and Han
et al (2006) both addressed the misuse of ar-
ticles. Chodorow et al (2007), Gamon et al
(2008), Hermet et al (2008) proposed several
techniques in detecting and correcting propo-
sition errors. In detecting errors and giving
suggestions, Liu et al (2000), Gamon et al
(2008) and Hermet et al (2008) make use of
1379
Neither DQG ?DY K DQG ?DY K DY K ?DQG
number of instances 50 19 30 1
average edit distance 2.88 0.05 2.17 1
percentage of identical pairs 0.40 0.95 0.5 0
average O length 14.18 9.00 12.53 17
average R length 14.98 9.05 12.47 16
QG cross entropy N/A 81.85 139.36 N/A
YK cross entropy N/A 51.2 N/A 103.75
Table 4: This table compares the two models on a ?trick? test corpus in which the earlier and
later drafts are reversed. If a model is trained to prefer more fluent English sentences are the
revision, it should be perplexed on this corpus.
information retrieval techniques. Chodorow
et al (2007) instead treat it as a classification
problem and employed a maximum entropy
classifier. Similar to our approach, Brockett
et al (2006) view error correction as a Ma-
chine Translation problem. But their transla-
tion system is built on phrase level, with the
purpose of correcting local errors such as mass
noun errors.
The problem of error correction at a syn-
tactic level is less explored. Lee and Seneff
(2008) examined the task of correcting verb
form misuse by applying tree template match-
ing rules. The parse tree transformation rules
are learned from synthesized training data.
6 Conclusion
This paper investigates the suitability of
syntax-driven MT approaches for modeling
the revision writing process of ESL learn-
ers. We have considered both the Yamada &
Knight tree-to-string model, which only con-
siders syntactic information from the typically
more fluent revised text, as well as Quasi-
Synchronous Grammar, a tree-to-tree model
that attempts to learn syntactic transforma-
tion patterns between the students? original
and revised texts. Our results suggests that
while QG offers a greater degree of freedom,
thus allowing for a better coverage of the
transformations, YK has a lower entropy on
the test corpus. Moreover, when presented
with an alternative ?trick? corpus in which the
?revision? is in fact the earlier draft, YK was
more perplexed than QG. These results sug-
gest that the YK model may be a promising
approach for automatic grammar error correc-
tion.
Acknowledgments
This work has been supported by NSF Grant
IIS-0745914. We thank Joel Tetreault and the
anonymous reviewers for their helpful com-
ments and suggestions.
References
Brockett, Chris, William B. Dolan, and Michael
Gamon. 2006. Correcting esl errors using
phrasal smt techniques. In Proceedings of
COLING-ACL 2006, Sydney, Australia, July.
Burstein, Jill, Martin Chodorow, and Claudia Lea-
cock. 2004. Automated essay evaluation: The
criterion online writing service. AI Magazine,
25(3).
Charniak, Eugene, Kevin Knight, and Kenji Ya-
mada. 2003. Syntax-based language models for
machine translation. In Proc. MT Summit IX,
New Orleans, Louisiana, USA.
Chodorow, Martin, Joel Tetreault, and Na-Rae
Han. 2007. Detection of grammatical errors
involving prepositions. In Proceedings of the
4th ACL-SIGSEM Workshop on Prepositions,
Prague, Czech Republic.
Das, Dipanjan and Noah A. Smith. 2009.
Paraphrase identification as probabilistic quasi-
synchronous recognition. In Proceedings of
ACL-IJCNLP 2009, Suntec, Singapore, August.
Foster, Jennifer, Joachim Wagner, and Josef van
Genabith. 2008. Adapting a WSJ-trained
1380
parser to grammatically noisy text. In Proceed-
ings of the 46th ACL on Human Language Tech-
nologies: Short Papers, Columbus, Ohio.
Gamon, Michael, Jianfeng Gao, Chris Brock-
ett, Alexandre Klementiev, William B. Dolan,
Dmitriy Belenko, and Lucy Vanderwende. 2008.
Using contextual speller techniques and lan-
guage modeling for ESL error correction. In
Proceedings of IJCNLP, Hyderabad, India.
Han, Na-Rae, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting errors in English article
usage by non-native speakers. Natural Language
Engineering, 12(02).
Han, Na-Rae, Joel Tetreault, Soo-Hwa Lee, and
Jin-Young Han. 2010. Using an error-annotated
learner corpus to develop and ESL/EFL er-
ror correction system. In Proceedings of LREC
2010, Valletta, Malta.
Hermet, Matthieu, Alain De?silets, and Stan Sz-
pakowicz. 2008. Using the web as a linguis-
tic resource to automatically correct Lexico-
Syntactic errors. In Proceedings of the LREC,
volume 8.
Klein, Dan and Christopher Manning. 2004.
Corpus-based induction of syntactic structure:
Models of dependency and constituency. In Pro-
ceedings of ACL 2004, Barcelona, Spain.
Knight, Kevin and Ishwar Chander. 1994. Auto-
mated postediting of documents. In Proceedings
of AAAI-94, Seattle, Washington.
Lee, John and Stephanie Seneff. 2008. Correcting
misuse of verb forms. Proceedings of the 46th
ACL, Columbus.
Liu, Ting, Ming Zhou, Jianfeng Gao, Endong
Xun, and Changning Huang. 2000. PENS: a
machine-aided english writing system for chi-
nese users. In Proceedings of the 38th ACL,
Hong Kong, China.
Lopez, Adam. 2008. Statistical machine transla-
tion. ACM Computing Surveys, 40(3), Septem-
ber.
Melamed, I. Dan, Giorgio Satta, and Ben Welling-
ton. 2004. Generalized multitext grammars. In
Proceedings of the 42nd ACL, Barcelona, Spain.
Nagata, Ryo, Atsuo Kawai, Koichiro Morihiro,
and Naoki Isu. 2006. A feedback-augmented
method for detecting errors in the writing of
learners of english. In Proceedings of COLING-
ACL 2006, Sydney, Australia, July.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical align-
ment models. Computational Linguistics, 29(1).
Smith, David A. and Jason Eisner. 2006. Quasi-
synchronous grammars: Alignment by soft pro-
jection of syntactic dependencies. In Proceed-
ings on the Workshop on Statistical Machine
Translation, New York City, June.
Smith, David A. and Jason Eisner. 2009.
Parser adaptation and projection with quasi-
synchronous grammar features. In Proceedings
of EMNLP 2009, Singapore, August.
Wang, Mengqiu, Noah A. Smith, and Teruko Mi-
tamura. 2007. What is the Jeopardy model?
a quasi-synchronous grammar for QA. In
Proceedings of EMNLP-CoNLL 2007, Prague,
Czech Republic, June.
Wu, Dekai. 1995. Stochastic inversion transduc-
tion grammars, with application to segmenta-
tion, bracketing, and alignment of parallel cor-
pora. In Proc. of the 14th Intl. Joint Conf. on
Artificial Intelligence, Montreal, Aug.
Yamada, Kenji and Kevin Knight. 2001. A
syntax-based statistical translation model. In
Proceedings of the 39th ACL, Toulouse, France.
Yi, Xing, Jianfeng Gao, and William B Dolan.
2008. A web-based english proofing system for
english as a second language users. In Proceed-
ings of IJCNLP, Hyderabad, India.
1381
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 683?691,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Redundancy Detection in ESL Writings
Huichao Xue and Rebecca Hwa
Department of Computer Science,
University of Pittsburgh,
210 S Bouquet St, Pittsburgh, PA 15260, USA
{hux10,hwa}@cs.pitt.edu
Abstract
This paper investigates redundancy detec-
tion in ESL writings. We propose a mea-
sure that assigns high scores to words and
phrases that are likely to be redundant
within a given sentence. The measure is
composed of two components: one cap-
tures fluency with a language model; the
other captures meaning preservation based
on analyzing alignments between words
and their translations. Experiments show
that the proposed measure is five times
more accurate than the random baseline.
1 Introduction
Writing concisely is challenging. It is especially
the case when writing in a foreign language that
one is still learning. As a non-native speaker, it
is more difficult to judge whether a word or a
phrase is redundant. This study focuses on auto-
matically detecting redundancies in English as a
Second Language learners? writings.
Redundancies occur when the writer includes
some extraneous word or phrase that do not add
to the meaning of the sentence but possibly make
the sentence more awkward to read. Upon re-
moval of the unnecessary words or phrases, the
sentence should improve in its fluency while main-
taining the original meaning. In the NUCLE
corpus (Dahlmeier and Ng, 2011), an annotated
learner corpus comprised of essays written by pri-
marily Singaporean students, 13.71% errors are
tagged as ?local redundancy errors?, making re-
dundancy error the second most frequent prob-
lem.
1
Although redundancies occur frequently, it has
not been studied as widely as other ESL errors. A
1
The most frequent error type is Wrong collocation/idiom
preposition, which comprises 15.69% of the total errors.
major challenge is that, unlike mistakes that vio-
late the grammaticality of a sentence, redundan-
cies do not necessarily ?break? the sentence. De-
termining which word or phrase is redundant is
more of a stylistic question; it is more subjective,
and sometimes difficult even for a native speaker.
To the best of our knowledge, this paper reports
a first study on redundancy detection. In particu-
lar, we focus on the task of defining a redundancy
measure that estimates the likelihood that a given
word or phrase within a sentence might be extrane-
ous. We propose a measure that takes into account
each word?s contribution to fluency and meaning.
The fluency component computes the language
model score of the sentence after the deletion of a
word or a phrase. The meaning preservation com-
ponent makes use of the sentence?s translation into
another language as pivot, then it applies a statis-
tical machine translation (SMT) alignment model
to infer the contribution of each word/phrase to the
meaning of the sentence. As a first experiment, we
evaluate our measures on their abilities in picking
the most redundant phrase of a given length. We
show that our measure is five times more accurate
than a random baseline.
2 Redundancies in ESL Writings
According to The Elements of Style (Strunk,
1918): concise writing requires that ?every word
tell.? In that sense, words that ?do not tell?
are redundant. Determining whether a certain
word/phrase is redundant is a stylistic question,
which is difficult to quantify. As a result, most an-
notation resources do not explicitly identify redun-
dancies. One exception is the NUCLE corpus. Be-
low are some examples from the NUCLE corpus,
where the bold-faced words/phrases are marked as
redundant.
Ex
1
: First of all , there should be a careful con-
sideration about what are the things that gov-
ernments should pay for.
683
Ex
2
: GM wishes to reposition itself as an inno-
vative company to the public.
Ex
3
: These findings are often unpredictable and
uncertain.
Ex
4
: . . . the cost incurred is not only just large
sum of money . . .
These words/phrases are considered redundant be-
cause they are unnecessary (e.g. Ex
1
, Ex
2
) or
repetitive (e.g. Ex
3
, Ex
4
).
However, in NUCLE?s annotation scheme,
some words that were marked redundant are re-
ally words that carry undesirable meanings. For
example:
Ex
5
: . . . through which they can insert a special
. . .
Ex
6
: . . . the analysis and therefore selection of
a single solution for adaptation. . .
Note that unlike redundancies, these undesirable
words/phrases change the sentences? meanings.
Despite the difference in definitions, our exper-
imental work uses the NUCLE corpus because
it provides many real world examples of redun-
dancy.
While redundancy detection has not yet been
widely studied, it is related to several areas of ac-
tive research, such as grammatical error correction
(GEC), sentence simplification and sentence com-
pression.
Work in GEC attempts to build automatic sys-
tems to detect/correct grammatical errors (Lea-
cock et al., 2010; Liu et al., 2010; Tetreault et al.,
2010; Dahlmeier and Ng, 2011; Rozovskaya and
Roth, 2010). Both redundancy detection and GEC
aim to improve students? writings. However, be-
cause redundancies do not necessarily break gram-
maticality, they have received little attention in
GEC.
Sentence compression and sentence simplifica-
tion also consider deleting words from input sen-
tences. However, these tasks have different goals.
Automated sentence simplification (Coster and
Kauchak, 2011) systems aim at reducing the gram-
matical complexity of an input sentence. To il-
lustrate the difference, consider the phrase ?crit-
ical reception.? A sentence simplification sys-
tem might rewrite it into ?reviews?; but a sys-
tem that removes redundancy should leave it un-
changed because neither ?critical? nor ?reception?
is extraneous. Moreover, consider the redundant
phrase ?had once before? in Ex
4
. A simplification
system does not need to change it because these
words do not add complexity to the sentence.
?	 ?and	 ?the	 ?cost	 ?incurred	 ?is	 ?not	 ?only	 ?just	 ?large	 ?sum	 ?of	 ?money	 ?? 
Figure 1: Among the three circled words, ?just?
is more redundant because deleting it hurts nether
fluency nor meaning.
Sentence compression systems (Jing, 2000;
Knight and Marcu, 2000; McDonald, 2006;
Clarke and Lapata, 2007) aim at shortening a
sentence while retaining the most important in-
formation and keeping it grammatically correct.
This goal distinguishes these systems from ours
in two major aspects. First, sentence compres-
sion systems assume that the original sentence is
well-written; therefore retaining words specific to
the sentence (e.g. ?uncertain? in Ex
3
) can be a
good strategy (Clarke and Lapata, 2007). In the
ESL context, however, even specific words could
still be redundant. For example, although ?un-
certain? is specific to Ex
3
, it is redundant, be-
cause its meaning is already implied by ?unpre-
dictable?. Second, sentence compression systems
try to shorten a sentence as much as possible, but
an ESL redundancy detector should leave as much
of the input sentences unchanged, if possible.
One challenge involved in redundancy detection
is that it often involves open class words (Ex
3
), as
well as multi-word expressions (Ex
1
, Ex
4
). Cur-
rent GEC systems dealing with such error types
are mostly MT based. MT systems tend to ei-
ther require large training corpora (Brockett et al.,
2006; Liu et al., 2010), or provide whole sentence
rewritings (Madnani et al., 2012). Hermet and
D?esilets (2009) attempted to extract single prepo-
sition corrections from whole sentence rewritings.
Our work incorporates alignments information to
handle complex changes on both word and phrase
levels.
In our approximation, we consider MT out-
put as an approximation of word/phrase mean-
ings. Using words in other languages to repre-
sent meanings has been explored in Carpuat and
Wu (2007), where the focus is the aligned words?
identities. Our work instead focuses more on how
many words each word is aligned to.
3 A Probabilistic Model of Redundancy
We consider a word or a phrase to be redundant
if deleting it results in a fluent English sentence
that conveys the same meaning as before. For
example, ?not? and ?the? are not considered re-
684
(a) Unaligned English
words are considered
redundant.
(b) Multiple English words
aligned to the same meaning
unit. These words are con-
sidered redundant.
Figure 2: Configurations our system consider as
redundant. In each figure, the shaded squares are
the words considered to be more redundant than
other words in the same figure.
dundant in Figure 1. This is because discarding
?not? would flip the sentence?s meaning; discard-
ing ?the? would lose a necessary determiner be-
fore a noun. In contrast, discarding ?just? would
hurt neither fluency nor meaning. It is thus con-
sidered to be more redundant.
Therefore, our computational model needs to
consider words? contributions to both fluency and
meaning. Figure 2 illustrates words? contribution
to meaning. In those two examples, each sub-
graph visualizes a sentence: English words corre-
spond to squares in the top row, while their mean-
ings correspond to circles in the bottom row. The
knowledge of which word represents what mean-
ing helps in evaluating its contribution. In partic-
ular, if a word does not connote any significant
meaning, deleting it would not affect the overall
sentence; if several words express the same mean-
ing, then deleting some of them might not affect
the overall sentence either. Also, deleting a more
semantically meaningful word (or phrase) is more
likely to cause a loss of meaning of the overall sen-
tence (e.g. uncertain v.s. the).
Our model computes a single probabilistic value
for both fluency judgment and meaning preserva-
tion ? the log-likelihood that after deleting a cer-
tain word or phrase of a sentence, the new sen-
tence is still fluent and conveys the same meaning
as before. This value reflects our definition of re-
dundancy ? the higher this probability, the more
redundant the given word/phrase is.
More formally, suppose an English sentence e
contains l
e
words: e = e
1
e
2
. . . e
l
e
; after some
sub-string e
s,t
= e
s
. . . e
t
(1 ? s ? t ? l
e
) is
deleted from e, we obtain a shorter sentence, de-
noted as e
s,t
?
. We wish to compute the quantity
R(s, t; e), the chance that the sub-string e
s,t
is re-
dundant in sentence e. We propose a probabilistic
model to formalize this notion.
LetM be a random variable over some meaning
representation; Pr(M |e) is the likelihood that M
carries the meaning of e. If the sub-string e
s,t
is
redundant, then the new sentence e
s,t
?
should still
express the same meaning; Pr(e
s,t
?
|M) computes
the likelihood that the after-deletion sentence can
be generated from meaning M .
R(s, t; e)
= log
?
M=m
Pr(m|e) Pr(e
s,t
?
|m)
= log
?
M=m
Pr(m|e) Pr(e
s,t
?
) Pr(m|e
s,t
?
)
Pr(m)
= log Pr(e
s,t
?
) + log
?
M=m
Pr(m|e
s,t
?
) Pr(m|e)
Pr(m)
= LM(e
s,t
?
) + AGR(M |e
s,t
?
, e) (1)
The first term LM(e
s,t
?
) is the after-deletion sen-
tence?s log-likelihood, which reflects its fluency.
We calculate the first term with a trigram language
model (LM).
The second term AGR(M |e
s,t
?
, e) can be inter-
preted as the chance that e and e
s,t
?
carry the same
meaning, discounted by ?chance agreement?. This
term captures meaning preservation.
The two terms above are complementary to each
other. Intuitively, LM prefers keeping common
words in e
s,t
?
(e.g. the, to) while AGR prefers
keeping words specific to e (e.g. disease, hyper-
tension).
To make the calculation of the second term
practical, we make two simplifying assumptions.
Assumption 1 A sentence?s meaning can be rep-
resented by its translations in another language; its
words? contributions to the meaning of the sen-
tence can be represented by the mapping between
the words in the original sentence and its transla-
tions (Figure 3).
Note that the choice of translation language may
impact the interpretation of words? contributions.
We will discuss about this issue in our experiments
(Section 5).
Assumption 2 Instead of considering all possi-
ble translations f for e, our computation will make
use of the most likely translation, f
?
.
685
?? ?? ? ???? ?? ????
??
?? ????
a new idea is created through results from rigorous process of research .
Figure 3: Illustration of Assumption 1 and Approximation 1. An English sentence?s meaning is presented
as a Chinese translation. Meanwhile, each (English) word?s contribution to the sentence meaning is
realized as a word alignment. For Approximation 1, note that sentence alignments normally won?t be
affected before/after deleting words (e.g. ?results from?) from the source sentence.
With the two approximations:
AGR(M |e
s,t
?
, e) ? log
Pr(f
?
|e
s,t
?
) Pr(f
?
|e)
Pr(f
?
)
= log Pr(f
?
|e
s,t
?
) + C
1
(e)
(We use C
i
(e) to denote constant numbers within
sentence e throughout the paper.)
We now rely on a statistical machine translation
model to approximate the translation probability
log Pr(f
?
|e
s,t
?
).
One naive way of calculating this probabil-
ity measure is to consult the MT system. This
method, however, is too computationally expen-
sive for one single input sentence. For a sentence
of length n, calculating the redundancy measure
for all chunks in it would require issuing O(n
2
)
translation queries. We propose an approximation
that instead calculates the difference of translation
probability caused by discarding e
s,t
, based on an
analysis on the alignment structure between e and
f
?
. We show the measure boils down to sum-
ming the expected number of aligned words for
each e
i
(s ? i ? t), and possibly weighting these
numbers by e
i
?s unigram probability. This method
requires one translation query, and O(n
2
) queries
into a language model, which is much more suit-
able for practical applications. Our method also
sheds light on the role of alignment structures in
the redundancy detection context.
3.1 Alignments Approximation
One key insight in our approximation is that the
alignment structure a between e
s,t
?
and f
?
would
be largely similar with the alignment structure be-
tween e and f
?
. We illustrate this notion in Fig-
ure 3. Note that after deleting two words ?results
from? from the source sentence in Figure 3, the
alignment structure remains unchanged elsewhere.
Also, ????, the word once connected with ?re-
sults?, can now be seen as connected to blanks.
We hence approximate log Pr(f
?
|e
s,t
?
) by
reusing the alignment structure between e
and f
?
. To make the alignment structures
compatible, we start with redefining e
s,t
?
as
e
1
, e
2
, . . . , e
s?1
,, . . . ,, e
t+1
, . . . , e
l
e
, where
the deleted words are left blank.
Let Pr(a|f, e) be the posterior distribution of
alignment structure between sentence pair (f, e).
Approximation 1 We formalize the similarity
between the alignment structures by assuming the
KL-divergence between their alignment distribu-
tions to be small.
D
KL
(a|f
?
, e; a|f
?
, e
s,t
?
) ? 0
This allows using Pr(a|f
?
, e) to help approximate
log Pr(f
?
|e
s,t
?
):
log Pr(f
?
|e
s,t
?
)
= log
?
a
Pr(a|f
?
, e)
Pr(f
?
, a|e
s,t
?
)
Pr(a|f
?
, e)
=
?
a
Pr(a|f
?
, e) log
Pr(f
?
, a|e
s,t
?
)
Pr(a|f
?
, e)
+
?
a
Pr(a|f
?
, e) log
(
Pr(f
?
|e
s,t
?
)/
Pr(f
?
, a|e
s,t
?
)
Pr(a|f
?
, e)
)
? ?? ?
D
KL
(a|f
?
,e;a|f
?
,e
s,t
?
)?0
?
?
a
Pr(a|f
?
, e) log Pr(f
?
|e
s,t
?
, a) + C
2
(e)
We then use an SMT model to calculate
log Pr(f
?
|e
s,t
?
, a), the translation probability under
a given alignment structure.
3.2 The Translation Model
Approximation 2 We will use IBM
Model 1 (Brown et al., 1993) to calculate
log Pr(f
?
|e
s,t
?
, a)
IBM Model 1 is one of the earliest statisti-
cal translation models. It helps us to compute
686
log Pr(f
?
|e
s,t
?
, a) by making explicit how each
word contributes to words it aligns with. In partic-
ular, to compute the probability that f is a transla-
tion of e, Pr(f |e), IBM Model 1 defined a gener-
ative alignment model where every word f
i
in f is
aligned with exactly one word e
a
i
in e, so that f
i
and e
a
i
are word level translations of each other.
?
a
Pr(a|f
?
, e) log Pr(f
?
|e
s,t
?
, a)
=
?
a
Pr(a|f
?
, e)
?
1?i?l
f
?
log Pr(f
i
?
|e
s,t
?a
i
)
=
?
a
Pr(a|f
?
, e)
?
1?i?l
f
?
log
Pr(f
i
?
|e
s,t
?a
i
)
Pr(f
i
?
|e
a
i
)
+ C
3
(e)
Note that
log
Pr(f
i
?
|e
s,t
?a
i
)
Pr(f
i
?
|e
a
i
)
=
{
0 , for a
i
/? {s . . . t}
log
Pr(f
i
?
|)
Pr(f
i
?
|e
a
i
)
, otherwise
?
a
Pr(a|f
?
, e)
?
1?i?l
f
?
log
Pr(f
i
?
|e
s,t
?a
i
)
Pr(f
i
?
|e
a
i
)
=
?
a
Pr(a|f
?
, e)
?
1?i?l
f
?
?
s?j?t
I
a
i
=j
log
Pr(f
i
?
|)
Pr(f
i
?
|e
j
)
=
?
s?j?t
?
1?i?l
f
?
Pr(a
i
= j|f
?
, e)
? ?? ?
A
i,j
log
Pr(f
i
?
|)
Pr(f
i
?
|e
j
)
? ?? ?
DIFF(e
s,t
?
,e)
Here A
i,j
= Pr(a
i
= j | f
?
, e), which is the
probability of the i-th word in the translation being
aligned to the j-th word in the original sentence.
3.3 Per-word Contribution
Through deductions,
R(s, t; e) = LM(e
s,t
?
) + DIFF(e
s,t
?
, e)
+C
1
(e) + C
2
(e) + C
3
(e)
the redundancy measure boils down to how we
define Pr(f
i
?
|
j
), which is: when we discard e
j
,
how do we generate the word it aligns f
i
?
with in
its translation. This value reflects e
j
?s contribution
in generating f
i
?
.
We approximate Pr(f
i
?
|
j
) in two ways.
1. Suppose that all words in the translation
are of equal importance. We assume
log
Pr(f
i
?
|)
Pr(f
i
?
|e
j
)
= ?C
c
, where C
c
is a constant
number. A larger C
c
value indicates a higher
importance of e
j
during the translation.
DIFF(e
s,t
?
, e) = ?C
c
?
s?j?t
?
1?i?l
f
?
A
i,j
= ?C
c
?
s?j?t
A(j) (2)
Here A(j) is the expected number of align-
ments to e
j
. This metric demonstrates the in-
tuition that words aligned to more words in
the translation are less redundant.
2. We note that rare words are often more im-
portant, and therefore harder to be generated.
We assume Pr(f
i
?
|) = Pr(e
j
|) Pr(f
i
?
|e
j
).
DIFF(e
s,t
?
, e)
=
?
s?j?t
?
1?i?l
f
?
A
i,j
log
Pr(e
j
|) Pr(f
i
?
|e
j
)
Pr(f
i
?
|e
j
)
=
?
s?j?t
A(j) log Pr(e
j
|) (3)
This gives us counts on how likely each
word is aligned with Chinese words ac-
cording to Pr(a|f
?
, e), where each word is
weighted by its importance log Pr(e
j
|). We
use e
j
?s unigram probability to approximate
log Pr(e
j
|).
When estimating the alignment probabilities
A
i,j
, we smooth the alignment result from Google
translation using Dirichlet-smoothing, where we
set ? = 0.1 empirically based on experiments in
the development dataset.
4 Experimental Setup
A fully automated redundancy detector has to de-
cide (1) whether a given sentence contains any re-
dundancy errors; (2) how many words constitute
the redundant part; and (3) which exact words are
redundant. In this paper, we focus on the third part
while assuming the first two are given. Thus, our
experimental task is: given a sentence known to
contain a redundant phrase of a particular length,
can that redundant phrase be accurately identified?
For most sentences in our study, this results in
choosing one from around 20 words/phrases.
While the task has a somewhat limited scope, it
allows us to see how we could formally measure
the difference between redundant words/phrases
687
and non-redundant ones. For each measure, we
observe whether it has assigned the highest score
to the redundant part of the sentence. We compare
the proposed redundancy model described in Sec-
tion 3 against a set of baselines and other potential
redundancy measures (to be described shortly).
To better understand different measures? perfor-
mance on function words vs. content words, we
also calculate the percentage of redundant func-
tion/content words that are detected successfully ?
accuracy in both categories. In our experiments,
we consider prepositions and determiners as func-
tion words; and we consider other words/phrases
as content words/phrases.
4.1 Redundancy Measures
To gain insight into redundancy error detection?s
difficulty, we first consider a random baseline.
random The random baseline assigns a random
score to each word/phrase. The resulting system
will pick one word/phrase of the given length at
random.
We consider relying on large scale language
models to decide redundancy.
trigram We use a trigram language model to
capture fluency, by calculating the log-likelihood
of the whole sentence after discarding the given
word/phrase. A higher probability indicates a
higher fluency.
round-trip Inspired by Madnani et al. (2012;
Hermet and D?esilets (2009), an MT system may
eliminate grammar errors with the help of large
scale language models. In this method, we analyze
which parts are considered redundant by an MT
system by comparing the original sentence with
its round-trip translation. We use Google trans-
late to first translate one sentence into a pivot lan-
guage, and then back to English. We measure one
phrase?s redundancy by the number of words that
disappeared after the round-trip. We determine if
one word disappeared in two ways:
extract word match: one word is considered
disappeared if the same word does not occur
in the round-trip.
aligned word: we use the Berkeley
aligner (DeNero and Klein, 2007) to
align original sentences with their round-trip
translations. Unaligned words are considered
to have disappeared.
We consider measures for words/phrases? con-
tributions to sentence meaning.
sig-score This measure accounts for whether
one word w
i
is capturing the gist of a sen-
tence (Clarke and Lapata, 2007)
2
. It was shown to
help decide whether one part should be discarded
during sentence compression.
I(w
i
) = ?
l
N
? f
i
log
F
a
F
i
f
i
and F
i
are the frequencies of w
i
in the current
document and a large corpus respectively; F
a
is
the number of all word occurrences in the corpus;
l is the number of clause constituents above w
i
;
N is the deepest level of clause embeddings. This
measure assigns low scores to document specific
words occurring at deep syntax levels.
align # We use the number of alignments that a
word/phrase has in the translation to measure its
redundancy, as deducted in Equation 2.
contrib We compute the word/phrase?s contri-
bution to meaning, according to Equation 3.
We consider the combinations of measures.
trigram + C
c
align # We use a linear combina-
tion between language model and align # (Equa-
tion 2). We tune C
c
on development data.
trigram+contrib This measure (as we proposed
in Section 3) is the sum of the trigram lan-
guage model component and the contrib compo-
nent which represents the phrase?s contribution to
meaning.
trigram+? round-trip/sig-score We combine
language model with round-trip and sig-score
linearly (McDonald, 2006; Clarke and Lapata,
2007). To obtain baselines that are as strong as
possible, we tune the weight ? on evaluation data
for best accuracy.
4.2 Pivot Languages
Our proposed model uses machine translation out-
puts from different pivot languages. To see which
language helps measuring redundancy, we com-
pare 52 pivot languages available at Google trans-
late
3
for meaning representation
4
.
2
We extend this measure, which was only defined for con-
tent words in Clarke and Lapata (2007), to include all English
words.
3
http://translate.google.com
4
These languages include Albanian (sq), Arabic (ar),
Azerbaijani (az), Irish (ga), Estonian (et), Basque (eu),
688
length count percentage
1 356 67.55%
2 80 15.18%
3 40 7.59%
4 18 3.42%
other 33 6.26%
Table 1: Length distribution of redundant chunks?
lengths in the evaluation data.
4.3 Data and Tools
We extract instances from the NUCLE cor-
pus (Dahlmeier and Ng, 2011), an error annotated
corpus mainly written by Singaporean students,
to conduct this study. The corpus is composed
of 1,414 student essays on various topics. An-
notations in NUCLE include error locations, er-
ror types, and suggested corrections. Redundancy
errors are marked by annotators as Rloc. In this
study, we only consider the cases where the sug-
gested correction is to delete the redundant part
(97.09% among all Rloc errors).
To construct our evaluation dataset, we
pick sentences with exactly one redundant
word/phrase. This is the most common case
(81.18%) among sentences containing redundant
words/phrases. We use 10% of the essays (336
sentences) for development purposes, and an-
other 200 essays as the evaluation corpus (527
sentences). A distribution of redundant chunks?
lengths in evaluation corpus is shown in Table 1.
We train a trigram language model using the
SRILM toolkit (Stolcke, 2002) on the Agence
France-Presse (afp) portion of the English Giga-
words corpus.
5 Experiments
The experiment aims to address the following
questions: (1) Does a sentence?s translation serve
as a reasonable approximation for its meaning? (2)
Byelorussian (be), Bulgarian (bg), Icelandic (is), Polish (pl),
Persian (fa), Boolean (language ((Afrikaans) (af), Danish
(da), German (de), Russian (ru), French (fr), Tagalog (tl),
Finnish (fi), Khmer (km), Georgian (ka), Gujarati (gu),
Haitian (Creole (ht), Korean (ko), Dutch (nl), Galician (gl),
Catalan (ca), Czech (cs), Kannada (kn), Croatian (hr), Latin
(la), Latvian (lv), Lao (lo), Lithuanian (lt), Romanian (ro),
Maltese (mt), Malay (ms), Macedonian (mk), Bengali (bn),
Norwegian (no), Portuguese (pt), Japanese (ja), Swedish (sv),
Serbian (sr), Esperanto (eo), Slovak (sk), Slovenian (sl),
Swahili (sw), Telugu (te), Tamil (ta), Thai (th), Turkish (tr),
Welsh (cy), Urdu (ur), Ukrainian (uk), Hebrew (iw), Greek
(el), Spanish (es), Hungarian (hu), Armenian (hy), Italian (it),
Yiddish (yi), Hindi (hi), Indonesian (id), English (en), Viet-
namese (vi), Simplified Chinese (zh-CN), Traditional Chi-
nese (zh-TW).
Metrics overall
function
words
content
words
random 4.44% 4.62% 4.36%
trigram 8.06% 3.95% 9.73%
sig-score 10.71% 22.16% 6.07%
round-trip (aligned word) 10.69% 12.72% 9.87%
round-trip (exact word
match)
5.75% 4.27% 6.35%
trigram + ? round-trip
(aligned word)
14.80% 11.84% 16.00%
trigram + ? round-trip
(exact word match)
9.49% 4.61% 11.47%
trigram + ? sig-score 11.01% 22.68% 6.28%
align # 5.04% 3.36% 5.72%
trigram + C
c
?align # 9.58% 4.61% 11.60%
contrib 8.59% 20.23% 3.87%
trigram + contrib 21.63% 38.16% 14.93%
Table 2: Redundancy part identification accuracies
for different redundancy metrics on NUCLE cor-
pus, using French as the pivot language.
If so, does the choice of the pivot language matter?
(3) How do the potentially conflicting goals of pre-
serving fluency versus preserving meaning impact
the definition of a redundancy measure?
Our experimental results are presented in Fig-
ure 4 and Table 2. In Figure 4 we compare using
different pivot languages in our proposed model;
in Table 2 we compare using different redundancy
metrics for the same pivot language ? French.
Figure 4: Using different pivot languages for re-
dundancy measurement.
First, compared to other measures, our proposed
model best captures redundancy. In particular, our
model picks the correct redundant chunk 21.63%
of the time, which is five times higher than the ran-
dom baseline. This suggests that using translation
to approximate sentence meanings is a plausible
option. Note that one partial reason for the low
figures is the limitation of data resources. During
error analysis, we found linkers/connectors (e.g.
moreover, however) and modal auxiliaries (e.g.
689
can, had) are often marked redundant when they
actually carry undesirable meanings (Ex
6
, Ex
5
).
These cases comprise a 16% portion among our
model?s failures. Despite this limitation, the evalu-
ation still suggests that current approaches are not
ready for a full redundancy detection pipeline.
Second, we find that the choice of pivot lan-
guage does make a difference. Experimental result
suggests that the system tends to achieve higher
redundancy detection accuracy when using trans-
lations of a language more similar to English. In
particular, when using European languages (e.g.
German (de), French (fr), Hungarian (hu) etc.) as
pivot, the system performs much better than using
Asian languages (e.g. Chinese (zh-CN), Japanese
(ja), Thai (th) etc.). One reason for this phe-
nomenon is that the default Google translation out-
put in Asian languages (as well as the alignment
between English and these languages) are orga-
nized into characters, while characters are not the
minimum meaning component. For example, in
Chinese, ???? is the translation of ?explana-
tion?, but the two characters ??? and ??? mean
?to solve? and ?to release? respectively. In the
alignment output, this will cause certain words be-
ing associated with more or less alignments than
others. In this case, the number of alignments no
longer directly reflect how many meaning units
a certain word helps to express. To confirm this
phenomenon, we tried improving the system using
Simplified Chinese as the pivot language by merg-
ing characters together. In particular, we applied
Chinese tokenization (Chang et al., 2008), and
then merged alignments accordingly. This raised
the system?s accuracy from 17.74% to 20.11%.
Third, to better understand the salient features
of a successful redundancy measure, we experi-
mented with using different components in isola-
tion. We find that the language model component
is better at detecting redundant content words,
while the alignment analysis component is better
at detecting redundant function words. The lan-
guage model detects the function word redundan-
cies with a worse accuracy than the random base-
line; the alignment analysis component also has a
worse accuracy than the random baseline on con-
tent words. However, the English language model
and the alignment analysis result can build on top
of each other when we analyze the redundancies.
We also found that alignments help us to better
account for each word?s contribution to the ?mean-
ing? of the sentence. A linear combination of a
language model score and our proposed measure
based on analysis of alignments best captures re-
dundancy. However, as our experimental results
suggest, it is necessary both to use alignments in
translation outputs, and to use them in a good
way. Alignments help isolating fluency from the
meaning component ? making them easy to inte-
grate. As our experiments demonstrated, although
methods comparing Google round-trip transla-
tion?s output with the original sentence could lead
to a 10.69% prediction accuracy, it is harder to
combine it with the English language model. This
is partly because of the non-orthogonality of these
two measures ? the English language model has
already been used in the round-trip translation re-
sult. Also, an information theoretical interpreta-
tion of alignments is essential for the model?s suc-
cess. For example, a more naive way of using
alignment results, align #, which counts the num-
ber of alignments, leads to a much lower accuracy.
6 Conclusions
Despite the prevalence of redundant phrases in
ESL writings, there has not been much work in the
automatic detection of these problems. We con-
duct a first study on developing a computational
model of redundancies. We propose to account for
words/phrases redundancies by comparing an ESL
sentence with outputs from off-the-shelf machine
translation systems. We propose a redundancy
measure based on this comparison. We show that
by interpreting the translation outputs with IBM
Models, redundancies can be measured by a lin-
ear combination of a language model score and
the words? contribution to the sentence?s mean-
ing. This measure accounts for both the fluency
and completeness of a sentence after removing one
chunk. The proposed measure outperforms the di-
rect round-trip translation and a random baseline
by a large margin.
Acknowledgements
This work is supported by U.S. National Science
Foundation Grant IIS-0745914. We thank the
anonymous reviewers for their suggestions; we
also thank Joel Tetreault, Janyce Wiebe, Wencan
Luo, Fan Zhang, Lingjia Deng, Jiahe Qian, Nitin
Madnani and Yafei Wei for helpful discussions.
690
References
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
smt techniques. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, ACL-44, pages 249?
256, Sydney, Australia. Association for Computa-
tional Linguistics.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 224?232. Association for
Computational Linguistics.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1?11.
Will Coster and David Kauchak. 2011. Learning to
simplify sentences using wikipedia. In Proceedings
of the Workshop on Monolingual Text-To-Text Gen-
eration, pages 1?9, Portland, Oregon, June. Associ-
ation for Computational Linguistics.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical error correction with alternating structure opti-
mization. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 915?923, Portland, Oregon, USA.
Association for Computational Linguistics.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Matthieu Hermet and Alain D?esilets. 2009. Using first
and second language models to correct preposition
errors in second language authoring. In Proceedings
of the Fourth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 64?72.
Association for Computational Linguistics.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310?315. Association for Computa-
tional Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National
Conference on Artificial Intelligence and Twelfth
Conference on Innovative Applications of Artificial
Intelligence, pages 703?710. AAAI Press.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated grammatical error detection for
language learners. Synthesis lectures on human lan-
guage technologies, 3(1):1?134.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?10, pages 1068?1076, Cam-
bridge, Massachusetts. Association for Computa-
tional Linguistics.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 182?
190, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceed-
ings of the 11th Conference of the European Chap-
ter of the Association for Computational Linguistics,
volume 6, pages 297?304. Association for Compu-
tational Linguistics.
Alla Rozovskaya and Dan Roth. 2010. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 961?970, Cambridge, Massachusetts. As-
sociation for Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proceedings of the interna-
tional conference on spoken language processing,
volume 2, pages 901?904.
William Strunk. 1918. The elements of style / by
William Strunk, Jr.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selec-
tion and error detection. In Proceedings of the
ACL 2010 Conference Short Papers, ACLShort ?10,
pages 353?358, Uppsala, Sweden. Association for
Computational Linguistics.
691
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 301?304,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Phrase-Based Translation with Prototypes of Short Phrases
Frank Liberato?, Behrang Mohit?, Rebecca Hwa??
?Department of Computer Science ?Intelligent Systems Program
University of Pittsburgh
{frank,behrang,hwa@cs.pitt.edu}
Abstract
We investigate methods of generating addi-
tional bilingual phrase pairs for a phrase-
based decoder by translating short sequences
of source text. Because our translation task
is more constrained, we can use a model that
employs more linguistically rich features than
a traditional decoder. We have implemented
an example of this approach. Experimental re-
sults suggest that the phrase pairs produced by
our method are useful to the decoder, and lead
to improved sentence translations.
1 Introduction
Recently, there have been a number of successful
attempts at improving phrase-based statistical ma-
chine translation by exploiting linguistic knowledge
such as morphology, part-of-speech tags, and syn-
tax. Many translation models use such knowledge
before decoding (Xia and McCord, 2004) and dur-
ing decoding (Birch et al, 2007; Gimpel and Smith,
2009; Koehn and Hoang, 2007; Chiang et al, 2009),
but they are limited to simpler features for practi-
cal reasons, often restricted to conditioning left-to-
right on the target sentence. Traditionally, n-best
rerankers (Shen et al, 2004) have applied expen-
sive analysis after the translation process, on both
the source and target side, though they suffer from
being limited to whatever is on the n-best list (Hasan
et al, 2007).
We argue that it can be desirable to pre-translate
parts of the source text before sentence-level decod-
ing begins, using a richer model that would typically
be out of reach during sentence-level decoding. In
this paper, we describe a particular method of gen-
erating additional bilingual phrase pairs for a new
source text, using what we call phrase prototypes,
which are are learned from bilingual training data.
Our goal is to generate improved translations of rel-
atively short phrase pairs to provide the SMT de-
coder with better phrasal choices. We validate the
idea through experiments on Arabic-English trans-
lation. Our method produces a 1.3 BLEU score in-
crease (3.3% relative) on a test set.
2 Approach
Re-ranking tends to use expensive features of the en-
tire source and target sentences, s and t, and align-
ments, a, to produce a score for the translation. We
will call this scoring function ?(s, t, a). While ?(?)
might capture quite a bit of linguistic information, it
can be problematic to use this function for decoding
directly. This is due to both the expense of com-
puting it, and the difficulty in using it to guide the
decoder?s search. For example, a choice of ?(?) that
relies on a top-down parser is difficult to integrate
into a left-to-right decoder (Charniak et al, 2003).
Our idea is to use an expensive scoring function
to guide the search for potential translations for part
of a source sentence, S, even if translating all of it
isn?t feasible. We can then provide these transla-
tions to the decoder, along with their scores, to in-
corporate them as it builds the complete translation
of S. This differs from approaches such as (Och and
Ney, 2004) because we generate new phrase pairs in
isolation, rather than incorporating everything into
the sentence-level decoder. The baseline system is
the Moses phrase-based translation system (Koehn
301
et al, 2007).
2.1 Description of Our Scoring Function
For this work, we consider a scoring function based
on part-of-speech (POS) tags, ?POS(?). It oper-
ates in two steps: it converts the source and target
phrases, plus alignments, into what we call a phrase
prototype, then assigns a score to it based on how
common that prototype was during training.
Each phrase pair prototype is a tuple containing
the source prototype, target prototype, and align-
ment prototype, respectively. The source and tar-
get prototypes are a mix of surface word forms and
POS tags, such as the Arabic string ?NN Al JJ?,
or the English string ?NN NN?. For example, the
source and target prototypes above might be used in
the phrase prototype ?NN0 Al JJ1 , NN1 NN0?,
with the alignment prototype specified implicitly via
subscripts for brevity. For simplicity, the alignment
prototype is restricted to allow a source or target
word/tag to be unaligned, plus 1:1 alignments be-
tween them. We do not consider 1:many, many:1, or
many:many alignments in this work.
For any input ?s, t, a?, it is possible to con-
struct potentially many phrase prototypes from it by
choosing different subsets of the source and target
words to represent as POS tags. In the above ex-
ample, the Arabic determiner Al could be converted
into an unaligned POS tag, making the source pro-
totype ?NN DT JJ?. For this work, we convert all
aligned words into POS tags. As a practical con-
cern, we insist that unaligned words are always kept
as their surface form.
?POS(s, t, a) assign a score based on the proba-
bility of the resulting prototypes; more likely proto-
types should yield higher scores. We choose:
?POS(s, t, a) = p(SP,AP |TP ) ? p(TP,AP |SP )
where SP is the source prototype constructed from
s, t, a. Similarly, TP and AP are the target and
alignment prototypes, respectively.
To compute ?POS(?), we must build a model for
each of p(SP,AP |TP ) and p(TP,AP |SP ). To do
this, we start with a corpus of aligned, POS-tagged
bilingual text. We then find phrases that are consis-
tent with (Koehn et al, 2003). As we extract these
phrase pairs, we convert each into a phrase proto-
type by replacing surface forms with POS tags for
all aligned words in the prototype.
After we have processed the bilingual training
text, we have collected a set of phrase prototypes
and a count of how often each was observed.
2.2 Generating New Phrases
To generate phrases, we scan through the source text
to be translated, finding any span of source words
that matches the source prototype of at least one
phrase prototype. For each such phrase, and for each
phrase prototype which it matches, we generate all
target phrases which also match the target and align-
ment prototypes.
To do this, we use a word-to-word dictionary to
generate all target phrases which honor the align-
ments required by the alignment prototype. For each
source word which is aligned to a POS tag in the tar-
get prototype, we substitute all single-word transla-
tions in our dictionary1.
For each target phrase that we generate, we must
ensure that it matches the target prototype. We give
each phrase to a POS tagger, and check the resulting
tags against any tags in the target prototype. If there
are no mismatches, then the phrase pair is retained
for the phrase table, else it is discarded. In the latter
case, ?POS(?) would assign this pair a score of zero.
2.3 Computing Phrase Weights
In the Moses phrase table, each entry has four pa-
rameters: two lexical weights, and the two condi-
tional phrase probabilities p(s|t) and p(t|s). While
the lexical weights can be computed using the stan-
dard method (Koehn et al, 2003), estimating the
conditional phrase probabilities is not straightfor-
ward for our approach because they are not ob-
served in bilingual training data. Instead, we esti-
mate the maximum conditional phrase probabilities
that would be assigned by the sentence-level decoder
for this phrase pair, as if it had generated the tar-
get string from the source string using the baseline
phrase table2. To do this efficiently, we use some
1Since we required that all unaligned target words are kept
as surface forms in the target prototype, this is sufficient. If we
did not insist this, then we might be faced with the unenviable
task of choosing a target languange noun, without further guid-
ance from the source text.
2If we use these probabilities for our generated phrase pair?s
probability estimates, then the sentence-level decoder would see
302
simplifying assumptions: we do not restrict how of-
ten a source word is used during the translation, and
we ignore distortion / reordering costs. These admit
a simple dynamic programming solution.
We must also include the score from ?POS(?), to
give the decoder some idea of our confidence in the
generated phrase pair. We include the phrase pair?s
score as an additional weight in the phrase table.
3 Experimental Setup
The Linguistic Data Consortium Arabic-English
corpus23 is used to train the baseline MT system
(34K sentences, about one million words), and to
learn phrase prototypes. The LDC multi-translation
Arabic-English corpus (NIST2003)4 is used for tun-
ing and testing; the tuning set consists of the first
500 sentences, and the test set consists of the next
500 sentences. The language model is a 4-gram
model built from the English side of the parallel cor-
pus, plus the English side of the wmt07 German-
English and French-English news commentary data.
The baseline translation system is Moses (Koehn
et al, 2007), with the msd-bidirectional-fe
reordering model. Evaluation is done using the
BLEU (Papineni et al, 2001) metric with four ref-
erences. All text is lowercased before evaluation;
recasing is not used. We use the Stanford Arabic
POS Tagging system, based on (Toutanova et al,
2003)5. The word-to-word dictionary that is used in
the phrase generation step of our method is extracted
from the highest-scoring translations for each source
word in the baseline phrase table. For some closed-
class words, we use a small, manually constructed
dictionary to reduce the noise in the phrase table that
exists for very common words. We use this in place
of a stand-alone dictionary to reduce the need for
additional resources.
4 Experiments
To see the effect on the BLEU score of the result-
ing sentence-level translation, we vary the amount
of bilingual data used to build the phrase prototypes.
(approximately) no difference between building the generated
phrase using the baseline phrase table, or using our generated
phrase pair directly.
3Catalogue numbers LDC2004T17 and LDC2004T18
4Catalogue number: LDC2003T18
5It is available at http://nlp.stanford.edu/software/tagger.shtml
 0.36 0.37 0.38 0.39 0.4 0.41 0.42  
0 5
00 1
000 1
500 2
000 2
500 3
000 3
500 4
000 4
500 5
000 0.6 0.6
5 0.7 0.75 0.8 0.85 0.9 0.95 1
BLEU
Percentage of Generated Phrases in Phrase Table
# Bilin
gual T
raining
 Sente
nces
Effect
 of Bili
gual D
ata on
 Arabi
c Dev
elopm
ent Se
t
Baseli
ne BL
EU
Our A
pproa
ch BL
EU
% Gen
erated
 Phras
es
Figure 1: Bilingual training size vs. BLEU score (mid-
dle line, left axis) and phrase table composition (top line,
right axis) on Arabic Development Set. The baseline
BLEU score (bottom line) is included for comparison.
As we increase the amount of training data, we ex-
pect that the phrase prototype extraction algorithm
will observe more phrase prototypes. This will cause
it to generate more phrase pairs, introducing both
more noise and more good phrases into the phrase
table. Because quite a few phrase prototypes are
built in any case, we require that each is seen at
least three times before we use it to generate phrases.
Phrase prototypes seen fewer times than this are dis-
carded before phrase generation begins. Varying this
minimum support parameter does not affect the re-
sults noticeably.
The results on the tuning set are seen in Figure 1.
The BLEU score on the tuning set generally im-
proves as the amount of bilingual training data is in-
creased, even as the percentage of generated phrases
approaches 100%. Manual inspection of the phrase
pairs reveals that many are badly formed; this sug-
gests that the language model is doing its job in fil-
tering out disfluent phrases.
Using the first 5,000 bilingual training sentences
to train our model, we compare our method to the
baseline moses system. Each system was tuned via
MERT (Och, 2003) before running it on the test set.
The tuned baseline system scores 38.45. Including
our generated phrases improves this by 1.3 points to
39.75. This is a slightly smaller gain than exists in
the tuning set experiment, due in part that we did not
303
run MERT for experiment shown in Figure 1.
5 Discussion
As one might expect, generated phrases both
help and hurt individual translations. A sentence
that can be translated starting with the phrase
?korea added that the syrian prime
minister? is translated by the baseline system as
?korean | foreign minister | added |
that | the syrian?. While ?the syrian
foreign minister? is an unambiguous source
phrase, the baseline phrase table does not include it;
the language and reordering models must stitch the
translation together. Ours method generates ?the
syrian foreign minister? directly.
Generated phrases are not always correct. For
example, a generated phrase causes our system to
choose ?europe role?, while the baseline sys-
tem picks ?the role of | europe?. While
the same prototype is used (correctly) for reordering
Arabic ?NN0 JJ1? constructs into English as ?NN1
NN0? in many instances, it fails in this case. The lan-
guage model shares the blame, since it does not pre-
fer the correct phrase over the shorter one. In con-
trast, a 5-gram language model based on the LDC
Web IT 5-gram counts6 prefers the correct phrase.
6 Conclusion
We have shown that translating short spans of source
text, and providing the results to a phrase-based
SMT decoder can improve sentence-level machine
translation. Further, it permits us to use linguisti-
cally informed features to guide the generation of
new phrase pairs.
Acknowledgements
This work is supported by U.S. National Science Foun-
dation Grant IIS-0745914. We thank the anonymous re-
viewers for their suggestions.
References
A. Birch, M. Osborne, and P. Koehn. 2007. CCG su-
pertags in factored statistical machine translation. In
Proc. of the Second Workshop on SMT.
6Catalogue number LDC2006T13.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for statistical machine transla-
tion. In Proceedings of MT Summit IX.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In NAACL
?09: Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Assoc. for Computational Linguistics.
K. Gimpel and N.A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP.
S. Hasan, R. Zens, and H. Ney. 2007. Are very large n-
best lists useful for SMT? Proc. NAACL, Short paper,
pages 57?60.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology-Volume 1, page 54.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
Annual meeting-Association for Computational Lin-
guistics, volume 45, page 2.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4):417?449.
F.J. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual
Meeting on Assoc. for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
Association for Computational Linguistics.
L. Shen, A. Sarkar, and F.J. Och. 2004. Discrimina-
tive reranking for machine translation. In Proceedings
of the Joint HLT and NAACL Conference (HLT 04),
pages 177?184.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL ?03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology.
F. Xia and M. McCord. 2004. Improving a statistical mt
system with automatically learned rewrite patterns. In
COLING ?04: Proceedings of the 20th international
conference on Computational Linguistics.
304
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 599?604,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Improved Correction Detection in Revised ESL Sentences
Huichao Xue and Rebecca Hwa
Department of Computer Science,
University of Pittsburgh,
210 S Bouquet St, Pittsburgh, PA 15260, USA
{hux10,hwa}@cs.pitt.edu
Abstract
This work explores methods of automat-
ically detecting corrections of individual
mistakes in sentence revisions for ESL
students. We have trained a classifier
that specializes in determining whether
consecutive basic-edits (word insertions,
deletions, substitutions) address the same
mistake. Experimental result shows that
the proposed system achieves an F
1
-score
of 81% on correction detection and 66%
for the overall system, out-performing the
baseline by a large margin.
1 Introduction
Quality feedback from language tutors can
help English-as-a-Second-Language (ESL) stu-
dents improve their writing skills. One of the tu-
tors? tasks is to isolate writing mistakes within
sentences, and point out (1) why each case is
considered a mistake, and (2) how each mistake
should be corrected. Because this is time consum-
ing, tutors often just rewrite the sentences with-
out giving any explanations (Fregeau, 1999). Due
to the effort involved in comparing revisions with
the original texts, students often fail to learn from
these revisions (Williams, 2003).
Computer aided language learning tools offer
a solution for providing more detailed feedback.
Programs can be developed to compare the stu-
dent?s original sentences with the tutor-revised
sentences. Swanson and Yamangil (2012) have
proposed a promising framework for this purpose.
Their approach has two components: one to de-
tect individual corrections within a revision, which
they termed correction detection; another to deter-
mine what the correction fixes, which they termed
error type selection. Although they reported a
high accuracy for the error type selection classifier
alone, the bottleneck of their system is the other
component ? correction detection. An analysis of
their system shows that approximately 70% of the
system?s mistakes are caused by mis-detections
in the first place. Their correction detection al-
gorithm relies on a set of heuristics developed
from one single data collection (the FCE corpus
(Yannakoudakis et al, 2011)). When determining
whether a set of basic-edits (word insertions, dele-
tions, substitutions) contributes to the same cor-
rection, these heuristics lack the flexibility to adapt
to a specific context. Furthermore, it is not clear if
the heuristics will work as well for tutors trained
to mark up revisions under different guidelines.
We propose to improve upon the correction de-
tection component by training a classifier that de-
termines which edits in a revised sentence address
the same error in the original sentence. The classi-
fier can make more accurate decisions adjusted to
contexts. Because the classifier were trained on re-
visions where corrections are explicitly marked by
English experts, it is also possible to build systems
adjusted to different annotation standards.
The contributions of this paper are: (1) We show
empirically that a major challenge in correction
detection is to determine the number of edits that
address the same error. (2) We have developed a
merging model that reduces mis-detection by 1/3,
leading to significant improvement in the accu-
racies of combined correction detection and er-
ror type selection. (3) We have conducted experi-
ments across multiple corpora, indicating that the
proposed merging model is generalizable.
2 Correction Detection
Comparing a student-written sentence with its re-
vision, we observe that each correction can be de-
composed into a set of more basic edits such as
word insertions, word deletions and word substi-
tutions. In the example shown in Figure 1, the
correction ?to change ? changing? is composed
of a deletion of to and a substitution from change
599
Figure 1: Detecting corrections from revisions. Our system detects individual corrections by comparing the original sentence
with its revision, so that each correction addresses one error. Each polygon corresponds to one correction; the labels are codes
of the error types. The codes follow the annotation standard in FCE corpus (Nicholls, 2003). In this example, W is incorrect
Word order; UT is Unnecessary preposiTion; FV is wrong Verb Form; RN is Nnoun needs to be Replaced; ID is IDiom error.
Figure 2: A portion of the example from Figure 1 undergoing the two-step correction detection process. The basic edits are
indicated by black polygons. The corrections are shown in red polygons.
(a) (b)
Figure 3: Basic edits extracted by the edit-distance algo-
rithm (Levenshtein, 1966) do not necessarily match our lin-
guistic intuition. The ideal basic-edits are shown in Figure
3a, but since the algorithm only cares about minimizing the
number of edits, it may end up extracting basic-edits shown
in Figure 3b.
to changing; the correction ?moment ? minute?
is itself a single word substitution. Thus, we can
build systems to detect corrections which operates
in two steps: (1) detecting the basic edits that took
place during the revision, and (2) merging those
basic edits that address the same error. Figure 2 il-
lustrates the process for a fragment of the example
sentence from Figure 1.
In practice, however, this two-step approach
may result in mis-detections due to ambiguities.
Mis-detections may be introduced from either
steps. While detecting basic edits, Figures 3 gives
an example of problems that might arise. Because
the Levenshtein algorithm only tries to minimize
the number of edits, it does not care whether the
edits make any linguistic sense. For merging basic
edits, Swanson and Yamangil applied a distance
heuristic ? basic-edits that are close to each other
(e.g. basic edits with at most one word lying in
between) are merged. Figure 4 shows cases for
which the heuristic results in the wrong scope.
These errors caused their system to mis-detect
30% of the corrections. Since mis-detected cor-
rections cannot be analyzed down the pipeline,
(a) The basic edits are addressing the same problem. But
these basic edits are non-adjacent, and therefore not merged by
S&Y?s algorithm.
(b) The basic edits in the above two cases address different
problems though they are adjacent. S&Y?s merging algorithm
incorrectly merges them.
Figure 4: Merging mistakes by the algorithm proposed in
Swanson and Yamangil (2012) (S&Y), which merges adja-
cent basic edits.
the correction detection component became the
bottle-neck of their overall system. Out of the
42% corrections that are incorrectly analyzed
1
,
30%/42%?70% are caused by mis-detections in
the first place. An improvement in correction de-
tection may increase the system accuracy overall.
We conducted an error analysis to attribute er-
rors to either step when the system detects a wrong
set of corrections for a sentence. We examine
the first step?s output. If the resulting basic ed-
its do not match with those that compose the ac-
tual corrections, we attribute the error to the first
step. Otherwise, we attribute the error to the sec-
ond step. Our analysis confirms that the merging
step is the bottleneck in the current correction de-
tection system ? it accounts for 75% of the mis-
detections. Therefore, to effectively reduce the
algorithm?s mis-detection errors, we propose to
1
Swanson and Yamangil reported an overall system with
58% F-score.
600
build a classifier to merge with better accuracies.
Other previous tasks also involve comparing
two sentences. Unlike evaluating grammar er-
ror correction systems (Dahlmeier and Ng, 2012),
correction detection cannot refer to a gold stan-
dard. Our error analysis above also highlights our
task?s difference with previous work that identify
corresponding phrases between two sentences, in-
cluding phrase extraction (Koehn et al, 2003) and
paraphrase extraction (Cohn et al, 2008). They
are fundamentally different in that the granularity
of the extracted phrase pairs is a major concern
in our work ? we need to guarantee each detected
phrase pair to address exactly one writing prob-
lem. In comparison, phrase extraction systems
aim to improve the end-to-end MT or paraphrasing
systems. A bigger concern is to guarantee the ex-
tracted phrase pairs are indeed translations or para-
phrases. Recent work therefore focuses on identi-
fying the alignment/edits between two sentences
(Snover et al, 2009; Heilman and Smith, 2010).
3 A Classifier for Merging Basic-Edits
Figures 4 highlights the problems with indiscrimi-
nantly merging basic-edits that are adjacent. Intu-
itively, it seems that the decision should be more
context dependent. Certain patterns may indicate
that two adjacent basic-edits are a part of the same
correction while others may indicate that they each
address a different problem. For example, in Fig-
ure 5a, when the insertion of one word is followed
by the deletion of the same word, the insertion
and deletion are likely addressing one single error.
This is because these two edits would combine to-
gether as a word-order change. On the other hand,
in Figure 5b, if one edit includes a substitution be-
tween words with the same POS?s, then it is likely
fixing a word choice error by itself. In this case, it
should not be merged with other edits.
To predict whether two basic-edits address the
same writing problem more discriminatively, we
train a Maximum Entropy binary classifier based
on features extracted from relevant contexts for
the basic edits. We use features in Table 1 in the
proposed classifier. We design the features to in-
dicate: (A) whether merging the two basic-edits
matches the pattern for a common correction. (B)
whether one basic-edit addresses one single error.
We train the classifier using samples extracted
from revisions where individual corrections are
explicitly annotated. We first extract the basic-
(a) The pattern indicates that
the two edits address the
same problem
(b) The pattern indicates that
the two edits do not address
the same problem
Figure 5: Patterns indicating whether two edits address the
same writing mistake.
Figure 6: Extracting training instances for the merger. Our
goal is to train classifiers to tell if two basic edits should
be merged (True or False). We break each correction (outer
polygons, also colored in red) in the training corpus into a set
of basic edits (black polygons). We construct an instance for
each consecutive pair of basic edits. If two basic edits were
extracted from the same correction, we will mark the outcome
as True, otherwise we will mark the outcome as False.
edits that compose each correction. We then create
a training instance for each pair of two consecutive
basic edits: if two consecutive basic edits need to
be merged, we will mark the outcome as True, oth-
erwise it is False. We illustrate this in Figure 6.
4 Experimental Setup
We combine Levenshtein algorithm with different
merging algorithms for correction detection.
4.1 Dataset
An ideal data resource would be a real-world col-
lection of student essays and their revisions (Tajiri
et al, 2012). However, existing revision corpora
do not have the fine-grained annotations necessary
for our experimental gold standard. We instead
use error annotated data, in which the corrections
were provided by human experts. We simulate the
revisions by applying corrections onto the original
sentence. The teachers? annotations are treated as
gold standard for the detailed corrections.
We considered four corpora with different ESL
populations and annotation standards, including
FCE corpus (Yannakoudakis et al, 2011), NU-
CLE corpus (Dahlmeier et al, 2013), UIUC cor-
pus
2
(Rozovskaya and Roth, 2010) and HOO2011
corpus (Dale and Kilgarriff, 2011). These corpora
all provide experts? corrections along with error
2
UIUC corpus contains annotations of essays collected
from ICLE (Granger, 2003) and CLEC (Gui and Yang, 2003).
601
Type name description
A
gap-between-edits Gap between the two edits. In particular, we use the number of words between the two edits?
original words, as well as the revised words. Note that Swanson and Yamangil?s approach is a
special case that only considers if the basic-edits have zero gap in both sentences.
tense-change We detect patterns such as: if the original-revision pair matches the pattern ?V-ing?to V?.
word-order-error Whether the basic-edits? original word set and the revised word set are the same (one or zero).
same-word-set If the original sentence and the revised sentence have the same word set, then it?s likely that all
the edits are fixing the word order error.
revised-to The phrase comprised of the two revised words.
B
editdistance=1 If one basic-edit is a substitution, and the original/revised word only has 1 edit distance, it
indicates that the basic-edit is fixing a misspelling error.
not-in-dict If the original word does not have a valid dictionary entry, then it indicates a misspelling error.
word-choice If the original and the revised words have the same POS, then it is likely fixing a word choice
error.
preposition-error Whether the original and the revised words are both prepositions.
Table 1: Features used in our proposed classifier.
corpus sentences
sentences with? 2 corrections
revised sentences
FCE 33,900 53.45%
NUCLE 61,625 48.74%
UIUC 883 61.32%
HOO2011 966 42.05%
Table 2: Basic statistics of the corpora that we consider.
type mark-ups. The basic statistics of the corpora
are shown in Table 2. In these corpora, around half
of revised sentences contains multiple corrections.
We have split each corpus into 11 equal parts. One
part is used as the development dataset; the rest are
used for 10-fold cross validation.
4.2 Evaluation Metrics
In addition to evaluating the merging algorithms
on the stand-alone task of correction detection, we
have also plugged in the merging algorithms into
an end-to-end system in which every automati-
cally detected correction is further classified into
an error type. We replicated the error type selector
described in Swanson and Yamangil (2012). The
error type selector?s accuracies are shown in Table
3
3
. We compare two merging algorithms, com-
bined with Levenshtein algorithm:
S&Y The merging heuristic proposed by Swan-
son and Yamangil, which merges the adjacent ba-
sic edits into single corrections.
MaxEntMerger We use the Maximum Entropy
classifier to predict whether we should merge the
two edits, as described in Section 3
4
.
We evaluate extrinsically the merging compo-
nents? effect on overall system performance by
3
Our replication has a slightly lower error type selection
accuracy on FCE (80.02%) than the figure reported by Swan-
son and Yamangil (82.5%). This small difference on error
type selection does not affect our conclusions about correc-
Corpus Error Types Accuracy
FCE 73 80.02%
NUCLE 27 67.36%
UIUC 8 80.23%
HOO2011 38 64.88%
Table 3: Error type selection accuracies on different cor-
pora. We use a Maximum Entropy classifier along with fea-
tures suggested by Swanson and Yamangil for this task. The
reported figures come from 10-fold cross validations on dif-
ferent corpora.
comparing the boundaries of system?s detected
corrections with the gold standard. We evaluate
both (1) the F-score in detecting corrections (2)
the F-score in correctly detecting both the correc-
tions? and the error types they address.
5 Experiments
We design experiments to answer two questions:
1. Do the additional contextual information
about correction patterns help guide the merging
decisions? How much does a classifier trained for
this task improve the system?s overall accuracy?
2. How well does our method generalize over re-
visions from different sources?
Our major experimental results are presented in
Table 4 and Table 6. Table 4 compares the over-
all educational system?s accuracies with different
merging algorithms. Table 6 shows the system?s
F
1
score when trained and tested on different cor-
pora. We make the following observations:
First, Table 4 shows that by incorporating cor-
rection patterns into the merging algorithm, the
tion detection.
4
We use the implementation at http://homepages.
inf.ed.ac.uk/lzhang10/maxent_toolkit.
html.
602
errors in correction detection step were reduced.
This led to a significant improvement on the over-
all system?s F
1
-score on all corpora. The improve-
ment is most noticeable on FCE corpus, where
the error in correction detection step was reduced
by 9%. That is, one third of the correction mis-
detections were eliminated. Table 5 shows that the
number of merging errors are significantly reduced
by the new merging algorithm. In particular, the
number of false positives (system proposes merges
when it should not) is significantly reduced.
Second, our proposed model is able to gener-
alize over different corpora. As shown in Table
6. The models built on corpora can generally im-
prove the correction detection accuracy
5
. Mod-
els built on the same corpus generally perform
the best. Also, as suggested by the experimental
result, among the four corpora, FCE corpus is a
comparably good resource for training correction
detection models with our current feature set. One
reason is that FCE corpus has many more training
instances, which benefits model training. We tried
varying the training dataset size, and test it on dif-
ferent corpora. Figure 7 suggests that the model?s
accuracies increase with the training corpus size.
6 Conclusions
A revision often contains multiple corrections that
address different writing mistakes. We explore
building computer programs to accurately detect
individual corrections in one single revision. One
major challenge lies in determining whether con-
secutive basic-edits address the same mistake. We
propose a classifier specialized in this task. Our
experiments suggest that: (1) the proposed classi-
fier reduces correction mis-detections in previous
systems by 1/3, leading to significant overall sys-
tem performance. (2) our method is generalizable
over different data collections.
Acknowledgements
This work is supported by U.S. National Sci-
ence Foundation Grant IIS-0745914. We thank
the anonymous reviewers for their suggestions;
we also thank Homa Hashemi, Wencan Luo, Fan
Zhang, Lingjia Deng, Wenting Xiong and Yafei
Wei for helpful discussions.
5
We currently do not evaluate the end-to-end system over
different corpora. This is because different corpora employ
different error type categorization standards.
Method Corpus Correction
Detection F
1
Overall
F
1
-score
S&Y FCE 70.40% 57.10%
MaxEntMerger FCE 80.96% 66.36%
S&Y NUCLE 61.18% 39.32%
MaxEntMerger NUCLE 63.88% 41.00%
S&Y UIUC 76.57% 65.08%
MaxEntMerger UIUC 82.81% 70.55%
S&Y HOO2011 68.73% 50.95%
MaxEntMerger HOO2011 75.71% 56.14%
Table 4: Extrinsic evaluation, where we plugged the two
merging models into an end-to-end feedback detection sys-
tem by Swanson and Yamangil.
Merging algorithm TP FP FN TN
S&Y 33.73% 13.46% 5.71% 47.10%
MaxEntMerger 36.04% 3.26% 3.41% 57.30%
Table 5: Intrinsic evaluation, where we evaluate the pro-
posed merging model?s prediction accuracy on FCE corpus.
This table shows a breakdown of true-positives (TP), false-
positives (FP), false-negatives (FN) and true-negatives (TN)
for the system built on FCE corpus.
training
testing
FCE NUCLE UIUC HOO2011
S&Y 70.44 61.18% 76.57% 68.73%
FCE 80.96% 61.26% 83.07% 75.43%
NUCLE 74.53% 63.88% 78.57% 74.73%
UIUC 77.25% 58.21% 82.81% 70.83%
HOO2011 71.94% 54.99% 71.19% 75.71%
Table 6: Correction detection experiments by building the
model on one corpus, and applying it onto another. We
evaluate the correction detection performance with F
1
score.
When training and testing on the same corpus, we run a 10-
fold cross validation.
101 102 103 104 105Number of sentences in the training corpus0.40
0.450.50
0.550.60
0.650.70
0.750.80
F 1 sco
re
HOO2011UIUCFCENUCLE
Figure 7: We illustrate the performance of correction detec-
tion systems trained on subsets of FCE corpus. Each curve in
this figure represents the F
1
-scores for correction detection
of the model trained on a subset of FCE and tested on differ-
ent corpora. When testing on FCE, we used
1
11
of the FCE
corpus, which we kept as development data.
603
References
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4):597?614.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
568?572, Montr?eal, Canada, June. Association for
Computational Linguistics.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS corpus of learner english. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22?31.
Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proceed-
ings of the 13th European Workshop on Natural Lan-
guage Generation, pages 242?249. Association for
Computational Linguistics.
Laureen A Fregeau. 1999. Preparing ESL students
for college writing: Two case studies. The Internet
TESL Journal, 5(10).
Sylviane Granger. 2003. The International Corpus of
Learner English: a new resource for foreign lan-
guage learning and teaching and second language
acquisition research. Tesol Quarterly, 37(3):538?
546.
Shicun Gui and Huizhong Yang. 2003. Zhong-
guo xuexizhe yingyu yuliaohu.(chinese learner en-
glish corpus). Shanghai: Shanghai Waiyu Jiaoyu
Chubanshe.
Michael Heilman and Noah A Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 1011?1019.
Association for Computational Linguistics.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 48?54.
Association for Computational Linguistics.
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707710.
D. Nicholls. 2003. The Cambridge Learner Corpus:
Error coding and analysis for lexicography and ELT.
In Proceedings of the Corpus Linguistics 2003 con-
ference, pages 572?581.
Alla Rozovskaya and Dan Roth. 2010. Annotating
ESL errors: Challenges and rewards. In Proceed-
ings of the NAACL HLT 2010 fifth workshop on inno-
vative use of NLP for building educational applica-
tions, pages 28?36. Association for Computational
Linguistics.
Matthew G Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Ben Swanson and Elif Yamangil. 2012. Correction
detection and error type selection as an ESL educa-
tional aid. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 357?361, Montr?eal, Canada, June.
Association for Computational Linguistics.
Toshikazu Tajiri, Mamoru Komachi, and Yuji Mat-
sumoto. 2012. Tense and aspect error correction for
ESL learners using global context. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers-Volume 2,
pages 198?202. Association for Computational Lin-
guistics.
Jason Gordon Williams. 2003. Providing feedback
on ESL students written assignments. The Internet
TESL Journal, 4(10).
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading esol texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 180?189. Association for Computational
Linguistics.
604
Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012),
pages 80?88, Jeju, Republic of Korea, 13 July 2012. c?2012 Association for Computational Linguistics
Recognizing Arguing Subjectivity and Argument Tags
Alexander Conrad, Janyce Wiebe, and Rebecca Hwa
Department of Computer Science
University of Pittsburgh
Pittsburgh PA, 15260, USA
{conrada,wiebe,hwa}@cs.pitt.edu
Abstract
In this paper we investigate two distinct
tasks. The first task involves detecting ar-
guing subjectivity, a type of linguistic sub-
jectivity on which relatively little work has
yet to be done. The second task involves
labeling instances of arguing subjectivity
with argument tags reflecting the concep-
tual argument being made. We refer to
these two tasks collectively as ?recogniz-
ing arguments?. We develop a new anno-
tation scheme and assemble a new anno-
tated corpus to support our learning ef-
forts. Through our machine learning ex-
periments, we investigate the utility of a
sentiment lexicon, discourse parser, and
semantic similarity measures with respect
to recognizing arguments. By incorpo-
rating information gained from these re-
sources, we outperform a unigram baseline
by a significant margin. In addition, we ex-
plore a two-phase approach to recognizing
arguments, with promising results.
1 Introduction
Subjectivity analysis is a thriving field within
natural language processing. However, most
research into subjectivity has focused on sen-
timent with respect to concrete things such
as product debates (e.g., (Somasundaran and
Wiebe, 2009), (Yu et al, 2011)) and movie re-
views (e.g., (He et al, 2011), (Maas et al, 2011),
(Pang and Lee, 2004)). Analysis often follows
the opinion-target paradigm, in which expres-
sions of sentiment are assessed with respect to
the aspects of the object(s) under consideration
towards which they are targeted. For example,
in the domain of smartphone reviews, aspects
could include product features such as the key-
board, screen quality, and battery life.
Although sentiment analysis is interesting
and important in its own right, this paradigm
does not seem to be the best match for fine-
grained analysis of ideological domains. While
sentiment is also present in documents from
this domain, previous work (Somasundaran and
Wiebe, 2010) has found that arguing subjec-
tivity, a less-studied form of subjectivity, is
more frequently employed and more relevant
for a robust assessment of ideological positions.
Whereas sentiment conveys the polarity of a
writer?s affect towards a topic, arguing subjec-
tivity is a type of linguistic subjectivity in which
a person expresses a controversial belief about
what is true or what action ought to be taken
regarding a central contentious issue (Somasun-
daran, 2010). For example, consider this sen-
tence about health care reform:
(1) Almost everyone knows that we
must start holding insurance compa-
nies accountable and give Americans a
greater sense of stability and security
when it comes to their health care.
In a traditional opinion-target or sentiment-
topic paradigm, perhaps this sentence could be
labeled as containing a negative sentiment to-
wards a topic representing ?insurance compa-
nies?, or a positive sentiment towards a topic
representing ?stability? or ?security?. However,
a reader of a political editorial or blog may be
more interested in why the author is negative to-
80
wards insurers, and how the author proposes to
improve stability of the healthcare system. By
focusing on the arguments conveyed through ar-
guing subjectivity, we aim to capture these kind
of conceptual reasons an author provides when
arguing for his or her position.
However, identifying when someone is arguing
is only part of the challenge. Since arguing sub-
jectivity is used to express arguments, the next
natural step is to identify the argument being
expressed through each instance of arguing sub-
jectivity. To illustrate this distinction, consider
the following three example spans:
(2) the bill is a job destroyer
(3) President Obamas signature do-
mestic policy will throw 100,000 peo-
ple out of work come January
(4) he can?t expand his business be-
cause he can?t afford the burden of
Obamacare
Each of these examples contains arguing
subjectivity, but more importantly, each ex-
presses roughly the same idea, namely, that the
recently-passed health care reform bill will cause
economic harm. This latent, shared idea giving
rise to each of the three spans is what we mean
by ?argument tag?.
However, although all three are related, exam-
ple spans (2) and (3) are more similar than (4)
in terms of the notions they convey: while the
first two explicitly are concerned with the loss
of jobs, the last focuses on business expansion
and the economy as a whole. If we were to tag
these three spans with respect to the argument
that each is making, should they all receive the
same tag, or should (4)?s tag be different?
To address these challenges, we propose in this
work a new annotation scheme for identifying
arguing subjectivity and a hierarchical model for
organizing ?argument tags?. In our hierarchical
model, (4) would receive a different tag from (2)
and (3), but because of the tags? relatedness all
would share the same parent tag.
In addition to presenting this new scheme for
labeling arguing subjectivity, we also explore
sentiment, discourse, and distributional similar-
ity as tools to enhance identification and classi-
fication of arguing subjectivity. Finally, we also
investigate splitting the arguing subjectivity de-
tection task up into two distinct phases: iden-
tifying expressions of arguing subjectivity, and
labelling each such expression with an appropri-
ate argument tag.
Since no corpora annotated for arguing sub-
jectivity yet exist, we gather and annotate a cor-
pus of blog posts and op-eds about a contro-
versial topic, namely, the recently-passed ?Oba-
maCare? health care reform bill.
2 Annotation Scheme
We designed our annotation scheme with two
goals in mind: identifying all spans of text which
express arguing subjectivity, and labelling each
such span with an argument tag. To address
the first goal, our annotators manually identified
and annotated spans of text containing arguing
subjectivity using the GATE environment1. An-
notators were instructed to identify spans of 1
sentence or less in which a writer ?conveys a
controversial private state concerning what she
believes to be true or what action she believes
should be taken? concerning the health care re-
form debate. To train our annotators to recog-
nize arguing subjectivity, we performed several
rounds of practice on a separate dataset. Be-
tween each round, our annotators met to discuss
their annotations and resolve disagreements.
As a heuristic to help distinguish between bor-
derline sentences, we advised our annotators to
imagine disputants from each side writing the
sentence in isolation. If a disputant from either
side could conceivably write the sentence, then
the sentence is likely objective. For example,
statements of accepted facts and statistics gen-
erally fall into this category. However, if only
one side could conceivably be the author of the
sentence, it is highly likely that the sentence ex-
presses a controversial belief relevant to the de-
bate and thus should be labeled as subjective.
Next, the annotators labeled each arguing
span with an argument tag. As illustrated in
earlier examples, an argument tag represents a
1http://gate.ac.uk/
81
controversial abstract belief expressed through
arguing subjectivity. Since the meanings of
many tags may be related, we organize these
tags in a hierarchical ?stance structure?. A
stance structure is a tree-based data structure
containing all of the argument tags associated
with a particular debate, organizing those tags
using ?is-a? relationships. Our stance structure
contains two levels of argument tags: upper-
level ?primary? argument tags and lower-level
?secondary? tags. Each primary tag has one of
the stances (either ?pro? or ?anti? in our case)
as its parent, while each secondary tag has a
primary tag as its parent2.
Political science ?arguing dimension? ap-
proaches to debate framing analysis served, in
part, as an inspiration for our stance structure
(Baumgartner et al, 2008). Also, as illustrated
in Section 1, this approach permits us additional
flexibility, supporting classification at different
levels of specificity depending on the task at
hand and the amount of data available. We en-
vision a future scenario in which a community of
users collaboratively builds a stance structure to
represent a new topic or debate, or in which an-
alysts build a stance structure to categorize the
issues expressed towards a proposed law, such
as in the context of e-rulemaking (Cardie et al,
2008).
Because each stance contains a large number
of argument tags, we back-off from each sec-
ondary argument tag to its primary argument
parent for the classification experiments. We
chose to do this in order to ensure that we have
a sufficient amount of data with which to train
the classifier.
3 Dataset
For this study, we chose to focus on online ed-
itorials and blog posts concerning the ongoing
debate over health insurance reform legislation
in the United States. Our intuition is that blogs
and editorials represent a genre rich in both
2Our stance structure contains an additional ?aspect?
level consisting of a-priori categories adopted from politi-
cal science research. However, we do not utilize this level
of the stance structure in this work.
?pro? documents 37
?pro? sentences 1,222
?anti? documents 47
?anti? sentences 1,456
total documents 84
total sentences 2,678
Table 1: Dataset summary statistics.
arguing subjectivity
objective 683
subjective 588
argument labels
no label 683
improves healthcare access 130
improves healthcare affordability 104
people dont know truth
about bill
75
controls healthcare costs 54
improves quality of healthcare 52
helps economy 51
bill should be passed 43
other argument 79
Table 2: Arguing and argument label statistics for
the ?pro? stance.
subjectivity and arguments. We collected docu-
ments written both before and after the passage
of the final ?Patient Protection and Affordable
Care Act? bill using the ?Google Blog Search?3
and ?Daily Op Ed?4 search portals. By choosing
a relatively broad time window, from early 2009
to late 2011, we aimed to capture a wide range
of arguments expressed throughout the debate.
The focus of this paper is on sentence-level
argument detection rather than document-level
stance classification (e.g., (Anand et al, 2011),
(Park et al, 2011), (Somasundaran and Wiebe,
2010), (Burfoot et al, 2011)). We treat stance
classification as a separate step preceding argu-
ing subjectivity detection, and thus provide or-
acle stance labels for our data.
We treat documents written from the ?pro?
3http://www.google.com/blogsearch
4http://www.dailyoped.com/
82
arguing subjectivity
objective 913
subjective 575
argument labels
no label 913
diminishes quality of care 122
too expensive 67
unpopular 60
hurts economy 55
expands govt 52
bill is politically motivated 44
other reforms more appropriate 35
other argument 140
Table 3: Arguing and argument label statistics for
the ?anti? stance.
stance and documents written from the ?anti?
stance as separate datasets. Being written from
different positions, the two stances will have dif-
ferent argument labels and may employ different
styles of arguing subjectivity. Table 1 provides
an overview of the size of this dataset. Summary
statistics concerning the density of arguing and
argument labels in the two sides of the dataset
is presented in Tables 2 and 3. However, since
it can be difficult to summarize a complex ar-
gument in a short phrase, many of these labels
by themselves do not clearly convey the meaning
they are meant to represent. To better illustrate
the meanings of some of the more ambiguous la-
bels, Table 4 presents several annotated example
spans for some of the more unclear ambiguous
argument labels.
4 Agreement Study
One of our authors performed annotation of our
corpus, the broad outlines of which are sketched
in the previous section. However, to assess inter-
annotator agreement for this annotation scheme,
we recruited a non-author to independently an-
notate a subset of our corpus consisting of 384
sentences across 10 documents. This non-author
both identified spans of arguing subjectivity and
assigned argument tags. She was given a stance
structure from which to select argument tags.
improves healthcare access
?Our reform will prohibit insurance compa-
nies from denying coverage because of your
medical history.?
?Let?s also not overlook the news from last
week about the millions of younger Americans
who are getting coverage thanks to consumer
protections that are now in place.?
improves healthcare affordability
? new health insurance exchanges will offer
competitive, consumer-centered health insur-
ance marketplaces...?
?Millions of seniors can now afford medication
they would otherwise struggle to pay for.?
people dont know truth about bill
?...the cynics and the naysayers will continue
to exploit fear and concerns for political gain.?
?Republican leaders, who see opportunities
to gain seats in the elections, have made
clear that they will continue to peddle fictions
about a government takeover of the health
care system and about costs too high to bear.?
unpopular
?The 1,000-page monstrosity that emerged in
various editions from Congress was done in by
widespread national revulsion...?
?Support for ObamaCare?s repeal is broad,
and includes one group too often overlooked
during the health care debate: America?s doc-
tors.?
expands govt
?...the real goal of the health care overhaul
was to enact the largest entitlement program
in history...?
?the new bureaucracy the health care legisla-
tion creates is so complex and indiscriminate
that its size and cost is ?currently unknow-
able.? ?
bill is politically motivated
?...tawdry backroom politics were used to sell
off favors in exchange for votes.?
?From the wildly improper gifts to senators
like Nebraska?s Ben Nelson to this week?s
backroom deals for unions...?
Table 4: Example annotated spans for several argu-
ment labels.
83
metric recall precision f-measure
agr 0.677 0.690 0.683
kappa for overlapping annotations 0.689
Table 5: Inter-annotator span agr (top) and argu-
ment label kappa on overlapping spans (bottom).
In assessing inter-annotator agreement on this
subset of the corpus, we must address two levels
of agreement, arguing spans and argument tags.
At first glance, how to assess agreement
of annotated arguing spans is not obvious.
Because our annotation scheme did not enforce
strict boundaries, we hypothesized that both
annotators would both frequently see an in-
stance of arguing subjectivity within a local
region of text, but would disagree with respect
to where the arguing begins and ends. Thus, we
adopt from (Wilson and Wiebe, 2003) the agr
directional agreement metric to measure the
degree of annotation overlap. Given two sets
of spans A and B annotated by two different
annotators, this metric measures the fraction
of spans in A which at least partially overlap
with any spans in B. Specifically, agreement is
computed as:
agr(A B) = A matching BA
When A is the gold standard set of annota-
tions, agr is equivalent to recall. Similarly, when
B is the gold standard, agr is equivalent to pre-
cision. For this evaluation, we treat the dataset
annotated by our primary annotator as the gold
standard. Table 5 presents these agr scores and
f-measures for the arguing spans.
Second, we measure agreement with respect
to the argument tags assigned by the two an-
notators. Continuing to follow the methodol-
ogy of (Wilson and Wiebe, 2003), we look at
each pair of annotations, one from each anno-
tator, which share at least a partial overlap.
For each such pair, we assess whether the two
spans share the same primary argument tag.
Scores for primary argument label agreement in
terms of Cohen?s kappa are also presented in Ta-
ble 5. Since this kappa score falls within the
range of 0.67 ? K ? 0.8, according to Krippen-
dorf?s scale (Krippendorff, 2004) this allows us
to draw tentative conclusions concerning a sig-
nificant level of tag agreement.
5 Methods
As discussed earlier, recognizing arguments can
be thought of in terms of two related but dif-
ferent tasks: recognizing a type of subjectivity,
and labeling instances of that subjectivity with
tags. We refer to the binary arguing subjectiv-
ity detection task as ?arg?, and to the multi-
class argument labeling task as ?tag?. For the
?tag? task, we create eight classes: one for each
of the seven most-frequent labels, and an eighth
into which we agglomerate the remaining less-
frequent labels. We only consider the sentences
known to be subjective (via oracle information)
for the ?tag? task.
We also perform a ?combined? task. This
third task is conceptually similar to the ?tag?
task, except that all sentences are considered
rather than only the subjective sentences. In ad-
dition to the eight classes used by ?tag?, ?com-
bined? adds an additional class for non-arguing
sentences. Finally, we also perform a two-stage
?arg+tag? task. In this two-stage task, the in-
stances labeled as subjective by the ?arg? clas-
sifier are passed as input to the ?tag? classifier.
The intuition behind this two-phase approach is
that the features most useful for identifying ar-
guing subjectivity may not be the most useful
for discriminating between argument tags, and
vice versa. For all of our classification tasks,
we treat both the ?pro? and ?anti? stances
separately, building separate classifiers for each
stance for each of the above tasks.
In general, we perform single-label classifi-
cation at the sentence level. However, sen-
tences containing multiple labels pose a chal-
lenge. Since this was an early exploratory work
on a very difficult task, we decided to handle
this situation by splitting sentences containing
multiple labels into separate instances for the
purpose of learning, assigning a single label to
each instance. However, only about 3% of the
sentences in our corpus contained multiple la-
84
bels. Thus, replacing this splitting step in the
future with another method that does not re-
quire oracle information, such as choosing the
label which covers the most words in the sen-
tence, is a reasonable simplification of the task.
Since discourse actions, such as contrasting,
restating, and identifying causation, play a sub-
stantial role in arguing, we hypothesize that in-
formation about the discourse roles played by
a span of text will help improve classification.
Although discourse parsers historically haven?t
been found to be effective for subjectivity anal-
ysis, a new parser (Lin et al, 2010) trained on
the Penn Discourse TreeBank (PDTB) tagset
(Prasad et al, 2008) has recently been released.
Previous work has demonstrated that this parser
can reliably detect discourse relationships be-
tween adjacent sentences (Lin et al, 2011), and
the PDTB tagset, being relatively flat, is con-
ducive to feature engineering for our task.
To give a feeling for the kind of discourse re-
lations identified by this parser, the following
example illustrates a concession relation identi-
fied in the corpus by the parser. The italicized
text represents the concession, while the bolded
text indicates the overall point that the author
is making. The underlined word was identified
by the parser as an explicit concessionary clue.
(7) the health care reform legisla-
tion that President Obama now seems
likely to sign into law , while an
unlovely mess , will be remembered
as a landmark accomplishment .
Using this automatic information, we define
features indicating the discourse relationships by
which the instance is connected to surrounding
text. Specifically, the class of discourse rela-
tionship connecting the target instance to the
previous instance, the relationship connecting it
to the following instance, and any internal dis-
course relationships by which the parts of the
instance are connected to each other are each
added as features. Since PDTB contains many
fine-grained discourse relations, we replace each
discourse relationship type inferred by the dis-
course parser with the parent top-level PDTB
discourse relationship class. We arrive at a total
of 15 binary discourse relationship features: (4
top-level classes + ?other?) x (connects to pre-
vious + connects to following + internal connec-
tion) = 15. We refer to these features as ?rels?.
As illustrated in our earlier examples, while
arguing subjectivity is different from sentiment,
the two types of subjectivity are often related.
Thus, we investigate incorporating sentiment
information based on the presence of unigram
clues from a publically-available sentiment lexi-
con5 (Wilson, 2005). Each clue in the lexicon is
marked as being either ?strong? or ?weak?.
We found that this lexicon was producing
many false hits for positive sentiment. Thus, a
span containing a minimum of two positive clues
of which at least one is marked as ?strong?, or
three positive ?weak? clues, is augmented with a
feature indicating positive sentiment. For nega-
tive sentiment the threshold is slightly lower, at
one ?strong? clue or two ?weak? clues. These
features are referred to as ?senti?.
A challenge to argument tag assignment is the
broad diversity of language through which in-
dividual entities or specific actions may be ref-
erenced, as illustrated in Examples (2-4) from
Section 1. To address this problem, we in-
vestigate expanding each instance with terms
that are most similar, according to a distribu-
tional model generated from Wikipedia articles,
to the nouns and verbs present within the in-
stance (Pantel et al, 2009). We refer to these
features as ?expn?, where n is the number of
most-similar terms with which to expand the in-
stance for each noun or verb. We experiment
with values of n = 5 and n = 10.
Subjectivity classification of small units of
text, such as individual microblog posts (Jiang
et al, 2011) and sentences (Riloff et al, 2003),
has been shown to benefit from additional con-
text. Thus, we augment the feature representa-
tion of each target sentence with features from
the two preceding and two following sentences.
These additional features are modified so that
they do not fall within the same feature space
5downloaded from http://www.cs.pitt.edu/mpqa/
subj_lexicon.html
85
feat.
abbrev.
elaboration
unigram
senti 2 binary features indicating posi-
tive or negative sentiment based on
presence of lexicon clues
rels 15 binary features indicating kinds
of discourse relationships and how
they connect instance to surround-
ing text
exp5 for each noun and verb in instance,
expand instance with top 5 most
distributionally similar words
exp10 for each noun and verb in instance,
expand instance with top 10 most
distributionally similar words
Table 6: Overview of features used in the arguing
and argument experiments.
as the features representing the target sentence.
Using the Naive Bayes classifier within the
WEKA machine learning toolkit (Hall et al,
2009), we explore the impact of the features de-
scribed above on our four experiment configu-
rations. We perform our experiments using k-
fold cross-validation, where k equals the num-
ber of documents within the stance. The test
set for each fold consists of a single document?s
instances. For the ?pro? dataset k = 37, while
for the ?anti? dataset k = 47.
6 Results
Table 7 presents the accuracy scores from each of
our stand-alone classifiers across combinations
of feature sets. Each feature set consists of
unigrams augmented with the designated addi-
tional features, as described in Section 5. To
evaluate the ?tag? classifier in isolation, we use
oracle information to provide this classifier with
only the subjective instances. To assess signif-
icance of the performance differences between
feature sets, we used the Pearson Chi-squared
test with Yates continuity correction.
Expansion of nouns and verbs with
distributionally-similar terms (?exp5?, ?exp10?)
plays the largest role in improving classifier
features arg tag comb.
unigram baseline 0.610 0.425 0.458
senti 0.614 0.426 0.459
rels 0.614 0.422 0.462
senti, rels 0.618 0.424 0.465
exp5 0.635 0.522 0.482
exp5, senti 0.638 0.515 0.486
exp5, rels 0.640 0.522 0.484
exp5, senti, rels 0.643 0.516 0.484
exp10 0.645 0.517 0.488
exp10, senti 0.647 0.515 0.489
exp10, rels 0.642 0.512 0.490
exp10, senti, rels 0.644 0.513 0.490
Table 7: Classifier accuracy for differing feature sets.
Significant improvement (p < 0.05) over baseline is
boldfaced (0.05 < p < 0.1 italicized). Underline in-
dicates best performance per column.
performance. While differences between con-
figurations using ?exp5? versus ?exp10? were
generally not significant, all of the configu-
rations incorporating some version of term
expansion outperformed the unigram baseline
by either a statistically significant margin
(p < 0.05) or by a margin that approached
significance (0.05 < p < 0.1).
Sentiment features consistently produce im-
provements in accuracy for the ?arg? and ?com-
bined? tasks. While these improvements are
promising, the lack of a significant margin of im-
provement when incorporating sentiment is sur-
prising. Since sentiment lexicons are known to
be highly domain-dependent (Pan et al, 2010),
it may be the case that, having been learned
from a general news corpus, the sentiment lexi-
con employed in this work is not the best match
for the domain of ?ObamaCare? blogs and edito-
rials. Similarly, the discourse features also fail to
produce significant improvements in accuracy.
Finally, we aim to test our hypothesis that
separating the ?arg? and ?tag? phases results in
improvement beyond treating the two in a single
?combined? phase. The first step of our hierar-
chy involves normal classification of all sentences
using the ?arg? classifier. Next, all sentences
judged to contain arguing subjectivity by ?arg?
86
arg features tag features acc.
exp5, senti, rels
exp5 0.506
exp5, rels 0.506
exp10 0.501
exp10
exp5 0.514
exp5, rels 0.513
exp10 0.512
exp10, senti
exp5 0.514
exp5, rels 0.513
exp10 0.512
Table 8: Accuracies of two-stage classifiers across dif-
ferent combinations of feature sets for the ?arg? and
?tag? phases. Italics indicate improvement over the
top ?combined? configuration which approaches sig-
nificance (0.05 < p < 0.1). Underline indicates best
overall performance.
are passed to the ?tag? classifier to have an ar-
gument tag assigned. We choose three promis-
ing feature sets for the ?arg? and ?tag? phases,
based on best performance in isolation.
Results of this hierarchical experiment are
presented in Table 8. We evaluate the hi-
erarchical system against the best-performing
?combined? single-phase systems from Table 7.
While all of the hierarchical configurations beat
the best ?combined? classifier, none beats the
top combined classifier by a significant margin,
although the best configurations approach sig-
nificance (0.05 < p < 0.1).
7 Related Work
Much recent work in ideological subjectivity
detection has focused on detecting a writer?s
stance in domains of varying formality, such as
online forums, debating websites, and op-eds.
(Anand et al, 2011) demonstrates the usefulness
of dependency relations, LIWC counts (Pen-
nebaker et al, 2001), and information about re-
lated posts for this task. (Lin et al, 2006) ex-
plores relationships between sentence-level and
document-level classification for a stance-like
prediction task.
Among the literature on ideological subjectiv-
ity, perhaps most similar to our work is (Soma-
sundaran and Wiebe, 2010). This paper investi-
gates the impact of incorporating arguing-based
and sentiment-based features into binary stance
prediction for debate posts. Also closely related
to our work is (Somasundaran et al, 2007). To
support answering of opinion-based questions,
this work investigates the use of high-precision
sentiment and arguing clues for sentence-level
sentiment and arguing prediction.
Another active area of related research focuses
on identifying important aspects towards which
sentiment is expressed within a domain. (He
et al, 2011) approaches this problem through
topic modeling, extending the joint sentiment-
topic (JST) model which aims to simultaneously
learn sentiment and aspect probabilities for a
unit of text. (Yu et al, 2011) takes a different
approach, investigating thesaurus methods for
learning aspects based on groups of synonymous
nouns within product reviews.
8 Conclusion
In this paper, we explored recognizing argu-
ments in terms of arguing subjectivity and ar-
gument tags. We presented and evaluated a
new annotation scheme to capture arguing sub-
jectivity and argument tags, and annotated a
new dataset. Utilizing existing sentiment, dis-
course, and distributional similarity resources,
we explored ways in which these three forms
of knowledge could be used to enhance argu-
ment recognition. In particular, our empirical
results highlight the important role played by
distributional similarity in all phases of detect-
ing arguing subjectivity and argument tags. We
have also provided tentative evidence suggesting
that addressing the problem of recognizing argu-
ments in two separate phases may be beneficial
to overall classification accuracy.
9 Acknowledgments
This material is based in part upon work sup-
ported by National Science Foundation award
#0916046. We would like to thank Patrick Pan-
tel for sharing his thesaurus of distributionally
similar words from Wikipedia with us, Amber
Boydstun for insightful conversations about de-
bate frame categorization, and the anonymous
reviewers for their useful feedback.
87
References
Pranav Anand, Marilyn Walker, Rob Abbott,
Jean E. Fox Tree, Robeson Bowmani, and Michael
Minor. 2011. Cats rule and dogs drool!: Classi-
fying stance in online debate. In WASSA, pages
1?9, Portland, Oregon, June.
F.R. Baumgartner, S.D. Boef, and A.E. Boydstun.
2008. The decline of the death penalty and the dis-
covery of innocence. Cambridge University Press.
Clinton Burfoot, Steven Bird, and Timothy Bald-
win. 2011. Collective classification of congres-
sional floor-debate transcripts. In ACL, pages
1506?1515, Portland, Oregon, USA, June.
Claire Cardie, Cynthia Farina, Adil Aijaz, Matt
Rawding, and Stephen Purpura. 2008. A study in
rule-specific issue categorization for e-rulemaking.
In DG.O, pages 244?253.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11:10?18, November.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics
for cross-domain sentiment classification. In ACL,
pages 123?131, Portland, Oregon, USA, June.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent twitter
sentiment classification. In ACL, pages 151?160,
Portland, Oregon, USA, June.
K. Krippendorff. 2004. Content analysis: an intro-
duction to its methodology. Sage.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on?: identifying perspectives at the document and
sentence levels. In CoNLL, pages 109?116.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. CoRR.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In ACL, pages 997?1006, Port-
land, Oregon, USA, June.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Dan Huang, Andrew Y. Ng, and Christopher
Potts. 2011. Learning word vectors for sentiment
analysis. In ACL, pages 142?150, Portland, Ore-
gon, USA, June.
Sinno Jialin Pan, Xiaochuan Ni, Jian tao Sun, Qiang
Yang, and Zheng Chen. 2010. Cross-domain senti-
ment classification via spectral feature alignment.
In WWW.
Bo Pang and Lillian Lee. 2004. A sentimental
education: Sentiment analysis using subjectivity
summarization based on minimum cuts. In ACL,
pages 271?278, Barcelona, Spain, July.
Patrick Pantel, Eric Crestan, Arkady Borkovsky,
Ana-Maria Popescu, and Vishnu Vyas. 2009.
Web-scale distributional similarity and entity set
expansion. In EMNLP, pages 938?947, Morris-
town, NJ, USA.
Souneil Park, Kyung Soon Lee, and Junehwa Song.
2011. Contrasting opposing views of news arti-
cles on contentious issues. In ACL, pages 340?349,
Portland, Oregon, USA, June.
James W Pennebaker, Roger J Booth, and Martha E
Francis. 2001. Linguistic inquiry and word count
(liwc): Liwc2001. Linguistic Inquiry, (Mahwah,
NJ):0.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0.
In LREC, May.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003. Learning subjective nouns using extraction
pattern bootstrapping. In CoNLL, pages 25?32.
Swapna Somasundaran and Janyce Wiebe. 2009.
Recognizing stances in online debates. In ACL-
AFNLP, pages 226?234.
Swapna Somasundaran and Janyce Wiebe. 2010.
Recognizing stances in ideological on-line debates.
In CAAGET, pages 116?124.
Swapna Somasundaran, Theresa Wilson, Janyce
Wiebe, and Veselin Stoyanov. 2007. Qa with atti-
tude: Exploiting opinion type analysis for improv-
ing question answering in on-line discussions and
the news. In ICWSM.
Swampa Somasundaran. 2010. Discourse-Level Re-
lations for Opinion Analysis. Ph.D. thesis, Uni-
versity of Pittsburgh, USA.
Theresa Wilson and Janyce Wiebe. 2003. Annotat-
ing opinions in the world press. In SIGdial, pages
13?22.
Theresa Wilson. 2005. Recognizing contextual
polarity in phrase-level sentiment analysis. In
EMNLP, pages 347?354.
Jianxing Yu, Zheng-Jun Zha, Meng Wang, and Tat-
Seng Chua. 2011. Aspect ranking: Identifying
important product aspects from online consumer
reviews. In ACL, pages 1496?1505, Portland, Ore-
gon, USA, June.
88
