Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 161?169,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Review Sentiment Scoring via a Parse-and-Paraphrase Paradigm 
 
 
Jingjing Liu, Stephanie Seneff 
MIT Computer Science & Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, MA 02139  
{jingl, seneff}@csail.mit.edu 
 
  
 
Abstract 
 
This paper presents a parse-and-paraphrase pa-
radigm to assess the degrees of sentiment for 
product reviews. Sentiment identification has 
been well studied; however, most previous 
work provides binary polarities only (positive 
and negative), and the polarity of sentiment is 
simply reversed when a negation is detected. 
The extraction of lexical features such as uni-
gram/bigram also complicates the sentiment 
classification task, as linguistic structure such 
as implicit long-distance dependency is often 
disregarded. In this paper, we propose an ap-
proach to extracting adverb-adjective-noun 
phrases based on clause structure obtained by 
parsing sentences into a hierarchical represen-
tation. We also propose a robust general solu-
tion for modeling the contribution of adver-
bials and negation to the score for degree of 
sentiment. In an application involving extract-
ing aspect-based pros and cons from restaurant 
reviews, we obtained a 45% relative improve-
ment in recall through the use of parsing me-
thods, while also improving precision. 
 
1 Introduction 
Online product reviews have provided an exten-
sive collection of free-style texts as well as prod-
uct ratings prepared by general users, which in 
return provide grassroots contributions to users 
interested in a particular product or service as 
assistance. Yet, valuable as they are, free-style 
reviews contain much noisy data and are tedious 
to read through in order to reach an overall con-
clusion. Thus, we conducted this study to auto-
matically process and evaluate product reviews 
in order to generate both numerical evaluation 
and textual summaries of users? opinions, with 
the ultimate goal of adding value to real systems 
such as a restaurant-guide dialogue system. 
Sentiment summarization has been well stu-
died in the past decade (Turney, 2002; Pang et al, 
2002; Dave et al, 2003; Hu and Liu, 2004a, 
2004b; Carenini et al, 2006; Liu et al, 2007). 
The polarity of users? sentiments in each seg-
ment of review texts is extracted, and the polari-
ties of individual sentiments are aggregated 
among all the sentences/segments of texts to give 
a numerical scaling on sentiment orientation.  
 Most of the work done for sentiment analysis 
so far has employed shallow parsing features 
such as part-of-speech tagging. Frequent adjec-
tives and nouns/noun phrases are extracted as 
opinion words and representative product fea-
tures. However, the linguistic structure of the 
sentence is usually not taken into consideration. 
High level linguistic features, if well utilized and 
accurately extracted, can provide much insight 
into the semantic meaning of user opinions and 
contribute to the task of sentiment identification. 
Furthermore, in addition to adjectives and 
nouns, adverbials and negation also play an im-
portant role in determining the degree of the 
orientation level. For example, ?very good? and 
?good? certainly express different degrees of 
positive sentiment. Also, in previous studies, 
when negative expressions are identified, the 
polarity of sentiment in the associated segment 
of text is simply reversed. However, semantic 
expressions are quite different from the absolute 
opposite values in mathematics. For example, 
?not bad? does not express the opposite meaning 
of ?bad?, which would be highly positive. Simp-
ly reversing the polarity of sentiment on the ap-
pearance of negations may result in inaccurate 
interpretation of sentiment expressions. Thus, a 
system which attempts to quantify sentiment 
while ignoring adverbials is missing a significant 
component of the sentiment score, especially if 
the adverbial is a negative word. 
161
Another challenging aspect of negation is 
proper scoping of the negative reference over the 
right constituent, which we argue, can be han-
dled quite well with careful linguistic analysis. 
Take the sentence ?I don?t think the place is very 
clean? as example. A linguistic approach asso-
ciating long-distance elements with semantic 
relations can identify that the negation ?not? 
scopes over the complement clause, thus extract-
ing ?not very clean? instead of ?very clean?.  
Our goal in modeling adverbials is to investi-
gate whether a simple linear correction model 
can capture the polarity contribution of all ad-
verbials. Furthermore, is it also appropriate to 
adjust for multiple adverbs, including negation, 
via a linear additive model? I.e., can ?not very 
good? be modeled as not(very(good))? The fact 
that ?not very good? seems to be less negative 
than ?not good? suggests that such an algorithm 
might work well. From these derivations we have 
developed a model which treats negations in the 
exact same way as modifying adverbs, via an 
accumulative linear offset model. This yields a 
very generic and straightforward solution to 
modeling the strength of sentiment expression. 
In this paper we utilize a parse-and-paraphrase 
paradigm to identify semantically related phrases 
in review texts, taking quantifiers (e.g., modify-
ing adverbs) and qualifiers (e.g., negations) into 
special consideration. The approach makes use 
of a lexicalized probabilistic syntactic grammar 
to identify and extract sets of adverb-adjective-
noun phrases that match review-related patterns. 
Such patterns are constructed based on well-
formed linguistic structure; thus, relevant phrases 
can be extracted reliably. 
We also propose a cumulative linear offset 
model to calculate the degree of sentiment for 
joint adjectives and quantifiers/qualifiers. The 
proposed sentiment prediction model takes mod-
ifying adverbs and negations as universal scales 
on strength of sentiment, and conducts cumula-
tive calculation on the degree of sentiment for 
the associated adjective. With this model, we can 
provide not only qualitative textual summariza-
tion such as ?good food? and ?bad service?, but 
also a numerical scoring of sentiment, i.e., ?how 
good the food is? and ?how bad the service is.? 
2 Related Work  
There have been many studies on sentiment 
classification and opinion summarization (Pang 
and Lee, 2004, 2005; Gamon et al, 2005; Popes-
cu and Etzioni, 2005; Liu et al, 2005; Zhuang et 
al., 2006; Kim and Hovy, 2006). Specifically, 
aspect rating as an interesting topic has also been 
widely studied (Titov and McDonald, 2008a; 
Snyder and Barzilay, 2007; Goldberg and Zhu, 
2006). Recently, Baccianella et. al. (2009) 
conducted a study on multi-facet rating of 
product reviews with special emphasis on how to 
generate vectorial representations of the text by 
means of POS tagging, sentiment analysis, and 
feature selection for ordinal regression learning. 
Titov and McDonald (2008b) proposed a joint 
model of text and aspect ratings which utilizes a 
modified LDA topic model to build topics that 
are representative of ratable aspects, and builds a 
set of sentiment predictors. Branavan et al (2008) 
proposed a method for leveraging unstructured 
annotations in product reviews to infer semantic 
document properties, by clustering user 
annotations into semantic properties and tying 
the induced clusters to hidden topics in the text.  
3 System Overview 
Our review summarization task is to extract sets 
of descriptor-topic pairs (e.g., ?excellent service?) 
from a set of reviews (e.g., for a particular res-
taurant), and to cluster the extracted phrases into 
representative aspects on a set of dimensions 
(e.g., ?food?, ?service? and ?atmosphere?). Dri-
ven by this motivation, we propose a three-stage 
system that automatically processes reviews. A 
block diagram is given in Figure 1.  
 
 
Figure 1.  Framework of review processing. 
 
The first stage is sentence-level data filtering. 
Review data published by general users is often 
in free-style, and a large fraction of the data is 
either ill-formed or not relevant to the task. We 
classify these as out of domain sentences. To fil-
ter out such noisy data, we collect unigram statis-
tics on all the relevant words in the corpus, and 
select high frequency adjectives and nouns. Any 
sentence that contains none of the high-
frequency nouns or adjectives is rejected from 
further analysis. The remaining in-domain sen-
tences are subjected to the second stage, parse 
162
analysis and semantic understanding, for topic 
extraction.  
From the parsable sentences we extract de-
scriptor-topic phrase patterns based on a careful-
ly-designed generation grammar. We then apply 
LM (language model) based topic clustering to 
group the extracted phrases into representative 
aspects. The third stage scores the degree of sen-
timent for adjectives, as well as the strength of 
sentiment for modifying adverbs and negations, 
which further refine the degree of sentiment of 
the associated adjectives. We then run a linear 
additive model to assign a combined sentiment 
score for each extracted phrase. 
The rest of the paper is structured as follows: 
In Section 4, we explain the linguistic analysis. 
In Section 5, we describe the cumulative model 
for assessing the degree of sentiment. Section 6 
provides a systematic evaluation, conducted on 
real data in the restaurant review domain har-
vested from the Web. Section 7 provides a dis-
cussion analyzing the results. Section 8 summa-
rizes the paper as well as pointing to future work. 
4 Linguistic Analysis 
4.1 Parse-and-Paraphrase 
Our linguistic analysis is based on a parse-and-
paraphrase paradigm. Instead of the flat structure 
of a surface string, the parser provides a hierar-
chical representation, which we call a linguistic 
frame (Xu et al, 2008). It preserves linguistic 
structure by encoding different layers of seman-
tic dependencies. The grammar captures syntac-
tic structure through a set of carefully con-
structed context free grammar rules, and employs 
a feature-passing mechanism to enforce long dis-
tance constraints. The grammar is lexicalized, 
and uses a statistical model to rank order compet-
ing hypotheses. It knows explicitly about 9,000 
words, with all unknown words being interpreted 
as nouns. The grammar probability model was 
trained automatically on the corpus of review 
sentences. 
To produce the phrases, a set of generation 
rules is carefully constructed to only extract sets 
of related adverbs, adjectives and nouns. The 
adjective-noun relationships are captured from 
the following linguistic patterns: (1) all adjec-
tives attached directly to a noun in a noun phrase, 
(2) adjectives embedded in a relative clause 
modifying a noun, and (3) adjectives related to 
nouns in a subject-predicate relationship in a 
clause. These patterns are compatible, i.e., if a 
clause contains both a modifying adjective and a 
predicate adjective related to the same noun, two 
adjective-noun pairs are generated by different 
patterns. As in, ?The efficient waitress was none-
theless very courteous.? It is a ?parse-and-
paraphrase-like? paradigm: the paraphrase tries 
to preserve the original words intact, while reor-
dering them and/or duplicating them into mul-
tiple NP units. Since they are based on syntactic 
structure, the generation rules can also be applied 
in any other domain involving opinion mining. 
An example linguistic frame is shown in Fig-
ure 2, which encodes the sentence ?The caesar 
with salmon or chicken is really quite good.? In 
this example, for the adjective ?good?, the near-
by noun ?chicken? would be associated with it if 
only proximity is considered. From the linguistic 
frame, however, we can easily associate ?caesar? 
with ?good? by extracting the head of the topic 
sub-frame and the head of the predicate sub-
frame, which are encoded in the same layer (root 
layer) of the linguistic frame. Also, we can tell 
from the predicate sub-frame that there is an ad-
verb ?quite? modifying the head word ?good?. 
The linguistic frame also encodes an adverb ?re-
ally? in the upstairs layer. A well-constructed 
generation grammar can create customized ad-
verb-adjective-noun phrases such as ?quite good 
caesar? or ?really quite good caesar?. 
{c cstatement 
  :topic {q caesar 
             :quantifier "def" 
             :pred {p with :topic {q salmon 
                                         :pred {p conjunction 
                                           :or {q chicken  }}}} 
  :adv "really" 
   :pred {p adj_complement 
            :pred {p adjective 
                    :adv "quite" 
                :pred {p quality :topic "good" }}}} 
Figure 2.  Linguistic frame for ?The caesar with 
salmon or chicken is really quite good.? 
Interpreting negation in English is not 
straightforward, and it is often impossible to do 
correctly without a deep linguistic analysis. Xu-
ehui Wu (2005) wrote: ?The scope of negation is 
a complex linguistic phenomenon. It is easy to 
perceive but hard to be defined from a syntactic 
point of view. Misunderstanding or ambiguity 
may occur when the negative scope is not un-
derstood clearly and correctly.? The majority 
rule for negation is that it scopes over the re-
mainder of its containing clause, and this works 
well for most cases. For example, Figure 3 shows 
163
the linguistic frame for the sentence ?Their menu 
was a good one that didn?t try to do too much.?  
{c cstatement   
:topic {q menu   :poss "their" } } 
   :complement {q pronoun   :name ?one? 
             :adj_clause {c cstatement 
                           :conjn "that" 
                           :negate "not" 
                           :pred {p try :to_clause  {p do 
                                           :topic {q object 
                                           :adv "too" 
                                           :quant "much" }}}} 
   :pred {p adjective 
                :pred {p quality :topic "good" }}} 
Figure 3.  Linguistic frame for ?Their menu was a 
good one that didn?t try to do too much.? 
Traditional approaches which do not consider 
the linguistic structure would treat the appear-
ance of ?not? as a negation and simply reverse 
the sentiment of the sentence to negative polarity, 
which is wrong as the sentence actually ex-
presses positive opinion for the topic ?menu?. In 
our approach, the negation ?not? is identified as 
under the sub-frame of the complement clause, 
instead of in the same or higher layer of the ad-
jective sub-frame; thus it is considered as unre-
lated to the adjective ?good?. In this way we can 
successfully predict the scope of the reference of 
the negation over the correct constituent of a sen-
tence and create proper association between ne-
gation and its modified words. 
4.2 LM-based Topic Clustering 
To categorize the extracted phrases into repre-
sentative aspects, we automatically group the 
identified topics into a set of clusters based on 
LM probabilities. The LM-based algorithm as-
sumes that topics which are semantically related 
have high probability of co-occurring with simi-
lar descriptive words. For example, ?delicious? 
might co-occur frequently with both ?pizza? and 
?dessert?. By examining the distribution of bi-
gram probability of these topics with correspond-
ing descriptive words, we can group ?pizza? and 
?dessert? into the same cluster of ?food?. 
We select a small set of the most common top-
ics, i.e., topics with the highest frequency counts, 
and put them into an initial set I. Then, for each 
candidate topic  outside set I, we calculate its 
probability given each topic  within the initial 
set I, given by:  
       | 	
  ? |
 ? |	
  
                       ? ,

 ? 
,


   
                     
?


 ? , 
 ? , 	
        (1) 
where A represents the set of all the adjectives in 
the corpus. For each candidate topic  , we 
choose the cluster of the initial topic   with 
which it has the highest probability score.  
There are also cases where a meaningful ad-
jective occurs in the absence of an associated 
topic, e.g., ?It is quite expensive.? We call such 
cases the ?widow-adjective? case. Without hard-
coded ontology matching, it is difficult to identi-
fy ?expensive? as a price-related expression. To 
discover such cases and associate them with re-
lated topics, we propose a ?surrogate topic? 
matching approach based on bigram probability.  
As aforementioned, the linguistic frame orga-
nizes all adjectives into separate clauses. Thus, 
we create a ?surrogate topic? category in the lin-
guistic frames for widow-adjective cases, which 
makes it easy to detect descriptors that are affi-
liated with uninformative topics like the pronoun 
?it?. We then have it generate phrases such as 
?expensive surrogate_topic? and use bigram 
probability statistics to automatically map each 
sufficiently strongly associated adjective to its 
most common topic among our major classes, 
e.g., mapping ?expensive? with its surrogate top-
ic ?price?. Therefore, we can generate sets of 
additional phrases in which the topic is ?halluci-
nated? from the widow-adjective.  
5 Assessment of Sentiment Strength 
5.1 Problem Formulation 
Given the sets of adverb-adjective-noun phrases 
extracted by linguistic analysis, our goal is to 
assign a score for the degree of sentiment to each 
phrase and calculate an average rating for each 
aspect. An example summary is given in Table 1. 
Table 1. Example of review summary. 
Aspect Extracted phrases Rating 
Atmosphere very nice ambiance, 
outdoor patio 4.8 
Food not bad meal, quite authentic food 4.1 
Place not great place, 
very smoky restaurant 2.8 
Price so high bill, high cost, 
not cheap price 2.2 
To calculate the numerical degree of sentiment, 
there are three major problems to solve: 1) how 
to associate numerical scores with textual senti-
ment; 2) whether to calculate sentiment scores 
for adjectives and adverbs jointly or separately; 3) 
164
whether to treat negations as special cases or in 
the same way as modifying adverbs.  
There have been studies on building sentiment 
lexicons to define the strength of sentiment of 
words. Esuli and Sebastiani (2006) constructed a 
lexical resource, SentiWordNet, a WordNet-like 
lexicon emphasizing sentiment orientation of 
words and providing numerical scores of how 
objective, positive and negative these words are. 
However, lexicon-based methods can be tedious 
and inefficient and may not be accurate due to 
the complex cross-relations in dictionaries like 
WordNet. Instead, our primary approach to sen-
timent scoring is to make use of collective data 
such as user ratings. In product reviews collected 
from online forums, the format of a review entry 
often consists of three parts: pros/cons, free-style 
text and user rating. We assume that user rating 
is normally consistent with the tone of the review 
text published by the same user. By associating 
user ratings with each phrase extracted from re-
view texts, we can easily associate numerical 
scores with textual sentiment.  
A simple strategy of rating assignment is to 
take each extracted adverb-adjective pair as a 
composite unit. However, this method is likely to 
lead to a large number of rare combinations, thus 
suffering from sparse data problems. Therefore, 
an interesting question to ask is whether it is 
feasible to assign to each adverb a perturbation 
score, which adjusts the score of the associated 
adjective up or down by a fixed scalar value. 
This approach thus hypothesizes that ?very ex-
pensive? is as much worse than ?expensive? as 
?very romantic? is better than ?romantic?. This 
allows us to pool all instances of a given adverb 
regardless of which adjective it is associated with, 
in order to compute the absolute value of the per-
turbation score for that adverb. Therefore, we 
consider adverbs and adjectives separately when 
calculating the sentiment score, treating each 
modifying adverb as a universal quantifier which 
consistently scales up/down the strength of sen-
timent for the adjectives it modifies. 
Furthermore, instead of treating negation as a 
special case, the universal model works for all 
adverbials. The model hypothesizes that ?not bad? 
is as much better than ?bad? as ?not good? is 
worse than ?good?, i.e., negations push posi-
tive/negative adjectives to the other side of sen-
timent polarity by a universal scale. This again, 
allows us to pool all instances of a given nega-
tion and compute the absolute value of the per-
turbation score for that negation, in the same way 
as dealing with modifying adverbs.  
5.2 Linear Additive Model 
For each adjective, we average all its ratings giv-
en by: 
Hypothesis Selection and Resolution
in the Mercury Flight Reservation System
Stephanie Seneff and Joseph Polifroni
Spoken Language Systems Group
Laboratory for Computer Science
Massachusetts Institute of Technology
Cambridge, Massachusetts 02139 USA
 
seneff,joe 
	 

	 	 
ABSTRACT
Interlingua-Based Broad-Coverage Korean-to-English 
Translation in CCLINC 
       Young-Suk Lee               Wu Sok Yi            Stephanie Seneff        Clifford J. Weinstein 
     MIT Lincoln Laboratory     MIT Lincoln Laboratory                 MIT/LCS                     MIT Lincoln Laboratory 
         244 Wood Street               244 Wood Street               77 Mass Avenue                     244 Wood Street 
     Lexington, MA 02420         Lexington, MA 02420      Cambridge, MA 02673             Lexington, MA 02420 
                U.S.A                                   U.S.A                                U.S.A                                       U.S.A 
         1-781-981-2703                 1-781-981-4609                1-617-254-0456             1-781-981-7621 
       YSL@LL.MIT.EDU           WUYI@LL.MIT.EDU     SENEFF@LCS.MIT.EDU          CJW@LL.MIT.EDU  
                    
  
ABSTRACT 
 
At MIT Lincoln Laboratory, we have been developing a Korean-
to-English machine translation system CCLINC (Common 
Coalition Language System at Lincoln Laboratory). The CCLINC 
Korean-to-English translation system  consists of two core 
modules, language understanding and generation modules 
mediated by a language neutral meaning representation called a 
semantic frame. The key features of the system include: (i) Robust 
efficient parsing of Korean (a verb final language with overt case 
markers, relatively free word order, and frequent omissions of 
arguments). (ii) High quality translation via word sense 
disambiguation and accurate word order generation of  the target 
language. (iii) Rapid system development and porting to new 
domains via knowledge-based automated acquisition of 
grammars. Having been trained on Korean newspaper articles on 
?missiles? and ?chemical biological warfare,? the system produces 
the translation output sufficient for content understanding of the 
original document. 
 
1. SYSTEM OVERVIEW 
 The CCLINC  The CCLINC Korean-to-English translation 
system is a component of the CCLINC Translingual Information 
System, the focus languages of which are English and Korean, 
[11,17]. Translingual Information System Structure is given in 
Figure 1.  
Given the input text or speech, the language understanding system 
parses the input, and transforms the parsing output into a language 
neutral meaning representation called a semantic frame, [16,17]. 
The semantic frame ? the key properties of which will be 
discussed in Section 2.3 ? becomes the input to the generation 
system. The generation system produces the target to the 
generation system, the semantic frame can be utilized for other 
applications such as translingual information extraction and 
 
 
 
 
  
language translation output after word order arrangement, 
vocabulary replacement, and the appropriate surface form 
realization in the target language, [6]. Besides serving as the input 
question-answering, [12].?  In this paper, we focus on the Korean-
to-English text  translation component of CCLINC.1 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. CCLINC Translingual Information System 
Structure 
 
2. ROBUST PARSING, MEANING 
REPRESENTATION, AND AUTOMATED 
GRAMMAR ACQUISITION 
                                                                
? This work was sponsored by the Defense Advanced Research 
Project Agency under the contract number F19628-00-C-0002. 
Opinions, interpretations, conclusions, and recommendations 
are those of the authors and are not necessarily endorsed by the 
United States Air Force. 
1 For other approaches to Korean-to-English translation, the 
readers are referred to Korean-to-English translation by Egedi, 
Palmer, Park and Joshi 1994, a transfer-based approach using 
synchronous tree adjoining grammar, [5], and Dorr 1997, a 
small-scale interlingua-based approach, using Jackendoff?s 
lexical conceptual structure as the interlingua, [4]. 
  
OTHER LANGUAGES
SEMANTIC FRAMES
(COMMON
COALITION
LANGUAGE)
SEMANTIC FRAMES
(COMMON
COALITION
LANGUAGE)
UNDERSTANDINGUNDERSTANDING UNDERSTANDINGUNDERSTANDING
GENERATIONGENERATION GENERATIONGENERATION
C4I
INFORMATION
ACCESS
C4I
INFORMATION
ACCESS
ENGLISH
TEXT OR
SPEECH
KOREAN
TEXT OR
SPEECH
1.1 Robust Parsing 
The CCLINC parsing module, TINA [16], implements the top-
down chart parsing and the best-first search techniques, driven by 
context free grammars rules compiled into a recursive transition 
network augmented by features, [8]. The following properties of 
Korean induce a great degree of ambiguity in the grammar: (i) 
relatively free word order for arguments --- given a sentence with 
three arguments, subject, object, indirect object, all 6 logical word 
order permutations are possible in reality, (ii) frequent omissions 
of subjects and objects, and (iii) the strict verb finality, [10]. Due 
to the free word order and argument omissions, the first word of 
an input sentence can be many way ambiguous  --- it can be a part 
of a subject, an object, and any other post-positional phrases.2  
The ambiguity introduced by the first input word grows rapidly as 
the parser processes subsequent input words. Verbs,  which 
usually play a crucial role in reducing the ambiguity in English by 
the subcategorization frame information, are not available until 
the end, [1,3,11]. 
Our solution to the ambiguity problem lies in a novel grammar 
writing technique, which reduces the ambiguity of the first input 
word. We hypothesize that (i) the initial symbol in the grammar 
(i.e. Sentence) always starts with the single category generic_np, 
the grammatical function (subject, object) of which is 
undetermined. This ensures that the ambiguity of the first input 
word is reduced to the number of different ways the category 
generic_np can be rewritten. (ii) The grammatical function of the 
generic_np is determined after the parser processes the following 
case marker via a trace mechanism.3   
Figure 2 illustrates a set of sample context free grammar rules, and 
Figure 3 (on the next page) is a sample parse tree for the input 
sentence ?URi Ga EoRyeoUn MunJe Reul PulEox Da (We solved 
a difficult problem).?4 
 
(i)           sentence ? generic_np clause sentence_marker 
(ii) clause ? subject generic_np object verbs 
(iii) subject ? subj_marker np_trace 
Figure 2. Sample context free grammar rules for  Korean 
 
                                                                
2 Post-positional phrases in Korean correspond to pre-positional 
phrases in English. We use the term post-positional phrase to 
indicate that the function words at issue are located after the 
head noun. 
3 The hypothesis that all sentences start with a single category 
generic_np is clearly over simplified. We can easily find a 
sentence starting with other elements such as coordination 
markers which do not fall under generic_np.  For the sentences 
which do not start with the category generic_np, we discard 
these elements for parsing purposes. And this method has 
proven to be quite effective in the overall design of the 
translation system, especially due to the fact that most of  non 
generic_np sentence initial elements (e.g. coordination markers, 
adverbs, etc.) do not contribute to the core meaning of the input 
sentence.  
4 Throughout this paper, ?subj_marker? stands for ?subject 
marker?, and ?obj_marker?, ?object marker?. 
The generic_np dominated by the initial symbol sentence in (i) of 
Figure 2 is parsed as an element moved from the position 
occupied by np_trace in (iii), and therefore corresponds to the 
category np_trace dominated by subject in Figure 3 (placed on 
the next page for space reasons).  All of the subsequent 
generic_np?s, which are a part of a direct object, an indirect 
object, a post-positional phrase, etc. are unitarily handled by the 
same trace mechanism. By hypothesizing that all sentences start 
with generic_np, the system can parse Korean robustly and 
efficiently.  The trace mechanism determines the grammatical 
function of generic_np by repositioning it after the appropriate 
case marker. 
Utilization of overt case markers to improve the parsing efficiency 
precisely captures  the commonly shared intuition for parsing 
relatively free word order languages with overt case markers such 
as Korean and Japanese, compared with parsing relatively strict 
word order languages with no overt case markers such as English:  
In languages like English, the verb of a sentence plays the crucial 
role in reducing the ambiguity via the verb subcategorization 
frame information on the co-occuring noun phrases, [1,3,11].   In 
languages like Korean, however, it is typically the case marker 
which identifies the grammatical function of the co-occuring noun 
phrase, assuming the role similar to that of verbs in English.  The 
current proposal is the first explicit implementation of this 
intuition, instantiated by the novel idea that all noun phrases are 
moved out of  the case marked phrases immediately following 
them. 
 
2.2 Meaning Representation and Generation 
The CCLINC Korean-to-English translation system achieves high 
quality translation by (i) robust mapping of the parsing output into 
the semantic frame, and  (ii) word sense disambiguation on the 
basis of the selection preference between two grammatical 
relations (verb-object, subject-verb, head-modifier) easily 
identifiable from the semantic frame, [13].  The former facilitates 
the accurate word order generation of various target language 
sentences, and the latter, the accurate choice of the target language 
word given multiple translation candidates for the same source 
language word. Given the parsing output in Figure 3, the system 
produces the semantic frame in Figure 4:5 
 
                                                                
5 Strictly speaking, the meaning representation in Figure 4 is not 
truly language neutral in that the terminal vocabularies are 
represented in Korean rather than in interlingua vocabulary. It is 
fairly straightforward to adapt our system to produce the meaning 
representation with the terminal vocabularies specified by an 
interlingua.  However, we have made a deliberate decision to 
leave the Korean vocabularies in the representation largely (1) to 
retain the system efficiency for mapping parsing output into 
meaning representation, and (2) for unified execution of 
automation algorithms for both Korean-to-English and English-
to-Korean translation. And we would like to point out that this 
minor compromise in meaning representation still ensures the 
major benefit of interlingua approach to machine translation, 
namely, 2 x N sets of grammar rules for N language pairs, as 
opposed to 2N. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
          {c statement 
                        :topic {q  pronoun 
                 :name ?URi? } 
               :pred {p pul_v 
                              :topic {q problem 
               :name ?MunJe?  
                              :pred {p EoRyeoUn } } } 
 
Figure 4. Semantic Frame  for the input sentence ?URi Ga 
EoRyeoUn MunJe Reul PulEox Da.? 
The semantic frame captures the core predicate-argument 
structure of the input sentence in a hierarchical manner, [9,10] 
(i.e. the internal argument, typically object, is embedded under the 
verb, and the external argument, typically subject, is at the same 
hierarchy as the main predicate, i.e. verb phrase in syntactic 
terms). The predicate and the arguments along with their 
representation categories are bold-faced in Figure 4. With the 
semantic frame as input, the generation system generates the 
English translation using the grammar rules in (1), and the Korean 
paraphrase using the grammar rules in (2). 
The semantic frame captures the core predicate-argument 
structure of the input sentence in a hierarchical manner, [9,10] 
(i.e. the internal argument, typically object, is embedded under the 
verb, and the external argument, typically subject, is at the same 
hierarchy as the main predicate, i.e. verb phrase in syntactic 
terms). The predicate and the arguments along with their 
representation categories are bold-faced in Figure 4. With the 
semantic frame as input, the generation system generates the 
English translation using the grammar rules in (1), and the Korean 
paraphrase using the grammar rules in (2). 
(1)  a. statement :topic :predicate               
       b. pul_v  :predicate :topic 
(2) a. statement :topic :predicate 
      b. pul_v  :topic :predicate 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
(1b) and (2b) state that the topic category for the object follows 
the verb predicate in English, whereas it precedes the verb 
predicate in Korean. 
The predicate-argument structure also provides a means for word 
sense disambiguation, [13,15]. The verb pul_v is at least two-way 
ambiguous between  solve and  untie. Word sense disambiguation 
is performed by applying the rules, as in (3). 
 (3) a .pul_v   b .pul_v 
                   problem pul+solve_v     thread  pul+untie_v 
 
(3a) states that if the verb pul_v occurs with an object of type 
problem, it is disambiguated as pul+solve_v. (3b) states that the  
verb occurring with an object of type thread is disambiguated as 
pul+untie_v. The disambiguated verbs are translated into solve 
and untie, respectively, in the Korean-to-English translation 
lexicon. 
 
1.2 Knowledge-Based Automated Acquisition 
of Grammars 
To overcome the knowledge bottleneck for robust translation and 
efficient system porting in an interlingua-based system [7], we 
have developed a technique for automated acquisition of grammar 
rules which leads to a simultaneous acquisition of  rules for (i) the 
parser, (ii) the mapper between the parser and the semantic frame, 
and (iii) the generator. 
The technique utilizes a list of words and their corresponding 
parts-of-speech in the corpus as the knowledge source, 
presupposes a set of knowledge-based rules to be derived from a 
word and its part-of-speech pair, and gets executed according to 
the procedure given in Figure 5. The rationale behind the 
technique is that (i) given a word and its part-of-speech, most of 
the syntactic rules associated with the word can be automatically 
derived according to the projection principle (the syntactic 
subj_marker 
sentence 
clause sentence_marker 
subject object verbs 
np_trace obj_marker modifier np_trace 
noun adj noun verb 
statement 
Ga URi Reul EoRyeoUn MunJe PulEox Da 
Figure 1. Parse Tree for the Sentence URi Ga   EoRyeoUn MunJe    Reul PulEox 
representation must observe the subcategorization properties of 
each lexical item) and the X-bar schema (major syntactic 
categories such as N, V, Adj, Adv project to the same syntactic 
structures)  in linguistic theories, [2], and (ii) the mapping from 
the syntactic structure to the semantic frame representation is 
algorithmic. The specific rules to be acquired for a language 
largely depend on the grammar of the language  for parsing.  
Some example rules acquired for the verb BaiChiHa (arrange) in 
Korean ? consistent with the parsing technique discussed in 
Section 2.1 ?  are given in (4) through (7). 
 
Initialization: Create the list of words and their parts-of-speech in 
the corpus. 
Grammar Update: For each word and its associated part-of-
speech, check to see whether or not the word and the rules 
associated with the corresponding part-of-speech occur in each 
lexicon and grammar.  
 If they already occur, do nothing. 
 If not:  
(i) Create the appropriate rules and vocabulary items 
for  each entry. 
(ii) Insert the newly created rules and vocabulary items 
into the appropriate positions of the 
grammar/lexicon files for the parser, the grammar 
file for the mapper between the parser and  the 
semantic frame, and the grammar/lexicon files for 
the generator . 
 
Figure 5.  Automated Gammar Acquistion Procedure 
 
(4) Rules for the parser6 
.verbs 
[negation] vBaiChiHa [negation] [aspect] [tense] [auxiliary] 
[negation] [aspect] [tense] [and_verbs] [or_verbs] 
 
.vBaiChiHa 
#BaiChiHa 
 
(5) Rules for the mapper from the parser to the semantic frame 
.bachiha_v 
vBaiChiHa 
                                                                
6 The rules for the parser for the verb tell in English are given 
below, to illustrate the dependency of the rules acquired  to the 
specific implementation of the grammar of the language for 
parsing: 
   .vp_tell 
     vtell [adverb_phrase] dir_object [v_pp]  
 vtell [adverb_phrase] indir_object dir_object  
 vtell [adverb_phrase] dir_object v_to_pp [v_pp] 
 vtell [adverb_phrase] dir_object that_clause 
 vtell [and_verb] [or_verb] [adverb_phrase] dir_object wh_clause 
 
   The contrast in  complexity of verb rules in (4) for Korean, and (i) 
for English, reflects the relative importance of the role played by 
verbs for parsing in each language. That is, verbs play the minimal 
role in Korean, and the major role in English for ambiguity 
reduction and efficiency improvement. 
 
(6) Lexicon for the generation vocabulary 
baichiha_v V2 ?arrang? 
V2    V ?e? ING ?ing? PP ?ed? THIRD ?es? ROOT ?e? 
PAST ?ed? PASSIVE ?ed? 
 
(7) Rules for the generation grammar 
baichiha_v        :predicate :conj :topic :sub_clause 
np-baichiha_v   :noun_phrase :predicate :conj :topic :sub_clause 
 
The system presupposes the flat phrase structure for a sentence in 
Korean, as shown in Figure 3, and therefore the rules for the verbs 
do not require the verb subcategorization information, as in (4). 
The optional elements such as [negation], [tense], etc. are possible 
prefixes and suffixes to be attached to the verb stem, illustrating a 
fairly complex verb morphology in this language. The rules for 
the generation grammar in (7) are the subcategorization frames for 
the verb arrange in English, which is the translation of the 
Korean verb baichiha_v, as given in (6).   
The current technique is quite effective in expanding the system?s 
capability when there is no large syntatically annotated corpus 
available from which we can derive and train the grammar rules,  
[14], and applicable across languages in so far as the notion of 
part-of-speech, the projection principle and the X-bar schema is 
language independent.  With this technique, manual acquisition of 
the knowledge database for the overall translation system is 
reduced to the acquisition of  (i) the bilingual lexicon, and (ii) the 
corpus specific top-level grammar rules which constitute less than 
20% of the total grammar rules in our system. And this has 
enabled us to produce a fairly large-scale interlingua-based 
translation system within a short period of time.  One apparent 
limitation of  the technique, however, is that it still requires the 
manual acquisition of corpus-specific rules  (i.e. the patterns 
which do not fall under the linguistic generalization).  And we are 
currently developing a technique for automatically deriving 
grammar rules and obtaining the rule production probabilities 
from a syntactically annotated corpus. 
 
3. EVALUATION AND RESEARCH 
ISSUES 
We have trained the system with about 1,600 Korean newspaper 
articles on ?missiles? and ?chemical biological warfare?, as in 
Table 1. 
Table 1. Korean-to-English translation training data statistics 
# of 
articles 
# of  
sents/article 
# of 
words/sent 
# of distinct 
      words 
1,631           24 17 15,220 
 
For quality evaluation, we have adopted a 5-point scale evaluation 
score, defined as follows.  Score 4: Translation is both accurate 
and natural. Score 3: Translation is accurate with minor 
grammatical errors which do not affect the intended meaning of 
the input, e.g. morphological errors such as ?swam vs. swimmed.? 
Score 2: Translation is partially accurate, and sufficient for 
content understanding. Most errors are due to inaccurate word 
choice, inaccurate word order, and partial translation. Score 1: 
Translation is word-for-word, and partial content understanding is 
possible. Score 0: There is no translation output, or no content 
understanding is possible.  
We have performed the quality evaluation on 410 clauses from the 
training data, and 80 clauses from the test data. We have 
conducted the evaluation in 3 phases. Eval 1: Baseline evaluation 
after grammar and lexicon acquisition. Eval 2: Evaluation after 
augmenting word sense disambiguation rules. Eval 3: Evaluation 
after augmenting word sense disambiguation rules and accurate 
word order generation rules. The purpose of the 3-phase 
evaluation was to examine the contribution of parsing, word sense 
disambiguation and accurate word order generation to the overall 
translation quality. Once the score had been assigned to each 
clause, the translation score was obtained by the formula:  (Sum 
of  the scores for each clause *  25) / Number of clauses 
evaluated. 
Evaluation results are shown in Table 2 and Table 3 in terms of 
parsing coverage (P) and the translation score (T).7 
Table 2. Translation Quality Evaluation on Training Data 
            Eval 1             Eval 2             Eval 3 
       P         T        P        T         P        T 
      92      58        94       69       94      74 
 
Table 3. Translation Quality Evaluation on Test Data 
            Eval 1             Eval 2             Eval 3 
        P       T         P        T         P       T 
      79      55       89       63       89      65 
 
For both training and test data, the baseline translation quality 
score is over 50, sufficient for content understanding of the 
documents. Word sense disambiguation (Eval 1 vs. Eval 2) 
increases the translation score by about 10%, indicating that 
effective word sense disambiguation has a great potential for 
improving the translation quality.    
We would like to point out  that the evaluations reported in this 
paper are performed on clauses rather than sentences (which often 
consist of more than one clause).  In a very recent evaluation, we 
have found out that evaluations on sentences decrease the overall 
translation score about by 15.  Nevertheless, the translation 
quality is still good enough for content understanding with some 
effort.  The primary cause for the lower translation scores when 
the evaluation unit is a sentence as opposed to a clause is due to 
either an incorrect clause boundary identification, or some 
information (e.g. missing arguments in embedded clauses) which 
cannot be easily recovered after a sentence  is fragmented into 
clauses. This has led to the ability to handle complex sentences as 
                                                                
7 We would like to note that the evaluation reported here was a 
self-evaluation of the system by a system developer, primarily to 
identify the key research issues in system development. We will 
report evaluation results by non system developers who have no 
knowledge of  Korean in the future.  A system evaluation by  a 
non-bilingual speaker will avoid the issue of implicitly utilizing 
the knowledge  the  evaluator has about the source language in 
the evaluation process. 
the primary research issue, and we are working out the solution of 
utilizing syntactically annotated corpus for both grammar and 
probability acquisition, as discussed in Section 2.3. 
 
4. SUMMARY AND ONGOING WORK 
We have described the key features of the CCLINC interlingua-
based Korean-to-English translation system which is capable of 
translating a large quantity of Korean newspaper articles on 
missiles and chemical biological warfare in real time. Translation 
quality evaluations on the training and test data indicate that the 
current system produces translation sufficient for content 
understanding of a document in the training domains.  The key 
research issues identified from the evaluations include (i) parsing 
complex sentences, (ii) automated acquisition of word sense 
disambiguation rules from the training corpus,  and (iii) 
development of discourse module to identify the referents of 
missing arguments.  Our solution to the key technical challenges 
crucially draws upon the utilization of annotated corpora: For 
complex sentence parsing, we acquire both rules and rule 
production probabilities from syntactically annotated corpus. For 
automated word sense disambiguation, we utilize a sense-tagged 
corpus to identify various senses of a word, and obtain 
probabilities for word senses in various contexts.  For discourse 
understanding, we are developing an algorithm for our 2-way 
speech translation work, [12], and plan to expand the module for 
document translations. 
 
5. ACKNOWLEDGMENTS  
We would like to acknowledge Dr. Jun-Tae Yoon, who provided 
us with a high-quality robust Korean morphological analyzer 
called morany during his stay at the Institute for Research in 
Cognitive Science, University of Pennsylvania as a postdoctoral 
fellow. Morany has served as a pre-processor of the understanding 
module in the CCLINC Korean-to-English translation system. 
 
6. REFERENCES 
[1] Srinivas Bangalore and Aravind Joshi. ?Some Novel 
Applications of Explnation-Based Learning for Parsing 
Lexicalized Tree-Adjoining Grammars,? Proceedings of  33rd 
Association for Computational Linguistics.  pp. 268?275. 1995.  
[2] Noam Chomsky. Barriers.  Linguistic Inquiry Monograph 13. 
MIT Press, Cambridge, MA. 1986. 
[3] Michael Collins. Three Generative, Lexicalized Models for 
Statistical Parsing. Procceedings of the 35th Annual Meeting of 
ACL. pp. 16?23. Madrid, Spain. July. 1997. 
[4] Bonnie Dorr. ?LCS-based Korean Parsing and Translation,? 
Ms. Institute for Advanced Computer Studies and Department of 
Computer Science, University of Maryland. 1997. 
[5] Diana Egedi, Martha Palmer, H-S. Park, Aravind Joshi. 
?Korean to English Translation Using Synchronous TAGs,? 
Proceedings of  the First Conference of the Association for 
Machine Translation in the Americas. pp. 48?55. Columbia, 
Maryland. October 1994. 
[6] James Glass, Joe Polifroni and Stephanie Seneff. 
?Multilingual Language Generation across Multiple Domains,? 
Proceedings of International Conference on Spoken Language 
Processing, pp. 983?986. Yokohama, Japan. September, 1994. 
[7] W.J. Hutchins and H.L. Somers. An Introduction to Machine 
Translation. Academic Press. London. 1992. 
[8] James Allen. Natural Language Understanding, 2nd Edition. 
Benjamin-Cummings Publisher. 1995 
[9] Ken Hale. ?Preliminary Remarks on Configurationality,? 
Proceedings of NELS 12,  pp. 86?96. 1982. 
[10] Young-Suk Lee. Scrambling as Case-Driven Obligatory 
Movement. PhD Thesis (IRCS Report No.: 93-06 ). University of  
Pennsylvania. 1993. 
[11] Young-Suk Lee, Clifford Weinstein, Stephanie Seneff, 
Dinesh Tummala, ?Ambiguity Resolution for Machine 
Translation of  Telegraphic Messages,? Proceedings of  the 35th 
Annual Meeting of ACL. pp. 120?127. Madrid, Spain. July 1997. 
[12] Young-Suk Lee and Clifford Weinstein. ?An Integrated 
Approach to English-Korean Translation and Translingual 
Information Access,? Proceedings of CSTAR Workshop.  
Schwetzingen, Germany.  September, 1999. 
[13] Young-Suk Lee, Clifford Weinstein, Stephanie Seneff, 
Dinesh Tummala. ?Word Sense Disambiguation for Machine 
Translation in Limited Domains,? Manuscript. Information 
Systems Technology Group. MIT Lincoln Laboraotry. January 
1999. 
[14]  Mitch Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. ?Building a large annotated corpus of English: the 
Penn Treebank,? Computational Linguistics 19 (2). pp. 313?
330. 1993. 
[15] Philip Resnik. ?Semantic Similarity in a Taxonomy: An 
Information-Based Measure and Its Application to Problems of 
Ambiguity in Natural Language,? Journal of Artificial 
Intelligence Research (JAIR) 11. pp. 95?130. 1999. 
[16] Stephanie Seneff. ?TINA: A Natural Language System for 
Spoken Language Applications,? Computational Linguistics 18 
(1). pp. 61?92. 1992. 
[17] Clifford Weinstein, Young-Suk Lee, Stephanie Seneff, 
Dinesh Tummala, Beth Carlson, John T. Lynch, Jung-Taik 
Hwang, Linda Kukolich. ?Automated English-Korean Translation 
for Enhanced Coalition Communications,? The Lincoln 
Laboratory Journal 10 (1).  pp. 35?60. 1997. 
 
 
 
 
 
 
Automatic Acquisition of Names Using
Speak and Spell Mode in Spoken Dialogue Systems
Grace Chung   , Stephanie Seneff  and Chao Wang 

Corporation for National Research Initiatives
1895 Preston White Drive, Suite 100, Reston, VA 22209
gchung@cnri.reston.va.us

Spoken Language Systems Group
MIT Laboratory for Computer Science
200 Technology Square, Cambridge, MA 02139

seneff,wangc  @sls.lcs.mit.edu
Abstract
This paper describes a novel multi-stage recog-
nition procedure for deducing the spelling and
pronunciation of an open set of names. The
overall goal is the automatic acquisition of un-
known words in a human computer conver-
sational system. The names are spoken and
spelled in a single utterance, achieving a con-
cise and natural dialogue flow. The first recog-
nition pass extracts letter hypotheses from the
spelled part of the waveform and maps them
to phonemic hypotheses via a hierarchical sub-
lexical model capable of generating grapheme-
phoneme mappings. A second recognition pass
determines the name by combining information
from the spoken and spelled part of the wave-
form, augmented with language model con-
straints. The procedure is integrated into a spo-
ken dialogue system where users are asked to
enroll their names for the first time. The acqui-
sition process is implemented in multiple paral-
lel threads for real-time operation. Subsequent
to inducing the spelling and pronunciation of a
new name, a series of operations automatically
updates the recognition and natural language
systems to immediately accommodate the new
word. Experiments show promising results for
letter and phoneme accuracies on a preliminary
dataset.

The research at CNRI was supported by DARPA un-
der contract number N66001-00-2-8922, monitored through
SPAWAR Systems Center, San Diego. The research at MIT was
supported by DARPA under contract number NBCH1020002
monitored through the Dept. of the Interior, National Business
Center, Acquisition Services Div., Fort Huachuca, AZ.
1 Introduction
Spoken dialogue systems are emerging as an effective
means for humans to access information spaces through
natural spoken interaction with computers. These sys-
tems are usually implemented in such a way that their
knowledge space is static, or is only augmented through
human intervention from the system developers. A sig-
nificant enhancement to the usability of such systems
would be the ability to automatically acquire new knowl-
edge through spoken interaction with its end users. Such
knowledge would include both the spelling and pronunci-
ation of a new word, as well as an understanding of its us-
age in the language (e.g., a semantic category). However,
this is a difficult task to carry out effectively, challeng-
ing both with regard to the automatic acquisition of the
sound-to-letter mapping from typically telephone-quality
speech, and the system level aspect of integrating the
usually off-line activities of system upgrade while seam-
lessly continuing the conversation with the user.
The research reported here is concerned with the ac-
quisition of the user?s name, which is entered via a ?speak
and spell? mode, spoken sequentially for the first and last
names respectively. It is our belief that this would be the
most natural way for the user to enter the information,
and therefore research has been focused on designing a
framework to support that model. Acquiring names is
particularly difficult, not only because English is known
to have highly irregular letter-to-sound rules, but also be-
cause American names come from a diverse collection of
language groups. With the speak and spell entry mode,
there are additional issues of locating the boundary be-
tween the spoken and spelled portions of the utterance,
and of formulating a joint solution.
The framework for acquiring new names is applied to
an enrollment phase of an existing spoken dialogue sys-
tem, the ORION task delegation system (Seneff et al,
2000). ORION allows users to specify tasks to be com-
pleted off-line, and to later be delivered to the user at a
designated time, via either telephone or e-mail. To en-
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 32-39
                                                         Proceedings of HLT-NAACL 2003
ter the enrollment phase, the user calls the ORION system
and responds to the prompt by saying, ?new user,? which
causes the system to enter a system-initiated subdialogue
soliciting a number of facts to be entered into a form that
will represent the system?s future knowledge of this indi-
vidual. The system solicits the user?s full name, a pass-
word for security measures, their work, home, and cell
phone numbers, and their e-mail address, finally asking
for the current time in order to establish the user?s refer-
ence time zone.
After all of the information has been entered, the sys-
tem confirms its proposed spellings for the names, and, if
verified by the user, automatically launches a system up-
date that enters this new information into both the speech
recognition component and the natural language (NL)
grammar. Thus, the next time the user calls the system,
they will be able to log on by speaking their name and
password. If the user rejects the proposed spelling, the
system solicits further input from them, in the form of a
telephone keypad entry of the spelling of the misrecog-
nized name. This information is then incorporated into
the search to propose a final hypothesis.
Central to our methodology is the application of
ANGIE (Seneff et al, 1996), a hierarchical framework
capturing subword structure information, employed here
to predict phoneme-grapheme mappings. In ANGIE,
corpus-based statistical methods are combined with ex-
plicit linguistic information to generalize from the obser-
vation space to unseen words.
Our approach extends work reported earlier (Chung
and Seneff, 2002), in which spelling and pronunciation
of unknown names are extracted from spoken input with
the additional constraint of telephone keypad input. This
work distinguishes itself in that, instead of requiring tele-
phone keypad entries, a user is asked to speak and spell
their name within a single utterance. The novelty lies in
the use of a multi-stage recognizer, where the first stage
proposes a letter graph derived from the spelled portion
of the waveform. A second recognition pass searches the
pronounced name part of the waveform; this final search
is constrained by a phoneme space derived from the letter
graph via ANGIE letter-to-sound mappings.
In the following, previous related work is outlined in
Section 2. Sections 3 details the technology of sound-to-
letter acquisition, and the techniques used to implement a
recognition engine to serve our unique needs. Section 4
is primarily concerned with the engineering aspects for
the real time implementation. Section 5 describes some
evaluation results. This paper concludes with a summary
and a look to the future in Section 6.
2 Previous Work
In the past, many researchers have worked on letter-to-
sound algorithms for text-to-speech conversion (Damper
et al, 1998). More recently, research is begin-
ning to emerge in bi-directional sound-letter gener-
ation and phoneme-to-grapheme conversion. These
topics are important for application to speech recog-
nition, for the purpose of automatically transcribing
out-of-vocabulary (OOV) words at the spoken input.
In (Meng et al, 1996), a hierarchical approach was
used for bi-directional sound-letter generation. On the
Brown Corpus, it achieves word accuracies of 65% for
spelling-to-pronunciation and 51% for pronunciation-to-
spelling. Rentzepopoulos (Rentzepopoulos and Kokki-
nakis, 1996) describes a hidden Markov model approach
for phoneme-to-grapheme conversion, in seven Euro-
pean languages on a number of corpora. The algo-
rithm gave high accuracies when applied to correctly
transcribed words but was not applied to real recogni-
tion output. The work of Marchand and Damper (Marc-
hand and Damper, 2000) addresses both phoneme-to-
grapheme and grapheme-to-phoneme conversion using
a fusion of data-driven and pronunciation-by-analogy
methods, obtaining word accuracies of 57.7% and 69.1%
for phoneme-to-grapheme and grapheme-to-phoneme ex-
periments respectively. These were performed on a cor-
pus of words from a general dictionary.
Some work has focused on proper names, since names
are a particularly challenging open set. In (Ngan et
al., 1998), the problem of generating pronunciations
for proper names is addressed. A 45.5% word error
rate is reported on a set of around 4500 names us-
ing a decision tree method. Font Llitjos (Font Llitjos
and Black, 2001) reports improvements on letter-to-
sound performance on names by adding language ori-
gin features, reporting 61.72% word accuracy on 56000
names. Galescu (Galescu and Allen, 2002) addresses
bi-directional sound-letter generation using a data-driven
joint   -gram method on proper nouns, yielding around
41% word accuracy for letter-to-sound and 68% word ac-
curacy for sound-to-letter.
Few have attempted to convert a spoken waveform
with an unknown word to a grapheme sequence. Using
a Dutch corpus, Decadt et al (Decadt et al, 2002) use a
memory-based phoneme-to-grapheme converter to derive
graphemic output from phonemic recognition hypothe-
ses. Results showed 46.3% accuracy on training data but
only 7.9% accuracy on OOV recognition test data. In
a German system, Schillo (Schillo et al, 2000) built a
grapheme recognizer for isolated words, towards the goal
of unconstrained recognition in German. Accuracies at-
tained are up to 72.89% for city names.
3 Approach
The approach adopted in this work utilizes a multi-pass
strategy consisting of two recognition passes on the spo-
ken waveform. The first-stage recognizer extracts the
Morphology
Syllabification
?Benjamin?
word
stressed root
n
n
codanuc+
eh+
onset
b!
b e
suffix
m
m!
sonset snuc
en
ni
unstressed  root
unuc
ah
a
jh
j
uonset
Phonemics
Graphemics
Figure 1: An example ANGIE parse tree for the word
?Benjamin.?
spelled letters from the spoken utterance, treating the pro-
nounced portion of the word as a generic OOV word.
This is followed by an intermediate stage, where the hy-
potheses of the letter recognition are used to construct
a pruned search space for a final sound-to-letter recog-
nizer which directly outputs grapheme sequences. The
ANGIE framework serves two important roles simultane-
ously: specifying the sound/letter mappings and provid-
ing language model constraints. The language model is
enhanced with a morph N-gram, where the morph units
are derived via corpus-based techniques. In the follow-
ing sections, we first describe the ANGIE framework, fol-
lowed by a detailed description of the multi-pass proce-
dure for computing the spelling and pronunciation of the
word from a waveform.
3.1 ANGIE Sound-to-Letter Framework
ANGIE is a hierarchical framework that encodes sub-
word structure using context-free rules and a probability
model. When trained, it can predict the sublexical struc-
ture of unseen words, based on observations from training
data. The framework has previously been applied in bi-
directional letter/sound generation (Seneff et al, 1996),
OOV detection in speech recognition (Chung, 2000), and
phonological modeling (Seneff and Wang, 2002).
A parsing algorithm in ANGIE produces regular parse
trees that comprise four distinct layers, capturing linguis-
tic patterns pertaining to morphology, syllabification,
phonemics and graphemics. An example parse for the
word ?Benjamin? is given in Figure 1. Encoded at the
pre-terminal-to-terminal layers are letter-sound map-
pings. The grammar is specified through context-free
rules; context dependencies are captured through a
superimposed probability model. The adopted model is
motivated by the need for a balance between sufficient
context constraint and potential sparse data problems
from a finite observation space. It is also desirable
for the model to be locally computable, for practical
reasons associated with the goal of attaching the learned
probabilities to the arcs in a finite state network. Given
these considerations, the probability formulation that has
been developed for ANGIE can be written as follows:
  
	   
	ffProceedings of NAACL HLT 2007, pages 468?475,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Automatic Assessment of Student Translations
for Foreign Language Tutoring
Chao Wang and Stephanie Seneff
Spoken Language Systems Group
MIT Computer Science and Artificial Intelligence Laboratory
The Stata Center, 32 Vassar Street, Cambridge, MA 02139
{wangc,seneff}@csail.mit.edu
Abstract
This paper introduces the use of speech
translation technology for a new type of
voice-interactive Computer Aided Lan-
guage Learning (CALL) application. We
describe a computer game we have devel-
oped, in which the system presents sen-
tences in a student?s native language to
elicit spoken translations in the target new
language. A critical technology is an al-
gorithm to automatically verify the ap-
propriateness of the student?s translation
using linguistic analysis. Evaluation re-
sults are presented on the system?s abil-
ity to match human judgment of the cor-
rectness of a student?s translation, for a set
of 1115 utterances collected from 9 learn-
ers of Mandarin Chinese translating flight
domain sentences. We also demonstrate
the effective use of context information to
improve both recognition performance on
non-native speech as well as the system?s
accuracy in judging the translation quality.
1 Introduction
It is widely recognized that one of the best ways
to learn a foreign language is through spoken di-
alogue with native speakers (Ehsani and Knodt,
1998). However, this is not a practical method in the
classroom setting. A potential solution to this prob-
lem is to rely on computer spoken dialogue systems
to role play a tutor and/or a conversational partner.
Ideally, a voice-interactive system can provide the
learner with endless opportunities for practice and
feedback. However, while a number of dialogue sys-
tems have been developed (or adapted) for language
learning purposes (Seneff et al, 2004; Johnson et
al., 2004), the issues of speech understanding of the
accented and disfluent utterances of a foreign lan-
guage student typically lead to unacceptable perfor-
mance (Eskenazi, 1999).
A relatively successful application of speech pro-
cessing technology is in the area of pronunciation
training (Eskenazi, 1999; Witt, 1999; Hardison,
2004). In this case, a learner repeats words or sen-
tences prompted by the computer, and receives feed-
back on the segmental and suprasegmental quality
of their speech. While such systems can be useful,
they do not help the student learn how to formulate
sentences within the language on their own.
In this paper, we introduce a novel speech trans-
lation game which aims to provide a fun and reli-
able environment for language learners to gain pro-
ficiency in communication. Our application is com-
plementary to pronunciation training in that we pro-
vide the opportunity for the students to practice ex-
pressing themselves in the new language. While
we don?t explicitly evaluate their pronunciations, we
provide implicit feedback in that they must speak
well enough for the speech recognizer to success-
fully process their utterances. The translation exer-
cise can also serve as a preparation stage in which
students can practice relevant vocabulary and sen-
tence constructs, in order to prepare themselves for
the more challenging task of interacting with a dia-
logue system in the same domain.
468
System: (greet and prompt user) Welcome! You are playing at level four. Okay, here?s your first sentence to
translate: The return date is Tuesday October 25.
User: (translate with error) fan3 hui2 ri4 qi1 shi4 si4 yue4 er4 shi2 wu3 hao4 xing1 qi1 er4.
System: (paraphrase in Chinese) fan3 hui2 ri4 qi1 si4 yue4 er4 shi2 wu3 hao4 xing1 qi1 er4.
System: (paraphrase in English) The return date is Tuesday April 25.
User: (repeat date) shi2 yue4 er4 shi2 wu3 hao4.
System: (paraphrase in Chinese) shi2 yue4 er4 shi2 wu3 hao4.
System: (paraphrase in English) October 25.
System: (respond to user) ?Great! You successfully translated the sentence in separate fragments. Let?s move
on to the next sentence: I need to fly to Hong Kong the day after tomorrow.
User: (click on the help button)
System: (repeat English prompt) I need to fly to Hong Kong the day after tomorrow.
System: (offer example translation) wo3 xu1 yao4 hou4 tian1 fei1 xiang1 gang3.
User: (imitate the translation) wo3 xu1 yao4 hou4 tian1 fei1 xiang1 gang3.
... ...
System: (respond to user) You translated nine out of ten sentences. You took on average 1.6 turns per sentence.
You have advanced to level five. Would you like to play another round?
Figure 1: Example interaction between a user and the system.
Our prototype centers on the task of translating
phrases and sentences from English into Chinese, in
the flight reservation domain. As illustrated by the
example dialogue in Figure 1, the system role plays
a language tutor interacting with a Mandarin learner.
The system prompts the student with randomly gen-
erated English sentences to elicit spoken Chinese
translations from the learner. The system para-
phrases each user utterance in both languages, to im-
plicitly inform the user of the system?s internal un-
derstanding, and judges whether the student has suc-
ceeded in the task. The system keeps track of how
many turns a user takes to complete all the sentences
in a game session, and rewards good performance by
advancing the student towards higher difficulty lev-
els. A convenient ?help? button allows the student to
request a translation of the current game sentence,
to help them overcome gaps in their knowledge of
the linguistic constructs or the vocabulary. The stu-
dent can also type any English sentences within the
domain to obtain a reference translation. The sys-
tem utilizes an interlingua-based bidirectional trans-
lation capability, described in detail in (Wang and
Seneff, 2006; Seneff et al, 2006). Both Chinese and
English sentences are parsed into a common mean-
ing representation, which we loosely refer to as an
?interlingua,? from which paraphrases in both lan-
guages can be automatically generated using formal
generation rules.
The key to a successful tutoring system lies in
its ability to provide immediate and pertinent feed-
back on the student?s performance, similar to a hu-
man tutor. A central focus of this paper is to ad-
dress the challenging problem of automatically as-
sessing the appropriateness of a student?s transla-
tion. At first glance, our task appears to share much
in common with machine translation (MT) evalua-
tion (Hovy et al, 2002). Indeed, both are trying to
assess the quality of the translation output, whether
it is produced by a computer or by a foreign lan-
guage student. Nevertheless, there also exist sev-
eral fundamental distinctions. Automatic MT eval-
uation methods, as represented by the well-known
Bleu metric (Papineni et al, 2001), assume the avail-
ability of human reference translations. The algo-
rithms typically compare MT outputs with reference
translations with the goal of producing a quality in-
dicator (on a numeric scale) that correlates with hu-
man judgement. In contrast, our algorithm operates
in the absence of human generated reference trans-
lations1 . Furthermore, our application requires the
evaluation algorithm to make accept/reject decisions
on each individual translation, in the same way as a
language tutor determines whether a translation is
acceptable or not. While our task is more demand-
ing, it is made possible by operating in restricted do-
mains.
The remainder of the paper is organized as fol-
lows. In Section 2, we present an interlingua-based
approach for verifying the correctness of the stu-
dent?s spoken translation. Section 3 describes the
1We employ a grammar of recursive rewrite rules to generate
a very large number of English prompt sentences. It would be
too costly and time-consuming to generate human translations
to cover this space.
469
evaluation framework, followed by results and dis-
cussions in Section 4. Finally, we discuss future
plans for extending our work.
2 Methodology
The two most important aspects in the human eval-
uation of translation quality are fluency and fi-
delity (Hovy et al, 2002). In our case, we con-
sider a student?s translation to be acceptable if it is
well-formed (high fluency) and conveys the same
meaning as the input sentence (high fidelity). We
designed our interlingua-based evaluation algorithm
following these two principles. The algorithm uses
parsability to verify fluency. Fidelity is examined
by extracting and comparing semantic information
from the translation pairs. In the following, we begin
by describing the basic steps involved in our transla-
tion verification algorithm. We then discuss differ-
ent strategies for integrating with the speech recog-
nition system.
2.1 Parsing
Our framework depends strongly on an ability to
parse both the English and Chinese sentences into a
common interlingual meaning representation. Pars-
ing is critical both for producing the two paraphrases
of the student?s utterance and for judging the qual-
ity of their provided translation. Both English and
Chinese grammars are needed to analyze the source
and target sides of each translation pair. The gram-
mars have been carefully constructed so that mean-
ing representations derived from both languages are
as similar as feasible.
We utilized a parser (Seneff, 1992) that is based
on an enhanced probabilistic context-free gram-
mar (PCFG), which captures dependencies beyond
context-free rules by conditioning on the external
left-context parse categories when predicting the
first child of each parent node. While we use a spe-
cific grammar for analyzing flight domain sentences,
we emphasize domain portability of the grammar by
using mainly syntactic information in the majority of
the parse tree rules. Semantics are introduced near
the terminals, mostly involving adjectives, verbs,
nouns and proper noun classes. Rules for general
semantic concepts such as dates and times are orga-
nized into sub-grammars that are easily embedded
into any domain. We have successfully applied the
same strategy in developing both the Chinese and
English grammars. Once a parse tree is obtained, se-
lected parse categories are extracted to form a hier-
archical meaning representation encoding both syn-
tactic and semantic information.
2.2 Semantic Information Comparison
In principle, we can directly compare the meaning
representations derived from the source and target
sides of the translation pair to determine their equiv-
alence. In practice, the meaning representation still
captures too much language-specific detail, which
makes the comparison prone to failure. Even the
pair of English utterances, ?How much is the second
flight?? and ?What is the price of the second flight??
have essentially the same meaning, but would not
produce identical meaning representations. Across
languages, this situation becomes much worse.
We adopted two complementary strategies to in-
crease the chance of a match between the English
prompt and the student translation. First, the English
prompt is translated into a reference Chinese trans-
lation using the existing interlingua translation capa-
bility. This extra step aims at reducing discrepancies
caused by syntactic structure differences between
the two languages. Secondly, we abstract from the
original meaning representation into a simple en-
coding of key-value (KV) pairs. This is accom-
plished using a language generation system (Baptist
and Seneff, 2000), with generation rules determin-
ing what information to extract from the original hi-
erarchical meaning representation. Figure 2 shows
a couple of examples of the KV representation that
we used for scoring.
Another important role of the KV generation step
is to bring in a flexible mechanism for defining
equivalence, which is a tricky task even for human
evaluators. For example, while it is somewhat ob-
vious that ?(1) Give me flights leaving around nine
p m? is equivalent to ?(2) Give me flights depart-
ing around nine p m,? it is unclear whether these
two sentences are equivalent to ?(3) Give me flights
around nine p m? or even ?(4) I would like to leave
around nine p m.? From a pragmatic point of view,
the same speaker intention can be inferred from the
four sentences. On the other hand, it can be ar-
gued that (1) and (2) are completely interchangeable
470
{c eform
:topic "fare"
:airline_name "united"
:trace "how much" }
lian2 he2 hang2 kong1 de5 piao4 jia4 shi4 duo1 shao3?
(What is the fare on United airlines?)
{c eform
:topic "flight"
:source "paris"
:departure_time {c eform
:clock_hour 9
:xm "a m" }}
shang4 wu3 jiu3 dian3 cong2 ba1 li2 chu1 fa1 de5 ban1 ji1.
(Flights from Paris leaving at nine o?clock in the morning.)
Figure 2: Frame representation of the key-value in-
formation for two example Chinese sentences.
while (3) and (4) could not substitute for (1) or (2)
in some circumstances. Criteria for equivalence can
be controlled by what is extracted from the mean-
ing representation. If only a departure time
key is generated for the sentences, then all four sen-
tences will be equivalent. However, if more infor-
mation is preserved in the KV pairs, for example, a
topic key with value flight, then sentence (4)
will not be considered as equivalent to sentences (1)-
(3). Considering that our intended application is lan-
guage tutoring, we lean towards a stricter criterion
for defining equivalence. The KV generation rules
are developed manually, guided by human-rated de-
velopment data. The KV inventory includes over 80
unique keys.
Once the KV pairs are obtained from the prompt
(reference) and the student translation (hypothesis),
a recursive procedure is applied to compare all the
keys in the reference and hypothesis KV frames.
Mismatches are tabulated into substitutions (differ-
ent values for the same key), deletions (extra keys in
the reference), and insertions (extra keys in the hy-
pothesis). A perfect match is achieved if there are
no mismatch errors. Figure 3 summarizes the proce-
dure to evaluate students? spoken translations.
Partial match for a good student translation is a
common problem caused by speech recognition er-
rors, particularly on dates and times. It is natural
for the student to just repeat the ?incorrect? piece
after noticing the error in the system?s paraphrases.
Hence, in the tutoring application, we added a sub-
match mode to the comparison algorithm, which
works in a divide-and-conquer manner. All match-
ing KV pairs in each turn are checked off from the
reference, and a subsequent submatch succeeds once
there are no remaining KV pairs unaccounted for.
One limitation of the incremental comparison algo-
rithm is that it ignores insertion errors. The tutoring
system provides a special reply message when a sen-
tence is translated via partial matches accomplished
over a series of utterances, to distinguish from the
case of a perfect match in a single turn, as illustrated
in the example dialogue.
2.3 Integration with Speech Recognition
A user?s utterance is first processed by the speech
recognizer to produce word hypotheses. The
recognizer is configured from a segment-based
speech recognition system (Glass, 2003), using Chi-
nese acoustic models trained on native speakers?
data (Wang et al, 2000a; Wang et al, 2000b). Tone
features are ignored in the acoustic models; how-
ever, the language model implicitly captures some
tone constraints. This is preferred over modeling
tone explicitly, considering that non-native speak-
ers typically make many tone errors. The language
model was initially trained on Chinese translations
of English sentences generated from the templates
used in the game, and later augmented with addi-
tional data collected from users. The recognizer can
output multiple hypotheses in the form of an N-best
list. The parser is able to convert the N-best list into
a lattice, and re-select a best hypothesis based on a
combination of recognition and parsing scores.
Poor recognition on non-native speech is a ma-
jor performance issue for CALL application. In our
domain, dates, times, and flight numbers are particu-
larly challenging entities for the recognizer. Recog-
nition error typically results in false rejection, caus-
ing frustration to the user. Since the system has
explicit knowledge of the sentence the student is
trying to produce, it should be feasible to exploit
this knowledge to improve speech understanding. A
plausible strategy is to dynamically adjust the rec-
ognizer?s language model in anticipation of what
the user is likely to say, as exemplified by dialogue
context dependent language models (Solsona et al,
2002).
In theory, we could use the automatically gener-
471
Key?Value
Rules
English
Grammar
Chinese
Rules
Chinese
Grammar
Recognition
Prompt
English
Translation
Chinese
HypothesisASR
Interlingua MT
KV Extraction
Language
Understanding
Language
Generation
Language
Understanding
Speech
Interlingua
Interlingua
Language
Generation Comparator
Key?ValueRef KV
Hyp KV
Decision
Student?s
Spoken Translation
Figure 3: Schematic of procedure to evaluate students? spoken translations.
ated reference translation to explicitly bias the lan-
guage model. However, one has to take care not to
bias towards the correct response so strongly that the
student is allowed to make mistakes with impunity.
Furthermore, this strategy would not generalize to
cover all the possible legitimate translations a stu-
dent might produce for that prompt. Instead, we de-
vised a simple strategy that overcomes these issues.
We select a preferred hypothesis from the N-best list
if its KV representation matches the reference. Thus
the student has to speak well enough for a correct an-
swer to appear somewhere in the N -best list, with-
out any manipulations of the recognizer?s language
model. If the parser fails to find a perfect match in
the N-best list, it will choose the hypothesis with the
best score, or fall back to the recognizer?s top hy-
pothesis if no parse theory could succeed.
3 Evaluation Framework
Given a translation pair, the goal of our algorithm is
to make the same accept/reject decision as a human
evaluator. Hence, we can evaluate our algorithm in
a classification framework. In this section, we first
present the data collection and labeling effort. We
then describe a baseline system based on a variant
of the Bleu metric. Finally we briefly describe the
metrics we used to evaluate our algorithms.
3.1 Data Collection and Labeling
During the course of developing a prototype game
system, two developers and two student testers inter-
acted extensively with the system. A total of 2527
Chinese waveforms, recorded during this process,
became development data for finding gaps in the
interlingua-based matching method and for tuning
parameters for the baseline method.
For evaluation, we use 1115 utterances collected
from 9 users with varying degrees of Chinese expo-
sure. These subjects were asked to play the transla-
tion game over the Web and fill out a survey after-
wards. They came from a rich background of Chi-
nese exposure, include advanced ?heritage? speak-
ers of Chinese (including dialects such as Cantonese
and Shanghainese), as well as novices who just
completed two semesters of a college-level Chinese
class.
The speech waveforms recorded from the interac-
tions were manually transcribed with orthography,
gender, and speaker information. The transcriber
was instructed to transcribe spontaneous speech ef-
fects, such as false starts and filled pauses. However,
tonal mispronunciations are completely ignored, and
segmental errors are largely ignored to the extent
that they do not result in a different syllable.
The translation pairs (the English prompt and the
orthographic transcription of the student translation)
were rated independently by two bilingual speakers
to provide reference labels for evaluating the verifi-
cation algorithm. The two raters, both native in Chi-
nese and fluent in English, labelled each translation
with either an ?accept? or a ?reject? label. Transla-
tions can be rejected because of bad language usage
(including false starts) or because of mismatches in
meaning. One labeller rated both development and
test data, while the second labeller only rated the test
data. The interlabeller agreement on the test data has
a kappa score (Uebersax, 1998) of 0.85. The subset
of data for which there was disagreement were rela-
belled by the two raters jointly to reach a consensus.
3.2 Baseline
The Bleu metric has been widely accepted as an
effective means to automatically evaluate the qual-
ity of machine translation outputs (Papineni et al,
472
2001). An interesting question is whether it would
be useful for the purpose of assessing the appro-
priateness of translations produced by non-native
speakers at a sentence by sentence granularity level.
We developed a simple baseline algorithm using the
NIST score, which is a slight variation of Bleu2.
Given an English prompt, the interlingua-based ma-
chine translation system first produces a reference
translation. The student?s translation is then com-
pared against the machine output to obtain a NIST
score. The translation is accepted if the score ex-
ceeds a certain threshold optimized on the develop-
ment data.
Figure 4 plots the Receiver Operating Character-
istics (ROC) curve of the baseline algorithm, ob-
tained by varying the NIST score acceptance thresh-
old. Each point on the curve represents a tradeoff
between accepting an erroneous translation (False
Accept) and rejecting a good one (False Reject). As
shown in the plot, the NIST score based ROC curve
is far from reaching the ideal top-left corner. For
language tutoring purposes, it is desirable to oper-
ate in the low false acceptance region. However, a
20% false acceptance rate will result in the system
rejecting over 35% of correct student translations.
The operating point that minimizes overall classifi-
cation error turns out to be biased towards leniency,
falsely accepting over 60% of translations that are
rejected by human raters. The resulting minimum
error rate on development data transcripts is 23.0%,
with a NIST score threshold of 3.16. The thresh-
old for automatic speech recognition (ASR) outputs
was optimized separately using the 1-best hypothe-
ses of utterances in the development data. The opti-
mal threshold on ASR outputs is 1.60, resulting in a
classification error rate of 24.1%. The majority clas-
sifier, corresponding to the (1, 1) point on the curve,
translates into a 31.6% error rate on the development
data.
3.3 Evaluation Metrics
We evaluated the overall system performance on test
data using human decisions as ground truth. Al-
2We determined empirically that the NIST score works
slightly better than the Bleu score in our application. The
scores are computed using the NIST MT scoring tool from:
ftp://jaguar.ncsl.nist.gov/mt
/resources/mteval-v11b.pl
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
False Positive (False Accept)
Tr
ue
 P
os
tiv
e 
(1 
? F
als
e R
eje
ct)
ROC on Dev Set Transcripts
 
 
ROC curve
Min Error Point
Figure 4: ROC curve by changing acceptance
threshold on the NIST score on transcriptions of de-
velopment data.
though we can not generate an ROC curve for our
proposed algorithm (because it is a non-parametric
method), we plot its performance along with the
ROC curve of the baseline system for a more thor-
ough comparison.
We evaluated the different ASR integration strate-
gies (1-best hypothesis, 10-best hypotheses, using
contextual constraints from reference KV) based on
sentence classification error rates as well as speech
recognition performance.
4 Results and Discussions
Table 1 summarizes the false accept, false reject, and
overall classification error rates on unseen test data.
With manual transcripts as inputs, the baseline al-
gorithm using the NIST score achieved a classifica-
tion error rate of 19.3%, as compared with 25.0%
for the trivial case of always accepting the user sen-
tence (Majority classifier). The KV-based algorithm
achieved a much better performance, with only a
7.1% classification error rate. This translates into
a kappa score of 0.86, which is slightly above the
level of agreement initially achieved by the two la-
bellers. Note that the performance difference com-
pared to the baseline system is mostly attributed to a
large reduction in the ?False Accept? category.
Interestingly, the NIST method degrades only
slightly when it is applied to the speech recognition
1-best output rather than the transcript. However,
this result is deceptive, as it is now even more bi-
473
False False Classification
Transcript Reject Accept Error
Majority 0.0% 100% 25.0%
NIST 8.0% 54.5% 19.6%
KV 7.3% 6.8% 7.2%
False False Classification
ASR Reject Accept Error
NIST 4.2% 77.1% 22.4%
KV 1-best 32.1% 4.3% 25.1%
KV 10-best 27.0% 7.2% 22.1%
KV Context 13.5% 14.7% 13.8%
Table 1: Classification results for various evaluation
systems, on both transcripts and automatic speech
recognition (ASR) outputs. Note that the ?KV Con-
text? condition favors a hypothesis that matches the
prompt KV.
ased towards a ?False Accept? strategy, causing over
three quarters of the students? erroneous utterances
to be accepted. The KV method is much more sus-
ceptible to speech recognition error because of its
deep linguistic analysis. For instance, any recog-
nition errors causing a parse failure will result in
a ?reject? decision, which explains the high error
rate when only the 1-best hypothesis is used. How-
ever, the KV algorithm can improve substantially by
searching the full N-best list (N = 10) for a plau-
sible analysis. When contextual information (KV
Context) is used, our simple strategy of favoring the
hypothesis matching the reference KV reduces the
classification error rate dramatically.
A plot of the receiver operating characteristics of
these methods in Figure 5 reveals a clear picture of
the performance differences. All of the KV points
are clustered in the upper left corner of the plot,
above the ROC curve of the NIST-based method.
The NIST-score based classifier (represented by the
square marker on the ROC curve) is heavily biased
towards making the acceptance decision (the major-
ity class). In contrast, the KV method operates in the
low ?False Accept? area. It achieves a much lower
false rejection rate when compared with the NIST
method operating at an equivalent false acceptance
point.
Although the classification error rate clearly im-
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
False Positive (False Accept)
Tr
ue
 P
os
tiv
e 
(1 
? F
als
e R
eje
ct)
ROC on Test Set Transcripts and ASR Outputs
 
 
NIST?transcript
KV?transcript
NIST?ASR (1?best)
KV?ASR (1?best)
KV?ASR (10?best)
KV?ASR (context)
Figure 5: Comparison of ROC of different methods.
Syllable Sentence
ER(%) RR(%) ER(%) RR(%)
1-best 11.6 - 40.4 -
10-best 10.7 7.8 38.7 4.2
Context 8.7 25.0 30.0 25.7
Table 2: Comparison of speech recognition per-
formance in syllable error rates and sentence error
rates, for three different strategies of utterance selec-
tion from an N -best list. (ER stands for error rate,
RR stands for relative reduction.)
proves when the KV method makes use of the N-
best list and incorporates contextual constraints, the
ROC plot seems to suggest that the error reduction
might simply be attributed to a shift in the operat-
ing point: the improvements are caused by a bias
towards making the majority class decision. We use
improvements in speech recognition to demonstrate
that this is not the case (at least not entirely). Table 2
summarizes the syllable and sentence error rates on
the test data, for the three configurations discussed
previously (1-best, 10-best, and Context). By using
a tighter integration with the parser with contextual
constraints, we greatly improved speech recognition
performance, marked by reductions of syllable and
sentence error rates by 25% and 25.7% respectively.
5 Conclusions and Future Work
In this paper, we have presented an algorithm for au-
tomatically assessing spoken translations produced
by language learners. The evaluation results demon-
474
strated that our method involving deep linguistic
analysis of the translation pair can achieve high con-
sistency with human decisions, and our strategy of
incorporating contextual constraints is effective at
improving speech recognition on non-native speech.
While our solution is domain specific, we emphasize
domain portability in the linguistic analysis mod-
ules, so that similar capabilities in other domains can
be quickly developed even in the absence of train-
ing data. Our interlingua framework also makes the
methodology agnostic to the direction of source and
target languages. A similar application for native
Mandarin speakers learning English could be instan-
tiated by using the same components for linguistic
analysis.
A major challenge in our problem is in determin-
ing equivalence between the meanings of a transla-
tion pair. While our approach of using a rule-based
generation system gives the developer great flexibil-
ity in deriving an appropriate KV representation, the
comparison algorithm is somewhat primitive: it re-
lies entirely on the generation rules to produce the
right KV representation. In future work, we plan
to apply machine learning techniques to this prob-
lem. With the data we have collected and labelled
(and the effort is ongoing), it becomes feasible to
examine the use of data-driven methods. As alluded
to in our evaluation methodology, we can cast the
problem into a classification framework. Lexical,
n-gram, and alignment based features can be ex-
tracted from the translation pairs, which can be fur-
ther enhanced by features obtained from deep lin-
guistic analysis. This will relieve the burden on the
semantic analysis component, and improve the over-
all portability of our approach.
We also plan to expand our application to many
other domains appropriate for language learning,
and test the effectiveness of the translation game as
a means for language learning.
6 Acknowledgements
This research is supported in part by ITRI and the
Cambridge MIT Initiative. The authors would like to
acknowledge Yushi Xu for annotating the data. We
are also grateful to Michael Collins and the anony-
mous reviews for their helpful comments and sug-
gestions.
References
L. Baptist, S. Seneff. 2000. Genesis-II: A versatile sys-
tem for language generation in conversational system
applications. In Proc. ICSLP, Beijing, China.
D. Ehsani, E. Knodt. 1998. Speech technology in
computer-aided language learnings: Strengths and
limitations of a new call paradigm. Language Learn-
ing & Technology, 2(1):54?73.
M. Eskenazi. 1999. Using automatic speech process-
ing for foreign language pronunciation tutoring: Some
issues and a prototype. Language Learning & Tech-
nology, 2(2):62?76.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17:137?152.
D. Hardison. 2004. Generalization of computer-assisted
prosody training: quantitative and qualitative findings.
Language Learning & Technology, 8(1):34?52.
E. Hovy, M. King, A. Popescu-Belis. 2002. Principles
of context-based machine translation evaluation. Ma-
chine Translation, 7(1):43?75.
W. L. Johnson, S. Marsella, H. Vihjalmsson. 2004.
The DARWARS tactical language training system. In
Proc. I/ITSEC.
K. Papineni, S. Roukos, T. Ward, W.-J. Zhu. 2001.
BLEU: a method for automatic evaluation of machine
translation. In Proc. ACL.
S. Seneff, C. Wang, J. Zhang. 2004. Spoken conversa-
tional interaction for language learning. In Proc. IN-
STIL/CALL.
S. Seneff, C. Wang, J. Lee. 2006. Combining linguis-
tic and statistical methods for bi-directional English
Chinese translation in the flight domain. In Proc. of
AMTA.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. Computational Lin-
guistics, 18(1).
R. A. Solsona, E. Fosler-Lussier, H.-K. J. Kuo,
A. Potamianos, I. Zitouni. 2002. Adaptive language
models for spoken dialogue systems. In ICASSP.
J. S. Uebersax. 1998. Diversity of decision-making mod-
els and the measurement of interrater agreement. Psy-
chological Bulletin, 101:140?146.
C. Wang, S. Seneff. 2006. High-quality speech transla-
tion in the flight domain. In Proc. of InterSpeech.
C. Wang, D. S. Cyphers, X. Mou, J. Polifroni, S. Sen-
eff, J. Yi, V. Zue. 2000a. MUXING: A telephone-
access Mandarin conversational system. In Proc. IC-
SLP, 715?718, Beijing, China.
H. C. Wang, F. Seide, C. Y. Tseng, L. S. Lee. 2000b.
MAT2000 ? Design, collection, and validation on a
Mandarin 2000-speaker telephone speech database. In
Proc. ICSLP, Beijing, China.
S. M. Witt. 1999. Use of Speech Recognition in
Computer-assisted Language Learning. Ph.D. thesis,
Department of Engineering, University of Cambridge,
Cambridge, UK.
475
Proceedings of NAACL HLT 2007, Companion Volume, pages 153?156,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Reversible Sound-to-letter/Letter-to-sound Modeling
based on Syllable Structure  
Stephanie Seneff
Spoken Language Systems Group
MIT Computer Science and Artificial Intelligence Laboratory
The Stata Center, 32 Vassar Street, Cambridge, MA 02139
seneff@csail.mit.edu
Abstract
This paper describes a new grapheme-to-
phoneme framework, based on a combi-
nation of formal linguistic and statisti-
cal methods. A context-free grammar is
used to parse words into their underly-
ing syllable structure, and a set of sub-
word ?spellneme? units encoding both
phonemic and graphemic information can
be automatically derived from the parsed
words. A statistical  -gram model can
then be trained on a large lexicon of words
represented in terms of these linguistically
motivated subword units. The frame-
work has potential applications in mod-
eling unknown words and in linking spo-
ken spellings with spoken pronunciations
for fully automatic new-word acquisition
via dialogue interaction. Results are re-
ported on sound-to-letter experiments for
the nouns in the Phonebook corpus.
1 Introduction
Spoken dialogue systems are emerging as an effec-
tive means for humans to access information spaces
through natural spoken interaction with comput-
ers. A significant enhancement to the usability of
such systems would be the automatic acquisition
of new knowledge through spoken interaction with
its end users. Such knowledge would include both

This research was supported by the Industrial Technology
Research Institute (ITRI) in Taiwan.
the spelling and pronunciation of a new word, ide-
ally leading to a successful match to an entry in a
large external database. To take advantage of an
integrated approach to recognizing the spoken and
spelled forms of a new word, there is a need for
a high-quality reversible phoneme-grapheme map-
ping system. This is a difficult task for English due
to the many inconsistencies in letter-to-sound rules
as a consequence of borrowings from multiple lan-
guage groups.
It is also increasingly the case that dialogue sys-
tems must dynamically adjust the recognizer vocab-
ulary to handle changing database contents. If a sys-
tem can reliably predict the pronunciation of a new
word algorithmically, especially if substantiated by
a spoken pronunciation of the word during active us-
age, it will be far more effective in satisfying chang-
ing user needs.
In this paper, we describe a new reversible
grapheme-to-phoneme framework based on combin-
ing formal linguistic knowledge with statistical data-
driven techniques. We first describe and motivate
our choice for the linguistic model. Section 3 de-
scribes the iterative process for obtaining a subword
baseforms lexicon used to train the statistical model.
Sections 4 and 5 present experiments and results for
sound-to-letter modeling on 5000 nouns. We con-
clude after a brief section on related work.
2 Linguistic Model
Our linguistic model is based on syllable structure,
but we felt that whole-syllable units would be too
large to adequately generalize to unseen data. We
thus decided to decompose syllables into onsets and
153
rhyme1 onset rhyme usyl rhyme usyl ambi rhyme
-aek s+ -ehl -axr -aam -ax+ tf -er+
a c c e l e r o m e t e r
Figure 1: Linguistic representation for the word ?accelerometer,? illustrating the structure of our model.
rhymes, which would then become subword pronun-
ciation units in a lexical baseforms file. These sub-
word units would, in turn, be specified in terms of
phonemic baseforms in a separate subword lexicon.
Thus the words in our training set are represented
in terms of subword units, which are converted into
phonemic baseforms by simple lookup of the sub-
word pronunciations.
A difficult aspect for English is to decide where
to place the syllable boundary within a sequence
of intersyllabic consonants. To guide this decision,
we made use of sonority constraints combined with
maximal stress and maximal onset principles. For
a select subset of intersyllable consonants, we in-
voke the special category ?ambi? for ?ambisyllabic,?
to allow the consonant to be ambiguously assigned.
In addition to onset and rhyme, we also include the
category ?affix,? to account for those instances of
(usually coronal) consonants that would lead to a
violation of sonority principles in the coda position
(e.g., ?fifths,? ?kept?, etc.), following linguistic the-
ory (Blevins, 1995).
We decided to distinguish the first stressed and
the first unstressed syllable from all other stressed
and unstressed syllables in the word, in order to en-
code separate statistics for the privileged first posi-
tion. We also combined onset and rhyme into a sin-
gle whole syllable unit for a selected subset of rel-
atively frequent unstressed syllables. In total, our
current inventory consists of 678 unique symbols.
An example hierarchical representation in our for-
malism is illustrated in Figure 1, for the word ?ac-
celerometer.?
3 Procedures
Our approach is based on a technique that exploits
a context-free grammar applied to a large lexicon to
aid in the preparation of a baseforms file encoding
the lexicon in terms of a set of linguistically moti-
vated subword units. The subword units, which en-
code syllabification and pronunciation, are initially
acrostics -ax+ kr+ -aas t -axk +s
actualities -aek ch+ -uw+ -ael -ax+ tf -iy+ +z
fabrications f+ -aeb r+ -ax+ k -ey+ shaxn +z
preferences pr+ -ehf rsyl -axn +s -axz
skepticism sk+ -ehp t -ax+ s+ -ihz -m
striplings str+ -ihp l+ -ihng +z
Figure 2: Sample entries from the subword lexicon.
derived automatically from a phonemic baseforms
file through simple rewrite rules. The grammar is
developed manually, a process that amounts to iden-
tifying all the possible ways to spell each subword
unit. In an iterative procedure, parse failures are
manually corrected either by modifying erroneous
pronunciations or by augmenting the rules govern-
ing permissible letter sequences for the subword
units. Through this process we have now converted
phonemic baseforms for a lexicon of 140,000 words
into the new subword units. Example entries in the
baseforms file are shown in Figure 2.
Once a grammar and a large lexicon of subword
baseforms are available, the next step is to cre-
ate a statistical language model encoding the letter-
subword mappings. We have decided to create a new
set of subword units, which we call ?spellnemes,?
combining the letter sequence and associated pro-
nunciation into a single symbolic tag, as illustrated
in Figure 3. The sequence of spellnemes associated
with each word in the lexicon can easily be obtained
by parsing the word, constrained by its subword re-
alization. The spellneme sequences for each word in
the lexicon are then used to train a trigram language
model. Our formalism currently has 2541 unique
spellnemes, on average nearly a 4-fold expansion
over the number of pronunciation-based subwords.
Derivative sound-to-letter and letter-to-sound sys-
tems are straightforward. For sound-to-letter, a pro-
vided phonemic transcript is exhaustively expanded
to a graph of all possible subword realizations, and
subsequently into a graph of all spellnemes asso-
154
b r   591  oo k   547  l   617  e t   263 
b r   591  oo k   547  l   617  i n e   281 
b r   591  oo k   547  l   617  y n   250 
b r   591  oo k   547  m   619  o n t   43 
Figure 3: Sample entries from the tagged corpus
which is used to train the statistics of the  -gram
language model. The numeric tags encode the asso-
ciated subword unit, each of which maps to a unique
phonemic sequence.
ciated with each subword. The trigram language
model is applied to produce an N-best list of the
top-scoring hypothesized spellneme sequences. The
letter-to-sound system exhaustively expands the let-
ter sequence into all possible spellneme sequences.
After applying the trigram language model, the N-
best list of spellneme sequences can be mapped to
the pronunciations by concatenation of the phone-
mic realizations of the individual subwords.
4 Experiments on Phonebook
We imagine a two-stage speech recognition frame-
work for a word spoken in isolation, in which the
first stage uses subword units that encode only pro-
nunciation, and produces an N-best list of hypothe-
sized pronunciations, represented as phonemic base-
forms. The second stage is tasked with hypothe-
sizing possible spellings from the provided phone-
mic baseforms, and then verifying them by a match
with a lexical entry. For the purposes of this paper,
we assume a perfect phonemic baseform as input,
and investigate the quality of the N-best list of hy-
pothesized spellings automatically generated by the
sound-to-letter system. We quantify performance by
measuring the depth of the correct word in the gen-
erated N-best list.
Our experiments were conducted on a set of
nearly 5000 nouns and proper nouns, a sub-
set of the 8000 word Phonebook vocabulary that
were identified as nouns using the Web site
http://www.comp.lancs.ac.uk/ucrel/claws/. We se-
lected this set of words for two reasons: (1) they
contain a substantial number of nouns not included
in our original training lexicon, and (2) they will al-
low us to conduct speech recognition experiments
from the available Phonebook corpus of words spo-
ken in isolation over the telephone.
The trigram training corpus was restricted to a
subset of 55,159 entries in our original lexicon,
containing the words that were tagged as nouns in
Comlex. We are interested in quantifying the gap
between in-vocabulary (IV) and out-of-vocabulary
(OOV) words, with respect to the training corpus.
We also measure the gains that can be realized
through manual repair of automatically generated
baseforms for training the sound-to-letter system.
Thus we conducted experiments on the following
four conditions:
1. Train on 55,159 nouns, test on the 3478 word
IV subset of Phonebook nouns.
2. Train on 55,159 nouns, test on the 1518 OOV
words in Phonebook.
3. Augment the training set with entries for the
1518 OOV words, that are obtained automati-
cally by processing them through the letter-to-
sound system. Test on the OOV subset.
4. Augment the training lexicon with manually
corrected pronunciations for the OOV subset.
Test on the OOV subset.
Items (3) and (4) will show us the degree to
which improvements can be gained through auto-
matic methods, once a new list of nouns becomes
available, as well as how much further gain can be
realized after manual correction. Automatic meth-
ods will be feasible for a dialogue system which can
extract from the Web a list of nouns appropriate for
the domain, but has no phonemic baseforms avail-
able for those nouns.
5 Results
Results are shown in Table 1. With an N-best list
of 30, the system has a very low failure rate for all
conditions. However, there is a marked difference in
performance in terms of the depth of the correct an-
swer. The mean depth is 2.07 for the OOV words, as
contrasted with only 1.15 for the IV words. Fully
automatic methods to improve the sound-to-letter
system lead to substantial gains, reducing the mean
depth to 1.54. Manual correction provides signifi-
cant further gains, achieving a mean depth of 1.13,
comparable to that of the original IV subset. There
were two cases where an incorrect match to a lexical
entry was found at a higher level in the N-best list
155
Top 1 Top 2 Top 3 Top 4 Top 5 Top 30 Mean Depth Failed
OOV 65.7% 80.7% 86.5% 90.0% 91.7% 98.4% 2.07 1.6%
plus auto 84.0% 91.6% 93.4% 94.7% 95.7% 99.0% 1.54 1.0%
plus manual 92.2% 98.0% 98.9% 99.3% 99.6% 99.9% 1.13 0.1%
IV 91.8% 97.5% 98.8% 99.3% 99.5% 100.0% 1.15 0.0%
Table 1: Percentage of words spelled correctly as a function of N-best depth for sound-to-letter experiments.
See text for discussion.
than the correct match. These were the homonym
pairs: carolyn/caroline and jasmine/jazzman.
Nouns that fail to appear in the top 30 can poten-
tially still be recovered through simple spell check-
ing methods. Using a conservative approach of al-
lowing only a single letter insertion, substitution or
deletion, and further, of requiring that the grammar
could parse the corrected word under the constraints
of the system?s proposed subwords, we were able to
recover over 60% of the failures.
6 Related Work
Many researchers have worked on letter-to-
sound modeling for text-to-speech conver-
sion (R. I. Damper and Gustafson, 1998). The
topic of bi-directional phoneme-to-grapheme
conversion is becoming important for application
to unknown words and new word acquisition
in speech understanding systems (Chung et al,
2003), although it is difficult to compare results
due to different representations and data sets.
In (Meng, 1996), a hierarchical approach was
used for bi-directional sound-letter generation.
(Rentzepopoulos and Kokkinakis, 1996) describes
a hidden Markov model approach for phoneme-to-
grapheme conversion, in seven European languages
evaluated on a number of corpora. (Marchand and
Damper, 2000) uses a fusion of data-driven and
pronunciation-by-analogy methods, obtaining word
accuracies of 57.7% and 69.1% for phoneme-to-
grapheme and grapheme-to-phoneme experiments
respectively, when evaluated on a general dictionary.
(Llitjos and Black, 2001) report improvements on
letter-to-sound performance on names by adding
language origin features, yielding 61.72% word
accuracy on 56,000 names. (Galescu and Allen,
2002) addresses bi-directional sound-letter gener-
ation using a data-driven joint  -gram method on
proper nouns, yielding around 41% word accuracy
for sound-to-letter and 68% word accuracy for
letter-to-sound.
7 Summary and Conclusions
In this paper, we report on a new technique for
reversible letter-to-sound sound-to-letter modeling,
which is based on linguistic theory and statistical
modeling. The system was evaluated on a set of
nearly 5000 nouns from the Phonebook domain,
separately for in-vocabulary and out-of-vocabulary
subsets, with respect to the training corpus for the
sound-to-letter system. In future work, we plan to
evaluate the effectiveness of the model for automatic
new word acquisition in spoken dialogue systems.
References
J. Blevins. 1995. The syllable in phonological theory. J. Gold-
smith, Ed., the Handbook of Phonological Theory. Black-
well, Oxford.
G. Chung, S. Seneff, and C. Wang. 2003. Automatic acquisi-
tion of names using speak and spell mode in spoken dialogue
systems. In Proc. of HLT-NAACL, Edmonton, Canada.
L. Galescu and J. Allen. 2002. Name pronunciation with a
joint n-gram model for bi-directional grapheme-to-phoneme
conversion. In Proc. ICSLP, pages 109?112, Denver, CO.
A. Font Llitjos and A. Black. 2001. Knowledge of language
origin improves pronunciation accuracy of proper names. In
Proc. Eurospeech, Aalborg, Denmark.
Y. Marchand and R. I. Damper. 2000. A multi-strategy ap-
proach to improving pronunciation by analogy. Computa-
tional Linguistics, 26(2):195?219.
H. Meng. 1996. Reversible letter-to-sound   sound-to-letter
generation based on parsing word morphology. Speech
Computation, 18(1):47?64.
M. J. Adamson R. I. Damper, Y. Marchand and K. Gustafson.
1998. Comparative evaluation of letter-to-sound conversion
techniques for English text-to-speech synthesis. In Proc.
IWSS, pages 53?58, Jenolan Caves, Australia.
P. Rentzepopoulos and G. K. Kokkinakis. 1996. Efficient mul-
tilingual phoneme-to-grapheme conversion based on HMM.
Computational Linguistics, 22(3).
156
NAACL HLT Demonstration Program, pages 13?14,
Rochester, New York, USA, April 2007. c?2007 Association for Computational Linguistics
Spoken Dialogue Systems for Language Learning  
Stephanie Seneff, Chao Wang, and Chih-yu Chao
Spoken Language Systems Group
MIT Computer Science and Artificial Intelligence Laboratory
The Stata Center, 32 Vassar Street, Cambridge, MA 02139, USA

seneff,wangc,chihyu  @csail.mit.edu
Abstract
This demonstration will illustrate interac-
tive computer games intended to help a
native speaker of English learn Mandarin.
These systems provide users with human-
like conversational exercises with contex-
tualized help mechanisms. Two distinctly
different activities, a translation game and
a dialogue game are illustrated. The level
of difficulty can be manipulated, and the
sentence variations covered by the sys-
tems familiarize users with different ex-
pressions of the same meaning. The sys-
tems preserve the qualities of a typical
computer system, being infinitely patient
and available any time of day. Students
will be able to repeatedly practice conver-
sation with no embarrassment.
1 Introduction
Mandarin Chinese is one of the most difficult lan-
guages for a native English speaker to learn. Chi-
nese is substantially more difficult to master than
the traditional European languages currently being
taught in America ? French, Spanish, German, etc.,
because of the lack of common roots in the vocab-
ulary, the novel tonal and writing systems, and the
distinctly different syntactic structure.
It is widely agreed among educators that the best
way to learn to speak a foreign language is to en-
gage in natural conversation with a native speaker of
the language. Yet this is also one of the most costly
ways to teach a language, due to the inherently one-
to-one student-teacher ratio that it implies.

This research is supported in part by the Industrial Tech-
nology Research Institute and the Cambridge MIT Initiative.
Recent research in the Spoken Language Systems
group at MIT has focused on the idea of designing
entertaining computer games as a device for teach-
ing a foreign language, with initial emphasis on the
language pair, English and Mandarin. The games are
accessible at a Web page, and the student?s speech is
captured from a headset microphone to support nat-
ural spoken dialogue interaction. The system can
also be installed to run completely stand-alone on
the local laptop computer.
2 Demonstrated Systems
The demonstrated systems comprise two related ac-
tivities, the translation game and the dialogue game.
The translation game serves as preparation for the
dialogue game: the user acquires expertise in speak-
ing within the domain in the target language. The
system randomly presents sentences in English and
asks the student to speak a sentence of equivalent
meaning in Mandarin. To imitate the competitive
spirit of video games, the system offers ten difficulty
levels, which are automatically adjusted depending
on the student?s monitored performance. After ad-
vancing to the highest difficulty level, they will sub-
sequently be much better equipped to converse with
the system within the dialogue game.
The dialogue game involves spoken conversa-
tional interaction to solve a particular scenario. The
student and computer are tasked with jointly solv-
ing a specified goal. Differing difficulty levels are
achieved via the device of a robotic tutor who assists
the student in solving their side of the conversation.
2.1 Translation Game
The translation game is motivated by the learning
approach advocated by Pimsleur (1967). By prac-
ticing translation repeatedly, language learners are
13
able to internalize the structures of the target lan-
guage, and thus the vocabulary, grammar rules, and
pronunciation are practiced concurrently. The user
begins by translating isolated vocabulary items in
Level 1, advancing to phrases and full sentences at
higher levels. The most difficult level, Level 10, in-
volves long and complicated sentences.
We have implemented this game in two domains:
(1) flight reservations, and (2) hobbies and sched-
ules. Details of the translation procedure can be
found in (Wang and Seneff, 2006), and the algo-
rithm for assessment is described in detail in (Wang
and Seneff, 2006). The input utterance is processed
through the speech recognizer and language under-
standing (Seneff, 1992) components, to achieve a
simple encoding of its meaning. The system com-
pares this meaning representation to one automati-
cally derived from the targeted English equivalent.
The system then speaks a paraphrase of the user?s
hypothesized utterance in both Chinese and En-
glish (Baptist and Seneff, 2000). If it has determined
that the student was successful, it congratulates them
and prompts them with the next English sentence for
translation. At any time, the student can ask for as-
sistance, in which case the system will provide them
with a ?correct? translation of the English utterance,
which they can then attempt to imitate.
2.2 Dialogue Game
In the dialogue game (Seneff, 2006), the user is
asked to solve a particular scenario, by role play-
ing a specified persona, which changes dynamically
every time the game is played. We will demonstrate
the dialogue game in the hobbies and schedules do-
main. The student is provided with a specification
of their preferences for participating in possible ac-
tivities (swimming, dancing, watching movies, etc.)
as well as a calendar specifying activities they are
planning to do in the next few days. They are tasked
with arranging with the computer to jointly partici-
pate in an activity that they both like, at a time when
both are free. Another option is for either party to
invite the other one to join them in an activity that is
already on their schedule.
In addition to the robotic dialogue partner, the stu-
dent is assisted in solving the task by a robotic tutor,
who helps them plan what to say next. The tutor
works with the same information that the student
has, and independently plans the student?s half of
the conversation. At each dialogue turn, it provides
a proposed response, based on the evolving dialogue
context. Five different difficulty levels have been
implemented, as follows:
1. Eavesdropping: The student can simply let the
tutor carry out their side of the conversation by
clicking a button to advance each dialogue turn.
2. Parroting: The system presents a proposed sen-
tence in pinyin on the screen, and the student
can just read it out loud well enough to be suc-
cessfully understood.
3. Translation: The system presents an English
sentence which the student needs to translate
into Chinese.
4. Characters: The system presents the Chinese
sentence in a character encoding.
5. Solo: The tutor stops being pro-active, but can
be consulted if necessary.
Both the translation game and the dialogue game
will be illustrated live in the demonstration. The
systems can be evaluated by two types of basic per-
formance measures: (1) for each system, the recog-
nition accuracy and the translation accuracy serve
as an index of quality; (2) calculating the success
rate in the translation game and the number of turns
taken to complete each dialogue will provide a quan-
titative view of interaction. Also a pre- & post-test
design in the user study will further confirm the ped-
agogic value of the systems. Ongoing and future
work involves expanding the domains supported and
introducing the games to the classroom setting.
References
Baptist, L. and S. Seneff. 2000. ?Genesis-II: A Ver-
satile System for Language Generation in Conversa-
tional System Applications,? Proc. ICSLP, III:271?
274.
Pimsleur, P. 1967. ?A Memory Schedule,? Modern Lan-
guage Journal, 51:73?75.
Seneff, S. 1992. ?TINA: A Natural Language System
for Spoken Language Applications,? Computational
Linguistics, 18(1):61?86.
Seneff, S. 2006. ?Interactive Computer Aids for Acquir-
ing Proficiency in Mandarin,? Keynote Speech, Proc.
ISCSLP, pp. 1?11.
Wang, C and S. Seneff. 2006. ?High-quality Speech
Translation in the Flight Domain,? Proc. INTER-
SPEECH.
Wang, C. and S. Seneff 2007. ?Automatic Assessment of
Student Translations for Foreign Language Tutoring,?
Proc. NAACL-HLT.
14
Proceedings of ACL-08: HLT, pages 174?182,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Correcting Misuse of Verb Forms
John Lee and Stephanie Seneff
Spoken Language Systems
MIT Computer Science and Artificial Intelligence Laboratory
Cambridge, MA 02139, USA
{jsylee,seneff}@csail.mit.edu
Abstract
This paper proposes a method to correct En-
glish verb form errors made by non-native
speakers. A basic approach is template match-
ing on parse trees. The proposed method im-
proves on this approach in two ways. To
improve recall, irregularities in parse trees
caused by verb form errors are taken into ac-
count; to improve precision, n-gram counts
are utilized to filter proposed corrections.
Evaluation on non-native corpora, represent-
ing two genres and mother tongues, shows
promising results.
1 Introduction
In order to describe the nuances of an action, a verb
may be associated with various concepts such as
tense, aspect, voice, mood, person and number. In
some languages, such as Chinese, the verb itself is
not inflected, and these concepts are expressed via
other words in the sentence. In highly inflected lan-
guages, such as Turkish, many of these concepts are
encoded in the inflection of the verb. In between
these extremes, English uses a combination of in-
flections (see Table 1) and ?helping words?, or aux-
iliaries, to form complex verb phrases.
It should come as no surprise, then, that the mis-
use of verb forms is a common error category for
some non-native speakers of English. For example,
in the Japanese Learners of English corpus (Izumi et
al., 2003), errors related to verbs are among the most
frequent categories. Table 2 shows some sentences
with these errors.
Form Example
base (bare) speak
base (infinitive) to speak
third person singular speaks
past spoke
-ing participle speaking
-ed participle spoken
Table 1: Five forms of inflections of English verbs (Quirk
et al, 1985), illustrated with the verb ?speak?. The base
form is also used to construct the infinitive with ?to?. An
exception is the verb ?to be?, which has more forms.
A system that automatically detects and corrects
misused verb forms would be both an educational
and practical tool for students of English. It may
also potentially improve the performance of ma-
chine translation and natural language generation
systems, especially when the source and target lan-
guages employ very different verb systems.
Research on automatic grammar correction has
been conducted on a number of different parts-of-
speech, such as articles (Knight and Chander, 1994)
and prepositions (Chodorow et al, 2007). Errors in
verb forms have been covered as part of larger sys-
tems such as (Heidorn, 2000), but we believe that
their specific research challenges warrant more de-
tailed examination.
We build on the basic approach of template-
matching on parse trees in two ways. To improve re-
call, irregularities in parse trees caused by verb form
errors are considered; to improve precision, n-gram
counts are utilized to filter proposed corrections.
We start with a discussion on the scope of our
174
task in the next section. We then analyze the spe-
cific research issues in ?3 and survey previous work
in ?4. A description of our data follows. Finally, we
present experimental results and conclude.
2 Background
An English verb can be inflected in five forms (see
Table 1). Our goal is to correct confusions among
these five forms, as well as the infinitive. These
confusions can be viewed as symptoms of one of
two main underlying categories of errors; roughly
speaking, one category is semantic in nature, and the
other, syntactic.
2.1 Semantic Errors
The first type of error is concerned with inappropri-
ate choices of tense, aspect, voice, or mood. These
may be considered errors in semantics. In the sen-
tence below, the verb ?live? is expressed in the sim-
ple present tense, rather than the perfect progressive:
He *lives there since June. (1)
Either ?has been living? or ?had been living? may
be the valid correction, depending on the context. If
there is no temporal expression, correction of tense
and aspect would be even more challenging.
Similarly, correcting voice and mood often re-
quires real-world knowledge. Suppose one wants
to say ?I am prepared for the exam?, but writes ?I
am preparing for the exam?. Semantic analysis of
the context would be required to correct this kind of
error, which will not be tackled in this paper1.
1If the input is ?I am *prepare for the exam?, however, we
will attempt to choose between the two possibilities.
Example Usage
I take a bath and *reading books. FINITE
I can?t *skiing well , but ... BASEmd
Why did this *happened? BASEdo
But I haven?t *decide where to go. EDperf
I don?t want *have a baby. INFverb
I have to save my money for *ski. INGprep
My son was very *satisfy with ... EDpass
I am always *talk to my father. INGprog
Table 2: Sentences with verb form errors. The intended
usages, shown on the right column, are defined in Table 3.
2.2 Syntactic Errors
The second type of error is the misuse of verb forms.
Even if the intended tense, aspect, voice and mood
are correct, the verb phrase may still be constructed
erroneously. This type of error may be further sub-
divided as follows:
Subject-Verb Agreement The verb is not correctly
inflected in number and person with respect to
the subject. A common error is the confusion
between the base form and the third person sin-
gular form, e.g.,
He *have been living there since June. (2)
Auxiliary Agreement In addition to the modal aux-
iliaries, other auxiliaries must be used when
specifying the perfective or progressive aspect,
or the passive voice. Their use results in a com-
plex verb phrase, i.e., one that consists of two
or more verb constituents. Mistakes arise when
the main verb does not ?agree? with the aux-
iliary. In the sentence below, the present per-
fect progressive tense (?has been living?) is in-
tended, but the main verb ?live? is mistakenly
left in the base form:
He has been *live there since June. (3)
In general, the auxiliaries can serve as a hint to
the intended verb form, even as the auxiliaries
?has been? in the above case suggest that the
progressive aspect was intended.
Complementation A nonfinite clause can serve as
complementation to a verb or to a preposition.
In the former case, the verb form in the clause
is typically an infinitive or an -ing participle; in
the latter, it is usually an -ing participle. Here
is an example of a wrong choice of verb form
in complementation to a verb:
He wants *live there. (4)
In this sentence, ?live?, in its base form, should
be modified to its infinitive form as a comple-
mentation to the verb ?wants?.
This paper focuses on correcting the above three
error types: subject-verb agreement, auxiliary agree-
ment, and complementation. Table 3 gives a com-
plete list of verb form usages which will be covered.
175
Form Usage Description Example
Base Form as BASEmd After modals He may call. May he call?
Bare Infinitive BASEdo ?Do?-support/-periphrasis; He did not call. Did he call?
emphatic positive I did call.
Base or 3rd person FINITE Simple present or past tense He calls.
Base Form as INFverb Verb complementation He wants her to call.
to-Infinitive
-ing INGprog Progressive aspect He was calling. Was he calling?
participle INGverb Verb complementation He hated calling.
INGprep Prepositional complementation The device is designed for calling
-ed EDperf Perfect aspect He has called. Has he called?
participle EDpass Passive voice He was called. Was he called?
Table 3: Usage of various verb forms. In the examples, the italized verbs are the ?targets? for correction. In comple-
mentations, the main verbs or prepositions are bolded; in all other cases, the auxiliaries are bolded.
3 Research Issues
One strategy for correcting verb form errors is to
identify the intended syntactic relationships between
the verb in question and its neighbors. For subject-
verb agreement, the subject of the verb is obviously
crucial (e.g., ?he? in (2)); the auxiliary is relevant
for resolving auxiliary agreement (e.g., ?has been?
in (3)); determining the verb that receives the com-
plementation is necessary for detecting any comple-
mentation errors (e.g., ?wants? in (4)). Once these
items are identified, most verb form errors may be
corrected in a rather straightforward manner.
The success of this strategy, then, hinges on accu-
rate identification of these items, for example, from
parse trees. Ambiguities will need to be resolved,
leading to two research issues (?3.2 and ?3.3).
3.1 Ambiguities
The three so-called primary verbs, ?have?, ?do? and
?be?, can serve as either main or auxiliary verbs.
The verb ?be? can be utilized as a main verb, but also
as an auxiliary in the progressive aspect (INGprog in
Table 3) or the passive voice (EDpass). The three ex-
amples below illustrate these possibilities:
This is work not play. (main verb)
My father is working in the lab. (INGprog)
A solution is worked out. (EDpass)
These different roles clearly affect the forms re-
quired for the verbs (if any) that follow. Dis-
ambiguation among these roles is usually straight-
forward because of the different verb forms (e.g.,
?working? vs. ?worked?). If the verb forms are in-
correct, disambiguation is made more difficult:
This is work not play.
My father is *work in the lab.
A solution is *work out.
Similar ambiguities are introduced by the other pri-
mary verbs2. The verb ?have? can function as an
auxiliary in the perfect aspect (EDperf ) as well as
a main verb. The versatile ?do? can serve as ?do?-
support or add emphasis (BASEdo), or simply act as
a main verb.
3.2 Automatic Parsing
The ambiguities discussed above may be expected
to cause degradation in automatic parsing perfor-
mance. In other words, sentences containing verb
form errors are more likely to yield an ?incorrect?
parse tree, sometimes with significant differences.
For example, the sentence ?My father is *work in
the laboratory? is parsed (Collins, 1997) as:
(S (NP My father)
(VP is (NP work))
(PP in the laboratory))
2The abbreviations ?s (is or has) and ?d (would or had) com-
pound the ambiguities.
176
The progressive form ?working? is substituted with
its bare form, which happens to be also a noun.
The parser, not unreasonably, identifies ?work? as
a noun. Correcting the verb form error in this sen-
tence, then, necessitates considering the noun that is
apparently a copular complementation.
Anecdotal observations like this suggest that one
cannot use parser output naively3. We will show that
some of the irregularities caused by verb form errors
are consistent and can be taken into account.
One goal of this paper is to recognize irregular-
ities in parse trees caused by verb form errors, in
order to increase recall.
3.3 Overgeneralization
One potential consequence of allowing for irregu-
larities in parse tree patterns is overgeneralization.
For example, to allow for the ?parse error? in ?3.2
and to retrieve the word ?work?, every determiner-
less noun would potentially be turned into an -ing
participle. This would clearly result in many invalid
corrections. We propose using n-gram counts as a
filter to counter this kind of overgeneralization.
A second goal is to show that n-gram counts can
effectively serve as a filter, in order to increase pre-
cision.
4 Previous Research
This section discusses previous research on process-
ing verb form errors, and contrasts verb form errors
with those of the other parts-of-speech.
4.1 Verb Forms
Detection and correction of grammatical errors, in-
cluding verb forms, have been explored in various
applications. Hand-crafted error production rules
(or ?mal-rules?), augmenting a context-free gram-
mar, are designed for a writing tutor aimed at deaf
students (Michaud et al, 2000). Similar strategies
with parse trees are pursued in (Bender et al, 2004),
and error templates are utilized in (Heidorn, 2000)
for a word processor. Carefully hand-crafted rules,
when used alone, tend to yield high precision; they
3According to a study on parsing ungrammatical sen-
tences (Foster, 2007), subject-verb and determiner-noun agree-
ment errors can lower the F-score of a state-of-the-art prob-
abilistic parser by 1.4%, and context-sensitive spelling errors
(not verbs specifically), by 6%.
may, however, be less equipped to detect verb form
errors within a perfectly grammatical sentence, such
as the example given in ?3.2.
An approach combining a hand-crafted context-
free grammar and stochastic probabilities is pursued
in (Lee and Seneff, 2006), but it is designed for a
restricted domain only. A maximum entropy model,
using lexical and POS features, is trained in (Izumi
et al, 2003) to recognize a variety of errors. It
achieves 55% precision and 23% recall overall, on
evaluation data that partially overlap with those of
the present paper. Unfortunately, results on verb
form errors are not reported separately, and compar-
ison with our approach is therefore impossible.
4.2 Other Parts-of-speech
Automatic error detection has been performed on
other parts-of-speech, e.g., articles (Knight and
Chander, 1994) and prepositions (Chodorow et al,
2007). The research issues with these parts-of-
speech, however, are quite distinct. Relative to verb
forms, errors in these categories do not ?disturb? the
parse tree as much. The process of feature extraction
is thus relatively simple.
5 Data
5.1 Development Data
To investigate irregularities in parse tree patterns
(see ?3.2), we utilized the AQUAINT Corpus of En-
glish News Text. After parsing the corpus (Collins,
1997), we artificially introduced verb form errors
into these sentences, and observed the resulting ?dis-
turbances? to the parse trees.
For disambiguation with n-grams (see ?3.3), we
made use of the WEB 1T 5-GRAM corpus. Prepared
by Google Inc., it contains English n-grams, up to
5-grams, with their observed frequency counts from
a large number of web pages.
5.2 Evaluation Data
Two corpora were used for evaluation. They were
selected to represent two different genres, and two
different mother tongues.
JLE (Japanese Learners of English corpus) This
corpus is based on interviews for the Stan-
dard Speaking Test, an English-language pro-
ficiency test conducted in Japan (Izumi et al,
177
Input Hypothesized Correction
None Valid Invalid
w/ errors false neg true pos inv pos
w/o errors true neg false pos
Table 4: Possible outcomes of a hypothesized correction.
2003). For 167 of the transcribed interviews,
totalling 15,637 sentences4, grammatical errors
were annotated and their corrections provided.
By retaining the verb form errors5, but correct-
ing all other error types, we generated a test set
in which 477 sentences (3.1%) contain subject-
verb agreement errors, and 238 (1.5%) contain
auxiliary agreement and complementation er-
rors.
HKUST This corpus6 of short essays was col-
lected from students, all native Chinese speak-
ers, at the Hong Kong University of Science
and Technology. It contains a total of 2556 sen-
tences. They tend to be longer and have more
complex structures than their counterparts in
the JLE. Corrections are not provided; how-
ever, part-of-speech tags are given for the orig-
inal words, and for the intended (but unwrit-
ten) corrections. Implications on our evaluation
procedure are discussed in ?5.4.
5.3 Evaluation Metric
For each verb in the input sentence, a change in verb
form may be hypothesized. There are five possible
outcomes for this hypothesis, as enumerated in Ta-
ble 4. To penalize ?false alarms?, a strict definition
is used for false positives ? even when the hypoth-
esized correction yields a good sentence, it is still
considered a false positive so long as the original
sentence is acceptable.
It can sometimes be difficult to determine which
words should be considered verbs, as they are not
4Obtained by segmenting (Reynar and Ratnaparkhi, 1997)
the interviewee turns, and discarding sentences with only one
word. The HKUST corpus was processed likewise.
5Specifically, those tagged with the ?v fml?, ?v fin? (cov-
ering auxiliary agreement and complementation) and ?v agr?
(subject-verb agreement) types; those with semantic errors (see
?2.1), i.e. ?v tns? (tense), are excluded.
6Provided by Prof. John Milton, personal communication.
clearly demarcated in our evaluation corpora. We
will thus apply the outcomes in Table 4 at the sen-
tence level; that is, the output sentence is considered
a true positive only if the original sentence contains
errors, and only if valid corrections are offered for
all errors.
The following statistics are computed:
Accuracy The proportion of sentences which, after
being treated by the system, have correct verb
forms. That is, (true neg + true pos) divided
by the total number of sentences.
Recall Out of all sentences with verb form errors,
the percentage whose errors have been success-
fully corrected by the system. That is, true pos
divided by (true pos+ false neg + inv pos).
Detection Precision This is the first of two types
of precision to be reported, and is defined as
follows: Out of all sentences for which the
system has hypothesized corrections, the per-
centage that actually contain errors, without re-
gard to the validity of the corrections. That is,
(true pos + inv pos) divided by (true pos +
inv pos + false pos).
Correction Precision This is the more stringent
type of precision. In addition to successfully
determining that a correction is needed, the sys-
tem must offer a valid correction. Formally, it is
true pos divided by (true pos + false pos +
inv pos).
5.4 Evaluation Procedure
For the JLE corpus, all figures above will be re-
ported. The HKUST corpus, however, will not be
evaluated on subject-verb agreement, since a sizable
number of these errors are induced by other changes
in the sentence7.
Furthermore, the HKUST corpus will require
manual evaluation, since the corrections are not an-
notated. Two native speakers of English were given
the edited sentences, as well as the original input.
For each pair, they were asked to select one of four
statements: one of the two is better, or both are
equally correct, or both are equally incorrect. The
7e.g., the subject of the verb needs to be changed from sin-
gular to plural.
178
Expected Tree {?usage?,...} Tree disturbed by substitution [?crr? ? ?err?]
{INGprog,EDpass} A dog is [sleeping?sleep]. I?m [living?live] in XXX city.
VP
be VP
crr/{VBG,VBN}
VP
be NP
err/NN
VP
be ADJP
err/JJ
{INGverb,INFverb} I like [skiing?ski] very much; She likes to [go?going] around
VP
*/V SG
VP
crr/{VBG,TO} ...
VP
*/V NP
err/NN
VP
*/V PP
to/TO SG
VP
err/VBG
INGprep I lived in France for [studying?study] French language.
PP
*/IN SG
VP
crr/VBG ...
PP
*/IN NP
err/NN
Table 5: Effects of incorrect verb forms on parse trees. The left column shows trees normally expected for the indicated
usages (see Table 3). The right column shows the resulting trees when the correct verb form ?crr? is replaced by ?err?.
Detailed comments are provided in ?6.1.
correction precision is thus the proportion of pairs
where the edited sentence is deemed better. Accu-
racy and recall cannot be computed, since it was im-
possible to distinguish syntactic errors from seman-
tic ones (see ?2).
5.5 Baselines
Since the vast majority of verbs are in their cor-
rect forms, the majority baseline is to propose no
correction. Although trivial, it is a surprisingly
strong baseline, achieving more than 98% for aux-
iliary agreement and complementation in JLE, and
just shy of 97% for subject-verb agreement.
For auxiliary agreement and complementation,
the verb-only baseline is also reported. It attempts
corrections only when the word in question is actu-
ally tagged as a verb. That is, it ignores the spurious
noun- and adjectival phrases in the parse tree dis-
cussed in ?3.2, and relies only on the output of the
part-of-speech tagger.
6 Experiments
Corresponding to the issues discussed in ?3.2 and
?3.3, our experiment consists of two main steps.
6.1 Derivation of Tree Patterns
Based on (Quirk et al, 1985), we observed tree pat-
terns for a set of verb form usages, as summarized
in Table 3. Using these patterns, we introduced verb
form errors into AQUAINT, then re-parsed the cor-
pus (Collins, 1997), and compiled the changes in the
?disturbed? trees into a catalog.
179
N -gram Example
be {INGprog, The dog is sleeping.
EDpass} ? The door is open.
verb {INGverb, I need to do this.
INFverb} ? I need beef for the curry.
verb1 *ing enjoy reading and
and {INGverb, going to pachinko
INFverb} go shopping and have dinner
prep for studying French language
{INGprep} ? a class for sign language
have I have rented a video
{EDperf} * I have lunch in Ginza
Table 6: The n-grams used for filtering, with examples
of sentences which they are intended to differentiate. The
hypothesized usages (shown in the curly brackets) as well
as the original verb form, are considered. For example,
the first sentence is originally ?The dog is *sleep.? The
three trigrams ?is sleeping .?, ?is slept .? and ?is sleep .?
are compared; the first trigram has the highest count, and
the correction ?sleeping? is therefore applied.
A portion of this catalog8 is shown in Table 5.
Comments on {INGprog,EDpass} can be found in
?3.2. Two cases are shown for {INGverb,INFverb}.
In the first case, an -ing participle in verb comple-
mentation is reduced to its base form, resulting in
a noun phrase. In the second, an infinitive is con-
structed with the -ing participle rather than the base
form, causing ?to? to be misconstrued as a preposi-
tion. Finally, in INGprep, an -ing participle in prepo-
sition complementation is reduced to its base form,
and is subsumed in a noun phrase.
6.2 Disambiguation with N-grams
The tree patterns derived from the previous step
may be considered as the ?necessary? conditions for
proposing a change in verb forms. They are not ?suf-
ficient?, however, since they tend to be overly gen-
eral. Indiscriminate application of these patterns on
AQUAINT would result in false positives for 46.4%
of the sentences.
For those categories with a high rate of false posi-
tives (all except BASEmd, BASEdo and FINITE), we
utilized n-grams as filters, allowing a correction
only when its n-gram count in the WEB 1T 5-GRAM
8Due to space constraints, only those trees with significant
changes above the leaf level are shown.
Hyp. False Hypothesized False
Usage Pos. Usage Pos.
BASEmd 16.2% {INGverb,INFverb} 33.9%
BASEdo 0.9% {INGprog,EDpass} 21.0%
FINITE 12.8% INGprep 13.7%
EDperf 1.4%
Table 7: The distribution of false positives in AQUAINT.
The total number of false positives is 994, represents less
than 1% of the 100,000 sentences drawn from the corpus.
corpus is greater than that of the original. The filter-
ing step reduced false positives from 46.4% to less
than 1%. Table 6 shows the n-grams, and Table 7
provides a breakdown of false positives in AQUAINT
after n-gram filtering.
6.3 Results for Subject-Verb Agreement
In JLE, the accuracy of subject-verb agreement er-
ror correction is 98.93%. Compared to the majority
baseline of 96.95%, the improvement is statistically
significant9. Recall is 80.92%; detection precision is
83.93%, and correction precision is 81.61%.
Most mistakes are caused by misidentified sub-
jects. Some wh-questions prove to be especially dif-
ficult, perhaps due to their relative infrequency in
newswire texts, on which the parser is trained. One
example is the question ?How much extra time does
the local train *takes??. The word ?does? is not
recognized as a ?do?-support, and so the verb ?take?
was mistakenly turned into a third person form to
agree with ?train?.
6.4 Results for Auxiliary Agreement &
Complementation
Table 8 summarizes the results for auxiliary agree-
ment and complementation, and Table 2 shows some
examples of real sentences corrected by the system.
Our proposed method yields 98.94% accuracy. It
is a statistically significant improvement over the
majority baseline (98.47%), although not significant
over the verb-only baseline10 (98.85%), perhaps a
reflection of the small number of test sentences with
verb form errors. The Kappa statistic for the man-
9p < 0.005 according to McNemar?s test.
10With p = 1?10?10 and p = 0.038, respectively, according
to McNemar?s test
180
Corpus Method Accuracy Precision Precision Recall
(correction) (detection)
JLE verb-only 98.85% 71.43% 84.75% 31.51%
all 98.94% 68.00% 80.67% 42.86%
HKUST all not available 71.71% not available
Table 8: Results on the JLE and HKUST corpora for auxiliary agreement and complementation. The majority baseline
accuracy is 98.47% for JLE. The verb-only baseline accuracy is 98.85%, as indicated on the second row. ?All? denotes
the complete proposed method. See ?6.4 for detailed comments.
Usage JLE HKUST
Count (Prec.) Count (Prec.)
BASEmd 13 (92.3%) 25 (80.0%)
BASEdo 5 (100%) 0
FINITE 9 (55.6%) 0
EDperf 11 (90.9%) 3 (66.7%)
{INGprog,EDpass} 54 (58.6%) 30 (70.0%)
{INGverb,INFverb} 45 (60.0%) 16 (59.4%)
INGprep 10 (60.0%) 2 (100%)
Table 9: Correction precision of individual correction
patterns (see Table 5) on the JLE and HKUST corpus.
ual evaluation of HKUST is 0.76, corresponding
to ?substantial agreement? between the two evalu-
ators (Landis and Koch, 1977). The correction pre-
cisions for the JLE and HKUST corpora are compa-
rable.
Our analysis will focus on {INGprog,EDpass} and
{INGverb,INFverb}, two categories with relatively
numerous correction attempts and low precisions,
as shown in Table 9. For {INGprog,EDpass}, many
invalid corrections are due to wrong predictions of
voice, which involve semantic choices (see ?2.1).
For example, the sentence ?... the main duty is study
well? is edited to ?... the main duty is studied well?,
a grammatical sentence but semantically unlikely.
For {INGverb,INFverb}, a substantial portion of the
false positives are valid, but unnecessary, correc-
tions. For example, there is no need to turn ?I like
cooking? into ?I like to cook?, as the original is per-
fectly acceptable. Some kind of confidence measure
on the n-gram counts might be appropriate for re-
ducing such false alarms.
Characteristics of speech transcripts pose some
further problems. First, colloquial expressions, such
as the word ?like?, can be tricky to process. In the
question ?Can you like give me the money back?,
?like? is misconstrued to be the main verb, and
?give? is turned into an infinitive, resulting in ?Can
you like *to give me the money back?. Second, there
are quite a few incomplete sentences that lack sub-
jects for the verbs. No correction is attempted on
them.
Also left uncorrected are misused forms in non-
finite clauses that describe a noun. These are typ-
ically base forms that should be replaced with -ing
participles, as in ?The girl *wear a purple skiwear
is a student of this ski school?. Efforts to detect this
kind of error had resulted in a large number of false
alarms.
Recall is further affected by cases where a verb is
separated from its auxiliary or main verb by many
words, often with conjunctions and other verbs in
between. One example is the sentence ?I used to
climb up the orange trees and *catching insects?.
The word ?catching? should be an infinitive comple-
menting ?used?, but is placed within a noun phrase
together with ?trees? and ?insects?.
7 Conclusion
We have presented a method for correcting verb
form errors. We investigated the ways in which verb
form errors affect parse trees. When allowed for,
these unusual tree patterns can expand correction
coverage, but also tend to result in overgeneration
of hypothesized corrections. N -grams have been
shown to be an effective filter for this problem.
8 Acknowledgments
We thank Prof. John Milton for the HKUST cor-
pus, Tom Lee and Ken Schutte for their assistance
with the evaluation, and the anonymous reviewers
for their helpful feedback.
181
References
E. Bender, D. Flickinger, S. Oepen, A. Walsh, and T.
Baldwin. 2004. Arboretum: Using a Precision Gram-
mar for Grammar Checking in CALL. Proc. In-
STIL/ICALL Symposium on Computer Assisted Learn-
ing.
M. Chodorow, J. R. Tetreault, and N.-R. Han. 2007.
Detection of Grammatical Errors Involving Preposi-
tions. In Proc. ACL-SIGSEM Workshop on Preposi-
tions. Prague, Czech Republic.
M. Collins. 1997. Three Generative, Lexicalised Models
for Statistical Parsing. Proc. ACL.
J. Foster. 2007. Treebanks Gone Bad: Generating a Tree-
bank of Ungrammatical English. In Proc. IJCAI Work-
shop on Analytics for Noisy Unstructured Data. Hy-
derabad, India.
G. Heidorn. 2000. Intelligent Writing Assistance.
Handbook of Natural Language Processing. Robert
Dale, Hermann Moisi and Harold Somers (ed.). Mar-
cel Dekker, Inc.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H.
Isahara. 2003. Automatic Error Detection in the
Japanese Learner?s English Spoken Data. In Compan-
ion Volume to Proc. ACL. Sapporo, Japan.
K. Knight and I. Chander. 1994. Automated Postediting
of Documents. In Proc. AAAI. Seattle, WA.
J. R. Landis and G. G. Koch. 1977. The Measurement of
Observer Agreement for Categorical Data. Biometrics
33(1):159?174.
L. Michaud, K. McCoy and C. Pennington. 2000. An In-
telligent Tutoring System for Deaf Learners of Written
English. Proc. 4th International ACM Conference on
Assistive Technologies.
J. Lee and S. Seneff. 2006. Automatic Grammar Cor-
rection for Second-Language Learners. In Proc. Inter-
speech. Pittsburgh, PA.
J. C. Reynar and A. Ratnaparkhi. 1997. A Maximum En-
tropy Approach to Identifying Sentence Boundaries.
In Proc. 5th Conference on Applied Natural Language
Processing. Washington, D.C.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman, New York.
182
Dialogue Management in the Mercury Flight Reservation System 
Stephan ie  Senef f  and  Joseph  Po l i f ron i  
Spoken Language Systems Group 
Laboratory  for Computer  Science 
Massachusetts Inst i tute of Technology 
545 Technology Square 
Cambridge,  MA 02139 
seneff ,joe @sls. Ics. mit. edu 
Abst rac t  
This paper describes the dialogue module of the 
Mercury systemewhich has been under development 
over the past year or two. Mercury provides tele- 
phone access to an on-line flight database, and allows 
users to plan and price itineraries between major air- 
ports worldwide. The main focus of this paper is the 
dialogue control strategy, which is based on a set of 
ordered rules as a mechanism tomanage complex di- 
alogue interactions. The paper also describes the in- 
teractions between the dialogue component and the 
other servers of the system, mediated via a central 
hub. We evaluated the system on 49 dialogues from 
users booking real flights, and report on a number of 
quantitative measures of the dialogue interaction. 
1 In t roduct ion  
Dialogue modeling is a critical and challenging as- 
pect of conversational systems, particularly when 
users are permitted flexibility with regard to defin- 
ing the constraints of the task. For systems that 
adopt a strict system-initiated approach, it is fea- 
sible to define a set of states and state transitions 
depending on the usually small number of possible 
user actions at each state. However, if the user is 
permitted to say anything within the scope of the 
recognizer at any time, such a finite-state solution 
becomes unwieldy. We are interested in the devel- 
opment of mixed-initiative systems, where the sys- 
tem may make specific requests or suggestions, but 
the user is not required to be compliant. Instead 
of a finite state dialogue model, we choose to de- 
compose dialogue state into a set of state variables. 
The activities for a given turn typically involve the 
sequential execution of a number of specialized rou- 
tines, each of which performs a specific part of the 
dialogue requirements and alters the state variables 
in particular ways. To determine which of the op- 
erations hould be performed, the system consults a
dialogue control table, which is specified in a simple 
scripting language. 
This paper describes experiments with using this 
approach to dialogue modeling in the context of our 
Mercury flight reservation system. Mercury allows 
users to plan air travel between 226 cities worldwide. 
Following log-on, the user interacts with the system 
to select the flights of their trip. When the flight 
plan is completed, the system takes the initiative 
to offer to price and email the itinerary. Finally, 
the system asks the user a few questions to help 
determine user satisfaction. 
The overall system makes use of the GALAXY ar- 
chitecture \[Seneffet al(1999)\], which consists of a 
number of specialized servers that communicate with 
one another via a central programmable hub. An au- 
dio server captures the user's speech via a Dialogic 
board, and transmits the waveform to the speech 
recognizer \[Glass et al(1996)\]. The language under- 
standing component \[Seneff (1992)\] parses a word 
graph produced by the recognizer and delivers a se- 
mantic frame, encoding the meaning of the utter- 
ance, to the discourse component. The output of 
the discourse component \[Seneff (1996)\] is the frame- 
in-context, which is transformed into a flattened E- 
form (electronic form) by the generation server. This 
E-form is delivered to the turn manager, and pro- 
vides the initial settings of the dialogue state. 
The turn manager consults the dialogue control 
table to decide which operations to perform, and 
typically engages in a module-to-module subdia- 
logue to retrieve tables from the database. It pre- 
pares a response frame, which may or may not in- 
clude tabular entries. The response frame is sent 
to the generation component \[Glass (1994)\] which 
transforms it in parallel into both a text string and 
an annotated string that specifies the input controls 
for the speech synthesizer. Finally, the speech syn- 
thesizer transmits a waveform to the audio server 
which then relays the spoken response to the user 
over the telephone. The entire dialogue is recorded 
in detail in a log file for later examination. 
2 Knowledge  Representat ions  
Mercury's dialogue manager manipulates linguistic 
and world knowledge represented in the form of se- 
mantic frames. At each turn it begins with an 
E-form representing the constraints of the current 
query as a set of (key: value) pairs. This E-form pro- 
U: John Doe 
S: Please speak the month and day you registered as your password, as in, may first. 
U: april first 
S: Welcome John. You have successfully logged onto Mercury. How may I help you? 
U: i want to travel to washington dulles on november fourth 
S: From Boston to Washington Dulles on November 4. Can you provide the approximate 
departure time or airline preference? 
U: i need to arrive at around nine a m 
I: Okay. Flights from Boston to Washington Dulles arriving at 9:00 am on November 4. 
One moment please. 
S: The best matching flight is a U S Airways flight arriving at 8:59 a.m. I also have a United 
flight arriving at 8:36 a.m. 
U: i what time does the united flight leave 
S: United flight 1819 departs at 7:00 am from Boston. Shall I book this flight for you? 
U: yes please 
S: United flight 1819 has been added to your itinerary. You depart Boston at 7:00 am and arrive 
in Washington Dulles at 8:36 am. Where would you like to go next? 
U: i would like to return to boston 
S: What date will you be returning on? 
U: the same day 
I: Okay. United flights from Washington Dulles to Boston on November 4. One moment please. 
S: Can you provide a departure or arrival time? The earliest nonstop leaves at 9:30 a.m. 
and arrives at 10:57 a.m. 
U: i would like to leave after three thirty 
S: The best matching flight is a United flight departing at 4:30 p.m. I also have a United flight 
departing at 5:30 p.m. 
U: i will take the first one 
S: United flight 1278 has been added to your itinerary. You depart Washington Dulles at 4:30 
pm and arrive in Boston at 6:07 pm. Shall I price your itinerary? 
U: yes please 
I: Pricing your itinerary will take a while. In the meantime, I'll summarize your itinerary for you. 
S: 
on November 4, you depart on United flight 1819 from Boston at 7:00 am and arrive in 
Washington Dulles at 8:36 am. From there, you depart on United flight 1278 at 4:30 pm 
and arrive in Boston at 6:07 pm. 
The price for your itinerary is $249.00. Shall I email your itinerary to you? 
U: yes 
S: Your itinerary has been emailed. Is there something else I can do for you? 
U: no thank you 
S: Thank you for calling! Have a nice day! 
Table 1: Example dialogue between a user and the system involving a real trip. "U" denotes a user utterance, 
"S" a system utterance, and 'T' an intermediate system response spoken prior to database retrieval. 
vides the initial values in the dialogue state, which 
evolves over the course of the turn as rules are exe- 
cuted. 
In addition to the dialogue state, Mercury also 
makes use of several other internal representations 
which retain state. These are packaged up into 
a turn-manager history frame which is passed to 
the hub for safe-keeping at the end of each turn, 
and returned with each subsequent turn. The user 
model contains a record of the system's current be- 
lief about the state of the user's flight plan. For 
example, if the user's first query were "I want to go 
to Denver next Friday morning, returning the fol- 
lowing Wednesday," the system would record that 
this is a round trip flight and would save the return 
date (unresolved, in case there was a recognition er- 
ror on the forward leg date) in the user model. It 
would retrieve from the user model, as the forward 
leg source, the default value provided by the user at 
enrollment. Thus the user model retains both the 
initial user preferences and all of the details of the 
current flight plan as they become available. The 
system also retains internally a list of all the flights 
that matched the user's constraint specifications, as 
well as a list of the subset of the flights that it has 
spoken about (e.g., the nonstop flights or the earliest 
12 
-airline 
~cLrce te 
~ a 
desL 
date des ine i 
Figure 1." An (incomplete) state diagram for the example system described in the text. 
flight). These flights become available for reference 
in subsequent queries uch as "book the third one," 
or "where does the united flight stop?". 
Whenever the system needs to go to the database, 
it provides an intermediate r ply frame which serves 
both to ask the user for patience and to confirm a 
consistent shared model of the dialogue history. The 
system's final response is also a frame, and it often 
contains a list of flights to be read back to the user. 
In other cases, it may be a yes-no question as in 
"Shall I email your itinerary to you?" The latter 
case also necessitates sending both to the discourse 
component a system-initiative context for yes/no re- 
sponses, and to the language understanding compo- 
nent a dialogue context filter that favors confirma- 
tions and denials in the N-best selection. A similar 
process takes place when the system prompts for in- 
formation such as a date or a departure city. 
Table 1 shows a dialogue between a user and the 
system involving a real trip between Boston and 
Washington D.C. The name and password informa- 
tion have been omitted for privacy reasons. In the 
figure, "U" stands for "user", "S" for system, and 
'T' for intermediate r sponse. 
3 Dialogue Management S rategies 
A finite state network is a convenient way to formu- 
late dialogue states, as long as the system is able to 
maintain strong control over the order in which slots 
are filled, and especially when slots are filled one at 
a time. However, if the user is permitted the flexi- 
bility to fill slots in any order, and to fill more than 
one slot in a single turn, then it becomes difficult 
and inefficient o define a finite state network that 
covers the entire space of possibilities. 
A finite state solution for a simple dialogue xam- 
!source - -  > prompt_source 
!destination - -  > prompt_destination 
!airline - -  > prompt_airline 
!date - -  > prompt_date 
nprompts > 1 - -  > mark_multiple 
nprompts = 0 - -  > retrieve_flights 
Figure 2: A dialogue control table corresponding to
the state diagram in Figure 2. 
ple is given in Figure 1. Here, we assume that the 
user can fill four slots (source, destination, date, and 
airline) and that all four must be filled before the 
system can retrieve flights. The slots can be filled in 
any order, and the user can fill any number of slots 
in each turn. For notational convenience, we repre- 
sent the states filled/unfilled by 1/0, and we order 
the fields as \[source, destination, date, airline\], such 
that state "1101" says that only date is missing. At 
each state the system prompts the user for all miss- 
ing slots. Thus, state 1000 would speak, "Can you 
provide one or more of destination, date, and air- 
line?" If the user provides more than a single field 
in a given turn, the system advances by more than 
one state in the state diagram. To fully specify all 
possibilities, even for this simple example, requires 
a large number of states in the state diagram. 
Figure 2 shows a set of ordered rules which achieve 
the same goals as the state diagram but with a 
greatly simplified dialogue control specification. In 
this case, state is encoded implicitly as a bit map 
corresponding to the filled/unfilled values of the four 
variables. The notation "!" stands for "NOT," 
meaning that this variable is not yet filled. Follow- 
ing the arrow is the name of the operation to be 
13 
clause request_keypad 
week\[ day \[ tel_date 
flight_number & !airline 
clause price & topic itinerary & truth_value deny 
clause book & num_.found =1 
num_nonstops > 1 & num_nonstops < 4 & arrival_time 
keypad..input 
resolve_relative_date 
need_airline 
dont_price_itinerary 
) add..flight_to.Jtinerary 
speak_three_arrivals 
Figure 3: Selected rules from the Mercury dialogue control table. "&" stands for "AND" and "\[" stands for 
"OR". 
performed when the conditions are met. Each op- 
eration is usually a specific function, and is free to 
augment the variable space by either altering the val- 
ues of pre-existing variables or introducing new vari- 
ables. In our simple example the four prompt opera- 
tions simply add their respective variables to the re- 
sponse frame and increment the variable "nprompt." 
The final function "mark_multiple" fires only if more 
than one of the preceding functions have fired. Its 
action is to insert "one or more of" into the response. 
The final "retrieve" operation only fires if none of the 
preceding operations fired. It is significant that state 
is dispersed not only over the variables but also over 
the operations. Thus, any single prompt state of 
the state transition matrix would involve up to two 
operations in the dialogue control table, and, con- 
versely, the "mark_multiple" operation is associated 
with several states. 
In addition to possibly altering the values of vari- 
ables in the dialogue state, operations also have the 
responsibility to return, upon completion, one of 
three "move" states. The majority of the operations 
return the state ~'CONTINUE," which means imply 
to continue xecution just beyond the rule that just 
fired. The two other possibilities are "STOP," i.e., 
exit from the dialogue control table, and "RESTART" 
which means to return to the top of the set of 
dialogue rules and start over. An (optional) dis- 
tinguished operation, which handles activities that 
should occur routinely at every turn, is executed 
upon exiting the dialogue control process. The tests 
on variables can be binary, arithmetic, or string 
matching on values. 
Our Mercury system makes use of this "ordered 
rules" strategy for dialogue control. However, the 
rules are dealing with a great deal more complexity 
than simply deciding what prompts to issue when 
certain slots are unfilled. There are currently atotal 
of over 200 rules in Mercury's dialogue control table. 
These rules can be grouped into several distinct cat- 
egories. Only nine of the rules involve prompts for 
missing information. Another nine are involved with 
logging into the system, i.e., acquiring the name and 
the password, which may have to be entered using 
the telephone keypad, either upon user request or 
as a consequence of recognition failure. Eleven rules 
are concerned with meta-level interactions such as 
apologies for missing services (no flight status in- 
formation available), and requests for help or rep- 
etition. Several rules have to do with determining 
whether the current request plausibly involves a ref- 
erence to a flight in a pre-existing flight list. This 
could be an obvious reference to the nth flight, or it 
could be an inquiry about the "united flight" or the 
"nine a.m. flight." Several more rules involve inter- 
preting various references to relative dates and/or 
times such as "the following Thursday," or "that 
same afternoon." 
The largest single category concerns preparing the 
reply frame, after the database retrieval has already 
taken place. Twenty six rules are concerned with 
this task, and they are keyed on a combination ofthe 
number and type of flights retrieved and the specific 
request of the user (e.g., "where do they connect?"). 
The purpose of these functions is to reduce the num- 
ber of database tuples returned for a given query to 
a manageable s t for a spoken response. This re- 
duction is based on a hierarchy of quality measures 
for each flight, beginning with any stated or inferred 
preference (e.g., a particular airport in the case of 
cities with multiple airports, or a particular airline 
in the case of a multi-leg booking where one leg has 
already been established) and including number of 
stops and length of flight. These functions also con- 
solidate information about the flights to be spoken, 
combining shared attributes. Our ultimate goal is to 
produce a response such as "I have found three non- 
stop United flights. Flight 100 leaves at 8:45, flight 
200 leaves at 9:10, and flight 300 leaves at 9:30." 
The dialogue control table facilitates this interac- 
tion by enabling the system developer to encode the 
constraints of the hierarchy in the rules. 
Finally, there are a miscellaneous set of rules that 
have to do with updating the user model, prepar- 
ing the intermediate r ply, pricing or emailing the 
itinerary, preparing the database query, filtering 
flights, or updating the itinerary. 
Since the operations are general functions, it is up 
to the system developer to decide how to parcel up 
the computational requirements into the individual 
operations. With experience, one acquires a set of 
guidelines to help formalize this process. As a gen- 
eral rule, it is preferrable to limit the use of nested 
function calls. Instead, an operation can set a vari- 
14 
11.5%WER \] words/turn I total turns I total t i m e 7  11 229 sec. 
Nutts: 41 90 55 31 9 226 
Figure 4: Some easily computed statistics on the 36 
successful bookings. 
able to indicate that another operation eeds to be 
called, and then the intended subroutine gets pro- 
moted to the status of a dialogue control operation. 
This has the effect of exposing more of the internal 
activities of the system to the dialogue control table, 
which serves as a very useful outline of system con- 
trol flow. Another general policy is that each oper- 
ation should be concerned with a single well-defined 
task, again in or~er~not to conceal complexity. 
Figure 3 shows several examples of actual rules in 
Mercury's dialogue control table. 
4 Data  Co l lec t ion  and  Eva luat ion  
Mercury first became available for data collection 
in October '99. Prospective users must first enroll 
by filling in a simple form on a Web page, where 
they enter, minimally, their name, email address, 
and password (a date). Once the user's name has 
been added to the recognizer and language under- 
standing components, they receive an email mes- 
sage informing them of the telephone number. Users 
are encouraged to attempt to book real trips. From 
late October to early December, we collected 49 dia- 
logues involving real flight bookings, and these form 
the basis for our evaluation studies. 
Overall, 73% of the bookings were successful 
(36/49). We used a very strict requirement for suc- 
cess. For example, in one dialogue considered un- 
successful the system did not know the airline that 
the user requested, and so the user compromised and 
booked the trip on a different airline. Three of the 
failures are due to the user simply hanging up in 
frustration, and three others are due to the system 
hanging up due to a misrecognized "good-bye." Two 
failures were due to user inattentiveness. The user 
believed that the trip was correctly booked, but a 
misrecognition produced a different itinerary than 
the one they were specifying. Finally, four of the 
failures involved completely correct bookings, but 
the system was unable to follow through with the 
pricing and/or emailing of the itinerary. Some of 
these involved inadequacies in the dialogue module, 
once the user did not provide the expected response 
to a system request. There was a striking difference 
in recognition error between the successful and the 
incomplete bookings (11.5% vs 26% WER). A heavy 
foreign accent accounted for some of the recognition 
problems. 
Some easily measurable statistics for the successes 
are given in Figure 4. These numbers were computed 
Figure 5: Distribution of evaluable user utterances 
in terms of number of new attributes introduced 
with each dialogue turn. IBR = Information Bit 
Rate. 
on the "core dialogue," defined as the interval subse- 
quent o logging on and up until the itinerary is fully 
specified, but has not yet been priced. On average 
users required less than four minutes to complete 
the core dialogue, although three outliers took more 
than seven minutes. 
4.1 Log File Evaluation 
We have long been interested in seeking evaluation 
metrics that are automatic and that can apply on a 
per-utterance basis but evaluate a significant portion 
of the system beyond the recognizer. In \[Polifroni et 
al. (1998)\] we proposed an E-form evaluation met- 
ric, which compares an E-form obtained by parsing 
the original orthography against hat obtained by 
parsing the selected recognizer hypothesis. We be- 
lieve this is a good metric for evaluating how well the 
recognizer and parser are doing, but it says nothing 
about the discourse and dialogue components. 
We recently devised two new evaluation metrics, 
which we believe are useful measures for assessing 
the performance ofthe recognizer, parser, discourse, 
and dialogue components, collectively. To compute 
the measures, we must reprocess the log file after the 
orthographic transcription has been provided for the 
user queries. Basically, both the recognizer hypothe- 
sis and the original orthography are run through the 
system utterance by utterance, with the discourse 
and dialogue states being maintained exclusively by 
the recognizer branch. For both branches, the E- 
form that is produced after the turn manager has fin- 
ished processing the query is sent to a special evalu- 
ation server. This server maintains a running record 
of all the attributes that appear in the orthography 
path, comparing them against heir counterparts in 
the recognizer path. 
The two parameters that emerge from comparing 
these E-forms we refer to as information bit rate 
(IBR) and user frustration (UF). IBR measures the 
average number of new attributes introduced per 
user query. A subsequent query that reiterates the 
same attribute is excluded since it did not intro- 
duce any new information. Thus if the user said, 
"I want to go from Seattle to Chicago on Decem- 
ber 27," and the system misrecognized the date as 
"December 22," then a subsequent query, "I said 
December 27" would be registered as contributing a 
0 count to the IBR parameter. The UF. parameter 
tabulates how many turns it took, on average, for an 
15 
intended attribute to be transmitted successfully to 
the system. Thus, in the example above, the source 
and destination each took one turn, but the date 
took two. 
There are some difficulties with rerunning the di- 
alogue at a later time. Both the system and the 
database are in a state of flux, and so the dialogue 
can become incoherent. For example, in one case the 
user said, "Book it," in response to a single flight 
being proposed, but due to changes in the flight 
schedule, the system proposed three flights in the 
rerun and the dialogue became incoherent from that 
point on. To help alleviate incoherence, we provide 
a mechanism to artificially offset the date, at least 
to assure that the dates they have selected haven't 
already pasaed. !_n spite of the above problems, we 
feel that these'evaluation metrics how considerable 
promise. 
In a pilot study, we processed a subset of our data 
through this evaluation configuration. We identi- 
fied a set of 17 attributes that could be monitored. 
Five percent of the utterances had orthographies 
that failed to parse. These are unevaluable with- 
out human reannotation, and are hence eliminated 
from the pool in the discussion below, although they 
clearly are likely to be very problematic. Figure 5 
summarizes the results for information bit rate for 
the remainder of the utterances. A surprisingly arge 
percentage of the utterances introduce no new con- 
cepts. Some, but not all, of these are similar to 
the date misrecognition example given above. Oth- 
ers are cases where the user was confused about 
the state of the system's knowledge, and decided to 
simply repeat all the preceding constraints just to 
make sure. Some are also misfirings of the endpoint 
detector producing content-free utterances such as 
"okay." In other cases the user intended an action, 
but the system's understanding mechanism was not 
sophisticated enough. For example "That's good" 
meaning "book it." We were pleased with the per- 
centage of sentences that contained more than one 
attribute. We believe that a typical directed di- 
alogue would have far fewer utterances with more 
than one attribute. 
Excluding the 5% of utterances whose orthogra- 
phy failed to parse, our system achieved a 1.05% user 
frustration rate. This means that, on average, one 
out of every 20 attributes had to be entered twice. 
We were very pleased with this number. 
5 Summary and Future Work 
This paper described our strategy for dialogue man- 
agement in the Mercury system. Overall, we have 
found it to be extremely powerful. While the Mer- 
cury system is still under active development, we feel 
that the anticipated extensions of capability will re- 
quire a straightforward process of expansions in the 
dialogue control table; i.e., the"system has not at- 
tained an unmanageable d gree of complexity in or- 
ganization. We believe that this is a direct conse- 
quence of the use of the dialogue control table. 
Mercury's turn manager deals with some issues, 
such as the interpretation f dates and times, that 
are of relevance to many other potential domains. 
We envision that at some future time we will have 
available a large library of operations of general util- 
ity that can be inserted into a new system to greatly 
accelerate he time required to bring the new domain 
up to full operation. 
References 
Seneff, S., R. Lan, and J. Polifroni. (1999) "Or- 
ganization, Communication, and Control in the 
GALAXY-II Conversational System," Proc. Eu- 
rospeeeh '99, Budapest, Hungary, pp. 1271-1274. 
Glass, J., J. Chang~ and M. McCandless. (1996) "A 
Probabilistic Framework for Feature-based Speech 
Recognition," Proc. ICSLP 'Y6, Philadelphia, PA, 
pp. 2277-2280. 
Seneff, S. (1992) "TINA: a Natural Language Sys- 
tem for Spoken Language Applications," Compu- 
tational Linguistics, 18/1, pp. 61-86. 
Seneff, S, D. Goddeau, C. Pao, and J. Polifroni. 
(1996) "Multimodal Discourse Modelling-in a 
Multi-user Multi-domain Environment," Proceed- 
ings, International Conference on Spoken Lan- 
guage Processing '96, pp 192-195, Oct. 3-6. 
Glass, J., J. Polifroni, & S. Seneff. (1994). "Multi- 
lingual Language Generation across Multiple Do- 
mains." Proc. International Conference on Spoken 
Language Processing (pp. 983-986). Yokohama. 
Polifroni, J., S. Seneff, J. Glass, and T.J. Hazen. 
(1998) "Evaluation Methodology for a Telephone- 
based Conversational System." Proc. LREC '98, 
pp. 43-50, Granada, Spain. 
16 
Flexible and Personalizable Mixed-Initiative Dialogue Systems
James Glass and Stephanie Seneff
Spoken Language Systems Group
Laboratory for Computer Science, MIT
Cambridge, MA, USA
{jrg,seneff}@sls.lcs.mit.edu
Abstract
This paper describes our vision for a future
time when end users of mixed-initiative spoken
dialogue systems will be able to dynamically
configure the system to suit their personalized
goals. We argue that spoken dialogue systems
will only become a common utility in society
once they can be reconfigured, essentially in-
stantaneously, to support a new working vocab-
ulary within a new domain or subdomain. For
example, if a user is interested in restaurants in
Seattle, the system would go off-line to gather
information from resources such as the Web,
and would infer from that knowledge an ap-
propriate working vocabulary, language mod-
els, and dialogue control mechanism for a sub-
sequent spoken conversation on this topic. In
addition to painting this vision, the paper also
discusses our recent research efforts directed
towards the technology development necessary
to realize this larger goal.
1 Introduction
Spoken dialogue systems are emerging as an effective
means for humans to access information spaces through
natural spoken interaction with computers. These sys-
tems are usually implemented with a static knowledge
space, or one that is only augmented through manual in-
tervention from the system developers. A significant en-
hancement to the usability of such systems would be the
ability to automatically acquire new knowledge through
interaction with its end users and its available knowledge
resources. We believe, in fact, that the main barrier to
wide acceptance of spoken dialogue systems is their cur-
rent lack of flexibility and personalization.
Over the past decade, researchers in the Spoken Lan-
guage Systems Group at MIT have been developing hu-
man language technologies for mixed initiative conversa-
tional systems, which are distinguished from the emerg-
ing deployed commercial systems in that the interaction
is natural and flexible, modelled after the style of human-
human dialogue (Zue and Glass, 2000). The development
of the Galaxy Communicator architecture (Seneff et al,
1998) has greatly accelerated the pace at which we as ex-
perts can configure complex dialogue systems in a wide
range of different domains. As the underlying technol-
ogy components have matured, our research focus has
evolved to include issues related to portability, modular-
ity, and dynamic configurability of system components.
We believe that the ability for naive system developers,
and even end users, to reconfigure existing systems to
manage their personal needs, will be crucial for the suc-
cessful use of these technologies.
We see several different ways in which such flexible
reconfiguration will become feasible in the near future.
Perhaps most critical is the initial preparation of a new
domain, where available on-line databases will be the cat-
alyst for defining the vocabulary and language models of
the domain, as well as the nature of the dialogue inter-
action needed to guide the user through the information
space (Polifroni et al, 2003). However, the ability to dy-
namically reconfigure based on new information will also
be extremely valuable. For instance, a hotel domain for
the entire U.S. might initially restrict name recognition
to the major chains such as Sheraton and Hyatt, since it
would not be feasible to support all hotel names in the
U.S. Once the user defines a specific geographic region,
the system can enhance the level of detail of its work-
ing vocabulary, but particular to the user?s narrower re-
quest, now supporting explicit mention of perhaps all ho-
tels by name within that designated region. Users would
be able to select hotels based on a number of attributes as
guided by the hotel database, and to specify constraints in
general world knowledge areas such as dates and prices.
Users should be able to further personalize the system
User: I?m interested in a restaurant in Boston?s
North End
Sys: I know of 53 restaurants in the North End.
Of these, 46 are Italian restaurants. 28 of them
are located on either Hanover or Salem Street.
User: Tell me about the ones on Hanover Street.
Sys: There is one seafood restaurant on Hanover
Street. The others are Italian.
User: I?m interested in the Seafood restaurant.
Sys: The Daily Catch is located on 323 Hanover
Street. The phone number is 617-523-8567.
The price range is between $12 and $18.
Figure 1: Illustration of a possible dialogue between a
user and a system in a restaurant domain.
by adding new words instantaneously to the working vo-
cabulary via spoken dialogue. This might also include
specifying the word?s semantic class: ?I want to add the
name John Doe to my rolodex.? When feasible, a user-
specified named entity, such as a restaurant, would be ver-
ified against Web sources to improve the system?s ability
to understand their request.
In order for this vision to become a reality, a num-
ber of specific technology goals must be met. First and
foremost, it is essential to develop tools that will enable
rapid configuration of dialogue systems in new domains
of knowledge, guided mainly from domain-dependent in-
formation sources. Our efforts in generic dialogue devel-
opment represent a strong initiative toward that goal (Po-
lifroni and Chung, 2002). Secondly, we need to be able
to support incremental update of vocabularies and lan-
guage models for speech recognition and understanding,
in essentially instantaneous time (Schalkwyk et al, 2003;
Seneff et al, 1998; Chung et al, 2003). This would allow
great flexibility within a single dialogue where the user
might ask about a named entity that is not yet known to
the system. Third, while we can make use of a large lex-
ical resource for pronunciation modeling, we must have
available as well a high-performance letter-to-sound ca-
pability, integrating multiple knowledge sources such as a
Web page, a spoken name, a spoken spelling of the name,
and/or a key-padded name (Chung and Seneff, 2002).
Fourth, we need to have intelligent knowledge acquisi-
tion systems, capable of populating a database from Web
sources, and extracting and organizing key elements from
the database (Polifroni et al, 2003).
These ideas can best be illustrated through a couple
of example scenarios. In Figure 1, the user begins with
a request for a restaurant in a neighborhood of Boston.
The system then rapidly configures itself to support the
appropriate sub-language, and is able to summarize lists
of restaurants meeting the constraints of the user?s subse-
quent queries, eventually leading to a unique selection.
For the scenario in Figure 2, the user has asked about
User: Can you tell me the phone number of the
Thaiku restaurant in Seattle?
Sys: I may not know the name of the restaurant.
Can you spell it for me?
User: t h a i k u
Sys: The phone number of Thaiku is 206-706-7807.
Figure 2: A sub-dialogue to enroll a new restaurant name.
the phone number for a restaurant they already know
about. The system parses the name within a complete
parse, but with a generic ?unknown word? as a stand-in
for the restaurant name. It can at this point go to the
Web and download a set of candidate restaurant names
for Seattle, to form additional constraints on a solicited
spelling. The integration of the spelling, the spoken pro-
nunciation, and the Web listing, we argue, potentially
provide enough constraint to solve the specific problem
with high accuracy. The system can now retrieve the re-
quested information from the Web.
2 Underlying Technologies
Over the past several years, we have been making ad-
vances on several fronts, directed toward the larger goal
of the vision outlined above. In this section, we will high-
light some of these, with pointers to the literature for an
in-depth description.
SpeechBuilder: Over the past few years, we have been
developing a set of utilities that would enable research
results to be migrated directly into application develop-
ment (Glass and Weinstein, 2001). Our goal is to enable
natural, mixed-initiative interfaces similar to those now
created manually by a relatively small group of expert de-
velopers. We make no distinction between the technology
components of SpeechBuilder and those of our most so-
phisticated dialogue systems, such as the Mercury flight
reservation domain (Seneff and Polifroni, 2000). Speech-
Builder employs a Web-based interface where developers
type in the specifics of their domain, guided by forms and
pull-down menus. Components such as recognition vo-
cabulary, parse rules, and semantic mappings are created
automatically from example sentences entered by the de-
veloper. In several recent short courses, naive developers
have been able to implement a new domain and converse
with it on the telephone in a matter of hours.
Language Modelling: Patchwork Grammars A seri-
ous limitation in today?s technology to immediate deploy-
ment of a new system is the chicken-and-egg problem of
the language model. System performance is critically tied
to the quality of the statistical language model, which typ-
ically depends on large domain-dependent corpora that
don?t exist until the domain is actually deployed and
widely used. We have initiated an effort to automatically
induce a grammar for a new domain from related content
of existing speech corpora for other domains combined
with knowledge derived from the content provider for the
new domain. For instance, our hotel domain can leverage
from an existing auto classified domain to extract patterns
for referring to prices, can induce a grammar for dates
from a flight domain, and can make use of statistics of
hotel counts to determine city probabilities. Parse rules
for general sub-domains such as dates, times, and prices
are organized into sub-grammars that are easily embed-
ded into any application, along with libraries for convert-
ing the resulting meaning representations into a canonical
format, such as ?27SEP2003.?
Flexible Vocabulary: We have recently realized our goal
of enabling users to automatically add a new word to an
existing system through natural interaction with the sys-
tem itself (Schalkwyk et al, 2003; Seneff et al, 1998;
Chung et al, 2003; Chung and Seneff, 2002; Seneff et
al., 2003). We have thus far applied this only to the en-
rollment of the user?s name as part of a personalization
phase (Seneff et al, 1998; Chung et al, 2003), through a
?speak and spell? mode. After confirmation, the system
reconfigures itself to fully support the word such that it
can now be understood in subsequent dialogue. A high
quality sound-to-letter framework (Chung et al, 2003)
and a new ability to automatically derive a class n-gram
from an NL grammar have facilitated this process (Sen-
eff et al, 2003). The recognizer update is currently im-
plemented via full recompilation, which can take up to a
minute of elapsed time, but efforts to support incremental
recognizer updates (Schalkwyk et al, 2003) hold promise
for essentially instantaneous new word addition.
Managing the Dialogue: One of the most time con-
suming aspect of dialogue system development today
is the implementation of the dialogue manager. To re-
duce this development phase, we have been creating a
set of domain-independent functions that can be special-
ized to a particular domain through passed parameters.
These functions perform such tasks as checking a query
for completeness, filtering the database results on user-
specified constraints, or making decisions on fuzzy at-
tributes such as ?near? (Polifroni and Chung, 2002).
One common but important subgoal in dialogue plan-
ning is to generate a succinct description of a set of re-
trieved entries. Our recent research in this area has fo-
cused on organizing database retrievals into a summary
meaning representation, by automatically clustering sets
into natural groupings. In parallel, we are developing
generation tools that will translate these summaries into
fluent English. For instance, in the hotel domain, the re-
sult set is automatically partitioned into ?cheap? or ?ex-
pensive? differently depending upon the city. By basing
such subjective categories on a content provider, we al-
leviate the burden of the system developer, while at the
same time producing a more intelligent system.
3 Summary and Conclusions
While there is inadequate space here to properly cover
such a large topic as flexible and rapidly reconfigurable
mixed-initiative dialogue systems, we hope that we have
managed to convey our long-term research goals ade-
quately and to provide the excitement that we ourselves
feel in our current efforts to turn this vision into a reality.
In fact, important subgoals that we have had for many
years, such as incremental vocabulary update, grammar
development and training through recycled resources,
and tools to enable rapid development of effective dia-
logue interaction, are now finally bearing fruit. We be-
lieve that this is a critical moment in the life of dia-
logue system research, and we anticipate exciting break-
throughs in the near future, leading to systems that are
not only useful but also easy to use and accommodating,
such that users will prefer them over alternative means of
acquiring their information needs.
References
G. Chung and S. Seneff, ?Integrating speech with keypad in-
put for automatic entry of spelling and pronunciation of new
words,? Proc. ICSLP, 2061?2064, Denver, CO, 2002.
G. Chung, S. Seneff, and C. Wang, ?Automatic acquisition of
names using speak and spell mode in spoken dialogue sys-
tems,? Proc. HLT-NAACL ?03, Edmonton, Canada, 2003.
J. Glass and E. Weinstein, ?SPEECHBUILDER: Facilitating
spoken dialogue system development,? Proc. Eurospeech,
1335?1338, Aalborg, Denmark, 2001.
J. Polifroni and G. Chung, ?Promoting portability in dialogue
management,? Proc. ICSLP, 2721?2724, Denver, CO, 2002.
J. Polifroni, G. Chung, and S. Seneff, ?Towards automatic gen-
eration of mixed-initiative dialogue systems from web con-
tent,? submitted to EUROSPEECH, 2003.
J. Schalkwyk, L. Hetherington, and E. Story, ?Speech recogni-
tion with dynamic grammars,? submitted to EUROSPEECH,
2003.
S. Seneff, G. Chung and C. Wang, ?Empowering end users
to personalize dialogue systems through spoken interaction,?
submitted to EUROSPEECH, 2003.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and V. Zue,
?Galaxy-II: A reference architecture for conversational sys-
tem development,? Proc. ICSLP, 931?934, Sydney, Aus-
tralia, 1998.
S. Seneff and J. Polifroni, ?Dialogue management in the MER-
CURY flight reservation system,? Proc. ANLP-NAACL Satel-
lite Workshop, 1?6, Seattle, WA, 2000.
S. Seneff, C. Wang and T. J. Hazen, ?Automatic induction of
N -gram language models from a natural language grammar,?
submitted to EUROSPEECH, 2003.
V. Zue and J. Glass, ?Conversational interfaces: Advances and
challenges,? Proc. IEEE, 88(8), 1166?1180, 2000.
Error Detection and Recovery in Spoken Dialogue Systems
Edward Filisko and Stephanie Seneff ?
Spoken Language Systems Group
MIT Computer Science and Artificial Intelligence Laboratory
200 Technology Square, Cambridge, MA 02139
{filisko,seneff}@csail.mit.edu
Abstract
This paper describes our research on both the
detection and subsequent resolution of recog-
nition errors in spoken dialogue systems. The
paper consists of two major components. The
first half concerns the design of the error detec-
tion mechanism for resolving city names in our
MERCURY flight reservation system, and an in-
vestigation of the behavioral patterns of users
in subsequent subdialogues involving keypad
entry for disambiguation. An important obser-
vation is that, upon a request for keypad en-
try, users are frequently unresponsive to the ex-
tent of waiting for a time-out or hanging up the
phone. The second half concerns a pilot ex-
periment investigating the feasibility of replac-
ing the solicitation of a keypad entry with that
of a ?speak-and-spell? entry. A novelty of our
work is the introduction of a speech synthesizer
to simulate the user, which facilitates develop-
ment and evaluation of our proposed strategy.
We have found that the speak-and-spell strat-
egy is quite effective in simulation mode, but it
remains to be tested in real user dialogues.
1 Introduction
Spoken dialogue systems are emerging as an intuitive in-
terface for providing conversational access to online in-
formation sources (Eckert et al, 1997; Gorin et al, 1997;
Dahlback et al, 1999; Zue et al, 2000; Walker et al,
2001; Glass and Seneff, 2003; Pieraccini et al, 1997;
Quast et al, 2003; J. Gustafson, 1999; Polifroni and
Chung, 2002; Denecke, 2002; Seneff, 2002; Zue and
Glass, 2000). While the effectiveness of such systems
? This research was supported by an industrial consortium
supporting the MIT Oxygen Alliance.
has improved significantly over the past several years, a
critical barrier to widespread deployment remains in the
form of communication breakdown at strategic points in
the dialogue, often when the user is trying to convey a
critical piece of information that the system repeatedly
misunderstands.
This paper focuses on the specific two-stage problem
of error detection and subsequent recovery, in a situa-
tion where the user is attempting to provide a named
entity which the system fails to understand. It is not
a straightforward process for the system even to notice
that it has made a mistake. Tedious confirmation subdia-
logues for every attribute provided would lead to annoy-
ance and widespread unwillingness to use the system at
all. Hence, the system should only invoke a confirmation
subdialogue when it perceives there to be a communica-
tion breakdown.
The second aspect of the problem, error recovery,
is also challenging. The system may persist in mis-
understanding repeated spoken renditions of the same
named entity, unless a substantially different tactic can
be adopted to assure higher communicative success.
The remainder of the paper is organized as follows.
Section 2 motivates why we think this is an interesting
and important problem. In Sections 3 and 4, we describe
the error detection and recovery strategies that have been
adopted in our MERCURY flight reservation system (Sen-
eff, 2002; Seneff and Polifroni, 2000), and we provide an
analysis of the degree to which error recovery was suc-
cessful, specifically for the case of entering a source or
destination city name. The approach used was to solicit
a keypad entry of the city in cases where the system de-
tected a communication breakdown. We have analyzed
a set of 172 cases where keypad entry of a city was so-
licited. One of the observations was that users were often
not very receptive to the idea of switching into keypad
mode to map the spelling of the city to a numeric code.
Whether this is the result of cognitive overload, confu-
sion, or some other reason is not clear, however, since we
were unable to interview users to identify why they chose
not to use the keypad.
Motivated by the apparent need for a more intuitive
error recovery strategy, we describe in Sections 5 and
6 a set of experiments that explore an alternative ap-
proach whereby the user is instead asked to speak and
spell the problematic city name. We have recently devel-
oped the capability to utilize a pronounced version of a
word to greatly enhance the accuracy of a letter recogni-
tion task, and have successfully integrated this technol-
ogy into a personal name enrollment task (Seneff et al,
2003; Chung et al, 2003). Our interest here was in evalu-
ating whether a similar technique would be useful for the
error recovery problem.
It is difficult, however, to develop and perfect an algo-
rithm involving multiple recognition passes, that is only
triggered sporadically in user conversations. Hence, we
discuss a novel approach to system development based on
simulating the completion of user dialogues beginning at
the point where the system had detected a communica-
tion breakdown. In other words, we utilize a speech syn-
thesizer to produce a speak-and-spell waveform that is
solicited in lieu of the keypad entry in the MERCURY di-
alogues we have analyzed. DECTalk1 acts as a user con-
tinuing the conversation from the point where the orig-
inal MERCURY system detected communication break-
down. The synthetic speech is processed through the
speak-and-spell recognition system. An analysis of the
rate of success is then an indicator of how promising the
method might be for real user dialogues. At the same
time, we have expanded the set of cities from the origi-
nal 500 known to the MERCURY system to a much larger
set of nearly 17,000 city names within the United States.
If successful, this new speak-and-spell mode would thus
greatly expand the number of cities that the system could
theoretically recognize.
2 Background and Motivation
Spoken conversational systems have great potential for
providing spoken language access to information sources.
However, such systems are only useful if they can under-
stand most of the content words in each utterance. Many
a user has been aggravated by conversational systems that
hypothesize the same incorrect words over and over, of-
ten due to ignorance of critical words the user is speak-
ing. The dialogue manager component is often of lit-
tle or no help, suggesting, ?Please try using shorter sen-
tences,? or ?I did not understand. Please rephrase your
query.? The system must be able to recognize compli-
cations such as misrecognitions, repetitive mistakes, and
out-of-vocabulary (OOV) words, and to react appropri-
1http://research.compaq.com/wrl/decarchives/dtj/dtjk01/
ately. A more successful interaction is achievable if the
dialogue manager is able to work with the user to resolve
an error in the course of the dialogue.
A common strategy used in human-human dialogue for
handling an OOV word is for one participant to ask the
other to repeat or spell the unknown word. This provides
the inquisitor with more detailed information about the
word, which can facilitate his understanding of the word.
Several dialogue systems have employed such a strategy
in dealing with a confusing or unknown word (Bauer and
Junkawitsch, 1999; Schramm et al, 2000). We aim to
employ such a speak-and-spell strategy in our system.
In this work, we focus on the class of place names,
specifically cities, states, and airports. Such names are
prevalent and problematic in any domain where geogra-
phy plays a dominant role. For example, a weather infor-
mation system or a flight reservation system must have
the city, state, or country names exactly correct in order
to be useful. In real user interactions, it is inevitable that
some city will be mentioned that is unknown to the sys-
tem. Such a name will often be misrecognized as a known
city. The dialogue manager must determine whether a
poorly scoring hypothesized city name is, in fact, a mis-
recognized but known city, or an entirely new word. Such
uncertainty must be resolved in a manner that is time-
efficient and does not overburden the user.
For example, in the case of a weather information sys-
tem, very common geographic names would most likely
be known to the recognizer (e.g., New York, Moscow).
If a user wants to know the weather in his hometown of
Menominee, for example, which is unknown to the rec-
ognizer, the system is faced with a problem. If the recog-
nizer is not equipped with an unknown word detector, the
closest word in the recognizer?s vocabulary will be cho-
sen as the best hypothesis. The user will then have to pro-
ceed through potentially many clarification turns in which
the system repeatedly hypothesizes incorrect city names.
Since ?Menominee? is OOV, the system will never find
the correct city name.
If, however, a large external database is available (e.g.,
US Census data), that the system could consult given a
hypothesized spelling of the unknown city, there is hope
that the system will find the user?s intended city name.
Such an approach of access to a much larger vocabulary
in spell mode has been successfully applied, for example,
in (Schramm et al, 2000). In the most extreme case, the
large external database would be the World Wide Web.
One could imagine the system posing a search query on
the Web to determine that the closest major city is Green
Bay, Wisconsin, located 60 miles south of Menominee,
so it may suffice to provide the weather for Green Bay.
It should be mentioned that the flexibility of allowing
the user to spell a word creates a new task for the system.
Instead of simply looking up a hypothesized word in a
User: I would like to fly to austin tomorrow morning [mis-recognized as ?Boston?]
System: Sorry. I?m not sure what you said. There are no flights from Boston to Boston.
User: austin texas
System: Okay, flights from Austin to Boston departing between 5:00 am and noon on Monday March tenth.
One moment please. If I have misunderstood you, you should say, Scratch that.
There are no nonstop flights. I have flights on American, Continental, and Delta. There are at least
6 flights departing between 8:00 a.m. and 9:00 a.m. Can you be more specific?
User: I would like to fly from boston massachusetts to austin texas
System: Please enter the departure city name using the telephone keypad ending with the pound key.
User: 267866# [user enters ?Boston? via keypad]
System: Okay from Boston to Austin. What date will you be traveling?
USER: I would like to leave tomorrow morning
System: Okay, flights from Boston to Austin departing between 5:00 am and noon on Monday March tenth.
One moment please. . . .
Figure 1: Example of a dialogue involving a successful entry of a departure city using the telephone keypad, initiated by the system
based on perceived confusion.
SYSTEM: Welcome Angie. How may I help you?
USER: I would like to <uh> [mis-recognized as: I would like to Atlanta]
SYSTEM: Okay from Hartford to Atlanta. What date will you be traveling?
USER: October thirty first.
SYSTEM: Okay, from Hartford to Atlanta on Wednesday October 31. Can you provide the approximate
departure time or airline?
USER: Late morning.
SYSTEM: Okay, flights from Hartford to Atlanta departing between 10:00 a.m. and noon on Wednesday October 31.
One moment please... I have a nonstop Delta flight leaving at 11:20 a.m. and arriving at 1:45 pm.
Would that work?
USER: No.
SYSTEM: There are no nonstop flights. I have a U S Airways flight connecting in Charlotte leaving at 10:45 a.m.,
a United flight connecting in Washington Dulles leaving at 11:20 a.m., and a U S Airways flight leaving
at noon. Please select one of these flights or change any constraint you have already specified.
USER: I would like to change the arrival city.
SYSTEM: Please enter the destination city using the telephone keypad ending with the pound key.
USER: 6752636# [user enters ?Orlando? via keypad]
SYSTEM: I heard you say you want to arrive in Orlando. Is that correct? If you need help, you can say, for example,
what cities do you know in Oregon.
USER: That is correct.
Figure 2: Example of a dialogue involving a successful entry of an arrival city using the telephone keypad, provoked by a specific
request by the user to change the arrival city.
lexicon, the system must now remain open to the possibil-
ity that the hypothesized word (i.e., sequence of letters)
is misspelled. In other words, a spelling checker must
be utilized. Depending on the robustness of the spelling
checker, the system may or may not be able to identify the
intended word from a misspelling. With a large database,
a potential misspelling could correspond to multiple data
items. In such a case, the system would need to initiate a
disambiguation subdialogue to resolve the ambiguity.
There are several pragmatic issues to consider in ob-
taining spelled data from a user whether via keypad
or speech. The problem of disambiguating keypad se-
quences has been addressed using both dictionary-based
(Davis, 1991) as well as probabilistic (MacKenzie et al,
2001) approaches. In both input modes, the user may
use abbreviations such as ?S T P E T E R S B U R G?
for ?Saint Petersburg?. Spoken spelling is especially dif-
ficult, because the recognition accuracy for spoken let-
ters can be quite low. For instance, the members of the
?E-set? (B, C, D, E, G, P, T, V, Z) are well-known for
being confusable to a recognizer, as discussed in previ-
ous studies (Marx and Schmandt, 1994). This problem is
compounded by the fact that humans spell words in cre-
ative ways. Some may spell in military style (e.g., ?Al-
pha Bravo Charlie? for ?A B C?) or in simile (e.g., ?B
as in ?Boy??). Some users may include the word ?space?
to mark the word boundaries of a multi-word sequence,
such as ?N E W space Y O R K?. Some may simply enter
a letter sequence containing several meaningful chunks,
as in ?N E W Y O R K J F K N Y C? for Kennedy Air-
port in New York City. Many of these issues have been
addressed in (Schramm et al, 2000).
3 MERCURY Error Recovery Strategy
The MERCURY system, accessible via a toll free num-
ber2, provides information about flights available for over
500 cities worldwide. We have invested considerable ef-
fort into making MERCURY intuitive to use and robust in
handling a wide range of different ways users might ex-
press their flight constraints and select the flights of the
itinerary. A typical user begins by logging on, providing
both his name and password, which allows the system to
look up some personalized information such as the e-mail
address and the preferred departure city. MERCURY?s di-
alogue plan involves arranging a trip one leg at a time.
Once the itinerary is fully specified, MERCURY offers to
price the itinerary and, subsequently, to send a detailed
record of the itinerary to the user via e-mail, which can
then be forwarded to a travel agent for the actual booking.
A critical aspect of flight dialogues is the successful
communication of the source, destination, and date, all of
which are susceptible to recognition error. MERCURY?s
default policy is to use implicit confirmation to communi-
cate to the user its interpretation of his utterances. In the
meantime, it monitors the evolution over time of these
three critical attributes. When it detects odd behavior, it
switches into a mode where keypad entry is solicited. The
keypad entry is matched against existing hypotheses and,
if a successful match is obtained, is assumed to be correct.
Otherwise, a verbal confirmation subdialogue, soliciting
a ?yes/no? answer, is invoked.
For source and destination, the system tabulates at
each turn whether the attribute was inherited, repeated, or
changed. If a change is detected after flights have already
been retrieved, the system prompts for spoken confirma-
tion of the surprising move, anticipating possible recog-
nition error. After two consecutive turns where the user
has either apparently repeated or replaced the departure
or arrival city, the system requests the user to enter the
city by spelling it using the telephone keypad. This strat-
egy is also used if a substitution/repetition of the city is
followed by an utterance that is not understood, or when-
ever the user explicitly requests to change the departure
or arrival city. It turns out that MERCURY?s 500 cities are
uniquely identifiable through their keypad codes; how-
ever, if this were not the case, a follow-up disambiguation
subdialogue could be arranged. This keypad mechanism
also provides the opportunity to confirm whether the de-
sired city is known or unknown.
A similar process takes place for dates. If the user
appears to repeat the date, without providing any other
information, there is the suspicion that a misrecognized
date has again been misrecognized the same way. In this
case, the system tries to find an alternative hypothesis for
the date by re-examining the N -best list of recognizer
21-877-MIT-TALK.
hypotheses, and, in any case, also asks for user confir-
mation. As is the case for cities, the system invokes the
keypad upon repeated date corrections.
Figures 1 and 2 provide two examples of user dia-
logues involving keypad city entry. Figure 1 illustrates
a dialogue where the conversation is clearly confused,
and the system eventually takes the initiative to invite
a keypad entry of the departure city. The user wanted
to go to ?Austin?, which the system misunderstood as
?Boston?. This particular user had a default departure
city of ?Boston?, which caused the system to suppose
that the user had requested a pragmatically unreasonable
flight from ?Boston? to ?Boston?. The user?s follow-up
fragment, ?Austin, Texas?, was correctly understood, but
misinterpreted as the departure city instead of the arrival
city, leading to further confusion. It was only after the
user had cleared up the confusion, with the complete ut-
terance, ?I would like to fly from Boston, Massachusetts
to Austin, Texas,? that the system was finally on the right
track, but by this point, it had identified difficulty with
the source, reacting by launching a keypad entry request,
with subsequent resolution.
Figure 2 shows an example subdialogue where the des-
tination city was successfully entered using the telephone
keypad, based on an explicit request on the part of the
user to change the destination. Interestingly, the user
delayed the correction until the system invited him to
change any constraint that was already specified. This
particular user probably believed that she was required
to respond to the prompts, although it is conceivable that
the user?s delayed response was due to inattentiveness.
This dialogue thus reveals some of the potential difficul-
ties encountered due to users? false assumptions about the
system?s behavior.
4 MERCURY Analysis
We have been collecting MERCURY data over the tele-
phone for the past several years (Seneff and Polifroni,
2000), involving user interactions with the system to
make flight reservations. In examining these dialogues,
we have come to the realization that, while keypadding
the date (as a four digit numeric code for month and day)
seems to be intuitive to users and therefore an effective
mechanism for correcting misunderstandings, the situa-
tion is far less effective in the case of city names.
A detailed analysis has thus been performed on all in-
stances where the system requested a source or destina-
tion entry via the keypad, and the user?s reactions to the
requests were observed and quantified. We found that this
strategy, when users were compliant, was generally suc-
cessful for determining the user?s desired source or desti-
nation. For example, if the user were to enter ?3387648?,
the system would understand ?DETROIT?, and the dia-
logue would smoothly continue.
In addition to many successful responses, however,
several errorful responses were also observed, including
misspelled words (e.g., ?TEIPEI? for ?TAIPEI?), out-
of-vocabulary words (e.g., ?DOMINICA?), or a string
of valid references that could not be resolved as a sin-
gle place name (e.g., ?NEWYORKJFKNYC? for ?New
York?s Kennedy Airport?). A user time-out or hang-up
was also common, and constituted a significant number
of responses.
A total of 172 instances were observed in which the
system prompted users to enter a source or destination
via the keypad. The number of occurrences is rather low
since this solicitation was only activated as a last resort.
The system then entered a state where speech was not an
option. The users? responses to these prompts are sum-
marized in Table 1. Most surprising is that nearly half of
the time, the user did not even attempt to use the keypad.
In only 88 of the cases did the user actively enter a key-
pad code. The user let a time-out occur in 50 cases, and
hung up the telephone in an additional 34 cases.
Description Count Percentage
user attempts at keypad entry 88/172 51.1%
time-outs 50/172 29.1%
hang-ups 34/172 19.8%
Table 1: Summary of users? responses to 172 system prompts
to enter a source or destination using the telephone keypad.
This attempt rate of 51.1% is significantly lower than
originally hoped. Even within the 88 compliant cases,
the results are disappointing, as shown in Table 2. In
61 cases, the keypad sequence entered by the user cor-
responded to a valid city or airport name. Most of these
were known to the system and were processed success-
fully. The remaining 30.7% of attempts consisted of mis-
spellings (such as a double tap on a key, substituting the
number ?0? for the letter ?o?, or terminating with ?*? in-
stead of ?#?) or apparent garbage.
Description Count Percentage
valid city/airport entered 61/88 69.3%
misspelled city/airport entered 19/88 21.6%
garbage entered (e.g, ?***#?) 8/88 9.1%
Table 2: Summary of a total of 88 user attempts at entering a
source or destination city or airport name using the telephone
keypad after being prompted by the system.
4.1 Discussion
Our results suggest that the strategy of prompting for key-
pad entry of questionable parameters shows potential for
recovering from situations in which the system is con-
fused about what the user has said. We believe that such
recovery can contribute to successful dialogue comple-
tion, as well as elevating the user?s tolerance level. Nev-
ertheless, our results also pose two questions that need to
be addressed: why do some users? attempts at keypad en-
try contain errors, and, more importantly, why do some
users not even attempt keypad entry?
It is not possible to know why an individual user was
unable to enter a valid keypad sequence; we had no mech-
anism to interview users about their behavior. We can,
however, speculate that the errorful sequences were due
to the non-intuitive nature of spelling with a telephone
keypad, a user?s unfamiliarity with the spelling of a given
word, typos, or a user?s confusion as to what qualified
as an acceptable entry (e.g., Are abbreviations and nick-
names allowed?).
We must also acknowledge the fact that what qualifies
as a valid keypad sequence depends on the spelling cor-
rection capabilities of the system. Even a simple spelling
checker (not utilized during the MERCURY data collec-
tion) could potentially allow the system to make sense of
an errorful keypad sequence.
In the case of a time-out, it is difficult to know what
each user was thinking as he waited. It is likely that the
user was hoping for a return to speech mode after the
time-out. The user may have hesitated for fear of send-
ing the system down an even more divergent path. It is
also possible that users were inattentive when the system
instructed them to terminate with the pound key, and that
they therefore entered the entire city, but without a termi-
nation code. Clearly a strategic modification to automat-
ically generate a ?#? after a significant pause might help
reduce this type of error.
The reason for a hang-up is more obvious, given the
dialogue context. For example, if the user had repeatedly
said that he wanted to fly to Anchorage and the system
had already hypothesized three other cities, it is under-
standable that he would have hung up in frustration.
The telephone keypad would seem to be a very prac-
tical mode of information entry given its physical acces-
sibility and limited ambiguity per key. This small set of
data in the flight domain, however, suggests that it is con-
fusing, annoying, or simply intimidating to many users.
The next challenge, then, is to utilize a similar error re-
covery strategy, but to adopt a different mode of informa-
tion entry, one that is more intuitive and less intimidating.
We discuss such an option in the next section.
5 Spoken Spelling
Allowing a user to spell a word has several benefits,
including maintaining a single mode of communication
(i.e., speech), as well as being less taxing, more efficient,
and more intuitive. Our goal is to make the user feel con-
fident that spelling a city name is a plausible request and
that it can be the most effective path to task completion.
Undeniably, spelling recognition comes with its own
set of problems, especially misrecognition of the spoken
letters. One way to minimize such errors is to incorpo-
rate limited spelling checking, such as allowing a single
insertion, deletion, or substitution per word. For example,
a spelling sequence recognized as ?T E N V E R? could
be mapped to ?D E N V E R? as the closest match in the
database. Obviously, a trade-off exists where overgener-
ous spelling correction could lead to a false hypothesis.
A great challenge in developing conversational sys-
tems is that dialogue strategies can only evolve through
extensive experimentation, which requires a large amount
of data, particularly for situations that occur rarely in ac-
tual dialogues. To expedite development and evaluation
of the recovery strategy, we decided to make use of sim-
ulated user data to artificially continue MERCURY dia-
logues beyond the point where the system had originally
asked for a keypad entry, as described in the next section.
6 User Simulation
To streamline exploration of alternative dialogue strate-
gies for error recovery, we have implemented a simulated
user that speaks and spells a city name using DECTalk.
A block diagram of our simulated user system is shown
in Figure 3. Each synthesized waveform3 contains a pro-
nunciation of the city name that a user was trying to com-
municate in the original dialogue, immediately followed
by a spoken spelling of that city name (e.g., ?Boston
B O S T O N?). The waveform is passed to a first stage
speech recognizer, which treats the spoken word as an un-
known word and proposes an N -best list of hypothesized
spellings for the synthesized letter sequence. For speech
recognition, we use the SUMMIT framework (Glass et
al., 1996), and the unknown word is modeled according
to techniques described in (Bazzi and Glass, 2002).
Following the first stage recognition, a two-stage
matching process first consults a list of ?cities in focus?
that were extracted as hypotheses from the original user?s
final utterance before the keypad turn. Subsequently, if a
match or conservative partial match is not found from the
short list, a large database of 17,000 city and state names
is consulted for a match or a partial match. In this case a
confirmation subdialogue ensues.
If a match is found, a geography server determines
whether the name is ambiguous. If so, a disambiguating
item (e.g., state name) is requested by the dialogue man-
ager. The simulated user then randomly chooses from
3While DECTalk speech is artificial, we have not explicitly
trained our recognizer on it, and thus we argue that it can serve
as an effective stand-in for real human speech.
a list of candidate state names provided by the geogra-
phy server. This utterance is currently also processed as
a speak-and-spell utterance, mainly because we are inter-
ested in obtaining more data on the performance of our
speak-and-spell system.
If no match is found in either the short list or the
external lexicon of known city names, another recogni-
tion cycle is initiated, in which the phonetic content of
the spoken word is used to enhance the performance of
the spelling recognizer, following procedures described
in (Chung et al, 2003). A letter-to-sound model is used
to map from a graph of letter hypotheses proposed by
the first stage recognizer to their corresponding plausi-
ble pronunciations, using techniques described in (Sen-
eff et al, 1996). The final set of hypotheses is obtained
by merging hypotheses produced from both halves of the
user utterance. Once again, both the short list and the
large lexicon are searched for a match.
The idea is that this second stage should only be in-
voked upon failure, in order to reduce the amount of com-
putation time required. An alternative strategy would be
for the system to unconditionally execute a second recog-
nition to obtain a potentially more correct hypothesis.
Such a strategy, however, would increase the system?s
overall processing time.
6.1 Results and Discussion
The simulation was performed on a total of 97 user ut-
terances, all of which MERCURY had designated as trou-
ble situations in the original dialogues. The utterances
utilized are those for which the system?s hypotheses con-
tained city names, whether or not the user had actually
mentioned a city name.
The simulation results are shown in Table 3. Out of
97 problematic sources and destinations generated by the
simulated user, 58 required disambiguation with a state
name (e.g., ?Boston in Georgia?). Therefore, 155 speak-
and-spell utterances were ultimately passed through the
synthesize-and-recognize simulation cycle. All but one
of the state names were correctly recognized. This high
performance is likely due to the correct state?s guaranteed
existence in the short list used by the spelling checker.
Our algorithm dictates that a second pass, which
integrates the spoken name portion of the waveform
with letter-to-sound hypotheses derived from the spoken
spelling portion, be omitted if a match is found in the first
pass. One question to ask is whether the system is being
overconfident in this strategy. The results in the table sup-
port the notion of using the second pass sparingly. In 68
cases, the system was sufficiently confident with its hy-
pothesized city after the first recognition pass to omit the
second pass; it made no errors in these decisions.
About a third of the time (29 cases), the system, find-
ing no match, initiated a second pass to incorporate pro-
Simulated
User
"Boston B?O?S?T?O?N"
"Iowa I?O?W?A"
City
Context
from
Mercury
Context
from
State
Geography
Server
Recognize
Unknown Word
and Letter Sequence
Rerecognize Using
Additional Information
the Unknown Word as
Spell Check
N?Best Hypotheses
on City/State Context
Large Database of
Spell Check on
Geographical Names
Match? Match?
Second
Pass?
Accepted
by User?
Geographically
Ambiguous?
CONTINUE
Choose
Best
Hypothesis
Hypothesis
with User
Confirm
NO
YES
YES YES
NONO
YES
NO
NO
YES
Figure 3: Flow chart detailing the two-pass dialogue strategy for recovering from a problematic source or destination.
Description Count Percentage
correct city after pass 1 68/68 100%
correct city after pass 2:
short list match 2/2 100%
database match 11/14 78.6%
no match (last resort) 5/13 38.5%
total cities correct 86/97 88.7%
Table 3: Simulation results for 97 speak-and-spell city names
showing the number of correct cities hypothesized by the sys-
tem, after each of two recovery passes. For pass 2, a match was
found on the short list or in the geographic database. No match
resulted in resorting to the best recognition hypothesis.
nunciation information. There were two instances where
the second-pass hypothesized city was found on the short
list of focus cities from the original user utterance; both
were correct. For the remainder, the large database was
consulted. The system proposed the correct city in nearly
79% of the cases. After failing to find any match, the sys-
tem attempted its last resort of proposing the best hypoth-
esis from the second-stage recognizer. Not surprisingly,
the system determined the correct city name in only 39%
of these cases. Nevertheless, this percentage suggests that
it is certainly better to perform the confirmation rather
than to simply tell the user that the city is unknown, given
that the recognizer may be correct without the aid of any
external lexicon.
The majority of incorrect city hypotheses were due to
limitations in the spelling checker and the absence of in-
ternational names in the geographic database. The cur-
rent spelling checker, while quite powerful, allows only a
single insertion, deletion, or substitution of a letter, or
a swap of two letters. We believe that a more robust
spelling checker can minimize many of these errors.
The system?s performance in hypothesizing the correct
candidate for nearly 89% of the problematic city names
is encouraging. These results show that this error recov-
ery strategy is largely successful in the synthesize-and-
recognize user simulation cycle. The simulated results
are, of course, biased in that the simulated user was co-
operative with all system requests. The results of the
MERCURY analysis in Section 4 show that an errorful or
nonexistent response from a user is a very likely possibil-
ity. The installation of this strategy in a real system will
require that user behavior be carefully monitored.
Although the prospects for the speak-and-spell input
mode are promising, we would not want to entirely aban-
don the use of the telephone keypad. It has been and re-
mains a convenient and effective means by which to spell
words. A more appropriate use of the keypad could be
as a back-off strategy after the spoken spelling has failed,
or in very noisy environments, where speech would be
nearly impossible. One advantage of the keypad is that,
barring mistakes, the system can be confident that when
?3? is pushed, one of the letters, ?D?, ?E?, or ?F?, is in-
tended. When combined with the spoken word being
spelled, such keypad ambiguity can be reduced even fur-
ther (Chung et al, 2003).
7 Future Work
While we feel that the speak-and-spell subdialogue inter-
action represents a promising strategy for error recovery
in situations of compromised recognition performance, it
remains to be seen whether it will work well in real user
dialogues. An obvious next step is to incorporate this
strategy into our MERCURY system in place of the key-
pad entry request, and then to assess how well users are
able to recover from errors and complete their dialogue
interactions.
We are quite encouraged by the effectiveness of the
strategy of involving DECTalk to simulate user utter-
ances, and we believe this idea has merit far beyond the
experiments conducted here. For example, we are inter-
ested in developing a new CSAIL Information domain,
which would allow users to call up and ask about the
phone number, e-mail address, etc., for the 800 members
of our laboratory. Many of these people have foreign-
derived names, which would likely be difficult to pro-
nounce, both for users and for DECTalk. However, it
would be straightforward to process all 800 first and last
names through simulated dialogues, and to obtain a list
of those names that failed to be understood by the sys-
tem. Developers? attention could then be drawn toward
the task of assuring these names are included explicitly
and are correctly pronounced in the main recognizer, as
well as augmenting the letter-to-sound system to include
those problematic words in its training corpus. It would
even be feasible to lexicalize difficult names within the
first stage of the speak-and-spell recognizer, such that an
explicit word hypothesis could be extracted from the spo-
ken word at that point.
References
J.G. Bauer and J. Junkawitsch. 1999. Accurate recognition of
city names with spelling as a fall back strategy. In Proc.
Eurospeech, volume 1, pages 263?266, Budapest, Hungary.
I. Bazzi and J. Glass. 2002. A multi-class approach for
modelling out-of-vocabulary words. In Proc. ICSLP, pages
1613?1616, Denver, Colorado.
G. Chung, S. Seneff, and C. Wang. 2003. Automatic acquisi-
tion of names using speak and spell mode in spoken dialogue
systems. In Proc. HLT-NAACL ?03, pages 32?39.
N. Dahlback, A. Flycht-Eriksson, A. Jonsson, and P. Qvarfordt.
1999. An architecture for multi-modal natural dialogue sys-
tems. In Proc. of ESCA Tutorial and Research Workshop
(ETRW) on Interactive Dialogue in Multi-Modal Systems.
J.R. Davis. 1991. Let your fingers do the spelling: Implicit
disambiguation of words spelled with the telephone keypad.
Avios Journal, 9:57?66.
M. Denecke. 2002. Rapid prototyping for spoken dialogue
systems. In Proc. COLING.
W. Eckert, E. Levin, and R. Pieraccini. 1997. User modelling
for spoken dialogue system evaluation. In Proc. IEEE ASR
Workshop.
J. Glass and S. Seneff. 2003. Flexible and personalizable
mixed-initiative dialogue systems. In HLT-NAACL 2003
Workshop on Research Directions in Dialogue Processing,
Edmonton, Canada.
J. Glass, J. Chang, and M. McCandless. 1996. A probabilistic
framework for feature-based speech recognition. In Proc.
ICSLP, pages 1?4, Philadelphia, PA.
A. Gorin, G. Riccardi, and J. Wright. 1997. How may i help
you. Speech Communication, 23:113?127.
M. Lundeberg J. Gustafson, N. Lindberg. 1999. The august
spoken dialogue system. In Proc. Eurospeech ?99.
I.S. MacKenzie, H. Kober, D. Smith, T. Jones, and E. Skepner.
2001. Letterwise: Prefix-based disambiguation for mobile
text input. In Proc. UIST, pages 111?120, Orlando, FL.
M. Marx and C. Schmandt. 1994. Putting people first: Specify-
ing proper names in speech interfaces. In Proc. UIST, pages
29?37, Marina del Rey, CA.
R. Pieraccini, E. Levin, and W. Eckert. 1997. AMICA: The
AT&T mixed initiative conversational architecture. In Proc.
EUROSPEECH, pages 1875?1878.
J. Polifroni and G. Chung. 2002. Promoting portability in dia-
logue management. In Proc. ICSLP, pages 2721?2724.
H. Quast, T. Scheideck, P. Geutner, A. Korthauer, and R. Bosch.
2003. Robodima: A dialogue-object based natural language
speech dialog system. In Proc. ASRU Workshop, pages 174?
179, Saint Thomas, Virgin Islands.
H. Schramm, B. Rueber, and A. Kellner. 2000. Strategies for
name recognition in automatic directory assistance systems.
Speech Communication, 31(4):329?338.
S. Seneff and J. Polifroni. 2000. Dialogue management in the
mercury flight reservation system. In Proc. ANLP-NAACL
2000, Satellite Workshop, pages 1?6.
S. Seneff, R. Lau, and H. Meng. 1996. ANGIE: A new frame-
work for speech analysis based on morpho-phonological
modelling. In Proc. ICSLP ?96, pages 110?113.
S. Seneff, G. Chung, and C. Wang. 2003. Empowering end
users to personalize dialogue systems through spoken inter-
action. In Proc. EUROSPEECH, pages 749?752.
S. Seneff. 2002. Response planning and generation in the mer-
cury flight reservation system. Computer Speech and Lan-
guage, 16:283?312.
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. Garofolo,
L. Hirschman, A. Le, S. Lee, S. Narayanan, K. Papineni,
B. Pellom, J. Polifroni, A. Potamianos, P. Brabhu, A. Rud-
nicky, G. Sanders, S. Seneff, D. Stallard, and S. Whittaker.
2001. Darpa communicator dialog travel planning systems:
The June 2000 data collection. In Proc. EUROSPEECH,
pages 1371?1374.
V. Zue and J. Glass. 2000. Conversational interfaces: Advances
and challenges. Proc. IEEE, 88(11):1166?1180.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J. Hazen, and
L. Hetherington. 2000. Jupiter: A telephone-based conver-
sational interface for weather information. IEEE Transac-
tions on Speech and Audio Processing, 8(1):85?96.
Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Multimodal Home Entertainment Interface via a Mobile Device
Alexander Gruenstein Bo-June (Paul) Hsu James Glass Stephanie Seneff
Lee Hetherington Scott Cyphers Ibrahim Badr Chao Wang Sean Liu
MIT Computer Science and Artificial Intelligence Laboratory
32 Vassar St, Cambridge, MA 02139 USA
http://www.sls.csail.mit.edu/
Abstract
We describe a multimodal dialogue system for
interacting with a home entertainment center
via a mobile device. In our working proto-
type, users may utilize both a graphical and
speech user interface to search TV listings,
record and play television programs, and listen
to music. The developed framework is quite
generic, potentially supporting a wide variety
of applications, as we demonstrate by integrat-
ing a weather forecast application. In the pro-
totype, the mobile device serves as the locus
of interaction, providing both a small touch-
screen display, and speech input and output;
while the TV screen features a larger, richer
GUI. The system architecture is agnostic to
the location of the natural language process-
ing components: a consistent user experience
is maintained regardless of whether they run
on a remote server or on the device itself.
1 Introduction
People have access to large libraries of digital con-
tent both in their living rooms and on their mobile
devices. Digital video recorders (DVRs) allow peo-
ple to record TV programs from hundreds of chan-
nels for subsequent viewing at home?or, increas-
ingly, on their mobile devices. Similarly, having
accumulated vast libraries of digital music, people
yearn for an easy way to sift through them from the
comfort of their couches, in their cars, and on the go.
Mobile devices are already central to accessing
digital media libraries while users are away from
home: people listen to music or watch video record-
ings. Mobile devices also play an increasingly im-
portant role in managing digital media libraries. For
instance, a web-enabled mobile phone can be used to
remotely schedule TV recordings through a web site
or via a custom application. Such management tasks
often prove cumbersome, however, as it is challeng-
ing to browse through listings for hundreds of TV
channels on a small display. Indeed, even on a large
screen in the living room, browsing alphabetically,
or by time and channel, for a particular show using
the remote control quickly becomes unwieldy.
Speech and multimodal interfaces provide a nat-
ural means of addressing many of these challenges.
It is effortless for people to say the name of a pro-
gram, for instance, in order to search for existing
recordings. Moreover, such a speech browsing ca-
pability is useful both in the living room and away
from home. Thus, a natural way to provide speech-
based control of a media library is through the user?s
mobile device itself.
In this paper we describe just such a prototype
system. A mobile phone plays a central role in pro-
viding a multimodal, natural language interface to
both a digital video recorder and a music library.
Users can interact with the system?presented as a
dynamic web page on the mobile browser?using
the navigation keys, the stylus, or spoken natural
language. In front of the TV, a much richer GUI is
also available, along with support for playing video
recordings and music.
In the prototype described herein, the mobile de-
vice serves as the locus of natural language in-
teraction, whether a user is in the living room or
walking down the street. Since these environments
may be very different in terms of computational re-
1
sources and network bandwidth, it is important that
the architecture allows for multiple configurations in
terms of the location of the natural language pro-
cessing components. For instance, when a device
is connected to a Wi-Fi network at home, recogni-
tion latency may be reduced by performing speech
and natural language processing on the home me-
dia server. Moreover, a powerful server may enable
more sophisticated processing techniques, such as
multipass speech recognition (Hetherington, 2005;
Chung et al, 2004), for improved accuracy. In sit-
uations with reduced network connectivity, latency
may be improved by performing speech recognition
and natural language processing tasks on the mobile
device itself. Given resource constraints, however,
less detailed acoustic and language models may be
required. We have developed just such a flexible ar-
chitecture, with many of the natural language pro-
cessing components able to run on either a server or
the mobile device itself. Regardless of the configu-
ration, a consistent user experience is maintained.
2 Related Work
Various academic researchers and commercial busi-
nesses have demonstrated speech-enabled interfaces
to entertainment centers. A good deal of the work
focuses on adding a microphone to a remote con-
trol, so that speech input may be used in addition
to a traditional remote control. Much commercial
work, for example (Fujita et al, 2003), tends to fo-
cus on constrained grammar systems, where speech
input is limited to a small set of templates corre-
sponding to menu choices. (Berglund and Johans-
son, 2004) present a remote-control based speech
interface for navigating an existing interactive tele-
vision on-screen menu, though experimenters man-
ually transcribed user utterances as they spoke in-
stead of using a speech recognizer. (Oh et al, 2007)
present a dialogue system for TV control that makes
use of concept spotting and statistical dialogue man-
agement to understand queries. A version of their
system can run independently on low-resource de-
vices such as PDAs; however, it has a smaller vo-
cabulary and supports a limited set of user utterance
templates. Finally, (Wittenburg et al, 2006) look
mainly at the problem of searching for television
programs using speech, an on-screen display, and a
remote control. They explore a Speech-In List-Out
interface to searching for episodes of television pro-
grams.
(Portele et al, 2003) depart from the model of
adding a speech interface component to an exist-
ing on-screen menu. Instead, they a create a tablet
PC interface to an electronic program guide, though
they do not use the television display as well. Users
may search an electronic program guide using con-
straints such as date, time, and genre; however, they
can?t search by title. Users can also perform typi-
cal remote-control tasks like turning the television
on and off, and changing the channel. (Johnston et
al., 2007) also use a tablet PC to provide an inter-
face to television content?in this case a database of
movies. The search can be constrained by attributes
such as title, director, or starring actors. The tablet
PC pen can be used to handwrite queries and to point
at items (such as actor names) while the user speaks.
We were also inspired by previous prototypes in
which mobile devices have been used in conjunc-
tion with larger, shared displays. For instance, (Paek
et al, 2004) demonstrate a framework for building
such applications. The prototype we demonstrate
here fits into their ?Jukebox? model of interaction.
Interactive workspaces, such as the one described in
(Johanson et al, 2002), also demonstrate the utility
of integrating mobile and large screen displays. Our
prototype is a departure from these systems, how-
ever, in that it provides for spoken interactions.
Finally, there is related work in the use of mobile
devices for various kinds of search. For instance, of-
ferings from Microsoft (Acero et al, 2008), Vlingo,1
and Promptu2 allow users to search for items like
businesses and songs using their mobile phones.
These applications differ from ours in that speech
is used only for search, without any accompanying
command and control capabilities. Also, these ser-
vices do not allow interaction with your own de-
vices at home. Efforts have been made to use mo-
bile devices for control of devices in the home, such
as in (Nichols and Myers, 2006), however these ef-
forts have not involved the use of speech as an input
modality.
1http://www.vlingo.com
2http://www.promptu.com
2
Na
vig
ati
on
 pa
d o
r s
tyl
us 
can
 
be
 us
ed
 to
 na
vig
ate
 G
UI
Sta
tus
 ic
on
 in
dic
ate
s c
on
ne
cti
on
 to
 se
rve
r, 
mi
cro
ph
on
e i
co
n w
he
n l
ist
en
ing
Ho
ld-
to-
tal
k 
bu
tto
n
Op
era we
b 
bro
ws
er
Sy
nc
hro
niz
ed
 se
lec
tio
n o
n m
ob
ile
 an
d 
tel
ev
isi
on
 di
spl
ay
s
Fir
efo
x
we
b b
row
ser
Ta
bs 
all
ow
 ea
sy 
sw
itc
hin
g 
am
on
g d
om
ain
s
Me
dia
 pl
ay
ba
ck
 
co
ntr
ols
Figure 1: User interface overview.
(a) Program Guide
(b) Scheduled Recordings
(c) Music Library
U: What?s on tonight at seven o?clock?
S: I found seventy entries. [Displays program guide.]
U: Are there any action movies on HBO tomorrow?
S: I found two entries. [Displays results.]
U: When is American Idol on?
S: I found three entries. [Displays results.]
U: Record the first one.
S: I have added American Idol to the list of programs to be
recorded. [Displays recording settings.]
U: Please record Nova on Wednesday night.
S: I have added Nova to the list of programs to be recorded.
[Displays recording settings.]
U: Show all my scheduled recordings.
S: There are seven scheduled recordings. [Shows list.]
U: Delete BBC World News.
S: I have deleted the scheduled recording: BBC World
News. [Updates List.]
U: Show my videos.
S: [Displays list of videos.]
U: Delete Frasier.
S: I have deleted the recording Frasier. [Updates list.]
U: Play Grey?s Anatomy.
S: Playing Grey?s Anatomy. [Plays recording.]
U: Stop Playing.
S: Stopping. [Stops playback.]
U: [Clicks on Music tab.]
S: [Displays artist list.]
U: Show albums by Billie Holiday.
S: I found three albums by Billie Holiday. [Shows albums.]
U: Please play A Hard Rain?s A-Gonna Fall by Bob Dylan.
S: Playing A Hard Rain?s A-Gonna Fall. [Plays song.]
Figure 2: Screenshots and an example interaction. Utterances are labeled with U for user and S for system.
3
3 User Experience
Our current prototype system implements the basic
functionalities that one expects from a home enter-
tainment center. Users can navigate through and
record programs from the television?s electronic pro-
gram guide, manage recording settings, and play
recorded videos. They can also browse and listen to
selections from their music libraries. However, un-
like existing prototypes, ours employs a smartphone
with a navigation pad, touch-sensitive screen, and
built-in microphone as the remote control. Figure 1
provides an overview of the graphical user interface
on both the TV and mobile device.
Mirroring the TV?s on-screen display, the proto-
type system presents a reduced view on the mobile
device with synchronized cursors. Users can navi-
gate the hierarchical menu structure using the arrow
keys or directly click on the target item with the sty-
lus. While away from the living room, or when a
recording is playing full screen, users can browse
and manage their media libraries using only the mo-
bile device.
While the navigation pad and stylus are great for
basic navigation and control, searching for media
with specific attributes, such as title, remains cum-
bersome. To facilitate such interactions, the cur-
rent system supports spoken natural language inter-
actions. For example, the user can press the hold-to-
talk button located on the side of the mobile device
and ask ?What?s on the National Geographic Chan-
nel this afternoon?? to retrieve a list of shows with
the specified channel and time. The system responds
with a short verbal summary ?I found six entries on
January seventh? and presents the resulting list on
both the TV and mobile displays. The user can then
browse the list using the navigation pad or press the
hold-to-talk button to barge in with another com-
mand, e.g. ?Please record the second one.? Depress-
ing the hold-to-talk button not only terminates any
current spoken response, but also mutes the TV to
minimize interference with speech recognition. As
the previous example demonstrates, contextual in-
formation is used to resolve list position references
and disambiguate commands.
The speech interface to the user?s music library
works in a similar fashion. Users can search by
artist, album, and song name, and then play the
songs found. To demonstrate the extensibility of
the architecture, we have also integrated an exist-
ing weather information system (Zue et al, 2000),
which has been previously deployed as a telephony
application. Users simply click on the Weather tab
to switch to this domain, allowing them to ask a wide
range of weather queries. The system responds ver-
bally and with a simple graphical forecast.
To create a natural user experience, we designed
the multimodal interface to allow users to seam-
lessly switch among the different input modalities
available on the mobile device. Figure 2 demon-
strates an example interaction with the prototype, as
well as several screenshots of the user interface.
4 System Architecture
The system architecture is quite flexible with re-
gards to the placement of the natural language pro-
cessing components. Figure 3 presents two possible
configurations of the system components distributed
across the mobile device, home media server, and
TV display. In 3(a), all speech recognition and nat-
ural language processing components reside on the
server, with the mobile device acting as the micro-
phone, speaker, display, and remote control. In 3(b),
the speech recognizer, language understanding com-
ponent, language generation component, and text-
to-speech (TTS) synthesizer run on the mobile de-
vice. Depending on the capabilities of the mobile
device and network connection, different configu-
rations may be optimal. For instance, on a power-
ful device with slow network connection, recogni-
tion latency may be reduced by performing speech
recognition and natural language processing on the
device. On the other hand, streaming audio via a fast
wireless network to the server for processing may
result in improved accuracy.
In the prototype system, flexible and reusable
speech recognition and natural language processing
capabilities are provided via generic components de-
veloped and deployed in numerous spoken dialogue
systems by our group, with the exception of an off-
the-shelf speech synthesizer. Speech input from the
mobile device is recognized using the landmark-
based SUMMIT system (Glass, 2003). The result-
ing N-best hypotheses are processed by the TINA
language understanding component (Seneff, 1992).
4
Gala
xy
Spee
ch?R
ecog
nize
r
Lang
uage
?Und
ersta
ndin
g
Dialo
gue?
Man
ager
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Web
?
Serv
er
Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Gala
xy
Dialo
gue?
Man
ager
Web
?
Serv
er Tele
visio
n Web
?Bro
wse
r
Med
ia?Pl
ayer
TV?G
uide
?
Med
ia
Mob
ile?D
evic
e
Web
?Bro
wse
r
Hom
e?M
edia
?Serv
er
Wea
ther
Mob
ile?
Man
ager
Aud
io?In
put?
/?Ou
tput
Spee
ch?R
ecog
nize
r
Lang
uage
?Gen
erat
ion
Text
?To?S
peec
h
Lang
uage
?Und
ersta
ndin
g
(a) (b)
Figure 3: Two architecture diagrams. In (a) speech recognition and natural language processing occur on the server,
while in (b) processing is primarily performed on the device.
Based on the resulting meaning representation, the
dialogue manager (Polifroni et al, 2003) incorpo-
rates contextual information (Filisko and Seneff,
2003), and then determines an appropriate response.
The response consists of an update to the graph-
ical display, and a spoken system response which
is realized via the GENESIS (Baptist and Seneff,
2000) language generation module. To support on-
device processing, all the components are linked via
the GALAXY framework (Seneff et al, 1998) with
an additional Mobile Manager component responsi-
ble for coordinating the communication between the
mobile device and the home media server.
In the currently deployed system, we use a mo-
bile phone with a 624 MHz ARM processor run-
ning the Windows Mobile operating system and
Opera Mobile web browser. The TV program and
music databases reside on the home media server
running GNU/Linux. The TV program guide data
and recording capabilities are provided via MythTV,
a full-featured, open-source digital video recorder
software package.3 Daily updates to the program
guide information typically contain hundreds of
unique channel names and thousands of unique pro-
gram names. The music library is comprised of
5,000 songs from over 80 artists and 13 major gen-
res, indexed using the open-source text search en-
gine Lucene.4 Lastly, the TV display can be driven
by a web browser on either the home media server or
a separate computer connected to the server via a fast
Ethernet connection, for high quality video stream-
ing.
3http://www.mythtv.org/
4http://lucene.apache.org/
While the focus of this paper is on the natural lan-
guage processing and user interface aspects of the
system, our work is actually situated within a larger
collaborative project at MIT that also includes sim-
plified device configuration (Mazzola Paluska et al,
2008; Mazzola Paluska et al, 2006), transparent ac-
cess to remote servers (Ford et al, 2006), and im-
proved security.
5 Mobile Natural Language Components
Porting the implementation of the various speech
recognizer and natural language processing com-
ponents to mobile devices with limited computa-
tion and memory presents both a research and en-
gineering challenge. Instead of creating a small vo-
cabulary, fixed phrase dialogue system, we aim to
support?on the mobile device?the same flexible
and natural language interactions currently available
on our desktop, tablet, and telephony systems; see
e.g., (Gruenstein et al, 2006; Seneff, 2002; Zue et
al., 2000). In this section, we summarize our ef-
forts thus far in implementing the SUMMIT speech
recognizer and TINA natural language parser. Ports
of the GENESIS language generation system and of
our dialogue manager are well underway, and we ex-
pect to have these components working on the mo-
bile device in the near future.
5.1 PocketSUMMIT
To significantly reduce the memory footprint and
overall computation, we chose to reimplement our
segment-based speech recognizer from scratch, uti-
lizing fixed-point arithmetic, parameter quantiza-
tion, and bit-packing in the binary model files.
The resulting PocketSUMMIT recognizer (Hether-
5
ington, 2007) utilizes only the landmark features,
initially forgoing segment features such as phonetic
duration, as they introduce algorithmic complexities
for relatively small word error rate (WER) improve-
ments.
In the current system, we quantize the mean and
variance of each Gaussian mixture model dimension
to 5 and 3 bits, respectively. Such quantization not
only results in an 8-fold reduction in model size, but
also yields about a 50% speedup by enabling table
lookups for Gaussian evaluations. Likewise, in the
finite-state transducers (FSTs) used to represent the
language model, lexical, phonological, and class di-
phone constraints, quantizing the FST weights and
bit-packing not only compress the resulting binary
model files, but also reduce the processing time with
improved processor cache locality.
In the aforementioned TV, music, and weather do-
mains with a moderate vocabulary of a few thou-
sand words, the resulting PocketSUMMIT recog-
nizer performs in approximately real-time on 400-
600 MHz ARM processors, using a total of 2-4
MB of memory, including 1-2 MB for memory-
mapped model files. Compared with equivalent non-
quantized models, PocketSUMMIT achieves dra-
matic improvements in speed and memory while
maintaining comparable WER performance.
5.2 PocketTINA
Porting the TINA natural language parser to mobile
devices involved significant software engineering to
reduce the memory and computational requirements
of the core data structures and algorithms. TINA
utilizes a best-first search that explores thousands of
partial parses when processing an input utterance.
To efficiently manage memory allocation given the
unpredictability of pruning invalid parses (e.g. due
to subject-verb agreement), we implemented a mark
and sweep garbage collection mechanism. Com-
bined with a more efficient implementation of the
priority queue and the use of aggressive ?beam?
pruning, the resulting PocketTINA system provides
identical output as server-side TINA, but can parse
a 10-best recognition hypothesis list into the corre-
sponding meaning representation in under 0.1 sec-
onds, using about 2 MB of memory.
6 Rapid Dialogue System Development
Over the course of developing dialogue systems for
many domains, we have built generic natural lan-
guage understanding components that enable the
rapid development of flexible and natural spoken di-
alogue systems for novel domains. Creating such
prototype systems typically involves customizing
the following to the target domain: recognizer lan-
guage model, language understanding parser gram-
mar, context resolution rules, dialogue management
control script, and language generation rules.
Recognizer Language Model Given a new do-
main, we first identify a set of semantic classes
which correspond to the back-end application?s
database, such as artist, album, and genre. Ideally,
we would have a corpus of tagged utterances col-
lected from real users. However, when building pro-
totypes such as the one described here, little or no
training data is usually available. Thus, we create
a domain-specific context-free grammar to generate
a supplemental corpus of synthetic utterances. The
corpus is used to train probabilities for the natural
language parsing grammar (described immediately
below), which in turn is used to derive a class n-
gram language model (Seneff et al, 2003).
Classes in the language model which corre-
spond to contents of the database are marked as
dynamic, and are populated at runtime from the
database (Chung et al, 2004; Hetherington, 2005).
Database entries are heuristically normalized into
spoken forms. Pronunciations not in our 150,000
word lexicon are automatically generated (Seneff,
2007).
Parser Grammar The TINA parser uses a prob-
abilistic context-free grammar enhanced with sup-
port for wh-movement and grammatical agreement
constraints. We have developed a generic syntac-
tic grammar by examining hundreds of thousands
of utterances collected from real user interactions
with various existing dialogue systems. In addition,
we have developed libraries which parse and inter-
pret common semantic classes like dates, times, and
numbers. The grammar and semantic libraries pro-
vide good coverage for spoken dialogue systems in
database-query domains.
6
To build a grammar for a new domain, a devel-
oper extends the generic syntactic grammar by aug-
menting it with domain-specific semantic categories
and their lexical entries. A probability model which
conditions each node category on its left sibling and
parent is then estimated from a training corpus of
utterances (Seneff et al, 2003).
At runtime, the recognizer tags the hypothesized
dynamic class expansions with their class names,
allowing the parser grammar to be independent of
the database contents. Furthermore, each semantic
class is designated either as a semantic entity, or as
an attribute associated with a particular entity. This
enables the generation of a semantic representation
from the parse tree.
Dialogue Management & Language Generation
Once an utterance is recognized and parsed, the
meaning representation is passed to the context res-
olution and dialogue manager component. The con-
text resolution module (Filisko and Seneff, 2003)
applies generic and domain-specific rules to re-
solve anaphora and deixis, and to interpret frag-
ments and ellipsis in context. The dialogue man-
ager then interacts with the application back-end
and database, controlled by a script customized for
the domain (Polifroni et al, 2003). Finally, the
GENESIS module (Baptist and Seneff, 2000) ap-
plies domain-specific rules to generate a natural lan-
guage representation of the dialogue manager?s re-
sponse, which is sent to a speech synthesizer. The
dialogue manager also sends an update to the GUI,
so that, for example, the appropriate database search
results are displayed.
7 Mobile Design Challenges
Dialogue systems for mobile devices present a
unique set of design challenges not found in tele-
phony and desktop applications. Here we describe
some of the design choices made while developing
this prototype, and discuss their tradeoffs.
7.1 Client/Server Tradeoffs
Towards supporting network-less scenarios, we have
begun porting various natural language processing
components to mobile platforms, as discussed in
Section 5. Having efficient mobile implementations
further allows the natural language processing tasks
to be performed on either the mobile device or the
server. While building the prototype, we observed
that the Wi-Fi network performance can often be un-
predictable, resulting in erratic recognition latency
that occasionally exceeds on-device recognition la-
tency. However, utilizing the mobile processor for
computationally intensive tasks rapidly drains the
battery. Currently, the component architecture in the
prototype system is pre-configured. A more robust
implementation would dynamically adjust the con-
figuration to optimize the tradeoffs among network
use, CPU utilization, power consumption, and user-
perceived latency/accuracy.
7.2 Speech User Interface
As neither open-mic nor push-to-talk with automatic
endpoint detection is practical on mobile devices
with limited battery life, our prototype system em-
ploys a hold-to-talk hardware button for microphone
control. To guide users to speak commands only
while the button is depressed, a short beep is played
as an earcon both when the button is pushed and
released. Since users are less likely to talk over
short audio clips, the use of earcons mitigates the
tendency for users to start speaking before pushing
down the microphone button.
In the current system, media audio is played over
the TV speakers, whereas TTS output is sent to
the mobile device speakers. To reduce background
noise captured from the mobile device?s far-field mi-
crophone, the TV is muted while the microphone
button is depressed. Unlike telephony spoken di-
alogue systems where the recognizer has to con-
stantly monitor for barge-in, the use of a hold-to-
talk button significantly simplifies barge-in support,
while reducing power consumption.
7.3 Graphical User Interface
In addition to supporting interactive natural lan-
guage dialogues via the spoken user interface, the
prototype system implements a graphical user in-
terface (GUI) on the mobile device to supplement
the TV?s on-screen interface. To faciliate rapid pro-
totyping, we chose to implement both the mobile
and TV GUI using web pages with AJAX (Asyn-
chronous Javascript and XML) techniques, an ap-
proach we have leveraged in several existing mul-
timodal dialogue systems, e.g. (Gruenstein et al,
7
2006; McGraw and Seneff, 2007). The resulting in-
terface is largely platform-independent and allows
display updates to be ?pushed? to the client browser.
As many users are already familiar with the TV?s
on-screen interface, we chose to mirror the same in-
terface on the mobile device and synchronize the
selection cursor. However, unlike desktop GUIs,
mobile devices are constrained by a small display,
limited computational power, and reduced network
bandwidth. Thus, both the page layout and infor-
mation detail were adjusted for the mobile browser.
Although AJAX is more responsive than traditional
web technology, rendering large formatted pages?
such as the program guide grid?is often still un-
acceptably slow. In the current implementation, we
addressed this problem by displaying only the first
section of the content and providing a ?Show More?
button that downloads and renders the full content.
While browser-based GUIs expedite rapid prototyp-
ing, deployed systems may want to take advantage
of native interfaces specific to the device for more
responsive user interactions. Instead of limiting the
mobile interface to reflect the TV GUI, improved us-
ability may be obtained by designing the interface
for the mobile device first and then expanding the
visual content to the TV display.
7.4 Client/Server Communication
In the current prototype, communication between
the mobile device and the media server consists of
AJAX HTTP and XML-RPC requests. To enable
server-side ?push? updates, the client periodically
pings the server for messages. While such an im-
plementation provides a responsive user interface, it
quickly drains the battery and is not robust to net-
work outages resulting from the device being moved
or switching to power-saving mode. Reestablish-
ing connection with the server further introduces la-
tency. In future implementations, we would like to
examine the use of Bluetooth for lower power con-
sumption, and infrared for immediate response to
common controls and basic navigation.
8 Conclusions & Future Work
We have presented a prototype system that demon-
strates the feasibility of deploying a multimodal,
natural language interface on a mobile device for
browsing and managing one?s home media library.
In developing the prototype, we have experimented
with a novel role for a mobile device?that of a
speech-enabled remote control. We have demon-
strated a flexible natural language understanding ar-
chitecture, in which various processing stages may
be performed on either the server or mobile device,
as networking and processing power considerations
require.
While the mobile platform presents many chal-
lenges, it also provides unique opportunities.
Whereas desktop computers and TV remote controls
tend to be shared by multiple users, a mobile device
is typically used by a single individual. By collect-
ing and adapting to the usage data, the system can
personalize the recognition and understanding mod-
els to improve the system accuracy. In future sys-
tems, we hope to not only explore such adaptation
possibilities, but also study how real users interact
with the system to further improve the user interface.
Acknowledgments
This research is sponsored by the TParty Project,
a joint research program between MIT and Quanta
Computer, Inc.; and by Nokia, as part of a joint MIT-
Nokia collaboration. We are also thankful to three
anonymous reviewers for their constructive feed-
back.
References
A. Acero, N. Bernstein, R. Chambers, Y. C. Jui, X. Li,
J. Odell, P. Nguyen, O. Scholz, and G. Zweig. 2008.
Live search for mobile: Web services by voice on the
cellphone. In Proc. of ICASSP.
L. Baptist and S. Seneff. 2000. Genesis-II: A versatile
system for language generation in conversational sys-
tem applications. In Proc. of ICSLP.
A. Berglund and P. Johansson. 2004. Using speech and
dialogue for interactive TV navigation. Universal Ac-
cess in the Information Society, 3(3-4):224?238.
G. Chung, S. Seneff, C. Wang, and L. Hetherington.
2004. A dynamic vocabulary spoken dialogue inter-
face. In Proc. of INTERSPEECH, pages 327?330.
E. Filisko and S. Seneff. 2003. A context resolution
server for the GALAXY conversational systems. In
Proc. of EUROSPEECH.
B. Ford, J. Strauss, C. Lesniewski-Laas, S. Rhea,
F. Kaashoek, and R. Morris. 2006. Persistent personal
names for globally connected mobile devices. In Pro-
ceedings of the 7th USENIX Symposium on Operating
Systems Design and Implementation (OSDI ?06).
8
K. Fujita, H. Kuwano, T. Tsuzuki, and Y. Ono. 2003.
A new digital TV interface employing speech recog-
nition. IEEE Transactions on Consumer Electronics,
49(3):765?769.
J. Glass. 2003. A probabilistic framework for segment-
based speech recognition. Computer Speech and Lan-
guage, 17:137?152.
A. Gruenstein, S. Seneff, and C. Wang. 2006. Scalable
and portable web-based multimodal dialogue interac-
tion with geographical databases. In Proc. of INTER-
SPEECH.
I. L. Hetherington. 2005. A multi-pass, dynamic-
vocabulary approach to real-time, large-vocabulary
speech recognition. In Proc. of INTERSPEECH.
I. L. Hetherington. 2007. PocketSUMMIT: Small-
footprint continuous speech recognition. In Proc. of
INTERSPEECH, pages 1465?1468.
B. Johanson, A. Fox, and T. Winograd. 2002. The in-
teractive workspaces project: Experiences with ubiq-
uitous computing rooms. IEEE Pervasive Computing,
1(2):67?74.
M. Johnston, L. F. D?Haro, M. Levine, and B. Renger.
2007. A multimodal interface for access to content in
the home. In Proc. of ACL, pages 376?383.
J. Mazzola Paluska, H. Pham, U. Saif, C. Terman, and
S. Ward. 2006. Reducing configuration overhead with
goal-oriented programming. In PerCom Workshops,
pages 596?599. IEEE Computer Society.
J. Mazzola Paluska, H. Pham, U. Saif, G. Chau, C. Ter-
man, and S. Ward. 2008. Structured decomposition of
adapative applications. In Proc. of 6th IEEE Confer-
ence on Pervasive Computing and Communications.
I. McGraw and S. Seneff. 2007. Immersive second lan-
guage acquisition in narrow domains: A prototype IS-
LAND dialogue system. In Proc. of the Speech and
Language Technology in Education Workshop.
J. Nichols and B. A. Myers. 2006. Controlling home and
office appliances with smartphones. IEEE Pervasive
Computing, special issue on SmartPhones, 5(3):60?
67, July-Sept.
H.-J. Oh, C.-H. Lee, M.-G. Jang, and Y. K. Lee. 2007.
An intelligent TV interface based on statistical dia-
logue management. IEEE Transactions on Consumer
Electronics, 53(4).
T. Paek, M. Agrawala, S. Basu, S. Drucker, T. Kristjans-
son, R. Logan, K. Toyama, and A. Wilson. 2004. To-
ward universal mobile interaction for shared displays.
In Proc. of Computer Supported Cooperative Work.
J. Polifroni, G. Chung, and S. Seneff. 2003. Towards
the automatic generation of mixed-initiative dialogue
systems from web content. In Proc. EUROSPEECH,
pages 193?196.
T. Portele, S. Goronzy, M. Emele, A. Kellner, S. Torge,
and J. te Vrugt. 2003. SmartKom-Home - an ad-
vanced multi-modal interface to home entertainment.
In Proc. of INTERSPEECH.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. GALAXY-II: A reference architecture
for conversational system development. In Proc. IC-
SLP.
S. Seneff, C. Wang, and T. J. Hazen. 2003. Automatic in-
duction of n-gram language models from a natural lan-
guage grammar. In Proceedings of EUROSPEECH.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. Computational Lin-
guistics, 18(1):61?86.
S. Seneff. 2002. Response planning and generation in
the MERCURY flight reservation system. Computer
Speech and Language, 16:283?312.
S. Seneff. 2007. Reversible sound-to-letter/letter-to-
sound modeling based on syllable structure. In Proc.
of HLT-NAACL.
K. Wittenburg, T. Lanning, D. Schwenke, H. Shubin, and
A. Vetro. 2006. The prospects for unrestricted speech
input for TV content search. In Proc. of AVI?06.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao, T. J.
Hazen, and L. Hetherington. 2000. JUPITER: A
telephone-based conversational interface for weather
information. IEEE Transactions on Speech and Audio
Processing, 8(1), January.
9
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64?72,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Dialogue-Oriented Review Summary Generation for Spoken Dialogue 
Recommendation Systems 
 
 
Jingjing Liu, Stephanie Seneff, Victor Zue 
MIT Computer Science & Artificial Intelligence Laboratory 
32 Vassar Street, Cambridge, MA 02139 
{jingl, seneff, zue}@csail.mit.edu 
 
 
 
Abstract 
In this paper we present an opinion summari-
zation technique in spoken dialogue systems. 
Opinion mining has been well studied for 
years, but very few have considered its appli-
cation in spoken dialogue systems. Review 
summarization, when applied to real dialogue 
systems, is much more complicated than pure 
text-based summarization. We conduct a sys-
tematic study on dialogue-system-oriented 
review analysis and propose a three-level 
framework for a recommendation dialogue 
system. In previous work we have explored a 
linguistic parsing approach to phrase extrac-
tion from reviews. In this paper we will de-
scribe an approach using statistical models 
such as decision trees and SVMs to select the 
most representative phrases from the ex-
tracted phrase set. We will also explain how 
to generate informative yet concise review 
summaries for dialogue purposes. Experimen-
tal results in the restaurant domain show that 
the proposed approach using decision tree al-
gorithms achieves an outperformance of 13% 
compared to SVM models and an improve-
ment of 36% over a heuristic rule baseline. 
Experiments also show that the decision-tree-
based phrase selection model can achieve ra-
ther reliable predictions on the phrase label, 
comparable to human judgment. The pro-
posed statistical approach is based on do-
main-independent learning features and can 
be extended to other domains effectively. 
1 Introduction 
Spoken dialogue systems are presently available 
for many purposes, such as weather inquiry (Zue 
et al, 2000), bus schedules and route guidance 
(Raux et al, 2003), customer service (Gorin et al, 
1997), and train timetable inquiry (Eckert et al, 
1993). These systems have been well developed 
for laboratory research, and some have become 
commercially viable. 
The next generation of intelligent dialogue sys-
tems is expected to go beyond factoid question 
answering and straightforward task fulfillment, by 
providing active assistance and subjective recom-
mendations, thus behaving more like human 
agents. For example, an intelligent dialogue sys-
tem may suggest which airline is a better choice, 
considering cost, flight duration, take-off time, 
available seats, etc.; or suggest which digital cam-
era is the most popular among teenagers or highest 
rated by professional photographers; or which res-
taurant is a perfect spot for a semi-formal business 
meeting or a romantic date. 
Luckily, there are enormous amounts of reviews 
published by general users on the web every day. 
These are perfect resources for providing subjec-
tive recommendations and collective opinions. If 
there exists a systematic framework that harvests 
these reviews from general users, extracts the es-
sence from the reviews and presents it appropriate-
ly in human-computer conversations, then we can 
enable dialogue systems to behave like a human 
shopping assistant, a travel agent, or a local friend 
who tells you where to find the best restaurant.  
Summarization from online reviews, therefore, 
plays an important role for such dialogue systems. 
There have been previous studies on review analy-
sis for text-based summarization systems (Mei et 
al., 2007; Titov and McDonald, 2008a; Branavan 
et al, 2008). Mixture models and topic models are 
used to predict the underlying topics of each doc-
ument and generate a phrase-level summary. An 
aspect rating on each facet is also automatically 
64
learned with statistical models (Snyder and Barzi-
lay, 2007; Titov and McDonald, 2008b; Baccia-
nella et al, 2009). These approaches are all very 
effective, and the review databases generated are 
well presented.  
So the first thought for developing a recom-
mendation dialogue system is to use such a cate-
gorized summary in a table-lookup fashion. For 
example, a dialogue system for restaurant recom-
mendations can look up a summary table as exem-
plified in Table 1, and generate a response 
utterance from each row: ?Restaurant A has good 
service and bad food; restaurant B has good ser-
vice and good food; restaurant C has great service 
and nice atmosphere; restaurant D has poor service 
and reasonable price.?  
 
Restaurant Summary 
A Good service, bad food, 
B Good service, good food 
C Great service, nice atmosphere 
D Poor service, reasonable price 
Table 1. A partial table of categorization-based review 
summaries. 
 
Such a dialogue system is, however, not very 
informative. First of all, there is too much redun-
dancy. Long utterances repeated in the same pat-
tern on the same topic are quite boring, and the 
information density is very low. Second, such a 
summary is too coarse-grained to be helpful. A 
user querying a restaurant recommendation system 
expects more fine-grained information such as 
house specials, wine selections and choices on 
desserts rather than just general ?good food.?  
In contrast to a ?text? summarization system, the 
textual space in a dialogue turn is often very li-
mited. Speech is inherently serial, and it cannot be 
skipped and scanned easily. A dialogue system 
which speaks long diatribes in each single conver-
sation turn would likely not be well received. We 
believe that the best unit of review summary for 
dialogue purposes is short succinct phrases (e.g., 
?amazing sauce? and ?romantic ambiance?) which 
are catalogued into categories to answer high-level 
questions such as ?How is the food at this restau-
rant?? or ?What about the atmosphere?? Also, 
domain-specific phrases are greatly needed to 
generate correct answers to specific queries, such 
as ?Which restaurants have fresh sushi?? and 
?Show me restaurants that serve fantastic cock-
tails.? 
Thus, the generally used correlated topic mod-
els, although very effective in text-based systems, 
are not quite suitable for interactive dialogue sys-
tems. The missing piece is an HCI-oriented (hu-
man computer interaction), fine-grained, 
informative yet concise review summarization. A 
good recommendation system should be able to 
provide sufficient yet specific information to help 
users choose a restaurant, a movie or a consumer 
product. Figure 1 shows an example of a conversa-
tion with a good recommendation system, which 1) 
provides a generalized yet high-density review 
summary in human-friendly dialogue; 2) provides 
both a coarse-grained summary (e.g., ?authentic 
food?) and fine-grained information such as house 
specialties.  
 
U: Are there any French restaurants in Cambridge? 
S: There are five French restaurants in Cambridge. 
Most of them are on Backbay Street. And most of 
them have attentive service. 
U: Show me the ones that have nice ambiance. 
S: Two of them have nice ambiance, Pierrot Bistrot 
Francais and Coriander. 
U: How about the food? 
S: Pierrot Bistrot Francais has authentic food, 
fresh salad, and absolutely delicious spaghetti 
carbonara. The food at Coriander is below aver-
age. It has very greasy chicken tikka masala and 
dry bread.  
Figure 1. Example of a conversation with a good rec-
ommendation dialogue system (?U? denotes the user 
and ?S? denotes the dialogue system. The responses to 
the user queries are produced by our system and the 
actual dialogue was an illustration of system capacities). 
 
Therefore, the task of developing recommenda-
tion dialogue systems is decomposed into three 
problems: 1) how to extract context-related phras-
es, both coarse-grained and fine-grained, from 
online reviews; 2) how to select a representative 
set from the extracted phrases to create an infor-
mative yet concise dialogue-oriented summary 
database; 3) how to generate human-friendly di-
alogue responses from the review summary data-
base.  
To tackle these problems, we propose a three-
level framework. In previous work (Liu and Seneff, 
2009), we explored the first level by proposing a 
linguistic parse-and-paraphrase paradigm for re-
65
view phrase extraction. In this paper, we address 
the second problem: dialogue-oriented review 
summary generation. We propose an automatic 
approach to classifying high/low informative 
phrases using statistical models. Experiments con-
ducted on a restaurant-domain dataset indicate that 
the proposed approach can predict phrase labels 
consistently with human judgment and can gener-
ate high-quality review summaries for dialogue 
purposes.  
The rest of the paper is organized as follows: 
Section 2 gives an overview of the three-level 
framework for recommendation dialogue systems. 
In Section 3, we explain the proposed approach to 
dialogue-oriented review summary generation. 
Section 4 provides a systematic evaluation of the 
proposed approach, and Section 5 gives a further 
discussion on the experimental results. Section 6 
summarizes the paper as well as pointing to future 
work. 
2 System Overview 
The three-level framework of a review-summary-
based recommendation dialogue system is shown 
in Figure 2. The bottom level is linguistic phrase 
extraction. In previous work (Liu and Seneff, 
2009), we employed a probabilistic lexicalized 
grammar to parse review sentences into a hierar-
chical representation, which we call a linguistic 
frame. From the linguistic frames, phrases are ex-
tracted by capturing a set of adjective-noun rela-
tionships. Adverbs and negations conjoined with 
the adjectives are also captured. We also calcu-
lated a numerical score for sentiment strength for 
each adjective and adverb, and further applied a 
cumulative offset model to assign a sentiment 
score to each phrase. 
The approach relies on linguistic features that 
are independent of frequency statistics; therefore it 
can retrieve very rare phrases such as ?very greasy 
chicken tikka masala? and ?absolutely delicious 
spaghetti carbonara?, which are very hard to derive 
from correlated topic models. Experimental results 
showed that the linguistic paradigm outperforms 
existing methods of phrase extraction which em-
ploy shallow parsing features (e.g., part-of-speech). 
The main contribution came from the linguistic 
frame, which preserves linguistic structure of a 
sentence by encoding different layers of semantic 
dependencies. This allows us to employ more so-
phisticated high-level linguistic features (e.g., long 
distance semantic dependencies) for phrase extrac-
tion. 
However, the linguistic approach fails to distin-
guish highly informative and relevant phrases 
from uninformative ones (e.g., ?drunken husband?, 
?whole staff?). To apply these extracted phrases 
within a recommendation dialogue system, we 
have to filter out low quality or irrelevant phrases 
and maintain a concise summary database. This is 
the second level: dialogue-oriented review sum-
mary generation.  
 
 
 
Figure 2. Three-level framework of review-based rec-
ommendation dialogue systems. 
 
The standard of highly informative and relevant 
phrases is a very subjective problem. To gain in-
sights on human judgment on this, the first two 
authors separately labeled a set of review-related 
phrases in a restaurant domain as ?good? and ?bad? 
summary phrases. We surveyed several subjects, 
all of whom indicated that, when querying a dialo-
gue system for information about a restaurant, 
they care much more about special dishes served 
in this restaurant than generic descriptions such as 
?good food.? This knowledge informed the annota-
tion task: to judge whether a phrase delivered by a 
dialogue recommendation system would be help-
66
ful for users to make a decision. Surprisingly, al-
though this is a difficult and subjective problem, 
the judgment from the two annotators is substan-
tially consistent. By examining the annotations we 
observed that phrases such as ?great value? and 
?good quality? are often treated as ?uninformative? 
as they are too common to be representative for a 
particular product, a restaurant or a movie. Phrases 
with neutral sentiment (e.g., ?green beans? and 
?whole staff?) are often considered as uninforma-
tive too. Phrases on specific topics such as house 
specialties (e.g., ?absolutely delicious spaghetti 
carbonara?) are what the annotators care about 
most and are often considered as highly relevant, 
even though they may have only been seen once in 
a large database.  
Driven by these criteria, from each phrase we 
extract a set of statistical features such as uni-
gram/bigram probabilities and sentiment features 
such as sentiment orientation degree of the phrase, 
as well as underlying semantic features (e.g., 
whether the topic of the phrase fits in a domain-
specific ontology). Classification models such as 
SVMs and decision tree algorithms are then 
trained on these features to automatically classify 
high/low informative phrases. Phrases identified 
as ?good? candidates are further pruned and cata-
logued to create concise summaries for dialogue 
purposes. 
After generating the review summary database, 
the third level is to modify the response generation 
component in dialogue systems to create genera-
lized and interactive conversations, as exemplified 
in Figure 1. The utterance from users is piped 
through speech recognition and language under-
standing. The meaning representation is then sent 
to the dialogue management component for re-
view-summary database lookup. A response is 
then generated by the language generation compo-
nent, and a speech utterance is generated by the 
synthesizer and sent back to the user. The dialogue 
system implementation is beyond the scope of this 
paper and will be discussed later in a separate pa-
per. 
3 Dialogue-oriented Review Summary 
Generation 
Given an inquiry from users, the answer from a 
recommendation system should be helpful and 
relevant. So the first task is to identify a phrase as 
?helpful? or not. The task of identifying a phrase as 
informative and relevant, therefore, is defined as a 
classification problem: 
 =  ? ? #? = ?  %#%&%=1          (1) 
where y is the label of a phrase, assigned as ?1? if 
the phrase is highly informative and relevant, and 
?-1? if the phrase is uninformative. #? is the feature 
vector extracted from the phrase, and  ?  is the 
coefficient vector.  
We employ statistical models such as SVMs 
(Joachims, 1998) and decision trees (Quinlan, 
1986) to train the classification model. For model 
learning, we employ a feature set including statis-
tical features, sentiment features and semantic 
features.  
Generally speaking, phrases with neutral senti-
ment are less informative than those with strong 
sentiment, either positive or negative. For example, 
?fried seafood appetizer?, ?baked halibut?, ?elec-
tronic bill? and ?red drink? do not indicate whether 
a restaurant is worth trying, as they did not express 
whether the fried seafood appetizer or the baked 
halibut are good or bad. Therefore, we take the 
sentiment score of each phrase generated from a 
cumulative offset model (Liu and Seneff, 2009) as 
a sentiment feature. Sentiment scores of phrases 
are exemplified in Table 2 (on a scale of 1 to 5).  
 
Phrase Sc. Phrase Sc. 
really welcoming 
atmosphere 
4.8 truly amazing flavor 4.6 
perfect portions  4.4 very tasty meat 4.3 
busy place 3.1 typical Italian restaurant 3.1 
a little bit high 
price 
2.2 pretty bad soup 1.8 
sloppy service 1.8 absolute worst service 1.4 
Table 2. Examples of sentiment scores of phrases. 
 
We also employ a set of statistical features for 
model training, such as the unigram probability of 
the adjective in a phrase, the unigram probability 
of the noun in a phrase, the unigram probability of 
the phrase and the bigram probability of the adjec-
tive-noun pair in a phrase.  
Statistical features, however, fail to reveal the 
underlying semantic meaning of phrases. For ex-
ample, phrases ?greasy chicken tikka masala? and 
?drunken husband? have the same n-gram proba-
bilities in our corpus (a single observation), but 
67
they should certainly not be treated as the same. 
To capture the semantic meanings of phrases, we 
first cluster the topics of phrases into generic se-
mantic categories. The language-model based al-
gorithm is given by: 
 
        '(() | (%) = ? '(() |+) ? '(+|(%)+?.  
                 =  ? '(+ ,())'(+) ?
'(+ ,(%)
'((%)+?.
  
                 =  1'((%)
? 1
'(+) ? '(+, ()) ? '(+, (%)+?.          (2) 
where A represents the set of all the adjectives in 
the corpus. We select a small set of initial topics 
with the highest frequency counts (e.g., ?food?, 
?service? and ?atmosphere?). For each of the other 
topics tc  (e.g., ?chicken?, ?waitress? and ?d?cor?), 
we calculate its similarity with each initial topic (% 
based on the bigram probability statistics. For 
those topics with conditional probability higher 
than a threshold for an initial topic (%, we assign 
them to the cluster of (%. We use this as a semantic 
feature, e.g., whether the topic of a phrase belongs 
to a generic semantic category. Table 3 gives some 
clustering examples. 
 
Category Relevant Topics 
food 
appetizer, beer, bread, fish, fries, ice 
cream, margaritas, menu, pizza, pasta, 
rib, roll, sauce, seafood, sandwich, 
steak, sushi, dessert, cocktail, brunch 
service waiter, staff, management, server, hostess, chef, bartender, waitstaff 
atmosphere d?cor, ambiance, music, vibe, setting, environment, crowd 
price bill, pricing, prices 
Table 3. Topic to semantic category clustering. 
 
This language-model-based method relies on 
bigram probability statistics and can well cluster 
highly frequent topics. Categories such as ?service? 
and ?atmosphere? contain very limited related top-
ics, most of which have high frequencies (e.g., 
?waiter?, ?staff?, ?ambiance? and ?vibe?). The cate-
gory ?food?, however, is very domain-specific and 
contains a very large vocabulary, from generic 
sub-categories such as ?sushi?, ?dessert? and 
?sandwich? as shown in the examples, to specific 
courses such as ?bosc pear bread pudding? and 
?herb roasted vermont pheasant wine cap mu-
shrooms?. These domain-specific topics have very 
low frequencies, yet they are very relevant and 
valuable. But many of them are discarded by the 
clustering. It would be a similar case in other do-
mains. For example, consumer products, movies 
and books all have domain-independent semantic 
categories (e.g., ?price? and ?released date?) and 
domain-specific categories (e.g., technical features 
of consumer products, casts of movies and authors 
of books). 
To recover these context-relevant topics, we 
employ domain context relations such as a con-
text-related ontology. A context-related ontology 
can be constructed from structured web resources 
such as online menus of restaurants, names of ac-
tors and actresses from movie databases, and spe-
cifications of products from online shops. An 
example of a partial online menu of a restaurant is 
shown in Figure 3. From these structured web re-
sources, we can build up a hierarchical ontology, 
based on which a set of semantic features can be 
extracted (e.g., whether a phrase contains a course 
name, or an actress?s name, or a dimension of 
technical features of a consumer product).  
 
Entree 
Roasted Pork Loin Wrapped In Bacon with watermelon and 
red onion salad spicy honey-mustard bbq sauce 
Spicy Halibut And Clam Roast with bacon braised greens, 
white beans and black trumpet mushrooms 
Parmesan and Caramelized Shallot Wrapper Style Ravi-
oli turnip greens and white truffle oil 
Herb Roasted Vermont Pheasant Wine Cap Mushrooms, 
Pearl Onions and Fava Beans 
Dessert 
Chocolate Tasting Plate of white chocolate bombe milk choc-
olate creme brul?e and dark chocolate flourless cake  
White Fruit Tasting Plate of warm apple strudel butterscotch, 
Bosc Pear bread pudding and toasted coconut panna cotta  
 
Entr?e Pork loin, bacon, watermelon, red onion 
salad, honey, mustard, bbq sauce 
Dessert  Chocolate, milk, cr?me brulee, cake 
Figure 3. Example of a partial online menu and an ex-
emplary ontology derived. 
 
After the classification, phrases identified as 
?highly informative and relevant? are clustered 
into different aspects according to the semantic 
category clustering and the hierarchical ontology. 
An average sentiment score for each aspect is then 
calculated:  
+/0(1() =
? 233?41
|41|
     (3) 
68
where 1(  represents the aspect s of entry t (e.g., a 
restaurant, a movie, or a consumer product), 41  
represents the set of phrases in the cluster of as-
pect s, and 23  represents the sentiment score of 
phrase j in the cluster. 
The set of phrases selected for one entry may 
come from several reviews on this single entry, 
and many of them may include the same noun 
(e.g., ?good fish?, ?not bad fish? and ?above-
average fish? for one restaurant). Thus, the next 
step is multi-phrase redundancy resolution. We 
select the phrase with a sentiment score closest to 
the average score of its cluster as the most repre-
sentative phrase on each topic:  
5 = +265%&3?4%(|23 ? +/0(1()|)    (4) 
where +/0(1()  represents the average sentiment 
score of aspect 1, 4%  represents the set of phrases 
on the same topic %  in the cluster 1 , and 23  
represents the sentiment score of phrase 3.  
This sequence of topic categorization, ontology 
construction, phrase pruning and redundancy eli-
mination leads to a summary database, which can 
be utilized for dialogue generation in spoken rec-
ommendation systems. A review summary data-
base entry generated by the proposed approaches 
is exemplified in Figure 4. 
 
{ restaurant "dali restaurant and tapas bar" 
     :atmosphere ( "wonderful evening", "cozy atmos- 
phere", "fun decor", "romantic date" ) 
     :atmosphere_rating "4.1" 
     :food ( "very fresh ingredients",  "tasty fish", 
"creative dishes",  "good sangria" ) 
     :food_rating "3.9"         
     :service ( "fast service" ) 
     :service_rating "3.9"    
     :general ("romantic restaurant","small space" ) 
     :general_rating "3.6"                 } 
Figure 4. Example of a review summary database entry 
generated by the proposed approaches. 
4 Experiments 
In this project, we substantiate the proposed ap-
proach in a restaurant domain for our spoken di-
alogue system (Gruenstein and Seneff, 2007), 
which is a web-based multimodal dialogue system 
allowing users to inquire about information about 
restaurants, museums, subways, etc. We harvested 
a data collection of 137,569 reviews on 24,043 
restaurants in 9 cities in the U.S. from an online 
restaurant evaluation website1. From the dataset, 
857,466 sentences were subjected to parse analysis; 
and a total of 434,372 phrases (114,369 unique 
ones) were extracted from the parsable subset 
(78.6%) of the sentences.  
Most pros/cons consist of well-formatted phras-
es; thus, we select 3,000 phrases extracted from 
pros/cons as training data. To generate a human 
judgment-consistent training set, we manually la-
bel the training samples with ?good? and ?bad? la-
bels. We then randomly select a subset of 3,000 
phrases extracted from review texts as the test set 
and label the phrases. The kappa agreement be-
tween two sets of annotations is 0.73, indicating 
substantial consistency. We use the two annotation 
sets as the ground truth. 
To extract context-related semantic features, we 
collect a large pool of well-formatted menus from 
an online resource2, which contains 16,141 restau-
rant menus. Based on the hierarchical structure of 
these collected menus, we build up a context-
related ontology and extract a set of semantic fea-
tures from the ontology, such as whether the topic 
of a phrase is on category-level (e.g., ?entr?e?, 
?dessert?, ?appetizers?, ?salad?), whether the topic 
is on course-level (e.g., ?Roasted Pork Loin?, ?Spi-
cy Halibut and Clam Roast?), and whether the top-
ic is on ingredient-level (e.g., ?beans?, ?chicken?, 
?mushrooms?, ?scallop?).  
We employ the three types of features as afore-
mentioned to train the SVMs and the decision tree 
models. To select the most valuable features for 
model training, we conducted a set of leave-one-
feature-out experiments for both the SVMs and the 
decision tree models. We found that all the fea-
tures except the adjective unigram probability 
contribute positively to model learning. From fur-
ther data analysis we observed that many phrases 
with popular adjectives have context-unrelated 
nouns, which makes the adjective unigram proba-
bility fail to become a dominant factor for phrase 
relevance. Using the adjective unigram probability 
as a learning feature will mislead the system into 
trusting an adjective that is common but has a poor 
bigram affinity to the noun in the phrase. Thus, we 
eliminate this feature for both the SVMs and the 
decision tree learning. 
                                                          
1 http://www.citysearch.com 
2 http://www.menupages.com  
69
 To evaluate the performance of the classifica-
tion models, we take a set of intuitively motivated 
heuristic rules as the baseline. Figure 5 gives the 
pseudo-code of the heuristic rule algorithm, which 
uses variations of all the features except the uni-
gram probability of adjectives. 
  
If(sentiment score of the phrase exists) 
if(sentiment score is within neutral range) label=-1; 
else    
if(phrase appeared in the training data) 
      if((3<frequency of phrase < 100))   label = 1; 
          else 
            if(frequency of phrase >= 100)   label = -1; 
              else    if(topic belongs to ontology)  label = 1; 
                        else   label = -1; 
       else 
            if(topic belongs to ontology)   label = 1; 
            else   label = -1; 
else 
if(phrase appeared in the training data) 
           if((3<frequency of phrase < 100))    
if(topic belongs to ontology)  label = 1; 
                   else   label = -1; 
           else 
              if(frequency of phrase >= 100)   label = -1; 
              else 
                   if(topic belongs to ontology)  label = 1; 
                   else   if(frequency of noun > 100) label = 1; 
                            else   label = -1; 
    else 
         if(topic belongs to ontology)  label = 1; 
         else     if(frequency of noun > 100)   label = 1; 
                    else   label = -1;                 
Figure 5. Pseudo-code of the heuristic rule algorithm. 
 
The performance of classification by different 
models is shown in Table 4. Although the heuris-
tic rule algorithm is complicated and involves hu-
man knowledge, the statistical models trained by 
SVMs and the decision tree algorithms both out-
perform the baseline significantly. The SVM mod-
el outperforms the baseline by 10.5% and 11.9% 
on the two annotation sets respectively. The deci-
sion tree model outperforms the baseline by 16.4% 
and 23.2% (average relative improvement of 36%), 
and it also outperforms the SVM model by 5.9% 
and 11.3% (average relative improvement of 13%). 
The classification model using the decision tree 
algorithm can achieve a precision of 77.9% and 
74.5% compared with the ground truth, which is 
quite comparable to human judgment (the preci-
sion of one annotation set based on the other is 
74%). This shows that the decision tree model can 
predict phrase labels as reliably as human judg-
ment. 
 
 Baseline SVM Decision tree 
Annotation 1 61.5% 72.0% 77.9% 
Annotation 2 51.3% 63.2% 74.5% 
Table 4. Precision of phrase classification using the 
heuristic rule baseline, the SVM model, and the deci-
sion tree algorithm. 
 
To gain further insight on the contributions of 
each feature set to the decision tree learning, Table 
5 gives the experimental results on leaving each 
feature out of model training. As shown, without 
semantic features, the precision is 70.6% and 65.4% 
on the two annotation sets, lower by 7.3% and 9.1% 
than the case of training the model with all the 
features (77.9% and 74.5%). This shows that the 
semantic features significantly contribute to the 
decision tree learning. 
 
Feature set A1 A2 
all features  77.9% 74.5% 
without bigram probability 
of adjective-noun pair 
56.6%  
(-21.3%) 
63.9%  
(-10.6%) 
without unigram probability 
of the phrase 
57.6%  
(-20.3%) 
64.3%  
(-10.2%) 
without unigram probability 
of the noun 
59.8%  
(-18.1%) 
67.8%  
(-6.7%) 
without sentiment score of 
the phrase 
63.4%  
(-14.5%) 
66.6%  
(-7.9%) 
without underlying semantic 
features  
70.6%  
(-7.3%) 
65.4%  
(-9.1%) 
Table 5. Performance of the decision tree model by 
leaving each feature out of model training (?A1? and 
?A2? represent the annotation set 1 and 2 respectively). 
 
The experimental results also show that the fea-
ture of bigram probability of the adjective-noun 
pair contributes the most to the model learning. 
Without this feature, the precision drops by 21.3% 
and 10.6%, reaching the lowest precision among 
all the leave-one-out experiments. This confirms 
our observation that although a single adjective is 
not dominant, the pair of the adjective and the 
noun that co-occurs with it plays an important role 
in the classification.  
The sentiment of phrases also plays an impor-
tant role. Without sentiment features, the precision 
70
drops to 63.4% and 66.6% respectively on the two 
annotations, decreasing by 14.5% and 7.9%. This 
shows that the sentiment features contribute sig-
nificantly to the classification.  
5 Discussions 
Experimental results show that the decision tree 
algorithm outperforms the SVMs on this particular 
classification problem, and it outperforms the heu-
ristic rule baseline significantly. Thus, although 
the identification of informativeness and relevance 
of phrases is a rather subjective problem, which is 
difficult to predict using only human knowledge, it 
can be well defined by decision trees. Part of the 
reason is that the decision tree algorithm can make 
better use of a combination of Boolean value fea-
tures (e.g., whether a topic belongs to a context-
related ontology) and continuous value features. 
Also, as the phrase classification task is very sub-
jective, it is very similar to a ?hierarchical if-else 
decision problem? in human cognition, where de-
cision tree algorithms can fit well. Figure 6 shows 
a partial simplified decision tree learned from our 
model, which can give an intuitive idea of the de-
cision tree models. 
6 Related Work 
Sentiment classification and opinion mining have 
been well studied for years. Most studies have fo-
cused on text-based systems, such as document-
level sentiment classification and sentence-level 
opinion aggregation (Turney, 2002; Pang et al, 
2002; Dave et al, 2003; Hu and Liu, 2004; Popes-
cu and Etzioni, 2005; Wilson et al, 2005; Zhuang 
et al, 2006; Kim and Hovy, 2006).  
There was a study conducted by Carenini et al 
in 2006, which proposed a combination of a sen-
tence extraction-based approach and a language 
generation-based approach for summarizing eva-
luative arguments. In our work, we utilize a lower-
level phrase-based extraction approach, which uti-
lizes high level linguistic features and syntactic 
structure to capture phrase patterns.  
There was also a study on using reviews to gen-
erate a dictionary of mappings between semantic 
representations and realizations of concepts for 
dialogue systems (Higashinaka et al, 2006; Higa-
shinaka, 2007). They also used the association 
between user ratings and reviews to capture se-
mantic-syntactic structure mappings. A set of fil-
tering rules was manually created to eliminate 
low-quality mappings. In our approach, we use an 
automatic approach to classifying high/low infor-
mative phrases. The learning features are domain-
independent with no hand-crafted rules, and can 
be extended to other domains effortlessly.  
7 Conclusions 
In this paper we proposed a three-level framework 
for review-based recommendation dialogue sys-
tems, including linguistic phrase extraction, dialo-
gue-oriented review summary generation, and 
human-friendly dialogue generation. The contribu-
tions of this paper are three-fold: 1) it identified 
and defined the research goal of utilizing opinion 
summarization for real human-computer conversa-
tion; 2) it formulated an evaluation methodology 
for high-density review summary for dialogue 
purposes; 3) it proposed an approach to automatic 
classification of high/low informative phrases us-
ing a decision tree model. Experimental results 
showed that the decision tree model significantly 
outperforms a heuristic rule baseline and the SVM 
model, and can resolve the phrase classification 
problem comparably to humans consistently. 
Future work will focus on: 1) applying the sen-
timent scoring model to noun/verb sentiment as-
sessment; 2) application of the review summary 
generation approach in other domains and other 
languages; 3) data collection on user engagement 
with our dialogue systems involving review-
summary evaluation. 
 
 
Figure 6. A partial simplified decision tree learned from 
our model. 
  
71
References  
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet Rating of Product Reviews. 
In Proceedings of European Conference on Informa-
tion Retrieval. 
S.R.K. Branavan, Harr Chen, Jacob Eisenstein, and 
Regina Barzilay. 2008. Learning document-level 
semantic properties from free-text annotations. In 
Proc. of ACL. 
Giuseppe Carenini, Raymond Ng, and Adam Pauls. 
2006. Multi-Document Summarization of Evaluative 
Text. In Proceedings of the Conference of the Euro-
pean Chapter of the Association for Computational 
Linguistics. 
Kushal Dave, Steve Lawrence, and David M. Pennock. 
2003. Mining the peanut gallery: opinion extraction 
and semantic classification of product reviews. In 
Proceedings of the International Conference on 
World Wide Web. 
W. Eckert, T. Kuhn, H. Niemann, S. Rieck, A. Scheuer, 
and E. G. Schukat-talamazzini. 1993. A Spoken Di-
alogue System for German Intercity Train Timetable 
Inquiries. In Proc. European Conf. on Speech Tech-
nology. 
Alexander Gruenstein and Stephanie Seneff. 2007. Re-
leasing a Multimodal Dialogue System into the 
Wild: User Support Mechanisms. In Proceedings of 
the 8th SIGdial Workshop on Discourse and Dialo-
gue, Antwerp, pages 111-119. 
A. L. Gorin, G. Riccardi and J. H. Wright. 1997. ?How 
may I help you?? Speech Communication, vol. 23, 
pp. 113?127.  
Ryuichiro Higashinaka, Rashmi Prasad and Marilyn 
Walker. 2006. Learning to Generate  
Naturalistic Utterances Using Reviews in Spoken 
Dialogue Systems. In Proceedings of COLING-ACL.  
Ryuichiro Higashinaka, Marilyn Walker and Rashmi 
Prasad. 2007. An Unsupervised Method  
for Learning Generation Dictionaries for Spoken Di-
alogue Systems by Mining User Reviews.  
Journal of ACM Transactions on Speech and Lan-
guage Processing. 
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 2004 
ACM SIGKDD international conference on Know-
ledge Discovery and Data mining. 
S.M. Kim and E.H. Hovy. 2006. Identifying and Ana-
lyzing Judgment Opinions. In Proc. of HLT/NAACL. 
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Para-
digm. In proceedings of EMNLP. 
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, 
and ChengXiang Zhai. 2007. Topic Sentiment Mix-
ture: Modeling Facets and Opinions in Weblogs. In 
Proc. of WWW. 
Bo Pang, Lillian Lee, and S. Vaithyanathan. 2002. 
Thumbs up? Sentiment classification using machine 
learning techniques. In Proceedings of EMNLP. 
A.M. Popescu and O. Etzioni. 2005. Extracting product 
features and opinions from reviews. In Proceedings 
of EMNLP. 
JR Quinlan, 1986. Induction of decision trees. Machine 
learning, Springer-Netherlands. 
A. Raux, B. Langner, A. Black, and M. Eskenazi. 2003. 
LET'S GO: Improving Spoken Dialog Systems for 
the Elderly and Non-natives. In Proc. Eurospeech. 
Benjamin Snyder and Regina Barzilay. 2007. Multiple 
Aspect Ranking using the Good Grief Algorithm. In 
Proceedings of NAACL-HLT. 
Ivan Titov and Ryan McDonald. 2008a. Modeling On-
line Reviews with Multi-Grain Topic Models. In 
Proc. of WWW. 
Ivan Titov and Ryan McDonald. 2008b. A Joint Model 
of Text and Aspect Ratings for Sentiment Summari-
zation. In Proceedings of the Annual Conference of 
the Association for Computational Linguistics. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Sentiment orientation applied to unsupervised classi-
fication of reviews. In Proceedings of the Annual 
Conference of the Association for Computational 
Linguistics. 
T. Joachims. 1998. Text categorization with support 
vector machines: Learning with many relevant fea-
tures. In Proc. of ECML, p. 137?142.  
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Senti-
ment Analysis. In Proc. of HLT/EMNLP. 
Victor Zue, Stephanie Seneff, James Glass, Joseph Po-
lifroni, Christine Pao, Timothy J. Hazen, and Lee 
Hetherington. 2000. JUPITER: A Telephone-Based 
Conversational Interface for Weather Information. In 
IEEE Transactions on Speech and Audio Processing, 
Vol. 8 , No. 1.  
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie 
review mining and summarization. In Proceedings of 
the 15th ACM international conference on Informa-
tion and knowledge management. 
72
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 83?86,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Utilizing Review Summarization in a Spoken Recommendation System 
 
 
Jingjing Liu, Stephanie Seneff and Victor Zue 
MIT Computer Science & Artificial Intelligence Laboratory 
Cambridge, U.S.A. 
{jingl,seneff,zue}@csail.mit.edu 
 
  
 
Abstract 
 
In this paper we present a framework for spoken rec-
ommendation systems. To provide reliable recom-
mendations to users, we incorporate a review summa-
rization technique which extracts informative opinion 
summaries from grass-roots users? reviews. The dia-
logue system then utilizes these review summaries to 
support both quality-based opinion inquiry and fea-
ture-specific entity search. We propose a probabilistic 
language generation approach to automatically creat-
ing recommendations in spoken natural language 
from the text-based opinion summaries. A user study 
in the restaurant domain shows that the proposed ap-
proaches can effectively generate reliable and helpful 
recommendations in human-computer conversations. 
 
1 Introduction 
Spoken dialogue systems are presently available 
for many purposes, such as flight reservations 
(Seneff and Polifroni, 2000), telephone calls 
routing (Gorin et al, 1997), and subway lookup 
(Johnston et al, 2002). Recently, we have been 
exploring a next generation of intelligent dia-
logue systems, which can behave like a human 
agent and provide proactive assistance and selec-
tive recommendations (e.g., highly-rated restau-
rants or hotels) to users.  
To enhance dialogue systems with intelligent 
services, we have to let the system ?grow? relia-
ble knowledge and intelligence. Luckily, there 
has recently been an explosive growth in the 
availability of public review sites (e.g., yelp.com, 
tripadvisor.com, etc.) which make a perfect re-
source for gathering collective opinions. In this 
paper, we will explore how to utilize summaries 
of public reviews to automatically generate rec-
ommendation-type conversations in spoken dia-
logue systems. An example of a conversation 
with our recommendation system is shown in 
Figure 1. 
 
U: Are there any American restaurants in Brighton? 
S: There are 61 American restaurants in Brighton. All of 
the rated restaurants have good or excellent ambiance. 
Many are on Washington Street, Commonwealth Avenue 
and Market Street. 
U: Show me the ones that have nice ambiance. 
S: I found 4 entries. Devlin's Restaurant and Bar is per-
fect for romantic date, Stockyard Restaurant has friendly 
atmosphere, Tasca Tapas Restaurant has excellent ambi-
ance, and Wings Express has friendly vibes. 
U: Which one has good martinis? 
S: Devlin's Restaurant and Bar serves American food. It 
has wonderful martinis, good wine, great fish. It is perfect 
for romantic date. Great place. 
Figure 1. A real conversation with our recom-
mendation dialogue system in the restaurant do-
main (?U? is the user and ?S? is the system). 
2 Dialogue Management 
In our previous work (Liu and Seneff, 2009; Liu 
et al, 2010) we proposed an approach to extract-
ing representative phrases and creating aspect 
ratings from public reviews. An example of an 
enhanced database entry in the restaurant domain 
is shown in Figure 2. Here, we use these ?sum-
mary lists? (e.g., ?:food?, ?:atmosphere?) as well 
as aspect ratings (e.g., ?:food_rating?) to address 
two types of recommendation inquires: ?feature-
specific? (e.g., asking for a restaurant that serves 
good martinis or authentic seafood spaghetti), 
and ?quality-based? (e.g., looking for restaurants 
with good food quality or nice ambiance). 
 
{q restaurant 
     :name "devlin?s restaurant and bar" 
     :atmosphere ("romantic date" "elegant decor") 
     :place ("great place") 
     :food ("wonderful martinis" "good wine" "great fish") 
     :atmosphere_rating "4.2" 
     :place_rating "4.2" 
     :food_rating "4.3" 
     :specialty ("martinis" "wine" "fish")     } 
Figure 2. A database entry in our system. 
83
2.1 Feature-specific Entity Search 
To allow the system to identify feature-related 
topics in users? queries, we modify the context-
free grammar in our linguistic parser by includ-
ing feature-specific topics (e.g., nouns in the 
summary lists) as a word class. When a feature-
specific query utterance is submitted by a user 
(as exemplified in Figure 3), our linguistic parser 
will generate a hierarchical structure for the ut-
terance, which encodes the syntactic and seman-
tic structure of the utterance and, especially, 
identifies the feature-related topics. A feature-
specific key-value pair (e.g., ?specialty: marti-
nis?) is then created from the hierarchical parsing 
structure, with which the system can filter the 
database and retrieve the entities that satisfy the 
constraints.  
 
Utterance 
?Are there any restaurants in Brighton that 
have good martinis?? 
 
Key-value 
pairs 
?topic: restaurant,  city: Brighton,  
  specialty: martinis? 
 
Database 
filters 
:specialty = ?martinis?  :city = ?Brighton? 
:entity_type  = ?restaurant? 
 
Figure 3. Procedure of feature-specific search. 
2.2 Quality-based Entity Search 
For quality-based questions, however, similar 
keyword search is problematic, as the quality of 
entities has variants of expressions. The assess-
ment of different degrees of sentiment in various 
expressional words is very subjective, which 
makes the quality-based search a hard problem.  
To identify the strength of sentiment in quali-
ty-based queries, a promising solution is to map 
textual expressions to scalable numerical scores. 
In previous work (Liu and Seneff, 2009), we 
proposed a method for calculating a sentiment 
score for each opinion-expressing adjective or 
adverb (e.g., ?bad?: 1.5, ?good?: 3.5, ?great?: 4.0, 
on a scale of 1 to 5). Here, we make use of these 
sentiment scores and convert the original key-
value pair to numerical values (e.g., ?great food? 
? ?food_rating: 4.0? as exemplified in Figure 
4). In this way, the sentiment expressions can be 
easily converted to scalable numerical key-value 
pairs, which will be used for filtering the data-
base by ?aspect ratings? of entities. As exempli-
fied in Figure 4, all the entities in the required 
range of aspect rating (i.e., ?:food_rating   4.0?) 
can be retrieved (e.g., the entity in Figure 2 with 
?food_rating = 4.3?). 
 
Utterance 
?Show me some american restaurants with 
great food? 
 
Key-value 
pairs 
?topic: restaurant, cuisine: american,  
property: food, quality: great? 
 
Converted 
k-v pairs 
?topic: restaurant, cuisine: american, 
food_rating: 4.0? 
 
Database 
filters 
:food_rating > ?4.0?  :cuisine = ?american? 
:entity_type =  ?restaurant? 
 
Figure 4. Procedure of qualitative entity search. 
3 Probabilistic Language Generation 
After corresponding entities are retrieved from 
the database based on the user?s query, the lan-
guage generation component will create recom-
mendations by expanding the summary lists of 
the retrieved database entries into natural lan-
guage utterances.  
Most spoken dialogue systems use predefined 
templates to generate responses. However, man-
ually defining templates for each specific linguis-
tic pattern is tedious and non-scalable. For ex-
ample, given a restaurant with ?nice jazz music, 
best breakfast spot, great vibes?, three templates 
have to be edited for three different topics (e.g., 
?<restaurant> plays <adjective> music?; ?<res-
taurant> is <adjective> breakfast spot?; ?<restau-
rant> has <adjective> vibes?). To avoid the hu-
man effort involved in the task, corpus-based 
approaches (Oh and Rudnicky, 2000; Rambow et 
al., 2001) have been developed for more efficient 
language generation. In this paper, we propose a 
corpus-based probabilistic approach which can 
automatically learn the linguistic patterns (e.g., 
predicate-topic relationships) from a corpus and 
generate natural sentences by probabilistically 
selecting the best-matching pattern for each top-
ic.  
The proposed approach consists of three stag-
es: 1) plant seed topics in the context-free gram-
mar; 2) identify semantic structures associated 
with the seeds; 3) extract association pairs of lin-
guistic patterns and the seeds, and calculate the 
probability of each association pair.  
First, we extract all the nouns and noun 
phrases that occur in the review summaries as the 
seeds. As aforementioned, our context-free 
grammar can parse each sentence into a hierar-
chical structure. We modify the grammar such 
that, when parsing a sentence which contains one 
of these seed topics, the parser can identify the 
seed as an ?active? topic (e.g., ?vibes?, ?jazz mu-
sic?, and ?breakfast spot?). 
84
The second stage is to automatically identify 
all the linguistic patterns associated with each 
seed. To do so, we use a large corpus as the re-
source pool and parse each sentence in the cor-
pus for linguistic analysis. We modify our parser 
such that, in a preprocessing step, the predicate 
and clause structures that are semantically related 
to the seeds will be assigned with identifiable 
tags. For example, if the subject or the comple-
ment of the clause (or the object of the predicate) 
is an ?active? topic (i.e., a seed), an ?active? tag 
will be automatically assigned to the clause (or 
the predicate). In this way, when examining syn-
tactic hierarchy of each sentence in the corpus, 
the system can encode all the linguistic patterns 
of clauses or predicate-topic relationships associ-
ated with the seeds with ?active? tags.  
Based on these tags, association pairs of ?ac-
tive? linguistic patterns and ?active? topics can 
be extracted automatically. For each seed topic, 
we calculate the probability of its co-occurrence 
with each of its associated patterns by: 
 
    (        |     )  
                        
?                         
    (1) 
 
where       is a seed topic, and          is 
every linguistic pattern associated with      . 
The probability of          for       is the 
percentage of the co-occurrences of          
and       among all the occurrences of       
in the corpus. This is similar to a bigram lan-
guage model. A major difference is that the lin-
guistic pattern is not necessarily the word adja-
cent to the seed. It can be a long distance from 
the seed with strong semantic dependencies, and 
it can be a semantic chunk of multiple words. 
The long distance semantic relationships are cap-
tured by our linguistic parser and its hierarchical 
encoding structure; thus, it is more reliable than 
pure co-occurrence statistics or bigrams. Figure 5 
shows some probabilities learned from a review 
corpus. For example, ?is? has the highest proba-
bility (0.57) among all the predicates that co-
occur with ?breakfast spot?; while ?have? is the 
best-match for ?jazz music?. 
 
Association pair Constituent Prob. 
?at? : ?breakfast spot? PP 0.07 
?is? : ?breakfast spot? Clause 0.57 
?for? : ?breakfast spot? PP 0.14 
?love?  : ?jazz music? VP 0.08 
?have? : ?jazz music? VP 0.23 
?enjoy?: ?jazz music? VP 0.08 
Figure 5.  Partial table of probabilities of associa-
tion pairs (VP: verb phrase; PP: preposition 
phrase).  
Given these probabilities, we can define pat-
tern selection algorithms (e.g., always select the 
pattern with the highest probability for each top-
ic; or rotates among different patterns from high 
to low probabilities), and generate response ut-
terances based on the selected patterns. The only 
domain-dependent part of this approach is the 
selection of the seeds. The other steps all depend 
on generic linguistic structures and are domain-
independent. Thus, this probabilistic method can 
be easily applied to generic domains for custom-
izing language generation. 
4 Experiments 
A web-based multimodal spoken dialogue sys-
tem, CityBrowser (Gruenstein and Seneff, 2007), 
developed in our group, can provide users with 
information about various landmarks such as the 
address of a museum, or the opening hours of a 
restaurant. To evaluate our proposed approaches, 
we enhanced the system with a review-summary 
database generated from a review corpus that we 
harvested from a review publishing web site 
(www.citysearch.com), which contains 137,569 
reviews on 24,043 restaurants.  
We utilize the platform of Amazon Mechani-
cal Turk (AMT) to conduct a series of user stud-
ies. To understand what types of queries the sys-
tem might potentially be handling, we first con-
ducted an AMT task by collecting restaurant in-
quiries from general users. Through this AMT 
task, 250 sentences were collected and a set of 
generic templates encoding the language patterns 
of these sentences was carefully extracted. Then 
10,000 sentences were automatically created 
from these templates for language model training 
for the speech recognizer.  
To evaluate the quality of recommendations, 
we presented the system to real users via custom-
ized AMT API (McGraw et al, 2010) and gave 
each subject a set of assignments to fulfill. Each 
assignment is a scenario of finding a particular 
restaurant, as shown in Figure 6. The user can 
talk to the system via a microphone and ask for 
restaurant recommendations.  
We also gave each user a questionnaire for a 
subjective evaluation and asked them to rate the 
system on different aspects. Through this AMT 
task we collected 58 sessions containing 270 ut-
terances (4.6 utterances per session on average) 
and 34 surveys. The length of the utterances var-
ies significantly, from ?Thank you? to ?Restau-
rants along Brattle Street in Cambridge with nice 
85
cocktails.? The average number of words per 
utterance is 5.3.  
 
 
Figure 6. Interface of our system in an AMT as-
signment. 
 
Among all the 58 sessions, 51 were success-
fully fulfilled, i.e., in 87.9% of the cases the sys-
tem provided helpful recommendations upon the 
user?s request and the user was satisfied with the 
recommendations. Among those seven failed 
cases, one was due to loud background noise, 
two were due to users? operation errors (e.g., 
clicking ?DONE? before finishing the scenario), 
and four were due to recognition performance.  
The user ratings in the 34 questionnaires are 
shown in Figure 7. On a scale of 0 (the center) to 
5 (the edge), the average rating is 3.6 on the eas-
iness of the system, 4.4 on the helpfulness of the 
recommendations, and 4.1 on the naturalness of 
the system response. These numbers indicate that 
the system is very helpful at providing recom-
mendation upon users? inquiries, and the re-
sponse from the system is present in a natural 
way that people could easily understand.  
 
 
Figure 7. Users? ratings from the questionnaires. 
 
The lower rating of ease of use is partially due 
to recognition errors. For example, a user asked 
for ?pancakes?, and the system recommended 
?pizza places? to him. In some audio clips rec-
orded, the background noise is relatively high. 
This may be due to the fact that some AMT 
workers work from home, where it can be noisy.   
5 Conclusions 
In this paper we present a framework for incor-
porating review summarization into spoken rec-
ommendation systems. We proposed a set of en-
tity search methods as well as a probabilistic lan-
guage generation approach to automatically cre-
ate natural recommendations in human-computer 
conversations from review summaries. A user 
study in the restaurant domain shows that the 
proposed approaches can make the dialogue sys-
tem provide reliable recommendations and can 
help general users effectively. 
Future work will focus on: 1) improving the 
system based on users? feedback; and 2) apply-
ing the review-based approaches to dialogue sys-
tems in other domains.  
 
Acknowledgments 
This research is supported by Quanta Computers, 
Inc. through the T-Party project.  
References  
Gorin, A., Riccardi, G., and Wright, J. H. 1997. How 
May I Help You? Speech Communications. Vol. 
23, pp. 113 ? 127.  
Gruenstein, A. and Seneff, S. 2007. Releasing a Mul-
timodal Dialogue System into the Wild: User Sup-
port Mechanisms. In Proc. the 8th SIGdial Work-
shop on Discourse and Dialogue, pp. 111?119.  
Johnston, M., Bangalore, S., Vasireddy, G., Stent, A., 
Ehlen, P., Walker, M., Whittaker, S., Maloor, P. 
2002. MATCH: An Architecture for Multimodal 
Dialogue Systems. In Proc. ACL, pp. 376 ? 383. 
Liu, J. and Seneff, S. 2009. Review sentiment scoring 
via a parse-and-paraphrase paradigm, In Proc. 
EMNLP, Vol. 1.  
Liu, J., Seneff, S. and Zue, V. 2010. Dialogue-
Oriented Review Summary Generation for Spoken 
Dialogue Recommendation Systems. In Proc. 
NAACL-HLT.  
McGraw, I., Lee, C., Hetherington, L., Seneff, S., 
Glass, J. 2010. Collecting Voices from the Cloud. 
In Proc. LREC.  
Oh, A.H. and Rudnicky, A.I. 2000. Stochastic Lan-
guage Generation for Spoken Dialogue Systems. In 
Proc. of ANLP-NAACL, pp. 27-32. 
Rambow, O., Bangalore, S., Walker, M. 2001. Natu-
ral Language Generation in Dialog Systems. In 
Proc. Human language technology research.  
Seneff, S. and Polifroni, J. 2000. Dialogue Manage-
ment in the Mercury Flight Reservation System. In 
Proc. Dialogue Workshop, ANLP-NAACL.  
0
1
2
3
4
5
1
2 3
4
5
6
7
8
9
10
11
12
13
14
15
1617
18
1920
21
22
23
24
25
26
27
28
29
30
31
32
33 34
Ease of use
Helpfulness
Naturalness
86
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 87?90,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Dialogue Management Based on Entities and Constraints  
 
 Yushi Xu  Stephanie Seneff 
Spoken Language Systems Group 
MIT Computer Science and Artificial Intelligence Laboratory 
 United States 
 {yushixu, seneff}@csail.mit.edu 
 
  
 
Abstract 
This paper introduces a new dialogue man-
agement framework for goal-directed conver-
sations. A declarative specification defines the 
domain-specific elements and guides the di-
alogue manager, which communicates with the 
knowledge sources to complete the specified 
goal. The user is viewed as another knowledge 
source. The dialogue manager finds the next 
action by a mixture of rule-based reasoning 
and a simple statistical model. Implementation 
in the flight-reservation domain demonstrates 
that the framework enables the developer to 
easily build a conversational dialogue system.  
1 Introduction 
Conversational systems can be classified into 
two distinct classes: goal-directed and casual 
chatting. For goal-directed systems, the system is 
usually more ?knowledgeable? than the user, and  
it attempts to satisfy user-specified goals. The 
system?s conversational strategies seek the most 
efficient path to reach closure and end the con-
versation (Smith, Hipp, & Biermann, 1995).  
    An essential commonality among different 
goal-directed applications is that, at the end of a 
successful conversation, the system presents the 
user with a ?goal? entity, be it a flight itinerary, a 
route path, or a shopping order. Different con-
versations result from different properties of the 
goal entities and different constraints set by the 
knowledge sources. The properties define the 
necessary and/or relevant information, such as 
flight numbers in the flight itinerary.  Constraints 
specify the means to obtain such information. 
For examples fields ?source?, ?destination? and 
?date? are required to search for a flight. Once 
the properties and constraints are known, dialo-
gue rules can easily map to dialogue actions. 
    This paper introduces a dialogue management 
framework for goal-directed conversation based 
on entity and knowledge source specification. 
The user is viewed as a collaborator with the di-
alogue manager, instead of a problem-raiser. The 
dialogue manager follows a set of definitions and 
constraints, and eventually realizes the goal enti-
ty. It also incorporates a simple statistical engine 
to handle certain decisions. 
2 Related Work 
In recent years, statistical methods have gained 
popularity in dialogue system research.  Partially 
Observable Markov decision processes have 
been the focus of a number of papers (Levin, 
Pieraccini, & Eckert, 1997; Scheffler & Young, 
2001; Frampton & Lemon, 2006; Williams & 
Young, 2007). These approaches turn the dialo-
gue interaction strategy into an optimization 
problem. The dialogue manager selects actions 
prescribed by the policy that maximizes the re-
ward function (Lemon & Pietquin, 2007). This 
machine learning formulation of the problem 
automates system development, thus freeing the 
developers from hand-coded rules. 
   Other researchers have continued research on 
rule-based frameworks, in part because they are 
easier to control and maintain. One common ap-
proach is to allow developers to specify the tasks, 
either using a conditioned sequential script (Zue, 
et al, 2000; Seneff, 2002), or using a task hie-
rarchy (Hochberg, Kambhatla, & Roukos, 2002). 
In (Bohus & Rudnicky, 2003)?s work, a tree of 
dialogue agents, each of which handles different 
dialogue actions, is specified to control the di-
alogue progress. The knowledge has also been 
specified either by first order logic (B?hler & 
Minker, 2005) or ontology information (Milward 
& Beveridge, 2004). 
3 Dialogue Manager 
Figure 1 illustrates the architecture of the pro-
posed dialogue management framework. Com-
87
munication with the dialogue manager (DM) is 
via ?E-forms? (Electronic forms), which consist 
of language-independent key-value pairs. The 
language understanding and language generation 
components mediate between the DM and vari-
ous knowledge sources (KS), including the user, 
to interpret the output from the KS and generate 
input that the KS can understand. Each KS han-
dles one or more sub-domains. For example, a 
date/time KS can resolve a date expression such 
as ?next Tuesday? to a unique date; a flight data-
base can provide flight information. The KSes 
are provided by the developer. They can be local 
(a library) or external (a separate executable).  
Within this architecture, the user is viewed as 
a special KS, who understands and speaks a nat-
ural language, so that the whole architecture is 
completely DM-centered, as shown in Figure 1. 
An external language understanding system 
parses the original input into an E-form, and an 
external language generation component con-
verts the output E-form into the desired natural 
language. Each particular communication with 
the user is analogous to other communications 
with the various KSes.  The user is always 
ranked the lowest in the priority list of the KSes, 
i.e., only when other knowledge sources cannot 
provide the desired information does the DM try 
to ask the user.  
 
Figure 1. System Framework. 
For example, in the flight reservation system, 
suppose the DM first tries to determine the 
source airport. If there exists a KS that contains 
this user?s home airport information, the DM will 
adopt it. If no other KS can provide the informa-
tion, the DM asks the user for the departure city. 
3.1 Entity-Based Specification  
Our framework uses an entity-based declarative 
domain specification. Instead of providing the 
action sequence in the domain, the developer 
provides the desired form of the goal entity, and 
the relationships among all relevant entities.  
    The specification is decomposed into two parts. 
The first part is the declaration of the knowledge 
sources. Each KS may contain one or more sub-
domains, and an associated ?nation? defines the 
language processing parameters. 
    The second part is the entity type definition. 
For a particular domain, there is one goal entity 
type, and an arbitrary number of other entity 
types, e.g., two entity types are defined in the 
flight reservation system: ?itinerary? and ?flight.?  
The definition of an entity type consists of a set 
of members, including their names, types and 
knowledge domain. A logical expression states 
the conditions under which the entity can be re-
garded as completed; e.g., a completed itinerary 
must contain one or more flights. The entity de-
finition can also include optional elements such 
as comparative/superlative modifiers or custo-
mized command-action and task-action map-
pings, described in more detail later. 
The entity-based specification has an advan-
tage over an action-based specification in two 
aspects. First, it is easier to define all the entities 
in a dialogue domain than to list all the possible 
actions, so the specification is more compact and 
readable. Secondly, the completion condition and 
the KS?s constraints capture the underlying mo-
tivation of the dialogue actions. 
 
Figure 2. The Main Loop of the DM. 
3.2 Dialogue Execution  
Similar to the Information-State-Update (Larsson 
& Traum, 2000) idea, the DM maintains an in-
ternal state space with all up-to-date information 
about the entities. It also keeps a task list tree 
with a root task ?complete goal.? In task execu-
tion, subtasks (child node) and/or subsequent 
(right sibling node) tasks are issued. Each time 
the left-most leaf task is executed, and when a 
task is completed, the DM checks all tasks and 
removes those that have been rendered obsolete.  
    Ten basic tasks are pre-defined in the DM, 
including complete_entity, inquire_ks, and some 
other tasks related to entity manipulation. A 
complete_entity task evaluates the completion 
 
Dialogue 
Manager 
Language 
Generation 
Language 
Understanding 
External KS Local KS User 
Domain Specification 
 E-form in 
Extract 
Information 
Issue New 
Tasks 
Retrieve Left-
Most Task 
Execute Task 
Remove 
Obsolete Tasks 
Pause? 
E-form out 
User Command? 
Y 
N 
Y 
N 
88
conditions and issues appropriate tasks if they 
are unmet. An inquire_ks task handles communi-
cation with the KSes, and issues subtasks if the 
query does not satisfy the constraints.  A default 
action associated with each task can be replaced 
by customized task-action mappings if needed. 
Figure 2 shows the main loop of the DM. The 
process loops until a ?pause? is signaled, which 
indicates to await the user?s spoken response. An 
example will be given in Section 4. 
3.3 Statistical Inference  
To cope with situations that rules cannot handle 
easily, the framework incorporates a simple sta-
tistical engine using a Space Vector Model.  It is 
designed only to support inference on specific 
small problems, for example, to decide when to 
ask the user for confirmation of a task. Models 
are built for each of the inference problems. The 
output label of a new data point is computed by 
weighting the labels of all existing data by their 
inverse distances to the new data point.   
    Equations (1) to (3) show the detailed math of 
the computation, where x is the new data point 
and dj is the j-th existing data point. ? is a fading 
coefficient which ranges from 0 and 1. ?, a cor-
rection weight, has a higher value for data points 
resulting from manual correction. ?(?)  is 1 when 
the two inputs are equal and 0 otherwise. sim(x, 
d) defines the similarity between the new data 
point and the existing data point. Function dis(?) 
indicates the distance for a particular dimension, 
which is specified by the developer. The weight 
for each dimension wi is proportional to the 
count of distinct values of the particular dimen-
sion c(Di) and the mutual information between 
the dimension and the output label.  = argmax ,  ? ,