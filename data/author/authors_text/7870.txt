Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 18?25,
Sydney, July 2006. c?2006 Association for Computational Linguistics
LEILA: Learning to Extract Information by Linguistic Analysis
Fabian M. Suchanek
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
suchanek@mpii.mpg.de
Georgiana Ifrim
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
ifrim@mpii.mpg.de
Gerhard Weikum
Max-Planck-Institute
for Computer Science
Saarbru?cken/Germany
weikum@mpii.mpg.de
Abstract
One of the challenging tasks in the con-
text of the Semantic Web is to automati-
cally extract instances of binary relations
from Web documents ? for example all
pairs of a person and the corresponding
birthdate. In this paper, we present LEILA,
a system that can extract instances of ar-
bitrary given binary relations from natu-
ral language Web documents ? without
human interaction. Different from previ-
ous approaches, LEILA uses a deep syn-
tactic analysis. This results in consistent
improvements over comparable systems
(such as e.g. Snowball or TextToOnto).
1 Introduction
1.1 Motivation
Search engines, question answering systems and
classification systems alike can greatly profit from
formalized world knowledge. Unfortunately, man-
ually compiled collections of world knowledge
(such as e.g. WordNet (Fellbaum, 1998)) often
suffer from low coverage, high assembling costs
and fast aging. In contrast, the World Wide Web
provides an enormous source of knowledge, as-
sembled by millions of people, updated constantly
and available for free. Since the Web data con-
sists mostly of natural language documents, a first
step toward exploiting this data would be to ex-
tract instances of given target relations. For exam-
ple, one might be interested in extracting all pairs
of a person and her birthdate (the birthdate-
relation), pairs of a company and the city of its
headquarters (the headquarters-relation) or
pairs of an entity and the concept it belongs to (the
instanceOf-relation). The task is, given a set
of Web documents and given a target relation, ex-
tracting pairs of entities that are in the target rela-
tion. In this paper, we propose a novel method for
this task, which works on natural language Web
documents and does not require human interac-
tion. Different from previous approaches, our ap-
proach involves a deep linguistic analysis, which
helps it to achieve a superior performance.
1.2 Related Work
There are numerous Information Extraction (IE)
approaches, which differ in various features:
? Arity of the target relation: Some systems are
designed to extract unary relations, i.e. sets of
entities (Finn and Kushmerick, 2004; Califf and
Mooney, 1997). In this paper we focus on the
more general binary relations.
? Type of the target relation: Some systems
are restricted to learning a single relation,
mostly the instanceOf-relation (Cimiano
and Vo?lker, 2005b; Buitelaar et al, 2004).
In this paper, we are interested in extracting
arbitrary relations (including instanceOf).
Other systems are designed to discover new
binary relations (Maedche and Staab, 2000).
However, in our scenario, the target relation is
given in advance.
? Human interaction: There are systems that re-
quire human intervention during the IE process
(Riloff, 1996). Our work aims at a completely
automated system.
? Type of corpora: There exist systems that can
extract information efficiently from formatted
data, such as HTML-tables or structured text
(Graupmann, 2004; Freitag and Kushmerick,
2000). However, since a large part of the Web
consists of natural language text, we consider in
this paper only systems that accept also unstruc-
tured corpora.
? Initialization: As initial input, some systems
require a hand-tagged corpus (J. Iria, 2005;
Soderland et al, 1995), other systems require
text patterns (Yangarber et al, 2000) or tem-
plates (Xu and Krieger, 2003) and again oth-
ers require seed tuples (Agichtein and Gravano,
2000; Ruiz-Casado et al, 2005; Mann and
Yarowsky, 2005) or tables of target concepts
(Cimiano and Vo?lker, 2005a). Since hand-
18
labeled data and manual text patterns require
huge human effort, we consider only systems
that use seed pairs or tables of concepts.
Furthermore, there exist systems that use the
whole Web as a corpus (Etzioni et al, 2004) or that
validate their output by the Web (Cimiano et al,
2005). In order to study different extraction tech-
niques in a controlled environment, however, we
restrict ourselves to systems that work on a closed
corpus for this paper.
One school of extraction techniques concen-
trates on detecting the boundary of interesting en-
tities in the text, (Califf and Mooney, 1997; Finn
and Kushmerick, 2004; Yangarber et al, 2002).
This usually goes along with the restriction to
unary target relations. Other approaches make
use of the context in which an entity appears
(Cimiano and Vo?lker, 2005a; Buitelaar and Ra-
maka, 2005). This school is mostly restricted to
the instanceOf-relation. The only group that
can learn arbitrary binary relations is the group
of pattern matching systems (Etzioni et al, 2004;
Agichtein and Gravano, 2000; Ravichandran and
Hovy, 2002; Brin, 1999; Soderland, 1999; Xu et
al., 2002; Ruiz-Casado et al, 2005; Mann and
Yarowsky, 2005). Surprisingly, none of these sys-
tems uses a deep linguistic analysis of the cor-
pus. Consequently, most of them are extremely
volatile to small variations in the patterns. For ex-
ample, the simple subordinate clause in the fol-
lowing example (taken from (Ravichandran and
Hovy, 2002)) can already prevent a surface pat-
tern matcher from discovering a relation between
?London? and the ?river Thames?: ?London, which has
one of the busiest airports in the world, lies on the banks
of the river Thames.?
1.3 Contribution
This paper presents LEILA (Learning to Extract
Information by Linguistic Analysis), a system that
can extract instances of an arbitrary given binary
relation from natural language Web documents
without human intervention. LEILA uses a deep
analysis for natural-language sentences as well as
other advanced NLP methods like anaphora reso-
lution, and combines them with machine learning
techniques for robust and high-yield information
extraction. Our experimental studies on a variety
of corpora demonstrate that LEILA achieves very
good results in terms of precision and recall and
outperforms the prior state-of-the-art methods.
1.4 Link Grammars
There exist different approaches for parsing nat-
ural language sentences. They range from sim-
ple part-of-speech tagging to context-free gram-
mars and more advanced techniques such as Lex-
ical Functional Grammars, Head-Driven Phrase
Structure Grammars or stochastic approaches. For
our implementation, we chose the Link Grammar
Parser (Sleator and Temperley, 1993). It is based
on a context-free grammar and hence it is simpler
to handle than the advanced parsing techniques.
At the same time, it provides a much deeper se-
mantic structure than the standard context-free
parsers. Figure 1 shows a simplified example of
a linguistic structure produced by the link parser
(a linkage).
A linkage is a connected planar undirected
graph, the nodes of which are the words of the sen-
tence. The edges are called links. They are labeled
with connectors. For example, the connector subj
in Figure 1 marks the link between the subject and
the verb of the sentence. The linkage must ful-
fill certain linguistic constraints, which are given
by a link grammar. The link grammar specifies
which word may be linked by which connector to
preceding and following words. Furthermore, the
parser assigns part-of-speech tags, i.e. symbols
identifying the grammatical function of a word in
a sentence. In the example in Figure 1, the let-
ter ?n? following the word ?composers? indentifies
?composers? as a noun.
Chopin was.v     great  among the composers.n of   his  time.n
subj compl mod
prepObj
mod
prepObj
detdet
Figure 1: A simple linkage
Figure 2 shows how the Link Parser copes with a
more complex example. The relationship between
the subject ?London? and the verb ?lies? is not dis-
rupted by the subordinate clause:
London, which has one of the busiest airports, lies on the banks of the river Thames. 
subj
mod subj obj prep
prepObj
det
sup mod
prepObj
det mod
prepObj
det grp
Figure 2: A complex linkage
We say that a linkage expresses a relation r, if
the underlying sentence implies that a pair of enti-
ties is in r. Note that the deep grammatical anal-
ysis of the sentence would allow us to define the
meaning of the sentence in a theoretically well-
founded way (Montague, 1974). For this paper,
however, we limit ourselves to an intuitive under-
standing of the notion of meaning.
We define a pattern as a linkage in which two
19
words have been replaced by placeholders. Figure
3 shows a pattern derived from the linkage in Fig-
ure 1 by replacing ?Chopin? and ?composers? by the
placeholders ?X? and ?Y?.
    X       was.v       great  among the       Y        of  his    time.n
subj compl mod
prepObj
mod
prepObj
detdet
Figure 3: A pattern
We call the (unique) shortest path from one
placeholder to the other the bridge, marked in bold
in the figure. The bridge does not include the
placeholders. Two bridges are regarded as equiva-
lent, if they have the same sequence of nodes and
edges, although nouns and adjectives are allowed
to differ. For example, the bridge in Figure 3 and
the bridge in Figure 4 (in bold) are regarded as
equivalent, because they are identical except for
a substitution of ?great? by ?mediocre?. A pattern
matches a linkage, if an equivalent bridge occurs
in the linkage. For example, the pattern in Figure
3 matches the linkage in Figure 4.
Mozart was.v clearly mediocre  among the composers.n.
subj
compl
mod
prepObj
detmod
Figure 4: A matching linkage
If a pattern matches a linkage, we say that the
pattern produces the pair of words that the link-
age contains in the position of the placeholders.
In Figure 4, the pair ?Mozart? / ?composers? is pro-
duced by the pattern in Figure 3.
2 System Description
2.1 Document Pre-Processing
LEILA accepts HTML documents as input. To
allow the system to handle date and number ex-
pressions, we normalize these constructions by
regular expression matching in combination with
a set of functions. For example, the expression
?November 23rd to 24th 1998? becomes ?1998-11-23
to 1998-11-24? and the expression ?0.8107 acre-feet?
becomes ?1000 cubic-meters?. Then, we split the
original HTML-document into two files: The first
file contains the proper sentences with the HTML-
tags removed. The second file contains the non-
grammatical parts, such as lists, expressions us-
ing parentheses and other constructions that can-
not be handled by the Link Parser. For example,
the character sequence ?Chopin (born 1810) was a
great composer? is split into the sentence ?Chopin
was a great composer? and the non-grammatical in-
formation ?Chopin (born 1810)?. The grammatical
file is parsed by the Link Parser.
The parsing allows for a restricted named entity
recognition, because the parser links noun groups
like ?United States of America? by designated con-
nectors. Furthermore, the parsing allows us to do
anaphora resolution. We use a conservative ap-
proach, which simply replaces a third person pro-
noun by the subject of the preceding sentence.
For our goal, it is essential to normalize nouns
to their singular form. This task is non-trivial,
because there are numerous words with irregular
plural forms and there exist even word forms that
can be either the singular form of one word or the
plural form of another. By collecting these excep-
tions systematically from WordNet, we were able
to stem most of them correctly with our Plural-to-
Singular Stemmer (PlingStemmer1). For the non-
grammatical files, we provide a pseudo-parsing,
which links each two adjacent items by an artifi-
cial connector. As a result, the uniform output of
the preprocessing is a sequence of linkages, which
constitutes the input for the core algorithm.
2.2 Core Algorithm
As a definition of the target relation, our algorithm
requires a function (given by a Java method) that
decides into which of the following categories a
pair of words falls:
? The pair can be an example for the target re-
lation. For instance, for the birthdate-
relation, the examples can be given by a list of
persons with their birth dates.
? The pair can be a counterexample. For the
birthdate-relation, the counterexamples can
be deduced from the examples (e.g. if ?Chopin?
/ ?1810? is an example, then ?Chopin? / ?2000?
must be a counterexample).
? The pair can be a candidate. For birthdate,
the candidates would be all pairs of a proper
name and a date that are not an example or a
counterexample.
? The pair can be none of the above.
The core algorithm proceeds in three phases:
1. In the Discovery Phase, it seeks linkages in
which an example pair appears. It replaces the
two words by placeholders, thus producing a
pattern. These patterns are collected as positive
patterns. Then, the algorithm runs through the
sentences again and finds all linkages that match
1available at http://www.mpii.mpg.de/ ?suchanek
20
a positive pattern, but produce a counterexam-
ple. The corresponding patterns are collected as
negative patterns2.
2. In the Training Phase, statistical learning is ap-
plied to learn the concept of positive patterns.
The result of this process is a classifier for pat-
terns.
3. In the Testing Phase, the algorithm considers
again all sentences in the corpus. For each link-
age, it generates all possible patterns by replac-
ing two words by placeholders. If the two words
form a candidate and the pattern is classified as
positive, the produced pair is proposed as a new
element of the target relation (an output pair).
In principle, the core algorithm does not depend on
a specific grammar or a specific parser. It can work
on any type of grammatical structures, as long as
some kind of pattern can be defined on them. It is
also possible to run the Discovery Phase and the
Testing Phase on different corpora.
2.3 Learning Model
The central task of the Discovery Phase is deter-
mining patterns that express the target relation.
These patterns are generalized in the Training
Phase. In the Testing Phase, the patterns are used
to produce the output pairs. Since the linguistic
meaning of the patterns is not apparent to the sys-
tem, the Discovery Phase relies on the following
hypothesis: Whenever an example pair appears
in a sentence, the linkage and the corresponding
pattern express the target relation. This hypoth-
esis may fail if a sentence contains an example
pair merely by chance, i.e. without expressing the
target relation. Analogously, a pattern that does
express the target relation may occasionally pro-
duce counterexamples. We call these patterns false
samples. Virtually any learning algorithm can deal
with a limited number of false samples.
To show that our approach does not depend
on a specific learning algorithm, we implemented
two classifiers for LEILA: One is an adaptive k-
Nearest-Neighbor-classifier (kNN) and the other
one uses a Support Vector Machine (SVM). These
classifiers, the feature selection and the statistical
model are explained in detail in (Suchanek et al,
2006). Here, we just note that the classifiers yield
a real valued label for a test pattern. This value
can be interpreted as the confidence of the classifi-
cation. Thus, it is possible to rank the output pairs
of LEILA by their confidence.
2Note that different patterns can match the same linkage.
3 Experiments
3.1 Setup
We ran LEILA on different corpora with increasing
heterogeneity:
? Wikicomposers: The set of all Wikipedia arti-
cles about composers (872 HTML documents).
We use it to see how LEILA performs on a docu-
ment collection with a strong structural and the-
matic homogeneity.
? Wikigeography: The set of all Wikipedia
pages about the geography of countries (313
HTML documents).
? Wikigeneral: A set of random Wikipedia arti-
cles (78141 HTML documents). We chose it to
assess LEILA?s performance on structurally ho-
mogenous, but thematically random documents.
? Googlecomposers: This set contains one doc-
ument for each baroque, classical, and roman-
tic composer in Wikipedia?s list of composers,
as delivered by a Google ?I?m feeling lucky?
search for the composer?s name (492 HTML
documents). We use it to see how LEILA per-
forms on a corpus with a high structural hetero-
geneity. Since the querying was done automat-
ically, the downloaded pages include spurious
advertisements as well as pages with no proper
sentences at all.
We tested LEILA on different target relations with
increasing complexity:
? birthdate: This relation holds between a person
and his birth date (e.g. ?Chopin? / ?1810?). It is
easy to learn, because it is bound to strong sur-
face clues (the first element is always a name,
the second is always a date).
? synonymy: This relation holds between two
names that refer to the same entity (e.g.
?UN?/?United Nations?). The relation is more so-
phisticated, since there are no surface clues.
? instanceOf: This relation is even more sophis-
ticated, because the sentences often express it
only implicitly.
We compared LEILA to different competitors. We
only considered competitors that, like LEILA, ex-
tract the information from a corpus without using
other Internet sources. We wanted to avoid run-
ning the competitors on our own corpora or on our
own target relations, because we could not be sure
to achieve a fair tuning of the competitors. Hence
we ran LEILA on the corpora and the target rela-
tions that our competitors have been tested on by
their authors. We compare the results of LEILA
with the results reported by the authors. Our com-
petitors, together with their respective corpora and
relations, are:
21
? TextToOnto3: A state-of-the-art representative
for non-deep pattern matching. The system pro-
vides a component for the instanceOf rela-
tion and takes arbitrary HTML documents as in-
put. For completeness, we also consider its suc-
cessor Text2Onto (Cimiano and Vo?lker, 2005a),
although it contains only default methods in its
current state of development.
? Snowball (Agichtein and Gravano, 2000):
A recent representative of the slot-extraction
paradigm. In the original paper, Snowball has
been tested on the headquarters relation.
This relation holds between a company and the
city of its headquarters. Snowball was trained
on a collection of some thousand documents
and then applied to a test collection. For copy-
right reasons, we only had access to the test col-
lection (150 text documents).
? (Cimiano and Vo?lker, 2005b) present a new sys-
tem that uses context to assign a concept to
an entity. We will refer to this system as the
CV-system. The approach is restricted to the
instanceOf-relation, but it can classify in-
stances even if the corpus does not contain ex-
plicit definitions. In the original paper, the sys-
tem was tested on a collection of 1880 files from
the Lonely Planet Internet site4.
For the evaluation, the output pairs of the sys-
tem have to be compared to a table of ideal pairs.
One option would be to take the ideal pairs from a
pre-compiled data base. The problem is that these
ideal pairs may differ from the facts expressed in
the documents. Furthermore, these ideal pairs do
not allow to measure how much of the document
content the system actually extracted. This is why
we chose to extract the ideal pairs manually from
the documents. In our methodology, the ideal pairs
comprise all pairs that a human would understand
to be elements of the target relation. This involves
full anaphora resolution, the solving of reference
ambiguities, and the choice of truly defining con-
cepts. For example, we accept Chopin as instance
of composer but not as instance of member,
even if the text says that he was a member of some
club. Of course, we expect neither the competi-
tors nor LEILA to achieve the results in the ideal
table. However, this methodology is the only fair
way of manual extraction, as it is guaranteed to
be system-independent. If O denotes the multi-
set of the output pairs and I denotes the multi-set
of the ideal pairs, then precision, recall, and their
3http://www.sourceforge.net/projects/texttoonto
4http://www.lonelyplanet.com/
harmonic mean F1 can be computed as
recall = |O ? I||I| precision =
|O ? I|
|O|
F1 = 2 ? recall ? precisionrecall + precision .
To ensure a fair comparison of LEILA to Snow-
ball, we use the same evaluation as employed in
the original Snowball paper (Agichtein and Gra-
vano, 2000), the Ideal Metric. The Ideal Metric
assumes the target relation to be right-unique (i.e.
a many-to-one relation). Hence the set of ideal
pairs is right-unique. The set of output pairs can
be made right-unique by selecting the pair with the
highest confidence for each first component. Du-
plicates are removed from the ideal pairs and also
from the output pairs. All output pairs that have
a first component that is not in the ideal set are
removed.
There is one special case for the CV-system,
which uses the Ideal Metric for the non-right-
unique instanceOf relation. To allow for a fair
comparison, we used the Relaxed Ideal Metric,
which does not make the ideal pairs right-unique.
The calculation of recall is relaxed as follows:
recall = |O ? I||{x|?y : (x, y) ? I}|
Due to the effort, we could extract the ideal pairs
only for a sub-corpus. To ensure significance in
spite of this, we compute confidence intervals for
our estimates: We interpret the sequence of out-
put pairs as a repetition of a Bernoulli-experiment,
where the output pair can be either correct (i.e.
contained in the ideal pairs) or not. The parameter
of this Bernoulli-distribution is the precision. We
estimate the precision by drawing a sample (i.e.
by extracting all ideal pairs in the sub-corpus). By
assuming that the output pairs are identically in-
dependently distributed, we can calculate a confi-
dence interval for our estimation. We report confi-
dence intervals for precision and recall for a con-
fidence level of ? = 95%. We measure precision
at different levels of recall and report the values
for the best F1 value. We used approximate string
matching techniques to account for different writ-
ings of the same entity. For example, we count
the output pair ?Chopin? / ?composer? as correct,
even if the ideal pairs contain ?Frederic Chopin? /
?composer?. To ensure that LEILA does not just
reproduce the example pairs, we list the percent-
age of examples among the output pairs. During
our evaluation, we found that the Link Grammar
parser does not finish parsing on roughly 1% of
the files for unknown reasons.
22
Table 1: Results with different relations
Corpus Relation System #D #O #C #I Precision Recall F1 %E
Wikicomposers birthdate LEILA(SVM) 87 95 70 101 73.68%? 8.86% 69.31%? 9.00% 71.43% 4.29%
Wikicomposers birthdate LEILA(kNN) 87 90 70 101 78.89%? 8.43% 70.30%? 8.91% 74.35% 4.23%
Wikigeography synonymy LEILA(SVM) 81 92 74 164 80.43%? 8.11% 45.12%? 7.62% 57.81% 5.41%
Wikigeography synonymy LEILA(kNN) 81 143 105 164 73.43%? 7.24% 64.02%? 7.35% 68.40% 4.76%
Wikicomposers instanceOf LEILA(SVM) 87 685 408 1127 59.56%? 3.68% 36.20%? 2.81% 45.03% 6.62%
Wikicomposers instanceOf LEILA(kNN) 87 790 463 1127 58.61%? 3.43% 41.08%? 2.87% 48.30% 7.34%
Wikigeneral instanceOf LEILA(SVM) 287 921 304 912 33.01%? 3.04% 33.33%? 3.06% 33.17% 3.62%
Googlecomposers instanceOf LEILA(SVM) 100 787 210 1334 26.68%? 3.09% 15.74%? 1.95% 19.80% 4.76%
Googlecomposers instanceOf LEILA(kNN) 100 840 237 1334 28.21%? 3.04% 17.77%? 2.05% 21.80% 8.44%
Googlec.+Wikic. instanceOf LEILA(SVM) 100 563 203 1334 36.06%? 3.97% 15.22%? 1.93% 21.40% 5.42%
Googlec.+Wikic. instanceOf LEILA(kNN) 100 826 246 1334 29.78%? 3.12% 18.44%? 2.08% 22.78% 7.72%
#O ? number of output pairs #D ? number of documents in the hand-processed sub-corpus
#C ? number of correct output pairs %E ? proportion of example pairs among the correct output pairs
#I ? number of ideal pairs Recall and Precision with confidence interval at ? = 95%
3.2 Results
3.2.1 Results on different relations
Table 1 summarizes our experimental results
with LEILA on different relations. For the birth-
date relation, we used Edward Morykwas? list of
famous birthdays5 as examples. As counterexam-
ples, we chose all pairs of a person that was in the
examples and an incorrect birthdate. All pairs of
a proper name and a date are candidates. We ran
LEILA on the Wikicomposer corpus. LEILA per-
formed quite well on this task. The patterns found
were of the form ?X was born in Y ? and ?X (Y )?.
For the synonymy relation we used all pairs
of proper names that share the same synset in
WordNet as examples (e.g. ?UN?/?United Na-
tions?). As counterexamples, we chose all pairs of
nouns that are not synonymous in WordNet (e.g.
?rabbit?/?composer?). All pairs of proper names are
candidates. We ran LEILA on the Wikigeography
corpus, because this set is particularly rich in syn-
onyms. LEILA performed reasonably well. The
patterns found include ?X was known as Y ? as well
as several non-grammatical constructions such as
?X (formerly Y )?.
For the instanceOf relation, it is difficult to se-
lect example pairs, because if an entity belongs
to a concept, it also belongs to all super-concepts.
However, admitting each pair of an entity and one
of its super-concepts as an example would result in
far too many false positives. The problem is to de-
termine for each entity the (super-)concept that is
most likely to be used in a natural language defini-
tion of that entity. Psychological evidence (Rosch
et al, 1976) suggests that humans prefer a certain
layer of concepts in the taxonomy to classify en-
tities. The set of these concepts is called the Ba-
sic Level. Heuristically, we found that the low-
est super-concept in WordNet that is not a com-
pound word is a good approximation of the ba-
5http://www.famousbirthdates.com
sic level concept for a given entity. We used all
pairs of a proper name and the corresponding ba-
sic level concept of WordNet as examples. We
could not use pairs of proper names and incorrect
super-concepts as counterexamples, because our
corpus Wikipedia knows more meanings of proper
names than WordNet. Therefore, we used all pairs
of a common noun and an incorrect super-concept
from WordNet as counterexamples. All pairs of
a proper name and a WordNet concept are candi-
dates.
We ran LEILA on the Wikicomposers corpus.
The performance on this task was acceptable, but
not impressive. However, the chances to obtain a
high recall and a high precision were significantly
decreased by our tough evaluation policy: The
ideal pairs include tuples deduced by resolving
syntactic and semantic ambiguities and anaphoras.
Furthermore, our evaluation policy demands that
non-defining concepts like member not be cho-
sen as instance concepts. In fact, a high propor-
tion of the incorrect assignments were friend,
member, successor and predecessor, de-
creasing the precision of LEILA. Thus, compared
to the gold standard of humans, the performance
of LEILA can be considered reasonably good. The
patterns found include the Hearst patterns (Hearst,
1992) ?Y such as X?, but also more complex pat-
terns like ?X was known as a Y ?, ?X [. . . ] as Y ?, ?X
[. . . ] can be regarded as Y ? and ?X is unusual among
Y ?. Some of these patterns could not have been
found by primitive regular expression matching.
To test whether thematic heterogeneity influ-
ences LEILA, we ran it on the Wikigeneral corpus.
Finally, to try the limits of our system, we ran it on
the Googlecomposers corpus. As shown in Table
1, the performance of LEILA dropped in these in-
creasingly challenging tasks, but LEILA could still
produce useful results. We can improve the results
on the Googlecomposers corpus by adding the Wi-
kicomposers corpus for training.
23
The different learning methods (kNN and SVM)
performed similarly for all relations. Of course, in
each of the cases, it is possible to achieve a higher
precision at the price of a lower recall. The run-
time of the system splits into parsing (? 40s for
each document, e.g. 3:45h for Wikigeography)
and the core algorithm (2-15min for each corpus,
5h for the huge Wikigeneral).
3.2.2 Results with different competitors
Table 2 shows the results for comparing LEILA
against various competitors (with LEILA in bold-
face). We compared LEILA to TextToOnto and
Text2Onto for the instanceOf relation on the
Wikicomposers corpus. TextToOnto requires an
ontology as source of possible concepts. We gave
it the WordNet ontology, so that it had the same
preconditions as LEILA. Text2Onto does not re-
quire any input. Text2Onto seems to have a preci-
sion comparable to ours, although the small num-
ber of found pairs does not allow a significant con-
clusion. Both systems have drastically lower recall
than LEILA.
For Snowball, we only had access to the test
corpus. Hence we trained LEILA on a small por-
tion (3%) of the test documents and tested on
the remaining ones. Since the original 5 seed
pairs that Snowball used did not appear in the col-
lection at our disposal, we chose 5 other pairs
as examples. We used no counterexamples and
hence omitted the Training Phase of our algorithm.
LEILA quickly finds the pattern ?Y -based X?. This
led to very high precision and good recall, com-
pared to Snowball ? even though Snowball was
trained on a much larger training collection.
The CV-system differs from LEILA, because its
ideal pairs are a table, in which each entity is as-
signed to its most likely concept according to a hu-
man understanding of the text ? independently of
whether there are explicit definitions for the entity
in the text or not. We conducted two experiments:
First, we used the document set used in Cimiano
and Vo?lker?s original paper (Cimiano and Vo?lker,
2005a), the Lonely Planet corpus. To ensure a
fair comparison, we trained LEILA separately on
the Wikicomposers corpus, so that LEILA cannot
have example pairs in its output. For the evalu-
ation, we calculated precision and recall with re-
spect to an ideal table provided by the authors.
Since the CV-system uses a different ontology, we
allowed a distance of 4 edges in the WordNet hi-
erarchy to count as a match (for both systems).
Since the explicit definitions that our system relies
on were sparse in the corpus, LEILA performed
worse than the competitor. In a second experi-
ment, we had the CV-system run on the Wikicom-
posers corpus. As the CV-system requires a set
of target concepts, we gave it the set of all con-
cepts in our ideal pairs. Furthermore, the sys-
tem requires an ontology on these concepts. We
gave it the WordNet ontology, pruned to the tar-
get concepts with their super-concepts. We evalu-
ated by the Relaxed Ideal Metric, again allowing
a distance of 4 edges in the WordNet hierarchy to
count as a match (for both systems). This time,
our competitor performed worse. This is because
our ideal table is constructed from the definitions
in the text, which our competitor is not designed
to follow. These experiments only serve to show
the different philosophies in the definition of the
ideal pairs for the CV-system and LEILA. The CV-
system does not depend on explicit definitions, but
it is restricted to the instanceOf-relation.
4 Conclusion and Outlook
We addressed the problem of automatically ex-
tracting instances of arbitrary binary relations
from natural language text. The key novelty of our
approach is to apply a deep syntactic analysis to
this problem. We have implemented our approach
and showed that our system LEILA outperforms
existing competitors.
Our current implementation leaves room for fu-
ture work. For example, the linkages allow for
more sophisticated ways of resolving anaphoras
or matching patterns. LEILA could learn nu-
merous interesting relations (e.g. country /
president or isAuthorOf) and build up an
ontology from the results with high confidence.
LEILA could acquire and exploit new corpora on
its own (e.g., it could read newspapers) and it
could use its knowledge to acquire and structure
its new knowledge more efficiently. We plan to
exploit these possibilities in our future work.
4.1 Acknowledgements
We would like to thank Eugene Agichtein for his
caring support with Snowball. Furthermore, Jo-
hanna Vo?lker and Philipp Cimiano deserve our
sincere thanks for their unreserved assistance with
their system.
References
[Agichtein and Gravano2000] E. Agichtein and L. Gravano.
2000. Snowball: extracting relations from large plain-text
collections. In ACM 2000, pages 85?94, Texas, USA.
[Brin1999] Sergey Brin. 1999. Extracting patterns and rela-
tions from the world wide web. In Selected papers from
the Int. Workshop on the WWW and Databases, pages
172?183, London, UK. Springer-Verlag.
[Buitelaar and Ramaka2005] P. Buitelaar and S. Ramaka.
2005. Unsupervised ontology-based semantic tagging
24
Table 2: Results with different competitors
Corpus M Relation System #D #O #C #I Prec Rec F1
Snowball corp. S headquarters LEILA(SVM) 54 92 82 165 89.13%? 6.36% 49.70%? 7.63% 63.81%
Snowball corp. S headquarters LEILA(kNN) 54 91 82 165 90.11%? 6.13% 49.70%? 7.63% 64.06%
Snowball corp. S headquarters Snowball 54 144 49 165 34.03%? 7.74% 29.70%? 6.97% 31.72%
Snowball corp. I headquarters LEILA(SVM) 54 50 48 126 96.00%? 5.43% 38.10%? 8.48% 54.55%
Snowball corp. I headquarters LEILA(kNN) 54 49 48 126 97.96%? 3.96% 38.10%? 8.48% 54.86%
Snowball corp. I headquarters Snowball 54 64 31 126 48.44%?12.24% 24.60%? 7.52% 32.63%
Wikicomposers S instanceOf LEILA(SVM) 87 685 408 1127 59.56%? 3.68% 36.20%? 2.81% 45.03%
Wikicomposers S instanceOf LEILA(kNN) 87 790 463 1127 58.61%? 3.43% 41.08%? 2.87% 48.30%
Wikicomposers S instanceOf Text2Onto 87 36 18 1127 50.00% 1.60%? 0.73% 3.10%
Wikicomposers S instanceOf TextToOnto 87 121 47 1127 38.84%? 8.68% 4.17%? 1.17% 7.53%
Wikicomposers R instanceOf LEILA(SVM) 87 336 257 744 76.49%? 4.53% 34.54%? 3.42% 47.59%
Wikicomposers R instanceOf LEILA(kNN) 87 367 276 744 75.20%? 4.42% 37.10%? 3.47% 49.68%
Wikicomposers R instanceOf CV-system 87 134 30 744 22.39% 4.03%? 1.41% 6.83%
Lonely Planet R instanceOf LEILA(SVM) ? 159 42 289 26.42%? 6.85% 14.53%? 4.06% 18.75%
Lonely Planet R instanceOf LEILA(kNN) ? 168 44 289 26.19%? 6.65% 15.22%? 4.14% 19.26%
Lonely Planet R instanceOf CV-system ? 289 92 289 31.83%? 5.37% 31.83%? 5.37% 31.83%
M ? Metric (S: Standard, I: Ideal Metric, R: Relaxed Ideal Metric). Other abbreviations as in Table 1
for knowledge markup. In W. Buntine, A. Hotho, and
Stephan Bloehdorn, editors, Workshop on Learning in Web
Search at the ICML 2005.
[Buitelaar et al2004] P. Buitelaar, D. Olejnik, and M. Sin-
tek. 2004. A protege plug-in for ontology extraction from
text based on linguistic analysis. In ESWS 2004, Herak-
lion, Greece.
[Califf and Mooney1997] M. Califf and R. Mooney. 1997.
Relational learning of pattern-match rules for informa-
tion extraction. ACL-97 Workshop in Natural Language
Learning, pages 9?15.
[Cimiano and Vo?lker2005a] P. Cimiano and J. Vo?lker.
2005a. Text2onto - a framework for ontology learn-
ing and data-driven change discovery. In A. Montoyo,
R. Munozand, and E. Metais, editors, Proc. of the 10th Int.
Conf. on Applications of Natural Language to Information
Systems, pages 227?238, Alicante, Spain.
[Cimiano and Vo?lker2005b] P. Cimiano and J. Vo?lker.
2005b. Towards large-scale, open-domain and ontology-
based named entity classification. In Int. Conf. on Recent
Advances in NLP 2005, pages 166?172.
[Cimiano et al2005] P. Cimiano, G. Ladwig, and S. Staab.
2005. Gimme the context: Contextdriven automatic se-
mantic annotation with cpankow. In Allan Ellis and Tat-
suya Hagino, editors, WWW 2005, Chiba, Japan.
[Etzioni et al2004] O. Etzioni, M. Cafarella, D. Downey,
S. Kok, A. Popescu, T. Shaked, S. Soderland, D. S. Weld,
and A. Yates. 2004. Web-scale information extraction
in knowitall (preliminary results). In WWW 2004, pages
100?110.
[Fellbaum1998] C. Fellbaum. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
[Finn and Kushmerick2004] A. Finn and N. Kushmerick.
2004. Multi-level boundary classification for information
extraction. In ECML 2004, pages 111?122.
[Freitag and Kushmerick2000] D. Freitag and N. Kushmer-
ick. 2000. Boosted wrapper induction. In American Nat.
Conf. on AI 2000.
[Graupmann2004] Jens Graupmann. 2004. Concept-based
search on semi-structured data exploiting mined semantic
relations. In EDBT Workshops, pages 34?43.
[Hearst1992] A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In ICCL 1992, Nantes,
France.
[J. Iria2005] F. Ciravegna J. Iria. 2005. Relation extraction
for mining the semantic web.
[Maedche and Staab2000] A. Maedche and S. Staab. 2000.
Discovering conceptual relations from text. In W. Horn,
editor, ECAI 2000, pages 85?94, Berlin, Germany.
[Mann and Yarowsky2005] Gideon Mann and David
Yarowsky. 2005. Multi-field information extraction and
cross-document fusion. In ACL 2005.
[Montague1974] R. Montague. 1974. Universal grammar.
In Formal Philosophy. Selected Papers of Richard Mon-
tague. Yale University Press.
[Ravichandran and Hovy2002] D. Ravichandran and
E. Hovy. 2002. Learning surface text patterns for a
question answering system. In ACL 2002, Philadelphia,
USA.
[Riloff1996] E. Riloff. 1996. Automatically generating ex-
traction patterns from untagged text. Annual Conf. on AI
1996, pages 1044?1049.
[Rosch et al1976] E. Rosch, C.B. Mervis, W.D. Gray, D.M.
Johnson, and P. Boyes-Bream. 1976. Basic objects in
natural categories. Cognitive Psychology, pages 382?439.
[Ruiz-Casado et al2005] Maria Ruiz-Casado, Enrique Al-
fonseca, and Pablo Castells. 2005. Automatic extraction
of semantic relationships for wordnet by means of pattern
learning from wikipedia. In NLDB 2006, pages 67?79.
[Sleator and Temperley1993] D. Sleator and D. Temperley.
1993. Parsing english with a link grammar. 3rd Int. Work-
shop on Parsing Technologies.
[Soderland et al1995] S. Soderland, D. Fisher, J. Aseltine,
and W. Lehnert. 1995. Crystal: Inducing a conceptual
dictionary. IJCAI 1995, pages 1314?1319.
[Soderland1999] S. Soderland. 1999. Learning information
extraction rules for semi-structured and free text. Machine
Learning, pages 233?272.
[Suchanek et al2006] Fabian M. Suchanek, Georgiana
Ifrim, and Gerhard Weikum. 2006. Combining Linguistic
and Statistical Analysis to Extract Relations from Web
Documents. In SIGKDD 2006.
[Xu and Krieger2003] F. Xu and H. U. Krieger. 2003. In-
tegrating shallow and deep nlp for information extraction.
In RANLP 2003, Borovets, Bulgaria.
[Xu et al2002] F. Xu, D. Kurz, J. Piskorski, and
S. Schmeier. 2002. Term extraction and mining
term relations from free-text documents in the financial
domain. In Int. Conf. on Business Information Systems
2002, Poznan, Poland.
[Yangarber et al2000] R. Yangarber, R. Grishman,
P. Tapanainen, and S. Huttunen. 2000. Automatic
acquisition of domain knowledge for information extrac-
tion. In ICCL 2000, pages 940?946, Morristown, NJ,
USA. Association for Computational Linguistics.
[Yangarber et al2002] R. Yangarber, W. Lin, and R. Grish-
man. 2002. Unsupervised learning of generalized names.
In ICCL 2002, pages 1?7, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
25
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1135?1145, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
PATTY: A Taxonomy of Relational Patterns with Semantic Types
Ndapandula Nakashole, Gerhard Weikum, Fabian Suchanek
Max Planck Institute for Informatics
Saarbru?cken, Germany
{nnakasho,weikum,suchanek}@mpi-inf.mpg.de
Abstract
This paper presents PATTY: a large resource
for textual patterns that denote binary relations
between entities. The patterns are semanti-
cally typed and organized into a subsumption
taxonomy. The PATTY system is based on ef-
ficient algorithms for frequent itemset mining
and can process Web-scale corpora. It har-
nesses the rich type system and entity popu-
lation of large knowledge bases. The PATTY
taxonomy comprises 350,569 pattern synsets.
Random-sampling-based evaluation shows a
pattern accuracy of 84.7%. PATTY has 8,162
subsumptions, with a random-sampling-based
precision of 75%. The PATTY resource
is freely available for interactive access and
download.
1 Introduction
Motivation. WordNet (Fellbaum 1998) is one of the
most widely used lexical resources in computer sci-
ence. It groups nouns, verbs, and adjectives into sets
of synonyms, and arranges these synonyms in a tax-
onomy of hypernyms. WordNet is limited to single
words. It does not contain entire phrases or pat-
terns. For example, WordNet does not contain the
pattern X is romantically involved with Y. Just like
words, patterns can be synonymous, and they can
subsume each other. The pattern X is romantically
involved with Y is synonymous with the pattern X is
dating Y. Both are subsumed by X knows Y. Patterns
for relations are a vital ingredient for many appli-
cations, including information extraction and ques-
tion answering. If a large-scale resource of relational
patterns were available, this could boost progress in
NLP and AI tasks.
Yet, existing large-scale knowledge bases are
mostly limited to abstract binary relationships be-
tween entities, such as ?bornIn? (Auer 2007; Bol-
lacker 2008; Nastase 2010; Suchanek 2007). These
do not correspond to real text phrases. Only the Re-
Verb system (Fader 2011) yields a larger number of
relational textual patterns. However, no attempt is
made to organize these patterns into synonymous
patterns, let ale into a taxonomy. Thus, the pat-
terns themselves do not exhibit semantics.
Goal. Our goal in this paper is to systematically
compile relational patterns from a corpus, and to im-
pose a semantically typed structure on them. The
result we aim at is a WordNet-style taxonomy of
binary relations. In particular, we aim at patterns
that contain semantic types, such as ?singer? sings
?song?. We also want to automatically generalize
syntactic variations such as sings her ?song? and
sings his ?song?, into a more general pattern sings
[prp] ?song? with POS tag [prp]. Analogously but
more demandingly, we want to automatically infer
that the above patterns are semantically subsumed
by the pattern ?musician? performs on ?musical
composition? with more general types for the entity
arguments in the pattern.
Compiling and organizing such patterns is chal-
lenging for the following reasons. 1) The number
of possible patterns increases exponentially with the
length of the patterns. For example, the string ?Amy
sings ?Rehab?? can give rise to the patterns ?singer?
sings ?song?, ?person? sings ?artifact?, ?person?
[vbz] ?entity?, etc. If wildcards for multiple words
are allowed (such as in ?person? sings * ?song?), the
number of possible patterns explodes. 2) A pattern
1135
can be semantically more general than another pat-
tern (when one relation is implied by the other re-
lation), and it can also be syntactically more gen-
eral than another pattern (by the use of placehold-
ers such as [vbz]). These two subsumption orders
have a non-obvious interplay, and none can be ana-
lyzed without the other. 3) We have to handle pattern
sparseness and coincidental matches. If the corpus
is small, e.g., the patterns ?singer? later disliked her
song ?song? and ?singer? sang ?song?, may apply to
the same set of entity pairs in the corpus. Still, the
patterns are not synonymous. 4) Computing mutual
subsumptions on a large set of patterns may be pro-
hibitively slow. Moreover, due to noise and vague
semantics, patterns may even not form a crisp tax-
onomy, but require a hierarchy in which subsump-
tion relations have to be weighted by statistical con-
fidence measures.
Contributions. In this paper, we present PATTY, a
large resource of relational patterns that are arranged
in a semantically meaningful taxonomy, along with
entity-pair instances. More precisely, our contribu-
tions are as follows:
1) SOL patterns: We define an expressive fam-
ily of relational patterns, which combines syntac-
tic features (S), ontological type signatures (O), and
lexical features (L). The crucial novelty is the addi-
tion of the ontological, semantic dimension to pat-
terns. When compared to a state-of-the-art pattern
language, we found that SOL patterns yield higher
recall while achieving similar precision.
2) Mining algorithms: We present efficient and
scalable algorithms that can infer SOL patterns and
subsumptions at scale, based on instance-level over-
laps and an ontological type hierarchy.
3) A large Lexical resource:. On the Wikipe-
dia corpus, we obtained 350,569 pattern synsets
with 84.7% precision. We make our pat-
tern taxonomy available for further research at
www.mpi-inf.mpg.de/yago-naga/patty/ .
The paper is structured as follows. Section 2 dis-
cusses related work. Section 3 outlines the basic
machinery for pattern extraction. Section 4 intro-
duces our SOL pattern model. Sections 5 and 6
present the syntactic and semantic generalization of
patterns. Section 7 explains how to arrange the pat-
terns into a taxonomy. Section 8 reports our experi-
mental findings.
2 Related Work
A wealth of taxonomic knowledge bases (KBs)
about entities and their semantic classes have be-
come available. These are very rich in terms of
unary predicates (semantic classes) and their entity
instances. However, the number of binary relations
(i.e., relation types, not instances) in these KBs is
usually small: Freebase (Bollacker 2008) has a few
thousand hand-crafted relations. WikiNet (Nastase
2010) has automatically extracted ca. 500 relations
from Wikipedia category names. DBpedia (Auer
2007) has automatically compiled ca. 8000 names
of properties from Wikipedia infoboxes, but these
include many involuntary semantic duplicates such
as surname and lastname. In all of these projects,
the resource contains the relation names, but not the
natural language patterns for them. The same is true
for other projects along these lines (Navigli 2010;
Philpot 2008; Ponzetto 2007; Suchanek 2007).
In contrast, knowledge base projects that auto-
matically populate relations from Web pages also
learn surface patterns for the relations: examples
are TextRunner/ReVerb (Banko 2007; Fader 2011),
NELL (Carlson 2010; Mohamed11), Probase (Wu
2011), the dynamic lexicon approach by (Hoffmann
2010; Wu 2008), the LDA-style clustering approach
by (Yao 2011), and projects on Web tables (Li-
maye 2010; Venetis 2011). Of these, only TextRun-
ner/ReVerb and NELL have made large pattern col-
lections publicly available.
ReVerb (Fader 2011) constrains patterns to verbs
or verb phrases that end with prepositions, while
PATTY can learn arbitrary patterns. More impor-
tantly, all methods in the TextRunner/ReVerb family
are blind to the ontological dimension of the enti-
ties in the patterns. Therefore, there is no notion of
semantic typing for relation phrases as in PATTY.
NELL (Carlson 2010) is based on a fixed set
of prespecified relations with type signatures, (e.g.,
personHasCitizenship: ?person? ? ?country?), and
learns to extract suitable noun-phrase pairs from a
large Web corpus. In contrast, PATTY discovers pat-
terns for relations that are a priori unknown.
1136
In OntExt (Mohamed11), the NELL architecture
was extended to automatically compute new re-
lation types (beyond the prespecified ones) for a
given type signature of arguments, based on a clus-
tering technique. For example, the relation mu-
sicianPlaysInstrument is found by clustering pat-
tern co-occurrences for the noun-phrase pairs that
fall into the specific type signature ?musician? ?
?musicinstrument?. This technique works for one
type signature at a time, and does not scale up to
mining a large corpus. Also, the technique is not
suitable for inferring semantic subsumptions. In
contrast, PATTY efficiently acquires patterns from
large-scale corpora and organizes them into a sub-
sumption hierarchy.
Class-based attribute discovery is a special case
of mining relational patterns (e.g., (Alfonseca 2010;
Pasca 2007; Pasca 2008; Reisinger 2009)). Given a
semantic class, such as movies or musicians, the task
is to determine relevant attributes, such as cast and
budget for movies, or albums and biography for mu-
sicians, along with their instances. Unlike PATTY?s
patterns, the attributes are not typed. They come
with a prespecified type for the domain, but without
any type for the range of the underlying relation.
There are further relation-centric tasks in NLP
and text mining that have commonalities with our
endeavor, but differ in fundamental ways. The
SemEval-2010 task on classification of semantic re-
lations between noun-phrase pairs (Hendrickx 2010)
aimed at predicting the relation for a given sentence
and pair of nominals, but used a fixed set of prespec-
ified relations. Another task in this research avenue
is to characterize and predict the argument types for
a given relation or pattern (Kozareva 2010; Nakov
2008). This is closer to KB population and less re-
lated to our task of discovering relational patterns
and systematically organizing them.
From a linguistic perspective, there is ample
work on patterns for unary predicates of the form
class(entity). This includes work on entailment of
classes, i.e., on is-a and subclassOf relationships.
Entailment among binary predicates of the form re-
lation(entity1, entity2) has received less attention
(Lin 2001; Chklovski 2004; Hashimoto 2009; Be-
rant 2011). These works focus solely on verbs, while
PATTY learns arbitrary phrases for patterns.
Several lexical resources capture verb categories
and entailment: WordNet 3.0 (Fellbaum 1998) con-
tains about 13,000 verb senses, with troponymy and
entailment relations; VerbNet (Kipper 2008) is a hi-
erarchical lexicon with more than 5,000 verb senses
in ca. 300 classes, including selectional preferences.
Again, all of these resources focus solely on verbs.
ConceptNet 5.0 (Havasi 2007) is a thesaurus of
commonsense knowledge built as a crowdsourcing
endeavor. PATTY, in contrast, is constructed fully
automatically from large corpora. Automatic learn-
ing of paraphrases and textual entailment has re-
ceived much attention (see the survey of (Androut-
sopoulos 2010)), but does not consider fine-grained
typing for binary relations, as PATTY does.
3 Pattern Extraction
This section explains how we obtain basic textual
patterns from the input corpus. We first apply the
Stanford Parser (Marneffe 2006) to the individual
sentences of the corpus to obtain dependency paths.
The dependency paths form a directed graph, with
words being nodes and dependencies being edges.
For example, the sentence ?Winehouse effortlessly
performed her song Rehab.? yields the following de-
pendency paths:
nsubj(performed-3, Winehouse-1)
advmod(performed-3, effortlessly-2)
poss(Rehab-6, her-4)
nn(Rehab-6, song-5)
dobj(performed-3, Rehab-6)
While our method also works with patterns obtained
from shallow features such as POS tags, we found
that dependency paths improve pattern extraction
precision especially on long sentences.
We then detect mentions of named entities in the
parsed corpus. For this purpose, we use a dictio-
nary of entities. This can be any resource that con-
tains named entities with their surface names and se-
mantic types (Auer 2007; Suchanek 2007; Hoffart
2011; Bollacker 2008). In our experiments, we used
the YAGO2 knowledge base (Hoffart 2011). We
match noun phrases that contain at least one proper
noun against the dictionary. For disamiguation, we
1137
use a simple context-similarity prior, as described
in (Suchanek 2009). We empirically found that this
technique has accuracy well above 80% (and higher
for prominent and thus frequently occurring enti-
ties). In our example, the entity detection yields the
entities Amy Winehouse and Rehab (song).
Whenever two named entities appear in the same
sentence, we extract a textual pattern. For this pur-
pose, we traverse the dependency graph to get the
shortest path that connects the two entities. In the
example, the shortest path between ?Winehouse?
and ?Rehab? is: Winehouse nsubj performed dobj
Rehab. In order to capture only relations that refer
to subject-relation-object triples, we only consider
shortest paths that start with subject-like dependen-
cies, such as nsubj, rcmod and partmod. To re-
flect the full meaning of the patterns, we expand the
shortest path with adverbial and adjectival modifiers,
for example the advmod dependency. The sequence
of words on the expanded shortest path becomes our
final textual pattern. In the example, the textual pat-
tern is Amy Winehouse effortlessly performed Rehab
(song).
4 SOL Pattern Model
Textual patterns are tied to the particular surface
form of the text. Therefore, we transform the textual
patterns into a new type of patterns, called syntactic-
ontologic-lexical patterns (SOL patterns). SOL pat-
terns extend lexico-syntactic patterns by ontological
type signatures for entities. The SOL pattern lan-
guage is expressive enough to capture fine-grained
relational patterns, yet simple enough to be dealt
with by efficient mining algorithms at Web scale.
A SOL pattern is an abstraction of a textual pat-
tern that connects two entities of interest. It is a
sequence of words, POS-tags, wildcards, and onto-
logical types. A POS-tag stands for a word of the
part-of-speech class. We introduce the special POS-
tag [word], which stands for any word of any POS
class. A wildcard, denoted ?, stands for any (pos-
sibly empty) sequence of words. Wildcards are es-
sential to avoid overfitting of patterns to the corpus.
An ontological type is a semantic class name (such
as ?singer?) that stands for an instance of that class.
Every pattern contains at least two types, and these
are designated as entity placeholders.
A string and a pattern match, if there is an order-
preserving bijection from sequences of words in the
string to items in the pattern, so that each item can
stand for the respective sequence of words. For ex-
ample, the pattern ?person??s [adj] voice * ?song?
matches the strings ?Amy Winehouse?s soft voice
in ?Rehab?? and ?Elvis Presley?s solid voice in his
song ?All shook up??. The type signature of a pat-
tern is the pair of the entity placeholders. In the ex-
ample, the type signature is person ? song. The
support set of a pattern is the set of pairs of entities
that appear in the place of the entity placeholders
in all strings in the corpus that match the pattern.
In the example, the support set of the pattern could
be {(Amy,Rehab), (Elvis, AllShookUp)}. Each
pair is called a support pair of the pattern.
Pattern B is syntactically more general than pat-
tern A if every string that matches A also matches
B. Pattern B is semantically more general than A
if the support set of B is a superset of the support
set of A. If A is semantically more general than B
and B is semantically more general than A, the pat-
terns are called synonymous. A set of synonymous
patterns is called a pattern synset. Two patterns, of
which neither is semantically more general than the
other, are called semantically different.
To generate SOL patterns from the textual pat-
terns, we decompose the textual patterns into n-
grams (n consecutive words). A SOL pattern con-
tains only the n-grams that appear frequently in the
corpus and the remaining word sequences are re-
placed by wildcards. For example, in the sentence
?was the first female to run for the governor of?
might give rise to the pattern * the first female * gov-
ernor of, if ?the first female? and ?governor of? are
frequent in the corpus.
To find the frequent n-grams efficiently, we apply
the technique of frequent itemset mining (Agrawal
1993; Srikant 1996): each sentence is viewed as a
?shopping transaction? with a ?purchase? of several
n-grams, and the mining algorithm computes the n-
gram combinations with large co-occurrence sup-
port1. These n-grams allow us to break down a sen-
1Our implementation restricts n-grams to length 3 and uses
up to 4 n-grams per sentence
1138
tence into wildcard-separated subsequences, which
yields an SOL pattern. We generate multiple pat-
terns with different types, one for each combination
of types that the detected entities have in the under-
lying ontology.
We quantify the statistical strength of a pattern by
means of its support set. For a given pattern p with
type signature t1 ? t2, the support of p is the size
of its support set. For confidence, we compare the
support-set sizes of p and an untyped variant pu of
p, in which the types ?t1? and ?t2? are replaced by
the generic type ?entity?. We define the confidence
of p as the ratio of the support-set sizes of p and pu.
5 Syntactic Pattern Generalization
Almost every pattern can be generalized into a syn-
tactically more general pattern in several ways: by
replacing words by POS-tags, by introducing wild-
cards (combining more n-grams), or by generaliz-
ing the types in the pattern. It is not obvious which
generalizations will be reasonable and useful. We
observe, however, that generalizing a pattern may
create a pattern that subsumes two semantically dif-
ferent patterns. For example, the generalization
?person? [vb] ?person? subsumes the two semanti-
cally different patterns ?person? loves ?person? and
?person? hates ?person?. This means that the pattern
is semantically meaningless.
Therefore, we proceed as follows. For every pat-
tern, we generate all possible generalizations. If a
generalization subsumes multiple patterns with dis-
joint support sets, we abandon the generalized pat-
tern. Otherwise, we add it to our set of patterns.
6 Semantic Pattern Generalization
The main difficulty in generating semantic subsump-
tions is that the support sets may contain spurious
pairs or be incomplete, thus destroying crisp set in-
clusions. To overcome this problem, we designed
a notion of a soft set inclusion, in which one set S
can be a subset of another set B to a certain degree.
One possible measure for this degree is the confi-
dence, i.e., the ratio of elements in S that are in B,
deg(S ? B) = |S ? B|/|S|. However, if a support
set S has only few elements due to sparsity, it may
become a subset of another support setB, even if the
two patterns are semantically different. Therefore,
one has to take into account also the support, i.e., the
size of the set S. Traditionally, this is done through a
weighted trade-off between confidence and support.
To avoid the weight tuning, we instead devised
a probabilistic model. We interpret S as a random
sample from the ?true? support set S? that the pattern
would have on an infinitely large corpus. We want
to estimate the ratio of elements of S? that are in
B. This ratio is a Bernoulli parameter that can be
estimated from the ratio of elements of the sample S
that are in B. We compute the Wilson score interval
[c ? d, c + d] (Brown 2001) for the sample. This
interval guarantees that with a given probability (set
a priori, usually to ? = 95%), the true ratio falls into
the interval [c ? d, c + d]. If the sample is small, d
is large and c is close to 0.5. If the sample is large,
d decreases and c approaches the naive estimation
|S ? B|/|S|. Thereby, the Wilson interval center
naturally balances the trade-off between confidence
and the support. Hence we define deg(S ? B) = c.
This estimator may degrade when the sample size
is too small We can alternatively use a conservative
estimator deg(S ? B) = c?d, i.e., the lower bound
of the Wilson score interval. This gives a low score
to the case where S ? B if we have few samples (S
is small).
7 Taxonomy Construction
We now have to arrange the patterns in a semantic
taxonomy. A baseline solution would compare ev-
ery pattern support set to every other pattern support
set in order to determine inclusion, mutual inclusion,
or independence. This would be prohibitively slow.
For this reason, we make use of a prefix-tree for fre-
quent patterns (Han 2005). The prefix-tree stores
support sets of patterns. We then developed an algo-
rithm for obtaining set intersections from the prefix-
tree.
7.1 Prefix-Tree Construction
Suppose we have pattern synsets and their support
sets as shown in Table 1. An entity pair in a support
set is denoted by a letter. For example, in the sup-
port set for the pattern ?Politican? was governor
of ?State?, the entry ?A,80? may denote the entity
1139
ID Pattern Synset & Support Sets
P1 ?Politician? was governor of ?State?
A,80 B,75 C,70
P2 ?Politician? politician from ?State?
A,80 B,75 C,70 D,66 E,64
P3 ?Person? daughter of ?Person?
F,78 G,75 H,66
P4 ?Person? child of ?Person?
I,88 J,87 F,78 G,75 K,64
Table 1: Pattern Synsets and their Support Sets
Root 
A p1,p2 
B 
C 
D 
p1,p2 
p1,p2 
p2 
E p2 
F 
G 
H 
p3 I 
J 
F 
p4 
G p4 
K p4 
p4 
p4 p3 p3 
Figure 1: Prefix-Tree for the Synsets in Table 1.
pair Arnold Schwarzenegger, California, with an oc-
currence frequency 80. The contents of the support
sets are used to construct a prefix-tree, where nodes
are entity pairs. If synsets have entity pairs in com-
mon, they share a common prefix; thus the shared
parts can be represented by one prefix-path in the
tree. This enables subsumptions to be directly ?read
off? from the tree, while representing the tree in a
compact manner. To increase the chance of shared
prefixes, entity pairs are inserted into the tree in de-
creasing order of occurrence frequency.
The prefix-tree of support sets is a prefix-tree aug-
mented with synset information stored at the nodes.
Each node (entity pair) stores the identifiers of the
pattern sysnets whose support sets contain that en-
tity pair. In addition, each node stores a link to the
next node with the same entity pair.
Figure 1 shows the tree for the pattern synsets
in Table 1. The left-most path contains synsets P1
and P2. The two patterns have a prefix in common,
thus they share the same path. This is reflected by
the synsets stored in the nodes in the path. Synsets
P2 and P3 belong to two different paths due to dis-
similar prefixes although they have common nodes.
Instead, their common nodes are connected by the
same-entity-pair links shown as dotted lines in Fig-
ure 1. These links are created whenever the entity
pair already exists in the tree but with a prefix differ-
ent from the prefix of the synset being added to the
tree. The size of the tree is at most the total num-
ber of entity pairs making up the supports sets of the
synsets. The height of the tree is at most the size of
the the largest support set.
7.2 Mining Subsumptions from the Prefix-Tree
To efficiently mine subsumptions from the prefix-
tree, we have to avoid comparing every path to every
other path as this introduces the same inefficiencies
that the baseline approach suffers from.
From the construction of the tree it follows that
for any node Ni in the tree, all paths containing Ni
can be found by following node Ni?s links includ-
ing the same-entity-pair links. By traversing the en-
tire path of a synset Pi, we can reach all the pattern
synsets sharing common nodes with Pi. This leads
to our main insight: if we start traversing the tree
bottom up, starting at the last node in P ?is support
set, we can determine exactly which paths are sub-
sumed by Pi. Traversing the tree this way for all
patterns gives us the sizes of the support set intersec-
tion. The determined intersection sizes can then be
used in the Wilson estimator to determine the degree
of semantic subsumption and semantic equivalence
of patterns.
7.3 DAG Construction
Once we have generated subsumptions between re-
lational patterns, there might be cycles in the graph
we generate. We ideally want to remove the minimal
total number of subsumptions whose removal results
in an a directed acyclic graph (DAG). This task is
related to the minimum feedback-arc-set problem:
given a directed graph, we want to remove the small-
est set of edges whose removal makes the remaining
graph acyclic. This is a well known NP-hard prob-
lem (Kann 1992). We use a greedy algorithm for
1140
removing cycles and eliminating redundancy in the
subsumptions, thus effectively constructing a DAG.
Starting with a list of subsumption edges ordered by
decreasing weights, we construct the DAG bottom-
up by adding the highest-weight subsumption edge.
This step is repeated for all subsumptions, where we
add a subsumption to the DAG only if it does not
introduce cycles or redundancy. Redundancy occurs
when there already exists a path, by transitivity of
subsumptions, between pattern synsets linked by the
subsumption. This process finally yields a DAG of
pattern synsets ? the PATTY taxonony.
8 Experimental Evaluation
8.1 Setup
The PATTY extraction and mining algorithms were
run on two different input corpora: the New York
Times archive (NYT) which includes about 1.8 Mil-
lion newspaper articles from the years 1987 to 2007,
and the English edition of Wikipedia (WKP), which
contains about 3.8 Million articles (as of June 21,
2011). Experiments were carried out, for each cor-
pus, with two different type systems: a) the type sys-
tem of YAGO2, which consists of about 350,000 se-
mantic classes from WordNet and the Wikipedia cat-
egory system, and b) the two-level domain/type hier-
archy of Freebase which consists of 85 domains and
a total of about 2000 types within these domains.
All relational patterns and their respective entity
pairs are stored in a MongoDB database. We evalu-
ated PATTY along four dimensions: quality of pat-
terns, quality of subsumptions, coverage, and de-
sign alternatives. These dimensions are discussed
in the following four subsections. We also per-
formed an extrinsic study to demonstrate the use-
fulness of PATTY for paraphrasing the relations
of DBpedia and YAGO2. In terms of runtimes,
he most expensive part is the pattern extraction,
where we identify pattern candidates through de-
pendency parsing and perform entity recognition
on the entire corpus. This phase runs about a
day for Wikipedia a cluster. All other phases of
the PATTY system take less than an hour. All
experimental data is available on our Web site at
www.mpi-inf.mpg.de/yago-naga/patty/.
8.2 Precision of Relational Patterns
To assess the precision of the automatically mined
patterns (patterns in this section always mean pattern
synsets), we sampled the PATTY taxonomy for each
combination of input corpus and type system. We
ranked the patterns by their statistical strength (Sec-
tion 4), and evaluated the precision of the top 100
pattern synsets. Several human judges were shown
a sampled pattern synset, its type signature, and a
few example instances, and then stated whether the
pattern synset indicates a valid relation or not. Eval-
uators checked the correctness of the type signature,
whether the majority of patterns in the synset is rea-
sonable, and whether the instances seem plausible.
If so, the synset was flagged as meaningful. The re-
sults of this evaluation are shown in column four of
Table 2, with a 0.9-confidence Wilson score inter-
val (Brown 2001). In addition, the same assessment
procedure was applied to randomly sampled synsets,
to evaluate the quality in the long tail of patterns.
The results are shown in column five of Table 2. For
the top 100 patterns, we achieve above 90% preci-
sion for Wikipedia, and above 80% for 100 random
samples.
Corpus Types Patterns Top 100 Random
NYT
YAGO2 86,982 0.89?0.06 0.72?0.09
Freebase 809,091 0.87 ?0.06 0.71?0.09
WKP
YAGO2 350,569 0.95?0.04 0.85?0.07
Freebase 1,631,531 0.93?0.05 0.80?0.08
Table 2: Precision of Relational Patterns
From the results we make two observations. First,
Wikipedia patterns have higher precision than those
from the New York Times corpus. This is because
some the language in the news corpus does not ex-
press relational information; especially the news on
stock markets produced noisy patterns picked up by
PATTY. However, we still manage to have a preci-
sion of close to 90% for the top 100 patterns and
around 72% for random sample on the NYT cor-
pus. The second observation is that the YAGO2
type system generally led to higher precision than
the Freebase type system. This is because YAGO2
has finer grained, ontologically clean types, whereas
Freebase has broader categories with a more liberal
1141
assignment of entities to categories.
8.3 Precision of Subsumptions
We evaluated the quality of the subsumptions by
assessing 100 top-ranked as well as 100 randomly
selected subsumptions. As shown in Table 3, a
large number of the subsumptions are correct. The
Wikipedia-based PATTY taxonomy has a random-
sampling-based precision of 75%.
Corpus Types # Edges Top 100 Random
NYT
YAGO2 12,601 0.86?0.07 0.68?0.09
Freebase 80,296 0.89?0.06 0.41?0.09
WKP
YAGO2 8,162 0.83?0.07 0.75?0.07
Freebase 20,339 0.85?0.07 0.62?0.09
Table 3: Quality of Subsumptions
Example subsumptions from Wikipedia are:
? ?person? nominated for ?award? =
?person? winner of ?award?
? ?person? ? s wife ?person? =
?person? ?s widow ?person?
8.4 Coverage
To evaluate the coverage of PATTY, we would need
a complete ground-truth resource that contains all
possible binary relations between entities. Unfor-
tunately, there is no such resource2. We tried to
approximate such a resource by manually compil-
ing all binary relations between entities that ap-
pear in Wikipedia articles of a certain domain. We
chose the domain of popular music, because it offers
a plethora of non-trivial relations (such as addict-
edTo(person,drug), coveredBy(musician,musician),
dedicatedSongTo(musician,entity))). We considered
the Wikipedia articles of five musicians (Amy Wine-
house, Bob Dylan, Neil Young, John Coltrane, Nina
Simone). For each page, two annotators hand-
extracted all relationship types that they would spot
in the respective articles. The annotators limited
themselves to relations where at least one argument
type is ?musician?. Then we formed the intersection
of the two annotators? outputs (i.e., their agreement)
2Lexical resources such as WordNet contain only verbs, but
not binary relations such as is the president of. Other resources
are likely incomplete.
as a reasonable gold standard for relations identifi-
able by skilled humans. In total, the gold-standard
set contains 163 relations.
We then compared our relational patterns to the
relations included in four major knowledge bases,
namely, YAGO2, DBpedia (DBP), Freebase (FB),
and NELL, limited to the specific domain of music.
Table 4 shows the absolute number of relations cov-
ered by each resource. For PATTY, the patterns were
derived from the Wikipedia corpus with the YAGO2
type system.
gold standard PATTY YAGO2 DBP FB NELL
163 126 31 39 69 13
Table 4: Coverage of Music Relations
PATTY covered 126 of the 163 gold-standard re-
lations. This is more than what can be found in large
semi-curated knowledge bases such as Freebase,
and twice as much as Wikipedia-infobox-based re-
sources such as DBpedia or YAGO offer. Some
PATTY examples that do not appear in the other re-
sources at all are:
? ?musician? PRP idol ?musician? for the relation
hasMusicalIdol
? ?person? criticized by ?organization? for
critizedByMedia
? ?person? headliner ?artifact? for headlinerAt
? ?person? successfully sued ?person? for suedBy
? ?musician? wrote hits for ?musician? for wrote-
HitsFor,
This shows (albeit anecdotically) that PATTY?s pat-
terns contribute added value beyond today?s knowl-
edge bases.
8.5 Pattern Language Alternatives
We also investigated various design alternatives to
the PATTY pattern language. We looked at three
main alternatives: the first is verb-phrase-centric
patterns advocated by ReVerb (Fader 2011), the sec-
ond is the PATTY language without type signatures
(just using sets of n-grams with syntactic general-
izations), and the third one is the full PATTY lan-
guage. The results for the Wikipedia corpus and the
1142
Reverb-style patterns PATTY without types PATTY full
# Patterns 5,996 184,629 350,569
Patterns Precision 0.96?0.03 0.74?0.08 0.95?0.04
# Subsumptions 74 15,347 8,162
Subsumptions Precision 0.79 ?0.09 0.58?0.09 0.83?0.07
# Facts 192,144 6,384,684 3,890,075
Facts Precision. 0.86 ?0.07 0.64?0.09 0.88 ?0.06
Table 5: Results for Different Pattern Language Alternatives
Relation Paraphrases Precision Sample Paraphrases
DBPedia/artist 83 0.96?0.03 [adj] studio album of, [det] song by . . .
DBPedia/associatedBand 386 0.74?0.11 joined band along, plays in . . .
DBPedia/doctoralAdvisor 36 0.558?0.15 [det] student of, under * supervision . . .
DBPedia/recordLabel 113 0.86?0.09 [adj] artist signed to, [adj] record label . . .
DBPedia/riverMouth 31 0.83?0.12 drains into, [adj] tributary of . . .
DBPedia/team 1,108 0.91?0.07 be * traded to, [prp] debut for . . .
YAGO/actedIn 330 0.88?0.08 starred in * film, [adj] role for . . .
YAGO/created 466 0.79?0.10 founded, ?s book . . .
YAGO/isLeaderOf 40 0.53?0.14 elected by, governor of . . .
YAGO/holdsPoliticalPosition 72 0.73?0.10 [prp] tenure as, oath as . . .
Table 6: Sample Results for Relation Paraphrasing
YAGO2 type system are shown in Table 5; preci-
sion figures are based on the respective top 100 pat-
terns or subsumption edges. We observe from these
results that the type signatures are crucial for pre-
cision. Moreover, the number of patterns, subsump-
tions and facts found by verb-phrase-centric patterns
(ReVerb (Fader 2011)), are limited in recall. Gen-
eral pattern synsets with type signatures, as newly
pursued in this paper, substantially outperform the
verb-phrase-centric alternative in terms of pattern
and subsumption recall while yielding high preci-
sion.
8.6 Extrinsic Study: Relation Paraphrasing
To further evaluate the usefulness of PATTY, we per-
formed a study on relation paraphrasing: given a re-
lation from a knowledge base, identify patterns that
can be used to express that relation. Paraphrasing
relations with high-quality patterns is important for
populating knowledge bases and counters the prob-
lem of semantic drifting caused by ambiguous and
noisy patterns.
We considered relations from two knowledge
bases, DBpedia and YAGO2, focusing on relations
that hold between entities and do not include literals.
PATTY paraphrased 225 DBpedia relations with a
total of 127,811 patterns, and 25 YAGO2 relations
with a total of 43,124 patterns. Among these we
evaluated a random sample of 1,000 relation para-
phrases. Table 6 shows precision figures for some
selected relations, along anecdotic example patterns.
Some relations are hard to capture precisely. For
DBPedia/doctoralAdvisor, e.g., PATTY picked up
patterns like ?worked with? as paraphrases. These
are not entirely wrong, but we evaluated them as
false because they are too general to indicate the
more specific doctoral advisor relation.
Overall, however, the paraphrasing precision is
high. Our evaluation showed an average precision
of 0.76?0.03 across all relations.
9 Conclusion and Future Directions
This paper presented PATTY, a large resource of text
patterns. Different from existing resources, PATTY
organizes patterns into synsets and a taxonomy, sim-
ilar in spirit to WordNet. Our evaluation shows
that PATTY?s patterns are semantically meaning-
ful, and that they cover large parts of the relations
of other knowledge bases. The Wikipedia-based
version of PATTY contains 350,569 pattern synsets
at a precision of 84.7%, with 8,162 subsumptions,
at a precision of 75%. The PATTY resource is
1143
freely available for interactive access and download
at www.mpi-inf.mpg.de/yago-naga/patty/.
Our approach harnesses existing knowledge bases
for entity-type information. However, PATTY is not
tied to a particular choice for this purpose. In fact,
it would be straightforward to adjust PATTY to us-
ing surface-form noun phrases rather than disam-
biguated entities, as long as we have means to infer
at least coarse-grained types (e.g., person, organiza-
tion, location). An interesting future direction is to
study this generalized setting. We would also like
to investigate the enhanced interplay of information
extraction and pattern extraction, and possible appli-
cations for question answering.
References
Ion Androutsopoulos, Prodromos Malakasiotis: A Sur-
vey of Paraphrasing and Textual Entailment Methods.
Journal of Artificial Intelligence Research 38: 135?
187, 2010
Rakesh Agrawal, Tomasz Imielinski, Arun N. Swami:
Mining Association Rules between Sets of Items in
Large Databases. SIGMOD Conference 1993
Enrique Alfonseca, Marius Pasca, Enrique Robledo-
Arnuncio: Acquisition of instance attributes via la-
beled and related instances. SIGIR 2010
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, Zachary G. Ives: DBpe-
dia: A Nucleus for a Web of Open Data. ISWC 2007,
data at http://dbpedia.org
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, Oren Etzioni: Open Information
Extraction from the Web. IJCAI 2007
Jonathan Berant, Ido Dagan, Jacob Goldberger: Global
Learning of Typed Entailment Rules. ACL 2011
Kurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, Jamie Taylor: Freebase: a collaboratively
created graph database for structuring human knowl-
edge. SIGMOD Conference 2008, data at http://
freebase.com
Lawrence D. Brown, T.Tony Cai, Anirban Dasgupta: In-
terval Estimation for a Binomial Proportion. Statistical
Science 16: 101?133, 2001
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., Tom M. Mitchell.
Toward an Architecture for Never-Ending Language
Learning. AAAI 2010, data at http://rtw.ml.
cmu.edu/rtw/
Timothy Chklovski, Patrick Pantel: VerbOcean: Mining
the Web for Fine-Grained Semantic Verb Relations.
EMNLP 2004; data available at http://demo.
patrickpantel.com/demos/verbocean/
Anthony Fader, Stephen Soderland, Oren Etzioni:
Identifying Relations for Open Information Extrac-
tion. EMNLP 2011, data at http://reverb.cs.
washington.edu
Christiane Fellbaum (Editor): WordNet: An Electronic
Lexical Database. MIT Press, 1998
Jiawei Han, Jian Pei , Yiwen Yin : Mining frequent pat-
terns without candidate generation. SIGMOD 2000.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Stijn De Saeger, Masaki Murata, Jun?ichi Kazama:
Large-Scale Verb Entailment Acquisition from the
Web. EMNLP 2009
Catherine Havasi, Robert Speer, and Jason Alonso. Con-
ceptNet 3: a Flexible, Multilingual Semantic Net-
work for Common Sense Knowledge, RANLP 2007;
data available at http://conceptnet5.media.
mit.edu/
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid O Seaghdha, Sebastian Pado, Marco
Pennacchiotti, Lorenza Romano, Stan Szpakowicz:
SemEval-2010 Task 8: Multi-Way Classification of
Semantic Relations Between Pairs of Nominals, 5th
ACL International Workshop on Semantic Evaluation,
2010; data available at http://www.isi.edu/
?kozareva/downloads.html
Johannes Hoffart, Fabian Suchanek, Klaus Berberich,
Edwin Lewis-Kelham, Gerard de Melo, Ger-
hard Weikum: YAGO2: Exploring and Query-
ing World Knowledge in Time, Space, Con-
text, and Many Languages. WWW 2011, data at
http://yago-knowledge.org
Raphael Hoffmann, Congle Zhang, Daniel S. Weld:
Learning 5000 Relational Extractors. ACL 2010
Vigo Kann: On the approximability of NP-complete opti-
mization problems. PhD thesis, Department of Numer-
ical Analysis and Computing Science, Royal Institute
of Technology, Stockholm. 1992.
Karin Kipper, Anna Korhonen, Neville Ryant,
Martha Palmer, A Large-scale Classification of
English Verbs, Language Resources and Evalua-
tion Journal, 42(1): 21-40, 2008, data available at
http://verbs.colorado.edu/?mpalmer/
projects/verbnet/downloads.html
Zornitsa Kozareva, Eduard H. Hovy: Learning Argu-
ments and Supertypes of Semantic Relations Using
Recursive Patterns. ACL 2010
Girija Limaye, Sunita Sarawagi, Soumen Chakrabarti:
Annotating and Searching Web Tables Using Entities,
Types and Relationships. PVLDB 3(1): 1338-1347
(2010)
Dekang Lin, Patrick Pantel: DIRT: discovery of inference
rules from text. KDD 2001
1144
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. Generating Typed Depen-
dency Parses from Phrase Structure Parses. LREC
2006.
Thahir Mohamed, Estevam R. Hruschka Jr., Tom M.
Mitchell: Discovering Relations between Noun Cat-
egories. EMNLP 2011
Ndapandula Nakashole, Martin Theobald, Gerhard
Weikum: Scalable Knowledge Harvesting with High
Precision and High Recall. WSDM 2011
Preslav Nakov, Marti A. Hearst: Solving Relational Simi-
larity Problems Using the Web as a Corpus. ACL 2008
Vivi Nastase, Michael Strube, Benjamin Boerschinger,
Ca?cilia Zirn, Anas Elghafari: WikiNet: A Very Large
Scale Multi-Lingual Concept Network. LREC 2010,
data at http://www.h-its.org/english/
research/nlp/download/wikinet.php
Roberto Navigli, Simone Paolo Ponzetto: BabelNet:
Building a Very Large Multilingual Semantic Net-
work. ACL 2010 data at http://lcl.uniroma1.
it/babelnet/
Marius Pasca, Benjamin Van Durme: What You Seek Is
What You Get: Extraction of Class Attributes from
Query Logs. IJCAI 2007
Marius Pasca, Benjamin Van Durme: Weakly-Supervised
Acquisition of Open-Domain Classes and Class At-
tributes from Web Documents and Query Logs. ACL
2008
Andrew Philpot, Eduard Hovy, Patrick Pantel: The
Omega Ontology, in: Ontology and the Lexicon,
Cambridge University Press, 2008, data at http:
//omega.isi.edu/
Simone Paolo Ponzetto, Michael Strube: Deriving a
Large-Scale Taxonomy from Wikipedia. AAAI 2007,
data at http://www.h-its.org/english/
research/nlp/download/wikitaxonomy.
php
Joseph Reisinger, Marius Pasca: Latent Variable Models
of Concept-Attribute Attachment. ACL/AFNLP 2009
Ramakrishnan Srikant, Rakesh Agrawal: Mining Se-
quential Patterns: Generalizations and Performance
Improvements. EDBT 1996
Fabian M. Suchanek, Gjergji Kasneci, Gerhard Weikum:
YAGO: a Core of Semantic Knowledge. WWW 2007
Fabian M. Suchanek, Mauro Sozio, Gerhard Weikum:
SOFIE: a self-organizing framework for information
extraction. WWW 2009
Lin Sun, Anna Korhonen: Hierarchical Verb Clustering
Using Graph Factorization. EMNLP 2011
Petros Venetis, Alon Y. Halevy, Jayant Madhavan, Marius
Pasca, Warren Shen, Fei Wu, Gengxin Miao, Chung
Wu: Recovering Semantics of Tables on the Web.
PVLDB 4(9): 528-538, 2011
Tom White: Hadoop: The Definitive Guide, 2nd Edition.
O?Reilly, 2010.
Fei Wu, Daniel S. Weld: Automatically refining the wiki-
pedia infobox ontology. WWW 2008
Wentao Wu, Hongsong Li, Haixun Wang, Kenny Q. Zhu:
Towards a Probabilistic Taxonomy of Many Concepts.
Technical Report MSR-TR-2011-25, Microsoft Re-
search, 2011
Limin Yao, Aria Haghighi, Sebastian Riedel, Andrew
McCallum: Structured Relation Discovery using Gen-
erative Models. EMNLP 2011
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, Ji-Rong
Wen: StatSnowball: a statistical approach to extracting
entity relationships. WWW 2009
1145
