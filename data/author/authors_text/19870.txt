AnnCorra : Building Tree-banks in  Indian Languages
Akshar Bharati
Rajeev Sangal
Vineet Chaitanya
Amba Kulkarni
Dipti Misra Sharma
International Institute of Information Technology
Hyderabad, india
{sangal, vc, amba, dipti}@iiit.net
K.V. Ramakrishnamacharyulu
Rashtirya Sanskrit Vidyapeetha, Tirupati, India
kvrk@sansknet.org
ABSTRACT
This paper describes a dependency based
tagging scheme  for creating tree banks for
Indian languages.  The scheme has been so
designed that  it is comprehensive,  easy to use
with linear notation and economical in typing
effort.  It is based on Paninian grammatical
model.
1.BACKGROUND
The name AnnCorra, shortened for
"Annotated Corpora", is for an electronic
lexical resource of annotated corpora. The
purpose behind this effort is to fill the
lacuna in such resources for Indian
languages. It will be an important resource
for the development of Indian language
parsers, machine learning of grammars,
lakshancharts (discrimination nets for
sense disambiguation) and a host of other
such tools.
 2. AIMS AND OBJECTIVE
      The aim of the project is to :
- develop a generalised linear
            syntacto- semantic tag scheme  for
            all Indian  languages
- annotate training corpus for all
             Indian  languages
      -     develop parallel tree-banks for all
             Indian languages
To fulfill the above aim - a marathon task
-  a collaborative model has been
concieved. Any collaborative model
implies involvement of several people
with varying levels of expertise. This case,
becomes further complicated as the tag
scheme to be designed has to be equally
efficient for all the Indian languages.
These languages, though quite similar,
are not identical in their syntactic
structures.  Thus the tag scheme demands
the following properties  :-
-  comprehensive enough to capture
   various sysntactic relations across
    languages.
     -  simple enough for anyone, with some
        background in linguistics,  to use.
     -  economical  in typing effort (the
        corpus has to be manually
        annotated).
3. AN ILLUSTRATION
The task can be better understood with the
help of an  illustration. Look at the
following sentence  from Hindi
0:: rAma  ne            moHana   ko
     'Rama'  'ErgPostP'  'Mohan'   'PostP'
      nIlI kitAba dI
     'blue' 'book'  'gave'
   'Rama gave the blue book to Mohan.'
Tree-1 is a representation of the above
verb, argument relationship within the
various constituents of sentence  0 -
                             dI
                 -------------------------
                 |            |                   |
            k1 |        k4|               k2|
                 |            |                   |
      rAma_ne  moHana_ko   kitAba
                                                 |
                                                 |nmod
                                                 |
                                              nIlI
                     Tree-1
Since the input for tagging is a text corpus
and the marking has to be done manually,
the tagging scheme is linearly designed.
Therefore, Sentence 0 will be marked as
follows -
rAma_ne/k1 moHana_ko/k4  [nIlI
'Ram postp'    Mohan postp'    'blue '
kitAba]/k2 dI::v
?book?        'gave'
The markings here represent
-     'di' (?give?) is the verb node
- ?rAma_ne' is  the 'karta' or  'agent'
(k1)
 
    of the verb 'dI',
- 'moHana_ko' is 'sampraadana' or
 
     'beneficiary' (k4) of verb 'dI' (?give?)
- '[nIlI kitAba]' ? (blue book) a noun
phrase - is the  'karma' or 'object' (k2)
of  the verb.
The elements joined by an underscore
represent one unit. Postpositions which
are separated by white space in the written
texts are actually the inflections of the
preceding noun or verb units. Therefore,
they are conjoined.
The modifier-modified elements are
paranthesised within square brackets.
Tags showing the name of the ARC (or
branch) are marked by '/'   immediately
after the constituent they relate to. '/' is
followed by the appropriate tagname.
Thus '/' specifies a relationship of a word
or constituent with another word or
constituent. In this case it is the
relationship of verb 'dI' with the other
elements in the sentence.
Tags denoting a type of node are  marked
by '::'.   '::v' indicates that 'dI' is a verbal
node.
The idea here is to mark only the specific
grammatical information. Certain
DEFAULT CONVENTIONS are left
unmarked. For example, the adjective 'nIlI'
(?blue?) of  'kitAba' (?book) has been left
unmarked in the above example since
normally noun modifiers precede the noun
they modify (adjectives precede nouns).
Such DEFAULT CONVENTIONS save
unnecessary typing effort.
4. GRAMMATICAL MODEL
It was quite natural to use Paninian
grammatical model for sentence analysis
 ( hence the tagnames) because :-
1) Paninian grammatical model is based
 
     on the analysis of an Indian  language
 
     (Sanskrit) it can deal better with the
 
    type of  constructions Indian languages
 
     have.
 
2) The model not only offers a
      mechanism  for SYNTACTIC analysis
      but also incorporates the SEMANTIC
      information (nowadays called
      dependency analysis). Thus making
      the   relationships more transparent.
      (For  details refer Bharati (1995).)
Following tags (most of which are  based
on Paninian grammatical model) have
been used in the above example.
k1 : kartaa (subject or agent)
k2 : karma (object)
k4 : sampradaana (beneficiary)
v  : kriyaa (verb)
Obviously the task is not an easy one.
Standardization of these tags will take
some time. Issues, while deciding the tags,
are many. Some examples are illustrated
below to show the kind of structures
which the linear tagging scheme will have
to deal with.
4.1. Multiple Verb Sentences
To mark the nouns-verb relations with the
above tags in single verb sentences is a
simple task. However, consider the
following sentence with two verbs :-
    1:   rAma ne   khAnA  khAkara
          am' 'postp' 'food'    'having_eaten'
           pAnI    pIyA
           'water'  'drank'
          `Ram drank water after eating the
           food.`
Sentence 1 has more than two verbs - one
non-finite (khAkara) and one finite
(piyA). The finite verb is the main verb.
Noun 'khAnA' is the object of verb
'khAkara', whereas noun 'pAnI' is the
object of verb 'piyA'. 'k2' is the tag for
object relation in our tagging scheme. Co-
indexing becomes the obvious solution for
such multiple relations.   Since there are
two verbs the tagging scheme allows them
to be named as   'i' and 'j' (using notation 'i'
and 'j'). By default 'i' refers to the main
verb and any successive verb by other
characters ('j' in the present case):
 rAma_ne      khAnA   khAkara::vkr:j
  'Ram_postp'  'food'      'having_eaten:j'
   pAnI   piyA::v:i
   'water' 'drank:i'
    This provides the facility to mark every
noun verb relationship.
rAma_ne/k1>i   khAnA/k2>j
khAkara::vkr:j   pAnI/k2>i piyA::v:i
Fortunately, there is no need to mark it so
"heavily". A number of notations can be
left out, and the DEFAULT rules tell us
how to interpret such "abbreviated'
annotation. Thus, for the above sentence,
the following annotation is sufficient and
is completely equivalent to the above :
     rAma_ne/k1   khAnA/k2
khAkara::vkr:j   pAnI/k2   piyA::v
Even though there are two verbs, there is
no need to name the verbs and refer to
them. Two default rules help us achieve
such brevity (without any ambiguity) :
(1) karta or k1 kaaraka always attaches to
 
     the last verb in a  sentence (Thus
 
    'rAma_ne/k1' attaches to the verb at
 
     the   end).
(2) all other kaarakas except k1, attach to
      the nearest verb on the right. Thus
      'khAnA/k2' attaches to 'khAkara' and
      'pAnI/k2' attaches to 'piyA', their
      respective nearest verbs on the right.
4.2. Compound Units
Sometimes two words combine together to
form a unit which has its own demands
and modifiers, not derivable from its parts.
For example, a noun and verb join
together to operate as a single unit,
namely as a verb. In the sentence 'rAma
(Rama) ne (postp) snAna(bath) kiyA
(did)',  'snAna'   and 'kiyA' together stand
for a verb 'snAna+kiyA' (bathed). Such
verbal compounds are like any other verb
having their own kaarakas.This sentence
would be marked as follows :
    rAma_ne/k1   snAna::v+ kiyA::v-
    'Ram_postp'   'bath+'       'did-'
     `Ram took a bath`
A 'v+' or a 'v-' indicates that the word
'snAna' or 'kiyA' are parts of a whole  (a
verb in this case). Taken together they
function as a single verb unit.  Such a
device which may appear to be more
powerful was needed to mark the 'single
unitness' of parts which may appear
separately in a sentence. Thus, the above
notation  allows even distant words to be
treated as a  single compound. Such
occurrences are fairly common in all
Indian languages as illustrated in the
following example from Hindi :
snAna::v+ to        mEMne/k1
 'bath'      'emph'    'I_erg'
subaHa_HI           kara_liyA_thA::v-
'morning_emph'     'had_done'
I had bathed (taken a bath) in the morning
itself.
'+'and ' - ' help in marking this relation
explicitly. (a more detail description of the
notation in 5.1)
4.3. Embedded Sentence
Tags are also designed to mark the
relations within  a complex sentence.
Consider the example below where a
complete sentence (having verb  'piyA'
(drank)) is a kaaraka of the main verb
'kaHA' (said).
moHana    ne        kaHA  ki      {rAma
'Mohan'  'postp' 'said'  'that ' {'Rama'
ne         pAnI     khAnA    khAkara
'postp' 'water' 'food'    'having eaten'
 piyA}.
'drank}
     (Mohan said that Ram drank water
after having eaten the food)
The embedded sentence can be first
marked as follows -
     --------- {rAma_ne/k1 pAnI/k2>j
khAnA/k2 khAkara::vkr piyA::v:j}::s.
The whole embedded sentence is the
'karma'  (object) or k2 of 'piyA' (drank):
The  relation  of the embedded sentence
relation as the object of the main verb is
co-indexed in the following way :-
 moHana_ne        kaHA::v:i      ki
  'Mohan_postp'   'said'             'that'
 rAma_ne/k1     pAnI/k2>j    khAnA/k2
'Rama_postp'         'water'      'food'
khAkara::vkr     piyA::v:j::s/k2>i
'having_eaten'    'drank'
Thus the device of naming the elements
and co-indexing them with their respective
arguments can be used most effectively.
5. TAGGING SCHEME
The tagging scheme contains : notations,
defaults, and  tagsets.
5.1. NOTATION
Certain special symbols such as double
colon,underscore, paranthesis etc. are
introduced first. Two sets of tags have
been provided (to mark the crucial ARC
and node information). However,  apart
from these symbols and tags, some special
notation is required to explicitly mark
certain disjointed, scattered and missing
elements in a sentence. Following
notation is adopted for marking these
elements :-
5.1. 1.   X+ ... X- : disjointed elements
As shown above (4.2),  when a single
lexical unit composed of more  than one
elements is  separated by other intervening
lexical units, its 'oneness' is expressed by
using '+' on the first element in the linear
order and '-' on the second element. '+'
indicates to look ahead for the other part
till you find an element with '-'. '-'
suggests, 'an element marked '+' is left
behind, to which it should get itself
attached'.
    Example - Verb 'snAna_karanA' (to
bathe) in Hindi can occur  disjointedly
 snAna   to         mEMne kiyA_thA
 'bath'     'emph'   'I'          'did'
para    phira     gaMdA   Ho_gayA
'but'    'again'    'dirty'       'became'
 `Bathe I did , but got dirty again.'
'snAna_karanA' is one verb unit in Hindi.
But its two components 'snAna' and
'karanA' can occur separately. Notation
'X+....X-' can capture the 'oneness' of these
two elements. So 'snAna.karanA'
(?bathe?) in the above sentence would be
marked as follows :
 snAna::v+   to         mEMne
    'bath'        'emph'   'I'
 kiyA_thA::v-    para   phira    gaMdA
  'did'                  but'    'again'    'dirty'
 Ho_gayA
 'became'
Another  example of  'scattered elements'
is  'agara .... to' construction of  Hindi.
 agara   tuma    kaHate   to         mEM
   'if'       'you'     'said'      'then'     'I'
A_ jAtA
'would_have_come'
   `Had you asked I would have come'
?agara' and 'to' together give the
'conditionality' sense. Though they never
occur linearly together they have a
'oneness' of meaning. Their dependency
on each other can also be expressed
through 'X+....X-' notation.
 agara::yo+  tuma  kaHto::yo-  mEM A_
jAtA    (tag 'yo' is for conjuncts)
5.1.2.   >i ....:i   : explicitly marked
dependency (:i is the head)
(a)  Example -- The sentence 1a below has
the dependency  structure given in T-2
 1a. phala    rAma     ne
      'fruit'     'Rama' 'Ergpostp'
      naHA_ kara        khAyA
      'having_bathed'    'ate'
    ' Rama ate the fruit after taking a bath'
                                  khAyA
                                     |
                       |----------|-----------------|
                  k1 |       naHA_kara::vkr    |k2
                       |                                     |
                      rAma_ne                     phala
                                   T.2
Default (5.2.5) states that all kaarakas
attach themselves to the nearest available
verb on the right. In (1a) above, the
nearest verb available to 'phala' (fruit) is
'naHA_kara'. However, 'phala' (fruit) is
not the 'k2' of 'naHA_kara'. It is the 'k2' of
the main verb 'khA'. Therefore, an explicit
marking is required to show this
relationship. The notation '>i...:i' makes
this explicit.  Therefore,
           phala/k2>i   rAma_ne   naHA_kara
khAyA::v:i
Where 'khAyA' is the 'head', thus marked
':i' and 'phala' is the dependent element,
thus marked '>i'. An element marked '>i'
always looks for another element marked
':i'.
(b)  Another example of such attachments
which need to be marked explicitly is
given below -
  2a. rAma,     moHana  Ora    shyAma
       'Rama',  'Mohan'    'and'    'Shyama'
Ae
'came'
                                      Ora
                                        |
                          |----------|---------------|
                          |             |                   |
                  rAma      moHana    shyAma
                                T-3
To show their attachment to 'Ora' (and) the
three elements 'rAma','moHana', 'shyAma'
have to be marked (as in 2b.) the
following way in our linear tagging
scheme.
   rAma>i,   moHana>i   Ora::yo:i
shyAma>i
The justification to treat 'Ora' as the head
and show the 'wholeness' of   all the
elements joined by '>i' to ':i' is made
explicit by the following examples-
    rAma,   Ora Haz,       moHana   Ora
     'Rama'  'and''yeah',     'Mohana'  'and'
    shyAma   Ae_ the
    'Shyama'  'had_come'
In this case there is an intervening element
'Ora HAz' (?and_yeah) between 'rAma' and
'moHana' etc. So paranthesis alone will
not resolve the issue of grouping the
constituents of a whole. (By
paranthesising, elements which are not
part of the whole will also be included.)
To avoid this the 'Ora' (and) has to be
treated as a head.
5.1.3.   0    : explicit marking of an ellipted
element (missing  elements).  Example -
     rAma      bAjZAra   gayA,    moHana
     'Rama'     'market'      'went'     'Mohana'
       ghara     Ora   Hari    skUla
       'home'     'and'  'Hari'    'school'
 ?Rama went to the market, Mohana home
and Hari to the school.?
The sentence above has two ellipted
elements. The second and third occurrence
of the verb 'gayA'(?went?). To draw a
complete tree the information of the
missing elements is crucial here.
Arguments 'moHana', 'ghara', 'Hari', and
'skUla' are left without a head, and their
dependency cannot be shown unless we
mark the 'ellipted' element.
 rAma     bAjZAra   gayA,   moHana
 'Rama' 'market'    'went', 'Mohana'
ghara   0  Ora    Hari     skUla 0
'home'      'and'     'Hari'    'school'
In cases where this information can be
retrieved from some other source
(DEFAULT ) it need not be marked. In the
above case it need not be marked.
However, there may be cases where
marking of the missing element is crucial
to show various relationships. In such
cases it has to be marked. Look at the
following example -
eka       Ora       sajjana
'one'      'more'   'gentleman'
kaHate_HEM   bacce        baDZe
 'says'                'children'    'big'
Ho_gaye_ HEM      kisI
'become'                   'nobody'
kI       bAta     naHIM  mAnate
'gen'  'saying'   'not'        'agree'
' One more gentleman says that the kids
have grown older and do not listen to
anybody.'
The above sentence does not have any
explicit 'yojaka(conjunct)', between two
sentences,
        a) bacce baDZe Ho gaye HEM and
            `kids have grown older'
        b) kisI kI bAta naHIM mAnate
             `do not listen to anybody'
Both these sentences together form the
'vAkyakarma(sentential object)' of the
verb 'kaHate HEM' (?say?).
So the analysis would be -
 [eka Ora sajjana]/k1 kaHate_HEM::v:i
  ?one? ?more? ?gentleman? ?says?
{{bacce/k1ud    baDZe/k1vid
    ?children?      ?big?
Ho_gaye_HEM::v}::s {kisI_kI/6
?become?                        ?nobody?_s?
bAta]/k2 naHIM::neg
?words?   ?not?
 mAnate::v}::s}/k2>I
?listen?
It appears to be a neatly tagged sentence.
However, some crucial information is
missing from this analysis. In the sentence
the relationship between the two sentences
within the larger sentential object is not
expressed. The problem now is how to do
it.  Use of '>i...:i' notation can help express
this. However, it needs the ':i' information
and since there is no explicit 'yojaka'
(conjunct) element between the two
sentences it will not be possible to mark it.
The information of the presence of a
'yojaka' (conjunct) which is the head of a
co-ordinate structure is CRUCIAL here.
Without its presence its dependency tree
cannot be drawn. The notation '0' can be
of help in such situations. '0' can be
marked in the appropriate place. This will
allow the tagging of the dependent
elements. Therefore, the revised tagging
would be -
 [eka Ora sajjana]/k1 kaHate_HEM::v:i
{{bacce/k1ud baDZe/k1vid
ho_gaye_HEM::v}::s>j 0::yo:j
{kisI_kI/6  bAta]/k2 naHIM::neg
mAnate::v}::s>j}/k2>i
Here the information of missing conjunct
has been marked by a '0'.
5.2. DEFAULTS
Apart from tagsets and special notations
the scheme also relies on certain defaults.
Defaults have been specified to save
typing by the human annotator. For
example, no sentence has to be marked ba
a sentence tag till it is crucial for the
dependency analysis. For example :
    rAma      ne        yaHa    socA         ki
    'Rama' 'postp' 'this'     'thought'   'that'
     moHana      AegA
    'Mohana' 'would_come'
  `Rama thought that Mohana would come'
This is a complex sentence where the
subordinate sentence is the object
complement of the verb 'socA'(?thought?) .
To indicate the relation of the subordinate
clause with the main verb, it has to
marked.
Similarly,  within the square paranthesis,
right most element is the Head. So there is
no need to mark it. Postpositions's
attachment to the previous noun is also
covered by the default rule. There are
other defaults which take care of modifier-
modified relationships. In short, the
general rules have been accounted for by
defaults and only the specific relations
have to be marked. Elements preceding
the head within paranthesis are to be
accepted as modifiers of the head.
However, In case the number of elements
within paranthesis is more than two (Head
plus two) and one or more of them do not
modify the head then it should be marked.
 Example -   [HalkI    nIlI   kitAba],
                      'light'  'blue'  'book'
Here, 'halkI'(?light?) can qualify both
'nIlI'(?blue?) and 'kitAba'(?book?).  In case
it is modifying 'kitAba'(?book?), say, in
terms of light weight, then it should be left
unmarked. But if it modifies 'nIlI'(?blue?),
in terms of light shade, then it SHOULD
be marked by adding '>' on the right of the
modifying element.
       'halkI'  [HalkI> nIlI  kitAba].
        ?light? [?light?> ?blue? ?book?]
Let us look at another  case where the
dependency has to be explicitly marked.
Participle form 'tA_HuA', in Hindi, can
modify either a noun or a verb. For
example take the Hindi sentence -
   mEMne/k1  dODZate_Hue::vkr
    'I_erg'          'running'
    ghoDZe_ko/k2  dekhA::v
     'horse'                'saw'
This ambiguous sentence may mean either
the following  :-
a) mEMne dODZate_Hue::vkr>i
ghoDZe_ko:i/k2 dekhA ;
     'I saw the horse while the horse was
running'
      Or
b) mEMne dODZate_Hue::vkr>i
ghoDZe_ko/k2 dekhA::v:I
  'While I was running I saw the horse'
There is no need to mark ':i' in sentence
(a). However (b)  will need  explicit
marking.
5.3.TAGSETS
The tagsets used here have been divided
into two categories -
        1) TAGSET-1 - Tags which express
relationships are marked by  a preceding '/'
. For example kaarakas are grammatical
relationships, thus they are marked '/k1',
'/k2', '/k3' etc.
        2) TAGSET-2 - Tags expressing
nodes are marked by  a preceding '::' verbs
etc. are nodes, so they will be marked '::v',
Certain conventions regarding the naming
of the tags are ;
         k = kaaraka, --  all the kaaraka tags
will begin with k-,
                Therefore, k1, k2, k3 etc.
          n = noun
          v = verb  -- eg. v, vkr etc.
6.  CONCLUSIONS
A tagging scheme has been designed to
annotate corpora for various Indian
languages. The objective has been to use
uniform tags for all the Indian languages
thereby evolving a standard which can be
followed for various syntactic analysis for
machine processing. The scheme is yet to
be yet implemented on corpora from
various languages. Some trial workshops
have been conducted to see its
applicability in other Indian languages.
However, once the actual task of tagging
begins one may come across cases which
are not covered by the present scheme.
The idea is to provide a basic scheme
which can later be improved and revised.
7. REFERENCES
Bharati, Akshar, Vineet Chaitanya and
Rajeev Sangal, "Natural  Language
Processing: A Paninian Perspective",
Prentice-Hall of India,
New Delhi, 1995.
Bharati, Akshar, Dipti M Sharma, Vineet
Chaitanya, Amba P Kulkarni,
Rajeev Sangal, Durgesh D Rao, LERIL :
Collaborative Effort for Creating
Lexical Resources, In Proc. of Workshop
on Language Resources in
Asian Languages, together with 6th NLP
Pacific Rim Symposium, Tokyo.
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1834?1843, Dublin, Ireland, August 23-29 2014.
Converting Phrase Structures to Dependency Structures in Sanskrit
Pawan Goyal
Department of CSE
Indian Institute of Technology
Kharagpur, India ? 721302
pawang@cse.iitkgp.ernet.in
Amba Kulkarni
Department of Sanskrit Studies
University of Hyderabad
Hyderabad, India ? 500046
apksh@uohyd.ernet.in
Abstract
Two annotations schemes for presenting the parsed structures are prevalent viz. the constituency
structure and the dependency structure. While the constituency trees mark the relations due to
positions, the dependency relations mark the semantic dependencies. Free word order languages
like Sanskrit pose more problems for constituency parses since the elements within a phrase are
dislocated. In this work, we show how the enriched constituency tree with the information of
displacement can help construct the unlabelled dependency tree automatically.
1 Introduction
Sanskrit has a rich tradition of linguistic analysis with intense discussions and argumentations on various
aspects of language analysis ranging from phonetics (?siks
.
?a), grammar (vy?akaran
.
a), logic (ny?aya), ritual
exegesis (karmam??m?am
.
s?a), and literary theory (alam
.
k?ara?s?astra) which is not only useful for analysing
Sanskrit but it also has much to offer computational linguistics in these areas. The series of symposia
in Sanskrit Computational Linguistics (Huet et al., 2009; Kulkarni and Huet, 2009), the consortium
project sponsored by the Technology Development for Indian Languages (TDIL) and the research of
individual scholars and the collaborations (Goyal et al., 2012) among them resulted into a) development
of several tools ranging from segmenters (Huet, 2009), morphological analysers (Kulkarni and Shukl,
2009), parsers (Goyal et al., 2009; Hellwig, 2009; Kumar, 2012; Kulkarni, 2013) to discourse annotators,
b) lexical resources ranging from dictionaries, WordNet (Kulkarni et al., 2010) to Knowledge-Nets (Nair,
2011), and c) annotated corpora [http://sanskrit.uohyd.ernet.in/scl].
P?an
.
inian grammar, the oldest dependency grammar, provides a formalism for annotation of the
sentences. While the Sanskrit consortium has annotated a few thousand sentences following the
dependency grammar, we also came across a very valuable source of annotation of Sanskrit sentences
following the constituency structure (Gillon, 1996). The constituency structure was enriched to suite
the requirements of Sanskrit. This aroused our curiosity to study the equivalence of the two annotation
schemes.
The importance of dependency structure has been well recognised by several computational linguists
(Culotta and Sorensen, 2004; Haghighi et al., 2005; Quirk et al., 2005) in the recent past. The dependency
format is preferred over the constituency not only from evaluation point of view (Lin, 1998) but also
because of its suitability (Marneffe et al., 2006) for a wide range of NLP tasks such as Machine
Translation (MT), information extraction, question answering etc.. This has upsurged several works
on converting a constituency structure into dependency. The parsers for English now produce the
dependency parse as well. Xia and Palmer discuss three different algorithms to convert dependency
structures to phrase structures for English (Xia and Palmer, 2001). Magerman gave a set of priority
lists, in the form of a head percolation table to find heads of constituents (Magerman, 1994). Yamada
and Matsumoto modified these head percolation rules further (Yamada and Matsumoto, 2003). Their
method was reimplemented by Nivre, who also defined certain heuristics to infer the arc labels in the
dependency tree produced (Nivre, 2006). Johansson and Nugues used a richer set of edge labels and
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1834
introduced links to handle long-distance phenomena such as wh-movement, topicalization, expletives
and gapping (Johansson and Nugues, 2007). Their conversion procedure made use of this extended
structure in Penn Treebank. De et al. described a system for generating typed dependency parsed from
the phrase structure parses (De Marneffe et al., 2006). (Palmer et al., 2009; Xia et al., 2009; Bhatt et al.,
2009) discuss a multi-layered representation framework for Hindi and Urdu, where the information from
syntactic as well as dependency parse is presented together.
In this work, we explore the relationship between enriched constituency structures and dependency
structures for Sanskrit language, with main emphasis on the conversion from constituency to dependency
structures. This work aims not only at designing an algorithm to convert Treebanks from one type of
representation to the other, but also to judge the adequacy of the enriched constituency structure from
parsing point of view. This paper has been organized as follows. Section 2 discusses the history and
origin of this work. Section 3 describes the background of the constituency and dependency structures,
utilized for Sanskrit language. Section 4 discusses the algorithm we used for converting constituency
structure into dependency structure. Section 5 describes the results obtained by our approach, with some
examples. Section 6 concludes this paper with the directions for future work.
2 Origin of the work
The dataset we are using in this work has its origin in the remarkable treatise on Sanskrit Syntax by Apte
(Apte, 1885) which is the most authentic book on Sanskrit Syntax even after 125 years. The work was
initiated in 1986 by Brendan Gillon, then a senior fellow at the American Institute of Indian Studies,
at Deccan College, put all the prose exercise sentences from Apte?s Student Guide onto 5 ? 7 cards,
assigning a syntactic parse to each sentence, giving each sentence an English translation and annotating
each sentence for miscellaneous syntactic and semantic facts. On the basis of these sentences, Brendan
Gillon published the grammar underlying his syntactic parse of these sentences (Gillon, 1996).
In 1991, Brendan Gillon transferred the material from a paper format to an electronic format, making
revisions. An example sentence in this dataset is given below:
Example{3}
Source{1.1.3 (P) <U 4.5.3>} % Apte{7,3}
Parse
[S [INJ haa ] [ADV katham ]
[NP1s [NP6 (mahaaraaja<Dasharathasya) ] (dharma<daaraa.h) ]
[VP 0 [NP1 (priya<sakhii) [NP6 me ] [NP1 Kaushalyaa ] ] ] ]
Gloss{Oh, how is it that the legal wife of King Dasharatha is my dear
friend Kaushalyaa}
Comment{copula: covert: predicational: NP1s VP }
Each example is given a serial number, its source - the corresponding reference in Apte?s book. Then,
its constituency parse is provided in a tree structure. The Sanskrit text is transliterated into Roman
using the Velthuis notation
1
. Finally, the gloss (translation) of the prose is provided along with some
observations regarding syntax in the field ?comment?. The proper nouns are transliterated following the
English convention of capitalisation. The constituency structure is enriched reflecting the morphological
information such as the case marker. The underlying constituency structure of the compounds is also
shown clearly marking the head of the compound. The requirement that constituency tree be a binary is
also done away with, resulting into a more flat structure than the normal hierarchical phrase structure.
In 2004, G?erard Huet re-engineered the document in order to parse it mechanically, and he verified its
correct syntactic structure after typographical corrections. He devised an abstract syntax to formalize this
constituency structure. In the abstract syntax, the above constituency structure is represented as below:
list Tag_tree.syntax =
[S
1
Originally developed in 1991 by Frans Velthuis for use with his devnag Devanagari font, designed for the TeX typesetting
system.
1835
[INJ ("haa", 1); ADV [("katham", 2)];
NP
([Case 1; Role Subject],
[NP ([Case 6], [N (Compound (Stem <mahaaraaja>, Stem <Dasharathasya>),
3)]);
N (Compound (Stem <dharma>, Stem <daaraa.h>), 4)]);
VP0
[NP
([Case 1],
[N (Compound (Stem <priya>, Stem <sakhii>), 5);
NP ([Case 6], [N (Stem <me>, 6)]);
NP ([Case 1], [N (Stem <Kaushalyaa>, 7)])]);
NIL 8]]]
Each stem is given a unique index. The syntax, while preserving the original structure of the text, gives
additional structuring with the word numbers, explicit case markers and stems for the compounds. While
these constituent trees preserve much of the tagging related information, they still do not have the gender
and number information for the substantives, for instance. This information can enhance the constituency
representation further.
The same set of sentences were also parsed manually by Sheetal Pokar, a research scholar at the
University of Hyderabad, showing the dependency structure. Sheetal followed the annotation guidelines
developed by the Consortium of Institutes working on the Development of Sanskrit Computational
Tools
2
. This tagset has a little above 40 tags marking various relations. The dependency tree for the
example 3, discussed above, is shown in Figure 1. It is a directed tree with nodes corresponding to
the words in the sentence and edges corresponding to the relation between the head and the modifier.
Each node has a number indicating the word index. A generic relation sambandhah
.
(R) is used if the
relation does not fall under any of the given tags. As one may notice, both the constituency as well as
the dependency structures posit a NULL verb ?to be? asti. Among the Indian schools dealing with verbal
cognition, not all schools accept the insertion of missing copula. We follow the grammarian school who
accept this insertion.
(a) (b)
Figure 1: Example 3: (a). Constituency Parse and (b). Dependency Parse
While the two structures in Figure 1 mark different kind of information, we notice that the dominance
relation in the constituency structure under each phrase corresponds to the the modifier-modified relation
in the dependency tree. This was the main motivation to develop the converter to convert a phrase
structure into an unlabelled dependency structure.
2
http://sanskrit.uohyd.ernet.in/scl
1836
3 Dependency and Constituency Structures
Verbal understanding of any utterance requires the knowledge of how words in that utterance are related
to each other. There are two major representational frameworks for representing this knowledge as a
parse tree viz. constituency and dependency parse trees. Constituency trees show how the individual units
in a sentence are grouped together leading to semantically richer phrases in the constituency structures.
The dependency structure, on the other hand, shows how each word is related to other words in the
sentence either directly or indirectly.
The constituency structure derives from the subject-predicate division of Latin and Greek grammars
that is based on term logic. Basic clause structure is understood in terms of a binary division of the clause
into subject (noun phrase NP) and predicate (verb phrase VP). These ideas originated with Leonard
Bloomfield (Bloomfield, 1962) and were further developed by a number of American structuralist
linguists, including Harris (Harris, 1955) and Wells (Wells, 1947). Though these grammars were
initially conceived as applying only to phrases, it was shown that such rules could be used for analyzing
compounds as well as derivational morphology for Sanskrit. Gillon showed that the same extension
works for classical Sanskrit as well (Gillon, 1995).
The dependency analysis dates back to P?an
.
ini who uses the syntactico-semantic relations called k?araka
relations for the linguistic analysis of a sentence. In modern times, the seminal work of Tesni`ere
(Tesni`ere, 1959) became the basis for the work on dependency grammar. Meaning-Text Theory (Mel?cuk,
1988), Word grammar (Hudson, 1984), Functional Generative Description (Segall et al., 1986) are some
of the flavours of the dependency grammar. A dependency parse is generally modelled as a directed tree
with nodes representing the words and edges representing the possible relations between them. A typed
dependency parse also labels the relations. For every element (word or morph) in a sentence, there is just
one node in the syntactic structure.
Xia and Palmer (Xia and Palmer, 2001) discuss three different algorithms to convert dependency
structures to phrase structures for English. They also attempt to clarify the differences in representational
coverage of the two approaches. In the first stage, they identify the head of each constituent of the
sentence, which is further modified to retrieve the semantic head. In the second phase, they label each
of the dependency extracted with a grammatical relation using patterns defined over the phrase structure
tree. (Palmer et al., 2009; Xia et al., 2009; Bhatt et al., 2009) discuss a multi-layered representation
framework for Hindi and Urdu, where the information from syntactic as well as dependency parse is
presented together. They first construct a dependency parse and then convert it into the constituency
parse tree using conversion rules. A conversion rule is a (DS pattern, PS pattern) pair, where DS and PS
correspond to dependency and phrase structure respectively.
The constituency parse we are dealing with being enriched with linguistic information pertaining to the
morphology of the simple as well as compound words, it was much simpler to convert this structure into
a dependency structure. In the next section, we will discuss our algorithm for converting a constituency
structure to a dependency structure.
4 Conversion from Constituency to Dependency Structure in Sanskrit
The notion of ?head? is very important for both the constituency and dependency structures. In the
constituency structure, the head determines the main properties of the phrase. Head may have several
levels of projection. In the dependency structure, on the other hand, the head is linked to its dependents.
The core of the algorithm is to identify the head of each phrase in the constituency tree and establish its
relation with the head of its parent node. And also to establish the relation between the head with its
dependents. The head for each XP is the node X within that XP. Thus the head for an NP is the noun,
head for a VP is the verb, and so on. The head for the S is the head of a VP, in case it is a simple sentence,
and head of the VP of the main clause in case it is a complex sentence. In case of complex sentences,
we identified the main clause taking clues from the connectives. Each relation is named after the XP of
the modifier. Our algorithm for finding the head node is implemented on the abstract syntax discussed in
section 2 before. A rough outline of the algorithm is:
1)The head of VP is the ROOT node in the dependency tree.
1837
a) In the case of sentences with sub-ordinate clauses, identify the main clause taking clues from the
connectives.
b) Head of a clause is an auxiliary, if present, otherwise the main verb is the head.
c) In the case of sentences with quotative markers, the verb of the main clause is the head.
(The later rule is stronger than the previous. )
2) All the XPs within VP are dependent on the ROOT.
3) If S is the parent of VP, then all the XPs which are children of S are also dependent on this ROOT.
Finding the head for each node was not trivial though, as many of the parses involve dislocated phrases,
which were not fully marked. We had to enrich the constituency trees by incorporating the dislocation
information, which was provided in comments and was missing from the tree notations. We used ?!? and
?$? to indicate the dislocation. ?!? indicates the position from where a component is dislocated, while
?$? indicates the dislocated component. An example of a constituency parse, enriched with dislocation
information is given below.
Example{2}
Source{1.1.2 (P) <V 3.28; V 3.6.3>} % Apte{7,2}
Parse
[S [ADV sarvatra ] [NP6 audarikasya $1]
[VP 0 [NP1 abhyavahaaryam [PRT eva ] ] ]
[NP1s !1 vi.saya.h ] ]
Gloss{In every case, a glutton?s object is only food.}
Comment{copula: covert: predicational: VP NP1s
"eva" in predicate NP
left extraposition from NP1s of NP6 within MC, modulo adverbial ADV.}
This constituency tree involves one dislocated phrases. This information is marked with ?!1?for the
place from which it is dislocated and with ?$1? for the phrase that has been dislocated. This dislocation
information is used by our algorithm to find the right relata for the dislocated words. In case of more
than one dislocated phrases, they are numbered sequentially.
5 Results and Discussions
We implemented our algorithm on a dataset of 232 sentences and matched the output of our algorithm
with the Gold dataset, the dependency graph constructed manually for the sentences by Sheetal. Figure
2 shows the dependency structure for Example 3, produced by our conversion algorithm.
Figure 2: Dependency graph constructed by conversion from constituency structure
The conversion algorithm captures the relations between various constituents and produces a
dependency graph. Labels of the dependency graph correspond to the intermediate nodes in the phrase
structure. On comparing the graph in Figure 2 with the graph in Figure 1, we find that most of the
original connections were captured. However, there was a mismatch in the relation between words
kau?saly?a, priyasakh?? and me. This discrepancy is because of the non-agreement between the annotators
as to which one is the head. In case of sentences with apposition, in Sanskrit, the two annotators have
difference of opinion as to which among the two is the head. This resulted in the mismatch between the
two graphs. The relations in this graph are labelled by the dominating XP.
1838
P?an
.
ini?s grammar provides rules for assigning case markers given the syntactico-semantic relation
between the relata. Inverting these rules it should be possible to get the relation labels. These relation
labels, however, will not be obtained deterministically. The non-determinism will lead to multiple
labelled dependency structures. Hence we could not assign the labels from the tagset, and resorted
to the names of the phrases which the word belongs to.
Below we give an example where we found an exact match between the manual dependency graph
and the dependency graph, produced by our conversion algorithm, using the constituency parse of the
sentence.
Example{29}
[S [VP [NP7 tatra ] [CNJ ca ]
[NP5 [NP6 [AP6 (((nikhila<(dhara.nii<tala))<parya.tana)<khinnasya) ]
(nija<balasya) ]
(vizraama<heto.h) ]
[NP2 [AP2 katipayaan ] divasaan ]
ati.s.that ] ]
Gloss{And he remained there for a few days in order to rest his army
exhausted from roaming the entire surface of the earth.}
(a) (b)
Figure 3: Example 29: (a). Dependency graph constructed manually and (b). Dependency graph
constructed by conversion from constituency structure
Out of 232 cases, we found 97 such cases with exact match. For the rest of the cases,
1) In 40 cases, number of words in dependency and phrase-converted graph are different. For example,
the words such as kad?acit ?probably?, yadyapi ?even if?, tath?api ?even then?, athav?a ?or? etc. were treated
in one structure as a single word while in the other as two words. These words at morphological level
consist of two morphemes which have independent existence. So it was natural to treat these as two
words, in the constituency trees. However, at the semantic level, these two words indicate a single
meaning which at times is non-compositional. The annotator of dependency graph has treated these as a
single word.
2) In 95 cases, one or more relations do not match. These were due to various reasons such as
1. differences in the treatment and identification of adjectives,
2. disagreement in the attachment, and
3. cases of ellipsis, null head, and cases where the treatment of conjunct ?ca? (and) differs.
1839
These differences are very much important from linguistic analysis point of view. However due to space
constraint we illustrate here only two cases where the treatment of the two annotators differ.
Example{72}
[S [VOC sakhi [VOC Vaasanti ] ]
[VP 0 [NP4 du.hkhaaya [PRT eva ] [NP6 su-h.rdaam ] ] ]
[NP1s [ADV idaaniim ] [NP6 Raamasya ] darshanam ] ]
Gloss{Oh my friend Vaasanti, seeing Raama now leads only to
the unhappiness of his friends.}
(a) (b)
Figure 4: Example 72: (a). Dependency graph constructed manually and (b). Dependency graph
constructed by conversion from constituency structure
In this example, there are two places where the annotators disagree.
a) The attachment of the word id?an??m (now).
b) The decision of head in case of vocative with a modifier.
The null copula (VP 0) in the constituency structure is replaced by the Sanskrit verbal form asti
uniformly. The annotator of dependency graph has provided a more appropriate verb bhavis
.
yati.
However, we ignore this difference.
Let us look at another example, where the ambiguity in morphological analysis has led to the ambiguity
in the readings resulting in the disagreement in the parse.
Example{46}
[S [NP1s aarya.h ]
[VP daapayatu [NP2 [NP6 me ]
[NP4 (Vaisha.mpaayana<aanayanaaya) ]
(gamana<abhyanuj?naam) ]
[NP3 taatena ] ] ]
Gloss{May you, sir, make my father give me permission to go to bring
Vaisha.mpaayana.}
In this example, the pronominal form me is ambiguous between two readings. It can be either a genitive
or a dative of the first person pronoun asmad ?I?. The sub-ordinate clause is analysed by Gillon as ?the
permission for going to bring Vai?samp?ayan by me?. In Sanskrit the first person pronoun in such cases
takes genitive case marker. Sheetal on the other hand has analysed it as ?the permission to me for going to
bring Vai?samp?ayan?, where me is analysed with dative case. It is clear that ?to bring Vai?samp?ayan? is the
purpose for going. But since ?permission for going? is a compound in Sanskrit
3
, and the ?permission?
being the head of this compound, Sheetal avoided linking ?to bring Vai?samp?ayana? with ?permission
to go? as it results into an ?asamartha sam?asa (incompatible compound formation, where the external
modifier connects the modifier component of a compound and not the head). This has resulted into the
differences in annotation. Such compounds are not rare. Thus, in order to provide a correct parse, in
case of dependency structure, it is necessary to show the internal structure of the compounds as well,
3
In Sanskrit a compound is always written as a single word without any space in between.
1840
(a) (b)
Figure 5: Example 46: (a). Dependency graph constructed manually and (b). Dependency graph
constructed by conversion from constituency structure
rather than treating a compound unanalysed. This will allow one to connect the elements to the part of a
compound other than the head. Similar treatment is necessary in the phrase structure annotation as well.
We end with example 2 from section 4, where the dislocation information in the constituency tree
helps in retrieving the correct dependency structure. In this example, even though the word audarikasya
has been displaced, the displacement information in the parse tree positions it at the correct place in the
dependency tree constructed from the constituency structure.
(a) (b)
Figure 6: Example 2: (a). Constituency structure and (b). Dependency graph constructed by conversion
from constituency structure
6 Conclusions and Future Work
This work focussed mainly on conversion from constituency to dependency structure. The sentences in
our dataset are chosen from (Apte, 1885), which is an authentic book for higher learning of Sanskrit,
covering a wide range of grammatical constructions. The tool was tested on a dataset of 232 sentences
and the initial results were encouraging. Specifically, most of the cases of mismatch were linguistic
issues and need further discussion. The phrase labels indicating the case labels is an important extension
of the constituency trees to accommodate morphologically rich languages. The enriched constituency
structure has an advantage of recording the word order, and at the same time marking the dislocation
information. In this work, we have shown that such enriched constituency tree can help construct the
unlabelled dependency tree automatically. Further, one may also try inferring the dependency relation
names, and use statistical parsing to resolve non-determinism favouring popular usages.
1841
Another interesting aspect would be to try the other way conversion, that is, from dependency to
phrase structure. The main challenge for this conversion is to find out the projection table corresponding
to each lexical item. This work will also be a first step towards an abstract syntax, which can inherit
the properties of both the constituency and dependency structures. If so, this would be an alternative
formalism for tagging the Sanskrit corpus.
Acknowledgements
The authors would like to acknowledge the discussions with G?erard Huet, INRIA Paris Rocquencourt,
towards enriching the abstract syntax. The work was also supported by Emilie Aussant and Sylvie
Archaimbault, laboratoire HTL, Universit?e Paris Diderot.
References
V?aman Shivar?am Apte. 1885. The Student?s Guide to Sanskrit Composition. A Treatise on Sanskrit Syntax for Use
of Schools and Colleges. Lokasamgraha Press, Poona, India.
Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009.
A multi-representational and multi-layered treebank for hindi/urdu. In Proceedings of the Third Linguistic
Annotation Workshop, pages 186?189. Association for Computational Linguistics.
Leonard Bloomfield. 1962. Language. New York: Holt.
A. Culotta and J. Sorensen. 2004. Dependency tree kernels for relation extraction. In 42nd Annual Meeting of the
Association for Computational Linguistics (ACL), pages 423?429, Barcelona, Spain.
Monali Das and Amba Kulkarni. 2013. Discourse level tagger for mah?abh?as
.
ya - a Sanskrit commentary on
p?an
.
ini?s grammar. In Proceedings of the 10th International Conference on NLP, Delhi, India.
Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed
dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449?454.
Brendan S. Gillon. 1995. Autonomy of word formation: evidence from Classical Sanskrit. Indian Linguistics, 56
(1-4), pages 15?52.
Brendan S Gillon. 1996. Word order in classical sanskrit. Indian Linguistics, 57(1-4):1?35.
Brendan S. Gillon. 2009. Tagging classical Sanskrit compounds. In Amba Kulkarni and G?erard Huet, editors,
Sanskrit Computational Linguistics 3, pages 98?105. Springer-Verlag LNAI 5406.
Pawan Goyal, Vipul Arora, and Laxmidhar Behera. 2009. Analysis of Sanskrit text: Parsing and semantic
relations. In G?erard Huet, Amba Kulkarni, and Peter Scharf, editors, Sanskrit Computational Linguistics 1
& 2, pages 200?218. Springer-Verlag LNAI 5402.
Pawan Goyal, G?erard Huet, Amba Kulkarni, Peter Scharf, and Ralph Bunker. 2012. A distributed platform for
sanskrit processing. In Proceedings of 24th COLING, Mumbai, India.
A.D. Haghighi, A.Y. Ng, and C.D. Manning. 2005. Robust textual inference via graph matching. In
Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 387?394, Vancouver, Canada.
Zellig S Harris. 1955. From phoneme to morpheme. Language, 31(2):190?222.
Oliver Hellwig. 2009. Extracting dependency trees from Sanskrit texts. In Amba Kulkarni and G?erard Huet,
editors, Sanskrit Computational Linguistics 3, pages 106?115. Springer-Verlag LNAI 5406.
R. Hudson. 1984. Word Grammar. Basil Blackwell, Oxford.
G?erard Huet, Amba Kulkarni, and Peter Scharf, editors. 2009. Sanskrit Computational Linguistics 1 & 2.
Springer-Verlag LNAI 5402.
G?erard Huet. 2009. Formal structure of Sanskrit text: Requirements analysis for a mechanical Sanskrit
processor. In G?erard Huet, Amba Kulkarni, and Peter Scharf, editors, Sanskrit Computational Linguistics 1
& 2. Springer-Verlag LNAI 5402.
1842
Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In
Proceedings of the 16th Nordic Conference on Computational Linguistics (NODALIDA), pages 105?112.
Amba Kulkarni and Monali Das. 2012. Discourse analysis of sanskrit texts. In Proceedings of the workshop on
Advances in Discourse Analysis and its Computational Aspects, 24th COLING, Mumbai, India.
Amba Kulkarni and G?erard Huet, editors. 2009. Sanskrit Computational Linguistics 3. Springer-Verlag LNAI
5406.
Amba Kulkarni and K. V. Ramakrishnamacharyulu. 2013. Parsing Sanskrit texts: Some relation specific issues. In
Malhar Kulkarni, editor, Proceedings of the 5th International Sanskrit Computational Linguistics Symposium.
D. K. Printworld(P) Ltd.
Amba Kulkarni and Devanand Shukl. 2009. Sanskrit morphological analyser: Some issues. Indian Linguistics,
70(1-4):169?177.
Malhar Kulkarni, Chaitali Dangarikar, Irawati Kulkarni, Abhishek Nanda, and Pushpak Bhattacharyya. 2010.
Introducing sanskrit wordnet. In Christiane Felbaum Pushpak Bhattacharyya and Piek Vossen, editors,
Principles, Construction and Application of Multilingual Wordnets, Proceedings of the Global Wordnet
Conference, 2010. Narosa Publishing House, New Delhi.
Amba Kulkarni. 2013. A deterministic dependency parser with dynamic programming for Sanskrit. In
Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages
157?166, Prague, Czech Republic, August. Charles University in Prague, Matfyzpress, Prague, Czech Republic.
Anil Kumar. 2012. An automatic Sanskrit Compound Processing. Ph.D. thesis, University of Hyderabad,
Hyderabad.
Dekang Lin. 1998. Dependency-based evaluation of minipar. In Workshop on the evaluation of Parsing Systems,
Granada, Spain.
David Mitchell Magerman. 1994. Natural Language Parsing As Statistical Pattern Recognition. Ph.D. thesis,
Stanford, CA, USA. UMI Order No. GAX94-22102.
Marie-Catherine De Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In Proceedings of LREC-06.
I. Mel?cuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press, Albany.
Sivaja Nair. 2011. The Knowledge Structure in Amarako?sa. Ph.D. thesis, University of Hyderabad, Hyderabad.
Joakim Nivre. 2006. Inductive dependency parsing. Springer.
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, Owen Rambow, Dipti Misra Sharma, and Fei Xia. 2009.
Hindi syntax: Annotating dependency, lexical predicate-argument structure, and phrase structure. In The 7th
International Conference on Natural Language Processing, pages 14?17.
C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt.
In 43rd Annual Meeting of the Association for, Computational Linguistics (ACL), pages 271?279, Ann Arbor,
USA.
Peter Scharf and Malcolm Hyman. 2009. Linguistic Issues in Encoding Sanskrit. Motilal Banarsidass, Delhi.
P. Segall, E. Hajiov, and J. Panevov. 1986. The Meaning of the Sentence in its Semantic and Pragmatic Aspects.
Springer, Heidelberg.
L Tesni`ere. 1959.
?
El?ements de syntaxe structurale. Klinksieck, Paris.
Rulon S Wells. 1947. Immediate constituents. Language, 23(2):81?117.
Fei Xia and Martha Palmer. 2001. Converting dependency structures to phrase structures. In Proceedings
of the first international conference on Human language technology research, pages 1?5. Association for
Computational Linguistics.
Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer, and Dipti Misra Sharma. 2009. Towards a
multi-representational treebank. In The 7th International Workshop on Treebanks and Linguistic Theories.
Groningen, Netherlands.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines.
Proceedings of IWPT, 3.
1843
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 48?51, Dublin, Ireland, August 23-29 2014.
Sanskrit Linguistics Web Services
Ge?rard Huet
Inria Paris-Rocquencourt
gerard.huet@inria.fr
Amba Kulkarni
University of Hyderabad
apksh@uohyd.ernet.in
Abstract
We propose to demonstrate a collection of tools for Sanskrit Computational Linguistics
developed by cooperating teams in the general setting of Web services. These services
offer a systematic architecture integrating multilingual lexicons, morphological generation
and analysis, segmentation and parsing, and interlink with the Sanskrit Library digital
repository. They may be used as distributed Internet services, or installed as local tools
on individual users workstations.
1 Community building
Sanskrit is the primary culture-bearing language of India, with a continuous production of
literature in all fields of human endeavour over the course of four millennia. It benefited from
a strong linguistics tradition, established from early times, and notably from the grammar
composed by Pa?n
.
ini around the fourth century B.C.E., and commented since by innumerable
grammatical treatises. This fairly complete descriptive apparatus took a prescriptive character,
resulting in a constrained evolution of the language within its official grammar, leading to
its stability as a semi-formal language. On the other hand, multiple styles of writing treatises,
commentaries, and even poetry, led to a variety of specific dialects, both in prose and in versified
form.
The efforts towards developing tools for the computational treatment of Sanskrit have been
steadily progressing both at national as well as international level. A Sanskrit Computational
Linguistics consortium funded by the Indian Government coordinates the development of con-
sistent tools within 7 research institutes. In 2007, the first of a series of International Sanskrit
Computational Linguistics Symposia was organized in Paris with the aim of gathering a commu-
nity of teams sharing ideas as well as linguistic resources, and developing inter-operable software.
These symposia have benefited the computer scientists from the grammatical expertise of the
traditional scholars, while the traditional scholars could see the practical applications of the
thousand of years old theories.
Within this general effort, specific tools were developed at Inria in Paris and University of
Hyderabad for the analysis of Sanskrit texts, designed as inter-communicating Web services. A
specific human-machine interface was developed, allowing annotation experts to produce tagged
tree banks for the Sanskrit Library, a digital TEI-conformant repository of Sanskrit corpus.
This joint work was presented at COLING-2012 (Goyal et al., 2012). We herein propose to
demonstrate the current functionalities of this software platform.
2 Architecture of components
It was deemed counter-productive to attempt to build a monolithical rigid system, and we
turned rather to developing on various sites independent components, communicating with each
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.
0/
48
other as Web services interchanging XML data. This allows freedom of programming languages
and environments, operating systems, and even linguistic resources. This also permits greater
flexibility of independent versioning of the components. Furthermore, HTML and client-side
scripting provide a standardized solution to a common user interface, with easy multilingual
display through Unicode.
Currently the system supports several lexicon resources. A specific core lexicon, the Sanskrit
Heritage dictionary (Huet, 2004), has been developed as the seed resource for morphological
databases. Digitalized versions of the Monier-Williams (Monier-Williams et al., 1999) and the
Apte dictionaries are being progressively integrated by semi-automatic alignment. Amarakos?a,
the oldest thesaurus of Sanskrit, has also been digitalized (Nair and Kulkarni, 2010), providing
the ontological information following the Indian tradition.
A number of components use the Zen Computational Linguistics Toolkit
1
, a systematic func-
tional programming library of finite-state tools. It handles lexicon management, phonological
computation, morphology generation (both generative and inflexional), and segmentation. The
segmentation component is specially important, in view of sandhi. Sanskrit text is represented
as the result of phonetic smoothing, whose efficient inversion is problematic. An original solution
to sandhi analysis was developed (Huet, 2005; Huet, 2006).
Morphological databanks are produced mechanically from the core dictionary stems, informed
with their production parameters. Sharing techniques give highly compressed data structures
loadable dynamically in process memory, eschewing the use of costly database technology. The
morphological treatment processes are not strictly speaking Pa?n
.
inian, but it is possible to relate
them precisely to Pa?n
.
inian derivations (Goyal and Huet, 2012). Independently, databanks issued
from the Sanskrit traditional repositories have been digitalized (Bharati et al., 2006) and linked
to the As
.
t
.
a?dhya?y?? simulator which generates the nominal forms following the Pa?n
.
inian process
(Goyal et al., 2009). Lemma alignment algorithms permit inter-operability of these various
resources, seen as plug-in components.
Segmentation leads to morphological tagging, a complete but much over-generating process.
A typical sentence may have billions of possible analyses. A graphical user-interface, providing
a fully shared view of all segmentations, has been developed (Huet and Goyal, 2013). It is
designed to be very fast, and to allow human annotators easy inspection of the segment features.
Experiments with semi-automatic annotation of parts of the Sanskrit Library
2
validated the
approach.
A deterministic tabulated dependency parser has been designed (Kulkarni, 2013). The appli-
cation of local constraints at an early stage and stacking of intermediate results along dynamic
programming yield fast results. The graphical user-interface of the segmenter is adapted to the
parser in order to show the shared view of all possible solutions in a compact form.
Annotated corpus statistics are being used to build the language and grammar models for
various tasks. An experimental version of a segmenter (Kumar et al., 2010) is developed that
constraints the sequence of constituents by a language model and uses the split model based on
the empirical data of sandhi rules for splitting. These empirical models may further be used to
build weighted finite state automaton to prioritize the solutions.
3 Salient novel features
The segmentation algorithm uses a novel approach to finite state technology, through Effective
Eilenberg machines (Huet and Razet, 2008). Although Sanskrit has huge literature, only a
negligible part of it has been tagged for various levels of analyses. Thus use of statistical
techniques or machine learning is almost ruled out. While machines are good at syntactic
analysis, for semantic compatibility of solutions we still depend on human assistance. This calls
for a suitable interface which can represent billions of solutions with all relevant linguistic details
1
http://yquem.inria.fr/
~
huet/ZEN/
2
http://sanskritlibrary.org
49
to be displayed on a screen. This requirement led us to develop a tabulated display interface,
using efficiently a compact shared representation of solutions, presenting an ergonomical solution
to human assistance. This interface was also further adapted to display all possible sentential
parses in a compact tabular format for choosing the correct parse.
A new technology of forms alignment, indexed by their morphological production history
(?unique naming?), allows uniform access to various dictionaries, despite possibly conflicting
homophony partitions.
The consistently structured core lexical repository, together with lexicon alignment, al-
lows the automatic production of derived human-readable dictionaries under the Baby-
loo/Stardict/Goldendict formats, with consistent hypertext linking to grammatical processes.
Some requirements of Sanskrit computational tools are very specific. Sanskrit has a vast
literature spreading over several knowledge domains. Most of the important Sanskrit literature is
already translated into several languages. In spite of this, scholars want to have access to original
sources, and thus development of the computational tools with convenient user interfaces that
allow seamless connectivity to and from the lexical resources, generation engines and analysis
tools becomes meaningful. Further, the availability of As
.
t
.
a?dhya?y??, an almost complete grammar
for Sanskrit, also puts demands on the developers to authenticate the inverse process of analysis
by the generative rules of grammars. These considerations have resulted in the development of
suitable interfaces linking various resources and tools through Web services.
4 Software engineering and deployment issues
The Web services approach allows independent development of components, seen as XML trans-
ducers keeping a history of interactions through the argument structure of the CGI invocations.
This allows independent development, archiving and distribution of modules developed in C,
PERL, Ocaml, Python, Java, Javascript, etc.
Parametrization of the various platform interconnections allows for distributed use through
Internet, as well as local use on workstations. Extreme programming methodology allows for
agile development with high frequency releasing and a fast user feedback.
The software, as well as linguistic resources, are available under open-source licences.
5 Demonstration scenario
The tools will be demonstrated on a few typical sentences, showing various usages of the software.
The first presentation will demonstrate the Sanskrit Heritage segmenter on an input sentence.
It will show how to select a segmentation solution using the graphical interface, then the way
to refine the solution using the dependency parser of the Hyderabad Sanskrit Computational
Linguistics analyser, in order to get its dependency structure. A dual presentation will start
from the Hyderabad analyser, using the Heritage segmenter as a front end. Finally it will be
shown how to access the analyser tools from marked-up corpus in the Sanskrit library. Settings
allow switching between the various lexicons, and displaying grammatical information either in
romanized Western style, or in Devana?gar?? traditional Indian style. The demo will also include
linking to actual Pa?n
.
inian derivation process to ensure precision in the analysis. If time permits,
the Goldendict versions of the lexicons will be shown, informed with grammatical information.
Acknowledgement
The Inria ?Sanskrit Heritage? platform benefited from important contributions of Pawan Goyal,
notably in its graphical interface. We wish also to thank Peter Scharf for his cooperation on
the Sanskrit Library interface. Various components of the software at University of Hyderabad
were developed with support from TDIL Programme, DeitY, Government of India for the project
?Development of Sanskrit computational toolkit and Sanskrit-Hindi Machine Translation system?
with contributions from Sivaja Nair, Anil Kumar, Karunakar, Devanand Shukl and Pavankumar.
50
References
Akshar Bharati, Amba Kulkarni, and V. Sheeba. 2006. Building a wide coverage Sanskrit morphological
analyser: A practical approach. First National Symposium on Modeling and Shallow Parsing of Indian
Languages, IIT Mumbai.
Pawan Goyal and Ge?rard Huet. 2012. Completeness analysis of a Sanskrit reader. In Proceedings, 5th
International Symposium on Sanskrit Computational Linguistics. DK Publisher.
Pawan Goyal, Amba Kulkarni, and Laxmidhar Behera. 2009. Computer simulation of As
.
t
.
a?dhya?y??:
Some insights. In Ge?rard Huet, Amba Kulkarni, and Peter Scharf, editors, Sanskrit Computational
Linguistics 1 & 2, pages 139?161. Springer-Verlag LNAI 5402.
Pawan Goyal, Ge?rard Huet, Amba Kulkarni, Peter Scharf, and Ralph Bunker. 2012. A distributed plat-
form for Sanskrit processing. In 24th International Conference on Computational Linguistics (COL-
ING), Mumbai.
Ge?rard Huet and Pawan Goyal. 2013. Design of a lean interface for Sanskrit corpus annotation. In
Proceedings, ICON13, Hyderabad.
Ge?rard Huet and Beno??t Razet. 2008. Computing with relational machines. ICON?2008 tutorial.
Ge?rard Huet. 2003. Towards computational processing of Sanskrit. In International Conference on
Natural Language Processing (ICON).
Ge?rard Huet. 2004. Design of a lexical database for Sanskrit. In Workshop on Enhancing and Using
Electronic Dictionaries, COLING 2004. International Conference on Computational Linguistics.
Ge?rard Huet. 2005. A functional toolkit for morphological and phonological processing, application to a
Sanskrit tagger. J. Functional Programming, 15,4:573?614.
Ge?rard Huet, 2006. Themes and Tasks in Old and Middle Indo-Aryan Linguistics, Eds. Bertil Tikkanen
and Heinrich Hettrich, chapter Lexicon-directed Segmentation and Tagging of Sanskrit, pages 307?325.
Motilal Banarsidass, Delhi.
Ge?rard Huet. 2007. Shallow syntax analysis in Sanskrit guided by semantic nets constraints. In Pro-
ceedings of the 2006 International Workshop on Research Issues in Digital Libraries, New York, NY,
USA. ACM.
Amba Kulkarni and Devanand Shukl. 2009. Sanskrit morphological analyser: Some issues. Indian
Linguistics, 70(1-4):169?177.
Amba Kulkarni, Sheetal Pokar, and Devanand Shukl. 2010. Designing a constraint based parser for
Sanskrit. In G N Jha, editor, Proceedings of the 4th International Sanskrit Computational Linguistics
Symposium. Springer-Verlag LNAI 6465.
Amba Kulkarni. 2013. A deterministic dependency parser with dynamic programming for Sanskrit. In
Proceedings of the Second International Conference on Dependency Linguistics (DepLing 2013), pages
157?166, Prague, August. Charles University Matfyzpress, Prague, Czech Republic.
Anil Kumar, Vipul Mittal, and Amba Kulkarni. 2010. Sanskrit compound processor. In G N Jha, editor,
Proceedings of the International Sanskrit Computational Linguistics Symposium. Springer-Verlag LNAI
6465.
Vipul Mittal. 2010. Automatic sanskrit segmentizer using finite state transducers. In Proceedings
of the ACL 2010 Student Research Workshop, pages 85?90, Uppsala, Sweden, July. Association for
Computational Linguistics.
M. Monier-Williams, E. Leumann, and C. Cappeller. 1999. A Sanskrit-English Dictionary: Etymological
And Philologically Arranged With Special Reference To Cognate Indo-European Languages. Asian
Educational Services.
Sivaja S. Nair and Amba Kulkarni. 2010. The knowledge structure in Amarakos?a. In G N Jha, editor,
Proceedings of the International Sanskrit Computational Linguistics Symposium. Springer-Verlag LNAI
6465.
Peter Scharf and Malcolm Hyman. 2009. Linguistic Issues in Encoding Sanskrit. Motilal Banarsidass,
Delhi.
S.C. Vasu. 1980. The As
.
t
.
a?dhya?y?? of Pa?n
.
ini. Motilal Banarsidass.
51
