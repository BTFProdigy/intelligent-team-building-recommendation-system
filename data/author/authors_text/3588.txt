Example-based Speech Intention Understanding and
Its Application to In-Car Spoken Dialogue System
Shigeki Matsubara? Shinichi Kimura? Nobuo Kawaguchi?
Yukiko Yamaguchi? and Yasuyoshi Inagaki?
?Information Technology Center, Nagoya University
?Graduate School of Engineering, Nagoya University
CIAIR, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
matubara@itc.nagoya-u.ac.jp
Abstract
This paper proposes a method of speech inten-
tion understanding based on dialogue examples.
The method uses a spoken dialogue corpus with
intention tags to regard the intention of each in-
put utterance as that of the sentence to which it
is the most similar in the corpus. The degree of
similarity is calculated according to the degree
of correspondence in morphemes and dependen-
cies between sentences, and it is weighted by
the dialogue context information. An exper-
iment on inference of utterance intentions us-
ing a large-scale in-car spoken dialogue corpus
of CIAIR has shown 68.9% accuracy. Further-
more, we have developed a prototype system of
in-car spoken dialogue processing for a restau-
rant retrieval task based on our method, and
confirmed the feasiblity of the system.
1 Introduction
In order to interact with a user naturally and
smoothly, it is necessary for a spoken dialogue
system to understand the intentions of utter-
ances of the user exactly. As a method of speech
intention understanding, Kimura et al has pro-
posed a rule-based approach (Kimura et al,
1998). They have defined 52 kinds of utterance
intentions, and constructed rules for inferring
the intention from each utterance by taking ac-
count of the intentions of the last utterances, a
verb, an aspect of the input utterance, and so
on. The huge work for constructing the rules,
however, cannot help depending on a lot of
hands, and it is also difficult to modify the rules.
On the other hand, a technique for tagging di-
alogue acts has been proposed so far (Araki et
al., 2001). For the purpose of concretely deter-
mining the operations to be done by the system,
the intention to be inferred should be more de-
tailed than the level of dialogue act tags such as
?yes-no question? and ?wh question?.
This paper proposes a method of understand-
ing speeches intentions based on a lot of dia-
logue examples. The method uses the corpus in
which the utterance intention has given to each
sentence in advance. We have defined the ut-
terance intention tags by extending an annota-
tion scheme of dialogue act tags, called JDTAG
(JDRI, 2000), and arrived at 78 kinds of tags
presently. To detail an intention even on the
level peculiar to the task enables us to describe
the intention linking directly to operations of
the system.
In the technique for the intention inference,
the degree of similarity of each input utter-
ance with every sentence in a corpus is calcu-
lated. The calculation is based on the degree of
morphologic correspondence and that of depen-
dency correspondence. Furthermore, the degree
of similarity is weighted by using dialogue con-
text information. The intention of the utterance
to which the maximum score is given in the cor-
pus, will be accepted as that of the input utter-
ance. Our method using dialogue examples has
the advantage that it is not necessary to con-
struct rules for inferring the intention of every
utterance and that the system can also robustly
cope with the diversity of utterances.
An experiment on intention inference has
been made by using a large-scale corpus of spo-
ken dialogues. The experimental result, provid-
ing 68.9% accuracy, has shown our method to
be feasible and effective. Furthermore, we have
developed, based on our method, a prototype
system of in-car spoken dialogue processing for
a restaurant retrieval task, and confirmed the
feasiblity of the system.
Chikaku-ni chushajo-wa aru-ka-na
(Is there a parking lot nearby?)
Kono chikaku-ni firstfood aru?
(Is there a first food shop near here?
Mosburger-ga gozai-masu-ga
(Mosburger is near here.)
Spoken dialogue 
corpus with
intention tags
?????????
???????????
??????????
????????
???????????
?????????????
Dependency and morpheme analysis
System?s
speech
Intensions
probability
Calculation
of similarity
weighting
Utterance intension: ?parking lot question?
Context information
User?s
speech
Figure 1: Flow of the intention inference pro-
cessing
2 Outline of Example-based
Approach
Intentions of a speaker would appear in the vari-
ous types of phenomenon relevant to utterances,
such as phonemes, morphemes, keywords, sen-
tential structures, and contexts. An example-
based approach is expected to be effective for
developing the system which can respond to the
human?s complicated and diverse speeches. A
dialogue corpus, in which a tag showing an ut-
terance intention is given to each sentence, is
used for our approach. In the below, the outline
of our method is explained by using an inference
example.
Figure 1 shows the flow of our intention
inference processing for an input utterance
?Chikaku-ni chushajo-wa aru-ka-na ? (Is there
a parking lot nearby?)?. First, morphological
analysis and dependency analysis to the utter-
ance are carried out.
Then, the degree of similarity of each input
utterance with sentences in the corpus can be
calculated by using the degree of correspon-
dence since the information on both morphol-
ogy and dependency are given to all sentences
in the corpus in advance. In order to raise the
accuracy of the intention inference, moreover,
the context information is taken into consid-
eration. That is, according to the occurrence
probability of a sequence of intentions learned
from a dialogue corpus with the intention tags,
the degree of similarity with each utterance is
weighted based on the intentions of the last ut-
terances. Consequently, if the utterance whose
degree of similarity with the input utterance is
the maximum is ?sono chikaku-ni chushajo ari-
masu-ka? (Is there a parking lot near there?)?,
the intention of the input utterance is regarded
as ?parking lot question?.
3 Similarity and its Calculation
This section describes a technique for calculat-
ing the degree of similarity between sentences
using the information on both dependency and
morphology.
3.1 Degree of Similarity between
Sentences
In order to calculate the degree of similarity be-
tween two sentences, it can be considered to
make use of morphology and dependency infor-
mation. The calculation based on only mor-
phemes means that the similarity of only sur-
face words is taken into consideration, and thus
the result of similarity calculation may become
large even if they are not so similar from a struc-
tural point of view. On the other hand, the cal-
culation based on only dependency relations has
the problem that it is difficult to express the lex-
ical meanings for the whole sentence, in partic-
ular, in the case of spoken language. By using
both the information on morphology and de-
pendency, it can be expected to carry out more
reliable calculation.
Formula (1) defines the degree of similarity
between utterances as the convex combination
? of the degree of similarity on dependency, ?d,
and that on morpheme, ?m.
? = ??d + (1 ? ?)?m (1)
?d : the degree of similarity in dependency
?m: the degree of similarity in morphology
? : the weight coefficient (0 ? ? ? 1)
Section 3.2 and 3.3 explain ?d and ?m, re-
spectively.
3.2 Dependency Similarity
Generally speaking, a Japanese dependency re-
lation means the modification relation between
a bunsetsu and a bunsetsu. For example,
a spoken sentence ?kono chikaku-ni washoku-
no mise aru? (Is there a Japanese restau-
rant near here?)? consists of five bunsetsus of
?kono (here)?, ?chikaku-ni (near)?, ?washoku-
no (Japanese-style food)?, ?mise (a restau-
rant)?, ?aru (being)?, and there exist some de-
pendencies such that ?mise? modifies ?aru?. In
the case of this instance, the modifying bun-
setsu ?mise? and the modified bunsetsu ?aru?
are called dependent and head, respectively. It
is said that these two bunsetsus are in a depen-
dency relation. Likewise, ?kono?, ?chikaku-ni?
and ?washoku-no? modify ?chikaku-ni?, ?aru?
and ?mise?, respectively. In the following of this
paper, a dependency relation is expressed as the
order pair of bunsetsus like (mise, aru), (kono,
chikaku-ni).
A dependency relation expresses a part of
syntactic and semantic characteristics of the
sentence, and can be strongly in relation to the
intentional content. That is, it can be expected
that two utterances whose dependency relations
are similar each other have a high possibility
that the intentions are also so.
A formula (2) defines the degree of similar-
ity in Japanese dependency, ?D, between two
utterances SA and SB as the degree of corre-
spondence between them.
?d =
2CD
DA + DB
(2)
DA: the number of dependencies in SA
DB: the number of dependencies in SB
CD : the number of dependencies in corre-
spondence
Here, when the basic forms of independent
words in a head bunsetsu and in a dependent
bunsetsu correspond with each other, these de-
pendency relations are considered to be in cor-
respondence. For example, two dependencies
(chikaku-ni, aru) and (chikaku-ni ari-masu-ka)
correspond with each other because the inde-
pendent words of the head bunsetsu and the de-
pendent bunsetsu are ?chikaku? and ?aru?, re-
spectively. Moreover, each word class is given
to nouns and proper nouns characteristic of a
dialogue task. If a word which constitutes each
dependency belongs to the same class, these de-
pendencies are also considered to be in corre-
spondence.
3.3 Morpheme Similarity
A formula (3) defines the degree of similarity in
morpheme ?m between two sentences SA and
????????????? ??????????????
(Is there a Japanese restaurant near here?)
????
Japanese dependency
4 dependencies
common dependencies: 3
?d = 0.86
Japanese morpheme
common morphemes: 6
?m = 0.80
?
= 0.82
Degree of Similarity
If?= 0.4,
= 0.4*0.86+0.6*0.80
7 morphemes
User?s utterance unit: Si
????????????????????? ?????????? ????
(Is there a European restaurant nearby?)
Example of utterance: Se
3 dependencies 8 morphemes
Figure 2: Example of similarity calculation
SB.
?m =
2CM
MA + MB
(3)
MA: the number of morphemes in SA
MB: the number of morphemes in SB
CM : the number of morphemes in correspon-
dence
In our research, if a word class is given to
nouns and proper nouns characteristic of a di-
alogue task and two morphemes belong to the
same class, these morphemes are also consid-
ered to be in correspondence. In order to ex-
tract the intention of the sentence more simi-
lar as the whole sentence, not only independent
words and keywords but also all the morphemes
such as noun and particle are used for the cal-
culation on correspondence.
3.4 Calculation Example
Figure 2 shows an example of the calculation
of the degree of similarity between an input ut-
terance Si ?kono chikaku-ni washoku-no mise
aru? (Is there a Japanese restaurant near
here?)? and an example sentence in a corpus,
Se, ?chikaku-ni yoshoku-no mise ari-masu-ka (Is
there a European restaurant located nearby?)?,
when a weight coefficient ? = 0.4. The num-
ber of the dependencies of Si and Se is 4 and
3, respectively, and that of dependencies in cor-
respondence is 2, i.e., (chikaku, aru) and (mise,
aru). Moreover, since ?washoku (Japanese-style
food)? and ?yoshoku? (European-style food)
belong to the same word class, the dependencies
(washoku, aru) and (yoshoku, aru) also corre-
spond with each other. Therefore, the degree
of similarity in dependency ?d comes to 0.86
by the formula (2). Since the number of mor-
phemes of Si and Se are 7 and 8, respectively,
and that of morphemes in correspondence is 6,
i.e., ?chikaku?, ?ni?, ?no?, ?mise?, ?aru(i)? and
?wa(yo)shoku?. Therefore, ?m comes to 0.80
by a formula (3). As mentioned above, ? us-
ing both morphemes and dependencies comes
to 0.82 by a formula (1).
4 Utilizing Context Information
In many cases, the intention of a user?s utter-
ance occurs in dependence on the intentions of
the previous utterances of the user or those of
the person to which the user is speaking. There-
fore, an input utterance might also receive the
influence in the contents of the speeches before
it. For example, the user usually returns the
answer to it after the system makes a question,
and furthermore, may ask the system a ques-
tion after its response. Then, in our technique,
the degree of similarity ?, which has been ex-
plained in Section 3, is weighted based on the
intentions of the utterances until it results in a
user?s utterance. That is, we consider the oc-
currence of a utterance intention In at a certain
time n to be dependent on the intentions of the
last N ? 1 utterances. Then, the conditional
occurrence probability P (In|I
n?1
n?N+1) is defined
as a formula (4).
P (In|I
n?1
n?N+1) =
C(Inn?N+1)
C(In?1n?N+1)
(4)
Here, we write a sequence of utterance in-
tentions In?N+1 ? ? ?In as Inn?N+1, call it in-
tentions N-gram, and write the number of
appearances of them in a dialogue corpus as
C(Inn?N+1). Moreover, we call the conditional
occurrence probability of the formula (4), in-
tentions N-gram probability.
The weight assignment based on the inten-
tions sequences is accomplished by reducing the
value of the degree of similarity when the in-
tentions N-gram probability is smaller than a
threshold. That is, a formula (5) defines the de-
gree of similarity ? using the weight assignment
by intentions N-gram probability.
Search
Condition search
Parking search
Nearness question
Shop question
Business hours question
Distance question
Time question
Rank question
Menu price question
Number of car question
Parking price question Parking question
???intention tag
???dialogue act tag
???conditional tag
leafYes-no question Wh question ???
Unknown information
Unknown information
Figure 3: Decision tree of intention tag (a part)
? =
{
?? (P (In|I
n?1
n?N+1) ? ?)
? (otherwise)
(5)
?: weight coefficient (0 ? ? ? 1)
?: the degree of similarity
?: threshold
A typical example of the effect of using inten-
tions N-gram is shown below. For an input ut-
terance ?chikaku-ni chushajo-wa ari-masu-ka?
(Is there a parking lot located nearby?)?, the
degree of similarity with a utterance with a
tag ?parking lot question? which intends to
ask whether a parking lot is located around
the searched store, and a utterance with a tag
?parking lot search? which intends to search a
parking lot located nearby, becomes the maxi-
mum. However, if the input utterance has oc-
curred after the response intending that there
is no parking lot around the store, the system
can recognize its intention not to be ?parking
lot question? from the intentions N-gram prob-
abilities learned from the corpus, As a result,
the system can arrive at a correct utterance in-
tention ?parking lot search?.
5 Evaluation
In order to evaluate the effectiveness of our
method, we have made an experiment on ut-
terance intention inference.
5.1 Experimental Data
An in-car speech dialogue corpus which has
been constructed at CIAIR (Kawaguchi et al,
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
weight coefficient ?
a
c
c
u
r
a
c
y
recall precision
Figure 4: Relation between the weight coeffi-
cient ? and the accuracy (? = 0.3)
2001), was used as a corpus with intention tags,
and analyzed based on Japanese dependency
grammar (Matsubara et al, 2002). That is,
the intention tags were assigned manually into
all sentences in 412 dialogues about restaurant
search recorded on the corpus. The intentions
2-gram probability was learned from the sen-
tences of 174 dialogues in them. The standard
for assigning the intention tags was established
by extending the decision tree proposed as a di-
alogue tag scheme (JDRI, 2000). Consequently,
78 kinds of intention tags were prepared in all
(38 kinds are for driver utterances). The inten-
tion tag which should be given to each utter-
ance can be defined by following the extended
decision tree. A part of intention tags and the
sentence examples is shown in Table 1, and a
part of the decision tree for driver?s utterances
is done in Figure 3 1.
A word class database (Murao et al, 2001),
which has been constructed based on the cor-
pus, was used for calculating the rates of cor-
respondence in morphemes and dependencies.
Moreover, Chasen (Matsumoto et al, 99) was
used for the morphological analysis.
5.2 Experiment
5.2.1 Outline of Experiment
We have divided 1,609 driver?s utterances of
238 dialogues, which is not used for learning
the intentions 2-gram probability, into 10 pieces
equally, and evaluated by cross validation. That
is, the inference of the intentions of all 1,609 sen-
1In Figure 3, the description in condition branches is
omitted.
????
????
???
????
????
????
????
???
? ??? ??? ??? ??? ??? ??? ??? ??? ??? ?
????????????????????
?
?
?
?
?
?
?
?
???????????????????
?????????????????
Figure 5: Relation between the weight coeffi-
cient ? and the accuracy
tences was performed, and the recall and preci-
sion were calculated. The experiments based on
the following four methods of calculating the de-
gree of similarity were made, and their results
were compared.
1. Calculation using only morphemes
2. Calculation using only denpendencies
3. Calculation using both morphemes and
denpendencies (With changing the value of
the weight coefficient ?)
4. Calculation using intentions 2-gram prob-
abilities in addition to the condition of 3.
(With changing the value of the weight co-
efficient ? and as ? = 0)
5.2.2 Experimental Result
The experimental result is shown in Figure 4.
63.7% as the recall and 48.2% as the precision
were obtained by the inference based on the
above method 1 (i.e. ? = 0), and 62.6% and
58.6% were done in the method 2 (i.e. ? = 1.0).
On the other hand , in the experiment on the
method 3, the precision became the maximum
by ? = 0.2, providing 61.0%, and the recall by
? = 0.3 was 67.2%. The result shows our tech-
nique of using both information on morphology
and dependency to be effective.
When ? ? 0.3, the precision of the method
3 became lower than that of 1. This is because
the user speaks with driving a car (Kawaguchi
et al, 2000) and therefore there are much com-
paratively short utterances in the in-car speech
corpus. Since there is a few dependencies per
Table 1: Intention tags and their utterance examples
intention tag utterance example
search Is there a Japanese restaurant near here?
request Guide me to McDonald?s.
parking lot question Is there a parking lot?
distance question How far is it from here?
nearness question Which is near here?
restaurant menu question Are Chinese noodles on the menu?
Morphological & 
Intension
Intension
Action
Dependency analysis
Shop
information
database
Search
Response
inference
generation
In-car
spoken
dialogue
corpus with
intension tags Calculation
Intensions 2-gram
probabilityWeighting
Dictionary &
parsing rules
Intension-action
transfer rules
Context
stack
Decision
Analysis
Results
User?s Utterance
System?s utterance
Figure 6: Configuration of the prototype system
one utterance, a lot of sentences in the corpus
tend to have the maximum value in inference
using dependency information.
Next, the experimental result of the inference
using weight assignment by intentions 2-gram
probabilities, when considering as ? = 0.3, is
shown in Figure 5. At ? = 0.8, the maximum
values in both precision and recall were provided
(i.e., the precision is 68.9%). This shows our
technique of learning the context information
from the spoken dialogue corpus to be effective.
6 In-car Spoken Dialogue System
In order to confirm our technique for automat-
ically inferring the intentions of the user?s ut-
terances to be feasible and effective for task-
oriented spoken dialogue processing, a proto-
type system for restaurant retrieval has been
developed. This section describes the outline of
the system and its evaluation.
6.1 Implementation of the System
The configuration of the system is shown in Fig-
ure 6.
Table 2: Comparison between the results on in-
ferred intentions and those on given intentions
Inferred Given
Intentions num. rate num. rate
Correct 31 51.7% 42 70.0%
Partially corr. 5 8.3% 4 6.7%
Incorrect 7 11.7% 2 3.3%
No action 17 28.3% 12 20.0%
1. Morphological and dependency anal-
ysis: For the purpose of example-based
speech understanding, the morphological
and dependency analyses are given to each
user?s utterance by referring the dictionary
and parsing rules. Morphological analy-
sis is executed by Chasen (Matsumoto et
al., 99). Dependency parsing is done based
on a statistical approach (Matsubara et al,
2002).
2. Intentions inference: As section 3 and
4 explain, the intention of the user?s ut-
terance is inferred according to the degree
of similarity of it with each sentence in a
corpus, and the intentions 2-gram proba-
bilities.
3. Action: The transfer rules from the
user?s intentions to the system?s actions
have been made so that the system can
work as the user intends. We have al-
ready made the rules for all of 78 kinds
of intentions. The system decides the ac-
tions based on the rules, and executes
them. After that, it revises the context
stack. For example, if a user?s utterance
is ?kono chikaku-ni washoku-no mise ari-
masu-ka (Is there a Japanese restaurant
near here?)?, its intention is ?search?. In-
ferring it, the system retrieves the shop
information database by utilizing the key-
words such as ?washoku (Japanese restau-
rant)? and ?chikaku (near)?.
4. Response generation: The system re-
sponds based on templates which include
the name of shop, the number of shops, and
so on, as the slots.
6.2 Evaluation of the System
In order to confirm that by understanding the
user?s intention correctly the system can behave
appropriately, we have made an experiment on
the system. We used 1609 of driver?s utterances
in Section 5.2.1 as the learning data, and the
intentions 2-gram probabilities learned by 174
of dialogues in Section 5.1. Furthermore, 60 of
driver?s utterances which are not included in the
learning data were used for the test. We have
compared the results of the actions based on the
inferred intentions with those based on the given
correct intentions. The results have been classi-
fied into four groups: correct, partially correct,
incorrect, and no action.
The experimental result is shown in Table
2. The correct rate including partial correct-
ness provides 76.7% for the giving intentions
and 60.0% for the inferred intentions. We have
confirmed that the system could work appropri-
ately if correct intentions are inferred.
The causes that the system based on given
intentions did not behave appropriately for 14
utterances, have been investigated. 6 utterances
are due to the failure of keywords processing,
and 8 utterances are due to that they are out of
the system?s expectation. It is expected for the
improvement of the transfer rules to be effective
for the former. For the latter, it is considered
to turn the responses such as ?I cannot answer
the question. If the questions are about ? ? ?, I
can do that.?
7 Concluding Remarks
This paper has proposed the example-based
method for inferring speaker?s intention. The
intention of each input utterance is regarded as
that of the most similar utterance in the cor-
pus. The degree of similarity is calculated based
on the degrees of correspondence in both mor-
phemes and dependencies, taking account of the
effects of a sequence of the previous utterance?s
intentions. The experimental result using 1,609
driver?s utterances of CIAIR in-car speech cor-
pus has shown the feasibility of example-based
speech intention understanding. Furthermore,
we have developed a prototype system of in-car
spoken dialogue processing for a restaurant re-
trieval task based on our method.
Acknowledgement: The authors would like
to thank Dr. Hiroya Murao, Sanyo Electric Co.
LTD. for his helpful advice. This work is par-
tially supported by the Grand-in-Aid for COE
Research of the Ministry of Education, Sci-
ence, Sports and Culture, Japan. and Kayamori
Foundation of Information Science Advance-
ment.
References
Araki, M., Kimura, Y., Nishimoto, T. and Ni-
imi, Y.: Development of a Machine Learnable
Discourse Tagging Tool, Proc. of 2nd SIGdial
Workshop on Discourse and Dialogue, pp. 20?
25 (2001).
The Japanese Discouse Research Initiative
JDRI: Japanese Dialogue Corpus of Multi-
level Annotation, Proc. of 1st SIGdial Work-
shop on Discourse and Dialogue (2000).
Kawaguchi, N., Matsubara, S., Iwa, H., Ka-
jita, S, Takeda, K., Itakura, F. and Inagaki,
Y.: Construction of Speech Corpus in Mov-
ing Car Environment, Proc. of ICSLP-2000,
Vol. III, pp. 362?365 (2000).
Kawaguchi, N., Matsubara, S., Takeda, K.
and Itakura, F.: Multimedia Data Collec-
tion of In-car Speech Communication, Proc.
of Eurospeech-2001, pp. 2027?2030 (2001).
Kimura, H., Tokuhisa, M., Mera, K., Kai, K.
and Okada, N.: Comprehension of Intentions
and Planning for Response in Dialogue, Tech-
nical Report of IEICE, TL98-15, pp:25?32
(1998). (In Japanese)
Matsubara, S., Murase, T., Kawaguchi, N. and
Inagaki, Y.: Stochastic Dependency Parsing
of Spontaneous Japanese Spoken Language,
Proc. of COLING-2002 (2002).
Matsumoto, Y., Kitauchi, A., Yamashita, T.
and Hirano, Y.: Japanese Morphological
Analysis System Chasen version 2.0 Man-
ual, NAIST Techinical Report, NAIST-IS-
TR99009 (1999).
Murao, H., Kawaguchi, N., Matsubara, S. and
Inagaki, Y.: Example-based Query Genera-
tion for Spontaneous Speech, Proc. of ASRU-
2000 (2001).
Stochastic Dependency Parsing of
Spontaneous Japanese Spoken Language
Shigeki Matsubara? Takahisa Murase? Nobuo Kawaguchi?
and Yasuyoshi Inagaki?
?Information Technology Center/CIAIR, Nagoya University
?Graduate School of Engineering, Nagoya University
Furo-cho, Chikusa-ku, Nagoya, 464-8601, Japan
matubara@itc.nagoya-u.ac.jp
Abstract
This paper describes the characteristic features
of dependency structures of Japanese spoken
language by investigating a spoken dialogue cor-
pus, and proposes a stochastic approach to de-
pendency parsing. The method can robustly
cope with inversion phenomena and bunsetsus
which don?t have the head bunsetsu by relax-
ing the syntactic dependency constraints. The
method acquires in advance the probabilities
of dependencies from a spoken dialogue corpus
tagged with dependency structures, and pro-
vides the most plausible dependency structure
for each utterance on the basis of the probabili-
ties. An experiment on dependency parsing for
driver?s utterances in CIAIR in-car spoken dia-
logue corpus has been made. The experimental
result has shown our method to be effective for
robust parsing of spoken language.
1 Introduction
With the recent advances of the continuous
speech recognition technology, a considerable
number of studies have been made on spoken
dialogue systems. For the purpose of smooth
interaction with the user, it is necessary for the
system to understand the spontaneous speech.
Since spoken language includes a lot of gram-
matically ill-formed linguistic phenomena such
as fillers, hesitations and self-repairs, grammar-
oriented approaches are not necessarily suited
to spoken language processing. A technique for
robust parsing is thus strongly required.
This paper describes the characteristic fea-
tures of Japanese spoken language on the ba-
sis of investigating a large-scale spoken dialogue
corpus from the viewpoint of dependency, and
moreover, proposes a method of dependency
parsing by taking account of such the features.
The conventional methods of dependency pars-
ing have assumed the following three syntactic
constraints (Kurohashi and Nagao, 1994):
1. No dependency is directed from right to
left.
2. Dependencies don?t cross each other.
3. Each bunsetsu 1 , except the last one, de-
pends on only one bunsetsu.
As far as we have investigated the corpus, how-
ever, many spoken utterance do not satisfy
these constraints because of inversion phenom-
ena, bunsetsus which don?t have the head bun-
setsu, and so on. Therefore, our parsing method
relaxes the first and third ones among the above
three constraints, that is, permits the depen-
dency direction from right to left and the bun-
setsu which doesn?t depend on any bunsetsu.
The parsing results are expressed by partial de-
pendency structures.
The method acquires in advance the proba-
bilities of dependencies from a spoken dialogue
corpus tagged with dependency structures, and
provides the most plausible dependency struc-
ture for each utterance on the basis of the prob-
abilities. Several techniques for dependency
parsing based on stochastic approaches have
been proposed so far. Fujio and Matsumoto
have used the probability based on the fre-
quency of cooccurrence between two bunsetsus
for dependency parsing (Fujio and Matsumoto,
1998). Uchimoto et al have proposed a tech-
nique for learning the dependency probability
model based on a maximum entropy method
(Uchimoto et al, 1999). However, since these
1A bunsetsu is one of the linguistic units in Japanese,
and roughly corresponds to a basic phrase in English.
A bunsetsu consists of one independent word and more
than zero ancillary words. A dependency is a modifica-
tion relation between two bunsetsus.
techniques are for written language, whether
they are available for spoken language or not
is not clear. As the technique for stochas-
tic parsing of spoken language, Den has sug-
gested a new idea for detecting and parsing
self-repaired expressions, however, the phenom-
ena with which the framework can cope are re-
stricted (Den, 1995).
On the other hand, our method provides the
most plausible dependency structures for nat-
ural speeches by utilizing stochastic informa-
tion. In order to evaluate the effectiveness of our
method, an experiment on dependency parsing
has been made. In the experiment, all driver?s
utterances in 81 spoken dialogues of CIAIR in-
car speech dialogue corpus (Kawaguchi et al,
2001) have been used. The experimental result
has shown our method to be available for robust
parsing of spontaneously spoken language.
2 Linguistic Analysis of Spontaneous
Speech
We have investigated spontaneously spoken ut-
terances in an in-car speech dialogue corpus
which is constructed at the Center for Inte-
grated Acoustic Information Research (CIAIR),
Nagoya University (Kawaguchi et al, 2001) The
corpus contains speeches of dialogue between
drivers and navigators (humans, a Wizard of
OZ system, or a spoken dialogue system) and
their transcripts.
2.1 CIAIR In-car Speech Dialogue
Corpus
Data collection project of in-car speech dia-
logues at CIAIR has started in 1999 (Kawaguchi
et al, 2002). The project has developed a pri-
vate car, and been collecting a total of about
140 hours of multimodal data such as speeches,
images, locations and so on. These data would
be available for investigating in-car speech dia-
logues.
The speech files are transcribed into ASCII
text files by hand. The example of a tran-
script is shown in Figure 1. As an advance
analysis, discourse tags are assigned to fillers,
hesitations, and so on. Furthermore, each
speech is segmented into utterance units by
a pause, and the exact start time and end
time are provided for them. The environ-
mental information about sex (male/female),
speaker?s role (driver/navigator), dialogue task
Figure 1: Sample transcription of dialogue
speech
(navigation/information retrieval/...), noise
(noisy/clean) is provided for each utterance
unit.
In order to study the features of in-car dia-
logue speeches, we have investigated all driver?s
utterance units of 195 dialogues. The num-
ber per utterance unit of fillers, hesitations and
slips, are 0.34, 0.07, 0,04, respectively. The fact
that the frequencies are not less than those of
human-human conversations suggests the in-car
speech of the corpus to be spontaneous.
2.2 Dependency Structure of Spoken
Language
In order to characterize spontaneous dialogue
speeches from the viewpoint of dependency,
we have constructed a spoken language cor-
pus with dependency structures. Dependency
analyses have been provided by hand for all
driver?s utterance units in 81 spoken dialogues
of the in-car speech corpus. The specifications
of part-of-speeches and dependency grammars
are in accordance with those of Kyoto Corpus
(Kurohashi and Nagao, 1997), which is one of
Japanese text corpora. We have provided the
following criteria for the linguistic phenomena
peculiar to spoken language:
? There is no bunsetsu on which fillers and
hesitations depend. They forms depen-
dency structures independently.
? A bunsetsu whose head bunsetsu is omitted
doesn?t depend on any bunsetsu.
? The specification of part-of-speeches has
been provided for the phrases peculiar to
spoken language by adding lexical entries
to the dictionary.
? We have defined one conversational turn as
a unit of dependency parsing. The depen-
Table 1: Corpus data for dependency analysis
Dialogues 81
Utterance units 7,781
Conversational turns 6,078
Bunsetsus 24,993
Dependencies 11,789
Dependencies per unit 1.52
Dependencies per turn 1.94
dencies might be over two utterance units,
but be not hardly over two conversational
turns.
The outline of the corpus with dependency anal-
yses is shown in Table 1. There exist 11,789
dependencies for 24,993 bunsetsus 2. The av-
erage number of dependencies per turn is 1.94,
and is exceedingly less than that of written lan-
guage such as newspaper articles (about 10 de-
pendencies). This does not necessarily mean
that dependency parsing of spoken language is
easy than that of written language. It is also
required to specify the bunsetsu with no head
bunsetsu because every bunsetsu does not de-
pend on another bunsetsu. In fact, the bunset-
sus which don?t have the head bunsetsu occupy
52.8% of the whole.
Next, we investigated inversion phenomena
and dependencies over two utterance units. 320
inversions, 3.8% of all utterance turns and
about 0.04 times per turn, are in this data. This
fact means that the inversion phenomena can
not be ignored in spoken language processing.
About 86.5% of inversions appear at the last
bunsetsu. On the other hand, 73 dependen-
cies, providing 5.4% of 1,362 turns consisting
of more than two units, are over two utterance
units. Therefore, we can conclude that utter-
ance units are not always sufficient as parsing
units of spoken language.
3 Stochastic Dependency Parsing
Our method provides the most plausible depen-
dency analysis for each spoken language utter-
ance unit by relaxing syntactic constraints and
utilizing stochastic information acquired from a
large-scale spoken dialogue corpus. In this pa-
per, we define one turn as a parsing unit accord-
2The frequency of filler bunsetsus is 3,049.
0
1000
2000
3000
4000
5000
6000
7000
8000
-4 -2 0 2 4 6 8
Distance between bunsetsus
Number of dependencies
-3 -1 1 3 5 7
Figure 2: Distance between dependencies and
its frequencies
ing to the result of our investigation described
in Section 2.2
3.1 Dependency Structural Constraints
As Section 1 has already pointed out, most
conventional techniques for Japanese depen-
dency parsing have assumed three syntactic
constraints. Since the phenomena which are not
hardly in written language appear frequently in
spoken language, the actual dependency struc-
ture does not satisfy such the constraints. Our
method relaxes the constraints for the purpose
of robust dependency parsing. That is, our
method considers that the bunsetsus, which
don?t have the head bunsetsu, such as fillers
and hesitations, depend on themselves (relax-
ing the constraint that each bunsetsu depends
on another only one bunsetsu). Moreover, we
permit that a bunsetsu depends on its left-side
bunsetsu to cope with the inversion phenomena
(relaxing the constraint that dependencies are
directed from left to right) 3.
3.2 Utilizing Stochastic Information
Our method calculates the plausibility of the
dependency structure by utilizing the stochastic
information. The following attributes are used
for that:
? Basic forms of independent words of a de-
pendent bunsetsu b
i
and a head bunsetsu
3Since the phenomena that dependencies cross each
other is very few, the constraint is not relaxed.
Table 2: Examples of the types of dependencies
Dependent bunsetsu Type of dependency
denwa-ga (telephone) case particle ?ga?
mise-de (at a store) case particle ?de?
hayaku (early) continuous form
ookii (big) adnominal form
kaeru (can buy) adnominal form
chotto (briefly) adverb
b
j
: h
i
, h
j
? Part-of-speeches of independent words of a
dependent bunsetsu b
i
and a head bunsetsu
b
j
: t
i
, t
j
? Type of the dependency of a bunsetsu b
i
:
r
i
? Distance between bunsetsus b
i
and b
j
: d
ij
? Number of pauses between bunsetsus b
i
and b
j
: p
ij
? Location of a dependent bunsetsu b
i
: l
i
Here, if a dependent bunsetsu b
i
has an ancillary
word, the type of the dependencies of a bunsetsu
b
i
, r
i
, is the lexicon, part-of-speech and conju-
gated form of the word, and if not so, r
i
is the
part-of-speech and the conjugated form of the
last morpheme. Table 2 shows several exam-
ples of the types of dependencies. The location
of the dependent bunsetsu means whether it is
the last one of the turn or not. As Section 2 in-
dicates, the method uses the location attribute
for calculating the probability of the inversion,
because most inverse phenomena tend to appear
at the last of the turn.
The probability of the dependency between
bunsetsus are calculated using these attribute
values as follows:
P (i rel? j|B)
=
C(i ? j, h
i
, h
j
, t
i
, t
j
, r
i
)
C(h
i
, h
j
, t
i
, t
j
, r
i
)
(1)
?
C(i ? j, r
i
, d
ij
, p
ij
, l
i
)
C(r
i
, d
ij
, p
ij
, l
i
)
Here, C is a cooccurrence frequency function
and B is a sequence of bunsetsus (b
1
b
2
? ? ?b
n
).
In the formula (1), the first term of the right
hand side expresses the probability of cooccur-
rence between the independent words, and the
second term does that of the distance between
bunsetsus. The problem of data sparseness is re-
duced by considering these phenomena to be in-
dependent each other and separating the prob-
abilities into two terms. The probability that a
bunsetsu which doesn?t have the head bunsetsu
can also be calculated in formula (1) by con-
sidering such the bunsetsu to depend on itself
(i.e., i = j). The probability that a dependency
structure for a sequence of bunsetsus B is S can
be calculated from the dependency probabilities
between bunsetsus as follows.
P (S|B) =
n
?
i=1
P (i rel? j|B) (2)
For a sequence of bunsetsus, B, the method
identifies the dependency structure with
?argmax
S
P (S|B)? satisfying the following
conditions:
? Dependencies do not cross each other.
? Each bunsetsu doesn?t no more than one
head bunsetsu.
That is, our method considers the dependency
structure whose probability is maximum to be
the most plausible one.
3.3 Parsing Example
The parsing example of a user?s utterance
sentence including a filler ?eto?, a hesita-
tion ?so?, a inversion between ?nai-ka-na? and
?chikaku-ni?, and a pause, ?Eto konbini nai-ka-
na ?pause? so sono chikaku-ni (Is there a conve-
nience store near there?)? is as follows:
The sequence of bunsetsus of the sentence is
?[eto (well)],[konbini (convenience store)],[nai-
ka-na (Is there?)],?pause?, [so], [sono (there)],
[chikaku-ni (near)]?. The types of dependent
of bunsetsus and the dependency probabilities
between bunsetsus are shown in Table 2 and
3, respectively. Table 3 expresses that, for in-
stance, the probability that ?konbini? depends
on ?nai-ka-na? is 0.40. Moreover, the probabil-
ity of that ?eto? depends on ?eto? means that
the probability of that ?eto? does not depend
on any bunsetsu. Calculating the probability
of every possible structure according to Table
3, that of the dependency structure shown in
Figure 3 becomes the maximum.
Table 3: Dependency probabilities between bunsetsus
eto konbini nai-ka-na so soko-no chikaku-ni
eto (well) 1.00 0.00 0.00 0.00 0.00 0.00
konbini (convenience store) 0.00 0.01 0.40 0.00 0.00 0.00
nai-ka-na (Is there?) 0.00 0.00 0.88 0.00 0.00 0.00
so (hesitation) 0.00 0.00 0.00 1.00 0.00 0.00
soko-no (there) 0.00 0.02 0.00 0.00 0.00 0.75
chikaku-ni (near) 0.00 0.00 0.80 0.00 0.00 0.02
      eto        konbini          nai-kana                   so    soko-no  chikaku-ni<pose>
Figure 3: Dependency structure of ?eto konbini
nai-kana ?pose? so soko-no chikaku-ni?
Table 4: Experimental result of dependency
parsing
Item (a) (b) (a)+(b)
Precision 82.0% 88.5% 85.5%
Recall 64.3% 83.3% 73.8%
(a): The result for 241 bunsetsus with a head
(b): The result for 240 bunsetsus with no head
(a)+(b): The result for 481 bunsetsus
4 Parsing Experiment
In order to evaluate the effectiveness of our
method, an experiment on dependency pars-
ing has been made using a corpus of CIAIR
(Kawaguchi et al, 2001).
4.1 Outline of Experiment
We used the same data as that for our investiga-
tions in Section 2.2. That is, among all driver?s
utterance units of 81 dialogues, 100 turns were
used for the test data, and 5978 turns for the
learning data. The test data, the average bun-
setsus per turn is 4.81, consists of 481 depen-
dencies.
4.2 Experimental Result
The results of the parsing experiment are shown
partially in Figure 4. Table 4 shows the evalu-
ation. For the parsing accuracy, both precision
and recall are measured. 355 of 415 dependen-
cies extracted by our method are correct depen-
dencies, providing 85.5% for precision rate and
73.8% for recall rate. We have confirmed that
the parsing accuracy of our method for spoken
language is as high as that of another meth-
ods for written language (Fujio and Matsumoto,
1998; Uchimoto et al, 1999).
Our method correctly specified 200 of 240
bunsetsus which don?t have the head bunsetsu.
Most of them are fillers, hesitations and so on.
It became clear that it is effective to utilize the
dependency probabilities for identifying them.
5 Concluding Remarks
This paper has proposed a method for depen-
dency parsing of Japanese spoken language.
The method can execute the robust analysis by
relaxing syntactic constraints of Japanese and
utilizing stochastic information. An experiment
on CIAIR in-car spoken dialogue corpus has
shown our method to be effective for sponta-
neous speech understanding.
This experiment has been made on the as-
sumption that the speech recognition system
has a perfect performance. Since the tran-
script generated by a continuous speech recog-
nition system, however, might include a lot of
recognition errors, exceedingly robust parsing
technologies are strongly required. In order to
demonstrate our method to be practical for au-
tomatic speech transcription, an experiment us-
ing a continuous speech recognition system will
be done.
Acknowledgement: The authors would like
to thank all members of SLP Group in our lab-
oratory for their contribution to the construc-
tion of the Japanese spoken language corpus
with the dependency analysis. This work is par-
tially supported by the Grand-in-Aid for COE
Example of correct parsing for inversion
e-to nedan-wa ryoho oshiete-morae-masu-ka daitai
(Well, could you tell me both prices?) 
Example of incorrect parsing (1)
Hm.. chushajo-no aru aru-ka-naa
(Is there a caf? with a parking lot nearby ?) 
chikaku-ni kissaten-te
Example of incorrect parsing (2)
Hm.. ramen-ga
(Is there a caf? with a parking lot nearby ?) 
ikura-gurai-no arun-ka
correct result
incorrect result
right answer
Figure 4: The results of parsing experiment (a part)
Research of the Ministry of Education, Science,
Sports and Culture, Japan and Aritificial Intel-
ligence Research Promotion Foundation.
References
Den, Y.: A Unified Approach to Parsing Spoken
Natural Language, Proceedings of 3rd Natu-
ral Language Processing Pacific Rime Sympo-
sium (NLPRS?95), pp. 574?579 (1995).
Fujio, M. and Matsumoto, Y.: Japanese Depen-
dency Structure Analysis based on Lexical-
ized Statistics, Proceedings of 3rd Conference
on Empirical Method for Natural Language
Processing (EMNLP?98), pp. 87?96 (1998).
Kawaguchi, N., Matsubara, S., Takeda, K.,
and Itakura, F.: Multi-Dimensional Data Ac-
quisition for Integrated Acoustic Information
Research, Proceedings of 3rd International
Conference on Language Resources and Eval-
uation (LREC2002), pp. 2043?2046 (2002).
Kawaguchi, N., Matsubara, S., Takeda, K. and
Itakura, F.: Multimedia Data Collection of
In-car Speech Communication, Proceedings of
7th European Conference on Speech Commu-
nication and Technology (Eurospeech2001),
pp. 2027?2030 (2001).
Kurohashi, S. and Nagao, M.: Kyoto Univer-
sity Text Courpus Project, Proceedings of 3rd
Conference of Association for Natural Lan-
guage Processing, pages:115?118 (1997). (In
Japanese)
Kurohashi, S. and Nagao, M.: ?KN Parser:
Japanese Dependency/Case Structure Ana-
lyzer? Proceedings of Workshop on Sharable
Natural Language Resources, pages:48?95
(1994).
Matsumoto, Y., Kitauchi, A., Yamashita, T.
and Hirano, Y.: Japanese Morphological
Analysis System Chasen version 2.0 Man-
ual, NAIST Techinical Report, NAIST-IS-
TR99009 (1999).
Uchimoto, K., Sekine, S. and Isahara, K.:
Japanese Dependency Structure Analysis
based on Maximum Entropy Models, Pro-
ceedings of 9th European Chapter of the
Association for Computational Linguistics
(EACL?99), pp. 196?203 (1999).
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 169?176,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Dependency Parsing of Japanese Spoken Monologue
Based on Clause Boundaries
Tomohiro Ohno?a) Shigeki Matsubara? Hideki Kashioka?
Takehiko Maruyama] and Yasuyoshi Inagaki\
?Graduate School of Information Science, Nagoya University, Japan
?Information Technology Center, Nagoya University, Japan
?ATR Spoken Language Communication Research Laboratories, Japan
]The National Institute for Japanese Language, Japan
\Faculty of Information Science and Technology, Aichi Prefectural University, Japan
a)ohno@el.itc.nagoya-u.ac.jp
Abstract
Spoken monologues feature greater sen-
tence length and structural complexity
than do spoken dialogues. To achieve high
parsing performance for spoken mono-
logues, it could prove effective to sim-
plify the structure by dividing a sentence
into suitable language units. This paper
proposes a method for dependency pars-
ing of Japanese monologues based on sen-
tence segmentation. In this method, the
dependency parsing is executed in two
stages: at the clause level and the sen-
tence level. First, the dependencies within
a clause are identified by dividing a sen-
tence into clauses and executing stochastic
dependency parsing for each clause. Next,
the dependencies over clause boundaries
are identified stochastically, and the de-
pendency structure of the entire sentence
is thus completed. An experiment using
a spoken monologue corpus shows this
method to be effective for efficient depen-
dency parsing of Japanese monologue sen-
tences.
1 Introduction
Recently, monologue data such as a lecture and
commentary by a professional have been consid-
ered as human valuable intellectual property and
have gathered attention. In applications, such as
automatic summarization, machine translation and
so on, for using these monologue data as intel-
lectual property effectively and efficiently, it is
necessary not only just to accumulate but also to
structure the monologue data. However, few at-
tempts have been made to parse spoken mono-
logues. Spontaneously spoken monologues in-
clude a lot of grammatically ill-formed linguistic
phenomena such as fillers, hesitations and self-
repairs. In order to robustly deal with their extra-
grammaticality, some techniques for parsing of di-
alogue sentences have been proposed (Core and
Schubert, 1999; Delmonte, 2003; Ohno et al,
2005b). On the other hand, monologues also have
the characteristic feature that a sentence is gen-
erally longer and structurally more complicated
than a sentence in dialogues which have been dealt
with by the previous researches. Therefore, for
a monologue sentence the parsing time would in-
crease and the parsing accuracy would decrease. It
is thought that more effective, high-performance
spoken monologue parsing could be achieved by
dividing a sentence into suitable language units for
simplicity.
This paper proposes a method for dependency
parsing of monologue sentences based on sen-
tence segmentation. The method executes depen-
dency parsing in two stages: at the clause level
and at the sentence level. First, a dependency rela-
tion from one bunsetsu1 to another within a clause
is identified by dividing a sentence into clauses
based on clause boundary detection and then ex-
ecuting stochastic dependency parsing for each
clause. Next, the dependency structure of the en-
tire sentence is completed by identifying the de-
pendencies over clause boundaries stochastically.
An experiment on monologue dependency pars-
ing showed that the parsing time can be drasti-
1A bunsetsu is the linguistic unit in Japanese that roughly
corresponds to a basic phrase in English. A bunsetsu con-
sists of one independent word and more than zero ancillary
words. A dependency is a modification relation in which a
dependent bunsetsu depends on a head bunsetsu. That is, the
dependent bunsetsu and the head bunsetsu work as modifier
and modifyee, respectively.
169
??
???
??
???
??? ????
???
??
????
?????
?
???
????
???
?
????
????
??
?Dependency relation whose dependent bunsetsu is not the last bunsetsu of a clause
?Dependency relation whose dependent bunsetsu is the last bunsetsu of a clause
?Bunsetsu
?Clause boundary
?Clause
The public opinion poll that the Prime Minister?s Office announced the other day indicates that 
the ratio of people advocating capital punishment is nearly 80%
the other
day
that the 
Prime
Minister?s
Office
announced The 
public
opinion
poll
indicates
that
capital
punishment
advocating the ratio 
of people
nearly
80%
is
Figure 1: Relation between clause boundary and
dependency structure
cally shortened and the parsing accuracy can be
increased.
This paper is organized as follows: The next
section describes a parsing unit of Japanese mono-
logue. Section 3 presents dependency parsing
based on clause boundaries. The parsing experi-
ment and the discussion are reported in Sections
4 and 5, respectively. The related works are de-
scribed in Section 6.
2 Parsing Unit of Japanese Monologues
Our method achieves an efficient parsing by adopt-
ing a shorter unit than a sentence as a parsing unit.
Since the search range of a dependency relation
can be narrowed by dividing a long monologue
sentence into small units, we can expect the pars-
ing time to be shortened.
2.1 Clauses and Dependencies
In Japanese, a clause basically contains one verb
phrase. Therefore, a complex sentence or a com-
pound sentence contains one or more clauses.
Moreover, since a clause constitutes a syntacti-
cally sufficient and semantically meaningful lan-
guage unit, it can be used as an alternative parsing
unit to a sentence.
Our proposed method assumes that a sentence
is a sequence of one or more clauses, and every
bunsetsu in a clause, except the final bunsetsu,
depends on another bunsetsu in the same clause.
As an example, the dependency structure of the
Japanese sentence:
????????????????????
?????????????????????
?????????????The public opinion
poll that the Prime Minister?s Office announced
the other day indicates that the ratio of people
advocating capital punishment is nearly 80%)
is presented in Fig. 1. This sentence consists of
four clauses:
? ?????????????? (that the
Prime Minister?s Office announced the other
day)
? ?????????? (The public opinion
poll indicates that)
? ?????????? (advocating capital
punishment)
? ???????????????????
(the ratio of people is nearly 80%)
Each clause forms a dependency structure (solid
arrows in Fig. 1), and a dependency relation from
the final bunsetsu links the clause with another
clause (dotted arrows in Fig. 1).
2.2 Clause Boundary Unit
In adopting a clause as an alternative parsing unit,
it is necessary to divide a monologue sentence
into clauses as the preprocessing for the follow-
ing dependency parsing. However, since some
kinds of clauses are embedded in main clauses,
it is fundamentally difficult to divide a mono-
logue into clauses in one dimension (Kashioka and
Maruyama, 2004).
Therefore, by using a clause boundary anno-
tation program (Maruyama et al, 2004), we ap-
proximately achieve the clause segmentation of
a monologue sentence. This program can iden-
tify units corresponding to clauses by detecting
the end boundaries of clauses. Furthermore, the
program can specify the positions and types of
clause boundaries simply from a local morpho-
logical analysis. That is, for a sentence mor-
phologically analyzed by ChaSen (Matsumoto et
al., 1999), the positions of clause boundaries are
identified and clause boundary labels are inserted
there. There exist 147 labels such as ?compound
clause? and ?adnominal clause.? 2
In our research, we adopt the unit sandwiched
between two clause boundaries detected by clause
boundary analysis, were called the clause bound-
ary unit, as an alternative parsing unit. Here, we
regard the label name provided for the end bound-
ary of a clause boundary unit as that unit?s type.
2The labels include a few other constituents that do not
strictly represent clause boundaries but can be regarded as be-
ing syntactically independent elements, such as ?topicalized
element,? ?conjunctives,? ?interjections,? and so on.
170
Table 1: 200 sentences in ?Asu-Wo-Yomu?
sentences 200
clause boundary units 951
bunsetsus 2,430
morphemes 6,017
dependencies over clause boundaries 94
2.3 Relation between Clause Boundary Units
and Dependency Structures
To clarify the relation between clause boundary
units and dependency structures, we investigated
the monologue corpus ?Asu-Wo-Yomu 3.? In the
investigation, we used 200 sentences for which
morphological analysis, bunsetsu segmentation,
clause boundary analysis, and dependency pars-
ing were automatically performed and then modi-
fied by hand. Here, the specification of the parts-
of-speech is in accordance with that of the IPA
parts-of-speech used in the ChaSen morphologi-
cal analyzer (Matsumoto et al, 1999), the rules
of the bunsetsu segmentation with those of CSJ
(Maekawa et al, 2000), the rules of the clause
boundary analysis with those of Maruyama et
al. (Maruyama et al, 2004), and the dependency
grammar with that of the Kyoto Corpus (Kuro-
hashi and Nagao, 1997).
Table 1 shows the results of analyzing the 200
sentences. Among the 1,479 bunsetsus in the dif-
ference set between all bunsetsus (2,430) and the
final bunsetsus (951) of clause boundary units,
only 94 bunsetsus depend on a bunsetsu located
outside the clause boundary unit. This result
means that 93.6% (1,385/1,479) of all dependency
relations are within a clause boundary unit. There-
fore, the results confirmed that the assumption
made by our research is valid to some extent.
3 Dependency Parsing Based on Clause
Boundaries
In accordance with the assumption described in
Section 2, in our method, the transcribed sentence
on which morphological analysis, clause bound-
ary detection, and bunsetsu segmentation are per-
formed is considered the input 4. The dependency
3Asu-Wo-Yomu is a collection of transcriptions of a TV
commentary program of the Japan Broadcasting Corporation
(NHK). The commentator speaks on some current social is-
sue for 10 minutes.
4It is difficult to preliminarily divide a monologue into
sentences because there are no clear sentence breaks in mono-
logues. However, since some methods for detecting sentence
boundaries have already been proposed (Huang and Zweig,
2002; Shitaoka et al, 2004), we assume that they can be de-
tected automatically before dependency parsing.
parsing is executed based on the following proce-
dures:
1. Clause-level parsing: The internal depen-
dency relations of clause boundary units are
identified for every clause boundary unit in
one sentence.
2. Sentence-level parsing: The dependency
relations in which the dependent unit is the fi-
nal bunsetsu of the clause boundary units are
identified.
In this paper, we describe a sequence of clause
boundary units in a sentence as C1 ? ? ?Cm, a se-
quence of bunsetsus in a clause boundary unit Ci
as bi1 ? ? ? bini , a dependency relation in which the
dependent bunsetsu is a bunsetsu bik as dep(bik),
and a dependency structure of a sentence as
{dep(b11), ? ? ? , dep(bmnm?1)}.
First, our method parses the dependency struc-
ture {dep(bi1), ? ? ? , dep(bini?1)} within the clause
boundary unit whenever a clause boundary unit
Ci is inputted. Then, it parses the dependency
structure {dep(b1n1), ? ? ? , dep(bm?1nm?1)}, which is a
set of dependency relations whose dependent bun-
setsu is the final bunsetsu of each clause boundary
unit in the input sentence. In addition, in both of
the above procedures, our method assumes the fol-
lowing three syntactic constraints:
1. No dependency is directed from right to left.
2. Dependencies don?t cross each other.
3. Each bunsetsu, except the final one in a sen-
tence, depends on only one bunsetsu.
These constraints are usually used for Japanese de-
pendency parsing.
3.1 Clause-level Dependency Parsing
Dependency parsing within a clause boundary
unit, when the sequence of bunsetsus in an input
clause boundary unit Ci is described as Bi (=
bi1 ? ? ? bini), identifies the dependency structure
Si (= {dep(bi1), ? ? ? , dep(bini?1)}), which max-
imizes the conditional probability P (Si|Bi). At
this level, the head bunsetsu of the final bunsetsu
bini of a clause boundary unit is not identified.
Assuming that each dependency is independent
of the others, P (Si|Bi) can be calculated as fol-
lows:
P (Si|Bi) =
ni?1?
k=1
P (bik rel? bil|Bi), (1)
171
where P (bik
rel? bil|Bi) is the probability that a bun-
setsu bik depends on a bunsetsu bil when the se-
quence of bunsetsus Bi is provided. Unlike the
conventional stochastic sentence-by-sentence de-
pendency parsing method, in our method, Bi is
the sequence of bunsetsus that constitutes not a
sentence but a clause. The structure Si, which
maximizes the conditional probability P (Si|Bi),
is regarded as the dependency structure of Bi and
calculated by dynamic programming (DP).
Next, we explain the calculation of P (bik
rel?
bil|Bi). First, the basic form of independent words
in a dependent bunsetsu is represented by hik, its
parts-of-speech tik, and type of dependency rik,
while the basic form of the independent word in
a head bunsetsu is represented by hil , and its parts-
of-speech til . Furthermore, the distance between
bunsetsus is described as diikl. Here, if a dependent
bunsetsu has one or more ancillary words, the type
of dependency is the lexicon, part-of-speech and
conjugated form of the rightmost ancillary word,
and if not so, it is the part-of-speech and conju-
gated form of the rightmost morpheme. The type
of dependency rik is the same attribute used in
our stochastic method proposed for robust depen-
dency parsing of spoken language dialogue (Ohno
et al, 2005b). Then diikl takes 1 or more than 1,
that is, a binary value. Incidentally, the above
attributes are the same as those used by the con-
ventional stochastic dependency parsing methods
(Collins, 1996; Ratnaparkhi, 1997; Fujio and Mat-
sumoto, 1998; Uchimoto et al, 1999; Charniak,
2000; Kudo and Matsumoto, 2002).
Additionally, we prepared the attribute eil to in-
dicate whether bil is the final bunsetsu of a clause
boundary unit. Since we can consider a clause
boundary unit as a unit corresponding to a sim-
ple sentence, we can treat the final bunsetsu of a
clause boundary unit as a sentence-end bunsetsu.
The attribute that indicates whether a head bun-
setsu is a sentence-end bunsetsu has often been
used in conventional sentence-by-sentence parsing
methods (e.g. Uchimoto et al, 1999).
By using the above attributes, the conditional
probability P (bik
rel? bil|Bi) is calculated as fol-
lows:
P (bik rel? bil|Bi) (2)
?= P (bik rel? bil|hik, hil, tik, til, rik, diikl, eil)
= F (b
i
k
rel? bil, hik, hil, tik, til, rik, diikl, eil)
F (hik, hil, tik, til, rik, diikl, eil)
.
Note that F is a co-occurrence frequency function.
In order to resolve the sparse data problems
caused by estimating P (bik
rel? bil|Bi) with formula
(2), we adopted the smoothing method described
by Fujio and Matsumoto (Fujio and Matsumoto,
1998): if F (hik, hil, tik, til, rik, diikl, eil) in formula (2)
is 0, we estimate P (bik
rel? bil|Bi) by using formula
(3).
P (bik rel? bil|Bi) (3)
?= P (bik rel? bil|tik, til, rik, diikl, eil)
= F (b
i
k
rel? bil, tik, til, rik, diikl, eil)
F (tik, til, rik, diikl, eil)
3.2 Sentence-level Dependency Parsing
Here, the head bunsetsu of the final bunsetsu
of a clause boundary unit is identified. Let
B (=B1 ? ? ?Bn) be the sequence of bunset-
sus of one sentence and Sfin be a set of de-
pendency relations whose dependent bunsetsu is
the final bunsetsu of a clause boundary unit,
{dep(b1n1), ? ? ? , dep(bm?1nm?1)}; then Sfin, which
makes P (Sfin|B) the maximum, is calculated by
DP. The P (Sfin|B) can be calculated as follows:
P (Sfin|B) =
m?1?
i=1
P (bini
rel? bjl |B), (4)
where P (bini
rel? bjl |B) is the probability that a
bunsetsu bini depends on a bunsetsu bjl when the
sequence of the sentence?s bunsetsus, B, is pro-
vided. Our method parses by giving consideration
to the dependency structures in each clause bound-
ary unit, which were previously parsed. That is,
the method does not consider all bunsetsus lo-
cated on the right-hand side as candidates for a
head bunsetsu but calculates only dependency re-
lations within each clause boundary unit that do
not cross any other relation in previously parsed
dependency structures. In the case of Fig. 1,
the method calculates by assuming that only three
bunsetsus ??? (the ratio of people),? or ???
????? (is)? can be the head bunsetsu of the
bunsetsu ???????? (advocating).?
In addition, P (bini
rel? bjl |B) is calculated as in
Eq. (5). Equation (5) uses all of the attributes used
in Eq. (2), in addition to the attribute sjl , which
indicates whether the head bunsetsu of bjl is the
final bunsetsu of a sentence. Here, we take into
172
Table 2: Size of experimental data set (Asu-Wo-
Yomu)
test data learning data
programs 8 95
sentences 500 5,532
clause boundary units 2,237 26,318
bunsetsus 5,298 65,821
morphemes 13,342 165,129
Note that the commentator of each program is different.
Table 3: Experimental results on parsing time
our method conv. method
average time (msec) 10.9 51.9
programming language: LISP
computer used: Pentium4 2.4 GHz, Linux
account the analysis result that about 70% of the
final bunsetsus of clause boundary units depend on
the final bunsetsu of other clause boundary units 5
and also use the attribute ejl at this phase.
P (bini
rel? bjl |B) (5)
?= P (bini
rel?bjl |hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
= F (b
ini
rel?bjl , hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
F (hini , hjl , tini , tjl , rini , dijnil, e
j
l , sjl )
4 Parsing Experiment
To evaluate the effectiveness of our method for
Japanese spoken monologue, we conducted an ex-
periment on dependency parsing.
4.1 Outline of Experiment
We used the spoken monologue corpus? Asu-
Wo-Yomu,?annotated with information on mor-
phological analysis, clause boundary detection,
bunsetsu segmentation, and dependency analy-
sis6. Table 2 shows the data used for the ex-
periment. We used 500 sentences as the test
data. Although our method assumes that a depen-
dency relation does not cross clause boundaries,
there were 152 dependency relations that contra-
dicted this assumption. This means that the depen-
dency accuracy of our method is not over 96.8%
(4,646/4,798). On the other hand, we used 5,532
sentences as the learning data.
To carry out comparative evaluation of our
method?s effectiveness, we executed parsing for
5We analyzed the 200 sentences described in Section 2.3
and confirmed 70.6% (522/751) of the final bunsetsus of
clause boundary units depended on the final bunsetsu of other
clause boundary units.
6Here, the specifications of these annotations are in accor-
dance with those described in Section 2.3.
0
50
100
150
200
250
300
350
400
0 5 10 15 20 25 30
Pa
rs
in
g 
tim
e 
[m
se
c]
Length of sentence [number of bunsetsu]
our method
conv. method
Figure 2: Relation between sentence length and
parsing time
the above-mentioned data by the following two
methods and obtained, respectively, the parsing
time and parsing accuracy.
? Our method: First, our method provides
clause boundaries for a sequence of bunset-
sus of an input sentence and identifies all
clause boundary units in a sentence by per-
forming clause boundary analysis (CBAP)
(Maruyama et al, 2004). After that, our
method executes the dependency parsing de-
scribed in Section 3.
? Conventional method: This method parses
a sentence at one time without dividing it into
clause boundary units. Here, the probability
that a bunsetsu depends on another bunsetsu,
when the sequence of bunsetsus of a sentence
is provided, is calculated as in Eq. (5), where
the attribute e was eliminated. This conven-
tional method has been implemented by us
based on the previous research (Fujio and
Matsumoto, 1998).
4.2 Experimental Results
The parsing times of both methods are shown in
Table 3. The parsing speed of our method im-
proves by about 5 times on average in comparison
with the conventional method. Here, the parsing
time of our method includes the time taken not
only for the dependency parsing but also for the
clause boundary analysis. The average time taken
for clause boundary analysis was about 1.2 mil-
lisecond per sentence. Therefore, the time cost of
performing clause boundary analysis as a prepro-
cessing of dependency parsing can be considered
small enough to disregard. Figure 2 shows the re-
lation between sentence length and parsing time
173
Table 4: Experimental results on parsing accuracy
our method conv. method
bunsetsu within a clause boundary unit (except final bunsetsu) 88.2% (2,701/3,061) 84.7% (2,592/3,061)
final bunsetsu of a clause boundary unit 65.6% (1,140/1,737) 63.3% (1,100/1,737)
total 80.1% (3,841/4,798) 76.9% (3,692/4,798)
Table 5: Experimental results on clause boundary
analysis (CBAP)
recall 95.7% (2,140/2,237)
precision 96.9% (2,140/2,209)
for both methods, and it is clear from this figure
that the parsing time of the conventional method
begins to rapidly increase when the length of a
sentence becomes 12 or more bunsetsus. In con-
trast, our method changes little in relation to pars-
ing time. Here, since the sentences used in the
experiment are composed of 11.8 bunsetsus on av-
erage, this result shows that our method is suitable
for improving the parsing time of a monologue
sentence whose length is longer than the average.
Table 4 shows the parsing accuracy of both
methods. The first line of Table 4 shows the
parsing accuracy for all bunsetsus within clause
boundary units except the final bunsetsus of the
clause boundary units. The second line shows
the parsing accuracy for the final bunsetsus of
all clause boundary units except the sentence-end
bunsetsus. We confirmed that our method could
analyze with a higher accuracy than the conven-
tional method. Here, Table 5 shows the accu-
racy of the clause boundary analysis executed by
CBAP. Since the precision and recall is high, we
can assume that the clause boundary analysis ex-
erts almost no harmful influence on the following
dependency parsing.
As mentioned above, it is clear that our method
is more effective than the conventional method in
shortening parsing time and increasing parsing ac-
curacy.
5 Discussions
Our method assumes that dependency relations
within a clause boundary unit do not cross clause
boundaries. Due to this assumption, the method
cannot correctly parse the dependency relations
over clause boundaries. However, the experi-
mental results indicated that the accuracy of our
method was higher than that of the conventional
method.
In this section, we first discuss the effect of our
method on parsing accuracy, separately for bun-
Table 6: Comparison of parsing accuracy between
conventional method and our method (for bunsetsu
within a clause boundary unit except final bun-
setsu)``````````conv. method
our method
correct incorrect total
correct 2,499 93 2,592
incorrect 202 267 469
total 2,701 360 3,061
setsus within clause boundary units (except the fi-
nal bunsetsus) and the final bunsetsus of clause
boundary units. Next, we discuss the problem of
our method?s inability to parse dependency rela-
tions over clause boundaries.
5.1 Parsing Accuracy for Bunsetsu within a
Clause Boundary Unit (except final
bunsetsu)
Table 6 compares parsing accuracies for bunsetsus
within clause boundary units (except the final bun-
setsus) between the conventional method and our
method. There are 3,061 bunsetsus within clause
boundary units except the final bunsetsu, among
which 2,499 were correctly parsed by both meth-
ods. There were 202 dependency relations cor-
rectly parsed by our method but incorrectly parsed
by the conventional method. This means that our
method can narrow down the candidates for a head
bunsetsu.
In contrast, 93 dependency relations were cor-
rectly parsed solely by the conventional method.
Among these, 46 were dependency relations over
clause boundaries, which cannot in principle be
parsed by our method. This means that our method
can correctly parse almost all of the dependency
relations that the conventional method can cor-
rectly parse except for dependency relations over
clause boundaries.
5.2 Parsing Accuracy for Final Bunsetsu of a
Clause Boundary Unit
We can see from Table 4 that the parsing accuracy
for the final bunsetsus of clause boundary units by
both methods is much worse than that for bunset-
sus within the clause boundary units (except the
final bunsetsus). This means that it is difficult
174
Table 7: Comparison of parsing accuracy between
conventional method and our method (for final
bunsetsu of a clause boundary unit)``````````conv. method
our method
correct incorrect total
correct 1037 63 1,100
incorrect 103 534 637
total 1,140 597 1,737
Table 8: Parsing accuracy for dependency rela-
tions over clause boundaries
our method conv. method
recall 1.3% (2/152) 30.3% (46/152)
precision 11.8% (2/ 17) 25.3% (46/182)
to identify dependency relations whose dependent
bunsetsu is the final one of a clause boundary unit.
Table 7 compares how the two methods parse
the dependency relations when the dependent bun-
setsu is the final bunsetsu of a clause bound-
ary unit. There are 1,737 dependency relations
whose dependent bunsetsu is the final bunsetsu of
a clause boundary unit, among which 1,037 were
correctly parsed by both methods. The number
of dependency relations correctly parsed only by
our method was 103. This number is higher than
that of dependency relations correctly parsed by
only the conventional method. This result might
be attributed to our method?s effect; that is, our
method narrows down the candidates internally for
a head bunsetsu based on the first-parsed depen-
dency structure for clause boundary units.
5.3 Dependency Relations over Clause
Boundaries
Table 8 shows the accuracy of both methods for
parsing dependency relations over clause bound-
aries. Since our method parses based on the as-
sumption that those dependency relations do not
exist, it cannot correctly parse anything. Al-
though, from the experimental results, our method
could identify two dependency relations over
clause boundaries, these were identified only be-
cause dependency parsing for some sentences was
performed based on wrong clause boundaries that
were provided by clause boundary analysis. On
the other hand, the conventional method correctly
parsed 46 dependency relations among 152 that
crossed a clause boundary in the test data. Since
the conventional method could correctly parse
only 30.3% of those dependency relations, we can
see that it is in principle difficult to identify the
dependency relations.
6 Related Works
Since monologue sentences tend to be long and
have complex structures, it is important to con-
sider the features. Although there have been
very few studies on parsing monologue sentences,
some studies on parsing written language have
dealt with long-sentence parsing. To resolve the
syntactic ambiguity of a long sentence, some of
them have focused attention on the ?clause.?
First, there are the studies that focused atten-
tion on compound clauses (Agarwal and Boggess,
1992; Kurohashi and Nagao, 1994). These tried
to improve the parsing accuracy of long sentences
by identifying the boundaries of coordinate struc-
tures. Next, other research efforts utilized the three
categories into which various types of subordinate
clauses are hierarchically classified based on the
?scope-embedding preference? of Japanese subor-
dinate clauses (Shirai et al, 1995; Utsuro et al,
2000). Furthermore, Kim et al (Kim and Lee,
2004) divided a sentence into ?S(ubject)-clauses,?
which were defined as a group of words containing
several predicates and their common subject. The
above studies have attempted to reduce the pars-
ing ambiguity between specific types of clauses in
order to improve the parsing accuracy of an entire
sentence.
On the other hand, our method utilizes all types
of clauses without limiting them to specific types
of clauses. To improve the accuracy of long-
sentence parsing, we thought that it would be more
effective to cyclopaedically divide a sentence into
all types of clauses and then parse the local de-
pendency structure of each clause. Moreover,
since our method can perform dependency pars-
ing clause-by-clause, we can reasonably expect
our method to be applicable to incremental pars-
ing (Ohno et al, 2005a).
7 Conclusions
In this paper, we proposed a technique for de-
pendency parsing of monologue sentences based
on clause-boundary detection. The method can
achieve more effective, high-performance spoken
monologue parsing by dividing a sentence into
clauses, which are considered as suitable language
units for simplicity. To evaluate the effectiveness
of our method for Japanese spoken monologue, we
conducted an experiment on dependency parsing
of the spoken monologue sentences recorded in
the ?Asu-Wo-Yomu.? From the experimental re-
175
sults, we confirmed that our method shortened the
parsing time and increased the parsing accuracy
compared with the conventional method, which
parses a sentence without dividing it into clauses.
Future research will include making a thorough
investigation into the relation between dependency
type and the type of clause boundary unit. After
that, we plan to investigate techniques for identi-
fying the dependency relations over clause bound-
aries. Furthermore, as the experiment described in
this paper has shown the effectiveness of our tech-
nique for dependency parsing of long sentences
in spoken monologues, so our technique can be
expected to be effective in written language also.
Therefore, we want to examine the effectiveness
by conducting the parsing experiment of long sen-
tences in written language such as newspaper arti-
cles.
8 Acknowledgements
This research was supported in part by a contract
with the Strategic Information and Communica-
tions R&D Promotion Programme, Ministry of In-
ternal Affairs and Communications and the Grand-
in-Aid for Young Scientists of JSPS. The first au-
thor is partially supported by JSPS Research Fel-
lowships for Young Scientists.
References
R. Agarwal and L. Boggess. 1992. A simple but use-
ful approach to conjunct indentification. In Proc. of
30th ACL, pages 15?21.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of 1st NAACL, pages 132?139.
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proc. of 34th ACL,
pages 184?191.
Mark G. Core and Lenhart K. Schubert. 1999. A syn-
tactic framework for speech repairs and other dis-
ruptions. In Proc. of 37th ACL, pages 413?420.
R. Delmonte. 2003. Parsing spontaneous speech. In
Proc. of 8th EUROSPEECH, pages 1999?2004.
M. Fujio and Y. Matsumoto. 1998. Japanese depen-
dency structure analysis based on lexicalized statis-
tics. In Proc. of 3rd EMNLP, pages 87?96.
J. Huang and G. Zweig. 2002. Maximum entropy
model for punctuation annotation from speech. In
Proc. of 7th ICSLP, pages 917?920.
H. Kashioka and T. Maruyama. 2004. Segmentation
of semantic unit in Japanese monologue. In Proc. of
ICSLT-O-COCOSDA 2004, pages 87?92.
M. Kim and J. Lee. 2004. Syntactic analysis of long
sentences based on s-clauses. In Proc. of 1st IJC-
NLP, pages 420?427.
T. Kudo and Y. Matsumoto. 2002. Japanese depen-
dency analyisis using cascaded chunking. In Proc.
of 6th CoNLL, pages 63?69.
S. Kurohashi and M. Nagao. 1994. A syntactic analy-
sis method of long Japanese sentences based on the
detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
S. Kurohashi and M. Nagao. 1997. Building a
Japanese parsed corpus while improving the parsing
system. In Proc. of 4th NLPRS, pages 451?456.
K. Maekawa, H. Koiso, S. Furui, and H. Isahara. 2000.
Spontaneous speech corpus of Japanese. In Proc. of
2nd LREC, pages 947?952.
T. Maruyama, H. Kashioka, T. Kumano, and
H. Tanaka. 2004. Development and evaluation
of Japanese clause boundaries annotation program.
Journal of Natural Language Processing, 11(3):39?
68. (In Japanese).
Y. Matsumoto, A. Kitauchi, T. Yamashita, and Y. Hi-
rano, 1999. Japanese Morphological Analysis Sys-
tem ChaSen version 2.0 Manual. NAIST Technical
Report, NAIST-IS-TR99009.
T. Ohno, S. Matsubara, H. Kashioka, N. Kato, and
Y. Inagaki. 2005a. Incremental dependency pars-
ing of Japanese spoken monologue based on clause
boundaries. In Proc. of 9th EUROSPEECH, pages
3449?3452.
T. Ohno, S. Matsubara, N. Kawaguchi, and Y. Inagaki.
2005b. Robust dependency parsing of spontaneous
Japanese spoken language. IEICE Transactions on
Information and Systems, E88-D(3):545?552.
A. Ratnaparkhi. 1997. A liner observed time statistical
parser based on maximum entropy models. In Proc.
of 2nd EMNLP, pages 1?10.
S. Shirai, S. Ikehara, A. Yokoo, and J. Kimura. 1995.
A new dependency analysis method based on se-
mantically embedded sentence structures and its per-
formance on Japanese subordinate clause. Jour-
nal of Information Processing Society of Japan,
36(10):2353?2361. (In Japanese).
K. Shitaoka, K. Uchimoto, T. Kawahara, and H. Isa-
hara. 2004. Dependency structure analysis and sen-
tence boundary detection in spontaneous Japanese.
In Proc. of 20th COLING, pages 1107?1113.
K. Uchimoto, S. Sekine, and K. Isahara. 1999.
Japanese dependency structure analysis based on
maximum entropy models. In Proc. of 9th EACL,
pages 196?203.
T. Utsuro, S. Nishiokayama, M. Fujio, and Y. Mat-
sumoto. 2000. Analyzing dependencies of Japanese
subordinate clauses based on statistics of scope em-
bedding preference. In Proc. of 6th ANLP, pages
110?117.
176
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 683?690,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Simultaneous English-Japanese Spoken Language Translation
Based on Incremental Dependency Parsing and Transfer
Koichiro Ryu
Graduate School of
Information Science,
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya, 464-8601, Japan
ryu@el.itc.nagoya-u.ac.jp
Shigeki Matsubara
Information Technology Center,
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya, 464-8601, Japan
Yasuyoshi Inagaki
Faculty of
Information Science
and Technology,
Aichi Prefectural University
Nagakute-cho, Aichi-gun,
Aichi-ken, 480-1198, Japan
Abstract
This paper proposes a method for incre-
mentally translating English spoken lan-
guage into Japanese. To realize simulta-
neous translation between languages with
different word order, such as English and
Japanese, our method utilizes the feature
that the word order of a target language
is flexible. To resolve the problem of
generating a grammatically incorrect sen-
tence, our method uses dependency struc-
tures and Japanese dependency constraints
to determine the word order of a transla-
tion. Moreover, by considering the fact
that the inversion of predicate expressions
occurs more frequently in Japanese spo-
ken language, our method takes advan-
tage of a predicate inversion to resolve the
problem that Japanese has the predicate at
the end of a sentence. Furthermore, our
method includes the function of canceling
an inversion by restating a predicate when
the translation is incomprehensible due to
the inversion. We implement a prototype
translation system and conduct an exper-
iment with all 578 sentences in the ATIS
corpus. The results indicate improvements
in comparison to two other methods.
1 Introduction
Recently, speech-to-speech translation has be-
come one of the important research topics in
machine translation. Projects concerning speech
translation such as TC-STAR (Hoge, 2002) and
DARPA Babylon have been executed, and con-
ferences on spoken language translation such as
IWSLT have been held. Though some speech
translation systems have been developed so far
(Frederking et al, 2002; Isotani et al, 2003; Liu
et al, 2003; Takezawa et al, 1998), these systems,
because of their sentence-by-sentence translation,
cannot start to translate a sentence until it has been
fully uttered. The following problems may arise in
cross-language communication:
? The conversation time become long since it
takes much time to translate
? The listener has to wait for the translation
since such systems increase the difference be-
tween the beginning time of the speaker?s ut-
terance and the beginning time of its transla-
tion
These problems are likely to cause some awk-
wardness in conversations. One effective method
of improving these problems is that a translation
system begins to translate the words without wait-
ing for the end of the speaker?s utterance, much as
a simultaneous interpreter does. This has been ver-
ified as possible by a study on comparing simul-
taneous interpretation with consecutive interpreta-
tion from the viewpoint of efficiency and smooth-
ness of cross-language conversations (Ohara et al,
2003).
There has also been some research on simulta-
neous machine interpretation with the aim of de-
veloping environments that support multilingual
communication (Mima et al, 1998; Casacuberta
et al, 2002; Matsubara and Inagaki, 1997).
To realize simultaneous translation between
languages with different word order, such as En-
glish and Japanese, our method utilizes the feature
that the word order of a target language is flexi-
ble. To resolve the problem that translation sys-
tems generates grammatically dubious sentence,
683
our method utilizes dependency structures and
Japanese dependency constraints to determine the
word order of a translation. Moreover, by consid-
ering the fact that the inversion of predicate ex-
pressions occurs more frequently in Japanese spo-
ken language, our method employs predicate in-
version to resolve the problem that Japanese has
the predicate at the end of the sentence. Further-
more, our method features the function of cancel-
ing an inversion by restating a predicate when the
translation is incomprehensible due to the inver-
sion. In the research described in this paper, we
implement a prototype translation system, and to
evaluate it, we conduct an experiment with all 578
sentences in the ATIS corpus.
This paper is organized as follows: Section
2 discusses an important problem in English-
Japanese simultaneous translation and explains the
idea of utilizing flexible word order. Section 3 in-
troduces our method for the generation in English-
Japanese simultaneous translation, and Section 4
describes the configuration of our system. Section
5 reports the experimental results, and the paper
concludes in Section 6.
2 Japanese in Simultaneous
English-Japanese Translation
In this section, we describe the problem of the
difference of word order between English and
Japanese in incremental English-Japanese transla-
tion. In addition, we outline an approach of si-
multaneous machine translation utilizing linguis-
tic phenomena, flexible word order, and inversion,
characterizing Japanese speech.
2.1 Difference of Word Order between
English and Japanese
Let us consider the following English:
(E1) I want to fly from San Francisco to Denver
next Monday.
The standard Japanese for (E1) is
(J1) raishu-no (?next?) getsuyobi-ni (?Monday?)
San Francisco-kara (?from?) Denver-he (?to?)
tobi-tai-to omoi-masu (?want to fly?).
Figure 1 shows the output timing when the trans-
lation is generated as incrementally as possible
in consideration of the word alignments between
(E1) and (J1). In Fig. 1, the flow of time is shown
from top to bottom. In this study, we assume
that the system translates input words chunk-by-
chunk. We define a simple noun phrase (e.g. San
OutputInput
raishu-no ( next) getsuyobi-ni ( Monday)
San Francisco-kara ( from)
Denver-he ( to)
tobi-tai-to omoi-masu ( want to fly)
next Monday 
I
want to fly
from
San Francisco
to
Denver
Figure 1: The output timing of the translation (J1)
OutputInput
raishu-no ( next) getsuyobi-ni ( Monday)next Monday 
I
want to fly
from
Denver-he ( to) tobi-tai-to omoi-masu ( want to fly)Denver
San Francisco-kara ( from)San Francisco
to
Figure 2: The output timing of the translation (J2)
Francisco, Denver and next Monday), a predicate
(e.g. want to fly) and each other word (e.g. I, from,
to) as a chunk. There is ?raishu-no getsuyobi-ni?
(?next Monday?) at the beginning of the transla-
tion (J1), and there is ?next Monday? correspond-
ing to ?raishu-no getsuyobi-ni? at the end of the
sentence (E1). Thus, the system cannot output
?raishu-no getsuyobi-ni? and its following trans-
lation until the whole sentence is uttered. This is
a fatal flaw in incremental English-Japanese trans-
lation because there exists an essential difference
between English and Japanese in the word order. It
is fundamentally impossible to cancel these prob-
lems as long as we assume (J1) to be the transla-
tion of (E1).
2.2 Utilizing Flexible Word Order in
Japanese
Japanese is a language with a relatively flexible
word order. Thus, it is possible that a Japanese
translation can be accepted even if it keeps the
word order of an English sentence. Let us con-
sider the following Japanese:
(J2) San Francisco-kara (?from?) Denver-he (?to?)
tobi-tai-to omoi-masu (?want to fly?) raishu-no
(?next?) getsuyobi-ni (?Monday?).
(J2) can be accepted as the translation of the sen-
tence (E1) and still keep the word order as close as
possible to the sentence (E1). Figure 2 shows the
output timing when the translation is generated as
incrementally as possible in consideration of the
word alignments between (E1) and (J2). The fig-
ure demonstrates that a translation system might
684
be able to output ?San Francisco -kara (?from?)?
when ?San Francisco? is input and ?Denver-he
(?to?) tobi-tai-to omoi-masu (?want to fly?)? when
?Denver? is input. If a translation system out-
puts the sentence (J2) as the translation of the
sentence (E1), the system can translate it incre-
mentally. The translation (J2) is not necessarily
an ideal translation because its word order differs
from that of the standard translation and it has an
inverted sentence structure. However the transla-
tion (J2) can be easily understood due to the high
flexibility of word order in Japanese. Moreover, in
spoken language machine translation, the high de-
gree of incrementality is preferred to that of qual-
ity. Therefore, our study positively utilizes flexi-
ble word order and inversion to realize incremen-
tal English-Japanese translation while keeping the
translation quality acceptable.
3 Japanese Generation based on
Dependency Structure
When an English-Japanese translation system in-
crementally translates an input sentence by utiliz-
ing flexible word order and inversion, it is pos-
sible that the system will generate a grammati-
cally incorrect Japanese sentence. Therefore, it
is necessary for the system to generate the trans-
lation while maintaining the translation quality at
an acceptable level as a correct Japanese sentence.
In this section, we describe how to generate an
English-Japanese translation that retains the word
order of the input sentence as much as possible
while keeping the quality acceptable.
3.1 Dependency Grammar in English and
Japanese
Dependency grammar illustrates the syntactic
structure of a sentence by linking individual
words. In each link, modifiers (dependents) are
connected to the word that they modify (head). In
Japanese, the dependency structure is usually de-
fined in terms of the relation between phrasal units
called bunsetsu1. The Japanese dependency rela-
tions are satisfied with the following constraints
(Kurohashi and Nagao, 1997):
? No dependency is directed from right to left.
? Dependencies do not cross each other.
1A bunsetsu is one of the linguistic units in Japanese, and
roughly corresponds to a basic phrase in English. A bunsetsu
consists of one independent word and more than zero ancil-
lary words. A dependency is a modification relation between
two bunsetsus.
Dependent
bunsetsu
Head 
bunsetsu
Dependency relation
Raishu-no getsuyobi-ni San Francisco-kara Denver-he  tobi-tai-to omoi-masu .
( next)       ( Monday)                      ( from)           ( to)       ( want to fly) 
Figure 3: The dependency structures of translation (J1)
San Francisco-kara Denver-he   tobi-tai-to omoi-masu raishu-no   getsuyobi-ni .
( from)           ( to)       ( want to fly)         ( next)     ( Monday)
Dependent
bunsetsu
Head 
bunsetsu
Inversion
Figure 4: The dependency structures of translation (J2)
? Each bunsetsu, except the last one, depends
on only one bunsetsu.
The translation (J1) is satisfied with these con-
straints as shown in Fig. 3. A sentence satis-
fying these constraints is deemed grammatically
correct sentence in Japanese. To meet this require-
ment, our method parses the dependency relations
between input chunks and generates a translation
satisfying Japanese dependency constraints.
3.2 Inversion
In this paper, we call the dependency relations
heading from right to left ?inversions?. Inversions
occur more frequently in spontaneous speech than
in written text in Japanese. That is to say, there
are some sentences in Japanese spoken language
that do not satisfy the constraint mentioned above.
Translation (J2) does not satisfy this constraint, as
shown in Fig. 4. We investigated the inversions
using the CIAIR corpus (Ohno et al, 2003) and
found the following features:
Feature 1 92.2% of the inversions are that the
head bunsetsu of the dependency relation is
a predicate. (predicate inversion)
Feature 2 The more the number of dependency
relations that depend on a predicate increases,
the more the frequency of predicate inver-
sions increases.
Feature 3 There are not three or more inversions
in a sentence.
From Feature 1, our method utilizes a predicate
inversion to retain the word order of an input sen-
tence. It also generates a predicate when the num-
ber of dependency relations that depend on a pred-
icate exceeds the constant R (from Feature 2). If
there are three or more inversions in the transla-
tion, the system cancels an inversion by restating
a predicate (from Feature 3).
685
Input
Output
POS tagging
Chunking
Syntactic parsing 
Transfer into dependency structure
Syntactic transfer
Lexicon transfer
Particle translation
POS dictionary
Chunk dictionary
Syntactic rule
Lexicon transfer
rule
Particle 
translation rule
Parsing
Transfer
Generation
Predicate translation
Determine word-order of translation
Predicate
translation rule
Figure 5: Configuration of our system
4 System Configuration
Figure 5 shows the configuration of our system.
The system translates an English speech transcript
into Japanese incrementally. It is composed of
three modules: incremental parsing, transfer and
generation. In the parsing module the parser deter-
mines the English dependency structure for input
words incrementally. In the transfer module, struc-
ture and lexicon transfer rules transform the En-
glish dependency structure into the Japanese case
structure. As for the generation module, the sys-
tem judges whether the translation of each chunk
can be output, and if so, outputs the translation
of the chunk. Figure 6 shows the processing flow
when the fragment ?I want to fly from San Fran-
cisco to Denver? of?2.1?is input. In the follow-
ing subsections we explain each module, referring
to Fig. 6.
4.1 Incremental Dependency Parsing
First, the system performs POS tagging for input
words and chunking (c.f. ?Chunk? in Fig. 6).
Next, we explain how to parse the English
phrase structure (c.f. ?English phrase structure? in
Fig. 6). When we parse the phrase structure for in-
put words incrementally, there arises the problem
of ambiguity; our method needs to determine only
one parsing result at a time. To resolve this prob-
lem our system selects the phrase structure of the
maximum likelihood at that time by using PCFG
(Probabilistic Context-Free Grammar) rules. To
resolve the problem of the processing time our sys-
tem sets a cut-off value.
NP_subj (I)
NP(?)
PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly)
PP(to)
IN(from) IN(to)
NP(San Francisco)
NP(Denver)
*
*
Transfer into dependency structure
Syntactic parsing
POS Tagging & Chunking
English
dependency
structure
English
phrase
structure
Chunk
?NP_subj? ?VP? ?IN? ?NP? ?TO? ?NP?
I           want       to      fly     from   San Francisco    to      Denver  
I       want       to      fly     from   San Francisco   to    Denver  
I     want_to_fly from     ?San Francisco? to     Denver           ?
<predicate>
Syntacitc transfer &
Lexicon transfer
Lexicon transfer rule
San Francisco San Francisco
Denver                  Denver
I                                 nil 
want to fly           tobu (fly)  <hope>
Particle translation &
Predicate translation
Particle translation rule
Japanese
case
structure
Japanese
dependency
structure
<subject>
<from>
<subj>
<to>
nil   tobu(fly) <hope>     San Francisco       Denver         ?
Predicate translation rule
tobu(fly) <hope>   
tobi-tai-to-omoi-masu
nil  tobi-tai-to omoi-masu San Francisco-kara Denver-he           ?
(want-to-fly)                                  (from)               (to)
nil   San Francisco-kara Denver-he       tobi-tai-to omoi-masu ?
(from)            (to)         (want-to-fly)
Deside word-order of translation
<null>
San Francisco-kara Denver-he tobi-tai-to omoi-masu
(from)          (to)           (want-to-fly)
Output
<from>
<to>
tobu(fly)
kara (from)
he (to)
Syntactic transfer rule
<subj>
nil
nil
Japanese
translation
Input words
translation
*
Parsing
module
Transfer
module
Generation
module
tobu(fly)
Figure 6: The translation flow for the fragment ?I
want to fly from San Francisco to Denver?
Furthermore, the system transforms the English
phrase structure into an English dependency struc-
ture (c.f. ?English dependency structure? in Fig.
6). The dependency structure for the sentence can
be computed from the phrase structure for the in-
put words by defining the category for each rule in
CFG, called a ?head child? (Collins, 1999). The
head is indicated using an asterisk * in the phrase
structure of Fig. 6. In the ?English phrase struc-
ture,? the chunk in parentheses at each node is
the head chunk of the node that is determined by
the head information of the syntax rules. If the
head chunk (e.g. ?from?) of a child node (e.g.
PP(from)) differs from that of its parent node (e.g.
VP(want-to-fly)), the head chunk (e.g. ?from?) of
the child node depends on the head chunk (e.g.
?want-to-fly?) of the parent node. Some syntax
rules are also annotated with subject and object
information. Our system uses such information to
add Japanese function words to the translation of
the subject chunk or the object chunk in the gener-
ation module. To use a predicate inversion in the
686
generation module the system has to recognize the
predicate of an input sentence. This system recog-
nizes the chunk (e.g. ?want to fly?) on which the
subject chunk (e.g. ?I?) depends as a predicate.
4.2 Incremental Transfer
In the transfer module, structure and lexicon trans-
fer rules transform the English dependency struc-
ture into the Japanese case structure (?Japanese
case structure? in Fig. 6). In the structure transfer,
the system adds a type of relation to each depen-
dency relation according to the following rules.
? If the dependent chunk of a dependency rela-
tion is a subject or object (e.g. ?I?), then the
type of such dependency relation is ?subj? or
?obj?.
? If a chunk A (e.g. ?San Francisco?) indirectly
depends on another chunk B (e.g. ?want-
to-fly?) through a preposition (e.g. ?from?),
then the system creates a new dependency re-
lation where A depends on B directly, and the
type of the relation is the preposition.
? The type of the other relations is ?null?.
In the lexicon transfer, the system transforms each
English chunk into its Japanese translation.
4.3 Incremental Generation
In the generation module, the system transforms
the Japanese case structure into the Japanese de-
pendency structure by translating a particle and
a predicate. In attaching a particle (e.g. ?kara?
(from)) to the translation of a chunk (e.g. ?San
Francisco?), the system determines the attached
particle (e.g. ?kara? (from)) by particle transla-
tion rules. In translating a predicate (e.g. ?want
to fly?), the system translates a predicate by pred-
icate translation rules, and outputs the translation
of each chunk using the method described in Sec-
tion 3.
4.4 Example of Translation Process
Figure 7 shows the processing flow for the En-
glish sentence, ?I want to fly from San Francisco
to Denver next Monday.? In Fig. 7 the underlined
words indicate that they can be output at that time.
5 Experiment
5.1 Outline of Experiment
To evaluate our method, we conducted a transla-
tion experiment was made as follows. We imple-
mented the system in Java language on a 1.0-GHz
PentiumM PC with 512 MB of RAM. The OS was
Windows XP. The experiment used all 578 sen-
tences in the ATIS corpus with a parse tree, in the
Penn Treebank (Marcus et al 1993). In addition,
we used 533 syntax rules, which were extracted
from the corpus? parse tree. The position of the
head child in the grammatical rule was defined ac-
cording to Collins? method (Collins, 1999).
5.2 Evaluation Metric
Since an incremental translation system for spo-
ken dialogues is required to realize a quick and
informative response to support smooth communi-
cation, we evaluated the translation results of our
system in terms of both simultaneity and quality.
To evaluate the translation quality of our sys-
tem, each translation result of our system was as-
signed one of four ranks for translation quality by
a human translator:
A (Perfect): no problems in either information or
grammar
B (Fair): easy to understand but some important
information is missing or it is grammatically
flawed
C (Acceptable): broken but understandable with
effort
D (Nonsense): important information has been
translated incorrectly
To evaluate the simultaneity of our system, we
calculated the average delay time for translating
chunks using the following expression:
Average delay time =
?
k
d
k
n , (1)
where d
k
is the virtual elapsed time from inputting
the kth chunk until outputting its translated chunk.
(When a repetition is used, d
k
is the elapsed time
from inputting the kth chunk until restate its trans-
lated chunk.) The virtual elapsed time increases
by one unit of time whenever a chunk is input, n
is the total number of chunks in all of the test sen-
tences.
The average delay time is effective for evalu-
ating the simultaneity of translation. However, it
is difficult to evaluate whether our system actu-
ally improves the efficiency of a conversation. To
do so, we measured ?the speaker? and the inter-
preter?s utterance time.? ?The speaker? and the in-
terpreter ?utterance time? runs from the start time
of a speaker?s utterance to the end time of its trans-
lation. We cannot actually measure actual ?the
687
Table 1: Comparing our method (Y) with two other methods (X, Z)
Quality Average Speaker and interpreter
Method A A+B A+B+C delay time utterance time (sec)
X 7 (1.2%) 48 (8.3%) 92 (15.9%) 0 4.7
Y 40 (6.9%) 358 (61.9%) 413 (71.5%) 2.79 6.0
Z


















3.79 6.4
 







        
 	



 










	




































	











	





















 	

			
	

	
					
				

	
Figure 8: The relation between the speaker?s ut-
terance time and the time from the end time of the
speaker?s utterance to the end time of the transla-
tion
speaker? and the interpreter? utterance time? be-
cause our system does not include speech recog-
nition and synthesis. Thus, the processing time
of speech recognition and transfer text-to-speech
synthesis is zero, and the speaker?s utterance time
and the interpreter?s utterance time is calculated
virtually by assuming that the speaker?s and inter-
preter?s utterance speed is 125 ms per mora.
5.3 Experiment Results
To evaluate the translation quality and simultane-
ity of our system, we compared the translation re-
sults of our method (Y) with two other methods.
One method (X) translates the input chunks with
no delay time. The other method (Z) translates the
input chunks by waiting for the whole sentence to
be input, in as consecutive translation. We could
not evaluate the translation quality of the method
Z because we have not implemented the method Z.
And we virtually compute the delay time and the
utterance time. Table 1 shows the estimation re-
sults of methods X, Y and Z. Note, however, that
we virtually calculated the average delay time and
the speaker?s and interpreter?s utterance times in
method Z without translating the input sentence.
Table 1 indicates that our method Y achieved
a 55.6% improvement over method X in terms
of translation quality and a 1.0 improvement over
method Z for the average delay time.
Figure 8 shows the relation between the
speaker?s utterance time and the time from the end
time of the speaker?s utterance to the end time of
the translation. According to Fig. 8, the longer a
speaker speaks, the more the system reduces the
time from the end time of the speaker?s utterance
to the end time of the translation.
In Section 3, we explained the constant R. Ta-
ble 2 shows increases in R from 0 to 4, with the
results of the estimation of quality, the average de-
lay time, the number of inverted sentences and the
number of sentences with restatement. When we
set the constant to R = 2, the average delay time
improved by a 0.08 over that of method Y, and
the translation quality did not decrease remark-
ably. Note, however, that method Y did not utilize
any predicate inversions.
To ascertain the problem with our method,
we investigated 165 sentences whose translations
were assigned the level D when the system trans-
lated them by utilizing dependency constraints.
According to the investigation, the system gener-
ated grammatically incorrect sentences in the fol-
lowing cases:
? There is an interrogative word (e.g. ?what??
?which?) in the English sentence (64 sen-
tences).
? There are two or more predicates in the En-
glish sentence (25 sentences).
? There is a coordinate conjunction (e.g.
?and???or?) in the English sentence (21 sen-
tences).
Other cases of decreases in the translation quality
occurred when a English sentence was ill-formed
or when the system fails to parse.
6 Conclusion
In this paper, we have proposed a method for in-
crementally translating English spoken language
into Japanese. To realize simultaneous translation
688
Table 2: The results of each R (0 ? R ? 4)
Quality Average Sentences Sentences
R A A+B A+B+C delay time with inversion with restatement
0 8 (1.4%) 152 (26.3%) 363 (62.8%) 2.51 324 27
1 14 (2.4%) 174 (30.1%) 364 (63.0%) 2.53 289 29
2 36 (6.2%) 306 (52.9%) 396 (68.5%) 2.71 73 5
3 39 (6.7%) 344 (59.5%) 412 (71.3%) 2.79 28 2
4 40 (7.0%) 358 (61.9%) 412 (71.3%) 2.79 3 2
our method utilizes the feature that word order is
flexible in Japanese, and determines the word or-
der of a translation based on dependency struc-
tures and Japanese dependency constraints. More-
over, our method employs predicate inversion and
repetition to resolve the problem that Japanese has
a predicate at the end of a sentence. We imple-
mented a prototype system and conducted an ex-
periment with 578 sentences in the ATIS corpus.
We evaluated the translation results of our sys-
tem in terms of quality and simultaneity, confirm-
ing that our method achieved a 55.6% improve-
ment over the method of translating by retaining
the word order of an original with respect to trans-
lation quality, and a 1.0 improvement over the
method of consecutive translation regarding aver-
age delay time.
Acknoledgments
The authors would like to thank Prof. Dr. Toshiki
Sakabe. They also thank Yoshiyuki Watanabe,
Atsushi Mizuno and translator Sachiko Waki for
their contribution to our study.
References
F. Casacuberta, E. Vidal and J. M. Vilar. 2002. Ar-
chitectures for speech-to-speech translation using
finite-state models, Proceedings of Workshop on
Speech-to-Speech Translation: Algorithms and Sys-
tem, pages 39-44.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing, Ph.D. Thesis, University
of Pennsylvania,
R. Frederking, A. Blackk, R. Brow, J. Moody, and
E. Stein-brecher, 2002. Field Testing the Tongues
Speech-to-Speech Machin Translation System, Pro-
ceedings of the 3rd International Conference on
Language Resources and Evaluation(LREC-2002)
pages 160-164.
H. Hoge. 2002. Project Proposal TC-STAR: Make
Speech to Speech Translation Real, Proceedings of
the 3rd International Conference on Language Re-
sources and Evaluation(LREC-2002), pages 136-
141.
R. Isotani, K. Yamada, S. Ando, K. Hanazawa, S.
Ishikawa and K. Iso. 2003. Speech-to-Speech Trans-
lation Software PDAs for Travel Conversation, NEC
Research and Development, 44, No.2 pages 197-
202.
S. Kurohashi and M. Nagao. 1997. Building a Japanese
Parsed Corpus while Improving the Parsing System,
Proceedings of 4th Natural Language Processing
Pacific Rim Symposium, pages 451-456.
F. Liu, Y. Gao, L. Gu and M. Picheny. 2003. Noise Ro-
bustness in Speech to Speech Translation, IBM Tech
Report RC22874.
M. P. Marcus, B. Santorini and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank, Computational Linguis-
tics, 19(2):310-330.
S. Matsubara and Y. Inagaki. 1997. Incremental Trans-
fer in English-Japanese Machine Translation, IE-
ICE Transactions on Information and Systems,
(11):1122-1129.
H. Mima, H. Iida and O. Furuse. 1998. Simultaneous
Interpretation Utilizing Example-based Incremental
Transfer, Proceedings of 17th International Confer-
ence on Computational Linguistics and 36th Annual
Meeting of Association for Computational Linguis-
tics, pages 855-861.
M. Ohara, S. Matsubara, K. Ryu, N. Kawaguchi and Y.
Inagaki. 2003. Temporal Features of Cross-Lingual
Communication Mediated by Simultaneous Inter-
preting: An Analysis of Parallel Translation Cor-
pus in Comparison to Consecutive Interpreting, The
Journal of the Japan Association for Interpretation
Studies pages 35-53.
T. Ohno, S. Matsubrara, N. Kawaguchi and Y. In-
agaki. 2003. Spiral Construction of Syntactically
Annotated Spoken Language Corpus, Proceedings
of 2003 IEEE International Conference on Natural
Language Processing and Knowledge Engineering,
pages 477-483.
T. Takezawa, T. Morimoto, Y. Sagisaka, N. Campbell,
H. Iida, F. Sugaya, A. Yokoo and S. Yamamoto.
1998. A Japanese-to-English Speech Translation
System:ATR-MATRIX, Proceedings of 5th Interna-
tional Conference on Spoken Language Processing,
pages 957-960.
689
English dependency structure
Input
.
.
raishu-no
( next)
getsuyobi-ni
( Monday)
next 
Monday
Denver-he ( to)
tobi-tai-to omoi-
masu
( want to fly)
Denver
to
San Francisco
-kara ( from)
San 
Francisco
from
want to fly
nil
I
Output
Japanese dependency structure
Parse tree
NP_subj (I)
NP(next Monday)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(to)
IN(from) IN(to)NP(San Francisco) NP(Denver)
*
* *
$($)
S0($)
*
NP_subj (I)
NP(next Monday)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(to)
IN(from) IN(to)NP(San Francisco) NP(Denver)
*
* *
NP_subj (I)
NP(?)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(to)
IN(from) IN(to)NP(San Francisco) NP(Denver)
*
* *
NP_subj (I)
NP(?)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(?)
IN(from) NP(San Francisco)
*
*
NP_subj (I)
NP(?)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(?)
IN(from) NP(San Francisco)
*
*
IN(to) NP(?)*
NP_subj (I)
NP(?)PP(from)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(?)
IN(from) NP(?)
*
*
NP_subj (I)
NP(?)PP(?)
VP (want_to_fly)
S (want_to_fly)
*
?VP?(want_to_fly) PP(?)
*
NP_subj (I) VP (?)
S (?)
*
I   want_to_fly from   San Francisco   to  Denver  next Monday  $
I   want_to_fly from   San Francisco   to  Denver  next Monday $(?)
I   want_to_fly from   San Francisco   to  Denver      NP(?) 
I   want_to_fly from   San Francisco   to      NP(?)     NP(?) 
I   want_to_fly from   San Francisco       PP(?)       NP(?)
I   want_to_fly from         NP(?)       PP(?)        NP(?)
$(?)
S0(?)
*
I   want_to_fly PP(?)          PP(?)        NP(?)
I      VP(?) 
nil  San Francisco-kara Denver-he  tobi-tai-to omoi-masu
raishu-no getsuyobi-ni $(?)
nil  San Francisco-kara
Denver-he tobi-tai-to omoi-masu NP(?)
nil San Francisco-kara
NP(?)-he NP(?)      tobi-tai-to omoi-masu
nil 
San Francisco-kara PP(?) NP(?)      tobi-tai-to omoi-masu
nil 
NP(?)-kara PP(?) NP(?)      tobi-tai-to omoi-masu
nil 
PP(?) PP(?) NP(?)     tobi-tai-to omoi-masu
nil VP(?)
nil  San Francisco-kara Denver-he  tobi-tai-to omoi-masu
raishu-no getsuyobi-ni $($)
Figure 7: The translation flow for ?I want to fly from San Francisco to Denver next Monday.?
690
Example-based Spoken Dialogue System using WOZ System Log
Hiroya MURAO *,**, Nobuo KAWAGUCHI **,? Shigeki MATSUBARA **,?
Yukiko YAMAGUCHI? Yasuyoshi INAGAKI?
* Digital Systems Development Center,SANYO Electric Co., Ltd.,
Hirakata-shi, Osaka, 573-8534 Japan
** Center for Integrated Acoustic Information Research,Nagoya University,
? Information Technology Center, Nagoya University,
Furo-cho, Chikusa-ku, Nagoya-shi, 464-8603 Japan
? The Faculty of Information Science and Technology, Aichi Prefectural University,
Nagakute-cho, Aichi-gun, Aichi, 480-1198, Japan
murao@hr.hm.rd.sanyo.co.jp
Abstract
This paper proposes a new framework for
a spoken dialogue system based on dia-
logue examples between human subjects
and the Wizard of OZ (WOZ) system. Us-
ing this framework and a model of infor-
mation retrieval dialogue, a spoken dia-
logue system for retrieving shop informa-
tion while driving in a car has been de-
signed. The system refers to the dialogue
examples to find an example that is suit-
able for generating a query or a reply. The
authors have also constructed a large-scale
dialogue database using a WOZ system,
which enables efficient collection of dia-
logue examples.
1 Introduction
Against the background of ever-increasing comput-
ing power, techniques for constructing spoken di-
alogue systems using large-scale speech and text
corpora have become the target of much research
(Levin et al, 1998; Young, 2002). In prior re-
search, the authors have proposed a spoken-dialogue
control technique using dialogue examples with the
aim of performing flexible dialogue control dur-
ing information-retrieval dialogue and of achieving
speech understanding robust against speech recog-
nition errors (Murao et al, 2001). This technique
uses input speech data and supplementary informa-
tion corresponding to input speech such as retrieval
formulas (queries) to form ?examples? that decide
system action. A system using this technique can-
not run effectively, however, without a large volume
of example data. Traditionally, though, collecting
human-to-human dialogue data and manually pro-
viding such supplementary information for each in-
stance of input speech has required considerable la-
bor.
In this paper, we address this problem and pro-
pose a new technique for constructing an example-
based dialogue system using, as example data, the
dialogue performed between a human subject and a
pseudo-spoken-dialogue system based on the Wiz-
ard of OZ (WOZ) scheme. We also describe a
specific spoken dialogue system for information re-
trieval that we constructed using this technique.
2 Dialogue Processing Based on Examples
We first provide an overview of example-based dia-
logue processing that we previously proposed (Mu-
rao et al, 2001).
2.1 Model of information retrieval dialogue
Given a scenario in which a human operator
searches an information database and returns infor-
mation to a user, dialog between the operator and
user can be modeled as shown in Fig. 1. The ele-
ments of this model are described below.
1. Request The user tells the operator the con-
tents of an inquiry and demands reference.
2. Retrieval The operator receiving the user?s re-
quest generates a query after referencing do-
main knowledge and current dialogue context
Domain Knowledge
and
Dialogue Context
(1)Request
(4)Reply
(2)Retrieval
(3)Search
     Results
Search
Tool
Information
Database
Queries
OperatorUser
request to query
result
 to re
ply
Search
Results
Figure 1: Information flow of information retrieval
dialogue
and then processes the query indirectly by ma-
nipulating a search tool such as an ordinary
computer.
3. Search results The search tool generates
search results.
4. Reply The operator returns a reply to the user
based on search results and dialogue context.
Setting up information flow in this way allows
us to view operator behavior in the following way.
Specifically, the operator in Fig. 1 makes two deci-
sions in the process of advancing dialog.
Decision 1: Generate a query after listening to user
speech
Decision 2: Generate a reply after receiving search
results
Here, an experienced operator would use more
than just the superficial information obtained from
user speech. To generate a query or reply that
best suits the user?s need at that time, the opera-
tor would also make use of domain knowledge, di-
alogue context, and the search results themselves.
In other words, this kind of dialogue processing can
be viewed as a mapping operation from input infor-
mation such as user speech and domain knowledge
to output information such as a query. With this in
mind, we considered whether a ?decision? to guide
such dialogue could be automatically performed by
referring to actual examples of behavior manifested
by an experienced human operator. In short, we de-
cided to store a large volume of dialogue examples,
i.e., mapping information, and to determine output
information for certain input information on the ba-
sis of mapping information stored in similar dia-
logue examples.
2.2 Generation of queries and replies based on
examples
2.2.1 Structure of example data
The two ?decisions? performed during the time of
information retrieval dialogue between the user and
operator can be expressed as a mapping between the
following input and output information.
? Input/output information in the decision for
generating a query:
Input User speech and dialogue context
Output Query
? Input/output information in the decision for
generating a reply:
Input User speech, dialogue context, and
search results
Output Reply
It is therefore sufficient to save those items that
cover such input and output information. Specifi-
cally, a large number of example data can be col-
lected using the following information as elements
to construct an example database.
1. Text of user speech
2. Query
3. Reply text
4. Search results
5. Dialogue context (past speech, grounding in-
formation, conversational objects , etc.)
The following describes the procedure for gener-
ating a query or reply with respect to input speech
by referencing an example database.
2.2.2 Query generation process
From among the examples in the example
database, the system extracts the one most similar
to the input speech and the dialogue context at that
time. It then adjusts the query in that example to fit
the input speech and generates a new query.
2.2.3 Reply generation process
The system performs a search based on the gen-
erated query and receives search results. It then ex-
tracts the most similar example from the example
database with respect to input speech, the dialogue
context at that time, and the search results. Finally,
the system adjusts the reply in that example to fit the
current conditions and generates a new reply.
2.3 Problem points
Operating a dialogue system based on dialogue
examples requires the construction of an example
database as described above. Constructing a large-
scale example database, moreover, requires a large
volume of dialogue text in which supplementary in-
formation such as queries and search results has
been provided with respect to input speech.
Up to now, we have been constructing an exam-
ple database by first collecting human-to-human di-
alogue and converting speech to text and then as-
signing queries, search results, and the like to each
instance of input speech. This, however, is a labori-
ous process. In addition, example data constructed
on the basis of human-to-human dialogue data may
have features different from those of human-to-
dialogue-system dialogue data. In other words, we
cannot call the above approach an optimal method
for constructing example data.
3 Construction of an Example Database
using the WOZ System
We propose the Wizard of OZ (WOZ) system as
one means of efficiently collecting dialogue data
that includes supplementary information attached to
speech. Carrying on a dialogue using WOZ makes
it possible to collect the information needed for
constructing an example database while collecting
speech data.
3.1 WOZ system
When carrying on a dialogue using the WOZ sys-
tem, the user feels that he or she is talking to a com-
pletely mechanical system despite the fact that a hu-
man being is actually being used for some of the
elements making up the dialogue system. Collect-
ing dialogue data by WOZ should therefore result in
Touch Panel
and
Display
Information
Database
Query 
Generation
Part
Speech 
Output
Reply 
Generation
Part
  Query
Reply Text
Log 
Information
Tree Structured
Keywords
  Search 
Results
The Operator
Speech Input
User
The WoZ Software
Reply-Statement
Bigram
Search 
Execution
Part
Speech 
Symthesis
Figure 2: Configuration of Wizard of OZ system
data that is closer to dialogue that would occur be-
tween a human and a machine.
Collecting spoken dialogue data using the WOZ
system has actually been performed a number of
times in the past (MADCOW, 1992; Bertenstam et
al., 1995; Life et al, 1996; Eskenazi et al, 1999;
San-Segundo et al, 2001; Lemmela and Boda, 2002;
Yoma et al, 2002). The objective of those stud-
ies, however, was to collect, analyze, and evaluate
dialogue data between people and artificial objects,
and in many cases, only one of the artificial-object?s
functions was taken over by a human, for example,
the speech recognition function.
Our study, however, goes further than the above.
In particular, we create special software (called
WOZ software) that allows a human being to per-
form the functions of interpreting user speech, gen-
erating queries and executing searches, and generat-
ing replies. We then propose a framework that en-
ables the operator (wizard) to carry on a dialogue
with the user while operating this WOZ software so
that obtained data can be used later to perform di-
rect control of a dialogue system. Specifically, we
configure a pseudo-spoken-dialogue system (WOZ)
consisting of WOZ software and an operator, hold
information retrieval dialogue between this system
and human subjects, and save the queries ,search re-
sults and reply statements generated at this time as
log information. We then use this log information
and text-converted speech to construct an example
database that can be used for dialogue control.
3.2 System configuration
Figure 2 shows the entire configuration of the WOZ
system that we constructed. In this configuration,
keywords 
search 
results
type of 
keywords
control buttons
Figure 3: An example of display of Wizard of OZ system (1): Query generation part
text input
buttons
search 
results
type of 
keywords
control buttons 
&
standard phrases
Figure 4: An example of display of Wizard of OZ system (2): Reply generation part
the WOZ software, which was created using the
C++ language, runs on a personal computer under
Windows2000. It consists of a screen for generat-
ing queries (query part) and a screen for generating
replies (reply part). Figures 3 and 4 show sample
screens of these parts. This GUI adopts a touch-
panel system to facilitate operations ? an operator
only has to touch a button on one of these screens
to generate a query, search an information database,
generate a reply, or output synthesized speech.
WOZ software must feature high operability to
achieve natural dialogue between the WOZ system
and a human user. When designing WOZ software
on the basis of a human-to-human dialogue corpus
that we previously collected, we used the following
techniques to enable the system to operate in real
time while carrying on a dialogue with the user.
First, the query part arranges keywords in a tree
structure by search type so that appropriate key-
words can be selected at a touch to generate a query
and retrieve information quickly 1 . Search results
are displayed at the bottom of the screen in list form.
Second, the reply part displays text-input buttons
for generating replies and a list of search results.
The text-input buttons correspond to words, phrases,
and short standard sentences, and pushing them in
1Queries that deal with context in regard to input speech are
currently not defined for the sake of simplicity in software op-
eration.
?Hungry, but not enough time.
?You want to eat Chineese noodle.
?Search Japanese food restaurant.
Figure 5: Examples of prompting panels
an appropriate order generates a reply in text form.
The arrangement of these text-input buttons on the
screen is based on connection frequency between
text elements (reply-statement bigram) as previously
determined from the human-to-human dialogue cor-
pus mentioned above. In other words, each text-
input button represents a text entry having the high-
est frequency of following the immediately previous
text entry to the left, which makes for quick genera-
tion of a reply. Furthermore, to enable quick input,
the section of the screen displaying the search results
has been designed so that the name portion of each
result can be touched directly and automatically in-
cluded in the reply. The generated reply in text form
is finally output in voice form via the speech synthe-
sis section of the system.
Switching back and forth between the query and
reply parts can be performed as needed using a
switch button. The reply part also includes but-
tons for instantly generating words and short phrases
of confirmation and encouragement (e.g., ?yes,? ?I
see?) while the user is speaking to create as natural
a dialogue as possible.
3.3 Collecting dialogue data by the WOZ
system
We targeted shop-information retrieval while driv-
ing a car as an information-retrieval application
based on spoken dialogue, and collected dialogue
data between the WOZ system and human subjects
(Kawaguchi et al, 2002). This data was collected
within an automobile driven by subjects each of
whom acted as a user searching for information. A
personal computer running the WOZ software was
placed in the automobile with the ?wizard? sitting
in the back seat. All spoken dialogue was recorded
using another personal computer.
Data collection was performed according to the
following procedure for a duration of about five min-
Table 1: Collected WOZ data
Number of Speech length Speech Units
sessions (min.)User WOZ User WOZ
487 499 791 13,828 12,487
utes per subject.
? A prompting panel such as shown in Fig. 5 is
presented to the subject.
? The subject converses freely with WOZ based
on the prompting panel shown.
The wizard operates the WOZ system while lis-
tening to the subject, that is, the wizard performs an
appropriate search and returns a reply using speech
synthesis 2 .
Table 1 shows the scale of collected data.
3.4 Constructing an example database using
WOZ log information
WOZ software was designed to output detailed log
information. This information consists mainly of
the following items. All log information is recorded
with time stamps.
? Speaker ID (input by the wizard when initiating
a dialogue)
? Query generated for the input speech in ques-
tion
? Search results returned for the generated query
(number of hits and shop IDs)
? Text of reply generated by the operator (wiz-
ard)
A saved WOZ log can be used to efficiently con-
struct an example database by the following proce-
dure. To begin with, a written record of user speech
is made based on the voice recording of spoken di-
alog with time information added. Next, based on
2The wizard generates queries, performs searches, and gen-
erates replies to the extent possible for speech to which defined
queries can be applied. If a query cannot be generated, the wiz-
ard will not keep trying and will generate only an appropriate
response.
(Well, search convenience stores near here.)
(I found CIRCLE-K Makinohara store and SUNKUS Kamenoi store near here.)
Search results
Dialogue history
The most similar 
example 
for query generation
The most similar 
example 
for reply generation
Input text
(Result of speech
 recognition)
Reply
Figure 6: A view of example-based dialogue system
Table 2: Configuration of constructed example
database
Number of Number of
sessions examples
243 1,206
the time information in the log output by WOZ soft-
ware, a correspondence is established between user
speech and queries and between search results and
replies.
We constructed an example database using a por-
tion of dialogue data collected in the above manner.
Table 2 summarizes the data used for this purpose.
Query and search-result correspondences were es-
tablished for about 20% of all user speech excluding
speech outside of the task in question and speech
outside of query specifications.
4 Spoken Dialogue System using Dialogue
Examples
We here describe a dialogue system that runs using
the example database that we constructed (see (Mu-
rao et al, 2001) for details). The task is to search for
shop information while inside an automobile. This
system was implemented using the C++ language
under Windows2000. Figure 6 shows a screen shot
of this example-based dialogue system.
4.1 System configuration
The following describes the components of this sys-
tem with reference to Fig. 7.
Dialogue example database (DEDB): Consists of
data constructed from dialogue text and log in-
formation output from WOZ software. Dia-
logue text is subjected to morphological anal-
ysis 3, and words essential to advancing the di-
alogue (e.g., shop name, facility name, food
name) are assigned word class tags based on
classes given to these words beforehand ac-
cording to meaning.
Word Class Database (WCDB): Consists of
words essential to the task in question and
classes given to them according to meaning.
Word classes are determined empirically based
on dialogue within the dialogue corpus.
Shop Information Database (SIDB): Consists of
a collection of information on about 800 restau-
rants and shops in Nagoya, the same as that
used in the WOZ system.
Speech Recognition: Uses ?Japanese Dictation
Toolkit(Kawahara et al, 2000)?. The lan-
guage model was created from the previously
collected human-to-human dialogue corpus.
3Using ChaSen morphological-analysis software for the
Japanese language (Asahara and Matsumoto, 2000).
Speech
Input
    Speech
Recognition   Query Generation
Search
Speech
Output
Dialogue Example
Database
(DEDB)
Word class
Database
(WCDB)
Shop Information
Database
(SIDB)
 Speech
Synthesis    Reply Generation
Figure 7: Configuration of example-based dialogue
system
Query Generation: Extracts from the DEDB the
example closest to current input speech and
conditions, modifies the query in that example
according to current conditions, and outputs the
result.
Search execution: Accesses the SIDB using the
generated query and obtains search results.
Reply Generation: Extracts from the DEDB the
example closest to input speech and search re-
sults, modifies the reply in that example ac-
cording to current conditions, and outputs the
result.
Speech Synthesis: Outputs replies in voice form
using a Japanese TTS (Text To Speech) soft-
ware ?EleganTalk Ver. 2.1? by Sanyo Electric
Co., Ltd. .
4.2 Operation
The following describes system operation (see Fig.
8 for a specific operation example).
Step 1: Extracting similar example for query
For a speech recognition result, the system
extracts the most similar example from the
DEDB. The robustness of the similarity cal-
culation between the input utterance and the
utterance in the DEDB should be considered
against the speech recognition error. Therefore,
a keyword matching method using the word
class information is adopted. For a speech
recognition result combined with a morpholog-
ical analysis result, independent words and the
Input:   Etto, spaghetti no omise ni ikitai na.
            (I'd like to go to a spaghetti restaurant.)
Keywords: [10: spaghetti],[omise (shop)],[iku (go)]
 1st:  U: <10:Curry> no [omise] ni [iki]tain desu kedo            
(I'd like to go to a curry restaurant. )
         Q: search KEY=<10:curry>
 2nd: U: <10: Ramen(noodles)> wo <tabe> ni [iki] taina       
(I'd like to eat noodles.)
         Q: search KEY=<10:ramen>
 3rd: U: [10: Spaghetti] de <yu-mei> na <tokoro> ga iidesu 
            ( I prefer a popular resutaurant for spaghetti.)
         Q: search KEY=<10:spaghetti>
Step1: Extracting similar example for query
Step2: Query Modification
Query in the similar case:      search KEY=<10:curry>
   Matched keywords pair:       ( <10:curry> , <10:spaghetti> )
                 Output Query:      search KEY=<10:spaghetti>
Step3: Search
  Iutput Query:      search KEY=<10:spaghetti>
 Search Result:     RESULT=NONE
Input:   Etto, spaghetti no omise ni ikitai na.
(I'd like to go to a spaghetti restaurant.)
Keywords: [10: spaghetti],[omise (shop)],[iku (go)]
1st:   U:<10: Ramen(noodles)> wo <tabe> ni [iki] taina 
(I'd like to eat noodles.)
        Q: search KEY=<10:ramen>
        S:<10:Ramen(noodles)> no [omise] wa chikaku ni arimasen 
              ( There are no noodle restaurants near here.)
        A: RESULT=NONE
2nd:  U:<10:Curry> no [omise] ni [iki]tain desu kedo  
(I'd like to go to a curry restaurant. )
        Q: search KEY=<10:curry>
        S:Hai, Curry no omise wa 5-ken arimasu          
             (Well, I found 5 curry restaurants.)
        A: RESULT=5, ID1=120,..,ID5=565
Step4: Extracting similar example for reply
 Search Result:     RESULT=NONE
Similar cases
{Similar cases
Step5: Reply Modification
     Reply in the similar case: 
                   <10:Ramen(noodles)> no [omise] wa chikaku ni arimasen 
                       ( There are no noodle restaurants near here.)
       Matched keywords pair:  
                  ( <10:Ramen(noodles)> , <10:spaghetti> )
                      Output Reply:  
                  <10:spaghetti> no [omise] wa chikaku ni arimasen
                       ( There are no spaghetti restaurants near here.)
Figure 8: Example of query and reply generation
important words to which the word class tags
are assigned according to the information in
the WCDB are regarded as the keywords, and
their similarity is calculated as follows. For
each transcription of a user?s utterances in the
DEDB, the number of matched words and the
number of important words which belong to
the same word class are accumulated with the
correspondent weight and the result is treated
as the similarity. The utterance which marks
the highest similarity is regarded as the most
similar one.
Step2: Query Modification The query for the ex-
tracted example is modified with reference to
the input utterance. The modification is per-
formed by replacing the keywords in the refer-
ence query using word class information.
Step 3: Search The SIDB is searched by using the
modified query and a search result is obtained.
Step 4: Extracting similar example for reply
The system extracts the most similar example
from the DEDB, by taking account of not only
the similarity between the input utterance and
the utterance in examples but also that between
the number of items in the search result and
that in the examples. Here, a total similarity
score is computed by performing a weighted
summation of two values: the utterance sim-
ilarity score and the search-results similarity
score obtained from the difference between
the number of search results in an example
and that obtained in Step 3. The search-results
similarity score is computed as follows.
When the number of search results by mod-
ified query is 0: Give the highest score to
examples in the example database with 0 num-
ber of search results and the lowest score to all
other examples.
When the number of search results by mod-
ified query is 1 or more: Give the high-
est score to examples in the example database
with the same number of search results and an
increasingly lower score as difference in the
number of search results becomes larger (use
heuristics).
For example, if not even one search result could
be obtained by the modified query, examples in
the example database with not even one search
result constitute a match.
Step 5: Reply Modification The reply statement
for the extracted example is modified with ref-
erence to the input utterance. The modification
is performed by replacing the words in the ref-
erence reply statement by using word class in-
formation. Then a speech synthesis module is
used to produce a reply speech.
4.3 Adding, modification, and deletion of
example data
This system allows example data to be added, mod-
ified, and deleted. When a failed operation occurs
while carrying on a dialogue, for example, buttons
located at the bottom of the screen can be used to
modify existing example data, add new examples,
and delete unnecessary examples.
5 Conclusion
This paper has proposed an efficient technique for
collecting example data using the Wizard of OZ
(WOZ) system for the purpose of guiding spoken di-
alogue using dialogue examples. This technique has
the following effects.
? Knowledge buried in the WOZ system log
(conversions from input speech to query and
reply, etc.) can be used as dialogue system
knowledge.
? Because dialogue is collected using the WOZ
system, the examples so collected are close to
dialogue that would occur in an environment
with an actual dialogue system. In other words,
dialogue examples can be collected under con-
ditions close to human-to-machine dialogue.
? The labor involved in recording speech neces-
sary for construction of an example database
can be reduced.
In future research, we plan to evaluate dialogue-
processing performance and context processing us-
ing example databases constructed with the WOZ
system.
References
Masayuki Asahara and Yuji Matsumoto. 2000. Extended
models and tools for high-performance part-of-speech
tagger. In Proceedings of COLING 2000, July.
J. Bertenstam, M. Blomberg, R. Carlson, K. Elenius,
B. Granstrom, J. Gustafson, S. Hunnicutt, J. Hogberg,
R. Lindell, L. Neovius, A. de Serpa-Leitao, L. Nord,
and N. Strom. 1995. The waxholm application data-
base. In Proceedings of Eurospeech-95, volume 1,
pages 833?836.
Maxine Eskenazi, Alexander Rudnicky, Karin Gregory,
Paul Constantinides Robert Brennan, Christina Ben-
nett, and Jwan Allen. 1999. Data collection and pro-
cessing in the carnegie mellon communicator. In Pro-
ceedings of Eurospeech-99, volume 6, pages 2695?
2698.
Nobuo Kawaguchi, Shigeki Matsubara, Kazuya Takeda,
and Fumitada Itakura. 2002. Multi-dimensional
data acquisition for integrated acoustic information
research. In Proc. of 3rd International Language
Resources and Evaluation Conference (LREC-2002),
pages 2043?2046.
T. Kawahara, A. Lee, T. Kobayashi, K. Takeda, N. Mine-
matsu, S. Sagayama, K. Itou, A. Ito, M. Yamamoto,
A. Yamada, T. Utsuro, and K. Shikano. 2000. Free
software toolkit for japanese large vocabulary contin-
uous speech recognition. In Proceedings of ICSLP-
2000, volume 4, pages 476?479.
Saija-Maaria Lemmela and Peter Pal Boda. 2002. Effi-
cient combination of type-in and wizard-of-oz tests in
speech interface development process. In Proceedings
of ICSLP-2002, pages 1477?1480.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1998. Using markov decision processes for learning
dialogue strategies. In Proceedings of ICASSP98, vol-
ume 1, pages 201?204.
A. Life, I. Salter, J.N. Temem, F. Bernard, S. Rosset,
S. Bennacef, and L. Lamel. 1996. Data collection
for the mask kiosk: Woz vs prototype system. In Pro-
ceedings of ICSLP-96, pages 1672?1675.
MADCOW. 1992. Multi-site data collection for a spo-
ken language corpus. In DARPA Speech and Natural
Language Workshop ?92.
Hiroya Murao, Nobuo Kawaguchi, Shigeki Matsubara,
and Yasuyoshi Inagaki. 2001. Example-based query
generation for spontaneous speech. In Proceedings of
2001 IEEE Workshop on Automatic Speech Recogni-
tion and Understanding (ASRU2001).
R. San-Segundo, J.M. Montero, J.M. Gutierrez, A. Gal-
lardo, J.D. Romeral, and J.M. Pardo. 2001. A
telephone-based railway information system for span-
ish: Development of a methodology for spoken dia-
logue design. In Proceedings of SIGdial-2001, pages
140?148.
Nestor Becerra Yoma, Angela Cortes, Mauricio Hormaz-
abal, and Enrique Lopez. 2002. Wizard of oz evalua-
tion of a dialogue with communicator system in chile.
In Proceedings of ICSLP-2002, pages 2701?2704.
Steve Young. 2002. Talking to machines (statistically
speaking). In Proceedings of ICSLP-2002, pages 9?
16.
Stochastically Evaluating the Validity of Partial Parse Trees in
Incremental Parsing
Yoshihide Kato1, Shigeki Matsubara2 and Yasuyoshi Inagaki3
Graduate School of International Development, Nagoya University 1
Information Technology Center, Nagoya University 2
Furo-cho, Chikusa-ku, Nagoya, 464-8601 Japan
Faculty of Information Science and Technology, Aichi Prefectural University 3
1522-3 Ibaragabasama, Kumabari, Nagakute-cho, Aichi-gun, 480-1198 Japan
yosihide@gsid.nagoya-u.ac.jp
Abstract
This paper proposes a method for evaluating the
validity of partial parse trees constructed in incre-
mental parsing. Our method is based on stochastic
incremental parsing, and it incrementally evaluates
the validity for each partial parse tree on a word-
by-word basis. In our method, incremental parser
returns partial parse trees at the point where the va-
lidity for the partial parse tree becomes greater than
a threshold. Our technique is effective for improv-
ing the accuracy of incremental parsing.
1 Introduction
Real-time spoken language processing systems,
such as simultaneous machine interpretation sys-
tems, are required to quickly respond to users? utter-
ances. To fulfill the requirement, the system needs
to understand spoken language at least incremen-
tally (Allen et al, 2001; Inagaki and Matsubara,
1995; Milward and Cooper, 1994), that is, to ana-
lyze each input sentence from left to right and ac-
quire the content.
Several incremental parsing methods have been
proposed to date (Costa et al, 2001; Haddock,
1987; Matsubara et al, 1997; Milward, 1995;
Roark, 2001). These methods construct candidate
partial parse trees for initial fragments of the input
sentence on a word-by-word basis. However, these
methods contain local ambiguity problems that par-
tial parse trees representing valid syntactic relations
can not be determined without using information
from the rest of the input sentence.
On the other hand, Marcus proposed a method
of deterministically constructing valid partial parse
trees by looking ahead several words (Marcus,
1980), while Kato et al proposed an incremental
parsing which delays the decision of valid partial
parse trees (Kato et al, 2000). However, it is hard to
say that these methods realize broad-coverage incre-
mental parsing. The method in the literature (Mar-
cus, 1980) uses lookahead rules, which are con-
structed by hand, but it is not clear whether broad
coverage lookahead rules can be obtained. The
incremental parsing in the literature (Kato et al,
2000), which is based on context free grammar, is
infeasible to deal with large scale grammar, because
the parser exhaustively searches all candidate partial
parse trees in top-down fashion.
This paper proposes a probabilistic incremental
parser which evaluates the validity of partial parse
trees. Our method extracts a grammar from a tree-
bank, and the incremental parsing uses a beam-
search strategy so that it realizes broad-coverage
parsing. To resolve local ambiguity, the parser in-
crementally evaluates the validity of partial parse
trees on a word-by-word basis, and delays the deci-
sion of which partial parse trees should be returned,
until the validity for the partial parse tree becomes
greater than a threshold. Our technique is effective
for improving the accuracy of incremental parsing.
This paper is organized as follows: The next
section proposes a probabilistic incremental parser.
Section 3 discusses the validity of partial parse tree
constructed in incremental parsing. Section 4 pro-
poses a method of incrementally evaluating the va-
lidity of partial parse tree. In section 5, we report an
experimental evaluation of our method.
2 TAG-based Incremental Parsing
Our incremental parsing is based on tree adjoining
grammar (TAG) (Joshi, 1985). This section pro-
poses a TAG-based incremental parsing method.
2.1 TAG for Incremental Parsing
Firstly, we propose incremental-parsing-oriented
TAG (ITAG). An ITAG comprises two sets of ele-
mentary trees just like TAG: initial trees and auxil-
iary trees. The difference between ITAG and TAG
is the form of elementary trees. Every ITAG ini-
tial tree is leftmost-expanded. A tree is leftmost-
expanded if it is of the following forms:
1. [t]X , where t is a terminal symbol and X is a
nonterminal symbol.
SNP VPPRPI
VPVB NPfound
NPDT NN
a
NNdime NPDT NNthe
NN
wood
Initial trees:1 2
5 7 8
10
VPVB NPfound
3 ADJP
PPIN NPin
NPNP* PPIN NPin
VPVP*
Auxiliary trees:1 2
NPDT NN
a
6 JJ NPDT NNthe
9 JJ
VPVBfound
4
Figure 1: Examples of ITAG elementary trees
2. [?X1 ? ? ?Xk]X , where ? is a leftmost expanded
tree, X1, . . . , Xk, X are nonterminal symbols.
On the other hand, every ITAG auxiliary tree is of
the following form:
[X??X1 ? ? ?Xk]X
where ? is a leftmost expanded tree and X ,
X1, . . . , Xk are nonterminal symbols. X? is called
a foot node. Figure 1 shows examples of ITAG ele-
mentary trees.
These elemental trees can be combined by using
two operations: substitution and adjunction.
substitution The substitution operation replaces a
leftmost nonterminal leaf of a partial parse tree
? with an initial tree ? having the same nonter-
minal symbol at its root. We write s? for the
operation of substituting ? and s?(?) for the
result of applying s? to ?.
adjunction The adjunction operation splits a par-
tial parse tree ? at a nonterminal node having
no nonterminal leaf, and inserts an auxiliary
tree ? having the same nonterminal symbol at
its root. We write a? for the operation of ad-
joining ? and a?(?) for the result of applying
a? to ?.
The substitution operation is similar to rule expan-
sion of top-down incremental parsing such as (Mat-
subara et al, 1997; Roark, 2001). Furthermore,
by introducing the adjunction operation to incre-
mental parsing, we can expect that local ambiguity
of left-recursive structures is decreased (Lombardo
and Sturt, 1997).
Our proposed incremental parsing is based on
ITAG. When i-th word wi is scanned, the parser
combines elementary trees for wi with partial parse
trees for w1 ? ? ?wi?1 to construct the partial parse
trees for w1 ? ? ?wi?1wi.
As an example, let us consider incremental pars-
ing of the following sentence by using ITAG shown
in Figure 1:
I found a dime in the wood. (1)
Table 1 shows the process of tree construction
for the sentence (1). When the word ?found? is
scanned, partial parse trees #3, #4 and #5 are con-
structed by applying substitution operations to par-
tial parse tree #2 for the initial fragment ?I?. When
the word ?in? is scanned, partial parse trees #12 and
#13 are constructed by applying adjunction opera-
tions to partial parse tree #10 for the initial frag-
ment ?I found a dime?. This example shows that
the ITAG based incremental parsing is capable of
constructing partial parse trees of initial fragments
for every word input.
2.2 ITAG Extraction from Treebank
Here, we propose a method for extracting an ITAG
from a treebank to realize broad-coverage incre-
mental parsing. Our method decomposes parse trees
in treebank to obtain ITAG elementary trees. The
decomposition is as follows:
? for each node ?1 having no left-sibling, if the
parent ?p has the same nonterminal symbol as
?1, split the parse tree at ?1 and ?p, and com-
bine the upper tree and the lower tree. ?1 of
intermediate tree is a foot node.
? for each node ?2 having only one left-sibling,
if the parent ?p does not have the same nonter-
minal symbol as the left-sibling ?1 of ?2, split
the parse tree at ?2.
? for the other node ? in the parse tree, split the
parse tree at ?.
For example, The initial trees ?1, ?2, ?5, ?7 ?8 and
?10 and the auxiliary tree ?2 are extracted from the
parse tree #18 in Table 1.
Our proposed tree extraction is similar to the TAG
extractions proposed in the literatures (Chen and
Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999).
The main difference between these methods is the
position of nodes at which parse trees are split.
While the methods in the literatures (Chen and
Vijay-Shanker, 2000; Chiang, 2003; Xia, 1999) uti-
lize a head percolation rule to split the parse trees at
complement nodes, our method splits the parse trees
Table 1: Incremental parsing process of ?I found a dime in the wood.?
word # partial parse tree
1 s
I 2 [[[I]prp]npvp]s
found 3 [[[I]prp]np[[found]vbnp]vp]s
4 [[[I]prp]np[[found]vbnp adjp]vp]s
5 [[[I]prp]np[[found]vb]vp]s
a 6 [[[I]prp]np[[found]vb[[a]dtnn]np]vp]s
7 [[[I]prp]np[[found]vb[[a]dtjj nn]np]vp]s
8 [[[I]prp]np[[found]vb[[a]dtnn]npadjp]vp]s
9 [[[I]prp]np[[found]vb[[a]dtjj nn]npadjp]vp]s
dime 10 [[[I]prp]np[[found]vb[[a]dt[dime]nn]np]vp]s
11 [[[I]prp]np[[found]vb[[a]dt[dime]nn]npadjp]vp]s
in 12 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]innp]pp]vp]s
13 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]innp]pp]np]vp]s
the 14 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dtnn]np]pp]vp]s
15 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dtjj nn]np]pp]vp]s
16 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dtnn]np]pp]np]vp]s
17 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dtjj nn]np]pp]np]vp]s
wood 18 [[[I]prp]np[[[found]vb[[a]dt[dime]nn]np]vp[[in]in[[the]dt[wood]nn]np]pp]vp]s
19 [[[I]prp]np[[found]vb[[[a]dt[dime]nn]np[[in]in[[the]dt[wood]nn]np]pp]np]vp]s
at left recursive nodes and nodes having left-sibling.
The elementary trees extracted by our method are of
the forms described in section 2.1, and can be com-
bined from left to right on a word-by-word basis.
The property is suitable for incremental parsing. On
the other hand, the elementary trees obtained by the
method based on head information does not neces-
sarily have this property 1.
2.3 Probabilistic ITAG
This section describes probabilistic ITAG (PITAG)
which is utilized by evaluating partial parse trees in
incremental parsing. PITAG assigns a probability
to the event that an elementary tree is combined by
substitution or adjunction with another tree.
We induce the probability by maximum likeli-
hood estimation. Let ? be an initial tree and X be
the root symbol of ?. The probability that ? is sub-
stituted is calculated as follows:
P (s?) = C(s?)?
???I(X) C(s??)
(2)
where C(s?) is the count of the number of times of
applying substitution s? in the treebank, and I(X)
is the set of initial trees whose root is labeled with
X .
1For example, the tree extraction based on head informa-
tion splits the parse tree #18 at the node labeled with dt to ob-
tain the elementary tree [a]dt for ?a?. However, the tree [a]dt
cannot be combined with the partial parse tree for ?I found?,
since substitution node labeled with dt exists in the initial tree
[dt[dime]nn]np for ?dime? and not the partial parse trees for ?I
found?.
Let ? be a auxiliary tree and X be the root symbol
of ?. The probability that ? is adjoined is calculated
as follows:
P (a?) = C(a?)C(X) (3)
where C(X) is the count of the number of occur-
rences of symbol X . The probability that adjunction
is not applied is calculated as follows:
P (nilX) = 1?
?
??A(X)
P (a?) (4)
where nilX means that the adjunction is not applied
to a node labeled with X , and A(X) is the set of all
auxiliary trees whose root is labeled X .
In this PITAG formalism, the probability that el-
ementary trees are combined at each node depends
only on the nonterminal symbol of that node 2.
The probability of a parse tree is calculated by the
product of the probability of the operations which
are used in construction of the parse tree. For ex-
ample, the probability of each operation is given as
shown in Table 2. The probability of the partial
parse tree #12, which is constructed by using s?1 ,
s?2 , s?5 , s?7 , nilNP and a?2 , is 1 ? 0.7 ? 0.3 ?
0.5? 0.7? 0.7 = 0.05145.
We write P (?) for the probability of a partial
parse tree ?.
2The PITAG formalism corresponds to SLG(1) in the liter-
ature (Carroll and Weir, 2003).
Table 2: Probability of operations
operation probability
s?1 1.0
s?2 0.7
s?7 , s?10 0.5
s?5 , s?8 0.3
s?4 , s?6 , s?9 0.2
s?3 0.1
a?1 0.3
a?2 0.7
nilNP 0.7
nilV P 0.3
2.4 Parsing Strategies
In order to improve the efficiency of the parsing, we
adapt two parsing strategies as follows:
? If two partial parse trees have the same se-
quence of nodes to which ITAG operations are
applicable, then the lower probability tree can
be safely discarded.
? The parser only keeps n-best partial parse trees.
3 Validity of Partial Parse Trees
This section gives some definitions about the valid-
ity of a partial parse tree. Before describing the va-
lidity of a partial parse tree, we define the subsump-
tion relation between partial parse trees.
Definition 1 (subsumption relation) Let ? and ?
be partial parse trees. Then we write ? ? ? , if
s?(?) = ? , for some initial tree ? or a?(?) = ? ,
for some auxiliary tree ?. Let ?? be the reflexive
transitive closure of ?. We say that ? subsumes ? ,
if ? ?? ? . 2
That ? subsumes ? means that ? is the result of ap-
plying a substitution or an adjunction to ?. Figure 2
shows the subsumption relation between the partial
parse trees constructed for the sentence (1).
If a partial parse tree for an initial fragment repre-
sents a syntactic relation correctly, the partial parse
tree subsumes the correct parse tree for the input
sentence. We say that such a partial parse tree is
valid. The validity of a partial parse tree is defined
as follows:
Definition 2 (valid partial parse tree) Let ? be a
partial parse tree and w1 ? ? ?wn be an input sen-
tence. We say that ? is valid for w1 ? ? ?wn if ? sub-
sumes the correct parse tree for w1 ? ? ?wn. 2
#1 I #2 #3 #6
#7
found a dime #10 #12 #14 #18
#19#16#13
in the wood
subsumption relation
#4 #8
#9
#11
#15
#17
#5
Figure 2: Subsumption relation between partial
parse trees
#1 I #2 #3 #6
#7
found a dime #10 #12 #14 #18
#19#16#13
in the wood
subsumption relation
#4 #8
#9
#11
#15
#17
valid partial parse tree#5
Figure 3: Valid partial parse trees
For example, assume that the #18 is correct parse
tree for the sentence (1). Then partial parse tree #3
is valid for the sentence (1), because #3 ?? #18. On
the other hand, partial parse tree #4 and #5 are not
valid for (1). Figure 3 shows the valid partial parse
trees for the sentence (1).
4 Evaluating the Validity of Partial Parse
Tree
The validity of a partial parse tree for an initial frag-
ment depends on the rest of the sentence. For ex-
ample, the validity of the partial parse trees #3, #4
and #5 depends on the remaining input that follows
the word ?found.? This means that the validity dy-
namically varies for every word input. We define a
conditional validity of partial parse tree:
V (? | w1 ? ? ?wj) =
?
??Sub(?,w1???wj) P (?)?
??T (w1???wj) P (?)
(5)
where ? is a partial parse tree for an initial frag-
ment w1 ? ? ?wi(i ? j), T (w1 ? ? ?wj) is the set of
constructed partial parse trees for the initial frag-
ment w1 ? ? ?wj and Sub(?,w1 ? ? ?wj) is the subset
of T (w1 ? ? ?wj) whose elements are subsumed by ?.
The equation (5) represents the validity of ? on the
condition w1 ? ? ?wj . ? is valid for input sentence
if and only if some partial parse tree for w1 ? ? ?wj
subsumed by ? is valid. The equation 5 is the ratio
of such partial parse trees to the constructed partial
parse trees.
4.1 Output Partial Parse Trees
Kato et al proposed a method of delaying the deci-
sion of which partial parse trees should be returned
as the output, until the validity of partial parse trees
are guaranteed (Kato et al, 2000). The idea of
delaying the decision of the output is interesting.
However, delaying the decision until the validity are
guaranteed may cause the loss of incrementality of
the parsing.
To solve the problem, in our method, the in-
cremental parser returns high validity partial parse
trees rather than validity guaranteed partial parse
trees.
When the j-th word wj is scanned, our incremen-
tal parser returns the following partial parse:
argmax{?:V (?,w1???wj)??}l(?) (6)
where ? is a threshold between [0, 1] and l(?) is
the length of the initial fragment which is yielded
by ?. The output partial parse tree is the one for
the longest initial fragment in the partial parse trees
whose validity are greater than a threshold ?.
4.2 An Example
Let us consider a parsing example for the sentence
(1). We assume that the threshold ? = 0.8.
Let us consider when the partial parse tree
#3, which is valid for (1), is returned as output.
When the word ?found? is scanned, partial parse
trees #3, #4 and #5 are constructed. That is,
T (I found) = {#3,#4,#5}. As shown in Figure
2, Sub(#3, I found) = {#3}. Furthermore,
P (#3) = 0.7, P (#4) = 0.1 and P (#5) = 0.2.
Therefore, V alidity(#3, I found) =
0.7/(0.7 + 0.1 + 0.2) = 0.7. Because
V alidity(#3, I found) < ?, partial parse tree
#3 is not returned as the output at this point. The
parser only keeps #3 as a candidate partial parse
tree.
When the next word ?a? is scanned, partial parse
trees #6, #7, #8 and #9 are constructed, where
P (#6) = 0.21, P (#7) = 0.14, P (#8) = 0.03 and
P (#9) = 0.02. Sub(#3, I found a) = {#6,#7}.
Therefore, V alidity(#3, I found a) = (0.21 +
0.14)/(0.21+0.14+0.03+0.02) = 0.875. Because
V alidity(#3, I found a) ? ?, partial parse tree #3
is returned as the output.
Table 3 shows the output partial parse tree for ev-
ery word input.
Our incremental parser delays the decision of the
output as shown in this example.
Table 3: Output partial parse trees
input word output partial parse tree
I #2
found
a #3
dime #10
in #12
the
wood #18
5 Experimental Results
To evaluate the performance of our proposed
method, we performed a parsing experiment. The
parser was implemented in GNU Common Lisp on a
Linux PC. In the experiment, the inputs of the incre-
mental parser are POS sequences rather than word
sequences. We used 47247 initial trees and 2931
auxiliary trees for the experiment. The elementary
trees were extracted from the parse trees in sec-
tions 02-21 of the Wall Street Journal in Penn Tree-
bank (Marcus et al, 1993), which is transformed
by using parent-child annotation and left factoring
(Roark and Johnson, 1999). We set the beam-width
at 500.
The labeled precision and recall of the parsing
are 80.8% and 78.5%, respectively for the section
23 in Penn Treebank. We used the set of sentences
for which the outputs of the incremental parser are
identical to the correct parse trees in the Penn Tree-
bank. The number of these sentences is 451. The
average length of these sentences is 13.5 words.
We measured the delays and the precisions for va-
lidity thresholds 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0.
We define the degree of delay as follows: Let
s = w1 ? ? ?wn be an input sentence and oj(s) be
the partial parse tree that is the output when the j-th
word wj is scanned. We define the degree of delay
when j-th word is scanned as follows:
D(j, s) = j ? l(oj(s)) (7)
We define maximum delay Dmax(s) and average
delay Dave(s) as follows:
Dmax(s) = max1?j?nD(j, s) (8)
Dave(s) = 1n
n?
j=1
D(j, s) (9)
The precision is defined as the percentage of valid
partial parse trees in the output.
Moreover, we measured the precision of the pars-
ing whose delay is always 0 and which returns the
Table 4: Precisions and delays
precision(%) Dmax Dave
? = 1.0 100.0 11.9 6.4
? = 0.9 97.3 7.5 2.9
? = 0.8 95.4 6.4 2.2
? = 0.7 92.5 5.5 1.8
? = 0.6 88.4 4.5 1.3
? = 0.5 83.0 3.4 0.9
baseline 73.6 0.0 0.0
0
2
4
6
8
10
12
14
70 75 80 85 90 95 100
delay(number of words)
precision(%)
Dmax
3
33333
3
Dave
?
?????
?
baseline
2
2
Figure 4: Relation between precision and delay
partial parse tree having highest probability. We call
it the parsing baseline.
Table 4 shows the precisions and delays. Figure
4 illustrates the relation between the precisions and
delays.
The experimental result demonstrates that there
is a precision/delay trade-off. Our proposed method
increases the precision in comparison with the base-
line, while returning the output is delayed. When
? = 1, it is guaranteed that the output partial parse
trees are valid, that is, our method is similar to the
method in the literature (Kato et al, 2000). In com-
parison with this case, our method when ? < 1 dra-
matically decreases the delay.
Although the result does not necessarily demon-
strates that our method is the best one, it achieves
both high-accuracy and short-delay to a certain ex-
tent.
6 Concluding Remarks
In this paper, we have proposed a method of evalu-
ating the validity that a partial parse tree constructed
in incremental parsing becomes valid. The method
is based on probabilistic incremental parsing. When
a word is scanned, the method incrementally calcu-
lates the validity for each partial parse tree and re-
turns the partial parse tree whose validity is greater
than a threshold. Our method delays the decision of
which partial parse tree should be returned.
To evaluate the performance of our method, we
conducted a parsing experiment using the Penn
Treebank. The experimental result shows that our
method improves the accuracy of incremental pars-
ing.
The experiment demonstrated a precision/delay
trade-off. To evaluate overall performance of in-
cremental parsing, we would like to investigate a
single measure into which delay and precision are
combined.
Acknowledgement
This work is partially supported by the Grant-in-Aid
for Scientific Research of the Ministry of Education,
Science, Sports and Culture, Japan (No. 15300044),
and The Tatematsu Foundation.
References
J. Allen, G. Ferguson, and A. Stent. 2001. An Ar-
chitecture for More Realistic Conversational Sys-
tems. In Proceedings of International Confer-
ence of Intelligent User Interfaces, pages 1?8.
J. Carroll and D. Weir. 2003. Encoding Frequency
Information in Stochastic Parsing Models. In
R. Bod, R. Scha, and K. Sima?an, editors, Data-
Oriented Parsing, pages 43?60. CSLI Publica-
tions, Stanford.
J. Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proceedings of the 6th International Workshop on
Parsing Technologies, pages 65?76.
D. Chiang. 2003. Statistical Parsing with an Auto-
matically Extracted Tree Adjoining Grammar. In
R. Bod, R. Scha, and K. Sima?an, editors, Data-
Oriented Parsing, pages 299?316. CSLI Publica-
tions, Stanford.
F. Costa, V. Lombardo, P. Frasconi, and Soda G.
2001. Wide Coverage Incremental Parsing by
Learning Attachment Preferences. In Proceed-
ings of the 7th Congress of the Italian Association
for Artificial Intelligence, pages 297?307.
N. J. Haddock. 1987. Incremental Interpretation
and Combinatory Categorial Grammar. In Pro-
ceedings of the 10th International Joint Confer-
ence on Artificial Intelligence, pages 661?663.
Y. Inagaki and S. Matsubara. 1995. Models for In-
cremental Interpretation of Natural Language. In
Proceedings of the 2nd Symposium on Natural
Language Processing, pages 51?60.
A. K. Joshi. 1985. Tree Adjoining Grammar: How
Much Context-Sensitivity is required to provide
reasonable structural descriptions? In D. R.
Dowty, L. Karttunen, and A. Zwicky, editors,
Natural Language Parsing, pages 206?250. Cam-
bridge University Press, Cambridge.
Y. Kato, S. Matsubara, K. Toyama, and Y. Ina-
gaki. 2000. Spoken Language Parsing based on
Incremental Disambiguation. In Proceedings of
the 6th International Conference on Spoken Lan-
guage Processing, volume 2, pages 999?1002.
V. Lombardo and P. Sturt. 1997. Incremental Pro-
cessing and Infinite Local Ambiguity. In Pro-
ceedings of the 19th Annual Conference of the
Cognitive Science Siciety, pages 448?453.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a Large Anno-
tated Corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):310?330.
M Marcus. 1980. A Theory of Syntactic Recog-
nition for Natural Language. MIT Press, Cam-
brige, MA.
S. Matsubara, S. Asai, K. Toyama, and Y. Inagaki.
1997. Chart-based Parsing and Transfer in In-
cremental Spoken Language Translation. In Pro-
ceedings of the 4th Natural Language Processing
Pacific Rim Symposium, pages 521?524.
D. Milward and R. Cooper. 1994. Incremental In-
terpretation: Applications, Theory, and Relation-
ship to Dynamic Semantics. In Proceedings of
the 15th International Conference on Computa-
tional Linguistics, pages 748?754.
D. Milward. 1995. Incremental Interpretation of
Categorial Grammar. In Proceedings of the 7th
Conference of European Chapter of the Associ-
ation for Computational Linguistics, pages 119?
126.
B. Roark and M. Johnson. 1999. Efficient Prob-
abilistic Top-down and Left-corner Parsing. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics, pages
421?428.
B. Roark. 2001. Probabilistic Top-Down Parsing
and Language Modeling. Computational Lin-
guistics, 27(2):249?276.
F. Xia. 1999. Extracting Tree Adjoining Gram-
mars from Bracketed corpora. In Proceedings of
the 5th Natural Language Processing Pacific Rim
Symposium, pages 398?403.
