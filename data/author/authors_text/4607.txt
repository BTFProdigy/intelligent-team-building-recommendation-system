Restoring an Elided Entry Word in a Sentence  
for Encyclopedia QA System  
Soojong Lim 
Speech/Language 
Information Research 
Department 
ETRI, Korea  
isj@etri.re.kr 
Changki Lee 
Speech/Language Informa-
tion Research Department 
ETRI, Korea  
leeck@etri.re.kr 
Myoung-Gil Jang 
Speech/Language Informa-
tion Research Department 
ETRI, Korea  
mgjang@etri.re.kr 
 
 
 
 
 
Abstract 
This paper presents a hybrid model for 
restoring an elided entry word for en-
cyclopedia QA system. In Korean en-
cyclopedia, an entry word is frequently 
omitted in a sentence. If the QA system 
uses a sentence without an entry word, 
it cannot provide a right answer. For 
resolving this problem, we combine a 
rule-based approach with Maximum 
Entropy model to use the merit of each 
approach. A rule-based approach uses 
caseframes and sense classes. The re-
sult shows that combined approach 
gives a 20% increase over our baseline.  
1 Introduction 
Ellipsis is a linguistic phenomenon that peo-
ple omit a word or phrase not to repeat a same 
word or phrase in a sentence or a document. 
Usually, ellipsis involves the use of clauses that 
are not syntactically complete sentences (Allen, 
1995) but the fact does not apply to all cases. An 
ellipsis occurring in encyclopedia documents in 
Korean is an example.  
 
 (Entry word: Kim Daejung) 
 
Korean: ??[gongro]?[ro] 2000?[nyeon]  
????? [nobel pyeonghwasang]?[eul] 
???[batatda].  
English: won the Nobel prize for peace in 
2000 by meritorious deed. 
 
In QA system(Kim et al 2004), it answers a 
question using the predicate-argument relation 
as in the following example.  
 
Korean: 2000?[nyeon]?[e] ????? [no-
belpyeonghwasang ] ?[eul]  ?? [bateun ] 
??[saram]?[eun]? 
English: Who?s the winner of the Nobel prize for 
peace on 2000? 
 
??(subj:??, obj:?????, adv:2000?) 
( batda(subj:saram, obj: nobelpyeonghwasang, 
adv:ichunnyeon) 
win(subj:who, obj:the Nobel prize for peace, 
adv:2000) 
 
Entry word: ??? 
(Entry word: Kim Daejun) 
 
??(subj:NULL(???), obj:?????, 
adv:2000?, ??) 
(batda(subj:NULL(kimdaejung), obj, nobelpyeongh-
wasang, adv: ichunnyeon, gongro) 
win(subj:NULL(Kim Daejung), obj:the Nobel prize 
for peace, adv:2000, deed) 
 
If an entry word of Korean encyclopedia per-
forms a function of a subject or an objects, it is 
frequently omitted in the sentences of the Ko-
rean encyclopedia. If the QA system uses the 
result in the above example, it cannot find who 
won the Nobel prize for peace in the year of 
2000.  We need to restore an entry word as a 
subject or an object to answer a right question.  
In this paper, to overcome this problem, we 
first try to classify entry words in encyclopedia 
into sense classes and determine which sense 
classes are restored to the subjects or the objects. 
Then we use caseframes for determining sense 
215
classes which are not restored using sense 
classes. If there is no caseframes, we use a sta-
tistical method, ME model, for determining 
whether the entry word is restored or not. Be-
cause each approach has both strength and 
weakness, we combine three approaches to 
achieve a better performance.  
2 Related Work 
Ellipsis is a pervasive phenomenon in natural 
languages. While previous work provides im-
portant insight into the abstract syntactic and 
semantic representations that underlie ellipsis 
phenomena, there has been little empirically 
oriented work on ellipsis.  
There are only two similar empirical experi-
ments done for this task. First is Hardt?s algo-
rithm(Hardt, 1997) for detecting VPE in the 
Penn Treebank. It achieves precision levels of 
44% and recall of 53%, giving an F-Measure of 
48% using a simple search technique, which 
relies on the annotation having identified empty 
expressions correctly. Second is Nielsen?s ma-
chine learning techniques(Nielsen, 2003). They 
only try to detect of elliptical verbs using four 
different machine learning techniques,  Trans-
formation-based learning, Maximum entropy 
modeling, Decision Tree Learning, Memory 
Based Learning. It achieves precision levels of 
85.14% and recall of 69.63%, giving an F-
Measure of 76.61%. There are 4 steps: detection, 
identification of antecedents, difficult antece-
dents, resolving antecedents. Because this study 
only concentrates on the detection, a comparison 
with our study is inadequate.  
We combine rule-based techniques with ma-
chine learning technique for using the merit of 
each technique.  
3 Restoring an Elided Entry Word 
We use three kinds of algorithms: A caseframe 
algorithm, an acceptable sense class algorithm, 
and Maximum Entropy (ME) algorithm. For 
knowing a strength and weakness points of each 
algorithm, we do experiments on each algorithm. 
Then we combine algorithms for higher per-
formance.  
Our system answers in three ways: restoring 
an  entry word as a subject, restoring an entry 
word as an object, and does not restore an entry 
word. We evaluate an algorithm in two ways. 
First, we evaluate all answers with precision. 
Second,  we  evaluate just two answers, restor-
ing an entry word as a subject and object, with 
F-measure.  
 
recallprecision
recallprecisionmeasureF
foundwordsentryelidedall
foundwordsentryelidedcorrectprecision
settestinwordsentryelidedall
foundwordsentryelidedcorrectrecall
+
??=?
=
=
2
 
3.1 Using Caseframes 
We use modified caseframes constructed for 
Korean-Chinese machine translation. The format 
of Korean-Chinese machine translation case 
frame is as the following: 
 
A=Sense_code!case_particle verb > Chinese > 
Korean Sentence 
A=??(saram)!?(ga) B=??(jangso)!?(ro) 
?(ga)!?(da) > A 0x53bb:v B [?(geu)[A]?(ga) 
??(bada)[B]?(ro) ??(gada)] 
A=Person!subj B=Location!adv go. 
 
In the caseframe, we only use Sense Class, 
case particle marker, and the verb. The case-
frame used in this research consists of 30,000 
verbs and 153,000 caseframes.  
The sense class used in this research is se-
lected from the nodes of the ETRI Lexical Con-
cept Network for Korean Nouns which consists 
of about 60,000 nodes. (If we include proper 
nouns, the total entry of ETRI Lexical Concept 
Network for Korean Nouns is about 300,000 
nodes).  
First, we analyze a sentence using depend-
ency parser (LIM, 2004), and then we convert a 
result of a parser into the caseframe format. We 
determine to restore an entry word if there is an 
exactly matched caseframe of a target except a 
sense class of an entry word.  
Table 1 shows an example.  
First, we analyze a sentence using depend-
ency parser (LIM, 2004), and then we convert a 
result of a parser into the caseframe format. We 
determine to restore an entry word if there is an 
exactly matched caseframe of a target except a 
sense class of an entry word.  
216
Table 1. An Example of Caserframe Algorithm 
Input Entry word: Along Bay  
Sense: Location 
Sentence: Located in East of Haiphong 
Parsing Locate(subj:NULL, obj:NULL, adv: east
of Haiphong) 
Caseframe of sentence  
direction!e locate 
Matching 24265-2 A=Location!ga B=Location!eseo
C=direction!e 
24265-4 A=Location!ga B=direction!e 
24265-8 A=weather!ga B=direction!e 
24265-12 A=direction!e 
24265-17 A=body!ga B=direction!e 
decision Restoring an entry word as a subject 
 
The result of caseframe algorithm is in table 
2. The result of caseframe algorithm shows that 
it has a high precision but a relatively low recall 
because it is impossible to construct caseframes 
for all sentences.  
 
Table 2. Result of Caseframe Algorithm 
 Subject Object Sum 
Precision 88.16 6.38 56.91 
Recall 59.29 27.28 56.45 
F-measure 70.90 10.34 56.68 
 
3.2 Acceptable Sense Class 
All entry words in the encyclopedia belong to at 
least one sense class. We verify all 444 sense 
classes to see whether they could be restored in 
a sentence.  We set a precision threshold 50% 
and we fix 36 sense classes to ?acceptable sense 
class?. An acceptable sense class is a sense class 
that if an entry word is included in an acceptable 
sense class, we unconditionally restore an entry 
word in a sentence. Our verification tells that 
there is only acceptable sense classes for sub-
jects. Table 3 shows acceptable sense classes. 
 
Table 3. Acceptable Sense Classes 
PERSON, ORGANIZATION, STUDY, WORK, 
LOCATION, ANIMAL, PLANT, ART,  
BUILDING, BUSINESS MATTERS, POSITION,  
SPORTS, CLOTHES, ESTABLISHMENT, 
 PUBLICATION, MEANS of TRANSPORTATION, 
EQUIPMENT, SITUATION, HARDWARE,  
BROADCASTING, HUMAN RACE, EXISTENCE, 
BRANCH, MATERIAL OBJECT, WEAPON,  
EXPLOSIVE, LANGUAGE, FACILITIES,  
ACTION, SYMBOL, TOPOGRAPHY, ROAD,  
ECONOMY, ADVERTISEMENT, EVENT, TOMB
The result of acceptable sense class algo-
rithm is presented in table 4. Because we cannot 
get acceptable sense classes for objects, F-
measure of object is 0.  
 
Table 4.  Result of ASC Algorithm 
 Subject Object Sum 
Precision 58.14 0.0 58.14 
Recall 66.37 0.0 60.48 
F-measure 61.98 0.0 59.29 
 
3.3 Maximum Entropy Modeling 
Maximum entropy modeling uses features, 
which can be complex, to provide a statistical 
model of the observed data which has the high-
est possible entropy, such that no assumptions 
about the data are made. 
 
)(maxarg* pHp =
*p
( pH
Cp?
 
where is the most uniform distribution, C is a set 
of probability distributions under the constraints and 
 is entropy of ) p .  
 
 Ratnaparkhi(Ratnaparkhi 98) makes a strong 
argument for the use of maximum entropy 
modes, and demonstrates their use in a variety of 
NLP tasks.  
The Maximum Entropy Toolkit was used for 
the experiments.1  
Because maximum entropy allows for a wide 
range of features, we can use various features, 
such as lexical feature, POS feature, sense fea-
ture, and syntactic feature. Each feature consists 
of subfeatures: 
 
Lexical feature; 
Verb_lex : lexeme of a target verb  
Verb_e_lex : lexeme of a suffix attatched 
to a target verb 
 
POS feature;  
Verb_pos : pos of a target verb 
Verb_e_pos : pos of a suffix attatch to a 
target verb 
 
Sense feature; 
                                                          
1 Downloadable from  
http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.ht
ml 
217
Ti_res_code: where sense of an entry 
word is included in acceptable sense class 
Verb_cf_subj, obj: whether a sense of en-
try word is included in caseframe of a 
targe verb 
Ti_sense : sense class of entry word 
 
Syntactic feature; 
Tree_posi: position of parse tree 
Rel_type: relation type between verbs in 
a sentence 
Sen_subj, sen_obj : existence of subject 
or object 
 
Hybrid feature; 
 Pair =(sense class of entry word, verb) 
 
 Table 5 shows an example of features that 
we use for finding an elided entry word.  
Previous work using ME model adopted dis-
tance-based context for training. Because we use 
syntactic features, we can use not only distance-
based context but also predicate-argument based 
context. The training data for ME algorithm 
consist of verbs in the encyclopedia document 
and their syntactic arguments. Each verb-
arguments set is augmented with the information 
that signifies whether a subject, an object or nei-
ther of them should be restored. For training, we 
use a dependency parser[Lim, 2004]. A preci-
sion of this parser is about 75%. The results of 
ME model algorithm is shown in table 6. The 
results of ME model shows that its score is the 
lowest of all. We guess the reason is that there is 
not enough training data for covering all sense 
classes.  
 
 
Table 5. An Example of Features 
Entry word, 
Sentence 
!TI Cirsotrema perplexam 
!SENSE Animal 
!VERB live 
!SENT lives in a tidal zone 
Lexical 
feature 
verb_lex=??(salda) verb_e_lex=?
(myeo) 
POS feature verb_pos=4 verb_e_pos=24 
Sense fea-
ture 
ti_res_code=1 verb_cf_subj=1  
verb_cf_obj=0 ti_sense=Animal 
Syntactic 
feature 
tree_posi=high rel_type=-1 sen_subj=
0 sen_obj=0 
Hybrid fea-
ture 
pair=(Animal, live) 
 
Table 6. Result of ME Model 
 Subject Object Sum 
Precision 62.50 40.0 60.87 
Recall 35.40 18.18 33.87 
F-measure 45.20 25.00 43.52 
 
3.4 Combining Algorithms 
Different algorithms have different characteris-
tics. For example, the acceptable sense class 
algorithm has relatively high recall but low pre-
cision, while the opposite holds true for the 
caseframe algorithm,  we need to combine algo-
rithms for maximizing advantages of each algo-
rithm. 
First, we combine the acceptable sense class 
algorithm with the ME model. We process the 
problem using the sense class algorithm. Instead 
of applying the algorithm exactly, we use the 
ME model for helping the acceptable sense class 
algorithm. If the acceptable sense class algo-
rithm determines a restoration, we adopt the 
case to ME model. Then if the score of ME 
model is over the negative threshold, we deter-
mine not to restore an entry word.  
Second, we combine the caseframe algorithm 
with the ME model. We process the cases not 
resolved in the first processing time using the 
caseframe algorithm. We try to match case-
frames exactly to sentence with an entry word 
sense code. If we cannot find the exactly match-
ing caseframe, we try matching caseframes par-
tially. In this case, a precision is maybe lower 
than an exact match, we also use the ME model 
for reliability. If the score of ME model is over 
the positive threshold, we determine to restore 
an entry word.  
4 Result and Conclusion 
For ME model, we made a training set 
manually. The training set consists of 2895 sen-
tences: 916 sentences for restoring an entry 
word as a subject, 232 sentences for restoring an 
entry word as an object, 1756 sentences for not 
restoring any. For a test, we randomly selected 
277 sentences.  
We did 6 kinds of experiments. Using Case-
frame algorithm(CF), Acceptable sense class 
algorithm(ASC), ME model(ME) and combine 
ASC with CF(ASC_CF), ASC with ME 
218
(ASC_ME), and ASC with CF and 
ME(ASC_CF_ME). 
 
Table 7. Result of Combined Algorithm 
 Recall Precision F-measure
baseline 100.00 31.64 48.07 
ASC_CF_ME 78.23 60.25 68.07 
ASC_CF 68.55 50.00 57.82 
ASC_ ME 79.03 59.39 67.82 
 
The performance of the methods is calculated 
using recall, precision and F-measure.  
Table 7 and Figure 1 show the performance 
of each experiment.  
Our proposed approach (ASC_CF_ME) 
gives the best results among all experiments, 
with an F-measure of 68.1%, followed closely 
by ASC_ME. This gives a 20% increase over 
our baseline. For testing a portability of our ap-
proach, we experiment the noun phrase ellipsis 
(NPE) detection. The performance of NPE is 
alike an elided entry word. Recall is 69.31, Pre-
cision is 65.05, and F-measure is 67.12. So we 
expect the performance of our approach not to 
drop when applied to NPE or other ellipsis prob-
lem. The results so far are encouraging, and 
show that the approach taken is capable of pro-
ducing a robust and accurate system.  
In this paper, we suggested the approach that 
restores an elided entry word for Encyclopedia 
QA systems combining an acceptable sense 
class algorithm, a caseframe algorithm, and ME 
model.  
For future work, we plan to pursue the fol-
lowing research. First, we will use various ma-
chine learning methods and compare them with 
the ME model. Second, because we plan to ap-
ply this approach in the encyclopedia document, 
we need to design the more general approach to 
use other ellipsis phenomenon. Third, we try to 
find a method for enhancing performance of 
restoring elided entry words as the object.  
References 
James Allen. 1995. Natural Language Under-
standing, Benjamin/Cummings Publishing 
Company, 449~455 
Leif Arda Nielsen. 2003. Using Machine Learn-
ing Techniques for VPE detection, RANLP 03, 
Bulgaria. 
Daniel Hardt. 1997. An empirical approach to 
vp ellipsis, Computational Linguistics, 23(4).  
0
10
20
30
40
50
60
CF AS
C ME
AS
C_C
F
AS
C_M
E
AS
C_C
F_M
E
70
80
90
Precision
Recall
F-Measure
 
Figure 1. Comparison of All Results 
 
Adwait Ratnaparkhi. 1998. Maximum Entropy 
Models for Natural LANGUAGE Ambiguity 
Resolution, Unpublished PhDthesis, University 
of Pennsylvania.  
Lim soojong. 2004. Dependency Relation 
Analysis Using Caseframe for Encyclopedia 
Question-Answering Systems, IECON, Korea. 
H. J. Kim, H. J. Oh, C. H. Lee., et al 2004.  The 
3-step Answer Processing Method for Encyclo-
pedia Question-Answering System: AnyQues-
tion 1.0. The Proceedings of Asia Information 
Retrieval Symposium (AIRS) 309-312 
 
219
MMR-based feature selection for text categorization 
 
Changki Lee 
Dept. of Computer Science and Engineering
Pohang University of Science & Technology
San 31, Hyoja-Dong, Pohang,  
790-784, South Korea 
phone: +82-54-279-5581   
leeck@postech.ac.kr
Gary Geunbae Lee 
Dept. of Computer Science and Engineering
Pohang University of Science & Technology
San 31, Hyoja-Dong, Pohang,  
790-784, South Korea 
phone: +82-54-279-5581   
gblee@postech.ac.kr
 
Abstract 
We introduce a new method of feature selec-
tion for text categorization. Our MMR-based 
feature selection method strives to reduce re-
dundancy between features while maintaining 
information gain in selecting appropriate fea-
tures for text categorization. Empirical results 
show that MMR-based feature selection is 
more effective than Koller & Sahami?s 
method, which is one of greedy feature selec-
tion methods, and conventional information 
gain which is commonly used in feature selec-
tion for text categorization. Moreover, MMR-
based feature selection sometimes produces 
some improvements of conventional machine 
learning algorithms over SVM which is 
known to give the best classification accuracy. 
1  Introduction 
Text categorization is the problem of automatically as-
signing predefined categories to free text documents. A 
growing number of statistical classification methods and 
machine learning techniques have been applied to text 
categorization in recent years [9]. 
A major characteristic, or difficulty, of text catego-
rization problems is the high dimensionality of the fea-
ture space [10]. The native feature space consists of the 
unique terms that occur in documents, which can be tens 
or hundreds of thousands of terms for even a moderate-
sized text collection. This is prohibitively high for many 
machine learning algorithms. If we reduce the set of 
features considered by the algorithm, we can serve two 
purposes. We can considerably decrease the running 
time of the learning algorithm, and we can increase the 
accuracy of the resulting model. In this line, a number 
of researches have recently addressed the issue of fea-
ture subset selection [2][4][8]. Yang and Pederson 
found information gain (IG) and chi-square test (CHI) 
most effective in aggressive term removal without los-
ing categorization accuracy in their experiments [8]. 
Another major characteristic of text categorization 
problems is the high level of feature redundancy [11]. 
While there are generally many different features rele-
vant to classification task, often several such cues occur 
in one document. These cues are partly redundant. Na-
?ve Bayes, which is a popular learning algorithm, is 
commonly justified using assumptions of conditional 
independence or linked dependence [12]. However, the-
ses assumptions are generally accepted to be false for 
text. To remove these violations, more complex de-
pendence models have been developed [13]. 
Most previous works of feature selection empha-
sized only the reduction of high dimensionality of the 
feature space [2][4][8]. The most popular feature selec-
tion method is IG. IG works well with texts and has 
often been used. IG looks at each feature in isolation 
and measures how important it is for the prediction of 
the correct class label. In cases where all features are 
not redundant with each other, IG is very appropriate. 
But in cases where many features are highly redundant 
with each other, we must utilize other means, for exam-
ple, more complex dependence models. 
In this paper, for the high dimensionality of the fea-
ture space and the high level of feature redundancy, we 
propose a new feature selection method which selects 
each feature according to a combined criterion of infor-
mation gain and novelty of information. The latter 
measures the degree of dissimilarity between the feature 
being considered and previously selected features. 
Maximal Marginal Relevance (MMR) provides pre-
cisely such functionality [5]. So we propose MMR-
based feature selection method which strives to reduce 
redundancy between features while maintaining infor-
mation gain in selecting appropriate features for text 
categorization. 
In machine learning field, some greedy methods that 
add or subtract a single feature at a time have been de-
veloped for feature selection [3][14]. S. Della Pietra et 
al. proposed a method for incrementally constructing 
random field [14]. Their method builds increasingly 
complex fields to approximate the empirical distribution 
of a set of training examples by allowing features. Fea-
tures are incrementally added to the field using a top-
down greedy algorithm, with the intent of capturing the 
salient properties of the empirical sample while allow-
ing generalization to new configurations. However the 
method is not simple, and this is problematic both com-
putationally and statistically in large-scale problems. 
Koller and Sahami proposed another greedy feature 
selection method which provides a mechanism for 
eliminating features whose predictive information with 
respect to the class is subsumed by the other features [3]. 
This method is also based on the Kullback-Leibler di-
vergence to minimize the amount of predictive informa-
tion lost during feature elimination. 
In order to compare the performances of our method 
and greedy feature selection methods, we implemented 
Koller and Sahami?s method, and empirically tested it in 
section 4. 
We also compared the performance of conventional 
machine learning algorithms using our feature selection 
method with Support Vector Machine (SVM) using all 
features in section 4. Previous works show that SVM 
consistently achieves good performance on text catego-
rization tasks, outperforming existing methods substan-
tially and significantly [10][11]. With its ability to 
generalize well in high dimensional feature spaces and 
high level of feature redundancy, SVM is known that it 
does not need any feature selection [11]. 
The remainder of this paper is organized as follows. 
In section 2, we describe the Maximal Marginal Rele-
vance, and in section 3, we describe the MMR-based 
feature selection. Section 4 presents the in-depth ex-
periments and the results. Section 5 concludes the re-
search. 
2 Maximal Marginal Relevance 
Most modern IR search engines produce a ranked list of 
retrieved documents ordered by declining relevance to 
the user's query. In contrast, the need for ?relevant nov-
elty? was motivated as a potentially superior criterion. A 
first approximation to relevant novelty is to measure the 
relevance and the novelty independently and provide a 
linear combination as the metric. 
The linear combination is called ?marginal rele-
vance? - i.e. a document has high marginal relevance if 
it is both relevant to the query and contains minimal 
similarity to previously selected documents. In docu-
ment retrieval and summarization, marginal relevance is 
strived to maximize, hence the method is labeled 
?Maximal Marginal Relevance? (MMR) [5]. 
?
?
?
?
?
?
???
=
??
),(max)1(),(max 21\ jiSDiSRD DDSimQDSimArg
MMR
ji
??
 
where C={D1,?,Di,?} is a document collection (or 
document stream); Q is a query or user profile; R = 
IR(C, Q, ? ), i.e., the ranked list of documents retrieved 
by an IR system, given C and Q and a relevance thresh-
old ? , below which it will not retrieve documents (?   
can be degree of match or number of documents); S is 
the subset of documents in R which is already selected; 
R\S is the set difference, i.e. the set of as yet unselected 
documents in R; Sim1 is the similarity metric used in 
document retrieval and relevance ranking between 
documents (passages) and a query; and Sim2 can be the 
same as Sim1 or a different metric. 
3  MMR-based Feature Selection 
We propose a MMR-based feature selection which 
selects each feature according to a combined criterion of 
information gain and novelty of information. We define 
MMR-based feature selection as follows: 
?
?
?
?
?
?
???
=
??
)|;(max)1();(max
_
\
CwwIGpairCwIGArg
FSMMR
jiSwiSRw ji
??
  
where C is the set of class labels, R is the set of candi-
date features, S is the subset of features in R which was 
already selected, R\S is the set difference, i.e. the set of 
as yet unselected features in R, IG is the information 
gain scores, and IGpair is the information gain scores of 
co-occurrence of the word (feature) pairs. IG and IGpair 
are defined as follows: 
?
?
?
+
+
?=
k
ikiki
k
ikiki
k
kki
wCpwCpwp
wCpwCpwp
CpCpCwIG
)|(log)|()(                 
)|(log)|()(                 
)(log)();(
 
?
?
?
+
+
?=
k
jikjikji
k
jikjikji
k
kkji
wCpwCpwp
wCpwCpwp
CpCpCwwIGpair
)|(log)|()(                
)|(log)|()(                
)(log)()|;(
,,,
,,,
 
where p(wi) is the probability that word wi occurred, iw  
means that word wi doesn?t occur, p(Ck) is the probabil-
ity of the k-th class value, p(Ck|wi) is the conditional 
probability of the k-th class value given that wi occurred, 
p(wi,j) is the probability that wi and wj co-occurred, and 
iw  means that wi and wj doesn?t co-occur but wi or wj 
can occur (i.e. )(1)( ,, jiji wpwp ?= ). 
Given the above definition, MMR_FS computes in-
crementally the information gain scores when the 
parameter ? =1, and computes a maximal diversity 
among the features in R when ? =0. For intermediate 
values of ?  in the interval [0,1], a linear combination of 
both criteria is optimized. 
4 Experiments 
In order to compare the performance of MMR-based 
feature selection method with conventional IG and 
greedy feature selection method (Koller & Sahami?s 
method, labeled ?Greedy?), we evaluated the three fea-
ture selection methods with four different learning algo-
rithms: naive Bayes, TFIDF/Rocchio, Probabilistic 
Indexing (PrTFIDF [7]) and Maximum Entropy using 
Rainbow [6]. 
We also compared the performance of conventional 
machine learning algorithms using our feature selection 
method and SVM using all features. 
MMR-based feature selection and greedy feature se-
lection method (Koller & Sahami?s method) requires 
quadratic time with respect to the number of features. 
To reduce this complexity, for each data set, we first 
selected 1000 features using IG, and then we applied 
MMR-based feature selection and greedy feature selec-
tion method to the selected 1000 features. 
For all datasets, we did not remove stopwords. The 
results reported on all dataset are averaged over 10 
times of different test/training splits. A random subset 
of 20% of the data considered in an experiment was 
used for testing (i.e. we used Rainbow?s ?--test-set=0.2? 
and ?--test=10? options), because Rainbow does not 
support 10-fold cross validation. 
MMR-based feature selection method needs to tune 
for ? . It appears that a tuning method based on held-out 
data is needed here. We tested our method using 11 ?  
values (i.e. 0, 0.1, 0.2, ?, 1) and selected the best ?  
value. 
4.1 Reuters-21578 
The Reuters-21578 corpus contains 21578 articles taken 
from the Reuters newswire. Each article is typically 
designated into one or more semantic categories such as 
?earn?, ?trade?, ?corn? etc., where the total number of 
categories is 114. 
Following [3], we constructed a subset from Reuter 
corpus. The subset is comprised of articles on the topic 
?coffee?, ?iron-steel?, and ?livestock?.  
4.2 WebKB 
This data set contains WWW-pages collected from 
computer science departments of various universities in 
January 1997 by the World Wide Knowledge Base 
(WebKb) project of the CMU text learning group. The 
8282 pages were manually classified into 7 categories: 
?course?, ?department?, ?faculty?, ?project?, ?staff?, ?stu-
dent? and ?other?. Following [1], we discarded the cate-
gories ?other?, ?department? and ?staff?. The remaining 
part of the corpus contains 4199 documents in four 
categories. 
4.3 Experimental Results 
 
Figure 1 displays the performance curves for four dif-
ferent machine learning algorithms on the subset of 
Reuters after term selection using MMR-based feature 
selection (number of features is 25). When the parame-
ter ? =0.5, most machine learning algorithms have best 
performance and significant improvements compared to 
conventional information gain (i.e. ? =1) and SVM us-
ing all features. 
 
Table 1. WebKB. 
 
 
Table 1 shows the performance of four machine 
learning algorithms on WebKB using three feature se-
lection methods and all features (41763 terms). In this 
data set, again MMR-based feature selection has best 
performance and significant improvements compared to 
greedy method and IG. Using MMR-based feature se-
lection, for example, the vocabulary is reduced from 
41763 terms to 200 (a 99.5% reduction), and the accu-
racy is improved from 85.26% to 90.49% in Na?ve 
Bayes. Using greedy method and IG, however, the accu-
racy is improved from 85.26% to about 87% in Na?ve 
Figure 1. MMR feature selection for four machine 
learning algorithms on Reuters (#features=25).
Bayes. PrTFIDF is most sensitive to feature selection 
method. Using MMR-based feature selection the best 
accuracy is 82.47%. Using greedy method and IG, how-
ever, the best accuracy is only 72~74%. In this dataset, 
however, MMR-based feature selection does not pro-
duce improvements of conventional machine learning 
algorithms over SVM. 
The observation in Reuters and WebKB are highly 
consistent. MMR-based feature selection is consistently 
more effective than greedy method and IG on two data 
sets, and sometimes produces improvements even over 
the best SVM. 
5 Conclusion 
In this paper, we proposed a MMR-based feature selec-
tion method which strives to reduce redundancy be-
tween features while maintaining information gain in 
selecting appropriate features for text categorization. 
We carried out extensive experiments to verify the 
proposed method. Based on the experiment results, we 
can verify that MMR-based feature selection is more 
effective than Koller & Sahami?s method, which is one 
kind of greedy methods, and conventional information 
gain which is commonly used in feature selection for 
text categorization. Besides, MMR-based feature selec-
tion method sometimes produces improvements of con-
ventional machine learning algorithms over SVM which 
is known to give the best classification accuracy. 
A disadvantage in using MMR-based feature selection 
is that the computational cost of computing the pairwise 
information gain (i.e. IGpair) is quadratic time with 
respect to the number of features. To reduce this compu-
tational cost, we can use MMR-based feature selection 
method on the reduced feature set resulting from IG as 
our experiments in section 4. Another drawback of our 
method is the need to tune for ? . It appears that a tun-
ing method based on held-out data is needed here 
References 
[1] Andrew Mccallum and Kamal Nigam. 1998.  A 
Comparison of Event Models for Naive Bayes Text 
Classification. In AAAI-98 Workshop on Learning 
for Text Categorization. 
[2] David D. Lewis and Marc Ringuette. 1994.  A Com-
parison of Two Learning Algorithms for Text Cate-
gorization. In Proceedings of SDAIR-94, 3rd Annual 
Symposium on Document Analysis and Information 
Retrieval. 
[3] Daphne Koller and Mehran Sahami. 1996.  Toward 
Optimal Feature Selection. In Proceedings of ICML-
96, 13th International Conference on Machine Learn-
ing. 
[4] Hinrich Sch?tze and David A. Hull, and Jan O. 
Pedersen. 1995.  A Comparison of Classifiers and 
Document Representations for the Routing Problem. 
In Proceedings of the 18th Annual International 
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval. 
[5] Jaime Carbonell and Jade Goldstein. 1998.  The Use 
of MMR, Diversity-Based Reranking for Reordering 
Documents and Producing Summaries. In Proceed-
ings of the 21st ACM-SIGIR International Confer-
ence on Research and Development in Information 
Retrieval. 
[6] McCallum and Andrew Kachites. 1996.  Bow: A 
toolkit for statistical language modelling, text re-
trieval, classification and clustering. 
http://www.cs.cmu.edu/~mccallum/bow. 
[7] Thorsten Joachims. 1997.  A probabilistic analysis 
of the Rocchio algorithm with TFIDF for text catego-
rization. In Proceedings of ICML-97, 14th Interna-
tional Conference on Machine Learning. 
[8] Yiming Yang and Jan O. Pedersen. 1997.  A Com-
parative Study on Feature Selection in Text Catego-
rization. In Proceedings of ICML-97, 14th 
International Conference on Machine Learning. 
[9] Yiming Yang and Xin Liu. 1999.  A re-examination 
of text categorization methods. In Proceedings of the 
22nd ACM-SIGIR International Conference on Re-
search and Development in Information Retrieval. 
[10] Thorsten Joachims. 1998.  Text Categorization with 
Support Vector Machines: Learning with Many Rele-
vant Features. In Proceedings of ECML-98, 10th 
European Conference on Machine Learning. 
[11] Thorsten Joachims. 2001.  A Statistical Learning 
Model of Text Classification for Support Vector Ma-
chines. In Proceedings of the 24th ACM-SIGIR In-
ternational Conference on Research and 
Development in Information Retrieval. 
[12] William S. Cooper. 1991.  Some Inconsistencies 
and Misnomers in Probabilistic Information Re-
trieval. In Proceedings of the 14th ACM SIGIR In-
ternational Conference on Research and 
Development in Information Retrieval. 
[13] Mehran Sahami. 1998.  Using Machine Learning to 
Improve Information Access. PhD thesis, Stanford 
University. 
[14] Stephen Della Pietra, Vincent Della Pietra, and 
John Lafferty. 1997.  Inducing Features of Random 
Fields. IEEE Transactions on Pattern Analysis and 
Machine Intelligence. 
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 875?879,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Balanced Korean Word Spacing with Structural SVM 
Changki Lee*                  Edward Choi                  Hyunki Kim *Kangwon National University, Chuncheon-si, Gangwondo, 200-701, Korea Electronics and Telecommunications Research Institute, Daejeon, 305-350, Korea leeck@kangwon.ac.kr             mp2893@gmail.com                 hkk@etri.re.kr  Abstract 
Most studies on statistical Korean word spac-ing do not utilize the information provided by the input sentence and assume that it was completely concatenated. This makes the word spacer ignore the correct spaced parts of the input sentence and erroneously alter them. To overcome such limit, this paper proposes a structural SVM-based Korean word spacing method that can utilize the space information of the input sentence. The experiment on sen-tences with 10% spacing errors showed that our method achieved 96.81% F-score, while the basic structural SVM method only achieved 92.53% F-score. The more the input sentence was correctly spaced, the more accu-rately our method performed. 1 Introduction Automatic word spacing is a task to decide boundaries between words, which is frequently used for correcting spacing errors of text mes-sages, Tweets, or Internet comments before using them in information retrieval applications (Lee and Kim, 2012). It is also often used in post-processing optical character recognition (OCR) or voice recognition (Lee et al., 2007). Except for some Asian languages such as Chinese, Japa-nese and Thai, most languages have explicit word spacing that improves the readability of the text and helps readers better understand the meaning of it. Korean especially has a tricky word spacing system and users often make mis-takes, which makes automatic word spacing an interesting and essential task. In order to easily acquire the training data, most studies on statistical Korean word spacing assume that well-spaced raw text (e.g. newspaper articles) is perfectly spaced and use it for training (Lee and Kim, 2012; Lee and Kim, 2013; Lee et al., 2007; Shim, 2011). This approach, however, cannot observe incorrect spacing since the as-sumption makes the training data devoid of nega-tive example. Consequently, word spacers cannot use the spacing information given by the user, and erroneously alter the correctly spaced parts 
of the sentence. To utilize the user-given spacing information, a corpus of input sentences and their correctly spaced version is necessary. Construct-ing such corpus, however, requires much time and resource. In this paper, to resolve such issue, we propose a structural SVM-based Korean word spacing model that can utilize the word spacing infor-mation given by the user. We name the proposed model ?Balanced Word Spacing Model (BWSM)?. Our approach trains a basic structural SVM-based Korean word spacing model as in (Lee and Kim, 2013), and tries to obtain the sen-tence which achieves the maximum score for the basic model while minimally altering the input sentence. In the following section, we discuss related studies. In Section 3, the proposed method and its relation to Karush-Kuhn-Tucker (KKT) con-dition are explained. The experiment and discus-sion is presented in Section 4. Finally, in Section 5, the conclusion and future work for this study is given. 2 Related Work There are two common approaches to Korean word spacing: rule-based approach and statistical approach. In rule-based approach, it is not easy to construct rules and maintain them. Further-more, it requires morphological analysis to apply rule-based approach, which slows down the pro-cess. Recent studies, therefore, mostly focus on the statistical approach.  Most statistical approaches use well-spaced raw corpus as training data (e.g. newspaper arti-cles) assuming that they are perfectly spaced. This is to avoid the expensive job of constructing new training data. Lee et al. (2007) treated the word spacing task as a sequence labeling prob-lem on the input sentence which is a sequence of syllables. They proposed a method based on Hidden Markov Model (HMM). Shim (2011) also considered the word spacing task as a se-quence labeling problem and proposed a method using Conditional Random Field (CRF) (Lafferty et al., 2001), which is a well-known powerful model for sequence labeling tasks. Lee and Kim 
875
(2013) tried to solve the sequence labeling prob-lem using structural SVM (Tsochantaridis et al., 2004; Joachims et al., 2009; Lee and Jang 2010; Shalev-Shwartz et al., 2011). The studies above (Lee and Kim, 2013; Lee et al., 2007; Shim, 2011), however, do not take ad-vantage of the spacing information provided by the user, and often erroneously alter the correctly spaced part of the sentence. Lee et al. (2007) tries to resolve this issue by combining an HMM model with an additional confidence model con-structed from another corpus. Given an input sentence, they first apply the basic HMM model to obtain a candidate sentence. For every differ-ent word spacing between the input sentence and the candidate sentence, they calculate and com-pare the confidence using the confidence model, and whichever gets the higher confidence is used. The spacing accuracy was improved from 97.52% to 97.64%1.  This study is similar to (Lee et al., 2007) in that it utilizes the spacing information given by the user. But unlike (Lee et al., 2007), BWSM uses structural SVM as the basic model and do not require an additional confidence model. Fur-thermore, while Lee et al. (2007) compares the spacing confidence for each syllable to obtain the final outcome, BWSM considers the whole sen-tence when altering its spacing, enabling it to achieve higher improvement on performance (from 92.53% F-score to 96.81% F-score). 3 Balanced Word Spacing Model Like previous studies, the proposed model treats the Korean word spacing task as a sequence la-beling problem. The label consists of B and I, which are assigned to each syllable of the sen-tence. Assuming that x = <x1, x2, ?, xT> is a se-quence of total T syllables of the input sentence and y = <y1, y2, ?, yT> is a sequence of labels for each syllable, an example could be given as follows2:   Input: ah/beo/ji/ga   bang/eh   deul/eo/ga/sin/da (Father entered the room) x = <ah, beo, ji, ga, bang, eh, deul, eo, ga, sin, da> y = <B,    I,    I,   I,    B,     I,     B,    I,   I,    I,    I> Figure 1: An example of word spacing.  In order to utilize the spacing information pro-vided by the user, we propose a new model, the 
                                                1 Accuracy was calculated based on syllables. 2 Slashes are used for distinguishing between syllables. 
Balanced Word Spacing Model that adheres to the following principles:  1. The model must obtain the most likely se-quence of labels(y*), while minimally altering the user-given sequence of labels (????? ). 2. We assume that it costs ? per syllable to change the spacing of the original sentence, in order to keep the original spacing information as much as possible.  Mathematically formulating the above princi-ples would give us the following equation:  ?? = argmax ????? ?, ? ? ? ? ? ????? , ?          (1)  In Equation 1, ????? ?, ?  calculates how com-patible the sequence of label y is with the input sentence x. It is calculated by a basic word spac-ing model as in (Lee and Kim, 2013). ? ????? , ?  counts the number of different la-bels between the user-given sequence of labels ?????  and an arbitrary sequence of labels ?. ?? of Equation 1 can be obtained by setting the gra-dient of ????? ?, ? ? ? ? ? ????? , ?  to 0, which is equivalent to the following equation:  ?????? ?, ?? =  ?? ? ?? ????? , ??                         (2)  In order to view the proposed model in a dif-ferent perspective, we consider BWSM in terms of Karush-Kuhn-Tucker (KKT) condition. KKT condition is a technique for solving optimization problems with inequality constraints. It is a gen-eralized version of Lagrange multipliers, which is a technique for solving optimization problems with equality constraints. Converting the afore-mentioned principles to a constrained optimiza-tion problem gives:  Maximize: ?????? ?, ?   subject ?to ?? ????? , ? ? ?                                      (3)  Equation 3 tries to obtain y that maximizes ????? ?, ? , namely the score of the basic model, while maintaining ? ????? , ?  below b, which is equivalent to altering the word spacing of the input sentence less than or equal to b times. To solve this constrained optimization problem, we apply KKT condition and define a new Lagran-gian function as follows:  ? ?, ?, ? = ????? ?, ? ? ? ? ?? ? ? , ? ? ?       (4)  
876
Setting the gradient of the Equation 4 to zero, namely ?? ?, ?, ? = 0 , we get the following necessary conditions:  Stationarity: ??????? ?, ?? = ?? ?? ????? , ??   Primal feasibility: ? ????? , ?? ? ?  Dual ?feasibility: ??? ? 0  Complementary ?slackness: ??? ? ????? , ?? ? ? = 0   (5)  Comparing Equation 1 with Equation 4 reveals that they are the same except the constant b. And ?? which satisfies the conditions of Equation 5, and hence the solution to Equation 4, is also the same as ?? which satisfies Equation 2, and hence the solution to Equation 1. For the basic word spacing model, we use margin rescaled version of structural SVM as Lee and Kim (2013). The objective function of structural SVM is as follows:  min?,? ?? ? ? ?? + ?? ???? , ? ? ??. ?. ? ???, ?? ? 0  ??, ?? ? Y\??:???? ?? , ? ? ? ?? , ? ? ??  where ??? ?? , ? = ? ?? , ?? ? ? ?? , ?                  (6)  In Equation 6, ?? , ??  represents the i-th se-quence of syllables and its correct spacing labels. ? ?? , ?  is a loss function that counts the number of different labels between the correct labels ?? and the predicted sequence of labels ?. ? ?, ?  is a typical feature vector function. The features used for the basic word spacing model are the same features used in (Lee and Kim, 2013). Since structural SVM was used for the basic word spacing model, the score function of Equa-tion 1 becomes ????? ?, ? = ??? ?, ? . We propose two approaches for implementing Equation 1.   1. N-best re-ranking: N-best sequences of spac-ing labels are obtained using the basic struc-tural SVM model. For each of the sequence, ??? ????? , ??  is calculated and subtracted from ????? ?, ? . The result of the subtrac-tion is used to re-rank the sequences, and the one with the highest rank is chosen. 2. Modified Viterbi search: Viterbi search algo-rithm, which is used in the basic word spacing model to solve ?? = argmax ??? ?, ? , is modified to solve ?? = argmax ??? ?, ? ?? ? ? ????? , ? . Both ? ?, ?  and ? ????? , ?  can be calculated syllable by syl-lable, which makes it easy to modify Viterbi search algorithm.  
The first approach seems straightforward and easy, but it would take a long time to obtain N-best sequences of labels. Furthermore, the correct label sequence might not be in those N-best se-quences, hence degrading the overall perfor-mance. The second approach is fast since it does not calculate N-best sequences, and unlike the first approach, will always consider the correct label sequence as a candidate. 4 Experiment In order to compare the performance of BWSM with HMM-based Korean word spacing and structural SVM-based Korean word spacing, we use Sejong raw corpus (Kang and Kim, 2004) as train data and ETRI POS tagging corpus as test data3. Pegasos-struct algorithm from (Lee and Kim, 2013) was used to train the basic structural SVM-based model. The optimal value for the tradeoff variable C of structural SVM was found after conducting several experiments4.  The rate of word spacing error varies depend-ing on the corpus. Newspaper articles rarely have word spacing errors but text messages or Tweets frequently contain word spacing errors. To re-flect such variety, we randomly insert spacing errors into the test set to produce various test sets with spacing error rate 0%, 10%, 20%, 35%, 50%, 60%, and 70%5.  
 Figure 2: Word-based F-score of N-best re-ranking approach.  Figure 2 shows the relation between ?(x-axis) and word-based F-score6(y-axis) of N-best re-                                                3 The number of words for the training set and test set are 26 million and 290,000 respectively. 4 We experimented with 10, 100, 1000, 10000, 100000 and 1000000, the optimal value being 100000. 5 We altered the input to the system and retained the origi-nal gold standard?s space unit. 6 Word-based F-score = 2*Precword*Recallword / (Precword + Recallword), Precword = (# of correctly spaced words) / (the total number of words produced by the system), 
877
ranking approach using test sets with different spacing error rate. When ? = 0 , BWSM be-comes a normal structural SVM-based model. As ? increases, F-score also increases for a while but decreases afterward. And F-score increases more when using test sets with low error rate. It is worth noticing that when using the test set with 0% error rate, as ? increases, F-score con-verges to 98%. The reason it does not reach 100% is that the correct label sequence is sometimes not included in the N-best sequences.  
 Figure 3: Word-based F-score of modified Viterbi search.  Figure 3 shows the relation between ?(x-axis) and word-based F-score(y-axis) of modified Viterbi search approach using test sets with dif-ferent spacing error rate. The graphs are similar to Figure 2, but F-score reaches higher values compared to N-best re-ranking approach. Notice that, when using the test set with 0% error rate, F-score becomes 100% as ? surpasses 3. This is because, unlike N-best re-ranking approach, modified Viterbi search approach considers all possible sequences as candidates.  From Figure 2 and 3, it can be seen that BWSM, which takes into consideration the spac-ing information provided by the user, can im-prove performance significantly. It is also appar-ent that modified Viterbi search approach outper-forms N-best re-ranking approach. The optimal value for ? varies as test sets with different error rate are used. It is natural that, for test sets with low error rate, the optimal value of ? increases, thus forcing the model to more utilize the user-given spacing information. It is difficult to auto-matically obtain the optimal ? for an arbitrary input sentence. Therefore we set ? to 1, which, according to Figure 3, is more or less the optimal value for most of the test sets.                                                                         Recallword = (# of correctly spaced words) / (the total num-ber of words in the test data) 
Model Syllable based precision Word  based precision HMM (Lee et al., 2007) S-SVM (Lee and Kim, 2013) 98.44 99.01 90.31 92.53 Modified Viterbi (error rate 10%) Modified Viterbi (error rate 20%) Modified Viterbi (error rate 35%) 99.64 99.55 99.35 96.81 96.21 95.01 Table 1: Precision of BWSM and previous  studies  With ? set to 1, and using modified Viterbi search algorithm, the performance of BWSM is shown in Table 1 with other previous studies (Lee and Kim, 2013; Lee et al., 2007). Table 1 shows that BWSM gives superior performance than other studies that do not utilize user-given spacing information.  
 Figure 4: Word-based F-score of modified Viterbi search on Tweets.  We also collected Tweets from Twitter and tested modified Viterbi algorithm on them. Fig-ure 4 shows the relation between ? (x-axis) and word-based F-score (y-axis). The raw Tweets showed word-based F-score of approximate 91%, and the basic structural SVM model (? = 0) showed somewhat inferior 88%. Modified Viterbi algorithm showed the similar behavior as Figure 3, showing 93.2~93.4% word-based F-score when ? was set to 0.5~1. Figure 4 shows that BWSM is effective not only on text with randomly inserted spacing errors, but also on actual data, Tweets. 5 Conclusion In this paper, we proposed BWSM, a new struc-tural SVM-based Korean word spacing model that utilizes user-given spacing information. BWSM can obtain the most likely sequence of spacing labels while minimally altering the word spacing of the input sentence. Experiments on test sets with various error rate showed that BWSM significantly improved word-based F-
878
score, from 95.47% to 98.39% in case of the test set with 10% error rate.  For future work, there are two interesting di-rections. First is to improve BWSM so that it can automatically obtain the optimal value of ? for an arbitrary sentence. This will require a training set consisting of text with actual human spacing errors and its corrected version. Second is to ap-ply BWSM to other interesting problems such as named entity recognition (NER). Newspaper ar-ticles often use certain symbols such as quotation marks or brackets around the titles of movies, songs and books. Such symbols can be viewed as user-given input, which BWSM will try to re-spect as much as possible while trying to find the most likely named entities. Acknowledgments  This work was supported by the IT R&D pro-gram of MSIP/KEIT (10044577, Development of Knowledge Evolutionary WiseQA Platform Technology for Human Knowledge Augmented Services). We would like to thank the anony-mous reviewers for their comments. References  Thorsten Joachims, Thomas Finley, and Chun-Nam John Yu. 2009. Cutting-plane training of structural SVMs. Machine Learning, Vol. 77, No. 1. Beom-mo Kang and Hunggyu Kim. 2004. Sejong Korean Corpora in the making. In Proceedings of the LREC, 1747-1750. John Lafferty, Andrew McCallum and Fernando Pe-reira. 2001. Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data. In Proceedings of the ICML, 282-289. Changki Lee and Myung-Gil Jang. 2010. A Modified Fixed-threshold SMO for 1-Slack Structural SVM. ETRI Journal, Vol.32, No.1, 120-128. Changki Lee and Hyunki Kim. 2012. Automatic Ko-rean word spacing using structural SVM. In Pro-ceedings of the KCC, 270-272. Changki Lee and Hyunki Kim. 2013. Automatic Ko-rean word spacing using Pegasos algorithm. Infor-mation Processing and Management, Vol. 49, No. 1, 370-379. Do-Gil Lee, Hae-Chang Rim and Dongsuk Yook. 2007. Automatic word spacing using probabilistic models based on character n-grams. Intelligent Sys-tems IEEE, Vol. 22, No. 1, 28-35. Seung-Wook Lee, Hae-Chang Rim and So-Young Park. 2007. A new approach for Korean word spac-
ing incorporating confidence value of user?s input. In Proceedings of the ALPIT, 92-97. Kwang-Sup Shim. 2011. Automatic word spacing based on Conditional Random Fields. Korean Journal of Cognitive Science, Vol. 22, No. 2, 217-233. Shai Shalev-Shwartz, Yoram Singer, and Nathan Sre-bro. 2004. Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, Vol. 127, No. 1. Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the ICML, 104-111.   
879
