Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 9?16,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Viterbi Based Alignment between Text Images and their Transcripts?
Alejandro H. Toselli, Vero?nica Romero and Enrique Vidal
Institut Tecnolo`gic d?Informa`tica
Universitat Polite`cnica de Vale`ncia
Cam?? de Vera s/n
46071 - Vale`ncia, Spain
[ahector,vromero,evidal]@iti.upv.es
Abstract
An alignment method based on the Viterbi
algorithm is proposed to find mappings be-
tween word images of a given handwrit-
ten document and their respective (ASCII)
words on its transcription. The approach
takes advantage of the underlying segmen-
tation made by Viterbi decoding in hand-
written text recognition based on Hidden
Markov Models (HMMs). Two HMMs
modelling schemes are evaluated: one using
78-HMMs (one HMM per character class)
and other using a unique HMM to model all
the characters and another to model blank
spaces. According to various metrics used
to measure the quality of the alignments, en-
couraging results are obtained.
1 Introduction
Recently, many on-line digital libraries have been
publishing large quantities of digitized ancient hand-
written documents, which allows the general pub-
lic to access this kind of cultural heritage resources.
This is a new, comfortable way of consulting and
querying this material. The Biblioteca Valenciana
Digital (BiValDi)1 is an example of one such digital
library, which provides an interesting collection of
handwritten documents.
?This work has been supported by the EC (FEDER), the
Spanish MEC under grant TIN2006-15694-C02-01, and by the
Conseller??a d?Empresa, Universitat i Cie`ncia - Generalitat Va-
lenciana under contract GV06/252.
1http://bv2.gva.es
Several of these handwritten documents include
both, the handwritten material and its proper tran-
scription (in ASCII format). This fact has moti-
vated the development of methodologies to align
these documents and their transcripts; i.e. to gen-
erate a mapping between each word image on a doc-
ument page with its respective ASCII word on its
transcript. This word by word alignment would al-
low users to easily find the place of a word in the
manuscript when reading the corresponding tran-
script. For example, one could display both the
handwritten page and the transcript and whenever
the mouse is held over a word in the transcript, the
corresponding word in the handwritten image would
be outlined using a box. In a similar way, whenever
the mouse is held over a word in the handwritten im-
age, the corresponding word in the transcript would
be highlighted (see figure 1). This kind of alignment
can help paleography experts to quickly locate im-
age text while reading a transcript, with useful ap-
plications to editing, indexing, etc. In the opposite
direction, the alignment can also be useful for people
trying to read the image text directly, when arriving
to complex or damaged parts of the document.
Creating such alignments is challenging since the
transcript is an ASCII text file while the manuscript
page is an image. Some recent works address this
problem by relying on a previous explicit image-
processing based word pre-segmentation of the page
image, before attempting the transcription align-
ments. For example, in (Kornfield et al, 2004),
the set of previously segmented word images and
their corresponding transcriptions are transformed
into two different times series, which are aligned
9
Figure 1: Screen-shot of the alignment prototype interface displaying an outlined word (using a box) in the
manuscript (left) and the corresponding highlighted word in the transcript (right).
using dynamic time warping (DTW). In this same
direction, (Huang and Srihari, 2006), in addition to
the word pre-segmentation, attempt a (rough) recog-
nition of the word images. The resulting word string
is then aligned with the transcription using dynamic
programming.
The alignment method presented here (hencefor-
ward called Viterbi alignment), relies on the Viterbi
decoding approach to handwritten text recogni-
tion (HTR) based on Hidden Markov Models
(HMMs) (Bazzi et al, 1999; Toselli et al, 2004).
These techniques are based on methods originally
introduced for speech recognition (Jelinek, 1998).
In such HTR systems, the alignment is actually a
byproduct of the proper recognition process, i.e. an
implicit segmentation of each text image line is ob-
tained where each segment successively corresponds
to one recognized word. In our case, word recogni-
tion is not actually needed, as we do already have
the correct transcription. Therefore, to obtain the
segmentations for the given word sequences, the so-
called ?forced-recognition? approach is employed
(see section 2.2). This idea has been previously ex-
plored in (Zimmermann and Bunke, 2002).
Alignments can be computed line by line in cases
where the beginning and end positions of lines are
known or, in a more general case, for whole pages.
We show line-by-line results on a set of 53 pages
from the ?Cristo-Salvador? handwritten document
(see section 5.2). To evaluate the quality of the ob-
tained alignments, two metrics were used which give
information at different alignment levels: one mea-
sures the accuracy of alignment mark placements
and the other measures the amount of erroneous as-
10
0.3
0.7 0.8
0.2
0.9
0.1
0.8
0.2
0.7
0.3
Figure 2: Example of 5-states HMM modeling (feature vectors sequences) of instances of the character ?a?
within the Spanish word ?cuarenta? (forty). The states are shared among all instances of characters of the
same class. The zones modelled by each state show graphically subsequences of feature vectors (see details
in the magnifying-glass view) compounded by stacking the normalized grey level and its both derivatives
features.
signments produced between word images and tran-
scriptions (see section 4).
The remainder of this paper is organized as fol-
lows. First, the alignment framework is introduced
and formalized in section 2. Then, an implemented
prototype is described in section 3. The alignment
evaluation metrics are presented in section 4. The
experiments and results are commented in section 5.
Finally, some conclusions are drawn in section 6.
2 HMM-based HTR and Viterbi alignment
HMM-based handwritten text recognition is briefly
outlined in this section, followed by a more detailed
presentation of the Viterbi alignment approach.
2.1 HMM HTR Basics
The traditional handwritten text recognition problem
can be formulated as the problem of finding a most
likely word sequence w? = ?w1, w2, . . . , wn?, for
a given handwritten sentence (or line) image rep-
resented by a feature vector sequence x = xp1 =
?x1, x2, . . . , xp?, that is:
w? = arg max
w
Pr(w|x)
= arg max
w
Pr(x|w) ? Pr(w) (1)
where Pr(x|w) is usually approximated by
concatenated character Hidden Markov Models
(HMMs) (Jelinek, 1998; Bazzi et al, 1999),
whereas Pr(w) is approximated typically by an
n-gram word language model (Jelinek, 1998).
Thus, each character class is modeled by a con-
tinuous density left-to-right HMM, characterized by
a set of states and a Gaussian mixture per state. The
Gaussian mixture serves as a probabilistic law to
model the emission of feature vectors by each HMM
state. Figure 2 shows an example of how a HMM
models a feature vector sequence corresponding to
11
b0 b3 b4 b5 b6 bn=7
x1
w1 w3 w4 w5 w6 xp
wn=7
b1 b2
w2
Figure 3: Example of segmented text line image along with its resulting deslanted and size-normalized
image. Moreover, the alignment marks (b0 . . . b8) which delimit each of the words (including word-spaces)
over the text image feature vectors sequence x.
character ?a?. The process to obtain feature vector
sequences from text images as well as the training of
HMMs are explained in section 3.
HMMs as well as n-grams models can be rep-
resented by stochastic finite state networks (SFN),
which are integrated into a single global SFN by re-
placing each word character of the n-gram model by
the corresponding HMM. The search involved in the
equation (1) to decode the input feature vectors se-
quence x into the more likely output word sequence
w?, is performed over this global SFN. This search
problem is adequately solved by the Viterbi algo-
rithm (Jelinek, 1998).
2.2 Viterbi Alignment
As a byproduct of the Viterbi solution to (1), the
feature vectors subsequences of x aligned with each
of the recognized words w1, w2, . . . , wn can be ob-
tained. These implicit subsequences can be visual-
ized into the equation (1) as follows:
w? = arg max
w
?
b
Pr(x,b|w) ? Pr(w) (2)
where b is an alignment; that is, an ordered se-
quence of n+1 marks ?b0, b1, . . . , bn?, used to de-
marcate the subsequences belonging to each recog-
nized word. The marks b0 and bn always point out
to the first and last components of x (see figure 3).
Now, approximating the sum in (2) by the domi-
nant term:
w? ? arg max
w
max
b
Pr(x,b|w) ? Pr(w) (3)
where b? is the optimal alignment. In our case,
we are not really interested in proper text recogni-
tion because the transcription is known beforehand.
Let w? be the given transcription. Now, Pr(w) in
equation 3 is zero for all w except w?, for which
Pr(w?) = 1. Therefore,
b? = arg max
b
Pr(x,b|w?) (4)
which can be expanded to,
b? = arg max
b
Pr(x, b1|w?)Pr(x, b2|b1, w?) . . .
. . . P r(x, bn|b1b2 . . . bn?1, w?)
(5)
Assuming independence of each bi mark from
b1b2 . . . bi?1 and assuming that each subsequence
xbibi?1 depends only of w?i, equation (5) can be rewrit-
ten as,
b? = arg max
b
Pr(xb1b0 |w?1) . . . P r(x
bn
bn?1 |w?n) (6)
This simpler Viterbi search problem is known as
?forced recognition?.
3 Overview of the Alignment Prototype
The implementation of the alignment prototype in-
volved four different parts: document image prepro-
cessing, line image feature extraction, HMMs train-
ing and alignment map generation.
12
Document image preprocessing encompasses the
following steps: first, skew correction is carried out
on each document page image; then background
removal and noise reduction is performed by ap-
plying a bi-dimensional median filter (Kavalliera-
tou and Stamatatos, 2006) on the whole page im-
age. Next, a text line extraction process based on
local minimums of the horizontal projection profile
of page image, divides the page into separate line
images (Marti and Bunke, 2001). In addition con-
nected components has been used to solve the situ-
ations where local minimum values are greater than
zero, making impossible to obtain a clear text line
separation. Finally, slant correction and non-linear
size normalization are applied (Toselli et al, 2004;
Romero et al, 2006) on each extracted line image.
An example of extracted text line image is shown
in the top panel of figure 3, along with the result-
ing deslanted and size-normalized image. Note how
non-linear normalization leads to reduced sizes of
ascenders and descenders, as well as to a thiner un-
derline of the word ?ciudadanos?.
As our alignment prototype is based on Hid-
den Markov Models (HMMs), each preprocessed
line image is represented as a sequence of feature
vectors. To do this, the feature extraction mod-
ule applies a grid to divide line image into N ?
M squared cells. In this work, N = 40 is cho-
sen empirically (using the corpus described further
on) and M must satisfy the condition M/N =
original image aspect ratio. From each cell, three
features are calculated: normalized gray level, hor-
izontal gray level derivative and vertical gray level
derivative. The way these three features are deter-
mined is described in (Toselli et al, 2004). Columns
of cells or frames are processed from left to right
and a feature vector is constructed for each frame
by stacking the three features computed in its con-
stituent cells.
Hence, at the end of this process, a sequence of
M 120-dimensional feature vectors (40 normalized
gray-level components, 40 horizontal and 40 vertical
derivatives components) is obtained. An example of
feature vectors sequence, representing an image of
the Spanish word ?cuarenta? (forty) is shown in fig-
ure 2.
As it was explained in section 2.1, characters are
modeled by continuous density left-to-right HMMs
with 6 states and 64 Gaussian mixture components
per state. This topology (number of HMM states and
Gaussian densities per state) was determined by tun-
ing empirically the system on the corpus described
in section 5.1. Once a HMM ?topology? has been
adopted, the model parameters can be easily trained
from images of continuously handwritten text (with-
out any kind of segmentation) accompanied by the
transcription of these images into the correspond-
ing sequence of characters. This training process is
carried out using a well known instance of the EM
algorithm called forward-backward or Baum-Welch
re-estimation (Jelinek, 1998).
The last phase in the alignment process is the gen-
eration of the mapping proper by means of Viterbi
?forced recognition?, as discussed in section 2.2.
4 Alignment Evaluation Metrics
Two kinds of measures have been adopted to evalu-
ate the quality of alignments. On the one hand, the
average value and standard deviation (henceforward
called MEAN-STD) of the absolute differences be-
tween the system-proposed word alignment marks
and their corresponding (correct) references. This
gives us an idea of the geometrical accuracy of the
alignments obtained. On the other hand, the align-
ment error rate (AER), which measures the amount
of erroneous assignments produced between word
images and transcriptions.
Given a reference mark sequence r =
?r0, r1, . . . , rn? along with an associated to-
kens sequence w = ?w1, w2, . . . , wn?, and a
segmentation marks sequence b = ?b0, b1, . . . , bn?
(with r0 =b0 ? rn =bn), we define the MEAN-STD
and AER metrics as follows:
MEAN-STD: The average value and standard devi-
ation of absolute differences between reference and
proposed alignment marks, are given by:
? =
?n?1
i=1 di
n ? 1 ? =
?
?n?1
i=1 (di ? ?)2
n ? 1 (7)
where di = |ri ? bi|.
13
w1 w3 w4 w5 w6 wn=7w2
r0 r3 r4 r5 r6 r7
x1 xp
r1 r2
b7b1 b2 b3 b4 b6b5b0
m7m5m3m1
Figure 4: Example of AER computation. In this case N = 4 (only no word-space are considered:
w1, w3, w5, w7) and w5 is erroneously aligned with the subsequence xb6b5 (m5 /? (b4, b5)). The resulting
AER is 25%.
AER: Defined as:
AER(%) =100N
?
j:wj 6=b
ej
ej =
{
0 bj?1 <mj <bj
1 otherwise
(8)
where b stands for the blank-space token, N < n is
the number of real words (i.e., tokens which are not
b, and mj = (rj?1 + rj)/2.
A good alignment will have a ? value close to 0
and small ?. Thus, MEAN-STD gives us an idea of
how accurate are the automatically computed align-
ment marks. On the other hand, AER assesses align-
ments at a higher level; that is, it measures mis-
matches between word-images and ASCII transcrip-
tions (tokens), excluding word-space tokens. This is
illustrated in figure 4, where the AERwould be 25%.
5 Experiments
In order to test the effectiveness of the presented
alignment approach, different experiments were car-
ried out. The corpus used, as well as the experiments
carried out and the obtained results, are reported in
the following subsections.
5.1 Corpus description
The corpus was compiled from the legacy handwrit-
ing document identified as Cristo-Salvador, which
was kindly provided by the Biblioteca Valenciana
Digital (BIVALDI). It is composed of 53 text page
images, scanned at 300dpi and written by only one
writer. Some of these page images are shown in the
figure 5.
As has been explained in section 3, the page im-
ages have been preprocessed and divided into lines,
resulting in a data-set of 1,172 text line images.
In this phase, around 4% of the automatically ex-
tracted line-separation marks were manually cor-
rected. The transcriptions corresponding to each line
image are also available, containing 10,911 running
words with a vocabulary of 3,408 different words.
To test the quality of the computed alignments, 12
pages were randomly chosen from the whole corpus
pages to be used as references. For these pages the
true locations of alignment marks were set manually.
Table 1 summarized the basic statistics of this cor-
pus and its reference pages.
Number of: References Total Lexicon
pages 12 53 ?
text lines 312 1,172 ?
words 2,955 10,911 3,408
characters 16,893 62,159 78
Table 1: Basic statistics of the database
5.2 Experiments and Results
As mentioned above, experiments were carried out
computing the alignments line-by-line. Two differ-
ent HMM modeling schemes were employed. The
first one models each of the 78 character classes us-
ing a different HMM per class. The second scheme
uses 2 HMMs, one to model all the 77 no-blank
character classes, and the other to model only the
blank ?character? class. The HMM topology was
identical for all HMMs in both schemes: left-to-
right with 6 states and 64 Gaussian mixture com-
14
Figure 5: Examples page images of the corpus ?Cristo-Salvador?, which show backgrounds of big variations
and uneven illumination, spots due to the humidity, marks resulting from the ink that goes through the paper
(called bleed-through), etc.
ponents per state.
As has been explained in section 4, two different
measures have been adopted to evaluate the quality
of the obtained alignments: the MEAN-STD and the
AER. Table 2 shows the different alignment evalu-
ation results obtained for the different schemes of
HMM modeling.
78-HMMs 2-HMMs
AER (%) 7.20 25.98
? (mm) 1.15 2.95
? (mm) 3.90 6.56
Table 2: Alignment evaluation results 78-HMMs
and 2-HMMs.
From the results we can see that using the 78
HMMs scheme the best AER is obtained (7.20%).
Moreover, the relative low values of ? and ? (in mil-
limeters) show that the quality of the obtained align-
ments (marks) is quite acceptable, that is they are
very close to their respective references. This is il-
lustrated on the left histogram of figure 6.
The two typical alignment errors are known as
over-segmentation and under-segmentation respec-
tively. The over-segmentation error is when one
word image is separated into two or more fragments.
The under-segmentation error occurs when two or
more images are grouped together and returned as
one word. Figure 7 shows some of them.
6 Remarks and Conclusions
Given a manuscript and its transcription, we propose
an alignment method to map every word image on
the manuscript with its respective ASCII word on
the transcript. This method takes advantage of the
implicit alignment made by Viterbi decoding used
in text recognition with HMMs.
The results reported in the last section should be
considered preliminary.
Current work is under way to apply this align-
ment approach to the whole pages, which represents
a more general case where the most corpora do not
have transcriptions set at line level.
References
I. Bazzi, R. Schwartz, and J. Makhoul. 1999. An Om-
nifont Open-Vocabulary OCR System for English and
15
02
4
6
8
10
12
0 1 2 3 4 5 6
Fr
eq
ue
nc
y
(%
)
|Segi ? Refi| (mm)
mean
0
1
2
3
4
5
6
0 1 2 3 4 5 6
Fr
eq
ue
nc
y
(%
)
|Segi ? Refi| (mm)
mean
Figure 6: |ri ? bi| distribution histograms for 78-HMMs (left) and 2-HMMs (right) modelling schemes.
Figure 7: Word alignment for 6 lines of a particularly noisy part of the corpus. The four last words on the
second line as well as the last line illustrate some of over-segmentation and under-segmentation error types.
Arabic. IEEE Trans. on PAMI, 21(6):495?504.
Chen Huang and Sargur N. Srihari. 2006. Mapping Tran-
scripts to Handwritten Text. In Suvisoft Ltd., editor,
Tenth International Workshop on Frontiers in Hand-
writing Recognition, pages 15?20, La Baule, France,
October.
F. Jelinek. 1998. Statistical Methods for Speech Recog-
nition. MIT Press.
Ergina Kavallieratou and Efstathios Stamatatos. 2006.
Improving the quality of degraded document images.
In DIAL ?06: Proceedings of the Second International
Conference on Document Image Analysis for Libraries
(DIAL?06), pages 340?349, Washington, DC, USA.
IEEE Computer Society.
E. M. Kornfield, R. Manmatha, and J. Allan. 2004. Text
Alignment with Handwritten Documents. In First In-
ternational Workshop on Document Image Analysis
for Libraries (DIAL), pages 195?209, Palo Alto, CA,
USA, January.
U.-V. Marti and H. Bunke. 2001. Using a Statistical Lan-
guage Model to improve the preformance of an HMM-
Based Cursive Handwriting Recognition System. Int.
Journal of Pattern Recognition and Artificial In telli-
gence, 15(1):65?90.
V. Romero, M. Pastor, A. H. Toselli, and E. Vidal. 2006.
Criteria for handwritten off-line text size normaliza-
tion. In Procc. of The Sixth IASTED international
Conference on Visualization, Imaging, and Image Pro-
cessing (VIIP 06), Palma de Mallorca, Spain, August.
A. H. Toselli, A. Juan, D. Keysers, J. Gonzlez, I. Sal-
vador, H. Ney, E. Vidal, and F. Casacuberta. 2004.
Integrated Handwriting Recognition and Interpretation
using Finite-State Models. Int. Journal of Pattern
Recognition and Artificial Intelligence, 18(4):519?
539, June.
M. Zimmermann and H. Bunke. 2002. Automatic Seg-
mentation of the IAM Off-Line Database for Hand-
written English Text. In ICPR ?02: Proceedings of
the 16 th International Conference on Pattern Recog-
nition (ICPR?02) Volume 4, page 40035, Washington,
DC, USA. IEEE Computer Society.
16
Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 107?111,
Avignon, France, 24 April 2012. c?2012 Association for Computational Linguistics
Natural Language Inspired Approach for Handwritten Text Line
Detection in Legacy Documents?
Vicente Bosch Campos
Inst. Tec. de Informa?tica
Univ. Polite?cnica Valencia
Valencia - Spain
vbosch@iti.upv.es
Alejandro He?ctor Toselli
Inst. Tec. de Informa?tica
Univ. Polite?cnica Valencia
Valencia - Spain
ahector@iti.upv.es
Enrique Vidal
Inst. Tec. de Informa?tica
Univ. Polite?cnica Valencia
Valencia - Spain
evidal@iti.upv.es
Abstract
Document layout analysis is an important
task needed for handwritten text recogni-
tion among other applications. Text lay-
out commonly found in handwritten legacy
documents is in the form of one or more
paragraphs composed of parallel text lines.
An approach for handwritten text line de-
tection is presented which uses machine-
learning techniques and methods widely
used in natural language processing. It is
shown that text line detection can be accu-
rately solved using a formal methodology,
as opposed to most of the proposed heuris-
tic approaches found in the literature. Ex-
perimental results show the impact of us-
ing increasingly constrained ?vertical lay-
out language models? in text line detection
accuracy.
1 Introduction
Handwritten text transcription is becoming an in-
creasingly important task, in order to provide his-
torians and other researchers new ways of index-
ing, consulting and querying the huge amounts of
historic handwritten documents which are being
published in on-line digital libraries.
Transcriptions of such documents are currently
obtained with solutions that range from the use of
systems that aim at fully automatic handwritten
text recognition (Bazzi et al, 1999)
(HTR), to computer assisted transcription
(CATTI), were the users participate interactively
in the proper transcription process (Toselli et al,
2009).
?Work supported under the MIPRCV ?Consolider
Ingenio 2010? program (CSD2007-00018), MITTRAL
(TIN2009-14633-C03-01) and also Univ. Polite?cnica Valen-
cia (PAID-05-11)
The basic input to these systems consists of text
line images. Hence, text line detection and ex-
traction from a given document page image be-
comes a necessary preprocessing step in any kind
of transcription systems. Furthermore the quality
of line segmentation directly influences the final
accuracy achieved by such systems.
Detection of handwritten text lines in an im-
age entails a greater difficulty, in comparison with
printed text lines, due to the inherent properties of
handwritten text: variable inter-line spacing, over-
lapping and touching strokes of adjacent hand-
written lines, etc.
The difficulty is further increased in the case
of ancient documents, due to common problems
appearing in them: presence of smear, significant
background variations and uneven illumination,
spots due to the humidity, and marks resulting
from the ink that goes through the paper (gener-
ally called ?bleed-through?).
Among the most popular state-of-the art meth-
ods involved in handwritten text line detection
we find four main families: based on (ver-
tical) projection profiles (Likforman-Sulem et
al., 2007), on the Hough transform (Likforman-
Sulem et al, 1995), the repulsive-attractive net-
work approach (O?ztop et al, 1999) and finally
the so-called stochastic methods (Vinciarelli et al,
2004), which combine probabilistic models such
as Hidden Markov Models (HMMs) along with
dynamic programming techniques (e.g. Viterbi
algorithm) to derive optimal paths between over-
lapping text lines.
It is worth noting that, most of the mentioned
approaches somewhat involve heuristic adjust-
ments of their parameters, which have to be prop-
erly tuned according to the characteristics of each
107
task in order to obtain adequate results.
In this work, the text line detection problem in
legacy handwritten documents is approached by
using machine-learning techniques and methods
which are widely used in natural language pro-
cessing (NLP).
It is shown that the text line detection problem
can be solved by using a formal methodology, as
opposed to most of the currently proposed heuris-
tic based approaches found in the literature.
2 Statistical Framework for Text Line
Detection
For the work presented in this paper, we assume
that the input image (of a page or selected region)
contains one or more paragraphs of single-column
parallel text with no images or diagram figures.
Additionally, we assume that the input image has
been properly preprocessed so as to ensure that
their text lines are roughly horizontal. These as-
sumptions are reasonable enough for most legacy
handwritten documents.
Similarly to how the statistic framework of
automatic speech recognition (ASR) is estab-
lished, the handwritten text line detection prob-
lem can be also formulated as the problem of
finding the most likely text lines sequence, h? =
?h1, h2, . . . , hn?, for a given handwritten page
image represented by a sequence of observations1
o = ?o1, o2, . . . , om?, that is:
h? = argmax
h
P (h | o) (1)
Using the Bayes? rule we can decompose the
probability P (h | o) into two terms:
h? = argmax
h
P (o | h) ? P (h) (2)
In the jargon of NLP these probabilities rep-
resent the morphological and syntactic knowl-
edge levels, respectively. As it happens in ASR,
P (o | h) is typically approximated by HMMs,
which model vertical page regions, while P (h)
by a ?language model? (LM), which restricts how
those regions are composed in order to form an
actual page. In what follows, a detailed descrip-
tion of this modelling scheme is given.
1Henceforward, in the context of this formal framework,
each time it is mentioned image of page or selected text, we
are implicitly referring to the input feature vector sequence
?o? describing it.
2.1 Modelling
In our line detection approach four different kinds
of vertical regions are defined:
Blank Line-region (BL): Large rectangular re-
gion of blank space usually found at the start
and the end of a page (top and bottom mar-
gins).
Normal text Line-region (NL): Region oc-
cupied by the main body of a normal
handwritten text line.
Inter Line-region (IL): Defined as the region
found within two consecutive normal text
lines, characterized by being crossed by the
ascenders and descenders belonging to the
adjacent text lines.
Non-text Line-region (NT): Stands for every-
thing which does not belong to any of the
other regions.
Figure 1: Examples of the different kind of line-
regions.
We model each of these regions by an HMM
which is trained with instances of such regions.
Basically, each line-region HMM is a stochastic
finite-state device that models the succession of
feature vectors extracted from instances of this
line-region image. In turn, each HMM state
generates feature vectors following an adequate
parametric probabilistic law; typically, a mixture
of Gaussian densities. The adequate number of
states and Gaussians per state may be conditioned
by the available amount of training data.
Once an HMM ?topology? (number of states
and structure) has been adopted, the model pa-
rameters can be easily trained from instances (se-
quences of features vectors) of full images con-
taining a sequence of line-regions (without any
108
kind of segmentation) accompanied by the refer-
ence labels of these images into the correspond-
ing sequence of line-region classes. This training
process is carried out using a well known instance
of the EM algorithm called forward-backward or
Baum-Welch re-estimation (Jelinek, 1998).
The syntactic modelling level is responsible for
the way that the different line regions are com-
posed in order to produce a valid page structure.
For example we can force that NL and NT line
regions must always be followed by IL inter-line
regions: NL+IL and NT+IL. We can also use
the LM to impose restrictions about the mini-
mum or maximum number of line-regions to be
detected. The LM for our text line detection ap-
proach, consists in a stochastic finite state gram-
mar (SFSG) which recognizes valid sequences of
elements (line regions): NL+IL, NT+IL and BL.
Both modelling levels, morphological and syn-
tactical, which are represented by finite-state au-
tomaton, can be integrated into a single global
model on which Eq. (2) is easily solved; that is,
given an input sequence of raw feature vectors,
an output string of recognized sequence of line-
region labels is obtained. In addition the vertical
position of each detected line and and line-region
is obtained as a by-product.
3 System Architecture
Page layoutcorpus
LM ModelOff-line  lineHMMs
HMM Training LM Training
Preprocessing
Feature Extraction
Decoding
Training
Page Images
Cleaned PageImages
Feature Vectors
Type label and Region positioncoordinates
Figure 2: Global scheme of the handwritten text line
detection process.
The flow diagram of Fig. 2 displays the overall
process of the proposed handwritten text line de-
tection approach. It is composed of four different
phases: image preprocessing, feature extraction,
HMMs and LM training and decoding. Next we
will overview the first two phases, preprocessing
and feature extraction, since the rest has already
been covered in the preceding section.
3.1 Preprocessing Phase
Initially performing background removal and
noise reduction is carried out by applying a bi-
dimensional median filter on them. The resulting
image skew is corrected by applying vertical pro-
jection profile and RLSA (Wong and Wahl, 1982),
along with standard techniques to calculate the
skew angle.
3.2 Feature Extraction Phase
As our text line detection approach is based on
HMMs, each preprocessed image must be rep-
resented as a sequence of feature vectors.This is
done by dividing the already preprocessed image
(from left-to-right) into D non-overlapping rect-
angular regions with height equal to the image-
height (see Fig. 3).
In each of these rectangular regions we calcu-
late the vertical grey level histogram. RLSA is
applied to obtain a more emphasized vertical pro-
jection profile. Finally, to eliminate local maxima
on the obtained vertical projection profiles, they
are smoothed with a rolling median filter (Man-
matha and Srimal, 1999) (see Fig. 3) . In this way,
Figure 4: Review of the impact of the RLSA and
rolling media filter on the histogram calculation of a
sample line.
a D-dimensional feature vector is constructed for
each page/block image pixels row, by stacking the
D projection profile values corresponding to that
row. Hence, at the end of this process, a sequence
of L D-dimensional feature vectors is obtained,
where L is the image height.
4 Experimental Setup and Results
In order to study the efficacy of the line detection
approach proposed in this paper, different experi-
ments were carried out. We are mainly interested
in assessing the impact upon final text line detec-
109
1 2 3 4 5
Figure 3: Partial page image visualization of 5 (D = 5) rectangular regions across over 3 handwritten text lines.
For each region, its vertical projection profile is also plotted.
tion accuracy of employing increasingly restric-
tive LMs.
4.1 Corpus Description
Experiments are carried out with corpus compiled
from a XIX century Spanish manuscript identified
as ?Cristo-Salvador? (CS), which was kindly pro-
vided by the Biblioteca Valenciana Digital (Bi-
VaLDi)2. This is a rather small document com-
posed of 53 colour images of text pages, scanned
at 300 dpi and written by a single writer. Some
page images examples are shown in Fig. 5.
Figure 5: Examples of pages images from CS corpus.
In this work we employ the so-called book
partition, which has been defined for this data-
set (Romero et al, 2007). Its test set contains
the last 20 page images were as the training set
is composed of the 33 remaining pages. Table 1
summarizes the relevant information of this parti-
tion.
Table 1: Basic statistics of the Cristo-Salvador corpus
partition.
Number of: Training Test Total
Pages 33 20 53
Normal-text lines (NL) 685 497 1 182
Blank Lines (BL) 73 70 143
Non-text Lines (NT) 16 8 24
Inter Lines (IL) 701 505 1 206
Each page was annotated with a succession of
reference labels (NL, NT, BL and IL) indicating
2http://bv2.gva.es.
the kind of line-regions that composed it. Such
references were generated by executing standard
methods for text line detection based on vertical
projection profiles, which were afterwards manu-
ally labelled, verified, adjusted and/or rectified by
a human operator to ensure correctness.
4.2 Evaluation Measures
We measure the quality of the text line detec-
tion by means of the ?line error rate? (LER)
which is performed by comparing the sequences
of automatically obtained region labels with the
corresponding reference label sequences. The
LER is computed in the same way as the well
known WER, with equal costs assigned to dele-
tions, insertions and substitutions (McCowan et
al., 2004).
4.3 Experiments and Results
A series of experiments were performed on the CS
corpus using a simple hold-out validation as per
the CS ?book? partition. Initially some param-
eters were set up: feature extraction dimension
D, HMM topology (number of states and Gaus-
sians),number of Baum-Welch iterations, and de-
coding grammar scale factor (GSF) and word in-
sertion penalty (WIP). After some informal exper-
imentation, adequate values were found for sev-
eral of them: feature vectors dimension of 2, left-
to-right HMMs with 4 states topology, 32 Gaus-
sian mixtures per state trained by running 3 cycles
of Baum-Welch re-estimation algorithm. The re-
maining parameters, all related with the decoding
process itself, were tuned to obtain the best figures
for each of the two following language models:
the prior and conditional represented by topolog-
ically different SFSGs. The prior model transi-
tion probabilities are estimated from the training
set as the fraction of the number of appearances
of each vertical region label over the whole count
of labels. The conditional model also considers
the previous label in order to perform the estima-
tion. These estimates resemble the uni-gram and
110
bi-gram LMs calculations, except no smoothing
strategy is implemented here.
Additionally, it is defined for each test page a
line-number constrained LM which uses the con-
ditional probabilities to populate the model but
enforces a total number of possible line-regions to
detect as per the number of reference line-region
labels of that test page. Table 2 reports the ob-
tained LER results for each of these LMs.
Table 2: Best detection LER(%) obtained for each
kind of language model: Prior, Conditional and Line-
Number Constrained.
LM WIP GSF LER(%)
Prior -32 8 0.86
Conditional -8 16 0.70
LN-Constrained -128 1 0.34
As can be seen, the more restrictive the LM
is, the better accuracy is achieved. Concerning
the line-number constrained, they are really con-
ceived for its utilization in (parts of) documents
or document collections with homogeneous num-
bers of lines per page.
5 Conclusions
We have presented a new approach for text line
detection by using a statistical framework similar
to that already employed in many topics of NLP.
It avoids the traditional heuristics approaches usu-
ally adopted for this task.
The accuracy of this approach is similar to or
better than that of current state of the art solutions
found in the literature. We find that the detected
baselines provided by our approach are of better
quality (visually closer to the actual line) than cur-
rent heuristic methods as can be seen in 6.
Figure 6: Image shows the difference between our pro-
posed method (upper side of each coloured region )
and the histogram projection method (lower side)
In the future we will extend this approach not
only to detect, but also to classify line-region
types in order to determine for example titles,
short lines, beginning and and end of paragraphs,
etc. Furthermore, it is envisioned that the pro-
posed stochastic framework serves as a corner-
stone to implementing interactive approaches to
line detection similar to those used for handwrit-
ten text transcription used in (Toselli et al, 2009).
References
Issam Bazzi, Richard Schwartz, and John Makhoul.
1999. An omnifont open-vocabulary OCR system
for English and Arabic. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 21(6):495?
504.
Frederick Jelinek. 1998. Statistical methods for
speech recognition. MIT Press.
Laurence Likforman-Sulem, Anahid Hanimyan, and
Claudie Faure. 1995. A hough based algorithm
for extracting text lines in handwritten documents.
Document Analysis and Recognition, International
Conference on, 2:774.
Laurence Likforman-Sulem, Abderrazak Zahour, and
Bruno Taconet. 2007. Text line segmentation of
historical documents: a survey. International Jour-
nal on Document Analysis and Recognition, 9:123?
138, April.
Raghavan Manmatha and Nitin Srimal. 1999. Scale
space technique for word segmentation in handwrit-
ten documents. In Proceedings of the Second In-
ternational Conference on Scale-Space Theories in
Computer Vision, SCALE-SPACE ?99, pages 22?
33, London, UK. Springer-Verlag.
Iain A. McCowan, Darren Moore, John Dines, Daniel
Gatica-Perez, Mike Flynn, Pierre Wellner, and
Herve? Bourlard. 2004. On the use of informa-
tion retrieval measures for speech recognition eval-
uation. Idiap-RR Idiap-RR-73-2004, IDIAP, Mar-
tigny, Switzerland, 0.
Vero?nica Romero, Alejandro He?ctor Toselli, Luis
Rodr??guez, and Enrique Vidal. 2007. Com-
puter Assisted Transcription for Ancient Text Im-
ages. In International Conference on Image Anal-
ysis and Recognition (ICIAR 2007), volume 4633
of LNCS, pages 1182?1193. Springer-Verlag, Mon-
treal (Canada), August.
Alejandro He?ctor Toselli, Vero?nica Romero, Moise?s
Pastor, and Enrique Vidal. 2009. Multimodal inter-
active transcription of text images. Pattern Recog-
nition, 43(5):1824?1825.
Alessandro Vinciarelli, Samy Bengio, and Horst
Bunke. 2004. Off-line recognition of uncon-
strained handwritten texts using hmms and statisti-
cal language models. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 26(6):709?720,
june.
Kwan Y. Wong and Friedrich M. Wahl. 1982. Doc-
ument analysis system. IBM Journal of Research
and Development, 26:647?656.
Erhan O?ztop, Adem Y. Mu?layim, Volkan Atalay, and
Fatos Yarman-Vural. 1999. Repulsive attractive
network for baseline extraction on document im-
ages. Signal Processing, 75(1):1 ? 10.
111
