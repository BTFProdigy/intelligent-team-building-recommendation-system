Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 705?712
Manchester, August 2008
A Method for Automatic POS Guessing of Chinese Unknown Words 
Likun Qiu 
Department of Chinese Language and 
Literature, Peking University / No.5 Yi-
heyuan Road, Haidian District, Beijing, 
China 100871 
NEC Laboratories, China 
qiulk@pku.edu.cn 
Changjian Hu, Kai Zhao 
NEC Laboratories, China / 14F, Build-
ing.A, Innovation Plaza, No.1 
Tsinghua Science Park, Zhongguancun 
East Road, Haidian District, Beijing, 
China 100084 
{huchangjian, zhaokai} 
@research.nec.com.cn 
 
Abstract 
This paper proposes a method for auto-
matic POS (part-of-speech) guessing of 
Chinese unknown words. It contains two 
models. The first model uses a machine-
learning method to predict the POS of 
unknown words based on their internal 
component features. The credibility of 
the results of the first model is then 
measured. For low-credibility words, the 
second model is used to revise the first 
model?s results based on the global con-
text information of those words. The ex-
periments show that the first model 
achieves 93.40% precision for all words 
and 86.60% for disyllabic words, which 
is a significant improvement over the 
best results reported in previous studies, 
which were 89% precision for all words 
and 74% for disyllabic words. Further, 
the second model improves the results by 
0.80% precision for all words and 1.30% 
for disyllabic words. ? 
1 Introduction 
Since written Chinese does not use blank spaces 
to denote word boundaries, Chinese word seg-
mentation becomes an essential task for natural 
language processing, as in many other Asian lan-
guages (Thai, Japanese, Tibetan, etc.). It is diffi-
cult to build a complete dictionary comprising all 
words, for new words are constantly being cre-
ated. As such, unknown words may greatly influ-
ence the effectiveness of text processing. Studies 
                                                 
?2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
on unknown words include detection, POS 
guessing, sense classification, etc. Current meth-
ods for automatic unknown word detection have 
been relatively successful and widely used in 
many systems, yet automatic POS guessing for 
unknown words still remains a challenge for 
natural language processing research. 
The task of POS guessing is quite different 
from traditional POS tagging. Traditional POS 
tagging involves assigning a single POS tag to a 
word token, provided that it is known what POS 
tag this word can take on in principle. This task 
requires a lexicon that lists possible POS tags for 
all words. However, unknown words are not in 
the lexicon, so the task of POS guessing of un-
known words involves the guessing of a correct 
POS for an unknown word from the whole POS 
set of the current language. Obviously, tradi-
tional methods of POS tagging cannot effectively 
solve the problem of POS guessing of unknown 
words. 
In previous work, two types of features have 
been used for the task of POS guessing of un-
known Chinese words. One type is contextual 
feature, including local contextual features and 
global contextual features, and the other is inter-
nal component feature. Previous work has mainly 
used context information to guess the POS tags 
of unknown Chinese words, while a few designs 
looked at internal component features. Although 
there have been some attempts to combine the 
two types of features together, no reasonable ex-
planation of the relationship between the two 
types of features has been given. 
It is well known that the properties of a struc-
ture always depend on its internal component 
structure. As such, it is natural for us to wonder 
whether models based on internal component 
features alone can perform the POS guessing task 
for unknown Chinese words with both high pre-
705
cision and high recall. Here we present a model 
based on internal component features of un-
known words using CRFs (conditional random 
fields). The results are very good, especially for 
multi-syllabic words (excluding disyllabic 
words). 
While in the previous model the precision of 
POS guessing of disyllabic words is relatively 
high, there is still much room for further im-
provement. Considering that the usages of a 
word in real text can show the properties of a 
word, and that these features may be a useful 
complement to internal component features, we 
designed a scheme to effectively utilize the two 
types of features together. In this scheme, credi-
bility scores for former guessing results are com-
puted, and only those words with relatively lower 
credibility scores are revised by the model based 
on global context information. The model based 
on global context information simulates the be-
havior of linguists in judging the POS of a word. 
Though the recall of the latter model is low, it 
can revise some incorrect guessing results of the 
initial model. 
According to Lu (2005), there are six main 
types of unknown Chinese words: 
(1) Abbreviation (acronym): e.g., ?? zhong-
mei (China-U.S). 
(2) Proper names (person?s name, place name, 
company name): e.g., ??? Wang Anshi 
(person?s name), ?? Penang (an island in 
Malaysia; place name), ?? Huawei (com-
pany name). 
(3) Derived words (words with affixes): e.g., ?
?? zong-jingli (general manager), ??? 
xiandai-hua (modernize). 
(4) Compounds: e.g., ??  huoyun (obtain 
permission), ?? nisha (mud), ?? tubian 
(sudden change) 
(5) Numeric compounds: e.g., ???? siqian 
riyuan (four thousand Japanese yen), ??
??? 2003nian (year 2003) 
(6) Reduplicated words: e.g., ? ? ? ? 
yingbuyinggai (should or should not), ??
?? chuchujinjin (go in and go out) 
Proper names and numeric compounds are all 
nouns, so they don?t have the problem of POS 
guessing. We will focus on abbreviation, derived 
words, compounds, and reduplicated words. 
The remainder of this paper is organized as 
follows: in Section 2, we introduce some previ-
ous work on POS guessing of unknown Chinese 
words. Sections 3, 4, and 5 describe our proposed 
method, which includes two models and an addi-
tional process linking the two models together. In 
detail, Section 3 considers POS guessing for un-
known Chinese words as a sequence-labeling 
problem and proposes a model based on internal 
component features to solve this task. In Section 
4, we compute a credibility score for each guess-
ing result based on the sequence type of the 
words? internal component structure. This links 
the model in Section 3 and that of Section 5 to-
gether. Section 5 describes a model based on 
global context information to revise the guessing 
results of the initial model that have relatively 
lower credibility scores. Section 6 shows the ex-
periments and results of our methods and a com-
parison with previous work. Section 7 presents 
our conclusions. 
2 Previous Work 
Considering the features used during POS guess-
ing, we have classified previous studies on POS 
guessing of unknown words into three types. 
The first type use only contextual features, in-
cluding local context and global context.  For 
example, Nakagawa and Matsumoto (2006) pro-
posed a probabilistic model to guess the POS 
tags of unknown words by considering all the 
occurrences of unknown words with the same 
lexical form in a document. The parameters were 
estimated using Gibbs sampling. They also at-
tempted to apply the model to semi-supervised 
learning, and conducted experiments on multiple 
corpora. The highest precision in the Chinese 
corpus of their experiments was 67.85%. 
The second type use only internal component 
features, such as that of Chen and Bai (1997) and 
Wu and Jiang (2000). Chen and Bai (1997) ex-
amined all unknown nouns, verbs, and adjectives 
and reported 69.13% precision using Dice met-
rics to measure the affix-category association 
strength and an affix-dependent entropy weight-
ing scheme for determining the weightings be-
tween prefix-category and suffix-category asso-
ciations. This approach is effective in processing 
derived words such as ??? xiandai-hua (mod-
ernize), but performs poorly when encountering 
compounds such as ?? baozhi (inflation-proof). 
Wu and Jiang (2000) calculated P(Cat,Pos,Len) 
for each character, where Cat is the POS of a 
word containing the character, Pos is the position 
of the character in that word, and Len is the 
length of that word. They then calculated the 
POS probabilities for each unknown word as the 
joint probabilities of P(Cat,Pos,Len) for its com-
706
ponent characters. This approach was applied to 
unknown nouns, verbs, and adjectives of two to 
four characters in length. This approach exhibits 
lower recall for multi-syllabic words even if the 
training corpus is significantly large.  
The third type attempt to combine internal 
component features and context information, 
such as that of Lu (2005) and Goh et al (2006). 
Lu (2005) describes a hybrid model that com-
bines a rule-based model with two statistical 
models for the task of POS guessing of unknown 
Chinese words. The rule-based model includes 
35 manual rules concerning the type, length, and 
internal structure of unknown words, and the two 
statistical models utilize context information and 
the likelihood for a character to appear in a par-
ticular position of words of a particular length 
and POS category, one of which is Wu and Ji-
ang?s (2000) model. It achieves a precision of 
89.00%, a significant improvement over the best 
result reported in previous studies, which was 
69.00%. Goh et al (2006) propose a method for 
guessing the part-of-speech tags of detected un-
known words using contextual and internal com-
ponent features with maximum entropy models. 
Both Lu (2005) and Goh et al (2006) use only 
local context and not global context. As far as 
internal component features are concerned, Lu 
(2005) uses only the word category feature in his 
rule-based model while Goh et al (2006) uses 
only the first character and last character features. 
From the above studies, we may find that meth-
ods based on internal component features are 
very promising, but this kind of features still 
needs much more attention. Moreover, none of 
them has proved that methods based on context 
information can improve the results of methods 
based on internal component features. They only 
attempted to utilize different types of features 
together and to give simultaneous results for both 
types of features. 
Our method is among the third type of studies, 
but is different from the rest in the scheme of 
combining the two types of features together. In 
our method, internal component features play a 
more important role. We will prove that a model 
based on this type of features alone can perform 
very well. The other type of features acts as a 
useful supplement and can improve the results of 
some words in a certain degree. The two models 
are linked together by assigning a credibility 
score for each POS guessing result generated by 
the initial model. The results with a relatively 
lower credibility score are identified and put 
through reconsideration by a method based on 
global context information. 
3 Model Based on Internal Component 
Features 
In this model, we consider the task of POS 
guessing of unknown words as a problem of se-
quence labeling. The inspiration for this ap-
proach came from our observations of how hu-
mans understand a word. Usually an unknown 
word is regarded by human as a sequence of 
characters or morphemes that can be partitioned 
into several segments, where each segment is 
relatively coherent in meaning. 
3.1 Conditional Random Fields 
In contrast with other models for labeling se-
quences, such as HMM and MEMM, CRFs are 
good at avoiding the label bias problem. They 
condition on the entire observation sequence, 
thus avoiding the need for independent assump-
tions between observations and vastly expanding 
the set of features that can be incorporated into 
the model without violating its assumptions. One 
of the main advantages of a conditional model is 
its ability to explore a diverse range of features 
relevant to a specific task. As many studies have 
shown, CRFs are the best models for solving the 
sequence labeling problem (Lafferty et al, 2001; 
Vail et al, 2007). So we chose to use CRFs to 
solve the POS guessing problem. 
3.2 The POS Guessing Model 
While training, we use the words of a dictionary 
as training data. Those words will be considered 
as sentences and then segmented and assigned 
POS-tags by a standard word segmentation and 
POS tagging tool. By training with this data, we 
will obtain a POS guessing model for unknown 
words. While testing, we still consider an un-
known word as a sentence and process it with the 
same tool. 
In the dictionary, most words have one POS-
tag while a few have more. The monosyllabic 
words were omitted from the training. 
Feature Analysis 
In our CRFs model, we employed three main 
types of features: the components of words, the 
lengths of those components, and the POS tags of 
those components. 
Before the CRFs training, we analyzed the in-
ternal component structure of the dictionary 
words and assigned a proper POS-tag to each 
component. There are four analysis schemes that 
707
are different from each other in two aspects. The 
first aspect is the type of the component, which 
may be a character or immediate constituent (IC). 
Here, ?immediate constituent? means constitu-
ents that directly form a word. For example, ?
???? kexuejishubu (department of science 
and technology) has the following constituents: 
? ke, ? xue, ? ji, ? shu, ? bu, ?? kexue, 
and ?? jishu, in which only ?? kexue, ?? 
jishu and ? bu are the immediate constituents of 
????? kexuejishubu. The second aspect is 
regarding the consideration of the POS-tag of the 
component. The four analysis schemes are listed 
in Table 1. 
 
POS 
 
Component 
With  Without  
Character Scheme 1 Scheme 2 
Immediate 
Constituent 
Scheme 3 Scheme 4 
Table 1. Analysis Schemes 
In Scheme 1 and Scheme 2, the tool segments 
a dictionary word until all components are char-
acters, and in Scheme 1 only, each component is 
given a POS tag. In Scheme 3 and Scheme 4, the 
tool segments a dictionary word only once to get 
its immediate constituents, and in Scheme 3 only, 
each component is given a POS tag. For instance, 
?????  kexuejishubu (department of sci-
ence and technology) will be segmented as ??/N 
?/N ?/N ?/N ?/N? in Scheme 1 and ???/N 
??/N ?/N? in Scheme 3. 
Feature Template Selection 
For each type of feature, we used the five tem-
plates in Figure 1. So in Schemes 1 and 3 there 
are 15 templates, and in Schemes 2 and 4 there 
are 10 templates. 
For instance, when training, ????? will 
be transformed as ??/N/N_B??/N/N_M ?
/N/N_E in Scheme 3, in which N denotes that the 
POS of the word ????? is noun, while B, 
M and E denote the beginning, middle and end 
positions of the word. 
 
U01:%x[-1,i]: the former component?s ith feature
U02:%x[0,i]: the current component?s ith feature
U03:%x[1,i]: the next component?s ith feature 
U04:%x[-1,i]/%x[0,i]: the former component?s 
ith feature and the current component?s ith fea-
ture  
U05:%x[0,i]/%x[1,i]: the current component?s 
ith feature and the next component?s ith feature 
Figure 1. List of Templates(i=1-3) 
By using a dictionary with these feature tem-
plates for the training, we obtain a POS guessing 
model for unknown Chinese words. If an un-
known word such as ?? yongdian (electricity 
used) is tested, it would be analyzed as ?/V ?
/N (to use, electricity) for the feature extraction 
and then it would be tagged as ? /V/N_B ?
/N/N_E. That is, the word is assigned a POS of 
noun. 
4 Credibility Computation 
The initial model is based on the hypothesis that 
the syntactical properties of a word depend on its 
internal structure. But the internal structure of 
some words are ambiguous. For instance, both ?
? yongyu (vocabulary [literally, ?used words?]) 
?and ?  yongjing (exert [literally, ?use 
strength?]) both have the sequence V1N1 (which 
is combined by a POS sequence of ?VN? and a 
length sequence of ?11?), yet the former one is a 
noun and the latter one is a verb.  
There are some other sequences like V1N1. 
All the words (especially disyllabic words) 
fitting to these sequences bring difficulty for 
POS guessing model in Section 3. In this section, 
we attempt to identify those words by computing 
a credibility score for each type of sequence. The 
lower the score, less credible the result of the 
model.  
In detail, Formula 1 is used to compute the 
credibility of a word that has a certain type of 
sequence, e.g., ? ?  yongyu (vocabulary 
[literally, ?used words?])  with the sequence 
?V1N1?. 
)(
)|()|( 1
k
jkjk
k SCount
PPSCountPPSCount
C +
=?=
=
      (1) 
In Formula 1, kC  denotes the credibility score 
of words that have the kth type of sequence SK. 
SK denotes a sequence as P1L1P2L2??PnLn, in 
which n means the quantity of components in the 
sequence SK; Pn and Ln mean the POS and length 
of the nth component of any word that has the 
sequence SK, respectively; Count(SK) denotes the 
quantity of words in the dictionary that have the 
sequence SK; and Count(SK|P=Pj) and 
Count(SK|P=Pj+1) denote the quantity of words in 
the dictionary that have the sequence SK  and are 
tagged as POS Pj and Pj+1, respectively, in which 
Pj and Pj+1 are the two POSs that make the value 
of Count(SK|P=Pj) a maximum of two. Some se-
quences with lower credibility scores are listed in 
Table 2. 
708
For instance, the sequence type of the word ?
?  yongdian (electricity used) is V1N1, so its 
credibility score is 0.65. If the threshold is 0.8, 
this word will be considered a low-credibility 
word and put through reconsideration by the fol-
lowing model. 
 
 
 
Sequence Credibility 
Score 
Proportion 
Vg1Ng1 0.7 0.50% 
V1Ng1 0.67 1.90% 
Vg1N1 0.67 0.55% 
V1N1 0.65 2.99% 
V1Vn2 0.57 0.04% 
A1A1 0.55 0.22% 
N1A1 0.48 0.15% 
V1V2 0.44 0.05% 
V1Vi2 0.25 0.08% 
Table 2. Examples of Sequences with Low 
Credibility Scores 
 
5 Model Based on Global Contextual 
Features 
Words with relatively lower credibility scores 
(given in Section 4) will be revised by a model 
based on global context. In this paper, we im-
plement a model of voting by syntactical tem-
plates, which derived from research results of 
linguists. 
This process requires a relatively large corpus 
that can provide enough context instances for 
each under-processed word. It is difficult for us 
to find a corpus that can provide enough in-
stances of most unknown words, because many 
of such instances have only been used for a rela-
tively short time. In this paper, we use search 
engine as the source of corpus, i.e., throw a word 
to a search engine and pick out instances from 
the returned snippets. 
Linguists have summarized systematic rules 
for judging the POS of a Chinese word based on 
its global distribution in real text (Guo, 2002). 
For example, generally a verb or adjective can be 
modified by the word ??? (not) while a noun 
cannot. Based on this knowledge, we designed a 
set of syntactical templates listed in Table 3. The 
templates indicate whether a word can be used in 
such ways. 
For every word, we build phrases based on 
these templates (see Table 3 for instances of ?
? xihuan (like)) and send the phrases to a search 
engine as queries. For each query, the search 
engine returns some snippets, which are 
generally in sentence form. Then each word gets 
three scores through a voting process in which 
the sentences act as ?voters.? The three scores, 
Score(N), Score(V), and Score(A), denote the 
likelihood score for the word to be a noun, a verb 
or an adjective, respectively. Each voter votes by 
following the criteria given in Figure 2. In Figure 
2, Value(N), Value(V), and Value(A) are  con-
stant values that are used to balance the three 
scores. 
 
Table 3. Syntactical Templates with Instances of 
????1 
 
If the unknown word follows a transitive verb 
and is at the end of a sentence or subsentence, 
Score(N)+=Value(N); 
If the unknown word follows a quantitative word 
and is at the end of a sentence or subsentence, 
Score(N)+=Value(N); 
If the unknown word follows the word ???, 
??? or ???, Score(V)+=Value(V); 
If the unknown word follows the word ????, 
???? or ???? and is at the end of a sentence 
or subsentence, or there is a following word that 
is not a verb, Score(V)+=Value(N); 
If the unknown word follows the word ???, 
??? or ????, Score(A)+=Value(A). 
Figure 2. Criteria for Voting 
For each instance, Score(N), Score(V), and 
Score(A) will be added to the scores Value(N), 
Value(V), and Value(A), respectively. 
Although these templates are effective, there 
are some exceptions brought by morphology 
analysis errors or other reasons, so we use an 
outstanding method to filter the exceptions. We 
                                                 
1 Here ?*? means the structure is invalid. 
T
em
plates 
~ ?
+
~ 
?
?
+
~ 
?
?
+
~ 
?
?
+
~ 
?
+
~ 
?
+
~ 
?
+
~ 
?
?
+
~ Instance 
?
?
?
?
?
?
?
?
?
?
?
?
?
* 
?
?
?
?
* 
?
?
? 
?
?
? 
?
?
?
?
?
?
?
709
compute an outstanding value with Formula 2 to 
judge whether the voting result is acceptable. 
))((
))(('))((
POSScoreMax
POSScoreMaxPOSScoreMax
O
?
=
    (2) 
In Formula 2, O  means the outstanding value 
of a voting result; ))(( POSScoreMax  means 
the maximum score among the three scores and 
))((' POSScoreMax means the maximum score 
between the other scores. If O  is larger than a 
threshold, we assume the voting result to be ac-
ceptable and adopt the result to revise the POS 
guessing result of the initial model.  
For instance, Score(N), Score(V), and Score(A) 
of the word?? yongyu (vocabulary [literally, 
?used words?]) are 50, 5 and 3 respectively. So 
O(??)=(50-5)/50=0.9. 
6 Experiments and Results 
6.1 Data Preparation 
The model based on CRFs is trained on the 
Modern Chinese Grammar Information 
Dictionary (Yu, 1998) and tested on the 
Contemporary Chinese Corpus of Peking 
University (Yu et al, 2002). The corpus is seg-
mented and POS-tagged. Both the dictionary and 
corpus were constructed by the Institute of Com-
putational Linguistics, Peking University. The 
corpus was built using the content of all the news 
articles of the People?s Daily newspaper pub-
lished in China from January to June 1998. We 
selected all verbs, nouns and adjectives from the 
dictionary, excluding monosyllabic words, as 
training data. The nouns, verbs and adjectives in 
the corpus but not in the dictionary were consid-
ered to be unknown words and used as testing 
data. The distribution of word length of the train-
ing and testing data is presented in Table 4. 
 
Word Length Training Testing 
Disyllabic 40,103 11,108 
Tri-syllabic 12,167 12,901 
Four-character 1,180 1,055 
Five-character 0 279 
Total 53,450 25,343 
Table 4. Distribution of Word Length in Training 
and Testing Data 
We used ICTCLAS 1.0 (Zhang, 2002) to do 
word segmentation and POS tagging, because 
ICTCLAS is known as one of the best tools for 
those functions. ?CRF++, Yet Another CRF? 
toolkit (Kudo, 2005) was used as the implemen-
tation of CRFs model and www.Baidu.com as 
the search engine for our model based on contex-
tual features. 
6.2 Results of the Proposed Method 
The results for the four schemes of our method 
based on internal component features are listed in 
Table 52. From these results we may see that 
Scheme 1 is the best and Scheme 3 the second 
best, which means POS-tag of internal compo-
nents is very useful feature in the POS guessing 
work. The comparison between Scheme 1 and 
Scheme 3 indicates that character-based scheme 
is good for processing tri-syllabic words and 
five-character words while IC-based scheme is 
good for processing disyllabic words and four-
character words. Considering that most tri-
syllabic words and five-character words are de-
rivative words, while disyllabic words and four-
character words are compounds, the results show 
that the character-based scheme is good for proc-
essing derivative words while the IC-based 
scheme is good for processing compounds. All 
the following improvements will be based on 
Scheme 1. 
We assign the threshold of credibility score as 
0.8, and then there are 2,234 words with a credi-
bility score lower than the threshold. These 
words are then put through the revision process. 
In the revision model, we set the values of 
Value(N), Value(V), Value(A) and the out-
standing threshold as 4, 1, 1, and 0.5, respec-
tively, based on experience. All above thresholds 
are experimentally determined. Finally, 1,357 out 
of the 2,234 words pass the outstanding examina-
tion. Among them, 462 results were different 
from the former results and 302 of those were 
correctly revised, which resulted in the precision 
of disyllabic words reaching 87.90% (see Table 
6). Moreover, other 895 words, which have the 
same result in the two models, reaches the preci-
sion of 91.2%. That means the credibility will be 
very high when the two models generate the 
same result. 
Although we believe that the former method 
may have equal effectiveness to most man-made 
rules, there are still several rules that must be 
incorporated in order to simplify our machine-
learning method. Here we incorporated two 
reduplication rules to process two types of 
reduplicated unknown words, respectively. The 
form of the first kind of words is ?V1?V2,  such 
as ????  yingbuyinggai (should or should 
                                                 
2 Precision, recall, F-measure are the same. 
710
Table 7. Results of Wu & Jiang's (2000) 
Table 6. Results of Revision by Voting Model 
and Two Rules 
not) and the form of the second kind of words is 
?V1V1V2V2,? such as ???? chuchujinjin (go 
in and go out). If a four-character word is 
associated with one of the two forms and the first 
character is a verb, we revise its POS as a verb 
tag. 
The two reduplication rules correctly revised 
68 four-character words, which increased the 
precision of four-character words to 97.10%, a 
significant improvement over the previous best 
result, which was 92.89% (see Table 6). 
 
 
6.3 Comparison with Previous Work 
Wu & Jiang?s (2000)3 method is the most analo-
gous with our method, yet they did not directly 
report the results in their paper. In this paper, we 
                                                 
3 Lu (2005) implemented Wu & Jiang?s (2000) model with 
a relatively small corpus as the training data. The precision 
of Wu & Jiang?s method reported by the paper is 77.90% 
with a recall of 63.82%. 
implement their model using the same data as 
our method. The results of Wu & Jiang?s (2000) 
model are listed in Table 7. It shows that their 
model can guess the POS for disyllabic words 
with a relatively good F-measure (83.60%). How-
ever, the recall is not high for disyllabic 
(79.11%) and tri-syllabic (82.70%) words, and 
quite low for four-character (20.95%) and five-
character (0%) words. Our model in Section 3 
not only improves F-measure to 93.40%, but also 
improves recalls to 86.60%, 99.22%, 92.03% and 
100% for multi-syllabic words in turn (Table 5, 
Scheme 1).  
Lu (2005) proposed a hybrid model that 
achieved a precision of 89% for all words and 
74% for disyllabic words. Compared with that 
method, the hybrid model in this paper improves 
the precision to 94.20% for all words and 
87.90% for disyllabic words. Although the ex-
periments were not taken on the same data, the 
figures reflect the difference of power between 
methods in a certain degree. 
7 Conclusion and Future Work 
The results of this experiment show that our 
model based on internal component features can 
achieve quite good results in POS guessing for 
unknown Chinese words, both in precision and 
recall. This proves that the internal component 
Word Length Precision of 
Scheme 1 
Precision of 
Scheme 2 
Precision of  
Scheme 3 
Precision of  
Scheme 4 
Best Result
Disyllabic 86.60% 86.01 86.65% 85.21% 86.65% 
Tri-syllabic 99.22% 99.17% 98.65% 97.48% 99.22% 
Four-character 92.03% 91.47% 92.89% 89.76% 92.89% 
Five-character 100.00% 98.20% 98.92% 98.92% 100.00% 
Total 93.40% 93.08% 93.15% 91.80% 93.40% 
Word 
Length 
Total 
number 
Precision (Correspond-
ing value of before) 
Disyllabic 11,108 87.90% (86.60%) 
Tri-syllabic 12,901 99.22% (99.22%) 
Four-
character 
1,055 97.10% (92.89%) 
Five-
character 
279 100%    (100%) 
Total 25,343 94.20% (93.40%) 
Word Length Total Num-
ber 
Tagged 
Number 
Precision Recall F-measure (Corre-
sponding value of 
our method) 
Disyllabic 11,108 10,408 84.43% 79.11% 81.68%  (87.90%) 
Tri-syllabic 12,901 11,091 96.20% 82.70% 88.94%  (99.22%) 
Four-character 1,055 225 98.22% 20.95% 34.53%  (97.10%) 
Five-character 279 0 0 0 0             (100%) 
Total 25,343 21,724 90.58% 77.65% 83.60%   (94.20%) 
Table 5. Results for Four Schemes of The Model Based on Internal Component Features 
711
features of unknown words can be very useful in 
POS guessing. Moreover, the trained model 
based on internal component features is universal 
and robust. One evidence is that the model can 
identify POS correctly for most five-character 
words, even when there is no training data for 
that type of words. 
Our results also show that the contextual fea-
tures of unknown words can be an important 
complement to help improve POS guessing. Al-
though models based on contextual features 
alone can?t achieve the same precision and recall 
as models based on internal component features 
do, we may use contextual features as a comple-
ment in processing those words with ambiguous 
structure. 
In contrast with Lu (2005), we don?t use many 
manual rules. This does not mean that we believe 
those rules are useless in POS guessing. In fact, 
our initial model based on the CRFs model has 
learned the structure rules of Chinese words and 
can even give a credibility score for each rule. 
That is, most of the rules have been incorporated 
by the utilization of the CRFs model. 
In the future, to improve the results, we at-
tempt to manually revise the training data.  No-
tice that the training data was formed by seg-
menting and tagging POS of each word in a dic-
tionary using an existing tool like ICTCLAS. 
However, these tools usually generate quite a 
few errors on the words, because they are de-
signed to handle sentence but not word. These 
errors were not revised in the experiment, which 
damaged the performance. Thus, by manually 
revising the training data, we hope to improve 
the results in a certain degree. 
Although our experiments are mainly based on 
contemporary Chinese, we believe that this 
method will also be applicable to other Asian 
languages such as Japanese. 
References 
Andy Wu and Zixin Jiang. 2000. Statistically-
enhanced New Word Identification in a Rule-based 
Chinese System. In Proceedings of the 2nd Chinese 
Language Processing Workshop, pages 46?51. 
Chao-Jan Chen, Ming-Hong Bai, and Keh-Jiann Chen. 
1997. Category Guessing for Chinese Unknown 
Words. In Proceedings of the Natural Language 
Processing Pacific Rim Symposium, pages 35?40. 
Chooi-Ling Goh, Masayuki Asahara, and Yuji Ma-
tsumoto. 2006. Machine Learning-based Methods to 
Chinese Unknown Word Detection and POS Tag 
Guessing. In Journal of Chinese Language and Com-
puting 16 (4):185-206 
Douglas L. Vail, Manuela M. Veloso, and John D. 
Lafferty. 2007. Conditional Random Fields for Activ-
ity Recognition. In Proceedings of 2007 International 
Joint Conference on Autonomous Agents and Multi-
agent Systems. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: Probabilis-
tic Models for Segmenting and Labeling Sequence 
Data. In Proceedings of International Conference on 
Machine Learning. 
Kevin Zhang. ICTCLAS1.0. http://www.nlp.org.cn/ 
project/project.php?proj_id=6.  
Rui Guo. 2002. Studies on Part-of-speech of Contem-
porary Chinese. Commercial Press, Beijing, China. 
Shiwen Yu. 1998. Dictionary of Modern Chinese 
Grammar Information. Tsinghua University Press. 
Beijing, China. 
Shiwen Yu, Huiming Duan, Xuefeng Zhu, and Bing 
Sun. 2002. The Basic Processing of Contemporary 
Chinese Corpus at Peking University. Technical Re-
port, Institute of Computational Linguistics, Peking 
University, Beijing, China. 
T Nakagawa, Y Matsumoto. 2006. Guessing Parts-of-
speech of Unknown Words Using Global Information. 
In Proceedings of the 21st International Conference 
on Computational Linguistics and 44th Annual Meet-
ing of Association for Computational Linguistics, 
pages 705?712. 
Taku Kudo. 2005. CRF++: Yet Another CRF toolkit. 
http://chasen.org/~taku/software/CRF++. 
Xiaofei Lu. 2005. Hybrid Methods for POS Guessing 
of Chinese Unknown Words. In Proceedings of the 
43th Annual Meeting of Association for Computational 
Linguistics Student Research Workshop, pages 1?6. 
712
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 758?768,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Optimal Incremental Parsing via Best-First Dynamic Programming?
Kai Zhao1 James Cross1
1Graduate Center
City University of New York
365 Fifth Avenue, New York, NY 10016
{kzhao,jcross}@gc.cuny.edu
Liang Huang1,2
2Queens College
City University of New York
6530 Kissena Blvd, Queens, NY 11367
huang@cs.qc.cuny.edu
Abstract
We present the first provably optimal polyno-
mial time dynamic programming (DP) algo-
rithm for best-first shift-reduce parsing, which
applies the DP idea of Huang and Sagae
(2010) to the best-first parser of Sagae and
Lavie (2006) in a non-trivial way, reducing
the complexity of the latter from exponential
to polynomial. We prove the correctness of
our algorithm rigorously. Experiments con-
firm that DP leads to a significant speedup
on a probablistic best-first shift-reduce parser,
and makes exact search under such a model
tractable for the first time.
1 Introduction
Best-first parsing, such as A* parsing, makes con-
stituent parsing efficient, especially for bottom-up
CKY style parsing (Caraballo and Charniak, 1998;
Klein and Manning, 2003; Pauls and Klein, 2009).
Traditional CKY parsing performs cubic time exact
search over an exponentially large space. Best-first
parsing significantly speeds up by always preferring
to explore states with higher probabilities.
In terms of incremental parsing, Sagae and Lavie
(2006) is the first work to extend best-first search to
shift-reduce constituent parsing. Unlike other very
fast greedy parsers that produce suboptimal results,
this best-first parser still guarantees optimality but
requires exponential time for very long sentences
in the worst case, which is intractable in practice.
Because it needs to explore an exponentially large
space in the worst case, a bounded priority queue
becomes necessary to ensure limited parsing time.
?This work is mainly supported by DARPA FA8750-13-2-
0041 (DEFT), a Google Faculty Research Award, and a PSC-
CUNY Award. In addition, we thank Kenji Sagae and the
anonymous reviewers for their constructive comments.
On the other hand, Huang and Sagae (2010) ex-
plore the idea of dynamic programming, which is
originated in bottom-up constituent parsing algo-
rithms like Earley (1970), but in a beam-based non
best-first parser. In each beam step, they enable
state merging in a style similar to the dynamic pro-
gramming in bottom-up constituent parsing, based
on an equivalence relation defined upon feature val-
ues. Although in theory they successfully reduced
the underlying deductive system to polynomial time
complexity, their merging method is limited in that
the state merging is only between two states in the
same beam step. This significantly reduces the num-
ber of possible merges, because: 1) there are only
a very limited number of states in the beam at the
same time; 2) a lot of states in the beam with differ-
ent steps cannot be merged.
We instead propose to combine the idea of dy-
namic programming with the best-first search frame-
work, and apply it in shift-reduce dependency pars-
ing. We merge states with the same features set
globally to further reduce the number of possible
states in the search graph. Thus, our DP best-first al-
gorithm is significantly faster than non-DP best-first
parsing, and, more importantly, it has a polynomial
time complexity even in the worst case.
We make the following contributions:
? theoretically, we formally prove that our DP
best-first parsing reaches optimality with poly-
nomial time complexity. This is the first time
that exact search under such a probabilistic
model becomes tractable.
? more interestingly, we reveal that our dynamic
programming over shift-reduce parsing is in
parallel with the bottom-up parsers, except that
we have an extra order constraint given by the
shift action to enforce left to right generation of
758
input w0 . . . wn?1
axiom 0 : ?0, ?: 0
sh
` : ?j, S? : c
`+ 1 : ?j + 1, S|wj? : c+ scsh(j, S)
j < n
rex
` : ?j, S|s1|s0? : c
`+ 1 : ?j, S|s1
xs0? : c+ screx(j, S|s1|s0)
rey
` : ?j, S|s1|s0? : c
`+ 1 : ?j, S|s1
ys0? : c+ screy(j, S|s1|s0)
Figure 1: Deductive system of basic non-DP shift-reduce
parsing. Here ` is the step index (for beam search), S is
the stack, c is the score of the precedent, and sca(x) is
the score of action a from derivation x. See Figure 2 for
the DP version.
partial trees, which is analogous to Earley.
? practically, our DP best-first parser is only ?2
times slower than a pure greedy parser, but is
guaranteed to reach optimality. In particular,
it is ?20 times faster than a non-DP best-first
parser. With inexact search of bounded prior-
ity queue size, DP best-first search can reach
optimality with a significantly smaller priority
queue size bound, compared to non-DP best-
first parser.
Our system is based on a MaxEnt model to meet
the requirement from best-first search. We observe
that this locally trained model is not as strong as
global models like structured perceptron. With that
being said, our algorithm shows its own merits in
both theory and practice. To find a better model for
best-first search would be an interesting topic for fu-
ture work.
2 Shift-Reduce and Best-First Parsing
In this section we review the basics of shift-reduce
parsing, beam search, and the best-first shift-reduce
parsing algorithm of Sagae and Lavie (2006).
2.1 Shift-Reduce Parsing and Beam Search
Due to space constraints we will assume some ba-
sic familiarity with shift-reduce parsing; see Nivre
(2008) for details. Basically, shift-reduce parsing
(Aho and Ullman, 1972) performs a left-to-right
scan of the input sentence, and at each step, chooses
either to shift the next word onto the stack, or to re-
duce, i.e., combine the top two trees on stack, ei-
ther with left as the root or right as the root. This
scheme is often called ?arc-standard? in the litera-
ture (Nivre, 2008), and is the basis of several state-
of-the-art parsers, e.g. Huang and Sagae (2010). See
Figure 1 for the deductive system of shift-reduce de-
pendency parsing.
To improve on strictly greedy search, shift-reduce
parsing is often enhanced with beam search (Zhang
and Clark, 2008), where b derivations develop in
parallel. At each step we extend the derivations in
the current beam by applying each of the three ac-
tions, and then choose the best b resulting deriva-
tions for the next step.
2.2 Best-First Shift-Reduce Parsing
Sagae and Lavie (2006) present the parsing prob-
lem as a search problem over a DAG, in which each
parser derivation is denoted as a node, and an edge
from node x to node y exists if and only if the corre-
sponding derivation y can be generated from deriva-
tion x by applying one action.
The best-first parsing algorithm is an applica-
tion of the Dijkstra algorithm over the DAG above,
where the score of each derivation is the priority.
Dijkstra algorithm requires the priority to satisfy
the superiority property, which means a descendant
derivation should never have a higher score than its
ancestors. This requirement can be easily satisfied if
we use a generative scoring model like PCFG. How-
ever, in practice we use a MaxEnt model. And we
use the negative log probability as the score to sat-
isfy the superiority:
x ? y ? x.score < y.score,
where the order x ? y means derivation x has a
higher priority than y.1
The vanilla best-first parsing algorithm inher-
its the optimality directly from Dijkstra algorithm.
However, it explores exponentially many derivations
to reach the goal configuration in the worst case.
We propose a new method that has polynomial time
complexity even in the worst case.
1For simplicity we ignore the case when two derivations
have the same score. In practice we can choose either one of
the two derivations when they have the same score.
759
3 Dynamic Programming for Best-First
Shift-Reduce Parsing
3.1 Dynamic Programming Notations
The key innovation of this paper is to extend best-
first parsing with the ?state-merging? method of dy-
namic programming described in Huang and Sagae
(2010). We start with describing a parsing configu-
ration as a non-DP derivation:
?i, j, ...s2s1s0?,
where ...s2s1s0 is the stack of partial trees, [i..j] is
the span of the top tree s0, and s1s2... are the re-
mainder of the trees on the stack.
The notation fk(sk) is used to indicate the features
used by the parser from the tree sk on the stack. Note
that the parser only extracts features from the top
d+1 trees on the stack.
Following Huang and Sagae (2010), f?(x) of a
derivation x is called atomic features, defined as the
smallest set of features s.t.
f?(i, j, ...s2s1s0) = f?(i, j, ...s?2s
?
1s
?
0)
? fk(sk) = fk(s
?
k), ?k ? [0, d].
The atomic feature function f?(?) defines an equiv-
alence relation ? in the space of derivations D:
?i, j, ...s2s1s0? ? ?i, j, ...s
?
2s
?
1s
?
0?
? f?(i, j, ...s2s1s0) = f?(i, j, ...s?2s
?
1s
?
0)
This implies that any derivations with the same
atomic features are in the same equivalence class,
and their behaviors are similar in shift and reduce.
We call each equivalence class a DP state. More
formally we define the space of all states S as:
S
?
= D/?.
Since only the top d+1 trees on the stack are used
in atomic features, we only need to remember the
necessary information and write the state as:
?i, j, sd...s0?.
We denote a derivation x?s state as [x]?. In the rest
of this paper, we always denote derivations with let-
ters x, y, and z, and denote states with letters p, q,
and r.
The deductive system for dynamic programming
best-first parsing is adapted from Huang and Sagae
(2010). (See the left of Figure 2.) The difference is
that we do not distinguish the step index of a state.
This deductive system describes transitions be-
tween states. However, in practice we use one state?s
best derivation found so far to represent the state.
For each state p, we calculate the prefix score, p.pre,
which is the score of the derivation to reach this
state, and the inside score, p.ins , which is the score
of p?s top tree p.s0. In addition we denote the shift
score of state p as p.sh
?
= scsh(p), and the reduce
score of state p as p.re
?
= scre(p). Similarly we
have the prefix score, inside score, shift score, and
reduce score for a derivation.
With this deductive system we extend the concept
of reducible states with the following definitions:
The set of all states with which a state p can
legally reduce from the right is denoted L(p), or left
states. (see Figure 3 (a)) We call any state q ? L(p)
a left state of p. Thus each element of this set would
have the following form:
L(?i, j, sd...s0?)
?
={?h, i, s?d...s
?
0? |
fk(s
?
k?1)=fk(sk), ?k ? [1, d]} (1)
in which the span of the ?left? state?s top tree ends
where that of the ?right? state?s top tree begins, and
fk(sk) = fk(s?k?1) for all k ? [1, d].
Similarly, the set of all states with which a state p
can legally reduce from the left is denoted R(p), or
right states. (see Figure 3 (a)) For two states p, q,
p ? L(q)? q ? R(p)
3.2 Algorithm 1
We constrain the searching time with a polynomial
bound by transforming the original search graph
with exponentially many derivations into a graph
with polynomial number of states.
In Algorithm 1, we maintain a chart C and a prior-
ity queue Q , both of which are based on hash tables.
Chart C can be formally defined as a function
mapping from the space of states to the space of
derivations:
C : S ? D.
In practice, we use the atomic features f?(p) as the
signature of state p, since all derivations in the same
state share the same atomic features.
760
sh
state p:
? , j, sd...s0?: (c, )
?j, j + 1, sd?1...s0, wj? : (c+ ?, 0)
j < n PRED
? , j, A? ?.B?? : (c, )
?j, j, B ? .?? : (c+s, s)
(B ? ?) ? G
rex
state q:
?k, i, s?d...s
?
0?: (c
?, v?)
state p:
?i, j, sd...s0?: ( , v)
?k, j, s?d...s
?
1, s
?
0
xs0? : (c
?+v+?, v?+v+?)
q ? L(p) COMP
?k, i, A??.B?? : (c?, v?) ?i, j, B? : ( , v)
?k, j, A? ?B.?? : (c?+v, v?+v)
Figure 2: Deductive systems for dynamic programming shift-reduce parsing (Huang and Sagae, 2010) (left, omitting
rey case), compared to weighted Earley parsing (Stolcke, 1995) (right). Here ? = scsh(p), ? = scsh(q) + screx(p),
s = sc(B ? ?), G is the set of CFG rules, ?i, j, B? is a surrogate for any ?i, j, B ? ?.?, and is a wildcard that
matches anything.
. . .
L(p)
sh sh
. . .
R(p)p
. . .
L(p)
sh
. . .
T (p)p
(a) L(p) andR(p) (b) T (p) = R(L(p))
Figure 3: Illustrations of left states L(p), right states R(p), and left corner states T (p). (a) Left states L(p) is the set
of states that can be reduced with p so that p.s0 will be the right child of the top tree of the result state. Right states
R(p) is the set of states that can be reduced with p so that p.s0 will be the left child of the top tree of the result state.
(b) Left corner states T (p) is the set of states that have the same reducibility as shifted state p, i.e., ?p? ? L(p), we
have ?q ? T (p), q ? R(p?). In both (a) and (b), thick sh arrow means shifts from multiple states; thin sh arrow means
shift from a single state.
We use C [p] to retrieve the derivation in C that
is associated with state p. We sometimes abuse this
notation to say C [x] to retrieve the derivation asso-
ciated with signature f?(x) for derivation x. This is
fine since we know derivation x?s state immediately
from the signature. We say state p ? C if f?(p) is
associated with some derivation in C . A derivation
x ? C if C [x] = x. Chart C supports operation
PUSH, denoted as C [x]? x, which associate a sig-
nature f?(x) with derivation x.
Priority queue Q is defined similarly as C , except
that it supports the operation POP that pops the high-
est priority item.
Following Stolcke (1995) and Nederhof (2003),
we use the prefix score and the inside score as the
priority in Q :
x ? y ? x.pre < y.pre or
(x.pre = y.pre and x.ins < y.ins), (2)
Note that, for simplicity, we again ignore the spe-
cial case when two derivations have the same prefix
score and inside score. In practice for this case we
can pick either one of them. This will not affect the
correctness of our optimality proof in Section 5.1.
In the DP best-first parsing algorithm, once a
derivation x is popped from the priority queue Q ,
as usual we try to expand it with shift and reduce.
Note that both left and right reduces are between
the derivation x of state p = [x]? and an in-chart
derivation y of left state q = [y]? ? L(p) (Line 10
of Algorithm 1), as shown in the deductive system
(Figure 2). We call this kind of reduction left expan-
sion.
We further expand derivation x of state p with
some in-chart derivation z of state r s.t. p ? L(r),
i.e., r ? R(p) as in Figure 3 (a). (see Line 11 of
Algorithm 1.) Derivation z is in the chart because it
is the descendant of some other derivation that has
been explored before x. We call this kind of reduc-
tion right expansion.
Our reduction with L andR is inspired by Neder-
hof (2003) and Knuth (1977) algorithm, which will
be discussed in Section 4.
761
Algorithm 1 Best-First DP Shift-Reduce Parsing.
Let LC (x)
?
= C [L([x]?)] be in-chart derivations of
[x]??s left states
Let RC (x)
?
= C [R(p)] be in-chart derivations of
[x]??s right states
1: function PARSE(w0 . . . wn?1)
2: C ? ? . empty chart
3: Q ? {INIT} . initial priority queue
4: while Q 6= ? do
5: x? POP(Q)
6: if GOAL(x) then return x . found best parse
7: if [x]? 6? C then
8: C [x]? x . add x to chart
9: SHIFT(x,Q)
10: REDUCE(LC (x), {x},Q) . left expansion
11: REDUCE({x},RC (x),Q) . right expansion
12: procedure SHIFT(x,Q)
13: TRYADD(sh(x),Q) . shift
14: procedure REDUCE(A,B,Q)
15: for (x, y) ? A?B do . try all possible pairs
16: TRYADD(rex(x, y),Q) . left reduce
17: TRYADD(rey(x, y),Q) . right reduce
18: function TRYADD(x, Q)
19: if [x]? 6? Q or x ? Q[x] then
20: Q[x]? x . insert x into Q or update Q[x]
3.3 Algorithm 2: Lazy Expansion
We further improve DP best-first parsing with lazy
expansion.
In Algorithm 2 we only show the parts that are
different from Algorithm 1.
Assume a shifted derivation x of state p is a direct
descendant from derivation x? of state p?, then p ?
R(p?), and we have:
?ys.t . [y]? = q ? REDUCE({p
?},R(p?)), x ? y
which is proved in Section 5.1.
More formally, we can conclude that
?ys.t . [y]? = q ? REDUCE(L(p), T (p)), x ? y
where T (p) is the left corner states of shifted state
p, defined as
T (?i, i+1, sd...s0?)
?
={?i, h, s?d...s
?
0? |
fk(s
?
k)=fk(sk), ?k ? [1, d]}
which represents the set of all states that have the
same reducibility as a shifted state p. In other words,
T (p) = R(L(p)),
Algorithm 2 Lazy Expansion of Algorithm 1.
Let TC (x)
?
= C [T ([x]?)] be in-chart derivations of
[x]??s left-corner states
1: function PARSE(w0 . . . wn?1)
2: C ? ? . empty chart
3: Q ? {INIT} . initial priority queue
4: while Q 6= ? do
5: x? POP(Q)
6: if GOAL(x) then return x . found best parse
7: if [x]? 6? C then
8: C [x]? x . add x to chart
9: SHIFT(x,Q)
10: REDUCE(x.lefts, {x},Q) . left expansion
11: else if x.action is sh then
12: REDUCE(x.lefts, TC (x),Q) . right expan.
13: procedure SHIFT(x,Q)
14: y ? sh(x)
15: y.lefts ? {x} . initialize lefts
16: TRYADD(y,Q)
17: function TRYADD(x, Q)
18: if [x]? ? Q then
19: if x.action is sh then . maintain lefts
20: y ? Q[x]
21: if x ? y then Q[x]? x
22: Q[x].lefts ? y.lefts ? x.lefts
23: else if x ? Q[x] then
24: Q[x]? x
25: else . x 6? Q
26: Q[x]? x
which is illustrated in Figure 3 (a). Intuitively, T (p)
is the set of states that have p?s top tree, p.s0, which
contains only one node, as the left corner.
Based on this observation, we can safely delay the
REDUCE({x},RC (x)) operation (Line 11 in Algo-
rithm 1), until the derivation x of a shifted state is
popped out from Q . This helps us eliminate unnec-
essary right expansion.
We can delay even more derivations by extending
the concept of left corner states to reduced states.
Note that for any two states p, q, if q?s top tree q.s0
has p?s top tree p.s0 as left corner, and p, q share the
same left states, then derivations of p should always
have higher priority than derivations of q. We can
further delay the generation of q?s derivations until
p?s derivations are popped out.2
2We did not implement this idea in experiments due to its
complexity.
762
4 Comparison with Best-First CKY and
Best-First Earley
4.1 Best-First CKY and Knuth Algorithm
Vanilla CKY parsing can be viewed as searching
over a hypergraph(Klein and Manning, 2005), where
a hyperedge points from two nodes x, y to one node
z, if x, y can form a new partial tree represented by
z. Best-first CKY performs best-first search over
the hypergraph, which is a special application of the
Knuth Algorithm (Knuth, 1977).
Non-DP best-first shift-reduce parsing can be
viewed as searching over a graph. In this graph, a
node represents a derivation. A node points to all its
possible descendants generated from shift and left
and right reduces. This graph is actually a tree with
exponentially many nodes.
DP best-first parsing enables state merging on
the previous graph. Now the nodes in the hyper-
graph are not derivations, but equivalence classes of
derivations, i.e., states. The number of nodes in the
hypergraph is no longer always exponentially many,
but depends on the equivalence function, which is
the atomic feature function f?(?) in our algorithms.
DP best-first shift-reduce parsing is still a special
case of the Knuth algorithm. However, it is more dif-
ficult than best-first CKY parsing, because of the ex-
tra topological order constraints from shift actions.
4.2 Best-First Earley
DP best-first shift-reduce parsing is analogous to
weighted Earley (Earley, 1970; Stolcke, 1995), be-
cause: 1) in Earley the PRED rule generates states
similar to shifted states in shift-reduce parsing; and,
2) a newly completed state also needs to check all
possible left expansions and right expansions, simi-
lar to a state popped from the priority queue in Al-
gorithm 1. (see Figure 2)
Our Algorithm 2 exploits lazy expansion, which
reduces unnecessary expansions, and should be
more efficient than pure Earley.
5 Optimality and Polynomial Complexity
5.1 Proof of Optimality
We define a best derivation of state [x]? as a deriva-
tion x such that ?y ? [x]?, x  y.
Note that each state has a unique feature signa-
ture. We want to prove that Algorithm 1 actually fills
the chart by assigning a best derivation to its state.
Without loss of generality, we assume Algorithm 1
fills C with derivations in the following order:
x0, x1, x2, . . . , xm
where x0 is the initial derivation, xm is the first goal
derivation in the sequence, and C [xi] = xi, 0 ? i ?
m. Denote the status of chart right after xk being
filled as Ck. Specially, we define C?1 = ?
However, we do not have superiority as in non-DP
best-first parsing. Because we use a pair of prefix
score and inside score, (pre, ins), as priority (Equa-
tion 2) in the deductive system (Figure 2). We have
the following property as an alternative for superior-
ity:
Lemma 1. After derivation xk has been filled into
chart, ?x s.t. x ? Q , and x is a best derivation
of state [x]?, then x?s descendants can not have a
higher priority than xk.
Proof. Note that when xk pops out, x is still in Q ,
so xk  x. Assume z is x?s direct descendant.
? If z = sh(x) or z = re(x, ), based on the de-
ductive system, x ? z, so xk  x ? z.
? If z = re(y, x), y ? L(x), assume z ? xk.
z.pre = y.pre + y.sh + x.ins + x.re
We can construct a new derivation x? ? x by
appending x?s top tree, x.s0 to y?s stack, and
x?.pre = y.pre + y.sh + x.ins < z.pre
So x? ? z ? xk  x, which contradicts that x
is a best derivation of its state.
With induction we can easily show that any descen-
dants of x can not have a higher priority than xk.
We can now derive:
Theorem 1 (Stepwise Completeness and Optimal-
ity). For any k, 0 ? k ? m, we have the following
two properties:
?x ? xk, [x]? ? Ck?1 (Stepwise Completeness)
?x ? xk, xk  x (Stepwise Optimality)
763
Proof. We prove by induction on k.
1. For k = 0, these two properties trivially hold.
2. Assume this theorem holds for k = 2, ..., i?1.
For k = i, we have:
a) [Proof for Stepwise Completeness]
(Proof by Contradiction) Assume ?x ? xi
s.t. [x]? 6? Ci?1.Without loss of generality we
take a best derivation of state [x]? as x. x must
be derived from other best derivations only.
Consider this derivation transition hypergraph,
which starts at initial derivation x0 ? Ci?1, and
ends at x 6? Ci?1.
There must be a best derivation x? in this tran-
sition hypergraph, s.t. all best parent deriva-
tion(s) of x? are in Ci?1, but not x?.
If x? is a reduced derivation, assume x??s best
parent derivations are y ? Ci?1, z ? Ci?1.
Because y and z are best derivations, and they
are in Ci?1, from Stepwise Optimality on k =
1, ..., i? 1, y, z ? {x0, x1, . . . , xi?1}. From
Line 7-11 in Algorithm 1, x? must have been
pushed into Q when the latter of y, z is popped.
If x? is a shifted derivation, similarly x? must
have been pushed into Q .
As x? 6? Ci?1, x? must still be in Q when xi is
popped. However, from Lemma 1, none of x??s
descendants can have a higher priority than xi,
which contradicts x ? xi.
b) [Proof for Stepwise Optimality]
(Proof by Contradiction) Assume ?x ? xi
s.t. x ? xi. From Stepwise Completeness on
k = 1, ..., i, x ? Ci?1, which means the state
[xi]? has already been assigned to x, contra-
dicting the premise that xi is pushed into chart.
Both of the two properties have very intuitive
meanings. Stepwise Optimality means Algorithm 1
only fills chart with a best derivation for each state.
Stepwise Completeness means every state that has
its best derivation better than best derivation pi must
have been filled before pi, this guarantees that the
rex
?h??, h
?
k...i
? : (c?, v?) ?h?, h
i...j
? : ( , v)
?h??, h
k...j
? : (c? + v + ?, v? + v + ?)
Figure 4: Example of shift-reduce with dynamic pro-
gramming: simulating an edge-factored model. GSS
is implicit here, and rey case omitted. Here ? =
scsh(h??, h?) + screx(h
?, h).
global best goal derivation is captured by Algo-
rithm 1.
More formally we have:
Theorem 2 (Optimality of Algorithm 1). The first
goal derivation popped off the priority queue is the
optimal parse.
Proof. (Proof by Contradiction.) Assume ?x, x is
the a goal derivation and x ? xm. Based on Step-
wise Completeness of Theorem 1, x ? Cm?1, thus x
has already been popped out, which contradicts that
xm is the first popped out goal derivation.
Furthermore, we can see our lazy expansion ver-
sion, i.e., Algorithm 2, is also optimal. The key ob-
servation is that we delay the reduction of derivation
x? and a derivation of right states R([x?]?) (Line 11
of Algorithm 1), until shifted derivation, x = sh(x?),
is popped out (Line 11 of Algorithm 2). However,
this delayed reduction will not generate any deriva-
tion y, s.t. y ? x, because, based on our deduc-
tive system (Figure 2), for any such kind of reduced
derivations y, y.pre = x?.pre+x?.sh+y.re+y.ins ,
while x.pre = x?.pre + x?.sh .
5.2 Analysis of Time and Space Complexity
Following Huang and Sagae (2010) we present the
complexity analysis for our DP best-first parsing.
Theorem 3. Dynamic programming best-first pars-
ing runs in worst-case polynomial time and space,
as long as the atomic features function satisfies:
? bounded: ? derivation x, |?f(x)| is bounded by
a constant.
? monotonic:
764
? horizontal: ?k, fk(s) = fk(t) ?
fk+1(s) = fk+1(t), for all possible trees
s, t.
? vertical: ?k, fk(sys?) = fk(tyt?) ?
fk(s) = fk(t) and fk(sxs?) = fk(txt?)?
fk(s?) = fk(t?), for all possible trees s, s?,
t, t?.
In the above theorem, boundness means we can
only extract finite information from a derivation, so
that the atomic feature function f?(?) can only dis-
tinguish a finite number of different states. Mono-
tonicity requires the feature representation fk sub-
sumes fk+1. This is necessary because we use the
features as signature to match all possible left states
and right states (Equation 1). Note that we add the
vertical monotonicity condition following the sug-
gestion from Kuhlmann et al (2011), which fixes
a flaw in the original theorem of Huang and Sagae
(2010).
We use the edge-factored model (Eisner, 1996;
McDonald et al, 2005) with dynamic programming
described in Figure 4 as a concrete example for com-
plexity analysis. In the edge-factored model the fea-
ture set consists of only combinations of informa-
tion from the roots of the two top trees s1, s0, and
the queue. So the atomic feature function is
f?(p) = (i, j, h(p.s1), h(p.s0))
where h(s) returns the head word index of tree s.
The deductive system for the edge-factored model
is in Figure 4. The time complexity for this deduc-
tive system is O(n6), because we have three head
indexes and three span indexes as free variables in
the exploration. Compared to the work of Huang
and Sagae (2010), we reduce the time complexity
from O(n7) to O(n6) because we do not need to
keep track of the number of the steps for a state.
6 Experiments
In experiments we compare our DP best-first parsing
with non-DP best-first parsing, pure greedy parsing,
and beam parser of Huang and Sagae (2010).
Our underlying MaxEnt model is trained on the
Penn Treebank (PTB) following the standard split:
Sections 02-21 as the training set and Section 22 as
the held-out set. We collect gold actions at differ-
ent parsing configurations as positive examples from
model score accuracy # states time
greedy ?1.4303 90.08% 125.8 0.0055
beam? ?1.3302 90.60% 869.6 0.0331
non-DP ?1.3269 90.70% 4, 194.4 0.2622
DP ?1.3269 90.70% 243.2 0.0132
Table 1: Dynamic programming best-first parsing reach
optimality faster. *: for beam search we use beam size of
8. (All above results are averaged over the held-out set.)
gold parses in PTB to train the MaxEnt model. We
use the feature set of Huang and Sagae (2010).
Furthermore, we reimplemented the beam parser
with DP of Huang and Sagae (2010) for compari-
son. The result of our implementation is consistent
with theirs. We reach 92.39% accuracy with struc-
tured perceptron. However, in experiments we still
use MaxEnt to make the comparison fair.
To compare the performance we measure two sets
of criteria: 1) the internal criteria consist of the
model score of the parsing result, and the number
of states explored; 2) the external criteria consist of
the unlabeled accuracy of the parsing result, and the
parsing time.
We perform our experiments on a computer with
two 3.1GHz 8-core CPUs (16 processors in total)
and 64GB RAM. Our implementation is in Python.
6.1 Search Quality & Speed
We first compare DP best-first parsing algorithm
with pure greedy parsing and non-DP best-first pars-
ing without any extra constraints.
The results are shown in Table 1. Best-first pars-
ing reaches an accuracy of 90.70% in the held-out
set. Since that the MaxEnt model is locally trained,
this accuracy is not as high as the best shift-reduce
parsers available now. However, this is sufficient for
our comparison, because we aim at improving the
search quality and efficiency of parsing.
Compared to greedy parsing, DP best-first pars-
ing reaches a significantly higher accuracy, with ?2
times more parsing time. Given the extra time in
maintaining priority queue, this is consistent with
the internal criteria: DP best-first parsing reaches a
significantly higher model score, which is actually
optimal, exploring twice as many as states.
On the other hand, non-DP best-first parsing also
achieves the optimal model score and accuracy.
765
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0  10  20  30  40  50  60  70
a
vg
. p
ar
sin
g 
tim
e 
(se
cs
)
sentence length
non-DP
DP
beam
Figure 5: DP best-first significantly reduces parsing time.
Beam parser (beam size 8) guarantees linear parsing time.
Non-DP best-first parser is fast for short sentences, but
the time grows exponentially with sentence length. DP
best-first parser is as fast as non-DP for short sentences,
but the time grows significantly slower.
However, it explores?17 times more states than DP,
with an unbearable average time.
Furthermore, on average our DP best-first parsing
is significantly faster than the beam parser, because
most sentences are short.
Figure 5 explains the inefficiency of non-DP best-
first parsing. As the time complexity grows expo-
nentially with the sentence length, non-DP best-first
parsing takes an extremely long time for long sen-
tences. DP best-first search has a polynomial time
bound, which grows significantly slower.
In general DP best-first parsing manages to reach
optimality in tractable time with exact search. To
further investigate the potential of this DP best-
first parsing, we perform inexact search experiments
with bounded priority queue.
6.2 Parsing with Bounded Priority Queue
Bounded priority queue is a very practical choice
when we want to parse with only limited memory.
We bound the priority queue size at 1, 2, 5, 10,
20, 50, 100, 500, and 1000, and once the priority
queue size exceeds the bound, we discard the worst
one in the priority queue. The performances of non-
DP best-first parsing and DP best-first parsing are
illustrated in Figure 6 (a) (b).
Firstly, in Figure 6 (a), our DP best-first pars-
ing reaches the optimal model score with bound
50, while non-DP best-first parsing fails even with
bound 1000. Also, in average with bound 1000,
compared to non-DP, DP best-first only needs to ex-
plore less than half of the number of states.
Secondly, for external criteria in Figure 6 (b), both
algorithms reach accuracy of 90.70% in the end. In
speed, with bound 1000, DP best-first takes ?1/3
time of non-DP to parse a sentence in average.
Lastly, we also compare to beam parser with beam
size 1, 2, 4, 8. Figure 6 (a) shows that beam parser
fails to reach the optimality, while exploring signif-
icantly more states. On the other hand, beam parser
also fails to reach an accuracy as high as best-first
parsers. (see Figure 6 (b))
6.3 Simulating the Edge-Factored Model
We further explore the potential of DP best-first
parsing with the edge-factored model.
The simplified feature set of the edge-factored
model reduces the number of possible states, which
means more state-merging in the search graph. We
expect more significant improvement from our DP
best-first parsing in speed and number of explored
states.
Experiment results confirms this. In Figure 6 (c)
(d), curves of DP best-first diverge from non-DP
faster than standard model (Figure 6 (a) (b)).
7 Conclusions and Future Work
We have presented a dynamic programming algo-
rithm for best-first shift-reduce parsing which is
guaranteed to return the optimal solution in poly-
nomial time. This algorithm is related to best-first
Earley parsing, and is more sophisticated than best-
first CKY. Experiments have shown convincingly
that our algorithm leads to significant speedup over
the non-dynamic programming baseline, and makes
exact search tractable for the first-time under this
model.
For future work we would like to improve the per-
formance of the probabilistic models that is required
by the best-first search. We are also interested in
exploring A* heuristics to further speed up our DP
best-first parsing.
766
-1.45
-1.4
-1.35
-1.3
 0  100 200 300 400 500 600 700 800 900
a
v
g.
 m
od
el
 s
co
re
 o
n 
he
ld
-o
ut
# of states
bound=50 bound=1000
beam=8
non-DP
DP
beam
-1.3269
 90
 90.2
 90.4
 90.6
 90.8
 0  0.01  0.02  0.03  0.04  0.05
a
v
g.
 a
cc
ur
ac
y 
(%
) o
n 
he
ld
-o
ut
parsing time (secs)
bound=50
bound=1000
beam=8
non-DP
DP
beam
90.70
(a) search quality vs. # of states (b) parsing accuracy vs. time
-1.4
-1.36
-1.32
-1.28
-1.24
 0  100 200 300 400 500 600 700 800 900
a
v
g.
 m
od
el
 s
co
re
 o
n 
he
ld
-o
ut
# of states
bound=20 bound=500 beam=8
non-DP
DP
beam
-1.2565
 89.8
 90
 90.2
 0  0.01  0.02  0.03  0.04  0.05
a
v
g.
 a
cc
ur
ac
y 
(%
) o
n 
he
ld
-o
ut
parsing time (secs)
bound=20 bound=500
beam=8
non-DP
DP
beam
90.25
(c) search quality vs. # of states (edge-factored) (d) parsing accuracy vs. time (edge-factored)
Figure 6: Parsing performance comparison between DP and non-DP. (a) (b) Standard model with features of Huang
and Sagae (2010). (c) (d) Simulating edge-factored model with reduced feature set based on McDonald et al (2005).
Note that to implement bounded priority queue we use two priority queues to keep track of the worst elements, which
introduces extra overhead, so that our bounded parser is slower than the unbounded version for large priority queue
size bound.
767
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume I:
Parsing of Series in Automatic Computation. Prentice
Hall, Englewood Cliffs, New Jersey.
Sharon A Caraballo and Eugene Charniak. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. Computational Linguistics, 24(2):275?298.
Jay Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94?102.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
of COLING.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL 2010.
Dan Klein and Christopher D Manning. 2003. A* pars-
ing: Fast exact Viterbi parse selection. In Proceedings
of HLT-NAACL.
Dan Klein and Christopher D Manning. 2005. Pars-
ing and hypergraphs. In New developments in parsing
technology, pages 351?372. Springer.
Donald Knuth. 1977. A generalization of Dijkstra?s al-
gorithm. Information Processing Letters, 6(1).
Marco Kuhlmann, Carlos Go?mez-Rodr??guez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Proceed-
ings of ACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth?s algorithm. Computational Linguis-
tics, pages 135?143.
Joakim Nivre. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Linguis-
tics, 34(4):513?553.
Adam Pauls and Dan Klein. 2009. Hierarchical search
for parsing. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 557?565. Association for
Computational Linguistics.
Kenji Sagae and Alon Lavie. 2006. A best-first proba-
bilistic shift-reduce parser. In Proceedings of ACL.
Andreas Stolcke. 1995. An efficient probabilistic
context-free parsing algorithm that computes prefix
probabilities. Computational Linguistics, 21(2):165?
201.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-based
and transition-based dependency parsing using beam-
search. In Proceedings of EMNLP.
768
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 908?913,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Online Learning for Inexact Hypergraph Search
Hao Zhang
Google
haozhang@google.com
Liang Huang Kai Zhao
City University of New York
{lhuang@cs.qc,kzhao@gc}.cuny.edu
Ryan McDonald
Google
ryanmcd@google.com
Abstract
Online learning algorithms like the percep-
tron are widely used for structured predic-
tion tasks. For sequential search problems,
like left-to-right tagging and parsing, beam
search has been successfully combined with
perceptron variants that accommodate search
errors (Collins and Roark, 2004; Huang et
al., 2012). However, perceptron training with
inexact search is less studied for bottom-up
parsing and, more generally, inference over
hypergraphs. In this paper, we generalize
the violation-fixing perceptron of Huang et
al. (2012) to hypergraphs and apply it to the
cube-pruning parser of Zhang and McDonald
(2012). This results in the highest reported
scores on WSJ evaluation set (UAS 93.50%
and LAS 92.41% respectively) without the aid
of additional resources.
1 Introduction
Structured prediction problems generally deal with
exponentially many outputs, often making exact
search infeasible. For sequential search problems,
such as tagging and incremental parsing, beam
search coupled with perceptron algorithms that ac-
count for potential search errors have been shown
to be a powerful combination (Collins and Roark,
2004; Daume? and Marcu, 2005; Zhang and Clark,
2008; Huang et al, 2012). However, sequen-
tial search algorithms, and in particular left-to-right
beam search (Collins and Roark, 2004; Zhang and
Clark, 2008), squeeze inference into a very narrow
space. To address this, Huang (2008) formulated
constituency parsing as approximate bottom-up in-
ference in order to compactly represent an exponen-
tial number of outputs while scoring features of ar-
bitrary scope. This idea was adapted to graph-based
dependency parsers by Zhang and McDonald (2012)
and shown to outperform left-to-right beam search.
Both these examples, bottom-up approximate de-
pendency and constituency parsing, can be viewed
as specific instances of inexact hypergraph search.
Typically, the approximation is accomplished by
cube-pruning throughout the hypergraph (Chiang,
2007). Unfortunately, as the scope of features at
each node increases, the inexactness of search and
its negative impact on learning can potentially be ex-
acerbated. Unlike sequential search, the impact on
learning of approximate hypergraph search ? as well
as methods to mitigate any ill effects ? has not been
studied. Motivated by this, we develop online learn-
ing algorithms for inexact hypergraph search by gen-
eralizing the violation-fixing percepron of Huang et
al. (2012). We empirically validate the benefit of
this approach within the cube-pruning dependency
parser of Zhang and McDonald (2012).
2 Structured Perceptron for Inexact
Hypergraph Search
The structured perceptron algorithm (Collins, 2002)
is a general learning algorithm. Given training in-
stances (x, y?), the algorithm first solves the decod-
ing problem y? = argmaxy?Y(x)w ? f(x, y) given
the weight vector w for the high-dimensional fea-
ture representation f of the mapping (x, y), where
y? is the prediction under the current model, y? is the
gold output and Y(x) is the space of all valid outputs
for input x. The perceptron update rule is simply:
w? = w + f(x, y?) ? f(x, y?).
The convergence of original perceptron algorithm
relies on the argmax function being exact so that
the conditionw ?f(x, y?) > w ?f(x, y?) (modulo ties)
always holds. This condition is called a violation
because the prediction y? scores higher than the cor-
rect label y?. Each perceptron update moves weights
908
A B C D E F
G H I J
K L
M
N
Figure 1: A hypergraph showing the union of the gold
and Viterbi subtrees. The hyperedges in bold and dashed
are from the gold and Viterbi trees, respectively.
away from y? and towards y? to fix such violations.
But when search is inexact, y? could be suboptimal
so that sometimes w ? f(x, y?) < w ? f(x, y?). Huang
et al (2012) named such instances non-violations
and showed that perceptron model updates for non-
violations nullify guarantees of convergence. To ac-
count for this, they generalized the original update
rule to select an output y? within the pruned search
space that scores higher than y?, but is not necessar-
ily the highest among all possibilities, which repre-
sents a true violation of the model on that training
instance. This violation fixing perceptron thus re-
laxes the argmax function to accommodate inexact
search and becomes provably convergent as a result.
In the sequential cases where y? has a linear struc-
ture such as tagging and incremental parsing, the
violation fixing perceptron boils down to finding
and updating along a certain prefix of y?. Collins
and Roark (2004) locate the earliest position in a
chain structure where y?pref is worse than y?pref by
a margin large enough to cause y? to be dropped
from the beam. Huang et al (2012) locate the po-
sition where the violation is largest among all pre-
fixes of y?, where size of a violation is defined as
w ? f(x, y?pref) ? w ? f(x, y?pref).
For hypergraphs, the notion of prefix must be gen-
eralized to subtrees. Figure 1 shows the packed-
forest representation of the union of gold subtrees
and highest-scoring (Viterbi) subtrees at every gold
node for an input. At each gold node, there are
two incoming hyperedges: one for the gold subtree
and the other for the Viterbi subtree. After bottom-
up parsing, we can compute the scores for the gold
subtrees as well as extract the corresponding Viterbi
subtrees by following backpointers. These Viterbi
subtrees need not necessarily to belong to the full
Viterbi path (i.e., the Viterbi tree rooted at node N ).
An update strategy must choose a subtree or a set of
subtrees at gold nodes. This is to ensure that the
model is updating its weights relative to the inter-
section of the search space and the gold path.
Our first update strategy is called single-node
max-violation (s-max). Given a gold tree y?, it tra-
verses the gold tree and finds the node n on which
the violation between the Viterbi subtree and the
gold subtree is the largest over all gold nodes. The
violation is guaranteed to be greater than or equal to
zero because the lower bound for the max-violation
on any hypergraph is 0 which happens at the leaf
nodes. Then we choose the subtree pair (y?n, y?n) and
do the update similar to the prefix update for the se-
quential case. For example, in Figure 1, suppose the
max-violation happens at node K , which covers the
left half of the input x, then the perceptron update
would move parameters to the subtree represented
by nodes B , C , H and K and away from A ,
B , G and K .
Our second update strategy is called parallel max-
violation (p-max). It is based on the observation that
violations on non-overlapping nodes can be fixed
in parallel. We define a set of frontiers as a set
of nodes that are non-overlapping and the union of
which covers the entire input string x. The frontier
set can include up to |x| nodes, in the case where the
frontier is equivalent to the set of leaves. We traverse
y? bottom-up to compute the set of frontiers such
that each has the max-violation in the span it cov-
ers. Concretely, for each node n, the max-violation
frontier set can be defined recursively,
ft(n) =
{
n, if n = maxv(n)
?
ni?children(n) ft(ni), otherwise
where maxv(n) is the function that returns the node
with the absolute maximum violation in the subtree
rooted at n and can easily be computed recursively
over the hypergraph. To make a perceptron update,
we generate the max-violation frontier set for the en-
tire hypergraph and use it to choose subtree pairs
?
n?ft(root(x))(y?n, y
?
n), where root(x) is the root of
the hypergraph for input x. For example, in Figure 1,
if the union of K and L satisfies the definition of
ft, then the perceptron update would move feature
909
weights away from the union of the two Viterbi sub-
trees and towards their gold counterparts.
In our experiments, we compare the performance
of the two violation-fixing update strategies against
two baselines. The first baseline is the standard up-
date, where updates always happen at the root node
of a gold tree, even if the Viterbi tree at the root node
leads to a non-violation update. The second baseline
is the skip update, which also always updates at the
root nodes but skips any non-violations. This is the
strategy used by Zhang and McDonald (2012).
3 Experiments
We ran a number of experiments on the cube-
pruning dependency parser of Zhang and McDonald
(2012), whose search space can be represented as a
hypergraph in which the nodes are the complete and
incomplete states and the hyperedges are the instan-
tiations of the two parsing rules in the Eisner algo-
rithm (Eisner, 1996).
The feature templates we used are a superset of
Zhang and McDonald (2012). These features in-
clude first-, second-, and third-order features and
their labeled counterparts, as well as valency fea-
tures. In addition, we also included a feature tem-
plate from Bohnet and Kuhn (2012). This tem-
plate examines the leftmost child and the rightmost
child of a modifier simultaneously. All other high-
order features of Zhang and McDonald (2012) only
look at arcs on the same side of their head. We
trained the parser with hamming-loss-augmented
MIRA (Crammer et al, 2006), following Martins et
al. (2010). Based on results on the English valida-
tion data, in all the experiments, we trained MIRA
with 8 epochs and used a beam of size 6 per node.
To speed up the parser, we used an unlabeled
first-order model to prune unlikely dependency arcs
at both training and testing time (Koo and Collins,
2010; Martins et al, 2013). We followed Rush and
Petrov (2012) to train the first-order model to min-
imize filter loss with respect to max-marginal filter-
ing. On the English validation corpus, the filtering
model pruned 80% of arcs while keeping the oracle
unlabeled attachment score above 99.50%. During
training only, we insert the gold tree into the hy-
pergraph if it was mistakenly pruned. This ensures
that the gold nodes are always available, which is
required for model updates.
3.1 English and Chinese Results
We report dependency parsing results on the Penn
WSJ Treebank and the Chinese CTB-5 Treebank.
Both treebanks are constituency treebanks. We gen-
erated two versions of dependency treebanks by ap-
plying commonly-used conversion procedures. For
the first English version (PTB-YM), we used the
Penn2Malt1 software to apply the head rules of Ya-
mada and Matsumoto and the Malt label set. For
the second English version (PTB-S), we used the
Stanford dependency framework (De Marneffe et
al., 2006) by applying version 2.0.5 of the Stan-
ford parser. We split the data in the standard way:
sections 2-21 for training; section 22 for validation;
and section 23 for evaluation. We utilized a linear
chain CRF tagger which has an accuracy of 96.9%
on the validation data and 97.3% on the evaluation
data2. For Chinese, we use the Chinese Penn Tree-
bank converted to dependencies and split into train/-
validation/evaluation according to Zhang and Nivre
(2011). We report both unlabeled attachment scores
(UAS) and labeled attachment scores (LAS), ignor-
ing punctuations (Buchholz and Marsi, 2006).
Table 1 displays the results. Our improved
cube-pruned parser represents a significant improve-
ment over the feature-rich transition-based parser of
Zhang and Nivre (2011) with a large beam size. It
also improves over the baseline cube-pruning parser
without max-violation update strategies (Zhang and
McDonald, 2012), showing the importance of up-
date strategies in inexact hypergraph search. The
UAS score on Penn-YM is slightly higher than the
best result known in the literature which was re-
ported by the fourth-order unlabeled dependency
parser of Ma and Zhao (2012), although we did
not utilize fourth-order features. The LAS score on
Penn-YM is on par with the best reported by Bohnet
and Kuhn (2012). On Penn-S, there are not many
existing results to compare with, due to the tradition
of reporting results on Penn-YM in the past. Never-
theless, our result is higher than the second best by
a large margin. Our Chinese parsing scores are the
highest reported results.
1http://stp.lingfil.uu.se//?nivre/research/Penn2Malt.html
2The data was prepared by Andre? F. T. Martins as was done
in Martins et al (2013).
910
Penn-YM Penn-S CTB-5
Parser UAS LAS Toks/Sec UAS LAS Toks/Sec UAS LAS Toks/Sec
Zhang and Nivre (2011) 92.9- 91.8- ?680 - - - 86.0- 84.4- -
Zhang and Nivre (reimpl.) (beam=64) 93.00 91.98 800 92.96 90.74 500 85.93 84.42 700
Zhang and Nivre (reimpl.) (beam=128) 92.94 91.91 400 93.11 90.84 250 86.05 84.50 360
Koo and Collins (2010) 93.04 - - - - - - - -
Zhang and McDonald (2012) 93.06 91.86 220 - - - 86.87 85.19 -
Rush and Petrov (2012) - - - 92.7- - 4460 - - -
Martins et al (2013) 93.07 - 740 92.82 - 600 - - -
Qian and Liu (2013) 93.17 - 180 - - - 87.25 - 100
Bohnet and Kuhn (2012) 93.39 92.38 ?120 - - - 87.5- 85.9- -
Ma and Zhao (2012) 93.4- - - - - - 87.4- - -
cube-pruning w/ skip 93.21 92.07 300 92.92 90.35 200 86.95 85.23 200
w/ s-max 93.50 92.41 300 93.59 91.17 200 87.78 86.13 200
w/ p-max 93.44 92.33 300 93.64 91.28 200 87.87 86.24 200
Table 1: Parsing results on test sets of the Penn Treebank and CTB-5. UAS and LAS are measured on all tokens except
punctuations. We also include the tokens per second numbers for different parsers whenever available, although the
numbers from other papers were obtained on different machines. Speed numbers marked with ? were converted from
sentences per second.
The speed of our parser is around 200-300 tokens
per second for English. This is faster than the parser
of Bohnet and Kuhn (2012) which has roughly the
same level of accuracy, but is slower than the parser
of Martins et al (2013) and Rush and Petrov (2012),
both of which only do unlabeled dependency pars-
ing and are less accurate. Given that predicting la-
bels on arcs can slow down a parser by a constant
factor proportional to the size of the label set, the
speed of our parser is competitive. We also tried to
prune away arc labels based on observed labels for
each POS tag pair in the training data. By doing so,
we could speed up our parser to 500-600 tokens per
second with less than a 0.2% drop in both UAS and
LAS.
3.2 Importance of Update Strategies
The lower portion of Table 1 compares cube-pruning
parsing with different online update strategies in or-
der to show the importance of choosing an update
strategy that accommodates search errors. The max-
violation update strategies (s-max and p-max) im-
proved results on both versions of the Penn Treebank
as well as the CTB-5 Chinese treebank. It made
a larger difference on Penn-S relative to Penn-YM,
improving as much as 0.93% in LAS against the skip
update strategy. Additionally, we measured the per-
centage of non-violation updates at root nodes. In
the last epoch of training, on Penn-YM, there was
24% non-violations if we used the skip update strat-
egy; on Penn-S, there was 36% non-violations. The
portion of non-violations indicates the inexactness
 92
 92.2
 92.4
 92.6
 92.8
 93
 93.2
 93.4
 93.6
 93.8
 94
 1  2  3  4  5  6  7  8
UA
S
epochs
UAS on Penn-YM dev
s-max
p-max
skip
standard
Figure 2: Constrast of different update strategies on the
validation data set of Penn-YM. The x-axis is the number
of training epochs. The y-axis is the UAS score. s-max
stands for single-node max-violation. p-max stands for
parallel max-violation.
of the underlying search. Search is harder on Penn-S
due to the larger label set. Thus, as expected, max-
violation update strategies improve most where the
search is the hardest and least exact.
Figure 2 shows accuracy per training epoch on the
validation data. It can be seen that bad update strate-
gies are not simply slow learners. More iterations
of training cannot close the gap between strategies.
Forcing invalid updates on non-violations (standard
update) or simply ignoring them (skip update) pro-
duces less accurate models overall.
911
ZN 2011 (reimpl.) skip s-max p-max Best Published?
Language UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS
SPANISH 86.76 83.81 87.34 84.15 87.96 84.95 87.68 84.75 87.48 84.05
CATALAN 94.00 88.65 94.54 89.14 94.58 89.05 94.98 89.56 94.07 89.09
JAPANESE 93.10 91.57 93.40 91.65 93.26 91.67 93.20 91.49 93.72 91.7-
BULGARIAN 93.08 89.23 93.52 89.25 94.02 89.87 93.80 89.65 93.50 88.23
ITALIAN 87.31 82.88 87.75 83.41 87.57 83.22 87.79 83.59 87.47 83.50
SWEDISH 90.98 85.66 90.64 83.89 91.62 85.08 91.62 85.00 91.44 85.42
ARABIC 78.26 67.09 80.42 69.46 80.48 69.68 80.60 70.12 81.12 66.9-
TURKISH 76.62 66.00 76.18 65.90 76.94 66.80 76.86 66.56 77.55 65.7-
DANISH 90.84 86.65 91.40 86.59 91.88 86.95 92.00 87.07 91.86 84.8-
PORTUGUESE 91.18 87.66 91.69 88.04 92.07 88.30 92.19 88.40 93.03 87.70
GREEK 85.63 78.41 86.37 78.29 86.14 78.20 86.46 78.55 86.05 77.87
SLOVENE 84.63 76.06 85.01 75.92 86.01 77.14 85.77 76.62 86.95 73.4-
CZECH 87.78 82.38 86.92 80.36 88.36 82.16 88.48 82.38 90.32 80.2-
BASQUE 79.65 71.03 79.57 71.43 79.59 71.52 79.61 71.65 80.23 73.18
HUNGARIAN 84.71 80.16 85.67 80.84 85.85 81.02 86.49 81.67 86.81 81.86
GERMAN 91.57 89.48 91.23 88.34 92.03 89.44 91.79 89.28 92.41 88.42
DUTCH 82.49 79.71 83.01 79.79 83.57 80.29 83.35 80.09 86.19 79.2-
AVG 86.98 81.55 87.33 81.56 87.76 82.08 87.80 82.14
Table 2: Parsing Results for languages from CoNLL 2006/2007 shared tasks. When a language is in both years,
we use the 2006 data set. The best results with ? are the maximum in the following papers: Buchholz and Marsi
(2006), Nivre et al (2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al (2013), For
consistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script. ZN 2011 (reimpl.) is
our reimplementation of Zhang and Nivre (2011), with a beam of 64. Results in bold are the best among ZN 2011
reimplementation and different update strategies from this paper.
3.3 CoNLL Results
We also report parsing results for 17 languages from
the CoNLL 2006/2007 shared-task (Buchholz and
Marsi, 2006; Nivre et al, 2007). The parser in
our experiments can only produce projective depen-
dency trees as it uses an Eisner algorithm backbone
to generate the hypergraph (Eisner, 1996). So, at
training time, we convert non-projective trees ? of
which there are many in the CoNLL data ? to projec-
tive ones through flattening, i.e., attaching words to
the lowest ancestor that results in projective trees. At
testing time, our parser can only predict projective
trees, though we evaluate on the true non-projective
trees.
Table 2 shows the full results. We sort the
languages according to the percentage of non-
projective trees in increasing order. The Spanish
treebank is 98% projective, while the Dutch tree-
bank is only 64% projective. With respect to the
Zhang and Nivre (2011) baseline, we improved UAS
in 16 languages and LAS in 15 languages. The im-
provements are stronger for the projective languages
in the top rows. We achieved the best published
UAS results for 7 languages: Spanish, Catalan, Bul-
garain, Italian, Swedish, Danish, and Greek. As
these languages are typically from the more projec-
tive data sets, we speculate that extending the parser
used in this study to handle non-projectivity will
lead to state-of-the-art models for the majority of
languages.
4 Conclusions
We proposed perceptron update strategies for in-
exact hypergraph search and experimented with
a cube-pruning dependency parser. Both single-
node max-violation and parallel max-violation up-
date strategies signficantly improved parsing results
over the strategy that ignores any invalid udpates
caused by inexactness of search. The update strate-
gies are applicable to any bottom-up parsing prob-
lems such as constituent parsing (Huang, 2008) and
syntax-based machine translation with online learn-
ing (Chiang et al, 2008).
Acknowledgments: We thank Andre? F. T. Martins
for the dependency converted Penn Treebank with
automatic POS tags from his experiments; the re-
viewers for their useful suggestions; the NLP team
at Google for numerous discussions and comments;
Liang Huang and Kai Zhao are supported in part by
DARPA FA8750-13-2-0041 (DEFT), PSC-CUNY,
and a Google Faculty Research Award.
912
References
B. Bohnet and J. Kuhn. 2012. The best of bothworlds
- a graph-based completion model for transition-based
parsers. In Proc. of EACL.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
CoNLL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. of ACL.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. of ACL.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
H. Daume? and D. Marcu. 2005. Learning as search
optimization: Approximate large margin methods for
structured prediction. In Proc. of ICML.
M. De Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc. of LREC.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. of COL-
ING.
L. Huang, S. Fayong, and G. Yang. 2012. Structured
perceptron with inexact search. In Proc. of NAACL.
L. Huang. 2008. Forest reranking: Discriminative pars-
ing with non-local features. In Proc. of ACL.
T. Koo and M. Collins. 2010. Efficient third-order de-
pendency parsers. In Proc. of ACL.
X. Ma and H. Zhao. 2012. Fourth-order dependency
parsing. In Proc. of COLING.
A. F. T. Martins, N. Smith, E. P. Xing, P. M. Q. Aguiar,
and M. A. T. Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Proc. of EMNLP.
A. F. T. Martins, M. B. Almeida, and N. A. Smith. 2013.
Turning on the turbo: Fast third-order non-projective
turbo parsers. In Proc. of ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc. of
EMNLP-CoNLL.
X. Qian and Y. Liu. 2013. Branch and bound algo-
rithm for dependency parsing with non-local features.
TACL, Vol 1.
A. Rush and S. Petrov. 2012. Efficient multi-pass depen-
dency pruning with vine parsing. In Proc. of NAACL.
Y. Zhang and S. Clark. 2008. A Tale of Two
Parsers: Investigating and Combining Graph-based
and Transition-based Dependency Parsing. In Proc.
of EMNLP.
H. Zhang and R. McDonald. 2012. Generalized higher-
order dependency parsing with cube pruning. In Proc.
of EMNLP.
Y. Zhang and J. Nivre. 2011. Transition-based depen-
dency parsing with rich non-local features. In Proc. of
ACL-HLT, volume 2.
913
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1112?1123,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Max-Violation Perceptron and Forced Decoding for Scalable MT Training
Heng Yu1?
1Institute of Computing Tech.
Chinese Academy of Sciences
yuheng@ict.ac.cn
Liang Huang2? Haitao Mi3
2Queens College & Grad. Center
City University of New York
{huang@cs.qc,kzhao@gc}.cuny.edu
Kai Zhao2
3T.J. Watson Research Center
IBM
hmi@us.ibm.com
Abstract
While large-scale discriminative training has
triumphed in many NLP problems, its defi-
nite success on machine translation has been
largely elusive. Most recent efforts along this
line are not scalable (training on the small
dev set with features from top ?100 most fre-
quent words) and overly complicated. We in-
stead present a very simple yet theoretically
motivated approach by extending the recent
framework of ?violation-fixing perceptron?,
using forced decoding to compute the target
derivations. Extensive phrase-based transla-
tion experiments on both Chinese-to-English
and Spanish-to-English tasks show substantial
gains in BLEU by up to +2.3/+2.0 on dev/test
over MERT, thanks to 20M+ sparse features.
This is the first successful effort of large-scale
online discriminative training for MT.
1 Introduction
Large-scale discriminative training has witnessed
great success in many NLP problems such as pars-
ing (McDonald et al, 2005) and tagging (Collins,
2002), but not yet for machine translation (MT) de-
spite numerous recent efforts. Due to scalability is-
sues, most of these recent methods can only train
on a small dev set of about a thousand sentences
rather than on the full training set, and only with
2,000?10,000 rather ?dense-like? features (either
unlexicalized or only considering highest-frequency
words), as in MIRA (Watanabe et al, 2007; Chiang
et al, 2008; Chiang, 2012), PRO (Hopkins and May,
2011), and RAMP (Gimpel and Smith, 2012). How-
ever, it is well-known that the most important fea-
tures for NLP are lexicalized, most of which can not
?Work done while visiting City University of New York.
?Corresponding author.
be seen on a small dataset. Furthermore, these meth-
ods often involve complicated loss functions and
intricate choices of the ?target? derivations to up-
date towards or against (e.g. k-best/forest oracles, or
hope/fear derivations), and are thus hard to replicate.
As a result, the classical method of MERT (Och,
2003) remains the default training algorithm for MT
even though it can only tune a handful of dense fea-
tures. See also Section 6 for other related work.
As a notable exception, Liang et al (2006) do
train a structured perceptron model on the train-
ing data with sparse features, but fail to outperform
MERT. We argue this is because structured percep-
tron, like many structured learning algorithms such
as CRF and MIRA, assumes exact search, and search
errors inevitably break theoretical properties such as
convergence (Huang et al, 2012). Empirically, it
is now well accepted that standard perceptron per-
forms poorly when search error is severe (Collins
and Roark, 2004; Zhang et al, 2013).
To address the search error problem we propose a
very simple approach based on the recent framework
of ?violation-fixing perceptron? (Huang et al, 2012)
which is designed specifically for inexact search,
with a theoretical convergence guarantee and excel-
lent empirical performance on beam search pars-
ing and tagging. The basic idea is to update when
search error happens, rather than at the end of the
search. To adapt it to MT, we extend this framework
to handle latent variables corresponding to the hid-
den derivations. We update towards ?gold-standard?
derivations computed by forced decoding so that
each derivation leads to the exact reference transla-
tion. Forced decoding is also used as a way of data
selection, since those reachable sentence pairs are
generally more literal and of higher quality, which
the training should focus on. When the reachable
subset is small for some language pairs, we augment
1112
it by including reachable prefix-pairs when the full
sentence pair is not.
We make the following contributions:
1. Our work is the first successful effort to scale
online structured learning to a large portion of
the training data (as opposed to the dev set).
2. Our work is the first to use a principled learning
method customized for inexact search which
updates on partial derivations rather than full
ones in order to fix search errors. We adapt it
to MT using latent variables for derivations.
3. Contrary to the common wisdom, we show that
simply updating towards the exact reference
translation is helpful, which is much simpler
than k-best/forest oracles or loss-augmented
(e.g. hope/fear) derivations, avoiding sentence-
level BLEU scores or other loss functions.
4. We present a convincing analysis that it is the
search errors and standard perceptron?s inabil-
ity to deal with them that prevent previous
work, esp. Liang et al (2006), from succeed-
ing.
5. Scaling to the training data enables us to engi-
neer a very rich feature set of sparse, lexical-
ized, and non-local features, and we propose
various ways to alleviate overfitting.
For simplicity and efficiency reasons, in this paper
we use phrase-based translation, but our method has
the potential to be applicable to other translation
paradigms. Extensive experiments on both Chinese-
to-English and Spanish-to-English tasks show statis-
tically significant gains in BLEU by up to +2.3/+2.0
on dev/test over MERT, and up to +1.5/+1.5 over
PRO, thanks to 20M+ sparse features.
2 Phrase-Based MT and Forced Decoding
We first review the basic phrase-based decoding al-
gorithm (Koehn, 2004), which will be adapted for
forced decoding.
2.1 Background: Phrase-based Decoding
We will use the following running example from
Chinese to English from Mi et al (2008):
0 1 2 3 4 5 6
Figure 1: Standard beam-search phrase-based decoding.
Bu`sh??
Bush
yu?
with
Sha?lo?ng
Sharon
ju?x??ng
hold
le
-ed
hu?`ta?n
meeting
?Bush held a meeting with Sharon?
Phrase-based decoders generate partial target-
language outputs in left-to-right order in the form
of hypotheses (or states) (Koehn, 2004). Each hy-
pothesis has a coverage vector capturing the source-
language words translated so far, and can be ex-
tended into a longer hypothesis by a phrase-pair
translating an uncovered segment. For example, the
following is one possible derivation:
(0 ) : (0, ??)
(?1 ) : (s1, ?Bush?)
r1
(? ???6) : (s2, ?Bush held talks?)
r2
(???3???) : (s3, ?Bush held talks with Sharon?)
r3
where a ? in the coverage vector indicates the source
word at this position is ?covered? and where each
si is the score of each state, each adding the rule
score and the distortion cost (dc) to the score of the
previous state. To compute the distortion cost we
also need to maintain the ending position of the last
phrase (e.g., the 3 and 6 in the coverage vectors).
In phrase-based translation there is also a distortion-
limit which prohibits long-distance reorderings.
The above states are called?LM states since they
do not involve language model costs. To add a bi-
gram model, we split each ?LM state into a series
of +LM states; each +LM state has the form (v,a)
where a is the last word of the hypothesis. Thus a
+LM version of the above derivation might be:
(0 ,<s>) : (0, ?<s>?)
(?1 ,Bush) : (s?1, ?<s> Bush?)
r1
(? ???6,talks) : (s?2, ?<s> Bush held talks?)
r2
(???3???,Sharon) : (s?3, ?<s> Bush held ... with Sharon?)
r3
1113
0 1 2 3 4 5 6
Bush held
held talks
talks with
with Sharon
Sharon
Figure 2: Forced decoding and y-good derivation lattice.
where the score of applying each rule now also in-
cludes a combination cost due to the bigrams formed
when applying the phrase-pair, e.g.
s?3 = s?2 + s(r3) +dc(|6?3|)? logPlm(with | talk)
To make this exponential-time algorithm practi-
cal, beam search is the standard approximate search
method (Koehn, 2004). Here we group +LM states
into n bins, with each bin Bi hosting at most b states
that cover exactly i Chinese words (see Figure 1).
2.2 Forced Decoding
The idea of forced decoding is to consider only those
(partial) derivations that can produce (a prefix of)
the exact reference translation (assuming single ref-
erence). We call these partial derivations ?y-good?
derivations (Daume?, III and Marcu, 2005), and those
that deviate from the reference translation ?y-bad?
derivations. The forced decoding algorithm is very
similar to +LM decoding introduced above, with the
new ?forced decoding LM? to be defined as only
accepting two consecutive words on the reference
translation, ruling out any y-bad hypothesis:
Pforced (b | a) =
{
1 if ?j, s.t. a = yj and b = yj+1
0 otherwise
In the +LM state, we can simply replace the
boundary word by the index on the reference trans-
lation:
(0 ,0) : (0, ?<s>?)
(?1 ,1) : (w?1, ?<s> Bush?)
r1
(? ???6,3) : (w?2, ?<s> Bush held talks?)
r2
(???3???,5) : (w?3, ?<s> Bush held talks with Sharon?)
r3
The complexity of this forced decoding algorithm
is reduced to O(2nn3) where n is the source sen-
tence length, without the expensive bookkeeping for
English boundary words.
Lia?
nhe?
guo?
pa`i
qia?
n
50 gua?
nch
a?iy
ua?n
jia?n
du?
Bo?l
?`we?
iya`
hu??
fu`
m??n
zhu?
zhe`
ngz
h?`
y??la?
i
sho?
uc?`
qua?
ngu?
o
da`x
ua?n
P            
U.N.
 P           
sent
  P          
50
   P         
observers
             
to
    P        
monitor
          P  
the
          P  
1st
           PP
election
         P   
since
     P       
Bolivia
      P      
restored
       PP    
democracy
5
33
4
1
Figure 3: Example of unreachable sentence pair and
reachable prefix-pair. The first big jump is disallowed for
a distortion limit of 4, but we can still extract the top-left
box as a reachable prefix-pair. Note that this example is
perfectly reachable in syntax-based MT.
2.3 Reachable Prefix-Pairs
In practice, many sentence pairs in the parallel text
fail in forced decoding due to two reasons:
1. distortion limit: long-distance reorderings are
disallowed but are very common between lan-
guages with very different word orders such as
English and Chinese.
2. noisy alignment and phrase limit: the word-
alignment quality (typically from GIZA++) are
usually very noisy, which leads to unnecessar-
ily big chunks of rules beyond the phrase limit.
If we only rely on the reachable whole sentence
pairs, we will not be able to use much of the training
set for Chinese-English. So we propose to augment
the set of reachable examples by considering reach-
able prefix-pairs (see Figure 3 for an example).
3 Violation-Fixing Perceptron for MT
Huang et al (2012) establish a theoretical frame-
work called ?violation-fixing perceptron? which is
tailored for structured learning with inexact search
and has provable convergence properties. The high-
level idea is that standard full update does not fix
search errors; to do that we should instead up-
date when search error occurs, e.g., when the gold-
1114
standard derivation falls below the beam. Huang et
al. (2012) show dramatic improvements in the qual-
ity of the learned model using violation-fixing per-
ceptron (compared to standard perceptron) on incre-
mental parsing and part-of-speech tagging.
Since phrase-based decoding is also an incremen-
tal search problem which closely resembles beam-
search incremental parsing, it is very natural to em-
ploy violation-fixing perceptron here for MT train-
ing. Our goal is to produce the exact reference trans-
lation, or in other words, we want at least one y-good
derivation to survive in the beam search.
To adapt the violation-fixing perceptron frame-
work to MT we need to extend the framework
to handle latent variables since the gold-standard
derivation is not observed. This is done in a way
similar to the latent variable structured perceptron
(Zettlemoyer and Collins, 2005; Liang et al, 2006;
Sun et al, 2009) where each update is from the best
(y-bad) derivation towards the best y-good deriva-
tion in the current model; the latter is a constrained
search which is exactly forced decoding in MT.
3.1 Notations
We first establish some necessary notations. Let
?x, y? be a sentence pair in the training data, and
d = r1 ? r2 ? . . . ? r|d|
be a (partial) derivation, where each ri =
?c(ri), e(ri)? is a rule, i.e., a Chinese-English
phrase-pair. Let |c(d)| ?= ?i |c(ri)| be the num-
ber of Chinese words covered by this derivation, and
e(d) ?= e(r1) ? e(r2) . . . ? e(r|d|) be the English pre-
fix generated so far. Let D(x) be the set of all pos-
sible partial derivations translating part of the input
sentence x. Let pre(y) ?= {y[0:j] | 0 ? j ? |y|}
be the set of prefixes of the reference translation y,
and good i(x, y) be the set of partial y-good deriva-
tions whose English side is a prefix of the reference
translation y, and whose Chinese projection covers
exactly i words on the input sentence x, i.e.,
good i(x, y)
?
= {d ? D(x) | e(d)?pre(y), |c(d)|= i}.
Conversely, we define the set of y-bad partial deriva-
tions covering i Chinese words to be:
bad i(x, y) ?= {d ? D(x) | e(d) /?pre(y), |c(d)|= i}.
Basically, at each bin Bi, y-good derivations
good i(x, y) and y-bad ones bad i(x, y) compete for
the b slots in the bin:
B0 = {} (1)
Bi = topb
?
j=1..l
{d ? r | d ? Bi?j , |c(r)| = j} (2)
where r is a rule covering j Chinese words, l is
the phrase-limit, and topb S is a shorthand for
argtopbd?S w ? ?(x, d) which selects the top b
derivations according to the current model w.
3.2 Algorithm 1: Early Update
As a special case of violation-fixing perceptron,
early update (Collins and Roark, 2004) stops decod-
ing whenever the gold derivation falls off the beam,
makes an update on the prefix so far and move on
to the next example. We adapt it to MT as fol-
lows: if at a certain bin Bi, all y-good derivations
in good i(x, y) have fallen off the bin, then we stop
and update, rewarding the best y-good derivation in
good i(x, y) (with respect to current model w), and
penalizing the best y-bad derivation in the same step:
d+i (x, y)
?
= argmax
d?goodi(x,y)
w ??(x, d) (3)
d?i (x, y)
?
= argmax
d?badi(x,y)?Bi
w ??(x, d) (4)
w? w + ??(x, d+i (x, y), d?i (x, y)) (5)
where ??(x, d, d?) ?= ?(x, d)??(x, d?) is a short-
hand notation for the difference of feature vectors.
Note that the set good i(x, y) is independent of the
beam search and current model and is instead pre-
computed in the forced decoding phase, whereas the
negative signal d?i (x, y) depends on the beam.
In practice, however, there are exponentially
many y-good derivations for each reachable sen-
tence pair, and our goal is just to make sure (at least)
one y-good derivation triumphs at the end. So it
is possible that at a certain bin, all y-good partial
derivations fall off the bin, but the search can still
continue and produce the exact reference translation
through some other y-good path that avoids that bin.
For example, in Figure 1, the y-good states in steps
3 and 5 are not critical; it is totally fine to miss them
in the search as long as we save the y-good states
1115
ea
r
l
y
m
a
x
-
v
i
o
l
a
t
i
o
n
best in the beam
worst in the beam
d i
d+i d+i?
d i?
d+|x|
dy|x|
s
t
d
l
o
c
a
l
standard update 
is invalid
m
o
d
e
l
w
d |x|
Figure 4: Illustration of four update methods. The blue
paths denote (possibly lots of) gold-standard derivations
from forced decoding. Standard update in this case is
invalid as it reinforces the error of w (Huang et al, 2012).
in bins 1, 4 and 6. So we actually use a ?softer?
version of the early update algorithm: only stop and
update when there is no hope to continue. To be
more concrete, let l denote the phrase-limit then we
stop where there are l consecutive bins without any
y-good states, and update on the first among them.
3.3 Algorithm 2: Max-Violation Update
While early update learns substantially better mod-
els than standard perceptron in the midst of inex-
act search, it is also well-known to be converging
much slower than the latter, since each update is
on a (short) prefix. Huang et al (2012) propose an
improved method ?max-violation? which updates at
the worst mistake instead of the first, and converges
much faster than early update with similar or better
accuracy. We adopt this idea here as follows: decode
the whole sentence, and find the step i? where the
difference between the best y-good derivation and
the best y-bad one is the biggest. This amount of dif-
ference is called the amount of ?violation? in Huang
et al (2012), and the place of maximum violation is
intuitively the site of the biggest mistake during the
search. More formally, the update rule is:
i? ?= argmin
i
w ???(x, d+i (x, y), d?i (x, y)) (6)
w? w + ??(x, d+i?(x, y), d?i?(x, y)) (7)
3.4 Previous Work: Standard and Local Updates
We compare the above new update methods with the
two existing ones from Liang et al (2006).
Standard update (also known as ?bold update?
in Liang et al (2006)) simply updates at the very
end, from the best derivation in the beam towards the
best gold-standard derivation (regardless of whether
it survives the beam search):
w? w + ??(x, d+|x|(x, y), d
?
|x|(x, y)) (8)
Local update, however, updates towards the
derivation in the final bin that is most similar to the
reference y, denoted dy|x|(x, y):
dy|x|(x, y) = argmax
d?B|x|
Bleu+1(y, e(d)) (9)
w? w + ??(x, dy|x|(x, y), d
?
|x|(x, y))
(10)
where Bleu+1(?, ?) returns the sentence-level BLEU.
Liang et al (2006) observe that standard update
performs worse than local update, which they at-
tribute to the fact that the former often update to-
wards a gold derivation made up of ?unreasonable?
rules. Here we give a very different but theoreti-
cally more reasonable explanation based on the the-
ory of Huang et al (2012), who define an update
??(x, d+, d?) to be invalid if d+ scores higher
than d? (i.e., w ? ??(x, d+, d?) > 0, or update
?w points to the same direction as w in Fig. 4), in
which case there is no ?violation? or mistake to fix.
Perceptron is guaranteed to converge if all updates
are valid. Clearly, early and max-violation updates
are valid. But standard update is not: it is possible
that at the end of search, the best y-good derivation
d+|x|(x, y), though pruned earlier in the search, rankseven higher in the current model than anything in the
final bin (see Figure 4). In other words, there is no
mistake at the final step, while there must be some
search error in earlier steps which expels the y-good
subderivation. We will see in Section 5.3 that invalid
updates due to search errors are indeed the main rea-
son why standard update fails. Local update, how-
ever, is always valid in that definition.
Finally, it is worth noting that in terms of imple-
mentation, standard and max-violation are the easi-
est, while early update is more involved.
4 Feature Design
Our feature set includes the following 11 dense fea-
tures: LM, four conditional and lexical translation
probabilities (pc(e|f), pc(f |e), pl(e|f), pl(f |e)),
length and phrase penalties, distortion cost, and
three lexicalized reordering features. All these fea-
tures are inherited from Moses (Koehn et al, 2007).
1116
(?1 ,Bush) : (s?1, ?<s> Bush?)
(? ???6,talks) : (s?2, ?<s> Bush held talks?)
r2
</s>ju?x??ng le hu?`ta?nyu? Sha?lo?ngBu`sh??<s>
held talksBush<s>
r1 r2
features for applying r2 on span x[3:6]
WordEdges
c(r2)[0] = ju?x??ng, c(r2)[?1] = hu?`ta?n
e(r2)[0] = held, e(r2)[?1] = talks
x[2:3] = Sha?lo?ng, x[6:7] = </s>, |c(r2)| = 3
... (combos of the above atomic features) ...
non-local e(r0 ? r1)[?2:] ? id(r2)id(r1) ? id(r2)
Figure 5: Examples of WordEdges and non-local features. The notation uses the Python style subscript syntax.
4.1 Local Sparse Features: Ruleid & WordEdges
We first add the rule identification feature for each
rule: id(ri). We also introduce lexicalized Word-
Edges features, which are shown to be very effec-
tive in parsing (Charniak and Johnson, 2005) and
MT (Liu et al, 2008; He et al, 2008) literatures.
We use the following atomic features when apply-
ing a rule ri = ?c(ri), e(ri)?: the source-side length
|c(ri)|, the boundary words of both c(ri) and e(ri),
and the surrounding words of c(ri) on the input sen-
tence x. See Figure 5 for examples. These atomic
features are concatenated to generate all kinds of
combo features.
Chinese English class size budget
word 52.9k 64.2k 5
characters - 3.7k - 3
Brown cluster, full string 200 3
Brown cluster, prefix 6 6 8 2
Brown cluster, prefix 4 4 4 2
POS tag 52 36 2
word type - 4 - 1
Table 1: Various levels of backoff for WordEdges fea-
tures. Class size is estimated on the small Chinese-
English dataset (Sec. 5.3). The POS tagsets are ICT-
CLAS for Chinese (Zhang et al, 2003) and Penn Tree-
bank for English (Marcus et al, 1993).
4.2 Addressing Overfitting
With large numbers of lexicalized combo features
we will face the overfitting problem, where some
combo features found in the training data are too
rare to be seen in the test data. Thus we propose
three ways to alleviate this problem.
First, we introduce various levels of backoffs for
each word w (see Table 1). We include w?s Brown
cluster and its prefixes of lengths 4 and 6 (Brown et
al., 1992), and w?s part-of-speech tag. If w is Chi-
nese we also include its word type (punctuations,
digits, alpha, or otherwise) and (leftmost or right-
most) character. In such a way, we significantly in-
crease the feature coverage on unseen data.
However, if we allow arbitrary combinations, we
can extract a hexalexical feature (4 Chinese + 2 En-
glish words) for a local window in Figure 5, which
is unlikely to be seen at test time. To control model
complexity we introduce a feature budget for each
level of backoffs, shown in the last column in Ta-
ble 1. The total budget for a combo feature is the
sum of the budgets of all atomic features. In our ex-
periments, we only use the combo features with a
total budget of 10 or less, i.e., we can only include
bilexical but not trilexical features, and we can in-
clude for example combo features with one Chinese
word plus two English tags (total budget: 9).
Finally, we use two methods to alleviate overfit-
ting due to one-count rules: for large datasets, we
simply remove all one-count rules, but for small
datasets where out-of-vocabulary words (OOVs)
abound, we use a simple leave-one-out method:
when training on a sentence pair (x, y), do not use
the one-count rules extracted from (x, y) itself.
4.3 Non-Local Features
Following the success of non-local features in pars-
ing (Huang, 2008) and MT (Vaswani et al, 2011),
we also introduce them to capture the contextual in-
formation in MT. Our non-local features, shown in
Figure 5, include bigram rule-ids and the concatena-
tion of a rule id with the translation history, i.e. the
last two English words. Note that we also use back-
offs (Table 1) for the words included. Experiments
(Section 5.3) show that although the set of non-local
features is just a tiny fraction of all features, it con-
tributes substantially to the improvement in BLEU.
1117
Scale Language Training Data Reachability ?BLEU SectionsPair # sent. # words sent. words # feats # refs dev/test
small CH-EN 30K 0.8M/1.0M 21.4% 8.8% 7M 4 +2.2/2.0 5.2, 5.3large 230K 6.9M/8.9M 32.1% 12.7% 23M +2.3/2.0 5.2, 5.4
large SP-EN 174K 4.9M/4.3M 55.0% 43.9% 21M 1 +1.3/1.1 5.5
Table 2: Overview of all experiments. The ?BLEU column shows the absolute improvements of our method MAX-
FORCE on dev/test sets over MERT. The Chinese datasets also use prefix-pairs in training (see Table 3).
5 Experiments
In order to test our approach in different language
pairs, we conduct three experiments, shown in Ta-
ble 2, on two significantly different language pairs
(long vs. short distance reorderings), Chinese-to-
English (CH-EN) and Spanish-to-English (SP-EN).
5.1 System Preparation and Data
We base our experiments on Cubit, a state-of-art
phrase-based system in Python (Huang and Chiang,
2007).1 We set phrase-limit to 7 in rule extraction,
and beam size to 30 and distortion limit 6 in de-
coding. We compare our violation-fixing percep-
tron with two popular tuning methods: MERT (Och,
2003) and PRO (Hopkins and May, 2011).
For word alignments we use GIZA++-`0
(Vaswani et al, 2012) which produces sparser align-
ments, alleviating the garbage collection problem.
We use the SRILM toolkit (Stolcke, 2002) to train a
trigram language model with modified Kneser-Ney
smoothing on 1.5M English sentences.
Our dev and test sets for CH-EN task are from the
newswire portion of 2006 and 2008 NIST MT Eval-
uations (616/691 sentences, 18575/18875 words),
with four references.2 The dev and test sets for SP-
EN task are from newstest2012 and newstest2013,
with only one reference. Below both MERT and PRO
tune weights on the dev set, while our method on the
training set. Specifically, our method only uses the
dev set to know when to stop training.
5.2 Forced Decoding Reachability on Chinese
As mentioned in Section 2.2, we perform forced de-
coding to select reachable sentences from the train-
1http://www.cis.upenn.edu/?lhuang3/cubit/. We
will release the new version at http://acl.cs.qc.edu.
2We use the ?average? reference length to compute the
brevity penalty factor, which does not decrease with more ref-
erences unlike the ?shortest? heuristic.
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
 10  20  30  40  50  60  70
R
at
io
 o
f c
om
pl
et
e 
co
ve
ra
ge
Sentence length
dist-unlimited
dist-6
dist-4
dist-2
dist-0
Figure 6: Reachability ratio vs. sentence length on the
small CH-EN training set.
small large
sent. words sent. words
full 21.4% 8.8% 32.1% 12.7%
+prefix 61.3% 24.6% 67.3% 32.8%
Table 3: Ratio of sentence reachability and word cover-
age on the two CH-EN training data (distortion limit: 6).
ing data; this part is done with exact search with-
out any beam pruning. Figure 6 shows the reacha-
bility ratio vs. sentence length on the small CH-EN
training data, where the ratio decreases sharply with
sentence length, and increases with distortion limit.
We can see that there are a lot of long distance re-
orderings beyond small distortion limits. In the ex-
treme case of unlimited distortion, a large amount of
sentences will be reachable, but at the cost of much
slower decoding (O(n2V 2) in beam search decod-
ing, andO(2nn3) in forced decoding). In fact forced
decoding is too slow in the unlimited mode that we
only plot reachability for sentences up to 30 words.
Table 3 shows the statistics of forced decoding on
both small and large CH-EN training sets. In the
1118
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 100000
 5  10  15  20  25  30  35  40  45  50
A
ve
ra
ge
 n
um
be
r 
of
 d
er
iv
at
io
ns
Sentence length
dist-6
dist-4
dist-2
dist-0
Figure 7: Average number of derivations in gold lattices.
small data-set, 21.4% sentences are fully reachable
which only contains 8.8% words (since shorter sen-
tences are more likely to be reachable). Larger data
improves reachable ratios significantly thanks to bet-
ter alignment quality, but still only 12.7% words can
be used. In order to add more examples for per-
ceptron training, we pick all non-trivial reachable
prefix-pairs (with 5 or more Chinese words) as addi-
tional training examples (see Section 2.2). As shown
in Table 3, with prefix-pairs we can use about 1/4 of
small data and 1/3 of large data for training, which is
10x and 120x bigger than the 616-sentence dev set.
After running forced decoding, we obtain gold
translation lattice for each reachable sentence (or
prefix) pair. Figure 7 shows, as expected, the av-
erage number of gold derivations in these lattices
grows exponentially with sentence length.
5.3 Analysis on Small Chinese-English Data
Figure 8 shows the BLEU scores of different learn-
ing algorithms on the dev set. MAXFORCE3 per-
forms the best, peaking at iteration 13 while early
update learns much slower (the first few iterations
are faster than other methods due to early stopping
but this difference is immaterial later). The local and
standard updates, however, underperform MERT; in
particular, the latter gets worse as training goes on.
As analysized in Section 3.4, the reason why stan-
dard update (or ?bold update? in Liang et al (2006))
fails is that inexact search leads to many invalid up-
dates. This is confirmed by Figure 9, where more
3Stands for Max-Violation Perceptron w/ Forced Decoding
17
18
19
20
21
22
23
24
25
26
 2  4  6  8  10  12  14  16  18  20
B
LE
U
Number of iteration
Max
Forc
e
MERT
early
local
standard
Figure 8: BLEU scores on the heldout dev set for different
update methods (trained on small CH-EN data).
50%
60%
70%
80%
90%
 2  4  6  8  10 12 14 16 18 20 22 24 26 28 30
R
at
io
beam size
+non-local features
standard perceptron
Figure 9: Ratio of invalid updates in standard update.
than half of the updates remain invalid even at a
beam of 30. These analyses provide an alternative
but theoretically more reasonable explanation to the
findings of Liang et al (2006): while they blame
?unreasonable? gold derivations for the failure of
standard update, we observe that it is the search er-
rors that make the real difference, and that an up-
date that respects search errors towards a gold sub-
derivation is indeed helpful, even if that subderiva-
tion might be ?unreasonable?.
In order to speedup training, we use mini-batch
parallelization of Zhao and Huang (2013) which has
been shown to be much faster than previous paral-
lelization methods. We set the mini-batch size to
24 and train MAXFORCE with 1, 6, and 24 cores
on a small subset of the our original reachable sen-
1119
 22
 23
 24
 0  0.5  1  1.5  2  2.5  3  3.5  4
B
LE
U
Time
MERT PRO-dense
minibatch(24-core)
minibatch(6-core)
minibatch(1 core)
single processor
Figure 10: Minibatch parallelization speeds up learning.
10
12
14
16
18
20
22
24
26
 2  4  6  8  10  12  14  16
B
LE
U
Number of iteration
MaxForce
MERT
PRO-dense
PRO-medium
PRO-large
Figure 11: Comparison between different training meth-
ods. Ours trains the training set while others on dev set.
tences. The number of sentence pairs in this subset
is 1,032, which contains similar number of words to
our 616-sentence dev set (since reachable sentences
are much shorter). Thus, it is reasonable to compare
different learning algorithms in terms of speed and
performance. Figure 10 shows that first of all, mini-
batch improves BLEU even in the serial setting, and
when run on 24 cores, it leads to a speedup of about
7x. It is also interesting to know that on 1 CPU,
minibatch perceptron takes similar amount of time
to reach the same performance as MERT and PRO.
Figure 11 compares the learning curves of MAX-
FORCE, MERT, and PRO. We test PRO in three
different ways: PRO-dense (dense features only),
PRO-medium (dense features plus top 3K most fre-
18
19
20
21
22
23
24
25
26
 2  4  6  8  10  12  14  16
B
LE
U
Number of iteration
MERT
+non-local
+word-edges
+ruleid
dense
Figure 12: Incremental contributions of different feature
sets (dense features, ruleid, WordEdges, and non-local).
type count % BLEU
dense 11 - 22.3
+ruleid +9,264 +0.1% +0.8
+WordEdges +7,046,238 +99.5% +2.0
+non-local +22,536 +0.3% +0.7
all 7,074,049 100% 25.8
Table 4: Feature counts and incremental BLEU improve-
ments. MAXFORCE with all features is +2.2 over MERT.
quent sparse features4), and PRO-large (dense fea-
tures plus all sparse features). The results show that
PRO-dense performs almost the same as MERT but
with a stabler learning curve while PRO-medium im-
proves by +0.6. However, PRO-large decreases the
performance significantly, which indicates PRO is
not scalable to truly sparse features. By contrast,
our method handles large-scale sparse features well
and outperforms all other methods by a large margin
and with a stable learning curve.
We also investigate the individual contribution
from each group of features (ruleid, WordEdges, and
non-local features). So we perform experiments by
adding each group incrementally. Figure 12 shows
the learning curves and Table 4 lists the counts and
incremental contributions of different feature sets.
With dense features alone MAXFORCE does not do
4To prevent overfitting we remove all lexicalized features
and only use Brown clusters. It is difficult to engineer the right
feature set for PRO, whereas MAXFORCE is much more robust.
1120
system algorithm # feat. dev test
Moses MERT 11 25.5 22.5
Cubit
MERT 11 25.4 22.5
PRO
11 25.6 22.6
3K 26.3 23.0
36K 17.7 14.3
MAXFORCE 23M 27.8 24.5
Table 5: BLEU scores (with four references) using the
large CH-EN data. Our approach is +2.3/2.0 over MERT.
well because perceptron is known to suffer from fea-
tures of vastly different scales. Adding ruleid helps,
but still not enough. WordEdges (which is the vast
majority of features) improves BLEU by +2.0 points
and outperforms MERT, when sparse features totally
dominate dense features. Finally, the 0.3% non-local
features contribute a final +0.7 in BLEU.
5.4 Results on Large Chinese-English Data
Table 5 shows all BLEU scores for different learn-
ing algorithms on the large CH-EN data. The MERT
baseline on Cubit is essentially the same as Moses.
Our MAXFORCE activates 23M features on reach-
able sentences and prefixes in the training data, and
takes 35 hours to finish 15 iterations on 24 cores,
peaking at iteration 13. It achieves significant im-
provements over other approaches: +2.3/+2.0 points
over MERT and +1.5/+1.5 over PRO-medium on de-
v/test sets, respectively.
5.5 Results on Large Spanish-English Data
In SP-EN translation, we first run forced decod-
ing on the training set, and achieve a very high
reachability of 55% (with the same distortion limit
of 6), which is expected since the word order be-
tween Spanish and English are more similar than
than between Chinese and English, and most SP-
EN reorderings are local. Table 6 shows that MAX-
FORCE improves the translation quality over MERT
by +1.3/+1.1 BLEU on dev/test. These gains are
comparable to the improvements on the CH-EN task,
since it is well accepted in MT literature that a
change of ? in 1-reference BLEU is roughly equiva-
lent to a change of 2? with 4 references.
system algorithm # feat. dev test
Moses MERT 11 27.4 24.4
Cubit MAXFORCE 21M 28.7 25.5
Table 6: BLEU scores (with one reference) on SP-EN.
6 Related Work
Besides those discussed in Section 1, there are also
some research on tuning sparse features on the train-
ing data, but they integrate those sparse features into
the MT log-linear model as a single feature weight,
and tune its weight on the dev set (e.g. (Liu et al,
2008; He et al, 2008; Wuebker et al, 2010; Simi-
aner et al, 2012; Flanigan et al, 2013; Setiawan
and Zhou, 2013; He and Deng, 2012; Gao and He,
2013)). By contrast, our approach learns sparse fea-
tures only on the training set, and use dev set as held-
out to know when to stop.
Forced decoding has been used in the MT litera-
ture. For example, open source MT systems Moses
and cdec have implemented it. Liang et al (2012)
also use the it to boost the MERT tuning by adding
more y-good derivations to the standard k-best list.
7 Conclusions and Future Work
We have presented a simple yet effective approach
of structured learning for machine translation which
scales, for the first time, to a large portion of the
whole training data, and enables us to tune a rich set
of sparse, lexical, and non-local features. Our ap-
proach results in very significant BLEU gains over
MERT and PRO baselines. For future work, we will
consider other translation paradigms such as hierar-
chical phrase-based or syntax-based MT.
Acknowledgement
We thank the three anonymous reviewers for helpful sug-
gestions. We are also grateful to David Chiang, Dan
Gildea, Yoav Goldberg, Yifan He, Abe Ittycheriah, and
Hao Zhang for discussions, and Chris Callison-Burch,
Philipp Koehn, Lemao Liu, and Taro Watanabe for help
with datasets. Huang, Yu, and Zhao are supported by
DARPA FA8750-13-2-0041 (DEFT), a Google Faculty
Research Award, and a PSC-CUNY Award, and Mi by
DARPA HR0011-12-C-0015. Yu is also supported by the
China 863 State Key Project (No. 2011AA01A207). The
views and findings in this paper are those of the authors
and are not endorsed by the US or Chinese governments.
1121
References
Peter Brown, Peter Desouza, Robert Mercer, Vincent
Pietra, and Jenifer Lai. 1992. Class-based n-gram
models of natural language. Computational linguis-
tics, 18(4):467?479.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Ann Ar-
bor, Michigan, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159?1187.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Hal Daume?, III and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin meth-
ods for structured prediction. In Proceedings of ICML.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell. 2013.
Large-scale discriminative training for statistical ma-
chine translation using held-out line search. In Pro-
ceedings of NAACL 2013.
Jianfeng Gao and Xiaodong He. 2013. Training mrf-
based phrase translation models using gradient ascent.
In Proceedings of NAACL:HLT, pages 450?459, At-
lanta, Georgia, June.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proceedings of NAACL 2012.
Xiaodong He and Li Deng. 2012. Maximum expected
bleu training of phrase and lexicon translation models.
In Proceedings of ACL.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of COLING, pages
321?328, Manchester, UK, August.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proceed-
ings of NAACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
ACL: HLT, Columbus, OH, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of ACL:
Demonstrations.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of AMTA, pages 115?124.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative
approach to machine translation. In Proceedings of
COLING-ACL, Sydney, Australia, July.
Huashen Liang, Min Zhang, and Tiejun Zhao. 2012.
Forced decoding for minimum error rate training in
statistical machine translation. Journal of Computa-
tional Information Systems, (8):861868.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of EMNLP, pages 89?97, Honolulu,
Hawaii, October.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19:313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL.
Franz Joseph Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Hendra Setiawan and Bowen Zhou. 2013. Discrimi-
native training of 150 million translation parameters
and its application to pruning. In Proceedings of
NAACL:HLT, pages 335?341, Atlanta, Georgia, June.
ACL.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in SMT. In
Proceedings of ACL, Jeju Island, Korea.
1122
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP, vol-
ume 30, pages 901?904.
Xu Sun, Takuya Matsuzaki, and Daisuke Okanohara.
2009. Latent variable perceptron algorithm for struc-
tured classification. In Proceedings of IJCAI.
Ashish Vaswani, Haitao Mi, Liang Huang, and David
Chiang. 2011. Rule markov models for fast tree-to-
string translation. In Proceedings of ACL 2011, Port-
land, OR.
Ashish Vaswani, Liang Huang, and David Chiang. 2012.
Smaller Alignment Models for Better Translations:
Unsupervised Word Alignment with the L0-norm. In
Proceedings of ACL.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proceedings of EMNLP-
CoNLL.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of ACL, pages 475?484, Uppsala,
Sweden, July.
Luke Zettlemoyer and Michael Collins. 2005. Learning
to map sentences to logical form: Structured classifi-
cation with probabilistic categorial grammars. In Pro-
ceedings of UAI.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer ict-
clas. In Proceedings of the second SIGHAN workshop
on Chinese language processing, pages 184?187.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDon-
ald. 2013. Online learning with inexact hypergraph
search. In Proceedings of EMNLP 2013.
Kai Zhao and Liang Huang. 2013. Minibatch and paral-
lelization for online large margin structured learning.
In Proceedings of NAACL 2013.
1123
Proceedings of NAACL-HLT 2013, pages 370?379,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Minibatch and Parallelization for Online Large Margin Structured Learning
Kai Zhao1
1Computer Science Program, Graduate Center
City University of New York
kzhao@gc.cuny.edu
Liang Huang2,1
2Computer Science Dept, Queens College
City University of New York
huang@cs.qc.cuny.edu
Abstract
Online learning algorithms such as perceptron
and MIRA have become popular for many
NLP tasks thanks to their simpler architec-
ture and faster convergence over batch learn-
ing methods. However, while batch learning
such as CRF is easily parallelizable, online
learning is much harder to parallelize: previ-
ous efforts often witness a decrease in the con-
verged accuracy, and the speedup is typically
very small (?3) even with many (10+) pro-
cessors. We instead present a much simpler
architecture based on ?mini-batches?, which
is trivially parallelizable. We show that, un-
like previous methods, minibatch learning (in
serial mode) actually improves the converged
accuracy for both perceptron and MIRA learn-
ing, and when combined with simple paral-
lelization, minibatch leads to very significant
speedups (up to 9x on 12 processors) on state-
of-the-art parsing and tagging systems.
1 Introduction
Online structured learning algorithms such as the
structured perceptron (Collins, 2002) and k-best
MIRA (McDonald et al, 2005) have become more
and more popular for many NLP tasks such as de-
pendency parsing and part-of-speech tagging. This
is because, compared to their batch learning counter-
parts, online learning methods offer faster conver-
gence rates and better scalability to large datasets,
while using much less memory and a much simpler
architecture which only needs 1-best or k-best de-
coding. However, online learning for NLP typically
involves expensive inference on each example for 10
or more passes over millions of examples, which of-
ten makes training too slow in practice; for example
systems such as the popular (2nd-order) MST parser
(McDonald and Pereira, 2006) usually require the
order of days to train on the Treebank on a com-
modity machine (McDonald et al, 2010).
There are mainly two ways to address this scala-
bility problem. On one hand, researchers have been
developing modified learning algorithms that allow
inexact search (Collins and Roark, 2004; Huang et
al., 2012). However, the learner still needs to loop
over the whole training data (on the order of mil-
lions of sentences) many times. For example the
best-performing method in Huang et al (2012) still
requires 5-6 hours to train a very fast parser.
On the other hand, with the increasing popularity
of multicore and cluster computers, there is a grow-
ing interest in speeding up training via paralleliza-
tion. While batch learning such as CRF (Lafferty
et al, 2001) is often trivially parallelizable (Chu et
al., 2007) since each update is a batch-aggregate of
the update from each (independent) example, online
learning is much harder to parallelize due to the de-
pendency between examples, i.e., the update on the
first example should in principle influence the de-
coding of all remaining examples. Thus if we de-
code and update the first and the 1000th examples
in parallel, we lose their interactions which is one
of the reasons for online learners? fast convergence.
This explains why previous work such as the itera-
tive parameter mixing (IPM) method of McDonald
et al (2010) witnesses a decrease in the accuracies
of parallelly-learned models, and the speedup is typ-
ically very small (about 3 in their experiments) even
with 10+ processors.
We instead explore the idea of ?minibatch? for on-
line large-margin structured learning such as percep-
tron and MIRA. We argue that minibatch is advan-
tageous in both serial and parallel settings.
First, for minibatch perceptron in the serial set-
370
ting, our intuition is that, although decoding is done
independently within one minibatch, updates are
done by averaging update vectors in batch, provid-
ing a ?mixing effect? similar to ?averaged parame-
ters? of Collins (2002) which is also found in IPM
(McDonald et al, 2010), and online EM (Liang and
Klein, 2009).
Secondly, minibatch MIRA in the serial setting
has an advantage that, different from previous meth-
ods such as SGD which simply sum up the up-
dates from all examples in a minibatch, a minibatch
MIRA update tries to simultaneously satisfy an ag-
gregated set of constraints that are collected from
multiple examples in the minibatch. Thus each mini-
batch MIRA update involves an optimization over
many more constraints than in pure online MIRA,
which could potentially lead to a better margin. In
other words we can view MIRA as an online version
or stepwise approximation of SVM, and minibatch
MIRA can be seen as a better approximation as well
as a middleground between pure MIRA and SVM.1
More interestingly, the minibatch architecture is
trivially parallelizable since the examples within
each minibatch could be decoded in parallel on mul-
tiple processors (while the update is still done in se-
rial). This is known as ?synchronous minibatch?
and has been explored by many researchers (Gim-
pel et al, 2010; Finkel et al, 2008), but all previ-
ous works focus on probabilistic models along with
SGD or EM learning methods while our work is the
first effort on large-margin methods.
We make the following contributions:
? Theoretically, we present a serial minibatch
framework (Section 3) for online large-margin
learning and prove the convergence theorems
for minibatch perceptron and minibatch MIRA.
? Empirically, we show that serial minibatch
could speed up convergence and improve the
converged accuracy for both MIRA and percep-
tron on state-of-the-art dependency parsing and
part-of-speech tagging systems.
? In addition, when combined with simple (syn-
chronous) parallelization, minibatch MIRA
1This is similar to Pegasos (Shalev-Shwartz et al, 2007) that
applies subgradient descent over a minibatch. Pegasos becomes
pure online when the minibatch size is 1.
Algorithm 1 Generic Online Learning.
Input: dataD = {(x(t), y(t))}nt=1 and feature map ?
Output: weight vector w
1: repeat
2: for each example (x, y) in D do
3: C ? FINDCONSTRAINTS(x, y,w) . decoding
4: if C 6= ? then UPDATE(w, C)
5: until converged
leads to very significant speedups (up to 9x on
12 processors) that are much higher than that of
IPM (McDonald et al, 2010) on state-of-the-art
parsing and tagging systems.
2 Online Learning: Perceptron and MIRA
We first present a unified framework for online
large-margin learning, where perceptron and MIRA
are two special cases. Shown in Algorithm 1, the
online learner considers each input example (x, y)
sequentially and performs two steps:
1. find the set C of violating constraints, and
2. update the weight vector w according to C.
Here a triple ?x, y, z? is said to be a ?violating con-
straint? with respect to model w if the incorrect la-
bel z scores higher than (or equal to) the correct
label y in w, i.e., w ? ??(?x, y, z?) ? 0, where
??(?x, y, z?) is a short-hand notation for the up-
date vector ?(x, y) ? ?(x, z) and ? is the feature
map (see Huang et al (2012) for details). The sub-
routines FINDCONSTRAINTS and UPDATE are anal-
ogous to ?APIs?, to be specified by specific instances
of this online learning framework. For example, the
structured perceptron algorithm of Collins (2002)
is implemented in Algorithm 2 where FINDCON-
STRAINTS returns a singleton constraint if the 1-best
decoding result z (the highest scoring label accord-
ing to the current model) is different from the true
label y. Note that in the UPDATE function, C is al-
ways a singleton constraint for the perceptron, but
we make it more general (as a set) to handle the
batch update in the minibatch version in Section 3.
On the other hand, Algorith 3 presents the k-best
MIRA Algorithm of McDonald et al (2005) which
generalizes multiclass MIRA (Crammer and Singer,
2003) for structured prediction. The decoder now
371
Algorithm 2 Perceptron (Collins, 2002).
1: function FINDCONSTRAINTS(x, y,w)
2: z ? argmaxs?Y(x) w ??(x, s) . decoding
3: if z 6= y then return {?x, y, z?}
4: else return ?
5: procedure UPDATE(w, C)
6: w? w + 1|C|
?
c?C ??(c) . (batch) update
Algorithm 3 k-best MIRA (McDonald et al, 2005).
1: function FINDCONSTRAINTS(x, y,w)
2: Z ? k-bestz?Y(x)w ??(x, z)
3: Z ? {z ? Z | z 6= y,w ???(?x, y, z?) ? 0}
4: return {(?x, y, z?, `(y, z)) | z ? Z}
5: procedure UPDATE(w, C)
6: w? argmin
w?:?(c,`)?C, w????(c)?`
?w? ?w?2
finds the k-best solutions Z first, and returns a set
of violating constraints in Z, The update in MIRA
is more interesting: it searches for the new model
w? with minimum change from the current model
w so that w? corrects each violating constraint by
a margin at least as large as the loss `(y, z) of the
incorrect label z.
Although not mentioned in the pseudocode, we
also employ ?averaged parameters? (Collins, 2002)
for both perceptron and MIRA in all experiments.
3 Serial Minibatch
The idea of serial minibatch learning is extremely
simple: divide the data into dn/me minibatches
of size m, and do batch updates after decoding
each minibatch (see Algorithm 4). The FIND-
CONSTRAINTS and UPDATE subroutines remain un-
changed for both perceptron and MIRA, although
it is important to note that a perceptron batch up-
date uses the average of update vectors, not the sum,
which simplifies the proof. This architecture is of-
ten called ?synchronous minibatch? in the literature
(Gimpel et al, 2010; Liang and Klein, 2009; Finkel
et al, 2008). It could be viewed as a middleground
between pure online learning and batch learning.
3.1 Convergence of Minibatch Perceptron
We denote C(D) to be the set of all possible violat-
ing constraints in data D (cf. Huang et al (2012)):
C(D) = {?x, y, z? | (x, y) ? D, z ? Y(x)? {y}}.
Algorithm 4 Serial Minibatch Online Learning.
Input: data D, feature map ?, and minibatch size m
Output: weight vector w
1: Split D into dn/me minibatches D1 . . . Ddn/me
2: repeat
3: for i? 1 . . . dn/me do . for each minibatch
4: C ? ?(x,y)?DiFINDCONSTRAINTS(x, y,w)
5: if C 6= ? then UPDATE(w, C) . batch update
6: until converged
A training set D is separable by feature map ?
with margin ? > 0 if there exists a unit oracle vec-
tor u with ?u? = 1 such that u ???(?x, y, z?) ? ?,
for all ?x, y, z? ? C(D). Furthermore, let radius
R ? ???(?x, y, z?)? for all ?x, y, z? ? C(D).
Theorem 1. For a separable datasetD with margin
? and radius R, the minibatch perceptron algorithm
(Algorithms 4 and 2) will terminate after tminibatch
updates where t ? R2/?2.
Proof. Let wt be the weight vector before the tth
update; w0 = 0. Suppose the tth update happens
on the constraint set Ct = {c1, c2, . . . , ca} where
a = |Ct|, and each ci = ?xi, yi, zi?. We convert
them to the set of update vectors vi = ??(ci) =
??(?xi, yi, zi?) for all i. We know that:
1. u ? vi ? ? (margin on unit oracle vector)
2. wt ? vi ? 0 (violation: zi dominates yi)
3. ?vi?2 ? R2 (radius)
Now the update looks like
wt+1 = wt +
1
|Ct|
?
c?Ct
??(c) = wt +
1
a
?
i vi.
(1)
We will bound ?wt+1? from two directions:
1. Dot product both sides of the update equa-
tion (1) with the unit oracle vector u, we have
u ?wt+1 = u ?wt +
1
a
?
i u ? vi
? u ?wt +
1
a
?
i ? (margin)
= u ?wt + ? (
?
i = a)
? t? (by induction)
372
Since for any two vectors a and b we have
?a??b? ? a?b, thus ?u??wt+1? ? u?wt+1 ?
t?. As u is a unit vector, we have ?wt+1? ? t?.
2. On the other hand, take the norm of both sides
of Eq. (1):
?wt+1?2 = ?wt +
1
a
?
i vi?
2
=?wt?2 + ?
?
i
1
avi?
2 +
2
a
wt ?
?
i vi
??wt?2 + ?
?
i
1
avi?
2 + 0 (violation)
??wt?2 +
?
i
1
a?vi?
2 (Jensen?s)
??wt?2 +
?
i
1
aR
2 (radius)
=?wt?2 +R2 (
?
i = a)
?tR2 (by induction)
Combining the two bounds, we have
t2?2 ? ?wt+1?2 ? tR2
thus the number of minibatch updates t ? R2/?2.
Note that this bound is identical to that of pure
online perceptron (Collins, 2002, Theorem 1) and is
irrelevant to minibatch size m. The use of Jensen?s
inequality is inspired by McDonald et al (2010).
3.2 Convergence of Minibatch MIRA
We also give a proof of convergence for MIRA with
relaxation.2 We present the optimization problem in
the UPDATE function of Algorithm 3 as a quadratic
program (QP) with slack variable ?:
wt+1 ?argmin
wt+1
?wt+1 ?wt?2 + ?
s.t. wt+1 ? vi ? `i ? ?, for all(ci, `i) ? Ct
where vi = ??(ci) is the update vector for con-
straint ci. Consider the Lagrangian:
L =?wt+1 ?wt?2 + ? +
|Ct|?
i=1
?i(`i ?w? ? vi ? ?)
?i ? 0, for 1 ? i ? |Ct|.
2Actually this relaxation is not necessary for the conver-
gence proof. We employ it here solely to make the proof shorter.
It is not used in the experiments either.
Set the partial derivatives to 0 with respect to w? and
? we have:
w? = w +
?
i ?ivi (2)
?
i ?i = 1 (3)
This result suggests that the weight change can al-
ways be represnted by a linear combination of the
update vectors (i.e. normal vectors of the constraint
hyperplanes), with the linear coefficencies sum to 1.
Theorem 2 (convergence of minibatch MIRA). For
a separable dataset D with margin ? and radius R,
the minibatch MIRA algorithm (Algorithm 4 and 3)
will make t updates where t ? R2/?2.
Proof. 1. Dot product both sides of Equation 2
with unit oracle vector u:
u ?wt+1 = u ?wt +
?
i ?iu ? vi
?u ?wt +
?
i ?i? (margin)
=u ?wt + ? (Eq. 3)
=t? (by induction)
2. On the other hand
?wt+1?2 = ?wt +
?
i ?ivi?
2
=?wt?2 + ?
?
i ?ivi?
2 + 2 wt ?
?
i ?ivi
??wt?2 + ?
?
i ?ivi?
2 + 0 (violation)
??wt?2 +
?
i ?iv
2
i (Jensen?s)
??wt?2 +
?
i ?iR
2 (radius)
=?wt?2 +R2 (Eq. 3)
?tR2 (by induction)
From the two bounds we have:
t2?2 ? ?wt+1?2 ? tR2
thus within at most t ? R2/?2 minibatch up-
dates MIRA will converge.
4 Parallelized Minibatch
The key insight into parallelization is that the calcu-
lation of constraints (i.e. decoding) for each exam-
ple within a minibatch is completely independent of
373
update
1
3
4
2
update
update
update
6
5
8
7
update
update
update
update
12
9
10
11
update
update
update
update
15
14
13
16
update
update
update
update
?
update
3
1
4
6
5
8
7
2
12
15
14
9
13
16
10
11
?
update
?
3
1
4
6
5
8
7
2
12
15
14
9
13
16
10
11
update
?
update
?
update
1
2
3
4
6
5
8
7
update
update
9
10
update
12
11
14
13
15
16
update
update
update
update
(a) IPM (b) unbalanced (c) balanced (d) asynchronous
Figure 1: Comparison of various methods for parallelizing online learning (number of processors p = 4). (a) iterative
parameter mixing (McDonald et al, 2010). (b) unbalanced minibatch parallelization (minibatch size m = 8). (c)
minibatch parallelization after load-balancing (within each minibatch). (d) asynchronous minibatch parallelization
(Gimpel et al, 2010) (not implemented here). Each numbered box denotes the decoding of one example, and ?
denotes an aggregate operation, i.e., the merging of constraints after each minibatch or the mixing of weights after
each iteration in IPM. Each gray shaded box denotes time wasted due to synchronization in (a)-(c) or blocking in (d).
Note that in (d) at most one update can happen concurrently, making it substantially harder to implement than (a)-(c).
Algorithm 5 Parallized Minibatch Online Learning.
Input: D, ?, minibatch sizem, and # of processors p
Output: weight vector w
Split D into dn/me minibatches D1 . . . Ddn/me
Split each Di into m/p groups Di,1 . . . Di,m/p
repeat
for i? 1 . . . dn/me do . for each minibatch
for j ? 1 . . .m/p in parallel do
Cj ? ?(x,y)?Di,j FINDCONSTRAINTS(x, y,w)
C ? ?jCj . in serial
if C 6= ? then UPDATE(w, C) . in serial
until converged
other examples in the same batch. Thus we can eas-
ily distribute decoding for different examples in the
same minibatch to different processors.
Shown in Algorithm 5, for each minibatchDi, we
split Di into groups of equal size, and assign each
group to a processor to decode. After all processors
finish, we collect all constraints and do an update
based on the union of all constraints. Figure 1 (b) il-
lustrates minibatch parallelization, with comparison
to iterative parameter mixing (IPM) of McDonald et
al. (2010) (see Figure 1 (a)).
This synchronous parallelization framework
should provide significant speedups over the serial
mode. However, in each minibatch, inevitably,
some processors will end up waiting for others to
finish, especially when the lengths of sentences vary
substantially (see the shaded area in Figure 1 (b)).
To alleviate this problem, we propose ?per-
minibatch load-balancing?, which rearranges the
sentences within each minibatch based on their
lengths (which correlate with their decoding times)
so that the total workload on each processor is bal-
anced (Figure 1c). It is important to note that this
shuffling does not affect learning at all thanks to the
independence of each example within a minibatch.
Basically, we put the shortest and longest sentences
into the first thread, the second shortest and second
longest into the second thread, etc. Although this is
not necessary optimal scheduling, it works well in
practice. As long as decoding time is linear in the
length of sentence (as in incremental parsing or tag-
ging), we expect a much smaller variance in process-
ing time on each processor in one minibatch, which
is confirmed in the experiments (see Figure 8).3
3In IPM, however, the waiting time is negligible, since the
workload on each processor is almost balanced, analogous to
a huge minibatch (Fig. 1a). Furthermore, shuffling does affect
learning here since each thread in IPM is a pure online learner.
So our IPM implementation does not use load-balancing.
374
5 Experiments
We conduct experiments on two typical structured
prediction problems: incremental dependency pars-
ing and part-of-speech tagging; both are done on
state-of-the-art baseline. We also compare our
parallelized minibatch algorithm with the iterative
parameter mixing (IPM) method of McDonald et
al. (2010). We perform our experiments on a
commodity 64-bit Dell Precision T7600 worksta-
tion with two 3.1GHz 8-core CPUs (16 processors
in total) and 64GB RAM. We use Python 2.7?s
multiprocessing module in all experiments.4
5.1 Dependency Parsing with MIRA
We base our experiments on our dynamic program-
ming incremental dependency parser (Huang and
Sagae, 2010).5 Following Huang et al (2012), we
use max-violation update and beam size b = 8. We
evaluate on the standard Penn Treebank (PTB) us-
ing the standard split: Sections 02-21 for training,
and Section 22 as the held-out set (which is indeed
the test-set in this setting, following McDonald et
al. (2010) and Gimpel et al (2010)). We then ex-
tend it to employ 1-best MIRA learning. As stated
in Section 2, MIRA separates the gold label y from
the incorrect label z with a margin at least as large
as the loss `(y, z). Here in incremental dependency
parsing we define the loss function between a gold
tree y and an incorrect partial tree z as the number
of incorrect edges in z, plus the number of correct
edges in y which are already ruled out by z. This
MIRA extension results in slightly higher accuracy
of 92.36, which we will use as the pure online learn-
ing baseline in the comparisons below.
5.1.1 Serial Minibatch
We first run minibatch in the serial mode with
varying minibatch size of 4, 16, 24, 32, and 48 (see
Figure 2). We can make the following observations.
First, except for the largest minibatch size of 48,
minibatch learning generally improves the accuracy
4We turn off garbage-collection in worker processes oth-
erwise their running times will be highly unbalanced. We also
admit that Python is not the best choice for parallelization, e.g.,
asychronous minibatch (Gimpel et al, 2010) requires ?shared
memory? not found in the current Python (see also Sec. 6).
5Available at http://acl.cs.qc.edu/. The version
with minibatch parallelization will be available there soon.
 90.75
 91
 91.25
 91.5
 91.75
 92
 92.25
 92.5
 0  1  2  3  4  5  6  7  8
a
c
c
u
ra
c
y 
on
 h
el
d-
ou
t
wall-clock time (hours)
m=1
m=4
m=16
m=24
m=32
m=48
Figure 2: Minibatch with various minibatch sizes (m =
4, 16, 24, 32, 48) for parsing with MIRA, compared to
pure MIRA (m = 1). All curves are on a single CPU.
of the converged model, which is explained by our
intuition that optimization with a larger constraint
set could improve the margin. In particular, m = 16
achieves the highest accuracy of 92.53, which is a
0.27 improvement over the baseline.
Secondly, minibatch learning can reach high lev-
els of accuracy faster than the baseline can. For ex-
ample, minibatch of size 4 can reach 92.35 in 3.5
hours, and minibatch of size 24 in 3.7 hours, while
the pure online baseline needs 6.9 hours. In other
words, just minibatch alone in serial mode can al-
ready speed up learning. This is also explained by
the intuition of better optimization above, and con-
tributes significantly to the final speedup of paral-
lelized minibatch.
Lastly, larger minibatch sizes slow down the con-
vergence, with m = 4 converging the fastest and
m = 48 the slowest. This can be explained by the
trade-off between the relative strengths from online
learning and batch update: with larger batch sizes,
we lose the dependencies between examples within
the same minibatch.
Although larger minibatches slow down conver-
gence, they actually offer better potential for paral-
lelization since the number of processors p has to be
smaller than minibatch size m (in fact, p should di-
vide m). For example, m = 24 can work with 2, 3,
4, 6, 8, or 12 processors while m = 4 can only work
with 2 or 4 and the speed up of 12 processors could
easily make up for the slightly slower convergence
375
 91.4
 91.6
 91.8
 92
 92.2
 92.4
 0  1  2  3  4  5  6  7  8
a
c
c
u
ra
c
y
baseline
m=24,p=1
m=24,p=4
m=24,p=12
 91.4
 91.6
 91.8
 92
 92.2
 92.4
 0  1  2  3  4  5  6  7  8
a
c
c
u
ra
c
y
wall-clock time (hours)
baseline
IPM,p=4
IPM,p=12
Figure 3: Parallelized minibatch is much faster than iter-
ative parameter mixing. Top: minibatch of size 24 using
4 and 12 processors offers significant speedups over the
serial minibatch and pure online baselines. Bottom: IPM
with the same processors offers very small speedups.
rate. So there seems to be a ?sweetspot? of mini-
batch sizes, similar to the tipping point observed in
McDonald et al (2010) when adding more proces-
sors starts to hurt convergence.
5.1.2 Parallelized Minibatch vs. IPM
In the following experiments we use minibatch
size of m = 24 and run it in parallel mode on vari-
ous numbers of processors (p = 2 ? 12). Figure 3
(top) shows that 4 and 12 processors lead to very
significant speedups over the serial minibatch and
pure online baselines. For example, it takes the 12
processors only 0.66 hours to reach an accuracy of
92.35, which takes the pure online MIRA 6.9 hours,
amounting to an impressive speedup of 10.5.
We compare our minibatch parallelization with
the iterative parameter mixing (IPM) of McDonald
et al (2010). Figure 3 (bottom) shows that IPM not
only offers much smaller speedups, but also con-
verges lower, and this drop in accuracy worsens with
more processors.
Figure 4 gives a detailed analysis of speedups.
Here we perform both extrinsic and intrinsic com-
parisons. In the former, we care about the time to
reach a given accuracy; in this plot we use 92.27
which is the converged accuracy of IPM on 12 pro-
cessors. We choose it since it is the lowest accu-
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 2  4  6  8  10  12
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
sp
ee
du
ps
number of processors
minibatch(extrinsic)
minibatch(intrinsic)
IPM(extrinsic)
IPM(intrinsic)
Figure 4: Speedups of minibatch parallelization vs. IPM
on 1 to 12 processors (parsing with MIRA). Extrinsic
comparisons use ?the time to reach an accuracy of 92.27?
for speed calculations, 92.27 being the converged accu-
racy of IPM using 12 processors. Intrinsic comparisons
use average time per iteration regardless of accuracy.
racy among all converged models; choosing a higher
accuracy would reveal even larger speedups for our
methods. This figure shows that our method offers
superlinear speedups with small number of proces-
sors (1 to 6), and almost linear speedups with large
number of processors (8 and 12). Note that even
p = 1 offers a speedup of 1.5 thanks to serial mini-
batch?s faster convergence; in other words, within
the 9 fold speed-up at p = 12, parallelization con-
tributes about 6 and minibatch about 1.5. By con-
trast, IPM only offers an almost constant speedup of
around 3, which is consistent with the findings of
McDonald et al (2010) (both of their experiments
show a speedup of around 3).
We also try to understand where the speedup
comes from. For that purpose we study intrinsic
speedup, which is about the speed regardless of ac-
curacy (see Figure 4). For our minibatch method,
intrinsic speedup is the average time per iteration
of a parallel run over the serial minibatch base-
line. This answers the questions such as ?how CPU-
efficient is our parallelization? or ?how much CPU
time is wasted?. We can see that with small num-
ber of processors (2 to 4), the efficiency, defined as
Sp/p where Sp is the intrinsic speedup for p pro-
cessors, is almost 100% (ideal linear speedup), but
with more processors it decreases to around 50%
with p = 12, meaning about half of CPU time is
376
 96.8
 96.85
 96.9
 96.95
 97
 97.05
 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
a
c
c
u
ra
c
y 
on
 h
el
d-
ou
t
wall-clock time (hours)
m=1
m=16
m=24
m=48
Figure 5: Minibatch learning for tagging with perceptron
(m = 16, 24, 32) compared with baseline (m = 1) for
tagging with perceptron. All curves are on single CPU.
wasted. This wasting is due to two sources: first, the
load-balancing problem worsens with more proces-
sors, and secondly, the update procedure still runs in
serial mode with p? 1 processors sleeping.
5.2 Part-of-Speech Tagging with Perceptron
Part-of-speech tagging is usually considered as a
simpler task compared to dependency parsing. Here
we show that using minibatch can also bring better
accuracies and speedups for part-of-speech tagging.
We implement a part-of-speech tagger with aver-
aged perceptron. Following the standard splitting of
Penn Treebank (Collins, 2002), we use Sections 00-
18 for training and Sections 19-21 as held-out. Our
implementation provides an accuracy of 96.98 with
beam size 8.
First we run the tagger on a single processor with
minibatch sizes 8, 16, 24, and 32. As in Figure 5, we
observe similar convergence acceleration and higher
accuracies with minibatch. In particular, minibatch
of size m = 16 provides the highest accuracy of
97.04, giving an improvement of 0.06. This im-
provement is smaller than what we observe in MIRA
learning for dependency parsing experiments, which
can be partly explained by the fast convergence of
the tagger, and that perceptron does not involve op-
timization in the updates.
Then we choose minibatch of size 24 to investi-
gate the parallelization performance. As Figure 6
(top) shows, with 12 processors our method takes
only 0.10 hours to converge to an accuracy of 97.00,
compared to the baseline of 96.98 with 0.45 hours.
We also compare our method with IPM as in Fig-
 96.8
 96.85
 96.9
 96.95
 97
 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
a
c
c
u
ra
c
y baseline
m=24,p=1
m=24,p=4
m=24,p=12
 96.8
 96.85
 96.9
 96.95
 97
 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8
a
c
c
u
ra
c
y
wall-clock time (hours)
baseline
IPM,p=4
IPM,p=12
Figure 6: Parallelized minibatch is faster than iterative
parameter mixing (on tagging with perceptron). Top:
minibatch of size 24 using 4 and 12 processors offers
significant speedups over the baselines. Bottom: IPM
with the same 4 and 12 processors offers slightly smaller
speedups. Note that IPM with 4 processors converges
lower than other parallelization curves.
ure 6 (bottom). Again, our method converges faster
and better than IPM, but this time the differences are
much smaller than those in parsing.
Figure 7 uses 96.97 as a criteria to evaluate the
extrinsic speedups given by our method and IPM.
Again we choose this number because it is the lowest
accuracy all learners can reach. As the figure sug-
gests, although our method does not have a higher
pure parallelization speedup (intrinsic speedup), it
still outperforms IPM.
We are interested in the reason why tagging ben-
efits less from minibatch and parallelization com-
pared to parsing. Further investigation reveals that
in tagging the working load of different processors
are more unbalanced than in parsing. Figure 8 shows
that, when p is small, waiting time is negligible, but
when p = 12, tagging wastes about 40% of CPU
cycles and parser about 30%. By contrast, there
is almost no waiting time in IPM and the intrinsic
speedup for IPM is almost linear. The communica-
tion overhead is not included in this figure, but by
comparing it to the speedups (Figures 4 and 7), we
conclude that the communication overhead is about
10% for both parsing and tagging at p = 12.
377
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
 2  4  6  8  10  12
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 12
sp
ee
du
p 
ra
tio
number of processors
minibatch(extrinsic)
minibatch(intrinsic)
IPM(extrinsic)
IPM(intrinsic)
Figure 7: Speedups of minibatch parallelization and IPM
on 1 to 12 processors (tagging with perceptron). Extrin-
sic speedup uses ?the time to reach an accuracy of 96.97?
as the criterion to measure speed. Intrinsic speedup mea-
sures the pure parallelization speedup. IPM has an al-
most linear intrinsic speedup but a near constant extrinsic
speedup of about 3 to 4.
 0
 10
 20
 30
 40
 50
 60
 2  4  6  8  10  12
%
 o
f w
ai
tin
g 
tim
e
number of processors
parser(balanced)
tagger(balanced)
parser(unbalanced)
tagger(unbalanced)
Figure 8: Percentage of time wasted due to synchroniza-
tion (waiting for other processors to finish) (minibatch
m = 24), which corresponds to the gray blocks in Fig-
ure 1 (b-c). The number of sentences assigned to each
processor decreases with more processors, which wors-
ens the unbalance. Our load-balancing strategy (Figure 1
(c)) alleviates this problem effectively. The communica-
tion overhead and update time are not included.
6 Related Work and Discussions
Besides synchronous minibatch and iterative param-
eter mixing (IPM) discussed above, there is another
method of asychronous minibatch parallelization
(Zinkevich et al, 2009; Gimpel et al, 2010; Chiang,
2012), as in Figure 1. The key advantage of asyn-
chronous over synchronous minibatch is that the for-
mer allows processors to remain near-constant use,
while the latter wastes a significant amount of time
when some processors finish earlier than others in a
minibatch, as found in our experiments. Gimpel et
al. (2010) show significant speedups of asychronous
parallelization over synchronous minibatch on SGD
and EM methods, and Chiang (2012) finds asyn-
chronous parallelization to be much faster than IPM
on MIRA for machine translation. However, asyn-
chronous is significantly more complicated to imple-
ment, which involves locking when one processor
makes an update (see Fig. 1 (d)), and (in languages
like Python) message-passing to other processors af-
ter update. Whether this added complexity is worth-
while on large-margin learning is an open question.
7 Conclusions and Future Work
We have presented a simple minibatch paralleliza-
tion paradigm to speed up large-margin structured
learning algorithms such as (averaged) perceptron
and MIRA. Minibatch has an advantage in both se-
rial and parallel settings, and our experiments con-
firmed that a minibatch size of around 16 or 24 leads
to a significant speedups over the pure online base-
line, and when combined with parallelization, leads
to almost linear speedups for MIRA, and very signif-
icant speedups for perceptron. These speedups are
significantly higher than those of iterative parame-
ter mixing of McDonald et al (2010) which were
almost constant (3?4) in both our and their own ex-
periments regardless of the number of processors.
One of the limitations of this work is that although
decoding is done in parallel, update is still done in
serial and in MIRA the quadratic optimization step
(Hildreth algorithm (Hildreth, 1957)) scales super-
linearly with the number of constraints. This pre-
vents us from using very large minibatches. For
future work, we would like to explore parallelized
quadratic optimization and larger minibatch sizes,
and eventually apply it to machine translation.
Acknowledgement
We thank Ryan McDonald, Yoav Goldberg, and Hal
Daume?, III for helpful discussions, and the anony-
mous reviewers for suggestions. This work was
partially supported by DARPA FA8750-13-2-0041
?Deep Exploration and Filtering of Text? (DEFT)
Program and by Queens College for equipment.
378
References
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. J. Machine
Learning Research (JMLR), 13:1159?1187.
C.-T. Chu, S.-K. Kim, Y.-A. Lin, Y.-Y. Yu, G. Bradski,
A. Ng, and K. Olukotun. 2007. Map-reduce for ma-
chine learning on multicore. In Advances in Neural
Information Processing Systems 19.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceedings
of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991, March.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Kevin Gimpel, Dipanjan Das, and Noah Smith. 2010.
Distributed asynchronous online learning for natural
language processing. In Proceedings of CoNLL.
Clifford Hildreth. 1957. A quadratic programming pro-
cedure. Naval Research Logistics Quarterly, 4(1):79?
85.
Liang Huang and Kenji Sagae. 2010. Dynamic program-
ming for linear-time incremental parsing. In Proceed-
ings of ACL 2010.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Proceed-
ings of NAACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML.
Percy Liang and Dan Klein. 2009. Online em for unsu-
pervised models. In Proceedings of NAACL.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proceedings of EACL.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd ACL.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Proceedings of NAACL, June.
Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.
2007. Pegasos: Primal estimated sub-gradient solver
for svm. In Proceedings of ICML.
M. Zinkevich, A. J. Smola, and J. Langford. 2009. Slow
learners are fast. In Advances in Neural Information
Processing Systems 22.
379
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 628?633,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Efficient Implementation of Beam-Search Incremental Parsers?
Yoav Goldberg
Dept. of Computer Science
Bar-Ilan University
Ramat Gan, Tel Aviv, 5290002 Israel
yoav.goldberg@gmail.com
Kai Zhao Liang Huang
Graduate Center and Queens College
City University of New York
{kzhao@gc, lhuang@cs.qc}.cuny.edu
{kzhao.hf, liang.huang.sh}.gmail.com
Abstract
Beam search incremental parsers are ac-
curate, but not as fast as they could be.
We demonstrate that, contrary to popu-
lar belief, most current implementations
of beam parsers in fact run in O(n2),
rather than linear time, because each state-
transition is actually implemented as an
O(n) operation. We present an improved
implementation, based on Tree Structured
Stack (TSS), in which a transition is per-
formed in O(1), resulting in a real linear-
time algorithm, which is verified empiri-
cally. We further improve parsing speed
by sharing feature-extraction and dot-
product across beam items. Practically,
our methods combined offer a speedup of
?2x over strong baselines on Penn Tree-
bank sentences, and are orders of magni-
tude faster on much longer sentences.
1 Introduction
Beam search incremental parsers (Roark, 2001;
Collins and Roark, 2004; Zhang and Clark, 2008;
Huang et al, 2009; Huang and Sagae, 2010;
Zhang and Nivre, 2011; Zhang and Clark, 2011)
provide very competitive parsing accuracies for
various grammar formalisms (CFG, CCG, and de-
pendency grammars). In terms of purning strate-
gies, they can be broadly divided into two cat-
egories: the first group (Roark, 2001; Collins
and Roark, 2004) uses soft (aka probabilistic)
beams borrowed from bottom-up parsers (Char-
niak, 2000; Collins, 1999) which has no control
of complexity, while the second group (the rest
and many more recent ones) employs hard beams
borrowed from machine translation (Koehn, 2004)
which guarantee (as they claim) a linear runtime
O(kn) where k is the beam width. However, we
will demonstrate below that, contrary to popular
?Supported in part by DARPA FA8750-13-2-0041 (DEFT).
belief, in most standard implementations their ac-
tual runtime is in fact O(kn2) rather than linear.
Although this argument in general also applies to
dynamic programming (DP) parsers,1 in this pa-
per we only focus on the standard, non-dynamic
programming approach since it is arguably still the
dominant practice (e.g. it is easier with the popular
arc-eager parser with a rich feature set (Kuhlmann
et al, 2011; Zhang and Nivre, 2011)) and it bene-
fits more from our improved algorithms.
The dependence on the beam-size k is because
one needs to do k-times the number of basic opera-
tions (feature-extractions, dot-products, and state-
transitions) relative to a greedy parser (Nivre and
Scholz, 2004; Goldberg and Elhadad, 2010). Note
that in a beam setting, the same state can expand
to several new states in the next step, which is usu-
ally achieved by copying the state prior to making
a transition, whereas greedy search only stores one
state which is modified in-place.
Copying amounts to a large fraction of the
slowdown of beam-based with respect to greedy
parsers. Copying is expensive, because the state
keeps track of (a) a stack and (b) the set of
dependency-arcs added so far. Both the arc-set and
the stack can grow to O(n) size in the worst-case,
making the state-copy (and hence state-transition)
an O(n) operation. Thus, beam search imple-
mentations that copy the entire state are in fact
quadratic O(kn2) and not linear, with a slowdown
factor of O(kn) with respect to greedy parsers,
which is confirmed empirically in Figure 4.
We present a way of decreasing the O(n) tran-
sition cost to O(1) achieving strictly linear-time
parsing, using a data structure of Tree-Structured
Stack (TSS) that is inspired by but simpler than
the graph-structured stack (GSS) of Tomita (1985)
used in dynamic programming (Huang and Sagae,
2010).2 On average Treebank sentences, the TSS
1The Huang-Sagae DP parser (http://acl.cs.qc.edu)
does run in O(kn), which inspired this paper when we ex-
perimented with simulating non-DP beam search using GSS.
2Our notion of TSS is crucially different from the data
628
input: w0 . . . wn?1
axiom 0 : ?0, ?: ?
SHIFT
` : ?j, S? : A
`+ 1 : ?j + 1, S|wj? : A
j < n
REDUCEL
` : ?j, S|s1|s0? : A
`+ 1 : ?j, S|s0? : A ? {s1xs0}
REDUCER
` : ?j, S|s1|s0? : A
`+ 1 : ?j, S|s1? : A ? {s1ys0}
goal 2n? 1 : ?n, s0?: A
Figure 1: An abstraction of the arc-standard de-
ductive system Nivre (2008). The stack S is a list
of heads, j is the index of the token at the front of
the buffer, and ` is the step number (beam index).
A is the arc-set of dependency arcs accumulated
so far, which we will get rid of in Section 4.1.
version, being linear time, leads to a speedup of
2x?2.7x over the naive implementation, and about
1.3x?1.7x over the optimized baseline presented
in Section 5.
Having achieved efficient state-transitions, we
turn to feature extraction and dot products (Sec-
tion 6). We present a simple scheme of sharing
repeated scoring operations across different beam
items, resulting in an additional 7 to 25% speed in-
crease. On Treebank sentences, the methods com-
bined lead to a speedup of ?2x over strong base-
lines (?10x over naive ones), and on longer sen-
tences they are orders of magnitude faster.
2 Beam Search Incremental Parsing
We assume familiarity with transition-based de-
pendency parsing. The unfamiliar reader is re-
ferred to Nivre (2008). We briefly describe a
standard shift-reduce dependency parser (which is
called ?arc-standard? by Nivre) to establish nota-
tion. Parser states (sometimes called configura-
tions) are composed of a stack, a buffer, and an
arc-set. Parsing transitions are applied to states,
and result in new states. The arc-standard system
has three kinds of transitions: SHIFT, REDUCEL,
structure with the same name in an earlier work of Tomita
(1985). In fact, Tomita?s TSS merges the top portion of the
stacks (more like GSS) while ours merges the bottom por-
tion. We thank Yue Zhang for informing us that TSS was
already implemented for the CCG parser in zpar (http://
sourceforge.net/projects/zpar/) though it was not men-
tioned in his paper (Zhang and Clark, 2011).
and REDUCER, which are summarized in the de-
ductive system in Figure 1. The SHIFT transition
removes the first word from the buffer and pushes
it to the stack, and the REDUCEL and REDUCER
actions each add a dependency relation between
the two words on the top of the stack (which is
achieved by adding the arc s1xs0 or s1ys0 to the
arc-set A), and pops the new dependent from the
stack. When reaching the goal state the parser re-
turns a tree composed of the arcs in the arc-set.
At parsing time, transitions are chosen based on
a trained scoring model which looks at features
of the state. In a beam parser, k items (hypothe-
ses) are maintained. Items are composed of a state
and a score. At step i, each of the k items is ex-
tended by applying all possible transitions to the
given state, resulting in k ? a items, a being the
number of possible transitions. Of these, the top
scoring k items are kept and used in step i+1. Fi-
nally, the tree associated with the highest-scoring
item is returned.
3 The Common Implementation of State
The stack is usually represented as a list or an array
of token indices, and the arc-set as an array heads
of length n mapping the word at position m to the
index of its parent. In order to allow for fast fea-
ture extraction, additional arrays are used to map
each token to its left-most and right-most modi-
fier, which are used in most incremental parsers,
e.g. (Huang and Sagae, 2010; Zhang and Nivre,
2011). The buffer is usually implemented as a
pointer to a shared sentence object, and an index j
to the current front of the buffer. Finally, it is com-
mon to keep an additional array holding the tran-
sition sequence leading to the current state, which
can be represented compactly as a pointer to the
previous state and the current action. The state
structure is summarized below:
class state
stack[n] of token_ids
array[n] heads
array[n] leftmost_modifiers
array[n] rightmost_modifiers
int j
int last_action
state previous
In a greedy parser, state transition is performed in-
place. However, in a beam parser the states cannot
be modified in place, and a state transition oper-
ation needs to result in a new, independent state
object. The common practice is to copy the cur-
rent state, and then update the needed fields in the
copy. Copying a stack and arrays of size n is an
629
O(n) operation. In what follows, we present a way
to perform transitions in O(1).
4 Efficient State Transitions
4.1 Distributed Representation of Trees
The state needs to keep track of the set of arcs
added to the tree so far for two reasons:
(a) In order to return the complete tree at the end.
(b) In order to compute features when parsing.
Observe that we do not in fact need to store any
arc in order to achieve (a) ? we could reconstruct
the entire set by backtracking once we reach the
final configuration. Hence, the arc-set in Figure 1
is only needed for computing features. Instead of
storing the entire arc-set, we could keep only the
information needed for feature computation. In
the feature set we use (Huang and Sagae, 2010),
we need access to (1) items on the buffer, (2)
the 3 top-most elements of the stack, and (3) the
current left-most and right-most modifiers of the
two topmost stack elements. The left-most and
right-most modifiers are already kept in the state
representation, but store more information than
needed: we only need to keep track of the mod-
ifiers of current stack items. Once a token is re-
moved from the stack it will never return, and we
will not need access to its modifiers again. We
can therefore remove the left/rightmost modifier
arrays, and instead have the stack store triplets
(token, leftmost_mod, rightmost_mod). The
heads array is no longer needed. Our new state
representation becomes:
class state
stack[n] of (tok, left, right)
int j
int last_action
state previous
4.2 Tree Structured Stack: TSS
We now turn to handle the stack. Notice that the
buffer, which is also of size O(n), is represented
as a pointer to an immutable shared object, and is
therefore very efficient to copy. We would like to
treat the stack in a similar fashion.
An immutable stack can be implemented func-
tionally as a cons list, where the head is the top
of the stack and the tail is the rest of the stack.
Pushing an item to the stack amounts to adding a
new head link to the list and returning it. Popping
an item from the stack amounts to returning the
tail of the list. Notice that, crucially, a pop opera-
tion does not change the underlying list at all, and
a push operation only adds to the front of a list.
Thus, the stack operations are non-destructive, in
the sense that once you hold a reference to a stack,
the view of the stack through this reference does
not change regardless of future operations that are
applied to the stack. Moreover, push and pop op-
erations are very efficient. This stack implementa-
tion is an example of a persistent data structure ? a
data structure inspired by functional programming
which keeps the old versions of itself intact when
modified (Okasaki, 1999).
While each client sees the stack as a list, the un-
derlying representation is a tree, and clients hold
pointers to nodes in the tree. A push operation
adds a branch to the tree and returns the new
pointer, while a pop operation returns the pointer
of the parent, see Figure 3 for an example. We call
this representation a tree-structured stack (TSS).
Using this stack representation, we can replace
the O(n) stack by an integer holding the item at
the top of the stack (s0), and a pointer to the tail of
the stack (tail). As discussed above, in addition
to the top of the stack we also keep its leftmost and
rightmost modifiers s0L and s0R. The simplified
state representation becomes:
class state
int s0, s0L, s0R
state tail
int j
int last_action
state previous
State is now reduced to seven integers, and the
transitions can be implemented very efficiently as
we show in Figure 2. The parser state is trans-
formed into a compact object, and state transitions
are O(1) operations involving only a few pointer
lookups and integer assignments.
4.3 TSS vs. GSS; Space Complexity
TSS is inspired by the graph-structured stack
(GSS) used in the dynamic-programming parser of
Huang and Sagae (2010), but without reentrancy
(see also Footnote 2). More importantly, the state
signature in TSS is much slimmer than that in
GSS. Using the notation of Huang and Sagae, in-
stead of maintaining the full DP signature of
f?DP(j, S) = (j, fd(sd), . . . , f0(s0))
where sd denotes the dth tree on stack, in non-DP
TSS we only need to store the features f0(s0) for
the final tree on the stack,
f?noDP(j, S) = (j, f0(s0)),
630
def Shift(state)
newstate.s0 = state.j
newstate.s0L = None
newstate.s0R = None
newstate.tail = state
newstate.j = state.j + 1
return newstate
def ReduceL(state)
newstate.s0 = state.s0
newstate.s0L = state.tail.s0
newstate.s0R = state.s0R
newstate.tail = state.tail.tail
newstate.j = j
return newstate
def ReduceR(state)
newstate.s0 = state.tail.s0
newstate.s0L = state.tail.s0L
newstate.s0R = state.s0
newstate.tail = state.tail.tail
newstate.j = j
return newstate
Figure 2: State transitions implementation in the TSS representation (see Fig. 3 for the tail pointers).
The two lines on s0L and s0R are specific to feature set design, and can be expanded for richer feature
sets. To conserve space, we do not show the obvious assignments to last_action and previous.
b
1 2
 c
3
a
a d
4
b
c
0
b
c
c
L
R
L
R
sh sh sh sh
sh
sh
Figure 3: Example of tree-structured stack. The
forward arrows denote state transitions, and the
dotted backward arrows are the tail pointers to
the stack tail. The boxes denote the top-of-stack at
each state. Notice that for b = shift(a) we perform
a single push operation getting b.tail = a, while
for b = reduce(a) transition we perform two pops
and a push, resulting in b.tail = a.tail.tail.
thanks to the uniqueness of tail pointers (?left-
pointers? in Huang and Sagae).
In terms of space complexity, each state is re-
duced from O(n) in size to O(d) with GSS and
to O(1) with TSS,3 making it possible to store the
entire beam in O(kn) space. Moreover, the con-
stant state-size makes memory management easier
and reduces fragmentation, by making it possible
to pre-allocate the entire beam upfront. We did
not explore its empirical implications in this work,
as our implementation language, Python, does not
support low-level memory management.
4.4 Generality of the Approach
We presented a concrete implementation for the
arc-standard system with a relatively simple (yet
state-of-the-art) feature set. As in Kuhlmann et
al. (2011), our approach is also applicable to
other transitions systems and richer feature-sets
with some additional book-keeping. A well-
3For example, a GSS state in Huang and Sagae?s experi-
ments also stores s1, s1L, s1R, s2 besides the f0(s0) fea-
tures (s0, s0L, s0R) needed by TSS. d is treated as a con-
stant by Huang and Sagae but actually it could be a variable.
documented Python implementation for the la-
beled arc-eager system with the rich feature set
of Zhang and Nivre (2011) is available on the first
author?s homepage.
5 Fewer Transitions: Lazy Expansion
Another way of decreasing state-transition costs
is making less transitions to begin with: instead
of performing all possible transitions from each
beam item and then keeping only k of the re-
sulting states, we could perform only transitions
that are sure to end up on the next step in the
beam. This is done by first computing transition
scores from each beam item, then keeping the top
k highest scoring (state, action) pairs, perform-
ing only those k transitions. This technique is
especially important when the number of possi-
ble transitions is large, such as in labeled parsing.
The technique, though never mentioned in the lit-
erature, was employed in some implementations
(e.g., Yue Zhang?s zpar). We mention it here for
completeness since it?s not well-known yet.
6 (Partial) Feature Sharing
After making the state-transition efficient, we turn
to deal with the other major expensive operation:
feature-extractions and dot-products. While we
can?t speed up the process, we observe that some
computations are repeated in different parts of the
beam, and propose to share these computations.
Notice that relatively few token indices from a
state can determine the values of many features.
For example, knowing the buffer index j deter-
mines the words and tags of items after location
j on the buffer, as well as features composed of
combinations of these values.
Based on this observation we propose the no-
tion of a state signature, which is a set of token
indices. An example of a state signature would
be sig(state) = (s0, s0L, s1, s1L), indicating the
indices of the two tokens at the top of the stack to-
gether with their leftmost modifiers. Given a sig-
631
Figure 4: Non-linearity of the standard beam
search compared to the linearity of our TSS beam
search for labeled arc-eager and unlabeled arc-
standard parsers on long sentences (running times
vs. sentence length). All parsers use beam size 8.
nature, we decompose the feature function ?(x)
into two parts ?(x) = ?s(sig(x)) + ?o(x), where
?s(sig(x)) extracts all features that depend exclu-
sively on signature items, and ?o(x) extracts all
other features.4 The scoring function w ? ?(x) de-
composes into w ? ?s(sig(x)) + w ? ?o(x). Dur-
ing beam decoding, we maintain a cache map-
ping seen signatures sig(state) to (partial) tran-
sition scores w ? ?s(sig(state)). We now need
to calculate w ? ?o(x) for each beam item, but
w ? ?s(sig(x)) only for one of the items sharing
the signature. Defining the signature involves a
natural balance between signatures that repeat of-
ten and signatures that cover many features. In the
experiments in this paper, we chose the signature
function for the arc-standard parser to contain all
core elements participating in feature extraction5,
and for the arc-eager parser a signature containing
only a partial subset.6
7 Experiments
We implemented beam-based parsers using the
traditional approach as well as with our proposed
extension and compared their runtime.
The first experiment highlights the non-linear
behavior of the standard implementation, com-
pared to the linear behavior of the TSS method.
4One could extend the approach further to use several sig-
natures and further decompose the feature function. We did
not pursue this idea in this work.
5s0,s0L,s0R,s1,s1L,s1R,s2,j.
6s0, s0L, s0R,s0h,b0L,j, where s0h is the parent of
s0, and b0L is the leftmost modifier of j.
system plain plain plain plain +TSS+lazy
+TSS +lazy +TSS +feat-share
(sec 3) (sec 4) (sec 5) +lazy (sec 6)
ArcS-U 20.8 38.6 24.3 41.1 47.4
ArcE-U 25.4 48.3 38.2 58.2 72.3
ArcE-L 1.8 4.9 11.1 14.5 17.3
Table 1: Parsing speeds for the different tech-
niques measured in sentences/sec (beam size 8).
All parsers are implemented in Python, with dot-
products in C. ArcS/ArcE denotes arc-standard
vs. arc-eager, L/U labeled (stanford deps, 49 la-
bels) vs. unlabeled parsing. ArcS use feature set
of Huang and Sagae (2010) (50 templates), and ArcE
that of Zhang and Nivre (2011) (72 templates).
As parsing time is dominated by score computa-
tion, the effect is too small to be measured on
natural language sentences, but it is noticeable
for longer sentences. Figure 4 plots the runtime
for synthetic examples with lengths ranging from
50 to 1000 tokens, which are generated by con-
catenating sentences from Sections 22?24 of Penn
Treebank (PTB), and demonstrates the non-linear
behavior (dataset included). We argue parsing
longer sentences is by itself an interesting and
potentially important problem (e.g. for other lan-
guages such as Arabic and Chinese where word
or sentence boundaries are vague, and for pars-
ing beyond sentence-level, e.g. discourse parsing
or parsing with inter-sentence dependencies).
Our next set of experiments compares the actual
speedup observed on English sentences. Table 1
shows the speed of the parsers (sentences/sec-
ond) with the various proposed optimization tech-
niques. We first train our parsers on Sections 02?
21 of PTB, using Section 22 as the test set. The
accuracies of all our parsers are at the state-of-
the-art level. The final speedups are up to 10x
against naive baselines and ?2x against the lazy-
transitions baselines.
8 Conclusions
We demonstrated in both theory and experiments
that the standard implementation of beam search
parsers run in O(n2) time, and have presented im-
proved algorithms which run in O(n) time. Com-
bined with other techniques, our method offers
significant speedups (?2x) over strong baselines,
or 10x over naive ones, and is orders of magnitude
faster on much longer sentences. We have demon-
strated that our approach is general and we believe
it will benefit many other incremental parsers.
632
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of NAACL.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of ACL.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Yoav Goldberg and Michael Elhadad. 2010. An ef-
ficient algorithm for easy-first non-directional de-
pendency parsing. In Proceedings of HLT-NAACL,
pages 742?750.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of ACL 2010.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine transla-
tion models. In Proceedings of AMTA, pages 115?
124.
Marco Kuhlmann, Carlos Gmez-Rodrguez, and Gior-
gio Satta. 2011. Dynamic programming algorithms
for transition-based dependency parsers. In Pro-
ceedings of ACL.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of english text. In Proceedings
of COLING, Geneva.
Joakim Nivre. 2008. Algorithms for deterministic in-
cremental dependency parsing. Computational Lin-
guistics, 34(4):513?553.
Chris Okasaki. 1999. Purely functional data struc-
tures. Cambridge University Press.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Masaru Tomita. 1985. An efficient context-free pars-
ing algorithm for natural languages. In Proceedings
of the 9th international joint conference on Artificial
intelligence - Volume 2, pages 756?764.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of EMNLP.
Yue Zhang and Stephen Clark. 2011. Shift-reduce ccg
parsing. In Proceedings of ACL.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL, pages 188?193.
633
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 785?790,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Hierarchical MT Training using Max-Violation Perceptron
Kai Zhao
?
Liang Huang
?
?
Graduate Center & Queens College
City University of New York
{kzhao@gc,huang@cs.qc}.cuny.edu
Haitao Mi
?
Abe Ittycheriah
?
?
T. J. Watson Research Center
IBM
{hmi,abei}@us.ibm.com
Abstract
Large-scale discriminative training has be-
come promising for statistical machine
translation by leveraging the huge train-
ing corpus; for example the recent effort
in phrase-based MT (Yu et al, 2013) sig-
nificantly outperforms mainstream meth-
ods that only train on small tuning sets.
However, phrase-based MT suffers from
limited reorderings, and thus its training
can only utilize a small portion of the bi-
text due to the distortion limit. To address
this problem, we extend Yu et al (2013)
to syntax-based MT by generalizing their
latent variable ?violation-fixing? percep-
tron from graphs to hypergraphs. Exper-
iments confirm that our method leads to
up to +1.2 BLEU improvement over main-
stream methods such as MERT and PRO.
1 Introduction
Many natural language processing problems in-
cluding part-of-speech tagging (Collins, 2002),
parsing (McDonald et al, 2005), and event extrac-
tion (Li et al, 2013) have enjoyed great success us-
ing large-scale discriminative training algorithms.
However, a similar success on machine translation
has been elusive, where the mainstream methods
still tune on small datasets.
What makes large-scale MT training so hard
then? After numerous attempts by various re-
searchers (Liang et al, 2006; Watanabe et al,
2007; Arun and Koehn, 2007; Blunsom et al,
2008; Chiang et al, 2008; Flanigan et al, 2013;
Green et al, 2013), the recent work of Yu et al
(2013) finally reveals a major reason: it is the vast
amount of (inevitable) search errors in MT decod-
ing that astray learning. To alleviate this prob-
lem, their work adopts the theoretically-motivated
framework of violation-fixing perceptron (Huang
et al, 2012) tailed for inexact search, yielding
great results on phrase-based MT (outperforming
Collins (02)
inexact
??
search
Huang et al (12)
latent
??
variable
Yu et al (13)
? hypergraph ?
Zhang et al (13) ??
variable
this work
Figure 1: Relationship with previous work.
small-scale MERT/PRO by a large margin for the
first time). However, the underlying phrase-based
model suffers from limited distortion and thus can
only employ a small portion (about 1/3 in their Ch-
En experiments) of the bitext in training.
To better utilize the large training set, we
propose to generalize from phrase-based MT to
syntax-based MT, in particular the hierarchical
phrase-based translation model (HIERO) (Chiang,
2005), in order to exploit sentence pairs beyond
the expressive capacity of phrase-based MT.
The key challenge here is to extend the latent
variable violation-fixing perceptron of Yu et al
(2013) to handle tree-structured derivations and
translation hypergraphs. Luckily, Zhang et al
(2013) have recently generalized the underlying
violation-fixing perceptron of Huang et al (2012)
from graphs to hypergraphs for bottom-up parsing,
which resembles syntax-based decoding. We just
need to further extend it to handle latent variables.
We make the following contributions:
1. We generalize the latent variable violation-
fixing perceptron framework to inexact
search over hypergraphs, which subsumes
previous algorithms for PBMT and bottom-
up parsing as special cases (see Fig. 1).
2. We show that syntax-based MT, with its bet-
ter handling of long-distance reordering, can
exploit a larger portion of the training set,
which facilitates sparse lexicalized features.
3. Experiments show that our training algo-
rithm outperforms mainstream tuning meth-
ods (which optimize on small devsets) by
+1.2 BLEU over MERT and PRO on FBIS.
785
id rule
r
0
S? ?X
1
,X
1
?
r
1
S? ?S
1
X
2
,S
1
X
2
?
r
2
X? ?B`ush??,Bush?
r
3
X? ?Sh?al?ong,Sharon?
r
4
X? ?hu`?t?an, talks?
r
5
X? ?y?u X
1
j?ux??ng X
2
,
held X
2
with X
1
?
r
6
X? ?y?u Sh?al?ong, with Sharon?
r
7
X? ?X
1
j?ux??ng X
2
,
X
1
held X
2
?
S
[0:5]
X
[1:5]
X
[4:5]
hu`?t?an
5
j?ux??ng
4
X
[2:3]
Sh?al?ong
3
|
y?u
2
S
[0:1]
X
[0:1]
0
B`ush??
1
S
X
X
Sharon
5
with
4
X
talks
3
held
2
S
X
0
Bush
1
S
[0:5]
X
[1:5]
X
[4:5]
hu`?t?an
5
j?ux??ng
4
X
[1:3]
Sh?al?ong
3
y?u
2
S
[0:1]
X
[0:1]
0
B`ush??
1
S
X
X
talks
5
held
4
X
Sharon
3
with
2
S
X
0
Bush
1
(a) HIERO rules (b) gold derivation (c) Viterbi derivation
Figure 2: An example of HIERO translation.
X[0:1] X[2:3] X[4:5]
X[1:5]
X[1:3]
S[0:1]
S[0:5]
Figure 3: A ?LM hypergraph with two deriva-
tions: the gold derivation (Fig. 2b) in solid lines,
and the Viterbi derivation (Fig. 2c) in dashed lines.
2 Review: Syntax-based MT Decoding
For clarity reasons we will describe HIERO decod-
ing as a two-pass process, first without a language
model, and then integrating the LM. This section
mostly follows Huang and Chiang (2007).
In the first, ?LM phase, the decoder parses the
source sentence using the source projection of the
synchronous grammar (see Fig. 2 (a) for an ex-
ample), producing a?LM hypergraph where each
node has a signature N
[i:j]
, where N is the nonter-
minal type (either X or S in HIERO) and [i : j] is
the span, and each hyperedge e is an application
of the translation rule r(e) (see Figure 3).
To incorporate the language model, each node
also needs to remember its target side boundary
words. Thus a ?LM node N
[i:j]
is split into mul-
tiple +LM nodes of signature N
a?b
[i:j]
, where a and
b are the boundary words. For example, with a bi-
gram LM, X
held?Sharon
[1:5]
is a node whose translation
starts with ?held? and ends with ?Sharon?.
More formally, the whole decoding process can
be cast as a deductive system. Take the partial
translation of ?held talks with Sharon? in Figure 2
(b) for example, the deduction is
X
Sharon?Sharon
[2:3]
: s
1
X
talks?talks
[4:5]
: s
2
X
held?Sharon
[1:5]
: s
1
+ s
2
+ s(r
5
) + ?
r
5
,
where s(r
5
) is the score of rule r
5
, and the LM
combo score ? is log P
lm
(talks | held)P
lm
(with |
talks)P
lm
(Sharon | with).
3 Violation-Fixing Perceptron for HIERO
As mentioned in Section 1, the key to the success
of Yu et al (2013) is the adoption of violation-
fixing perceptron of Huang et al (2012) which
is tailored for vastly inexact search. The general
idea is to update somewhere in the middle of the
search (where search error happens) rather than at
the very end (standard update is often invalid). To
adapt it to MT where many derivations can output
the same translation (i.e., spurious ambiguity), Yu
et al (2013) extends it to handle latent variables
which correspond to phrase-based derivations. On
the other hand, Zhang et al (2013) has generalized
Huang et al (2012) from graphs to hypergraphs
for bottom-up parsing, which resembles HIERO
decoding. So we just need to combine the two
generalizing directions (latent variable and hyper-
graph, see Fig. 1).
3.1 Latent Variable Hypergraph Search
The key difference between bottom-up parsing
and MT decoding is that in parsing the gold tree
for each input sentence is unique, while in MT
many derivations can generate the same reference
translation. In other words, the gold derivation to
update towards is a latent variable.
786
Here we formally define the latent variable
?max-violation? perceptron over a hypergraph for
MT training. For a given sentence pair ?x, y?, we
denote H(x) as the decoding hypergraph of HI-
ERO without any pruning. We say D ? H(x) if
D is a full derivation of decoding x, and D can be
derived from the hypergraph. Let good(x, y) be
the set of y-good derivations for ?x, y?:
good(x, y)
?
= {D ? H(x) | e(D) = y},
where e(D) is the translation from derivation D.
We then define the set of y-good partial derivations
that cover x
[i:j]
with root N
[i:j]
as
good
N
[i:j]
(x, y)
?
= {d ? D | D ? good(x, y),
root(d) = N
[i:j]
}
We further denote the real decoding hypergraph
with beam-pruning and cube-pruning as H
?
(x).
The set of y-bad derivations is defined as
bad
N
[i:j]
(x, y)
?
= {d ? D | D ? H
?
(x, y),
root(d) = N
[i:j]
, d 6? good
N
[i:j]
(x, y)}.
Note that the y-good derivations are defined over
the unpruned whole decoding hypergraph, while
the y-bad derivations are defined over the real de-
coding hypergraph with pruning.
The max-violation method performs the update
where the model score difference between the
incorrect Viterbi partial derivation and the best
y-good partial derivation is maximal, by penaliz-
ing the incorrect Viterbi partial derivation and re-
warding the y-good partial derivation.
More formally, we first find the Viterbi partial
derivation d
?
and the best y-good partial deriva-
tion d
+
for each N
[i:j]
group in the pruned +LM
hypergraph:
d
+
N
[i:j]
(x, y)
?
= argmax
d?good
N
[i:j]
(x,y)
w ??(x, d),
d
?
N
[i:j]
(x, y)
?
= argmax
d?bad
N
[i:j]
(x,y)
w ??(x, d),
where ?(x, d) is the feature vector for derivation
d. Then it finds the group N
?
[i
?
:j
?
]
with the max-
imal score difference between the Viterbi deriva-
tion and the best y-good derivation:
N
?
[i
?
:j
?
]
?
= argmax
N
[i:j]
w ???(x, d
+
N
[i:j]
(x, y), d
?
N
[i:j]
(x, y)),
and update as follows:
w? w + ??(x, d
+
N
?
[i
?
:j
?
]
(x, y), d
?
N
?
[i
?
:j
?
]
(x, y)),
where ??(x, d, d
?
)
?
= ?(x, d)??(x, d
?
).
3.2 Forced Decoding for HIERO
We now describe how to find the gold derivations.
1
Such derivations can be generated in way similar
to Yu et al (2013) by using a language model tai-
lored for forced decoding:
P
forced
(q | p) =
{
1 if q = p+ 1
0 otherwise
,
where p and q are the indices of the boundary
words in the reference translation. The +LM node
now has signature N
p?q
[i:j]
, where p and q are the in-
dexes of the boundary words. If a boundary word
does not occur in the reference, its index is set to
? so that its language model score will always be
??; if a boundary word occurs more than once in
the reference, its ?LM node is split into multiple
+LM nodes, one for each such index.
2
We have a similar deductive system for forced
decoding. For the previous example, rule r
5
in
Figure 2 (a) is rewritten as
X? ?y?u X
1
j?ux??ng X
2
, 1 X
2
4 X
1
?,
where 1 and 4 are the indexes for reference words
?held? and ?with? respectively. The deduction for
X
[1:5]
in Figure 2 (b) is
X
5?5
[2:3]
: s
1
X
2?3
[4:5]
: s
2
X
1?5
[1:5]
: s(r
5
) + ?+ s
1
+ s
2
r
5
,
where ? = log
?
i?{1,3,4}
P
forced
(i+ 1 | i) = 0.
4 Experiments
Following Yu et al (2013), we call our max-
violation method MAXFORCE. Our implemen-
tation is mostly in Python on top of the cdec
system (Dyer et al, 2010) via the pycdec in-
terface (Chahuneau et al, 2012). In addition, we
use minibatch parallelization of (Zhao and Huang,
1
We only consider single reference in this paper.
2
Our formulation of index-based language model fixes a
bug in the word-based LM of Yu et al (2013) when a sub-
string appears more than once in the reference (e.g. ?the
man...the man...?); thanks to Dan Gildea for pointing it out.
787
2013) to speedup perceptron training. We evalu-
ate MAXFORCE for HIERO over two CH-EN cor-
pora, IWSLT09 and FBIS, and compare the per-
formance with vanilla n-best MERT (Och, 2003)
from Moses (Koehn et al, 2007), Hypergraph
MERT (Kumar et al, 2009), and PRO (Hopkins
and May, 2011) from cdec.
4.1 Features Design
We use all the 18 dense features from cdec, in-
cluding language model, direct translation prob-
ability p(e|f), lexical translation probabilities
p
l
(e|f) and p
l
(f |e), length penalty, counts for the
source and target sides in the training corpus, and
flags for the glue rules and pass-through rules.
For sparse features we use Word-Edges fea-
tures (Charniak and Johnson, 2005; Huang, 2008)
which are shown to be extremely effective in
both parsing and phrase-based MT (Yu et al,
2013). We find that even simple Word-Edges
features boost the performance significantly, and
adding complex Word-Edges features from Yu et
al. (2013) brings limited improvement and slows
down the decoding. So in the following experi-
ments we only use Word-Edges features consisting
of combinations of English and Chinese words,
and Chinese characters, and do not use word clus-
ters nor word types. For simplicity and efficiency
reasons, we also exclude all non-local features.
4.2 Datasets and Preprocessing
Our first corpus, IWSLT09, contains ?30k
short sentences collected from spoken language.
IWSLT04 is used as development set in MAX-
FORCE training, and as tuning set for n-best
MERT, Hypergraph MERT, and PRO. IWSLT05
is used as test set. Both IWSLT04 and IWSLT05
contain 16 references.We mainly use this corpus
to investigate the properties of MAXFORCE.
The second corpus, FBIS, contains ?240k sen-
tences. NIST06 newswire is used as development
set for MAXFORCE training, and as tuning set
for all other tuning methods. NIST08 newswire
is used as test set. Both NIST06 newswire
and NIST08 newswire contain 4 references. We
mainly use this corpus to demonstrate the perfor-
mance of MAXFORCE in large-scale training.
For both corpora, we do standard tokeniza-
tion, alignment and rule extraction using the cdec
tools. In rule extraction, we remove all 1-count
rules but keep the rules mapping from one Chi-
nese word to one English word to help balancing
sent. words
phrase-based MT 32% 12%
HIERO 35% 30%
HIERO (all rules) 65% 55%
Table 1: Reachability comparison (on FBIS) be-
tween phrase-based MT reported in Yu et al
(2013) (without 1-count rules) and HIERO (with
and without 1-count rules).
 0
 0.2
 0.4
 0.6
 0.8
 1
 20  40  60  80  100
fo
rc
ed
 d
ec
od
ab
le
 ra
tio
sentence length
loose
tight
Figure 4: Reachability vs. sent. length on FBIS.
See text below for ?loose? and ?tight?.
between overfitting and coverage. We use a tri-
gram language model trained from the target sides
of the two corpora respectively.
4.3 Forced Decoding Reachability
We first report the forced decoding reachability for
HIERO on FBIS in Table 1. With the full rule set,
65% sentences and 55% words of the whole cor-
pus are forced decodable in HIERO. After pruning
1-count rules, our forced decoding covers signif-
icantly more words than phrase-based MT in Yu
et al (2013). Furthermore, in phrase-based MT,
most decodable sentences are very short, while
in HIERO the lengths of decodable sentences are
more evenly distributed.
However, in the following experiments, due to
efficiency considerations, we use the ?tight? rule
extraction in cdec that is more strict than the
standard ?loose? rule extraction, which generates
a reduced rule set and, thus, a reduced reachabil-
ity. We show the reachability distributions of both
tight and loose rule extraction in Figure 4.
4.4 Evaluation on IWSLT
For IWSLT, we first compare the performance
from various update methods in Figure 5. The
max-violation method is more than 15 BLEU
788
 30
 35
 40
 45
 2  4  6  8  10  12  14  16  18  20
BL
EU
 o
n 
de
v
iteration
Max-Violation
local update
skip
standard update
Figure 5: Comparison of various update methods.
 42
 43
 44
 45
 46
 47
 2  4  6  8  10  12  14  16  18  20
BL
EU
 o
n 
de
v
iteration
sparse features
dense features
Hypergraph MERT
PRO
n-best MERT
Figure 6: Sparse features (Word-Edges) contribute
?2 BLEU points, outperforming PRO and MERT.
points better than the standard perceptron (also
known as ?bold-update? in Liang et al (2006))
which updates at the root of the derivation tree.
3,4
This can be explained by the fact that in train-
ing ?58% of the standard updates are invalid (i.e.,
they do not fix any violation). We also use the
?skip? strategy of Zhang et al (2013) which up-
dates at the root of the derivation only when it fixes
a search error, avoiding all invalid updates. This
achieves ?10 BLEU better than the standard up-
date, but is still more than ?5 BLEU worse than
Max-Violation update. Finally we also try the
?local-update? method from Liang et al (2006)
which updates towards the derivation with the best
Bleu
+1
in the root group S
[0:|x|]
. This method is
about 2 BLEU points worse than max-violation.
We further investigate the contribution of sparse
features in Figure 6. On the development set,
max-violation update without Word-Edges fea-
tures achieves BLEU similar to n-best MERT and
3
We find that while MAXFORCE generates translations of
length ratio close to 1 during training, the length ratios on
dev/test sets are significantly lower, due to OOVs. So we
run a binary search for the length penalty weight after each
training iteration to tune the length ratio to ?0.97 on dev set.
4
We report BLEU with averaged reference lengths.
algorithm # feats dev test
n-best MERT 18 44.9 47.9
Hypergraph MERT 18 46.6 50.7
PRO 18 45.0 49.5
local update perc. 443K 45.6 49.1
MAXFORCE 529K 47.4 51.5
Table 2: BLEU scores (with 16 references) of var-
ious training algorithms on IWSLT09.
algorithm # feats dev test
Hypergraph MERT 18 27.3 23.0
PRO 18 26.4 22.7
MAXFORCE 4.5M 27.7 23.9
Table 3: BLEU scores (with 4 references) of vari-
ous training algorithms on FBIS.
PRO, but lower than Hypergraph MERT. Adding
simple Word-Edges features improves BLEU by
?2 points, outperforming the very strong Hyper-
graph MERT baseline by?1 point. See Table 2 for
details. The results of n-best MERT, Hypergraph
MERT, and PRO are averages from 3 runs.
4.5 Evaluation on FBIS
Table 3 shows BLEU scores of Hypergraph MERT,
PRO, and MAXFORCE on FBIS. MAXFORCE ac-
tives 4.5M features, and achieves +1.2 BLEU over
PRO and +0.9 BLEU over Hypergraph MERT. The
training time (on 32 cores) for Hypergraph MERT
and PRO is about 30 min. on the dev set, and is
about 5 hours for MAXFORCE on the training set.
5 Conclusions
We have presented a latent-variable violation-
fixing framework for general structured predic-
tion problems with inexact search over hyper-
graphs. Its application on HIERO brings signif-
icant improvement in BLEU, compared to algo-
rithms that are specially designed for MT tuning
such as MERT and PRO.
Acknowledgment
Part of this work was done during K. Z.?s intern-
ship at IBM. We thank Martin
?
Cmejrek and Lemao
Liu for discussions, David Chiang for pointing
us to pycdec, Dan Gildea for Footnote 2, and
the anonymous reviewers for comments. This
work is supported by DARPA FA8750-13-2-0041
(DEFT), DARPA HR0011-12-C-0015 (BOLT),
and a Google Faculty Research Award.
789
References
Abhishek Arun and Philipp Koehn. 2007. On-
line learning methods for discriminative training of
phrase based statistical machine translation. Proc.
of MT Summit XI, 2(5):29.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In ACL, pages 200?208.
Victor Chahuneau, Noah Smith, and Chris Dyer. 2012.
pycdec: A python interface to cdec. Prague Bulletin
of Mathematical Linguistics, (98).
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180,
Ann Arbor, Michigan, June.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of EMNLP
2008.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL.
Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.
2013. Large-scale discriminative training for statis-
tical machine translation using held-out line search.
In Proceedings of NAACL 2013.
Spence Green, Sida Wang, Daniel Cer, and Christo-
pher D Manning. 2013. Fast and adaptive online
training of feature-rich translation models. to ap-
pear) ACL.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Fast decoding with integrated language models.
In Proceedings of ACL, Prague, Czech Rep., June.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of NAACL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the ACL: HLT, Columbus, OH, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit
for statistical machine translation. In Proceedings
of ACL.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of
the Joint Conference of ACL and AFNLP.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event
extraction via structured prediction with global fea-
tures. In Proceedings of ACL.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In Proceed-
ings of COLING-ACL, Sydney, Australia, July.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd ACL.
Franz Joseph Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings of
EMNLP-CoNLL.
Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.
2013. Max-violation perceptron and forced decod-
ing for scalable MT training. In Proceedings of
EMNLP.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan Mc-
Donald. 2013. Online learning with inexact hyper-
graph search. In Proceedings of EMNLP.
Kai Zhao and Liang Huang. 2013. Minibatch and par-
allelization for online large margin structured learn-
ing. In Proceedings of NAACL 2013.
790
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Tutorials, pages 4?5,
Baltimore, Maryland, USA, 22 June 2014.
c?2014 Association for Computational Linguistics
Scalable Large-Margin Structured Learning:
Theory and Algorithms
Liang Huang Kai Zhao Lemao Liu
Graduate Center and Queens College, City University of New York
{liang.huang.sh, kzhao.hf, lemaoliu}@gmail.com
1 Motivations
Much of NLP tries to map structured input (sen-
tences) to some form of structured output (tag se-
quences, parse trees, semantic graphs, or trans-
lated/paraphrased/compressed sentences). Thus
structured prediction and its learning algorithm
are of central importance to us NLP researchers.
However, when applying machine learning to
structured domains, we often face scalability is-
sues for two reasons:
1. Even the fastest exact search algorithms for
most NLP problems (such as parsing and
translation) is too slow for repeated use on the
training data, but approximate search (such
as beam search) unfortunately breaks down
the nice theoretical properties (such as con-
vergence) of existing machine learning algo-
rithms.
2. Even with inexact search, the scale of the
training data in NLP still makes pure online
learning (such as perceptron and MIRA) too
slow on a single CPU.
This tutorial reviews recent advances that ad-
dress these two challenges. In particular, we will
cover principled machine learning methods that
are designed to work under vastly inexact search,
and parallelization algorithms that speed up learn-
ing on multiple CPUs. We will also extend struc-
tured learning to the latent variable setting, where
in many NLP applications such as translation and
semantic parsing the gold-standard derivation is
hidden.
2 Contents
1. Overview of Structured Learning
(a) key challenge 1: search efficiency
(b) key challenge 2: interactions between
search and learning
2. Structured Perceptron
(a) the basic algorithm
(b) the geometry of convergence proof
(c) voted and averaged perceptrons, and ef-
ficient implementation tricks
(d) applications in tagging, parsing, etc.
3. Structured Perceptron under Inexact Search
(a) convergence theory breaks under inex-
act search
(b) early update
(c) violation-fixing perceptron
(d) applications in tagging, parsing, etc.
?coffee break?
4. From Perceptron to MIRA
(a) 1-best MIRA; geometric solution
(b) k-best MIRA; hildreth algorithm
(c) MIRA with all constraints; loss-
augmented decoding
(d) MIRA under inexact search
5. Large-Margin Structured Learning with La-
tent Variables
(a) examples: machine translation, seman-
tic parsing, transliteration
(b) separability condition and convergence
proof
(c) latent-variable perceptron under inexact
search
(d) applications in machine translation
6. Parallelizing Large-Margin Structured
Learning
(a) iterative parameter mixing (IPM)
(b) minibatch perceptron and MIRA
4
3 Instructor Biographies
Liang Huang is an Assistant Professor at the City
University of New York (CUNY). He received
his Ph.D. in 2008 from Penn and has worked
as a Research Scientist at Google and a Re-
search Assistant Professor at USC/ISI. His work
is mainly on the theoretical aspects (algorithms
and formalisms) of computational linguistics, as
well as theory and algorithms of structured learn-
ing. He has received a Best Paper Award at ACL
2008, several best paper nominations (ACL 2007,
EMNLP 2008, and ACL 2010), two Google Fac-
ulty Research Awards (2010 and 2013), and a Uni-
versity Graduate Teaching Prize at Penn (2005).
He has given two tutorials at COLING 2008 and
NAACL 2009, being the most popular tutorial at
both venues.
Kai Zhao is a Ph.D. candidate at the City Univer-
sity of New York (CUNY), working with Liang
Huang. He received his B.S. from the Univer-
sity of Science and Technology in China (USTC).
He has published on structured prediction, online
learning, machine translation, and parsing algo-
rithms. He was a summer intern with IBM TJWat-
son Research Center in 2013.
Lemao Liu is a postdoctoral research associate at
the City University of New York (CUNY), work-
ing with Liang Huang. He received his Ph.D. from
the Harbin Institute of Technology in 2013. Much
of his Ph.D. work was done while visiting NICT,
Japan, under Taro Watanabe. His research area is
machine translation and machine learning.
5
