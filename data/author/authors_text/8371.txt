Proceedings of the 12th Conference of the European Chapter of the ACL, pages 799?807,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
MINT: A Method for Effective and Scalable Mining of  
Named Entity Transliterations from Large Comparable Corpora 
Raghavendra Udupa         K Saravanan         A Kumaran        Jagadeesh Jagarlamudi*          
Microsoft Research India 
Bangalore 560080 INDIA 
 [raghavu,v-sarak,kumarana,jags}@microsoft.com 
 
Abstract 
In this paper, we address the problem of min-
ing transliterations of Named Entities (NEs) 
from large comparable corpora. We leverage 
the empirical fact that multilingual news ar-
ticles with similar news content are rich in 
Named Entity Transliteration Equivalents 
(NETEs). Our mining algorithm, MINT, uses 
a cross-language document similarity model to 
align multilingual news articles and then 
mines NETEs from the aligned articles using a 
transliteration similarity model. We show that 
our approach is highly effective on 6 different 
comparable corpora between English and 4 
languages from 3 different language families. 
Furthermore, it performs substantially better 
than a state-of-the-art competitor.   
1 Introduction 
Named Entities (NEs) play a critical role in many 
Natural Language Processing and Information 
Retrieval (IR) tasks.  In Cross-Language Infor-
mation Retrieval (CLIR) systems, they play an 
even more important role as the accuracy of their 
transliterations is shown to correlate highly with 
the performance of the CLIR systems (Mandl 
and Womser-Hacker, 2005, Xu and Weischedel, 
2005).  Traditional methods for transliterations 
have not proven to be very effective in CLIR. 
Machine Transliteration systems (AbdulJaleel 
and Larkey, 2003; Al-Onaizan and Knight, 2002; 
Virga and Khudanpur, 2003) usually produce 
incorrect transliterations and translation lexcions 
such as hand-crafted or statistical dictionaries are 
too static to have good coverage of NEs1 occur-
ring in the current news events. Hence, there is a 
critical need for creating and continually updat-
                                                 
* Currently with University of Utah. 
1 New NEs are introduced to the vocabulary of a lan-
guage every day. On an average, 260 and 452 new 
NEs appeared daily in the XIE and AFE segments of 
the LDC English Gigaword corpora respectively. 
ing multilingual Named Entity transliteration 
lexicons. 
The ubiquitous availability of comparable 
news corpora in multiple languages suggests a 
promising alternative to Machine Transliteration, 
namely, the mining of Named Entity Translitera-
tion Equivalents (NETEs) from such corpora. 
News stories are typically rich in NEs and there-
fore, comparable news corpora can be expected 
to contain NETEs (Klementiev and Roth, 2006; 
Tao et al, 2006). The large quantity and the per-
petual availability of news corpora in many of 
the world?s languages, make mining of NETEs a 
viable alternative to traditional approaches. It is 
this opportunity that we address in our work. 
    In this paper, we detail an effective and scala-
ble mining method, called MINT (MIning 
Named-entity Transliteration equivalents), for 
mining of NETEs from large comparable corpo-
ra. MINT addresses several challenges in mining 
NETEs from large comparable corpora: exhaus-
tiveness (in mining sparse NETEs), computa-
tional efficiency (in scaling on corpora size), 
language independence (in being applicable to 
many language pairs) and linguistic frugality (in 
requiring minimal external linguistic resources).   
Our contributions are as follows: 
? We give empirical evidence for the hypo-
thesis that news articles in different languages 
with reasonably similar content are rich sources 
of NETEs (Udupa, et al, 2008).  
? We demonstrate that the above insight can 
be translated into an effective approach for min-
ing NETEs from large comparable corpora even 
when similar articles are not known a priori. 
? We demonstrate MINT?s effectiveness on 
4 language pairs involving 5 languages (English, 
Hindi, Kannada, Russian, and Tamil) from 3 dif-
ferent language families, and its scalability on 
corpora of vastly different sizes (2,000 to 
200,000 articles).  
? We show that MINT?s performance is sig-
nificantly better than a state of the art method 
(Klementiev and Roth, 2006). 
 
799
We discuss the motivation behind our ap-
proach in Section 2 and present the details in 
Section 3.  In Section 4, we describe the evalua-
tion process and in Section 5, we present the re-
sults and analysis.  We discuss related work in 
Section 6.  
2 Motivation 
MINT is based on the hypothesis that news ar-
ticles in different languages with similar content 
contain highly overlapping set of NEs. News 
articles are typically rich in NEs as news is about 
events involving people, locations, organizations, 
etc2. It is reasonable to expect that multilingual 
news articles reporting the same news event 
mention the same NEs in the respective languag-
es. For instance, consider the English and Hindi 
news reports from the New York Times and the 
BBC on the second oath taking of President Ba-
rack Obama (Figure 1). The articles are not pa-
rallel but discuss the same event. Naturally, they 
mention the same NEs (such as Barack Obama, 
John Roberts, White House) in the respective 
languages, and hence, are rich sources of NETEs.    
Our empirical investigation of comparable 
corpora confirmed the above insight. A study of 
                                                 
2 News articles from the BBC corpus had, on an 
average, 12.9 NEs and new articles from the The 
New Indian Express, about 11.8 NEs. 
 
200 pairs of similar news articles published by 
The New Indian Express in 2007 in English and 
Tamil showed that 87% of the single word NEs 
in the English articles had at least one translitera-
tion equivalent in the conjugate Tamil articles.  
The MINT method leverages this empirically 
backed insight to mine NETEs from such compa-
rable corpora.   
However, there are several challenges to the 
mining process: firstly, vast majority of the NEs 
in comparable corpora are very sparse; our anal-
ysis showed that 80% of the NEs in The New 
Indian Express news corpora appear less than 5 
times in the entire corpora.  Hence, any mining 
method that depends mainly on repeated occur-
rences of the NEs in the corpora is likely to miss 
vast majority of the NETEs.  Secondly, the min-
ing method must restrict the candidate NETEs 
that need to be examined for match to a reasona-
bly small number, not only to minimize false 
positives but also to be computationally efficient.  
Thirdly, the use of linguistic tools and resources 
must be kept to a minimum as resources are 
available only in a handful of languages.  Finally, 
it is important to use as little language-specific 
knowledge as possible in order to make the min-
ing method applicable across a vast majority of 
languages of the world.  The MINT method pro-
posed in this paper addresses all the above is-
sues. 
 
800
3 The MINT Mining Method 
MINT has two stages. In the first stage, for 
every document in the source language side, the 
set of documents in the target language side with 
similar news content are found using a cross-
language document similarity model. In the 
second stage, the NEs in the source language 
side are extracted using a Named Entity Recog-
nizer (NER) and, subsequently, for each NE in a 
source language document, its transliterations are 
mined from the corresponding target language 
documents. We present the details of the two 
stages of MINT in the remainder of this section. 
3.1 Finding Similar Document Pairs  
The first stage of MINT method (Figure 2) works 
on the documents from the comparable corpora 
(CS, CT) in languages S and T and produces a col-
lection AS,T  of similar article pairs (DS, DT).  Each 
article pair (DS, DT) in AS,T consists of an article 
(DS) in language S and an article (DT) in language 
T, that have similar content. The cross-language 
similarity between DS and DT, as measured by the 
cross-language similarity model MD, is at least ? 
> 0. 
 
Cross-language Document Similarity Model: 
The cross-language document similarity model 
measures the degree of similarity between a pair 
of documents in source and target languages.  
We use the negative KL-divergence between 
source and target document probability distribu-
tions as the similarity measure. 
  Given two documents DS, DT in source and tar-
get languages respectively, with 
TS VV , denoting 
the vocabulary of source and target languages, 
the similarity between the two documents is giv-
en by the KL-divergence measure, -KL(DS || DT), 
as: 
?
? TTw ST
TT
ST
V Dwp
DwpDwp )|(
)|(log)|(
  
where p(w | D) is the likelihood of word w in D. 
As we are interested in target documents which 
are similar to a given source document, we can 
ignore the numerator as it is independent of the 
target document.  Finally, expanding p(wT | Ds) 
as 
)|()|( SVw TSS wwpDwpSS??
we specify the 
cross-language similarity score as follows: 
 
Cross-language similarity =       
)|(log)|()|( TTSTw w SS DwpwwpDwpTVT SVS? ?? ?
 
 
3.2 Mining NETEs from Document Pairs  
The second stage of the MINT method works on 
each pair of articles (DS, DT) in the collection AS,T  
and produces a set PS,T of NETEs. Each pair (?S, 
?T) in PS,T  consists of an NE ?S in language S, and 
a token ?T in language T, that are transliteration 
equivalents of each other.  Furthermore, the 
transliteration similarity between ?S and ?T, as 
measured by the transliteration similarity model 
MT, is at least ? > 0. Figure 3 outlines this algo-
rithm.  
 
Discriminative Transliteration Similarity 
Model:  
The transliteration similarity model MT measures 
the degree of transliteration equivalence between 
a source language and a target language term.  
Input: Comparable news corpora (CS, CT) in languages (S,T)  
           Crosslanguage Document Similarity Model MD for (S, T) 
           Threshold score ?. 
Output: Set AS,T of pairs of similar articles (DS, DT) from (CS, CT). 
1 AS,T  ? ? ;         // Set of Similar articles (DS, DT) 
2 for each article DS in CS do 
3     XS   ? ? ;       // Set of candidates for DS. 
4      for each article dT  in CT  do 
5         score = CrossLanguageDocumentSimilarity(DS,dT,MD); 
6         if (score ? ?) then XS  ? XS  ? (dT , score) ; 
7      end 
8     DT  = BestScoringCandidate(XS); 
9    if (DT  ? ?) then AS,T  ? AS,T  ? (DS, DT) ; 
10 end 
CrossLanguageSimilarDocumentPairs 
Figure 2. Stage 1 of MINT 
Input:  
      Set AS,T  of similar documents (DS, DT)  in languages  
(S,T),   
      Transliteration Similarity Model MT for (S, T),  
      Threshold score ?. 
Output: Set PS,T  of NETEs (?S, ?T) from  AS,T ; 
1   PS,T  ? ? ;  
2   for each pair of articles (DS, DT) in AS,T  do 
3        for each named entity ?S in DS do  
4            YS ? ? ; // Set of candidates for ?S. 
5            for each candidate eT  in DT  do 
6                 score = TransliterationSimilarity(?S, eT, MT) ; 
7                 if (score ? ?)  then   YS  ?  YS ? (eT , score) ; 
8            end 
9            ?T  = BestScoringCandidate(YS) ;  
10          if (?T  ? null) then PS,T  ?  PS,T  ? (?S, ?T) ; 
11      end 
12 end 
TransliterationEquivalents 
Figure 3. Stage 2 of MINT 
801
We employ a logistic function as our translitera-
tion similarity model MT, as follows: 
 
 TransliterationSimilarity (?S,eT,MT) = 
),( TS1
1
ewte ?????
 
where ? (?S, eT) is the feature vector for the pair 
(?S, eT) and w is the weights vector.  Note that the 
transliteration similarity takes a value in the 
range [0..1]. The weights vector w is learnt dis-
criminatively over a training corpus of known 
transliteration equivalents in the given pair of 
languages. 
 
Features: The features employed by the model 
capture interesting cross-language associations 
observed in (?S, eT): 
 
? All unigrams and bigrams from the 
source and target language strings. 
? Pairs of source string n-grams and target 
string n-grams such that difference in the 
start positions of the source and target n-
grams is at most 2. Here n ? ?2,1? . 
? Difference in the lengths of the two 
strings.  
 
Generative Transliteration Similarity Model: 
We also experimented with an extension of He?s 
W-HMM model (He, 2007). The transition prob-
ability depends on both the jump width and the 
previous source character as in the W-HMM 
model. The emission probability depends on the 
current source character and the previous target 
character unlike the W-HMM model (Udupa et 
al., 2009). Instead of using any single alignment 
of characters in the pair (wS, wT), we marginalize 
over all possible alignments: 
? ? ? ? ? ?11
1
11 ,|,|| 1 ??
?
???? jajajj
A
m
j
nm tstpsaapstP
jj
 
 
Here, 
jt
(and resp. 
is ) denotes the j
th (and resp. 
ith) character in wT (and resp. wS) and maA 1? is 
the hidden alignment between wT and wS where 
jt
is aligned to 
jas
, ,m,j ?1? . We estimate 
the parameters of the model using the EM algo-
rithm. The transliteration similarity score of a 
pair (wS, wT) is log P(wT  | wS) appropriately trans-
formed. 
 
 
4 Experimental Setup 
Our empirical investigation consists of experi-
ments in three data environments, with each en-
vironment providing answer to specific set of 
questions, as listed below: 
 
1. Ideal Environment (IDEAL): Given a collec-
tion AS,T of oracle-aligned article pairs (DS, DT) 
in S and T, how effective is Stage 2 of MINT in 
mining NETE from AS,T? 
2. Near Ideal Environment (NEAR-IDEAL): 
Let AS,T  be a collection of similar article pairs 
(DS, DT) in S and T. Given comparable corpora 
(CS, CT) consisting of only articles from AS,T, but 
without the knowledge of pairings between the 
articles,  
a. How effective is Stage 1 of MINT in re-
covering AS,T  from (CS, CT) ? 
b. What is the effect of Stage 1 on the 
overall effectiveness of MINT? 
3. Real Environment (REAL): Given large 
comparable corpora (CS, CT), how effective is 
MINT, end-to-end? 
 
The IDEAL environment is indeed ideal for 
MINT since every article in the comparable cor-
pora is paired with exactly one similar article in 
the other language and the pairing of articles in 
the comparable corpora is known in advance.  
We want to emphasize here that such corpora are 
indeed available in many domains such as tech-
nical documents and interlinked multilingual 
Wikipedia articles. In the IDEAL environment, 
only Stage 2 of MINT is put to test, as article 
alignments are given.  
In the NEAR-IDEAL data environment, every 
article in the comparable corpora is known to 
have exactly one conjugate article in the other 
language though the pairing itself is not known 
in advance.  In such a setting, MINT needs to 
discover the article pairing before mining NETEs 
and therefore, both stages of MINT are put to 
test.  The best performance possible in this envi-
ronment should ideally be the same as that of 
IDEAL, and any degradation points to the short-
coming of the Stage 1 of MINT.  These two en-
vironments quantify the stage-wise performance 
of the MINT method.    
Finally, in the data environment REAL, we 
test MINT on large comparable corpora, where 
even the existence of a conjugate article in the 
target side for a given article in the source side of 
the comparable corpora is not guaranteed, as in 
802
any normal large multilingual news corpora. In 
this scenario both the stages of MINT are put to 
test.  This is the toughest, and perhaps the typical 
setting in which MINT would be used.  
4.1 Comparable Corpora 
In our experiments, the source language is Eng-
lish whereas the 4 target languages are from 
three different language families (Hindi from the 
Indo-Aryan family, Russian from the Slavic fam-
ily, Kannada and Tamil from the Dravidian fami-
ly). Note that none of the five languages use a 
common script and hence identification of cog-
nates, spelling variations, suffix transformations, 
and other techniques commonly used for closely 
related languages that have a common script are 
not applicable for mining NETEs.  Table 1 sum-
marizes the 6 different comparable corpora that 
were used for the empirical investigation; 4 for 
the IDEAL and NEAR-IDEAL environments (in 
4 language pairs), and 2 for the REAL environ-
ment (in 2 language pairs). 
 
Cor-
pus 
Source -
Target 
Data 
Environ-
ment 
Articles (in 
Thousands) 
Words (in 
Millions) 
Src Tgt Src Tgt 
EK-S 
English- 
Kannada 
IDEAL& 
NEAR-IDEAL 
2.90 2.90 0.42 0.34 
ET-S 
English- 
Tamil 
IDEAL& 
NEAR-IDEAL 
2.90 2.90 0.42 0.32 
ER-S 
English- 
Russian 
IDEAL& 
NEAR-IDEAL 
2.30 2.30 1.03 0.40 
EH-S 
English- 
Hindi 
IDEAL& 
NEAR-IDEAL 
11.9 11.9 3.77 3.57 
EK-L 
English- 
Kannada 
REAL 103.8 111.0 27.5 18.2 
ET-L 
English- 
Tamil 
REAL 103.8 144.3 27.5 19.4 
Table 1: Comparable Corpora 
 
The corpora can be categorized into two sepa-
rate groups, group S (for Small) consisting of 
EK-S, ET-S, ER-S, and EH-S and group L (for 
Large) consisting of EK-L and ET-L. Corpora in 
group S are relatively small in size, and contain 
pairs of articles that have been judged by human 
annotators as similar. Corpora in group L are two 
orders of magnitude larger in size than those in 
group S and contain a large number of articles 
that may not have conjugates in the target side. 
In addition the pairings are unknown even for the 
articles that have conjugates. All comparable 
corpora had publication dates, except EH-S, 
which is known to have been published over the 
same year. 
The EK-S, ET-S, EK-L and ET-L corpora are 
from The New Indian Express news paper, whe-
reas the EH-S corpora are from Web Dunia and 
the ER-S corpora are from BBC/Lenta News 
Agency respectively. 
4.2 Cross-language Similarity Model  
The cross-language document similarity model 
requires a bilingual dictionary in the appropriate 
language pair. Therefore, we generated statistical 
dictionaries for 3 language pairs (from parallel 
corpora of the following sizes: 11K sentence 
pairs in English-Kannada, 54K in English-Hindi, 
and 14K in English-Tamil) using the GIZA++ 
statistical alignment tool (Och et al, 2003), with 
5 iterations each of IBM Model 1 and HMM.  
We did not have access to an English-Russian 
parallel corpus and hence could not generate a 
dictionary for this language pair. Hence, the 
NEAR-IDEAL experiments were not run for the 
English-Russian language pair.   
Although the coverage of the dictionaries was 
low, this turned out to be not a serious issue for 
our cross-language document similarity model as 
it might have for topic based CLIR (Ballesteros 
and Croft, 1998). Unlike CLIR, where the query 
is typically smaller in length compared to the 
documents, in our case we are dealing with news 
articles of comparable size in both source and 
target languages.  
When many translations were available for a 
source word, we considered only the top-4 trans-
lations.  Further, we smoothed the document 
probability distributions with collection frequen-
cy as described in (Ponte and Croft, 1998). 
4.3 Transliteration Similarity Model  
The transliteration similarity models for each of 
the 4 language pairs were produced by learning 
over a training corpus consisting of about 16,000 
single word NETEs, in each pair of languages.  
The training corpus in English-Hindi, English-
Kannada and English-Tamil were hand-crafted 
by professionals, the English-Russian name pairs 
were culled from Wikipedia interwiki links and 
were cleaned heuristically.  Equal number of 
negative samples was used for training the mod-
els. To produce the negative samples, we paired 
each source language NE with a random non-
matching target language NE.  No language spe-
cific features were used and the same feature set 
was used in each of the 4 language pairs making 
MINT language neutral.   
In all the experiments, our source side lan-
guage is English, and the Stanford Named Entity 
Recognizer (Finkel et al 2005) was used to ex-
tract NEs from the source side article.  It should 
be noted here that while the precision of the NER 
803
used was consistently high, its recall was low, 
(~40%) especially in the New Indian Express 
corpus, perhaps due to the differences in the data 
used for training the NER and the data on which 
we used it.   
4.4 Performance Measures  
Our intention is to measure the effectiveness of 
MINT by comparing its performance with the 
oracular (human annotator) performance.  As 
transliteration equivalents must exist in the 
paired articles to be found by MINT, we focus 
only on those NEs that actually have at least one 
transliteration equivalent in the conjugate article. 
Three performance measures are of interest to 
us: the fraction of distinct NEs from source lan-
guage for which we found at least one translitera-
tion in the target side (Recall on distinct NEs), 
the fraction of distinct NETEs (Recall on distinct 
NETEs) and the Mean Reciprocal Rank (MRR) 
of the NETEs mined.  Since we are interested in 
mining not only the highly frequent but also the 
infrequent NETEs, recall metrics measure how 
effective our method is in mining NETEs ex-
haustively. The MRR score indicates how effec-
tive our method is in preferring the correct ones 
among candidates. 
To measure the performance of MINT, we 
created a test bed for each of the language pairs. 
The test beds are summarized in Table 2.  
The test beds consist of pairs of similar ar-
ticles in each of the language pairs. It should be 
noted here that as transliteration equivalents must 
exist in the paired articles to be found by MINT, 
we focus only on those NEs that actually have at 
least one transliteration equivalent in the conju-
gate article. 
5 Results & Analysis 
In this section, we present qualitative and quan-
titative performance of the MINT algorithm, in 
mining NETEs from comparable news corpora. 
All the results in Sections 5.1 to 5.3 were ob-
tained using the discriminative transliteration 
similarity model described in Section 3.2. The 
results using the generative transliteration simi-
larity model are discussed in Section 5.4. 
5.1 IDEAL Environment 
Our first set of experiments investigated the ef-
fectiveness of Stage 2 of MINT, namely the min-
ing of NETEs in an IDEAL environment. As 
MINT is provided with paired articles in this ex-
periment, all experiments for this environment 
were run on test beds created from group S cor-
pora (Table 2).  
 
 
Results in the IDEAL Environment:  
The recall measures for distinct NEs and distinct 
NETEs for the IDEAL environment are reported 
in Table 3.  
 
Test 
Bed 
Recall (%) 
Distinct NEs Distinct NETEs 
EK-ST 97.30 95.07 
ET-ST 99.11 98.06 
EH-ST 98.55 98.66 
ER-ST 93.33 85.88 
 Table 3: Recall of MINT in IDEAL 
 
Note that in the first 3 language pairs MINT was 
able to mine a transliteration equivalent for al-
most all the distinct NEs. The performance in 
English-Russian pair was relatively worse, per-
haps due to the noisy training data.   
In order to compare the effectiveness of 
MINT with a state-of-the-art NETE mining ap-
proach, we implemented the time series based 
Co-Ranking algorithm based on (Klementiev and 
Roth, 2006).  
 
Table 4 shows the MRR results in the IDEAL 
environment ? both for MINT and the Co-
Ranking baseline: MINT outperformed Co-
Ranking on all the language pairs, despite not 
using time series similarity in the mining 
process.  The high MRRs (@1 and @5) indicate 
that in almost all the cases, the top-ranked candi-
date is a correct NETE.  Note that Co-Ranking 
could not be run on the EH-ST test bed as the 
articles did not have a date stamp. Co-Ranking is 
crucially dependent on time series and hence re-
quires date stamps for the articles. 
 
Test Bed 
Comparable 
Corpora 
Article 
Pairs 
Distinct 
NEs 
Distinct 
NETEs 
EK-ST EK-S 200 481 710 
ET-ST ET-S 200 449 672 
EH-ST EH-S 200 347 373 
ER-ST ER-S 100 195 347 
Table 2: Test Beds for IDEAL & NEAR-IDEAL 
Test 
Bed 
MRR@1 MRR@5 
MINT CoRanking MINT CoRanking 
EK-ST 0.94 0.26 0.95 0.29 
ET-ST 0.91 0.26 0.94 0.29 
EH-ST 0.93 - 0.95 - 
ER-ST 0.80 0.38 0.85 0.43 
Table 4: MINT & Co-Ranking in IDEAL 
804
5.2 NEAR-IDEAL Environment 
The second set of experiments investigated the 
effectiveness of Stage 1 of MINT on comparable 
corpora that are constituted by pairs of similar 
articles, where the pairing information between 
the articles is with-held.  MINT reconstructed the 
pairings using the cross-language document si-
milarity model and subsequently mined NETEs. 
As in previous experiments, we ran our experi-
ments on test beds described in Section 4.4. 
 
Results in the NEAR-IDEAL Environment: 
There are two parts to this set of experiments. In 
the first part, we investigated the effectiveness of 
the cross-language document similarity model 
described in Section 3.1. Since we know the 
identity of the conjugate article for every article 
in the test bed, and articles can be ranked accord-
ing to the cross-language document similarity 
score, we simply computed the MRR for the 
documents identified in each of the test beds, 
considering only the top-2 results. Further, where 
available, we made use of the publication date of 
articles to restrict the number of target articles 
that are considered in lines 4 and 5 of the MINT 
algorithm in Figure 2.  Table 5 shows the results 
for two date windows ? 3 days and 1 year. 
 
 Test 
Bed 
MRR@1 MRR@2 
3 days 1 year 3 days 1 year 
EK-ST 0.99 0.91 0.99 0.93 
ET-ST 0.96 0.83 0.97 0.87 
EH-ST - 0.81 - 0.82 
Table 5: MRR of Stage 1 in NEAR-IDEAL 
 
Subsequently, the output of the Stage 1 was giv-
en as the input to the Stage 2 of the MINT me-
thod. In Table 6 we report the MRR @1 and @5 
for the second stage, for both time windows (3 
days & 1 year). 
 
It is interesting to compare the results of MINT 
in NEAR-IDEAL data environment (Table 6) 
with MINT?s results in IDEAL environment 
(Table 4). The drop in MRR@1 is small: ~2% 
for EK-ST and ~3% for ET-ST. For EH-ST the 
drop is relatively more (~12%) as may be ex-
pected since the time window (3 days) could not 
be applied for this test bed.  
5.3 REAL Environment 
The third set of experiments investigated the ef-
fectiveness of MINT on large comparable corpo-
ra. We ran the experiments on test beds created 
from group L corpora.   
 
 Test-beds for the REAL Environment: The 
test beds for the REAL environment (Table 7) 
consisted of only English articles since we do not 
know in advance whether these articles have any 
similar articles in the target languages. 
 
 Results in the REAL Environment: In real 
environment, we examined the top 2 articles of 
returned by Stage 1 of MINT, and mined NETEs 
from them. We used a date window of 3 in Stage 
1. Table 8 summarizes the results for the REAL 
environment. 
 
We observe that the performance of MINT is 
impressive, considering the fact that the compa-
rable corpora used in the REAL environment is 
two orders of magnitude larger than those used in 
IDEAL and NEAR-IDEAL environments. This 
implies that MINT is able to effectively mine 
NETEs whenever the Stage 1 algorithm was able 
to find a good conjugate for each of the source 
language articles.  
5.4 Generative Transliteration Similarity 
Model 
We employed the extended W-HMM translitera-
tion similarity model in MINT and used it in the 
IDEAL data environment.  Table 9 shows the 
results. 
Test 
Bed 
MRR@1 MRR@5 
3 days 1 year 3 days 1 year 
EK-ST 0.92 0.87 0.94 0.90 
ET-ST 0.88 0.74 0.91 0.78 
EH-ST - 0.82 - 0.87 
Table 6: MRR of Stage 2 in NEAR-IDEAL 
Test 
Bed 
Comparable 
Corpora 
Articles 
Distinct  
NEs 
EK-LT EK-L 100 306 
ET-LT ET-L 100 228 
Table 7: Test Beds for REAL 
 
Test Bed 
MRR 
@1 @5 
EK-LT 0.86 0.88 
ET-LT 0.82 0.85 
Table 8: MRR of Stage 2 in REAL 
Test Bed 
MRR 
@1 @5 
EK-S 0.85 0.86 
ET-S 0.81 0.82 
EH-S 0.91 0.93 
Table 9:  MRR of Stage 2 in IDEAL using genera-
tive transliteration similarity model 
805
We see that the results for the generative transli-
teration similarity model are good but not as 
good as those for the discriminative translitera-
tion similarity model. As we did not stem either 
the English NEs or the target language words, 
the generative model made more mistakes on 
inflected words compared to the discriminative 
model.   
5.5  Examples of Mined NETEs 
Table 10 gives some examples of the NETEs 
mined from the comparable news corpora.  
 
6  Related Work 
CLIR systems have been studied in several 
works (Ballesteros and Croft, 1998; Kraiij et al 
2003). The limited coverage of dictionaries has 
been recognized as a problem in CLIR and MT 
(Demner-Fushman & Oard, 2002; Mandl & 
Womser-hacker, 2005; Xu &Weischedel, 2005).  
In order to address this problem, different 
kinds of approaches have been taken, from learn-
ing transformation rules from dictionaries and 
applying the rules to find cross-lingual spelling 
variants (Pirkola et al, 2003), to  learning trans-
lation lexicon from monolingual and/or compa-
rable corpora (Fung, 1995; Al-Onaizan and 
Knight, 2002; Koehn and Knight, 2002; Rapp, 
1996). While these works have focused on find-
ing translation equivalents of all class of words, 
we focus specifically on transliteration equiva-
lents of NEs.  (Munteanu and Marcu, 2006; 
Quirk et al, 2007) addresses mining of parallel 
sentences and fragments from nearly parallel 
sentences. In contrast, our approach mines 
NETEs from article pairs that may not even have 
any parallel or nearly parallel sentences.   
NETE discovery from comparable corpora 
using time series and transliteration model was 
proposed in (Klementiev and Roth, 2006), and 
extended for NETE mining for several languages 
in (Saravanan and Kumaran, 2007).  However, 
such methods miss vast majority of the NETEs 
due to their dependency on frequency signatures.   
In addition, (Klementiev and Roth, 2006) may 
not scale for large corpora, as they examine 
every word in the target side as a potential trans-
literation equivalent. NETE mining from compa-
rable corpora using phonetic mappings was pro-
posed in (Tao et al, 2006), but the need for lan-
guage specific knowledge restricts its applicabili-
ty across languages.  We proposed the idea of 
mining NETEs from multilingual articles with 
similar content in (Udupa, et al, 2008). In this 
work, we extend the approach and provide a de-
tailed description of the empirical studies. 
7  Conclusion 
In this paper, we showed that MINT, a simple 
and intuitive technique employing cross-
language document similarity and transliteration 
similarity models, is capable of mining NETEs 
effectively from large comparable news corpora. 
Our three stage empirical investigation showed 
that MINT performed close to optimal on com-
parable corpora consisting of pairs of similar ar-
ticles when the pairings are known in advance. 
MINT induced fairly good pairings and performs 
exceedingly well even when the pairings are not 
known in advance. Further, MINT outperformed 
a state-of-the-art baseline and scaled to large 
comparable corpora.  Finally, we demonstrated 
the language neutrality of MINT, by mining 
NETEs from 4 language pairs (between English 
and one of Russian, Hindi, Kannada or Tamil) 
from 3 vastly different linguistic families. 
As a future work, we plan to use the ex-
tended W-HMM model to get features for the 
discriminative transliteration similarity model. 
We also want to use a combination of the cross-
language document similarity score and the 
transliteration similarity score for scoring the 
NETEs. Finally, we would like to use the mined 
NETEs to improve the performance of the first 
stage of MINT. 
Acknowledgments 
We thank Abhijit Bhole for his help and Chris 
Quirk for valuable comments. 
 
Language 
Pair 
Source NE Transliteration 
English-
Kannada 
Woolmer ??????? 
Kafeel ????? 
Baghdad ???????? 
English-Tamil Lloyd ??????  
Mumbai ?????????? 
Manchester ??????????? 
English-Hindi Vanhanen ??????? 
Trinidad ???????????  
Ibuprofen ?????????? 
English-
Russian 
Kreuzberg ?????????? 
Gaddafi ??????? 
Karadzic ???????? 
Table 10: Examples of Mined NETEs 
806
References 
AbdulJaleel, N. and Larkey, L.S. 2003. Statistical translite-
ration for English-Arabic cross language information re-
trieval. Proceedings of CIKM 2003.  
Al-Onaizan, Y. and Knight, K. 2002. Translating named 
entities using monolingual and bilingual resources. Pro-
ceedings of the 40th Annual Meeting of ACL. 
Ballesteros, L. and Croft, B. 1998. Dictionary Methods for 
Cross-Lingual Information Retrieval. Proceedings of 
DEXA?96.  
Chen, H., et al 1998. Proper Name Translation in Cross-
Language Information Retrieval. Proceedings of the 36th 
Annual Meeting of the ACL. 
Demner-Fushman, D., and Oard, D. W. 2002. The effect of 
bilingual term list size on dictionary-based cross-
language information retrieval. Proceedings of the 36th 
Hawaii International Conference on System Sciences.  
Finkel, J. Trond Grenager, and Christopher Manning. 2005. 
Incorporating Non-local Information into Information 
Extraction Systems by Gibbs Sampling. Proceedings of 
the 43nd Annual Meeting of the ACL. 
Fung, P. 1995. Compiling bilingual lexicon entries from a 
non-parallel English-Chinese corpus. Proceedings of the 
3rd Workshop on Very Large Corpora. 
Fung, P. 1995. A pattern matching method for finding noun 
and proper noun translations from noisy parallel corpora.  
Proceedings of ACL 1995.  
He. X. 2007: Using word dependent transition models in 
HMM based word alignment for statistical machine 
translation. In Proceedings of 2nd ACL Workshop on Sta-
tistical Machine Translation . 
Hermjakob, U., Knight, K., and Daume, H. 2008. Name 
translation in statistical machine translation: knowing 
when to transliterate. Proceedings ACL 2008. 
Klementiev, A. and Roth, D. 2006. Weakly supervised 
named entity transliteration and discovery from multilin-
gual comparable corpora. Proceedings of the 44th Annual 
Meeting of the ACL.  
Knight, K. and Graehl, J. 1998. Machine Transliteration. 
Computational Linguistics.  
Koehn, P. and Knight, K. 2002. Learning a translation lex-
icon from monolingual corpora. Proceedings of Unsu-
pervised Lexical Acquisition. 
Kraiij, W., Nie, J-Y. and  Simard, M. 2003. Emebdding 
Web-based Statistical Translation Models in Cross-
Language Information Retrieval. Computational Linguis-
tics., 29(3):381-419. 
Mandl, T., and Womser-Hacker, C.  2004. How do named 
entities contribute to retrieval effectiveness? Proceedings 
of the 2004 Cross Language Evaluation Forum Cam-
paign 2004. 
Mandl, T., and Womser-Hacker, C.  2005. The Effect of 
named entities on effectiveness in cross-language infor-
mation retrieval evaluation. ACM Symposium on Applied 
Computing.  
Munteanu, D. and Marcu D. 2006. Extracting parallel sub-
sentential fragments from non-parallel corpora. Proceed-
ings of the ACL 2006. 
Och, F. and Ney, H. 2003. A systematic comparison of var-
ious statistical alignment models. Computational Lin-
guistics. 
Pirkola, A., Toivonen, J., Keskustalo, H., Visala, K. and 
Jarvelin, K. 2003. Fuzzy translation of cross-lingual 
spelling variants. Proceedings of SIGIR 2003.  
Ponte, J. M. and Croft, B. 1998. A Language Modeling 
Approach to Information Retrieval. Proceedings of ACM 
SIGIR 1998.  
Quirk, C., Udupa, R. and Menezes, A. 2007. Generative 
models of noisy translations with applications to parallel 
fragments extraction. Proceedings of the 11th MT Sum-
mit. 
Rapp, R. 1996. Automatic identification of word transla-
tions from unrelated English and German corpora. Pro-
ceedings of ACL?99 
Saravanan, K. and Kumaran, A. 2007. Some experiments in 
mining named entity transliteration pairs from compara-
ble corpora. Proceedings of the 2nd International Work-
shop on Cross Lingual Information Access. 
Tao, T., Yoon, S., Fister, A., Sproat, R. and Zhai, C. 2006. 
Unsupervised named entity transliteration using temporal 
and phonetic correlation. Proceedings of EMNLP 2006.  
Udupa, R., Saravanan, K., Kumaran, A. and Jagarlamudi, J.  
2008.  Mining Named Entity Transliteration Equivalents 
from Comparable Corpora. Proceedings of the CIKM 
2008. 
Udupa, R., Saravanan, K., Bakalov, A. and Bhole, A.  2009.  
?They are out there if you know where to look?: Mining 
transliterations of OOV terms in cross-language informa-
tion retrieval. Proceedings of the ECIR 2009. 
Virga, P. and Khudanpur, S. 2003. Transliteration of proper 
names in cross-lingual information retrieval. Proceedings 
of the ACL Workshop on Multilingual and Mixed Lan-
guage Named Entity Recognition.  
Xu, J. and Weischedel, R. 2005. Empirical studies on the 
impact of lexical resources on CLIR performance. In-
formation Processing and Management. 
807
Computational Complexity of Statistical Machine Translation
Raghavendra Udupa U.
IBM India Research Lab
New Delhi
India
uraghave@in.ibm.com
Hemanta K. Maji
Dept. of Computer Science
University of Illinois at Urbana-Champaigne
hemanta.maji@gmail.com
Abstract
In this paper we study a set of prob-
lems that are of considerable importance
to Statistical Machine Translation (SMT)
but which have not been addressed satis-
factorily by the SMT research community.
Over the last decade, a variety of SMT
algorithms have been built and empiri-
cally tested whereas little is known about
the computational complexity of some of
the fundamental problems of SMT. Our
work aims at providing useful insights into
the the computational complexity of those
problems. We prove that while IBM Mod-
els 1-2 are conceptually and computation-
ally simple, computations involving the
higher (and more useful) models are hard.
Since it is unlikely that there exists a poly-
nomial time solution for any of these hard
problems (unless P = NP and P#P =
P), our results highlight and justify the
need for developing polynomial time ap-
proximations for these computations. We
also discuss some practical ways of deal-
ing with complexity.
1 Introduction
Statistical Machine Translation is a data driven
machine translation technique which uses proba-
bilistic models of natural language for automatic
translation (Brown et al, 1993), (Al-Onaizan et
al., 1999). The parameters of the models are
estimated by iterative maximum-likelihood train-
ing on a large parallel corpus of natural language
texts using the EM algorithm (Brown et al, 1993).
The models are then used to decode, i.e. trans-
late texts from the source language to the target
language 1 (Tillman, 2001), (Wang, 1997), (Ger-
mann et al, 2003), (Udupa et al, 2004). The
models are independent of the language pair and
therefore, can be used to build a translation sys-
tem for any language pair as long as a parallel
corpus of texts is available for training. Increas-
ingly, parallel corpora are becoming available
for many language pairs and SMT systems have
been built for French-English, German-English,
Arabic-English, Chinese-English, Hindi-English
and other language pairs (Brown et al, 1993), (Al-
Onaizan et al, 1999), (Udupa, 2004).
In SMT, every English sentence e is considered
as a translation of a given French sentence f with
probability Pr (f |e). Therefore, the problem of
translating f can be viewed as a problem of finding
the most probable translation of f :
e? = argmax
e
Pr(e|f) = argmax
e
Pr(f |e)P (e).
(1)
The probability distributions Pr(f |e) and
Pr(e) are known as translation model and lan-
guage model respectively. In the classic work on
SMT, Brown and his colleagues at IBM introduced
the notion of alignment between a sentence f and
its translation e and used it in the development of
translation models (Brown et al, 1993). An align-
ment between f = f1 . . . fm and e = e1 . . . el
is a many-to-one mapping a : {1, . . . ,m} ?
{0, . . . , l}. Thus, an alignment a between f and e
associates the french word fj to the English word
eaj 2. The number of words of f mapped to ei by
a is called the fertility of ei and is denoted by ?i.
Since Pr(f |e) = ?a Pr(f ,a|e), equation 1 can
1In this paper, we use French and English as the prototyp-
ical examples of source and target languages respectively.
2e0 is a special word called the null word and is used to
account for those words in f that are not connected by a to
any of the words of e.
25
be rewritten as follows:
e? = argmax
e
?
a
Pr(f ,a|e)Pr(e). (2)
Brown and his colleagues developed a series
of 5 translation models which have become to be
known in the field of machine translation as IBM
models. For a detailed introduction to IBM trans-
lation models, please see (Brown et al, 1993). In
practice, models 3-5 are known to give good re-
sults and models 1-2 are used to seed the EM it-
erations of the higher models. IBM model 3 is
the prototypical translation model and it models
Pr(f ,a|e) as follows:
P (f ,a|e) ? n
(
?0|
?l
i=1 ?i
)
?l
i=1 n (?i|ei)?i!
??mj=1 t
(
fj|eaj
)
??j: aj 6=0 d (j|i,m, l)
Table 1: IBM Model 3
Here, n(?|e) is the fertility model, t(f |e) is
the lexicon model and d(j|i,m, l) is the distortion
model.
The computational tasks involving IBM Models
are the following:
? Viterbi Alignment
Given the model parameters and a sentence
pair (f , e), determine the most probable
alignment between f and e.
a? = argmax
a
P (f ,a|e)
? Expectation Evaluation
This forms the core of model training via the
EM algorithm. Please see Section 2.3 for
a description of the computational task in-
volved in the EM iterations.
? Conditional Probability
Given the model parameters and a sentence
pair (f , e), compute P (f |e).
P (f |e) =
?
a
P (f ,a|e)
? Exact Decoding
Given the model parameters and a sentence f ,
determine the most probable translation of f .
e? = argmax
e
?
a
P (f ,a|e) P (e)
? Relaxed Decoding
Given the model parameters and a sentence f ,
determine the most probable translation and
alignment pair for f .
(e?,a?) = argmax
(e,a)
P (f ,a|e) P (e)
Viterbi Alignment computation finds applica-
tions not only in SMT but also in other areas
of Natural Language Processing (Wang, 1998),
(Marcu, 2002). Expectation Evaluation is the
soul of parameter estimation (Brown et al, 1993),
(Al-Onaizan et al, 1999). Conditional Proba-
bility computation is important in experimentally
studying the concentration of the probability mass
around the Viterbi alignment, i.e. in determining
the goodness of the Viterbi alignment in compar-
ison to the rest of the alignments. Decoding is
an integral component of all SMT systems (Wang,
1997), (Tillman, 2000), (Och et al, 2001), (Ger-
mann et al, 2003), (Udupa et al, 2004). Exact
Decoding is the original decoding problem as de-
fined in (Brown et al, 1993) and Relaxed Decod-
ing is the relaxation of the decoding problem typ-
ically used in practice.
While several heuristics have been developed
by practitioners of SMT for the computational
tasks involving IBM models, not much is known
about the computational complexity of these tasks.
In their seminal paper on SMT, Brown and his col-
leagues highlighted the problems we face as we go
from IBM Models 1-2 to 3-5(Brown et al, 1993)
3:
?As we progress from Model 1 to Model 5, eval-
uating the expectations that gives us counts be-
comes increasingly difficult. In Models 3 and 4,
we must be content with approximate EM itera-
tions because it is not feasible to carry out sums
over all possible alignments for these models. In
practice, we are never sure that we have found the
Viterbi alignment?.
However, neither their work nor the subsequent
research in SMT studied the computational com-
plexity of these fundamental problems with the
exception of the Decoding problem. In (Knight,
1999) it was proved that the Exact Decoding prob-
lem is NP-Hard when the language model is a bi-
gram model.
Our results may be summarized as follows:
3The emphasis is ours.
26
1. Viterbi Alignment computation is NP-Hard
for IBM Models 3, 4, and 5.
2. Expectation Evaluation in EM Iterations is
#P-Complete for IBM Models 3, 4, and 5.
3. Conditional Probability computation is
#P-Complete for IBM Models 3, 4, and 5.
4. Exact Decoding is #P-Hard for IBM Mod-
els 3, 4, and 5.
5. Relaxed Decoding is NP-Hard for IBM
Models 3, 4, and 5.
Note that our results for decoding are sharper
than that of (Knight, 1999). Firstly, we show that
Exact Decoding is #P-Hard for IBM Models 3-5
and not just NP-Hard. Secondly, we show that
Relaxed Decoding is NP-Hard for Models 3-5
even when the language model is a uniform dis-
tribution.
The rest of the paper is organized as follows.
We formally define all the problems discussed in
the paper (Section 2). Next, we take up each of the
problems discussed in this section and derive the
stated result for them (Section 3). After this, we
discuss the implications of our results (Section 4)
and suggest future directions (Section 5).
2 Problem Definition
Consider the functions f, g : ?? ? {0, 1}. We
say that g ?mp f (g is polynomial-time many-one
reducible to f ), if there exists a polynomial time
reduction r(.) such that g(x) = f(r(x)) for all
input instances x ? ??. This means that given a
machine to evaluate f(.) in polynomial time, there
exists a machine that can evaluate g(.) in polyno-
mial time. We say a function f is NP-Hard, if all
functions in NP are polynomial-time many-one
reducible to f . In addition, if f ? NP, then we
say that f is NP-Complete.
Also relevant to our work are counting func-
tions that answer queries such as ?how many com-
putation paths exist for accepting a particular in-
stance of input?? Let w be a witness for the ac-
ceptance of an input instance x and ?(x,w) be
a polynomial time witness checking function (i.e.
?(x,w) ? P). The function f : ?? ? N such that
f(x) =
?
w???
|w|?p(|x|)
?(x,w)
lies in the class #P, where p(.) is a polynomial.
Given functions f, g : ?? ? N, we say that g is
polynomial-time Turing reducible to f (i.e. g ?T
f ) if there is a Turing machine with an oracle for
f that computes g in time polynomial in the size
of the input. Similarly, we say that f is #P-Hard,
if every function in #P can be polynomial time
Turing reduced to f . If f is #P-Hard and is in
#P, then we say that f is #P-Complete.
2.1 Viterbi Alignment Computation
VITERBI-3 is defined as follows. Given the para-
meters of IBM Model 3 and a sentence pair (f , e),
compute the most probable alignment a? betwen f
and e:
a? = argmax
a
P (f ,a|e).
2.2 Conditional Probability Computation
PROBABILITY-3 is defined as follows. Given
the parameters of IBM Model 3, and a sen-
tence pair (f , e), compute the probability
P (f |e) =?a P (f ,a|e).
2.3 Expectation Evaluation in EM Iterations
(f, e)-COUNT-3, (?, e)-COUNT-3, (j, i,m, l)-
COUNT-3, 0-COUNT-3, and 1-COUNT-3 are de-
fined respectively as follows. Given the parame-
ters of IBM Model 3, and a sentence pair (f , e),
compute the following 4:
c(f |e; f , e) =
?
a
P (a|f , e)
?
j
?(f, fj)?(e, eaj ),
c(?|e; f , e) =
?
a
P (a|f , e)
?
i
?(?, ?i)?(e, ei),
c(j|i,m, l; f , e) =
?
a
P (a|f , e)?(i, aj),
c(0; f , e) =
?
a
P (a|f , e)(m? 2?0), and
c(1; f , e) =
?
a
P (a|f , e)?0.
2.4 Decoding
E-DECODING-3 and R-DECODING-3 are defined
as follows. Given the parameters of IBM Model 3,
4As the counts are normalized in the EM iteration, we can
replace P (a|f , e) by P (f ,a|e) in the Expectation Evaluation
tasks.
27
and a sentence f , compute its most probable trans-
lation according to the following equations respec-
tively.
e? = argmax
e
?
a
P (f ,a|e) P (e)
(e?,a?) = argmax
(e,a)
P (f ,a|e) P (e).
2.5 SETCOVER
Given a collection of sets C = {S1, . . . ,Sl} and
a set X ? ?li=1Si, find the minimum cardinality
subset C? of C such that every element in X be-
longs to at least one member of C?.
SETCOVER is a well-known NP-Complete
problem. If SETCOVER ?mp f , then f is NP-
Hard.
2.6 PERMANENT
Given a matrixM = [Mj,i]n?n whose entries are
either 0 or 1, compute the following:
perm(M) = ?pi
?n
j=1Mj,pij where pi is a per-
mutation of 1, . . . , n.
This problem is the same as that of counting the
number of perfect matchings in a bipartite graph
and is known to be #P-Complete (?). If PERMA-
NENT ?T f , then f is #P-Hard.
2.7 COMPAREPERMANENTS
Given two matrices A = [Aj,i]n?n and B =
[Bj,i]n?n whose entries are either 0 or 1, determine
which of them has a larger permanent. PERMA-
NENT is known to be Turing reducible to COM-
PAREPERMANENTS (Jerrum, 2005) and therefore,
if COMPAREPERMANENTS ?T f , then f is #P-
Hard.
3 Main Results
In this section, we present the main reductions
for the problems with Model 3 as the translation
model. Our reductions can be easily carried over
to Models 4?5 with minor modifications. In order
to keep the presentation of the main ideas simple,
we let the lexicon, distortion, and fertility models
to be any non-negative functions and not just prob-
ability distributions in our reductions.
3.1 VITERBI-3
We show that VITERBI-3 is NP-Hard.
Lemma 1 SETCOVER ?mp VITERBI-3.
Proof: We give a polynomial time many-one
reduction from SETCOVER to VITERBI-3. Given
a collection of sets C = {S1, . . . ,Sl} and a set
X ? ?li=1Si, we create an instance of VITERBI-3
as follows:
For each set Si ? C, we create a word ei (1 ? i ?
l). Similarly, for each element vj ? X we create
a word fj (1 ? j ? |X| = m). We set the model
parameters as follows:
t (fj|ei) =
{
1 if vj ? Si
0 otherwise
n (?|e) =
{
1
2?! if ? 6= 0
1 if ? = 0
d (j|i,m, l) = 1.
Now consider the sentences e =
e1 . . . el and f = f1 . . . fm.
P (f ,a|e) = n
(
?0|
l?
i=1
?i
) l?
i=1
n (?i|ei)?i!
?
m?
j=1
t
(
fj|eaj
) ?
j: aj 6=0
d (j|i,m, l)
=
l?
i=1
1
21??(?i,0)
We can construct a cover for X from the output
of VITERBI-3 by defining C? = {Si|?i > 0}. We
note that P (f ,a|e) = ?ni=1 121??(?i,0) = 2
?|C?|
.
Therefore, Viterbi alignment results in the mini-
mum cover for X.
3.2 PROBABILITY-3
We show that PROBABILITY-3 is #P-Complete.
We begin by proving the following:
Lemma 2 PERMANENT ?T PROBABILITY-3.
Proof: Given a 0, 1-matrix M =
[Mj, i]n?n, we define f = f1 . . . fn and e =
e1 . . . en where each ei and fj is distinct and set
the Model 3 parameters as follows:
t (fj|ei) =
{
1 if Mj,i = 1
0 otherwise
n (?|e) =
{
1 if ? = 1
0 otherwise
d (j|i, n, n) = 1.
28
Clearly, with the above parameter setting,
P (f ,a|e) = ?nj=1Mj, aj if a is a permutation
and 0 otherwise. Therefore,
P (f |e) =
?
a
P (f ,a|e)
=
?
a is a permutation
n?
j=1
Mj, aj = perm (M)
Thus, by construction, PROBABILITY-3 com-
putes perm (M). Besides, the construction con-
serves the number of witnesses. Hence, PERMA-
NENT ?T PROBABILITY-3.
We now prove that
Lemma 3 PROBABILITY-3 is in #P.
Proof: Let (f , e) be the input to
PROBABILITY-3. Let m and l be the lengths
of f and e respectively. With each alignment
a = (a1, a2, . . . , am) we associate a unique num-
ber na = a1a2 . . . am in base l + 1. Clearly,
0 ? na ? (l + 1)m ? 1. Let w be the binary
encoding of na. Conversely, with every binary
string w we can associate an alignment a if the
value of w is in the range 0, . . . , (l + 1)m ? 1. It
requires O (m log (l + 1)) bits to encode an align-
ment. Thus, given an alignment we can compute
its encoding and given the encoding we can com-
pute the corresponding alignment in time polyno-
mial in l and m. Similarly, given an encoding we
can compute P (f ,a|e) in time polynomial in l and
m. Now, if p(.) is a polynomial, then function
f (f , e) =
?
w?{0,1}?
|w|?p(|?f , e?|)
P (f ,a|e)
is in #P. Choose p (x) = dx log2 (x + 1)e.
Clearly, all alignments can be encoded using at
most p (| (f , e) |) bits. Therefore, if (f , e) com-
putes P (f |e) and hence, PROBABILITY-3 is in
#P.
It follows immediately from Lemma 2 and
Lemma 3 that
Theorem 1 PROBABILITY-3 is #P-Complete.
3.3 (f, e)-COUNT-3
Lemma 4 PERMANENT ?T (f, e)-COUNT-3.
Proof: The proof is similar to that of
Lemma 2. Let f = f1 f2 . . . fn f? and e =
e1 e2 . . . en e?. We set the translation model para-
meters as follows:
t (f |e) =
?
??
??
1 if f = fj, e = ei and Mj,i = 1
1 if f = f? and e = e?
0 otherwise.
The rest of the parameters are set as in Lemma 2.
Let A be the set of alignments a, such that an+1 =
n+1 and an1 is a permutation of 1, 2, . . . , n. Now,
c
(
f? |e?; f , e
)
=
?
a
P (f ,a|e)
n+1?
j=1
?(f? , fj)?(e?, eaj )
=
?
a?A
P (f ,a|e)
n+1?
j=1
?(f? , fj)?(e?, eaj )
=
?
a?A
P (f ,a|e)
=
?
a?A
n?
j=1
Mj, aj = perm (M) .
Therefore, PERMANENT ?T COUNT-3.
Lemma 5 (f, e)-COUNT-3 is in #P.
Proof: The proof is essentially the same as
that of Lemma 3. Note that given an encoding w,
P (f ,a|e)?mj=1 ? (fj, f) ?
(
eaj , e
)
can be evalu-
ated in time polynomial in |(f , e)|.
Hence, from Lemma 4 and Lemma 5, it follows
that
Theorem 2 (f, e)-COUNT-3 is #P-Complete.
3.4 (j, i,m, l)-COUNT-3
Lemma 6 PERMANENT ?T (j, i,m, l)-COUNT-
3.
Proof: We proceed as in the
proof of Lemma 4 with some modifica-
tions. Let e = e1 . . . ei?1e?ei . . . en and
f = f1 . . . fj?1f? fj . . . fn. The parameters
are set as in Lemma 4. Let A be the set of
alignments, a, such that a is a permutation of
1, 2, . . . , (n + 1) and aj = i. Observe that
P (f ,a|e) is non-zero only for the alignments in
A. It follows immediately that with these para-
meter settings, c(j|i, n, n; f , e) = perm (M) .
Lemma 7 (j, i,m, l)-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 3 (j, i,m, l)-COUNT-3 is #P-
Complete.
29
3.5 (?, e)-COUNT-3
Lemma 8 PERMANENT ?T (?, e)-COUNT-3.
Proof: Let e = e1 . . . ene? and f =
f1 . . . fn
k
? ?? ?
f? . . . f? . Let A be the set of alignments
for which an1 is a permutation of 1, 2, . . . , n and
an+kn+1 =
k
? ?? ?
(n + 1) . . . (n + 1). We set
n (?|e) =
?
??
??
1 if ? = 1 and e 6= e?
1 if ? = k and e = e?
0 otherwise.
The rest of the parameters are set as in Lemma 4.
Note that P (f ,a|e) is non-zero only for the align-
ments in A. It follows immediately that with these
parameter settings, c(k|e?; f , e) = perm (M) .
Lemma 9 (?, e)-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 4 (?, e)-COUNT-3 is #P-Complete.
3.6 0-COUNT-3
Lemma 10 PERMANENT ?T 0-COUNT-3.
Proof: Let e = e1 . . . en and f = f1 . . . fnf? .
Let A be the set of alignments, a, such that an1 is
a permutation of 1, . . . , n and an+1 = 0. We set
t (f |e) =
?
??
??
1 if f = fj, e = ei and Mj, i = 1
1 if f = f? and e = NULL
0 otherwise.
The rest of the parameters are set as in Lemma 4.
It is easy to see that with these settings, c(0;f ,e)(n?2) =
perm (M) .
Lemma 11 0-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 5 0-COUNT-3 is #P-Complete.
3.7 1-COUNT-3
Lemma 12 PERMANENT ?T 1-COUNT-3.
Proof: We set the parameters as in
Lemma 10. It follows immediately that
c(1; f , e) = perm (M) .
Lemma 13 1-COUNT-3 is in #P.
Proof: Similar to the proof of Lemma 5.
Theorem 6 1-COUNT-3 is #P-Complete.
3.8 E-DECODING-3
Lemma 14 COMPAREPERMANENTS ?T E-
DECODING-3
Proof: Let M and N be the two 0-1 matri-
ces. Let f = f1f2 . . . fn, e(1) = e(1)1 e
(1)
2 . . . e
(1)
n
and e(2) = e(2)1 e
(2)
2 . . . e
(2)
n . Further, let e(1) and
e(2) have no words in common and each word
appears exactly once. By setting the bigram lan-
guage model probabilities of the bigrams that oc-
cur in e(1) and e(2) to 1 and all other bigram prob-
abilities to 0, we can ensure that the only trans-
lations considered by E-DECODING-3 are indeed
e(1) and e(2) and P
(
e(1)
)
= P
(
e(2)
)
= 1. We
then set
t (f |e) =
?
??
??
1 if f = fj, e = e(1)i and Mj,i = 1
1 if f = fj, e = e(2)i and Nj,i = 1
0 otherwise
n (?|e) =
{
1 ? = 1
0 otherwise
d (j|i, n, n) = 1.
Now, P
(
f |e(1)
)
= perm (M), and P
(
f |e(2)
)
=
perm (N ). Therefore, given the output of E-
DECODING-3 we can find out which of M and
N has a larger permanent.
Hence E-DECODING-3 is #P?Hard.
3.9 R-DECODING-3
Lemma 15 SETCOVER ?mp R-DECODING-3
Proof: Given an instance of SETCOVER, we
set the parameters as in the proof of Lemma 1 with
the following modification:
n (?|e) =
{
1
2?! if ? > 0
0 otherwise.
Let e be the optimal translation obtained by solv-
ing R-DECODING-3. As the language model is
uniform, the exact order of the words in e is not
important. Now, we observe that:
? e contains words only from the set
{e1, e2, . . . , el}. This is because, there can-
not be any zero fertility word as n (0|e) = 0
and the only words that can have a non-zero
fertility are from {e1, e2, . . . , el} due to the
way we have set the lexicon parameters.
? No word occurs more than once in e. Assume
on the contrary that the word ei occurs k > 1
30
times in e. Replace these k occurrences by
only one occurrence of ei and connect all the
words connected to them to this word. This
would increase the score of e by a factor of
2k?1 > 1 contradicting the assumption on
the optimality of e.
As a result, the only candidates for e are subsets of
{e1, e2, . . . , el} in any order. It is now straight for-
ward to verify that a minimum set cover can be re-
covered from e as shown in the proof of Lemma 1.
3.10 IBM Models 4 and 5
The reductions are for Model 3 can be easily ex-
tended to Models 4 and 5. Thus, we have the fol-
lowing:
Theorem 7 Viterbi Alignment computation is
NP-Hard for IBM Models 3? 5.
Theorem 8 Expectation Evaluation in the EM
Steps is #P-Complete for IBM Models 3? 5.
Theorem 9 Conditional Probability computation
is #P-Complete for IBM Models 3? 5.
Theorem 10 Exact Decoding is #P-Hard for
IBM Models 3? 5.
Theorem 11 Relaxed Decoding is NP-Hard for
IBM Models 3? 5 even when the language model
is a uniform distribution.
4 Discussion
Our results answer several open questions on the
computation of Viterbi Alignment and Expectation
Evaluation. Unless P = NP and P#P = P,
there can be no polynomial time algorithms for
either of these problems. The evaluation of ex-
pectations becomes increasingly difficult as we go
from IBM Models 1-2 to Models 3-5 exactly be-
cause the problem is #P-Complete for the latter
models. There cannot be any trick for IBM Mod-
els 3-5 that would help us carry out the sums over
all possible alignments exactly. There cannot exist
a closed form expression (whose representation is
polynomial in the size of the input) for P (f |e) and
the counts in the EM iterations for Models 3-5.
It should be noted that the computation of
Viterbi Alignment and Expectation Evaluation is
easy for Models 1-2. What makes these computa-
tions hard for Models 3-5? To answer this ques-
tion, we observe that Models 1-2 lack explicit fer-
tility model unlike Models 3-5. In the former mod-
els, fertility probabilities are determined by the
lexicon and alignment models. Whereas, in Mod-
els 3-5, the fertility model is independent of the
lexicon and alignment models. It is precisely this
freedom that makes computations on Models 3-5
harder than the computations on Models 1-2.
There are three different ways of dealing with
the computational barrier posed by our problems.
The first of these is to develop a restricted fertil-
ity model that permits polynomial time computa-
tions. It remains to be found what kind of parame-
terized distributions are suitable for this purpose.
The second approach is to develop provably good
approximation algorithms for these problems as is
done with many NP-Hard and #P-Hard prob-
lems. Provably good approximation algorithms
exist for several covering problems including Set
Cover and Vertex Cover. Viterbi Alignment is itself
a special type of covering problem and it remains
to be seen whether some of the techniques devel-
oped for covering algorithms are useful for finding
good approximations to Viterbi Alignment. Sim-
ilarly, there exist several techniques for approxi-
mating the permanent of a matrix. It needs to be
explored if some of these ideas can be adapted for
Expectation Evaluation.
As the third approach to deal with complex-
ity, we can approximate the space of all possi-
ble (l + 1)m alignments by an exponentially large
subspace. To be useful such large subspaces
should also admit optimal polynomial time al-
gorithms for the problems we have discussed in
this paper. This is exactly the approach taken
by (Udupa, 2005) for solving the decoding and
Viterbi alignment problems. They show that very
efficient polynomial time algorithms can be de-
veloped for both Decoding and Viterbi Alignment
problems. Not only the algorithms are prov-
ably superior in a computational complexity sense,
(Udupa, 2005) are also able to get substantial im-
provements in BLEU and NIST scores over the
Greedy decoder.
5 Conclusions
IBM models 3-5 are widely used in SMT. The
computational tasks discussed in this work form
the backbone of all SMT systems that use IBM
models. We believe that our results on the compu-
tational complexity of the tasks in SMT will result
in a better understanding of these tasks from a the-
oretical perspective. We also believe that our re-
sults may help in the design of effective heuristics
31
for some of these tasks. A theoretical analysis of
the commonly employed heuristics will also be of
interest.
An open question in SMT is whether there ex-
ists closed form expressions (whose representation
is polynomial in the size of the input) for P (f |e)
and the counts in the EM iterations for models 3-5
(Brown et al, 1993). For models 1-2, closed form
expressions exist for P (f |e) and the counts in the
EM iterations for models 3-5. Our results show
that there cannot exist a closed form expression
(whose representation is polynomial in the size of
the input) for P (f |e) and the counts in the EM
iterations for Models 3-5 unless P = NP.
References
K. Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational
Linguistics.
Brown, P. et al 1993. The Mathematics of Machine
Translation: Parameter Estimation. Computational
Linguistics, 2(19):263?311.
Al-Onaizan, Y. et al 1999. Statistical Machine Trans-
lation: Final Report. JHU Workshop Final Report.
R. Udupa, and T. Faruquie. 2004. An English-Hindi
Statistical Machine Translation System. Proceed-
ings of the 1st IJCNLP.
Y. Wang, and A. Waibel. 1998. Modeling with Struc-
tures in Statistical Machine Translation. Proceed-
ings of the 36th ACL.
D. Marcu and W. Wong. 2002. A Phrase-Based, Joint
Probability Model for Statistical Machine Transla-
tion. Proceedings of the EMNLP.
L. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189?
201.
M. Jerrum. 2005. Personal communication.
C. Tillman. 2001. Word Re-ordering and Dynamic
Programming based Search Algorithm for Statistical
Machine Translation. Ph.D. Thesis, University of
Technology Aachen 42?45.
Y. Wang and A. Waibel. 2001. Decoding algorithm in
statistical machine translation. Proceedings of the
35th ACL 366?372.
C. Tillman and H. Ney. 2000. Word reordering and
DP-based search in statistical machine translation.
Proceedings of the 18th COLING 850?856.
F. Och, N. Ueffing, and H. Ney. 2000. An efficient A*
search algorithm for statistical machine translation.
Proceedings of the ACL 2001 Workshop on Data-
Driven Methods in Machine Translation 55?62.
U. Germann et al 2003. Fast Decoding and Optimal
Decoding for Machine Translation. Artificial Intel-
ligence.
R. Udupa, H. Maji, and T. Faruquie. 2004. An Al-
gorithmic Framework for the Decoding Problem in
Statistical Machine Translation. Proceedings of the
20th COLING.
R. Udupa and H. Maji. 2005. Theory of Alignment
Generators and Applications to Statistical Machine
Translation. Proceedings of the 19th IJCAI.
32
An Algorithmic Framework for the Decoding Problem in
Statistical Machine Translation
Raghavendra Udupa U Tanveer A Faruquie
IBM India Research Lab
Block-1A, IIT, Hauz Khas
New Delhi - 110 016
India
{uraghave, ftanveer}@in.ibm.com
Hemanta K Maji
Dept. of Computer Science
and Engineering, IIT Kanpur
Kanpur - 208 016
India,
hkmaji@iitk.ac.in
Abstract
The decoding problem in Statistical Ma-
chine Translation (SMT) is a computation-
ally hard combinatorial optimization prob-
lem. In this paper, we propose a new al-
gorithmic framework for solving the decod-
ing problem and demonstrate its utility. In
the new algorithmic framework, the decod-
ing problem can be solved both exactly and
approximately. The key idea behind the
framework is the modeling of the decod-
ing problem as one that involves alternat-
ing maximization of two relatively simpler
subproblems. We show how the subprob-
lems can be solved efficiently and how their
solutions can be combined to arrive at a so-
lution for the decoding problem. A fam-
ily of provably fast decoding algorithms can
be derived from the basic techniques under-
lying the framework and we present a few
illustrations. Our first algorithm is a prov-
ably linear time search algorithm. We use
this algorithm as a subroutine in the other
algorithms. We believe that decoding algo-
rithms derived from our framework can be
of practical significance.
1 Introduction
Decoding is one of the three fundamental prob-
lems in classical SMT (translation model and
language model being the other two) as pro-
posed by IBM in the early 1990?s (Brown et al,
1993). In the decoding problem we are given the
language and translation models and a source
language sentence and are asked to find the
most probable translation for the sentence. De-
coding is a discrete optimization problem whose
search space is prohibitively large. The chal-
lenge is, therefore, in devising a scheme to ef-
ficiently search the solution space for the solu-
tion.
Decoding is known to belong to a class of com-
putational problems popularly known as NP-
hard problems (Knight, 1999). NP-hard prob-
lems are known to be computationally hard and
have eluded polynomial time algorithms (Garey
and Johnson, 1979). The first algorithms for
the decoding problem were based on what is
known among the speech recognition commu-
nity as stack-based search (Jelinek, 1969). The
original IBM solution to the decoding prob-
lem employed a restricted stack-based search
(Berger et al, 1996). This idea was further ex-
plored by Wang and Waibel (Wang and Waibel,
1997) who developed a faster stack-based search
algorithm. In perhaps the first work on the
computational complexity of Decoding, Kevin
Knight showed that the problem is closely re-
lated to the more famous Traveling Salesman
problem (TSP). Independently, Christoph Till-
man adapted the Held-Karp dynamic program-
ming algorithm for TSP (Held and Karp, 1962)
to Decoding (Tillman, 2001). The original Held-
Karp algorithm for TSP is an exponential time
dynamic programming algorithm and Tillman?s
adaptation to Decoding has a prohibitive com-
plexity of O
(
l3m22m
) ? O (m52m) (where m
and l are the lengths of the source and tar-
get sentences respectively). Tillman and Ney
showed how to improve the complexity of the
Held-Karp algorithm for restricted word re-
ordering and gave a O
(
l3m4
) ? O (m7) algo-
rithm for French-English translation (Tillman
and Ney, 2000). An optimal decoder based on
the well-known A? heuristic was implemented
and benchmarked in (Och et al, 2001). Since
optimal solution can not be computed for prac-
tical problem instances in a reasonable amount
of time, much of recent work has focused on
good quality suboptimal solutions. An O
(
m6
)
greedy search algorithm was developed (Ger-
mann et al, 2003) whose complexity was re-
duced further to O
(
m2
)
(Germann, 2003).
In this paper, we propose an algorithmic
framework for solving the decoding problem and
show that several efficient decoding algorithms
can be derived from the techniques developed in
the framework. We model the search problem
as an alternating search problem. The search,
therefore, alternates between two subproblems,
both of which are much easier to solve in prac-
tice. By breaking the decoding problem into
two simpler search problems, we are able to pro-
vide handles for solving the problem efficiently.
The solutions of the subproblems can be com-
bined easily to arrive at a solution for the orig-
inal problem. The first subproblem fixes an
alignment and seeks the best translation with
that alignment. Starting with an initial align-
ment between the source sentence and its trans-
lation, the second subproblem asks for an im-
proved alignment. We show that both of these
problems are easy to solve and provide efficient
solutions for them. In an iterative search for a
local optimal solution, we alternate between the
two algorithms and refine our solution.
The algorithmic framework provides handles
for solving the decoding problem at several lev-
els of complexity. At one extreme, the frame-
work yields an algorithm for solving the decod-
ing problem optimally. At the other extreme, it
yields a provably linear time algorithm for find-
ing suboptimal solutions to the problem. We
show that the algorithmic handles provided by
our framework can be employed to develop a
very fast decoding algorithm which finds good
quality translations. Our fast suboptimal search
algorithms can translate sentences that are 50
words long in about 5 seconds on a simple com-
puting facility.
The rest of the paper is devoted to the devel-
opment and discussion of our framework. We
start with a mathematical formulation of the
decoding problem (Section 2). We then develop
the alternating search paradigm and use it to
develop several decoding algorithms (Section 3).
Next, we demonstrate the practical utility of our
algorithms with the help of results from our ini-
tial experiments (Section 5).
2 Decoding
The decoding problem in SMT is one of finding
the most probable translation e? in the target
language of a given source language sentence f
in accordance with the Fundamental Equation
of SMT (Brown et al, 1993):
e? = argmaxe Pr(f |e)Pr(e). (1)
In the remainder of this paper we will refer
to the search problem specified by Equation 1
as STRICT DECODING.
Rewriting the translation model Pr(f |e) as
?
a Pr(f ,a|e), where a denotes an alignment
between the source sentence and the target sen-
tence, the problem can be restated as:
e? = argmaxe
?
a
Pr(f ,a|e)Pr(e). (2)
Even when the translation model Pr(f |e) is
as simple as IBM Model 1 and the language
model Pr(e) is a bigram language model, the
decoding problem is NP-hard (Knight, 1999).
Unless P = NP, there is no hope of an efficient
algorithm for the decoding problem. Since the
Fundamental Equation of SMT does not yield
an easy handle to design a solution (exact or
even an approximate one) for the problem, most
researchers have instead worked on solving the
following relatively simpler problem (Germann
et al, 2003):
(e?, a?) = argmax(e,a) Pr(f ,a|e)Pr(e). (3)
We call the search problem specified
by Equation 3 as RELAXED DECODING.
Note that RELAXED DECODING relaxes
STRICT DECODING to a joint optimization
problem. The search in RELAXED DECODING
is for a pair (e?, a?). While RELAXED DECODING
is simpler than STRICT DECODING, it is also,
unfortunately, NP hard for even IBM Model
1 and Bigram language model. Therefore, all
practical solutions to RELAXED DECODING
have focused on finding suboptimal solutions.
The challenge is in devising fast search strate-
gies to find good suboptimal solutions. Table 1
lists the combinatorial optimization problems
in the domain of decoding.
In the remainder of the paper,m and l denote
the length of the source language sentence and
its translation respectively.
3 Framework for Decoding
We begin with a couple of useful observations
about the decoding problem. Although decep-
tively simple, these observations are very cru-
cial for developing our framework. They are
the source for algorithmic handles for breaking
the decoding problem into two relatively eas-
ier search problems. The first of these observa-
tions concerns with solving the problem when
we know in advance the mapping between the
source and target sentences. This leads to the
development of an extremely simple algorithm
for decoding when the alignment is known (or
Problem Search
STRICT DECODING e? = argmaxePr(f |e)Pr(e)
RELAXED DECODING (e?, a?) = argmax(e,a)Pr(f ,a|e)Pr(e)
FIXED ALIGNMENT DECODING e? = argmaxePr(f , a?|e)Pr(e)
VITERBI ALIGNMENT a? = argmaxaPr(f ,a|e?)
Table 1: Combinatorial Search Problems in Decoding
can be guessed). Our second observation is on
finding a better alignment between the source
and target sentences starting with an initial
(possibly suboptimal) alignment. The insight
provided by the two observations are employed
in building a powerful algorithmic framework.
3.1 Handles for attacking the Decoding
Problem
Our goal is to arrive at algorithmic handles
for attacking RELAXED DECODING. In this sec-
tion, we make couple of useful observations and
develop algorithmic handles from the insight
provided by them. The first of the two observa-
tions is:
Observation 1 For a given target length l and
a given alignment a? that maps source words to
target positions, it is easy to compute the opti-
mal target sentence e?.
e? = argmaxe Pr(f , a?|e)Pr(e). (4)
Let us call the search problem specified by
Equation 4 as FIXED ALIGNMENT DECODING.
What Observation 1 is saying is that once the
target sentence length and the source to tar-
get mapping is fixed, the optimal target sen-
tence (with the specified target length and
alignment) can be computed efficiently. As
we will show later, the optimal solution for
FIXED ALIGNMENT DECODING can be com-
puted in O (m) time for IBM models 1-5 using
dynamic programming. As we can always guess
an alignment (as is the case with many decoding
algorithms in the literature), the above observa-
tion provides an algorithmic handle for finding
suboptimal solutions for RELAXED DECODING.
Our second observation is on computing the
optimal alignment between the source sentence
and the target sentence.
Observation 2 For a given target sentence e?,
it is easy to compute the optimal alignment a?
that maps the source words to the target words.
a? = argmaxa Pr(f ,a|e?). (5)
It is easy to determine the optimal (Viterbi)
alignment between the source sentence and its
translation. In fact, for IBM models 1 and 2,
the Viterbi alignment can be computed using a
straight forward algorithm in O (ml) time. For
higher models, an approximate Viterbi align-
ment can be computed iteratively by an iter-
ative procedure called local search. In each it-
eration of local search, we look in the neighbor-
hood of the current best alignment for a better
alignment (Brown et al, 1993). The first itera-
tion can start with any arbitrary alignment (say
the Viterbi alignment of Model 2). It is possi-
ble to implement one iteration of local search in
O (ml) time. Typically, the number of iterations
is bounded in practice by O (m), and therefore,
local search takes O
(
m2l
)
time.
Our framework is not strictly dependent on
the computation of an optimal alignment. Any
alignment which is better than the current
alignment is good enough for it to work. It is
straight forward to find one such alignment us-
ing restricted swaps and moves in O (m) time.
In the remainder of this paper, we use the term
Viterbi to denote any linear time algorithm for
computing an improved alignment between the
source sentence and its translation.
3.2 Illustrative Algorithms
In this section, we show how the handles pro-
vided by the above two observations can be em-
ployed to solve RELAXED DECODING. The two
handles are in some sense complementary to
each other. When the alignment is known, we
can efficiently determine the optimal translation
with that alignment. On the other hand, when
the translation is known, we can efficiently de-
termine a better alignment. Therefore, we can
use one to improve the other. We begin with the
following simple linear time decoding algorithm
which is based on the first observation.
Algorithm NaiveDecode
Input: Source language sentence f of length
m > 0.
Optional Inputs: Target sentence length l,
alignment a? between the source words and tar-
get positions.
Output: Target language sentence e? of length
l.
1. If l is not specified, let l = m.
2. If an alignment is not specified, guess some
alignment a?.
3. Compute the optimal translation e? by solv-
ing FIXED ALIGNMENT DECODING,
i.e., e? = argmaxe Pr(f , a?|e)Pr(e).
4. return e?.
When the length of the translation is not
specified, NaiveDecode assumes that the trans-
lation is of the same length as the source sen-
tence. If an alignment that maps the source
words to target positions is not specified, the
algorithm guesses an alignment a? (a? can be the
trivial alignment that maps the source word fj
to target position j, that is, a?j = j, or can
be guessed more intelligently). It then com-
putes the optimal translation for the source
sentence f , with the length of the target sen-
tence and the alignment between the source and
the target sentences kept fixed to l and a? re-
spectively, by maximizing Pr(f , a?|e)Pr(e). As
FIXED ALIGNMENT DECODING can be solved
in O (m) time, NaiveDecode takes only O(m)
time.
The value of NaiveDecode lies not in itself per
se, but in its instrumental role in designing more
superior algorithms. The power of NaiveDecode
can be demonstrated with the following optimal
algorithm for RELAXED DECODING.
Algorithm NaiveOptimalDecode
Input: Source language sentence f of length
m > 0.
Output: Target language sentence e? of length
l, m2 ? l ? 2m.
1. Let e? = null and a? = null.
2. For each l = m2 , . . . , 2m do
3. For each alignment a between the source
words and the target positions do
(a) Let e = NaiveDecode(f , l,a).
(b) If Pr (f , e,a) > Pr (f , e?, a?) then
i. e? = e
ii. a? = a.
4. return (e?, a?).
NaiveOptimalDecode considers various tar-
get lengths and all possible alignments be-
tween the source words and the target posi-
tions. For each target length l and alignment
a it employs NaiveDecode to find the best so-
lution. There are (l + 1)m candidate align-
ments for a target length l and O (m) can-
didate target lengths. Therefore, NaiveOp-
timalDecode explores ? (m(l + 1)m) alignments.
For each of these candidate alignments, it
makes a call to NaiveDecode. The time com-
plexity of NaiveOptimalDecode is, therefore,
O
(
m2(l + 1)m
)
. Although an exponential time
algorithm, it can compute the optimal solution
for RELAXED DECODING.
With NaiveDecode and NaiveOptimalDecode
we have demonstrated the power of the algo-
rithmic handle provided by Observation 1. It
is important to note that these two algorithms
are at the two extremities of the spectrum.
NaiveDecode is a linear time decoding algorithm
that computes a suboptimal solution for RE-
LAXED DECODING while NaiveOptimalDecode
is an exponential time algorithm that computes
the optimal solution. What we want are algo-
rithms that are close to NaiveDecode in com-
plexity and to NaiveOptimalDecode in qual-
ity. It is possible to reduce the complexity of
NaiveOptimalDecode significantly by carefully
reducing the number of alignments that are ex-
amined. Instead of examining all ?(m(l+1)m)
alignments, if we examine only a small num-
ber, say g (m), alignments in NaiveOptimalDe-
code, we can find a solution in O (mg (m)) time.
In the next section, we show how to restrict
the search to only a small number of promis-
ing alignments.
3.3 Alternating Maximization
We now show how to use the two algorithmic
handles to come up with a fast search paradigm.
We alternate between searching the best trans-
lation given an alignment and searching the
best alignment given a translation. Since the
two subproblems are complementary, they can
be used to improve the solution computed by
one another by alternating between the two
problems.
Algorithm AlternatingSearch
Input: Source language sentence f of length
m > 0.
Output: Target language sentence e(o) of
length l (m/2 ? l ? 2m).
1. Let e(o) = null and a(o) = null.
2. For each l = m/2, . . . , 2m do
(a) Let e = null and a = null.
(b) While there is improvement in solution
do
i. Let e = NaiveDecode (f , l,a).
ii. Let a? = V iterbi (f , e).
(c) If Pr (f , e,a) > Pr (f , e(o),a(o)) then
i. e(o) = e
ii. a(o) = a.
3. return e(o).
AlternatingSearch searches for a good trans-
lation by varying the length of the tar-
get sentence. For a sentence length l,
the algorithm finds a translation of length
l and then iteratively improves the trans-
lation. In each iteration it solves two
subproblems: FIXED ALIGNMENT DECODING
and VITERBI ALIGNMENT. The input to each
iteration are the source sentence f , the tar-
get sentence length l, and an alignment a be-
tween the source and target sentences. So, Al-
ternatingSearch finds a better translation e for
f by solving FIXED ALIGNMENT DECODING.
For this purpose it employs NaiveDecode. Hav-
ing computed e, the algorithm computes a bet-
ter alignment (a?) between e and f by solving
VITERBI ALIGNMENT using Viterbi algorithm.
The new alignment thus found is used by the al-
gorithm in the subsequent iteration. At the end
of each iteration the algorithm checks whether
it has made progress. The algorithm returns the
best translation of the source f across a range
of target sentence lengths.
The analysis of AlternatingSearch is compli-
cated by the fact that the number of iterations
(see step 2.b) depends on the input. It is rea-
sonable to assume that the length of the source
sentence (m) is an upper bound on the number
of iterations. In practice, however, the number
of iterations is typically O (1). There are 3m/2
candidate sentence lengths for the translation
(l varies from m/2 to 2m) and both NaiveDe-
code and Viterbi are O (m). Therefore, the time
complexity of AlternatingSearch is O
(
m2
)
.
4 A Linear Time Algorithm for
FIXED ALIGNMENT DECODING
A key component of all our algorithms is
a linear time algorithm for the problem
FIXED ALIGNMENT DECODING. Recall that in
FIXED ALIGNMENT DECODING, we are given
the target length l and a mapping a? from source
words to target positions. The goal is then to
find the optimal translation with a? as the align-
ment. In this section, we give a dynamic pro-
gramming based solution to this problem. Our
solution is based on a new formulation of IBM
translation models. We begin our discussion
with a few technical definitions.
Alignment a? maps each of the source words
fj, j = 1, . . . ,m to a target position in the range
[0, . . . , l]. Define a mapping ? from [0, . . . , l] to
subsets of {1, . . . ,m} as follows:
?(i) = {j : j ? {1, . . . ,m} ? a?j = i} ? i = 0, . . . , l.
?(i) is the set of source positions which are
mapped to the target location i by the align-
ment a? and the fertility of the target position i
is ?i = |?(i)|.
We can rewrite each of the IBM models
Pr (f , a?|e) as follows:
Pr (f , a?|e) = ?
l
?
i=1
TiDiNi.
Table 2 shows the breaking of Pr (f , a?|e) into
the constituents Ti,Di and Ni. As a conse-
quence, we can write Pr (f , a?|e)Pr (e) as:
Pr (f , a?|e)Pr (e) = ??
l
?
i=1
TiDiNiLi
where Li = trigram(ei|ei?2, ei?1) and ? is the
trigram probability of the boundary word.
The above reformulation of the optimiza-
tion function of the decoding problem allows
us to employ Dynamic Programming for solv-
ing FIXED ALIGNMENT DECODING efficiently.
Note that each word ei has only a constant num-
ber of candidates in the vocabulary. Therefore,
the set of words e1, . . . , el that maximizes the
LHS of the above optimization function can be
found in O (m) time using the standard Dy-
namic Programming algorithm (Cormen et al,
2001).
5 Experiments and Results
In this section we describe our experimental
setup and present the initial results. Our goal
Model ? Ti Di Ni
1 ?(m|l)(l+1)m
?
k??(i) t(fk |ei) 1 1
2 ?(m|l) ?k??(i) t(fk |ei)
?
k??(i) a(i|k,m, l) 1
3 n(?0|m)pm?2?00 p?01
?
k??(i) t(fk |ei)
?
k??(i) d(k|i,m, l) ?i! n(?i|ei)
Table 2: Pr (f, a?|e) for IBM Models
was not only to evaluate the performance of our
algorithms on real data, but also to evaluate
how easy it is to code the algorithm and whether
a straightforward implementation of the algo-
rithm with no parameter tuning can give satis-
factory results.
We implemented the algorithms in C++ and
conducted the experiments on an IBM RS-6000
dual processor machine with 1 GB of RAM. We
built a French-English translation model (IBM
Model 3) by training over a corpus of 100 K sen-
tence pairs from the Hansard corpus. The trans-
lation direction was from French to English. We
built an English language model by training
over a corpus consisting of about 800 million
words. We divided the test sentences into sev-
eral classes based on their length. Each length
class consisted of 300 test French sentences.
We implemented four algorithms -1.1 (NaiveDe-
code), 1.2 (Alternating Search with l restricted
to m), 2.1 (NaiveDecode with l varying from
m/2 to 2m) and 2.2 (Alternating Search). In
order to compare the performance of the al-
gorithms proposed in this paper with a previ-
ous decoding algorithm, we also implemented
the dynamic programming based algorithm by
(Tillman, 2001). For each of the algorithms, we
computed the following:
1. Average time taken for translation for
each length class.
2. NIST score of the translations for each
length class.
3. Average value of the optimization
function for the translations for each
length class.
The results of the experiments are summa-
rized in Plots 1, 2 and 3. In all the plots, the
length class is denoted by the x-axis. 11-20 indi-
cates the class with sentences of length between
11 words to 20 words. 51 indicates the group
of sentences with sentence length 51 or more.
Plot 1 shows the average time taken by the al-
gorithms for translating the sentences in each
length class. Time is shown in seconds on a log
scale. Plot 2 shows the NIST score of the trans-
lations for each length class while Plot 3 shows
the average log score of the translations (-ve log
of Pr (f ,a|e)Pr (e)) again for each length class.
It can be seen from Plot 1 that all of our al-
gorithms are indeed very fast in practice. They
are, in fact, an order faster than the Held-Karp
algorithm. Our algorithms are able to trans-
late even long sentences (50+ words) in a few
seconds.
Plot 3 shows that the log scores of the trans-
lations computed by our algorithms are very
close to those computed by the Held-Karp al-
gorithm. Plot 2 compares the NIST scores ob-
tained with each of the algorithm. Among the
four algorithms based on our framework, Al-
gorithm 2.2 gives the best NIST scores as ex-
pected. Although, the log scores of our algo-
rithms are comparable to those of the Held-
Karp algorithm, our NIST scores are lower. It
should be noted that the mathematical quan-
tity that our algorithm tries to optimize is the
log score. Plot 3 shows that our algorithms are
quite good at finding solutions with good scores.
 0.01
 0.1
 1
 10
 100
 1000
 10000
0-10 11-20 21-30 31-40 41-50 51-
Ti
m
e 
in 
se
co
nd
s
Sentence Length
Decoding Time
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 1: Average decoding time
6 Conclusions
The algorithmic framework developed in this
paper is powerful as it yields several decoding
algorithms. At one end of the spectrum is a
provably linear time algorithm for computing
a suboptimal solution and at the other end is
an exponential time algorithm for computing
 3
 3.5
 4
 4.5
 5
 5.5
 6
 6.5
 7
0-10 11-20 21-30 31-40 41-50 51-
NI
ST
 S
co
re
Sentence Length
NIST Scores
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 2: NIST scores
 0
 50
 100
 150
 200
 250
 300
 350
 400
0-10 11-20 21-30 31-40 41-50 51-
log
sc
or
e
Sentence Length
Logscores
"algorithm 1.1"
"algorithm 1.2"
"algorithm 2.1"
"algorithm 2.2"
"algorithm H-K"
Figure 3: Log score
the optimal solution. We have also shown that
alternating maximization can be employed to
come up with O
(
m2
)
decoding algorithm. Two
questions in this connection are:
1. Is it possible to reduce the complexity
of AlternatingSearch to O (m)?
2. Instead of exploring each alignment
separately, is it possible to explore a
bunch of alignments in one shot?
Answers to these questions will result in faster
and more efficient decoding algorithms.
7 Acknowledgements
We are grateful to Raghu Krishnapuram for his
insightful comments on an earlier draft of this
paper and Pasumarti Kamesam for his help dur-
ing the course of this work.
References
A. Berger, P. Brown, S. Della Pietra, V. Della
Pietra, A. Kehler, and R. Mercer. 1996. Lan-
guage translation apparatus and method us-
ing context-based translation models. United
States Patent 5,510,981.
P. Brown, S. Della Pietra, V. Della Pietra,
and R. Mercer. 1993. The mathematics of
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
T. H. Cormen, C. E. Leiserson, R. L. Rivest,
and C. Stein. 2001. The MIT Press, Cam-
bridge.
M. R. Garey and D. S. Johnson. 1979. W. H.
Freeman and Company, New York.
U. Germann, M. Jahr, D. Marcu, and K. Ya-
mada. 2003. Fast decoding and optimal de-
coding for machine translation. Artificial In-
telligence.
Ulrich Germann. 2003. Greedy decoding for
statistical machine translation in almost lin-
ear time. In Proceedings of HLT-NAACL
2003. Edmonton, Canada.
M. Held and R. Karp. 1962. A dynamic pro-
gramming approach to sequencing problems.
J. SIAM, 10(1):196?210.
F. Jelinek. 1969. A fast sequential decoding al-
gorithm using a stack. IBM Journal Reseach
and Development, 13:675?685.
Kevin Knight. 1999. Decoding complexity in
word-replacement translation models. Com-
putational Linguistics, 25(4).
F. Och, N. Ueffing, and H. Ney. 2001. An ef-
ficient a* search algorithm for statistical ma-
chine translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Methods in
Machine Translation, pages 55?62. Toulouse,
France.
C. Tillman and H. Ney. 2000. Word reorder-
ing and dp-based search in statistical machine
translation. In Proceedings of the 18th COL-
ING, pages 850?856. Saarbrucken, Germany.
Christoph Tillman. 2001. Word re-ordering
and dynamic programming based search
algorithm for statistical machine transla-
tion. Ph.D. Thesis, University of Technology
Aachen, pages 42?45.
R. Udupa and T. Faruquie. 2004. An english-
hindi statistical machine translation system.
In Proceedings of the 1st IJCNLP, pages 626?
632. Sanya, Hainan Island, China.
Y. Wang and A. Waibel. 1997. Decoding al-
gorithm in statistical machine translation. In
Proceedings of the 35th ACL, pages 366?372.
Madrid, Spain.
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1256?1265,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Hashing-based Approaches to Spelling Correction of Personal Names
Raghavendra Udupa
Microsoft Research India
Bangalore, India
raghavu@microsoft.com
Shaishav Kumar
Microsoft Research India
Bangalore, India
v-shaisk@microsoft.com
Abstract
We propose two hashing-based solutions to
the problem of fast and effective personal
names spelling correction in People Search
applications. The key idea behind our meth-
ods is to learn hash functions that map similar
names to similar (and compact) binary code-
words. The two methods differ in the data
they use for learning the hash functions - the
first method uses a set of names in a given lan-
guage/script whereas the second uses a set of
bilingual names. We show that both methods
give excellent retrieval performance in com-
parison to several baselines on two lists of
misspelled personal names. More over, the
method that uses bilingual data for learning
hash functions gives the best performance.
1 Introduction
Over the last few years, People Search has emerged
as an important search service. Unlike general Web
Search and Enterprise Search where users search for
information on a wide range of topics including peo-
ple, products, news, events, etc., People Search is
about people. Hence, personal names are used pre-
dominantly as queries in People Search. As in gen-
eral Web Search, a good percentage of queries in
People Search is misspelled. Naturally, spelling cor-
rection of misspelled personal names plays a very
important role in not only reducing the time and ef-
fort needed by users to find people they are search-
ing for but also in ensuring good user experience.
Spelling errors in personal names are of a differ-
ent nature compared to those in general text. Long
before People Search became widely popular, re-
searchers working on the problem of personal name
matching had recognized the human tendency to be
inexact in recollecting names from the memory and
specifying them. A study of personal names in
hospital databases found that only 39% of the er-
rors in the names were single typographical errors
(Friedman and Sideli, 1992)1. Further, multiple and
long distance typographical errors (Gregzorz Kon-
drak for Grzegorz Kondrak), phonetic errors (as in
Erik Bryl for Eric Brill), cognitive errors (as in Sil-
via Cucerzan for Silviu Cucerzan) and word substi-
tutions (as in Rob Moore for Bob Moore) are ob-
served relatively more frequently in personal names
compared to general text.
In addition to within-the-word errors, People
Search queries are plagued by errors that are not
usually seen in general text. The study by Fried-
man and Sideli discovered that 36% of the errors
were due to addition or deletion of a word (as in
Ricardo Baeza for Ricardo Baeza-Yates) (Friedman
and Sideli, 1992). Although word addition and dele-
tion generally do not come under the purview of
spelling correction, in People Search they are im-
portant and need to be addressed.
Standard approaches to general purpose spelling
correction are not well-suited for correcting mis-
spelled personal names. As pointed out by
(Cucerzan and Brill, 2004), these approaches ei-
ther try to correct individual words (and will fail to
correct Him Clijsters to Kim Clijsters) or employ
features based on relatively wide context windows
1In contrast, 80% of misspelled words in general text are due
to single typographical errors as found by (Damerau, 1964).
1256
which are not available for queries in Web Search
and People Search. Spelling correction techniques
meant for general purpose web-queries require large
volumes of training data in the form of query logs
for learning the error models (Cucerzan and Brill,
2004), (Ahmad and Kondrak, 2005). However,
query logs are not available in some applications
(e.g. Email address book search). Further, un-
like general purpose web-queries where word order
often matters, in People Search word order is lax
(e.g. I might search for either Kristina Toutanova or
Toutanova Kristina). Therefore, spelling correction
techniques that rely crucially on bigram and higher
order language models will fail on queries with a dif-
ferent word order than what is observed in the query
log.
Unlike general purpose Web Search where it is
not reasonable to assume the availability of a high-
coverage trusted lexicon, People Search typically
employs large authoritative name directories. For
instance, if one is searching for a friend on Face-
book, the correct spelling of the friend?s name exists
in the Facebook people directory2 (assuming that the
friend is a registered user of Facebook at the time of
the search). Similarly, if one is searching for a con-
tact in Enterprise address book, the correct spelling
of the contact is part of the address book. In fact,
even in Web Search, broad-coverage name directo-
ries are available in the form of Wikipedia, IMDB,
etc. The availability of large authoritative name di-
rectories that serve as the source of trusted spellings
of names throws open the possibility of correcting
misspelled personal names with the help of name
matching techniques (Pfeifer et al, 1996), (Chris-
ten, 2006), (Navarro et al, 2003). However, the best
of the name matching techniques can at best work
with a few thousand names to give acceptable re-
sponse time and accuracy. They do not scale up to
the needs of People Search applications where the
directories can have millions of names.
In this work, we develop hashing-based name
similarity search techniques and employ them for
spelling correction of personal names. The motiva-
tion for using hashing as a building block of spelling
correction is the following: given a query, we want
to return the global best match in the name directory
2http://www.facebook.com/directory/people/
that exceeds a similarity threshold. As matching the
query with the names in the directory is a time con-
suming task especially for large name directories,
we solve the search problem in two stages:
? NAME BUCKETING: For each token of the
query, we do an approximate nearest neighbor
search of the name tokens of the directory and
produce a list of candidates, i.e., tokens that are
approximate matches of the query token. Using
the list of candidate tokens, we extract the list
of candidate names which contain at least one
approximately matching token.
? NAME MATCHING: We do a rigorous match-
ing of the query with candidate names.
Clearly, our success in finding the right name sug-
gestion for the query in the NAME MATCHING
stage depends crucially on our success in getting
the right name suggestion in the list of candidates
produced by the NAME BUCKETING stage search.
Therefore, we need a name similarity search tech-
nique that can ensure very high recall without pro-
ducing too many candidates. Hashing is best suited
for this task of fast and approximate name match-
ing. We hash the query tokens as well as directory
tokens into d bit binary codes. With binary codes,
finding approximate matches for a query token is as
easy as finding all the database tokens that are at a
Hamming distance of r or less from the query token
in the binary code representation (Shakhnarovich et
al., 2008), (Weiss et al, 2008). When the binary
codes are compact, this search can be done in a frac-
tion of a second on directories containing millions
of names on a simple processor.
Our contributions are:
? We develop a novel data-driven technique for
learning hash functions for mapping similar
names to similar binary codes using a set of
names in a given language/script (i.e. monolin-
gual data). We formulate the problem of learn-
ing hash functions as an optmization problem
whose relaxation can be solved as a generalized
Eigenvalue problem. (Section 2.1).
? We show that hash functions can also be learnt
using bilingual data in the form of name equiv-
alents in two languages. We formulate the
1257
problem of learning hash functions as an opt-
mization problem whose relaxation can be
solved using Canonical Correlation Analysis.
(Section 2.2)
? We develop new similarity measures for match-
ing names (Section 3.1).
? We evaluate the two methods systematically
and compare our performance against multiple
baselines. (Section 5).
2 Learning Hash Functions
In this section, we develop two techniques for learn-
ing hash functions using names as training data. In
the first approach, we use monolingual data consist-
ing of names in a language whereas in the second we
use bilingual name pairs. In both techniques, the key
idea is the same: we learn hash functions that map
similar names in the training data to similar code-
words.
2.1 M-HASH: Learning with Monolingual
Names Data
Let (s, s?) be a pair of names and w (s, s?) be their
similarity3. We are given a set of name pairs T =
{(s, s?)} as the training data. Let ? (s) ? Rd1 be the
feature representation of s. We want to learn a hash
function f that maps each name to a d bit codeword:
f : s 7? {?1, 1}d. We also want the Hamming dis-
tance of the codeword of s to the codeword of s? be
small when w (s, s?) is large. Further, we want each
bit of the codewords to be either 1 or ?1 with equal
probablity and the successive bits of the codewords
to be uncorrelated. Thus we arrive at the following
optimization problem4:
minimize :
?
(s,s?)?T
w
(
s, s?
) ?
?f (s)? f
(
s?
)?
?2
s.t. :
?
s:(s,s?)?T
f (s) = 0
?
s:(s,s?)?T
f (s) f (s)T = ?2Id
f (s) , f
(
s?
)
? {?1, 1}d
3We used 1? length normalized Edit Distance between s
and s? as w (s, s?).
4Note that the Hamming distance of a codeword y to another
codeword y? is 14 ?y ? y
??2.
where Id is an identity matrix of size d? d.
Note that the second constraint helps us avoid the
trap of mapping all names to the same codeword and
thereby making the Hamming error zero while satis-
fying the first and last constraints.
It can be shown that the above minimization prob-
lem is NP-hard even for 1-bit codewords (Weiss et
al., 2008). Further, the optimal solution gives code-
words only for the names in the training data. As we
want f to be defined for all s, we address the out-of-
sample extension problem by relaxing f as follows5:
fR (s) = A
T? (s) =
(
aT1 ? (s) , . . . , a
T
d ? (s)
)T
(1)
where A = [a1, . . . , ad] ? Rd1?d is a rank d matrix
(d ? d1).
After the linear relaxation (Equation 1), the first
constraint simply means that the data be centered,
i.e., have zero mean. We center ? by subtracting the
mean of ? from every ? (s) ? ? to get ??.
Subsequent to the above relaxation, we get the
following optimization problem:
minimize : Tr AT ??L??TA (2)
s.t. : (3)
AT ????TA = ?2Id (4)
whereL is the graph Laplacian for the similarity ma-
trix W defined by the pairwise similarities w (s, s?).
The minimization problem can be transformed
into a generalized Eigenvalue problem and solved
efficiently using either Cholesky factorization or QZ
algorithm (Golub and Van Loan, 1996):
??L??TA = ????TA? (5)
where ? is a d? d diagonal matrix.
OnceA has been estimated from the training data,
the codeword of a name s can be produced by bina-
rizing each coordinate of fR (s):
f (s) =
(
sgn
(
aT1 ? (s)
)
, . . . , sgn
(
aTd ? (s)
))T
(6)
where sgn(u) = 1 if u > 0 and?1 otherwise for all
u ? R.
5In contrast to our approach, Spectral Hashing, a well-
known hashing technique, makes the unrealistic assumption
that the training data is sampled from a multidimensional uni-
form distribution to address the out-of-sample extension prob-
lem (Weiss et al, 2008).
1258
In the reminder of this work, we call the system
that uses the hash function learnt from monolingual
data as M-HASH.
2.2 B-HASH: Learning with Bilingual Names
Data
Let (s, t) be a pair of name s and its transliteration
equivalent t in a different language/script. We are
given the set T = {(s, t)} as the training data. Let
? (s) ? Rd1 (and resp. ? (t) ? Rd2) be the feature
representation of s (and resp. t). We want to learn
a pair of hash functions f, g that map names to d bit
codewords: f : s 7? {?1, 1}d, g : t 7? {?1, 1}d.
We also want the Hamming distance of the code-
word of a name to the codeword of its transliteration
be small. As in Section 2.1, we want each bit of the
codewords to be either 1 or?1 with equal probablity
and the successive bits of the codewords to be uncor-
related. Thus we arrive at the following optimization
problem:
minimize :
?
(s,t)?T
?f (s)? g (t)?2
s.t. :
?
s:(s,t)?T
f (s) = 0
?
t:(s,t)?T
g (t) = 0
?
s:(s,t)?T
f (s) f (s)T = ?2Id
?
t:(s,t)?S
g (t) g (t)T = ?2Id
f (s) , g (t) ? {?1, 1}d
where Id is an identity matrix of size d? d.
As we want f (and resp. g) to be defined for all s
(and resp. t), we relax f (and resp. g) as follows:
fR (s) = A
T? (s) (7)
gR (t) = B
T? (s) (8)
where A = [a1, . . . , ad] ? Rd1?d and B =
[b1, . . . , bd] ? Rd2?d are rank d matrices.
As before, we center ? and ? to get ?? and ?? re-
spectively. Thus, we get the following optimization
problem:
minimize : Tr H
(
A,B; ??, ??
)
(9)
s.t. : (10)
AT ????TA = ?2Id (11)
BT ????TB = ?2Id (12)
where H
(
A,B; ??, ??
)
=
(
AT ???BT ??
)(
AT ???BT ??
)T
.
The minimization problem can be solved as a gen-
eralized Eigenvalue problem:
????TB = ????TA? (13)
????TA = ????TB? (14)
where ? is a d ? d diagonal matrix. Further, Equa-
tions 13 and 14 find the canonical coefficients of ??
and ?? (Hardoon et al, 2004).
As with monolingual learning, we get the code-
word of s by binarizing the coordinates of fR (s)6:
f (s) =
(
sgn
(
aT1 ? (s)
)
, . . . , sgn
(
aTd ? (s)
))T
(15)
In the reminder of this work, we call the system
that uses the hash function learnt from bilingual data
as B-HASH.
3 Similarity Score
In this section, we develop new techniques for com-
puting the similarity of names at token level as well
as a whole. We will use these techniques in the
NAME MATCHING stage of our algorithm (Sec-
tion 4.2.1).
3.1 Token-level Similarity
We use a logistic function over multiple distance
measures to compute the similarity between name
tokens s and s?:
K
(
s, s?
)
=
1
1 + e?
?
i ?idi(s,s
?)
. (16)
While a variety of distance measures can be
employed in Equation 16, two obvious choices
6As a biproduct of bilingual learning, we can hash names in
the second language using g:
g (t) =
(
sgn
(
bT1 ? (t)
)
, . . . , sgn
(
bTd ? (t)
))T
1259
are the normalized Damerau-Levenshtein edit dis-
tance between s and s? and the Hamming dis-
tance between the codewords of s and s? (=
?f (s)? f (s?)?). In our experiments, we found that
the continuous relaxation ?fR (s)? fR (s?)? was
better than ?f (s)? f (s?)? and hence we used it
with Damerau-Levenshtein edit distance. We esti-
mated ?1 and ?2 using a small held out set.
3.2 Multi-token Name Similarity
Let Q = s1s2 . . . sI and D = s?1s
?
2 . . . s
?
J be two
multi-token names. To compute the similarity be-
tween Q and D, we first form a weighted bipartite
graph with a node for each si and a node for each s?j
and set edge weight toK
(
si, s?j
)
. We then compute
the weight (?max) of the maximum weighted match-
ing7 in this graph. The similarity between Q and D
is then computed as
K (Q,D) =
?max
|I ? J + 1|
. (17)
4 Spelling Correction using Hashing
In this section, we describe our algorithm for
spelling correction using hashing as a building
block.
4.1 Indexing the Name Directory
Given a name directory, we break each name into its
constituent tokens and form a set of distinct name to-
kens. Using the name tokens and the original names,
we build an inverted index which, for each name to-
ken, lists all the names that have the token as a con-
stituent. Further, we hash each name token into a d
bit codeword as described in Equation 6 (and resp.
Equation 15) when using the hash function learnt on
monolingual data (and resp. bilingual data) and store
in a hash table.
4.2 Querying the Name Directory
Querying is done in two stages:
NAME BUCKETING and NAME MATCHING.
7In practice, a maximal matching computed using a greedy
approach suffices since many of the edges in the bipartite graph
have low weight.
4.2.1 Name Bucketing
Given a query Q = s1s2 . . . sI , we hash each si
into a codeword yi and retrieve all codewords in the
hash table that are at a Hamming distance of r or
less from yi. We rank the name tokens thus retrieved
using the token level similarity score of Section 3.1
and retain only the top 100. Using the top tokens, we
get al names which contain any of the name tokens
as a constituent to form the pool of candidates C for
the NAME MATCHING stage.
4.2.2 Name Matching
First we find the best match for a query Q in the
set of candidates C as follows:
D? = argmax
D?C
K (Q,D) . (18)
Next we suggest D? as the correction for Q if
K (Q,D?) exceeds a certain empirically determined
threshold.
5 Experiments and Results
We now discuss the experiments we conducted to
study the retrieval performance of the two hashing-
based approaches developed in the previous sec-
tions. Apart from evaluating the systems on test sets
using different name directories, we were interested
in comparing our systems with several baselines, un-
derstanding the effect of some of the choices we
made (e.g. training data size, conjugate language)
and comparative analysis of retrieval performance
on queries of different complexity.
5.1 Experimental Setup
We tested the proposed hashing-based spelling cor-
rection algorithms on two test sets:
? DUMBTIONARY: 1231 misspellings of var-
ious names from Dumbtionary8 and a name
directory consisting of about 550, 000 names
gleaned from the English Wikipedia. Each of
the misspellings had a correct spelling in the
name directory.
? INTRANET: 200 misspellings of employees
taken from the search logs of the intranet
of a large organization and a name directory
8http://www.dumbtionary.com
1260
consisting of about 150, 000 employee names.
Each of the misspellings had a correct spelling
in the name directory.
Table 1 shows the average edit distance of a mis-
spelling from the correct name. Compared to
DUMBTIONARY, the misspellings in INTRANET
are more severe as the relatively high edit distance
indicates. Thus, INTRANET represents very hard
cases for spelling correction.
Test Set Average Std. Dev.
DUMBTIONARY 1.39 0.76
INTRANET 2.33 1.60
Table 1: Edit distance of a misspelling from the correct
name.
5.1.1 Training
For M-HASH, we used 30,000 single token
names in English (sampled from the list of names
in the Internet Movie Database9) as training data
and for B-HASH we used 14,941 parallel single to-
ken names in English-Hindi 10. Each name was
represented as a feature vector over character bi-
grams. Thus, the name token Klein has the bigrams
{?k, kl, le, ei, in, n?} as the features.
We learnt the hash functions from the training
data by solving the generalized Eigenvalue problems
of Sections 2.1 and 2.2. For both M-HASH and B-
HASH we used the top 32 Eigenvectors to form the
hash function resulting in a 32 bit representation for
every name token11.
5.1.2 Performance Metric
We measured the performance of all the systems
using Precision@1, the fraction of names for which
a correct spelling was suggested at Rank 1.
5.1.3 Baselines
The baselines are two popular search engines
(S1 and S2), Double Metaphone (DM), a widely
9http://www.imdb.com
10We obtained the names from the organizers
of NEWS2009 workshop (http://www.acl-ijcnlp-
2009.org/workshops/NEWS2009/pages/sharedtask.html).
11We experimented with codewords of various lengths and
found that the 32 bit representation gave the best tradeoff be-
tween retrieval accuracy and speed.
used phonetic search algorithm (Philips, 2000) and
BM25, a very popular Information Retrieval algo-
rithm (Manning et al, 2008). To use BM25 algo-
rithm for spelling correction, we represented each
name as a bag of bigrams and set the parameters K
and b to 2 and 0.75 respectively.
5.2 Results
5.2.1 DUMBTIONARY
Table 2 compares the results of the hashing-based
systems with the baselines on DUMBTIONARY. As
the misspellings in DUMBTIONARY are relatively
easier to correct, all the systems give reasonably
good retrieval results. Nevertheless, the results of
M-HASH and B-HASH are substantially better than
the baselines. M-HASH reduced the error over the
best baseline (S1) by 13.04% whereas B-HASH re-
duced by 46.17% (Table 6).
M-HASH B-HASH S1 S2 DM BM25
87.93 92.53 86.12 79.33 78.95 84.70
Table 2: Precision@1 of the various systems on DUMB-
TIONARY.
To get a deeper understanding of the retrieval per-
formance of the various systems, we studied queries
of varying complexity of misspelling. Table 3 com-
pares the results of our systems with S1 for queries
that are at various edit distances from the correct
names. We observe that M-HASH and B-HASH are
better than S1 in dealing with relatively less severe
misspellings. More interestingly, B-HASH is con-
sistently and significantly better than S1 even when
the misspellings are severe.
Distance M-HASH B-HASH S1
1 96.18 96.55 89.59
2 81.79 87.42 75.76
3 44.07 67.80 59.65
4 21.05 31.58 29.42
5 0.00 37.50 0.00
Table 3: Precision@1 for queries at various edit distances
on DUMBTIONARY.
5.2.2 INTRANET
For INTRANET, search engines could not be used
as baselines and therefore we compare our systems
1261
with Double Metaphone and BM25 in Table 4. We
observe that both M-HASH and B-HASH give sign-
ficantly better retrieval results than the baselines. M-
HASH reduced the error by 36.20% over Double
Metaphone whereas B-HASH reduced it by 51.73%.
Relative to BM25, M-HASH reduced the error by
31.87% whereas B-HASH reduced it by 48.44%.
M-HASH B-HASH DM BM25
70.65 77.79 54.00 56.92
Table 4: Precision@1 of the various systems on IN-
TRANET.
Table 5 shows the results of our systems for
queries that are at various edit distances from the
correct names. We observe that the retrieval results
for each category of queries are consistent with the
results on DUMBTIONARY. As before, B-HASH
gives signficantly better results than M-HASH.
Distance M-HASH B-HASH
1 82.76 87.93
2 57.14 72.86
3 34.29 65.71
4 38.46 53.85
5 6.67 26.67
Table 5: Precision@1 for queries at various edit distances
on INTRANET.
Test Set M-HASH B-HASH
DUMBTIONARY 13.04 46.17
INTRANET 36.20 51.73
Table 6: Percentage error reduction over the best base-
line.
5.2.3 Effect of Training Data Size
As both M-HASH and B-HASH are data driven
systems, the effect of training data size on retrieval
performance is important to study. Table 7 com-
pares the results for systems trained with various
amounts of training data on DUMBTIONARY. B-
HASH trained with just 1000 name pairs gives
95.5% of the performance of B-HASH trained with
15000 name pairs. Similarly, M-HASH trained with
1000 names gives 98.5% of the performance of
M-HASH trained with 30000 name pairs. This is
probably because the spelling mistakes in DUMB-
TIONARY are relatively easy to correct.
Table 8 shows the results on INTRANET. We see
that increase in the size of training data brings sub-
stantial returns for B-HASH. In contrast, M-HASH
gives the best results at 5000 and does not seem to
benefit from additional training data.
Size M-HASH B-HASH
1000 86.60 88.34
5000 87.36 91.13
10000 86.96 92.53
15000 87.19 92.20
30000 87.93 -
Table 7: Precision@1 on DUMBTIONARY as a function
of training data size.
Size M-HASH B-HASH
1000 66.04 66.03
5000 70.65 72.67
10000 68.09 75.26
15000 68.60 77.79
30000 65.40 -
Table 8: Precision@1 on INTRANET as a function of
training data size.
5.2.4 Effect of Conjugate Language
In Sections 5.2.1 and 5.2.2, we saw that bilingual
data gives substantially better results than monolin-
gual data. In the experiments with bilingual data,
we used English-Hindi data for training B-HASH.
A natural question to ask is what happens when we
use someother language, say Hebrew or Russian or
Tamil, instead of Hindi. In other words, does the
retrieval performance, on an average, vary substan-
tially with the conjugate language?
Table 9 compares the results on DUMB-
TIONARY when B-HASH was trained using
English-Hindi, English-Hebrew, English-Russian,
and English-Tamil data. We see that the retrieval
results are good despite the differences in the script
and language. Clearly, the source language (English
in our experiments) benefits from being paired with
any target language. However, some languages seem
1262
to give substantially better results than others when
used as the conjugate language. For instance, Hindi
as a conjugate for English seems to be better than
Tamil. At the time of writing this paper, we do not
know the reason for this behavior. We believe that a
combination of factors including feature representa-
tion, training data, and language-specific confusion
matrix need to be studied in greater depth to say any-
thing conclusively about conjugate languages.
Conjugate DUMBTIONARY INTRANET
Hindi 92.53 77.79
Hebrew 91.30 71.68
Russian 89.42 64.94
Tamil 90.48 69.12
Table 9: Precision@1 of B-HASH for various conjugate
languages.
5.2.5 Error Analysis
We looked at cases where either M-HASH or
B-HASH (or both) failed to suggest the correct
spelling. It turns out that in the DUMBTIONARY
test set, for 81 misspelled names, both M-HASH and
B-HASH failed to suggest the correct name at rank
1. Similarly, in the case of INTRANET test set, both
M-HASH and B-HASH failed to suggest the correct
name at rank 1 for 47 queries. This suggests that
queries that are difficult for one system are also in
general difficult for the other system. However, B-
HASH was able to suggest correct names for some
of the queries where M-HASH failed. In fact, in the
INTRANET test set, whenever B-HASH failed, M-
HASH also failed. And interestingly, in the DUMB-
TIONARY test set, the average edit distance of the
query and the correct name for the cases where M-
HASH failed to get the correct name in top 10 while
B-HASH got it at rank 1 was 2.96. This could be be-
cause M-HASH attempts to map names with smaller
edit distances to similar codewords.
Table 10 shows some interesting cases we found
during error analysis. For the first query, M-HASH
suggested the correct name whereas B-HASH did
not. For the second query, both M-HASH and B-
HASH suggested the correct name. And for the third
query, B-HASH suggested the correct name whereas
M-HASH did not.
Query M-HASH B-HASH
John Tiler John Tyler John Tilley
Ddear Dragba Didear Drogba Didear Drogba
James Pol James Poe James Polk
Table 10: Error Analysis.
5.3 Query Response Time
The average query response time is a measure of
the speed of a system and is an important factor
in real deployments of a Spelling Correction sys-
tem. Ideally, one would like the average query re-
sponse time to be as small as possible. However, in
practice, average query response time is not only a
function of the algorithm?s computational complex-
ity but also the computational infrastructure support-
ing the system. In our expriments, we used a sin-
gle threaded implementation of M-HASH and B-
HASH on an Intel Xeon processor (2.86 GHz). Ta-
ble 11 shows the average query response time. We
note that M-HASH is substantially slower than B-
HASH. This is because the number of collisions
in the NAME BUCKETING stage is higher for M-
HASH.
We would like to point out that both
NAME BUCKETING and NAME MATCHING
stages can be multi-threaded on a multi-core ma-
chine and the query response time can be decreased
by an order easily. Further, the memory footprint
of the system is very small and the codewords
require 4.1 MB for the employees name directory
(150,000 names) and 13.8 MB for the Wikipedia
name directory (550,000 names).
Test Set MHASH BHASH
DUMBTIONARY 190 87
INTRANET 148 75
Table 11: Average response time in milliseconds (single
threaded system running on 2.86 GHz Intel Xeon Proces-
sor).
6 Related Work
Spelling correction of written text is a well stud-
ied problem (Kukich, 1996), (Jurafsky and Mar-
tin, 2008). The first approach to spelling correc-
1263
tion made use of a lexicon to correct out-of-lexicon
terms by finding the closest in-lexicon word (Dam-
erau, 1964). The similarity between a misspelled
word and an in-lexicon word was measured using
Edit Distance (Jurafsky and Martin, 2008). The next
class of approaches applied the noisy channel model
to correct single word spelling errors (Kernighan et
al., 1990), (Brill and Moore, 2000). A major flaw of
single word spelling correction algorithms is they do
not make use of the context of the word in correcting
the errors. The next stream of approaches explored
ways of exploiting the word?s context (Golding and
Roth, 1996), (Cucerzan and Brill, 2004). Recently,
several works have leveraged the Web for improved
spelling correction (Chen et al, 2007),(Islam and
Inkpen, 2009), (Whitelaw et al, 2009). Spelling cor-
rection algorithms targeted for web-search queries
have been developed making use of query logs and
click-thru data (Cucerzan and Brill, 2004), (Ah-
mad and Kondrak, 2005), (Sun et al, 2010). None
of these approaches focus exclusively on correcting
name misspellings.
Name matching techniques have been studied in
the context of database record deduplication, text
mining, and information retrieval (Christen, 2006),
(Pfeifer et al, 1996). Most techniques use one or
more measures of phonetic similarity and/or string
similarity. The popular phonetic similarity-based
techniques are Soundex, Phonix, and Metaphone
(Pfeifer et al, 1996). Some of the string similarity-
based techniques employ Damerau-Levenshtein edit
distance, Jaro distance or Winkler distance (Chris-
ten, 2006). Data driven approaches for learning edit
distance have also been proposed (Ristad and Yiani-
los, 1996). Most of these techniques either give poor
retrieval performance on large name directories or
do not scale.
Hashing techniques for similarity search is also a
well studied problem (Shakhnarovich et al, 2008).
Locality Sensitive Hashing (LSH) is a theoretically
grounded data-oblivious approach for using random
projections to define the hash functions for data ob-
jects with a single view (Charikar, 2002), (Andoni
and Indyk, 2006). Although LSH guarantees that
asymptotically the Hamming distance between the
codewords approaches the Euclidean distance be-
tween the data objects, it is known to produce long
codewords making it practically inefficient. Re-
cently data-aware approaches that employ Machine
Learning techniques to learn hash functions have
been proposed and shown to be a lot more effective
than LSH on both synthetic and real data. Semantic
Hashing employs Restricted Boltzmann Machine to
produce more compact codes than LSH (Salakhutdi-
nov and Hinton, 2009). Spectral Hashing formalizes
the requirements for a good code and relates them to
the problem of balanced graph partitioning which is
known to be NP hard (Weiss et al, 2008). To give
a practical algorithm for hashing, Spectral Hashing
assumes that the data are sampled from a multidi-
mensional uniform distribution and solves a relaxed
partitioning problem.
7 Conclusions
We developed two hashing-based techniques for
spelling correction of person names in People
Search applications.To the best of our knowledge,
these are the first techniques that focus exclusively
on correcting spelling mistakes in person names.
Our approach has several advantages over other
spelling correction techniques. Firstly, we do not
suggest incorrect suggestions for valid queries un-
like (Cucerzan and Brill, 2004). Further, as we sug-
gest spellings from only authoritative name direc-
tories, the suggestions are always well formed and
coherent. Secondly, we do not require query logs
and other resources that are not easily available un-
like (Cucerzan and Brill, 2004), (Ahmad and Kon-
drak, 2005). Neither do we require pairs of mis-
spelled names and their correct spellings for learn-
ing the error model unlike (Brill and Moore, 2000)
or large-coverage general purpose lexicon for unlike
(Cucerzan and Brill, 2004) or pronunciation dictio-
naries unlike (Toutanova and Moore, 2002). Thirdly,
we correct the query as a whole unlike (Ahmad and
Kondrak, 2005) and can handle word order changes
unlike (Cucerzan and Brill, 2004). Fourthly, we
do not iteratively process misspelled name unlike
(Cucerzan and Brill, 2004). Fifthly, we handle large
name directories efficiently unlike the spectrum of
name matching techniques discussed in (Pfeifer et
al., 1996). Finally, our training data requirement is
relatively small.
As future work, we would like to explore the pos-
sibility of learning hash functions using 1) bilingual
1264
and monolingual data together and 2) multiple con-
jugate languages.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. In
HLT ?05: Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, pages 955?962, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Alexandr Andoni and Piotr Indyk. 2006. Near-optimal
hashing algorithms for approximate nearest neighbor
in high dimensions. In FOCS, pages 459?468.
Rahul Bhagat and Eduard H. Hovy. 2007. Phonetic mod-
els for generating spelling variants. In IJCAI, pages
1570?1575.
Mikhail Bilenko, Raymond J. Mooney, William W. Co-
hen, Pradeep D. Ravikumar, and Stephen E. Fienberg.
2003. Adaptive name matching in information inte-
gration. IEEE Intelligent Systems, 18(5):16?23.
E. Brill and R. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceedings
of ACL ?00, pages 286?293.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In STOC, pages 380?388.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improving
query spelling correction using web search results. In
EMNLP-CoNLL, pages 181?189.
P. Christen. 2006. A comparison of personal name
matching: techniques and practical issues. Techni-
cal Report TR-CS-06-02, Dept. of Computer Science,
ANU, Canberra.
William W. Cohen, Pradeep D. Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of string
distance metrics for name-matching tasks. In IIWeb,
pages 73?78.
S Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of EMNLP ?04, pages
293?300.
F.J. Damerau. 1964. A technique for computer detection
and correction of spelling errors. Communications of
ACM, 7(3):171?176.
C. Friedman and R. Sideli. 1992. Tolerating spelling
errors during patient validation. Computers and
Biomedical Research, 25:486?509.
Andrew R. Golding and Dan Roth. 1996. Applying win-
now to context-sensitive spelling correction. CoRR,
cmp-lg/9607024.
Gene H. Golub and Charles F. Van Loan. 1996. Matrix
Computations. Johns Hopkins University Press, Balti-
more, MD, 3rd edition.
David R. Hardoon, Sa?ndor Szedma?k, and John Shawe-
Taylor. 2004. Canonical correlation analysis: An
overview with application to learning methods. Neu-
ral Computation, 16(12):2639?2664.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using google web 1tn-gram data
set. In CIKM, pages 1689?1692.
D. Jurafsky and J.H. Martin. 2008. Speech and Lan-
guage Processing. Prentice-Hall.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on a
noisy channel model. In COLING, pages 205?210.
K. Kukich. 1996. Techniques for automatically correct-
ing words in a text. Computing Surveys, 24(4):377?
439.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schtze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
G. Navarro, R. Baeza-Yates, and J. Azevedo-Arcoverde.
2003. Matchsimile: a flexible approximate matching
tool for searching proper names. Journal of the Amer-
ican Society for Information Science and Technology,
54(1):3?15.
U. Pfeifer, T. Poersch, and N. Fuhr. 1996. Retrieval ef-
fectiveness of proper name search methods. Informa-
tion Processing and Management, 32(6):667?679.
L. Philips. 2000. The double metaphone search algo-
rithm. C/C++ Users Journal.
Eric Sven Ristad and Peter N. Yianilos. 1996. Learning
string edit distance. CoRR, cmp-lg/9610005.
Ruslan Salakhutdinov and Geoffrey E. Hinton. 2009. Se-
mantic hashing. Int. J. Approx. Reasoning, 50(7):969?
978.
Gregory Shakhnarovich, Trevor Darrell, and Piotr In-
dyk. 2008. Nearest-neighbor methods in learning
and vision. IEEE Transactions on Neural Networks,
19(2):377?377.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.
2010. Learning phrase-based spelling error models
from clickthrough data. In Proceedings of ACL 2010.
K. Toutanova and R. Moore. 2002. Pronounciation mod-
eling for improved spelling correction. In Proceedings
of ACL ?02, pages 141?151.
Yair Weiss, Antonio B. Torralba, and Robert Fergus.
2008. Spectral hashing. In NIPS, pages 1753?1760.
Casey Whitelaw, Ben Hutchinson, Grace Chung, and Ged
Ellis. 2009. Using the web for language independent
spellchecking and autocorrection. In EMNLP, pages
890?899.
1265
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 930?940,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Improving Bilingual Projections via Sparse Covariance Matrices
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA
jags@umiacs.umd.edu
Raghavendra Udupa
Microsoft Research
Bangalore, India
raghavu@microsoft.com
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Abhijit Bhole
Microsoft Research
Bangalore, India
v-abbhol@microsoft.com
Abstract
Mapping documents into an interlingual rep-
resentation can help bridge the language bar-
rier of cross-lingual corpora. Many existing
approaches are based on word co-occurrences
extracted from aligned training data, repre-
sented as a covariance matrix. In theory, such
a covariance matrix should represent seman-
tic equivalence, and should be highly sparse.
Unfortunately, the presence of noise leads to
dense covariance matrices which in turn leads
to suboptimal document representations. In
this paper, we explore techniques to recover
the desired sparsity in covariance matrices in
two ways. First, we explore word association
measures and bilingual dictionaries to weigh
the word pairs. Later, we explore different
selection strategies to remove the noisy pairs
based on the association scores. Our experi-
mental results on the task of aligning compa-
rable documents shows the efficacy of sparse
covariance matrices on two data sets from two
different language pairs.
1 Introduction
Aligning documents from different languages arises
in a range of tasks such as parallel phrase extrac-
tion (Gale and Church, 1991; Rapp, 1999), mining
translations for out-of-vocabulary words for statis-
tical machine translation (Daume III and Jagarla-
mudi, 2011) and document retrieval (Ballesteros and
Croft, 1996; Munteanu and Marcu, 2005). In this
task, we are given a comparable corpora and some
documents in one language are assumed to have a
comparable document in the other language and the
goal is to recover this hidden alignment. In this pa-
per, we address this problem by mapping the docu-
ments into a common subspace (interlingual repre-
sentation). This common subspace generalizes the
notion of vector space model for cross-lingual ap-
plications (Turney and Pantel, 2010).
Most of the existing approaches use manually
aligned document pairs to find a common subspace
in which the aligned document pairs are maximally
correlated. The sub-space can be found using ei-
ther generative approaches based on topic modeling
(Mimno et al, 2009; Jagarlamudi and Daume? III,
2010; Zhang et al, 2010; Vu et al, 2009) or dis-
criminative approaches based on variants of Princi-
pal Component Analysis (PCA) and Canonical Cor-
relation Analysis (CCA) (Susan T. Dumais, 1996;
Vinokourov et al, 2003; Platt et al, 2010; Haghighi
et al, 2008). Both styles rely on document level
term co-occurrences to find the latent representation.
The discriminative approaches capture essential
word co-occurrences in terms of two monolingual
covariance matrices and a cross-covariance matrix.
Subsequently, they use these covariance matrices to
find projection directions in each language such that
aligned documents lie close to each other (Sec. 2).
The strong reliance of these approaches on the co-
variance matrices leads to problems, especially with
the noisy data caused either by the noisy words
in a document or the noisy document alignments.
Noisy data is not uncommon and is usually the case
with data collected from community based resources
such as Wikipedia. This degrades performance of a
930
variety of tasks, such as transliteration Mining (Kle-
mentiev and Roth, 2006; Hermjakob et al, 2008;
Ravi and Knight, 2009) and multilingual web search
(Gao et al, 2009).
In this paper, we address the problem of identi-
fying and removing noisy entries in the covariance
matrices. We address this problem in two stages.
In the first stage, we explore the use of word asso-
ciation measures such as Mutual Information (MI)
and Yule?s ? (Reis and Judd, 2000) in computing
the strength of a word pair (Sec. 3.1). We also
explore the use of bilingual dictionaries developed
from cleaner resources such as parallel data. In the
second stage, we use the association strengths in fil-
tering out the noisy word pairs from the covariance
matrices. We pose this as a word pair selection prob-
lem and explore multiple strategies (Sec. 3.2).
We evaluate the utility of sparse covariance ma-
trices in improving the bilingual projections incre-
mentally (Sec. 4). We first report results on syn-
thetic multi-view data where the true correspon-
dences between features of different views are avail-
able. Moreover, this also lets us systematically ex-
plore the effect of noise level on the accuracy. Our
experimental results show a significant improvement
when the true correspondences are available. Later,
we report our experimental results on the document
alignment task on Europarl and Wikipedia data sets
and on two language pairs. We found that sparsify-
ing the covariance matrices helps in general, but us-
ing cleaner resource such bilingual dictionaries per-
formed best.
2 Canonical Correlation Analysis (CCA)
In this section, we describe how Canonical Correla-
tion Analysis is used to solve the problem of align-
ing bilingual documents. We mainly focus on repre-
senting the solution of CCA in terms of covariance
matrices. Since most of the existing discriminative
approaches are variants of CCA, showing the advan-
tage of recovering sparseness in CCA makes it appli-
cable to the other variants as well.
Given a training data of n aligned document pairs,
CCA finds projection directions for each language,
so that the documents when projected along these di-
rections are maximally correlated (Hotelling, 1936).
Let X (d1?n) and Y (d2?n) be the representation
of data in both the languages and further assume that
the data is centered (subtract the mean vector from
each document i.e. xi?xi??x and yi ? yi??y).
Then CCA finds projection directions a and b which
maximize:
aTXY Tb?
aTXXTa
?
bTY Y Tb
s.t. aTXXTa = 1 & bTY Y Tb = 1
The projection directions are obtained by solving the
generalized eigen system:
[
0 Cxy
Cyx 0
] [
a
b
]
=
[
(1-?)Cxx+?I 0
0 (1-?)Cyy+?I
] [
a
b
]
(1)
where Cxx = XXT , Cyy = Y Y T are the monolin-
gual covariance matrices, Cxy = XY T is the cross-
covariance matrix and ? is the regularization param-
eter. Using these eigenvectors as columns, we form
the projection matrices A and B. These projection
matrices are used to map documents in both the lan-
guages into interlingual representation.
Given any new pair of documents, their similarity
is computed by first mapping them into the lower di-
mensions space and computing the cosine similarity
between their projections. In general, using all the
eigenvectors is sub optimal and thus retaining top
eigenvectors leads to better generalizability.
3 Covariance Selection
As shown above, the underlying objective function
in most of the discriminative approaches is of the
form aTXY Tb. This can be rewritten as :
aTXY Tb =
n?
k=1
?xk,a??yk,b?
=
n?
k=1
( d1?
i=1
Xi,kai ?
d2?
j=1
Yj,kbj
)
=
d1?
i=1
d2?
j=1
aibj
( n?
k=1
Xi,kYj,k
)
=
d1,d2?
i,j=1
aibjCxyij (2)
Similarly, the constraints can also be rewritten as?d1
i,j=1 aiajCxxij = 1 and
?d2
i,j=1 bibjC
yy
ij = 1.
931
Maximizing this objective function, under the
constraints, involves a careful selection of the vec-
tors a and b such that aibj is high whenever Cxyij
is high. So, every non-zero entry of the cross-
covariance matrix restricts the choice of the pro-
jection directions. While this may not be a severe
problem when the training data is clean, but this is
very uncommon especially in the case of high di-
mensional data like text documents. Moreover, the
inherent ambiguity of natural languages increases
the chances of seeing a noisy word in any docu-
ment. Every occurrence of a noisy word will have a
non-zero contribution towards the covariance matrix
making it dense, which in turn prevents the selection
of appropriate projection directions.
In this section, we describe some techniques to
recover the sparsity by removing the noisy entries
from the covariance matrices. We break this task
into two sub problems: computing an association
score for every word pair and then using an appro-
priate strategy to identify the noisy pairs based on
their weights. We explore multiple ways to address
both the steps in the following two sections. For
the sake of convenience and clarity, we describe our
techniques in the context of cross-covariance ma-
trix between English and Spanish language pair. But
these techniques extend directly to monolingual co-
variance matrices, and to different language pairs as
well.
3.1 Computing Word Pair Association
The first step in filtering out the noisy word co-
occurrences is to use an appropriate measure to com-
pute the strength of word pairs (English and Span-
ish words). This is a well studied problem and sev-
eral association measures have been proposed in the
NLP literature (Dunning, 1993; Inkpen and Hirst,
2002; Moore, 2004). These association measures
can be divided into groups based on the statistics
they use (Hoang et al, 2009). Here we explore a few
of them for sparsifying the cross-covariance matrix.
3.1.1 Covariance
The first option is to use the cross-covariance
matrix itself. As noted above, when the data ma-
trix is centered, the cross-covariance of an English
word (ei) with a Spanish word (fj) is given by?n
k=1 XikYjk. It measures the strength with which
two words co-occur together. This measure uses in-
formation about the occurrence of a word pair in
aligned documents and doesn?t use other statistics
such as ?how often this pair doesn?t co-occur to-
gether? and so on.
3.1.2 Mutual Information
Association measures like covariance and Point-
wise Mutual Information, which only use the fre-
quency with which a word pair co-occurs, often
overestimate the strength of low frequent words
(Moore, 2004). On the other hand, measures
like Log-likelihood ratio (Dunning, 1993) and Mu-
tual Information (MI) use other statistics like the
marginal probabilities of each of the words.
For any two words, ei and fj , let n11, n10, n01
and n00 denote the number of documents in which
both the words co-occur, only English word occurs,
only Spanish word occurs and none of the words oc-
cur. Then the Mutual Information of this word pair
is given by:
MI(ei, fj) =
1
n
?
i,j?{0,1}
nij log
nij ? n
ninj
(3)
where ni and nj denote the number of documents
in which the English and the Spanish word occurs
and n is the total number of documents. We treat
the occurrence of a word in a document slightly dif-
ferent from others, we treat a word as occurring in
a document if it has occurred more than its average
frequency in the corpus. Log-likelihood ratio and
the MI differ only in terms of the constant they use,
so we use only MI in our experiments.
3.1.3 Yule?s ?
Yule?s ? is another popular association measure
used in psychology (Reis and Judd, 2000). It uses
same statistics used by Mutual Information but dif-
fers in the way in which they are combined. MI con-
verts the frequencies into probabilities before com-
puting the association measure where as Yule?s ?
uses the observed frequencies directly, and doesn?t
make any assumptions about the underlying proba-
bility distributions. Given the same interpretation of
the variables as introduced in the previous section,
the Yule?s ? is estimated as:
? =
?n00n11 ?
?n01n10?n00n11 +
?n01n10
(4)
932
This way of combining the frequencies bears simi-
larity with the log-odds ratio.
3.1.4 Bilingual Dictionary
The above three association measures use the
same training data that is available to compute the
covariance matrices in CCA. Thus, their utility in
bringing additional information, which is not cap-
tured by the covariance matrices, is arguable (our
experiments show that they are indeed helpful).
Moreover, they use document level co-occurrence
information which is coarse compared to the co-
occurrence at sentence level or the translational in-
formation provided by a bilingual dictionary. So,
we use bilingual dictionaries as our final resource to
weigh the word co-occurrences. Notice that, using
bilingual information brings in information gleaned
from an external corpus.
We use translation tables learnt using Giza++
(Och and Ney, 2003) on Europarl data set. Since the
translation tables are asymmetric, we combine trans-
lation tables from both the directions. We first use a
threshold on the conditional probability to filter out
the low probability ones and then convert them into
joint probabilities before combining. For each word
pair (ei, fj), we compute the score as:
1
2
(
P (ei|fj)P (fj) + P (fj|ei)P (ei)
)
While the first three association measures can also
be applied to monolingual data, bilingual dictionary
can?t be used for weighting monolingual word pairs.
So in this case, we use either of the above mentioned
techniques for weighting monolingual word pairs.
3.2 Selection Strategies
The next step after computing association measure
for all word pairs is to use them in selecting the pairs
that need to be retained. In this section, we describe
some approaches such as thresholding and matching
for the word pair selection.
3.2.1 Thresholding
A straight forward way to remove the noisy word
co-occurrences is to zero out the entries of the
cross-covariance matrix that are lower than a thresh-
old. To understand the motivation, consider the
rewritten objective function of CCA, aTXY Tb =
?
ij C
xy
ij aibj . This is linear in terms of the individ-
ual components of the cross-covariance matrix. So,
if we want to remove some of the entries of the co-
variance matrix with minimal change in the value of
the objective function, then the optimal choice is to
sort the entries of the covariance matrix and filter out
the less confident word pairs.
3.2.2 Relative Thresholding
While the thresholding strategy described in the
above section is very simple, it is often biased by
the frequent words. Since a frequent word co-occurs
with other words often, it naturally tends to have
high association with most of the other words. As
a result, absolute thresholding tends to remove all
the less frequent word pairs while leaving the co-
occurrences of the frequent words untouched. Even-
tually, this may lead to zeroing out some of the rows
or the columns of the cross-covariance matrix.
To circumvent this, we try thresholding at word
level. For every English word, we choose a few
Spanish words that have high association and vice
versa. Since the nearest neighbour property is asym-
metric, we take the union of all the selected word
pairs. That is, we retain a word pair, if either the
Spanish word is in the top ranked list of the English
word or vice versa.
3.2.3 Maximal Matching
Though relative thresholding overcomes the prob-
lem of zeroing out entire rows or columns posed by
direct thresholding, it is still biased by the frequent
words. The high association measure of a frequent
English word with many Spanish words, makes it a
nearest neighbour for lot of Spanish words. One way
to prevent this is to discourage an already selected
English word from associating with a new Spanish
word. This requires a global knowledge of all the
selected pairs and can not be done by looking at the
individual words, as is the case with the greedy strat-
egy employed by the relative thresholding.
We use matching to solve this problem. We for-
mulate the selection of the word pairs as a network
flow problem (Jagarlamudi et al, 2011). The objec-
tive is to select word pairs that have high association
measure while constraining each word to be asso-
ciated with only a few words from other language.
Let Iij denote an indicator variable taking a value of
933
0 or 1 depending on if the word pair (ei, fj) is se-
lected or not. We want each word to be associated
with k words from other language, i.e.?j Iij = k
and
?
i Iij = k. Moreover, we want word pairs
with high association score to be selected. We can
encode this objective and the constraints as the fol-
lowing optimization problem:
argmax
I
d1,d2?
i,j=1
Cxyij Iij (5)
?i
?
j
Iij = k; ?j
?
i
Iij = k; ?i, j Iij ? {0, 1}
If k = 1, then this problem reduces to a linear as-
signment problem and can be solved optimally us-
ing the Hungarian algorithm (Jonker and Volgenant,
1987). For other values of k, this can be solved by
relaxing the constraint Iij ? {0, 1} to 0 ? Iij ? 1.
The optimal solution of the relaxed problem can be
found efficiently using linear programming (Ravin-
dra et al, 1993). The uni-modular nature of the
constraints guarantees an integral solution (Schri-
jver, 2003), so relaxing the original integer problem
doesn?t introduce any error in the optimal solution.
3.2.4 Monolingual Augmentation
The above three selection strategies operate on the
covariance matrices independently. In this section
we propose to combine them. Specifically, we pro-
pose to augment the set of selected bilingual word
pairs using the monolingual word pairs. We first use
any of the above mentioned strategies to select bilin-
gual and monolingual word pairs. Let Ixy, Ixx and
Iyy be the binary matrices that indicate the selected
word pairs based on the bilingual and monolingual
association scores. Then the monolingual augmen-
tation strategy updates Ixy in the following way:
Ixy ? Binarize(IxxIxyIyy)
i.e., we multiply Ixy with the monolingual selection
matrices and then binarize the resulting matrix. Our
monolingual augmentation is motivated by the fol-
lowing probabilistic interpretation:
P (x, y) =
?
x?,y?
P (x|x?)P (y|y?)P (x?, y?)
which can be rewritten as P ? T xP (T y)T where
T x and T y are monolingual state transition matrices.
3.3 Our Approach
In this section we summarize our approach for the
task of finding aligned documents from a cross-
lingual comparable corpora. The training phase in-
volves finding projection directions for documents
of both the languages. We compute the covariance
matrices using the training data. Then we use any
of the word association measures (Sec. 3.1) along
with a selection criteria (Sec. 3.2) to recover the
sparseness in either only the cross-covariance or all
of the covariance matrices. Let Ixy, Ixx and Iyy
be the binary matrices which represent the word
pairs that are selected based on the chosen sparsi-
fication technique. Now, we replace the covariance
matrices in Eq. 1 as follows: Cxx ? Cxx ? Ixx,
Cyy ? Cyy ? Iyy and Cxy ? Cxy ? Ixy where
? denotes the element-wise matrix product. Subse-
quently, we solve the generalized eigenvalue prob-
lem shown in Eq. 1 to obtain the projection direc-
tions. Let A and B be the matrices formed with top
eigenvectors of Eq. 1 as the columns. These pro-
jection matrices are used to map documents into the
interlingual representation. Such an interlingual rep-
resentation is useful in many tasks like cross-lingual
text categorization (Bel et al, 2003) multilingual
web search (Gao et al, 2009) and so on.
During the testing, given an English document x,
finding an aligned Spanish document involves solv-
ing:
argmax
y
xT
(
(ABT )? Ixy
)
y
?
xT
(
(AAT )? Ixx
)
x
?
yT
(
(BBT )? Iyy
)
y
If the documents are normalized before hand, then
the above equation reduces to computing only the
numerator.
4 Experiments
4.1 Experimental Setup
We experiment with the task of finding aligned doc-
uments from a cross-lingual comparable corpora. In
this task, we are given comparable corpora consist-
ing of two document collections, each in a differ-
ent language. As the corpora are comparable, some
documents in one collection have a comparable doc-
ument in the other collection. The task is to recover
934
this hidden alignment. The recovered alignment is
compared against the ground truth.
We evaluate our idea of sparsifying the covari-
ance matrices incrementally. We first evaluate the
effectiveness of our approach on synthetic data, as
it enables us to systematically study the effect of
noise. Subsequently, we evaluate each of the above
discussed sparsification strategies on real world data
sets. We have discussed four possible ways for
computing word association measure and three ap-
proaches for word pair selection. That leaves us 12
different ways for sparsifying the covariance matri-
ces, with each method having parameters to control
the amount of sparseness. We use a small amount of
development data for model selection and parameter
tuning and choose a few promising models. Finally,
we compare these selected models with state-of-the-
art baselines on two language pairs and on two dif-
ferent data sets.
In each case, we use the training data to learn
the projection directions. And then, for each of the
test documents, we find the aligned document from
other language. We report average accuracy of the
top ranked document and also the Mean Reciprocal
Rank (MRR) of the true aligned document.
4.2 Synthetic Data
We follow the generative story introduced in Bach
and Jordan (2005) to generate synthetic multi-view
data. Their method does not assume any correspon-
dence between the feature dimensions of both the
views. We modify their approach slightly so that
we know the actual correspondence between the fea-
tures. We use these true feature correspondences for
sparsification of the cross-covariance matrix.
We first generate a d dimensional vector in the
common latent space and then use the projection
matrices to map it into the individual feature spaces
as follows:
z ? N (0, Id)
x|z ? (W1z + ?1) + ? N (0, Id1)
y|z ? (W1z + ?2) + ? N (0, Id2)
Notice that we use the same projection matrix W1
for both the views, this ensures a one-to-one corre-
spondence between the features of both the spaces.
Moreover, we also introduce a parameter ? which
controls the amount of noise in the data.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  1.5  2  2.5  3  3.5  4
Sparse MRR
Sparse Accuracy
CCA MRR
CCA Accuracy
Figure 1: Accuracy of CCA and our sparsified version
with the noise parameter.
We generate a total of 3000 pairs of points and use
2000 of them for training the models and the rest
for evaluation. We use the true feature correspon-
dences to form the cross-covariance selection ma-
trix Ixy (Sec. 3.3). For this experiment, we use the
full monolingual covariance matrices. We train both
CCA and our sparse version on the training data and
evaluate them on the test data. We repeat this mul-
tiple times and report the average accuracies. Fig. 1
shows the performance of CCA and our sparse CCA,
as we vary the noise parameter ? from 1 to 4. It
is very clear that the sparse version performs sig-
nificantly better than CCA. As the noise increases,
the performance of CCA drops quickly. This exper-
iment demonstrates a significant performance gain
when the true correspondences are available. But
this information is not available in the case of real
world data sets, so we try to approximate it.
4.3 Model Selection
As we have discussed, there are several choices for
computing the association measure and for selecting
the word pairs to be retained. And each of them have
sparsity parameters, giving raise to many possible
models. For model selection, we use approximately
5000 document pairs collected from the Wikipedia
between English and Spanish. We use the cross-
language links provided as the ground truth. We to-
kenize the documents, retain only the most frequent
2000 words in each language and convert the docu-
935
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 2000  4000  6000  8000  10000  12000  14000  16000  18000  20000
MI+Match
Yule+Match
Cov+Match
MI+RelThreshold
Yule+RelThreshold
Cov+RelThreshold
MI+Threshold
Yule+Threshold
Cov+Threshold
CCA
Figure 2: Comparison of the word association measures
along with different selection criteria. The x-axis plots
the number of non-zero entries in the covariance matrices
and the y-axis plots the accuracy of top-ranked document.
ments into TFIDF vectors. We use 60% of the data
for training different models and the rest for evaluat-
ing the models. We choose a few promising models
based on this development set results and evaluate
them on bigger data sets.
4.3.1 Selection Strategies
In the first experiment, we combine the three
association measures, Covariance (Cov), MI and
Yule?s ?, with the three selection criteria, Thresh-
old, Relative Threshold (RelThreshold) and Match-
ing (Match). Fig. 2 shows the performance of these
different combinations with varying levels of spar-
sity in the covariance matrices. The horizontal line
represents the performance of CCA on this data set.
We start with 2000 non-zero entries in the covari-
ance matrices and experiment up to 20,000 non-zero
entries. Since our data set has 2000 words in each
language, 2000 non-zero entries in a covariance ma-
trix implies that, on an average, every word is as-
sociated with only one word. This results in highly
sparse covariance matrices.
Overall, we observe that reducing the level of
sparsity , i.e. selecting more number of elements in
the covariance matrices, increases the performance
slightly and then decreases again. From the figure, it
seems that sparsifying the covariance matrices might
help in improving the performance of the task. But
it is interesting to note that not all the models per-
form better than CCA. In fact, both the models that
achieve better scores use Matching as the selection
criteria. This suggests that, apart from the weighting
of the word pairs, appropriate selection of the word
pairs is also equally important. In the rest of the ex-
periments we mainly report results with Matching as
the selection criterion. From this figure, we observe
that Mutual Information and Yule?s ? perform com-
petitively but they consistently outperform models
that use covariance as the association measure. So
in the rest of the experiments we report results with
MI or Yule?s ?.
4.3.2 Amount of Sparsity
In the previous experiment, we used same level
of sparsity for all the covariance matrices, i.e. same
number of associations were selected for each word
in all the three covariance matrices. In the following
experiment, we use different levels of sparsity for
the individual covariance matrices. Fig. 3 shows the
performance of Yule+Match and Dictionary+Match
combinations with different levels of sparsity. In
the Yule+Match combination, we use Yule?s ? as-
sociation measure for weighting the word pairs and
use matching for selection. In the Dictionary+Match
combination, we use bilingual dictionary for sparsi-
fying cross-covariance matrix, i.e. we keep all the
word pairs whose conditional translation probabil-
ity is above a threshold. And for monolingual word
pairs, we use MI for weighting and matching for
word pair selection.
For each level of sparsity of the cross-covariance
matrix, we experiment with different levels of spar-
sity on the monolingual covariance matrices. ?Only
XY? indicates we use the full monolingual covari-
ance matrices. In ?Match(k)? runs, we allow each
word to be associated with a total of k words (Eq. 5).
?Aug? indicates that we use monolingual augmen-
tation to refine the sparsity of the cross-covariance
matrix (Sec. 3.2.4).
From both the figures 3(a) and 3(b), we observe
that ?Only XY? run (dark blue) performs poorly
compared to the other runs, indicating that sparsify-
ing all the covariance matrices is better than spar-
sifying only the cross-covariance matrix. In the
936
(a) Performance of Yule+Match combination. The x-axis plots
the number of Spanish words selected per each English word
and vice versa. This determines the sparsity of Cxy. Matching
is used as selection criteria for all the covariance matrices.
(b) Performance of Dictionary+Match combination. The x-axis
plots the threshold on bilingual translation probability and it deter-
mines the sparsity of Cxy. Matching is used to select only the mono-
lingual sparsity.
Figure 3: Comparison of Yule+Match and Dictionary+Match combination with different levels of sparsity for the
covariance matrices. In both the figures, the x-axis plots the sparsity of the cross-covariance matrix and for each
value we try different levels of sparsity on the monolingual covariance matrices (which are grouped together). The
description of these individual runs is provided in the relevant parts of the text. The y-axis plots the accuracy of the
top-ranked document. CCA achieves 61% accuracy on this data set.
Yule+Match combination, Fig. 3(a), all the runs
seem to be performing better when each English
word is allowed to associate with 2 or 3 Spanish
words and vice versa. Among different ways of se-
lecting the monolingual word pairs, Match(2)+Aug
performs better than the remaining runs. So we use
Match(2)+Aug combination for the Yule?s ? mea-
sure.
Unlike the Yule+Match combinations, there is no
clear winner for Dictionary+Match combinations.
First of all, the performance increase as we increase
the translation probability threshold and then de-
creases again (indicated by the ?Average? perfor-
mance in Fig. 3(b)). On an average, all the sys-
tems perform better with a threshold of 0.01, which
we use in our final experiments. In this case, both
Match(1) and Match(2)+Aug runs (orange and green
bars respectively) perform competitively so we use
both of these models in our final experiments.
In both the above experiments, the performance
bars are very similar when we use MI instead of
Yule and vice versa for weighting monolingual word
pairs. Thus, to illustrate the main ideas we chose
Yule?s ? for the former combination and MI for the
latter combination.
4.3.3 Promising Models
Based on the above experiments, we choose the
following combinations for our final experiments.
Yule(l)+Match(k), where l ? {2, 3} is the number
of Spanish words allowed for each English word
and vice versa and k=2 is the number of monolin-
gual word associations for each word. We also run
both these combinations with monolingual augmen-
tation, indicated by Yule(l)+Match(k)+Aug. For
dictionary based weighting, Dictionary+Match(k),
we choose a translation probability threshold of 0.01
and try k ? {1, 2}. Again, we run these combina-
tions with monolingual augmentation identified by
Dictionary+Match(k)+Aug.
4.4 Results
For our final results, we choose data in two language
pairs (English-Spanish and English-German) from
two different resources, Europarl (Koehn, 2005) and
Wikipedia. For Europarl data sets, we artificially
make them comparable by considering the first half
937
Wikipedia Europarl
English-Spanish English-German English-Spanish English-German
Acc. MRR Acc. MRR Acc. MRR Acc. MRR
CCA 0.776 0.852 0.570 0.699 0.872 0.920 0.748 0.831
OPCA 0.781 0.856 0.570 0.700 0.870 0.920 0.748 0.831
Yule(2)+Match(2) 0.798? 0.866? 0.576 0.703 0.901? 0.939? 0.780? 0.853?
Yule(2)+Match(2)+Aug 0.811? 0.876? 0.602? 0.723? 0.883 0.927 0.771? 0.847?
Yule(3)+Match(2) 0.803? 0.870? 0.572 0.700 0.856 0.907 0.747 0.830
Yule(3)+Match(2)+Aug 0.793? 0.861? 0.610? 0.726? 0.878+ 0.925+ 0.763+ 0.843?
Dictionary+Match(1) 0.811? 0.875? 0.656? 0.762? 0.928? 0.957? 0.874? 0.922?
Dictionary+Match(2) 0.811? 0.876? 0.623? 0.736? 0.923? 0.955? 0.853? 0.907?
Dictionary+Match(2)+Aug 0.825? 0.885? 0.630? 0.735? 0.897? 0.935? 0.866? 0.917?
Table 1: Performance of our models in comparison with CCA and OPCA on English-Spanish and English-German
language pairs. ? and + indicate statistical significance measured by paired t-test at p=0.01 and 0.05 levels respectively.
When an improvement is significant at p=0.01 it is automatically significant at p=0.05 and hence is not shown.
of English document and the second half of its
aligned foreign language document (Mimno et al,
2009). For Wikipedia data set, we use the cross-
language link as the ground truth. For each of these
data sets, we choose approximately 5000 aligned
document pairs. We remove the stop words and keep
all the words that occur in at least five documents.
After the preprocessing, on an average, we are left
with 4700 words in each language. Subsequently we
convert the documents into their TFIDF representa-
tion.
In Platt et al (2010), the authors compare differ-
ent systems on the comparable document retrieval
task and show that discriminative approaches work
better compared to their generative counter parts.
So, here we compare only with the state-of-the-
art discriminative systems such as CCA and OPCA
(Platt et al, 2010). For each of the systems, we re-
port the average results of five-fold cross validation.
We divide the data into 3:1:1 ratio for training, vali-
dation and test sets. The validation data set is used to
select the best number of dimensions of the common
sub space. For both CCA and our models, we set the
regularization parameter ? to 0.3 which we found
works well in a relevant but different experiments.
For OPCA, we manually tried different regulariza-
tion parameters ranging from 0.0001 to 1 and found
that a value of 0.001 worked best.
The results are shown in Table 1. On these data
sets, both CCA and OPCA performed competitively.
OPCA takes advantage of the common vocabulary
in both the languages. But in our data sets, vocab-
ulary of both the languages is treated differently, so
it is not surprising that they give almost the same
results. From the results, it is clear that sparsify-
ing the covariance matrices helps improving the ac-
curacies significantly. In all the four data sets, the
best performing method always used dictionary for
cross-lingual sparsity selection. This indicates that
using fine granular information such as a bilingual
dictionary gleaned from an external source is very
helpful in improving the accuracies. Among the
models that rely solely on the training data, models
that use monolingual augmentation performed bet-
ter on Wikipedia data set, while models that do not
use augmentation performed better on Europarl data
sets. This suggests that, when the aligned documents
are clean (closer to being parallel) the statistics com-
puted from cross-lingual corpora are trustworthy. As
the documents become comparable, we need to use
monolingual statistics to refine the bilingual statis-
tics. Moreover, these models achieve higher gains in
the case of Wikipedia data set compared to the gains
in Europarl. This conforms with our initial hunch
that, when the training data is clean the covariance
matrices tend to be less noisy.
5 Discussion
In this paper, we have proposed the idea of sparsi-
fyng covariance matrices to improve bilingual pro-
938
jection directions. We are not aware of any NLP
research that attempts to recover the sparseness of
the covariance matrices to improve the projection
directions. Our work is different from the sparse
CCA (Hardoon and Shawe-Taylor, 2011; Rai and
Daume? III, 2009) proposed in the Machine Learning
literature. Their objective is to find projection di-
rections such that the original documents are repre-
sented as a sparse vectors in the common sub-space.
Another seemingly relevant but different direction
is the sparse covariance matrix selection research
(Banerjee et al, 2005). The objective in this work
is to find matrices such that the inverse of the co-
variance matrix is sparse which has applications in
Gaussian processes.
In this paper, we tried sparsification in the con-
text of CCA only but our technique is general and
can be applied to its variants like OPCA. Our ex-
perimental results show that using external informa-
tion such as bilingual dictionaries which is gleaned
from cleaner resources brings significant improve-
ments. Moreover, we also observe that computing
word pair association measures from the same train-
ing data along with an appropriate selection criteria
can also yield significant improvements. This is cer-
tainly encouraging and in future we would like to
explore more sophisticated techniques to recover the
sparsity based on the training data itself.
6 Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This material is partially supported
by the National Science Foundation under Grant No.
1139909.
References
Francis R. Bach and Michael I. Jordan. 2005. A proba-
bilistic interpretation of canonical correlation analysis.
Technical report, Dept Statist Univ California Berke-
ley CA Tech.
Lisa Ballesteros and W. Bruce Croft. 1996. Dictio-
nary methods for cross-lingual information retrieval.
In Proceedings of the 7th International Conference
on Database and Expert Systems Applications, DEXA
?96, pages 791?801, London, UK. Springer-Verlag.
Onureena Banerjee, Alexandre d?Aspremont, and Lau-
rent El Ghaoui. 2005. Sparse covariance selection
via robust maximum likelihood estimation. CoRR,
abs/cs/0506023.
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization.
Hal Daume III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 407?412, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Comput. Linguist.,
19(1):61?74, March.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th annual meeting on Associ-
ation for Computational Linguistics, pages 177?184,
Morristown, NJ, USA. Association for Computational
Linguistics.
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.
2009. Exploiting bilingual information to improve
web search. In Proceedings of Human Language Tech-
nologies: The 2009 Conference of the Association for
Computational Linguistics, ACL-IJCNLP ?09, pages
1075?1083, Morristown, NJ, USA. ACL.
Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, and
Dan Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of ACL-08:
HLT, pages 771?779, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
David R. Hardoon and John Shawe-Taylor. 2011. Sparse
canonical correlation analysis. Journal of Machine
Learning, 83(3):331?353.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Hung Huu Hoang, Su Nam Kim, and Min-Yen Kan.
2009. A Re-examination of Lexical Association Mea-
sures. In Proceedings of ACL-IJCNLP 2009 Workshop
on Multiword Expressions: Identification, Interpre-
tation, Disambiguation and Applications, Singapore,
August. Association for Computational Linguistics.
H. Hotelling. 1936. Relation between two sets of vari-
ables. Biometrica, 28:322?377.
Diana Zaiu Inkpen and Graeme Hirst. 2002. Ac-
quiring collocations for lexical choice between near-
synonyms. In Proceedings of the ACL-02 workshop
on Unsupervised lexical acquisition - Volume 9, ULA
?02, pages 67?76, Stroudsburg, PA, USA. Association
for Computational Linguistics.
939
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Advances in Information Retrieval,
32nd European Conference on IR Research, ECIR,
volume 5993, pages 444?456, Milton Keynes, UK.
Springer.
Jagadeesh Jagarlamudi, Hal Daume III, and Raghavendra
Udupa. 2011. From bilingual dictionaries to interlin-
gual document representations. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 147?152, Portland, Oregon, USA, June. Associ-
ation for Computational Linguistics.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 817?824, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Summit,
pages 79?86, Phuket, Thailand. AAMT, AAMT.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 880?889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 333?340, Barcelona, Spain, July. Association
for Computational Linguistics.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Comput. Linguist., 31:477?
504, December.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
John C. Platt, Kristina Toutanova, and Wen-tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 251?261,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Piyush Rai and Hal Daume? III. 2009. Multi-label pre-
diction via sparse infinite cca. In Advances in Neural
Information Processing Systems, Vancouver, Canada.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, ACL ?99, pages 519?526,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 37?45, Boulder, Colorado, June. Association for
Computational Linguistics.
K. Ahuja Ravindra, L. Magnanti Thomas, and B. Orlin
James. 1993. Network Flows: Theory, Algorithms,
and Applications. Prentice-Hall, Inc.
Harry T Reis and Charles M Judd. 2000. Handbook of
Research Methods in Social and Personality Psychol-
ogy. Cambridge University Press.
Alexander Schrijver. 2003. Combinatorial Optimization.
Springer.
Michael L. Littman Susan T. Dumais, Thomas K. Lan-
dauer. 1996. Automatic cross-linguistic information
retrieval using latent semantic indexing. In Working
Notes of the Workshop on Cross-Linguistic Informa-
tion Retrieval, SIGIR, pages 16?23, Zurich, Switzer-
land. ACM.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
J. Artif. Intell. Res. (JAIR), 37:141?188.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation
of text via cross-language correlation analysis. In
Advances in Neural Information Processing Systems,
pages 1473?1480, Cambridge, MA. MIT Press.
Thuy Vu, AiTi Aw, and Min Zhang. 2009. Feature-based
method for document alignment in comparable news
corpora. In EACL, pages 843?851.
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010.
Cross-lingual latent topic extraction. In Proceed-
ings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 1128?1137, Up-
psala, Sweden, July. Association for Computational
Linguistics.
940
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 204?213,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Incorporating Lexical Priors into Topic Models
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA
jags@umiacs.umd.edu
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Raghavendra Udupa
Microsoft Research
Bangalore, India
raghavu@microsoft.com
Abstract
Topic models have great potential for help-
ing users understand document corpora.
This potential is stymied by their purely un-
supervised nature, which often leads to top-
ics that are neither entirely meaningful nor
effective in extrinsic tasks (Chang et al
2009). We propose a simple and effective
way to guide topic models to learn topics
of specific interest to a user. We achieve
this by providing sets of seed words that a
user believes are representative of the un-
derlying topics in a corpus. Our model
uses these seeds to improve both topic-
word distributions (by biasing topics to pro-
duce appropriate seed words) and to im-
prove document-topic distributions (by bi-
asing documents to select topics related to
the seed words they contain). Extrinsic
evaluation on a document clustering task
reveals a significant improvement when us-
ing seed information, even over other mod-
els that use seed information na??vely.
1 Introduction
Topic models such as Latent Dirichlet Allocation
(LDA) (Blei et al 2003) have emerged as a pow-
erful tool to analyze document collections in an
unsupervised fashion. When fit to a document
collection, topic models implicitly use document
level co-occurrence information to group seman-
tically related words into a single topic. Since the
objective of these models is to maximize the prob-
ability of the observed data, they have a tendency
to explain only the most obvious and superficial
aspects of a corpus. They effectively sacrifice per-
formance on rare topics to do a better job in mod-
eling frequently occurring words. The user is then
left with a skewed impression of the corpus, and
perhaps one that does not perform well in extrin-
sic tasks.
To illustrate this problem, we ran LDA on
the most frequent five categories of the Reuters-
21578 (Lewis et al 2004) text corpus. This doc-
ument distribution is very skewed: more than half
of the collection belongs to the most frequent cat-
egory (?Earn?). The five topics identified by the
LDA are shown in Table 1. A brief observation
of the topics reveals that LDA has roughly allo-
cated topics 1 & 2 for the most frequent class
(?Earn?) and one topic for the subsequent two
frequent classes (?Acquisition? and ?Forex?) and
merged the least two frequent classes (?Crude?
and ?Grain?) into a single topic. The red colored
words in topic 5 correspond to the ?Crude? class
and blue words are from the ?Grain? class.
This leads to the situation where the topics
identified by LDA are not in accordance with the
underlying topical structure of the corpus. This
is a problem not just with LDA: it is potentially
a problem with any extension thereof that have
focused on improving the semantic coherence of
the words in each topic (Griffiths et al 2005;
Wallach, 2005; Griffiths et al 2007), the doc-
ument topic distributions (Blei and McAuliffe,
2008; Lacoste-Julien et al 2008) or other aspects
(Blei. and Lafferty., 2009).
We address this problem by providing some ad-
ditional information to the model. Initially, along
with the document collection, a user may provide
higher level view of the document collection. For
instance, as discussed in Section 4.4, when run
on historical NIPS papers, LDA fails to find top-
ics related to Brain Imaging, Cognitive Science or
Hardware, even though we know from the call for
204
mln, dlrs, billion, year, pct, company, share, april, record, cts, quarter, march, earnings, stg, first, pay
mln, NUM, cts, loss, net, dlrs, shr, profit, revs, year, note, oper, avg, shrs, sales, includes
lt, company, shares, corp, dlrs, stock, offer, group, share, common, board, acquisition, shareholders
bank, market, dollar, pct, exchange, foreign, trade, rate, banks, japan, yen, government, rates, today
oil, tonnes, prices, mln, wheat, production, pct, gas, year, grain, crude, price, corn, dlrs, bpd, opec
Table 1: Topics identified by LDA on the frequent-5 categories of the Reuters corpus. The categories are Earn,
Acquisition, Forex, Grain and Crude (in the order document frequency).
1 company, billion, quarter, shrs, earnings
2 acquisition, procurement, merge
3 exchange, currency, trading, rate, euro
4 grain, wheat, corn, oilseed, oil
5 natural, gas, oil, fuel, products, petrol
Table 2: An example for sets of seed words (seed top-
ics) for the frequent-5 categories of the Reuters-21578
categorization corpus. We use them as running exam-
ple in the rest of the paper.
papers that such topics should exist in the corpus.
By allowing the user to provide some seed words
related to these underrepresented topics, we en-
courage the model to find evidence of these top-
ics in the data. Importantly, we only encourage
the model to follow the seed sets and do not force
it. So if it has compelling evidence in the data
to overcome the seed information then it still has
the freedom to do so. Our seeding approach in
combination with the interactive topic modeling
(Hu et al 2011) will allow a user to both explore
a corpus, and also guide the exploration towards
the distinctions that he/she finds more interesting.
2 Incorporating Seeds
Our approach to allowing a user to guide the topic
discovery process is to let him provide seed infor-
mation at the level of word type. Namely, the user
provides sets of seed words that are representative
of the corpus. Table 2 shows an example of seed
sets one might use for the Reuters corpus. This
kind of supervision is similar to the seeding in
bootstrapping literature (Thelen and Riloff, 2002)
or prototype-based learning (Haghighi and Klein,
2006). Our reliance on seed sets is orthogonal
to existing approaches that use external knowl-
edge, which operate at the level of documents
(Blei and McAuliffe, 2008), tokens (Andrzejew-
ski and Zhu, 2009) or pair-wise constraints (An-
drzejewski et al 2009).
We build a model that uses the seed words
in two ways: to improve both topic-word and
document-topic probability distributions. For
ease of exposition, we present these ideas sep-
arately and then in combination (Section 2.3).
To improve topic-word distributions, we set up
a model in which each topic prefers to gener-
ate words that are related to the words in a seed
set (Section 2.1). To improve document-topic
distributions, we encourage the model to select
document-level topics based on the existence of
input seed words in that document (Section 2.2).
Before moving on to the details of our mod-
els, we briefly recall the generative story of the
LDA model and the reader is encouraged to refer
to (Blei et al 2003) for further details.
1. For each topic k = 1 ? ? ? T,
? choose ?k ? Dir(?).
2. For each document d, choose ?d ? Dir(?).
? For each token i = 1 ? ? ?Nd:
(a) Select a topic zi ? Mult(?d).
(b) Select a word wi ? Mult(?zi).
where T is the number of topics, ?, ? are hyper-
parameters of the model and ?k and ?d are topic-
word and document-topic Multinomial probabil-
ity distributions respectively.
2.1 Word-Topic Distributions (Model 1)
In regular topic models, each topic k is defined
by a Multinomial distribution ?k over words. We
extend this notion and instead define a topic as a
mixture of two Multinomial distributions: a ?seed
topic? distribution and a ?regular topic? distribu-
tion. The seed topic distribution is constrained to
only generate words from a corresponding seed
set. The regular topic distribution may generate
any word (including seed words). For example,
seed topic 4 (in Table 2) can only generate the
five words in its set. The word ?oil? can be gener-
ated by seed topics 4 and 5, as well as any regular
205
?sT?rT?s1?r1
doc
z=1 z=2 z=T? ? ? ? ? ? ? ? ?
?11 ? ?1 ?T1? ?T
Figure 1: Tree representation of a document in Model
1.
topic. We want to emphasize that, like any regular
topic, each seed topic is a non-uniform probabil-
ity distribution over the words in its set. The user
only inputs the sets of seed words and the model
will infer their probability distributions.
For the sake of simplicity, we describe our
model by assuming a one-to-one correspondence
between seed and regular topics. This assumption
can be easily relaxed by duplicating the seed top-
ics when there are more regular topics. As shown
in Fig. 1, each document is a mixture over T top-
ics, where each of those topics is a mixture of
a regular topic (?r? ) and its associated seed topic
(?s? ) distributions. The parameter ?k controls the
probability of drawing a word from the seed topic
distribution versus the regular topic distribution.
For our first model, we assume that the corpus is
generated based on the following generative pro-
cess (its graphical notation is shown in Fig. 2(a)):
1. For each topic k=1? ? ? T,
(a) Choose regular topic ?rk ? Dir(?r).
(b) Choose seed topic ?sk ? Dir(?s).
(c) Choose ?k ? Beta(1, 1).
2. For each document d, choose ?d ? Dir(?).
? For each token i = 1 ? ? ?Nd:
(a) Select a topic zi ? Mult(?d).
(b) Select an indicator xi ? Bern(?zi)
(c) if xi is 0
? Select a word wi ? Mult(?rzi).
// choose from regular topic
(d) if xi is 1
? Select a word wi ? Mult(?szi).
// choose from seed topic
The first step is to generate Multinomial distribu-
tions for both seed topics and regular topics. The
seed topics are drawn in a way that constrains
their distribution to only generate words in the
corresponding seed set. Then, for each token in a
document, we first generate a topic. After choos-
ing a topic, we flip a (biased) coin to pick either
the seed or the regular topic distribution. Once
this distribution is selected we generate a word
from it. It is important to note that although there
are 2?T topic-word distributions in total, each
document is still a mixture of only T topics (as
shown in Fig. 1). This is crucial in relating seed
and regular topics and is similar to the way top-
ics and aspects are tied in TAM model (Paul and
Girju, 2010).
To understand how this model gathers words
related to seed words, consider a seed topic (say
the fourth row in Table 2) with seed words {grain,
wheat, corn, etc. }. Now by assigning all the re-
lated words such as ?tonnes?, ?agriculture?, ?pro-
duction? etc. to its corresponding regular topic,
the model can potentially put high probability
mass on topic z = 4 for agriculture related doc-
uments. Instead, if it places these words in an-
other regular topic, say z = 3, then the document
probability mass has to be distributed among top-
ics 3 and 4 and as a result the model will pay a
steeper penalty. Thus the model uses seed topic
to gather related words into its associated regu-
lar topic and as a consequence the document-topic
distributions also become focussed.
We have experimented with two ways of choos-
ing the binary variable xi (step 2b) of the gener-
ative story. In the first method, we fix this sam-
pling probability to a constant value which is in-
dependent of the chosen topic (i.e. ?i = ??, ?i =
1 ? ? ? T). And in the second method we learn the
probability as well (Sec. 4).
2.2 Document-Topic distributions (Model 2)
In the previous model we used seed words to im-
prove topic-word probability distributions. Here
we propose a model to explore the use of seed
words to improve document-topic probability dis-
tributions. Unlike the previous model, we will
present this model in the general case where the
number of seed topics is not equal to the number
of regular topics. Hence, we associate each seed
set (we refer seed set as group for conciseness)
with a Multinomial distribution over the regular
topics which we call group-topic distribution.
To give an overview of our model, first, we
transfer the seed information from words onto
206
DT
? ?
?r ?r
?s
Nd
x z
w
(a) Model 1
DT
?
?
? ?
?r
~b
?r Nd
?
?
z
w
g
(b) Model 2
DT
?
?
? ?
?r
~b
?r
?s
Nd
?
?
x z
w
g
(c) SeededLDA
Figure 2: The graphical notation of all the three models. In Model 1 we use seed topics to improve the topic-word
probability distributions. In Model 2, the seed topic information is first transfered to the document level based
on the document tokens and then it is used to improve document-topic distributions. In the final, SeededLDA,
model we combine both the models. In Model 1 and SeededLDA, we dropped the dependency of ?s on hyper
parameter ?s since it is observed. And, for clarity, we also dropped the dependency of x on ?.
the documents that contain them. Then, the
document-topic distribution is drawn in a two step
process: we sample a seed set (g for group) and
then use its group-topic distribution (?g) as prior
to draw the document-topic distribution (?d). We
used this two step process, to allow flexible num-
ber of seed and regular topics, and to tie the topic
distributions of all the documents within a group.
We assume the following generative story and its
graphical notation is shown in Fig. 2(b).
1. For each k = 1? ? ? T,
(a) Choose ?rk ? Dir(?r).
2. For each seed set s = 1? ? ? S,
(a) Choose group-topic distribution ?s ?
Dir(?). // the topic distribution for sth
group (seed set) ? a vector of length T.
3. For each document d,
(a) Choose a binary vector~b of length S.
(b) Choose a document-group distribution
?d ? Dir(?~b).
(c) Choose a group variable g ? Mult(?d)
(d) Choose ?d ? Dir(?g). // of length T
(e) For each token i = 1 ? ? ?Nd:
i. Select a topic zi ? Mult(?d).
ii. Select a word wi ? Mult(?rzi).
We first generate T topic-word distributions
(?k) and S group-topic distributions (?s). Then
for each document, we generate a list of seed sets
that are allowed for this document. This list is
represented using the binary vector ~b. This bi-
nary vector can be populated based on the docu-
ment words and hence it is treated as an observed
variable. For example, consider the (very short!)
document ?oil companies have merged?. Accord-
ing to the seed sets from Table 2, we define a bi-
nary vector that denotes which seed topics contain
words in this document. In this case, this vec-
tor ~b = ?1, 1, 0, 1, 1?, indicating the presence of
seeds from sets 1, 2, 4 and 5.1 As discussed in
(Williamson et al 2010), generating binary vec-
tor is crucial if we want a document to talk about
topics that are less prominent in the corpus.
The binary vector ~b, that indicates which seeds
exist in this document, defines a mean of a
Dirichlet distribution from which we sample a
document-group distribution, ?d (step 3b). We
set the concentration of this Dirichlet to a hy-
perparamter ? , which we set by hand (Sec. 4);
thus, ?d ? Dir(?~b). From the resulting multino-
mial, we draw a group variable g for this docu-
ment. This group variable brings clustering struc-
ture among the documents by grouping the docu-
ments that are likely to talk about same seed set.
Once the group variable (g) is drawn, we
choose the document-topic distribution (?d) from
a Dirichlet distribution with the group?s-topic dis-
tribution as the prior (step 3d). This step ensures
that the topic distributions of documents within
each group are related. The remaining sampling
1As a special case, if no seed word is found in the docu-
ment,~b is defined as the all-ones vector.
207
process proceeds like LDA. We sample a topic
for each word and then generate a word from its
corresponding topic-word distribution. Observe
that, if the binary vector is all ones and if we
set ?d = ?d then this model reduces to the LDA
model with ? and ?r as the hyperparameters.
2.3 SeededLDA
Both of our models use seed words in different
ways to improve topic-word and document-topic
distributions respectively. We can combine both
the above models easily. We refer to the combined
model as SeededLDA and its generative story is
as follows (its graphical notation is shown in Fig.
2(c)). The variables have same semantics as in the
previous models.
1. For each k=1? ? ? T,
(a) Choose regular topic ?rk ? Dir(?r).
(b) Choose seed topic ?sk ? Dir(?s).
(c) Choose ?k ? Beta(1, 1).
2. For each seed set s = 1? ? ? S,
(a) Choose group-topic distribution ?s ?
Dir(?).
3. For each document d,
(a) Choose a binary vector~b of length S.
(b) Choose a document-group distribution
?d ? Dir(?~b).
(c) Choose a group variable g ? Mult(?d).
(d) Choose ?d ? Dir(?g). // of length T
(e) For each token i = 1 ? ? ?Nd:
i. Select a topic zi ? Mult(?d).
ii. Select an indicator xi ? Bern(?zi).
iii. if xi is 0
? Select a word wi ? Mult(?rzi).
iv. if xi is 1
? Select a word wi ? Mult(?szi).
In the SeededLDA model, the process for gen-
erating group variable of a document is same as
the one described in the Model 2. And like in the
Model 2, we sample a document-topic probability
distribution as a Dirichlet draw with the group-
topic distribution of the chosen group as prior.
Subsequently, we choose a topic for each token
and then flip a biased coin. We choose either the
seed or the regular topic based on the result of the
coin toss and then generate a word from its distri-
bution.
2.4 Automatic Seed Selection
In (Andrzejewski and Zhu, 2009; Andrzejewski
et al 2009), the seed information is provided
manually. Here, we describe the use of feature se-
lection techniques, prevalent in the classification
literature, to automatically derive the seed sets. If
we want the topicality structure identified by the
LDA to align with the underlying class structure,
then the seed words need to be representative of
the underlying topicality structure. To enable this,
we first take class labeled data (doesn?t need to
be multi-class labeled data unlike (Ramage et al
2009)) and identify the discriminating features for
each class. Then we choose these discriminating
features as the initial sets of seed words. In prin-
ciple, this is similar to the prototype driven unsu-
pervised learning (Haghighi and Klein, 2006).
We use Information Gain (Mitchell, 1997) to
identify the required discriminating features. The
Information Gain (IG) of a word (w) in a class (c)
is given by
IG(c, w) = H(c) ?H(c|w)
whereH(c) is the entropy of the class andH(c|w)
is the conditional entropy of the class given the
word. In computing Information Gain, we bina-
rize the document vectors and consider whether a
word occurs in any document of a given class or
not. Thus obtained ranked list of words for each
class are filtered for ambiguous words and then
used as initial sets of seed words to be input to the
model.
3 Related Work
Seed-based supervision is closely related to the
idea of seeding in the bootstrapping literature for
learning semantic lexicons (Thelen and Riloff,
2002). The goals are similar as well: growing
a small set of seed examples into a much larger
set. A key difference is the type of semantic in-
formation that the two approaches aim to capture:
semantic lexicons are based on much more spe-
cific notions of semantics (e.g. all the country
names) than the generic ?topic? semantics of topic
models. The idea of seeding has also been used
in prototype-driven learning (Haghighi and Klein,
2006) and shown similar efficacies for these semi-
supervised learning approaches.
LDAWN (Boyd-Graber et al 2007) models
sets of words for the word sense disambiguation
208
task. It assumes that a topic is a distribution
over synsets and relies on the Wordnet to obtain
the synsets. The most related prior work is that
of (Andrzejewski et al 2009), who propose the
use Dirichlet Forest priors to incorporate Must
Link and Cannot Link constraints into the topic
models. This work is analogous to constrained
K-means clustering (Wagstaff et al 2001; Basu
et al 2008). A must link between a pair word
types represents that the model should encourage
both the words to have either high or low prob-
ability in any particular topic. A cannot link be-
tween a word pair indicates both the words should
not have high probability in a single topic. In the
Dirichlet Forest approach, the constraints are first
converted into trees with words as the leaves and
edges having pre-defined weights. All the trees
are joined to a dummy node to form a forest. The
sampling for a word translates into a random walk
on the forest: starting from the root and selecting
one of its children based on the edge weights until
you reach a leaf node.
While the Dirichlet Forest method requires su-
pervision in terms of Must link and Cannot link
information, the Topics In Sets (Andrzejewski and
Zhu, 2009) model proposes a different approach.
Here, the supervision is provided at the token
level. The user chooses specific tokens and re-
strict them to occur only with in a specified list of
topics. While this needs minimal changes to the
inference process of LDA, it requires information
at the level of tokens. The word type level seed
information can be converted into token level in-
formation (like we do in Sec. 4) but this prevents
their model from distinguishing the tokens based
on the word senses.
Several models have been proposed which use
supervision at the document level. Supervised
LDA (Blei and McAuliffe, 2008) and DiscLDA
(Lacoste-Julien et al 2008) try to predict the cat-
egory labels (e.g. sentiment classification) for
the input documents based on a document labeled
data. Of these models, the most related one to
SeededLDA is the LabeledLDA model (Ramage
et al 2009). Their model operates on multi-class
labeled corpus. Each document is assumed to be
a mixture over a known subset of topics (classes)
with each topic being a distribution over words.
The process of generating document topic distri-
bution in LabeledLDA is similar to the process
of generating group distribution in our Model 2
(Sec. 2.2). However our model differs from La-
beledLDA in the subsequent steps. Rather than
using the group distribution directly, we sam-
ple a group variable and use it to constrain the
document-topic distributions of all the documents
within this group. Moreover, in their model the
binary vector is observed directly in the form of
document labels while, in our case, it is automat-
ically populated based on the document tokens.
Interactive topic modeling brings the user into
the loop, by allowing him/her to make suggestions
on how to improve the quality of the topics at each
iteration (Hu et al 2011). In their approach, the
authors use Dirichlet Forest method to incorpo-
rate the user?s preferences. In our experiments
(Sec. 4), we show that SeededLDA performs bet-
ter than Dirichlet Forest method, so SeededLDA
when used with their framework can allow an user
to explore a document collection in a more mean-
ingful manner.
4 Experiments
We evaluate different aspects of the model sep-
arately. Our experimental setup proceeds as fol-
lows: a) Using an existing model, we evaluate the
effectiveness of automatically derived constraints
indicating the potential benefits of adding seed
words into the topic models. b) We evaluate each
of our proposed models in different settings and
compare with multiple baseline systems.
Since our aim is to overcome the domi-
nance of majority topics by encouraging the
topicality structure identified by the topic mod-
els to align with that of the document cor-
pus, we choose extrinsic evaluation as the
primary evaluation method. We use docu-
ment clustering task and use frequent-5 cate-
gories of Reuters-21578 corpus (Lewis et al
2004) and four classes from the 20 News-
groups data set (i.e.?rec.autos?, ?sci.electronics?,
?comp.hardware? and ?alt.atheism?). For both
the corpora we do the standard preprocessing
of removing stopwords and infrequent words
(Williamson et al 2010).
For all the models, we use a Collapsed Gibbs
sampler (Griffiths and Steyvers, 2004) for the in-
ference process. We use the standard hyperparam-
eters values ? = 1.0, ? = 0.01 and ? = 1.0 and
run the sampler for 1000 iterations, but one can
use techniques like slice sampling to estimate the
hyperparameters (Johnson and Goldwater, 2009).
209
Reuters 20 Newsgroups
F-measure VI F-measure VI
LDA 0.64 (?.05) 1.26 (?.16) 0.77 (?.06) 0.9 (?.13)
Dirichlet Forest 0.67? (?.02) 1.17 (?.11) 0.79(?.01) 0.83?(?.03)
? over LDA (+4.68%) (-7.1%) (+2.6%) (-7.8%)
Table 3: The effect of adding constraints by Dirichlet Forest Encoding. For Variational Information (VI) a lower
score indicates a better clustering. ? indicates statistical significance at p = 0.01 as measured by the t-test. All
the four improvements are significant at p = 0.05.
We run all the models with the same number of
topics as the number of clusters. Then, for each
document, we find the topic that has maximum
probability in the posterior document-topic distri-
bution and assign it to that cluster. The accuracy
of the document clustering is measured in terms
of F-measure and Variation of Information. F-
measure is calculated based on the pairs of doc-
uments, i.e. if two documents belong to a cluster
in both ground truth and the clustering proposed
by the system then it is counted as correct, other-
wise it is counted as wrong. Variational Informa-
tion (VI) of two clusterings X and Y is given as
(Meila?, 2007):
VI(X,Y ) = H(X) +H(Y ) ? 2I(X,Y )
whereH(X) denotes the entropy of the clustering
X and I(X,Y ) denotes the mutual information
between the two clusterings. For VI, a lower value
indicates a better clustering. All the accuracies are
averaged over 25 different random initializations
and all the significance results are measured using
the t-test at p = 0.01.
4.1 Seed Extraction
The seeds were extracted automatically (Sec. 2.4)
based on a small sample of labeled data other than
the test data. We first extract 25 seeds words per
each class and then remove the seed words that
appear in more than one class. After this filtering,
on an average, we are left with 9 and 15 words per
each seed topic for Reuters and 20 Newsgroups
corpora respectively.
We use the existing Dirichlet Forest method to
evaluate the effectiveness of the automatically ex-
tracted seed words. The Must and Cannot links
required for the supervision (Andrzejewski et al
2009) are automatically obtained by adding a
must-link between every pair of words belonging
to the same seed set and a split constraint between
every pair of words belonging to different sets.
The accuracies are averaged over 25 different ran-
dom initializations and are shown in Table 3. We
have also indicated the relative performance gains
compared to LDA. The significant improvement
over the plain LDA demonstrates the effectiveness
of the automatic extraction of seed words in topic
models.
4.2 Document Clustering
In the next experiment, we compare our models
with LDA and other baselines. The first baseline
(maxCluster) simply counts the number of tokens
in each document from each of the seed topics and
assigns the document to the seed topic that has
most tokens. This results in a clustering of doc-
uments based on the seed topic they are assigned
to. This baseline evaluates the effectiveness of the
seed words with respect to the underlying cluster-
ing. Apart from the maxCluster baseline, we use
LDA and z-labels (Andrzejewski and Zhu, 2009)
as our baselines. For z-labels, we treat all the to-
kens of a seed word in the same way. Table 4
shows the comparison of our models with respect
to the baseline systems.2 Comparing the perfor-
mance of maxCluster to that of LDA, we observe
that the seed words themselves do a poor job in
clustering the documents.
We experimented with two variants of Model 1.
In the first run (Model 1) we sample the ?k value,
i.e. the probability of choosing a seed topic for
each topic. While in the ?Model 1 (?? = 0.7)? run,
we fix this probability to a constant value of 0.7 ir-
respective of the topic.3 Though both the models
2The code used for LDA baseline in Tables 3 and 4
are different. For Table 3, we use the code available from
http://pages.cs.wisc.edu/?andrzeje/research/df lda.html.
We use our own version for Table 4. We tried to produce
a comparable baseline by running the former for more
iterations and with different hyperparameters. In Table 3,
we report their best results.
3We chose this value based on intuition; it is not tuned.
210
Reuters 20 Newsgroups
F-measure VI F-measure VI
maxCluster 0.53 1.75 0.58 1.44
LDA 0.66 (?.04) 1.2 (?.12) 0.76 (?.06) 0.9 (?.14)
z-labels 0.73 (?.01) 1.04 (?.01) 0.8 (?.00) 0.82 (?.01)
? over LDA (+10.6%) (-13.3%) (+5.26%) (-8.8%)
Model 1 0.69 (?.00) 1.13 (?.01) 0.8 (?.01) 0.81 (?.02)
Model 1 (?? = 0.7) 0.73 (?.00) 1.09 (?.01) 0.8 (?.01) 0.81 (?.02)
Model 2 0.66 (?.04) 1.22 (?.1) 0.77 (?.07) 0.85 (?.12)
SeededLDA 0.76? (?.01) 0.99? (?.03) 0.81? (?.01) 0.75? (?.02)
? over LDA (+15.5%) (-17.5%) (+6.58%) (-16.7%)
Table 4: Accuracies on document clustering task with different models. ? indicates significant improvement
compared to the z-labels approach, as measured by the t-test with p = 0.01. The relative performance gains are
with respect to the LDA model and are provided for comparison with Dirichlet Forest method (in Table 3.)
performed better than LDA, fixing the probabil-
ity gave better results. When we attempt to learn
this value, the model chooses to explain some of
the seed words by the regular topics. On the other
hand, when ? is fixed, it explains almost all the
seed words based on the seed topics. The next
row (Model 2) indicates the performance of our
second model on the same data sets. The first
model seems to be performing better than the sec-
ond model, which is justifiable since the latter
uses seed topics indirectly. Though the variants
of Model 1 and Model 2 performed better than
the LDA, they fell short of the z-labels approach.
Table 4 also shows the performance of our com-
bined model (SeededLDA) on both the corpora.
When the models are combined, the performance
improves over each of them and is also better than
the baseline systems. As explained before, our in-
dividual models improve both the topic-word and
document-topic distributions respectively. But it
turns out that the knowledge learnt by both the in-
dividual models is complementary to each other.
As a result the combined model performed better
than the individual models and other baseline sys-
tems. Comparing the last rows of Tables 4 and 3,
we notice that the relative performance gains ob-
served in the case of SeededLDA is significantly
higher than the performance gains obtained by
incorporating the constraints using the Dirichlet
Forest method. Moreover, as indicated in the Ta-
ble 4, SeededLDA achieves significant gains over
the z-labels approach as well.
We have also provided the standard intervals
for each of the approaches. A quick inspection of
these intervals reveals the superior performance
of SeededLDA compared to all the baselines. The
standard deviation of the F-measures over dif-
ferent random initializations of our our model is
about 1% for both the corpora while it is 4% and
6% for the LDA on Reuters and 20 Newsgroups
corpora respectively. The reduction in the vari-
ance, across all the approaches that use seed infor-
mation, shows the increased robustness of the in-
ference process when using seed words. From the
accuracies in both the tables, it is clear that Seed-
edLDA model out-performs other models which
try to incorporate seed information into the topic
models.
4.3 Effect of Ambiguous Seeds
In the following experiment we study the effect
of ambiguous seeds. We allow a seed word to oc-
cur in multiple seed sets. Table 6 shows the cor-
responding results. The performance drops when
we add ambiguous seed words, but it is still higher
than that of the LDA model. This suggests that the
quality of the seed topics is determined by the dis-
criminative power of the seed words rather than
the number of seed words in each seed topic. The
topics identified by the SeededLDA on Reuters
corpus are shown in the Table 5. With the help of
the seed sets, the model is able to split the ?Grain?
and ?Crude? into two separate topics which were
merged into a single topic by the plain LDA.
4.4 Qualitative Evaluation on NIPS papers
We ran LDA and SeededLDA models on the NIPS
papers from 2001 to 2010. For this corpus, the
seed words are chosen from the call for proposal.
211
group, offer, common, cash, agreement, shareholders, acquisition, stake, merger, board, sale
oil, price, prices, production, lt, gas, crude, 1987, 1985, bpd, opec, barrels, energy, first, petroleum
0, mln, cts, net, loss, 2, dlrs, shr, 3, profit, 4, 5, 6, revs, 7, 9, 8, year, note, 1986, 10, 0, sales
tonnes, wheat, mln, grain, week, corn, department, year, export, program, agriculture, 0, soviet, prices
bank, market, pct, dollar, exchange, billion, stg, today, foreign, rate, banks, japan, yen, rates, trade
Table 5: Topics identified by SeededLDA on the frequent-5 categories of Reuters corpus
Reuters 20 Newsgroups
F VI F VI
LDA 0.66 1.2 0.76 0.9
SeededLDA 0.76 0.99 0.81 0.75
SeededLDA 0.71 1.08 0.79 0.78(amb)
Table 6: Effect of ambiguous seed words on Seed-
edLDA.
There are 10 major areas with sub areas under
each of them. We ran both the models with 10 top-
ics. For SeededLDA, the words in each of the ar-
eas are selected as seed words and we filter out the
ambiguous seed words. Upon a qualitative obser-
vation of the output topics, we found that LDA has
identified seven major topics and left out ?Brain
Imaging?, ?Cognitive Science and Artificial In-
telligence? and ?Hardware Technologies? areas.
Not surprisingly, but reassuringly, these areas are
underrepresented among the NIPS papers. On the
other hand, SeededLDA successfully identifies all
of the major topics. The topics identified by LDA
and SeededLDA are shown in the supplementary
material.
5 Discussion
In traditional topic models, a symmetric Dirich-
let distribution is used as prior for topic-word dis-
tributions. A first attempt method to incorporate
seed words into the model is to use an asymmetric
Dirichlet distribution as prior for the topic-word
distributions (also called as Informed priors). For
example, to encourage Topic 5 to align with a seed
set we can choose an asymmetric prior of the form
~?5 = {?, ? ? ? , ? + c, ? ? ? , ?}, i.e. we increase
the component values corresponding to the seed
words by a positive constant value. This favors
the desired seed words to be drawn with a higher
probability from this topic. But, it is argued else-
where that words drawn from such distributions
rarely pick words other than the seed words (An-
drzejewski et al 2009). Moreover, since, in our
method each seed topic is a distribution over the
seed words, the convex combination of regular
and seed topics can be seen as adding different
weights (ci) to different components of the prior
vector. Thus our Model 1 can be seen as an asym-
metric generalization of the Informed priors.
For comparability purposes, in this paper, we
experimented with same number of regular topics
as the number of seed topics. But as explained in
the modeling part, our model is general enough
to handle situation with unequal number of seed
and regular topics. In this case, we assume that
the seed topics indicate a higher level of topical-
ity structure of the corpus and associate each seed
topic (or group) with a distribution over the regu-
lar topics. On the other hand, in many NLP appli-
cations, we tend to have only a partial information
rather than high-level supervision. In such cases,
one can create some empty seed sets and tweak
the model 2 to output a 1 in the binary vector cor-
responding to these seed sets. In this paper, we
used information gain to select the discriminating
seed words. But in the real world applications,
one can use publicly available ODP categorization
data to obtain the higher level seed words and thus
explore the corporal in a more meaningful way.
In this paper, we have explored two methods
to incorporate lexical prior into the topic mod-
els, combining them into a single model that we
call SeededLDA. From our experimental analysis,
we found that automatically derived seed words
can improve clustering performance significantly.
Moreover, we found out that allowing a seed word
to be shared across multiple sets of seed words de-
grades the performance.
6 Acknowledgments
We thank the anonymous reviewers for their help-
ful comments. This material is partially supported
by the National Science Foundation under Grant
No. IIS-1153487.
212
References
Andrzejewski, D. and Zhu, X. (2009). Latent dirichlet
allocation with topic-in-set knowledge. In Proceed-
ings of the NAACL HLT 2009 Workshop on Semi-
Supervised Learning for Natural Language Pro-
cessing, SemiSupLearn ?09, pages 43?48, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Andrzejewski, D., Zhu, X., and Craven, M. (2009). In-
corporating domain knowledge into topic modeling
via dirichlet forest priors. In ICML ?09: Proceed-
ings of the 26th Annual International Conference
on Machine Learning, pages 25?32, New York, NY,
USA. ACM.
Basu, S., Ian, D., and Wagstaff, K. (2008). Con-
strained Clustering : Advances in Algorithms, The-
ory, and Applications. Chapman & Hall/CRC Pres.
Blei, D. and McAuliffe, J. (2008). Supervised topic
models. In Advances in Neural Information Pro-
cessing Systems 20, pages 121?128, Cambridge,
MA. MIT Press.
Blei., D. M. and Lafferty., J. (2009). Topic models. In
Text Mining: Theory and Applications. Taylor and
Francis.
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). La-
tent dirichlet alcation. Journal of Maching Learn-
ing Research, 3:993?1022.
Boyd-Graber, J., Blei, D. M., and Zhu, X. (2007). A
topic model for word sense disambiguation. In Em-
pirical Methods in Natural Language Processing.
Chang, J., Boyd-Graber, J., Wang, C., Gerrish, S., and
Blei, D. M. (2009). Reading tea leaves: How hu-
mans interpret topic models. In Neural Information
Processing Systems.
Griffiths, T., Steyvers, M., and Tenenbaum, J. (2007).
Topics in semantic representation. Psychological
Review, 114(2):211?244.
Griffiths, T. L. and Steyvers, M. (2004). Finding sci-
entific topics. Proceedings of National Academy of
Sciences USA, 101 Suppl 1:5228?5235.
Griffiths, T. L., Steyvers, M., Blei, D. M., and Tenen-
baum, J. B. (2005). Integrating topics and syntax.
In Advances in Neural Information Processing Sys-
tems, volume 17, pages 537?544.
Haghighi, A. and Klein, D. (2006). Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguis-
tics, HLT-NAACL ?06, pages 320?327, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hu, Y., Boyd-Graber, J., and Satinoff, B. (2011). In-
teractive topic modeling. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
- Volume 1, HLT ?11, pages 248?257, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Johnson, M. and Goldwater, S. (2009). Improving
nonparameteric bayesian inference: experiments
on unsupervised word segmentation with adap-
tor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
317?325, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Lacoste-Julien, S., Sha, F., and Jordan, M. (2008).
DiscLDA: Discriminative learning for dimensional-
ity reduction and classification. In Proceedings of
NIPS ?08.
Lewis, D. D., Yang, Y., Rose, T. G., and Li, F. (2004).
Rcv1: A new benchmark collection for text catego-
rization research. J. Mach. Learn. Res., 5:361?397.
Meila?, M. (2007). Comparing clusterings?an infor-
mation based distance. J. Multivar. Anal., 98:873?
895.
Mitchell, T. M. (1997). Machine Learning. McGraw-
Hill, New York.
Paul, M. and Girju, R. (2010). A two-dimensional
topic-aspect model for discovering multi-faceted
topics. In AAAI.
Ramage, D., Hall, D., Nallapati, R., and Manning,
C. D. (2009). Labeled LDA: a supervised topic
model for credit attribution in multi-labeled cor-
pora. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 1 - Volume 1, EMNLP ?09, pages 248?
256, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Thelen, M. and Riloff, E. (2002). A bootstrapping
method for learning semantic lexicons using extrac-
tion pattern contexts. In In Proc. 2002 Conf. Empir-
ical Methods in NLP (EMNLP).
Wagstaff, K., Cardie, C., Rogers, S., and Schro?dl, S.
(2001). Constrained k-means clustering with back-
ground knowledge. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, ICML ?01, pages 577?584, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Wallach, H. M. (2005). Topic modeling: beyond bag-
of-words. In NIPS 2005 Workshop on Bayesian
Methods for Natural Language Processing.
Williamson, S., Wang, C., Heller, K. A., and Blei,
D. M. (2010). The IBP compound dirichlet pro-
cess and its application to focused topic modeling.
In ICML, pages 1151?1158.
213
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 492?500,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving the Multilingual User Experience of Wikipedia Using
Cross-Language Name Search
Raghavendra Udupa
Microsoft Research India
Bangalore, India.
Mitesh Khapra ?
Indian Institute of Technology Bombay
Powai, India.
Abstract
Although Wikipedia has emerged as a power-
ful collaborative Encyclopedia on the Web, it
is only partially multilingual as most of the
content is in English and a small number of
other languages. In real-life scenarios, non-
English users in general and ESL/EFL 1 users
in particular, have a need to search for rele-
vant English Wikipedia articles as no relevant
articles are available in their language. The
multilingual experience of such users can be
significantly improved if they could express
their information need in their native language
while searching for English Wikipedia arti-
cles. In this paper, we propose a novel cross-
language name search algorithm and employ
it for searching English Wikipedia articles in
a diverse set of languages including Hebrew,
Hindi, Russian, Kannada, Bangla and Tamil.
Our empirical study shows that the multilin-
gual experience of users is significantly im-
proved by our approach.
1 Introduction
Since its inception in 2001, Wikipedia has emerged
as the most famous free, web-based, collaborative,
and multilingual encyclopedia with over 13 million
articles in over 270 languages. However, Wikipedia
exhibits severe asymmetry in the distribution of its
content in the languages of the world with only a
small number of languages dominating (see Table
?This work was done when the author was a summer intern
at Microsoft Research India.
1English as Second Language and English as Foreign Lan-
guage.
1). As a consequence, most users of the under-
represented languages of the world have no choice
but to consult foreign language Wikipedia articles
for satisfying their information needs.
Table 1: Linguistic asymmetry of Wikipedia
Language Speakers Contributors Articles
English 1500M 47.1% 3,072,373
Russian 278M 5.2% 441,860
Hebrew 10M 0.7% 97,987
Hindi 550M 0.06% 50,926
Bangla 230M 0.02% 20,342
Tamil 66M 0.04% 19,472
Kannada 47M 0.02% 7,185
Although consulting foreign language Wikipedia
is not a solution for the problem of linguistic asym-
metry, in the specific case of ESL/EFL users who
form a sizable fraction of Internet users of the world
2
, it is arguably the most practical option today. Typ-
ically, ESL/EFL users are reasonably good at read-
ing and extracting relevant information from English
content but not so good at expressing their infor-
mation needs in English. In particular, getting the
spellings of foreign names in English correctly is
very difficult for most ESL/EFL users due to the dif-
ferences in the way a foreign name is pronounced
in the native languages. For instance, Japanese
EFL speakers often break consonant clusters in for-
eign names using vowels (see Table 2) and Hindi
ESL speakers find it difficult to differentiate between
?an?, ?en?, and ?on? in English names (such as ?Clin-
2As per some estimates, there are about 1 Billion ESL and
EFL speakers in the world today and their number is growing.
492
ton?) and will most likely use ?an? (?Clintan?).
Table 2: Influence of native language on the English
spelling of names.
Wikipedia
Entity Hindi Japanese Kannada
Stephen
Hawking
Stefan
Hoking
Suchifun
Houkingu
Steephan
Haakimg
Paul Krug-
man
Pol Crugmun PooruKuruguman
Paal Kraga-
man
Haroun
al-Rashid
Haroon
al-Rashid
Haruun
aru-Rasheedo
Haroon
al-Rasheed
Subrahmaniya
Bharati
Subramaniya
Bharati
Suburaamaniya
Bahaarachi
Subrahmanya
Bharathi
In principle, English spell-checkers (Ahmad and
Kondrak, 2005) can handle the problem of incor-
rect spellings in the queries formed by ESL/EFL
users. But in practice, there are two difficulties.
Firstly, most English spell-checkers do not have a
good coverage of names which form the bulk of user
queries. Secondly, spelling correction of names is
difficult because spelling mistakes are markedly in-
fluenced by the native language of the user. Not
surprisingly, Wikipedia?s inbuilt spell-checker sug-
gests ?Suchin Housing? as the only alternative to the
query ?Suchifun Houkingu? instead of the correct
entity ?Stephen Hawking? (See Table 3 for more ex-
amples).
The inability of ESL/EFL speakers to express
their information needs correctly in English and the
poor performance of spell-checkers highlight the
need for a practical solution for the linguistic asym-
metry problem of Wikipedia. In this work, we argue
the multilingual user experience of ESL/EFL users
can be significantly improved by allowing them to
express their information need in their native lan-
guage. While it might seem that we would need
a fully functional cross-language retrieval system
that supports translation of non-English queries to
English, we note that a good number of the pages
in Wikipedia are on people. This empirical fact
allows us to improve the multilingual experience
of ESL/EFL Wikipedia users by means of cross-
language name search which is less resource de-
manding than a fully functional cross-language re-
trieval system.
There are several challenges that need to be ad-
dressed in order to enable cross-language name
Table 3: Spelling suggestions by Wikipedia.
User Input
Wikipedia?s
Suggestion
Correct Spelling
Suchifun Houkingu Suchin Housing Stephen Hawking
Stefan Hoking Stefan Ho king Stephen Hawking
Pol Crugman Poll Krugman Paul Krugman
Paal Kragaman Paul Krugman Paul Krugman
Suburaamaniya Ba-
haarachi
Subramaniya
Baracchi
Subrahmaniya
Bharati
search in Wikipedia.
? Firstly, name queries are expressed by
ESL/EFL users in the native languages using
the orthography of those languages. Translit-
erating the name into Latin script using a
Machine Transliteration system is an option
but state-of-the-art Machine Transliteration
technologies are still far away from producing
the correct transliteration. Further, as pointed
out by (Udupa et al, 2009a), it is not enough
if a Machine Transliteration system generates
a correct transliteration; it must produce the
transliteration that is present in the Wikipedia
title.
? Secondly, there are about 6 million titles (in-
cluding redirects) in English Wikipedia which
rules out the naive approach of comparing the
query with every one of the English Wikipedia
titles for transliteration equivalence as is done
typically in transliteration mining tasks. A
practical cross-language name search system
for Wikipedia must be able to search millions
of Wikipedia titles in a fraction of a second and
return the most relevant titles.
? Thirdly, names are typically multi-word and
as a consequence there might not be an ex-
act match between the query and English
Wikipedia titles. Any cross-language name
search system for Wikipedia must be able
to deal with multi-word names and partial
matches effectively.
? Fourthly, the cross-language name search sys-
493
tem must be tolerant to spelling variations in
the query as well as the Wikipedia titles.
In this work, we propose a novel approach to
cross-language name search in Wikipedia that ad-
dresses all the challenges described above. Fur-
ther, our approach does not depend on either spell-
checkers or Machine Transliteration. Rather we
transform the problem into a geometric search prob-
lem and employ a state-of-the-art geometric algo-
rithm for searching a very large database of names.
This enables us to accurately search the relevant
Wikipedia titles for a given user query in a fraction
of a second even on a single processor.
1.1 Our Contributions
Our contributions can be summarized as follows:
1. We introduce a language and orthography in-
dependent geometric representation for single-
word names (Section 3.1).
2. We model the problem of learning the geo-
metric representation of names as a multi-view
learning problem and employ the machinery
of Canonical Correlation Analysis (CCA) to
compute a low-dimensional Euclidean feature
space. We map both foreign single-word names
and English single-word names to points in the
common feature space and the similarity be-
tween two single-word names is an exponen-
tially decaying function of the squared geomet-
ric distance between the corresponding points
(Section 3).
3. We model the problem of searching a database
of names as a geometric nearest neighbor prob-
lem in low-dimensional Euclidean space and
employ the well-known ANN algorithm for
approximate nearest neighbors to search for
the equivalent of a query name in the English
Wikipedia titles (Arya et al, 1998) (Section
3.3).
4. We introduce a simple and efficient algorithm
for computing the similarity scores of multi-
word names from the single-word similarity
scores (Section 3.4).
5. We show experimentally that our approach sig-
nificantly improves the multilingual experience
of ESL/EFL users (Section 4).
2 Related Work
Although approximate similarity search is well-
studied, we are not aware of any non-trivial cross-
language name search algorithm in the litera-
ture. However, several techniques for mining name
transliterations from monolingual and comparable
corpora have been studied (Pasternack and Roth,
2009), (Goldwasser and Roth, 2008), (Klementiev
and Roth, 2006), (Sproat et al, 2006), (Udupa et al,
2009b). These techniques employ various translit-
eration similarity models. Character unigrams and
bigrams were used as features to learn a discrimi-
native transliteration model and time series similar-
ity was combined with the transliteration similarity
model (Klementiev and Roth, 2006). A generative
transliteration model was proposed and used along
with cross-language information retrieval to mine
named entity transliterations from large comparable
corpora (Udupa et al, 2009b). However, none of
these transliteration similarity models are applicable
for searching very large name databases as they rely
on brute-force search. Not surprisingly, (Pasternack
and Roth, 2009) report that ?.. testing [727 single
word English names] with fifty thousand [Russian]
candidates is a large computational hurdle (it takes
our model about seven hours)?.
Several algorithms for string similarity search
have been proposed and applied to various problems
(Jin et al, 2005). None of them are directly applica-
ble to cross-language name search as they are based
on the assumption that the query string shares the
same alphabet as the database strings.
Machine Transliteration has been studied exten-
sively in the context of Machine Translation and
Cross-Language Information Retrieval (Knight and
Graehl, 1998), (Virga and Khudanpur, 2003), (Kuo
et al, 2006), (Sherif and Kondrak, 2007), (Ravi and
Knight, 2009), (Li et al, 2009), (Khapra and Bhat-
tacharyya, 2009). However, Machine Transliteration
followed by string similarity search gives less-than-
satisfactory solution for the cross-language name
search problem as we will see later in Section 4.
CCA was introduced by Hotelling in 1936 and has
494
been applied to various problems including CLIR,
Text Clustering, and Image Retrieval (Hardoon et
al., 2004). Recently, CCA has gained importance
in the Machine Learning community as a technique
for multi-view learning. CCA computes a common
semantic feature space for two-view data and al-
lows users to query a database using either of the
two views. CCA has been used in bilingual lexi-
con extraction from comparable corpora (Gaussier
et al, 2004) and monolingual corpora (Haghighi et
al., 2008).
Nearest neighbor search is a fundamental prob-
lem where challenge is to preprocess a set of points
in some metric space into a geometric data struc-
ture so that given a query point, its k-nearest neigh-
bors in the set can be reported as fast as possi-
ble. It has applications in many areas including pat-
tern recognition and classification, machine learn-
ing, data compression, data mining, document re-
trieval and statistics. The brute-force search algo-
rithm can find the nearest neighbors in running time
proportional to the product of the number of points
and the dimension of the metric space. When the di-
mension of the metric space is small, there exist al-
gorithms which give better running time than brute-
force search. However, the search time grows expo-
nentially with the dimension and none of the algo-
rithms do significantly better than brute-force search
for high-dimensional data. Fortunately, efficient al-
gorithms exist if instead of exact nearest neighbors,
we ask for approximate nearest neighbors (Arya et
al., 1998).
3 Cross-Language Name Search as a
Geometric Search Problem
The key idea behind our approach is the following:
if we can embed names as points (or equivalently
as vectors) in a suitable geometric space, then the
problem of searching a very large database of names
can be casted as a geometric search problem, i.e. one
of finding the nearest neighbors of the query point in
the database.
As illustrative examples, consider the names
Stephen and Steven. A simple geometric represen-
tation for these names is the one induced by their
corresponding features: {St, te, ep, ph, he, en} and
{St, te, ev, ve, en} 3. In this representation, each
character bigram constitutes a dimension of the geo-
metric feature space whose coordinate value is the
number of times the bigram appears in the name.
It is possible to find a low-dimensional representa-
tion for the names by using Principal Components
Analysis or any other dimensionality reduction tech-
nique on the bigram feature vectors. However, the
key point to note is that once we have an appropri-
ate geometric representation for names, the similar-
ity between two names can be computed as
Kmono (name1, name2) = e?||?1??2||
2/22 (1)
where ?1 and ?2 are the feature vectors of the two
names and  is a constant. Armed with the geomet-
ric similarity measure, we can leverage geometric
search techniques for finding names similar to the
query.
In the case of cross-language name search, we
need a feature representation of names that is lan-
guage/script independent. Once we map names in
different languages/scripts to the same feature space,
we can essentially treat similarity search as a geo-
metric search problem.
3.1 Language/Script Independent Geometric
Representation of Names
To obtain language/script independent geometric
representation of names, we start by forming the lan-
guage/script specific feature vectors as described in
Section 3. Given two names, Stephen in Latin script
and -VFPn in Devanagari script, we form the corre-
sponding character bigram feature vectors ? (using
features {St, te, ep, ph, en}) and ? (using features
{-V, VF, FP, Pn}) respectively. We then map these
vectors to a common geometric feature space using
two linear transformations A and B:
?? AT? = ?s ? Rd (2)
? ? BT? = ?s ? Rd (3)
The vectors ?s and ?s can be viewed as lan-
guage/script independent representation of the
names Stephen and -VFPn.
3Here, we have employed character bigrams as features. In
principle, we can use any suitable set of features including pho-
netic features extracted from the strings.
495
3.1.1 Cross-Language Similarity of Names
In order to search a database of names in English
when the query is in a native language, say Hindi, we
need to be able to measure the similarity of a name in
Devangari script with names in Latin script. The lan-
guage/script independent representation gives a nat-
ural way to measure the similarity of names across
languages. By embedding the language/script spe-
cific feature vectors ? and ? in a common feature
space via the projections A and B, we can com-
pute the similarity of the corresponding names as
follows:
Kcross (name1, name2) = e?||?s??s||
2/22 (4)
It is easy to see from Equation 4 that the similarity
score of two names is small when the projections of
the names are negatively correlated.
3.2 Learning Common Feature Space using
CCA
Ideally, the transformations A and B should be such
that similar names in the two languages are mapped
to close-by points in the common geometric fea-
ture space. It is possible to learn such transforma-
tions from a training set of name transliterations in
the two languages using the well-known multi-view
learning framework of Canonical Correlation Anal-
ysis (Hardoon et al, 2004). By viewing the lan-
guage/script specific feature vectors as two represen-
tations/views of the same semantic object, the entity
whose name is written as Stephen in English and as
-VFPn in Hindi, we can employ the machinery of
CCA to find the transformations A and B.
Given a sample of multivariate data with two
views, CCA finds a linear transformation for each
view such that the correlation between the projec-
tions of the two views is maximized. Consider
a sample Z = {(xi, yi)}Ni=1 of multivariate data
where xi ? Rm and yi ? Rn are two views of the
object. Let X = {xi}Ni=1 and Y = {yi}Ni=1. As-
sume thatX and Y are centered4, i.e., they have zero
mean. Let a and b be two directions. We can project
X onto the direction a to get U = {ui}Ni=1 where
ui = aTxi. Similarly, we can project Y onto the di-
rection b to get the projections V = {vi}ni=1 where
4If X and Y are not centered, they can be centered by sub-
tracting the respective means.
vi = bT yi. The aim of CCA is to find a pair of di-
rections (a, b) such that the projections U and V are
maximally correlated. This is achieved by solving
the following optimization problem:
? = max(a,b)
< Xa,Xb >
||Xa||||Xb||
= max(a,b)
aTXY T b?
aTXXT a
?
bTY Y T b
The objective function of Equation 5 can be max-
imized by solving the following generalized eigen
value problem (Hardoon et al, 2004):
XY T
(
Y Y T
)?1
Y XTa = ?2XXTa
(
Y Y T
)?1
Y XTa = ?b
The subsequent basis vectors can be found
by adding the orthogonality of bases con-
straint to the objective function. Although
the number of basis vectors can be as high as
min{Rank(X), Rank(Y )}, in practice, only the
first few basis vectors are used since the correlation
of the projections is high for these vectors and small
for the remaining vectors.
Let A and B be the first d > 0 basis vectors com-
puted by CCA.
Figure 1: Projected names (English-Hindi).
3.2.1 Common Geometric Feature Space
As described in Section 3.1, we represent names
as points in the common geometric feature space de-
fined by the projection matrices A and B. Figure 1
496
shows a 2-dimensional common feature space com-
puted by CCA for English (Latin script) and Hindi
(Devanagari script) names. As can be seen from the
figure, names that are transliterations of each other
are mapped to near-by points in the common feature
space.
Figure 2 shows a 2-dimensional common feature
space for English (Latin script) and Russian (Cyrillic
script) names. As can be seen from the figure, names
that are transliterations of each other are mapped to
near-by points in the common feature space.
Figure 2: Projected names (English-Russian).
3.3 Querying the Name Database
Given a database D = {ei}Mi=1 of single-word
names in English, we first compute their lan-
guage/script specific feature vectors ?(i), i =
1, . . . ,M . We then compute the projections ?(i)s =
AT?(i). Thus, we transform the name database D
into a set of vectors {?(1)s , . . . , ?(M)s } in Rd.
Given a query name h in Hindi, we compute its
language/script specific feature vector ? and project
it on to the common feature space to get ?s =
BT? ? Rd. Names similar to h in the database D
can be found as solutions of the k-nearest neighbor
problem:
eik = argmaxei?D?{eij }k?1j=1 Kcross (ei, h)
= argmaxei?D?{eij }k?1j=1 e
?||?(i)s ??s||2/22
= argminei?D?{eij }k?1j=1 ||?
(i)
s ? ?s||
Unfortunately, computing exact k-nearest neigh-
bors in dimensions much higher than 8 is difficult
and the best-known methods are only marginally
better than brute-force search (Arya et al, 1998).
Fortunately, there exist very efficient algorithms for
computing approximate nearest neighbors and in
practice they do nearly as well as the exact near-
est neighbors algorithms (Arya et al, 1998). It is
also possible to control the tradeoff between accu-
racy and running time by specifiying a maximum
approximation error bound. We employ the well-
known Approximate Nearest Neighbors (aka ANN)
algorithm by Arya and Mount which is known to do
well in practice when d ? 100 (Arya et al, 1998).
3.4 Combining Single-Word Similarities
The approach described in the previous sections
works only for single-word names. We need to com-
bine the similarities at the level of individual words
into a similarity function for multi-word names. To-
wards this end, we form a weighted bipartite graph
from the two multi-word names as follows:
We first tokenize the Hindi query name into sin-
gle word tokens and find the nearest English neigh-
bors for each of these Hindi tokens using the method
outlined section 3.3. We then find out all the En-
glish Words which contain one or more of the En-
glish neighbors thus fetched. Let E = e1e2 . . . eI
be one such multi-word English name and H =
h1h2 . . . hJ be the multi-word Hindi query. We form
a weighted bipartite graph G = (S ? T,W ) with a
node si for the ith word ei in E and node tj for the
jth word hj in H . The weight of the edge (si, tj) is
set as wij = Kcross (ei, hj).
Let w be the weight of the maximum weighted
bipartite matching in the graph G. We define the
similarity between E and H as follows:
Kcross (E,H) =
w
|I ? J |+ 1 . (5)
The numerator of the right hand side of Equation
5 favors name pairs which have a good number of
high quality matches at the individual word level
whereas the denominator penalizes pairs that have
disproportionate lengths.
Note that, in practice, both I and J are small and
hence we can find the maximum weighted bipartite
matching very easily. Further, most edge weights in
497
Figure 3: Combining Single-Word Similarities.
the bipartite graph are negligibly small. Therefore,
even a greedy matching algorithm suffices in prac-
tice.
4 Experiments and Results
In the remainder of this section, we refer to our sys-
tem by GEOM-SEARCH.
4.1 Experimental Setup
We tested our cross language name search system
using six native languages, viz., Russian, Hebrew,
Hindi, Kannada, Tamil and Bangla. For each of
these languages, we created a test set consisting of
1000 multi-word name queries and found manually
the most relevant Wikipedia article for each query in
the test set. The Wikipedia articles thus found and
all the redirect titles that linked to them formed the
gold standard for evaluating the performance of our
system.
In order to compare the performance of GEOM-
SEARCH with a reasonable baseline, we imple-
mented the following baseline: We used a state-of-
the art Machine Transliteration system to generate
the best transliteration of each of the queries. We
used the edit distance between the transliteration and
the single-word English name as the similarity score.
We combined single word similarities using the ap-
proach described in Section 3.4. We refer to this
baseline by TRANS-SEARCH.
Note that several English Wikipedia names some-
times get the same score for a query. Therefore,
we used a tie-aware mean-reciprocal rank measure
to evaluate the performance (McSherry and Najork,
2008).
4.2 GEOM-SEARCH
The training and search procedure employed by
GEOM-SEARCH are described below.
4.2.1 CCA Training
We learnt the linear transformations A and B that
project the language/script specific feature vectors to
the common feature space using the approach dis-
cussed in Section 3.2. The learning algorithm re-
quires a training set consisting of pairs of single-
word names in English and the respective native lan-
guage. We used approximately 15, 000 name pairs
for each native language.
A key parameter in CCA training is the number of
dimensions of the common feature space. We found
the optimal number of dimensions using a tuning set
consisting of 1, 000 correct name pairs and 1, 000
incorrect name pairs for each native language. We
found that d = 50 is a very good choice for each
native language.
Another key aspect of training is the choice of
language/script specific features. For the six lan-
guages we experimented with and also for English,
we found that character bigrams formed a good set
of features. We note that for languages such as Chi-
nese, Japanese, and Korean, unigrams are the best
choice. Also, for these languages, it may help to
syllabify the English name.
4.2.2 Search
As a pre-processing step, we extracted a list of 1.3
million unique words from the Wikipedia titles. We
computed the language/script specific feature vector
for each word in this list and projected the vector to
the common feature space as described in Section
3.1. The low-dimensional embeddings thus com-
puted formed the input to the ANN algorithm.
We tokenized each query in the native language
into constituent words. For each constituent, we first
computed the language/script specific feature vector,
projected it to the common feature space, and found
the k-nearest neighbors using the ANN algorithm.
We used k=100 for all our experiments.
After finding the nearest neighbors and the corre-
sponding similarity scores, we combined the scores
using the approach described in Section 3.4.
4.3 TRANS-SEARCH
The training and search procedure employed by
TRANS-SEARCH are described below.
498
Figure 4: Top scoring English Wikipedia page retrieved by GEOM-SEARCH
4.3.1 Transliteration Training
We used a state-of-the-art CRF-based translitera-
tion technique for transliterating the native language
names (Khapra and Bhattacharyya, 2009). We used
CRF++, an open-source CRF training tool, to train
the transliteration system. We used exactly the
same features and parameter settings as described in
(Khapra and Bhattacharyya, 2009). As in the case of
CCA, we use around 15, 000 single word name pairs
in the training.
4.3.2 Search
The preprocessing step for TRANS-SEARCH is
the same as that for GEOM-SEARCH. We translit-
erated each constituent of the query into English and
find all single-word English names that are at an edit
distance of at most 3. We computed the similarity
score as described in Section 3.4.
4.4 Evaluation
We evaluated the performance of GEOM-SEARCH
and TRANS-SEARCH using a tie-aware mean re-
ciprocal rank (MRR). Table 4 compares the average
time per query and the MRR of the two systems.
GEOM-SEARCH performed significantly better
than the transliteration based baseline system for all
the six languages. On an average, the relevant En-
glish Wikipedia page was found in the top 2 re-
sults produced by GEOM-SEARCH for all the six
native languages. Clearly, this shows that GEOM-
SEARCH is highly effective as a cross-langauge
name search system. The good results also validate
our claim that cross-language name search can im-
Table 4: MRR and average time per query (in seconds)
for the two systems.
Language GEOM TRANS
Time MRR Time MRR
Hin 0.51 0.686 2.39 0.485
Tam 0.23 0.494 2.16 0.291
Kan 1.08 0.689 2.17 0.522
Ben 1.30 0.495 ? ?
Rus 0.15 0.563 1.65 0.476
Heb 0.65 0.723 ? ?
prove the multi-lingual user experience of ESL/EFL
users.
5 Conclusions
GEOM-SEARCH, a geometry-based cross-
language name search system for Wikipedia,
improves the multilingual experience of ESL/EFL
users of Wikipedia by allowing them to formulate
queries in their native languages. Further, it is easy
to integrate a Machine Translation system with
GEOM-SEARCH. Such a system would find the
relevant English Wikipedia page for a query using
GEOM-SEARCH and then translate the relevant
Wikipedia pages to the native language using the
Machine Translation system.
6 Acknowledgement
We thank Jagadeesh Jagarlamudi and Shaishav Ku-
mar for their help.
499
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. In
HLT ?05: Proceedings of the conference on Human
Language Technology and Empirical Methods in Nat-
ural Language Processing, pages 955?962, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth
Silverman, and Angela Y. Wu. 1998. An optimal
algorithm for approximate nearest neighbor searching
fixed dimensions. J. ACM, 45(6):891?923.
?Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herve? De?jean. 2004. A geometric
view on bilingual lexicon extraction from comparable
corpora. In ACL, pages 526?533.
Dan Goldwasser and Dan Roth. 2008. Transliteration as
constrained optimization. In EMNLP, pages 353?362.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
David R. Hardoon, Sa?ndor Szedma?k, and John Shawe-
Taylor. 2004. Canonical correlation analysis: An
overview with application to learning methods. Neu-
ral Computation, 16(12):2639?2664.
Liang Jin, Nick Koudas, Chen Li, and Anthony K. H.
Tung. 2005. Indexing mixed types for approximate
retrieval. In VLDB, pages 793?804.
Mitesh Khapra and Pushpak Bhattacharyya. 2009. Im-
proving transliteration accuracy using word-origin de-
tection and lexicon lookup. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009). Association for Computational
Linguistics.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilingual
comparable corpora. In HLT-NAACL.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Jin-Shea Kuo, Haizhou Li, and Ying-Kuei Yang. 2006.
Learning transliteration lexicons from the web. In
ACL.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML, pages 282?289.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report of news 2009 machine
transliteration shared task. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009). Association for Computational
Linguistics.
Frank McSherry and Marc Najork. 2008. Computing
information retrieval performance measures efficiently
in the presence of tied scores. In ECIR, pages 414?
421.
Jeff Pasternack and Dan Roth. 2009. Learning better
transliterations. In CIKM, pages 177?186.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
NAACL-HLT.
Hanan Samet. 2006. Foundations of Multidimensional
and Metric Data Structures (The Morgan Kaufmann
Series in Computer Graphics). Morgan Kaufmann,
August.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In ACL.
Richard Sproat, Tao Tao, and ChengXiang Zhai. 2006.
Named entity transliteration with comparable corpora.
In ACL.
Raghavendra Udupa, K. Saravanan, Anton Bakalov, and
Abhijit Bhole. 2009a. ?they are out there, if you
know where to look?: Mining transliterations of oov
query terms for cross-language information retrieval.
In ECIR, pages 437?448.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-
gadeesh Jagarlamudi. 2009b. Mint: A method for ef-
fective and scalable mining of named entity transliter-
ations from large comparable corpora. In EACL, pages
799?807.
Paola Virga and Sanjeev Khudanpur. 2003. Transliter-
ation of proper names in cross-language applications.
In SIGIR, pages 365?366.
500
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 147?152,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
From Bilingual Dictionaries to Interlingual Document Representations
Jagadeesh Jagarlamudi
University of Maryland
College Park, USA
jags@umiacs.umd.edu
Hal Daume? III
University of Maryland
College Park, USA
hal@umiacs.umd.edu
Raghavendra Udupa
Microsoft Research India
Bangalore, India
raghavu@microsoft.com
Abstract
Mapping documents into an interlingual rep-
resentation can help bridge the language bar-
rier of a cross-lingual corpus. Previous ap-
proaches use aligned documents as training
data to learn an interlingual representation,
making them sensitive to the domain of the
training data. In this paper, we learn an in-
terlingual representation in an unsupervised
manner using only a bilingual dictionary. We
first use the bilingual dictionary to find candi-
date document alignments and then use them
to find an interlingual representation. Since
the candidate alignments are noisy, we de-
velop a robust learning algorithm to learn
the interlingual representation. We show that
bilingual dictionaries generalize to different
domains better: our approach gives better per-
formance than either a word by word transla-
tion method or Canonical Correlation Analy-
sis (CCA) trained on a different domain.
1 Introduction
The growth of text corpora in different languages
poses an inherent problem of aligning documents
across languages. Obtaining an explicit alignment,
or a different way of bridging the language barrier,
is an important step in many natural language pro-
cessing (NLP) applications such as: document re-
trieval (Gale and Church, 1991; Rapp, 1999; Balles-
teros and Croft, 1996; Munteanu and Marcu, 2005;
Vu et al, 2009), Transliteration Mining (Klementiev
and Roth, 2006; Hermjakob et al, 2008; Udupa et
al., 2009; Ravi and Knight, 2009) and Multilingual
Web Search (Gao et al, 2008; Gao et al, 2009).
Aligning documents from different languages arises
in all the above mentioned problems. In this pa-
per, we address this problem by mapping documents
into a common subspace (interlingual representa-
tion)1. This common subspace generalizes the no-
tion of vector space model for cross-lingual applica-
tions (Turney and Pantel, 2010).
There are two major approaches for solving the
document alignment problem, depending on the
available resources. The first approach, which
is widely used in the Cross-lingual Information
Retrieval (CLIR) literature, uses bilingual dictio-
naries to translate documents from one language
(source) into another (target) language (Ballesteros
and Croft, 1996; Pirkola et al, 2001). Then stan-
dard measures such as cosine similarity are used to
identify target language documents that are close to
the translated document. The second approach is to
use training data of aligned document pairs to find a
common subspace such that the aligned document
pairs are maximally correlated (Susan T. Dumais,
1996; Vinokourov et al, 2003; Mimno et al, 2009;
Platt et al, 2010; Haghighi et al, 2008) .
Both kinds of approaches have their own strengths
and weaknesses. Dictionary based approaches treat
source documents independently, i.e., each source
language document is translated independently of
other documents. Moreover, after translation, the re-
lationship of a given source document with the rest
of the source documents is ignored. On the other
hand, supervised approaches use all the source and
target language documents to infer an interlingual
1We use the phrases ?common subspace? and ?interlingual
representation? interchangeably.
147
representation, but their strong dependency on the
training data prevents them from generalizing well
to test documents from a different domain.
In this paper, we propose a technique that com-
bines the advantages of both these approaches. At a
broad level, our approach uses bilingual dictionaries
to identify initial noisy document alignments (Sec.
2.1) and then uses these noisy alignments as train-
ing data to learn a common subspace. Since the
alignments are noisy, we need a learning algorithm
that is robust to the errors in the training data. It is
known that techniques like CCA overfit the training
data (Rai and Daume? III, 2009). So, we start with an
unsupervised approach such as Kernelized Sorting
(Quadrianto et al, 2009) and develop a supervised
variant of it (Sec. 2.2). Our supervised variant learns
to modify the within language document similarities
according to the given alignments. Since the origi-
nal algorithm is unsupervised, we hope that its su-
pervised variant is tolerant to errors in the candidate
alignments. The primary advantage of our method is
that, it does not use any training data and thus gen-
eralizes to test documents from different domains.
And unlike the dictionary based approaches, we use
all the documents in computing the common sub-
space and thus achieve better accuracies compared
to the approaches which translate documents in iso-
lation.
There are two main contributions of this work.
First, we propose a discriminative technique to learn
an interlingual representation using only a bilingual
dictionary. Second, we develop a supervised variant
of Kernelized Sorting algorithm (Quadrianto et al,
2009) which learns to modify within language doc-
ument similarities according to a given alignment.
2 Approach
Given a cross-lingual corpus, with an underlying un-
known document alignment, we propose a technique
to recover the hidden alignment. This is achieved
by mapping documents into an interlingual repre-
sentation. Our approach involves two stages. In the
first stage, we use a bilingual dictionary to find ini-
tial candidate noisy document alignments. The sec-
ond stage uses a robust learning algorithm to learn a
common subspace from the noisy alignments iden-
tified in the first step. Subsequently, we project all
the documents into the common subspace and use
maximal matching to recover the hidden alignment.
During this stage, we also learn mappings from the
document spaces onto the common subspace. These
mappings can be used to convert any new document
into the interlingual representation. We describe
each of these two steps in detail in the following two
sub sections (Sec. 2.1 and Sec. 2.2).
2.1 Noisy Document Alignments
Translating documents from one language into an-
other language and finding the nearest neighbours
gives potential alignments. Unfortunately, the re-
sulting alignments may differ depending on the di-
rection of the translation owing to the asymmetry
of bilingual dictionaries and the nearest neighbour
property. In order to overcome this asymmetry, we
first turn the documents in both languages into bag
of translation pairs representation.
We follow the feature representation used in Ja-
garlamudi and Daume? III (2010) and Boyd-Graber
and Blei (2009). Each translation pair of the bilin-
gual dictionary (also referred as a dictionary en-
try) is treated as a new feature. Given a docu-
ment, every word is replaced with the set of bilin-
gual dictionary entries that it participates in. If
D represents the TFIDF weighted term ? docu-
ment matrix and T is a binary matrix matrix of size
no of dictionary entries? vocab size, then convert-
ing documents into a bag of dictionary entries is
given by the linear operation X(t) ? TD.2
After converting the documents into bag of dic-
tionary entries representation, we form a bipartite
graph with the documents of each language as a
separate set of nodes. The edge weight Wij be-
tween a pair of documents x(t)i and y
(t)
j (in source
and target language respectively) is computed as the
Euclidean distance between those documents in the
dictionary space. Let piij indicate the likeliness of
a source document x(t)i is aligned to a target doc-
ument y(t)j . We want each document to align to at
least one document from other language. Moreover,
we want to encourage similar documents to align
to each other. We can formulate this objective and
the constraints as the following minimum cost flow
2Superscript (t) indicates that the data is in the form of bag
of dictionary entries
148
problem (Ravindra et al, 1993):
argmin
pi
m,n
?
i,j=1
Wijpiij (1)
?i
?
j
piij = 1 ; ?j
?
i
piij = 1
?i, j 0 ? piij ? C
where C is some user chosen constant, m and n
are the number of documents in source and target
languages respectively. Without the last constraint
(piij ? C) this optimization problem always gives an
integral solution and reduces to a maximum match-
ing problem (Jonker and Volgenant, 1987). Since
this solution may not be accurate, we allow many-to-
many mapping by setting the constant C to a value
less than one. In our experiments (Sec. 3), we
found that setting C to a value less than 1 gave bet-
ter performance analogous to the better performance
of soft Expectation Maximization (EM) compared
to hard-EM. The optimal solution of Eq. 1 can be
found efficiently using linear programming (Ravin-
dra et al, 1993).
2.2 Supervised Kernelized Sorting
Kernelized Sorting is an unsupervised technique to
align objects of different types, such as English and
Spanish documents (Quadrianto et al, 2009; Ja-
garalmudi et al, 2010). The main advantage of this
method is that it only uses the intra-language doc-
ument similarities to identify the alignments across
languages. In this section, we describe a supervised
variant of Kernelized Sorting which takes a set of
candidate alignments and learns to modify the intra-
language document similarities to respect the given
alignment. Since Kernelized Sorting does not rely
on the inter-lingual document similarities at all, we
hope that its supervised version is robust to noisy
alignments.
Let X and Y be the TFIDF weighted term ?
document matrices in both the languages and let
Kx and Ky be their linear dot product kernel ma-
trices, i.e. , Kx = XTX and Ky = Y TY .
Let ? ? {0, 1}m?n denote the permutation matrix
which captures the alignment between documents of
different languages, i.e. piij = 1 indicates docu-
ments xi and yj are aligned. Then Kernelized Sort-
ing formulates ? as the solution of the following op-
timization problem (Gretton et al, 2005):
argmax
?
tr(Kx?Ky?T ) (2)
= argmax
?
tr(XTX ? Y TY ?T ) (3)
In our supervised version of Kernelized Sorting,
we fix the permutation matrix (to say ??) and mod-
ify the kernel matrices Kx and Ky so that the ob-
jective function is maximized for the given permu-
tation. Specifically, we find a mapping for each lan-
guage, such that when the documents are projected
into their common subspaces they are more likely to
respect the alignment given by ??. Subsequently, the
test documents are also projected into the common
subspace and we return the nearest neighbors as the
aligned pairs.
Let U and V be the mappings for the required sub-
space in both the languages, then we want to solve
the following optimization problem:
argmax
U,V
tr(XTUUTX ?? Y TV V TY ??T )
s.t. UTU = I & V TV = I (4)
where I is an identity matrix of appropriate size. For
brevity, let Cxy denote the cross-covariance matrix
(i.e. Cxy = X??Y T ) then the above objective func-
tion becomes:
argmax
U,V
tr(UUTCxyV V TCTxy)
s.t. UTU = I & V TV = I (5)
We have used the cyclic property of the trace func-
tion while rewriting Eq. 4 to Eq. 5. We use alterna-
tive maximization to solve for the unknowns. Fixing
V (to say V0), rewriting the objective function using
the cyclic property of the trace function, forming the
Lagrangian and setting its derivative to zero results
in the following solution:
CxyV0V T0 CTxy U = ?u U (6)
For the initial iteration, we can substitute V0V T0 as
identity matrix which leaves the kernel matrix un-
changed. Similarly, fixing U (to U0) and solving the
optimization problem for V results:
CTxyU0UT0 Cxy V = ?v V (7)
149
In the special case where both V0V T0 and U0UT0
are identity matrices, the above equations reduce to
CxyCTxy U = ?u U and CTxyCxy V = ?v V . In
this particular case, we can simultaneously solve for
both U and V using Singular Value Decomposition
(SVD) as:
USV T = Cxy (8)
So for the first iteration, we do the SVD of the cross-
covariance matrix and get the mappings. For the
subsequent iterations, we use the mappings found by
the previous iteration, as U0 and V0, and solve Eqs.
6 and 7 alternatively.
2.3 Summary
In this section, we describe our procedure to recover
document alignments. We first convert documents
into bag of dictionary entries representation (Sec.
2.1). Then we solve the optimization problem in Eq.
1 to get the initial candidate alignments. We use the
LEMON3 graph library to solve the min-cost flow
problem. This step gives us the piij values for every
cross-lingual document pair. We use them to form
a relaxed permutation matrix (??) which is, subse-
quently, used to find the mappings (U and V ) for
the documents of both the languages (i.e. solv-
ing Eq. 8). We use these mappings to project both
source and target language documents into the com-
mon subspace and then solve the bipartite matching
problem to recover the alignment.
3 Experiments
For evaluation, we choose 2500 aligned docu-
ment pairs from Wikipedia in English-Spanish and
English-German language pairs. For both the data
sets, we consider only words that occurred more
than once in at least five documents. Of the words
that meet the frequency criterion, we choose the
most frequent 2000 words for English-Spanish data
set. But, because of the compound word phe-
nomenon of German, we retain all the frequent
words for English-German data set. Subsequently
we convert the documents into TFIDF weighted vec-
tors. The bilingual dictionaries for both the lan-
guage pairs are generated by running Giza++ (Och
and Ney, 2003) on the Europarl data (Koehn, 2005).
3https://lemon.cs.elte.hu/trac/lemon
En ? Es En ? De
Word-by-Word 0.597 0.564
CCA (? = 0.3) 0.627 0.485
CCA (? = 0.5) 0.628 0.486
CCA (? = 0.8) 0.637 0.487
OPCA 0.688 0.530
Ours (C = 0.6) 0.67 0.604
Ours (C = 1.0) 0.658 0.590
Table 1: Accuracy of different approaches on the
Wikipedia documents in English-Spanish and English-
German language pairs. For CCA, we regularize the
within language covariance matrices as (1??)XXT+?I
and the regularization parameter ? value is also shown.
We follow the process described in Sec. 2.3 to re-
cover the document alignment for our method.
We compare our approach with a dictionary based
approach, such as word-by-word translation, and
supervised approaches, such as CCA (Vinokourov
et al, 2003; Hotelling, 1936) and OPCA (Platt
et al, 2010). Word-by-word translation and our
approach use bilingual dictionary while CCA and
OPCA use a training corpus of aligned documents.
Since the bilingual dictionary is learnt from Eu-
roparl data set, for a fair comparison, we train su-
pervised approaches on 3000 document pairs from
Europarl data set. To prevent CCA from overfitting
to the training domain, we regularize it heavily. For
OPCA, we use a regularization parameter of 0.1 as
suggested by Platt et al (2010). For all the systems,
we construct a bipartite graph between the docu-
ments of different languages, with edge weight be-
ing the cross-lingual similarity given by the respec-
tive method and then find maximal matching (Jonker
and Volgenant, 1987). We report the accuracy of the
recovered alignment.
Table 1 shows accuracies of different methods on
both Spanish and German data sets. For comparison
purposes, we trained and tested CCA on documents
from same domain (Wikipedia). It achieves 75% and
62% accuracies for the two data sets respectively
but, as expected, it performed poorly when trained
on Europarl articles. On the English-German data
set, a simple word-by-word translation performed
better than CCA and OPCA. For both the language
pairs, our model performed better than word-by-
word translation method and competitively with the
150
supervised approaches. Note that our method does
not use any training data.
We also experimented with few values of the pa-
rameter C for the min-cost flow problem (Eq. 1).
As noted previously, setting C = 1 will reduce the
problem into a linear assignment problem. From
the results, we see that solving a relaxed version of
the problem gives better accuracies but the improve-
ments are marginal (especially for English-German).
4 Discussion
For both language pairs, the accuracy of the first
stage of our approach (Sec. 2.1) is almost same as
that of word-by-word translation system. Thus, the
improved performance of our system compared to
word-by-word translation shows the effectiveness of
the supervised Kernelized sorting.
The solution of our supervised Kernelized sorting
(Eq. 8) resembles Latent Semantic Indexing (Deer-
wester, 1988). Except, we use a cross-covariance
matrix instead of a term ? document matrix. Effi-
cient algorithms exist for solving SVD on arbitrarily
large matrices, which makes our approach scalable
to large data sets (Warmuth and Kuzmin, 2006). Af-
ter solving Eq. 8, the mappings U and V can be
improved by iteratively solving the Eqs. 6 and 7 re-
spectively. But it leads the mappings to fit the noisy
alignments exactly, so in this paper we stop after
solving the SVD problem.
The extension of our approach to the situation
with different number of documents on each side is
straight forward. The only thing that changes is the
way we compute alignment after finding the projec-
tion directions. In this case, the input to the bipar-
tite matching problem is modified by adding dummy
documents to the language that has fewer documents
and assigning a very high score to edges that connect
to the dummy documents.
5 Conclusion
In this paper we have presented an approach to re-
cover document alignments from a comparable cor-
pora using a bilingual dictionary. First, we use the
bilingual dictionary to find a set of candidate noisy
alignments. These noisy alignments are then fed into
supervised Kernelized Sorting, which learns to mod-
ify within language document similarities to respect
the given alignments.
Our approach exploits two complimentary infor-
mation sources to recover a better alignment. The
first step uses cross-lingual cues available in the
form of a bilingual dictionary and the latter step
exploits document structure captured in terms of
within language document similarities. Experimen-
tal results show that our approach performs better
than dictionary based approaches such as a word-
by-word translation and is also competitive with su-
pervised approaches like CCA and OPCA.
References
Lisa Ballesteros and W. Bruce Croft. 1996. Dictio-
nary methods for cross-lingual information retrieval.
In Proceedings of the 7th International Conference
on Database and Expert Systems Applications, DEXA
?96, pages 791?801, London, UK. Springer-Verlag.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Uncertainty
in Artificial Intelligence.
Scott Deerwester. 1988. Improving Information Re-
trieval with Latent Semantic Indexing. In Christine L.
Borgman and Edward Y. H. Pai, editors, Proceed-
ings of the 51st ASIS Annual Meeting (ASIS ?88), vol-
ume 25, Atlanta, Georgia, October. American Society
for Information Science.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Proceedings of the 29th annual meeting on Associ-
ation for Computational Linguistics, pages 177?184,
Morristown, NJ, USA. Association for Computational
Linguistics.
Wei Gao, John Blitzer, and Ming Zhou. 2008. Using
english information in non-english web search. In iN-
EWS ?08: Proceeding of the 2nd ACM workshop on
Improving non english web searching, pages 17?24,
New York, NY, USA. ACM.
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong.
2009. Exploiting bilingual information to improve
web search. In Proceedings of Human Language Tech-
nologies: The 2009 Conference of the Association for
Computational Linguistics, ACL-IJCNLP ?09, pages
1075?1083, Morristown, NJ, USA. ACL.
Arthur Gretton, Arthur Gretton, Olivier Bousquet, Olivier
Bousquet, Er Smola, Bernhard Schlkopf, and Bern-
hard Schlkopf. 2005. Measuring statistical depen-
dence with hilbert-schmidt norms. In Proceedings of
Algorithmic Learning Theory, pages 63?77. Springer-
Verlag.
151
Aria Haghighi, Percy Liang, Taylor B. Kirkpatrick, and
Dan Klein. 2008. Learning bilingual lexicons from
monolingual corpora. In Proceedings of ACL-08:
HLT, pages 771?779, Columbus, Ohio, June. Associa-
tion for Computational Linguistics.
Ulf Hermjakob, Kevin Knight, and Hal Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proceedings of ACL-
08: HLT, pages 389?397, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
H. Hotelling. 1936. Relation between two sets of vari-
ables. Biometrica, 28:322?377.
Jagadeesh Jagaralmudi, Seth Juarez, and Hal Daume? III.
2010. Kernelized sorting for natural language process-
ing. In Proceedings of AAAI Conference on Artificial
Intelligence.
Jagadeesh Jagarlamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned compara-
ble corpora. In Advances in Information Retrieval,
32nd European Conference on IR Research, ECIR,
volume 5993, pages 444?456, Milton Keynes, UK.
Springer.
R. Jonker and A. Volgenant. 1987. A shortest augment-
ing path algorithm for dense and sparse linear assign-
ment problems. Computing, 38(4):325?340.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, ACL-44,
pages 817?824, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing: Volume 2 - Volume 2, EMNLP ?09,
pages 880?889, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Comput. Linguist., 31:477?
504, December.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Ari Pirkola, Turid Hedlund, Heikki Keskustalo, and
Kalervo Jrvelin. 2001. Dictionary-based cross-
language information retrieval: Problems, methods,
and research findings. Information Retrieval, 4:209?
230.
John C. Platt, Kristina Toutanova, and Wen-tau Yih.
2010. Translingual document representations from
discriminative projections. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, EMNLP ?10, pages 251?261,
Stroudsburg, PA, USA.
Novi Quadrianto, Le Song, and Alex J. Smola. 2009.
Kernelized sorting. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances in Neural
Information Processing Systems 21, pages 1289?1296.
Piyush Rai and Hal Daume? III. 2009. Multi-label pre-
diction via sparse infinite cca. In Advances in Neural
Information Processing Systems, Vancouver, Canada.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated english and german cor-
pora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, ACL ?99, pages 519?526,
Stroudsburg, PA, USA.
Sujith Ravi and Kevin Knight. 2009. Learning phoneme
mappings for transliteration without parallel data. In
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 37?45, Boulder, Colorado, June.
K. Ahuja Ravindra, L. Magnanti Thomas, and B. Orlin
James. 1993. Network flows: Theory, algorithms, and
applications.
Michael L. Littman Susan T. Dumais, Thomas K. Lan-
dauer. 1996. Automatic cross-linguistic information
retrieval using latent semantic indexing. In Working
Notes of the Workshop on Cross-Linguistic Informa-
tion Retrieval, SIGIR, pages 16?23, Zurich, Switzer-
land. ACM.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
J. Artif. Intell. Res. (JAIR), 37:141?188.
Raghavendra Udupa, K. Saravanan, A. Kumaran, and Ja-
gadeesh Jagarlamudi. 2009. Mint: A method for ef-
fective and scalable mining of named entity transliter-
ations from large comparable corpora. In EACL, pages
799?807. The Association for Computer Linguistics.
Alexei Vinokourov, John Shawe-taylor, and Nello Cris-
tianini. 2003. Inferring a semantic representation
of text via cross-language correlation analysis. In
Advances in Neural Information Processing Systems,
pages 1473?1480, Cambridge, MA. MIT Press.
Thuy Vu, AiTi Aw, and Min Zhang. 2009. Feature-based
method for document alignment in comparable news
corpora. In EACL, pages 843?851.
Manfred K. Warmuth and Dima Kuzmin. 2006. Ran-
domized pca algorithms with regret bounds that are
logarithmic in the dimension. In Neural Information
Processing Systems, pages 1481?1488.
152
