Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 410?418,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Bridging Lexical Gaps between Queries and Questions on
Large Online Q&A Collections with Compact Translation Models
Jung-Tae Lee? and Sang-Bum Kim? and Young-In Song? and Hae-Chang Rim?
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
?Search Business Team, SK Telecom, Seoul, Korea
?Dept. of Computer Science & Engineering, Korea University, Seoul, Korea
{jtlee,sbkim,song,rim}@nlp.korea.ac.kr
Abstract
Lexical gaps between queries and questions
(documents) have been a major issue in ques-
tion retrieval on large online question and
answer (Q&A) collections. Previous stud-
ies address the issue by implicitly expanding
queries with the help of translation models
pre-constructed using statistical techniques.
However, since it is possible for unimpor-
tant words (e.g., non-topical words, common
words) to be included in the translation mod-
els, a lack of noise control on the models can
cause degradation of retrieval performance.
This paper investigates a number of empirical
methods for eliminating unimportant words in
order to construct compact translation mod-
els for retrieval purposes. Experiments con-
ducted on a real world Q&A collection show
that substantial improvements in retrieval per-
formance can be achieved by using compact
translation models.
1 Introduction
Community-driven question answering services,
such as Yahoo! Answers1 and Live Search QnA2,
have been rapidly gaining popularity among Web
users interested in sharing information online. By
inducing users to collaboratively submit questions
and answer questions posed by other users, large
amounts of information have been collected in the
form of question and answer (Q&A) pairs in recent
years. This user-generated information is a valu-
able resource for many information seekers, because
1http://answers.yahoo.com/
2http://qna.live.com/
users can acquire information straightforwardly by
searching through answered questions that satisfy
their information need.
Retrieval models for such Q&A collections
should manage to handle the lexical gaps or word
mismatches between user questions (queries) and
answered questions in the collection. Consider the
two following examples of questions that are seman-
tically similar to each other:
? ?Where can I get cheap airplane tickets??
? ?Any travel website for low airfares??
Conventional word-based retrieval models would
fail to capture the similarity between the two, be-
cause they have no words in common. To bridge the
query-question gap, prior work on Q&A retrieval by
Jeon et al (2005) implicitly expands queries with the
use of pre-constructed translation models, which lets
you generate query words not in a question by trans-
lation to alternate words that are related. In prac-
tice, these translation models are often constructed
using statistical machine translation techniques that
primarily rely on word co-occurrence statistics ob-
tained from parallel strings (e.g., question-answer
pairs).
A critical issue of the translation-based ap-
proaches is the quality of translation models con-
structed in advance. If no noise control is conducted
during the construction, it is possible for translation
models to contain ?unnecessary? translations (i.e.,
translating a word into an unimportant word, such as
a non-topical or common word). In the query expan-
sion viewpoint, an attempt to identify and decrease
410
the proportion of unnecessary translations in a trans-
lation model may produce an effect of ?selective?
implicit query expansion and result in improved re-
trieval. However, prior work on translation-based
Q&A retrieval does not recognize this issue and uses
the translation model as it is; essentially no attention
seems to have been paid to improving the perfor-
mance of the translation-based approach by enhanc-
ing the quality of translation models.
In this paper, we explore a number of empiri-
cal methods for selecting and eliminating unimpor-
tant words from parallel strings to avoid unnecessary
translations from being learned in translation models
built for retrieval purposes. We use the term compact
translation models to refer to the resulting models,
since the total number of parameters for modeling
translations would be minimized naturally. We also
present experiments in which compact translation
models are used in Q&A retrieval. The main goal of
our study is to investigate if and how compact trans-
lation models can improve the performance of Q&A
retrieval.
The rest of this paper is organized as follows.
The next section introduces a translation-based re-
trieval model and accompanying techniques used to
retrieve query-relevant questions. Section 3 presents
a number of empirical ways to select and eliminate
unimportant words from parallel strings for training
compact translation models. Section 4 summarizes
the compact translation models we built for retrieval
experiments. Section 5 presents and discusses the
results of retrieval experiments. Section 6 presents
related works. Finally, the last section concludes the
paper and discusses future directions.
2 Translation-based Retrieval Model
This section introduces the translation-based lan-
guage modeling approach to retrieval that has been
used to bridge the lexical gap between queries and
already-answered questions in this paper.
In the basic language modeling framework for re-
trieval (Ponte and Croft, 1998), the similarity be-
tween a query Q and a document D for ranking may
be modeled as the probability of the document lan-
guage model MD built from D generating Q:
sim(Q,D) ? P (Q|MD) (1)
Assuming that query words occur independently
given a particular document language model, the
query-likelihood P (Q|MD) is calculated as:
P (Q|MD) =
?
q?Q
P (q|MD) (2)
where q represents a query word.
To avoid zero probabilities in document language
models, a mixture between a document-specific
multinomial distribution and a multinomial distribu-
tion estimated from the entire document collection
is widely used in practice:
P (Q|MD) =
?
q?Q
[
(1? ?) ? P (q|MD)
+? ? P (q|MC)
]
(3)
where 0 < ? < 1 and MC represents a language
model built from the entire collection. The probabil-
ities P (w|MD) and P (w|MC) are calculated using
maximum likelihood estimation.
The basic language modeling framework does not
address the issue of lexical gaps between queries
and question. Berger and Lafferty (1999) viewed
information retrieval as statistical document-query
translation and introduced translation models to map
query words to document words. Assuming that
a translation model can be represented by a condi-
tional probability distribution of translation T (?|?)
between words, we can model P (q|MD) in Equa-
tion 3 as:
P (q|MD) =
?
w?D
T (q|w)P (w|MD) (4)
where w represents a document word.3
The translation probability T (q|w) virtually rep-
resents the degree of relationship between query
word q and document word w captured in a differ-
ent, machine translation setting. Then, in the tra-
ditional information retrieval viewpoint, the use of
translation models produce an implicit query expan-
sion effect, since query words not in a document are
mapped to related words in the document. This im-
plies that translation-based retrieval models would
make positive contributions to retrieval performance
only when the pre-constructed translation models
have reliable translation probability distributions.
3The formulation of our retrieval model is basically equiva-
lent to the approach of Jeon et al (2005).
411
2.1 IBM Translation Model 1
Obviously, we need to build a translation model in
advance. Usually the IBM Model 1, developed in
the statistical machine translation field (Brown et al,
1993), is used to construct translation models for
retrieval purposes in practice. Specifically, given a
number of parallel strings, the IBM Model 1 learns
the translation probability from a source word s to a
target word t as:
T (t|s) = ??1s
N?
i
c(t|s;Ji) (5)
where ?s is a normalization factor to make the sum
of translation probabilities for the word s equal to 1,
N is the number of parallel string pairs, and Ji is the
ith parallel string pair. c(t|s; Ji) is calculated as:
c(t|s; Ji) =
( P (t|s)
P (t|s1) + ? ? ?+ P (t|sn)
)
?freqt,Ji ? freqs,Ji (6)
where {s1, . . . , sn} are words in the source text in
J i. freqt,Ji and freqs,Ji are the number of times
that t and s occur in Ji, respectively.
Given the initial values of T (t|s), Equations (5)
and (6) are used to update T (t|s) repeatedly until
the probabilities converge, in an EM-based manner.
Note that the IBM Model 1 solely relies on
word co-occurrence statistics obtained from paral-
lel strings in order to learn translation probabilities.
This implies that if parallel strings have unimportant
words, a resulted translation model based on IBM
Model 1 may contain unimportant words with non-
zero translation probabilities.
We alleviate this drawback by eliminating unim-
portant words from parallel strings, avoiding them
from being included in the conditional translation
probability distribution. This naturally induces the
construction of compact translation models.
2.2 Gathering Parallel Strings from Q&A
Collections
The construction of statistical translation models
previously discussed requires a corpus consisting of
parallel strings. Since monolingual parallel texts are
generally not available in real world, one must arti-
ficially generate a ?synthetic? parallel corpus.
Question and answer as parallel pairs: The
simplest approach is to directly employ questions
and their answers in the collections by setting ei-
ther as source strings and the other as target strings,
with the assumption that a question and its cor-
responding answer are naturally parallel to each
other. Formally, if we have a Q&A collection as
C = {D1, D2, . . . , Dn}, where Di refers to an ith
Q&A data consisting of a question qi and its an-
swer ai, we can construct a parallel corpus C ? as
{(q1, a1), . . . , (qn, an)}?{(a1, q1), . . . , (an, qn)} =
C ? where each element (s, t) refers to a parallel pair
consisting of source string s and target string t. The
number of parallel string samples would eventually
be twice the size of the collections.
Similar questions as parallel pairs: Jeon et
al. (2005) proposed an alternative way of auto-
matically collecting a relatively larger set of par-
allel strings from Q&A collections. Motivated
by the observation that many semantically identi-
cal questions can be found in typical Q&A collec-
tions, they used similarities between answers cal-
culated by conventional word-based retrieval mod-
els to automatically group questions in a Q&A col-
lection as pairs. Formally, two question strings qi
and qj would be included in a parallel corpus C ?
as {(qi, qj), (qj , qi)} ? C ? only if their answer
strings ai and aj have a similarity higher than a
pre-defined threshold value. The similarity is cal-
culated as the reverse of the harmonic mean of ranks
as sim(ai, aj) = 12( 1rj + 1ri ), where rj and ri refer to
the rank of the aj and ai when ai and aj are given as
queries, respectively. This approach may artificially
produce much more parallel string pairs for training
the IBM Model 1 than the former approach, depend-
ing on the threshold value.4
To our knowledge, there has not been any study
comparing the effectiveness of the two approaches
yet. In this paper, we try both approaches and com-
pare the effectiveness in retrieval performance.
3 Eliminating Unimportant Words
We adopt a term weight ranking approach to iden-
tify and eliminate unimportant words from parallel
strings, assuming that a word in a string is unim-
4We have empirically set the threshold (0.05) for our exper-
iments.
412
Figure 1: Term weighting results of tf-idf and TextRank (window=3). Weighting is done on underlined words only.
portant if it holds a relatively low significance in the
document (Q&A pair) of which the string is origi-
nally taken from. Some issues may arise:
? How to assign a weight to each word in a doc-
ument for term ranking?
? How much to remove as unimportant words
from the ranked list?
The following subsections discuss strategies we use
to handle each of the issues above.
3.1 Assigning Term Weights
In this section, the two different term weighting
strategies are introduced.
tf-idf: The use of tf-idf weighting on evaluating
how unimportant a word is to a document seems to
be a good idea to begin with. We have used the fol-
lowing formulas to calculate the weight of word w
in document D:
tf -idfw,D = tfw,D ? idfw (7)
tfw,D = freqw,D|D| , idfw = log
|C|
dfw
where freqw,D refers to the number of times w oc-
curs in D, |D| refers to the size of D (in words), |C|
refers to the size of the document collection, and dfw
refers to the number of documents where w appears.
Eventually, words with low tf-idf weights may be
considered as unimportant.
TextRank: The task of term weighting, in fact,
has been often applied to the keyword extraction
task in natural language processing studies. As
an alternative term weighting approach, we have
used a variant of Mihalcea and Tarau (2004)?s Tex-
tRank, a graph-based ranking model for keyword
extraction which achieves state-of-the-art accuracy
without the need of deep linguistic knowledge or
domain-specific corpora.
Specifically, the ranking algorithm proceeds as
follows. First, words in a given document are added
as vertices in a graph G. Then, edges are added be-
tween words (vertices) if the words co-occur in a
fixed-sized window. The number of co-occurrences
becomes the weight of an edge. When the graph is
constructed, the score of each vertex is initialized
as 1, and the PageRank-based ranking algorithm is
run on the graph iteratively until convergence. The
TextRank score of a word w in document D at kth
iteration is defined as follows:
Rkw,D = (1? d)+ d ?
?
?j:(i,j)?G
ei,j?
?l:(j,l)?G ej,l
Rk?1w,D
(8)
where d is a damping factor usually set to 0.85, and
ei,j is an edge weight between i and j.
The assumption behind the use of the variant of
TextRank is that a word is likely to be an important
word in a document if it co-occurs frequently with
other important words in the document. Eventually,
words with low TextRank scores may be considered
as unimportant. The main differences of TextRank
compared to tf-idf is that it utilizes the context infor-
mation of words to assign term weights.
Figure 1 demonstrates that term weighting results
of TextRank and tf-idf are greatly different. Notice
that TextRank assigns low scores to words that co-
413
Corpus: (Q?A) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 90,441 73
25%Removal 90,326 (?0.1%) 73,021 (?19.3%) 73 (?0.0%) 44 (?39.7%)
50%Removal 90,230 (?0.2%) 72,225 (?20.1%) 72 (?1.4%) 43 (?41.1%)
75%Removal 88,763 (?1.9%) 65,268 (?27.8%) 53 (?27.4%) 38 (?47.9%)
Avg.Score 66,412 (?26.6%) 31,849 (?64.8%) 14 (?80.8%) 18 (?75.3%)
Table 1: Impact of various word elimination strategies on translation model construction using (Q?A) corpus.
Corpus: (Q?Q) Vocabulary Size (%chg) Average Translations (%chg)
tf-idf TextRank tf-idf TextRank
Initial 34,485 442
25%Removal 34,374 (?0.3%) 26,900 (?22.0%) 437 (?1.1%) 282 (?36.2%)
50%Removal 34,262 (?0.6%) 26,421 (?23.4%) 423 (?4.3%) 274 (?38.0%)
75%Removal 32,813 (?4.8%) 23,354 (?32.3%) 288 (?34.8%) 213 (?51.8%)
Avg.Score 28,613 (?17.0%) 16,492 (?52.2%) 163 (?63.1%) 164 (?62.9%)
Table 2: Impact of various word elimination strategies on translation model construction using (Q?Q) corpus.
occur only with stopwords. This implies that Tex-
tRank weighs terms more ?strictly? than the tf-idf
approach, with use of contexts of words.
3.2 Deciding the Quantity to be Removed from
Ranked List
Once a final score (either tf-idf or TextRank score)
is obtained for each word, we create a list of words
ranked in decreasing order of their scores and elim-
inate the ones at lower ranks as unimportant words.
The question here is how to decide the proportion or
quantity to be removed from the ranked list.
Removing a fixed proportion: The first ap-
proach we have used is to decide the number of
unimportant words based on the size of the original
string. For our experiments, we manually vary the
proportion to be removed as 25%, 50%, and 75%.
For instance, if the proportion is set to 50% and an
original string consists of ten words, at most five
words would be remained as important words.
Using average score as threshold: We also have
used an alternate approach to deciding the quantity.
Instead of eliminating a fixed proportion, words are
removed if their score is lower than the average score
of all words in a document. This approach decides
the proportion to be removed more flexibly than the
former approach.
4 Building Compact Translation Models
We have initially built two parallel corpora from
a Q&A collection5, denoted as (Q?A) corpus and
(Q?Q) corpus henceforth, by varying the methods
in which parallel strings are gathered (described in
Section 2.2). The (Q?A) corpus consists of 85,938
parallel string pairs, and the (Q?Q) corpus contains
575,649 parallel string pairs.
In order to build compact translation models, we
have preprocessed the parallel corpus using differ-
ent word elimination strategies so that unimpor-
tant words would be removed from parallel strings.
We have also used a stoplist6 consisting of 429
words to remove stopwords. The out-of-the-box
GIZA++7 (Och and Ney, 2004) has been used to
learn translation models using the pre-processed par-
allel corpus for our retrieval experiments. We have
also trained initial translation models, using a par-
allel corpus from which only the stopwords are re-
moved, to compare with the compact translation
models.
Eventually, the number of parameters needed
for modeling translations would be minimized if
unimportant words are eliminated with different ap-
5Details on this data will be introduced in the next section.
6http://truereader.com/manuals/onix/stopwords1.html
7http://www.fjoch.com/GIZA++.html
414
proaches. Table 1 and 2 shows the impact of various
word elimination strategies on the construction of
compact translation models using the (Q?A) corpus
and the (Q?Q) corpus, respectively. The two tables
report the size of the vocabulary contained and the
average number of translations per word in the re-
sulting compact translation models, along with per-
centage decreases with respect to the initial transla-
tion models in which only stopwords are removed.
We make these observations:
? The translation models learned from the (Q?Q)
corpus have less vocabularies but more aver-
age translations per word than the ones learned
from the (Q?A) corpus. This result implies that
a large amount of noise may have been cre-
ated inevitably when a large number of parallel
strings (pairs of similar questions) were artifi-
cially gathered from the Q&A collection.
? The TextRank strategy tends to eliminate larger
sets of words as unimportant words than the
tf-idf strategy when a fixed proportion is re-
moved, regardless of the corpus type. Recall
that the TextRank approach assigns weights to
words more strictly by using contexts of words.
? The approach to remove words according to
the average weight of a document (denoted as
Avg.Score) tends to eliminate relatively larger
portions of words as unimportant words than
any of the fixed-proportion strategies, regard-
less of either the corpus type or the ranking
strategy.
5 Retrieval Experiments
Experiments have been conducted on a real world
Q&A collection to demonstrate the effectiveness of
compact translation models on Q&A retrieval.
5.1 Experimental Settings
In this section, four experimental settings for the
Q&A retrieval experiments are described in detail.
Data: For the experiments, Q&A data have been
collected from the Science domain of Yahoo! An-
swers, one of the most popular community-based
question answering service on the Web. We have
obtained a total of 43,001 questions with a best an-
swer (selected either by the questioner or by votes of
other users) by recursively traversing subcategories
of the Science domain, with up to 1,000 question
pages retrieved.8
Among the obtained Q&A pairs, 32 Q&A pairs
have been randomly selected as the test set, and the
remaining 42,969 questions have been the reference
set to be retrieved. Each Q&A pair has three text
fields: question title, question content, and answer.9
The fields of each Q&A pair in the test set are con-
sidered as various test queries; the question title,
the question content, and the answer are regarded
as a short query, a long query, and a supplementary
query, respectively. We have used long queries and
supplementary queries only in the relevance judg-
ment procedure. All retrieval experiments have been
conducted using short queries only.
Relevance judgments: To find relevant Q&A
pairs given a short query, we have employed a pool-
ing technique used in the TREC conference series.
We have pooled the top 40 Q&A pairs from each
retrieval results generated by varying the retrieval
algorithms, the search field, and the query type.
Popular word-based models, including the Okapi
BM25, query-likelihood language model, and pre-
vious translation-based models (Jeon et al, 2005),
have been used.10
Relevance judgments have been done by two stu-
dent volunteers (both fluent in English). Since
many community-based question answering ser-
vices present their search results in a hierarchical
fashion (i.e. a list of relevant questions is shown
first, and then the user chooses a specific question
from the list to see its answers), a Q&A pair has been
judged as relevant if its question is semantically sim-
ilar to the query; neither quality nor rightness of the
answer has not been considered. When a disagree-
ment has been made between two volunteers, one of
the authors has made the final judgment. As a result,
177 relevant Q&A pairs have been found in total for
the 32 short queries.
Baseline retrieval models: The proposed ap-
8Yahoo! Answers did not expose additional question pages
to external requests at the time of collecting the data.
9When collecting parallel strings from the Q&A collection,
we have put together the question title and the question content
as one question string.
10The retrieval model using compact translation models has
not been used in the pooling procedure.
415
proach to Q&A retrieval using compact translation
models (denoted as CTLM henceforth) is compared
to three baselines:
QLM: Query-likelihood language model for re-
trieval (equivalent to Equation 3, without use of
translation models). This model represents word-
based retrieval models widely used in practice.
TLM(Q?Q): Translation-based language model
for question retrieval (Jeon et al, 2005). This model
uses IBM Model 1 learned from the (Q?Q) corpus
of which stopwords are removed.
TLM(Q?A): A variant of the translation-based ap-
proach. This model uses IBM model 1 learned from
the (Q?A) corpus.
Evaluation metrics: We have reported the re-
trieval performance in terms of Mean Average Pre-
cision (MAP) and Mean R-Precision (R-Prec).
Average Precision can be computed based on the
precision at each relevant document in the ranking.
Mean Average Precision is defined as the mean of
the Average Precision values across the set of all
queries:
MAP (Q) = 1|Q|
?
q?Q
1
mq
mq?
k=1
Precision(Rk) (9)
where Q is the set of test queries, mq is the number
of relevant documents for a query q, Rk is the set of
ranked retrieval results from the top until rank posi-
tion k, and Precision(Rk) is the fraction of relevant
documents in Rk (Manning et al, 2008).
R-Precision is defined as the precision after
R documents have been retrieved where R is
the number of relevant documents for the current
query (Buckley and Voorhees, 2000). Mean R-
Precision is the mean of the R-Precisions across the
set of all queries.
We take MAP as our primary evaluation metric.
5.2 Experimental Results
Preliminary retrieval experiments have been con-
ducted using the baseline QLM and different fields
of Q&A data as retrieval unit. Table 3 shows the
effectiveness of each field.
The results imply that the question title field is the
most important field in our Yahoo! Answers collec-
tion; this also supports the observation presented by
Retrieval unit MAP R-Prec
Question title 0.1031 0.2396
Question content 0.0422 0.0999
Answer 0.0566 0.1062
Table 3: Preliminary retrieval results.
Model MAP R-Prec
(%chg) (%chg)
QLM 0.1031 0.2396
TLM(Q?Q)* 0.1121 0.2251
(49%) (?6%)
CTLM(Q?Q) 0.1415 0.2425
(437%) (41%)
TLM(Q?A) 0.1935 0.3135
(488%) (431%)
CTLM(Q?A) 0.2095 0.3585
(4103%) (450%)
Table 4: Comparisons with three baseline retrieval mod-
els. * indicates that it is equivalent to Jeon et al (2005)?s
approach. MAP improvements of CTLMs have been
tested to be statistically significant using paired t-test.
Jeon et al (2005). Based on the preliminary obser-
vations, all retrieval models tested in this paper have
ranked Q&A pairs according to the similarity scores
between queries and question titles.
Table 4 presents the comparison results of three
baseline retrieval models and the proposed CTLMs.
For each method, the best performance after empir-
ical ? parameter tuning according to MAP is pre-
sented.
Notice that both the TLMs and CTLMs have out-
performed the word-based QLM. This implies that
word-based models that do not address the issue of
lexical gaps between queries and questions often fail
to retrieve relevant Q&A data that have little word
overlap with queries, as noted by Jeon et al (2005).
Moreover, notice that the proposed CTLMs have
achieved significantly better performances in all
evaluation metrics than both QLM and TLMs, regard-
less of the parallel corpus in which the incorporated
translation models are trained from. This is a clear
indication that the use of compact translation models
built with appropriate word elimination strategies is
effective in closing the query-question lexical gaps
416
(Q?Q) MAP (%chg)
tf-idf TextRank
Initial 0.1121
25%Rmv 0.1141 (41.8) 0.1308 (416.7)
50%Rmv 0.1261 (412.5) 0.1334 (419.00)
75%Rmv 0.1115 (?0.5) 0.1160 (43.5)
Avg.Score 0.1056 (?5.8) 0.1415 (426.2)
Table 5: Contributions of various word elimination strate-
gies on MAP performance of CTLM(Q?Q).
(Q?A) MAP (%chg)
tf-idf TextRank
Initial 0.1935
25%Rmv 0.2095 (48.3) 0.1733 (?10.4)
50%Rmv 0.2085 (47.8) 0.1623 (?16.1)
75%Rmv 0.1449 (?25.1) 0.1515 (?21.7)
Avg.Score 0.1168 (?39.6) 0.1124 (?41.9)
Table 6: Contributions of various word elimination strate-
gies on MAP performance of CTLM(Q?A).
for improving the performance of question retrieval
in the context of language modeling framework.
Note that the retrieval performance varies by the
type of training corpus; CTLM(Q?A) has outper-
formed CTLM(Q?Q) significantly. This proves the
statement we made earlier that the (Q?Q) corpus
would contain much noise since the translation mod-
els learned from the (Q?Q) corpus tend to have
smaller vocabulary sizes but significantly more aver-
age translations per word than the ones learned from
the (Q?A) corpus.
Table 5 and 6 show the effect of various word
elimination strategies on the retrieval performance
of CTLMs in which the incorporated compact trans-
lation models are trained from the (Q?Q) corpus and
the (Q?A) corpus, respectively. It is interesting to
note that the importance of modifications in word
elimination strategies also varies by the type of train-
ing corpus.
The retrieval results indicate that when the trans-
lation model is trained from the ?less noisy? (Q?A)
corpus, eliminating a relatively large proportions of
words may hurt the retrieval performance of CTLM.
In the case when the translation model is trained
from the ?noisy? (Q?Q) corpus, a better retrieval
performance may be achieved if words are elimi-
nated appropriately to a certain extent.
In terms of weighting scheme, the TextRank ap-
proach, which is more ?strict? than tf-idf in elim-
inating unimportant words, has led comparatively
higher retrieval performances on all levels of re-
moval quantity when the translation model has been
trained from the ?noisy? (Q?Q) corpus. On the con-
trary, the ?less strict? tf-idf approach has led better
performances when the translation model has been
trained from the ?less noisy? (Q?A) corpus.
In summary, the results imply that the perfor-
mance of translation-based retrieval models can be
significantly improved when strategies for building
of compact translation models are chosen properly,
regarding the expected noise level of the parallel cor-
pus for training the translation models. In a case
where a noisy parallel corpus is given for training
of translation models, it is better to get rid of noise
as much as possible by using ?strict? term weight-
ing algorithms; when a less noisy parallel corpus is
given for building the translation models, a tolerant
approach would yield better retrieval performance.
6 Related Works
Our work is most closely related to Jeon et
al. (2005)?s work, which addresses the issue of
word mismatch between queries and questions in
large online Q&A collections by using translation-
based methods. Apart from their work, there have
been some related works on applying translation-
based methods for retrieving FAQ data. Berger et
al. (2000) report some of the earliest work on FAQ
retrieval using statistical retrieval models, includ-
ing translation-based approaches, with a small set
of FAQ data. Soricut and Brill (2004) present an an-
swer passage retrieval system that is trained from 1
million FAQs collected from the Web using trans-
lation methods. Riezler et al (2007) demonstrate
the advantages of translation-based approach to an-
swer retrieval by utilizing a more complex trans-
lation model also trained from a large amount of
data extracted from FAQs on the Web. Although all
of these translation-based approaches are based on
the statistical translation models, including the IBM
Model 1, none of them focus on addressing the noise
issues in translation models.
417
7 Conclusion and Future Work
Bridging the query-question gap has been a major is-
sue in retrieval models for large online Q&A collec-
tions. In this paper, we have shown that the perfor-
mance of translation-based retrieval on real online
Q&A collections can be significantly improved by
using compact translation models of which the noise
(unimportant word translations) is properly reduced.
We have also observed that the performance en-
hancement may be achieved by choosing the appro-
priate strategies regarding the strictness of various
term weighting algorithms and the expected noise
level of the parallel data for learning such transla-
tion models.
Future work will focus on testing the effective-
ness of the proposed method on a larger set of Q&A
collections with broader domains. Since the pro-
posed approach cannot handle many-to-one or one-
to-many word transformations, we also plan to in-
vestigate the effectiveness of phrase-based transla-
tion models in closing gaps between queries and
questions for further enhancement of Q&A retrieval.
Acknowledgments
This work was supported by Microsoft Research
Asia. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the sponsor.
References
Adam Berger, Rich Caruana, David Cohn, Dayne Fre-
itag, and Vibhu Mittal. 2000. Bridging the Lexical
Chasm: Statistical Approaches to Answer-Finding. In
Proceedings of the 23rd Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval, pages 192?199.
Adam Berger and John Lafferty. 1999. Information Re-
trieval as Statistical Translation. In Proceedings of the
22nd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 222?229.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Chris Buckley and Ellen M. Voorhees. 2000. Evaluating
Evaluation Measure Stability. In Proceedings of the
23rd Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 33?40.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding Similar Questions in Large Question and An-
swer Archives. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, pages 84?90.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing Order into Text. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 404?411.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Jay M. Ponte and W. Bruce Croft. 1998. A Language
Modeling Approach to Information Retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
Machine Translation for Query Expansion in Answer
Retrieval. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
pages 464?471.
Radu Soricut and Eric Brill. 2004. Automatic Question
Answering: Beyond the Factoid. In Proceedings of
the 2004 Human Language Technology and Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 57?64.
418
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 29?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
K-QARD: A Practical Korean Question Answering Framework for
Restricted Domain
Young-In Song, HooJung Chung,
Kyoung-Soo Han, JooYoung Lee,
Hae-Chang Rim
Dept. of Computer Science & Engineering
Korea University
Seongbuk-gu, Seoul 136-701, Korea
 song, hjchung, kshan, jylee
rim@nlp.korea.ac.kr
Jae-Won Lee
Computing Lab.
Samsung Advanced Institute of Technology
Nongseo-ri, Giheung-eup,
Yongin-si, Gyeonggi-do 449-712, Korea
jwonlee@samsung.com
Abstract
We present a Korean question answer-
ing framework for restricted domains,
called K-QARD. K-QARD is developed to
achieve domain portability and robustness,
and the framework is successfully applied
to build question answering systems for
several domains.
1 Introduction
K-QARD is a framework for implementing a fully
automated question answering system including
the Web information extraction (IE). The goal of
the framework is to provide a practical environ-
ment for the restricted domain question answering
(QA) system with the following requirements:
  Domain portability: Domain adaptation of
QA systems based on the framework should
be possible with minimum human efforts.
  Robustness: The framework has to provide
methodologies to ensure robust performance
for various expressions of a question.
For the domain portability, K-QARD is de-
signed as a domain-independent architecture and
it keeps all domain-dependent elements in exter-
nal resources. In addition, the framework tries to
employ various techniques for reducing the human
effort, such as simplifying rules based on linguis-
tic information and machine learning approaches.
Our effort for the robustness is focused the
question analysis. Instead of using a technique
for deep understanding of the question, the ques-
tion analysis component of K-QARD tries to ex-
tract only essential information for answering us-
ing the information extraction technique with lin-
guistic information. Such approach is helpful for
NL Answer
Question Analysis
Web Information 
Extraction
Answer Finding
Answer 
Generation
Database
Web Page
NL Question
Web Page
Semantic frames
TE/TR rules
Domain ontology
Training examples
Answer frames
Domain-dependent
External Resources
Domain-independent
Framework
Figure 1: Architecture of K-QARD
not only the robustness but also the domain porta-
bility because it generally requires smaller size of
hand-crafted rules than a complex semantic gram-
mar.
K-QARD uses the structural information auto-
matically extracted from Web pages which include
domain-specific information for question answer-
ing. It has the disavantage that the coverage of QA
system is limited, but it can simplify the question
answering process with robust performance.
2 Architecture of K-QARD
As shown in Figure 1, K-QARD has four major
components: Web information extraction, ques-
tion analysis, answer finding, and answer gener-
ation.
The Web information extraction (IE) compo-
nent extracts the domain-specific information for
question answering from Web pages and stores
the information into the relational database. For
the domain portability, the Web IE component
is based on the automatic wrapper induction ap-
proach which can be learned from small size of
training examples.
The question analysis component analyzes an
29
input question, extracts important information us-
ing the IE approach, and matches the question with
pre-defined semantic frames. The component out-
puts the best-matched frame whose slots are filled
with the information extracted from the question.
In the answer finding component, K-QARD re-
trieves the answers from the database using the
SQL generation script defined in each semantic
frame. The SQL script dynamically generates
SQL using the values of the frame slots.
The answer generation component provides the
answer to the user as a natural language sentence
or a table by using the generation rules and the
answer frames which consist of canned texts.
3 Question Analysis
The key component for ensuring the robustness
and domain portability is the question analy-
sis because it naturally requires many domain-
dependent resources and has responsibility to
solve the problem caused by various ways of ex-
pressing a question. In K-QARD, a question is an-
alyzed using the methods devised by the informa-
tion extraction approach. This IE-based question
analysis method consists of several steps:
1. Natural language analysis: Analyzing the
syntactic structure of the user?s question and
also identifiying named-entities and some im-
portant words, such as domain-specific pred-
icate or terms.
2. Question focus recognition: Finding the
intention of the user?s question using the
question focus classifier. It is learned from
the training examples based on decision
tree(C4.5)(Quinlan, 1993).
3. Template Element(TE) recognition: Find-
ing important concept for filling the slots
of the semantic frame, namely template el-
ements, using the rules, NE information, and
ontology, etc.
4. Template Relation(TR) recognition: Find-
ing the relation between TEs and a question
focus based on TR rules, and syntactic infor-
mation, etc.
Finally, the question analysis component selects
the proper frame for the question and fills proper
values of each slot of the selected frame.
Compared to other question analysis methods
such as the complex semantic grammar(Martin et
al., 1996), our approach has several advantages.
First, it shows robust performance for the variation
of a question because IE-based approach does not
require the understanding of the entire sentence. It
is sufficient to identify and process only the impor-
tant concepts. Second, it also enhances the porta-
bility of the QA systems. This method is based on
the divide-and-conquer strategy and uses only lim-
ited context information. By virture of these char-
acteristics, the question analysis can be processed
by using a small number of simple rules.
In the following subsections, we will describe
each component of our question analyzer in K-
QARD.
3.1 Natural language analysis
The natural language analyzer in K-QARD iden-
tifies morphemes, tags part-of-speeches to them,
and analyzes dependency relations between the
morphemes. A stochastic part-of-speech tagger
and dependency parser(Chung and Rim, 2004) for
the Korean language are trained on a general do-
main corpus and are used for the analyzer. Then,
several domain-specific named entities, such as a
TV program name, and general named entities,
such as a date, in the question are recognized us-
ing our dictionary and pattern-based named entity
tagger(Lee et al, 2004). Finally some important
words, such as domain-specific predicates, ter-
minologies or interrogatives, are replaced by the
proper concept names in the ontology. The man-
ually constructed ontology includes two different
types of information: domain-specific and general
domain words.
The role of this analyzer is to analyze user?s
question and transform it to the more generalized
representation form. So, the task of the question
focus recognition and the TE/TR recognition can
be simplified because of the generalized linguistic
information without decreasing the performance
of the question analyzer.
One of possible defects of using such linguis-
tic information is the loss of the robustness caused
by the error of the NLP components. However,
our IE-based approach for question analysis uses
the very restricted and essential contextual infor-
mation in each step and can avoid such a risk suc-
cessfully.
The example of the analysis process of this
30
Question :   ??? NBC?? ??? ?? ?? ????
(today) (on NBC)
(at night)
(program)
(play)
(what)
(?What movie will be played on NBC tonight?? in English)
(1) : 
Natural Language Analysis
????/NE_Date
(today)
?NBC?/NE_Channel
(on NBC)
????/NE_Time
(at night)
????/C_what
(what)
????/C_prog
(program)
????/C_play
(play)
(2) : 
Question Focus Recognition
????/NE_Date
(today)
?NBC?/NE_Channel
(on NBC)
????/NE_Time
(at night)
????/C_what
(what)
????/C_prog
(program)
????/C_play
(play)
Question focus region
Question focus : QF_program
a 
(3) : 
TE Recognition
????/NE_Date
(today)
?NBC?/NE_Channel
(on NBC)
????/NE_Time
(at night)
Question focus : QF_program
TE_BEGINDATE
TE_BEGINTime
TE_CHANNEL
(4) : 
TR Recognition
????/NE_Date
(today)
?NBC?/NE_Channel
(on NBC)
????/NE_Time
(at night)
TE_BEGINDATE
TE_BEGINTime
TE_CHANNEL
REL_OK
REL_OK
REL_OK
Translation of Semantic Frame
FRM : PROGRAM_QUESTION
Question focus : QF_program
Begin Date : ?Today?
Begin Time : ?Night?
Channel : ?NBC?
Question focus : QF_program
?NE_*? denotes that the corresponding word is named entity of *.
?C_*? denotes that the corresponding word is belong to the concept C_* in the ontology.
?TE_*? denotes that the corresponding word is template element whose type is *.
?REL_OK? indicates that the corresponding TE and question focus are related.
Figure 2: Example of Question Analysis Process in K-QARD
component is shown in Figure 2-(1).
3.2 Question focus recognition
We define a question focus as a type of informa-
tion that a user wants to know. For example, in
the question  What movies will be shown on TV
tonight?, the question focus is a program title, or
titles. For another example, the question focus is
a current rainfall in a question  San Francisco is
raining now, is it raining in Los Angeles too?.
To find the question focus, we define question
focus region, a part of a question that may contain
clues for deciding the question focus. The ques-
tion focus region is identified with a set of simple
rules which consider the characteristic of the Ko-
rean interrogatives. Generally, the question focus
region has a fixed pattern that is typically used in
interrogative questions(Akiba et al, 2002). Thus
a small number of simple rules is enough to cover
the most of question focus region pattern. Figure
2-(2) shows the part recognized as a question fo-
cus region in the sample question.
After recognizing the region, the actual focus of
the question is determined with features extracted
from the question focus region. For the detection,
we build the question focus classifier using deci-
sion tree (C4.5) and several linguistic or domain-
specific features such as the kind of the interroga-
tive and the concept name of the predicate.
Dividing the focus recognition process into two
parts helps to increase domain portability. While
the second part of deciding the actual question fo-
cus is domain-dependent because every domain-
application has its own set of question foci, the
first part that recognizes the question focus region
is domain-independent.
3.3 TE recognition
In the TE identification phase, pre-defined words,
phrases, and named entities are identified as slot-
filler candidates for appropriate slots, according to
TE tagging rules. For instance, movie and NBC
are tagged as Genre and Channel in the sample
question  Tell me the movie on NBC tonight. (i.e.
movie will be used to fill Genre slot and NBC
will be used to fill Channel slot in a semantic
frame). The hand-crafted TE tagging rules basi-
cally consider the surface form and the concept
name (derived from domain ontologies) of a target
word. The context surrounding the target word or
word dependency information is also considered
in some cases. In the example question of Figure
2, the date expression ?  (today)?, time expres-
sion ? (night)? and the channel name ?MBC?
are selected as TE candidates.
In K-QARD, such identification is accom-
plished by a set of simple rules, which only ex-
amines the semantic type of each constituent word
in the question, except the words in the question
region. It is mainly because of our divide-and-
conquer strategy motivated by IE. The result of
this component may include some wrong template
elements, which do not have any relation to the
user?s intention or the question focus. However,
they are expected to be removed in the next com-
ponent, the TR recognizer which examines the re-
lation between the recognized TE and the question
focus.
31
(1) Broadcast-domain QA system
(2) Answer for sample question, 
?What soap opera will be played on MBC tonight??
Figure 3: Broadcast-domain QA System using K-QARD
3.4 TR recognition
In the TR recognition phase, all entities identified
in the TE recognition phase are examined whether
they have any relationships with the question fo-
cus region of the question. For example, in the
question  Is it raining in Los Angeles like in San
Francisco?, both Los Angeles and San Francisco
are identified as a TE. However, by the TR recog-
nition, only Los Angeles is identified as a related
entity with the question focus region.
Selectional restriction and dependency relations
between TEs are mainly considered in TR tagging
rules. Thus, the TR rules can be quite simplified.
For example, many relations between the TEs and
the question region can be simply identified by ex-
amining whether there is a syntactic dependency
between them as shown in Figure 2-(4). Moreover,
to make up for the errors in dependency parsing,
lexico-semantic patterns are also encoded in the
TR tagging rules.
4 Application of K-QARD
To evaluate the K-QARD framework, we built re-
stricted domain question answering systems for
the several domains: weather, broadcast, and traf-
fic. For the adaptation of QA system to each do-
main, we rewrote the domain ontology consisting
of about 150 concepts, about 30 TE/TR rules, and
7-23 semantic frames and answer templates. In
addition, we learned the question focus classifier
from training examples of about 100 questions for
the each domain. All information for the ques-
tion answering was automatically extracted using
the Web IE module of K-QARD, which was also
learned from training examples consisting of sev-
eral annotated Web pages of the target Web site. It
took about a half of week for two graduate stu-
dents who clearly understood the framework to
build each QA system. Figure 3 shows an example
of QA system applied to the broadcast domain.
5 Conclusion
In this paper, we described the Korean question
answering framework, namely K-QARD, for re-
stricted domains. Specifically, this framework is
designed to enhance the robustness and domain
portability. To achieve this goal, we use the IE-
based question analyzer using the generalized in-
formation acquired by several NLP components.
We also showed the usability of K-QARD by suc-
cessfully applying the framework to several do-
mains.
References
T. Akiba, K. Itou, A. Fujii, and T Ishikawa. 2002.
Towards speech-driven question answering: Exper-
iments using the NTCIR-3 question answering col-
lection. In Proceedings of the Third NTCIR Work-
shop.
H. Chung and H. Rim. 2004. Unlexicalized de-
pendency parser for variable word order languages
based on local contextual pattern. Lecture Note in
Computer Science, (2945):112?123.
J. Lee, Y. Song, S. Kim, H. Chung, and H. Rim. 2004.
Title recognition using lexical pattern and entity dic-
tionary. In Proceedings of the 1st Asia Information
Retrieval Symposium (AIRS2004), pages 345?348.
P. Martin, F. Crabbe, S. Adams, E. Baatz, and
N. Yankelovich. 1996. Speechacts: a spoken lan-
guage framework. IEEE Computer, 7(29):33?40.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA.
32
A Practical QA System in Restricted Domains
Hoojung Chung, Young-In Song, Kyoung-Soo Han,
Do-Sang Yoon, Joo-Young Lee, Hae-Chang Rim
Dept. of Comp. Science and Engineering
Korea University
Seoul 136-701 Korea
{hjchung,sprabbit,kshan,yds5004,jylee,rim}@nlp.korea.ac.kr
Soo-Hong Kim
Dept. of Comp. Software Engineering
Sangmyung University
Chonan 330-720 Korea
soohkim@smuc.ac.kr
Abstract
This paper describes an on-going research for a
practical question answering system for a home
agent robot. Because the main concern of the QA
system for the home robot is the precision, rather
than coverage (No answer is better than wrong an-
swers), our approach is try to achieve high accuracy
in QA. We restrict the question domains and extract
answers from the pre-selected, semi-structured doc-
uments on the Internet. A named entity tagger and a
dependency parser are used to analyze the question
accurately. User profiling and inference rules are
used to infer hidden information that is required for
finding a precise answer. Testing with a small set of
queries on weather domain, the QA system showed
90.9% of precision and 75.0% of recall.
1 Introduction
During the last decade, automatic question-
answering has become an interesting research field
and resulted in a significant improvement in its
performance, which has been largely driven by
the TREC (Text REtrieval Conference) QA Track
(Voorhees, 2004). The best of the systems in the QA
Track is able to answer questions correctly 70% of
the time (Light et al, 2003). The 70% of accuracy
is, of course, high enough to surprise the researchers
of this field, but, on the other hand, the accuracy is
not enough to satisfy the normal users in the real
world, who expect more precise answers.
The difficulty of constructing open-domain
knowledge base is one reason for the difficulties of
open-domain question answering. Since question
answering requires understanding of natural lan-
guage text, the QA system requires much linguis-
tic and common knowledge for answering correctly.
The simplest approach to improve the accuracy of a
question answering system might be restricting the
domain it covers. By restricting the question do-
main, the size of knowledge base to build becomes
smaller.
This paper describes our restricted domain ques-
tion answering system for an agent robot in home
environment. One of the roles of the home agent
robot is to be able to answer the practical ques-
tions such as weather information, stock quote, TV
broadcasting schedule, traffic information etc. via a
speech interface. The agent should provide high-
precision answers, otherwise the users will not trust
the entire functions of the home agent robot, which
includes not only the ability of question answering
but also the speech interface for controlling home
appliances. That means no answer is preferred to
a wrong answer and the primary concern in our re-
search is improving the precision of the question an-
swering system.
In this paper, we present a question answering
system which is restricted to answer only to the
questions on weather forecasts 1, and provide some
experimental results of the restricted QA system.
To achieve the high accuracy, the QA system pro-
cesses the semi-structured text data on the Inter-
net and store it in the form of relational database.
The domain specific hand-coded ontology contain-
ing weather terms and cities is manually built for the
question analysis and the inference process.
The remainder of the paper is organized as fol-
lows. Section 2 describes the overall architecture
of the QA system. Section 3 describes the prac-
tical QA system. Section 4 evaluates the system
and reports the limitation of the QA system. Sec-
tion 5 compares our system with other QA systems.
Section 6 concludes with some directions for future
work.
2 Overall Architecture
The overall framework of the QA system is pre-
sented in Figure 1. The QA system consists of two
major parts; the IE (Information Extractor) engine
and the QA engine.
1We?ve developed the QA system for a TV broadcast sched-
ule domain as well, which is more complex to process than the
weather forecasts QA, but have not evaluated it yet. So, in this
paper, we present the system for weather forecasts only.
QA
Engine
DBMS
IE
Engine
Web Browser
Internet
(WWW)
Web Interface
Speech Processor
QA SYSTEM
Figure 1: Overall architecture of the Question Answering System
The IE engine consists of two parts; a web
crawler and a wrapper. The web crawler down-
loads the selected webpages from the website of the
Korea Meteorological Administration (KMA) every
hour. The website provides current weather con-
ditions and 7 day-forecasts for dozens of Korean
cities. The wrapper, which is a set of extraction
rules, is used to extract weather information from
the webpages . The extracted information is stored
in the database.
TheQA engine performs three-phase processing:
First, it analyzes natural language questions and
translates the questions into Structured Query Lan-
guage (SQL) statements. Second, the SQL queries
are directed to a DBMS to retrieve the answers in
the database. Finally, the result from the DBMS
is converted to natural language sentences for out-
put. Figure 2 depicts overall processes for the QA
engine. A DBMS (Currently, Oracle Database) is
used for managing extracted data. A speech proces-
sor can be merged with the system when it is used
in the home agent robot, which provides speech in-
terface. A web interface is used for providing web-
based QA service with the QA system.
3 A Practical QA System
The question answering starts from extracting
weather information from the web site. The user
request is analyzed with the question analyzer and
the appropriate query frame is selected, and then the
request is translated into the SQL expression. The
SQL query is used to retrieve the correct answer
from the database, which stores weather informa-
tion from the webpages. Finally, natural language
Question
Analyzer
Named Entity
Tagger
Temporal Data
Normalizer
Query Frame
Classifier
Keywords
Natural Language
Question
SQL
Generator
Query
Frame
User Profile
SQL
Query
Inference
Rules
NL Answer
Generator
Query
Result
Natural Language
Answer
QA Engine
DBMS
Figure 2: The QA Engine
answer is generated based on the every result ex-
tracted from the DBMS.
3.1 Information Extraction
The weather information in the webpages is semi-
structured. Semi-structured resources generally do
not employ unrestricted natural language text, but
rather exhibit a fair degree of structure (Kushmer-
ick, 1997). Therefore, information can be accu-
rately and easily extracted from the webpage, com-
pared to IE from unstructured data.
On the other hand, semi-structured resources
are usually formatted for use by people, and con-
tain irrelevant elements that must be ignored, such
as images, advertisements, and HTML formatting
tags (Figure 3). Thus information extraction from
the semi-structured documents is not entirely triv-
ial. Currently, the QA system is using hand-coded
wrappers. However, we are developing an auto-
matic process of constructing wrappers (wrapper in-
duction) for semi-structured resources and that can
detect the modification of the web page design and
adapt the wrapper according to the modification, au-
tomatically, like (Sigletos et al, 2003).
Presently, the IE engine extracts following infor-
mation :
? Current observation: weather summary, visi-
bility, temperature, wind, relative humidity
? 7 days-forecasts : weather summary, forecast
temperature (highest/lowest).
3.2 Question Analysis
First, user?s request is analyzed with the query an-
alyzer as represented in Figure 2. The analyzer ex-
tracts several keywords that describing the question,
such as event word, date, time, and location, by us-
ing a dependency parser, and the user question is
represented only by these extracted keywords.
The named entity tagger is used to identify tem-
poral expressions, place names, and weather events.
The tagger consults the domain-dependent ontology
for recognizing weather events, and the domain-
independent ontology for place names. The ontol-
ogy for the weather events consists of event con-
cepts, which are similar to Synset in WORDNET
(Fellbaum, 1998). For example, rain and umbrella
are in same event concept in the domain ontology
for weather events, because the questions about us-
ing umbrella are usually asking about raining (e.g.
Will I need to bring umbrella tomorrow? and Will it
be raining tomorrow?)
The temporal data normalizer converts temporal
expressions such as today, this weekend and now
into absolute values that can be used in querying to
the database.
Seoul, March. 11., wide spread dust, (-/-)
Seoul, March. 12., cloudy, (0/11)
Seoul, March, 13., Sunny, (1/11)
...
Figure 3: Wrappers extracts weather information
from the semi-structured documents
If the information on date, time, or location is
not expressed in the user?s request, the question an-
alyzer infers the missing information. The infer-
ence rules, which are built based on our observation
on various user questions, are domain-independent,
because the omission of temporal or spatial infor-
mation is common not only in weather information
question, but also in questions for other domains.
The user profile is used for the inference in
query analysis. We observed many people omit the
place name in the weather-domain question. Unlike
the temporal information, it is impossible to guess
the current location without any user information.
Thus, we store some user-related information in the
user profile. Portfolio of stocks or favorite TV pro-
grams can be stored in the user profile if the QA sys-
tem processes queries on stock quote or TV sched-
ule domain.
Let?s take an example of the query analysis. The
following keywords are extracted from the question
?Is it raining??
EVENT : rain
DATE : 03/12/04
TIME : 02:20
CITY : Seoul
Even though the time, date, and city is not explic-
itly mentioned in the question, the question analyzer
infers the information with the user profile and the
inference rules.
3.3 Query Frame Decision
Restricting the question domain and information re-
source, we could restrict the scope of user request.
That is, there is a finite number of expected ques-
tion topics. Each expected question topic is defined
as a single query frame. The following are query
frame examples. They are used for processing the
query for the precipitation forecast for the next day,
diurnal range of today, current wind direction, and
current temperature, respectively.
[PRECIPITATION_TOMORROW]
[DIURNALRANGE_TODAY]
[WINDDIRECTION_CURRENT]
[TEMPERATURE_CURRENT]
Each frame has a rule for SQL generation. PRE-
CIPITATION TOMORROW has the following
SQL generation rule.
[PRECIPITATION_TOMORROW]
SELECT date, amprecpr, pmprecpr FROM
forecast tbl WHERE $date $city
date, amprecpr and pmprecpr are field names in the
database table forecast tbl, which mean date, the
precipitation probability of morning and afternoon
of the day. The rule generates the SQL statement
that means: retrieve the precipitation probability of
tomorrow morning and afternoon from the DB table
which stores forecast information.
Here is another example, which is the SQL gen-
eration rule for [DIURNALRANGE TODAY].
[DIURNALRANGE_TODAY]
SELECT city, max(temp)-main(temp) FROM
current tbl WHERE $date $city group by city
Analyzing a question means selecting a query
frame in this system. Thus, it is important to se-
lect the appropriate query frame for the user request.
The selection process is a great influence on the pre-
cision of the system, while there is not much likeli-
hood of errors in other processes, such as generating
SQL query from the selected query frame, retriev-
ing DB records, and generating an answer.
As represented in Figure 2, the extracted event,
temporal and spatial keywords are used for selecting
an appropriate query frame. Currently, we are us-
ing a hand-coded decision tree-like classifier for se-
lecting an appropriate query frame for the extracted
keywords. If a question isn?t proper for the handling
Is it raining?
?
EVENT : rain
DATE : 03/12/04
TIME : 02:20
CITY : Seoul
?
The frame [RAIN_CURRENT] is selected.
?
SELECT time, city, weather FROM current tbl
WHERE time=?03/12/04/0200?, city=?Seoul?
Figure 4: Interpreting the natural language question
to the SQL query
SELECT time, city, weather FROM current tbl
WHERE time=?03/12/04/0200?, city=?Seoul?
?
DBMS returns ?03/12/04/0200 Seoul Sunny?
?
On 2:00 p.m., Seoul is sunny.
Figure 5: Answer generation from the result of
query
domain, the classifier rejects it. Machine learned
classifier is being developed in order to substitute
for the hand-coded classifier.
3.4 SQL Generation
If a query frame is selected for a question, an SQL
query statement is generated from the SQL pro-
duction rule of the frame. The query is sent to
the DBMS to acquire the records that match to the
query. Figure 4 depicts the conversion from a natu-
ral language question to its SQL expression.
3.5 Answer Generation
Based on the result of the DBMS, a natural lan-
guage answer is generated. We use a rule based
answer generation method. Each query frame has
an answer generation pattern for the frame. For
example, DIURNALRANGE TODAY has the
following generation pattern.
[DIURNALRANGE_TODAY]
The diurnal temperature range of $date($1) is $2?C
$1 and $2 are the the first and second field value of
the queried result. $date() is the function that con-
verts a normalized date expression to a natural lan-
guage expression. Figure 5 shows the answer gener-
ated from the SQL query shown in Figure 4 (More
sample outputs from the QA System are presented
on the Appendix) .
4 Evaluation and Limitation
We have evaluated our domain restricted QA sys-
tem based on precision and recall, and investigated
the limitation of the our approach to the restricted-
domain QA system.
For evaluation, we?ve collected 50 weather ques-
tions from 10 graduate students. Precision and re-
call rates are 90.9 % and 75.0% respectively.
The low recall rate is due to some questions re-
lated to invalid date and topic. The system provides
weather forecasts for 7 days from the querying day.
But some of queries are asking for a future weather
outlook which is out of range ( e.g. Will it be very
hot summer this year? or Will it be snow on this
Christmas?). Some questions asked the information
that the database doesn?t contain, such as UVI (ul-
traviolet index).
The primary reason for the wrong answer is the
failure of invalid topic rejection. It is due to the
insufficient of weather-domain ontology data. For
example, from the question What is the discom-
fort index calculated from the today?s weather?,
the keyword discomfort index was not extracted
while weather was extracted, because the former
was not in the ontology. So the query frame
WEATHER TODAY was misselected and the sys-
tem generated the wrong answer Seoul will be sunny
on March 9th 2004.
An error was caused by the flaw of our keyword-
based query frame decision approach. For the ques-
tion Can the flight for Jeju Island take off today?,
the extracted keywords are
EVENT : flight take_off
DATE : 03/12/04
CITY : Jeju
In order to know whether the flight can take off
or not, the weather information of the departure city
instead of the destination city (i.e. Jeju) should be
returned, but our keyword based approach failed to
make an appropriate query. To solve this problem,
more sophisticated semantic representation, rather
than the sequence of keywords, is required for the
question.
5 Related Works
In this section, we compare our system with other
QA-related approaches and briefly describe the dis-
tinctive characteristics of our system. Open-domain
QA systems in QA track mostly extract answers
from unstructrued documents. In the contrast, our
system extracts answers from semi-structured web
pages, which are pre-selected by us, because our
system aims to achieve high precision with the sac-
rifice of the coverage of questions.
Natural language front ends for databases
(Copestake and Jones, 1990) and our system handle
user questions similarly. However, our system has
information extraction part that makes the database
be updated regularly and automatically. Moreover,
our system returns natural language responses to
users.
The START system (Katz, 1997) is a web-based
QA system. It uses World Wide Web as knowledge
resource. Unstructured natural language sentences
are indexed in the form of ternary expressions and
stored in RDB. The START system covers much
wider domain of questions than ours, however, it
seems that the system returns more wrong answers
than ours, because we extract the answer only from
semi-structured documents.
The Jupiter system (Zue et al, 2000) is a con-
versational system that provides weather informa-
tion over the phone. Based on the Galaxy architec-
ture (Goddeau et al, 1994), Jupiter recognizes user
question over the phone, parses the question with
the TINA language understanding system (Seneff,
1992) and generates SQL and natural language an-
swer with the GENESIS system (Baptist and Sen-
eff, 2000). The generated answer is synthesized
with the ENVOICE system. Even the Jupiter system
deals with the same domain, ours can process a bit
wider-range of weather topics. Our QA system can
cover the question which requires inferences such
as When is the best day for washing my car in this
week? Moreover, our system has an ability of infer-
ring missing information from the user profile and
the inferring algorithm.
6 Conclusion
This paper describes the practical QA system for re-
stricted domains. To be practically used, our sys-
tem tries to achieve high precision at the sacrifice of
question coverage.
To achieve high accuracy, we pre-designate semi-
structured information resource webpages and ex-
tracted domain-specific information from them. We
also prepare a domain-specific ontology and query
frames for the question analysis. The user?s request
in natural language is converted into SQL expres-
sion to generate an answer for the question. Testing
with a small set of queries on weather domain, the
QA system showed 90.9% of precision and 75.0%
of recall. By restricting the coverage of questions,
our system could achieve relatively high precision.
However, the figures are not enough for a real prac-
tical system.
Question 
Analyzer
for Domain 2
Question 
Analyzer
for Domain 1
Question 
Analyzer
for Domain n
Domain
Classifier
SQL Generator
for Domain 1
SQL Generator
for Domain 2
SQL Generator
for Domain n
.
.
.
.
.
.
.
.
.
Natural Language
Question
Query Frame
Classifier for
Domain 1
Query Frame
Classifier for
Domain 2
Query Frame
Classifier for
Domain n
QA Engine
for Domain 1
QA Engine
for Domain 2
QA Engine
for Domain n
Figure 6: A domain classifier selects a proper re-
stricted domain QA engine
Much work is left for our future work. First,
we are expanding the domain for the system. A
domain classifier will be added to the QA sys-
tem to process multiple-domain questions, as rep-
resented in Figure 6. We will separate domain de-
pendent resources (query frames, ontology contain-
ing domain-dependent information, and etc.) and
domain independent resources (linguistic resources,
and ontology for domain-independent information)
to allow easier domain expansion.
Second, the information extractor has to be up-
graded. Currently, the QA system is using hand-
coded wrappers, and the wrappers cannot extract
necessary information robustly when the webpages
are modified. We are developing an information ex-
tractor that can recognize the modification of the
webpages and modify the wrappers automatically.
The upgraded information extractor will improve
the robustness of our system.
Finally, we will increase the size of ontology to
cover more question types. From the experimenta-
tion, we realize that a larger ontology for weather
terms is necessary to classify a question correctly.
It seems more query frames are necessary for more
proper answers to the users? requests.
References
L. Baptist and S. Seneff. 2000. Genesis-II: A ver-
stile system for language generation in conversa-
tional system applications. In Proceedings of In-
ternational Conference on Spoken Language Pro-
cessing, October.
A. Copestake and K. Sparck Jones. 1990. Natural
language interfaces to databases. The Knowledge
Engineering Review, 5(4):225?249.
C. Fellbaum. 1998. WordNet: an Electronic Lexi-
cal Database. The MIT Press.
D. Goddeau, E. Brill, J. Glass., C. Pao, M. Phillips,
J. Polifrini, S. Seneff, and V. Zue. 1994. Galaxy:
A human language interface to on-line travel in-
formation. In Proceedings of International Con-
ference on Spoken Language Processing, pages
707?710.
B. Katz. 1997. Annotating the World Wide Web
using natural language. In Proceedings of the 5th
RIAO conference on Computer Assisted Informa-
tion Searching on the Internet.
N. Kushmerick. 1997. Wrapper Inductin for In-
formation Extraction. Ph.D. thesis, University of
Washington.
M. Light, A. Ittycheriah, A. Latto, and N. Mac-
Cracken. 2003. Reuse in question answering: A
preliminary study. In New Directions in Question
Answering: Papers from the 2003 AAAI Sympo-
sium, pages 78?86.
S. Seneff. 1992. Tina: A natural language system
for spoken language applications. Computational
Linguistics, 18(1):pp. 61?86.
G. Sigletos, G. Paliouras, C. D. Spyropoulos, and
M. Hatzopoulos. 2003. Mining web sites us-
ing wrapper induction, named entities and post-
processing. In 1st European Web Mining Forum
Workshop at ECML/PKDD-2003.
Ellen .M. Voorhees. 2004. Overview of the TREC
2003 question answering track. In Proceedings
of the 12th Text Retrieval Conference.
V. Zue, S. Seneff, J. Glass, J. Polifroni, C. Pao,
T. J. Hazen, and L. Hetherington. 2000. Jupiter:
A telephone-based conversational interface for
weather information. IEEE Transactions on
Speech and Audio Processing, pages 100?112.
Appendix: Example of QA
Q: Is it cold outside?
A: The temperature of Seoul at 1 p.m. is 11? C.
Q: Will it be cold today?
A: The highest temperature of Seoul on March 9th
2004, will be 12? C.
Q: What?s the diurnal temperature range of today?
A: The diurnal temperature range of Seoul is 1?C.
Q: I am planing a picnic to Yongin on these week-
ends, and which day would be better for the
picnic?
A: Yongin will be sunny on March 3rd 2004. Yon-
gin will be partly cloudy on March 4th 2004.
Q: What is the ultraviolet index?
A: The system cannot answer to that topic.
Q: Can I have car wash today?
A: The precipitation probability of Seoul this after-
noon is 10%.
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1048?1056,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Word or Phrase?
Learning Which Unit to Stress for Information Retrieval?
Young-In Song? and Jung-Tae Lee? and Hae-Chang Rim?
?Microsoft Research Asia, Beijing, China
?Dept. of Computer & Radio Communications Engineering, Korea University, Seoul, Korea
yosong@microsoft.com?, {jtlee,rim}@nlp.korea.ac.kr?
Abstract
The use of phrases in retrieval models has
been proven to be helpful in the literature,
but no particular research addresses the
problem of discriminating phrases that are
likely to degrade the retrieval performance
from the ones that do not. In this paper, we
present a retrieval framework that utilizes
both words and phrases flexibly, followed
by a general learning-to-rank method for
learning the potential contribution of a
phrase in retrieval. We also present use-
ful features that reflect the compositional-
ity and discriminative power of a phrase
and its constituent words for optimizing
the weights of phrase use in phrase-based
retrieval models. Experimental results on
the TREC collections show that our pro-
posed method is effective.
1 Introduction
Various researches have improved the quality
of information retrieval by relaxing the tradi-
tional ?bag-of-words? assumption with the use of
phrases. (Miller et al, 1999; Song and Croft,
1999) explore the use n-grams in retrieval mod-
els. (Fagan, 1987; Gao et al, 2004; Met-
zler and Croft, 2005; Tao and Zhai, 2007) use
statistically-captured term dependencies within a
query. (Strzalkowski et al, 1994; Kraaij and
Pohlmann, 1998; Arampatzis et al, 2000) study
the utility of various kinds of syntactic phrases.
Although use of phrases clearly helps, there still
exists a fundamental but unsolved question: Do all
phrases contribute an equal amount of increase in
the performance of information retrieval models?
Let us consider a search query ?World Bank Crit-
icism?, which has the following phrases: ?world
?This work was done while Young-In Song was with the
Dept. of Computer & Radio Communications Engineering,
Korea University.
bank? and ?bank criticism?. Intuitively, the for-
mer should be given more importance than its con-
stituents ?world? and ?bank?, since the meaning
of the original phrase cannot be predicted from
the meaning of either constituent. In contrast, a
relatively less attention could be paid to the lat-
ter ?bank criticism?, because there may be alter-
nate expressions, of which the meaning is still pre-
served, that could possibly occur in relevant docu-
ments. However, virtually all the researches ig-
nore the relation between a phrase and its con-
stituent words when combining both words and
phrases in a retrieval model.
Our approach to phrase-based retrieval is moti-
vated from the following linguistic intuitions: a)
phrases have relatively different degrees of signif-
icance, and b) the influence of a phrase should be
differentiated based on the phrase?s constituents in
retrieval models. In this paper, we start out by
presenting a simple language modeling-based re-
trieval model that utilizes both words and phrases
in ranking with use of parameters that differenti-
ate the relative contributions of phrases and words.
Moreover, we propose a general learning-to-rank
based framework to optimize the parameters of
phrases against their constituent words for re-
trieval models that utilize both words and phrases.
In order to estimate such parameters, we adapt the
use of a cost function together with a gradient de-
scent method that has been proven to be effective
for optimizing information retrieval models with
multiple parameters (Taylor et al, 2006; Metzler,
2007). We also propose a number of potentially
useful features that reflect not only the characteris-
tics of a phrase but also the information of its con-
stituent words for minimizing the cost function.
Our experimental results demonstrate that 1) dif-
ferentiating the weights of each phrase over words
yields statistically significant improvement in re-
trieval performance, 2) the gradient descent-based
parameter optimization is reasonably appropriate
1048
to our task, and 3) the proposed features can dis-
tinguish good phrases that make contributions to
the retrieval performance.
The rest of this paper is organized as follows.
The next section discusses previous work. Section
3 presents our learning-based retrieval framework
and features. Section 4 reports the evaluations of
our techniques. Section 5 finally concludes the pa-
per and discusses future work.
2 Previous Work
To date, there have been numerous researches to
utilize phrases in retrieval models. One of the
most earliest work on phrase-based retrieval was
done by (Fagan, 1987). In (Fagan, 1987), the ef-
fectiveness of proximity-based phrases (i.e. words
occurring within a certain distance) in retrieval
was investigated with varying criteria to extract
phrases from text. Subsequently, various types
of phrases, such as sequential n-grams (Mitra et
al., 1997), head-modifier pairs extracted from syn-
tactic structures (Lewis and Croft, 1990; Zhai,
1997; Dillon and Gray, 1983; Strzalkowski et al,
1994), proximity-based phrases (Turpin and Mof-
fat, 1999), were examined with conventional re-
trieval models (e.g. vector space model). The ben-
efit of using phrases for improving the retrieval
performance over simple ?bag-of-words? models
was far less than expected; the overall perfor-
mance improvement was only marginal and some-
times even inconsistent, specifically when a rea-
sonably good weighting scheme was used (Mitra
et al, 1997). Many researchers argued that this
was due to the use of improper retrieval models
in the experiments. In many cases, the early re-
searches on phrase-based retrieval have only fo-
cused on extracting phrases, not concerning about
how to devise a retrieval model that effectively
considers both words and phrases in ranking. For
example, the direct use of traditional vector space
model combining a phrase weight and a word
weight virtually yields the result assuming inde-
pendence between a phrase and its constituent
words (Srikanth and Srihari, 2003).
In order to complement the weakness, a number
of research efforts were devoted to the modeling
of dependencies between words directly within re-
trieval models instead of using phrases over the
years (van Rijsbergen, 1977; Wong et al, 1985;
Croft et al, 1991; Losee, 1994). Most stud-
ies were conducted on the probabilistic retrieval
framework, such as the BIM model, and aimed on
producing a better retrieval model by relaxing the
word independence assumption based on the co-
occurrence information of words in text. Although
those approaches theoretically explain the relation
between words and phrases in the retrieval con-
text, they also showed little or no improvements
in retrieval effectiveness, mainly because of their
statistical nature. While a phrase-based approach
selectively incorporated potentially-useful relation
between words, the probabilistic approaches force
to estimate parameters for all possible combina-
tions of words in text. This not only brings
parameter estimation problems but causes a re-
trieval system to fail by considering semantically-
meaningless dependency of words in matching.
Recently, a number of retrieval approaches have
been attempted to utilize a phrase in retrieval mod-
els. These approaches have focused to model sta-
tistical or syntactic phrasal relations under the lan-
guage modeling method for information retrieval.
(Srikanth and Srihari, 2003; Maisonnasse et al,
2005) examined the effectiveness of syntactic re-
lations in a query by using language modeling
framework. (Song and Croft, 1999; Miller et al,
1999; Gao et al, 2004; Metzler and Croft, 2005)
investigated the effectiveness of language model-
ing approach in modeling statistical phrases such
as n-grams or proximity-based phrases. Some of
them showed promising results in their experi-
ments by taking advantages of phrases soundly in
a retrieval model.
Although such approaches have made clear dis-
tinctions by integrating phrases and their con-
stituents effectively in retrieval models, they did
not concern the different contributions of phrases
over their constituents in retrieval performances.
Usually a phrase score (or probability) is simply
combined with scores of its constituent words by
using a uniform interpolation parameter, which
implies that a uniform contribution of phrases
over constituent words is assumed. Our study is
clearly distinguished from previous phrase-based
approaches; we differentiate the influence of each
phrase according to its constituent words, instead
of allowing equal influence for all phrases.
3 Proposed Method
In this section, we present a phrase-based retrieval
framework that utilizes both words and phrases ef-
fectively in ranking.
1049
3.1 Basic Phrase-based Retrieval Model
We start out by presenting a simple phrase-based
language modeling retrieval model that assumes
uniform contribution of words and phrases. For-
mally, the model ranks a document D according to
the probability of D generating phrases in a given
query Q, assuming that the phrases occur indepen-
dently:
s(Q;D) = P (Q|D) ?
|Q|?
i=1
P (qi|qhi , D) (1)
where qi is the ith query word, qhi is the head word
of qi, and |Q| is the query size. To simplify the
mathematical derivations, we modify Eq. 1 using
logarithm as follows:
s(Q;D) ?
|Q|?
i=1
log[P (qi|qhi , D)] (2)
In practice, the phrase probability is mixed with
the word probability (i.e. deleted interpolation) as:
P (qi|qhi ,D)??P (qi|qhi ,D)+(1??)P (qi|D) (3)
where ? is a parameter that controls the impact of
the phrase probability against the word probability
in the retrieval model.
3.2 Adding Multiple Parameters
Given a phrase-based retrieval model that uti-
lizes both words and phrases, one would definitely
raise a fundamental question on how much weight
should be given to the phrase information com-
pared to the word information. In this paper, we
propose to differentiate the value of ? in Eq. 3
according to the importance of each phrase by
adding multiple free parameters to the retrieval
model. Specifically, we replace ? with well-
known logistic function, which allows both nu-
merical and categorical variables as input, whereas
the output is bounded to values between 0 and 1.
Formally, the input of a logistic function is a
set of evidences (i.e. feature vector) X generated
from a given phrase and its constituents, whereas
the output is the probability predicted by fitting X
to a logistic curve. Therefore, ? is replaced as fol-
lows:
?(X) = 11 + e?f(X) ? ? (4)
where ? is a scaling factor to confine the output to
values between 0 and ?.
f(X) = ?0 +
|X|?
i=1
?ixi (5)
where xi is the ith feature, ?i is the coefficient pa-
rameter of xi, and ?0 is the ?intercept?, which is
the value of f(X) when all feature values are zero.
3.3 RankNet-based Parameter Optimization
The ? parameters in Eq. 5 are the ones we wish
to learn for resulting retrieval performance via pa-
rameter optimization methods. In many cases, pa-
rameters in a retrieval model are empirically de-
termined through a series of experiments or auto-
matically tuned via machine learning to maximize
a retrieval metric of choice (e.g. mean average
precision). The most simple but guaranteed way
would be to directly perform brute force search
for the global optimum over the entire parame-
ter space. However, not only the computational
cost of this so-called direct search would become
undoubtfully expensive as the number of parame-
ters increase, but most retrieval metrics are non-
smooth with respect to model parameters (Met-
zler, 2007). For these reasons, we propose to adapt
a learning-to-rank framework that optimizes mul-
tiple parameters of phrase-based retrieval models
effectively with less computation cost and without
any specific retrieval metric.
Specifically, we use a gradient descent method
with the RankNet cost function (Burges et al,
2005) to perform effective parameter optimiza-
tions, as in (Taylor et al, 2006; Metzler, 2007).
The basic idea is to find a local minimum of a cost
function defined over pairwise document prefer-
ence. Assume that, given a query Q, there is
a set of document pairs RQ based on relevance
judgements, such that (D1, D2) ? RQ implies
document D1 should be ranked higher than D2.
Given a defined set of pairwise preferences R, the
RankNet cost function is computed as:
C(Q,R) =
?
?Q?Q
?
?(D1,D2)?RQ
log(1 + eY ) (6)
whereQ is the set of queries, and Y = s(Q;D2)?
s(Q;D1) using the current parameter setting.
In order to minimize the cost function, we com-
pute gradients of Eq. 6 with respect to each pa-
rameter ?i by applying the chain rule:
?C
??i =
?
?Q?Q
?
?(D1,D2)?RQ
?C
?Y
?Y
??i (7)
where ?C?Y and ?Y??i are computed as:
?C
?Y =
exp[s(Q;D2)? s(Q;D1)]
1 + exp[s(Q;D2)? s(Q;D1)] (8)
1050
?Y
??i =
?s(Q;D2)
??i ?
?s(Q;D1)
??i (9)
With the retrieval model in Eq. 2 and ?(X),
f(X) in Eq. 4 and 5, the partial derivate of
s(Q;D) with respect to ?i is computed as follows:
?s(Q;D)
??i
=
|Q|?
i=1
xi?(X)(1? ?(X)? )?(P (qi|qhi,D)?P (qi|D))
?(X)P (qi|qhi , D) + (1? ?(X))P (qi|D)
(10)
3.4 Features
We experimented with various features that are
potentially useful for not only discriminating a
phrase itself but characterizing its constituents. In
this section, we report only the ones that have
made positive contributions to the overall retrieval
performance. The two main criteria considered
in the selection of the features are the followings:
compositionality and discriminative power.
Compositionality Features
Features on phrase compositionality are designed
to measure how likely a phrase can be represented
as its constituent words without forming a phrase;
if a phrase in a query has very high composition-
ality, there is a high probability that its relevant
documents do not contain the phrase. In this case,
emphasizing the phrase unit could be very risky in
retrieval. In the opposite case that a phrase is un-
compositional, it is obvious that occurrence of a
phrase in a document can be a stronger evidence
of relevance than its constituent words.
Compositionality of a phrase can be roughly
measured by using corpus statistics or its linguis-
tic characteristics; we have observed that, in many
times, an extremely-uncompositional phrase ap-
pears as a noun phrase, and the distance between
its constituent words is generally fixed within a
short distance. In addition, it has a tendency to be
used repeatedly in a document because its seman-
tics cannot be represented with individual con-
stituent words. Based on these intuitions, we de-
vise the following features:
Ratio of multiple occurrences (RMO): This is a
real-valued feature that measures the ratio of the
phrase repeatedly used in a document. The value
of this feature is calculated as follows:
x =
?
?D;count(wi?whi ,D)>1
count(wi?whi , D)
count(wi ? whi , C) + ?
(11)
where wi ? whi is a phrase in a given query,
count(x, y) is the count of x in y, and ? is a small-
valued constant to prevent unreliable estimation
by very rarely-occurred phrases.
Ratio of single-occurrences (RSO): This is a bi-
nary feature that indicates whether or not a phrase
occurs once in most documents containing it. This
can be regarded as a supplementary feature of
RMO.
Preferred phrasal type (PPT): This feature indi-
cates the phrasal type that the phrase prefers in a
collection. We consider only two cases (whether
the phrase prefers verb phrase or adjective-noun
phrase types) as features in the experiments1.
Preferred distance (PD): This is a binary feature
indicating whether or not the phrase prefers long
distance (> 1) between constituents in the docu-
ment collection.
Uncertainty of preferred distance (UPD): We also
use the entropy (H) of the modification distance
(d) of the given phrase in the collection to measure
the compositionality; if the distance is not fixed
and is highly uncertain, the phrase may be very
compositional. The entropy is computed as:
x = H(p(d = x|wi ? whi)) (12)
where d ? 1, 2, 3, long and all probabilities are
estimated with discount smoothing. We simply
use two binary features regarding the uncertainty
of distance; one indicates whether the uncertainty
of a phrase is very high (> 0.85), and the other
indicates whether the uncertainty is very low (<
0.05)2.
Uncertainty of preferred phrasal type (UPPT): As
similar to the uncertainty of preferred distance, the
uncertainty of the preferred phrasal type of the
phrase can be also used as a feature. We consider
this factor as a form of a binary feature indicating
whether the uncertainty is very high or not.
Discriminative Power Features
In some cases, the occurrence of a phrase can be a
valuable evidence even if the phrase is very likely
to be compositional. For example, it is well known
that the use of a phrase can be effective in retrieval
when its constituent words appear very frequently
in the collection, because each word would have a
very low discriminative power for relevance. On
the contrary, if a constituent word occurs very
1For other phrasal types, significant differences were not
observed in the experiments.
2Although it may be more natural to use a real-valued fea-
ture, we use these binary features because of the two practical
reasons; firstly, it could be very difficult to find an adequate
transformation function with real values, and secondly, the
two intervals at tails were observed to be more important than
the rest.
1051
rarely in the collection, it could not be effective
to use the phrase even if the phrase is highly un-
compositional. Similarly, if the probability that a
phrase occurs in a document where its constituent
words co-occur is very high, we might not need to
place more emphasis on the phrase than on words,
because co-occurrence information naturally in-
corporated in retrieval models may have enough
power to distinguish relevant documents. Based
on these intuitions, we define the following fea-
tures:
Document frequency of constituents (DF): We
use the document frequency of a constituent as
two binary features: one indicating whether the
word has very high document frequency (>10%
of documents in a collection) and the other one
indicating whether it has very low document fre-
quency (<0.2% of documents, which is approxi-
mately 1,000 in our experiments).
Probability of constituents as phrase (CPP): This
feature is computed as a relative frequency of doc-
uments containing a phrase over documents where
two constituent words appear together.
One interesting fact that we observe is that doc-
ument frequency of the modifier is generally a
stronger evidence on the utility of a phrase in re-
trieval than of the headword. In the case of the
headword, we could not find an evidence that it
has to be considered in phrase weighting. It seems
to be a natural conclusion, because the importance
of the modifier word in retrieval is subordinate to
the relation to its headword, but the headword is
not in many phrases. For example, in the case of
the query ?tropical storms?, retrieving a document
only containing tropical can be meaningless, but a
document about storm can be meaningful. Based
on this observation, we only incorporate document
frequency features of syntactic modifiers in the ex-
periments.
4 Experiments
In this section, we report the retrieval perfor-
mances of the proposed method with appropriate
baselines over a range of training sets.
4.1 Experimental Setup
Retrieval models: We have set two retrieval mod-
els, namely the word model and the (phrase-based)
one-parameter model, as baselines. The ranking
function of the word model is equivalent to Eq. 2,
with ? in Eq. 3 being set to zero (i.e. the phrase
probability makes no effect on the ranking). The
ranking function of the one-parameter model is
also equivalent to Eq. 2, with ? in Eq. 3 used ?as
is? (i.e. as a constant parameter value optimized
using gradient descent method, without being re-
placed to a logistic function). Both baseline mod-
els cannot differentiate the importance of phrases
in a query. To make a distinction from the base-
line models, we will name our proposed method
as a multi-parameter model.
In our experiments, all the probabilities in all
retrieval models are smoothed with the collection
statistics by using dirichlet priors (Zhai and Laf-
ferty, 2001).
Corpus (Training/Test): We have conducted
large-scale experiments on three sets of TREC?s
Ad Hoc Test Collections, namely TREC-6, TREC-
7, and TREC-8. Three query sets, TREC-6 top-
ics 301-350, TREC-7 topics 351-400, and TREC-
8 topics 401-450, along with their relevance judg-
ments have been used. We only used the title field
as query.
When performing experiments on each query
set with the one-parameter and the multi-
parameter models, the other two query sets have
been used for learning the optimal parameters. For
each query in the training set, we have generated
document pairs for training by the following strat-
egy: first, we have gathered top m ranked doc-
uments from retrieval results by using the word
model and the one-parameter model (by manually
setting ? in Eq. 3 to the fixed constants, 0 and 0.1
respectively). Then, we have sampled at most r
relevant documents and n non-relevant documents
from each one and generated document pairs from
them. In our experiments, m, r, and n is set to
100, 10, and 40, respectively.
Phrase extraction and indexing: We evaluate
our proposed method on two different types of
phrases: syntactic head-modifier pairs (syntac-
tic phrases) and simple bigram phrases (statisti-
cal phrases). To index the syntactic phrases, we
use the method proposed in (Strzalkowski et al,
1994) with Connexor FDG parser3, the syntactic
parser based on the functional dependency gram-
mar (Tapanainen and Jarvinen, 1997). All neces-
sary information for feature values were indexed
together for both syntactic and statistical phrases.
To maintain indexes in a manageable size, phrases
3Connexor FDG parser is a commercial parser; the demo
is available at: http://www.connexor.com/demo
1052
Test set ? Training set
6 ? 7+8 7 ? 6+8 8 ? 6+7
Model Metric \ Query all partial all partial all partial
Word MAP 0.2135 0.1433 0.1883 0.1876 0.2380 0.2576
(Baseline 1) R-Prec 0.2575 0.1894 0.2351 0.2319 0.2828 0.2990
P@10 0.3660 0.3333 0.4100 0.4324 0.4520 0.4517
One-parameter MAP 0.2254 0.1633? 0.1988 0.2031 0.2352 0.2528
(Baseline 2) R-Prec 0.2738 0.2165 0.2503 0.2543 0.2833 0.2998
P@10 0.3820 0.3600 0.4540 0.4971 0.4580 0.4621
Multi-parameter MAP 0.2293? 0.1697? 0.2038? 0.2105? 0.2452 0.2701
(Proposed) R-Prec 0.2773 0.2225 0.2534 0.2589 0.2891 0.3099
P@10 0.4020 0.3933 0.4540 0.4971 0.4700 0.4828
Table 1: Retrieval performance of different models on syntactic phrases. Italicized MAP values with
symbols ? and ? indicate statistically significant improvements over the word model according to Stu-
dent?s t-test at p < 0.05 level and p < 0.01 level, respectively. Bold figures indicate the best performed
case for each metric.
that occurred less than 10 times in the document
collections were not indexed.
4.2 Experimental Results
Table 1 shows the experimental results of the three
retrieval models on the syntactic phrase (head-
modifier pair). In the table, partial denotes the
performance evaluated on queries containing more
than one phrase that appeared in the document col-
lection4; this shows the actual performance differ-
ence between models. Note that the ranking re-
sults of all retrieval models would be the same as
the result of the word model if a query does not
contain any phrases in the document collection,
because P (qi|qhi , D) would be calculated as zero
eventually. As evaluation measures, we used the
mean average precision (MAP), R-precision (R-
Prec), and precisions at top 10 ranks (P@10).
As shown in Table 1, when a syntactic phrase is
used for retrieval, one-parameter model trained by
gradient-descent method generally performs bet-
ter than the word model, but the benefits are in-
consistent; it achieves approximately 15% and 8%
improvements on the partial query set of TREC-
6 and 7 over the word model, but it fails to show
any improvement on TREC-8 queries. This may
be a natural result since the one-parameter model
is very sensitive to the averaged contribution of
phrases used for training. Compared to the queries
in TREC-6 and 7, the TREC-8 queries contain
more phrases that are not effective for retrieval
4The number of queries containing a phrase in TREC-6,
7, and 8 query set is 31, 34, and 29, respectively.
(i.e. ones that hurt the retrieval performance when
used). This indicates that without distinguishing
effective phrases from ineffective phrases for re-
trieval, the model trained from one training set for
phrase would not work consistently on other un-
seen query sets.
Note that the proposed model outperforms all
the baselines over all query sets; this shows that
differentiating relative contributions of phrases
can improve the retrieval performance of the one-
parameter model considerably and consistently.
As shown in the table, the multi-parameter model
improves by approximately 18% and 12% on the
TREC-6 and 7 partial query sets, and it also
significantly outperforms both the word model
and the one-parameter model on the TREC-8
query set. Specifically, the improvement on the
TREC-8 query set shows one advantage of using
our proposed method; by separating potentially-
ineffective phrases and effective phrases based on
the features, it not only improves the retrieval
performance for each query but makes parameter
learning less sensitive to the training set.
Figure 1 shows some examples demonstrating
the different behaviors of the one-parameter model
and the multi-parameters model. On the figure, the
un-dotted lines indicate the variation of average
precision scores when ? value in Eq. 3 is manu-
ally set. As ? gets closer to 0, the ranking formula
becomes equivalent to the word model.
As shown in the figure, the optimal point of ? is
quiet different from query to query. For example,
in cases of the query ?ferry sinking? and industrial
1053
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?ferry sinking?
varing lambdaone-parametermultiple-parameter
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?industrial espionage?
varing lambdaone-parametermultiple-parameter
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5
A
v
g
P
r
lambda
Performance variation for the query ? declining birth rates?
varing lambdaone-parametermultiple-parameter 0.2
0.25
0.3
0.35
0.4
0.45
0 0.1 0.2 0.3 0.4 0.5
A
v
g
P
r
lambda
Performance variation for the query ?amazon rain forest?
varing lambdaone-parametermultiple-parameter
Figure 1: Performance variations for the queries ?ferry sinking?, ?industrial espionage?, ?declining birth
rate? and ?Amazon rain forest? according to ? in Eq. 3.
espionage? on the upper side, the optimal point is
the value close to 0 and 1 respectively. This means
that the occurrences of the phrase ?ferry sinking?
in a document is better to be less-weighted in
retrieval while ?industrial espionage? should be
treated as a much more important evidence than its
constituent words. Obviously, such differences are
not good for one-parameter model assuming rela-
tive contributions of phrases uniformly. For both
opposite cases, the multi-parameter model signifi-
cantly outperforms one-parameter model.
The two examples at the bottom of Figure 1
show the difficulty of optimizing phrase-based re-
trieval using one uniform parameter. For example,
the query ?declining birth rate? contains two dif-
ferent phrases, ?declining rate? and ?birth rate?,
which have potentially-different effectiveness in
retrieval; the phrase ?declining rate? would not
be helpful for retrieval because it is highly com-
positional, but the phrase ?birth rate? could be a
very strong evidence for relevance since it is con-
ventionally used as a phrase. In this case, we
can get only small benefit from the one-parameter
model even if we find optimal ? from gradient
descent, because it will be just a compromised
value between two different, optimized ?s. For
such query, the multi-parameter model could be
more effective than the one-parameter model by
enabling to set different ?s on phrases accord-
ing to their predicted contributions. Note that the
multi-parameter model significantly outperforms
the one-parameter model and all manually-set ?s
for the queries ?declining birth rate? and ?Amazon
rain forest?, which also has one effective phrase,
?rain forest?, and one non-effective phrase, ?Ama-
zon forest?.
Since our method is not limited to a particular
type of phrases, we have also conducted experi-
ments on statistical phrases (bigrams) with a re-
duced set of features directed applicable; RMO,
RSO, PD5, DF, and CPP; the features requiring
linguistic preprocessing (e.g. PPT) are not used,
because it is unrealistic to use them under bigram-
based retrieval setting. Moreover, the feature UPD
is not used in the experiments because the uncer-
5In most cases, the distance between words in a bigram
is 1, but sometimes, it could be more than 1 because of the
effect of stopword removal.
1054
Test ? Training
Model Metric 6 ? 7+8 7 ? 6+8 8 ? 6+7
Word MAP 0.2135 0.1883 0.2380
(Baseline 1) R-Prec 0.2575 0.2351 0.2828
P@10 0.3660 0.4100 0.4520
One-parameter MAP 0.2229 0.1979 0.2492?
(Baseline 2) R-Prec 0.2716 0.2456 0.2959
P@10 0.3720 0.4500 0.4620
Multi-parameter MAP 0.2224 0.2025? 0.2499?
(Proposed) R-Prec 0.2707 0.2457 0.2952
P@10 0.3780 0.4520 0.4600
Table 2: Retrieval performance of different models, using statistical phrases.
tainty of preferred distance does not vary much for
bigram phrases. The results are shown in Table 2.
The results of experiments using statistical
phrases show that multi-parameter model yields
additional performance improvement against
baselines in many cases, but the benefit is in-
significant and inconsistent. As shown in Table 2,
according to the MAP score, the multi-parameter
model outperforms the one-parameter model on
the TREC-7 and 8 query sets, but it performs
slightly worse on the TREC-6 query set.
We suspect that this is because of the lack
of features to distinguish an effective statistical
phrases from ineffective statistical phrase. In our
observation, the bigram phrases also show a very
similar behavior in retrieval; some of them are
very effective while others can deteriorate the per-
formance of retrieval models. However, in case
of using statistical phrases, the ? computed by our
multi-parameter model would be often similar to
the one computed by the one-parameter model,
when there is no sufficient evidence to differen-
tiate a phrase. Moreover, the insufficient amount
of features may have caused the multi-parameter
model to overfit to the training set easily.
The small size of training corpus could be an an-
other reason. The number of queries we used for
training is less than 80 when removing a query not
containing a phrase, which is definitely not a suf-
ficient amount to learn optimal parameters. How-
ever, if we recall that the multi-parameter model
worked reasonably in the experiments using syn-
tactic phrases with the same training sets, the lack
of features would be a more important reason.
Although we have not mainly focused on fea-
tures in this paper, it would be strongly necessary
to find other useful features, not only for statistical
phrases, but also for syntactic phrases. For exam-
ple, statistics from query logs and the probability
of snippet containing a same phrase in a query is
clicked by user could be considered as useful fea-
tures. Also, the size of the training data (queries)
and the document collection may not be sufficient
enough to conclude the effectiveness of our pro-
posed method; our method should be examined in
a larger collection with more queries. Those will
be one of our future works.
5 Conclusion
In this paper, we present a novel method to differ-
entiate impacts of phrases in retrieval according
to their relative contribution over the constituent
words. The contributions of this paper can be sum-
marized in three-fold: a) we proposed a general
framework to learn the potential contribution of
phrases in retrieval by ?parameterizing? the fac-
tor interpolating the phrase weight and the word
weight on features and optimizing the parameters
using RankNet-based gradient descent algorithm,
b) we devised a set of potentially useful features
to distinguish effective and non-effective phrases,
and c) we showed that the proposed method can be
effective in terms of retrieval by conducting a se-
ries of experiments on the TREC test collections.
As mentioned earlier, the finding of additional
features, specifically for statistical phrases, would
be necessary. Moreover, for a thorough analysis
on the effect of our framework, additional experi-
ments on larger and more realistic collections (e.g.
the Web environment) would be required. These
will be our future work.
1055
References
Avi Arampatzis, Theo P. van der Weide, Cornelis H. A.
Koster, and P. van Bommel. 2000. Linguistically-
motivated information retrieval. In Encyclopedia of
Library and Information Science.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,
Matt Deeds, Nicole Hamilton, and Greg Hullender.
2005. Learning to rank using gradient descent. In
Proceedings of ICML ?05, pages 89?96.
W. Bruce Croft, Howard R. Turtle, and David D. Lewis.
1991. The use of phrases and structured queries in
information retrieval. In Proceedings of SIGIR ?91,
pages 32?45.
Martin Dillon and Ann S. Gray. 1983. Fasit: A
fully automatic syntactically based indexing system.
Journal of the American Society for Information Sci-
ence, 34(2):99?108.
Joel L. Fagan. 1987. Automatic phrase indexing for
document retrieval. In Proceedings of SIGIR ?87,
pages 91?101.
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Gui-
hong Cao. 2004. Dependence language model for
information retrieval. In Proceedings of SIGIR ?04,
pages 170?177.
Wessel Kraaij and Rene?e Pohlmann. 1998. Comparing
the effect of syntactic vs. statistical phrase indexing
strategies for dutch. In Proceedings of ECDL ?98,
pages 605?617.
David D. Lewis and W. Bruce Croft. 1990. Term clus-
tering of syntactic phrases. In Proceedings of SIGIR
?90, pages 385?404.
Robert M. Losee, Jr. 1994. Term dependence: truncat-
ing the bahadur lazarsfeld expansion. Information
Processing and Management, 30(2):293?303.
Loic Maisonnasse, Gilles Serasset, and Jean-Pierre
Chevallet. 2005. Using syntactic dependency and
language model x-iota ir system for clips mono and
bilingual experiments in clef 2005. In Working
Notes for the CLEF 2005 Workshop.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of SIGIR ?05, pages 472?479.
Donald Metzler. 2007. Using gradient descent to opti-
mize language modeling smoothing parameters. In
Proceedings of SIGIR ?07, pages 687?688.
David R. H. Miller, Tim Leek, and Richard M.
Schwartz. 1999. A hidden markov model informa-
tion retrieval system. In Proceedings of SIGIR ?99,
pages 214?221.
Mandar Mitra, Chris Buckley, Amit Singhal, and Claire
Cardie. 1997. An analysis of statistical and syn-
tactic phrases. In Proceedings of RIAO ?97, pages
200?214.
Fei Song and W. Bruce Croft. 1999. A general lan-
guage model for information retrieval. In Proceed-
ings of CIKM ?99, pages 316?321.
Munirathnam Srikanth and Rohini Srihari. 2003. Ex-
ploiting syntactic structure of queries in a language
modeling approach to ir. In Proceedings of CIKM
?03, pages 476?483.
Tomek Strzalkowski, Jose Perez-Carballo, and Mihnea
Marinescu. 1994. Natural language information re-
trieval: Trec-3 report. In Proceedings of TREC-3,
pages 39?54.
Tao Tao and ChengXiang Zhai. 2007. An exploration
of proximity measures in information retrieval. In
Proceedings of SIGIR ?07, pages 295?302.
Pasi Tapanainen and Timo Jarvinen. 1997. A non-
projective dependency parser. In Proceedings of
ANLP ?97, pages 64?71.
Michael Taylor, Hugo Zaragoza, Nick Craswell,
Stephen Robertson, and Chris Burges. 2006. Opti-
misation methods for ranking functions with multi-
ple parameters. In Proceedings of CIKM ?06, pages
585?593.
Andrew Turpin and Alistair Moffat. 1999. Statisti-
cal phrases for vector-space information retrieval. In
Proceedings of SIGIR ?99, pages 309?310.
C. J. van Rijsbergen. 1977. A theoretical basis for the
use of co-occurrence data in information retrieval.
Journal of Documentation, 33(2):106?119.
S. K. M. Wong, Wojciech Ziarko, and Patrick C. N.
Wong. 1985. Generalized vector spaces model in
information retrieval. In Proceedings of SIGIR ?85,
pages 18?25.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to ad hoc information retrieval. In Proceedings of
SIGIR ?01, pages 334?342.
Chengxiang Zhai. 1997. Fast statistical parsing of
noun phrases for document indexing. In Proceed-
ings of ANLP ?97, pages 312?319.
1056
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 430?439,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Mining Name Translations from Entity Graph Mapping?
Gae-won You? Seung-won Hwang? Young-In Song? Long Jiang? Zaiqing Nie?
?Pohang University of Science and Technology, Pohang, Republic of Korea
{gwyou,swhwang}@postech.ac.kr
?Microsoft Research Asia, Beijing, China
{yosong,longj,znie}@microsoft.com
Abstract
This paper studies the problem of mining en-
tity translation, specifically, mining English
and Chinese name pairs. Existing efforts
can be categorized into (a) a transliteration-
based approach leveraging phonetic similar-
ity and (b) a corpus-based approach exploiting
bilingual co-occurrences, each of which suf-
fers from inaccuracy and scarcity respectively.
In clear contrast, we use unleveraged re-
sources of monolingual entity co-occurrences,
crawled from entity search engines, repre-
sented as two entity-relationship graphs ex-
tracted from two language corpora respec-
tively. Our problem is then abstracted as find-
ing correct mappings across two graphs. To
achieve this goal, we propose a holistic ap-
proach, of exploiting both transliteration sim-
ilarity and monolingual co-occurrences. This
approach, building upon monolingual corpora,
complements existing corpus-based work, re-
quiring scarce resources of parallel or compa-
rable corpus, while significantly boosting the
accuracy of transliteration-based work. We
validate our proposed system using real-life
datasets.
1 Introduction
Entity translation aims at mapping the entity names
(e.g., people, locations, and organizations) in source
language into their corresponding names in target
language. While high quality entity translation is es-
sential in cross-lingual information access and trans-
?This work was done when the first two authors visited Mi-
crosoft Research Asia.
lation, it is non-trivial to achieve, due to the chal-
lenge that entity translation, though typically bear-
ing pronunciation similarity, can also be arbitrary,
e.g., Jackie Chan and ? (pronounced Cheng
Long). Existing efforts to address these challenges
can be categorized into transliteration- and corpus-
based approaches. Transliteration-based approaches
(Wan and Verspoor, 1998; Knight and Graehl, 1998)
identify translations based on pronunciation similar-
ity, while corpus-based approaches mine bilingual
co-occurrences of translation pairs obtained from
parallel (Kupiec, 1993; Feng et al, 2004) or compa-
rable (Fung and Yee, 1998) corpora, or alternatively
mined from bilingual sentences (Lin et al, 2008;
Jiang et al, 2009). These two approaches have com-
plementary strength? transliteration-based similar-
ity can be computed for any name pair but cannot
mine translations of little (or none) phonetic simi-
larity. Corpus-based similarity can support arbitrary
translations, but require highly scarce resources of
bilingual co-occurrences, obtained from parallel or
comparable bilingual corpora.
In this paper, we propose a holistic approach,
leveraging both transliteration- and corpus-based
similarity. Our key contribution is to replace the
use of scarce resources of bilingual co-occurrences
with the use of untapped and significantly larger
resources of monolingual co-occurrences for trans-
lation. In particular, we extract monolingual co-
occurrences of entities from English and Chinese
Web corpora, which are readily available from en-
tity search engines such as PeopleEntityCube1, de-
ployed by Microsoft Research Asia. Such engine
1http://people.entitycube.com
430
automatically extracts people names from text and
their co-occurrences to retrieve related entities based
on co-occurrences. To illustrate, Figure 1(a) demon-
strates the query result for ?Bill Gates,? retrieving
and visualizing the ?entity-relationship graph? of re-
lated people names that frequently co-occur with
Bill in English corpus. Similarly, entity-relationship
graphs can be built over other language corpora, as
Figure 1(b) demonstrates the corresponding results
for the same query, from Renlifang2 on ChineseWeb
corpus. From this point on, for the sake of simplic-
ity, we refer to English and Chinese graphs, simply
asGe andGc respectively. Though we illustrate with
English-Chinese pairs in the paper, our method can
be easily adapted to other language pairs.
In particular, we propose a novel approach of ab-
stracting entity translation as a graph matching prob-
lem of two graphsGe andGc in Figures 1(a) and (b).
Specifically, the similarity between two nodes ve
and vc in Ge and Gc is initialized as their transliter-
ation similarity, which is iteratively refined based on
relational similarity obtained from monolingual co-
occurrences. To illustrate this, an English news ar-
ticle mentioning ?Bill Gates? and ?Melinda Gates?
evidences a relationship between the two entities,
which can be quantified from their co-occurrences
in the entire English Web corpus. Similarly, we
can mine Chinese news articles to obtain the re-
lationships between ???? and ???H??
?. Once these two bilingual graphs of people and
their relationships are harvested, entity translation
can leverage these parallel relationships to further
evidence the mapping between translation pairs, as
Figure 1(c) illustrates.
To highlight the advantage of our proposed ap-
proach, we compare our results with commercial
machine translators (1) Engkoo3 developed in Mi-
crosoft Research Asia and (2) Google Translator4.
In particular, Figure 2 reports the precision for two
groups? ?heads? that belong to top-100 popular peo-
ple (determined by the number of hits), among ran-
domly sampled 304 people names5 from six graph
pairs of size 1,000 each, and the remaining ?tails?.
Commercial translators such as Google, leveraging
2http://renlifang.msra.cn
3http://www.engkoo.com
4http://translate.google.com
5See Section 4 for the sampling process.
Ours Google Engkoo0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Pre
cis
ion
 
 
Tail
Head
Figure 2: Comparison for Head and Tail datasets
bilingual co-occurrences that are scarce for tails,
show significantly lower precision for tails. Mean-
while, our work, depending solely on monolin-
gual co-occurrences, shows high precision, for both
heads and tails.
Our focus is to boost translation accuracy for
long tails with non-trivial Web occurrences in each
monolingual corpus, but not with much bilingual co-
occurrences, e.g., researchers publishing actively in
two languages but not famous enough to be featured
in multi-lingual Wikipedia entries or news articles.
As existing translators are already highly accurate
for popular heads, this focus well addresses the re-
maining challenges for entity translation.
To summarize, we believe that this paper has the
following contributions:
? We abstract entity translation problem as
a graph mapping between entity-relationship
graphs in two languages.
? We develop an effective matching algo-
rithm leveraging both pronunciation and co-
occurrence similarity. This holistic approach
complements existing approaches and en-
hances the translation coverage and accuracy.
? We validate the effectiveness of our approach
using various real-life datasets.
The rest of this paper is organized as follows. Sec-
tion 2 reviews existing work. Section 3 then devel-
ops our framework. Section 4 reports experimental
results and Section 5 concludes our work.
431
(a) English PeopleEntityCube Ge (b) Chinese Renlifang Gc
(c) Abstracting translation as graph mapping
Figure 1: Illustration of entity-relationship graphs
2 Related Work
In this section, we first survey related efforts, cate-
gorized into transliteration-based and corpus-based
approaches. Our approach leveraging both is com-
plementary to these efforts.
2.1 Transliteration-based Approaches
Many name translations are loosely based on
phonetic similarity, which naturally inspires
transliteration-based translation of finding the
translation with the closest pronunciation similarity,
using either rule-based (Wan and Verspoor, 1998) or
statistical (Knight and Graehl, 1998; Li et al, 2004)
approaches. However, people are free to designate
arbitrary bilingual names of little (or none) pho-
netic similarity, for which the transliteration-based
approach is not effective.
2.2 Corpus-based Approaches
Corpus-based approach can mine arbitrary transla-
tion pairs, by mining bilingual co-occurrences from
parallel and comparable bilingual corpora. Using
parallel corpora (Kupiec, 1993; Feng et al, 2004),
e.g., bilingual Wikipedia entries on the same per-
son, renders high accuracy but suffers from high
scarcity. To alleviate such scarcity, (Fung and Yee,
432
1998; Shao and Ng, 2004) explore a more vast re-
source of comparable corpora, which share no par-
allel document- or sentence-alignments as in paral-
lel corpora but describe similar contents in two lan-
guages, e.g., news articles on the same event. Al-
ternatively, (Lin et al, 2008) extracts bilingual co-
occurrences from bilingual sentences, such as an-
notating terms with their corresponding translations
in English inside parentheses. Similarly, (Jiang et
al., 2009) identifies potential translation pairs from
bilingual sentences using lexical pattern analysis.
2.3 Holistic Approaches
The complementary strength of the above two ap-
proaches naturally calls for a holistic approach,
such as recent work combining transliteration-
and corpus-based similarity mining bilingual co-
occurrences using general search engines. Specifi-
cally, (Al-Onaizan and Knight, 2002) uses translit-
eration to generate candidates and then web corpora
to identify translations. Later, (Jiang et al, 2007)
enhances to use transliteration to guide web mining.
Our work is also a holistic approach, but leverag-
ing significantly larger corpora, specifically by ex-
ploiting monolingual co-occurrences. Such expan-
sion enables to translate ?long-tail? people entities
with non-trivial Web occurrences in each monolin-
gual corpus, but not much bilingual co-occurrences.
Specifically, we initialize name pair similarity using
transliteration-based approach, and iteratively rein-
forces base similarity using relational similarity.
3 Our Framework
Given two graphsGe = (Ve, Ee) andGc = (Vc, Ec)
harvested from English and Chinese corpora respec-
tively, our goal is to find translation pairs, or a set S
of matching node pairs such that S ? Ve ? Vc. Let
R be a |Ve|-by-|Vc| matrix where each Rij denotes
the similarity between two nodes i ? Ve and j ? Vc.
Overall, with the matrix R, our approach consists
of the following three steps, as we will discuss in the
following three sections respectively:
1. Initialization: computing base translation sim-
ilarities Rij between two entity nodes using
transliteration similarity
2. Reinforcement model: reinforcing the trans-
lation similarities Rij by exploiting the mono-
lingual co-occurrences
3. Matching extraction: extracting the matching
pairs from the final translation similarities Rij
3.1 Initialization with Transliteration
We initialize the translation similarity Rij as the
transliteration similarity. This section explains how
to get the transliteration similarity between English
and Chinese names using an unsupervised approach.
Formally, let an English name Ne =
(e1, e2, ? ? ? , en) and a Chinese name Nc =
(c1, c2, ? ? ? , cm) be given, where ei is an English
word and Ne is a sequence of the words, and ci
is a Chinese character and Nc is a sequence of
the characters. Our goal is to compute a score
indicating the similarity between the pronunciations
of the two names.
We first convert Nc into its Pinyin representation
PYc = (s1, s2, ? ? ? , sm), where si is the Pinyin rep-
resentation of ci. Pinyin is the romanization rep-
resentation of pronunciation of Chinese character.
For example, the Pinyin representation of Ne =
(?Barack?, ?Obama?) is PYc =(?ba?, ?la?, ?ke?,
?ao?, ?ba?, ?ma?). The Pinyin representations of
Chinese characters can be easily obtained from Chi-
nese character pronunciation dictionary. In our ex-
periments, we use an in-house dictionary, which
contains pronunciations of 20, 774 Chinese charac-
ters. For the Chinese characters having multiple pro-
nunciations, we only use the most popular one.
Calculation of transliteration similarity between
Ne and Nc is now transformed to calculation of pro-
nunciation similarity between Ne and PYc. Because
letters in Chinese Pinyins and English strings are
pronounced similarly, we can further approximate
pronunciation similarity between Ne and PYc us-
ing their spelling similarity. In this paper, we use
Edit Distance (ED) to measure the spelling similar-
ity. Moreover, since words in Ne are transliterated
into characters in PYc independently, it is more ac-
curate to compute the ED between Ne and PYc, i.e.,
EDname(Ne, PYc), as the sum of the EDs of all
component transliteration pairs, i.e., every ei in Ne
and its corresponding transliteration (si) in PYc. In
other words, we need to first align all sj?s in PYc
with corresponding ei in Ne based on whether they
433
are translations of each other. Then based on the
alignment, we can calculate EDname(Ne, PYc) us-
ing the following formula.
EDname(Ne, PYc) =
?
i
ED(ei, esi) (1)
where esi is a string generated by concatenating all
si?s that are aligned to ei and ED(ei, esi) is the
Edit Distance between ei and esi, i.e., the mini-
mum number of edit operations (including inser-
tion, deletion and substitution) needed to transform
ei into esi. Because an English word usually con-
sists of multiple syllables but every Chinese charac-
ter consists of only one syllable, when aligning ei?s
with sj?s, we add the constraint that each ei is al-
lowed to be aligned with 0 to 4 si?s but each si can
only be aligned with 0 to 1 ei. To get the align-
ment between PYc and Ne which has the minimal
EDname(Ne, PYc), we use a Dynamic Program-
ming based algorithm as defined in the following
formula:
EDname(N1,ie , PY 1,jc ) = min(
EDname(N1,i?1e , PY 1,jc ) + Len(ei),
EDname(N1,ie , PY 1,j?1c ) + Len(sj),
EDname(N1,i?1e , PY 1,j?1c ) + ED(ei, sj),
EDname(N1,i?1e , PY 1,j?2c ) + ED(ei, PY j?1,jc ),
EDname(N1,i?1e , PY 1,j?3c ) + ED(ei, PY j?2,jc ),
EDname(N1,i?1e , PY 1,j?4c ) + ED(ei, PY j?3,jc ))
where, given a sequence X = (x1, x2, ? ? ?)
such that xi is a word, X i,j is the subsequence
(xi, xi+1, ? ? ? , xj) of X and Len(X) is the number
of letters except spaces in the sequence X . For in-
stance, the minimal Edit Distance between the En-
glish name ?Barack Obama? and the Chinese Pinyin
representation ?ba la ke ao ba ma? is 4, as the
best alignment is: ?Barack? ? ?ba la ke? (ED: 3),
?Obama?? ?ao ba ma? (ED: 1). Finally the translit-
eration similarity between Nc and Ne is calculated
using the following formula.
Simtl(Nc, Ne) = 1?
EDname(Ne, PYc)
Len(PYc) + Len(Ne)
(2)
For example, Simtl(?Barack Obama?, ??n
.???j?) is 1? 411+12 = 0.826.
3.2 Reinforcement Model
From the initial similarity, we model our problem as
an iterative approach that iteratively reinforces the
similarityRij of the nodes i and j from the matching
similarities of their neighbor nodes u and v.
The basic intuition is built on exploiting the sim-
ilarity between monolingual co-occurrences of two
different languages. In particular, we assume two
entities with strong relationship co-occur frequently
in both corpora. In order to express this intuition, we
formally define an iterative reinforcement model as
follows. Let Rtij denote the similarity of nodes i and
j at t-th iteration:
Rt+1ij = ?
?
(u,v)k?Bt(i,j,?)
Rtuv
2k
+ (1? ?)R0ij (3)
The model is expressed as a linear combination
of (a) the relational similarity
?
Rtuv/2k and (b)
transliteration similarity R0ij . (? is the coefficient
for interpolating two similarities.)
In the relational similarity, Bt(i, j, ?) is an or-
dered set of the best matching pairs between neigh-
bor nodes of i and ones of j such that ?(u, v)k ?
Bt(i, j, ?), Rtuv ? ?, where (u, v)k is the match-
ing pair with k-th highest similarity score. We con-
sider (u, v) with similarity over some threshold ?,
or Rtuv ? ?, as a matching pair. In this neighbor
matching process, if many-to-many matches exist,
we select only one with the greatest matching score.
Figure 3 describes such matching process more for-
mally. N(i) andN(j) are the sets of neighbor nodes
of i and j, respectively, and H is a priority queue
sorting pairs in the decreasing order of similarity
scores.
Meanwhile, note that, in order to express that
the confidence for matching (i, j) progressively con-
verges as the number of matched neighbors in-
creases, we empirically use decaying coefficient
1/2k for Rtuv, because
??
k=1 1/2k = 1.
3.3 Matching Extraction
After the convergence of the above model, we get
the |Ve|-by-|Vc| similarity matrix R?. From this
matrix, we extract one-to-one matches maximizing
the overall similarity.
More formally, this problem can be stated as
the maximum weighted bipartite matching (West,
434
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
Bt(i, j, ?)? {}
?u ? N(i),?v ? N(j) : H.push(u, v;Rtuv)
while H is not empty do
(u, v; s)? H.pop()
if s < ? then
break
end if
if neither u nor v are matched yet then
Bt(i, j, ?)? Bt(i, j, ?) ? {(u, v)}
end if
end while
return Bt(i, j, ?)
Figure 3: How to get the ordered set Bt(i, j, ?)
2000)? Given two groups of entities Ve and Vc from
the two graphs Ge and Gc, we can build a weighted
bipartite graph is G = (V,E), where V = Ve ? Vc
and E is a set of edges (u, v) with weight R?uv. To
filter out null alignment, we construct only the edges
with weight R?uv ? ?. From this bipartite graph,
the maximum weighted bipartite matching problem
finds a set of pairwise non-adjacent edges S ? E
such that
?
(u,v)?S R?uv is the maximum. Well-
known algorithms include Hungarian algorithm with
time complexity of O(|V |2 log |V |+ |V ||E|) (West,
2000).
In this paper, to speed up processing, we consider
a greedy alternative with the following steps? (1)
choose the pair with the highest similarity score, (2)
remove the corresponding row and column from the
matrix, and (3) repeat (1) and (2) until their match-
ing scores are over a specific threshold ?.
4 Experiments
This section reports our experimental results to eval-
uate our proposed approach. First, we report our ex-
perimental setting in Section 4.1. Second, we vali-
date the effectiveness and the scalability of our ap-
proach over a real-life dataset in Section 4.2.
4.1 Experimental Settings
This section describes (1) how we collect the En-
glish and Chinese EntityCube datasets, (2) how to
build ground-truth test datasets for evaluating our
framework, and (3) how to set up three parameters
?, ?, and ?.
First, we crawled Ge = (Ve, Ee) and Gc =
(Vc, Ec) from English and Chinese EntityCubes.
Specifically, we built a graph pairs (Ge, Gc) expand-
ing from a ?seed pair? of nodes se ? Ve and sc ? Vc
until the number of nodes for each graph becomes
1,0006. More specifically, when we build a graph
Ge by expanding from se, we use a queue Q. We
first initialize Q by pushing the seed node se. We
then iteratively pop a node ve from Q, save ve into
Ve, and push its neighbor nodes in decreasing order
of co-occurrence scores with ve. Similarly, we can
get Gc from a counterpart seed node vc. By using
this procedure, we built six graph pairs from six dif-
ferent seed pairs. In particular, the six seed nodes
are English names and its corresponding Chinese
names representing a wide range of occupation do-
mains (e.g., ?Barack Obama,? ?Bill Gates,? ?Britney
Spears,? ?Bruno Senna,? ?Chris Paul,? and ?Eminem?)
as Table 1 depicts. Meanwhile, though we demon-
strate the effectiveness of the proposed method for
mining name translations in Chinese and English
languages, the method can be easily adapted to other
language pairs.
Table 1: Summary for graphs and test datasets obtained
from each seed pair
i |Ve|, |Vc| |Ti| English Name Chinese Name
1 1,000 51 Barack Obama ?n.???j
2 1,000 52 Bill Gates ??
3 1,000 40 Britney Spears Y}????
4 1,000 53 Bruno Senna Y0L??
5 1,000 51 Chris Paul .????[
6 1,000 57 Eminem ???
Second, we manually searched for about 50
?ground-truth? matched translations for each graph
pair to build test datasets Ti, by randomly selecting
nodes within two hops7 from the seed pair (se, sc),
since nodes outside two hops may include nodes
whose neighbors are not fully crawled. More specif-
ically, due to our crawling process expanding to add
neighbors from the seed, the nodes close to the seed
have all the neighbors they would have in the full
graph, while those far from the node may not. In or-
der to pick the nodes that well represent the actual
6Note, this is just a default setting, which we later increase
for scalability evaluation in Figure 6.
7Note that the numbers of nodes within two hops in Ge and
Gc are 327 and 399 on average respectively.
435
neighbors, we built test datasets among those within
two hops. However, this crawling is used for the
evaluation sake only, and thus does not suggest the
bias in our proposed framework. Table 1 describes
the size of such test dataset for each graph pair.
Lastly, we set up the three parameters ?, ?, and
? using 6-fold cross validation with 6 test datasets
Ti?s of the graphs. More specifically, for each
dataset Ti, we decide ?i and ?i such that average
MRR for the other 5 test datasets is maximized.
(About MRR, see more details of Equation (4) in
Section 4.2.) We then decide ?i such that average
F1-score is maximized. Figure 4 shows the average
MRR for ?i and ?i with default values ? = 0.66
and ? = 0.2. Based on these results, we set ?i with
values {0.2, 0.15, 0.2, 0.15, 0.2, 0.15} that optimize
MRR in datasets T1, . . . T6, and similarly ?i with
{0.67, 0.65, 0.67, 0.67, 0.65, 0.67}. We also set ?i
with values {0.63, 0.63, 0.61, 0.61, 0.61, 0.61} opti-
mizing F1-score with the same default values ? =
0.2 and ? = 0.66. We can observe the variances
of optimal parameter setting values are low, which
suggests the robustness of our framework.
4.2 Experimental Results
This section reports our experimental results using
the evaluation datasets explained in previous sec-
tion. For each graph pair, we evaluated the ef-
fectiveness of (1) reinforcement model using MRR
measure in Section 4.2.1 and (2) overall framework
using precision, recall, and F1 measures in Sec-
tion 4.2.2. We also validated (3) scalability of our
framework over larger scale of graphs (with up to
five thousand nodes) in Section 4.2.3. (In all experi-
mental results, Bold numbers indicate the best per-
formance for each metric.)
4.2.1 Effectiveness of reinforcement model
We evaluated the reinforcement model over
MRR (Voorhees, 2001), the average of the recipro-
cal ranks of the query results as the following for-
mula:
MRR = 1
|Q|
?
q?Q
1
rankq
(4)
Each q is a ground-truth matched pair (u, v) such
that u ? Ve and v ? Vc, and rankq is the rank of the
similarity score of Ruv among all Ruk?s such that
k ? Vc. Q is a set of such queries. By comparing
MRRs for two matricesR0 andR?, we can validate
the effectiveness of the reinforcement model.
? Baseline matrix (R0): using only the translit-
eration similarity score, i.e., without reinforce-
ment
? Reinforced matrix (R?): using the reinforced
similarity score obtained from Equation (3)
Table 2: MRR of baseline and reinforced matrices
Set MRRBaseline R0 Reinforced R?
T1 0.6964 0.8377
T2 0.6213 0.7581
T3 0.7095 0.7989
T4 0.8159 0.8378
T5 0.6984 0.8158
T6 0.5982 0.8011
Average 0.6900 0.8082
We empirically observed that the iterative model
converges within 5 iterations. In all experiments, we
used 5 iterations for the reinforcement.
Table 2 summarizes our experimental results. As
these figures show, MRR scores significantly in-
crease after applying our reinforcement model ex-
cept for the set T4 (on average from 69% to 81%),
which indirectly shows the effectiveness of our rein-
forcement model.
4.2.2 Effectiveness of overall framework
Based on the reinforced matrix, we evaluated
the effectiveness of our overall matching framework
using the following three measures?(1) precision:
how accurately the method returns matching pairs,
(2) recall: how many the method returns correct
matching pairs, and (3) F1-score: the harmonic
mean of precision and recall. We compared our ap-
proach with a baseline, mapping two graphs with
only transliteration similarity.
? Baseline: in matching extraction, using R0 as
the similarity matrix by bypassing the rein-
forcement step
? Ours: using R?, the similarity matrix con-
verged by Equation (3)
436
0.1 0.15 0.2 0.25 0.30.77
0.78
0.79
0.8
0.81
0.82
0.83
0.84
0.85
?(?=0.66)
AVG
(MR
R)
 
 
?1?2?3?4?5?6
0.61 0.63 0.65 0.67 0.690.74
0.76
0.78
0.8
0.82
0.84
? (?=0.2)
AVG
(MR
R)
 
 
?1
?2
?3
?4
?5
?6
0.57 0.59 0.61 0.63 0.650.68
0.69
0.7
0.71
0.72
0.73
0.74
?(?=0.2, ?=0.66)
AVG
(F1?
scor
e)
 
 
?1?2?3?4?5?6
Figure 4: Parameter setup for ?, ?, and ?
In addition, we compared ours with the machine
translators of Engkoo and Google. Table 3 summa-
rizes our experimental results.
As this table shows, our approach results in the
highest precision (about 80% on average) without
compromising the best recall of Google, i.e., 61%
of Google vs. 63% of ours. Overall, our approach
outperforms others in all three measures.
Meanwhile, in order to validate the translation ac-
curacy over popular head and long-tail, as discussed
in Section 1, we separated the test data into two
groups and analyzed the effectiveness separately.
Figure 5 plots the number of hits returned for the
names from Google search engine. According to the
distribution, we separate the test data into top-100
popular people with the highest hits and the remain-
ing, denoted head and tail, respectively.
0 50 100 150 200 250 300 35010
4
105
106
107
108
Number of names
Nu
mb
er o
f hi
ts i
n G
oog
le
Figure 5: Distribution over number of hits
Table 4 shows the effectiveness with both
datasets, respectively. As difference of the effective-
ness between tail and head (denoted diff ) with re-
spect to three measures shows, our approach shows
stably high precision, for both heads and tails.
4.2.3 Scalability
To validate the scalability of our approach, we
evaluated the effectiveness of our approach over the
number of nodes in two graphs. We built larger six
graph pairs (Ge, Gc) by expanding them from the
seed pairs further until the number of nodes reaches
5,000. Figure 6 shows the number of matched trans-
lations according to such increase. Overall, the num-
ber of matched pairs linearly increases as the num-
ber of nodes increases, which suggests scalability.
The ratio of node overlap in two graphs is about be-
tween 7% and 9% of total node size.
1000 2000 3000 4000 500050
100
150
200
250
300
350
|Ve| and |Vc|
# m
atc
hed
 tra
nsla
tion
s
Figure 6: Matched translations over |Ve| and |Vc|
5 Conclusion
This paper abstracted name translation problem as a
matching problem of two entity-relationship graphs.
This novel approach complements existing name
translation work, by not requiring rare resources
of parallel or comparable corpus yet outperforming
the state-of-the-art. More specifically, we combine
bilingual phonetic similarity and monolingual Web
co-occurrence similarity, to compute a holistic no-
tion of entity similarity. To achieve this goal, we de-
437
Table 3: Precision, Recall, and F1-score of Baseline, Engkoo, Google, and Ours over test sets Ti
Set Precision Recall F1-scoreEngkoo Google Baseline Ours Engkoo Google Baseline Ours Engkoo Google Baseline Ours
T1 0.5263 0.4510 0.5263 0.8974 0.3922 0.4510 0.1961 0.6863 0.4494 0.4510 0.2857 0.7778
T2 0.7551 0.75 0.7143 0.8056 0.7115 0.75 0.2885 0.5577 0.7327 0.75 0.4110 0.6591
T3 0.5833 0.7925 0.5556 0.7949 0.5283 0.7925 0.1887 0.5849 0.5545 0.7925 0.2817 0.6739
T4 0.5 0.45 0.7368 0.7353 0.425 0.45 0.35 0.625 0.4595 0.45 0.4746 0.6757
T5 0.6111 0.3137 0.5 0.7234 0.4314 0.3137 0.1765 0.6667 0.5057 0.3137 0.2609 0.6939
T6 0.5636 0.8947 0.6 0.8605 0.5438 0.8947 0.1053 0.6491 0.5536 0.8947 0.1791 0.74
AVG 0.5899 0.6086 0.6055 0.8028 0.5054 0.6086 0.2175 0.6283 0.5426 0.6086 0.3155 0.7034
Table 4: Precision, Recall, and F1-score of Engkoo, Google, and Ours with head and tail datasets
Method Precision Recall F1-scorehead tail diff head tail diff head tail diff
Engkoo 0.6082 0.5854 0.0229 0.59 0.4706 0.1194 0.5990 0.5217 0.0772
Google 0.75 0.5588 0.1912 0.75 0.5588 0.1912 0.75 0.5588 0.1912
Ours 0.8462 0.7812 0.0649 0.66 0.6127 0.0473 0.7416 0.6868 0.0548
veloped a graph alignment algorithm that iteratively
reinforces the matching similarity exploiting rela-
tional similarity and then extracts correct matches.
Our evaluation results empirically validated the ac-
curacy of our algorithm over real-life datasets, and
showed the effectiveness on our proposed perspec-
tive.
Acknowledgments
This work was supported by Microsoft Research
Asia NLP theme funding and MKE (Ministry of
Knowledge Economy), Korea, under the ITRC (In-
formation Technology Research Center) support
program supervised by the IITA (Institute for In-
formation Technology Advancement) (IITA-2009-
C1090-0902-0045).
References
Yaser Al-Onaizan and Kevin Knight. 2002. Trans-
lating Named Entities Using Monolingual and Bilin-
gual Resources. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguistics
(ACL?02), pages 400?408. Association for Computa-
tional Linguistics.
Donghui Feng, Yajuan Lu?, and Ming Zhou. 2004.
A New Approach for English-Chinese Named En-
tity Alignment. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?04), pages 372?379. Association for Com-
putational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An IR Ap-
proach for Translating New Words from Nonparal-
lel,Comparable Texts. In Proceedings of the 17th In-
ternational Conference on Computational Linguistics
(COLING?98), pages 414?420. Association for Com-
putational Linguistics.
Long Jiang, Ming Zhou, Lee feng Chien, and Cheng Niu.
2007. Named Entity Translation withWebMining and
Transliteration. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence (IJ-
CAI?07), pages 1629?1634. Morgan Kaufmann Pub-
lishers Inc.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and
Qingsheng Zhu. 2009. Mining Bilingual Data from
the Web with Adaptively Learnt Patterns. In Proceed-
ings of the 47th Annual Meeting of the Association for
Computational Linguistics (ACL?09), pages 870?878.
Association for Computational Linguistics.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine Transliteration. Computational Linguistics,
24(4):599?612.
Julian Kupiec. 1993. An Algorithm for finding Noun
Phrase Correspondences in Bilingual Corpora. In Pro-
ceedings of the 31th Annual Meeting of the Association
for Computational Linguistics (ACL?93), pages 17?22.
Association for Computational Linguistics.
Haizhou Li, Zhang Min, and Su Jian. 2004. A Joint
Source-Channel Model for Machine Transliteration.
In Proceedings of the 42nd Annual Meeting on Associ-
ation for Computational Linguistics (ACL?04), pages
159?166. Association for Computational Linguistics.
Dekang Lin, Shaojun Zhao, Benjamin Van Durme, and
Marius Pasca. 2008. Mining Parenthetical Transla-
438
tions from the Web by Word Alignment. In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics (ACL?08), pages 994?
1002. Association for Computational Linguistics.
Li Shao and Hwee Tou Ng. 2004. Mining New Word
Translations from Comparable Corpora. In Proceed-
ings of the 20th International Conference on Computa-
tional Linguistics (COLING?04), pages 618?624. As-
sociation for Computational Linguistics.
Ellen M. Voorhees. 2001. The trec question answering
track. Natural Language Engineering, 7(4):361?378.
Stephen Wan and Cornelia Maria Verspoor. 1998. Auto-
matic English-Chinese Name Transliteration for De-
velopment of Multilingual Resources. In Proceed-
ings of the 17th International Conference on Compu-
tational Linguistics (COLING?98), pages 1352?1356.
Association for Computational Linguistics.
Douglas Brent West. 2000. Introduction to Graph The-
ory. Prentice Hall, second edition.
439
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 650?658,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Comparable Entity Mining from Comparative Questions 
 
 
Shasha Li1?Chin-Yew Lin2?Young-In Song2?Zhoujun Li3 
1National University of Defense Technology, Changsha, China 
2Microsoft Research Asia, Beijing, China 
3Beihang University, Beijing, China 
shashali@nudt.edu.cn1, {cyl,yosong}@microsoft.com2, 
lizj@buaa.edu.cn3 
  
 
Abstract 
Comparing one thing with another is a typical 
part of human decision making process. How-
ever, it is not always easy to know what to 
compare and what are the alternatives. To ad-
dress this difficulty, we present a novel way to 
automatically mine comparable entities from 
comparative questions that users posted on-
line. To ensure high precision and high recall, 
we develop a weakly-supervised bootstrapping 
method for comparative question identification 
and comparable entity extraction by leveraging 
a large online question archive. The experi-
mental results show our method achieves F1-
measure of 82.5% in comparative question 
identification and 83.3% in comparable entity 
extraction. Both significantly outperform an 
existing state-of-the-art method.  
1 Introduction 
Comparing alternative options is one essential 
step in decision-making that we carry out every 
day. For example, if someone is interested in cer-
tain products such as digital cameras, he or she 
would want to know what the alternatives are 
and compare different cameras before making a 
purchase. This type of comparison activity is 
very common in our daily life but requires high 
knowledge skill. Magazines such as Consumer 
Reports and PC Magazine and online media such 
as CNet.com strive in providing editorial com-
parison content and surveys to satisfy this need.  
In the World Wide Web era, a comparison ac-
tivity typically involves: search for relevant web 
pages containing information about the targeted 
products, find competing products, read reviews, 
and identify pros and cons. In this paper, we fo-
cus on finding a set of comparable entities given 
a user?s input entity. For example, given an enti-
ty, Nokia N95 (a cellphone), we want to find 
comparable entities such as Nokia N82, iPhone 
and so on.  
In general, it is difficult to decide if two enti-
ties are comparable or not since people do com-
pare apples and oranges for various reasons.  For 
example, ?Ford? and ?BMW? might be compa-
rable as ?car manufacturers? or as ?market seg-
ments that their products are targeting?, but we 
rarely see people comparing ?Ford Focus? (car 
model) and ?BMW 328i?.   Things also get more 
complicated when an entity has several functio-
nalities. For example, one might compare 
?iPhone? and ?PSP? as ?portable game player? 
while compare ?iPhone? and ?Nokia N95? as 
?mobile phone?. Fortunately, plenty of compara-
tive questions are posted online, which provide 
evidences for what people want to compare, e.g. 
?Which to buy, iPod or iPhone??. We call ?iPod? 
and ?iPhone? in this example as comparators.  In 
this paper, we define comparative questions and 
comparators as: 
 
? Comparative question: A question that in-
tends to compare two or more entities and it 
has to mention these entities explicitly in the 
question. 
? Comparator: An entity which is a target of 
comparison in a comparative question.  
 
According to these definitions, Q1 and Q2 be-
low are not comparative questions while Q3 is. 
?iPod Touch? and ?Zune HD? are comparators. 
 
Q1: ?Which one is better?? 
Q2: ?Is Lumix GH-1 the best camera?? 
Q3: ?What?s the difference between iPod 
Touch and Zune HD?? 
 
The goal of this work is mining comparators 
from comparative questions. The results would 
be very useful in helping users? exploration of 
650
alternative choices by suggesting comparable 
entities based on other users? prior requests.  
To mine comparators from comparative ques-
tions, we first have to detect whether a question 
is comparative or not. According to our defini-
tion, a comparative question has to be a question 
with intent to compare at least two entities. 
Please note that a question containing at least 
two entities is not a comparative question if it 
does not have comparison intent. However, we 
observe that a question is very likely to be a 
comparative question if it contains at least two 
entities. We leverage this insight and develop a 
weakly supervised bootstrapping method to iden-
tify comparative questions and extract compara-
tors simultaneously. 
To our best knowledge, this is the first attempt 
to specially address the problem on finding good 
comparators to support users? comparison activi-
ty. We are also the first to propose using com-
parative questions posted online that reflect what 
users truly care about as the medium from which 
we mine comparable entities. Our weakly super-
vised method achieves 82.5% F1-measure in 
comparative question identification, 83.3% in 
comparator extraction, and 76.8% in end-to-end 
comparative question identification and compa-
rator extraction which outperform the most rele-
vant state-of-the-art method by Jindal & Liu 
(2006b) significantly.   
The rest of this paper is organized as follows. 
The next section discusses previous works. Sec-
tion 3 presents our weakly-supervised method for 
comparator mining. Section 4 reports the evalua-
tions of our techniques, and we conclude the pa-
per and discuss future work in Section 5. 
 
2 Related Work 
2.1 Overview 
In terms of discovering related items for an enti-
ty, our work is similar to the research on recom-
mender systems, which recommend items to a 
user. Recommender systems mainly rely on simi-
larities between items and/or their statistical cor-
relations in user log data (Linden et al, 2003). 
For example, Amazon recommends products to 
its customers based on their own purchase histo-
ries, similar customers? purchase histories, and 
similarity between products. However, recom-
mending an item is not equivalent to finding a 
comparable item. In the case of Amazon, the 
purpose of recommendation is to entice their cus-
tomers to add more items to their shopping carts 
by suggesting similar or related items. While in 
the case of comparison, we would like to help 
users explore alternatives, i.e. helping them make 
a decision among comparable items. 
For example, it is reasonable to recommend 
?iPod speaker? or ?iPod batteries? if a user is 
interested in ?iPod?, but we would not compare 
them with ?iPod?. However, items that are com-
parable with ?iPod? such as ?iPhone? or ?PSP? 
which were found in comparative questions post-
ed by users are difficult to be predicted simply 
based on item similarity between them. Although 
they are all music players, ?iPhone? is mainly a 
mobile phone, and ?PSP? is mainly a portable 
game device. They are similar but also different 
therefore beg comparison with each other. It is 
clear that comparator mining and item recom-
mendation are related but not the same.  
Our work on comparator mining is related to 
the research on entity and relation extraction in 
information extraction (Cardie, 1997; Califf and 
Mooney, 1999; Soderland, 1999; Radev et al, 
2002; Carreras et al, 2003). Specifically, the 
most relevant work is by Jindal and Liu (2006a 
and 2006b) on mining comparative sentences and 
relations. Their methods applied class sequential 
rules (CSR) (Chapter 2, Liu 2006) and label se-
quential rules (LSR) (Chapter 2, Liu 2006) 
learned from annotated corpora to identify com-
parative sentences and extract comparative rela-
tions respectively in the news and review do-
mains. The same techniques can be applied to 
comparative question identification and compa-
rator mining from questions. However, their me-
thods typically can achieve high precision but 
suffer from low recall (Jindal and Liu, 2006b) 
(J&L). However, ensuring high recall is crucial 
in our intended application scenario where users 
can issue arbitrary queries. To address this prob-
lem, we develop a weakly-supervised bootstrap-
ping pattern learning method by effectively leve-
raging unlabeled questions.  
Bootstrapping methods have been shown to be 
very effective in previous information extraction 
research (Riloff, 1996; Riloff and Jones, 1999; 
Ravichandran and Hovy, 2002; Mooney and Bu-
nescu, 2005; Kozareva et al, 2008). Our work is 
similar to them in terms of methodology using 
bootstrapping technique to extract entities with a 
specific relation. However, our task is different 
from theirs in that it requires not only extracting 
entities (comparator extraction) but also ensuring 
that the entities are extracted from comparative 
questions (comparative question identification), 
which is generally not required in IE task. 
651
2.2 Jindal & Liu 2006 
In this subsection, we provide a brief summary 
of the comparative mining method proposed by 
Jindal and Liu (2006a and 2006b), which is used 
as baseline for comparison and represents the 
state-of-the-art in this area.  We first introduce 
the definition of CSR and LSR rule used in their 
approach, and then describe their comparative 
mining method. Readers should refer to J&L?s 
original papers for more details. 
CSR and LSR 
CSR is a classification rule. It maps a sequence 
pattern S(?1?2???) to a class C.  In our problem, 
C is either comparative or non-comparative. 
Given a collection of sequences with class in-
formation, every CSR is associated to two para-
meters: support and confidence. Support is the 
proportion of sequences in the collection contain-
ing S as a subsequence. Confidence is the propor-
tion of sequences labeled as C in the sequences 
containing the S. These parameters are important 
to evaluate whether a CSR is reliable or not. 
LSR is a labeling rule. It maps an input se-
quence pattern ?(?1?2??? ???)  to a labeled 
sequence ??(?1?2? ?? ???) by replacing one to-
ken (??) in the input sequence with a designated 
label (?? ). This token is referred as the anchor. 
The anchor in the input sequence could be ex-
tracted if its corresponding label in the labeled 
sequence is what we want (in our case, a compa-
rator). LSRs are also mined from an annotated 
corpus, therefore each LSR also have two para-
meters: support and confidence. They are simi-
larly defined as in CSR. 
Supervised Comparative Mining Method 
J&L treated comparative sentence identification 
as a classification problem and comparative rela-
tion extraction as an information extraction prob-
lem. They first manually created a set of 83 key-
words such as beat, exceed, and outperform that 
are likely indicators of comparative sentences. 
These keywords were then used as pivots to 
create part-of-speech (POS) sequence data. A 
manually annotated corpus with class informa-
tion, i.e. comparative or non-comparative, was 
used to create sequences and CSRs were mined. 
A Na?ve Bayes classifier was trained using the 
CSRs as features. The classifier was then used to 
identify comparative sentences. 
Given a set of comparative sentences, J&L 
manually annotated two comparators with labels 
$ES1 and $ES2 and the feature compared with 
label $FT for each sentence. J&L?s method was 
only applied to noun and pronoun. To differen-
tiate noun and pronoun that are not comparators 
or features, they added the fourth label $NEF, i.e. 
non-entity-feature. These labels were used as 
pivots together with special tokens li & rj
1 (token 
position), #start (beginning of a sentence), and 
#end (end of a sentence) to generate sequence 
data, sequences with single label only and mini-
mum support greater than 1% are retained, and 
then LSRs were created. When applying the 
learned LSRs for extraction, LSRs with higher 
confidence were applied first. 
J&L?s method have been proved effective in 
their experimental setups. However, it has the 
following weaknesses:  
 
? The performance of J&L?s method relies 
heavily on a set of comparative sentence in-
dicative keywords. These keywords were 
manually created and they offered no guide-
lines to select keywords for inclusion. It is 
also difficult to ensure the completeness of 
the keyword list.  
? Users can express comparative sentences or 
questions in many different ways. To have 
high recall, a large annotated training corpus 
is necessary. This is an expensive process.  
? Example CSRs and LSRs given in Jindal & 
Liu (2006b) are mostly a combination of 
POS tags and keywords. It is a surprise that 
their rules achieved high precision but low 
recall. They attributed most errors to POS 
tagging errors. However, we suspect that 
their rules might be too specific and overfit 
their small training set (about 2,600 sen-
tences). We would like to increase recall, 
avoid overfitting, and allow rules to include 
discriminative lexical tokens to retain preci-
sion. 
 
In the next section, we introduce our method to 
address these shortcomings. 
3 Weakly Supervised Method for Com-
parator Mining 
Our weakly supervised method is a pattern-based 
approach similar to J&L?s method, but it is dif-
ferent in many aspects: Instead of using separate 
CSRs and LSRs, our method aims to learn se-
                                                 
1 li marks a token is at the i
th 
position to the left of the pivot 
and rj marks a token is at j
th position to the right of the 
pivot where i and j are between 1 and 4 in J&L (2006b). 
652
quential patterns which can be used to identify 
comparative question and extract comparators 
simultaneously.  
In our approach, a sequential pattern is defined 
as a sequence S(s1s2? si ? sn) where si can be a 
word, a POS tag, or a symbol denoting either a 
comparator ($C), or the beginning (#start) or the 
end of a question (#end). A sequential pattern is 
called an indicative extraction pattern (IEP) if it 
can be used to identify comparative questions 
and extract comparators in them with high relia-
bility. We will formally define the reliability 
score of a pattern in the next section.  
Once a question matches an IEP, it is classified 
as a comparative question and the token se-
quences corresponding to the comparator slots in 
the IEP are extracted as comparators.  When a 
question can match multiple IEPs, the longest 
IEP is used 2 . Therefore, instead of manually 
creating a list of indicative keywords, we create a 
set of IEPs. We will show how to acquire IEPs 
automatically using a bootstrapping procedure 
with minimum supervision by taking advantage 
of a large unlabeled question collection in the 
following subsections. The evaluations shown in 
section 4 confirm that our weakly supervised 
method can achieve high recall while retain high 
precision. 
This pattern definition is inspired by the work 
of Ravichandran and Hovy (2002). Table 1 
shows some examples of such sequential pat-
terns. We also allow POS constraint on compara-
tors as shown in the pattern ?<, $C/NN or $C/NN 
? #end>?. It means that a valid comparator must 
have a NN POS tag. 
3.1 Mining Indicative Extraction Patterns 
Our weakly supervised IEP mining approach is 
based on two key assumptions:  
 
                                                 
2 It is because the longest IEP is likely to be the most specif-
ic and relevant pattern for the given question. 
 
Figure 1: Overview of the bootstrapping alogorithm  
 
? If a sequential pattern can be used to extract 
many reliable comparator pairs, it is very likely 
to be an IEP.  
? If a comparator pair can be extracted by an 
IEP, the pair is reliable. 
 
Based on these two assumptions, we design 
our bootstrapping algorithm as shown in Figure 1. 
The bootstrapping process starts with a single 
IEP. From it, we extract a set of initial seed com-
parator pairs. For each comparator pair, all ques-
tions containing the pair are retrieved from a 
question collection and regarded as comparative 
questions. From the comparative questions and 
comparator pairs, all possible sequential patterns 
are generated and evaluated by measuring their 
reliability score defined later in the Pattern Eval-
uation section. Patterns evaluated as reliable ones 
are IEPs and are added into an IEP repository.  
Then, new comparator pairs are extracted from 
the question collection using the latest IEPs. The 
new comparators are added to a reliable compa-
rator repository and used as new seeds for pattern 
learning in the next iteration. All questions from 
which reliable comparators are extracted are re-
moved from the collection to allow finding new 
patterns efficiently in later iterations. The 
process iterates until no more new patterns can 
be found from the question collection.  
There are two key steps in our method: (1) 
pattern generation and (2) pattern evaluation. In 
the following subsections, we will explain them 
in details.   
Pattern Generation 
To generate sequential patterns, we adapt the 
surface text pattern mining method introduced in 
(Ravichandran and Hovy, 2002). For any given 
comparative question and its comparator pairs, 
comparators in the question are replaced with 
symbol $Cs. Two symbols, #start and #end, are 
attached to the beginning and the end of a sen-
Sequential Patterns 
<#start which city is better, $C or $C ? #end> 
<, $C or $C ? #end> 
<#start $C/NN or $C/NN ? #end> 
<which NN is better, $C or $C ?> 
<which city is JJR, $C or $C ?>  
<which NN is JJR, $C or $C ?> 
... 
Table 1: Candidate indicative extraction pattern (IEP) 
examples of the question ?which city is better, NYC or 
Paris?? 
 
653
tence in the question. Then, the following three 
kinds of sequential patterns are generated from 
sequences of questions: 
 
 
? Lexical patterns: Lexical patterns indicate 
sequential patterns consisting of only words 
and symbols ($C, #start, and #end). They are 
generated by suffix tree algorithm (Gusfield, 
1997) with two constraints: A pattern should 
contain more than one $C, and its frequency 
in collection should be more than an empiri-
cally determined number ?. 
? Generalized patterns: A lexical pattern can 
be too specific. Thus, we generalize lexical 
patterns by replacing one or more words with 
their POS tags. 2? ? 1 generalized patterns 
can be produced from a lexical pattern con-
taining N words excluding $Cs.  
? Specialized patterns: In some cases, a pat-
tern can be too general. For example, al-
though a question ?ipod or zune?? is com-
parative, the pattern ?<$C or $C>? is too 
general, and there can be many non-
comparative questions matching the pattern, 
for instance, ?true or false??. For this reason, 
we perform pattern specialization by adding 
POS tags to all comparator slots. For exam-
ple, from the lexical pattern ?<$C or $C>? 
and the question ?ipod or zune??, ?<$C/NN 
or $C/NN?>? will be produced as a specia-
lized pattern.  
 
Note that generalized patterns are generated from 
lexical patterns and the specialized patterns are 
generated from the combined set of generalized 
patterns and lexical patterns. The final set of 
candidate patterns is a mixture of lexical patterns, 
generalized patterns and specialized patterns. 
Pattern Evaluation  
According to our first assumption, a reliability 
score ??(??) for a candidate pattern ??  at itera-
tion k can be defined as follows: 
 
?? ?? =
 ?? (????? ? )??? ????
??1
?? (????)
        (1) 
 
, where ??  can extract known reliable comparator 
pairs ??? . ??
??1 indicates the reliable compara-
tor pair repository accumulated until the 
(? ? 1)?? iteration. ??(?) means the number of 
questions satisfying a condition x. The condition 
?? ? ???  denotes that ???  can be extracted from 
a question by applying pattern ??  while the con-
dition ?? ??  denotes any question containing 
pattern ?? .  
However, Equation (1) can suffer from in-
complete knowledge about reliable comparator 
pairs. For example, very few reliable pairs are 
generally discovered in early stage of bootstrap-
ping. In this case, the value of Equation (1) 
might be underestimated which could affect the 
effectiveness of equation (1) on distinguishing 
IEPs from non-reliable patterns. We mitigate this 
problem by a lookahead procedure. Let us denote 
the set of candidate patterns at the iteration k by 
? ? . We define the support ? for comparator pair  
?? ?  which can be extracted by ? 
?   and does not 
exist in the current reliable set:  
 
? ?? ? = ??( ? 
?
? ?? ?)     (2) 
 
where ? ? ? ?? ?  means that one of the patterns in 
? ?  can extract ???  in certain questions. Intuitive-
ly, if  ?? ?  can be extracted by many candidate 
patterns in ? ? , it is likely to be extracted as a 
reliable one in the next iteration. Based on this 
intuition, a pair ???  whose support S is more than 
a threshold ? is regarded as a likely-reliable pair. 
Using likely-reliable pairs, lookahead reliability 
score ?  ??  is defined: 
 
? ? ?? =
 ?? (????? i )??? ???? ???
?
?? (????)
      (3) 
 
, where ?? ???
?  indicates a set of likely-reliable 
pairs based on ? ? .  
By interpolating Equation (1) and (3), the final 
reliability score ?(??)?????
?  for a pattern is de-
fined as follows: 
 
?(??)?????
? = ? ? ?? ?? + (1? ?) ? ? 
?(??)     (4) 
 
Using Equation (4), we evaluate all candidate 
patterns and select patterns whose score is more 
than threshold ? as IEPs. All necessary parame-
ter values are empirically determined. We will 
explain how to determine our parameters in sec-
tion 4. 
4 Experiments 
4.1 Experiment Setup 
Source Data 
All experiments were conducted on about 60M 
questions mined from Yahoo! Answers? question 
title field. The reason that we used only a title 
654
field is that they clearly express a main intention 
of an asker with a form of simple questions in 
general.  
Evaluation Data 
Two separate data sets were created for evalua-
tion. First, we collected 5,200 questions by sam-
pling 200 questions from each Yahoo! Answers 
category3. Two annotators were asked to label 
each question manually as comparative, non-
comparative, or unknown. Among them, 139 
(2.67%) questions were classified as comparative,  
4,934 (94.88%) as non-comparative, and 127 
(2.44%) as unknown questions which are diffi-
cult to assess. We call this set SET-A. 
Because there are only 139 comparative ques-
tions in SET-A, we created another set which 
contains more comparative questions. We ma-
nually constructed a keyword set consisting of 53 
words such as ?or? and ?prefer?, which are good 
indicators of comparative questions. In SET-A, 
97.4% of comparative questions contains one or 
more keywords from the keyword set. We then 
randomly selected another 100 questions from 
each Yahoo! Answers category with one extra 
condition that all questions have to contain at 
least one keyword. These questions were labeled 
in the same way as SET-A except that their com-
parators were also annotated. This second set of 
questions is referred as SET-B. It contains 853 
comparative questions and 1,747 non-
comparative questions. For comparative question 
identification experiments, we used all labeled 
questions in SET-A and SET-B. For comparator 
extraction experiments, we used only SET-B. All 
the remaining unlabeled questions (called as 
SET-R) were used for training our weakly super-
vised method. 
As a baseline method, we carefully imple-
mented J&L?s method. Specifically, CSRs for 
comparative question identification were learned 
from the labeled questions, and then a statistical 
classifier was built by using CSR rules as fea-
tures. We examined both SVM and Na?ve Bayes 
(NB) models as reported in their experiments.  
For the comparator extraction, LSRs were 
learned from SET-B and applied for comparator 
extraction.  
To start the bootstrapping procedure, we ap-
plied the IEP ?<#start nn/$c vs/cc nn/$c ?/. 
#end>? to all the questions in SET-R and ga-
thered 12,194 comparator pairs as the initial 
seeds.  For our weakly supervised method, there 
                                                 
3 There are 26 top level categories in Yahoo! Answers. 
are four parameters, i.e. ?, ?, ?, and ?, need to be 
determined empirically. We first mined all poss-
ible candidate patterns from the suffix tree using 
the initial seeds. From these candidate patterns, 
we applied them to SET-R and got a new set of 
59,410 candidate comparator pairs. Among these 
new candidate comparator pairs, we randomly 
selected 100 comparator pairs and manually clas-
sified them into reliable or non-reliable compara-
tors. Then we found ? that maximized precision 
without hurting recall by investigating frequen-
cies of pairs in the labeled set. By this method, ? 
was set to 3 in our experiments. Similarly, the 
threshold parameters ? and ? for pattern evalua-
tion were set to 10 and 0.8 respectively. For the 
interpolation parameter ?  in Equation (3), we 
simply set the value to 0.5 by assuming that two 
reliability scores are equally important.  
As evaluation measures for comparative ques-
tion identification and comparator extraction, we 
used precision, recall, and F1-measure. All re-
sults were obtained from 5-fold cross validation. 
Note that J&L?s method needs a training data but 
ours use the unlabeled data (SET-R) with weakly 
supervised method to find parameter setting. 
This 5-fold evaluation data is not in the unla-
beled data. Both methods were tested on the 
same test split in the 5-fold cross validation. All 
evaluation scores are averaged across all 5 folds. 
For question processing, we used our own sta-
tistical POS tagger developed in-house4.  
4.2 Experiment Results 
Comparative Question Identification and 
Comparator Extraction 
Table 2 shows our experimental results. In the 
table, ?Identification only? indicates the perfor-
mances in comparative question identification, 
?Extraction only? denotes the performances of 
comparator extraction when only comparative 
questions are used as input, and ?All? indicates 
the end-to-end performances when question 
identification results were used in comparator 
extraction. Note that the results of J&L?s method 
on our collections are very comparable to what is 
reported in their paper.  
In terms of precision, the J&L?s method is 
competitive to our method in comparative ques-
                                                 
4  We used NLC-PosTagger which is developed by NLC 
group of Microsoft Research Asia. It uses the modified 
Penn Treebank POS set for its output; for example, NNS 
(plural nouns), NN (nouns), NP (noun phrases), NPS (plural 
noun phrases), VBZ (verb, present tense, 3rd person singu-
lar), JJ (adjective), RB(adverb), and so on. 
655
tion identification. However, the recall is signifi-
cantly lower than ours. In terms of recall, our 
method outperforms J&L?s method by 35% and 
22% in comparative question identification and 
comparator extraction respectively. In our analy-
sis, the low recall of J&L?s method is mainly 
caused by low coverage of learned CSR patterns 
over the test set.  
In the end-to-end experiments, our weakly su-
pervised method performs significantly better 
than J&L?s method. Our method is about 55% 
better in F1-measure. This result also highlights 
another advantage of our method that identifies 
comparative questions and extracts comparators 
simultaneously using one single pattern. J&L?s 
method uses two kinds of pattern rules, i.e. CSRs 
and LSRs. Its performance drops significantly 
due to error propagations. F1-measure of J&L?s 
method in ?All? is about 30% and 32% worse 
than the scores of ?Identification only? and ?Ex-
traction? only respectively, our method only 
shows small amount of performance decrease 
(approximately 7-8%).  
We also analyzed the effect of pattern genera-
lization and specialization. Table 3 shows the 
results. Despite of the simplicity of our methods, 
they significantly contribute to performance im-
provements. This result shows the importance of 
learning patterns flexibly to capture various 
comparative question expressions. Among the 
6,127 learned IEPs in our database, 5,930 pat-
terns are generalized ones, 171 are specialized 
ones, and only 26 patterns are non-generalized 
and specialized ones.  
To investigate the robustness of our bootstrap-
ping algorithm for different seed configurations, 
we compare the performances between two dif-
ferent seed IEPs. The results are shown in Table 
4. As shown in the table, the performance of our 
bootstrapping algorithm is stable regardless of 
significantly different number of seed pairs gen-
erated by the two IEPs. This result implies that 
our bootstrapping algorithm is not sensitive to 
the choice of IEP.  
Table 5 also shows the robustness of our boot-
strapping algorithm. In Table 5, ?All? indicates 
the performances that all comparator pairs from a 
single seed IEP is used for the bootstrapping, and 
?Partial? indicate the performances using only 
1,000 randomly sampled pairs from ?All?. As 
shown in the table, there is no significant per-
formance difference.  
In addition, we conducted error analysis for 
the cases where our method fails to extract cor-
rect comparator pairs: 
 
? 23.75% of errors on comparator extraction 
are due to wrong pattern selection by our 
simple maximum IEP length strategy.  
? The remaining 67.63% of errors come from 
comparative questions which cannot be cov-
ered by the learned IEPs. 
 
 
 Recall Precision F-score 
Original Patterns 0.689  0. 449 0.544 
+ Specialized 0.731  0.602 0.665 
+ Generalized 0.760  0.776 0.768 
Table 3: Effect of pattern specialization and Generali-
zation in the end-to-end experiments.  
 
Seed patterns # of resulted 
seed pairs 
F-score 
<#start nn/$c vs/cc nn/$c 
?/. #end>  
12,194 0.768 
<#start which/wdt is/vb 
better/jjr , nn/$c or/cc 
nn/$c ?/. #end> 
1,478 0.760 
Table 4: Performance variation over different initial 
seed IEPs in the end-to-end experiments 
 
Set  (# of seed pairs) Recall Precision F-score 
All (12,194) 0.760 0.774 0.768 
Partial (1,000) 0.724 0.763 0.743 
Table 5: Performance variation over different sizes of 
seed pairs generated from a single initial seed IEP 
?<#start nn/$c vs/cc nn/$c ?/. #end>?. 
 
 
Identification only 
(SET-A+SET-B) 
Extraction only 
(SET-B) 
All 
(SET-B) 
J&L (CSR) Our  
Method 
J&L 
(LSR) 
Our  
Method 
J&L Our  
Method SVM NB SVM NB 
Recall 0.601 0.537 0.817* 0.621 0.760* 0.373 0.363 0.760* 
Precision 0.847 0.851 0.833 0.861 0.916* 0.729 0.703 0.776* 
F-score 0.704 0.659 0.825* 0.722 0.833* 0.493 0.479 0.768* 
Table 2: Performance comparison between our method and Jindal and Bing?s Method (denoted as J&L). 
The values with * indicate statistically significant improvements over J&L (CSR) SVM or J&L (LSR) 
according to t-test  at p < 0.01 level. 
 
656
Examples of Comparator Extraction  
By applying our bootstrapping method to the 
entire source data (60M questions), 328,364 
unique comparator pairs were extracted from 
679,909 automatically identified comparative 
questions.  
Table 6 lists top 10 frequently compared enti-
ties for a target item, such as Chanel, Gap, in our 
question archive. As shown in the table, our 
comparator mining method successfully discov-
ers realistic comparators. For example, for ?Cha-
nel?, most results are high-end fashion brands 
such as ?Dior? or ?Louis Vuitton?, while the rank-
ing results for ?Gap? usually contains similar ap-
parel brands for young people, such as ?Old Navy? 
or ?Banana Republic?. For the basketball player 
?Kobe?, most of the top ranked comparators are 
also famous basketball players. Some interesting 
comparators are shown for ?Canon? (the compa-
ny name). It is famous for different kinds of its 
products, for example, digital cameras and prin-
ters, so it can be compared to different kinds of 
companies. For example, it is compared to ?HP?, 
?Lexmark?, or ?Xerox?, the printer manufacturers, 
and also compared to ?Nikon?, ?Sony?, or ?Kodak?, 
the digital camera manufactures.  Besides gener-
al entities such as a brand or company name, our 
method also found an interesting comparable 
entity for a specific item in the experiments. For 
example, our method recommends ?Nikon d40i?, 
?Canon rebel xti?, ?Canon rebel xt?, ?Nikon 
d3000?, ?Pentax k100d?, ?Canon eos 1000d? as 
comparators for the specific camera product ?Ni-
kon 40d?. 
Table 7 can show the difference between our 
comparator mining and query/item recommenda-
tion. As shown in the table, ?Google related 
searches? generally suggests a mixed set of two 
kinds of related queries for a target entity: (1) 
queries specified with subtopics for an original 
query (e.g., ?Chanel handbag? for ?Chanel?) and 
(2) its comparable entities (e.g., ?Dior? for ?Cha-
nel?). It confirms one of our claims that compara-
tor mining and query/item recommendation are 
related but not the same. 
5 Conclusion 
In this paper, we present a novel weakly super-
vised method to identify comparative questions 
and extract comparator pairs simultaneously. We 
rely on the key insight that a good comparative 
question identification pattern should extract 
good comparators, and a good comparator pair 
should occur in good comparative questions to 
bootstrap the extraction and identification 
process. By leveraging large amount of unla-
beled data and the bootstrapping process with 
slight supervision to determine four parameters, 
we found 328,364 unique comparator pairs and 
6,869 extraction patterns without the need of 
creating a set of comparative question indicator 
keywords.  
The experimental results show that our me-
thod is effective in both comparative question 
identification and comparator extraction. It sig-
 Chanel Gap iPod Kobe Canon 
1 Dior Old Navy Zune Lebron Nikon 
2 Louis Vuitton American Eagle mp3 player Jordan Sony 
3 Coach Banana Republic PSP MJ Kodak 
4 Gucci Guess by Marciano cell phone Shaq Panasonic 
5 Prada ACP Ammunition iPhone Wade Casio 
6 Lancome Old Navy brand Creative Zen T-mac Olympus 
7 Versace Hollister Zen Lebron James Hp 
8 LV Aeropostal iPod nano Nash Lexmark 
9 Mac American Eagle outfitters iPod touch KG Pentax 
10 Dooney Guess iRiver Bonds Xerox 
Table 6: Examples of comparators for different entities  
Chanel Gap iPod Kobe Canon 
Chanel handbag Gap coupons iPod nano Kobe Bryant stats Canon t2i 
Chanel sunglass Gap outlet iPod touch Lakers Kobe Canon printers 
Chanel earrings Gap card iPod best buy Kobe espn Canon printer drivers 
Chanel watches Gap careers iTunes Kobe Dallas Mavericks Canon downloads 
Chanel shoes Gap casting call Apple Kobe NBA Canon copiers 
Chanel jewelry Gap adventures iPod shuffle Kobe 2009 Canon scanner 
Chanel clothing Old navy iPod support Kobe san Antonio Canon lenses 
Dior Banana republic iPod classic Kobe Bryant 24 Nikon 
Table 7: Related queries returned by Google related searches for the same target entities in Table 6. The bold 
ones indicate overlapped queries to the comparators in Table 6. 
 
657
nificantly improves recall in both tasks while 
maintains high precision. Our examples show 
that these comparator pairs reflect what users are 
really interested in comparing. 
Our comparator mining results can be used for 
a commerce search or product recommendation 
system. For example, automatic suggestion of 
comparable entities can assist users in their com-
parison activities before making their purchase 
decisions. Also, our results can provide useful 
information to companies which want to identify 
their competitors.  
In the future, we would like to improve extrac-
tion pattern application and mine rare extraction 
patterns. How to identify comparator aliases such 
as ?LV? and ?Louis Vuitton? and how to separate 
ambiguous entities such ?Paris vs. London? as 
location and ?Paris vs. Nicole? as celebrity are 
all interesting research topics. We also plan to 
develop methods to summarize answers pooled 
by a given comparator pair.  
6 Acknowledgement  
This work was done when the first author 
worked as an intern at Microsoft Research Asia. 
References  
Mary Elaine Califf and Raymond J. Mooney. 1999. 
Relational learning of pattern-match rules for in-
formation extraction. In Proceedings of AAAI?99 
/IAAI?99. 
Claire Cardie. 1997. Empirical methods in informa-
tion extraction. AI magazine, 18:65?79.  
Dan Gusfield. 1997. Algorithms on strings, trees, and 
sequences: computer science and computational 
biology. Cambridge University Press, New York, 
NY, USA 
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. 
In Proceedings of WWW ?02, pages 517?526. 
Glen Jeh and Jennifer Widom. 2003. Scaling persona-
lized web search. In Proceedings of WWW ?03, 
pages 271?279. 
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings 
of SIGIR ?06, pages 244?251. 
Nitin Jindal and Bing Liu. 2006b. Mining compara-
tive sentences and relations. In Proceedings of 
AAAI ?06. 
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 
2008. Semantic class learning from the web with 
hyponym pattern linkage graphs. In Proceedings of 
ACL-08: HLT, pages 1048?1056.  
Greg Linden, Brent Smith and Jeremy York. 2003. 
Amazon.com Recommendations: Item-to-Item 
Collaborative Filtering. IEEE Internet Computing, 
pages 76-80.  
Raymond J. Mooney and Razvan Bunescu. 2005. 
Mining knowledge from text using information ex-
traction. ACM SIGKDD Exploration Newsletter, 
7(1):3?10. 
Dragomir Radev, Weiguo Fan, Hong Qi, and Harris 
Wu and Amardeep Grewal. 2002. Probabilistic 
question answering on the web. Journal of the 
American Society for Information Science and 
Technology, pages 408?419. 
Deepak Ravichandran and Eduard Hovy. 2002. 
Learning surface text patterns for a question ans-
wering system. In Proceedings of ACL ?02, pages 
41?47. 
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level 
bootstrapping. In Proceedings of AAAI ?99 
/IAAI ?99, pages 474?479. 
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of 
the 13th National Conference on Artificial Intelli-
gence, pages 1044?1049. 
Stephen Soderland. 1999. Learning information ex-
traction rules for semi-structured and free text. Ma-
chine Learning, 34(1-3):233?272.  
 
658
