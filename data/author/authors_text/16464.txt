NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1?7,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Induction of Frame-Semantic Representations
Ashutosh Modi Ivan Titov Alexandre Klementiev
Saarland University
Saarbru?cken, Germany
{amodi|titov|aklement}@mmci.uni-saarland.de
Abstract
The frame-semantic parsing task is challeng-
ing for supervised techniques, even for those
few languages where relatively large amounts
of labeled data are available. In this prelim-
inary work, we consider unsupervised induc-
tion of frame-semantic representations. An
existing state-of-the-art Bayesian model for
PropBank-style unsupervised semantic role
induction (Titov and Klementiev, 2012) is ex-
tended to jointly induce semantic frames and
their roles. We evaluate the model perfor-
mance both quantitatively and qualitatively by
comparing the induced representation against
FrameNet annotations.
1 Introduction
Shallow representations of meaning, and semantic
role labels in particular, have a long history in lin-
guistics (Fillmore, 1968). In this paper we focus on
frame-semantic representations: a semantic frame is
a conceptual structure describing a situation (or an
entity) and its participants (or its properties). Par-
ticipants and properties are associated with seman-
tic roles (also called frame elements). For example,
following the FrameNet annotation guidelines (Rup-
penhofer et al, 2006), in the following sentences:
(a) [COOK Mary] cooks [FOOD the broccoli]
[CONTAINER in a small pan].
(b) Sautee [FOOD the onions] [MANNER gently ]
[TEMP SETTING on low heat].
the same semantic frame Apply Heat is evoked
by verbs cook and sautee, and roles COOK and
FOOD in the sentence (a) are filled by Mary and
the broccoli, respectively. Note that roles are spe-
cific to the frame, not to the individual lexical units
(verbs cook and sautee, in the example).1
Most approaches to predicting these representa-
tions, called semantic role labeling (SRL), have re-
lied on large annotated datasets (Gildea and Juraf-
sky, 2002; Carreras and Ma`rquez, 2005; Surdeanu
et al, 2008; Hajic? et al, 2009). By far, most of
this work has focused on PropBank-style represen-
tations (Palmer et al, 2005) where roles are defined
for each individual verb, or even individual senses of
a verb. The only exceptions are modifiers and roles
A0 and A1 which correspond to proto-agent (a doer,
or initiator of the action) and proto-patient (an af-
fected entity), respectively. However, the SRL task
is known to be especially hard for the FrameNet-
style representations for a number of reasons, in-
cluding, the lack of cross-frame correspondence for
most roles, fine-grain definitions of roles and frames
in FrameNet, and relatively small amounts of statis-
tically representative data (Erk and Pado, 2006; Das
et al, 2010; Palmer and Sporleder, 2010; Das and
Smith, 2011). Another reason for reduced interest in
predicting FrameNet representations is the lack of
annotated resources for most languages, with anno-
tated corpora available or being developed only for
English (Ruppenhofer et al, 2006), German (Bur-
chardt et al, 2006), Spanish (Subirats, 2009) and
Japanese (Ohara et al, 2004).
Due to scarcity of labeled data, purely unsuper-
vised set-ups recently started to receive considerable
attention (Swier and Stevenson, 2004; Grenager and
Manning, 2006; Lang and Lapata, 2010; Lang and
1More accurately, FrameNet distinguishes core and non-
core roles with non-core roles mostly corresponding to mod-
ifiers, e.g., MANNER in sentence (b). Non-core roles are
expected to generalize across frames.
1
cooks
Mary
the broccoli in a small  pan  
CONTAINER
COOK FOOD
Apply_Heat
Figure 1: An example of a semantic dependency graph.
Lapata, 2011a; Lang and Lapata, 2011b; Titov and
Klementiev, 2012). However, all these approaches
have focused on PropBank-style representations.
This may seem somewhat unnatural as FrameNet
representations, though arguably more powerful, are
harder to learn in the supervised setting, harder to
annotate, and annotated data is available for a con-
siderably fewer languages. This is the gap which we
address in this preliminary study.
More specifically, we extend an existing state-
of-the-art Bayesian model for unsupervised seman-
tic role labeling and apply it to support FrameNet-
style semantics. In other words, our method jointly
induces both frames and frame-specific semantic
roles. We experiment only with verbal predicates
and evaluate the performance of the model with re-
spect to some natural baselines. Though the scores
for frame induction are not high, we argue that this is
primarily due to very high granularity of FrameNet
frames which is hard to reproduce for unsupervised
systems, as the implicit supervision signal is not ca-
pable of providing these distinctions.
2 Task Definition
In this work, we use dependency representations
of frame semantics. Dependency representations
for SRL (Johansson and Nugues, 2008) were made
popular by CoNLL-2008 and CoNLL-2009 shared
tasks (Surdeanu et al, 2008; Hajic? et al, 2009), but
for English were limited to PropBank. Recently,
English FrameNet was also released in the depen-
dency format (Bauer et al, 2012). Instead of pre-
dicting argument spans, in dependency representa-
tion the goal is, roughly, to predict the syntactic head
of the argument. The semantic dependency repre-
sentation for sentence (a) is shown in Figure 1, la-
bels on edges denote roles and labels on words de-
note frames. Note that in practice the structures can
be more complex, as, for example, arguments can
evoke their own frames or the same arguments can
be shared by multiple predicates, as in right node
raising constructions.
The SRL task, or more specifically frame-
semantic parsing task consists, at least conceptually,
of four stages: (1) identification of frame-evoking
elements(FEE), (2) identification of arguments, (3)
frame labeling and (4) role labeling. In this work,
we focus only on the frame labeling and role label-
ing stages, relying on gold standard (i.e. the oracle)
for FEEs and role identification. In other words, our
goal is to label (or cluster) edges and nodes in the
dependency graph, Figure 1. Since we focus in this
study on verbal predicates only, the first stage would
be trivial and the second stage could be handled with
heuristics as in much of previous work on unsuper-
vised SRL (Lang and Lapata, 2011a; Titov and Kle-
mentiev, 2012).
Additionally to considering only verbal predi-
cates, we also assume that every verb belongs to
a single frame. This assumption, though restric-
tive, may be reasonable in practice as (a) the dis-
tributions across frames (i.e. senses) are gener-
ally highly skewed, (b) current state-of-the-art tech-
niques for word-sense induction hardly beat most-
frequent-sense baselines in accuracy metrics (Man-
andhar et al, 2010). This assumption, or its minor
relaxations, is relatively standard in work on unsu-
pervised semantic parsing tasks (Poon and Domin-
gos, 2009; Poon and Domingos, 2010; Titov and
Klementiev, 2011). From the modeling prospective,
there are no major obstacles to relaxing this assump-
tion, but it would lead to a major explosion of the
search space and, as a result, slow inference.
3 Model and Inference
We follow previous work on unsupervised seman-
tic role labeling (Lang and Lapata, 2011a; Titov
and Klementiev, 2012) and associate arguments with
their frame specific syntactic signatures which we
refer to as argument keys:
? Active or passive verb voice (ACT/PASS).
? Argument position relative to predicate
(LEFT/RIGHT).
? Syntactic relation to its governor.
? Preposition used for argument realization.
Semantic roles are then represented as clusters of
argument keys instead of individual argument occur-
rences. This representation aids our models in in-
ducing high purity clusters (of argument keys) while
2
reducing their granularity. Thus, if an argument key
k is assigned to a role r (k ? r), all of its occurrences
are labeled r.
3.1 A model for frame-semantic parsing
Our approach is similar to the models of Titov and
Klementiev (2012; 2011). Please, see Section 5 for
a discussion of the differences.
Our model encodes three assumptions about
frames and semantic roles. First, we assume that
the distribution of lexical units (verbal predicates)
is sparse for each semantic frame. Second, we en-
force the selectional restriction assumption: we as-
sume that the distribution over potential argument
fillers is sparse for every role, implying that ?peaky?
distributions of arguments for each role r are pre-
ferred to flat distributions. Third, each role normally
appears at most once per predicate occurrence. Our
inference will search for a frame and role clustering
which meets the above requirements to the maximal
extent.
Our model associates three distributions with each
frame. The first one (?) models the selection of lex-
ical units, the second (?) governs the selection of ar-
gument fillers for each semantic role, and the third
(?) models (and penalizes) duplicate occurrence of
roles. Each frame occurrence is generated indepen-
dently given these distributions. Let us describe the
model by first defining how the set of model param-
eters and an argument key clustering are drawn, and
then explaining the generation of individual frame
instances. The generative story is formally presented
in Figure 2.
For each frame, we begin by drawing a dis-
tribution of its lexical units from a DP prior
DP (?, H(P )) with a small concentration parame-
ter ?, and a base distribution H(P ), pre-computed as
normalized counts of all verbs in our dataset. Next,
we generate a partition of argument keys Bf from
CRP(?) with each subset r ? Bf representing a sin-
gle frame specific semantic role. The crucial part
of the model is the set of selectional preference pa-
rameters ?f,r, the distributions of arguments x for
each role r of frame f . We represent arguments by
lemmas of their syntactic heads.2 In order to encode
2For prepositional phrases, we take as head the head noun of
the object noun phrase as it encodes crucial lexical information.
However, the preposition is not ignored but rather encoded in
the assumption about sparseness of the distributions
?f,r, we draw them from the DP prior DP (?, H(A))
with a small concentration parameter ?, the base
probability distribution H(A) is just the normalized
frequencies of arguments in the corpus. Finally,
the geometric distribution ?f,r is used to model the
number of times a role r appears with a given frame
occurrence. The decision whether to generate at
least one role r is drawn from the uniform Bernoulli
distribution. If 0 is drawn then the semantic role is
not realized for the given occurrence, otherwise the
number of additional roles r is drawn from the ge-
ometric distribution Geom(?f,r). The Beta priors
over ? indicate the preference towards generating at
most one argument for each role.
Now, when parameters and argument key cluster-
ings are chosen, we can summarize the remainder of
the generative story as follows. We begin by inde-
pendently drawing occurrences for each frame. For
each frame occurrence, we first draw its lexical unit.
Then for each role we independently decide on the
number of role occurrences. Then we generate each
of the arguments (seeGenArgument in Figure 2) by
generating an argument key kf,r uniformly from the
set of argument keys assigned to the cluster r, and fi-
nally choosing its filler xf,r, where the filler is either
a lemma or the syntactic head of the argument.
3.2 Inference
We use a simple approximate inference algo-
rithm based on greedy search for the maximum a-
posteriori clustering of lexical units and argument
keys. We begin by assigning each verbal predi-
cate to its own frame, and then iteratively choose
a pair of frames and merge them. Note that each
merge involves inducing a new set of roles, i.e. a
re-clustering of argument keys, for the new merged
frame. We use the search procedure proposed in
(Titov and Klementiev, 2012), in order to cluster ar-
gument keys for each frame.
Our search procedure chooses a pair of frames to
merge based on the largest incremental change to the
objective due to the merge. Computing the change
involves re-clustering of argument keys, so consider-
ing all pairs of initial frames containing single verbal
predicates is computationally expensive. Instead, we
the corresponding argument key.
3
Parameters:
for each frame f = 1, 2, . . . :
?f ? DP (?, H(P )) [distrib of lexical units]
Bf ? CRP (?) [partition of arg keys]
for each role r ? Bf :
?f,r ? DP (?, H(A)) [distrib of arg fillers]
?f,r ? Beta(?0, ?1) [geom distr for dup roles]
Data Generation:
for each frame f = 1, 2, . . . :
for each occurrence of frame f :
p ? ?f [draw a lexical unit]
for every role r ? Bf :
if [n ? Unif(0, 1)] = 1: [role appears at least once]
GenArgument(f, r) [draw one arg]
while [n ? ?f,r] = 1: [continue generation]
GenArgument(f, r) [draw more args]
GenArgument(f, r):
kf,r ? Unif(1, . . . , |r|) [draw arg key]
xf,r ? ?f,r [draw arg filler]
Figure 2: Generative story for the frame-semantic parsing
model.
prune the space of possible pairs of verbs using a
simple but effective pre-processing step. Each verb
is associated with a vector of normalized aggregate
corpus counts of syntactic dependents of the verb
(ignoring the type of dependency relation). Cosine
similarity of these vectors are then used to prune the
pairs of verbs so that only verbs which are distribu-
tionally similar enough are considered for a merge.
Finally, the search terminates when no additional
merges result in a positive change to the objective.
4 Experimental Evaluation
4.1 Data
We used the dependency representation of the
FrameNet corpus (Bauer et al, 2012). The corpus is
automatically annotated with syntactic dependency
trees produced by the Stanford parser. The data con-
sists of 158,048 sentences with 3,474 unique verbal
predicates and 722 gold frames.
4.2 Evaluation Metrics
We cannot use supervised metrics to evaluate our
models, since we do not have an alignment between
gold labels and clusters induced in the unsupervised
setup. Instead, we use the standard purity (PU) and
collocation (CO) metrics as well as their harmonic
mean (F1) to measure the quality of the resulting
clusters. Purity measures the degree to which each
cluster contains arguments (verbs) sharing the same
gold role (gold frame) and collocation evaluates the
degree to which arguments (verbs) with the same
gold roles (gold frame) are assigned to a single clus-
ter, see (Lang and Lapata, 2010). As in previous
work, for role induction, the scores are first com-
puted for individual predicates and then averaged
with the weights proportional to the total number oc-
currences of roles for each predicate.
4.3 Model Parameters
The model parameters were tuned coarsely by visual
inspection: ? = 1.e-5, ? = 1.e-4, ? = 1, ?0 = 100,
?1 = 1.e-10. Only a single model was evaluated
quantitatively to avoid overfitting to the evaluation
set.
4.4 Qualitative Evaluation
Our model induced 128 multi-verb frames from the
dataset. Out of 78,039 predicate occurrences in the
data, these correspond to 18,963 verb occurrences
(or, approximately, 25%). Some examples of the
induced multi-verb frames are shown in Table 1.
As we can observe from the table, our model clus-
ters semantically related verbs into a single frame,
even though they may not correspond to the same
gold frame in FrameNet. Consider, for example, the
frame (ratify::sign::accede): the verbs are semanti-
cally related and hence they should go into a single
frame, as they all denote a similar action.
Another result worth noting is that the model of-
ten clusters antonyms together as they are often used
in similar context. For example, consider the frame
(cool::heat::warm), the verbs cool, heat and warm,
all denote a change in temperature. This agrees well
with annotation in FrameNet. Similarly, we clus-
ter sell and purchase together. This contrasts with
FrameNet annotation as FrameNet treats them not
as antonyms but as different views on same situation
and according to their guidelines, different frames
are assigned to different views.
Often frames in FrameNet correspond to more
fine-grained meanings of the verbs, as we can see
in the example for (plait::braid::dye). The three de-
scribe a similar activity involving hair but FrameNet
4
Induced frames FrameNet frames corresponding to the verbs
(rush::dash::tiptoe) rush : [Self motion](150) [Fluidic motion](19)
dash : [Self motion](100)
tiptoe : [Self motion](114)
(ratify::sign::accede) ratify : [Ratification](41)
sign : [Sign agreement](81) [Hiring](18) [Text Creation](1)
accede : [Sign Agreement](31)
(crane::lean::bustle) crane : [Body movement](26)
lean: [Change posture](70) [Placing](22) [Posture](12)
bustle : [Self motion](55)
(cool::heat::warm) cool : [Cause temperature change](27)
heat: [Cause temperature change](52)
warm: [Cause temperature change](41) [Inchoative change of temperature](16)
(want::fib::dare) want : [Desiring](105) [Possession](44)
fib : [Prevarication](9)
dare : [Daring](21)
(encourage::intimidate::confuse) encourage : [Stimulus focus](49)
intimidate : [Stimulus focus](26)
confuse: [Stimulus focus](45)
(happen::transpire::teach) happen : [Event](38) [Coincidence](21) [Eventive affecting](1)
transpire : [Event](15)
teach : [Education teaching](7)
(do::understand::hope) do : [Intentionally affect](6) [Intentionally act](56)
understand : [Grasp](74) [Awareness](57) [Categorization](15)
hope : [Desiring](77)
(frighten::vary::reassure) frighten : [Emotion directed](44)
vary : [Diversity](24)
reassure : [Stimulus focus](35)
(plait::braid::dye) plait : [Hair configuration](11) [Grooming](12)
braid : [Hair configuration](7) [Clothing parts](6) [Rope manipulation](4)
dye : [Processing materials](18)
(sell::purchase) sell : [Commerce sell](107)
purchase : [Commerce buy](93)
(glisten::sparkle::gleam) glisten : [Location of light](52) [Light movement](1)
sparkle : [Location of light](23) [Light movement](3)
gleam : [Location of light](77) [Light movement](4)
(forestall::shush) forestall : [Thwarting](12)
shush : [Silencing](6)
Table 1: Examples of the induced multi-verb frames. The left column shows the induced verb clusters and the right
column lists the gold frames corresponding to each verb and the number in the parentheses are their occurrence counts.
gives them a finer distinction. Arguably, implicit su-
pervision signal present in the unlabeled data is not
sufficient to provide such fine-grained distinctions.
The model does not distinguish verb senses, i.e. it
always assigns a single frame to each verb, so there
is an upper bound on our clustering performance.
4.5 Quantitative Evaluation
Nowwe turn to quantitative evaluation of both frame
and role induction.
Frame Labeling. In this section, we evaluate how
well the induced frames correspond to the gold stan-
dard annotation. Because of the lack of relevant
previous work, we use only a trivial baseline which
places each verb in a separate cluster (NoCluster-
ing). The results are summarized in Table 3.
As we can see from the results, our model
achieves a small, but probably significant, improve-
ment in the F1-score. Though the scores are
fairly low, note that, as discussed in Section 4.4,
the model is severely penalized even for induc-
ing semantically plausible frames such as the frame
(plait::braid::dye).
Role Labeling. In this section, we evaluate how
well the induced roles correspond to the gold stan-
dard annotation. We use two baselines: one is
the syntactic baseline SyntF, which simply clus-
ters arguments according to the dependency rela-
5
PU CO F1
Our approach 78.9 71.0 74.8
NoFrameInduction 79.2 70.7 74.7
SyntF 69.9 73.3 71.6
Table 2: Role labeling performance.
tion to their head, as described in (Lang and La-
pata, 2010), and the other one is a version of our
model which does not attempt to cluster verbs and
only induces roles (NoFrameInduction). Note that
the NoFrameInduction baseline is equivalent to the
factoredmodel of Titov and Klementiev (2012). The
results are summarized in Table 2.
First, observe that both our full model and its sim-
plified version NoFrameInduction significantly out-
perform the syntactic baseline. It is important to
note that the syntactic baseline is not trivial to beat
in the unsupervised setting (Lang and Lapata, 2010).
Though there is a minor improvement from inducing
frames, it is small and may not be significant.3
Another observation is that the absolute scores
of all the systems, including the baselines, are sig-
nificantly below the results reported in Titov and
Klementiev (Titov and Klementiev, 2012) on the
CoNLL-08 version of PropBank in a comparable
setting (auto parses, gold argument identification):
73.9 % and 77.9 % F1 for SyntF and NoFrameIn-
duction, respectively. We believe that the main rea-
son for this discrepancy is the difference in the syn-
tactic representations. The CoNLL-08 dependencies
include function tags (e.g., TMP, LOC), and, there-
fore, modifiers do not need to be predicted, whereas
the Stanford syntactic dependencies do not provide
this information and the model needs to induce it.
It is clear from these results, and also from the
previous observation that only 25% of verb occur-
rences belong to multi-verb clusters, that the model
does not induce sufficiently rich clustering of verbs.
Arguably, this is largely due to the relatively small
size of FrameNet, as it may not provide enough evi-
dence for clustering. Given that our method is quite
efficient, a single experiment was taking around 8
hours on a single CPU, and the procedure is highly
parallelizable, the next step would be to use a much
larger and statistically representative corpus to in-
duce the representations.
3There is no well-established methodology for testing statis-
tical significance when comparing two clustering methods.
PU CO F1
Our approach 77.9 31.4 44.7
NoClustering 80.8 29.0 42.7
Table 3: Frame labeling performance.
Additional visual inspection suggest that the data
is quite noisy primarily due to mistakes in parsing.
The large proportion of mistakes can probably be ex-
plained by the domain shift: the parser is trained on
the WSJ newswire data and tested on more general
BNC texts.
5 Related Work
The space constraints do not permit us to pro-
vide a comprehensive overview of related work.
Aside from the original model of Titov and Klemen-
tiev (2012), the most related previous method is the
Bayesian method of Titov and Klementiev (2011).
In that work, along with predicate-argument struc-
ture, they also induce clusterings of dependency
tree fragments (not necessarily verbs). However,
their approach uses a different model for argument
generation, a different inference procedure, and it
has only been applied and evaluated on biomedi-
cal data. The same shallow semantic parsing task
has also been considered in the work of Poon and
Domingos (2009; 2010), but using a MLN model
and, again, only on the biomedical domain. An-
other closely related vein of research is on semi-
supervised frame-semantic parsing (Fu?rstenau and
Lapata, 2009; Das and Smith, 2011).
6 Conclusions
This work is the first to consider the task of unsuper-
vised frame-semantic parsing. Though the quantita-
tive results are mixed, we showed that meaningful
semantic frames are induced. In the future work, we
intend to consider much larger corpora and to focus
on a more general set-up by relaxing the assumption
that frames are evoked only by verbal predicates.
Acknowledgements
The authors acknowledge the support of the MMCI Clus-
ter of Excellence, and thank Caroline Sporleder, Alexis
Palmer and the anonymous reviewers for their sugges-
tions.
6
References
Daniel Bauer, Hagen Fu?rstenau, and Owen Rambow.
2012. The dependency-parsed framenet corpus. In
International conference on Language Resources and
Evaluation (LREC), Istanbul, Turkey.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,
and M. Pinkal. 2006. The SALSA corpus: a german
corpus resource for lexical semantics. In LREC.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 Shared Task: Semantic Role La-
beling. In CoNLL.
D. Das and N.A. Smith. 2011. Semi-supervised frame-
semantic parsing for unknown predicates. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies-Volume 1, pages 1435?1444. Associa-
tion for Computational Linguistics.
D. Das, N. Schneider, D. Chen, and N.A. Smith. 2010.
Probabilistic frame-semantic parsing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 948?956. Associa-
tion for Computational Linguistics.
K. Erk and S. Pado. 2006. Shalmaneser?a toolchain for
shallow semantic parsing. In Proceedings of LREC,
volume 6. Citeseer.
Charles J. Fillmore. 1968. The case for case. In Bach
E. and Harms R.T., editors, Universals in Linguistic
Theory, pages 1?88. Holt, Rinehart, andWinston, New
York.
Hagen Fu?rstenau and Mirella Lapata. 2009. Graph align-
ment for semi-supervised semantic role labeling. In
EMNLP.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
belling of semantic roles. Computational Linguistics,
28(3):245?288.
Trond Grenager and Christoph Manning. 2006. Un-
supervised discovery of a statistical verb lexicon. In
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
13th Conference on Computational Natural Language
Learning (CoNLL-2009), June 4-5.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In EMNLP.
Joel Lang and Mirella Lapata. 2010. Unsupervised in-
duction of semantic roles. In ACL.
Joel Lang and Mirella Lapata. 2011a. Unsupervised se-
mantic role induction via split-merge clustering. In
ACL.
Joel Lang and Mirella Lapata. 2011b. Unsupervised
semantic role induction with graph partitioning. In
EMNLP.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-
gach, and Sameer S. Pradhan. 2010. Semeval-2010
task 14: Word sense induction and disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation.
K.H. Ohara, S. Fujii, T. Ohori, R. Suzuki, H. Saito, and
S. Ishizaki. 2004. The japanese framenet project:
An introduction. In Proceedings of LREC-04 Satellite
Workshop Building Lexical Resources from Semanti-
cally Annotated Corpora(LREC 2004), pages 9?11.
Alexis Palmer and Caroline Sporleder. 2010. Evaluating
FrameNet-style semantic parsing: the role of coverage
gaps in FrameNet. In COLING.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP.
H. Poon and P. Domingos. 2010. Unsupervised ontol-
ogy induction from text. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 296?305. Association for Computa-
tional Linguistics.
Josef Ruppenhofer, Michael Ellsworth, Miriam
R. L. Petruck, Christopher R. Johnson, and
Jan Scheffczyk. 2006. Framenet ii: Ex-
tended theory and practice. available at http:
//framenet.icsi.berkeley.edu/index.
php?option=com_wrapper&Itemid=126.
C. Subirats. 2009. Spanish framenet: A frame-semantic
analysis of the spanish lexicon. Berlin/New York:
Mouton de Gruyter, pages 135?162.
Mihai Surdeanu, Adam Meyers Richard Johansson, Llu??s
Ma`rquez, and Joakim Nivre. 2008. The CoNLL-2008
shared task on joint parsing of syntactic and semantic
dependencies. In CoNLL 2008: Shared Task.
Richard Swier and Suzanne Stevenson. 2004. Unsuper-
vised semantic role labelling. In EMNLP.
Ivan Titov and Alexandre Klementiev. 2011. A Bayesian
model for unsupervised semantic parsing. In ACL.
Ivan Titov and Alexandre Klementiev. 2012. A bayesian
approach to semantic role induction. In Proc. EACL,
Avignon, France.
7
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 49?57,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Inducing Neural Models of Script Knowledge
Ashutosh Modi
MMCI,
Saarland University, Germany
amodi@mmci.uni-saarland.de
Ivan Titov
ILLC,
University of Amsterdam, Netherlands
titov@uva.nl
Abstract
Induction of common sense knowledge
about prototypical sequence of events has
recently received much attention (e.g.,
Chambers and Jurafsky (2008); Regneri
et al. (2010)). Instead of inducing this
knowledge in the form of graphs, as in
much of the previous work, in our method,
distributed representations of event real-
izations are computed based on distributed
representations of predicates and their ar-
guments, and then these representations
are used to predict prototypical event or-
derings. The parameters of the composi-
tional process for computing the event rep-
resentations and the ranking component
of the model are jointly estimated. We
show that this approach results in a sub-
stantial boost in performance on the event
ordering task with respect to the previous
approaches, both on natural and crowd-
sourced texts.
1 Introduction
It is generally believed that natural language un-
derstanding systems would benefit from incorpo-
rating common-sense knowledge about prototyp-
ical sequences of events and their participants.
Early work focused on structured representations
of this knowledge (called scripts (Schank and
Abelson, 1977)) and manual construction of script
knowledge bases. However, these approaches do
not scale to complex domains (Mueller, 1998;
Gordon, 2001). More recently, automatic induc-
tion of script knowledge from text have started
to attract attention: these methods exploit ei-
ther natural texts (Chambers and Jurafsky, 2008,
2009) or crowdsourced data (Regneri et al., 2010),
and, consequently, do not require expensive ex-
pert annotation. Given a text corpus, they ex-
tract structured representations (i.e. graphs), for
example chains (Chambers and Jurafsky, 2008)
or more general directed acyclic graphs (Regneri
et al., 2010). These graphs are scenario-specific,
nodes in them correspond to events (and associ-
ated with sets of potential event mentions) and arcs
encode the temporal precedence relation. These
graphs can then be used to inform NLP applica-
tions (e.g., question answering) by providing in-
formation whether one event is likely to precede
or succeed another. Note that these graphs en-
code common-sense knowledge about prototypi-
cal ordering of events rather than temporal order
of events as described in a given text.
Though representing the script knowledge as
graphs is attractive from the human interpretability
perspective, it may not be optimal from the appli-
cation point of view. More specifically, these rep-
resentations (1) require a model designer to choose
an appropriate granularity of event mentions (e.g.,
whether nodes in the graph should be associated
with verbs, or also their arguments); (2) do not
provide a mechanism for deciding which scenario
applies in a given discourse context and (3) often
do not associate confidence levels with informa-
tion encoded in the graph (e.g., the precedence re-
lation in Regneri et al. (2010)).
Instead of constructing a graph and using it to
provide information (e.g., prototypical event or-
dering) to NLP applications, in this work we ad-
vocate for constructing a statistical model which is
capable to ?answer? at least some of the questions
these graphs can be used to answer, but doing this
without explicitly representing the knowledge as a
graph. In our method, the distributed representa-
tions (i.e. vectors of real numbers) of event real-
izations are computed based on distributed repre-
sentations of predicates and their arguments, and
then the event representations are used in a ranker
to predict the prototypical ordering of events. Both
the parameters of the compositional process for
computing the event representation and the rank-
49
ing component of the model are estimated from
texts (either relying on unambiguous discourse
clues or natural ordering in text). In this way we
build on recent research on compositional distri-
butional semantics (Baroni and Zamparelli, 2011;
Socher et al., 2012), though our approach specif-
ically focuses on embedding predicate-argument
structures rather than arbitrary phrases, and learn-
ing these representation to be especially informa-
tive for prototypical event ordering.
In order to get an intuition why the embedding
approach may be attractive, consider a situation
where a prototypical ordering of events the bus
disembarked passengers and the bus drove away
needs to be predicted. An approach based on fre-
quency of predicate pairs (Chambers and Jurafsky,
2008) (henceforth CJ08), is unlikely to make a
right prediction as driving usually precedes disem-
barking. Similarly, an approach which treats the
whole predicate-argument structure as an atomic
unit (Regneri et al., 2010) will probably fail as
well, as such a sparse model is unlikely to be ef-
fectively learnable even from large amounts of un-
labeled data. However, our embedding method
would be expected to capture relevant features of
the verb frames, namely, the transitive use for the
predicate disembark and the effect of the particle
away, and these features will then be used by the
ranking component to make the correct prediction.
In previous work on learning inference
rules (Berant et al., 2011), it has been shown
that enforcing transitivity constraints on the
inference rules results in significantly improved
performance. The same is likely to be true for
the event ordering task, as scripts have largely
linear structure, and observing that a ? b and
b ? c is likely to imply a ? c. Interestingly, in
our approach we learn the model which satisfies
transitivity constraints, without the need for any
explicit global optimization on a graph. This
results in a significant boost of performance when
using embeddings of just predicates (i.e. ignoring
arguments) with respect to using frequencies of
ordered verb pairs, as in CJ08 (76% vs. 61% on
the natural data).
Our model is solely focusing on the ordering
task, and admittedly does not represent all the in-
formation encoded by a script graph structure. For
example, it cannot be directly used to predict a
missing event given a set of events (the narrative
cloze task (Chambers and Jurafsky, 2009)). Nev-
disembarked passengersbus
predicate embedding 
event embedding
arg embedding
Ta
1
Rp
Ta
2
f(e)
a
1
= C(bus) a
2
= C(passenger)p = C(disembark)
arg embedding
hidden layer
h
Ah
Figure 1: Computation of an event representation
for a predicate with two arguments (the bus disem-
barked passengers), an arbitrary number of argu-
ments is supported by our approach.
ertheless, we believe that the framework (a proba-
bilistic model using event embeddings as its com-
ponent) can be extended to represent other aspects
of script knowledge by modifying the learning ob-
jective, but we leave this for future work. In this
paper, we show how our model can be used to pre-
dict if two event mentions are likely paraphrases
of the same event.
The approach is evaluated in two set-ups. First,
we consider the crowdsourced dataset of Regneri
et al. (2010) and demonstrate that using our model
results in the 13.5% absolute improvement in F1
on event ordering with respect to their graph in-
duction method (84.1% vs. 70.6%). Secondly,
we derive an event ordering dataset from the Gi-
gaword corpus, where we also show that the em-
bedding method beats the frequency-based base-
line (i.e. reimplementation of the scoring compo-
nent of CJ08) by 22.8% in accuracy (83.5% vs.
60.7%).
2 Model
In this section we describe the model we use for
computing event representations as well as the
ranking component of our model.
2.1 Event Representation
Learning and exploiting distributed word repre-
sentations (i.e. vectors of real values, also known
as embeddings) have been shown to be benefi-
cial in many NLP applications (Bengio et al.,
2001; Turian et al., 2010; Collobert et al., 2011).
These representations encode semantic and syn-
tactic properties of a word, and are normally
50
learned in the language modeling setting (i.e.
learned to be predictive of local word context),
though they can also be specialized by learning
in the context of other NLP applications such as
PoS tagging or semantic role labeling (Collobert
et al., 2011). More recently, the area of dis-
tributional compositional semantics have started
to emerge (Baroni and Zamparelli, 2011; Socher
et al., 2012), they focus on inducing represen-
tations of phrases by learning a compositional
model. Such a model would compute a represen-
tation of a phrase by starting with embeddings of
individual words in the phrase, often this composi-
tion process is recursive and guided by some form
of syntactic structure.
In our work, we use a simple compositional
model for representing semantics of a verb frame
e (i.e. the predicate and its arguments). We will
refer to such verb frames as events. The model is
shown in Figure 1. Each word c
i
in the vocabu-
lary is mapped to a real vector based on the cor-
responding lemma (the embedding function C).
The hidden layer is computed by summing linearly
transformed predicate and argument
1
embeddings
and passing it through the logistic sigmoid func-
tion. We use different transformation matrices for
arguments and predicates, T and R, respectively.
The event representation f(e) is then obtained by
applying another linear transform (matrix A) fol-
lowed by another application of the sigmoid func-
tion. Another point to note in here is that, as in
previous work on script induction, we use lemmas
for predicates and specifically filter out any tense
markers as our goal is to induce common-sense
knowledge about an event rather than properties
predictive of temporal order in a specific discourse
context.
We leave exploration of more complex and
linguistically-motivated models for future work.
2
These event representations are learned in the con-
text of event ranking: the transformation parame-
ters as well as representations of words are forced
to be predictive of the temporal order of events.
In our experiments, we also consider initialization
of predicate and arguments with the SENNA word
embeddings (Collobert et al., 2011).
1
Only syntactic heads of arguments are used in this work.
If an argument is a coffee maker, we will use only the word
maker.
2
In this study, we apply our model in two very differ-
ent settings, learning from crowdsourced and natural texts.
Crowdsourced collections are relatively small and require not
over-expressive models.
2.2 Learning to Order
The task of learning stereotyped order of events
naturally corresponds to the standard ranking set-
ting. We assume that we are provided with se-
quences of events, and our goal is to capture this
order. We discuss how we obtain this learning ma-
terial in the next section. We learn a linear ranker
(characterized by a vectorw) which takes an event
representation and returns a ranking score. Events
are then ordered according to the score to yield
the model prediction. Note that during the learn-
ing stage we estimate not only w but also the
event representation parameters, i.e. matrices T ,
R and A, and the word embedding C. Note that
by casting the event ordering task as a global rank-
ing problem we ensure that the model implicitly
exploits transitivity of the relation, the property
which is crucial for successful learning from finite
amount of data, as we argued in the introduction
and will confirm in our experiments.
At training time, we assume that each training
example k is a list of events e
(k)
1
, . . . , e
(k)
n
(k)
pro-
vided in the stereotypical order (i.e. e
(k)
i
? e
(k)
j
if
i < j), n
(k)
is the length of the list k. We mini-
mize the L
2
-regularized ranking hinge loss:
?
k
?
i<j?n
(k)
max(0, 1?w
T
f(e
(k)
i
;?)+w
T
f(e
(k)
j
;?))
+ ?(?w?
2
+ ???
2
),
where f(e;?) is the embedding computed
for event e, ? are all embedding parame-
ters corresponding to elements of the matrices
{R,C, T,A}. We use stochastic gradient descent,
gradients w.r.t. ? are computed using back propa-
gation.
3 Experiments
We evaluate our approach in two different set-ups.
First, we induce the model from the crowdsourced
data specifically collected for script induction by
Regneri et al. (2010), secondly, we consider an
arguably more challenging set-up of learning the
model from news data (Gigaword (Parker et al.,
2011)), in the latter case we use a learning sce-
nario inspired by Chambers and Jurafsky (2008).
3
3
Details about downloading the data and models are at:
http://www.coli.uni-saarland.de/projects/smile/docs/nmReadme.txt
51
Precision (%) Recall (%) F1 (%)
BL EE
verb
MSA BS EE BL EE
verb
MSA BS EE BL EE
verb
MSA BS EE
Bus 70.1 81.9 80.0 76.0 85.1 71.3 75.8 80.0 76.0 91.9 70.7 78.8 80.0 76.0 88.4
Coffee 70.1 73.7 70.0 68.0 69.5 72.6 75.1 78.0 57.0 71.0 71.3 74.4 74.0 62.0 70.2
Fastfood 69.9 81.0 53.0 97.0 90.0 65.1 79.1 81.0 65.0 87.9 67.4 80.0 64.0 78.0 88.9
Return 74.0 94.1 48.0 87.0 92.4 68.6 91.4 75.0 72.0 89.7 71.0 92.8 58.0 79.0 91.0
Iron 73.4 80.1 78.0 87.0 86.9 67.3 69.8 72.0 69.0 80.2 70.2 69.8 75.0 77.0 83.4
Microw. 72.6 79.2 47.0 91.0 82.9 63.4 62.8 83.0 74.0 90.3 67.7 70.0 60.0 82.0 86.4
Eggs 72.7 71.4 67.0 77.0 80.7 68.0 67.7 64.0 59.0 76.9 70.3 69.5 66.0 67.0 78.7
Shower 62.2 76.2 48.0 85.0 80.0 62.5 80.0 82.0 84.0 84.3 62.3 78.1 61.0 85.0 82.1
Phone 67.6 87.8 83.0 92.0 87.5 62.8 87.9 86.0 87.0 89.0 65.1 87.8 84.0 89.0 88.2
Vending 66.4 87.3 84.0 90.0 84.2 60.6 87.6 85.0 74.0 81.9 63.3 84.9 84.0 81.0 88.2
Average 69.9 81.3 65.8 85.0 83.9 66.2 77.2 78.6 71.7 84.3 68.0 79.1 70.6 77.6 84.1
Table 1: Results on the crowdsourced data for the verb-frequency baseline (BL), the verb-only embed-
ding model (EE
verb
), Regneri et al. (2010) (MSA), Frermann et al. (2014)(BS) and the full model (EE).
3.1 Learning from Crowdsourced Data
3.1.1 Data and task
Regneri et al. (2010) collected descriptions (called
event sequence descriptions, ESDs) of various
types of human activities (e.g., going to a restau-
rant, ironing clothes) using crowdsourcing (Ama-
zon Mechanical Turk), this dataset was also com-
plemented by descriptions provided in the OMICS
corpus (Gupta and Kochenderfer, 2004). The
datasets are fairly small, containing 30 ESDs per
activity type in average (we will refer to different
activities as scenarios), but in principle the col-
lection can easily be extended given the low cost
of crowdsourcing. The ESDs list events forming
the scenario and are written in a bullet-point style.
The annotators were asked to follow the prototyp-
ical event order in writing. As an example, con-
sider a ESD for the scenario prepare coffee :
{go to coffee maker} ? {fill water in coffee
maker} ? {place the filter in holder} ? {place
coffee in filter} ? {place holder in coffee maker}
? {turn on coffee maker}
Regneri et al. also automatically extracted pred-
icates and heads of arguments for each event, as
needed for their MSA system and our composi-
tional model.
Though individual ESDs may seem simple, the
learning task is challenging because of the limited
amount of training data, variability in the used vo-
cabulary, optionality of events (e.g., going to the
coffee machine may not be mentioned in a ESD),
different granularity of events and variability in
the ordering (e.g., coffee may be put in the filter
before placing it in the coffee maker). Unlike our
work, Regneri et al. (2010) relies on WordNet to
provide extra signal when using the Multiple Se-
quence Alignment (MSA) algorithm. As in their
work, each description was preprocessed to extract
a predicate and heads of argument noun phrases to
be used in the model.
The methods are evaluated on human anno-
tated scenario-specific tests: the goal is to classify
event pairs as appearing in a stereotypical order or
not (Regneri et al., 2010).
4
The model was estimated as explained in Sec-
tion 2.2 with the order of events in ESDs treated
as gold standard. We used 4 held-out scenarios
to choose model parameters, no scenario-specific
tuning was performed, and the 10 test scripts were
not used to perform model selection. The selected
model used the dimensionality of 10 for event and
word embeddings. The initial learning rate and the
regularization parameter were set to 0.005 and 1.0,
respectively and both parameters were reduced by
the factor of 1.2 every epoch the error function
went up. We used 2000 epochs of stochastic gradi-
ent descent. Dropout (Hinton et al., 2012) with the
rate of 20% was used for the hidden layers in all
our experiments. When testing, we predicted that
the event pair (e
1
,e
2
) is in the stereotypical order
(e
1
? e
2
) if the ranking score for e
1
exceeded the
ranking score for e
2
.
3.1.2 Results and discussion
We evaluated our event embedding model (EE)
against baseline systems (BL , MSA and BS). MSA
is the system of Regneri et al. (2010). BS is a
hierarchical Bayesian model by Frermann et al.
(2014). BL chooses the order of events based on
the preferred order of the corresponding verbs in
the training set: (e
1
, e
2
) is predicted to be in the
4
The event pairs are not coming from the same ESDs
making the task harder as the events may not be in any tem-
poral relation.
52
stereotypical order if the number of times the cor-
responding verbs v
1
and v
2
appear in this order
in the training ESDs exceeds the number of times
they appear in the opposite order (not necessary at
adjacent positions); a coin is tossed to break ties
(or if v
1
and v
2
are the same verb). This frequency
counting method was previously used in CJ08.
5
We also compare to the version of our model
which uses only verbs (EE
verbs
). Note that
EE
verbs
is conceptually very similar to BL, as it es-
sentially induces an ordering over verbs. However,
this ordering can benefit from the implicit transi-
tivity assumption used in EE
verbs
(and EE), as we
discussed in the introduction. The results are pre-
sented in Table 1.
The first observation is that the full model im-
proves substantially over the baseline and the pre-
vious method (MSA) in F1 (13.5% improvement
over MSA and 6.5% improvement over BS). Note
also that this improvement is consistent across sce-
narios: EE outperforms MSA and BS on 9 scenar-
ios out of 10 and 8 out of 10 scenarios in case of
BS. Unlike MSA and BS, no external knowledge
(i.e. WordNet) was exploited in our method.
We also observe a substantial improvement in
all metrics from using transitivity, as seen by com-
paring the results of BL and EE
verb
(11% improve-
ment in F1). This simple approach already sub-
stantially outperforms the pipelined MSA system.
These results seem to support our hypothesis in
the introduction that inducing graph representa-
tions from scripts may not be an optimal strategy
from the practical perspective.
We performed additional experiments using the
SENNA embeddings (Collobert et al., 2011). In-
stead of randomly initializing arguments and pred-
icate embeddings (vectors), we initialized them
with pre-trained SENNA embeddings. We have
not observed any significant boost in performance
from using the initialization (average F1 of 84.0%
for EE). We attribute the lack of significant im-
provement to the following three factors. First
of all, the SENNA embeddings tend to place
antonyms / opposites near each other (e.g., come
and go, or end and start). However, ?opposite?
predicates appear in very different positions in
scripts. Additionally, the SENNA embeddings
have dimensionality of 50 which appears to be
5
They scored permutations of several events by summing
the logarithmed differences of the frequencies of ordered verb
pairs. However, when applied to event pairs, their approach
would yield exactly the same prediction rule as BL.
too high for small crowd-sourced datasets, as it
forces us to use larger matrices T and R. More-
over, the SENNA embeddings are estimated from
Wikipedia, and the activities in our crowdsourced
domain are perhaps underrepresented there.
3.1.3 Paraphrasing
Regneri et al. (2010) additionally measure para-
phrasing performance of the MSA system by com-
paring it to human annotation they obtained: a sys-
tem needs to predict if a pair of event mentions are
paraphrases or not. The dataset contains 527 event
pairs for the 10 test scenarios. Each pair consists
of events from the same scenario. The dataset is
fairly balanced containing from 47 to 60 examples
per scenario.
This task does not directly map to any statisti-
cal inference problem with our model. Instead we
use an approach inspired by the interval algebra of
Allen (1983).
Our ranking model maps event mentions to po-
sitions on the time line (see Figure 2). However,
it would be more natural to assume that events are
intervals rather than points. In principle, these in-
tervals can be overlapping to encode a rich set of
temporal relations (see (Allen, 1983)). However,
we make a simplifying assumption that the inter-
vals do not overlap and every real number belongs
to an interval. In other words, our goal is to induce
a segmentation of the line: event mentions corre-
sponding to the same interval are then regarded as
paraphrases.
One natural constraint on this segmentation is
the following: if two event mentions are from the
same training ESD, they cannot be assigned to the
same interval (as events in ESD are not supposed
to be paraphrases). In Figure 2 arcs link event
mentions from the same ESD. We look for a seg-
mentation which produces the minimal number of
segments and satisfy the above constraint for event
mentions appearing in training data.
Though inducing intervals given a set of tem-
poral constraints is known to be NP-hard in gen-
eral (see, e.g., (Golumbic and Shamir, 1993)), for
our constraints a simple greedy algorithm finds an
optimal solution. We trace the line from the left
maintaining a set of event mentions in the current
unfinished interval and create a boundary when the
constraint is violated; we repeat the process un-
til we processed all mentions. In Figure 2, we
would create the first boundary between arrive
in a restaurant and order beverages: order bev-
53
En
t
e
r
 
a
r
e
s
t
a
u
r
a
n
t
A
r
r
i
v
e
 
i
n
 
a
 
r
e
s
t
a
u
r
a
n
t
...
O
r
d
e
r
 
b
e
v
e
r
a
g
e
s
B
r
o
w
s
e
 
a
 
m
e
n
u
R
e
v
i
e
w
 
o
p
t
i
o
n
s
 
i
n
 
a
 
m
e
n
u
Figure 2: Events on the time line, dotted arcs link
events from the same ESD.
erages and enter a restaurant are from the same
ESD and continuing the interval would violate
the constraint. It is not hard to see that this re-
sults in an optimal segmentation. First, the seg-
mentation satisfies the constraint by construction.
Secondly, the number of segments is minimal as
the arcs which caused boundary creation are non-
overlapping, each of these arcs needs to be cut and
our algorithm cuts each arc exactly once.
This algorithm prefers to introduce a bound-
ary as late as possible. For example, it would
introduce a boundary between browse a menu
and review options in a menu even though the
corresponding points are very close on the line.
We modify the algorithm by moving the bound-
aries left as long as this move does not result
in new constraint violations and increases mar-
gin at boundaries. In our example, the boundary
would be moved to be between order beverages
and browse a menu, as desired.
The resulting performance is reported in Ta-
ble 2. We report results of our method, as well as
results for MSA, BS and a simple all-paraphrase
baseline which predict that all mention pairs in a
test set are paraphrases (APBL).
6
We can see that
interval induction technique results in a lower F1
than that of MSA or BS. This might be partially
due to not using external knowledge (WordNet) in
our method.
We performed extra analyses on the develop-
ment scenario doorbell. The analyses revealed that
the interval induction approach is not very robust
to noise: removing a single noisy ESD results in a
dramatic change in the interval structure induced
and in a significant increase of F1. Consequently,
soft versions of the constraint would be beneficial.
Alternatively, event embeddings (i.e. continuous
vectors) can be clustered directly. We leave this
6
The results for the random baseline are lower: F1 of
40.6% in average.
Scenario F1 (%)
APBL MSA BS EE
Take bus 53.7 74.0 47.0 63.5
Make coffee 42.1 65.0 52.0 63.5
Order fastfood 37.0 59.0 80.0 62.6
Return food back 64.8 71.0 67.0 81.1
Iron clothes 43.3 67.0 60.0 56.7
Microwave cooking
43.2 75.0 82.0 57.8
Scrambled eggs
57.6 69.0 76.0 53.0
Take shower 42.1 78.0 67.0 55.7
Answer telephone 71.0 89.0 81.0 79.4
Vending machine
56.1 69.0 77.0 69.3
Average 51.1 71.6 68.9 64.5
Table 2: Paraphrasing results on the crowdsourced
data for Regneri et al. (2010) (MSA), Frermann
et al. (2014)(BS) and the all-paraphrase baseline
(APBL) and using intervals induced from our
model (EE).
investigation for future work.
3.2 Learning from Natural Text
In the second set of experiments we consider a
more challenging problem, inducing knowledge
about the stereotyped ordering of events from nat-
ural texts. In this work, we are largely inspired
by the scenario of CJ08. The overall strategy is
the following: we process the Gigaword corpus
with a high precision rule-based temporal classi-
fier relying on explicit clues (e.g., ?then?, ?after?)
to get ordered pairs of events and then we train
our model on these pairs (note that clues used by
the classifier are removed from the examples, so
the model has to rely on verbs and their argu-
ments). Conceptually, the difference between our
approach and CJ08 is in using a different tempo-
ral classifier, not enforcing that event pairs have
the same protagonist, and learning an event em-
bedding model instead of scoring event sequences
based on verb-pair frequencies.
We also evaluate our system on examples ex-
tracted using the same temporal classifier (but val-
idated manually) which allows us to use much
larger tests set, and, consequently, provide more
detailed and reliable error analysis.
3.2.1 Data and task
The Gigaword corpus consists of news data from
different news agencies and newspapers. For test-
ing and development we took the AFP (Agence
France-Presse) section, as it appeared most differ-
ent from the rest when comparing sets of extracted
event pairs (other sections correspond mostly to
US agencies). The AFP section was not used for
54
Accuracy (%)
BL 60.7
CJ08 60.1
EE
verb
75.9
EE 83.5
Table 3: Results on the Gigaword data for the
verb-frequency baseline (BL), the verb-only em-
bedding model (EE
verb
), the full model (EE) and
CJ08 rules.
training. This selection strategy was chosen to cre-
ate a negative bias for our model which is more
expressive than the baseline methods and, conse-
quently, better at memorizing examples.
As a rule-based temporal classifier, we used
high precision ?happens-before? rules from the
VerbOcean system (Chklovski and Pantel, 2004).
Consider ?to ?verb-x? and then ?verb-y?? as one
example of such rule. We used predicted collapsed
Stanford dependencies (de Marneffe et al., 2006)
to extract arguments of the verbs, and used only
a subset of dependents of a verb.
7
This prepro-
cessing ensured that (1) clues which form part of
a pattern are not observable by our model both at
train and test time; (2) there is no systematic dif-
ference between both events (e.g., for collapsed
dependencies, the noun subject is attached to both
verbs even if the verbs are conjoined); (3) no in-
formation about the order of events in text is avail-
able to the models. Applying these rules resulted
in 22,446 event pairs for training, and we split
additional 1,015 pairs from the AFP section into
812 for final testing and 203 for development. We
manually validated random 50 examples and all 50
of them followed the correct temporal order, so we
chose not to hand correct the test set.
We largely followed the same training and eval-
uation regime as for the crowdsourced data. We
set the regularization parameter and the learning
rate to 0.01 and 5.e ? 4 respectively. The model
was trained for 600 epochs. The embedding sizes
were 30 and 50 dimensions for words and events,
respectively.
3.2.2 Results and discussion
In our experiments, as before, we use BL as a
baseline, and EE
verb
as a verb-only simplified
version of our approach. We used another baseline
7
The list of dependencies not considered: aux, auxpass,
attr, appos, cc, conj, complm, cop, dep, det, punct, mwe.
consisting of the verb pair ordering counts pro-
vided by Chambers and Jurafsky (2008).
8
We re-
fer this baseline as CJ08. Note also that BL can be
regarded as a reimplementation of CJ08 but with
a different temporal classifier. We report results in
Table 3.
The observations are largerly the same as be-
fore: (1) the full model substantially outperforms
all other approaches (p-level < 0.001 with the per-
mutation test); (2) enforcing transitivity is very
helpful (75.9 % for EE
verb
vs. 60.1% for BL).
Surprisingly CJ08 rules produce as good results
as BL, suggesting that maybe our learning set-ups
are not that different.
However, an interesting question is in which sit-
uations using a more expressive model, EE, is ben-
eficial. If these accuracy gains have to do with
memorizing the data, it may not generalize well
to other domains or datasets. In order to test this
hypothesis we divided the test examples in three
frequency bands according to the frequency of the
corresponding verb pairs in the training set (to-
tal, in both orders). There are 513, 249 and 50
event pairs in the bands corresponding to unseen
pairs of verbs, frequency ? 10 and frequency >
10, respectively. These counts emphasize that cor-
rect predictions on unseen pairs are crucial and
these are exactly where BL would be equivalent
to a random guess. Also, this suggest, even before
looking into the results, that memorization is irrel-
evant. The results for BL, CJ08, EE
verb
and EE
are shown in Figure 3.
One observation is that most gains for EE and
EE
verb
are due to an improvement on unseen pairs.
This is fairly natural, as both transitivity and in-
formation about arguments are the only sources
of information available. In this context it is im-
portant to note that some of the verbs are light,
in the sense that they have little semantic content
of their own (e.g., take, get) and the event seman-
tics can only be derived from analyzing their argu-
ments (e.g., take an exam vs. take a detour). On
the high frequency verb pairs all systems perform
equally well, except for CJ08 as it was estimated
from somewhat different data.
In order to understand how transitivity works,
we considered a few unseen predicate pairs where
the EE
verb
model was correctly predicting their
order. For many of these pairs there were no infer-
8
These verb pair frequency counts are available at
www.usna.edu/Users/cs/nchamber/data/schemas/acl09/verb-
pair-orders.gz
55
??
??
??
??
??
??
???
?????? ?????? ????
50.0
57.2
71.0
82.4
62.7
77.8
81.8
83.1
81.2
96.0
94.1
96.0
CJ08
BL
EEverb
EE
Figure 3: Results for different frequency bands:
unseen, medium frequency (between 1 and 10)
and high frequency (> 10) verb pairs.
ence chains of length 2 (e.g., chain of length 2 was
found for the pair accept ? carry: accept ? get
and get ? carry but not many other pairs). This
observation suggest that our model captures some
non-trivial transitivity rules.
4 Related Work
Additionally to the work on script induction
discussed above (Chambers and Jurafsky, 2008,
2009; Regneri et al., 2010), other methods for
unsupervised learning of event semantics have
been proposed. These methods include unsu-
pervised frame induction techniques (O?Connor,
2012; Modi et al., 2012). Frames encode situa-
tions (or objects) along with their participants and
properties (Fillmore, 1976). Events in these un-
supervised approaches are represented with cate-
gorical latent variables, and they are induced rely-
ing primarily on the selectional preferences? sig-
nal. The very recent work of Cheung et al. (2013)
can be regarded as their extension but Cheung et
al. also model transitions between events with
Markov models. However, neither of these ap-
proaches considers (or directly optimizes) the dis-
criminative objective of learning to order events,
and neither of them uses distributed representa-
tions to encode semantic properties of events.
As we pointed out before, our embedding ap-
proach is similar (or, in fact, a simplification of)
the phrase embedding methods studied in the re-
cent work on distributional compositional seman-
tics (Baroni and Zamparelli, 2011; Socher et al.,
2012). However, they have not specifically looked
into representing script information. Approaches
which study embeddings of relations in knowledge
bases (e.g., Riedel et al. (2013)) bear some similar-
ity to the methods proposed in this work but they
are mostly limited to binary relations and deal with
predicting missing relations rather than with tem-
poral reasoning of any kind.
Identification of temporal relations within a text
is a challenging problem and an active area of re-
search (see, e.g., the TempEval task (UzZaman
et al., 2013)). Many rule-based and supervised ap-
proaches have been proposed in the past. How-
ever, integration of common sense knowledge in-
duced from large-scale unannotated resources still
remains a challenge. We believe that our approach
will provide a powerful signal complementary to
information exploited by most existing methods.
5 Conclusions
We have developed a statistical model for rep-
resenting common sense knowledge about proto-
typical event orderings. Our model induces dis-
tributed representations of events by composing
predicate and argument representations. These
representations capture properties relevant to pre-
dicting stereotyped orderings of events. We learn
these representations and the ordering component
from unannotated data. We evaluated our model
in two different settings: from crowdsourced data
and natural news texts. In both set-ups our method
outperformed baselines and previously proposed
systems by a large margin. This boost in perfor-
mance is primarily caused by exploiting transitiv-
ity of temporal relations and capturing information
encoded by predicate arguments.
The primary area of future work is to exploit
our method in applications such as question an-
swering. Another obvious applications is discov-
ery of temporal relations within documents (Uz-
Zaman et al., 2013) where common sense knowl-
edge implicit in script information, induced from
large unannotated corpora, should be highly ben-
eficial. Our current model uses a fairly naive se-
mantic composition component, we plan to extend
it with more powerful recursive embedding meth-
ods which should be especially beneficial when
considering very large text collections.
6 Acknowledgements
Thanks to Lea Frermann, Michaela Regneri and
Manfred Pinkal for suggestions and help with the
data. This work is partially supported by the
MMCI Cluster of Excellence at the Saarland Uni-
versity.
56
References
James F Allen. 1983. Maintaining knowledge
about temporal intervals. Communications of
the ACM, 26(11):832?843.
Marco Baroni and Robert Zamparelli. 2011.
Nouns are vectors, adjectives are matrices: Rep-
resenting adjective-noun constructions in se-
mantic space. In Proceedings of EMNLP.
Yoshua Bengio, R?ejean Ducharme, and Pascal
Vincent. 2001. A neural probabilistic language
model. In Proceedings of NIPS.
Jonathan Berant, Ido Dagan, and Jacob Gold-
berger. 2011. Global learning of typed entail-
ment rules. In Proceedings of ACL.
Nathanael Chambers and Dan Jurafsky. 2009. Un-
supervised learning of narrative schemas and
their participants. In Proceedings of ACL.
Nathanael Chambers and Daniel Jurafsky. 2008.
Unsupervised learning of narrative event chains.
In Proceedings of ACL.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy
Vanderwende. 2013. Probabilistic frame induc-
tion. In Proceedings of NAACL.
Timothy Chklovski and Patrick Pantel. 2004. Ver-
bocean: Mining the web for fine-grained se-
mantic verb relations. In Proceedings of
EMNLP.
R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. 2011. Natural
language processing (almost) from scratch.
Journal of Machine Learning Research,
12:2493?2537.
Marie-Catherine de Marneffe, Bill MacCartney,
and Christopher D. Manning. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of LREC.
Charles Fillmore. 1976. Frame semantics and the
nature of language. Annals of the New York
Academy of Sciences, 280(1):20?32.
Lea Frermann, Ivan Titov, and Manfred Pinkal.
2014. A hierarchical bayesian model for un-
supervised induction of script knowledge. In
EACL, Gothenberg, Sweden.
Martin Charles Golumbic and Ron Shamir. 1993.
Complexity and algorithms for reasoning about
time: A graph-theoretic approach. Journal of
ACM, 40(5):1108?1133.
Andrew Gordon. 2001. Browsing image collec-
tions with representations of common-sense ac-
tivities. JAIST, 52(11).
Rakesh Gupta and Mykel J. Kochenderfer. 2004.
Common sense data acquisition for indoor mo-
bile robots. In Proceedings of AAAI.
Geoffrey E. Hinton, Nitish Srivastava, Alex
Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2012. Improving neural net-
works by preventing co-adaptation of feature
detectors. arXiv: CoRR, abs/1207.0580.
Ashutosh Modi, Ivan Titov, and Alexandre Kle-
mentiev. 2012. Unsupervised induction of
frame-semantic representations. In Proceedings
of the NAACL-HLT Workshop on Inducing Lin-
guistic Structure. Montreal, Canada.
Erik T. Mueller. 1998. Natural Language Process-
ing with Thought Treasure. Signiform.
Brendan O?Connor. 2012. Learning frames from
text with an unsupervised latent variable model.
CMU Technical Report.
Robert Parker, David Graff, Junbo Kong,
Ke Chen, and Kazuaki Maeda. 2011. En-
glish gigaword fifth edition. Linguistic Data
Consortium.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with
web experiments. In Proceedings of ACL.
Sebastian Riedel, Limin Yao, Andrew McCal-
lum, and Benjamin Marlin. 2013. Relation ex-
traction with matrix factorization and universal
schemas. TACL.
R. C Schank and R. P Abelson. 1977. Scripts,
Plans, Goals, and Understanding. Lawrence
Erlbaum Associates, Potomac, Maryland.
Richard Socher, Brody Huval, Christopher D.
Manning, and Andrew Y. Ng. 2012. Seman-
tic compositionality through recursive matrix-
vector spaces. In Proceedings of EMNLP.
Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and gen-
eral method for semi-supervised learning. In
Proceedings of ACL.
Naushad UzZaman, Hector Llorens, Leon Der-
czynski, James Allen, Marc Verhagen, and
James Pustejovsky. 2013. Semeval-2013 task
1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Proceedings
of SemEval.
57
