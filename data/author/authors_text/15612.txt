Proceedings of the ACL Student Research Workshop, pages 172?179,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Deepfix: Statistical Post-editing of Statistical Machine Translation Using
Deep Syntactic Analysis
Rudolf Rosa and David Marec?ek and Ales? Tamchyna
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{rosa,marecek,tamchyna}@ufal.mff.cuni.cz
Abstract
Deepfix is a statistical post-editing sys-
tem for improving the quality of statis-
tical machine translation outputs. It at-
tempts to correct errors in verb-noun va-
lency using deep syntactic analysis and a
simple probabilistic model of valency. On
the English-to-Czech translation pair, we
show that statistical post-editing of statis-
tical machine translation leads to an im-
provement of the translation quality when
helped by deep linguistic knowledge.
1 Introduction
Statistical machine translation (SMT) is the cur-
rent state-of-the-art approach to machine transla-
tion ? see e.g. Callison-Burch et al (2011). How-
ever, its outputs are still typically significantly
worse than human translations, containing vari-
ous types of errors (Bojar, 2011b), both in lexical
choices and in grammar.
As shown by many researchers, e.g. Bojar
(2011a), incorporating deep linguistic knowledge
directly into a translation system is often hard to
do, and seldom leads to an improvement of trans-
lation output quality. It has been shown that it is
often easier to correct the machine translation out-
puts in a second-stage post-processing, which is
usually referred to as automatic post-editing.
Several types of errors can be fixed by employ-
ing rule-based post-editing (Rosa et al, 2012b),
which can be seen as being orthogonal to the sta-
tistical methods employed in SMT and thus can
capture different linguistic phenomena easily.
But there are still other errors that cannot be cor-
rected with hand-written rules, as there exist many
linguistic phenomena that can never be fully de-
scribed manually ? they need to be handled statis-
tically by automatically analyzing large-scale text
corpora. However, to the best of our knowledge,
English Czech
go to the doctor j??t k doktorovi dative case
go to the centre j??t do centra genitive case
go to a concert j??t na koncert accusative case
go for a drink j??t na drink accusative case
go up the hill j??t na kopec accusative case
Table 1: Examples of valency of the verb ?to go?
and ?j??t?. For Czech, the morphological cases of
the nouns are also indicated.
Source: The government spends on the middleschools.
Moses: Vla?da utra?c?? str?edn?? s?koly.
Meaning: The government destroys the middleschools.
Reference: Vla?da utra?c?? za str?edn?? s?koly.
Meaning: The government spends on the middleschools.
Table 2: Example of a valency error in output of
Moses SMT system.
there is very little successful research in statistical
post-editing (SPE) of SMT (see Section 2).
In our paper, we describe a statistical approach
to correcting one particular type of English-to-
Czech SMT errors ? errors in the verb-noun va-
lency. The term valency stands for the way in
which verbs and their arguments are used together,
usually together with prepositions and morpholog-
ical cases, and is described in Section 4. Several
examples of the valency of the English verb ?to go?
and the corresponding Czech verb ?j??t? are shown
in Table 1.
We conducted our experiments using a state-of-
the-art SMT system Moses (Koehn et al, 2007).
An example of Moses making a valency error is
translating the sentence ?The government spends
on the middle schools.?, adapted from our devel-
opment data set. As shown in Table 2, Moses
translates the sentence incorrectly, making an er-
ror in the valency of the ?utra?cet ? s?kola? (?spend ?
school?) pair. The missing preposition changes the
meaning dramatically, as the verb ?utra?cet? is pol-
172
ysemous and can mean ?to spend (esp. money)? as
well as ?to kill, to destroy (esp. animals)?.
Our approach is to use deep linguistic analysis
to automatically determine the structure of each
sentence, and to detect and correct valency errors
using a simple statistical valency model. We de-
scribe our approach in detail in Section 5.
We evaluate and discuss our experiments in
Section 6. We then conclude the paper and pro-
pose areas to be researched in future in Section 7.
2 Related Work
The first reported results of automatic post-editing
of machine translation outputs are (Simard et al,
2007) where the authors successfully performed
statistical post-editing (SPE) of rule-based ma-
chine translation outputs. To perform the post-
editing, they used a phrase-based SMT system in a
monolingual setting, trained on the outputs of the
rule-based system as the source and the human-
provided reference translations as the target, to
achieve massive translation quality improvements.
The authors also compared the performance of the
post-edited rule-based system to directly using the
SMT system in a bilingual setting, and reported
that the SMT system alone performed worse than
the post-edited rule-based system. They then tried
to post-edit the bilingual SMT system with another
monolingual instance of the same SMT system,
but concluded that no improvement in quality was
observed.
The first known positive results in SPE of SMT
are reported by Oflazer and El-Kahlout (2007)
on English to Turkish machine translation. The
authors followed a similar approach to Simard
et al (2007), training an SMT system to post-
edit its own output. They use two iterations of
post-editing to get an improvement of 0.47 BLEU
points (Papineni et al, 2002). The authors used
a rather small training set and do not discuss the
scalability of their approach.
To the best of our knowledge, the best results re-
ported so far for SPE of SMT are by Be?chara et al
(2011) on French-to-English translation. The au-
thors start by using a similar approach to Oflazer
and El-Kahlout (2007), getting a statistically sig-
nificant improvement of 0.65 BLEU points. They
then further improve the performance of their
system by adding information from the source
side into the post-editing system by concatenat-
ing some of the translated words with their source
Direction Baseline SPE Context SPE
en?cs 10.85?0.47 10.70?0.44 10.73?0.49
cs?en 17.20?0.53 17.11?0.52 17.18?0.54
Table 3: Results of SPE approach of Be?chara et al
(2011) evaluated on English-Czech SMT.
words, eventually reaching an improvement of
2.29 BLEU points. However, similarly to Oflazer
and El-Kahlout (2007), the training data used are
very small, and it is not clear how their method
scales on larger training data.
In our previous work (Rosa et al, 2012b), we
explored a related but substantially different area
of rule-based post-editing of SMT. The resulting
system, Depfix, manages to significantly improve
the quality of several SMT systems outputs, using
a set of hand-written rules that detect and correct
grammatical errors, such as agreement violations.
Depfix can be easily combined with Deepfix,1 as
it is able to correct different types of errors.
3 Evaluation of Existing SPE
Approaches
First, we evaluated the utility of the approach of
Be?chara et al (2011) for the English-Czech lan-
guage pair. We used 1 million sentence pairs from
CzEng 1.0 (Bojar et al, 2012b), a large English-
Czech parallel corpus. Identically to the paper, we
split the training data into 10 parts, trained 10 sys-
tems (each on nine tenths of the data) and used
them to translate the remaining part. The second
step was then trained on the concatenation of these
translations and the target side of CzEng. We also
implemented the contextual variant of SPE where
words in the intermediate language are annotated
with corresponding source words if the alignment
strength is greater than a given threshold. We lim-
ited ourselves to the threshold value 0.8, for which
the best results are reported in the paper. We tuned
all systems on the dataset of WMT11 (Callison-
Burch et al, 2011) and evaluated on the WMT12
dataset (Callison-Burch et al, 2012).
Table 3 summarizes our results. The reported
confidence intervals were estimated using boot-
strap resampling (Koehn, 2004). SPE did not lead
to any improvements of BLEU in our experiments.
In fact, SPE even slightly decreased the score (but
1Depfix (Rosa et al, 2012b) performs rule-based post-
editing on shallow-syntax dependency trees, while Deepfix
(described in this paper) is a statistical post-editing system
operating on deep-syntax dependency trees.
173
the difference is statistically insignificant in all
cases).
We conclude that this method does not improve
English-Czech translation, possibly because our
training data is too large for this method to bring
any benefit. We therefore proceed with a more
complex approach which relies on deep linguistic
knowledge.
4 Deep Dependency Syntax, Formemes,
and Valency
4.1 Tectogrammatical dependency trees
Tectogrammatical trees are deep syntactic depen-
dency trees based on the Functional Generative
Description (Sgall et al, 1986). Each node in
a tectogrammatical tree corresponds to a content
word, such as a noun, a full verb or an adjec-
tive; the node consists of the lemma of the con-
tent word and several other attributes. Functional
words, such as prepositions or auxiliary verbs, are
not directly present in the tectogrammatical tree,
but are represented by attributes of the respective
content nodes. See Figure 1 for an example of two
tectogrammatical trees (for simplicity, most of the
attributes are not shown).
In our work, we only use one of the
many attributes of tectogrammatical nodes, called
formeme (Dus?ek et al, 2012). A formeme is a
string representation of selected morpho-syntactic
features of the content word and selected auxiliary
words that belong to the content word, devised to
be used as a simple and efficient representation of
the node.
A noun formeme, which we are most interested
in, consists of three parts (examples taken from
Figure 1):
1. The syntactic part-of-speech ? n for nouns.
2. The preposition if the noun has one (empty
otherwise), as in n:on+X or n:za+4.
3. A form specifier.
? In English, it typically marks the subject
or object, as in n:subj. In case of a
noun accompanied by a preposition, the
third part is always X, as in n:on+X.
? In Czech, it denotes the morphologi-
cal case of the noun, represented by
its number (from 1 to 7 as there are
seven cases in Czech), as in n:1 and
n:za+4.
t-treezone=en
government n:subj
spend v:fin
middle adj:attr
school n:on+X
t-treezone=cs
vl?da n:1
utr?cet v:fin
st?edn? adj:attr
?kola n:za+4
Figure 1: Tectogrammatical trees for the sentence
?The government spends on the middle schools.? ?
?Vla?da utra?c?? za str?edn?? s?koly.?; only lemmas and
formemes of the nodes are shown.
Adjectives and nouns can also have the
adj:attr and n:attr formemes, respectively,
meaning that the node is in morphological agree-
ment with its parent. This is especially important
in Czech, where this means that the word bears the
same morphological case as its parent node.
4.2 Valency
The notion of valency (Tesnie`re and Fourquet,
1959) is semantic, but it is closely linked to syn-
tax. In the theory of valency, each verb has one
or more valency frames. Each valency frame de-
scribes a meaning of the verb, together with argu-
ments (usually nouns) that the verb must or can
have, and each of the arguments has one or several
fixed forms in which it must appear. These forms
can typically be specified by prepositions and mor-
phological cases to be used with the noun, and thus
can be easily expressed by formemes.
For example, the verb ?to go?, shown in Ta-
ble 1, has a valency frame that can be expressed
as n:subj go n:to+X, meaning that the sub-
ject goes to some place.
The valency frames of the verbs ?spend?
and ?utra?cet? in Figure 1 can be written as
n:subj spend n:on+X and n:1 utra?cet
n:za+4; the subject (in Czech this is a noun in
nominative case) spends on an object (in Czech,
the preposition ?za? plus a noun in accusative
case).
In our work, we have extended our scope also
to noun-noun valency, i.e. the parent node can be
either a verb or a noun, while the arguments are al-
ways nouns. Practice has proven this extension to
be useful, although the majority of the corrections
174
performed are still of the verb-noun valency type.
Still, we keep the traditional notion of verb-noun
valency throughout the text, especially to be able
to always refer to the parent as ?the verb? and to
the child as ?the noun?.
5 Our Approach
5.1 Valency models
To be able to detect and correct valency errors, we
created statistical models of verb-noun valency.
We model the conditional probability of the noun
argument formeme based on several features of the
verb-noun pair. We decided to use the following
two models:
P (fn|lv, fEN ) (1)
P (fn|lv, ln, fEN ) (2)
where:
? fn is the formeme of the Czech noun argu-
ment, which is being modelled
? lv is the lemma of the Czech parent verb
? ln is the lemma of the Czech noun argument
? fEN is the formeme of the English noun
aligned to the Czech noun argument
The input is first processed by the model (1),
which performs more general fixes, in situations
where the (lv, fEN ) pair rather unambiguously de-
fines the valency frame required.
Then model (2) is applied, correcting some er-
rors of the model (1), in cases where the noun
argument requires a different valency frame than
is usual for the (lv, fEN ) pair, and making some
more fixes in cases where the correct valency
frame required for the (lv, fEN ) pair was too am-
biguous to make a correction according to model
(1), but the decision can be made once information
about ln is added.
We computed the models on the full training set
of CzEng 1.0 (Bojar et al, 2012b) (roughly 15 mil-
lion sentences), and smoothed the estimated prob-
abilities with add-one smoothing.
5.2 Deepfix
We introduce a new statistical post-editing system,
Deepfix, whose input is a pair of an English sen-
tence and its Czech machine translation, and the
output is the Czech sentence with verb-noun va-
lency errors corrected.
The Deepfix pipeline consists of several steps:
1. the sentences are tokenized, tagged and lem-
matized (a lemma and a morphological tag is
assigned to each word)
2. corresponding English and Czech words are
aligned based on their lemmas
3. deep-syntax dependency parse trees of the
sentences are built, the nodes in the trees are
labelled with formemes
4. improbable noun formemes are replaced with
correct formemes according to the valency
model
5. the words are regenerated according to the
new formemes
6. the regenerating continues recursively to chil-
dren of regenerated nodes if they are in
morphological agreement with their parents
(which is typical for adjectives)
To decide whether the formeme of the noun is
incorrect, we query the valency model for all pos-
sible formemes and their probabilities. If an alter-
native formeme probability exceeds a fixed thresh-
old, we assume that the original formeme is incor-
rect, and we use the alternative formeme instead.
For our example sentence, ?The government
spends on the middle schools.? ? ?Vla?da utra?c?? za
str?edn?? s?koly.?, we query the model (2) and get the
following probabilities:
? P(n:4 | utra?cet, s?kola, n:on+X) = 0.07
(the original formeme)
? P(n:za+4 | utra?cet, s?kola, n:on+X) = 0.89
(the most probable formeme)
The threshold for this change type is 0.86, is
exceeded by the n:za+4 formeme and thus the
change is performed: ?s?koly? is replaced by ?za
s?koly?.
5.3 Tuning the Thresholds
We set the thresholds differently for different types
of changes. The values of the thresholds that we
used are listed in Table 4 and were estimated man-
ually. We distinguish changes where only the
morphological case of the noun is changed from
changes to the preposition. There are three possi-
ble types of a change to a preposition: switching
one preposition to another, adding a new preposi-
tion, and removing an existing preposition. The
175
Correction type Thresholds for models(1) (2)
Changing the noun case only 0.55 0.78
Changing the preposition 0.90 0.84
Adding a new preposition ? 0.86
Removing the preposition ? ?
Table 4: Deepfix thresholds
change to the preposition can also involve chang-
ing the morphological case of the noun, as each
preposition typically requires a certain morpho-
logical case.
For some combinations of a change type and a
model, as in case of the preposition removing, we
never perform a fix because we observed that it
nearly never improves the translation. E.g., if a
verb-noun pair can be correct both with and with-
out a preposition, the preposition-less variant is
usually much more frequent than the prepositional
variant (and thus is assigned a much higher prob-
ability by the model). However, the preposition
often bears a meaning that is lost by removing it
? in Czech, which is a relatively free-word-order
language, the semantic roles of verb arguments
are typically distinguished by prepositions, as op-
posed to English, where they can be determined
by their relative position to the verb.
5.4 Implementation
The whole Deepfix pipeline is implemented in
Treex, a modular NLP framework (Popel and
Z?abokrtsky?, 2010) written in Perl, which provides
wrappers for many state-of-the-art NLP tools. For
the analysis of the English sentence, we use the
Morc?e tagger (Spoustova? et al, 2007) and the
MST parser (McDonald et al, 2005). The Czech
sentence is analyzed by the Featurama tagger2 and
the RUR parser (Rosa et al, 2012a) ? a parser
adapted to parsing of SMT outputs. The word
alignment is created by GIZA++ (Och and Ney,
2003); the intersection symmetrization is used.
6 Evaluation
6.1 Automatic Evaluation
We evaluated our method on three datasets:
WMT10 (2489 parallel sentences), WMT11 (3003
parallel sentences), and WMT12 (3003 parallel
sentences) by Callison-Burch et al (2010; 2011;
2012). For evaluation, we used outputs of a
state-of-the-art SMT system, Moses (Koehn et al,
2http://featurama.sourceforge.net/
2007), tuned for English-to-Czech translation (Bo-
jar et al, 2012a). We used the WMT10 dataset
and its Moses translation as our development data
to tune the thresholds. In Table 5, we report the
achieved BLEU scores (Papineni et al, 2002),
NIST scores (Doddington, 2002), and PER (Till-
mann et al, 1997).
The improvements in automatic scores are low
but consistently positive, which suggests that
Deepfix does improve the translation quality.
However, the changes performed by Deepfix are
so small that automatic evaluation is unable to re-
liably assess whether they are positive or negative
? it can only be taken as an indication.
6.2 Manual Evaluation
To reliably assess the performance of Deepfix,
we performed manual evaluation on the WMT12
dataset translated by the Moses system.
The dataset was evenly split into 4 parts and
each of the parts was evaluated by one of two an-
notators (denoted ?A? and ?B?). For each sentence
that was modified by Deepfix, the annotator de-
cided whether the Deepfix correction had a posi-
tive (?improvement?) or negative (?degradation?)
effect on the translation quality, or concluded that
this cannot be decided (?indefinite?) ? either be-
cause both of the sentences are correct variants, or
because both are incorrect.3
The results in Table 6 prove that the overall ef-
fect of Deepfix is positive: it modifies about 20%
of the sentence translations (569 out of 3003 sen-
tences), improving over a half of them while lead-
ing to a degradation in only a quarter of the cases.
We measured the inter-annotator agreement on
100 sentences which were annotated by both an-
notators. For 60 sentence pairs, both of the anno-
tators were able to select which sentence is better,
i.e. none of the annotators used the ?indefinite?
marker. The inter-annotator agreement on these
60 sentence pairs was 97%.4
3The evaluation was done in a blind way, i.e. the annota-
tors did not know which sentence is before Deepfix and which
is after Deepfix. They were also provided with the source En-
glish sentences and the reference human translations.
4If all 100 sentence pairs are taken into account, requiring
that the annotators also agree on the ?indefinite? marker, the
inter-annotator agreement is only 65%. This suggests that
deciding whether the translation quality differs significantly
is much harder than deciding which translation is of a higher
quality.
176
Dataset BLEU score (higher is better) NIST score (higher is better) PER (lower is better)Baseline Deepfix Difference Baseline Deepfix Difference Baseline Deepfix Difference
WMT10* 15.66 15.74 +0.08 5.442 5.470 +0.028 58.44% 58.26% -0.18
WMT11 16.39 16.42 +0.03 5.726 5.737 +0.011 57.17% 57.09% -0.08
WMT12 13.81 13.85 +0.04 5.263 5.283 +0.020 60.04% 59.91% -0.13
Table 5: Automatic evaluation of Deepfix on outputs of the Moses system on WMT10, WMT11 and
WMT12 datasets. *Please note that WMT10 was used as the development dataset.
Part Annotator Changed sentences Improvement Degradation Indefinite
1 A 126 57 (45%) 35 (28%) 34 (27%)
2 B 112 62 (55%) 29 (26%) 21 (19%)
3 A 150 88 (59%) 29 (19%) 33 (22%)
4 B 181 114 (63%) 42 (23%) 25 (14%)
Total 569 321 (56%) 135 (24%) 113 (20%)
Table 6: Manual evaluation of Deepfix on outputs of Moses Translate system on WMT12 dataset.
6.3 Discussion
When a formeme change was performed, it was
usually either positive or at least not harmful (sub-
stituting one correct variant for another correct
variant).
However, we also observed a substantial
amount of cases where the change of the formeme
was incorrect. Manual inspection of a sample of
these cases showed that there can be several rea-
sons for a formeme change to be incorrect:
? incorrect analysis of the Czech sentence
? incorrect analysis of the English sentence
? the original formeme is a correct but very rare
variant
The most frequent issue is the first one. This is
to be expected, as the Czech sentence is often er-
roneous, whereas the NLP tools that we used are
trained on correct sentences; in many cases, it is
not even clear what a correct analysis of an incor-
rect sentence should be.
7 Conclusion and Future Work
On the English-Czech pair, we have shown that
statistical post-editing of statistical machine trans-
lation outputs is possible, even when translating
from a morphologically poor to a morphologi-
cally rich language, if it is grounded by deep lin-
guistic knowledge. With our tool, Deepfix, we
have achieved improvements on outputs of two
state-of-the-art SMT systems by correcting verb-
noun valency errors, using two simple probabilis-
tic valency models computed on large-scale data.
The improvements have been confirmed by man-
ual evaluation.
We encountered many cases where the per-
formance of Deepfix was hindered by errors of
the underlying tools, especially the taggers, the
parsers and the aligner. Because the use of the
RUR parser (Rosa et al, 2012a), which is partially
adapted to SMT outputs parsing, lead to a reduc-
tion of the number of parser errors, we find the ap-
proach of adapting the tools for this specific kind
of data to be promising.
We believe that our method can be adapted
to other language pairs, provided that there is a
pipeline that can analyze at least the target lan-
guage up to deep syntactic trees. Because we only
use a small subset of information that a tectogram-
matical tree provides, it is sufficient to use only
simplified tectogrammatical trees. These could be
created by a small set of rules from shallow-syntax
dependency trees, which can be obtained for many
languages using already existing parsers.
Acknowledgments
This research has been supported by the 7th FP
project of the EC No. 257528 and the project
7E11042 of the Ministry of Education, Youth and
Sports of the Czech Republic.
Data and some tools used as a prerequisite
for the research described herein have been pro-
vided by the LINDAT/CLARIN Large Infrastruc-
tural project, No. LM2010013 of the Ministry of
Education, Youth and Sports of the Czech Repub-
lic.
We would like to thank two anonymous review-
ers for many useful comments on the manuscript
of this paper.
177
References
Hanna Be?chara, Yanjun Ma, and Josef van Genabith.
2011. Statistical post-editing for a statistical MT
system. MT Summit XIII, pages 308?315.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran.
2012a. Probes in a taxonomy of factored phrase-
based models. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 253?
260, Montre?al, Canada. Association for Computa-
tional Linguistics.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The joy of parallelism with CzEng
1.0. In Proceedings of the 8th International Confer-
ence on Language Resources and Evaluation (LREC
2012), pages 3921?3928, I?stanbul, Turkey. Euro-
pean Language Resources Association.
Ondr?ej Bojar. 2011a. Rich morphology and what
can we expect from hybrid approaches to MT. In-
vited talk at International Workshop on Using Lin-
guistic Information for Hybrid Machine Translation
(LIHMT-2011), November.
Ondr?ej Bojar. 2011b. Analyzing error types in
English-Czech machine translation. Prague Bulletin
of Mathematical Linguistics, 95:63?76, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145. Morgan Kauf-
mann Publishers Inc.
Ondr?ej Dus?ek, Zdene?k Z?abokrtsky?, Martin Popel, Mar-
tin Majlis?, Michal Nova?k, and David Marec?ek.
2012. Formemes in English-Czech deep syntac-
tic MT. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 267?274,
Montre?al, Canada. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP,
Barcelona, Spain.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91?98. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, pages 25?32. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL 2002,
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: modular NLP framework. In Proceedings of
the 7th international conference on Advances in nat-
ural language processing, IceTAL?10, pages 293?
304, Berlin, Heidelberg. Springer-Verlag.
Rudolf Rosa, Ondr?ej Dus?ek, David Marec?ek, and Mar-
tin Popel. 2012a. Using parallel features in pars-
ing of machine-translated sentences for correction of
grammatical errors. In Proceedings of Sixth Work-
shop on Syntax, Semantics and Structure in Statis-
tical Translation (SSST-6), ACL, pages 39?48, Jeju,
Korea. ACL.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek.
2012b. DEPFIX: A system for automatic correction
of Czech MT outputs. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
178
pages 362?368, Montre?al, Canada. Association for
Computational Linguistics.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The meaning of the sentence in its semantic and
pragmatic aspects. Springer.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical phrase-based post-editing. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics; Proceedings of the
Main Conference, pages 508?515, Rochester, New
York, April. Association for Computational Linguis-
tics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of
two worlds: Cooperation of statistical and rule-
based taggers for Czech. In Proceedings of the
Workshop on Balto-Slavonic Natural Language Pro-
cessing 2007, pages 67?74, Praha, Czechia. Uni-
verzita Karlova v Praze, Association for Computa-
tional Linguistics.
Lucien Tesnie`re and Jean Fourquet. 1959. Ele?ments de
syntaxe structurale. E?ditions Klincksieck, Paris.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alex Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated dp based search for statistical translation.
In European Conf. on Speech Communication and
Technology, pages 2667?2670.
179
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 330?336,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Improving Translation Model by Monolingual Data?
Ondr?ej Bojar and Ales? Tamchyna
bojar@ufal.mff.cuni.cz, a.tamchyna@gmail.com
Institute of Formal and Applied Linguistics,
Faculty of Mathematics and Physics, Charles University in Prague
Abstract
We use target-side monolingual data to ex-
tend the vocabulary of the translation model
in statistical machine translation. This method
called ?reverse self-training? improves the de-
coder?s ability to produce grammatically cor-
rect translations into languages with morphol-
ogy richer than the source language esp. in
small-data setting. We empirically evalu-
ate the gains for several pairs of European
languages and discuss some approaches of
the underlying back-off techniques needed to
translate unseen forms of known words. We
also provide a description of the systems we
submitted to WMT11 Shared Task.
1 Introduction
Like any other statistical NLP task, SMT relies on
sizable language data for training. However the par-
allel data required for MT are a very scarce resource,
making it difficult to train MT systems of decent
quality. On the other hand, it is usually possible to
obtain large amounts of monolingual data.
In this paper, we attempt to make use of the
monolingual data to reduce the sparseness of surface
forms, an issue typical for morphologically rich lan-
guages. When MT systems translate into such lan-
guages, the limited size of parallel data often causes
the situation where the output should include a word
form never observed in the training data. Even
though the parallel data do contain the desired word
? This work has been supported by the grants EuroMatrix-
Plus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the
Czech Republic), P406/10/P259, and MSM 0021620838.
in other forms, a standard phrase-based decoder has
no way of using it to generate the correct translation.
Reverse self-training addresses this problem by
incorporating the available monolingual data in the
translation model. This paper builds upon the idea
outlined in Bojar and Tamchyna (2011), describing
how this technique was incorporated in the WMT
Shared Task and extending the experimental evalu-
ation of reverse self-training in several directions ?
the examined language pairs (Section 4.2), data size
(Section 4.3) and back-off techniques (Section 4.4).
2 Related Work
The idea of using monolingual data for improving
the translation model has been explored in several
previous works. Bertoldi and Federico (2009) used
monolingual data for adapting existing translation
models to translation of data from different domains.
In their experiments, the most effective approach
was to train a new translation model from ?fake?
parallel data consisting of target-side monolingual
data and their machine translation into the source
language by a baseline system.
Ueffing et al (2007) used a boot-strapping tech-
nique to extend translation models using mono-
lingual data. They gradually translated additional
source-side sentences and selectively incorporated
them and their translations in the model.
Our technique also bears a similarity to de Gis-
pert et al (2005), in that we try to use a back-off
for surface forms to generalize our model and pro-
duce translations with word forms never seen in the
original parallel data. However, instead of a rule-
based approach, we take advantage of the available
330
Source English Target Czech Czech Lemmatized
Parallel (small) a cat chased. . . = koc?ka honila. . . koc?ka honit. . .
I saw a cat = vide?l jsem koc?ku vide?t by?t koc?ka
I read about a dog = c?etl jsem o psovi c???st by?t o pes
Monolingual (large) ? c?etl jsem o koc?ce c???st by?t o koc?ka
I read about a cat ? Use reverse translation backed-off by lemmas.
Figure 1: The essence of reverse self-training: a new phrase pair (?about a cat? = ?o koc?ce?) is learned based on a
small parallel corpus and large target-side monolingual texts.
data and learn these forms statistically. We are there-
fore not limited to verbs, but our system is only able
to generate surface forms observed in the target-side
monolingual data.
3 Reverse Self-Training
Figure 1 illustrates the core of the method. Using
available parallel data, we first train an MT system
to translate from the target to the source language.
Since we want to gather new word forms from the
monolingual data, this reverse model needs the abil-
ity to translate them. For that purpose we use a fac-
tored translation model (Koehn and Hoang, 2007)
with two alternative decoding paths: form?form
and back-off?form. We experimented with several
options for the back-off (simple stemming by trun-
cation or full lemmatization), see Section 4.4. The
decoder can thus use a less sparse representation of
words if their exact forms are not available in the
parallel data.
We use this reverse model to translate (much
larger) target-side monolingual data into the source
language. We preserve the word alignments of the
phrases as used in the decoding so we directly ob-
tain the word alignment in the new ?parallel? cor-
pus. This gives us enough information to proceed
with the standard MT system training ? we extract
and score the phrases consistent with the constructed
word alignment and create the phrase table.
We combine this enlarged translation model with
a model trained on the true parallel data and use
Minimum Error Rate Training (Och, 2003) to find
the balance between the two models. The final
model has four separate components ? two language
models (one trained on parallel and one on monolin-
gual data) and the two translation models.
We do not expect the translation quality to im-
prove simply because more data is included in train-
ing ? by adding translations generated using known
data, the model could gain only new combinations
of known words. However, by using a back-off
to less sparse units (e.g. lemmas) in the factored
target?source translation, we enable the decoder
to produce previously unseen surface forms. These
translations are then included in the model, reducing
the data sparseness of the target-side surface forms.
4 Experiments
We used common tools for phrase-based translation
? Moses (Koehn et al, 2007) decoder and tools,
SRILM (Stolcke, 2002) and KenLM (Heafield,
2011) for language modelling and GIZA++ (Och
and Ney, 2000) for word alignments.
For reverse self-training, we needed Moses to also
output word alignments between source sentences
and their translations. As we were not able to make
the existing version of this feature work, we added a
new option and re-implemented this funcionality.
We rely on automatic translation quality eval-
uation throughout our paper, namely the well-
established BLEU metric (Papineni et al, 2002). We
estimate 95% confidence bounds for the scores as
described in Koehn (2004). We evaluated our trans-
lations on lower-cased sentences.
4.1 Data Sources
Aside from the WMT 2011 Translation Task data,
we also used several additional data sources for the
experiments aimed at evaluating various aspects of
reverse self-training.
JRC-Acquis
We used the JRC-Acquis 3.0 corpus (Steinberger
et al, 2006) mainly because of the number of avail-
able languages. This corpus contains a large amount
331
Source Target Corpus Size (k sents) Vocabulary Size Ratio Baseline +Mono LM +Mono TM
Para Mono
English Czech 94 662 1.67 40.9?1.9 43.5?2.0 *44.3?2.0
English Finnish 123 863 2.81 27.0?1.9 27.6?1.8 28.3?1.7
English German 127 889 1.83 34.8?1.8 36.4?1.8 37.6?1.8
English Slovak 109 763 2.03 35.3?1.6 37.3?1.7 37.7?1.8
French Czech 95 665 1.43 39.9?1.9 42.5?1.8 43.1?1.8
French Finnish 125 875 2.45 26.7?1.8 27.8?1.7 28.3?1.8
French German 128 896 1.58 38.5?1.8 40.2?1.8 *40.5?1.8
German Czech 95 665 0.91 35.2?1.8 37.0?1.9 *37.3?1.9
Table 1: BLEU scores of European language pairs on JRC data. Asterisks in the last column mark experiments for
which MERT had to be re-run.
of legislative texts of the European Union. The fact
that all data in the corpus come from a single, very
narrow domain has two effects ? models trained on
this corpus perform mostly very well in that domain
(as documented e.g. in Koehn et al (2009)), but fail
when translating ordinary texts such as news or fic-
tion. Sentences in this corpus also tend to be rather
long (e.g. 30 words on average for English).
CzEng
CzEng 0.9 (Bojar and ?Zabokrtsky?, 2009) is a par-
allel richly annotated Czech-English corpus. It con-
tains roughly 8 million parallel sentences from a
variety of domains, including European regulations
(about 34% of tokens), fiction (15%), news (3%),
technical texts (10%) and unofficial movie subtitles
(27%). We do not make much use of the rich anno-
tation in this paper, however we did experiment with
using Czech lemmas (included in the annotation) as
the back-off factor for reverse self-training.
4.2 Comparison Across Languages
In order to determine how successful our approach
is across languages, we experimented with Czech,
Finnish, German and Slovak as target languages. All
of them have a rich morphology in some sense. We
limited our selection of source languages to English,
French and German because our method focuses on
the target language anyway. We did however com-
bine the languages with respect to the richness of
their vocabulary ? the source language has less word
forms in almost all cases.
Czech and Slovak are very close languages, shar-
ing a large portion of vocabulary and having a very
similar grammar. There are many inflectional rules
for verbs, nouns, adjectives, pronouns and numerals.
Sentence structure is exhibited by various agreement
rules which often apply over long distance. Most of
the issues commonly associated with rich morphol-
ogy are clearly observable in these languages.
German also has some inflection, albeit much less
complex. The main source of German vocabulary
size are the compound words. Finnish serves as an
example of agglutinative languages well-known for
the abundance of word forms.
Table 1 contains the summary of our experimen-
tal results. Here, only the JRC-Acquis corpus was
used for training, development and evaluation. For
every language pair, we extracted the first 10 per-
cent of the parallel corpus and used them as the par-
allel data. The last 70 percent of the same corpus
were our ?monolingual? data. We used a separate
set of 1000 sentences for the development and an-
other 1000 for testing.
Sentence counts of the corpora are shown in the
columns Corpus Size Para and Mono. The table
also shows the ratio between observed vocabulary
size of the target and source language. Except for
the German?Czech language pair, the ratios are
higher than 1. The Baseline column contains the
BLEU score of a system trained solely on the paral-
lel data (i.e. the first 10 percent). A 5-gram language
model was used. The ?+Mono LM? scores were
achieved by adding a 5-gram language model trained
on the monolingual data as a separate component
(its weight was determined by MERT). The last col-
umn contains the scores after adding the translation
model self-trained on target monolingual data. This
model was also added as another component and the
weights associated with it were found by MERT.
332
For the back-off in the reverse self-training, we
used a simple suffix-trimming heuristic suitable for
fusional languages: cut off the last three characters
of each word always keeping at least the first three
characters. This heuristic reduces the vocabulary
size to a half for Czech and Slovak but it is much
less effective for Finish and German (Table 2), as
can be expected from their linguistic properties.
Language Vocabulary reduced to (%)
Czech 52
Finnish 64
German 73
Slovak 51
Table 2: Reduction of vocabulary size by suffix trimming
We did not use any linguistic tools, such as mor-
phological analyzers, in this set of experiments. We
see the main point of this section in illustrating the
applicability of our technique on a wide range of lan-
guages, including languages for which such tools are
not available.
We encountered problems when using MERT to
balance the weights of the four model components.
Our model consisted of 14 features ? one for each
language model, five for each translation model
(phrase probability and lexical weight for both di-
rections and phrase penalty), word penalty and dis-
tortion penalty. The extra 5 weights of the reversely
trained translation model caused MERT to diverge in
some cases. Since we used the mert-moses.pl
script for tuning and kept the default parameters,
MERT ran for 25 iterations and stopped. As a result,
even though our method seemed to improve trans-
lation performance in most language pairs, several
experiments contradicted this observation. We sim-
ply reran the final tuning procedure in these cases
and were able to achieve an improvement in BLEU
as well. These language pairs are marked with a ?*?
sign in Table 1.
A possible explanation for this behaviour of
MERT is that the alternative decoding paths add a
lot of possible derivations that generate the same
string. To validate our hypothesis we examined a
diverging run of MERT for English?Czech transla-
tion with two translation models. Our n-best lists
contained the best 100 derivations for each trans-
Figure 2: Vocabulary ratio and BLEU score
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 0.8  1  1.2 1.4 1.6 1.8  2  2.2 2.4 2.6 2.8  3
G
ai
n 
in
 B
LE
U 
(ab
so
lut
e)
Vocabulary size ratio
en-cs
en-fi
en-de
en-sk
fr-cs
fr-fi
fr-dede-cs
lated sentence from the development data. On av-
erage (over all 1000 sentences and over all runs), the
n-best list only contained 6.13 different translations
of a sentence. The result of the same calculation
applied on the baseline run of MERT (which con-
verged in 9 iterations) was 34.85 hypotheses. This
clear disproportion shows that MERT had much less
information when optimizing our model.
Overall, reverse self-training seems helpful for
translating into morphologically rich languages. We
achieved promising gains in BLEU, even over the
baseline including a language model trained on the
monolingual data. The improvement ranges from
roughly 0.3 (e.g. German?Czech) to over 1 point
(English?German) absolute. This result also indi-
cates that suffix trimming is a quite robust heuristic,
useful for a variety of language types.
Figure 2 illustrates the relationship between vo-
cabulary size ratio of the language pair and the
improvement in translation quality. Although the
points are distributed quite irregularly, a certain ten-
dency towards higher gains with higher ratios is ob-
servable. We assume that reverse self-training is
most useful in cases where a single word form in the
source language can be translated as several forms in
the target language. A higher ratio between vocab-
ulary sizes suggests that these cases happen more
often, thus providing more space for improvement
using our method.
333
4.3 Data Sizes
We conducted a series of English-to-Czech experi-
ments with fixed parallel data and a varying size of
monolingual data. We used the CzEng corpus, 500
thousand parallel sentences and from 500 thousand
up to 5 million monolingual sentences. We used
two separate sets of 1000 sentences from CzEng for
development and evaluation. Our results are sum-
marized in Figure 3. The gains in BLEU become
more significant as the size of included monolingual
data increases. The highest improvement can be ob-
served when the data are largest ? over 3 points ab-
solute. Figure 4 shows an example of the impact on
translation quality ? the ?Mono? data are 5 million
sentences.
When evaluated from this point of view, our
method can also be seen as a way of considerably
improving translation quality for languages with lit-
tle available parallel data.
We also experimented with varying size of paral-
lel data (500 thousand to 5 million sentences) and its
effect on reverse self-training contribution. The size
of monolingual data was always 5 million sentences.
We first measured the percentage of test data word
forms covered by the training data. We calculated
the value for parallel data and for the combination of
parallel and monolingual data. For word forms that
appeared only in the monolingual data, a different
form of the word had to be contained in the parallel
data (so that the model can learn it through the back-
off heuristic) in order to be counted in. The differ-
ence between the first and second value can simply
be thought of as the upper-bound estimation of re-
verse self-training contribution. Figure 5 shows the
results along with BLEU scores achieved in transla-
tion experiments following this scenario.
Our technique has much greater effect for small
parallel data sizes; the amount of newly learned
word forms declines rapidly as the size grows.
Similarly, improvement in BLEU score decreases
quickly and becomes negligible around 2 million
parallel sentences.
4.4 Back-off Techniques
We experimented with several options for the back-
off factor in English?Czech translation. Data from
training section of CzEng were used, 1 million par-
Figure 3: Relation between monolingual data size and
gains in BLEU score
 26
 27
 28
 29
 30
 31
 32
 33
 0  1  2  3  4  5
BL
EU
Monolingual data size (millions of sentences)
Mono LM and TM
Mono LM
Figure 5: Varying parallel data size, surface form cov-
erage (?Parallel?, ?Parallel and Mono?) and BLEU score
(?Mono LM?, ?Mono LM and TM?)
 26
 28
 30
 32
 34
 36
 38
 0  0.5  1  1.5  2  2.5  3  3.5  4  4.5
 90
 92
 94
 96
 98
BL
EU
Co
ve
ra
ge
 o
f s
ur
fa
ce
 fo
rm
s 
(%
)
Parallel data size (millions of sentences)
Mono LM and TM
Mono LM
BL
EU
Co
ve
ra
ge
 o
f s
ur
fa
ce
 fo
rm
s 
(%
)
Parallel and Mono
Parallel
allel sentences and another 5 million sentences as
target-side monolingual data. As in the previous
section, the sizes of our development and evaluation
sets were a thousand sentences.
CzEng annotation contains lexically disam-
biguated word lemmas, an appealing option for our
purposes. We also tried trimming the last 3 charac-
ters of each word, keeping at least the first 3 charac-
ters intact. Stemming of each word to four charac-
ters was also evaluated (Stem-4).
Table 3 summarizes our results. The last column
shows the vocabulary size compared to original vo-
cabulary size, estimated on lower-cased words.
We are not surprised by stemming performing the
334
System Translation Gloss
Baseline Jsi tak zrcadla? Are youSG so mirrors? (ungrammatical)
+Mono LM Jsi neobjedna?vejte zrcadla? Did youSG don?t orderPL mirrors? (ungrammatical)
+Mono TM Uz? sis objednal zrcadla? Have youSG orderedSG the mirrors (for yourself) yet?
Figure 4: Translation of the sentence ?Did you order the mirrors?? by baseline systems and a reversely-trained system.
Only the last one is able to generate the correct form of the word ?order?.
worst ? the equivalence classes generated by this
simple heuristic are too broad. Using lemmas seems
optimal from the linguistic point of view, however
suffix trimming outperformed this approach in our
experiments. We feel that finding well-performing
back-off techniques for other languages merits fur-
ther research.
Back-off BLEU Vocabulary Size (%)
Baseline 31.82?3.24 100
Stem-4 32.73?3.19 19
Lemma 33.05?3.40 54
Trimmed Suffix 33.28?3.32 47
Table 3: Back-off BLEU scores comparison
4.5 WMT Systems
We submitted systems that used reverse self-
training (cu-tamchyna) for English?Czech and
English?German language pairs.
Our parallel data for German were constrained to
the provided set (1.9 million sentences). For Czech,
we used the training sections of CzEng and the sup-
plied WMT11 News Commentary data (7.3 million
sentences in total).
In case of German, we only used the supplied
monolingual data, for Czech we used a large col-
lection of texts for language modelling (i.e. uncon-
strained). The reverse self-training used only the
constrained data ? 2.3 million sentences in German
and 2.2 in Czech. In case of Czech, we only used
the News monolingual data from 2010 and 2011 for
reverse self-training ? we expected that recent data
from the same domain as the test set would improve
translation performance the most.
We achieved mixed results with these systems ?
for translation into German, reverse self-training did
not improve translation performance. For Czech,
we were able to achieve a small gain, even though
the reversely translated data contained less sentences
than the parallel data. Our BLEU scores were also
affected by submitting translation outputs without
normalized punctuation and with a slightly different
tokenization.
In this scenario, a lot of parallel data were avail-
able and we did not manage to prepare a reversely
trained model from larger monolingual data. Both
of these factors contributed to the inconclusive re-
sults.
Table 4 shows case-insensitive BLEU scores as
calculated in the official evaluation.
Target Language Mono LM +Mono TM
German 14.8 14.8
Czech 15.7 15.9
Table 4: Case-insensitive BLEU of WMT systems
5 Conclusion
We introduced a technique for exploiting monolin-
gual data to improve the quality of translation into
morphologically rich languages.
We carried out experiments showing improve-
ments in BLEU when using our method for trans-
lating into Czech, Finnish, German and Slovak with
small parallel data. We discussed the issues of in-
cluding similar translation models as separate com-
ponents in MERT.
We showed that gains in BLEU score increase
with growing size of monolingual data. On the other
hand, growing parallel data size diminishes the ef-
fect of our method quite rapidly. We also docu-
mented our experiments with several back-off tech-
niques for English to Czech translation.
Finally, we described our primary submissions to
the WMT 2011 Shared Translation Task.
335
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
182?189, Athens, Greece, March. Association for
Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Ondr?ej Bojar and Zdene?k ?Zabokrtsky?. 2009. CzEng
0.9: Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92:63?
83.
Adria` de Gispert, Jose? B. Marin?o, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Eurospeech 2005, pages 3185?3188, Lisbon, Portugal.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proc. of EMNLP.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL. The
Association for Computer Linguistics.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe. In
MT Summit XII.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In EMNLP, pages
388?395. ACL.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. pages 440?447, Hongkong,
China, October.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In ACL, pages 160?
167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In ACL, pages 311?318.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Daniel
Varga. 2006. The JRC-acquis: A multilingual
aligned parallel corpus with 20+ languages. CoRR,
abs/cs/0609058. informal publication.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit, June 06.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Semi-supervised model adaptation for statistical
machine translation. Machine Translation, 21(2):77?
94.
336
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 374?381,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Selecting Data for English-to-Czech Machine Translation ?
Ales? Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos? Stanojevic?, Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{tamchyna,galuscakova,kamran,bojar}@ufal.mff.cuni.cz,
milosh.stanojevic@gmail.com
Abstract
We provide a few insights on data selection for
machine translation. We evaluate the quality
of the new CzEng 1.0, a parallel data source
used in WMT12. We describe a simple tech-
nique for reducing out-of-vocabulary rate af-
ter phrase extraction. We discuss the bene-
fits of tuning towards multiple reference trans-
lations for English-Czech language pair. We
introduce a novel approach to data selection
by full-text indexing and search: we select
sentences similar to the test set from a large
monolingual corpus and explore several op-
tions of incorporating them in a machine trans-
lation system. We show that this method can
improve translation quality. Finally, we de-
scribe our submitted system CU-TAMCH-BOJ.
1 Introduction
Selecting suitable data is important in all stages of
creating an SMT system. For training, the data size
plays an essential role, but the data should also be as
clean as possible. The new CzEng 1.0 was prepared
with the emphasis on data quality and we evaluate
it against the previous version to show whether the
effect for MT is positive.
Out-of-vocabulary rate is another problem related
to data selection. We present a simple technique to
reduce it by including words that became spurious
OOVs during phrase extraction.
? This work was supported by the project EuroMatrixPlus
(FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of
the Czech Republic) and the Czech Science Foundation grants
P406/11/1499 and P406/10/P259.
Another topic we explore is to use multiple refer-
ences for tuning to make the procedure more robust
as suggested by Dyer et al (2011). We evaluate this
approach for translating from English into Czech.
The main focus of our paper however lies in pre-
senting a method for data selection using full-text
search. We index a large monolingual corpus and
then extract sentences from it that are similar to the
input sentences. We use these sentences in several
ways: to create a new language model, a new phrase
table and a tuning set. The method can be seen as
a kind of domain adaptation. We show that it con-
tributes positively to translation quality and we pro-
vide a thorough evaluation.
2 Data and Tools
2.1 Comparison of CzEng 1.0 and 0.9
As this year?s WMT is the first to include the new
version of CzEng (Bojar et al, 2012b), we carried
out a few experiments to compare its suitability for
MT with its predecessor, CzEng 0.9. Apart from
size (which has almost doubled), there are impor-
tant differences between the two versions. In CzEng
0.9, the largest portion by far came from movie sub-
titles (a data source of varying quality), followed by
EU legislation and technical manuals. On the other
hand, CzEng 1.0 has over 4 million sentence pairs
from fiction and nearly the same amount of data
from EU legislation. Roughly 3 million sentence
pairs come from movie subtitles. This proportion
of domains suggests a higher quality of data. More-
over, sentences in CzEng 1.0 were automatically fil-
tered using a maximum entropy classifier that uti-
374
Vocab. [k]
Corpus and Domain Sents BLEU En Cs
CzEng 0.9
all 1M
14.77?0.12 187 360
CzEng 1.0 15.23?0.18 221 396
CzEng 0.9
news 100k
14.34?0.05 53 125
CzEng 1.0 14.01?0.13 47 113
Table 1: Comparison of CzEng 0.9 and 1.0.
lized a variety of features.
We trained contrastive phrase-based Moses SMT
systems?the first one on 1 million randomly se-
lected sentence pairs from CzEng 0.9, the other on
the same amount of data from CzEng 1.0. Another
contrastive pair of MT systems was based on small
in-domain data only: 100k sentences from the news
sections of CzEng 0.9 and 1.0. For each experiment,
the random selection was done 5 times. In both
experiments, identical data were used for the LM
(News Crawl corpus from 2011), tuning (WMT10
test set) and evaluation (WMT11 test set).
Table 1 shows the results. The ? sign in this case
denotes the standard deviation over the 5 experi-
ments (each with a different random sample of train-
ing data). The results indicate that overall, CzEng
1.0 is a more suitable source of parallel data?most
likely thanks to the more favorable distribution of
domains. However in the small in-domain setting,
using CzEng 0.9 data resulted in significantly higher
BLEU scores.
The vocabulary size of the news section seems to
have dropped since 0.9. We attribute this to the filter-
ing: sentences with obscure words are hard to align
so they are likely to be filtered out (the word align-
ment score as output by Giza++ received a large
weight in the classifier training). These unusual
words then do not appear in the vocabulary.
2.2 Lucene
Apache Lucene1 is a high performance open-source
search engine library written in Java. We use Lucene
to take advantage of the information retrieval (IR)
technique for domain adaptation. Each sentence of
a large corpus is indexed as a separate document; a
document is the unit of indexing and searching in
Lucene. The sentences (documents) can then be re-
1http://lucene.apache.org
trieved based on Lucene similarity formula2, given
a ?query corpus?. Lucene uses Boolean model for
initial filtering of documents. Vector Space Model
with a refined version of Tf-idf statistic is then used
to score the remaining candidates.
In the normal IR scenario, the query is usually
small. However, for domain adaptation a query can
be a whole corpus. Lucene does not allow such
big queries. This problem is resolved by taking
the query corpus sentence by sentence and search-
ing many times. The final score of a sentence in the
index is calculated as the average of the scores from
the sentence-level queries. Methods that make use
of this functionality are discussed in Section 5.
3 Reducing OOV by Relaxing Alignments
Out-of-vocabulary (OOV) rate has been shown to
increase during phrase extraction (Bojar and Kos,
2010). This is due to unfortunate alignment of some
words?no consistent phrase pair that includes them
can be extracted. This issue can be partially over-
come by adding translations of these ?lost? words
(according to Giza++ word alignment) to the ex-
tracted phrase table. This is not our original tech-
nique, it was suggested by Mermer and Saraclar
(2011), though it is not included in the published ab-
stract.
The extraction of phrases in the (hierarchical) de-
coder Jane (Stein et al, 2011) offers a range of sim-
ilar heuristics. Tinsley et al (2009) also observes
gains when extending the set of phrases consistent
with the word alignment by phrases consistent with
aligned parses.
We evaluated this technique on two sets of train-
ing data?the news section of CzEng 1.0 and the
whole CzEng 1.0. The OOV rate of the phrase table
was reduced nearly to the corpus OOV rate in both
cases, however the improvement was negligible?
only a handful of the newly added words occurred
in the test set. Table 2 shows the results. Trans-
lation performance using the improved phrase table
was identical to the baseline.
2http://tiny.cc/ca2ccw
375
Test Set OOV % New
CzEng Sections Baseline Reduced Phrases
news (197k sents) 3.69 3.66 12034
all (14.8M sents) 1.09 1.09 154204
Table 2: Source-side phrase table OOV.
Sections 1 reference 3 references
news 11.37?0.47 11.62?0.50
all 16.07?0.55 15.90?0.57
Table 3: BLEU scores on WMT12 test set when tuning
on WMT11 test set towards one or more references.
4 Tuning to Multiple Reference
Translations
Tuning towards multiple reference translations has
been shown to help translation quality, see Dyer et
al. (2011) and the cited works. Thanks to the other
references, more possible translations of each word
are considered correct, as well as various orderings
of words.
We tried two approaches: tuning to one true refer-
ence and one pseudo-reference, and tuning to multi-
ple human-translated references.
For the first method, which resembles computer-
generated references via paraphrasing as used in
(Dyer et al, 2011), we created the pseudo-reference
by translating the development set using TectoMT,
a deep syntactic MT with rich linguistic processing
implemented in the Treex platform3. We hoped that
the very different output of this decoder would be
beneficial for tuning, however we achieved no im-
provement at all.
For the second experiment we used 3 translations
of WMT11 test set. One is the true reference dis-
tributed for the shared task and two were translated
manually from the German version of the data into
Czech. We achieved a small improvement in final
BLEU score when training on a small data set. On
the complete constrained training data for WMT12,
there was no improvement?in fact, the BLEU score
as evaluated on the WMT12 test set was negligibly
lower. Table 3 summarizes our results. The ? sign
denotes the confidence bounds estimated via boot-
strap resampling (Koehn, 2004).
3http://ufal.ms.mff.cuni.cz/treex/
Used Selected Sel. Sents Avg
Models per Trans. Total BLEU?std
None ? 0 12.39?0.06
LM ? 16k ? rand. sel. 12.18?0.06
LM 3 16k 12.73?0.04
LM 100 502k 14.21?0.11
LM 1000 3.8M 15.12?0.08
LM All Sents 18.3M 15.55?0.11
Table 4: Results of experiments with Lucene, language
model adapted.
5 Experiments with Domain Adaptation
Domain adaptation is widely recognized as a tech-
nique which can significantly improve translation
quality (Wu et al, 2008; Bertoldi and Federico,
2009; Daume? and Jagarlamudi, 2011). In our ex-
periments we tried to select sentences close to the
source side of the test set and use them to improve
the final translation.
The parallel data used in this section are only
small: the news section of CzEng 1.0 (197k sentence
pairs, 4.2M Czech words, 4.8M English words). We
tuned the models on WMT09 test set and evaluated
on WMT11 test set. The techniques examined here
rely on a large monolingual corpus to select data
from. We used all the monolingual data provided by
the organizers of WMT11 (18.3M sentences, 316M
words).
5.1 Tailoring the Language Model
Our first attempt was to tailor the language model
to the test set. Our approach is similar to Zhao et
al. (2004). In Moore and Lewis (2010), the authors
compare several approaches to selecting data for LM
and Axelrod et al (2011) extend their ideas and ap-
ply them to MT.
Naturally, we only used the source side of the test
set. First we translated the test set using a baseline
translation system. Lucene indexer was then used
to select sentences similar to the translated ones in
the large target-side monolingual corpus. Finally, a
new language model was created from the selected
sentences.
The weight of the new LM has to reflect the im-
portance of the language model during both MERT
tuning as well as final application on (a different)
test set. If the new LM were based only on the final
376
test set, MERT would underestimate its value and
vice versa. Therefore, we actually translated both
our development (WMT09) as well as final test set
(WMT11) using the baseline model and created a
LM relevant to their union.
The results of performed experiments with do-
main adaptation are in Table 4. To compensate for
low stability of MERT, we ran the optimization five
times and report the average BLEU achieved. The
? value indicates the standard deviation of the five
runs.
The first row provides the scores for the baseline
experiment with no tailored language model. We
have run the experiment for three values of selected
sentences per one sentence of the test corpus: 3,
100 and 1000 closest-matching sentences were ex-
tracted. With more and more data in the LM, the
scores increase. The second line in Table 4 confirms
the usefulness of the sentence selection. Picking the
same amount of 16k sentences randomly performs
worse. As the last row indicates, taking all available
data leads to the best score.
Note that when selecting the sentences, we used
lemmas instead of word forms to reduce data sparse-
ness. So Lucene was actually indexing the lemma-
tized version of the monolingual data and the base-
line translation translated English lemmas to Czech
lemmas when creating the ?query corpus?. The final
models were created from the original sentences, not
their lemmatized versions.
5.2 Tailoring the Translation Model
Reverse self-training is a trick that allows to improve
the translation model using (target-side) monolin-
gual data and can lead to a performance improve-
ment (Bojar and Tamchyna, 2011; Lambert et al,
2011).
In our scenario, we translated the selected sen-
tences (in the opposite direction, i.e. from the target
into the source language). Then we created a new
translation model (in the original direction) based on
the alignment of selected sentences and their reverse
translation. This new model is finally combined with
the baseline model and weighted by MERT. The
whole scenario is shown in Figure 1.
The results of our experiments are in Table 5. We
ran the experiment with translation model adaptation
for 100 most similar sentences selected by Lucene.
Each experiment was again performed five times.
Due to the low stability of tuning, we also tried in-
creasing the size of n-best lists used by MERT.
Experiments with tailored translation model are
significantly better than the baseline but the im-
provement against the experiment with only the lan-
guage model adapted (with the corresponding 100
sentences selected) is very small.
5.3 Discussion of Domain Adaptation
Experiments
According to the results, using Lucene improves
translation performance already in the case when
only three sentences are selected for each translated
sentence. Our results are further supported by the
contrastive setup that used a language model cre-
ated from a random selection of the same number of
sentences?the translation quality even slightly de-
graded.
On the other hand, adding more sentences to lan-
guage model further improves results and the best
result is achieved when the language model is cre-
ated using the whole monolingual corpus. This
could have two reasons:
Too good domain match. The domain of the
whole monolingual corpus is too close to the test
corpus. Adding the whole monolingual corpus is
thus the best option. For more diverse monolingual
data, some domain-aware subsampling like our ap-
proach is likely to actually help.
Our style of retrieval. Our queries to Lucene
represent sentences as simple bags of words. Lucene
prefers less frequent words and the structure of the
sentence is therefore often ignored. For example it
prefers to retrieve sentences with the same proper
name rather than sentences with similar phrases or
longer expressions. This may not be the best option
for language modelling.
Our method can thus be useful mainly in the case
when the data available are too large to be processed
as a whole. It can also highly reduce the compu-
tation power and time necessary to achieve good
translation quality: the result achieved using the lan-
guage model created via Lucene for 1000 selected
sentences is not significantly worse than the result
achieved using the whole monolingual corpus but
the required data are 5 times smaller.
377
Tes
t Se
t [EN
]
Tra
nsl
ate
d T
S [C
S]
Sen
ten
ces
 Sim
ilar
 to 
Tra
nsl
ate
d T
S [C
S]
Re
ver
se 
Tra
nsl
ate
d S
ent
enc
es 
Sim
ilar
 to 
Tra
nsl
ate
d T
S [E
N]
Luc
ene
Ba
sel
ine
 Tr
ans
lati
on 
[EN
->C
S]
Do
ma
in A
dap
ted
 LM
Re
ver
se 
Tra
nsl
atio
n T
M
Re
ver
se 
Tra
nsl
atio
n [C
S->
EN]
Ori
gin
al L
M
Ori
gin
al T
M
Tes
t Se
t [EN
]
Tra
nsl
ate
d T
est
 Se
t [C
S]
Fin
al T
ran
sla
tion
 [EN
->C
S]
Figure 1: Scenario of reverse self-training.
Used N-Best Sel. Sents Sel. Sents Avg
Models per Trans. Sent. Total BLEU?std
None 100 ? 0 12.39?0.06
None 200 ? 0 12.4?0.03
LM + TM 100 100 502k 14.32?0.13
LM + TM 200 100 502k 14.36?0.07
Table 5: Results of experiments with Lucene, translation model applied.
5.4 Tuning Towards Selected Data
Domain adaptation can also be done by selecting a
suitable development corpus (Zheng et al, 2010; Li
et al, 2004). The final model parameters depend on
the domain of the development corpus. By choos-
ing a development corpus that is close to our test
set we might tune in the right direction. We imple-
mented this adaptation by querying the source side
of our large parallel corpus using the source side of
the test corpus. After that, the development corpus
is constructed from the selected sentences and their
corresponding reference translations.
This experiment uses a fixed model based on the
news section of CzEng 1.0. We only use different
tuning sets and run the MERT optimization. All the
resulting systems are tested on the WMT11 test set:
Baseline system is tuned on 2489 sentence pairs
selected randomly from whole CzEng 1.0 parallel
corpus. Lucene system uses 2489 sentence pairs se-
lected from CzEng 1.0 using Lucene. The selection
is done by choosing the most similar sentences to the
source side of the final test set. WMT10 system is
System avg BLEU?std
Baseline 11.41?0.25
Lucene 12.31?0.01
WMT10 12.37?0.02
Perfect selection 12.64?0.02
Bad selection 6.37?0.64
Table 6: Results of tuning with different corpora
tuned on 2489 sentence pairs of WMT10 test set. To
identify an upper bound, we also include a Perfect
selection system which is tuned on the final WMT11
test set. Naturally, this is not a fair competitor.
In order to make the results more reliable, it is
necessary to repeat the experiment several times
(Clark et al, 2011). Lucene and the WMT10 system
were tuned 3 times while baseline system was tuned
9 times because of randomness in selection of tun-
ing corpora (3 different tuning corpora each tuned 3
times). The results are shown in Table 6.
Even though the variance of the baseline system
is high (because we randomly selected corpora 3
378
times), the difference in scores between baseline
and Lucene system is high enough to conclude that
tuning on Lucene-selected corpus helps translation
quality. Still it does not give better BLEU score
than system tuned on WMT10 corpus. One possi-
ble reason is that the whole CzEng 1.0 is of some-
what lower quality than the news section. Given that
our final test set (WMT11) is also from the news
domain, tuning towards WMT10 corpus probably
leads to a better domain adaptation that tuning to-
wards all the domains in CzEng.
The tuning set must not overlap with the training
set. To illustrate the problem, we did a small exper-
iment with the same settings as above and randomly
selected 2489 sentences from training corpora. We
again ran the random selection 3 times and tuned 3
times with each of the extracted tuning sets, see the
?Bad selection? in Table 6.
In all the experiments with badly selected sen-
tences, the distortion and language model get an
extremely low weight compared to the weights of
translation model. This is because they are not use-
ful in translation of tuning data which was already
seen during training. Instead of reordering two short
phrases A and B, system already knows the transla-
tion of the phrase A B so no distortion is needed. On
unseen sentences, such weights lead to poor results.
This amplifies a drawback of our approach:
source texts have to be known prior to system tuning
or even before phrase extraction.
There are methods available that could tackle this
problem. Wuebker et al (2010) store phrase pair
counts per sentence when extracting phrases and
thus they can reestimate the probabilities when a
sentence has to be excluded from the phrase tables.
For large parallel corpora, suffix arrays (Callison-
Burch et al, 2005) have been used. Suffix arrays
allow for a quick retrieval of relevant sentence pairs,
the phrase extraction is postponed and performed on
the fly for each input sentence. It is trivial to fil-
ter out sentences belonging to the tuning set during
this delayed extraction. With dynamic suffix arrays
(Levenberg et al, 2010), one could even simply re-
move the tuning sentences from the suffix array.
6 Submitted Systems
This paper covers the submissions CU-TAMCH-BOJ.
We translated from English into Czech. Our setup
was very similar to CU-BOJAR (Bojar et al, 2012a),
but our primary submission is tuned on multiple ref-
erence translations as described in Section 4.
Apart from the additional references, this is a con-
strained setup. CzEng 1.0 were the only parallel data
used in training. We used a factored model to trans-
late the combination of English surface form and
part-of-speech tag into Czech form+POS. We used
separate 6-gram language models trained on CzEng
1.0 (interpolated by domain) and all News Crawl
corpora (18.3M setences, interpolated by years).
Additionaly, we created an 8-gram language model
on target POS tags. For reordering, we employed a
lexicalized model trained on CzEng 1.0.
Table 7 summarizes the official result of the pri-
mary submission and a contrastive baseline (tuned to
just one reference translation). There is a slight de-
crease in BLEU, but the translation error rate (TER)
is slightly better when more references were used.
The differences are however very small, suggesting
that tuning to more references did not have any sig-
nificant effect.
System BLEU TER
multiple references 14.5 0.765
contrastive baseline 14.6 0.774
Table 7: Scores of the submitted systems.
7 Conclusion
We showed that CzEng 1.0 is of better overall qual-
ity than its predecessor. We described a technique
for reducing phrase-table OOV rate, but achieved no
improvement for WMT12. Similarly, tuning to mul-
tiple references did not prove very beneficial.
We introduced a couple of techniques that exploit
full-text search in large corpora. We showed that
adding selected sentences as an additional LM im-
proves translations. Adding a new phrase table ac-
quired via reverse self-training resulted only in small
gains. Tuning to selected sentences resulted in a
better system than tuning to a random set. How-
ever the Lucene-selected corpus fails to outperform
good-quality in-domain tuning data.
379
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 355?362, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 182?189, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran. 2012a.
Probes in a Taxonomy of Factored Phrase-Based Mod-
els. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics. Submit-
ted.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The Joy of Parallelism with CzEng
1.0. In Proceedings of LREC2012, Istanbul, Turkey,
May. ELRA, European Language Resources Associa-
tion. In print.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the ACL, pages 255?262.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
2011. Better Hypothesis Testing for Statistical Ma-
chine Translation: Controlling for Optimizer Instabil-
ity. In Proceedings of the Association for Computa-
tional Lingustics. Association for Computational Lin-
guistics.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies: short papers - Vol-
ume 2, HLT ?11, pages 407?412, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English Translation System. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 337?343, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on trans-
lation model adaptation using monolingual data. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 284?293, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 394?402.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2004. Adaptive development data selection for
log-linear model in statistical machine translation. In
In Proceedings of COLING 2004.
Coskun Mermer and Murat Saraclar. 2011. Un-
supervised Turkish Morphological Segmentation for
Statistical Machine Translation. Abstract at Ma-
chine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Daniel Stein, David Vilar, Stephan Peitz, Markus Freitag,
Matthias Huck, and Hermann Ney. 2011. A Guide to
Jane, an Open Source Hierarchical Translation Toolkit.
Prague Bulletin of Mathematical Linguistics, 95:5?18,
March.
John Tinsley, Mary Hearne, and Andy Way. 2009. Ex-
ploiting parallel treebanks to improve phrase-based
statistical machine translation. In Alexander F. Gel-
bukh, editor, CICLing, volume 5449 of Lecture Notes
in Computer Science, pages 318?331. Springer.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 993?1000, Stroudsburg, PA, USA. Association
for Computational Linguistics.
380
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
475?484.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and Hao
Yu. 2010. Domain adaptation for statistical machine
translation in development corpus selection. In Uni-
versal Communication Symposium (IUCS), 2010 4th
International, pages 2 ?7, oct.
381
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 92?98,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Chimera ? Three Heads for English-to-Czech Translation
Ondr?ej Bojar and Rudolf Rosa and Ales? Tamchyna
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague, Czech Republic
surname@ufal.mff.cuni.cz
Abstract
This paper describes our WMT submis-
sions CU-BOJAR and CU-DEPFIX, the lat-
ter dubbed ?CHIMERA? because it com-
bines on three diverse approaches: Tec-
toMT, a system with transfer at the deep
syntactic level of representation, factored
phrase-based translation using Moses, and
finally automatic rule-based correction of
frequent grammatical and meaning errors.
We do not use any off-the-shelf system-
combination method.
1 Introduction
Targeting Czech in statistical machine transla-
tion (SMT) is notoriously difficult due to the
large number of possible word forms and com-
plex agreement rules. Previous attempts to resolve
these issues include specific probabilistic models
(Subotin, 2011) or leaving the morphological gen-
eration to a separate processing step (Fraser et al,
2012; Marec?ek et al, 2011).
TectoMT (CU-TECTOMT, Galus?c?a?kova? et al
(2013)) is a hybrid (rule-based and statistical) MT
system that closely follows the analysis-transfer-
synthesis pipeline. As such, it suffers from many
issues but generating word forms in proper agree-
ments with their neighbourhood as well as the
translation of some diverging syntactic structures
are handled well. Overall, TectoMT sometimes
even ties with a highly tuned Moses configuration
in manual evaluations, see Bojar et al (2011).
Finally, Rosa et al (2012) describes Depfix, a
rule-based system for post-processing (S)MT out-
put that corrects some morphological, syntactic
and even semantic mistakes. Depfix was able to
significantly improve Google output in WMT12,
so now we applied it on an open-source system.
Our WMT13 system is thus a three-headed
creature where, hopefully: (1) TectoMT provides
missing word forms and safely handles some non-
parallel syntactic constructions, (2) Moses ex-
ploits very large parallel and monolingual data,
and boosts better lexical choice, (3) Depfix at-
tempts to fix severe flaws in Moses output.
2 System Description
TectoMT
Moses
cu-tectomt
Depfix
cu-bojar
cu-depfix = Chimera
Input
Figure 1: CHIMERA: three systems combined.
CHIMERA is a sequential combination of three
diverse MT systems as depicted in Figure 1. Each
of the intermediate stages of processing has been
submitted as a separate primary system for the
WMT manual evalution, allowing for a more thor-
ough analysis.
Instead of an off-the-shelf system combination
technique, we use TectoMT output as synthetic
training data for Moses as described in Section 2.1
and finally we process its output using rule-based
corrections of Depfix (Section 2.2). All steps di-
rectly use the source sentence.
2.1 Moses Setup for CU-BOJAR
We ran a couple of probes with reduced training
data around the setup of Moses that proved suc-
cessful in previous years (Bojar et al, 2012a).
2.1.1 Pre-processing
We use a stable pre-processing pipeline that in-
cludes normalization of quotation marks,1 tok-
enization, tagging and lemmatization with tools
1We do not simply convert them to unpaired ASCII quotes
but rather balance them and use other heuristics to convert
most cases to the typographically correct form.
92
Case recaser lc?form utc stc
BLEU 9.05 9.13 9.70 9.81
Table 1: Letter Casing
included in the Treex platform (Popel and
Z?abokrtsky?, 2010).
This year, we evaluated the end-to-end effect of
truecasing. Ideally, English-Czech SMT should be
trained on data where only names are uppercased
(and neither the beginnings of sentences, nor all-
caps headlines or exclamations etc). For these ex-
periments, we trained a simple baseline system on
1 million sentence pairs from CzEng 1.0.
Table 1 summarizes the final (case-sensitive!)
BLEU scores for four setups. The standard ap-
proach is to train SMT lowercase and apply a re-
caser, e.g. the Moses one, on the output. Another
option (denoted ?lc?form?) is to lowercase only
the source side of the parallel data. This more
or less makes the translation model responsible
for identifying names and the language model for
identifying beginnings of sentences.
The final two approaches attempt at ?truecas-
ing? the data, i.e. the ideal lowercasing of ev-
erything except names. Our simple unsupervised
truecaser (?utc?) uses a model trained on monolin-
gual data (1 million sentences in this case, same
as the parallel training data used in this experi-
ment) to identify the most frequent ?casing shape?
of each token type when it appears within a sen-
tence and then converts its occurrences at the be-
ginnings of sentences to this shape. Our super-
vised truecaser (?stc?) casts the case of the lemma
on the form, because our lemmatizers for English
and Czech produce case-sensitive lemmas to indi-
cate names. After the translation, only determinis-
tic uppercasing of sentence beginnings is needed.
We confirm that ?stc? as we have been using it
for a couple of years is indeed the best option, de-
spite its unpleasingly frequent omissions of names
(incl. ?Spojene? sta?ty?, ?the United States?). One
of the rules in Depfix tries to cast the case from
the source to the MT output but due to alignment
errors, it is not perfect in fixing these mistakes.
Surprisingly, the standard recasing worked
worse than ?lc?form?, suggesting that two Moses
runs in a row are worse than one joint search.
We consider using a full-fledged named entity
recognizer in the future.
Tokens [M]
Corpus Sents [M] English Czech
CzEng 1.0 14.83 235.67 205.17
Europarl 0.65 17.61 15.00
Common Crawl 0.16 4.08 3.63
Table 2: Basic Statistics of Parallel Data.
2.1.2 Factored Translation for Morphological
Coherence
We use a quite standard factored configuration of
Moses. We translate from ?stc? to two factors:
?stc? and ?tag? (full Czech positional morpholog-
ical tag). Even though tags on the target side make
the data somewhat sparser (a single Czech word
form typically represents several cases, numbers
or genders), we do not use any back-off or alterna-
tive decoding path. A high-order language model
on tags is used to promote grammatically correct
and coherent output. Our system is thus less prone
to errors in local morphological agreement.
2.1.3 Large Parallel Data
The main source of our parallel data was CzEng
1.0 (Bojar et al, 2012b). We also used Europarl
(Koehn, 2005) as made available by WMT13 orga-
nizers.2 The English-Czech part of the new Com-
mon Crawl corpus was quite small and very noisy,
so we did not include it in our training data. Ta-
ble 2 provides basic statistics of the data.
Processing large parallel data can be challeng-
ing in terms of time and computational resources
required. The main bottlenecks are word align-
ment and phrase extraction.
GIZA++ (Och and Ney, 2000) has been the
standard tool for computing word alignment in
phrase-based MT. A multi-threaded version exists
(Gao and Vogel, 2008), which also supports incre-
mental extensions of parallel data by applying a
saved model on a new sentence pair. We evaluated
these tools and measured their wall-clock time3 as
well as the final BLEU score of a full MT system.
Surprisingly, single-threaded GIZA++ was con-
siderably faster than single-threaded MGIZA. Us-
ing 12 threads, MGIZA outperformed GIZA++
but the difference was smaller than we expected.
Table 3 summarizes the results. We checked the
difference in BLEU using the procedure by Clark
et al (2011) and GIZA++ alignments were indeed
2http://www.statmt.org/wmt13/
translation-task.html
3Time measurements are only indicative, they were af-
fected by the current load in our cluster.
93
Alignment Wallclock Time BLEU
GIZA++ 71 15.5
MGIZA 1 thread 114 15.4
MGIZA 12 threads 51 15.4
Table 3: Rough wallclock time [hours] of word
alignment and the resulting BLEU scores.
Corpus Sents [M] Tokens [M]
CzEng 1.0 14.83 205.17
CWC Articles 36.72 626.86
CNC News 28.08 483.88
CNA 47.00 830.32
Newspapers 64.39 1040.80
News Crawl 24.91 444.84
Total 215.93 3631.87
Table 4: Basic Statistics of Monolingual Data.
little but significantly better than MGIZA in three
MERT runs.
We thus use the standard GIZA++ aligner.
2.1.4 Large Language Models
We were able to collect a very large amount of
monolingual data for Czech: almost 216 million
sentences, 3.6 billion tokens. Table 4 lists the
corpora we used. CWC Articles is a section of
the Czech Web Corpus (Spoustova? and Spousta,
2012). CNC News refers to a subset of the Czech
National Corpus4 from the news domain. CNA
is a corpus of Czech News Agency stories from
1998 to 2012. Newspapers is a collection of ar-
ticles from various Czech newspapers from years
1998 to 2002. Finally, News Crawl is the mono-
lingual corpus made available by the organizers of
WMT13.
We created an in-domain language model from
all the corpora except for CzEng (where we only
used the news section). We were able to train a 4-
gram language model using KenLM (Heafield et
al., 2013). Unfortunately, we did not manage to
use a model of higher order. The model file (even
in the binarized trie format with probability quan-
tization) was so large that we ran out of memory
in decoding.5 We also tried pruning these larger
models but we did not have enough RAM.
To cater for a longer-range coherence, we
trained a 7-gram language model only on the News
Crawl corpus (concatenation of all years). In this
case, we used SRILM (Stolcke, 2002) and pruned
n-grams so that (training set) model perplexity
4http://korpus.cz/
5Due to our cluster configuration, we need to pre-load lan-
guage models.
Token Order Sents Tokens ARPA.gz Trie
[M] [M] [GB] [GB]
stc 4 201.31 3430.92 28.2 11.8
stc 7 24.91 444.84 13.1 8.1
tag 10 14.83 205.17 7.2 3.0
Table 5: LMs used in CU-BOJAR.
does not increase more than 10?14. The data for
this LM exactly match the domain of WMT test
sets.
Finally, we model sequences of morphological
tags on the target side using a 10-gram LM es-
timated from CzEng. Individual sections of the
corpus (news, fiction, subtitles, EU legislation,
web pages, technical documentation and Navajo
project) were interpolated to match WMT test sets
from 2007 to 2011 best. This allows even out-of-
domain data to contribute to modeling of overall
sentence structure. We filtered the model using the
same threshold 10?14.
Table 5 summarizes the resulting LM files as
used in CU-BOJAR and CHIMERA.
2.1.5 Bigger Tuning Sets
Koehn and Haddow (2012) report benefits from
tuning on a larger set of sentences. We experi-
mented with a down-scaled MT system to com-
pare a couple of options for our tuning set: the
default 3003 sentences of newstest2011, the de-
fault and three more Czech references that were
created by translating from German, the default
and two more references that were created by post-
editing a variant of our last year?s Moses system
and also a larger single-reference set consisting
of several newstest years. The preliminary re-
sults were highly inconclusive: negligibly higher
BLEU scores obtained lower manual scores. Un-
able to pick the best configuration, we picked the
largest. We tune our systems on ?bigref?, as spec-
ified in Table 6. The dataset consists of 11583
source sentences, 3003 of which have 4 reference
translations and a subset (1997 sents.) of which
has 2 reference translations constructed by post-
editing. The dataset does not include 2010 data as
a heldout for other foreseen experiments.
2.1.6 Synthetic Parallel Data
Galus?c?a?kova? et al (2013) describe several possi-
bilities of combining TectoMT and phrase-based
approaches. Our CU-BOJAR uses one of the sim-
pler but effective ones: adding TectoMT output on
the test set to our training data. As a contrast to
94
English Czech # Refs # Snts
newstest2011 official + 3 more from German 4 3003
newstest2011 2 post-edits of a system 2 1997
similar to (Bojar et al, 2012a)
newstest2009 official 1 2525
newstest2008 official 1 2051
newstest2007 official 1 2007
Total 4 11583
Table 6: Our big tuning set (bigref).
CU-BOJAR, we also examine PLAIN Moses setup
which is identical but lacks the additional syn-
thetic phrase table by TectoMT.
In order to select the best balance between
phrases suggested by TectoMT and our parallel
data, we provide these data as two separate phrase
tables. Each phrase table brings in its own five-
tuple of scores, one of which, the phrase-penalty
functions as an indicator how many phrases come
from which of the phrase tables. The standard
MERT is then used to optimize the weights.6,7
We use one more trick compared to
Galus?c?a?kova? et al (2013): we deliberately
overlap our training and tuning datasets. When
preparing the synthetic parallel data, we use the
English side of newstests 08 and 10?13. The
Czech side is always produced by TectoMT. We
tune on bigref (see Table 6), so the years 08, 11
and 12 overlap. (We could have overlapped also
years 07, 09 and 10 but we had them originally
reserved for other purposes.) Table 7 summarizes
the situation and highlights that our setup is fair:
we never use the target side of our final evaluation
set newstest2013. Some test sets are denoted
?could have? as including them would still be
correct.
The overlap allows MERT to estimate how use-
ful are TectoMT phrases compared to the standard
phrase table not just in general but on the spe-
cific foreseen test set. This deliberate overfitting
to newstest 08, 11 and 12 then helps in translating
newstest13.
This combination technique in its current state
is rather expensive as a new phrase table is re-
quired for every new input document. However,
if we fix the weights for the TectoMT phrase ta-
6Using K-best batch MIRA (Cherry and Foster, 2012) did
not work any better in our setup.
7We are aware of the fact that Moses alternative decoding
paths (Birch and Osborne, 2007) with similar phrase tables
clutter n-best lists with identical items, making MERT less
stable (Eisele et al, 2008; Bojar and Tamchyna, 2011). The
issue was not severe in our case, CU-BOJAR needed 10 itera-
tions compared to 3 iterations needed for PLAIN.
Used in
Test Set Training Tuning Final Eval
newstest07 could have en+cs ?
newstest08 en+TectoMT en+cs ?
newstest09 could have en+cs ?
newstest10 en+TectoMT could have ?
newstest11 en+TectoMT en+cs ?
newstest12 en+TectoMT en+cs ?
newstest13 en+TectoMT ? en+cs
Table 7: Summary of test sets usage. ?en? and
?cs? denote the official English and Czech sides,
resp. ?TectoMT? denotes the synthetic Czech.
ble, we can avoid re-tuning the system (whether
this would degrade translation quality needs to be
empirically evaluated). Moreover, if we use a dy-
namic phrase table, we could update it with Tec-
toMT outputs on the fly, thus bypassing the need
to retrain the translation model.
2.2 Depfix
Depfix is an automatic post-editing tool for cor-
recting errors in English-to-Czech SMT. It is ap-
plied as a post-processing step to CU-BOJAR, re-
sulting in the CHIMERA system. Depfix 2013 is an
improvement of Depfix 2012 (Rosa et al, 2012).
Depfix focuses on three major types of language
phenomena that can be captured by employing lin-
guistic knowledge but are often hard for SMT sys-
tems to get right:
? morphological agreement, such as:
? an adjective and the noun it modifies have to
share the same morphological gender, num-
ber and case
? the subject and the predicate have to agree in
morphological gender, number and person, if
applicable
? transfer of meaning in cases where the same
meaning is expressed by different grammatical
means in English and in Czech, such as:
? a subject in English is marked by being a left
modifier of the predicate, while in Czech a
subject is marked by the nominative morpho-
logical case
? English marks possessiveness by the preposi-
tion ?of?, while Czech uses the genitive mor-
phological case
? negation can be marked in various ways in
English and Czech
? verb-noun and noun-noun valency?see (Rosa
et al, 2013)
Depfix first performs a complex lingustic anal-
95
System BLEU TER WMT Ranking
Appraise MTurk
CU-TECTOMT 14.7 0.741 0.455 0.491
CU-BOJAR 20.1 0.696 0.637 0.555
CU-DEPFIX 20.0 0.693 0.664 0.542
PLAIN Moses 19.5 0.713 ? ?
GOOGLE TR. ? ? 0.618 0.526
Table 8: Overall results.
ysis of both the source English sentence and its
translation to Czech by CU-BOJAR. The anal-
ysis includes tagging, word-alignment, and de-
pendency parsing both to shallow-syntax (?analyt-
ical?) and deep-syntax (?tectogrammatical?) de-
pendency trees. Detection and correction of errors
is performed by rule-based components (the va-
lency corrections use a simple statistical valency
model). For example, if the adjective-noun agree-
ment is found to be violated, it is corrected by
projecting the morphological categories from the
noun to the adjective, which is realized by chang-
ing their values in the Czech morphological tag
and generating the appropriate word form from the
lemma-tag pair using the rule-based generator of
Hajic? (2004).
Rosa (2013) provides details of the current ver-
sion of Depfix. The main additions since 2012 are
valency corrections and lost negation recovery.
3 Overall Results
Table 8 reports the scores on the WMT13 test
set. BLEU and TER are taken from the evalu-
ation web site8 for the normalized outputs, case
insensitive. The normalization affects typeset-
ting of punctuation only and greatly increases
automatic scores. ?WMT ranking? lists results
from judgments from Appraise and Mechanical
Turk. Except CU-TECTOMT, the manual evalua-
tion used non-normalized MT outputs. The fig-
ure is the WMT12 standard interpretation as sug-
gested by Bojar et al (2011) and says how often
the given system was ranked better than its com-
petitor across all 18.6k non-tying pairwise com-
parisons extracted from the annotations.
We see a giant leap from CU-TECTOMT to CU-
BOJAR, confirming the utility of large data. How-
ever, CU-TECTOMT had something to offer since it
improved over PLAIN, a very competitive baseline,
by 0.6 BLEU absolute. Depfix seems to slightly
worsen BLEU score but slightly improve TER; the
8http://matrix.statmt.org/
System # Tokens % Tokens
All 22920 76.44
Moses 3864 12.89
TectoMT 2323 7.75
Other 877 2.92
Table 9: CHIMERA components that contribute
?confirmed? tokens.
System # Tokens % Tokens
None 21633 79.93
Moses 2093 7.73
TectoMT 2585 9.55
Both 385 1.42
CU-BOJAR 370 1.37
Table 10: Tokens missing in CHIMERA output.
manual evaluation is similarly indecisive.
4 Combination Analysis
We now closely analyze the contributions of
the individual engines to the performance of
CHIMERA. We look at translations of the new-
stest2013 sets produced by the individual systems
(PLAIN, CU-TECTOMT, CU-BOJAR, CHIMERA).
We divide the newstest2013 reference tokens
into two classes: those successfully produced by
CHIMERA (Table 9) and those missed (Table 10).
The analysis can suffer from false positives as well
as false negatives, a ?confirmed? token can violate
some grammatical constraints in MT output and
an ?unconfirmed? token can be a very good trans-
lation. If we had access to more references, the
issue of false negatives would decrease.
Table 9 indicates that more than 3/4 of to-
kens confirmed by the reference were available
in all CHIMERA components: PLAIN Moses, CU-
TECTOMT alone but also in the subsequent combi-
nations CU-BOJAR and the final CU-DEPFIX.
PLAIN Moses produced 13% tokens that Tec-
toMT did not provide and TectoMT output
roughly 8% tokens unknown to Moses. However,
note that it is difficult to distinguish the effect of
different model weights: PLAIN might have pro-
duced some of those tokens as well if its weights
were different. The row ?Other? includes cases
where e.g. Depfix introduced a confirmed token
that none of the previous systems had.
Table 10 analyses the potential of CHIMERA
components. These tokens from the reference
were not produced by CHIMERA. In almost 80%
of cases, the token was not available in any 1-best
output; it may have been available in Moses phrase
96
tables or the input sentence.
TectoMT offered almost 10% of missed tokens,
but these were not selected in the subsequent com-
bination. The potential of Moses is somewhat
lower (about 8%) because our phrase-based com-
bination is likely to select wordings that score well
in a phrase-based model. 385 tokens were sug-
gested by both TectoMT and Moses alone, but the
combination in CU-BOJAR did not select them, and
finally 370 tokens were produced by the combina-
tion while they were not present in 1-best output of
neither TectoMT nor Moses. Remember, all these
tokens eventually did not get to CHIMERA output,
so Depfix must have changed them.
4.1 Depfix analysis
Table 11 analyzes the performance of the individ-
ual components of Depfix. Each evaluated sen-
tence was either modified by a Depfix component,
or not. If it was modified, its quality could have
been evaluated as better (improved), worse (wors-
ened), or the same (equal) as before. Thus, we can
evaluate the performance of the individual compo-
nents by the following measures:9
precision = #improved#improved+#worsened (1)
impact = #modified#evaluated (2)
useless = #equal#modified (3)
Please note that we make an assumption that if
a sentence was modified by multiple Depfix com-
ponents, they all have the same effect on its qual-
ity. While this is clearly incorrect, it is impossible
to accurately determine the effect of each individ-
ual component with the evaluation data at hand.
This probably skews especially the reported per-
formance of ?high-impact? components, which of-
ten operate in combination with other components.
The evaluation is computed on 871 hits in which
CU-BOJAR and CHIMERA were compared.
The results show that the two newest compo-
nents ? Lost negation recovery and Valency model
? both modify a large number of sentences. Va-
lency model seems to have a slightly negative ef-
fect on the translation quality. As this is the only
statistical component of Depfix, we believe that
this is caused by the fact that its parameters were
not tuned on the final CU-BOJAR system, as the
9We use the term precision for our primary measure for
convenience, even though the way we define it does not match
exactly its usual definition.
Depfix component Prc. Imp. Usl.
Aux ?be? agr. ? 1.4% 100%
No prep. without children ? 0.5% 100%
Sentence-initial capitalization 0% 0.1% 0%
Prepositional morph. case 0% 2.1% 83%
Preposition - noun agr. 40% 3.8% 70%
Noun number projection 41% 7.2% 65%
Valency model 48% 10.6% 66%
Subject - nominal pred. agr. 50% 3.8% 76%
Noun - adjective agr. 55% 17.8% 75%
Subject morph. case 56% 8.5% 57%
Tokenization projection 56% 3.0% 38%
Verb tense projection 58% 5.2% 47%
Passive actor with ?by? 60% 1.0% 44%
Possessive nouns 67% 0.9% 25%
Source-aware truecasing 67% 2.8% 50%
Subject - predicate agr. 68% 5.1% 57%
Pro-drop in subject 73% 3.4% 63%
Subject - past participle agr. 75% 6.3% 42%
Passive - aux ?be? agr. 77% 4.8% 69%
Possessive with ?of? 78% 1.5% 31%
Present continuous 78% 1.5% 31%
Missing reflexive verbs 80% 1.6% 64%
Subject categories projection 83% 3.7% 62%
Rehang children of aux verbs 83% 5.5% 62%
Lost negation recovery 90% 7.2% 38%
Table 11: Depfix components performance analy-
sis on 871 sentences from WMT13 test set.
tuning has to be done semi-manually and the fi-
nal system was not available in advance. On the
other hand, Lost negation recovery seems to have
a highly positive effect on translation quality. This
is to be expected, as a lost negation often leads to
the translation bearing an opposite meaning to the
original one, which is probably one of the most
serious errors that an MT system can make.
5 Conclusion
We have reached our chimera to beat Google
Translate. We combined all we have: a deep-
syntactic transfer-based system TectoMT, very
large parallel and monolingual data, factored setup
to ensure morphological coherence, and finally
Depfix, a rule-based automatic post-editing sys-
tem that corrects grammaticality (agreement and
valency) of the output as well as some features vi-
tal for adequacy, namely lost negation.
Acknowledgments
This work was partially supported by the grants
P406/11/1499 of the Grant Agency of the Czech
Republic, FP7-ICT-2011-7-288487 (MosesCore)
and FP7-ICT-2010-6-257528 (Khresmoi) of the
European Union and by SVV project number 267
314.
97
References
Alexandra Birch and Miles Osborne. 2007. CCG Su-
pertags in Factored Statistical Machine Translation.
In In ACL Workshop on Statistical Machine Trans-
lation, pages 9?16.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Proc.
of WMT, pages 330?336. ACL.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proc. of WMT, pages 1?11.
ACL.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran.
2012a. Probes in a Taxonomy of Factored Phrase-
Based Models. In Proc. of WMT, pages 253?260.
ACL.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928. ELRA.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proc. of NAACL/HLT, pages 427?436. ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proc. of ACL/HLT, pages 176?
181. ACL.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to Integrate Multi-
ple Rule-Based Machine Translation Engines into a
Hybrid System. In Proc. of WMT, pages 179?182.
ACL.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proc. of EACL 2012. ACL.
Petra Galus?c?a?kova?, Martin Popel, and Ondr?ej Bojar.
2013. PhraseFix: Statistical Post-Editing of Tec-
toMT. In Proc. of WMT13. Under review.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57. ACL.
Jan Hajic?. 2004. Disambiguation of rich inflection:
computational morphology of Czech. Karolinum.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proc. of WMT, pages 317?321. ACL.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit X, pages 79?86.
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with
grammatical post-processing. In Proc. of WMT,
pages 426?432. ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In ACL. ACL.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: Modular NLP Framework. In Hrafn Lofts-
son, Eirikur Ro?gnvaldsson, and Sigrun Helgadottir,
editors, IceTAL 2010, volume 6233 of Lecture Notes
in Computer Science, pages 293?304. Iceland Cen-
tre for Language Technology (ICLT), Springer.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek.
2012. DEPFIX: A system for automatic correction
of Czech MT outputs. In Proc. of WMT, pages 362?
368. ACL.
Rudolf Rosa, David Marec?ek, and Ales? Tamchyna.
2013. Deepfix: Statistical Post-editing of Statistical
Machine Translation Using Deep Syntactic Analy-
sis. Ba?lgarska akademija na naukite, ACL.
Rudolf Rosa. 2013. Automatic post-editing of phrase-
based machine translation outputs. Master?s thesis,
Charles University in Prague, Faculty of Mathemat-
ics and Physics, Praha, Czechia.
Johanka Spoustova? and Miroslav Spousta. 2012. A
High-Quality Web Corpus of Czech. In Proc. of
LREC. ELRA.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. on
Spoken Language Processing, volume 2, pages 901?
904.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. of
ACL/HLT, pages 230?238. ACL.
98
