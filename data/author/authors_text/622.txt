J avox: A Toolkit for Building Speech-Enabled Applications 
Michae l  S. Fu lkerson  and A lan  W.  B ie rmann 
Department  of Computer  Science 
Duke University 
Durham,  North Carol ina 27708, USA 
{msf, awb}@cs, duke. edu 
Abst rac t  
JAVOX provides a mechanism for the development 
of spoken-language systems from existing desktop 
applications. We present an architecture that al- 
lows existing Java 1 programs to be speech-enabled 
with no source-code modification, through the use 
of reflection and automatic modification to the ap- 
plication's compiled code. The grammars used in 
JAvox are based on the Java Speech Grammar For- 
mat (JSGF); JAVOX grammars have an additional 
semantic omponent based on our JAVOX Script- 
ing Language (JSL). JAVOX has been successfully 
demonstrated onreal-world applications. 
1 Overv iew 
JAVOX is an implemented set of tools that allows 
software developers to speech-enable existing appli- 
cations. The process requires no changes to the 
program's source code: Speech capacity is plugged- 
in to the existing code by modifying the compiled 
program as it loads. JAVOX is intended to provide 
similar functionality o that usually associated with 
menus and mouse actions in graphical user interfaces 
(GUIs). It is completely programmable - develop- 
ers can provide a speech interface to whatever func- 
tionality they desire. J ivox  has been successfully 
demonstrated with several GUI-based applications. 
Previous ystems to assist in the development of
spoken-langnage systems (SLSs) have focused on 
building stand-alone, customized applications, uch 
as (Sutton et al, 1996) and (Pargellis et al, 1999). 
The goal of the JAVOX toolkit is to speech-enable 
traditional desktop applications - this is similar to 
the goals of the MELISSA project (Schmidt et al, 
1998). It is intended to both speed the develop- 
ment of SLSs and to localize the speech-specific code 
within the application. JAVOX allows developers to 
add speech interfaces to applications at the end of 
the development process; SLSs no longer need to be 
built from the ground up. 
We will briefly present an overview of how JAVOX 
works, including its major modules. First, we 
1Java and Java Speech are registered trademarks of Sun 
Microsystems, Inc. 
will examine TRANSLATOR, the implemented JAVOX 
natural anguage processing (NLP) component; its 
role is to translate from natural language utterances 
to the JhVOX Scripting Language (JSL). Next, we 
will discuss JSL in conjunction with a discussion of 
EXECUTER, the interface between JAVOX and the 
application. We will explain the JhvOX infrastruc- 
ture and its current implementation in Java. In 
conclusion, we will discuss the current state of the 
project and where it is going. 
2 Bas ic  Operat ion  
J ivox can be used as the sole location of NLP for 
an application; the application is written as a non- 
speech-enabled program and JhvOX adds the speech 
capability. The current implementation is written 
in Java and works with Java programs. The linkage 
between the application program and JhvOX is cre- 
ated by modifying - at load time - all constructors in 
the application to register new objects with JAVOX. 
For this reason, the application's source code does 
not need any modification to enable JAVOX. A thor- 
ough discussion of this technique ispresented in Sec- 
tion 4. The schematic n Figure 1 shows a high-level 
overview of the JAVOX architecture. 
Issuing a voice command begins with a user ut- 
terance, which the speech recognizer processes and 
passes to the NLP component, TRANSLATOR. We 
are using the IBM implementation f Sun's Java 
Speech application program interface (API) (Sun 
Microsystems, Inc., 1998) in conjunction with IBM's 
VIAVOICE. The job of TRANSLATOR - or a differ- 
ent module conforming to its API - is to translate 
the utterance into a form that represents he corre- 
sponding program actions. The current implemen- 
tation of TRANSLATOR uses a context-free grammar, 
with each rule carrying an optional JSL fragment. 
A typical bottom-up arser processes utterances and 
a complete JSL program results. The resulting JSL 
is forwarded to EXECUTER, where the JSL code is 
executed. For example, in a hypothetical banking 
application, the utterance add $100 to the account 
might be translated into the JSL command: 
myBalance = myBa lance  + i00; 
105 
File Edit Tools 
Typical 
Desktop 
Application 
l 
~ r 
~ y 
Operating 
System 
"~" : ~l (virtual machine) 
, Translator =, ~. 
Executer I I 1 ~ ~  
( J Speech I I 
l J Recognizer J: 
Javox 
B" 
! 
i .  
Figure 1: Schematic of the JAVOX architecture. 
The job of EXECUTER - or a different module that 
conforms to EXECUTER'S API - is to execute and 
monitor upcalls into the running application. The 
upcalls are the actual functions that would be made 
by the appropriate mouse clicks or menu selections 
had the user not used speech. For this reason, we are 
currently concentrating our efforts on event-driven 
programs, the class of most GUI applications. Their 
structure is usually amenable to this approach. Our 
implementation f EXECUTER performs the upcalls 
by interpreting and executing JSL, though the tech- 
nology could be used with systems other than JSL. 
In the banking example, EXECUTER would identify 
the myBalemce variable and increment i by $100. 
The main JAVOX components, TRANSLATOR and 
EXECUTER, are written to flexible APIs. Develop- 
ers may choose to use their own custom components 
instead of these two. Those who want a different 
NLP scheme can implement a different version of 
TRANSLATOR and - as long as it outputs JSL - 
still use EXECUTER. Conversely, those who want a 
different scripting system can replace JSL and still 
use TRANSLATOR and even EXECUTER's low-level 
infrastructure. 
3 Javox  Grammars  
The JAVOX infrastructure is not tied to any par- 
ticular NLP method; in fact, the JAVOX grammar 
system is the second NLP implementation we have 
used. It is presented here because it is straightfor- 
ward, easy to implement, and surprisingly powerful. 
JAVOX grammars axe based on Sun's Java Speech 
Grammar Format (JSGF) (Sun Microsystems, Inc., 
1998). JSGF is a rule-based, speech-recognition 
grammar, designed to specify acceptable input to 
a recognizer. In JAVOX grammars, each JSGF  rule 
may be augmented with a fragment of JAVOX Script- 
ing Language code - we refer to JAVOX grammars as 
scriptable grammars. The result of parsing an utter- 
ance with a JAVOX grammar is a complete piece of 
JSL code, which is then interpreted to perform the 
action specified by the user. 
The process of speech-enabling an application in 
JAVOX consists of writing a grammar that con- 
tains the language to be used and the correspond- 
ing actions to be performed. Building on top of 
3SGF means - in many cases - only one file is 
needed to contain all application-specific informa- 
tion. JSL-specific code is automatically stripped 
from the grammar at runtime, leaving an ordinary 
JSGF grammar. This JSGF grammar is sent to a 
Java-Speech-compliant recognizer as its input gram- 
mar. In the current Java implementation, each Java 
source file (Foo. java) can have an associated JAVOX 
grammar file (Foo. gram) that contains all the infor- 
mation needed to speak to the application. Encap- 
sulating all natural anguage information in one file 
also means that porting the application to different 
languages i  far easier than in most SLSs. 
3.1 Ser ip tab le  Grammars  
Since JSGF grammars are primarily speech- 
recognition grammars, they lack the ability to en- 
code semantic information. They only possess a lim- 
ited tag mechanism. Tags allow the recognizer to 
output a canonical representation f the utterance 
instead of the recognition verbatim. For example, 
106 
publ ic  <ACTION> = move \[the\] <PART> <DIR>; 
publ ic  <PART> = eyes; 
publ ic  <PART> = ( cap I hat ); 
publ ic  <DIR> = up; 
publ ic  <DIR> = down; 
Grammar 1: A JSGF fragment from the Mr. Potato Head domain. 
the tag rm may be the output from both delete the 
file and remove it. 
Tags are not implemented in JAVOX grammars; 
instead, we augment he rules of JSGF with frag- 
ments of a scripting language, which contains much 
richer semantic information than is possible with 
tags. TRANSLATOR receives the raw utterance from 
the recognizer and translates it into the appropriate 
semantic representation. JAvox grammars do not 
mandate the syntax of the additional semantic por- 
tion. Though JSL is presented here, TRANSLATOR 
has been used to form Prolog predicates and Visual 
Basic fragments. 
JSGF rules can be explicitly made public or are 
implicitly private. Public rules can be imported by 
other grammars and can serve as the result of a 
recognition; a private rule can be used in a recog- 
nition, but cannot be the sole result. The five rules 
in Grammar 1 are from a JSGF-only grammar frag- 
ment from the Mr. Potato Head 2 domain (discussed 
later). Grammar 1 allows eight sentences, uch as 
move the eyes up, move the eyes down, move the 
cap up, move the cap down, and move cap up. Rule 
names are valid Java identifiers enclosed within an- 
gle brackets; the left-hand side (LHS) is everything 
to the left of the equality sign and the right-hand side 
(RHS) is everything to the right. JAVOX grammars 
include the standard constructs available in JSGF, 
these include: 
Impor ts  Any grammar file can be imported into 
other grammar files, though only public rules 
are exported. This allows for the creation 
of grammar libraries. When using JSL, Java 
classes can also be imported. 
Comments Grammars can be documented using 
Java comments: single-line comments ( / / )  and 
delimited ones (/* until */). 
Parenthesis Precedence can be modified with 
parentheses. 
A l te rnat ives  A vertical bar ( I ) can be used to sep- 
arate alternative elements, as in the <PART> rule 
of Grammar 1. 
Opt iona ls  Optional elements are enclosed within 
brackets (\[ and \] ), such as the in Grammar l's 
<ACTION> rule. 
2Mr. Potato Head is a registered trademark ofHasbro, Inc. 
K leene  Star  Operator  A postfix Kleene star (*) 
operator can be used to indicate that the pre- 
ceding element may occur zero or more times. 
P lus  Operator  A similar operator to indicate that 
an element may appear one or more times. 
A grammar's rules may be organized however the 
developer wishes. Some may choose to have one 
rule per utterance, while others may divide rules to 
the parts-of-speech level or group them by semantic 
value. In practice, we tend to write rules grouped by 
semantic value for nouns and verbs and at the parts- 
of-speech level for function words. Grammar 2shows 
the Mr. Potato Head grammar augmented with JSL 
fragments. 
The semantic omponent of each rule is separated 
from the RHS by a colon and delimited with a brace 
and colon ({: until :}). Using Grammar 2, the 
parse and translation for Move the cap up is shown 
in Figure 2. 
Each rule may have either one semantic fragment 
or any number of named fields. A single fragment 
is sufficient when there is a one-to-one correlation 
between a lexical item and its representation in the 
program. Occasionally, a single lexical item may re- 
quire several components to adequately express its 
meaning within a program. In Grammar 2, there 
is a one-to-one correlation between the direction of 
movement and the s l ideUp and s l ideDown func- 
tions in the <DIR> rules. These functions can also 
written as a single s l ide  function, with the direction 
of the movement given by two parametric variables 
(cos and sin). In this situation, the direction rule 
(<DIR.}/F>) needs to be expressed with two values, 
each known as a named field. The word up may be 
represented by the named fields cos and sin,  with 
the values 0 and 1 respectively. 
Another issue in JSL - which does not arise in the 
syntax-only JSGF - is the need to uniquely identify 
multiple sub-rules of the same type, when they oc- 
cur in the same rule. For example, in a geometry 
grammar, two <POINT>s may be needed in a rule to 
declare a <LINE>, as in: 
public <LINE> = make a line from 
<POINT> to <POINT> : ... 
Uniquely numbering the sub-rules eliminates the 
ambiguity as to which <POINT> is which. Numbering 
107 
publ ic  
public 
public 
public 
public 
public 
public 
public 
<ACTION> = move [the] <PART> <DIR> : {: <PART>.<DIR>();  :}; 
<PART> = eyes : {: Canvas.eyes0bj :}; 
<PART> = ( cap I hat ): {: Canvas.cap0bj :}; 
<DIR> = up : {: s l ideUp :}; 
<DIR> = down : {: s l ideDown :}; 
<ACTION_NF> = slide [the] <PART> <DIR> : {: <PART>.s l ide(<DIR:cos>,<DIR:s in>);  :}; 
<DIR_NF> = up : cos {: 0 :} 
sin {: 1 :}; 
<DIR_NF> = down : cos {: 0 :} 
sin {: -I :}; 
Grammar  2: A JAVOX grammar fragment for the Mr. Potato Head domain. 
| 
T 
? ?aava,=. e~re -Ob:l 
Up ( e =ZAdet~ 
Figure 2: The JAVOX translation process - NL  to JSL  - for Move the cap up. 
can be used in both the RttS and the semantic por- 
tion of a rule; numbering is not allowed in the LHS 
of a rule. Syntactically, sub-rules are numbered with 
a series of single quotes3: 
public <LINE> = make a line from 
<POINT'> to <POINT''> : ... 
3.2 Javox Scr ipt ing Language ( JSL)  
The JAVOX Scripting Language (JSL) is a stand- 
alone programming language, developed for use with 
the JAVOX infrastructure. JSL can be used to ma- 
nipulate a running Java program and can be thought 
of as an application-independent macro language. 
The EXECUTER module interprets JSL and per- 
forms the specified actions. The specifics of JSL 
are not important o understanding JAVOX; for this 
reason, only a brief summary is presented here. 
JSL can read of modify the contents of an ob- 
ject's fields (data members) and can execute m th- 
ods (member functions) on objects. Unlike Java, 
JSL is loosely-typed: Type checking is not done un- 
til a given method is executed. JSL has its own 
variables, which can hold objects from the host ap- 
plication; a JSL variable can store an object of 
any type and no casting is required. JSL supports 
Java's primitive types, Java's reference types (ob- 
jects), and Lisp-like lists. Though JSL does support 
3This representation is motivated by the grammars of 
(Hipp, 1992). 
Java's primitive types, they are converted into their 
reference-type equivalent. For example, an integer 
is stored as a java. lang. Integer and is converted 
back to an integer when needed. 
JSL has the standard control flow mechanisms 
found in most conventional programming languages, 
including if-else, for and while loops. With the 
exception of the evaluation of their boolean expres- 
sions, these constructs follow the syntax and behav- 
ior of their Java counterparts. Java requires that 
if-else conditions and loop termination criteria be 
a boolean value. JSL conditionals are more flexi- 
ble; in addition to booleans, it evaluates non-empty 
strings as true, empty strings as false, non-zero val- 
ues as true, zero as false, non-null objects as true, 
and nu l l  as false. 
In addition to Java's control f ow mechanisms, 
JSL also supports fo reach  loops, similar to those 
found in Perl. These loops iterate over both JSL 
lists and members of java.util.List, executing 
the associated code block on each item. JSL lists 
are often constructed by recursive rules in order to 
handle conjunctions, as seen in Section 5. 
4 Infrastructure 
The JAVOX infrastructure has been designed to com- 
pletely separate NLP  code from the application's 
code. The application still can be run without 
JAVOX, as a typical, non-speech-enabled program 
- it is only speech-enabled when run with JAVOX. 
108 
From the application's perspective, JAVOX operates 
at the systems-level and sits between the applica- 
tion and the operating system (virtual machine), as 
shown in Figure 1. TRANSLATOR interfaces with the 
speech recognizer and performs all necessary NLP. 
EXECUTER interfaces directly with the application 
and performs upcalls into the running program. 
Java has two key features that make it an ideal 
test platform for our experimental implementation: 
reflection and a redefineable loading scheme. Re- 
flection provides a running program the ability to 
inspect itself, sometimes called introspection. Ob- 
jects can determine their parent classes; every 
class is itself an object in Java (an instance of 
j ava.lang.Class). Methods, fields, constructors, 
and all class attributes can be obtained from a Class 
object. So, given an object, reflection can determine 
its class; given a class, reflection can find its meth- 
ods and fields. JAVOX uses reflection to (1) map 
from the JSL-textual representation of an object 
to the actual instance in the running program; (2) 
find the appropriate j ava.lang.reflect.Methods 
for an object/method-name combination; and (3) 
actually invoke the method, once all of its arguments 
are known. 
Reflection is very helpful in examining the appli- 
cation program's tructure; however, prior to using 
reflection, EXECUTER needs access to the objects in 
the running program. To obtain pointers to the ob- 
jects, JAVOX uses JOIE,  a load-time transformation 
tool (Cohen et al, 1998). JO IE  allows us to modify 
each application class as it is loaded into the virtual 
machine. The JAVOX transform adds code to every 
constructor in the application that registers the new 
object with Executer.  Conceptually, the following 
line is added to every constructor: 
Executer. register (this). 
This modification is done as the class is loaded, 
the compiled copy - on disk - is not changed. This 
allows the program to still be run without JhVOX, 
as a non-speech application. EXECUTER can  - once 
it has the registered objects - use reflection to ob- 
tain everything else it needs to perform the actions 
specified by the JSL. 
5 Example  
Our longest running test application has been a 
Mr. Potato Head program; that allows users to ma- 
nipulates a graphical representation of the classic 
children's toy. Its operations include those typically 
found in drawing programs, to include moving, recol- 
oring and hiding various pieces of Mr. Potato Head. 
Grammar 3 shows a portion of application's gram- 
mar needed to process the utterance Move the eyes 
and glasses up. The result of parsing this utterance 
is shown in Figure 3. 
Once TRANSLATOR has processed an utterance, it
forwards the resulting JSL fragment to EXECUTER. 
Figure 4 provides a reduced class diagram for the 
Mr. Potato Head application; the arrows correspond 
to the first iteration in the following trace. The 
following steps are performed as the JSL fragment 
from Figure 3 is interpreted: 
1. A new variable - local to EXECUTER - named 
$ i te r  is created. Any previously-declared vari- 
able with the same name is destroyed. 
2. The fo reach  loop starts by initializing the 
loop variable to the first item in the list: 
Canvas.eyes0bj. This object's name consists 
of two parts; the steps to locate the actual in- 
stance in the application are: 
(a) The first part of the name, Canvas, is 
mapped to the only instance of the Canvas 
class in the context of this application. 
JAVOX has a reference to the instance be- 
cause it registered with EXECUTER when it 
was created, thanks to a JO IE  transforma- 
tion. 
(b) The second part of the name, eyes0bj,  is 
found through reflection. Every instance of 
Canvas has a field named eyes0bj of type 
BodyPaxt. This field is the eyes0bj for 
which we are looking. 
3. Once eyes0bj is located, the appropriate 
method must be found. We determine - 
through reflection - that there are two meth- 
ods in the BodyPart class with the name move, 
as seen in Figure 4. 
4. We next examine the two arguments and de- 
termine them to be both integers. Had the ar- 
guments been objects, fields, or other method 
calls, this entire procedure would be done re- 
cursively on each. 
5. We examine each possible method and deter- 
mine that we need the one with two integer 
arguments, not the one taking a single Point  
argument. 
6. Now that we have the object, the method, and 
the arguments, the upcall is made and the 
method is executed in the application. The re- 
sult is that Mr. Potato Head's eyes move up on 
the screen. 
7. This process is repeated for glass0bj and the 
loop terminates. 
After this process, both the eyes and glasses have 
moved up 20 units and Executer waits for additional 
input. The application continues to accept mouse 
and keyboard commands, just as it would without 
speech. 
109 
public <modPOS> = move <PARTS> <DIR> : {: 
dim Slier; 
foreach $iter (<PARTS>) 
$iter.move(<DIR:X>,<DIR:Y>); 
:}; 
public <PARTS> = [<ART>] <PART> : {: [<PART>] :}; 
public <PARTS> = <PARTS> [<CONJ>] [<ART>] <PART> : {: 
public <DIR> = up : X {: 0 :} : Y {: -20 :}; 
public <DIR> = left : X {: -20 :} : Y {: 0 :}; 
public <ART> = (the [ a I an); 
public <CONJ> = ( and I plus ); 
public <PART> = eyes : {: Canvas.eyesObj :}; 
public <PART> = glasses : {: Canvas.glassObj :}; 
[<PARTS> , <PART>] : } ; 
Grammar 3: A detailed JAVOX grammar for the Mr. Potato Head domain. 
? ? r - -  +- -~ + 
<pJu~> -~> "_ c=,',vam.eye=ob::l 
I <=," : 
<co~>--~ ~ : ?+ 
" + +  I 
I <"="  I<=""I<""I I: 
<?+-'+ l ? =,.o. +,._ ,  
Figure 3: The translation process for the utterance Move the eyes and g/asses up. 
6 Discuss ion and Future Work 
In practice, building a JAvox-based, speech in- 
terface - for limited-functionality applications - is 
straightforward and reasonably quick. To date, we 
have used three diverse applications as our test plat- 
forms. Speech-enabling the last of these, an image 
manipulation program, took little more than one 
person-day. Though these applications have been 
small; we are beginning to explore JAvOX's scala- 
bility to larger applications. We are also develop- 
ing a library of JAVOX grammars for use with the 
standard Java classes. This resource will shorten 
development times even more; especially compared 
to building a SLS from the ground up. 
One of the existing challenges is to work with 
applications consisting entirely of dynamic objects, 
those that cannot be identified at load time. Some 
typical dynamic-object applications are drawing 
programs or presentation software; in both cases, 
the user creates the interesting objects during run- 
time. We have implemented a system in JSL which 
allows objects to be filtered based on an attribute, 
such as color in the utterance: Move the blue square. 
In situations where there is a one-to-one correla- 
tion between a lexical item in the grammar and an 
object in the program, it is often the case that the 
lexical item is very similar to the element's identi- 
fier. It is quite often the same word or a direct syn- 
onym. Since JAVOX is primarily performing upcalls 
based on existing functions within the program, it 
also can be predicted what type of objects will co- 
occur in utterances. In the Mr. Potato Head applio 
110 
f-/,pp f~A-d o-, ......................................................... 
~ Canv&i :,TFr 4tma 
/i I / i ?snvas { } / i J ?Snv" { } BOC~ylL-? { }
/ t L _ _  l=o~(~i==:~, 
Figure 4: A simplified class diagram for the Mr. Potato Head application. 
cation, we can assume that objects representing a 
Point or integers will occur when the user speaks 
of moving a BodyPart. We are developing a system 
to exploit hese characteristics to automatically gen- 
erate JAVOX grammars from an application's com- 
piled code. The automatically-generated grammars 
are intended to serve as a starting point for develop- 
ers - though they may certainly require some hand 
crafting. Our current, grammar-generation oolas- 
sumes the program is written with Java's standard 
naming conventions. It is imaginable that additional 
data sources - such as a sample corpus - will al- 
low us to more accurately generate grammars for an 
application. Though in its infancy, we believe this 
approach olds vast potential for SLS development. 
7 Conc lus ion  
JAVOX provides a fast and flexible method to add a 
speech-interface to existing Java applications. The 
application program requires no source-code modifi- 
cation: The JAVOX infrastructure provides all NLP 
capabilities. We have implemented a grammar and 
scripting system that is straightforward enough that 
inexperienced developers and those unfamiliar with 
NLP can learn it quickly. We have demonstrated the 
technology on several programs and are commencing 
work on more ambitious applications. The current 
implementation f JAVOX is available for download 
at: 
References 
Geoff A. Cohen, Jeffrey S. Chase, and David L. 
Kaminsky. 1998. Automatic program transforma- 
tion with JOIE. In USENIX Annual Technical 
Conference (N098), New Orleans, LA. 
D. Richard Hipp. 1992. A New Technique for Pars- 
ing Ill-formed Spoken Natural-language Dialog. 
Ph.D. thesis, Duke University. 
Andrew Pargellis, JeffKuo, and Chin-Hui Lee. 1999. 
Automatic dialogue generator creates user de- 
fined applications. In 6th European Conference on 
Speech Communication and Technology, volume 3, 
pages 1175--1178, Budapest, Hungary. 
Paul Schmidt, Sibylle Rieder, Axel Theofilidis, Mar- 
ius Groenendijk, Peter Phelan, Henrik Schulz, 
Thierry Declerck, and Andrew Brenenkamp. 
1998. Natural language access to software applica- 
tions. In Proceedings of COLING-ACL-98, pages 
1193-1197, Montreal, Quebec. 
Sun Microsystems, Inc. 1998. Java speech API spec- 
ification 1.0. 
Stephen Sutton, David G. Novick, Ronald A. Cole, 
Pieter Vermeulen, Jacques de Villiers, Johan 
Schalkwyk, and Mark Fanty. 1996. Building 
10,000 spoken-dialogue systems. In Proceedings of
the International Conference on Spoken Language 
Processing (ICSLP), pages 709--712, Philadel- 
phia, PA. 
http ://www. cs. duke. edu/~msf/j avox  
8 Acknowledgments  
This work has been partially supported by the De- 
fense Advanced Research Projects Agency under 
contract F30602-99-C-0060. 
111 
Data-Driven Strategies for an Automated Dialogue System 
Hilda HARDY, Tomek 
STRZALKOWSKI, Min WU 
ILS Institute 
University at Albany, SUNY 
1400 Washington Ave., SS262 
Albany, NY  12222   USA 
hhardy|tomek|minwu@ 
cs.albany.edu  
Cristian URSU, Nick WEBB 
Department of Computer Science 
University of Sheffield 
Regent Court, 211 Portobello St. 
Sheffield  S1 4DP   UK 
c.ursu@sheffield.ac.uk, 
n.webb@dcs.shef.ac.uk 
Alan BIERMANN, R. Bryce 
INOUYE, Ashley MCKENZIE 
Department of Computer Science 
Duke University 
P.O. Box 90129, Levine Science 
Research Center, D101  
Durham, NC  27708   USA 
awb|rbi|armckenz@cs.duke.edu 
 
Abstract 
We present a prototype natural-language 
problem-solving application for a financial 
services call center, developed as part of the 
Amiti?s multilingual human-computer 
dialogue project. Our automated dialogue 
system, based on empirical evidence from real 
call-center conversations, features a data-
driven approach that allows for mixed 
system/customer initiative and spontaneous 
conversation. Preliminary evaluation results 
indicate efficient dialogues and high user 
satisfaction, with performance comparable to 
or better than that of current conversational 
travel information systems. 
1 Introduction 
Recently there has been a great deal of interest in 
improving natural-language human-computer 
conversation. Automatic speech recognition 
continues to improve, and dialogue management 
techniques have progressed beyond menu-driven 
prompts and restricted customer responses. Yet 
few researchers have made use of a large body of 
human-human telephone calls, on which to form 
the basis of a data-driven automated system.  
The Amiti?s project seeks to develop novel 
technologies for building empirically induced 
dialogue processors to support multilingual 
human-computer interaction, and to integrate these 
technologies into systems for accessing 
information and services (http://www.dcs.shef.ac. 
uk/nlp/amities). Sponsored jointly by the European 
Commission and the US Defense Advanced 
Research Projects Agency, the Amiti?s Consortium 
includes partners in both the EU and the US, as 
well as financial call centers in the UK and France. 
A large corpus of recorded, transcribed 
telephone conversations between real agents and 
customers gives us a unique opportunity to analyze 
and incorporate features of human-human 
dialogues into our automated system. (Generic 
names and numbers were substituted for all 
personal details in the transcriptions.) This corpus 
spans two different application areas: software 
support and (a much smaller size) customer 
banking. The banking corpus of several hundred 
calls has been collected first and it forms the basis 
of our initial multilingual triaging application, 
implemented for English, French and German 
(Hardy et al, 2003a); as well as our prototype 
automatic financial services system, presented in 
this paper, which completes a variety of tasks in 
English. The much larger software support corpus 
(10,000 calls in English and French) is still being 
collected and processed and will be used to 
develop the next Amiti?s prototype. 
We observe that for interactions with structured 
data ? whether these data consist of flight 
information, spare parts, or customer account 
information ? domain knowledge need not be built 
ahead of time. Rather, methods for handling the 
data can arise from the way the data are organized. 
Once we know the basic data structures, the 
transactions, and the protocol to be followed (e.g., 
establish caller?s identity before exchanging 
sensitive information); we need only build 
dialogue models for handling various 
conversational situations, in order to implement a 
dialogue system. For our corpus, we have used a 
modified DAMSL tag set (Allen and Core, 1997) 
to capture the functional layer of the dialogues, and 
a frame-based semantic scheme to record the 
semantic layer (Hardy et al, 2003b). The ?frames? 
or transactions in our domain are common 
customer-service tasks: VerifyId, ChangeAddress, 
InquireBalance, Lost/StolenCard and Make 
Payment. (In this context ?task? and ?transaction? 
are synonymous.) Each frame is associated with 
attributes or slots that must be filled with values in 
no particular order during the course of the 
dialogue; for example, account number, name, 
payment amount, etc. 
2 Related Work 
Relevant human-computer dialogue research 
efforts include the TRAINS project and the 
DARPA Communicator program. 
The classic TRAINS natural-language dialogue 
project (Allen et al, 1995) is a plan-based system 
which requires a detailed model of the domain and 
therefore cannot be used for a wide-ranging 
application such as financial services. 
The US DARPA Communicator program has 
been instrumental in bringing about practical 
implementations of spoken dialogue systems. 
Systems developed under this program include 
CMU?s script-based dialogue manager, in which 
the travel itinerary is a hierarchical composition of 
frames (Xu and Rudnicky, 2000). The AT&T 
mixed-initiative system uses a sequential decision 
process model, based on concepts of dialog state 
and dialog actions (Levin et al, 2000). MIT?s 
Mercury flight reservation system uses a dialogue 
control strategy based on a set of ordered rules as a 
mechanism to manage complex interactions 
(Seneff and Polifroni, 2000). CU?s dialogue 
manager is event-driven, using a set of hierarchical 
forms with prompts associated with fields in the 
forms. Decisions are based not on scripts but on 
current context (Ward and Pellom, 1999). 
Our data-driven strategy is similar in spirit to 
that of CU. We take a statistical approach, in 
which a large body of transcribed, annotated 
conversations forms the basis for task 
identification, dialogue act recognition, and form 
filling for task completion.  
3 System Architecture and Components 
The Amiti?s system uses the Galaxy 
Communicator Software Infrastructure (Seneff et 
al., 1998). Galaxy is a distributed, message-based, 
hub-and-spoke infrastructure, optimized for spoken 
dialogue systems. 
  
 
Figure 1. Amiti?s System Architecture 
 
Components in the Amiti?s system (Figure 1) 
include a telephony server, automatic speech 
recognizer, natural language understanding unit, 
dialogue manager, database interface server, 
response generator, and text-to-speech conversion. 
3.1 Audio Components 
Audio components for the Amiti?s system are 
provided by LIMSI. Because acoustic models have 
not yet been trained, the current demonstrator 
system uses a Nuance ASR engine and TTS 
Vocalizer.  
To enhance ASR performance, we integrated 
static GSL (Grammar Specification Language) 
grammar classes provided by Nuance for 
recognizing several high-frequency items: 
numbers, dates, money amounts, names and yes-no 
statements. 
Training data for the recognizer were collected 
both from our corpus of human-human dialogues 
and from dialogues gathered using a text-based 
version of the human-computer system. Using this 
version we collected around 100 dialogues and 
annotated important domain-specific information, 
as in this example: ?Hi my name is [fname ; 
David] [lname ; Oconnor] and my account number 
is [account ; 278 one nine five].? 
Next we replaced these annotated entities with 
grammar classes. We also utilized utterances from 
the Amiti?s banking corpus (Hardy et al, 2002) in 
which the customer specifies his/her desired task, 
as well as utterances which constitute common, 
domain-independent speech acts such as 
acceptances, rejections, and indications of non-
understanding. These were also used for training 
the task identifier and the dialogue act classifier 
(Section 3.3.2). The training corpus for the 
recognizer consists of 1744 utterances totaling 
around 10,000 words. 
Using tools supplied by Nuance for building 
recognition packages, we created two speech 
recognition components: a British model in the UK 
and an American model at two US sites. 
For the text to speech synthesizer we used 
Nuance?s Vocalizer 3.0, which supports multiple 
languages and accents. We integrated the 
Vocalizer and the ASR using Nuance?s speech and 
telephony API into a Galaxy-compliant server 
accessible over a telephone line. 
3.2 Natural Language Understanding 
The goal of the language understanding 
component is to take the word string output of the 
ASR module, and identify key semantic concepts 
relating to the target domain. This is a specialized 
kind of information extraction application, and as 
such, we have adapted existing IE technology to 
this task.  
Hub 
Speech 
Recognition 
Dialogue 
Manager Database 
Server 
Nat?l Language 
Understanding 
Telephony 
Server 
Response      
Generation 
Customer 
Database 
Text-to-speech
Conversion 
We have used a modified version of the ANNIE 
engine (A Nearly-New IE system; Cunningham et 
al., 2002; Maynard, 2003). ANNIE is distributed as 
the default built-in IE component of the GATE 
framework (Cunningham et al, 2002). GATE is a 
pure Java-based architecture developed over the 
past eight years in the University of Sheffield 
Natural Language Processing group. ANNIE has 
been used for many language processing 
applications, in a number of languages both 
European and non-European. This versatility 
makes it an attractive proposition for use in a 
multilingual speech processing project. 
ANNIE includes customizable components 
necessary to complete the IE task ? tokenizer, 
gazetteer, sentence splitter, part of speech tagger 
and a named entity recognizer based on a powerful 
engine named JAPE (Java Annotation Pattern 
Engine; Cunningham et al, 2000). 
Given an utterance from the user, the NLU unit 
produces both a list of tokens for detecting 
dialogue acts, an important research goal inside 
this project, and a frame with the possible named 
entities specified by our application. We are 
interested particularly in account numbers, credit 
card numbers, person names, dates, amounts of 
money, locations, addresses and telephone 
numbers.  
In order to recognize these, we have updated the 
gazetteer, which works by explicit look-up tables 
of potential candidates, and modified the rules of 
the transducer engine, which attempts to match 
new instances of named entities based on local 
grammatical context. There are some significant 
differences between the kind of prose text more 
typically associated with information extraction, 
and the kind of text we are expecting to encounter. 
Current models of IE rely heavily on punctuation 
as well as certain orthographic information, such as 
capitalized words indicating the presence of a 
name, company or location. We have access to 
neither of these in the output of the ASR engine, 
and so had to retune our processors to data which 
reflected that. 
In addition, we created new processing 
resources, such as those required to spot number 
units and translate them into textual representations 
of numerical values; for example, to take ?twenty 
thousand one hundred and fourteen pounds?, and 
produce ??20,114?. The ability to do this is of 
course vital for the performance of the system. 
If none of the main entities can be identified 
from the token string, we create a list of possible 
fallback entities, in the hope that partial matching 
would help narrow the search space. 
For instance, if a six-digit account number is not 
identified, then the incomplete number recognized 
in the utterance is used as a fallback entity and sent 
to the database server for partial matching. 
Our robust IE techniques have proved 
invaluable to the efficiency and spontaneity of our 
data-driven dialogue system. In a single utterance 
the user is free to supply several values for 
attributes, prompted or unprompted, allowing tasks 
to be completed with fewer dialogue turns. 
3.3 Dialogue Manager 
The dialogue manager identifies the goals of the 
conversation and performs interactions to achieve 
those goals. Several ?Frame Agents?, implemented 
within the dialogue manager, handle tasks such as 
verifying the customer?s identity, identifying the 
customer?s desired transaction, and executing those 
transactions. These range from a simple balance 
inquiry to the more complex change of address and 
debit-card payment. The structure of the dialogue 
manager is illustrated in Figure 2. 
Rather than depending on a script for the 
progression of the dialogue, the dialogue manager 
takes a data-driven approach, allowing the caller to 
take the initiative. Completing a task depends on 
identifying that task and filling values in frames, 
but this may be done in a variety of ways: one at a 
time, or several at once, and in any order. 
For example, if the customer identifies himself 
or herself before stating the transaction, or even if 
he or she provides several pieces of information in 
one utterance?transaction, name, account number, 
payment amount?the dialogue manager is flexible 
enough to move ahead after these variations. 
Prompts for attributes, if needed, are not restricted 
to one at a time, but they are usually combined in 
the way human agents request them; for example, 
city and county, expiration date and issue number, 
birthdate and telephone number. 
 
 
 
Figure 2. Amiti?s Dialogue Manager 
If the system fails to obtain the necessary values 
from the user, reprompts are used, but no more 
than once for any single attribute. For the customer 
verification task, different attributes may be 
 
 
 
 
 
 
 
 
 Response Decision 
Input:  
from NLU via 
Hub (token string, 
language id, 
named entities) 
Task infoExternal files, 
domain-specific
Dialogue Act 
Classifier 
Frame Agent 
Task ID 
Frame Agent 
Verify-Caller 
Frame Agent 
DB Server 
Customer 
Database
 
 
 
 
 
 
Task Execution 
Frame Agents via Hub 
Dialogue History 
requested. If the system fails even after reprompts, 
it will gracefully give up with an explanation such 
as, ?I?m sorry, we have not been able to obtain the 
information necessary to update your address in 
our records. Please hold while I transfer you to a 
customer service representative.? 
3.3.1 Task ID Frame Agent 
For task identification, the Amiti?s team has 
made use of the data collected in over 500 
conversations from a British call center, recorded, 
transcribed, and annotated. Adapting a vector-
based approach reported by Chu-Carroll and 
Carpenter (1999), the Task ID Frame Agent is 
domain-independent and automatically trained. 
Tasks are represented as vectors of terms, built 
from the utterances requesting them. Some 
examples of labeled utterances are: ?Erm I'd like to 
cancel the account cover premium that's on my, 
appeared on my statement? [CancelInsurance] and 
?Erm just to report a lost card please? 
[Lost/StolenCard].   
The training process proceeds as follows: 
1. Begin with corpus of transcribed, annotated 
calls. 
2. Document creation: For each transaction, collect 
raw text of callers? queries. Yield: one 
?document? for each transaction (about 14 of 
these in our corpus). 
3. Text processing: Remove stopwords, stem 
content words, weight terms by frequency. 
Yield: one ?document vector? for each task. 
4. Compare queries and documents: Create ?query 
vectors.? Obtain a cosine similarity score for 
each query/document pair. Yield: cosine 
scores/routing values for each query/document 
pair. 
5. Obtain coefficients for scoring: Use binary 
logistic regression. Yield: a set of coefficients 
for each task. 
Next, the Task ID Frame Agent is tested on 
unseen utterances or queries: 
1. Begin with one or more user queries. 
2. Text processing: Remove stopwords, stem 
content words, weight terms (constant weights). 
Yield: ?query vectors?. 
3. Compare each query with each document. 
Yield: cosine similarity scores. 
4. Compute confidence scores (use training 
coefficients). Yield: confidence scores, 
representing the system?s confidence that the 
queries indicate the user?s choice of a particular 
transaction. 
Tests performed over the entire corpus, 80% of 
which was used for training and 20% for testing, 
resulted in a classification accuracy rate of 85% 
(correct task is one of the system?s top 2 choices). 
The accuracy rate rises to 93% when we eliminate 
confusing or lengthy utterances, such as requests 
for information about payments, statements, and 
general questions about a customer?s account. 
These can be difficult even for human annotators 
to classify. 
3.3.2 Dialogue Act Classifier 
The purpose of the DA Classifier Frame Agent 
is to identify a caller?s utterance as one or more 
domain-independent dialogue acts. These include 
Accept, Reject, Non-understanding, Opening, 
Closing, Backchannel, and Expression. Clearly, it 
is useful for a dialogue system to be able to 
identify accurately the various ways a person may 
say ?yes?, ?no?, or ?what did you say?? As with 
the task identifier, we have trained the DA 
classifier on our corpus of transcribed, labeled 
human-human calls, and we have used vector-
based classification techniques. Two differences 
from the task identifier are 1) an utterance may 
have multiple correct classifications, and 2) a 
different stoplist is necessary. Here we can filter 
out the usual stops, including speech dysfluencies, 
proper names, number words, and words with 
digits; but we need to include words such as yeah, 
uh-huh, hi, ok, thanks, pardon and sorry.  
Some examples of DA classification results are 
shown in Figure 3. For sure, ok, the classifier 
returns the categories Backchannel, Expression and 
Accept. If the dialogue manager is looking for 
either Accept or Reject, it can ignore Backchannel 
and Expression in order to detect the correct 
classification. In the case of certainly not, the first 
word has a strong tendency toward Accept, though 
both together constitute a Reject act.  
 
Text: ?sure, okay? Text: ?certainly not?
Categories returned: Backchannel, 
Expression, Accept 
Categories returned:
Reject, Accept 
Expression
Closing
Accept
Back.
0
0.2
0.4
0.6
0.8
1
Top four cosine scores
Expression
Accept Closing
Back.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Confidence scores
Reject
Reject-part
Accept Expression
0
0.1
0.2
0.3
0.4
0.5
0.6
Top four cosine scores
Reject
Accept Expression
Reject-part
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Confidence scores
Figure 3. DA Classification examples 
 
Our classifier performs well if the utterance is 
short and falls into one of the selected categories 
(86% accuracy on the British data); and it has the 
advantages of automatic training, domain 
independence, and the ability to capture a great 
variety of expressions. However, it can be 
inaccurate when applied to longer utterances, and it 
is not yet equipped to handle domain-specific 
assertions, questions, or queries about a 
transaction. 
3.4 Database Manager 
Our system identifies users by matching 
information provided by the caller against a 
database of user information. It assumes that the 
speech recognizer will make errors when the caller 
attempts to identify himself. Therefore perfect 
matches with the database entries will be rare. 
Consequently, for each record in the database, we 
attach a measure of the probability that the record 
is the target record. Initially, these measures are 
estimates of the probability that this individual will 
call. When additional identifying information 
arrives, the system updates these probabilities 
using Bayes? rule. 
Thus, the system might begin with a uniform 
probability estimate across all database records. If 
the user identifies herself with a name recognized 
by the machine as ?Smith?, the system will 
appropriately increment the probabilities of all 
entries with the name ?Smith? and all entries that 
are known to be confused with ?Smith? in 
proportion to their observed rate of substitution. Of 
course, all records not observed to be so 
confusable would similarly have their probabilities 
decreased by Bayes? rule. When enough 
information has come in to raise the probability for 
some record above a threshold (in our system 0.99 
probability), the system assumes that the caller has 
been correctly identified. The designer may choose 
to include a verification dialog, but our decision 
was to minimize such interactions to shorten the 
calls.  
Our error-correcting database system receives 
tokens with an identification of what field each 
token should represent. The system processes the 
tokens serially. Each represents an observation 
made by the speech recognizer. To process a token, 
the system examines each record in the database 
and updates the probability that the record is the 
target record using Bayes? rule: 
 
 
  
where rec is the event where the record under 
consideration is the target record.  
As is common in Bayes? rule calculations, the 
denominator P(obs) is treated as a scaling factor, 
and is not calculated explicitly. All probabilities 
are renormalized at the end of the update of all of 
the records. P(rec) is the previous estimate of the 
probability that the record is the target record. 
P(obs|rec) is the probability that the recognizer 
returned the observation that it did given that the 
target record is the current record under 
examination. For some of the fields, such as the 
account number and telephone number, the user 
responses consist of digits. We collected data on 
the probability that the speech recognition system 
we are using mistook one digit for another and 
calculated the values for P(obs|rec) from the data. 
For fields involving place names and personal 
names, the probabilities were estimated.  
Once a record has been selected (by virtue of its 
probability being greater than the threshold) the 
system compares the individual fields of the record 
with values obtained by the speech recognizer. If 
the values differ greatly, as measured by their 
Levenshtein distance, the system returns the field 
name to the dialogue manager as a candidate for 
additional verification. If no record meets the 
threshold probability criterion, the system returns 
the most probable record to the dialogue manager, 
along with the fields which have the greatest 
Levenshtein distance between the recognized and 
actual values, as candidates for reprompting.  
Our database contains 100 entries for the system 
tests described in this paper. We describe the 
system in a more demanding environment with one 
million records in Inouye et al (2004). In that 
project, we required all information to be entered 
by spelling the items out so that the vocabulary 
was limited to the alphabet plus the ten digits. In 
the current project, with fewer names to deal with, 
we allowed the complete vocabulary of the 
domain: names, streets, counties, and so forth.  
3.5 Response Generator 
Our current English-only system preserves the 
language-independent features of our original tri-
lingual generator, storing all language- and 
domain-specific information in separate text files. 
It is a template-based system, easily modified and 
extended. The generator constructs utterances 
according to the dialogue manager?s specification 
of one or more speech acts (prompt, request, 
confirm, respond, inform, backchannel, accept, 
reject), repetition numbers, and optional lists of 
attributes, values, and/or the person?s name. As far 
as possible, we modeled utterances after the 
human-human dialogues. 
For a more natural-sounding system, we 
collected variations of the utterances, which the 
generator selects at random. Requests, for 
example, may take one of twelve possible forms: 
Request, part 1 of 2: 
Can you just confirm | Can I have | Can I take | 
What is | What?s | May I have 
)(
)()|()|(
obsP
recPrecobsPobsrecP ?=
Request, part 2 of 2: 
[list of attributes], [person name]? | [list of 
attributes], please? 
Offers to close or continue the dialogue are 
similarly varied: 
Closing offer, part 1 of 2: 
Is there anything else | Anything else | Is there 
anything else at all 
Closing offer, part 2 of 2: 
I can do for you today? | I can help you with 
today? | I can do for you? | I can help you with? | 
you need today? | you need? 
4 Preliminary Evaluation 
Ten native speakers of English, 6 female and 4 
male, were asked to participate in a preliminary in-
lab system evaluation (half in the UK and half in 
the US). The Amiti?s system developers were not 
among these volunteers. Each made 9 phone calls 
to the system from behind a closed door, according 
to scenarios designed to test various customer 
identities as well as single or multiple tasks. After 
each call, participants filled out a questionnaire to 
register their degree of satisfaction with aspects of 
the interaction. 
Overall call success was 70%, with 98% 
successful completions for the VerifyId and 96% 
for the CheckBalance subtasks (Figure 4). 
?Failures? were not system crashes but simulated 
transfers to a human agent. There were 5 user 
terminations. 
Average word error rates were 17% for calls that 
were successfully completed, and 22% for failed 
calls. Word error rate by user ranged from 11% to 
26%. 
 
0.70
0.98 0.96
0.88 0.90
0.57
0.85
0.00
0.20
0.40
0.60
0.80
1.00
1.20
Ca
ll S
uc
ce
ss
Ve
rify
Id
Ch
ec
kB
ala
nc
e
Lo
stC
ar
d
Ma
ke
Pa
ym
en
t
Ch
an
ge
Ad
dr
es
s
Fin
ish
Di
alo
gu
e
 
Figure 4. Task Completion Rates 
Call duration was found to reflect the 
complexity of each scenario, where complexity is 
defined as the number of ?concepts? needed to 
complete each task. The following items are 
judged to be concepts: task identification; values 
such as first name, last name, house number, street 
and phone number; and positive or negative 
responses such as whether a new card is desired. 
Figures 5 and 6 illustrate the relationship between 
length of call and task complexity. It should be 
noted that customer verification, a task performed 
in every dialogue, requires a minimum of 3 
personal details to be verified against a database 
record, but may require more in the case of 
recognition errors. 
The overall average number of turns per 
dialogue was 18.28. The user spoke an average of 
6.89 words per turn and the system 11.42. 
User satisfaction for each call was assessed by 
way of a questionnaire containing five statements. 
These covered the clarity of the instructions, ease 
of doing the task, how well the system understands 
the caller, how well the system works, and the 
caller?s enjoyment of the system. Participants rated 
each on a five-point Likert scale. Summed results 
showed an average score of 20.45 over all users 
(range 5?25; higher = stronger agreement). 
 
0
2
4
6
8
10
12
1 2 3 4 5 6 7 8 9
Scenario
Nu
m
be
r o
f c
on
ce
pt
s
 
85.0
135.8
119.3
88.3
106.7 114.0
48.4
109.3
127.1
0.0
20.0
40.0
60.0
80.0
100.0
120.0
140.0
160.0
1 2 3 4 5 6 7 8 9
Figures 5 and 6. Scenario Complexity (top) and 
Average Call Duration by Scenario (in seconds) 
Although user satisfaction was high, we were 
more interested in identifying the major problems 
for the callers. Users were often frustrated by 
recognition failures and/or unsuccessful attempts 
to capture values such as a new street address, 
county, or phone number. Sometimes the system 
failed to determine that the user was finished. 
Because the system is designed to be non-
restrictive in accepting users? input, 
misidentifications were common. One user spoke 
for many by saying, ?There were times when the 
system would ask me to repeat information I had 
not yet given to it.? We plan to revise our strategy 
so that we are a little more cautious in our 
reprompts. Occasionally, the system misidentified 
the user?s desired task and had difficulty backing 
off gracefully and starting the correct task. We are 
working on improving our recovery strategies for 
these cases. 
 The following transcription of an interaction 
with our system illustrates the efficiency made 
possible when the data supplied by the caller drives 
the conversation: 
AMITI?S:  Hello, this is Am-ih-tee-eh! What can 
I do for you? 
USER:  yes this is Stella Lang. My card number 
is three one six seven one four. Can you tell me the 
balance on my account please? 
AMITI?S:  Can you just confirm the city and 
county? 
USER:  uh yes it?s London, Middlesex 
AMITI?S:  I?m sorry, I didn?t understand what 
you said.  Can you repeat, please? 
USER:  London, Middlesex 
AMITI?S:  All right, your balance is 1753 
pounds and 20 pence. Anything else I can do for 
you? 
USER:  yes I'd like to make a payment of fifty 
three pounds with my debit card please 
AMITI?S:  Can I have the debit card number, 
Miss Lang? 
USER:  sure, it's two eight seven four six six four 
nine five eight nine five three eight nine one, and it 
expires October two thousand six 
AMITI?S:  What's the issue number, please? 
USER:  one 
AMITI?S:  Ok, Miss Lang, I'll send in the 
payment amount of 53 pounds. Did I get that right? 
USER:  yes, what's my new balance please? 
AMITI?S:  Ok, that's gone through. Your 
payment will be credited overnight and will reflect 
on your next statement. Your balance is 1700 
pounds and 20 pence. Anything else I can do for 
you today? 
USER:  no, that's it thanks 
AMITI?S:  Thanks very much for your call, 
goodbye. 
USER:  bye 
5 Discussion, Future Work 
The preliminary evaluation reported here 
indicates promise for an automated dialogue 
system such as ours, which incorporates robust 
techniques for information extraction, record 
matching, task identification, dialogue act 
classification, and an overall data-driven strategy. 
Task duration and number of turns per dialogue 
both appear to indicate greater efficiency and 
corresponding user satisfaction than many other 
similar systems. In the DARPA Communicator 
evaluation, for example, between 60 and 79 calls 
were made to each of 8 participating sites (Walker, 
et al, 2001, 2002). A sample scenario for a 
domestic round-trip flight contained 8 concepts 
(airline, departure city, state, date, etc.). The 
average duration for such a call was over 300 
seconds; whereas our overall average was 104 
seconds. ASR accuracy rates in 2001 were about 
60% and 75%, for airline itineraries not completed 
and completed; and task completion rates were 
56%. Our average number of user words per turn, 
6.89, is also higher than that reported for 
Communicator systems. This number seems to 
reflect lengthier responses to open prompts, 
responses to system requests for multiple 
attributes, and greater user initiative. 
We plan to port the system to a new domain: 
from telephone banking to information-technology 
support. As part of this effort we are again 
collecting data from real human-human calls. For 
advanced speech recognition, we hope to train our 
ASR on new acoustic data. We also plan to expand 
our dialogue act classification so that the system 
can recognize more types of acts, and to improve 
our classification reliability.  
6 Acknowledgements 
This paper is based on work supported in part by 
the European Commission under the 5th 
Framework IST/HLT Programme, and by the US 
Defense Advanced Research Projects Agency. 
References 
J. Allen and M. Core. 1997. Draft of DAMSL: 
Dialog Act Markup in Several Layers. 
http://www.cs.rochester.edu/research/cisd/resour
ces/damsl/. 
J. Allen, L. K. Schubert, G. Ferguson, P. Heeman, 
Ch. L. Hwang, T. Kato, M. Light, N. G. Martin, 
B. W. Miller, M. Poesio, and D. R. Traum. 
1995. The TRAINS Project: A Case Study in 
Building a Conversational Planning Agent. 
Journal of Experimental and Theoretical AI, 7 
(1995), 7?48. 
Amiti?s, http://www.dcs.shef.ac.uk/nlp/amities.  
J. Chu-Carroll and B. Carpenter. 1999. Vector-
Based Natural Language Call Routing. 
Computational Linguistics, 25 (3): 361?388. 
H. Cunningham, D. Maynard, K. Bontcheva, V. 
Tablan. 2002. GATE: A Framework and 
Graphical Development Environment for Robust 
NLP Tools and Applications. Proceedings of the 
40th Anniversary Meeting of the Association for 
Computational Linguistics (ACL'02), 
Philadelphia, Pennsylvania. 
H. Cunningham and D. Maynard and V. Tablan. 
2000. JAPE: a Java Annotation Patterns Engine 
(Second Edition). Technical report CS--00--10, 
University of Sheffield, Department of 
Computer Science.  
DARPA, 
http://www.darpa.mil/iao/Communicator.htm. 
H. Hardy, K. Baker, L. Devillers, L. Lamel, S. 
Rosset, T. Strzalkowski, C. Ursu and N. Webb. 
2002. Multi-Layer Dialogue Annotation for 
Automated Multilingual Customer Service. 
Proceedings of the ISLE Workshop on Dialogue 
Tagging for Multi-Modal Human Computer 
Interaction, Edinburgh, Scotland. 
H. Hardy, T. Strzalkowski and M. Wu. 2003a. 
Dialogue Management for an Automated 
Multilingual Call Center. Research Directions in 
Dialogue Processing, Proceedings of the HLT-
NAACL 2003 Workshop, Edmonton, Alberta, 
Canada. 
H. Hardy, K. Baker, H. Bonneau-Maynard, L. 
Devillers, S. Rosset and T. Strzalkowski. 2003b. 
Semantic and Dialogic Annotation for 
Automated Multilingual Customer Service. 
Eurospeech 2003, Geneva, Switzerland. 
R. B. Inouye, A. Biermann and A. Mckenzie. 
2004. Caller Identification from Spelled-Out 
Personal Data Using a Database for Error 
Correction. Duke University Internal Report. 
E. Levin, S. Narayanan, R. Pieraccini, K. Biatov, 
E. Bocchieri, G. Di Fabbrizio, W. Eckert, S. 
Lee, A. Pokrovsky, M. Rahim, P. Ruscitti, and 
M. Walker. 2000. The AT&T-DARPA 
Communicator Mixed-Initiative Spoken Dialog 
System. ICSLP 2000. 
D. Maynard. 2003. Multi-Source and Multilingual 
Information Extraction. Expert Update. 
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, 
and V. Zue. 1998. Galaxy-II: A Reference 
Architecture for Conversational System 
Development. ICSLP 98, Sydney, Australia. 
S. Seneff and J. Polifroni. 2000. Dialogue 
Management in the Mercury Flight Reservation 
System. Satellite Dialogue Workshop, ANLP-
NAACL, Seattle, Washington. 
M. Walker, J. Aberdeen, J. Boland, E. Bratt, J. 
Garofolo, L. Hirschman, A. Le, S. Lee, S. 
Narayanan, K. Papineni, B. Pellom, J. Polifroni, 
A. Potamianos, P. Prabhu, A. Rudnicky, G. 
Sanders, S. Seneff, D. Stallard and S. Whittaker. 
2001. DARPA Communicator Dialog Travel 
Planning Systems: The June 2000 Data 
Collection. Eurospeech 2001. 
M. Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J. 
Garofolo, H. Hastie, A. Le, B. Pellom, A. 
Potamianos, R. Passonneau, R. Prasad, S. 
Roukos, G. Sanders, S. Seneff and D. Stallard. 
2002. DARPA Communicator Evaluation: 
Progress from 2000 to 2001. ICSLP 2002. 
W. Ward and B. Pellom. 1999. The CU 
Communicator System. IEEE ASRU, pp. 341?
344. 
W. Xu and A. Rudnicky. 2000. Task-based Dialog 
Management Using an Agenda. ANLP/NAACL 
Workshop on Conversational Systems, pp. 42?
47. 
 
A Measure  of Semant ic  Complex i ty  for Natura l  Language Systems 
Shannon Pollard*and Alan W. Biermann 
Department ofComputer Science, Duke University 
Box 90129, D224, LSRC, Durham, NC 27708-0129 
office: (919)660-6583 fax: (919)660-6519 
e-mail: shannon@cs.duke.edu 
Abstract 
This paper will describe a way to organize the salient 
objects, their attributes, and relationships between 
the objects in a given domain. This organization 
allows us to assign an information value to each col- 
lection, and to the domain as a whole, which cor- 
responds to the number of things to "talk about" 
in the domain. This number gives a measure of se- 
mantic complexity; that is, it will correspond to the 
number of objects, attributes, and relationships in 
the domain, but not to the level of syntactic diver- 
sity allowed when conveying these meanings. 
Defining a measure of semantic omplexity for a 
dialog system domain will give an insight towards 
making a complexity measurement standard. With 
such a standard, natural anguage programmers can 
measure the feasibility of making a natural lan- 
guage interface, compare different language proces- 
sors' ability to handle more and more complex do- 
mains, and quantify the abilities of the current state 
of the art in natural anguage processors. 
1 Introduction 
Quantification of task difficulty has been applied to 
many areas in artificial intelligence, including in- 
formation retrieval (Bagga, 1997) (Bagga and Bier- 
mann, 1997), machine learning (Niyogi, 1996), pars- 
ing and grammatical formalisms(G. Edward Bar- 
ton et al, 1987), and language learning in general 
(Ristad, 1993). In addition to providing a way of 
comparing systems, these measures quantify task 
complexity before a system is built. The goal of 
this paper is to measure the complexity of domains 
for dialog processing. With a standard measure of 
complexity, domains can be compared and analyzed 
without having to build the dialog system first. This 
measure would be an indication of the cost, amount 
of code, accuracy, reliability, and execution time of 
the finished dialog system specified by the domain. 
The hope is to have a single number or pair of num- 
bers that correlates trongly with these standard 
measures. 
* Supported by the Defense Advanced Research Projects 
Agency, CPoF project, Grant F30602-99-C-0060 
Specifically, if domain D1 has complexity C1 and 
domain D2 has complexity C2 where C2 > C1, then 
we would expect D2 to have a greater cost of soft- 
ware, more lines of code, less accuracy, less reliabil- 
ity, and longer execution time. 
Section 2 will describe the difference in seman- 
tic and syntactic complexity and explain why we 
consider each separately. In section 3 we define the 
terms in the complexity analysis, which is explained 
in section 4. Sections 5and 6 discuss how to compute 
information measures that are needed in the com- 
plexity analysis, and in sections 7 and 8 we present 
future work and conclude. 
2 Semant ic  vs .  Syntact i c  complex i ty  
The complexity measurement described above must 
be one that takes into account both the semantic and 
syntactic omplexity of the domain. Semantic om- 
plexity is the number of "things" that we can talk 
about in the domain. This will include all the ob- 
jects in the domain, the attributes of those objects to 
which one might refer, and the relationships between 
the objects that the user can express. Syntactic om- 
plexity refers to the variety of ways that the user will 
be allowed to refer to an object, attribute, or rela- 
tionship. For example, a domain could include only 
two boys but if the user is allowed to refer to them in 
many ways (e.g., "Bob", "Jim", "he", "they", "the 
two boys next to the water cooler at the back of the 
room"), then the domain is semantically simple but 
syntactically complex. Likewise a domain with 100 
objects that are each referred to only as Object1, 
Object2, etc.., is semantically complex but syntac- 
tically simple. 
Semantic and syntactic omplexities form a trade- 
Off when it comes to building a language processor 
for a domain. To build a reliable and accurate pro- 
cessor, the domain must be sufficiently restrained. 
The more syntactic variety allowed the user, the 
fewer objects allowed in the domain. So, the more 
objects in the world, the more restricted the user's 
grammar and vocabulary. This leads to a tendency 
to consider the two fronts separately, and then con- 
sider a complete complexity measure as a combina- 
42 
tion of both. Having measures of syntactic and se- 
mantic complexity separately will help to find where 
the best compromise lies. 
This paper addresses semantic omplexity only. It 
therefore does not completely define the complexity 
measure described in the introduction, but hopefully 
takes a step toward defining such a measure. Syntac- 
tic complexity measures such as grammar perplexity 
(Cole and Zue, 1995) should augment his semantic 
measure to give a full complexity measure. 
3 Domain Terms 
To analyze a domain's complexity, the domain 
expert must first specify the domain in which 
the system will work by determining the objects 
in the domain, each object's attributes, and the 
relationships between objects. Consider as an 
example the small domain of a simple army map, 
where there are a few objects on the map and the 
user can display, move, and show or set attributes 
of them. This example will be used to show how to 
define a domain using the following terms: 
Objects are the types of salient things in the 
domain. They correspond roughly to the subjects 
and objects of sentences used in the dialog. In the 
army display domain, the objects will be tanks, 
troops, bridges, forests, and hills. Notice that a 
type of object only needs to be specified once at this 
high level. Bridge is one object in our world, even 
though the actual program is able to distinguish 
many different bridges. 
Attributes of an object are the things that the 
program needs to know about the object in order to 
use it in the domain. They correspond roughly to 
adjectives that describe the object, or things that 
distinguish one of the objects from the others of 
that type. In our example, the domain requires the 
name and position of the bridge and the material 
of which the bridge is made. These three pieces of 
information include everything the system needs to 
know about any bridge. In the following figure, the 
attributes of an object are listed underneath each 
object type. 
Classes are objects, attributes, predicates, or 
other classes that are grouped together. A class 
can act as an object in the sense that it can have 
a name and have relationships with other objects. 
In our example domain, we will want to distinguish 
objects that can move from those that cannot, i.e., 
a MobileObject class as a grouping of Tanks and 
Troops. There are always three trivial classes: the 
class of all objects, all attributes (of all objects), 
and all predicates. 
43 
Tank i T roop  Bridge 
F~d/F~ FdcmMFoe Name 
ID No. i Nan~ 
Pcdd~ion Po6llloa Poattlola 
Range or sight ~ of sight Matcdal 
Range of art I~?  of art. 
I 
Forest Hi l l  
Name Name 
Area 
Arm , Elcvadon 
Politloa I Pomttion 
Figure 1: Example Domain Objects and Attributes 
Predicates are the relationships between the 
objects in the world. Any meaning that the user 
can convey using one or more of the objects hould 
be represented by a predicate. They correspond 
to the relationship words, like the verbs and 
prepositions in a sentence, and one can usually 
find the predicates needed from looking at the 
allowed operations. For the example domain, the 
following is the list of allowable predicates, in a 
typical programming language format to distinguis h 
predicates from arguments. 
Display(Object) \["Display the tanks"\] 
Move(MobileObject,Object) \["Move Troop at posi- 
tion 100, 400 to the hill"\] 
Show(Attribute,Object) \["Show the range of sight 
of Tank 434"\] 
Set(Object,Attribute,Attribute) \["The forest has an 
area of 100 square yards."\] 
Notice that classes can be written as predicate 
arguments to mean that any object in the class can 
be an argument. Specifically, the Object type refers 
to all objects, MobileObject refers to either Tank or 
Troop, and Attribute refers to any object's attribute. 
4 Complex i ty  Formulas  
Now that the domain is specified, we can anMyze 
its semantics by estimating the number of bits of in- 
formation conveyed by referring to each different as- 
pect of the domain. This is common in information 
theory (Ash, 1965); that is, when the user makes a 
statement, it must be encoded, and the number of 
bits needed to encode the statement is a measure of 
its information content. Since the number of bits re- 
quired to encode a statement in a given domain cor- 
responds directly to the number of salient objects, 
this information measurement is useful in assigning 
a semantic omplexity measurement. 
To get a. complexity measure for an entire do- 
main, we begin at the lowest level and make counts 
corresponding to the information content described 
above. The counts from lower levels are combined to 
give a higher level count. Specifically, first each at- 
tribute value for a specific object is computed, then 
attribute values are combined to give object values, 
which are combined to give class values, and so forth 
until a value for the entire domain is computed. 
Define B(X) to be the number of bits conveyed by 
an instance of random variable X, and IX\] to be the 
number of possible values of X. (Possible ways of 
computing B(X) will be given in the next sections.) 
The random variable will represent different events, 
depending on where we are in the complexity anal- 
ysis, but in general, the variable will represent the 
specification of possible attributes, objects, classes, 
or predicates. 
We start by defining the complexity of a single 
attribute for a single object. We give the formu- 
las for computing the different levels of complex- 
ity (attribute level, object level, etc) and then work 
through the example domain. 
The complexity of attribute i for object j, denoted 
ACatt~,obji is 
AGtt,,obji = B(A) 
where A is the specification of an attribute value. 
The object complexity of object j is the sum of all 
its attributes' complexities: 
OC?bj$ "~- E ACatt~,obji 
i 
A simple sum is used because identifying one ob- 
ject uniquely corresponds to knowing each of its at- 
tributes. Therefore, the sum of the attribute infor- 
mation is the same as the complete object informa- 
tion. 
Since objects can be grouped together into classes, 
a class complexity is the number of bits conveyed by 
distinguishing one type of object from that class, 
plus the maximum object complexity that occurs in 
that class: 
CC.,... = B(O) + max (OCob#) 
obj~class 
where O is the specification of an object in class. 
When a member of a class is specified, the amount 
of information conveyed is equal to the information 
in the object type specification (B(O)), plus the in- 
formation conveyed by the actual object itself. The 
most that can be is the maximum object complexity 
in the class. Classes of predicates and attributes are 
defined in the same way. 
For each predicate, the complexity is the sum of 
the complexities of its arguments: 
PCpred= E CC?,ass 
classearg 
This is the same as the object complexity as a sum 
of the complexities of its attributes. 
In general, predicate arguments will be classes. If 
a single object is the only possibility for an argu- 
ment rather than a class of objects, then the object 
complexity can be used. This would be the same as 
making a class of one object: the class complexity 
of one object is equal to the complexity of the one 
member of the class. 
The entire domain's emantic omplexity is then 
the same as the complexity of the class of all predi- 
cates defined for the domain. Specifically, for a do- 
main with a set of predicates P, the semantic om- 
plexity SEMC is 
SEMC = B(P) + max PCpred pred~P 
where P is the specification of a predicate in the 
domain. 
Any statement that the user can make should cor- 
respond to some predicate in the domain model. The 
information given in the sentence is the information 
given by the predicate specification (B(P)) plus the 
information given in the arguments to the predicate, 
which is as much as the greatest predicate complex- 
ity. 
5 Us ing  Equa l  P robab i l i ty  
Assumpt ions  
Now we find a formula for B(X), the bits of infor- 
mation conveyed when referring to certain parts of 
the domain. For the army map example, we assume 
that all objects are equally likely to be referred to, 
and all attributes, classes, and relationships are also 
equally likely. So a troop is as likely to be referred 
to as a tank, or as a forest, etc. Also, a tank on the 
map is equally likely to be friend, foe, or unknown. 
Every value for the attributes will be equally likely. 
Under this assumption, the number of bits of in- 
formation conveyed by referring to one entity out of 
v possible entities is log2v. That is, for the equally 
probable case, B(X) = log2\[X\[. 
Now we fill in the table from Figure 1, beginning 
with attribute values. A domain expert would decide 
how many different values are allowed for each at- 
tribute. In this example, we will specify that Tank's 
Priend/Foe value is either friend, foe, or unknown -
three possibilities. 
ACFriend/Foe,Tank -~- Iog23 ~ 2 
Assuming that there are 128 ID number possibili- 
ties, 65,000 positions, and 1,000 possible ranges, and 
assuming equal probability, we take the log of each 
number and fill in the complexity beside each at- 
tribute for that object. Following the hierarchy, we 
now add the attribute complexities to get the com- 
plexity of the tank object. 
44 
MoblleObJect 
Tank 45 
Fdend/Foe 2 
ID No. 7 
Position 16 
Range of sight 10 
Range of art. 10 
43 Troop 
Friend/Foe 2 
Name 5 
Position 16 
Range of sight 10 
Range of art. 10 
Bridge 
Name 
Position 
Material 
21 
2 
16 
Forest 2s Hill 
Name 2 Name 
Area 
Area 10 
Elevation 
Position 16 Position 
33 
2 
10 
5 
16 
Object 
Attribute 20 
Oblect 48 
MoblleObject 46 
Display(Object) 
Move(MobileObject, Object) 
Show(Attribute,Object) 
Set(Object,Attribute,Attribute) 
48 Total Semantic 
Complexity 
94 = 96 
68 
88 
Figure 2: Map Domain Complexity Analysis under the Equal Probability Assumption 
Now we have OCtank = 45 and let's say in like 
manner we get OCtroop = 43. These two types of 
objects comprise the MobileObject class, so now we 
can compute this complexity: 
CCMobileObject = log2 2 + maxobj~MobileObject (OCobj) 
= 1 + OC~.k  
= 46 
Similar formulas are used for predicate and com- 
plete domain complexity measurements, and the rest 
of the example should be obvious from Figure 2. 
6 More  Genera l  In fo rmat ion  
Measurement  
In most cases, the equal probability assumption will 
not hold. For example, the bridges in the domain 
can be made of any of eight materials, but if all of 
the visible bridges are made of wood, then the Mate- 
rial attribute for Bridge will probably be wood most 
of the time. In this case, referring to the "wooden 
bridge" on the map doesn't give much more informa- 
tion than just "bridge." For this more general case, 
define B(X) to be B(X1, X2, ...X,) where each Xi 
is a possible value of X. Also define pl,p2, ...Pn to 
be their associated probabilities. Then 
B(Xt ,  X2, ...Xn) = - Ep l  logpi 
i=1 
These probabilities can be determined using fre- 
quency counts from sample dialogs, or estimated 
based on domain knowledge. 
7 Future  Work  
The next step in this research is to obtain several do- 
mains that have been built into a dialog system and 
analyze them. The Circuit Fix-It Shoppe(Smith and 
D.R.Hipp, 1994) has been analyzed, but the results 
will only be interesting in comparison to other real 
domains. This comparison will not only help us ver- 
ify the correctness ofthe analyses, but also bring up 
possible situations that the analysis may not cover. 
Next, we will want to identify a measure of syn- 
tactic complexity. This could be related to gram- 
mar perplexity. It should take into account vocab- 
ulary size, grammar constraints, and the amount of 
ambiguity in the grammar. We would like to be 
able to analyze the domains with both the seman- 
tic complexity and the syntactic omplexity, and see 
that the results match our intuitions of complexity 
and the standards of lines of code, reliability, cost 
of software, and execution time. We would also be 
45 
interested in observing the correlation between the 
syntactic and semantic omplexities. 
8 Conclus ion 
This paper describes a way to organize the objects, 
attributes, classes, and relationships in a domain 
and to use these classifications to define a semantic 
domain complexity. This measurement, along with 
a syntactic omplexity measurement, will give nat- 
ural language programmers a way to quantify the 
complexity of a given domain in terms of real-world 
costs: cost of software, reliability, accuracy, and ex- 
ecution time. After defining a syntactic omplexity 
measure, domains can be analyzed against hese real 
costs to be sure that the measure is accurate. Such 
a measure will allow natural anguage systems pro- 
grammers a way to analyze domains and estimate 
the costs of building a natural anguage system be- 
forehand, based on the domain's emantic and syn- 
tactic constraints. A standard complexity measure 
will also allow a comparison of different language 
processors' ability to handle more and more com- 
plex domains and quantify the abilities of the cur- 
rent state of the art in natural anguage processors. 
References 
Robert B. Ash. 1965. Information Theory. Inter- 
science Publishers. 
Amit Bagga and Alan W. Biermann. 1997. Ana- 
lyzing the complexity of a domain with respect 
to an information extraction task. Proceedings of
the tenth International Conference on Research on 
Computational Linguistics (ROCLING X), pages 
175--94, August. 
Amit Bagga. 1997. Analyzing the performance of 
message understanding systems. In Proceedings 
of the Natural Language Processing Pacific Rim 
Symposium (NLPRS '97), pages 637---40, Decem- 
ber. 
Ron Cole and Victor Zue. 1995. Survey of the state 
of the art in human language technology, Novem- 
ber. 
Jr G. Edward Barton, Robert C. Berwick, and 
Eric Sven Ristad. 1987. Computational Complex- 
ity and Natural Language. The MIT Press, Cam- 
bridge, Massachusetts. 
Partha Niyogi. 1996. The Informational Complexity 
of Learning from Examples. Ph.D. thesis, MIT. 
Eric Sven Ristad. 1993. The Language Complexity 
Game. MIT Press. 
R.W. Smith and D.R.Hipp. 1994. Spoken Natural 
Language Dialog Systems: A Practical Approach. 
Oxford University Press, New York. 
46 
