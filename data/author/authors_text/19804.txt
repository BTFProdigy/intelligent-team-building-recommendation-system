Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1521?1532, Dublin, Ireland, August 23-29 2014.
3arif: A Corpus of Modern Standard and Egyptian Arabic Tweets 
Annotated for Epistemic Modality Using Interactive Crowdsourcing 
 
Rania Al-Sabbagh?, Roxana Girju?, Jana Diesner? 
?Department of Linguistics and Beckman Institute  
?School of Library and Information Science  
University of Illinois at Urbana-Champaign, USA 
{alsabba1, girju, jdiesner} @illinois.edu 
Abstract 
We present 3arif1, a large-scale corpus of Modern Standard and Egyptian Arabic tweets annotated for 
epistemic modality2. To create 3arif, we design an interactive crowdsourcing annotation procedure that 
splits up the annotation process into a series of simplified questions, dispenses with the requirement for 
expert linguistic knowledge and captures nested modality triggers and their attributes semi-
automatically.  
1 Introduction 
Epistemic modality, according to Palmer (2001), defines the speaker's subjective knowledge, beliefs 
and judgments about the world's states of affairs. Epistemic modality is used as a linguistic feature for 
multiple NLP tasks and applications, including sentiment analysis (Abdul-Mageed and Diab 2011), 
opinion mining (Benamara et al. 2012) and scientific discourse evaluation (Waard and Maat  2012), 
among others.  
To-date, there are no large-scale modality-annotated Arabic corpora compared to English (Baker et 
al. 2010, 2012; Rubinstein et al. 2013), Chinese (Cui and Chi 2013), Portuguese (Hendrickx et al. 
2012) and Japanese (Matsuyoshi et al. 2010). The creation of modality-annotated corpora is non-trivial 
because there is no consensus definition of modality and its attributes in theoretical linguistics to be 
rendered into annotation tasks and guidelines. Furthermore, most current modality annotation schemes 
rely on sophisticated theoretically-grounded guidelines that require annotators from linguistics back-
ground; hence, annotation is usually restricted to small-scale in-lab settings.  
In this paper, we present 3arif, a large-scale Arabic corpus annotated for epistemic modality. 3arif 
comprises 9822 unique tweets in Modern Standard Arabic (MSA) and Egyptian Arabic (EA), annotat-
ed for 9966 tokens that map to 214 unique types of epistemic modality. Each epistemic modality is 
annotated for sense, polarity, intensification, tense, holder(s) and scope(s). The reason that 3arif fea-
tures the tweets' genre with an emphasis on MSA and EA tweets is that it comes as part of a larger 
project to incorporate linguistic features, such as modality, with network-based features to automati-
cally identify the key players of Twitter's political discourse in counties of political unrest such as 
Egypt. We harvested 3arif from a variety of Twitter users including newspapers, TV stations, political 
campaigns, among others, as well as individuals. As a result 3arif is diglossic for MSA, the formal 
Arabic variety, and EA, the native Arabic dialect of Egypt.  
For the annotation of 3arif, we design a simplified procedure that depicts the following ideas: first, it 
defines each annotation task as a series of open and closed questions that do not require sophisticated 
linguistics background and, meanwhile, provide annotators with self-explanatory annotation guide-
lines; second, it is interactive so that questions are displayed/hidden based on annotators' prior an-
swers; and finally, it semi-automatically identifies and merges nested epistemic modality based on an-
notators' answers to a number of easy-to-administer questions.  
                                               
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
1 Pronounced as ?a:rif in Arabic IPA and as EArif in Buckwalter's transliteration scheme. It means I/he know(s). 
2 3arif is available at http://www.rania-alsabbagh.com/3arif.html 
1521
We evaluate our annotation results using Krippendorff's reliability (Krippendorff 2011) and agree-
ment. Results show high inter-annotator reliability and agreement rates and indicate that our annota-
tion scheme and procedure are efficient. The contribution of this research, therefore, is twofold: first, 
we create a novel resource for Arabic NLP which is expected to enhance research on modality auto-
matic identification and extraction; second, we present an efficient and easy-to-administer annotation 
procedure with interactive crowdsourcing potentials for the complex task of modality annotation. 
The rest of this paper is organized as follows: Section 2 outlines our annotation scheme including 
annotation tasks, guidelines and the interactive structure; Section 3 gives examples for the representa-
tion of the final annotation outputs; Section 4 describes corpus harvesting and sampling; Section 5 dis-
cusses the results and presents a disagreement analysis; Section 6 compares and contrasts our work to 
related work; and Section 7 highlights the points not covered in this current version of 3arif. 
2 Annotation Scheme 
Our annotation scheme consists of six tasks to label sense, polarity, intensification, tense, holders and 
scopes for each epistemic modality. Prior to the beginning of the interactive annotation procedure, we 
highlighted all candidate epistemic modalities in each tweet using a string-match algorithm and the 
lexicons from Al-Sabbagh et al. (2013, 2014).  The algorithm finds all potential epistemic modality 
triggers (i.e. words and phrases that may convey epistemic modality) within each tweet in our corpus 
and marks them as annotation units. A total of 9966 candidate epistemic modality triggers are high-
lighted in 9822 tweets. 
2.1 Task 1: Sense  
Sense annotation is to decide for each highlighted candidate trigger in context whether it actually con-
veys epistemic modality. The same lexical verb ???? A$Er is used as an epistemic modality trigger an-
ticipating a future possibility in example 1; but as a non-modal lexical verb in example 2.  
1. 3]?????? ????? 30?? ????? ??? ??[?? ????  
A$Er An[nA snksr rqm Al30 mlywn mtZAhr] 
I feel that [we will get 30+ million protesters]. 
2. #?????? ?????? ???? ?? ???? ??? ?????? ????: ????  
#hykl: A$Er bAlfxr wAlqlq >yDA fy *krY Hrb >ktwbr 
#Heikl: I feel proud but also worried when I remember October's war. 
We define sense annotation as a synonymy judgment task, following Al-Sabbagh et al. (2013). Epis-
temic modality is represented by an exemplar set manually selected so that: (1) each exemplar is an 
unambiguous epistemic trigger, (2) exemplars are in both MSA and EA, (3) exemplars comprise both 
simple words and multiword expressions, (4) exemplars are both affirmative and negative, and (5) ex-
emplars are of different lexical intensities. Furthermore, we create multiple versions of the same set so 
that we cover the inflections for gender, number, person, tense, mood, and aspect in Arabic. We then 
use the set that morphologically matches the candidate trigger to be annotated. Presented with a pre-
highlighted candidate trigger in context and the exemplar set, annotators are to decide whether the giv-
en candidate trigger is synonymous to the exemplar set, and is hence an epistemic modality trigger, or 
not.  
If an annotator decides that a given candidate trigger does not convey epistemic modality, no further 
questions about polarity, intensification, tense, holders or scopes are displayed. To guarantee that an-
notators do not select the non-synonymous option as an easy escape, they are not allowed to move 
forward without submitting at least one synonym of their own to the candidate trigger.  
Designing the interactive procedure as such results in disagreement propagation. If one annotator 
decides that a given candidate trigger is not epistemic, but another annotator decides that it is, the for-
mer will not have to answer any further questions about polarity, intensification, tense, holders or 
scopes; whereas the latter will have to provide answers for each of those annotation tasks. 
                                               
3 Throughout the examples, epistemic modality triggers are represented in boldface and scopes are in-between square brack-
ets.  
1522
2.2 Task 2: Polarity  
Task 2 uses as input the candidates labeled as valid epistemic modality triggers in Task 1 and labels 
each as either affirmative or negative. An affirmative trigger indicates that the speaker holds the given 
state of affairs (i.e. propositions) as TRUE; whereas a negative trigger indicates that the given proposi-
tions are held as FALSE by the speaker.  
To decide on whether the polarity is affirmative or negative, annotators are instructed to look for the 
absence/presence of such negation markers as: 
? Negation particles such as ?? m$ (not), ?? lA (not) and ??? gyr (not), among others. 
? Negation affixes like the circumfix m...$ in ???? mZn$ (I do not think). 
? Negative polarity items like ???? Emry (never) and ?? ??? lm yEd (no longer). 
? Negative auxiliaries where negation is placed on the past tense auxiliary as in ????? ???? mknt$ 
wAvq (I was not sure).  
? Inherently-negative triggers that encode negation in their lexical meanings such as ?????? 
mstHyl (impossible). 
Annotators are instructed that using multiple negation markers results in an affirmative sense. Thus, 
??? ?? ???????? lys mn AlmstHyl (it is not impossible) means that the proposition is actually possible 
according to the speaker. Put differently, it means that the speaker holds the proposition as TRUE. An-
notators are required to give the reason for negation if they decide that a given trigger is negative. 
2.3 Task 3: Intensification  
Epistemic modality triggers can have different lexical intensities (i.e. intensities encoded in the lexical 
meaning of the word/phrase regardless of the context). For instance, even without a context, Arabic 
speakers know that ?????  mt>kd (I am/he is sure) expresses higher possibility than ???????  mthy>ly (I 
imagine). When used in context, the trigger's lexical intensity can be maintained as is. Yet, it can also 
be amplified or mitigated by various linguistic means such as: 
? Modification: adverbs like ????? tmAmA (absolutely) and ?????? bAlfEl (indeed), among others, 
amplify lexical intensity; whereas mitigation can be caused by such adverbs as ?????? tqrybA (al-
most) and ????? gAlbA (most probably), among others.  
? Categorical negation typically amplifies lexical intensity as in  ???? ??????  m$ mmkn >bdA (it is 
not possible at all).  
? Emphatic expressions such as ?? qd (indeed) and ????? wAllh (I swear), among others, lead to lex-
ical intensity amplification.  
? Coordination of two or more triggers usually results in intensity amplification as in ???? ?????? 
EArf wmt>kd (I know and I am sure). 
The annotators' task for intensification is to decide for each candidate labeled as a valid epistemic 
modality trigger in Task 1 whether its lexical intensity is amplified (AMP), mitigated (MTG), or main-
tained (AS IS). During interactive annotation, annotators are asked to provide the reason for their selec-
tion; that is, whether the lexical intensity is affected by an adverb, categorical negation, an emphatic 
expression, coordination, or any other reason. 
2.4 Task 4: Tense  
In this version of 3arif, we work on the present and past tenses only. Thus, Task 4 is to decide for each 
valid epistemic trigger from Task 1 whether it is present (PRS) or past (PST). Tense can be marked ei-
ther morphologically by inflections and affixes or contextually by auxiliary verbs such as ??? kAn 
(was), among others. Annotators are also required to give their reasons for selecting either PRS or PST. 
2.5 Task 5: Holder  
Holder annotation is to identify the holder of the epistemic modality which is the ?RATIONAL entity 
that expresses its knowledge, beliefs or judgments about the world's states of affairs.  
1523
Holders can be ?RATIONAL entities as in example 3. The entity that is making the assumption that 
the former Palestinian president - Yasser Arafat - may have died of natural causes is the report issued 
by the French government.  
3. ???? ?????? ?????? ???? ?????# ????[: ????? ?????[  
tqryr frnsy: [wfAp #ErfAt rbmA tEwd lAsbAb TbyEyp] 
A French report: [natural causes might be behind the death of #Arafat].  
The holder is not necessarily the same as the trigger's grammatical subject. In example 4, the gram-
matical subject of ???? ybdw (seems) is ???? ???????????  AlAElAn Aldstwry (the constitutional declara-
tion). However, the entity that is making the judgment about this declaration is the French govern-
ment, which is then the real holder of ybdw. 
4. ??? ???? ??????? ?????? ???????? ???????? ?????? ????? ?? ???[: ?????[  
frnsA: [AlAElAn Aldstwry Aljdyd lmrsy lA ybdw Anh yslk AlAtjAh AlSHyH] 
France: [Morsi's new constitutional declaration does not seem to be a correct move].  
Twitter users do not only post their own knowledge, beliefs and judgments about the world's states 
of affairs, but also they (1) directly and indirectly quote others and (2) make assumptions about others' 
knowledge, beliefs and judgments. This means that we can have nested holders, according to Wiebe et 
al. (2005) and Saur? and Pustejovsky (2009), where we know about others' knowledge, beliefs and 
judgments only though the writer or the Twitter user in our case.  
In example 5, the Twitter user quotes Elbaradei stating that he may run for presidency if the people 
want him to. That is, the holder of the epistemic modality is actually Elbaradei not the Twitter user.  
5. ??? ??? ?????  ]????? ?? ???????? ???????[ ??: ????????  
AlbrAdEy: qd [>tr$H fy AntxAbAt Alr}Asp] <*A Tlb Al$Eb  
Elbaradei: I may [run for presidency] if the people want me to.  
The holder of the epistemic modality in example 6 is not the Twitter user, either. However, the Twit-
ter user is not quoting anyone here, but is rather making an assumption about what the Egyptian Na-
tional Party holds as TRUE. 
6.  ? ???? ????[?? ?????????? ??????[  #Jan25 
AlHzb AlwTny mqtnE An[h mmkn yrjE] #Jan25 
The National Party is convinced that [it may get back to authority]. #Jan25 
We can have two or more nested holders. In example 5, we have two: the first is ElBaradei and the 
second is the Twitter user who is quoting ElBaradei. Similarly, in example 6, we have two nested 
holders: the first is the Egyptian National Party and the second is the Twitter user who makes the as-
sumptions about the party's beliefs.  
In example 7, however, we have three nested holders. The first is ??????? AlAxwAn (the Muslim 
Brotherhood) that holds as TRUE the proposition that the Military Council is conspiring against them. 
That belief of the Muslim Brotherhood is communicated to us through the politician ??? ?????? Abw 
AlftwH (Abulfotoh) who is then the second holder. Yet, Abulfotoh has not posted his assumption 
about the Muslim Brotherhood's belief on his personal account. Instead, he has been quoted by another 
Twitter user, who is the third holder.  
7. ???? ?????? ?? ???????[??  ????????????? : ??? ??????[  
Abw AlftwH: AlAxwAn tSwrwA An [hnAk m&Amrp mn AlEskry] 
Abulfotoh: The Muslim Brotherhood members thought that [there was a conspiracy by the Military 
Council]. 
During the interactive procedure, annotators are first asked whether the holder is the same as the 
Twitter user. If not, more questions are displayed to determine: (1) who the real holder is; (2) whether 
the tweet is a(n) (in)direct quote (e.g. there are direct quotation markers or such words as ??? qAl (he 
said) and ??? SrH (he declared), among others), or the tweet conveys the Twitter user's assumptions 
about others. 
1524
When the holder is not the same as the Twitter user, annotators are asked to mark the boundaries of 
the linguistic unit that corresponds to the holder in the tweet's text, following the maximal length prin-
ciple from Szarvas et al. (2008), so that they mark the largest possible, meaningful linguistic unit. 
Hence, in example 8 the holder is the Islamist opponents in #KSA not only the Islamist opponents. 
8. ???# ?? ?????? ???? ???? ??[?? ?????? ????????# ?? ????????? ??????????[  
Al<slAmywn AlmEArDwn fy #AlsEwdyp mwqnwn >n[hA tsEY lqtl Alvwrp fy #mSr] 
Islamist opponents in #KSA know for sure that [it tries to put an end to #Egypt's revolution]. 
2.6 Task 6: Scope  
Scopes are the states of affairs modified by the epistemic modality triggers. Modality scopes in Arabic 
are most likely realized as clauses, deverbal nouns or to-infinitives, according to Al-Sabbagh et al. 
(2013). We use the same maximal length guideline from Task 5 so that the scope segment marked by 
the annotators is the largest possible segment typically delimited by: (1) punctuation markers and (2) 
subordinate conjunctions such as ???  lAn (because) and ?? lw (if), among others. 
In the case of nested triggers as in example 9, where a trigger and its scope are both embedded in 
another trigger's scope, the interactive procedure prompts the annotators to label each trigger and its 
scope separately at first. Afterwards, we automatically merge them as we further explain in Section 3. 
9. ????[ ???? ?[?? ????? ?????? ?????[[  #Jan25 
AlHzb AlwTny mqtnE >n[h mmkn [yrjE]] #Jan25 
The National Party is convinced that [it may [get back to power]] #Jan25 
Annotators are instructed that a single trigger may have one or more scopes. In example 10, the trig-
ger ????????? bythy>lhm (they imagine) scopes over two complement clauses, which annotators are re-
quired to identify. Furthermore, annotators are given the guideline that two or more triggers - typically 
conjoined by a coordinating conjunction - can share the same scope as in example 11. In the cases like 
example 11, each trigger and its attributes are first annotated separately and then once our system finds 
out that they share the same polarity, intensification, tense, holder, and scope, they are merged togeth-
er as we show in Section 3.  
10.  ?????? ??? ?? ?????? ??? ???????? [???] ?? ??????? ??? ???[??  ????????????????[  
>wlAdnA bythy>lhm An [dm AxwAthm rAH hdr] wAn[hm Endhm v>r mE AlslTp bkl >$kAlhA]  
Our children imagine that [their friends were killed for no reason] and that [they now have to take re-
venge from the authorities].  
11. ???????? ???? ?????? ?? [???? 12 % ?? ???????] ?????? ??? ?? ????? ???? 
AlbrAdEy EArf wmtAkd An [nsbp 12% bs htntxbh] wEl$An kdp m$ hyr$H nfsh 
Elbaradei knows and is sure that [only 12% will vote for him]. So, he will not run for presidency. 
Annotators are instructed that scopes are not necessarily adjacent to their triggers. In example 12, 
the scope starts three words to the right of its trigger ?????? bAqtnE (get convinced) given that the adver-
bial phrase ???? ????? Aktr wAktr (more and more) falls in between it and its scope.  
12.  ?? ??? ???????? ??????? ???? ????[???? ????? ?? ???????? ??? ?????[  
kl ywm byEdy bAqtnE Aktr wAktr An[nA knA mHtAjyn dktAtwr wTny EAdl] 
Every day, I get more and more convinced that [we needed a patriotic and fair dictator]. 
Annotators are also instructed that scopes can (1) precede, (2) follow or (3) surround their triggers. 
Many of the aforementioned examples have the scopes following their triggers. Yet, in example 13 the 
scope surrounds its trigger and in example 14 it precedes its trigger.  
13. ]???? ??? ???? ???? ???? ???? ????[  
 [wEwd mrsy lyst fymA ybdw dyn Elyh] 
[Morsi's promises are not seemingly doable]. 
14. ]????? ??? ]???? ????? ???? ????? ?????? ????? ?????? ????? ??? ?????? ????  
[Hmlp t$wyh vwrp ynAyr w<EAdp EqArb AlsAEp tmAmA <lY AlwrA' bd>t] fymA ybdw  
1525
[A campaign to distort the image of January's revolution and to restore everything back to its original 
state has started], seemingly.  
3 Final Output Representation  
All elicited answers during annotation are automatically organized into the representations illustrated 
in the examples below. The representation of example 15 reads as follows: the USER (i.e. the Twitter 
user) used to moderately hold as TRUE the proposition that the revolutionist candidates were unable to 
compete for presidency. We know that this is a past belief that the USER used to have because annota-
tors have labeled the trigger ????? tSwrt (I thought) as past (PST). There are no nested holders given 
that the USER is the same as the holder. The intensity value of MODerate comes from the fact that 
????? tSwrt (I thought) is of a moderate lexical intensity being weaker than such epistemic triggers as 
????? mtAkd (I am sure) and ???? EArf (I know) but stronger than such epistemic triggers as ??? AZn (I 
guess) and ??????? mthyAly (I imagine). Meanwhile, the lexical intensity of tSwrt is neither amplified 
nor mitigated; hence annotators have given it an AS IS intensification label in Task 3. Consequently, in 
the final annotation output the original lexical intensity value has been used to represent how far the 
holder used to consider his/her belief as TRUE. 
15.  ??????? ??? ?? ?????????????? ?????? [? ? ??????? ???????[  
fy AlbdAyp tSwrt An [mr$Hy Alvwrp ADEf mn AlmnAfsp llr}Asp] 
At first, I thought that [the revolutionist candidates are too weak to compete for presidency]. 
rep. USER, MOD PST TRUE, (mr$Hy Alvwrp ADEf mn AlmnAfsp llr}Asp) 
Example 16 shows how two epistemic modality triggers in the same tweet are given two separate 
representations because they share the same holder but neither the same intensity nor the same scopes. 
The first representation illustrates the epistemic trigger ??? ArY (I think) and reads as follows: the US-
ER currently holds as TRUE the proposition that the media is misleading the people; s/he is MODerately 
confident about that. The second representation is for the epistemic trigger ???? wADH (obviously). It 
indicates that the same USER strongly holds as TRUE the proposition that the media is trying to stop the 
change that the people are longing for. Both triggers are labeled as present (PRS) tense. Furthermore, 
both triggers are labeled as maintaining their lexical intensity AS IS. The trigger ??? ArY (I think) is 
then labeled in the final representation as being of MODerate intensity because it is weaker than ????? 
mtAkd (I am sure), for instance, but stronger than ??????? mthyAly (I imagine); whereas the trigger ???? 
wADH (obviously) is labeled as indicating a strong (STRG) belief being synonymous to ????? mtAkd (I 
am sure) and ???? AErf (I know) among other triggers that express speakers' high confidence about 
their knowledge, beliefs and judgments.  
16. ?? ??????? ??????? ???? ???? ??[?? ???? ]???? ???? ???? ?????? ????????[??  ???[  
ArY An [AlAElAm yqdm $bAb yxdrwn Al$Eb] wADH An[hm yqAwmwn Altgyyr Al*y nTmH lh] 
I think [the media presents young speakers who mislead the people]. Obviously, [they are resisting 
the change we are longing for]. 
rep1. USER, MOD PRS TRUE, (AlAElAm yqdm $bAb yxdrwn Al$Eb) 
rep2. USER, STRG PRS TRUE, (hm yqAwmwn Altgyyr Al*y nTmH lh) 
Example 17 illustrates how two coordinating epistemic triggers sharing the same polarity, tense, in-
tensification, holder and scope are represented. They are simply merged in one representation. The 
same example shows how assumptions made by Twitter users about others' knowledge, beliefs and 
judgments are represented. The representation reads as follows: the USER MODerately holds as TRUE 
the proposition that Elbaradei strongly (STRG) holds as TRUE that only 12% of the Egyptians will vote 
for him for presidency. The values of TRUE, MODerate and present (PRS) assigned to the USER's as-
sumption about Elbaradei are default values used to mark Twitter users' assumptions about others' 
knowledge, beliefs and judgments.  
17. ???????? ???? ?????? ?? [???? 12 % ?? ???????] ?????? ??? ?? ????? ???? 
AlbrAdEy EArf wmtAkd An [nsbp 12% bs htntxbh] wEl$An kdp m$ hyr$H nfsh 
Elbaradei knows and is sure that [only 12% will vote for him]. So, he will not run for presidency. 
rep. USER, MOD PRS TRUE, (AlbrAdEy, STRG PRS TRUE, (nsbp 12% bs htntxbh))  
1526
Example 18 represents an epistemic trigger with multiple scopes. The example also represents 
Twitter users making assumptions about others' knowledge, beliefs and judgments. As we mentioned 
in example 17, the values of TRUE, MODerate and present (PRS) assigned to the USER's assumption are 
assigned by default. The trigger ????????? bythy>lhm (they imagine) is labeled as a present (PRS) tense 
affirmative trigger. Its original lexical intensity - which is weak (WK) - is labeled as being maintained 
AS IS. The trigger ????????? bythy>lhm (they imagine) is of a weak lexical intensity because it is weaker 
than ????? mtAkd (I am sure) and even ??? AZn (I think). 
18. ??????? ??? ???????? ????? ??? ?? [???] ?? ??????? ??? ???[??  ??????????????? ?[  
>wlAdnA bythy>lhm An [dm AxwAthm rAH hdr] wAn[hm Endhm v>r mE AlslTp bkl >$kAlhA]  
Our children imagine that [their friends were killed for no reason] and that [they now have to take re-
venge from the authorities]. 
rep. USER, MOD PRS TRUE, (>wlAdnA, WK PRS TRUE ,(dm AxwAthm rAH hdr; hm Endhm v>r mE AlslTp 
bkl >$kAlhA)) 
Example 19 illustrates embedded triggers. Its representation reads as: the USER MODerately holds as 
TRUE that the Egyptian National Party strongly (STRG) holds as TRUE that it (i.e. the Egyptian National 
Party) may get back to ruling. It is important to notice that both the matrix trigger ????? mqtnE (is con-
vinced) and the embedded trigger (i.e. ???? mmkn (may)) share the same holder which is the Egyptian 
National Party.  
19.  ????[ ????? [?? ?????????? ??????[[  #Jan25 
AlHzb AlwTny mqtnE An[h mmkn [yrjE]] #Jan25 
The National Party is convinced that [it may [get back to power]]. 
rep. USER, MOD PRS TRUE, (AlHzb AlwTny, STRG PRS TRUE,(MOD PRS TRUE, (yrjE))) 
Example 20 shows how reported knowledge, beliefs and judgments are represented. The USER in 
this example has no other role but to report Darrag's strong belief that the army will interfere to stop 
the chaos.  
20. ???????#???? #??? #] ?????? ?? ???? ?????? ????????? #: [????  
drAj: [#Aljy$ HtmA sytdxl fy HAlp AlfwDY] #mSr #mrsy #AlAxwAn 
Darrag: [the #army will definitely interfere in the case of chaos] #Egypt #Morsi #Ikhwan 
rep. USER, report, (drAj, STRG PRS TRUE (#Aljy$ sytdxl fy HAlp AlfwDY)) 
4 Corpus Harvesting  
In order to restrict our corpus to political discourse and ensure that we compile a representative corpus 
of epistemic modality, we harvested our corpus so that each tweet (1) has at least one trendy political 
English or Arabic hashtag such as #Egypt and #???? mrsy (Morsi)4, and (2) has at least one epistemic 
modality trigger from the Arabic Modality Lexicons of Al-Sabbagh et al. (2013, 2014). Table 1 gives 
statistics for the sampled corpus that comprises 9822 unique tweets, with 9966 candidate epistemic 
modality triggers that map to 214 unique types. 
 Tokens Types 
Epistemic candidates 9966 214 
All words 175964 47696 
Table 1: Statistics for the sampled corpus  
5 Annotation Results 
5.1 Evaluation Methodology and Metrics  
Our annotation tasks are of two types: (1) Tasks 1-4 are label-based where there is a pre-defined set of 
labels from which annotators choose; and (2) Tasks 5-6 are segmentation-based where the output of 
the annotation is a text segment. For the segmentation-based tasks, we use an all-or-nothing method to 
                                               
4 A total of 304 unique English and Arabic hashtags are found in the sampled corpus. 
1527
measure reliability and agreement: for segments to be considered as agreement, they must share both 
the beginning and end boundaries. We use Krippendorff's alpha ? (Krippendorff 2011) as our inter-
annotator reliability measure, following the most recent work on modality annotation for other lan-
guages including English (Rubinstein et al. 2013) and Chinese (Cui and Chi 2013). For more details 
on Krippendorff's alpha and a comparison of inter-annotator agreement measures, we refer the reader 
to Artstein and Poesio (2008).   
5.2 Results 
We use the surveygizmo services to implement our interactive annotation procedure given that their 
survey structure is one that allows for using conditional branching and skip logic5. We distributed the 
survey on Twitter and we had three annotators participating. According to the short qualifying quiz 
given at the beginning of the survey, all three participants are native Egyptian Arabic (EA) speakers 
who have at least two-year experience with using Twitter. They are also university graduates who, 
therefore, master Modern Standard Arabic. None of the participants has a linguistics background. 
Table 2 shows alpha and agreement rates for each annotation task. We measure the rates in four dif-
ferent scenarios so that we can (1) estimate the effect of the inclusion of the NON-EPISTEMIC category 
agreement, (2) estimate the effect of disagreement propagation from Task 1, and (3) evaluate the 
guidelines and procedures for each annotation task separately. The four scenarios are:  
? w/NONE w/DP: candidates agreed upon as non-epistemic and disagreement propagating from 
Task 1 are both included. 
? w/NONE w/o DP: candidates agreed upon as non-epistemic are included, but disagreement prop-
agating from Task 1 is excluded.  
? w/o NONE w/DP: candidates agreed upon as non-epistemic are excluded, but disagreement prop-
agating from Task 1 is included.  
? w/o NONE w/o DP: candidates agreed upon as non-epistemic and disagreement propagating from 
Task 1 are both excluded. This scenario focuses on each annotation task separately without any 
distractions.  
  Alpha  Agreement 
  w/NONE w/o NONE w/NONE w/o NONE 
Annotation Task w/ DP w/o DP w/ DP w/o DP w/ DP w/o DP w/ DP w/o DP 
1 Sense -- 0.899 -- -- -- 0.949 -- -- 
2 Polarity 0.904 0.974 0.798 0.949 0.939 0.983 0.895 0.976 
3 Intensification 0.880 0.942 0.658 0.768 0.926 0.966 0.844 0.939 
4 Tense 0.911 0.995 0.772 0.983 0.947 0.997 0.909 0.994 
5 Holder 0.878 0.930 0.672 0.727 0.933 0.956 0.884 0.969 
6 Scope 0.825 0.916 0.620 0.618 0.899 0.955 0.819 0.911 
Table 2: Inter-annotator alpha reliability and agreement rates 
In the case of Task 1 (i.e. sense annotation), only the second scenario is applicable: we cannot ex-
clude the candidates agreed upon as non-epistemic because the target is to know how reliable the an-
notation is with regards to distinguishing between epistemic and non-epistemic candidates. It is the 
first annotation task, thus there is no prior disagreement propagation. From Table 2, we derive the fol-
lowing observations:  
? Disagreement in Task 1 propagates ~ 0.05 to 0.1 disagreement for the other annotation tasks. 
? Adding the agreed upon non-epistemic candidates yields up to ~ 0.2 gain for both alpha reliabil-
ity and agreement rates. 
? For an end-to-end automatic system that first identifies triggers and then their attributes, the 
benchmark rates are those from the w/NONE w/DP scenario. 
                                               
5 http://www.surveygizmo.com/ 
1528
5.3 Discussion and Disagreement Analysis 
Among the factors that lead to high inter-annotator alpha reliability and agreement rates are that: (1) 
the vast majority of negation is explicitly marked by negation particles that are easy to detect by hu-
man annotators; (2) the vast majority of triggers are used without any amplification or mitigation 
markers; and (3) punctuation markers are surprisingly informative for marking scope boundaries and 
direct quotations and, hence, holders. 
Sense-related disagreement is attributed to: (1) nominal triggers with main grammatical functions, 
(2) stative triggers, (3) opinionated-evidential triggers and (4) highly-polysemous triggers. 
The majority of epistemic triggers are adjunct constituents that add an extra-layer of meaning and 
can be removed without disturbing the syntactic structure of their propositions. Yet, in example 21, 
?????? AHtmAl (a possibility) is the grammatical subject of the proposition it modifies. Most of the ex-
emplars from Section 2.1 are adjuncts and, thus, none can be both a lexical and a grammatical substi-
tute for ?????? AHtmAl (a possibility) in such a context.  
21. ???? ?????? ]???? ???? ????? ??? ?????? ????? ????? ?????[??  ??????  
AHtmAl An [r}ys mntxb yHl Almjls AvnA' SyAgp dstwr jdyd] AHtmAl whmy 
The possibility that [an elected president dissolves the parliament during the constitution's write-up] is 
an unrealistic possibility.  
Stative triggers such as ???? yErf (he knows) and ???? ydrk (he realizes) invoke disagreement as to 
whether they indicate the acquisition of new information; that is, they literally mean perceive, or they 
mark confirmed beliefs as in be sure that. For example 22, the annotators have two interpretations: (1) 
a non-modal interpretation that whoever says so does not perceive that the Supreme Guide cannot 
make resolutions without the Brotherhood, and (2) a modal interpretation that whoever says so does 
not believe that the Supreme Guide cannot make resolutions without the Brotherhood. 
22.  ?????? ?? ?????? ??? ???? ??? ?????? ??? ???????[??  ???????? ???? ??? ?????? ??[  
Al*y yqwl h*A AlklAm lA yErf An [Almr$d lA ystTyE Ax* qrAr dwn AlrjwE AlY AljmAEp]. 
Whoever says so does not perceive/believe that [the Supreme Guide cannot make resolutions without 
the Brotherhood].  
Opinionated-evidential triggers like ???? yzEm (he claims) do not only mark reported speech, but al-
so they communicate the reporter's own opinion about the truth value of the reported proposition. They 
entail that from the reporter's perspective the proposition is FALSE. Hence, annotators disagree as to 
whether yzEm and similar triggers should be labeled as epistemic or not. We have eventually excluded 
such triggers as epistemic and have included them as evidential triggers for another corpus that is left 
for a future publication.  
Highly-polysemous triggers like ???? ymkn (can/possible) lead to disagreement because in many cas-
es even the context is ambiguous. In example 23, both interpretations of it is not possible that (epis-
temic) and it is not doable that (abilitive) seem to be acceptable.  
23.  ???? ?????"?" ????? ?????: "?? ???????? ???????????? ???" ???? ?? ?????"??? ???? ??? ?[ ??????"[  
lA ymkn [fhm ktAb mHmd mrsy "vA}r mn Al$rq" AlA btAml AlktAbyn AlmjAwryn: "srqAt Sgyrp" w 
"jnwn AlHkm"] 
It is not possible/doable [to understand Morsi's book - A Revolutionist from the East - without reading 
the other two books of Small Robberies and Ruling Mania]. 
Intensity-related disagreement is attributed to (1) intensity on the holder that propagates to the trig-
ger and (2) negation with moderate-intensity triggers. In example 24, the USER uses categorical nega-
tion on the holder  ? ????? ??????? ????  lA ywjd Ay AnsAn EAql (there is no one sane person). For some 
annotators, the power of categorical negation spreads to the trigger, moving its intensity up the scale. 
As for negation with moderate-intensity triggers, some annotators think that ?? ???? lA ymkn (not possi-
ble) is synonymous to impossible. Hence, they consider the negation as an amplification marker.  
24.  ??????? ????? ????????[???  ??????? ???? ?? ????? ????[  
lA ywjd >y AnsAn EAql yEtqd b>n [AlArhAb yEAlj bAlsyAsp]  
There is no one sane person who thinks that [terrorism can be defeated through politics]. 
1529
Polarity-related disagreement is mainly caused by negation due to (1) negated holders and (2) con-
textual negation. Negated holders as in example 24 perplex the annotators as to whether the negation 
scopes over the holder only or both the holder and the trigger. Thus, for some annotators, ????? yEtqd 
(he thinks) is affirmative; and for others it is negative. By contextual negation we mean using words 
such as ??????? Alm$klp (the problem) to describe triggers as in example 25. The USER says that the 
problem is to think that it is a small-scale conflict. To describe this as a problem means that the USER 
thinks of the proposition as FALSE; that is, according to the USER it is actually a large-scale conflict.  
25.  ?????? ????? ?? ??????? ?????? ???? ?????? ????[??  ???????????? ????[  
Alm$klp <nnA ntSwr <n [AlSrAE mHSwr fY AldA}rp AlDyqp AllY bntHrk fyhA] 
The problem is to think that [the conflict is only happening at this small-scale we are working on]. 
Holder-related disagreement is attributed mainly to generic nouns and impersonal pronouns such as 
????? Al$Eb (the people) and ?????? AlwAHd (one). Some annotators interpret them as implicitly refer-
ring to the USER. Therefore, they select the USER as the only holder with zero nesting in example 26. 
Other annotators interpret them as referring to people in general but not necessarily with the USER in-
cluded; and thus, they select two-level nested holders.  
26.  ?? ?????? ???? ????? ??????? ??????????????? ??????????? ?? ???? ??[??  ?????????[  
Al$Eb yErf An [AlmmArsp AldymwqrATyp hy Alty st>ty bAEDA' mjls Al$Eb wAlr}ys AlqAdm] 
People know that [democracy will result in real parliamentary and presidential elections]. 
Scope-related disagreement is attributed to (1) ambiguous subordinate conjunctions, (2) triggers 
modifiers, (3) absent punctuation markers, and (4) embedding within the scope boundaries. For in-
stance, in example 27, the adverbial clause starting with ??? bEd (after) confuses the annotators as to 
whether it is part of the scope or it describes the verb epistemic trigger ????? AtwqE (I expect).   
27. ???? ????? ?? ???????? ?????? ??? ???? ????? ????? ????? ????? ??????? ??????? ???[??? ??  ?????[  
AtwqE jdA An [AEtSAm AltHryr ytfD bnfs Tryqp fD AlAEtSAm AlAxyr bEd Zhwr A$kAl grybp fljAn 
AlAmn] 
I very much expect that [the sit-in in Tahrir will be broken up in the same way as the last sit-in after 
seeing some strange faces at the security checkpoints].  
Tense yields almost perfect inter-annotator alpha reliability and agreement rates. The one main disa-
greement factor, however, is such contexts as ?????? ???? Abtdyt ASdq (I started to believe). While the 
majority of annotators agree that such contexts mark present tense knowledge, beliefs and judgments, 
some annotators consider them as past tense.  
5.4 Majority Statistics for 3arif 
Based on majority annotations, Table 3 gives statistics for 3arif in terms of sense, polarity, intensifica-
tion and tense. Furthermore, approximately 62% of the triggers have zero-nested holders (i.e. the Twit-
ter user is the same as the holder). As for scope syntactic structures, they are distributed as 86% claus-
es, 9% deverbal nouns and the rest are to-infinitives.   
 Sense Polarity Intensification Tense 
 Epistemic Non-epistemic True False Amplified Mitigated As is Present Past 
Tokens 5591 4375 3425 2166 1083 330 4178 4399 1192 
Types 209 175 176 134 133 50 150 175 104 
Table 3: Majority statistics for 3arif 
6 Related Work 
Epistemic modality has been the focus of many annotation projects for multiple languages. Diab et al. 
(2009) annotate three belief categories for English: (1) committed belief is when writers indicate that 
they hold propositions as TRUE, (2) non-committed belief is when writers hold propositions as FALSE, 
and (3) not applicable is when propositions are not denoting beliefs at all. Interest is given to writers' 
beliefs only. Thus, a default value for the modality holder is the writer, and nested holders are not an-
1530
notated. Their corpus contains 10k words of running text from different domains and genres, including 
newswire, blog data, email and letter correspondence and transcribed dialogue data. Inter-annotator 
agreement rate is 0.95 including the NONE category where no belief markers exist. 
Baker et al. (2010, 2012) simultaneously annotate modality and modality-based negation to build 
modality taggers to enhance Urdu-English machine translation systems. Their annotation scheme dis-
tinguishes eight modality types: requirements, permissions, success, effort, intention, ability, desires 
and beliefs. Originally, their annotation scheme labels three attributes for each modality type: triggers, 
holders and targets (i.e. scopes). Yet, holders have not been eventually labeled. A unique feature of 
their annotation scheme is using a simplified operational procedure to label modality semantic mean-
ings. The procedure relies on a list of thirteen choices of the form of H (modal) [P true/false] where H 
is a holder and P is a proposition or an event. The annotators' task is then to select the best form to rep-
resent the modality meaning of a given trigger. Reported kappa ? inter-annotator agreement rates are 
0.82 for triggers and 0.76 for targets.  
Rubinstein et al. (2013) propose a linguistically-motivated scheme for modality annotation in the 
MPQA English corpus. They attain macro alpha inter-annotator reliability rates of 0.89 and 0.65 for 
sense and scope, respectively. Cui and Chi (2013) apply the same scheme from Rubinstein et al. 
(2013) to the Chinese Penn Treebank and get alpha inter-annotator reliability rates of 0.81and 0.39 for 
sense and scope annotation, respectively.  
Al-Sabbagh et al. (2013) annotate epistemic modality in MSA and EA tweets. We attain kappa inter-
annotator agreement rates of 0.90 and 0.93 for sense and scope annotation, respectively, for only 548 
epistemic tokens.  
Our annotation results, therefore, are comparable to the results in the literature. Furthermore, our an-
notation scheme is orthogonal to most of the aforementioned schemes. However, the key differences 
between our work and related work are:  
? We annotate nested modality, unlike Diab et al. (2009) and Baker et al. (2010, 2012).  
? We use a wider range of negation and intensification markers compared to prior work, especial-
ly Al-Sabbagh et al. (2013)  
? We use interactive crowdsourcing with simplified guidelines, unlike in-lab annotations includ-
ing Rubinstein et al. (2013) and Cui and Chi (2013), among others.  
7 Uncovered Points in 3arif 
The current version of 3arif does not cover modality entailment that example 28 illustrates. The USER 
criticizes whoever holds as TRUE the proposition that Egypt can blackmail UAE using the Iranian 
threat. This criticism entails that the USER holds the same proposition as FALSE. 
28.  ?????#?????  ??????????? ???? ?? ????? #[??  ??????? ??[  
yxTY' mn yZn An [#mSr ymkn An tsAwm #Al<mArAt bwrqp #<yrAn] 
Whoever thinks that [Egypt can blackmail #UAE using #Iran] is wrong. 
We do not also cover the future tense, the interrogative, the imperative or the hypothetical moods. 
This is because they have different interpretations when it comes to intensification and polarity that we 
do not cover in this version of 3arif but we will in future work.  
8 Conclusion 
We presented 3arif, a large-scale corpus annotated for epistemic modality in MSA and EA tweets. We 
used a simplified approach that defines each annotation task as a series of questions, implemented in-
teractively. Our scheme covers a wide range of the most common annotation units mentioned in the 
literature, including modality sense, polarity, intensification, tense, holders and scopes. We deal with 
nested holders that are crucial in a highly interactive genre such as tweets where users frequently quote 
others and make assumptions about them. We also automatically merge triggers with shared holders 
and scopes based on elicited annotators' answers. The annotation procedure yields reliable results and 
creates a novel resource for Arabic NLP. For future versions of the corpus, we plan to cover the points 
1531
from Section 7. 3arif will also be used to train and test an automatic machine learning system to iden-
tify epistemic modality and its attributes in MSA and EA tweets. 
References 
Muhammad Abdul-Mageed and Mona Diab. 2011. Subjectivity and Sentiment Annotation of Modern Standard 
Arabic Newswire. In Proceedings of the 5th Linguistic Annotation Workshop (LAW V), pages 110-118, Port-
land, Oregon, June 23-24, 2011. 
Rania Al-Sabbagh, Jana Diesner and Roxana Girju. 2013. Using the Semantic-Syntactic Interface for Reliable 
Arabic Modality Annotation. In Proceedings of IJCNLP'13, pages 410-418, Nagoya, Japan, October 14-18, 
2013.  
Rania Al-Sabbagh, Roxana Girju and Jana Diesner. 2014. Unsupervised Construction of a Lexicon and a Pattern 
Repository of Arabic Modal Multiword Expressions. In Proceedings of the 10th Workshop of Multiword Ex-
pressions at EACL 2014, pages 114-123, Gothenburg, Sweden, April 26-27, 2014. 
Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational 
Linguistics, volume 34, issue 4, pages 555-596. 
Kathrin Baker, Michael Bloodgood, Mona Diab, Bonnie Dorr, Nathaniel W. Filardo, Lori Levin and Christine 
Piatko. 2010. A Modality Lexicon and its Use in Automatic Tagging. In Proceedings of LREC'10, pages 1402-
1407, Valetta, Malta, May 19-21, 2010. 
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris Callison-Burch, Nathaniel W. Filardo, Christine 
Piatko, Lori Levin and Scott Miller. 2012. Modality and Negation in SIMT. Computational Linguistics. vol-
ume 38, issue 2, pages 411-438. 
Farah Benamara, Baptiste Chardon, Yannick Mathieu, Vladimir Popescu and Nicholas Asher. 2012. How do 
Negation and Modality Impact on Opinions. In Proceedings of the ACL-2012 Workshop on ExProM-2012, 
pages 10-18, Jeju, Republic of Korea, July 13, 2012. 
Yanyan Cui and Ting Chi. 2013. Annotating Modal Expressions in the Chinese Treebank. In Proceedings of the 
IWC 2013 Workshop on Annotation of Modal Meaning in Natural Language (WAMM), pages 24-32, Potsdam, 
Germany, March 19, 2013. 
Mona Diab, Lori Levin, Teruko Mitamura, Owen Rambow, Vinodkumar Prabhakaran, and Weiwei Guo. 2009. 
Committed Belief Annotation and Tagging. In Proceedings of the 3rd Linguistic Annotation Workshop, ACL-
IJCNLP'09, pages 68-73, Suntec, Singapore, August 6-7, 2009. 
Iris Hendrickx, Am?lia Mendes and Silvia Mencarelti. 2012. Modality in Text: A Proposal for Corpus Annota-
tion. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC'12), 
pages 1805-1812, Istanbul, Turkey, May 21-27, 2012. 
Klaus Krippendorff. 2011. Computing Krippendorff's Alpha Reliability. Annenberg School of Communication, 
Departmental Papers: University of Pennsylvania. 
Suguru Matsuyoshi, Megumi Eguchi, Chitose Sao, Koji Murakami, Kentaro Inui and Yuji Matsumoto. 2010. 
Annotating Event Mentions in Text with Modality Focus and Source Information.  In Proceedings of 
LREC'10,  pages 1456-1463, Valletta, Malta, May 19-21, 2010. 
Frank R. Palmer. 2001. Mood and Modality. 2nd Edition. Cambridge University Press, Cambridge, UK. 
Aynat Rubinstein, Hillary Harner, Elizabeth Krawczyk, Daniel Simoson, Graham Katz and Paul Portner. 2013. 
Toward Fine-Grained Annotation of Modality in Text. In Proceedings of the IWC 2013 Workshop on Annota-
tion of Modal Meaning in Natural Language (WAMM), pages 38-46, Potsdam, Germany, March 19, 2013.  
Roser Saur? and James Pustejovsky. 2009. FactBank: A Corpus Annotated with Event Factuality. Language Re-
sources and Evaluation, volume 43, pages 227-268. 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas and J?nos Csirik. 2008. The BioScope Corpus: Annotation 
for Negation, Uncertainty and their Scope in Biomedical Texts. In Proceedings of BioNLP 2008: Current 
Trends in Biomedical Natural Language Processing, pages 38-45, Columbus, Ohio, USA, June 2008. 
Anita de Waard and Henk Pander Maat. 2012. Epistemic Modality and Knowledge Attribution in Scientific Dis-
course: a Taxonomy of Types and Overview of Features. In Proceedings of the 50th ACL, pages 47-55, Jeju, 
Republic of Korea, July 12, 2012.  
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. Annotating Expressions of Opinions and Emotions in 
Language. Language Resources and Evaluation, volume 39, issue 2-3, pages 163-210. 
1532
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 48?57,
Gothenburg, Sweden, April 26, 2014.
c
?2014 Association for Computational Linguistics
Recognizing Causality in Verb-Noun Pairs
via Noun and Verb Semantics
Mehwish Riaz and Roxana Girju
Department of Computer Science and Beckman Institute
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{mriaz2,girju}@illinois.edu
Abstract
Several supervised approaches have been
proposed for causality identification by re-
lying on shallow linguistic features. How-
ever, such features do not lead to improved
performance. Therefore, novel sources
of knowledge are required to achieve
progress on this problem. In this paper,
we propose a model for the recognition of
causality in verb-noun pairs by employing
additional types of knowledge along with
linguistic features. In particular, we fo-
cus on identifying and employing seman-
tic classes of nouns and verbs with high
tendency to encode cause or non-cause re-
lations. Our model incorporates the in-
formation about these classes to minimize
errors in predictions made by a basic su-
pervised classifier relying merely on shal-
low linguistic features. As compared with
this basic classifier our model achieves
14.74% (29.57%) improvement in F-score
(accuracy), respectively.
1 Introduction
The automatic detection of causal relations is im-
portant for various natural language processing ap-
plications such as question answering, text sum-
marization, text understanding and event predic-
tion. Causality can be expressed using various nat-
ural language constructions (Girju and Moldovan,
2002; Chang and Choi, 2006). Consider the fol-
lowing examples where causal relations are en-
coded using (1) a verb-verb pair, (2) a noun-noun
pair and (3) a verb-noun pair.
1. Five shoppers were killed when a car blew up
at an outdoor market.
2. The attack on Kirkuk?s police intelligence
complex sees further deaths after violence
spilled over a nearby shopping mall.
3. At least 1,833 people died in hurricane.
Since, the task of automatic recognition of
causality is quite challenging, researchers have
addressed this problem by considering specific
constructions. For example, various models
have been proposed to identify causation between
verbs (Bethard and Martin, 2008; Beamer and
Girju, 2009; Riaz and Girju, 2010; Do et al., 2011;
Riaz and Girju, 2013) and between nouns (Girju
and Moldovan, 2002; Girju, 2003). Do et al.
(2011) have worked with verb-noun pairs for
causality detection but they focused only on a
small list of predefined nouns representing events.
In this paper, we focus on the task of identi-
fying causality encoded by verb-noun pairs (ex-
ample 3). We propose a novel model which first
predicts cause or non-cause relations using a su-
pervised classifier and then incorporates additional
types of knowledge to reduce errors in predictions.
Using a supervised classifier, our model identi-
fies causation by employing shallow linguistic fea-
tures (e.g., lemmas of verb and noun, words be-
tween verb and noun). Such features have been
used successfully for various NLP tasks (e.g., part-
of-speech tagging, named entity recognition, etc.)
but confinement to such features does not help
much to achieve performance for identifying cau-
sation (Riaz and Girju, 2013). Therefore, in our
model we plug in additional types of knowledge
to obtain better predictions for the current task.
For example, we identify the semantic classes of
nouns and verbs with high tendency to encode
cause or non-causal relations and use this knowl-
edge to achieve better performance. Specifically,
the contributions of this paper are as follows:
? In order to build a supervised classifier, we
use the annotations of FrameNet to generate a
training corpus of verb-noun instances encod-
ing cause and non-cause relations. We propose
a set of linguistic features to learn and identify
causal relations.
48
? In order to make intelligent predictions, it is
important for our model to have knowledge
about the semantic classes of nouns with high
tendency to encode causal or non-causal re-
lations. For example, a named entity such
as person, organization or location may have
high tendency to encode non-causality unless a
metonymic reading is associated with it. In our
approach, we identify such semantic classes of
nouns by exploiting a named entity recognizer,
the annotations of frame elements provided in
FrameNet and WordNet.
? Verbs are the important components of lan-
guage for expressing events of various types.
For example, Pustejovsky et al. (2003)
have classified events into eight semantic
classes: OCCURRENCE, PERCEPTION, RE-
PORTING, ASPECTUAL, STATE, I STATE,
I ACTION and MODAL. We argue that there
are some semantic classes in this list with high
tendency to encode cause or non-cause rela-
tions. For example, reporting events repre-
sented by verbs say, tell, etc., have high ten-
dency to just report other events instead of en-
coding causality with them. In our model, we
use such information to reduce errors in predic-
tions.
? Each causal relation is characterized by two
roles i.e., cause and its effect. In example 3
above, the noun ?hurricane? is cause and the
verb ?died? is its effect. However, a verb-noun
pair may not encode causality when a verb
and a noun represent same event. For exam-
ple, in instance ?Colin Powell presented fur-
ther evidence in his presentation.?, the verb
?presented? and the noun ?presentation? rep-
resent same event of ?presenting? and thus en-
coding non-cause relation with each other. In
our model, we determine the verb-noun pairs
representing same or distinct events to make
predictions accordingly.
? We adopt the framework of Integer Linear Pro-
gramming (ILP) (Roth and Yih, 2004; Do et al.,
2011) to combine all the above types of knowl-
edge for the current task.
This paper is organized as follows. In next sec-
tion, we briefly review the previous research done
for identifying causality. We introduce our model
and evaluation with discussion on results in sec-
tion 3 and 4, respectively. The section 5 of the
paper concludes our current research.
2 Related Work
In computational linguistics, researchers have al-
ways shown interest in the task of automatic
recognition of causal relations because success on
this task is critical for various natural language
applications (Girju, 2003; Chklovski and Pantel,
2004; Radinsky and Horvitz, 2013).
Following the successful employment of lin-
guistic features for various tasks (e.g., part-of-
speech tagging, named entity recognition, etc.),
initially NLP researchers proposed approaches re-
lying mainly on such features to identify causal-
ity (Girju, 2003; Bethard and Martin, 2008;
Sporleder and Lascarides, 2008; Pitler and
Nenkova, 2009; Pitler et al., 2009). However,
researchers have recently shifted their attention
from these features and tried to consider other
sources of knowledge for extracting causal rela-
tions (Beamer and Girju, 2009; Riaz and Girju,
2010; Do et al., 2011; Riaz and Girju, 2013).
For example, Riaz and Girju (2010) and Do et
al. (2011) have proposed unsupervised metrics for
learning causal dependencies between two events.
Do et al. (2011) have also incorporated minimal
supervision with unsupervised metrics. For a pair
of events (a, b), their model makes the decision of
cause or non-cause relation based on unsupervised
co-occurrence counts and then improves this deci-
sion by using minimal supervision from the causal
and non-causal discourse markers (e.g., because,
although, etc.).
In search of novel and effective types of knowl-
edge to identify causation between two verbal
events, Riaz and Girju (2013) have proposed a
model to learn a Knowledge Base (KB
c
) of verb-
verb pairs. In this knowledge base, the English
language verb-verb pairs are automatically clas-
sified into three categories: (1) Strongly Causal,
(2) Ambiguous and (2) Strongly Non-Causal. The
Strongly Causal and Strongly Non-Causal cate-
gories contain verb-verb pairs with highest and
least tendency to encode causality, respectively
and rest of the verb-verb pairs are considered am-
biguous with tendency to encode both types of
relations. They claim that this knowledge base
of verb-verb pairs is a rich source of causal as-
sociations. The incorporation of this resource
into a causality detection model can help identi-
fying causality with better performance. In this
research, we also try to go beyond the scope of
shallow linguistic features and identify additional
49
interesting types of knowledge for the current task.
3 Computational Model for Identifying
Causality
In this section, we introduce our model for iden-
tifying causality encoded by verb-noun pairs.
Specifically, we extract all main verbs and noun
phrases from a sentence and predict cause or non-
cause relation on verb-noun phrase (v-np) pairs.
In order to make task easier, we consider only
those v-np pairs where v (verb) is grammatically
connected to np (noun phrase). We assume that a v
and np are grammatically connected if there exists
a dependency relation between them in the depen-
dency tree. We apply a dependency parser (Marn-
effe et al., 2006) to identify such dependencies.
Our model first employs a supervised classifier re-
lying on linguistic features to make binary predic-
tions (i.e., does a verb-noun phrase pair encode a
cause or non-cause relation?). We then incorpo-
rate additional types of knowledge on top of these
binary predictions to improve performance.
3.1 Supervised Classifier
In this section, we propose a basic supervised
classifier to identify causation encoded by v-np
pairs. To set up this supervised classifier, we need
a training corpus of instances of v-np pairs en-
coding cause and non-cause relations. For this
purpose, we employ the annotations of FrameNet
project (Baker et al., 1998) provided for verbs.
For example, consider the following annotation
from FrameNet for the verb ?dying? with ar-
gument ?solvent abuse? where the pair ?dying-
solvent abuse? encodes causality.
A campaign has started to try to cut the
rising number of children dying [
cause
from solvent abuse].
To generate a training corpus, we collect anno-
tations of verbs from FrameNet s.t. the annotated
element (aka. frame element) is a noun phrase.
For example, we get a causal training instance of
?dying-solvent abuse? pair from the above anno-
tation. We assume that if a FrameNet?s annotated
element contains a verb in it then this may not rep-
resent a training instance of v-np pair. For exam-
ple, we do not consider the following annotation
in our training corpus where causality is encoded
between two verbs i.e., ?died-fell?.
A fitness fanatic died [
cause
when 26
stone of weights fell on him as he ex-
ercised].
After extracting training instances from
FrameNet, we assign them cause (c) and non-
cause (? c) labels. We manually examined the
inventory of labels of FrameNet and use the
following scheme to assign the c or ?c to each
training instance. All the annotations of FrameNet
with following labels are considered as causal
training instances and rest of the annotations are
considered as non-causal training instances.
Purpose, Internal cause, Result, Exter-
nal cause, Cause, Reason, Explanation,
Required situation, Purpose of Event,
Negative consequences, resulting ac-
tion, Effect, Cause of shine, Purpose of
Goods, Response action, Enabled situa-
tion, Grinding cause, Trigger
For this work, we have acquired 2, 158
(65, 777) cause (non-cause) training instances
from FrameNet. Since, the non-cause instances
are very large in number, our supervised model
tends to assign non-cause labels to almost all in-
stances. Therefore, we employ equal number of
cause and non-cause instances for training. In fu-
ture, we plan to extract more annotations from
the FrameNet and employ more than one human
annotators to assign the labels of cause and non-
cause relations to the full inventory of labels of
FrameNet.
? Lexical Features: verb, lemma of verb, noun
phrase, lemma of all words of noun phrase,
head noun of noun phrase, lemmas of all words
between verb and head noun of noun phrase.
? Semantic Features: We adopted this feature
from Girju (2003) to capture the semantics of
nouns. The 9 noun hierarchies of WordNet i.e.,
entity, psychological feature, abstraction, state,
event, act, group, possession, phenomenon are
used as this feature. Each of these hierarchies
is set to 1 if any sense of the head noun of noun
phrase lies in that hierarchy otherwise set to 0.
? Structural Features: This feature is applied
by considering both subject (i.e., sub in np)
and object (i.e., obj in np) of a verb. For ex-
ample, for a v-np pair the variable sub in np is
set to 1 if the subject of v is contained in np,
set to 0 if the subject of v is not contained in
np and set to -1 if the subject of v is not avail-
able in the instance. The subject and object of
a verb are its core arguments and may some-
time be part of an event represented by a verb.
Therefore, these argument may have high ten-
dency to encode non-cause relations.
50
We set up the following integer linear program
after acquiring predictions of c and ? c labels us-
ing our supervised classifier.
Z
1
= max
?
v-np?I
?
l?L
1
x
1
(v-np, l)P (v-np, l) (1)
?
l?L
1
x
1
(v-np, l) = 1 ? v-np ? I (2)
x
1
(v-np, l) ? {0, 1} ? v-np ? I ?l ? L
1
(3)
Here L
1
= {c,?c}, I is the set of all instances
of v-np pairs and x
1
(v-np, l) is the decision vari-
able set to 1 only if the label l ? L
1
is assigned
to v-np. The Equation 2 constraints that only one
label out of |L
1
| choices can be assigned to a v-np
pair. The equation 3 requires x
1
(v-np, l) to be a
binary variable. Specifically, we try to maximize
the objective function Z
1
(equation 1) which as-
signs the label cause or non-cause to all v-np pairs
(i.e., set the variables x
1
(v-np, l) to 1 or 0 for all
l ? L
1
and for all v-np pairs in I) depending
on the probabilities of assignment of labels (i.e.,
P (v-np, l))
1
. These probabilities can be obtained
by running a supervised classification algorithm
(e.g., Naive Bayes and Maximum Entropy). In our
experiments, we provide results using the follow-
ing probabilities acquired with Naive Bayes.
P (v-np, c) = 1.0?
?
n
k=1
logP (f
k
| c)
?
n
k=1
?
l?(c,?c)
logP (f
k
| l)
P (v-np,?c) = 1.0? P ((v, np), c) (4)
where f
k
is a feature, n is total number of fea-
tures and P (f
k
| l) is the smoothed probability of
a feature f
k
given the training instances of label l.
3.2 Knowledge of Semantic classes of nouns
Philospher Jaegwon Kim (Kim, 1993) (as cited by
Girju and Moldovan (2002)) pointed out that the
entities which represent either causes or effects are
often events, but also conditions, states, phenom-
ena, processes, and sometimes even facts. There-
fore, according to this our model should have
knowledge of the semantic classes of noun phrases
with high tendency to encode cause or non-cause
relations. Considering this type of knowledge, we
can automatically review and correct the wrong
predictions made by our basic supervised classi-
fier.
1
We use the integer linear program solver available at
http://sourceforge.net/projects/lpsolve/
We argue that if a noun phrase represents a
named entity then it can have least tendency to en-
code causal relations unless there is a metonymic
reading associated with it. For example, con-
sider the following cause and non-cause examples
where noun phrase is a named entity.
4. Sandy hit Cuba as a Category 3 hurricane.
5. Almost all the weapon sites in Iraq were de-
stroyed by the United States.
In example 4, Cuba is location and does not
encode causality. However, in example 5 the
pair ?destroyed-the United States? encode causal-
ity where a metonymic reading is associated with
the location. We apply Named Entity Recog-
nizer (Finkel et al., 2005) and assume if a noun
phrase is identified as a named entity then its cor-
responding verb-noun phrase pair encodes non-
cause relation. This constraint can lead to a false
negative prediction when the metonymic reading
is associated with a noun phrase. In order to
avoid as much false negatives as possible, we im-
ply the following simple rule i.e., if one of the
following cue words appear between a verb and a
noun phrase then do not apply the constraint stated
above.
by, from, because of, through, for
In our experiments, the above simple rule helps
avoiding some false negatives but in future any
subsequent improvement with a better metonomy
resolver (Markert and Nissim, 2009) should im-
prove the performance of our model.
In addition to named entities, there can be var-
ious noun phrases with least tendency to encode
causation. Consider the following example, where
?city? is a location and does not encode cause-
effect relation with the verb ?remained?.
Substantially fewer people remained in
the city during the Hurricane Ivan evac-
uation.
In this work, we identify the semantic classes
of noun phrases which do not normally represent
events, conditions, states, phenomena, processes
and thus have high tendency to encode non-cause
relations. For this purpose, we manually examine
the inventory of labels assigned to noun phrases
in FrameNet (see table 1) and classify these labels
into two classes (c
n
p and ?c
np
). Here, the class
c
np
(?c
np
) represents the labels of noun phrases
with high (less) tendency to encode cause-effect
relations. For example, the label ?Place? ? ?c
np
51
(see table 1) represents a location and it may have
least tendency to encode causality if metonymy is
not associated with it. Using the classification of
frame elements in table 1, we obtain the annota-
tions of noun phrases from FrameNet and catego-
rize these annotations into c
np
and ?c
np
classes.
On top of the annotations of these two semantic
classes, we build a supervised classifier for pre-
dicting c
np
or ?c
np
label for the noun phrases.
After obtaining predictions, we select all noun
phrases lying in class ?c
np
and apply the same
constraint stated above for the named entities. We
use the following set of features to set up a super-
vised classifier for c
np
and ?c
np
labels.
? Lexical Features: words of noun phrase, lem-
mas of all words of noun phrase, head word
of noun phrase, first two (three) (four) letters
of head noun of noun phrase, last two, (three)
(four) letters of head noun of noun phrase.
? Word Class Features: part-of-speech tags of
all words of noun phrase, part-of-speech tag of
head noun of noun phrase.
? Semantic Features: all (frequent) sense(s) of
head noun of noun phrase.
We have acquired 23,334 (81,279) training in-
stances of c
np
(?c
np
) class, respectively for this
work. We also use WordNet to obtain more train-
ing instances of these classes. We follow the ap-
proach similar to Girju and Moldovan (2002) and
adopt some senses of WordNet (shown in table 1)
to acquire training instances of noun phrases. For
example, considering the table 1, we assign ?c
np
label to any noun whose all senses in WordNet lie
in the semantic hierarchy originated by the sense
{time period, period of time, period}. Follow-
ing this scheme, we extract instances of nouns and
noun phrases from English GigaWord corpus and
assign the labels c
np
and ?c
np
to them by em-
ploying WordNet senses given in table 1. Girju
and Moldovan (2002) have used similar scheme
to rank noun phrases according to their tendency
to encode causation. In comparison to them, we
use the WordNet senses to increase the size of
our training set of noun phrases obtained using
FrameNet above. In addition to this, we build a
automatic classifier on the training data obtained
using labels of FrameNet and WordNet senses to
classify noun phrases of test instances into two se-
mantics classes (i.e., c
np
and ?c
np
). In our train-
ing corpus of there are 2, 214, 68 instances of noun
phrases (50% belongs to each of c
np
and ?c
np
classes).
We incorporate the knowledge of semantics of
nouns in our model by making the following ad-
ditions to the integer linear program introduced in
section 3.1.
Z
2
= Z
1
+
?
np:v-np?I
?
l?L
2
x
2
(np, l)P (np, l) (5)
?
l?L
2
x
2
(np, l) = 1 ? np : v-np ? I ?M (6)
x
2
(np, l) ? {0, 1} ? np : v-np ? I ?M (7)
?l ? L
2
x
1
(v-np,?c)? x
2
(np,?c
np
) ? 0 (8)
? np : v-np, ? v-np ? I ?M
Here L
2
= {c
np
,?c
np
} and M is the set of
instances of those v-np pairs for which we con-
sider the possibility of attachment of metonymic
reading with np, x
2
(np, l) is the decision variable
set to 1 only if the label l ? L
2
is assigned to
np. The Equation 6 constraints that only one la-
bel out of |L
2
| choices can be assigned to a np.
The equation 7 requires x
2
(np, l) to be a binary
variable. The constraint 8 assumes that if an np
belongs to the semantic class ?c
np
then its corre-
sponding pair v-np is assigned the label ?c. We
maximize the objective function Z
2
(equation 5)
of our integer linear program subject to the con-
straints introduced above. We predict the semantic
class of a noun phrase using the supervised classi-
fier for c
np
and ?c
np
classes and set the probabil-
ities i.e., P (np, l) = 1, P (np, {L
2
} ? {l}) = 0 if
the label l ? L
2
is assigned to np. Again we use
Naive Bayes to predict the labels for noun phrases.
Also before running this supervised classifier, we
run the named entity recognizer and assign ?c
np
labels to all noun phrases identified as named en-
tities. For our model, we apply named entity rec-
ognizer for seven classes i.e., LOCATION, PER-
SON, ORGANIZATION, DATE, TIME, MONEY,
PERCENT (Finkel et al., 2005).
3.3 Knowledge of Semantic classes of verbs
In this section, we introduce our method to in-
corporate the knowledge of semantic classes of
verbs to identify causation. Verbs are the com-
ponents of language for expressing events of var-
ious types. In TimeBank corpus, Pustejovsky et
al. (2003) have introduced eight semantic classes
of events i.e., OCCURRENCE, PERCEPTION,
REPORTING, ASPECTUAL, STATE, I STATE,
I ACTION and MODAL. According to the defi-
nitions of these classes provided by Pustejovsky
52
Semantic
Class
FrameNet Labels WordNet Senses
c
np
Event, Goal, Purpose, Cause, Internal
cause, External cause, Result, Means, Rea-
son, Phenomena
{act, deed, human action, human activity},
{phenomenon}, {state}, {psychological
feature}, {event}, {causal agent, cause,
causal agency}
?c
np
Artist, Performer, Duration, Time, Place,
Distributor, Area, Path, Direction, Sub-
region
{time period, period of time, period},
{measure, quantity, amount}, {group,
grouping}, {organization, organisation},
{time unit, unit of time}, {clock time, time}
Table 1: This table presents some examples of FrameNet labels in c
np
and ?c
np
classes. The full set of
labels in both semantic classes are given in appendix A. It also presents the WordNet senses of nouns
lying in c
np
and ?c
np
classes.
et al. (2003), the reporting events describe the
action of a person, declare something or narrate
an event e.g., the reporting events represented by
verbs say, tell, etc. Here, we argue that a reporting
event has the least tendency to encode causation
because such an event only describes or narrates
another event instead of encoding causality with
it. We assume that the verbs representing report-
ing events have least tendency to encode causa-
tion and thus their corresponding v-np pairs have
least tendency to encode causation. To add this
knowledge to our model, we consider two classes
of verbs i.e., c
v
and ?c
v
where the class c
v
(?c
v
)
contains the verbs with high (less) tendency to en-
code causation. Using above argument we claim
that all verbs representing reporting events belong
?c
v
class and verbs representing rest of the types
of events belong to c
v
class. We build a supervised
classifier which automatically classifies verbs into
c
v
and ?c
v
classes. We extract the instances of
verbal events (i.e., verbs or verbal phrases) from
TimeBank corpus and assign the labels c
v
and
?c
v
to these instances. Using these labeled in-
stances, we build a supervised classifier by adopt-
ing the same set features as introduced in Bethard
and Martin (2006) to identify semantic classes of
verbs. Due to space constraint, we refer the reader
to Bethard and Martin (2006) for the details of fea-
tures. Again we use Naive Bayes to take predic-
tions of c
v
and ?c
v
labels and their corresponding
probabilities using equation 4.
We incorporate the knowledge of semantics of
verbs in our model by making the following addi-
tions to the integer linear program.
Z
3
= Z
2
+
?
v:v-np?I
?
l?L
3
x
3
(v, l)P (v, l) (9)
?
l?L
3
x
3
(v, l) = 1 ? v : v-np ? I (10)
x
3
(v, l) ? {0, 1} ? v : v-np ? I ?l ? L
3
(11)
x
1
(v-np,?c)? x
3
(v,?c
v
) >= 0 (12)
? v : v-np, ? v-np ? I
x
3
(v, c
v
)? x
1
(v-np, c) >= 0 (13)
? v : v-np, ? v-np ? I
Here L
3
= {c
v
,?c
v
}, x
3
(v, l) is the decision
variable set to 1 only if the label l ? L
3
is as-
signed to v. The Equation 10 constraints that only
one label out of |L
3
| choices can be assigned to
a v. The equation 11 requires x
3
(v, l) to be a bi-
nary variable. The constraint 12 assumes if a verb
v belongs to the class c
v
(i.e., has least potential
to encode causation) then its corresponding pair
v-np encodes non-causality. The constraint 12 en-
forces that if a verb v belongs to the class ?c
v
then
its corresponding v-np pair is assigned the label
?c. Similarly, the constraint 16 enforces that if
a v-np pair encodes causality then its verb v has
potential to encode causal relation. We maximize
the objective function Z
3
subject to the constraints
introduced above.
3.4 Knowledge of Indistinguishable Verb and
Noun
As introduced earlier, each causal relation is char-
acterized by two roles i.e., cause and its effect. In
order to encode causal relation, two components
of an instance of verb-noun phrase pair need to
represent distinct events, processes or phenomena.
Employing simple lexical matching, we determine
if a verb and a noun phrase represent same event
or not as follows:
53
? We use NOMLEX (Macleod et al., 2009) to
transform a verb into its corresponding nomi-
nalization and use the following text segments
for lexical matching.
T
v
= [Subject] verb [Object]
2
T
n
= Head noun of noun phrase
? We remove stopwords and duplicate words
from T
v
and T
n
and take lemmas of all words.
If the subject or object or both arguments are
contained in noun phrase then we remove these
arguments from T
v
. We determine the proba-
bility of a verb (v) and a noun phrase (np) rep-
resenting same event as follows. If head noun
(i.e., T
n
) lexically matches with any word of T
v
then set P(v ? np) to 1 and 0 otherwise.
We assign non-cause relation if P(v ? np) = 1.
Next, we incorporate the knowledge of indistin-
guishable verb and noun in our model using the
following additions to our integer linear program.
Z
4
= Z
3
+
?
v-np?I
?
l?L
4
x
4
(v-np, l)P (v-np, l) (14)
?
l?L
4
x
4
(v-np, l) = 1 ? v-np ? I (15)
x
4
(v-np, l) ? {0, 1} ? v-np ? I, ?l ? L
4
x
1
(v-np,?c)? x
4
(v-np,?) ? 0 (16)
? v-np ? I
Here L
4
= {?, 6?} where the label ? (6?) rep-
resents same (distinct) events, x
4
(v-np, l) is the
decision variable set to 1 only if the label l ? L
4
is assigned to v-np. The Equation 15 constraints
that only one label out of |L
4
| choices can be as-
signed to a v-np pair. The equation 16 requires
x
4
(v-np, l) to be a binary variable. The con-
straint 16 enforces that if a v-np pair belongs to
the class ? then this pair is assigned the label ?c.
We maximize the objective function Z
4
subject to
the constraints introduced above.
4 Evaluation and Discussion
In this section we present the experiments, evalu-
ation procedures, and a discussion on the results
achieved through our model for the current task.
In order to evaluate our model, we generated a
test set with instances of form verb-noun phrase
where the verb is grammatically connected to the
noun phrase in an instance. For this purpose, we
2
Following Riaz ang Girju (2010), we assume that the
subject and object of a verb are parts of an event represented
by a verb. Therefore, we use these arguments along with a
verb for lexical matching with a noun phrase.
collected three wiki articles on the topics of Hurri-
cane Katrina, Iraq War and Egyptian Revolution
of 2011. We selected first 100 sentences from
these articles and applied part-of-speech tagger
(Toutanova et al., 2003) and dependency parser
(Marneffe et al., 2006) on these sentences. Using
each sentence, we extracted all verb-noun phrase
pairs where the verb has a dependency relation
with any word of noun phrase. We manually in-
spected all of the extracted instances and removed
those instances in which a word had been wrongly
classified as a verb by the part-of-speech tagger.
There are total 1106 instances in our test set. We
assigned the task of annotation of these instances
with cause and non-cause relations to a human
annotator. Using manipulation theory of causal-
ity (Woodward, 2008), we adopted the annotation
guidelines from Riaz and Girju (2010) which is as
follows: ?Assign cause label to a pair (a, b), if the
following two conditions are satisfied: (1), a tem-
porally precedes/overlap b in time, (2) while keep-
ing as many state of affairs constant as possible,
modifying a must entail predictably modifying b.
Otherwise assign non-cause label. ?
We have 149 (957) cause (non-cause) instances
in our test set
3
, respectively. We evaluate the per-
formance of our model using F-score and accuracy
evaluation measures (see table 2 for results).
The results in table 2 reveal that the ba-
sic supervised classifier is a naive model and
achieves only 27.27% F-score and 46.47% ac-
curacy. The addition of novel types of
knowledge introduced in section 3 (i.e., the
model Basic+SCN
M
+SCV+IVN) brings 14.74%
(29.57%) improvements in F-score (accuracy), re-
spectively. These results show that the knowledge
of semantics of nouns and verbs and the knowl-
edge of indistinguishable verb and noun are crit-
ical to achieve performance. The maximum im-
provement in results is achieved with the addition
of semantic classes of nouns (i.e., Basic+SCN
M
).
The consideration of association of metonymic
readings using model Basic+SCN
M
helps us to
maintain recall as compared with SCN
!M
and
therefore brings better F-score.
One can notice that almost all models suffer
from low precision which leads to lower F-scores.
Although, our model achieves 14.58% increase in
precision over basic supervised classifier, the lack
of high precision is still responsible for lower F-
3
We will make the test set available
54
Model Basic +SCN
!M
+SCN
M
+SCN
M
+SCV +SCN
M
+SCV+IVN
Accuracy 46.47 75.76 74.41 75.31 76.04
Precision 16.69 28.14 29.53 30.47 31.27
Recall 74.49 50.66 64.00 64.00 64.00
F-score 27.27 39.19 40.42 41.29 42.01
Table 2: This table presents results of the basic supervised classifier (i.e., Basic) and the models after
incrementally adding the knowledge of semantic classes of nouns without consideration of metonymic
readings (i.e., + SCN
!M
), the knowledge of semantic classes of nouns with consideration of metonymic
readings (i.e., + SCN
M
), the knowledge of semantic classes of verbs (i.e., +SCN
M
+SCV) and the knowl-
edge of indistinguishable verb and noun (i.e., +SCN
M
+SCV+IVN).
score. The highly skewed distribution of test set
with only 13.47% causal instances results in lots
of false positives. We manually examined false
positives to determine the language features which
may help us reducing more false positives without
affecting F-score. We noticed that the direct ob-
jects of the verbs are mostly part of the event rep-
resented by the verbs and therefore encodes non-
causation with the verbs. For example, consider
following instances:
6. The hurricane surge protection failures
prompted a lawsuit.
7. They provided weather forecasts.
In example 6, ?lawsuit? is the direct object of
the verb ?prompted? and is part of the event rep-
resented by the verb ?prompt?. However there
is a cause relation between ?protection failures?
and ?prompted?. Similarly in example 7, the di-
rect object ?forecasts? is part of the ?providing?
event and thus the noun phrase ?weather fore-
casts? encode non-cause relation with the verb
?provide?. Therefore, following this observation
we employed the training corpus of cause and non-
cause relations (see section 3.1) and learned the
structure of verb-noun phrase pairs encoding non-
cause relations most of the time. We considered
only those training instances where the subject
and/or object of the verb was available. For the
current purpose, we picked up following four fea-
tures (1) sub in np, (2) !sub in np, (3) obj in np
and (4) !obj in np. Just to remind the reader, the
feature sub in np (!sub in np) is set to 1 if the
subject of the verb is (not) contained in the noun
phrase np, respectively. For each of the above four
features, the percentage of cause and entropy of
relations with that feature are as follows:
? sub in np (%c = 34.72, Entropy = 0.931)
? !sub in np (%c = 59.71, Entropy = 0.972)
? obj in np (%c = 28.89, Entropy = 0.867)
? !obj in np (%c = 55.30, Entropy = 0.991).
There are two important observations from
above scores: (1) verbs mostly encode non-cause
relations with their objects and subjects (i.e., high
%?c with obj in np and sub in np), (2) among
obj in np and sub in np features, obj in np yields
least entropy i.e., there are least chances of encod-
ing causality of a verb with its object.
Considering the above statistics, we enforce the
constraint on each verb-noun phrase pair that if
the object of the verb is contained in the noun
phrase of the above pair then assigns non-cause
relation to that pair. Using this constraint, we ob-
tain 46.61% (80.74%) F-score (accuracy), respec-
tively. This confirms our observation that the ob-
ject of a verb is normally part of an event repre-
sented by the verb and thus it encodes non-cause
relation with the verb.
In this research, we have utilized novel types
of knowledge to improve the performance of our
model. In future, we need to consider more
additional information (e.g., predictions from
metonymy resolver) to achieve further progress.
5 Conclusion
In this paper, we have proposed a model for iden-
tifying causality in verb-noun pairs by employing
the knowledge of semantic classes of nouns and
verbs and the knowledge of indistinguishable noun
and verb of an instance along with shallow linguis-
tic features. Our empirical evaluation of model
has revealed that such novel types of knowledge
are critical to achieve a better performance on the
current task. Following the encouraging results
achieved by our model, we invite researchers to
investigate more interesting types of knowledge in
future to make further progress on the task of rec-
ognizing causality.
55
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In proceed-
ings of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics (COLING-ACL).
Brandon Beamer and Roxana Girju. 2009. Using
a Bigram Event Model to Predict Causal Poten-
tial. In proceedings of the Conference on Compu-
tational Linguistics and intelligent Text Processing
(CICLING).
Steven Bethard and James H. Martin. 2006. Identifica-
tion of Event Mentions and their Semantic Class. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of the Associ-
ation for Computational Linguistics (ACL).
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, volume 42 issue 3, 662678.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Association for
Computational Linguistics (ACL).
Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond.
Roxana Girju and Dan Moldovan. 2002. Mining An-
swers for Causation Questions. In American Asso-
ciations of Artificial Intelligence (AAAI), 2002 Sym-
posium.
Jaegwon Kim. 1993. Causes and Events. Mackie on
Causation. In Cansation, Oxford Readings in Phi-
losophy, ed. Ernest Sosa, and Michael Tooley, Ox-
ford University Press.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, Ruth Reeves. 1998. NOMLEX: A
Lexicon of Nominalizations. In proceedings of EU-
RALEX.
Katja Markert, Malvina Nissim 2009. Data and mod-
els for metonymy resolution. Language Resources
and Evaluation Volume 43 Issue 2, Pages 123?138.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP.
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics.
Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, (WSDM).
Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).
Mehwish Riaz and Roxana Girju 2013. Toward
a Better Understanding of Causality between Ver-
bal Events: Extraction and Analysis of the Causal
Power of Verb-Verb Associations. Proceedings of
the annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL).
Dan Roth and Wen-tau Yih 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In Proceedings of the Annual Con-
ference on Computational Natural Language Learn-
ing (CoNLL).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Technol-
ogy and North American Chapter of the Association
for Computational Linguistics (HLT-NAACL).
James Woodward. 2008. Causation and Manipulation.
Online Encyclopedia of Philosophy.
Appendix A. Semantic Classes of Nouns
This appendix presents the FrameNet labels we as-
sign to c
np
and ?c
np
classes (see section 3.2).
56
Semantic Class FrameNet Labels
c
np
Event, Goal, Purpose, Cause, Internal cause, External cause, Result, Means, Reason, Phenomena, Char-
acterization, Coordinated event, Final state, Information, Topic, Containing event, Mental content, Ac-
tion, Experience, Impactee, Impactor, Message, Question, Circumstances, Desired goal, Explanation,
Required situation, Complaint, Content, Activity, Intended goal, Phenomenon, State, Dependent state,
Forgery, Purpose of Event, Negative consequences, Inference, Appraisal, Noisy event, Function, Evi-
dence, Process, Paradigm, Standard, Old order, Focal occasion, Landmark occasion, resulting action,
Victim, Issue, Effect, State of affairs, Cause of shine, Qualification, Undesirable Event, Skill, Precept,
Outcome, Norm, Act, State of Affairs, Phenomenon 1, Phenomenon 2, Quality Eventuality, Expression,
Intended event, Cognate event, Epistemic stance, Goal conditions, Possession, Support Proposition,
Domain of Relevance, Charges, Idea, Initial subevent, Hypothetical event, Scene, Purpose of Goods,
Response action, Motivation, Executed, Affliction, Medication, Treatment, Stimulus, Last subevent,
Undesirable situation, Sleep state, Initial state, Enabled situation, Grinding cause, Finding, Case, Legal
Basis, Role of focal participant, Trigger, Authenticity, World state, Emotion, Emotional state, Evalua-
tion, New idea, Production, Performance, Undertaking, Destination event
?c
np
Artist, Performer, Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region, Creator, Copy,
Original, Iteration, Manner, Frequency, Agent, Body part, Depictive, Theme, Subregion, Area, De-
gree, Angle, Fixed location, Path shape, Addressee, Entity, Individual 1, Individual 2, Road, Distance,
Speaker, Medium, Clothing, Wearer, Bodypart of agent, Locus, Cognizer, Salient entity, Name, Inspec-
tor, Ground, Unwanted entity, Location of inspector, Researcher, Population, Searcher, Sought entity,
Instrument, Created entity, Components, Forgoer, Desirable, Bad entity, Dodger, Experiencer, Vehicle,
Self mover, Speed, Cotheme, Consecutive, Re encoding, Supplier, Individuals, Driver, Complainer,
Communicator, Protagonist, Attribute, Final value, Item, Initial value, Difference, Group, Value range,
Co participant, Perceiver agentive, Target symbol, Location of perceiver, Location, Expected entity,
Focal participant, Time of Event, Variable, Limits, Limit1, Limit2, Point of contact, Goods, Lessee,
Lessor, Money, Rate, Unit, Reversive, Perceiver passive, Sound, Sound source, Location of source,
Fidelity, Official, Selector, Role, Concessive, New leader, Body, Old leader, Leader, Governed, Result
size, Size change, Dimension, Initial size, Elapsed time, Interval, Category, Criteria, Text, Final cor-
relate, Correlate, Initial correlate, Manipulator, Side 1, Sides, Side 2, Perpetrator, Value 1, Value 2,
Actor, Partner 2, Partner 1, Partners, Figure, Resident, Co resident, Student, Subject, Institution, Level,
Teacher, Undergoer, Subregion bodypart, Course, Owner, Defendant, Judge, Co abductee, Location
of appearance, Material, Accused, Arraign authority, Hair, Configuration, Emitter, Beam, Amount of
progress, Evaluee, Patient, Buyer, Seller, Recipient, Relay, Relative location, Connector, Items, Part
1, Part 2, Parts, Whole, Name source, Payer, Fine, Executioner, Interlocutor 1, Interlocutor 2, Inter-
locutors, Healer, Food, Cook, Container, Heating instrument, Temperature setting, Resource controller,
Resource, Donor, Constant location, Carrier, Sender, Co theme, Transport means, Holding location,
Rope, Knot, Handle, Containing object, Fastener, Enclosed region, Container portal, Aggregate, Sus-
pect, Authorities, Offense, Source of legal authority, Ingestor, Ingestibles, Sleeper, Pieces, Goal area,
Period of iterations, Mode of transportation, Produced food, Ingredients, Cognizer agent, Excreter,
Excreta, Air, Perceptual source, Escapee, Undesirable location, Evader, Capture, Pursuer, Amount of
discussion, Means of communication, Periodicity, Author, Honoree, Reader, Child, Mother, Father,
Egg, Flammables, Flame, Kindler, Mass theme, Address, Intermediary, Communication, Location of
communicator, Firearm, Indicated entity, Hearer, Sub region, Member, Object, Organization, Guardian,
New Status, Arguer, Criterion, Liquid, Impactors, Force, Coparticipant, Holding Location, Legal basis,
Precipitation, Quantity, Voice, Duration of endstate, Period of Iterations, Employer, Employee, Task,
Position, Compensation, Field, Place of employment, Amount of work, Contract basis, Recipients, Hot
Cold source, Temperature goal, Temperature change, Hot/Cold source, Dryee, Temperature, Traveler,
Iterations, Baggage, Deformer, Resistant surface, Fluid, Injured Party, Avenger, Injury, Punishment,
Offender, Grinder, Profiled item, Standard item, Profiled attribute, Standard attribute, Extent, Source
emitter, Emission, Sub source, Item 1, Item 2, Parameter, Form, Chosen, Change agent, Injuring entity,
Severity, Substance, Delivery device, Entry path, Wrong, Amends, Grounds, Expressor, Basis, Signs,
Manufacturer, Product, Factory, Consumer, Interested party, Performer1, Performer2, Whole patient,
Destroyer, Exporting area, Importing area, Accuracy, Time of Eventuality, Indicator, Indicated, Au-
dience, Valued entity, Journey, Duration of end state, Killer, Beneficiary, Destination time, Landmark
time, Seat of emotion, Arguers, Arguer1, Arguer2, Company, Asset, Origin, Sound maker, Static object,
Themes, Heat source, Following distance, Perceiver, Intended perceiver, Location of expressor, Path of
gaze, Relatives, Final temperature, Particular iteration, Participant 1, Language
Table 3: This table presents the FrameNet labels we assign to c
np
and ?c
np
classes.
57
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 114?123,
Gothenburg, Sweden, 26-27 April 2014. c?2014 Association for Computational Linguistics
Unsupervised Construction of a Lexicon and a Repository of Variation
Patterns for Arabic Modal Multiword Expressions
Rania Al-Sabbagh?, Roxana Girju?, Jana Diesner?
?Department of Linguistics and Beckman Institute
?School of Library and Information Science
University of Illinois at Urbana-Champaign, USA
{alsabba1, girju, jdiesner}@illinois.edu
Abstract
We present an unsupervised approach to build
a lexicon of Arabic Modal Multiword
Expressions (AM-MWEs) and a repository of
their variation patterns. These novel resources
are likely to boost the automatic identification
and extraction  of AM-MWEs1.
1 Introduction
Arabic Modal Multiword Expressions (AM-
MWEs) are complex constructions that convey
modality senses. We define seven modality
senses, based on Palmer's (2001) cross-lingual
typology, which are (un)certainty, evidentiality,
obligation, permission, commitment, ability and
volition.
AM-MWEs range from completely fixed,
idiomatic and sometimes semantically-opaque
expressions, to morphologically, syntactically
and/or lexical productive constructions. As a
result, the identification and extraction of AM-
MWEs have to rely on both a lexicon and a
repository of their variation patterns. To-date and
to the best of our knowledge, neither resource is
available. Furthermore, AM-MWEs are quite
understudied despite the extensive research on
general-purpose Arabic MWEs.
To build both the lexicon and the repository,
we design a four-stage unsupervised method.
Stage 1, we use Log-Likelihood Ratio and a
root-based procedure to extract candidate AM-
MWEs from large Arabic corpora. Stage 2, we
use token level features with k-means clustering
to construct two clusters. Stage 3, from the
clustering output we extract patterns that
describe the morphological, syntactic and
semantic variations of AM-MWEs, and store
1 Both resources are available at
http://www.rania-alsabbagh.com/am-mwe.html
them in the pattern repository. Stage 4, we use
the most frequent variation patterns to bootstrap
low-frequency and new AM-MWEs. The final
lexicon and repository are manually inspected.
Both resources are made publicly available.
The contributions of this paper are: (1) we
address the lack of lexica and annotated
resources for Arabic linguistic modality; and
hence, we support NLP applications and domains
that use modality  to identify (un)certainty (Diab
et al. 2009), detect power relations (Prabhakaran
and Rambow 2013), retrieve politeness markers
(Danescu-Niculescu-Mizil et al. 2013), extract
and reconstruct storylines (Pareti et al. 2013) and
classify request-based emails (Lampert et al.
2010); (2) we provide both a lexicon and a
repository of variation patterns to help increase
recall while keeping precision high for the
automatic identification and extraction of
productive AM-MWEs; and (3) we explore the
morphological, syntactic and lexical properties of
the understudied AM-MWEs.
For the rest of this paper, Section 2 defines
AM-MWEs. Section 3 outlines related work.
Sections 4 describes our unsupervised method.
Section 5 describes manual verification and the
final resulting resources.
2 What are AM-MWEs?
AM-MWEs are complex constructions that
convey (un)certainty, evidentiality, obligation,
permission, commitment, ability and volition.
Based on their productivity, we define five types
of AM-MWEs:
Type 1 includes idiomatic expressions like  ????
????? HtmA wlAbd (must), ??? ???? lEl wEsY
(maybe) and ???? ???? fymA ybdw (seemingly).
Type 2 covers morphologically productive
expressions such as ???? ?? yrgb fy (he wants to)
and ???? ?? wAvq mn (sure about). They inflect
114
AM-MWEs Unigram Synonym(s) English GlossArabic Transliteration Arabic Transliteration
???? ????? ??? Eqdt AlEzm ElY  ????-???? Ezmt - nwyt I intended (to)
?? ?????? ?? fy AmkAny An ?????? ymknny I can/I have the ability to
??? ?????? ??? ldy AEtqAd bAn ????? AEtqd I think
???? ?????? ??? hnAk AHtmAl bAn ????????? yuHotamal possibly/there is a possibility that
Table 1: Example AM-MWEs and their unigram synonyms
for gender, number, person, and possibly for
tense, mood and aspect. Neither the head word
nor the preposition is replaceable by a synonym.
In the literature of MWEs, Type 2 is referred to
as phrasal verbs. In the literature of modality, it
is referred to as quasi-modals (i.e. modals that
subcategorize for prepositions).
Type 3 comprises lexically productive
expressions whose meanings rely on the head
noun, adjective or verb. If the head word is
replaced by another of the same grammatical
category but a different meaning, the meaning of
the entire expression changes. Hence, if we
replace the head adjective ??????? AlDrwry
(necessary) in ?? ?? ??????? mn AlDrwry An (it
is necessary to) with ?????? Almmkn (possible),
the meaning changes from obligation to
uncertainty.
Type 4 comprises syntactically productive
expressions. It is similar to Type 3 except that
the head words are modifiable and their
arguments, especially indirect objects, can be
included within the boundaries of the MWE.
Thus, the same expression from Type 3 can be
modified as in ?? ??? ?? ??????? mn AlDrwry jdA
An (it is very necessary to). Furthermore, we can
have an inserted indirect object as in ?? ???????
?? ???????? mn AlDrwry llmSryyn An (it is
necessary for Egyptians to).
Type 5 includes morphologically, lexically
and syntactically productive expressions like  ???
???? ?? ldy yqyn An (I have faith that).
Morphologically, the object pronoun in ??? ldy (I
have) inflects for person, gender and number.
Syntactically, the head noun can be modified by
adjectives as in ?? ??? ???? ???? ldy yqyn rAsx An
(I have a strong faith that). Lexically, the
meaning of the expression relies on the head
noun ???? yqyn (faith) which is replaceable for
other modality-based nouns such as ?? ??? ??? ldy
nyp An (I have an intention to).
Despite the semantic transparency and the
morpho-syntactic and lexical productivity of the
expressions in Types 3-5, we have three reasons
to consider them as AM-MWEs:
First, although the head words in those
expressions are transparent and productive, the
other components, including prepositions,
relative adverbials and verbs, are fixed and
conventionalized. In ?? ??????? ?? mn AlDrwry
An (literally: from the necessary to; gloss: it is
necessary to), the preposition ?? mn (from)
cannot be replaced by any other preposition. In
?????? ??? ???? hnAk AHtmAl bAn (there is a
possibility that), the relative adverbial ???? hnAk
(there is) cannot be replaced by another relative
adverbial such as ??? hnA (there is). In  ????? ??????
?? ?? yHdwny AlAml fy An (hope derives me to),
the head is the noun ????? AlAml (the hope).
Therefore, the lexical verb ?????? yHdwny (drives
me) cannot be replaced by other synonymous
verbs such as ?????? yqwdqny (leads me) or ??????
ydfEny (pushes/drives me).
Second, each of those expressions has a strictly
fixed word order. Even for expressions that allow
the insertion of modifiers and verb/noun
arguments, the inserted elements hold fixed
places within the boundaries of the expression.
Complex constructions that adhere to strict
constraints on word order but undergo lexical
variation are classified by Sag et al. (2002) as
semi-fixed MWEs.
Finally, each expression of those types is
lexically perceived as a one linguistic unit that
can be replaced in many contexts by a unigram
synonym as illustrated in Table 1. According to
Stubbs (2007) and Escart?n et al. (2013), the
perception of complex constructions as single
linguistic units is characteristic of MWEs.
3 Related Work
There is a plethora of research on general-
purpose Arabic MWEs. Yet, no prior work has
focused on AM-MWEs. Hawwari et al. (2012)
describe the manual construction of a repository
for Arabic MWEs that classifies them based on
their morpho-syntactic structures.
115
Corpus Token # Types # Description
Ajdir 113774517 2217557 a monolingual newswire corpus of Modern Standard Arabic
LDC ISI 28880558 532443 an LDC parallel Arabic-English corpus (Munteanu & Marcu 2007)
YADAC 6328248 457361 a dialectal Arabic corpus of Weblogs and tweets (Al-Sabbagh & Girju 2012)
Tashkeel 6149726 358950 a vowelized corpus of Classical and Modern Standard Arabic books
Total 41472307 3566311
Table 2: Statistics for the extraction corpora
Attia et al. (2010) describe the construction of
a lexicon of Arabic MWEs based on (1)
correspondence asymmetries between Arabic
Wikipedia titles and titles in 21 different
languages, (2) English MWEs extracted from
Princeton WordNet 3.0 and automatically
translated into Arabic, and (3) lexical association
measures.
Bounhas and Slimani (2009) use syntactic
patterns and Log-Likelihood Ratio to extract
environmental Arabic MWEs. They achieve
precision rates of 0.93, 0.66 and 0.67 for
bigrams, trigrams and quadrigrams, respectively.
Al-Sabbagh et al. (2013) manually build a
lexicon of Arabic modals with a small portion of
MWEs and quasi-modals. In this paper, quasi-
modals are bigram AM-MWEs. Hence, their
lexicon has 1,053 AM-MWEs.
Nissim and Zaninello (2013) build a lexicon
and a repository of variation patterns for MWEs
in the morphologically-rich Romance languages.
Similar to our research, their motivation to
represent the productivity of Romance MWEs
through variation patterns is to boost their
automatic identification and extraction. Another
similarity is that we define variation patterns as
part-of-speech sequences. The  difference
between their research and ours is that our
variation patterns have a wider scope because we
cover both the morpho-syntactic and lexical
variations of AM-MWEs, whereas their variation
patterns deal with morphological variation only.
4 The Unsupervised Method
4.1 Extracting AM-MWEs
4.1.1 Extraction Resources
Table 22 shows the token and type counts as well
as the descriptions of the corpora used for
extraction. For corpus preprocessing, (1) html
mark-up and diacritics are removed. (2) Meta-
2Ajdir: http://aracorpus.e3rab.com/
Tashkeel: http://sourceforge.net/projects/tashkeela/
linguistic information such as document and
segment IDs, section headers, dates and sources,
as well as English data are removed. (3)
Punctuation marks are separated from words. (4)
Words in Roman letters are removed. (5)
Orthographical normalization is done so that all
alef-letter variations are normalized to A, the
elongation letter (_) and word lengthening are
removed. (6) Finally, the corpus is tokenized and
Part-of-Speech (POS) tagged by MADAMIRA
(Pasha et a. 2014); the latest version of state-of-
the-art Arabic tokenizers and POS taggers.
4.1.2 Extraction Set-up and Results
We restrict the size of AM-MWEs in this paper
to quadrigrams. Counted grams include function
and content words but not affixes. Working on
longer AM-MWEs is left for future research.
The extraction of candidate AM-MWEs is
conducted in three steps:
Step 1: we use root-based information to
identify the words that can be possible
derivations of modality roots. For modality roots,
we use the Arabic Modality Lexicon from Al-
Sabbagh et al. (2013).
In order to identify possible derivations of
modality roots, we use RegExps. For instance,
we use the RegExp (\w*)m(\w*)k(\w*)n(\w*) to
identify words such as ?????? Almmkn (the
possible), ????? Atmkn (I manage) and ???????
bAmkAny (I can) which convey modality.
This RegExp-based procedure can result in
noise. For instance, the aforementioned RegExp
also returns the word ????????? AlAmrykAn
(Americans) which happens to have the same
three letters of the root in the same order
although it is not one of its derivations. Yet, the
procedure still filters out many irrelevant words
that have nothing to do with the modality roots.
Step 2: for the resulting words from Step 1, we
extract bigrams, trigrams and quadrigrams given
the frequency thresholds of 20, 15 and 10,
respectively.
116
In previous literature on MWEs with corpora
of 6-8M words, thresholds were set to 5, 8 and
10 for MWEs of different sizes. Given the large
size of our corpus, we decide to use higher
thresholds.
Step 3: for the extracted ngrams we use the
Log-Likelihood Ratio (LLR) to measure the
significance of association between the ngram
words. LLR measures the deviation between the
observed data and what would be expected if the
words within the ngram were independent. Its
results are easily interpretable: the higher the
score, the less evidence there is in favor of
concluding that the words are independent.
LLR is computed as  in Eq. 1 where Oij and Eij
are the observed and expected frequencies,
respectively3. LLR is not, however, the only
measure used in the literature of MWEs.
Experimenting with more association measures
is left for future work.
Eq. 1: LLR = 2 ? O log
Table 3 shows the unique type counts of the
extracted ngrams. The extracted ngrams include
both modal and non-modal MWEs. For instance,
both ?? ?????? ??? ?? mn Almmkn lnA An (it is
possible for us to) and ?? ???? ??? ???? fy Aqrb
wqt mmkn (as soon as possible) are extracted as
valid quadrigrams. Both have the word ????
mmkn (possible) derived from the root m-k-n.
Both are frequent enough to meet the frequency
threshold. The words within each quadrigram are
found to be significantly associated according to
LLR. Nevertheless, mn Almmkn lnA An is an
AM-MWE according to our definition in Section
2, but fy Aqrb wqt mmkn is not. This is because
the former conveys the modality sense of
possibility; whereas the latter does not.
Therefore, we need the second clustering stage in
our unsupervised method to distinguish modal
from non-modal MWEs.
Ngram size Unique Types
Bigrams 86645
Trigrams 43397
Quadrigrams 25634
Total 96031
Table 3: Statistics for the extracted MWEs
3 We use Banerjee and Pedersen's (2003) Perl
implementation of ngram association measures.
4.2 Clustering AM-MWEs
Clustering is the second stage of our
unsupervised method to build the lexicon of the
AM-MWEs and the repository of their variation
patterns. This stage takes as input the extracted
ngrams from the first extraction stage; and aims
to distinguish between the ngrams that convey
modality senses and the ngrams that do not.
4.2.1 Clustering Set-up
The clustering feature set includes token level
morphological, syntactic, lexical and positional
features. It also has a mixture of nominal and
continuous-valued features as we explain in the
subsequent sections.
4.2.1.1 Morphological  Features
Roots used to guide the extraction of candidate
AM-MWEs in Section 4.1.2 are used as
clustering morphological features. The reason is
that some roots have more modal derivations
than others. For instance, the derivations of the
root ?
 -
?
 -
? D-r-r include ????? Drwry
(necessary), ???????? bAlDrwrp (necessarily),
and ???? yDTr (he has to); all of which convey
the modality sense of obligation. Consequently,
to inform the clustering algorithm that a given
ngram was extracted based on the root D-r-r
indicates that it is more likely to be an AM-
MWE.
4.2.1.2 Syntactic Features
In theoretical linguistics, linguists claim that
Arabic modality triggers (i.e. words and phrases
that convey modality senses) subcategorize for
clauses, verb phrases, to-infinitives and deverbal
nouns. For details, we refer the reader to Mitchell
and Al-Hassan (1994), Brustad (2000), Badawi
et al. (2004) and Moshref (2012).
These subcategorization frames can be
partially captured at the token level. For
example, clauses can be marked by
complementizers, subject and demonstrative
pronouns and verbs. To-infinitives in Arabic are
typically marked by ?? An (to). Even deverbal
nouns can be detected with some POS tagsets
such as Buckwalter's (2002) that labels them as
NOUN.VN.
Based on this, we use the POS information
around the extracted ngrams as contextual
syntactic features for clustering. We limit the
117
window size of the contextual syntactic features
to ?1 words.
Furthermore, as we mentioned in Section 2, we
define AM-MWEs as expressions with fixed
word order. That is, the sequence of the POS tags
that represent the internal structure of the
extracted ngrams can be used as syntactic
features to distinguish modal from non-modal
MWEs.
4.2.1.3 Lexical Features
As we mentioned in Section 2, except for the
head words of the AM-MWEs, other components
are usually fixed and conventionalized.
Therefore, the actual lexical words of the
extracted ngrams can be distinguishing features
for AM-MWEs.
4.2.1.4 Positional Features
AM-MWEs, especially trigrams and quadrigrams
that scope over entire clauses, are expected to
come in sentence-initial positions. Thus we use
@beg (i.e. at beginning) to mark whether the
extracted ngrams occur at sentence-initial
positions.
4.2.1.5 Continuous Features
Except for nominal morphological and lexical
features, other features are continuous. They are
not extracted per ngram instance, but are defined
as weighted features across all the instances of a
target ngram.
Thus, @beg for ngrami is the probability of
ngrami to occur in a sentence-initial position. It
is computed as the frequency of ngrami
occurring at a sentence-initial position
normalized by the total number n of ngrami in
the corpus.
Similarly, POS features are continuous. For
instance, the probability that ngrami is followed
by a deverbal noun is the frequency of its POS+1
tagged as a deverbal noun normalized by the
total number n of ngrami in the corpus.
4.2.2 Clustering Resources
As we mentioned earlier, the extracted ngrams
from the extraction stage are the input for this
clustering stage. The root features are the same
roots used for extraction. The POS features are
extracted based on the output of MADAMIRA
(Pasha et al. 2014) that is used to preprocess the
corpus - Section 4.1.1. The positional features
are determined based on the availability of
punctuation markers for sentence boundaries.
We implement k-means clustering with k set to
two and the distance metric set to the Euclidean
distance4. The intuition for using k-means
clustering is that we want to identify AM-MWEs
against all other types of MWEs based on their
morpho-syntactic, lexical and positional features.
Thus the results of k-means clustering with k set
to two will be easily interpretable. Other
clustering algorithms might be considered for
future work.
4.2.3 Clustering Evaluation and Results
4.2.3.1 Evaluation Methodology
We use precision, recall and F1-score as
evaluation metrics, with three gold sets: BiSet,
TriSet and QuadSet, for bigrams, trigrams and
quadrigrams, respectively. Each gold set has
1000 positive data points (i.e. AM-MWEs).
The gold sets are first compiled from multiple
resources, including Mitchell and Al-Hassan
(1994), Brustad (2000), Badawi et al. (2004) and
Moshref (2012). Second, each compiled gold set
is further evaluated by two expert annotators.
They are instructed to decide whether a given
ngram is an AM-MWE or not according to the
following definitions of AM-MWEs:
? They convey modality senses - Section 1
? They have unigram synonyms
? They have fixed word orders
? Their function words are fixed
Inter-annotator kappa ? scores for the BiSet,
TriSet and QuadSet are 0.93, 0.95 and 0.96,
respectively. Most disagreement is attributed to
the annotators' failure to find unigram synonyms.
The positive BiSet includes (1) phrasal verbs
such as ????? ?? ytmkn mn (he manages to),  ????
?? yEjz En (he fails to) and ???? ? yHlm be (he
longs for), (2) prepositional phrases such as ??
?????? mn Almmkn (it is possible that) and ??
??????? fy AlHqyqp (actually), (3) nominal phrases
such as ???? ?? Amly hw (my hope is to) and (4)
AM-MWEs subcategorizing for
complementizers such as ???? ??? ySrH bAn (he
declares that) and ???? ?? yErf An (he knows
that).
4 We use the k-means clustering implementation from
Orange toolkit http://orange.biolab.si/
118
The positive TriSet includes verb phrases like
???? ?? ?? yf$l fy An (he fails to) and prepositional
phrases like ?? ???????? ?? mn AlmstHyl An (it is
impossible to) and ???? ????? ??? Endy AymAn bAn
(I have faith that).
The positive QuadSet includes verb phrases
such as  ?? ???????? ????? yHdwny AlAml fy An
(hope drives me to) and prepositional phrases
such as ?? ??? ??????? ?? mn gyr Almqbwl An (it is
unacceptable to).
With these gold sets, we first decide on the
best cluster per ngram size. We use an all-or-
nothing approach; that is, for the two clusters
created for bigrams, we select the cluster with
the highest exact matches with the BiSet to be
the best bigram cluster. We do the same thing for
the trigram and quadrigram clusters. With
information about the best cluster per ngram
size, our actual evaluation starts.
To evaluate clustered bigram AM-MWEs, we
consider the output of best bigram, trigram and
quadrigram clusters to allow for evaluating
bigrams with gaps. We also tolerate
morphological differences in terms of different
conjugations for person, gender, number, tense,
mood and aspect.
For example, true positives for the bigram
AM-MWE ????? ?? ytmkn mn (he manages to)
include its exact match and the morphological
alternations of ????? ?? Atmkn mn (I manage to)
and ????? ?? ntmkn mn (we manage to), among
others. In other words, if the output of the bigram
clustering has Atmkn mn or ntmkn mn but the
BiSet has only ytmkn mn, we consider this as a
true positive.
The bigram ytmkn mn can have a (pro)noun
subject after the verb ytmkn: ytmkn ((pro)noun
gap) mn. Thus, we consider the output of the
trigram best cluster. If we find instances such as
????? ?????? ?? ytmkn Alt}ys mn (the president
manages to) or ????? ??? ?? ntmkn nHn mn (we
manages to), we consider them as true positives
for the bigram ytmkn mn as long as the trigram
has the two defining words of the bigram,
namely the verb ytmkn in any of its conjugations
and the preposition mn.
The same bigram - ytmkn mn - can have two
gaps after the head verb ytmkn as in  ????? ??????
?????? ?? ytmkn Alr}ys AlmSry mn (the
Egyptian president manages to). For that reason,
we consider the best quadrigram cluster. If we
find ytmkn ((pro)noun gap) ((pro)noun gap) mn,
we consider this as a true positive for  the bigram
ytmkn mn as long as the two boundaries of the
bigrams are represented. We could not go any
further with more than two gaps because we did
not cluster beyond quadrigrams.
False positives for the bigram ytmkn mn would
be the bigrams ????? ?????? ytmkn Alr}ys (the
president manages) and ?????? ?? Alr}ys mn (the
president to) in the bigram cluster where one of
the bigram's components - either the verb or the
preposition - is missing.
False negatives of bigrams would be those
bigrams that could not be found in any of the
best clusters whether with or without gaps.
Similar to evaluating bigrams, we consider the
output of the trigram and quadrigram best
clusters to evaluate trigram AM-MWEs. We also
tolerate morphological productivity.
For instance, the trigram ????? ????? ??? EndnA
AymAn bAn (we have faith that) conjugated for
the first person plural is a true positive for the
gold set trigram ???? ????? ??? Endy AymAn bAn
(I have faith that), that is conjugated for the first
person singular.
The same trigram Endy AymAn bAn can have
two types of gaps. The first can be a noun-based
indirect object after the preposition End. Thus,
we can have ??? ????? ????? ??? End AlnAs AymAn
bAn (people have faith that). The second can be
an adjective after the head noun AymAn. Thus we
can have ???? ????? ???? ??? Endy AymAn mTlq
bAn (I have a strong faith that).
Consequently, in the output of the quadrigram
best cluster, if we find matches to Endy AymAn
(adjective gap) bAn in any conjugations of Endy,
or if we find any matches for End (noun gap)
AymAn bAn, we consider them as true positives
for the trigram Endy AymAn bAn .
If the pronoun in End is replaced by a noun
and the adjective gap is filled, we will have a
pentagram like ??? ????? ????? ???? ??? End AlnAs
AymAn mTlq bAn (people have a strong faith
that). Since we do not extract pentagrams, we
consider chunks such as ??? ????? ????? End AlnAs
AymAn (people have faith) and ????? ???? ???
AymAn mTlq bAn (strong faith that) as false
positive trigrams. This is because the former
misses the complementizer ??? bAn (in that), and
the latter misses the first preposition ??? End
(literally: in; gloss: have).
119
Since we do not cluster pentagrams, we could
not tolerate gaps in the output of the
quadrigrams. We, however, tolerate
morphological variation. As a result,  ?????? ?????
?? ?? yHdwnA AlAml fy An (hope drives us to) is
considered as a true positive for ?????? ????? ?? ??
yHdwny AlAml fy An (hope derives me to).
It is important to note that we do not consider
the next best cluster of the larger AM-MWEs
unless we do not find any true positives in the
AM-MWE's original cluster. For example, we do
not search for bigrams' true positives in the
trigram and quadrigram clusters, unless there are
not any exact matches of the gold-set bigrams in
the bigrams' best cluster itself. The same thing
applies when evaluating trigram AM-MWEs.
4.2.3.2 Clustering Results and Error Analysis
Table 4 shows the evaluation results for bigrams,
trigrams and quadrigrams. We attribute the good
results to our evaluation methodology in the first
place because it allows counting true positives
across clusters of different ngram sizes to
account for gaps and tolerates morphological
variations. Our methodology captures the
morphological productivity of AM-MWEs which
is expected given that Arabic is morphologically-
rich. It also accounts for the syntactic
productivity in terms of insertion.
Precision Recall F1
Bigrams 0.663 0.776 0.715
Trigrams 0.811 0.756 0.783
Quadrigrams 0.857 0.717 0.780
Table 4: Clustering evaluation results
Long dependencies are a source of errors at the
recall level. Clustering could not capture such
instances as ??????? ?????? ???? ????? ??? SrH
Alr}ys AlmSry Hsny mbArk b (the Egyptian
president Hosni Mubarak declared to) because
they go beyond our quadrigram limit.
Another type of recall errors results from AM-
MWEs that do not meet the extraction frequency
threshold despite the large size of our corpus.
Our positive gold sets are sampled from
theoretical linguistics studies in which the
included illustrative examples are not necessarily
frequent. For example, we could not find
instances for the volitive ???? ??? ytwq Aly (he
longs for).
Precision errors result from the fact that our
RegExp-based procedure to guide the first
extraction stage is noisy. For instance, the
RegExp (\w*)t(\w*)w(\w*)q(\w*) that was
supposed to extract the volitive ???? ytwq (he
longs) did not return any instances for the
intended modal but rather instances for ?????
ytwqf (he stops) which interestingly
subcategorizes for a preposition and a
complementizer as in ????? ?? ?? ytwqf En An
(literally: stops from to). This subcategorization
frame is the same for modals such as ???? ?? ??
yEjz En An (literally: unable from to).
Consequently, ????? ?? ?? ytwqf En An (he stops
from to) has been clustered as a trigram AM-
MWE although it does not convey any modality
senses. This highlights another reason for
precision errors. The subcategorization frames
and hence the syntactic features used for
clustering are not always distinctive for AM-
MWEs.
The @beg feature was the least informative
among all features. In the case of bigrams, they
are mostly lexical verbs that do not occur in
sentence initial positions. Meanwhile,
punctuation inconsistencies do not enable us to
reliably mark @beg for many ngrams.
4.3 Identifying Variation Patterns
Our target is to build a lexicon and a repository
of the variation patterns for AM-MWEs to boost
their automatic identification and extraction,
given their morpho-syntactic and lexical
productivity.
In order to identify variation patterns, we use
as input the best clusters from the previous
clustering stage and follow these steps:
? We keep all function words as is with their
lexical and POS representations
? We collapse all morphological tags for
gender, number, person, tense, mood, aspect
and case
? We add a HEAD tag to the head words (i.e.
words whose roots were used for extraction)
? We add a GAP tag for adverbs, pronouns and
other gap fillers to explicitly mark gap
locations
An example pattern for the root  ?
 -
 ?
 -
? T-m-H
(wish) is  ((HEAD/*IV*) + (AlY/PREP) +
(An/SUB_CONJ)) which reads as follows: a
120
trigram AM-MWE whose head is a verb in any
conjugation followed by the preposition AlY (to)
and the subordinate conjunction An (that; to).
Another pattern that results from the
aforementioned steps for the same root of T-m-H
is ((HEAD/*IV*) + (ADV/GAP) + (AlY/PREP) +
(An/SUB_CONJ)). It means that an adverb can be
inserted in-between the HEAD and the preposition
AlY (to).
4.4 Bootstrapping AM-MWEs
We use the patterns identified in the previous
stage in two ways: first, to extract low-frequency
AM-MWEs whose HEADs have the same roots as
the pattern's HEAD; and second, to extract AM-
MWEs that have the same lexical, POS patterns
but are not necessarily derived from the modality
roots we used in extraction.
For example, from the previous section we
used ((HEAD/*IV*) + (AlY/PREP) +
(An/SUB_CONJ)) to extract the third person
feminine plural conjugation of the root T-m-H in
the trigram ??? ??? ???? yTmHn AlY An (they
wish for) that occurred only once in the corpus.
We used the same pattern to extract ???? ??? ??
ySbw AlY An (he longs for) that has the same
pattern but whose HEAD'S root S-b-b was not in
our list of modality roots.
Among the new extracted AM-MWEs are the
expressions ?? ?????? ?? mn AlmwADH An (it is
clear that) and ?? ??????? ?? mn AlTbyEy An (it is
normal that) that share the same pattern with  ??
?????? ?? mn Almmkn An (it is possible that). We
decide to consider those expressions as AM-
MWEs although they are not epistemic in the
conventional sense. That is, they do not evaluate
the truth value of their clause-based propositions,
but rather presuppose the proposition as true, and
express the speakers' sentiment towards it.
This bootstrapping stage results in 358 AM-
MWEs. They are inspected during manual
verification.
5 Manual Verification and Final Results
We manually verify the best clusters, the
bootstrapped AM-MWEs and the constructed
patterns before including them in the final
lexicon and repository to guarantee accuracy.
Besides, we manually add modality senses to the
lexicon entries. We also manually complete the
morphological paradigms of the morphologically
productive AM-MWEs. That is, if we only have
the bigram ???? ?? yrgb fy (he longs for)
conjugated for the third singular masculine
person, we manually add the rest of the
conjugations.
The final lexicon is represented in XML and is
organized by modality senses and then roots
within each sense. The lexicon comprises 10,664
entries. The XML fields describe: the Arabic
string, the size of the AM-MWE, the corpus
frequency and the pattern ID. The pattern ID is
the link between the lexicon and the repository
because it maps each lexicon entry to its lexical,
POS pattern in the repository.
Roots Senses Sizes
A-m-l 710 Epistemic 4233 Bigrams 4806
A-k-d 693 Evidential 811 Trigrams 3244
r-g-b 396 Obligative 748 Quadrigrams 2614
$-E-r 378 Permissive 755
H-s-s 370 Commissive 111
q-n-E 312 Abilitive 676
E-q-d 293 Volitive 3330
Total: 10,664
Table 5: Statistics for the AM-MWE lexicon for the
top 7 roots and the distributions of modality senses
and AM-MWE sizes
If a lexicon entry is manually added, the tag
MANUAL is used for the corpus frequency field.
Table 5 gives more statistics about the lexicon in
terms of modality senses, AM-MWE sizes and
the top 7 frequent modality roots.
The XML repository is given in the three  POS
tagsets supported by MADAMIRA. The XML
fields describe: the pattern's ID, the POS of the
head and the pattern itself with the HEADs and
GAPs marked. Appendices A and B give
snapshots of the lexicon and the repository in
Buckwalter's POS tagset.
6 Conclusion and Outlook
We described the unsupervised construction of a
lexicon and a repository of variation patterns for
AM-MWEs to boost their automatic
identification and extraction. In addition to the
creation of novel resources, our research gives
insights about the morphological, syntactic and
lexical properties of such expressions. We also
propose an evaluation methodology that accounts
for the productive insertion patterns of AM-
MWEs and their morphological variations.
For future work, we will work on larger AM-
MWEs to cover insertion patterns that we could
121
not cover in this paper. We will experiment with
different association measures such as point-wise
mutual information. We will also try different
clustering algorithms.
Acknowledgement
This work was supported by grant NPRP 09-410-
1-069 of the Qatar National Research Fund. We
would also like to thank the four anonymous
reviewers for their constructive comments.
References
Rania Al-Sabbagh and Roxana Girju. 2012. YADAC:
Yet another Dialectal Arabic Corpus. Proc. of
LREC'12, Istanbul, Turkey, May 23-25 2012
Rania Al-Sabbagh, Jana Diesner and Roxana Girju.
2013. Using the Semantic-Syntactic Interface for
Reliable Arabic Modality Annotation. Proc. of
IJCNLP'13, Nagoya, Japan, October 14-18 2013
Mohammed Attia, Antonio Toral, Lamia Tounsi,
Pavel Pecina and Josef van Genbith. 2010.
Automatic Extraction of Arabic Multiword
Expressions. Proc. of the Workshop on MWE 2010,
Beijing, August 2010
Elsaid Badawi, M.G. Carter and Adrian Gully. 2004.
Modern Written Arabic: A Comprehensive
Grammar. UK: MPG Books Ltd
Satanjeev Banerjee and Ted Pedersen. 2003. The
Design, Implementation, and Use of the Ngram
Statistic Package. Proc. of CiCling'03,  Mexico
City, USA
Ibrahim Bounhas and Yahya Slimani. 2009. A Hybrid
Approach for Arabic Multi-Word Term Extraction.
Proceedings of NLP-KE 2009, Dalian, China,
September 24-27 2009
Kristen E. Brustad. 2000. The Syntax of Spoken
Arabic: A Comparative Study of Moroccan,
Egyptian, Syrian and Kuwaiti Dialects.
Georgetown Uni. Press, Washington DC, USA
Tim Buckwalter. 2002. Arabic Morphological
Analyzer. Technical Report, Linguistic Data
Consortium, Philadelphia
Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
Dan Jurafsky, Jure Leskovec and Christopher Potts.
2013. A Computational Approach to Politeness
with Application to Social Factors. Proc. of the 51st
ACL, , Sofia, Bulgaria, August 4-9  2013
Mona Diab, Lori Levin, Teruko Mitamura, Owen
Rambow, Vinodkumar Prabhakaran, and Weiwei
Guo. 2009. Committed Belief Annotation and
Tagging. Proc. of the 3rd LAW Workshop, ACL-
IJCNLP'09, pp. 68-73, Singapore
Carla Parra Escart?n, Gyri Sm?rdal Losnegaard, Gunn
Inger Lyse Samdal and Pedro Pati?o Garc?a. 2013.
Representing Multiword Expressions in Lexical and
Terminological Resources: An Analysis for Natural
Language Processing Purposes. Proc. of eLex 2013,
pages 338-357, Tallinn, Estonia, October 17-19
2013
Abdelati Hawwari, Kfir Bar and Mona Diab. 2012.
Building an Arabic Multiword Expressions
Repository. Proc. of the 50th ACL, pages 24-29,
Jeju, Republic of Korea, July 12 2012
Andrew Lampert, Robert Dale and Cecile Paris. 2010,
Detecting Emails Containing Requests for Action.
Proc. of the 2010 ACL, pages 984-992, Los
Angeles, California, June 2010
F. Mitchell and S. A. Al-Hassan. 1994. Modality,
Mood and Aspect in Spoken Arabic with Special
Reference to Egypt and the Levant. London and
NY: Kegan Paul International
Ola Moshref. 2012. Corpus Study of Tense, Aspect,
and Modality in Diglossic Speech in Cairene
Arabic. PhD Thesis. University of Illinois at
Urbana-Champaign
Dragos Stefan Munteanu and Daniel Marcu. 2007. ISI
Arabic-English Automatically Extracted Parallel
Text, Linguistic Data Consortium, Philadelphia
Malvin Nissim and Andrea Zaninello. 2013. A
Repository of Variation Patterns for Multiword
Expressions. Proc. of the 9th Workshop of MWE,
pp. 101-105, Atlanta, Georgia, June 13-14 2013
Frank R. Palmer. 2001. Mood and Modality. 2nd
Edition. Cambridge University Press, Cambridge,
UK
Silvia Pareti, Tim O'Keefe, Ioannis Konstas, James R.
Curran and Irena Koprinska. 2013. Automatically
Detecting and Attributing Indirect Quotations.
Proc. of the 2013 EMNLP, pages. 989-1000,
Washington, USA, October 18-21 2013
Arfath Pasha, Mohamed Al-Badrashiny, Ahmed El
Kholy, Ramy Eskander, Mona Diab, Nizar Habash,
Manoj Pooleery, Owen Rambow and Ryan Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. Proc. of the 9th International Conference
on Language Resources and Evaluation, Reykjavik,
Iceland, May 26-31 2014
Vinodkumar Prabhakaran and Owen Rambow. 2013.
Written Dialog and Social Power: Manifestations of
Different Types of Power in Dialog Behavior.
Proceedings of the 6th IJCNLP, pp. 216-224,
Nagoya, Japan, October 14-18  2013
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake and Dan Flickinger. 2002. Multiword
Expressions: A Pain in the Neck for NLP.
Proceedings of CiCling 2002, pages 1-15, Mexico
City, Mexico
Michael Stubbs. 2007. An Example of Frequent
English Phraseology: Distributions, Structures and
122
Functions. Language and Computers: Corpus
Linguistics 25 Years on, pages 89-105, (17)
Appendix A: A snapshot of the XML lexicon
<lexicon name="AM-MWE Lexicon v1.0">
<modality sense="abilitive">
<head root="q-d-r">
<am-mwe string="???? ???"  len="2" freq="283" patternID="23"> </am-mwe>
<am-mwe string="???? ?????? ???" len="3" freq="7" patternID="45"> </am-mwe>
...
</head>
</modality>
<modality sense="epistemic">
<head root="g-l-b">
<am-mwe string="?? ??????" len="2" freq="122" patternID="15"> </am-mwe>
...
</head>
<head root="H-w-l">
<am-mwe string="?????? ??" len="2" freq="70" patternID="10"> </am-mwe>
...
</head>
<head root="n-Z-r">
<am-mwe string="?? ??????? ???? ?? " len="4" freq="38" patternID="50"> </am-mwe>
...
</head>
</modality>
</lexicon>
Appendix B: A snapshot of the XML repository
<repository name="AM-MWE Variation Patterns v1.0">
<tagset name="Buckwalter" pos-tagger="MADAMIRA v1.0">
...
<pattern ID="10" head-pos="*+IV+*" pos="(HEAD)+ (An/SUB_CONJ)"></pattern>
...
<pattern ID="15" head-pos="DET+NOUN+*" pos="(fy/PREP)+(HEAD)"></pattern>
...
<pattern ID="23" head-pos="ADJ+*" pos="(HEAD)+(ElY/PREP)"> </pattern>
...
<pattern ID="45" head-pos="DET+NOUN+*" pos="(lyd/NOUN)+(PRON*/GAP)*+(HEAD)+(ElY/PREP)">
</pattern>
...
<pattern ID="50" head-pos="DET+NOUN+*" pos="(mn/PREP)+(HEAD)+(ADV/GAP)*+(An/SUB_CONJ)">
</pattern>
....
</tagset>
</repository>
123
Proceedings of the SIGDIAL 2014 Conference, pages 161?170,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
In-depth Exploitation of Noun and Verb Semantics
to Identify Causation in Verb-Noun Pairs
Mehwish Riaz and Roxana Girju
Department of Computer Science and Beckman Institute
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
{mriaz2,girju}@illinois.edu
Abstract
Recognition of causality is important to
achieve natural language discourse under-
standing. Previous approaches rely on
shallow linguistic features. In this work,
we propose to identify causality in verb-
noun pairs by exploiting deeper seman-
tics of nouns and verbs. Particularly, we
acquire and employ three novel types of
knowledge: (1) semantic classes of nouns
with a high and low tendency to encode
causality along with information regard-
ing metonymies, (2) data-driven seman-
tic classes of verbal events with the least
tendency to encode causality, and (3) ten-
dencies of verb frames to encode causal-
ity. Using these knowledge sources, we
achieve around 15% improvement in F-
score over a supervised classifier trained
using linguistic features.
1 Introduction
The identification of cause-effect relations is crit-
ical to achieve natural language discourse under-
standing. Causal relations are encoded in text us-
ing various linguistic constructions e.g., between
two verbs, a verb and a noun, two discourse seg-
ments, etc. In this research, we focus on identify-
ing causality encoded between a verb and a noun
(or noun phrase). For example, consider the fol-
lowing example:
1. At least 1,833 people died in the hurricane.
In example (1), the verb-noun phrase pair
?died?-?the hurricane? encodes causality where
event ?died? is the effect of ?hurricane? event.
Previously several approaches have been pro-
posed to identify causality between two verbs
(Bethard and Martin, 2008; Riaz and Girju, 2010;
Do et al., 2011; Riaz and Girju, 2013) and dis-
course segments (Sporleder and Lascarides, 2008;
Pitler and Nenkova, 2009; Pitler et al., 2009).
However, the problem of identifying causality in
verb-noun pairs has not received a considerable
attention. For example, Do et al. (2011) have
studied this task but they worked only with a list
of predefined nouns representing events. In this
work, we focus on the linguistic construction of
verb-noun (or noun phrase) pairs where noun can
be of any semantic type.
Traditional approaches for identifying causal-
ity mainly employ linguistic features (e.g., lexical
items, part-of-speech tags of words, etc.) in the
framework of supervised learning (Girju, 2003;
Sporleder and Lascarides, 2008; Bethard and Mar-
tin, 2008; Pitler and Nenkova, 2009; Pitler et al.,
2009) and do not involve deeper semantics of lan-
guage. Analysis of such approaches by Sporleder
and Lascarides (2008) have revealed that the lin-
guistic features are not always sufficient to achieve
a good performance on the task of identifying se-
mantic relations including causality. In this work,
we propose a model that deeply processes and
acquires the specific semantic information about
the participants of a verb-noun phrase (v-np) pair
(i.e., noun and verb semantics) to identify causal-
ity with a better performance over the baseline
model depending merely on shallow linguistic fea-
tures.
The work in this paper builds on our recent work
reported in Riaz and Girju (2014). In that previ-
ous model, we identified the semantic classes of
nouns and verbs with a high and low tendency to
encode causation. For example, a named entity
such as LOCATION may have the least tendency
to encode causation. We leveraged such informa-
tion about nouns to filter false positives. Sim-
ilarly, we utilized the TimeBank?s (Pustejovsky
et al., 2006) classification of verbal events (i.e.,
Occurrence, Perception, Aspectual, State, I State,
I Action and Reporting) and their definitions to
claim that the reporting events (e.g., say, tell, etc.)
161
just describe and narrate other events instead of
encoding causality with them. We proposed an In-
teger Linear Programming (ILP) model (Roth and
Yih, 2004; Do et al., 2011) to combine noun and
verb semantics with the decisions of a supervised
classifier which only relies on linguistic features.
In this paper, we extend our previous model by
acquiring and exploiting the following three novel
types of knowledge:
1. We learn the information about tendencies of
various verb frames to encode causation. For
example, our model identifies if the subject of
verb ?destroy? (?occur?) has a high (low) ten-
dency to encode causation. Such information
helps gain performance by exploiting causal
semantics of each verb frame separately. We
also learn and incorporate information about
the verb frames in general e.g., how likely it is
for the subject of any verb to encode causation
with its verb.
2. In Riaz and Girju (2014), we utilized the Time-
Bank?s definition of reporting events to argue
that such events have the least tendency to en-
code causation. Instead of relying on human
judgment we now introduce a data intensive ap-
proach to identify the TimeBank?s classes of
events with the least tendency to encode cau-
sation.
3. Although, information about the nouns with
the least tendency to encode causation helps to
filter false positives it can lead to false nega-
tives when metonymic readings are associated
with such nouns. Therefore, we introduce a
metonymy resolver on top of our current model
to avoid false negatives.
We provide details of our previously proposed
model in section 3. We introduce new model and
discuss its performance in sections 4 and 5. Sec-
tion 6 concludes the current research.
2 Relevant Work
In Natural Language Processing (NLP), re-
searchers are showing lots of interest in the task
of identifying causality due to its various applica-
tions e.g., question answering (Girju, 2003), sum-
marization (Chklovski and Pantel, 2004), future
prediction (Radinsky and Horvitz, 2013), etc.
Several approaches have been proposed to iden-
tify causality in pairs of verbal events (Bethard
and Martin, 2008; Riaz and Girju, 2010; Do et
al., 2011; Riaz and Girju, 2013) and discourse
segments (Sporleder and Lascarides, 2008; Pitler
and Nenkova, 2009; Pitler et al., 2009). However
causality a pervasive relation of language can be
encoded via various linguistic constructions. For
example, verbs and nouns are the key components
of language to represent events. Therefore in this
work we focus on identifying causality in verb-
noun pairs.
Previously researchers have followed the path
of utilizing linguistic features in the framework
of supervised learning (Girju, 2003; Bethard and
Martin, 2008; Sporleder and Lascarides, 2008;
Pitler and Nenkova, 2009; Pitler et al., 2009).
Though linguistic features are important but other
sources of knowledge are also critically required
to achieve progress on the current task.
In recent years, researchers have proposed un-
supervised metrics to identify causality between
events (Riaz and Girju, 2010; Do et al., 2011).
For example, Riaz and Girju (2010) and Do et
al. (2011) introduced unsupervised metrics to
learn causal dependencies between events. These
metrics mainly depend on probabilities of co-
occurrences of events and do not distinguish well
causality from any other types of correlation (Riaz
and Girju, 2013). In order to overcome this prob-
lem Riaz and Girju (2013) proposed some ad-
vanced metrics which combine probabilities of co-
occurrences of events with the supervised esti-
mates of cause and non-cause relations.
Considering the importance of employing rich
sources of knowledge other than linguistic features
for the current task, we have recently proposed a
model that incorporates semantic classes of nouns
and verbs with a high and low tendency to encode
causation (Riaz and Girju, 2014). In this work, we
exploit information about verb frames, data-driven
verb semantics and metonymies to achieve more
progress on our recent work.
3 Model for Recognizing Causality
In this section we provide an overview of our pre-
vious model (Riaz and Girju, 2014) for identifying
causality in v-np pairs where v (np) stands for verb
(noun phrase). This model works in the following
two stages: (1) A supervised classifier is used to
make binary predictions (i.e., the label cause (C)
or non-cause (?C)) employing linguistic features,
and (2) noun and verb semantics are then com-
bined with the predictions of supervised classifier
in the ILP framework to identify causality.
162
3.1 Supervised Classifier
To the best of our knowledge, there is no data
set of v-np pairs with the labels C and ?C avail-
able to us. For the current task we employ some
heuristics to extract a training corpus of v-np pairs
using FrameNet (Baker et al., 1998). FrameNet
provides frame elements for the verbs and hand
annotated examples (aka annotations) of these
frame elements. Consider the following annota-
tion from FrameNet ?They died [
Cause
from shot-
gun wounds]? where the frame element ?Cause?
is given for the verb ?died?. We remove the prepo-
sition ?from? from the above annotation of frame
element to acquire an instance of v-np (i.e., died-
shotgun wounds) pair. We extract all annotations
for verbs from FrameNet in which a frame element
must contain at least one noun and no verb in it.
We found such annotations for 729 distinct frame
elements. We manually assigned the labels C and
?C to these frame elements. Cause, Purpose, Rea-
son, Result, Explanation are some examples of the
frame elements to which we assigned the label C.
Using the above mentioned assignments of labels
C and ?C to frame elements, we have acquired
a training corpus of 4, 141 (77, 119) C (?C) in-
stances from FrameNet. In order to avoid class im-
balance while training we employ an equal num-
ber of instances of both labels.
Due to space constraints, we refer the reader to
Appendix A for the details of linguistic features
to build the supervised classifier. We employ both
Naive Bayes (NB) and Maximum Entropy (Max-
Ent) algorithms to acquire predictions and prob-
abilities of assignments of labels. We set up the
following ILP using these probabilities:
Z
1
= max
?
v-np?I
?
l?L
1
x
1
(v-np, l)P (v-np, l) (1)
?
l?L
1
x
1
(v-np, l) = 1 ? v-np ? I (2)
x
1
(v-np, l) ? {0, 1} ? v-np ? I ?l ? L
1
(3)
Here, L
1
= {C,?C}, I is the set of all v-np pairs.
x
1
(v-np, l) is a binary decision variable set to 1
only if the label l ? L
1
is assigned to a v-np pair
and only one label out of |L
1
| choices can be as-
signed to a v-np pair (see constraints 2 and 3). In
particular, we maximize the objective function Z
1
(1) assigning the labels l ? {L
1
} to v-np pairs de-
pending on the probabilities of assignments (i.e.,
P (v-np, l)) obtained through the supervised clas-
sifier.
3.2 Noun and Verb Semantics
We automatically acquire and employ semantic
classes of nouns and verbs with a high and low
tendency to encode causation. Such information
helps to reduce errors in predictions of the super-
vised classifier.
We derive two semantic classes of nouns for our
purpose i.e., C
np
and ?C
np
where the class C
np
(?C
np
) represents the noun phrases with a high
(low) tendency to encode causation. For exam-
ple, a noun phrase expression for a location has
the least tendency to encode causation unless a
metonymic reading is associated with it. In or-
der to acquire these classes, we extract annotations
of 936 distinct frame elements from FrameNet
in which a frame element must contain at least
one noun and no verb in it. These annotations
of frame elements roughly represent instances of
noun phrases (np). We manually assigned the la-
bels C
np
and ?C
np
to the frame elements. For
example, we assign the label ?C
np
to the frame
element ?Place? which represents a location (see
Appendix B for some examples of the frame ele-
ments with labels C
np
and ?C
np
). We also fol-
low the approach similar to Girju and Moldovan
(2002) to employ WordNet senses of nouns to ac-
quire more instances of the classes C
np
and ?C
np
(see Appendix B for the details). We have ac-
quired a total of 280, 212 instances of np (50%
for each of the two classes i.e., C
np
and ?C
np
)
using both FrameNet and WordNet. Using these
instances, we build a supervised classifier to iden-
tify the semantic class of np (see Appendix B for
the details of features to build the classifier). We
incorporate the knowledge of semantic classes of
nouns by making the following additions to ILP:
Z
2
= Z
1
+
?
v-np?I?M
?
l?L
2
x
2
(f
np
(v-np), l)P (f
np
(v-np), l)
(4)
?
l?L
2
x
2
(f
np
(v-np), l) = 1 ? v-np ? I ?M (5)
x
2
(f
np
(v-np), l) ? {0, 1} ? v-np ? I ?M (6)
?l ? L
2
x
1
(v-np,?C)? x
2
(f
np
(v-np),?C
np
) ? 0 (7)
? v-np ? I ?M
Here L
2
= {C
np
,?C
np
}. f
np
(v-np) is a func-
tion which returns np of a v-np pair. M is the set
of v-np pairs with metonymic readings associated
with np. Currently, this set is empty and in sec-
tion 4.3 we introduce a metonymy resolver to pop-
163
ulate this set. x
2
(f
np
(v-np), l) is a binary decision
variable set to 1 only if the label l ? L
2
is assigned
to np and only one label out of |L
2
| choices can
be assigned to np (see constraints 5 and 6). Con-
straint 7 enforces that if an np belongs to the class
?C
np
then its corresponding v-np pair is assigned
the label ?C. In particular, we maximize the ob-
jective function Z
2
(4) subject to the constraints
introduced till now. For each v-np pair, we predict
the semantic class of np using our supervised clas-
sifier for the labels l ? L
2
and set the probabilities
? i.e., P (f
np
(v-np), l) = 1, P (f
np
(v-np), {L
2
} ?
{l}) = 0 if the label l ? L
2
is assigned to np. Also
before running our supervised classifier, we run a
named entity recognizer (Finkel et al., 2005) and
assign the label ?C
np
to all noun phrases identi-
fied as named entities. We also determine associa-
tion of metonymies with the noun phrases identi-
fied as named entities.
For the current task we also acquire two seman-
tic classes of verbs i.e., C
e
v
and ?C
e
v
where the
class C
e
v
(?C
e
v
) contains the verbal events with a
high (low) tendency to encode causation. In order
to derive these two classes we exploit the Time-
Bank corpus (Pustejovsky et al., 2003) which pro-
vides seven semantic classes of verbal events ? i.e.,
Occurrence, Perception, Aspectual, State, I State,
I Action and Reporting. According to the defini-
tions of these classes, we claim that the report-
ing events (e.g., say, tell, etc.) just describe and
narrate other events instead of encoding causality
with them. Using this claim, we consider that all
instances of reporting verbal events of TimeBank
belong to the class ?C
e
v
and the rest of instances
of verbal events lie in the class C
e
v
. After ac-
quiring instances of the classes C
e
v
and ?C
e
v
, we
build a supervised classifier for these two classes.
We use the features introduced by Bethard and
Martin (2006) to build this classifier (see Bethard
and Martin (2006) for the details). Employing pre-
dictions and probabilities of assignments of the la-
bels C
e
v
and ?C
e
v
we add the following two con-
straints to ILP: (1) if the event represented by v
belongs to ?C
e
v
then the corresponding v-np pair
must be labeled with ?C and (2) if a v-np pair is
a causal pair then the event represented by v must
be labeled with C
e
v
.
4 Enriched Verb and Noun Semantics
This section describes the novel contributions of
this work i.e., identification of semantics of verb
frames, semantic classes of verbal events via a data
intensive approach and association of metonymic
readings with noun phrases to identify causality
with a better performance.
4.1 Verb Frames
We introduce a method to acquire tendencies of
various verb frames to encode causation. Consider
the following two examples to understand the ten-
dencies of verb frames of form {v, gr} to encode
causation where v is the verb and gr is the gram-
matical relation of np with the verb v.
1. The Great Storm of October 1987 almost totally de-
stroyed the eighty year old pinetum at Nymans Garden
in Sussex. (Cause (C))
2. The explosion occurred in the city?s main business area.
(Non-Cause (?C))
In above two examples the nps ?The Great
Storm of October 1987? and ?The explosion? have
the grammatical relations of subject with the verbs
?destroyed? and ?died?. In examples (1) and (2)
the verb frames {destroy, subject} and {occur,
subject} encode cause and non-cause relations.
These examples reveal that each verb frame has
its own tendency to encode causation. This type
of knowledge helps gain performance by exploit-
ing the semantics of each verb frame separately.
We leverage FrameNet annotations to acquire
such type of knowledge. We collect all annota-
tions of verbs from FrameNet and assign the la-
bels C and ?C to the frame elements as discussed
in section 3.1. In FrameNet, example (1) is given
as follows:
3. [
Cause
The Great Storm of October 1987] [
Degree
almost
totally] destroyed [
Undergoer
the eighty year old pinetum
at Nymans Garden in Sussex].
According to our assignments of labels C and
?C to the frame elements, example (1) is given
as ?[
C
The Great Storm of October 1987] [
?C
al-
most totally] destroyed [
?C
the eighty year old
pinetum at Nymans Garden in Sussex].?. After ac-
quiring instances of the labels C and ?C from ex-
ample (1), we populate the fields of a knowledge
base of verb frames (see Table 1). Fields of this
knowledge base are {v, gr}, count({v, gr},C) and
count({v, gr},?C). gr is the dependency relation
of the frame element with the verb v. We use Stan-
ford?s dependency parser (Marneffe et al., 2006)
to collect dependency relations. count({v, gr},C)
(count({v, gr},?C)) is the count of the label C
(?C) of the frame {v, gr}. As shown in Table 1,
164
for the frame element ?The Great Storm of Octo-
ber 1987?, the word ?Storm? has the dependency
relation of ?nsubj? with the verb ?destroy?. If
there exists more than one dependency relations
between the frame element and its verb then we
choose the very first relation in the text order. Ac-
cording to the counts given in Table 1, {destroy,
nsubj} has more tendency to encode a cause re-
lation than the non-cause one. We have acquired
7,156 and 114,898 instances of the labels C and
?C from FrameNet for populating the knowledge
base of verb frames. We compute tendencies of
verb frames to encode causality using the follow-
ing scores:
S({v, gr}, l) = S
1
({v, gr}, l)? S
2
({*, gr}, l) (8)
S
1
({v, gr}, l) =
count({v,gr},l)
count({v,gr},l)+count({v,gr},L
1
?{l})
S
2
({*, gr}, l) =
count({*,gr},l)
count({*,gr},l)+count({*,gr},L
1
?{l})
Counts of first component (S
1
) can be taken
from the knowledge base of verb frames of form
{v, gr}. The second component (S
2
) with counts
count({*, gr}, l) and count({*, gr}, L
1
? {l})
captures tendencies of verb frames in general.
For example, what is the tendency of any subject
to encode causality with its verb i.e., the score
S
2
({?, nsubj},C). We populate the knowledge
base of Table 1 with equal number of C and ?C
instances to calculate counts for S
2
. We make the
following additions to ILP to incorporate informa-
tion about verb frames:
Z
3
= Z
2
+
?
v-np?I?
g(v-np)?KB?
f
np
(v-np)?C
np
?
l?L
1
x
3
(g(v-np), l)S(g(v-np), l)
(9)
?
l?L
1
x
3
(g(v-np), l) = 1 ?
v-np?I?
g(v-np)?KB?
f
np
(v-np)?C
np
(10)
x
3
(g(v-np), l) ? {0, 1} ?l ? L
1
,?
v-np?I?
g(v-np)?KB?
f
np
(v-np)?C
np
(11)
x
3
(g(v-np), l) ? x
1
(v-np, l) ?l ? L
1
, (12)
?
v-np?I
?g(v-np)?KB
? f
np
(v-np)?C
np
x
1
(v-np, l) ? x
3
(g(v-np), l) ?l ? L
1
, (13)
?
v-np?I?
g(v-np)?KB?
f
np
(v-np)?C
np
Here, KB is the knowledge base of verb frames
and g(v-np) is the function which returns the verb
frame i.e., {v, gr}. This function returns NULL
value if there is no grammatical relation between
v and np in an instance. The above changes in
ILP are only applicable for the v-np pairs with
{v, gr} count({v, gr},C) count({v, gr},?C)
{destroy,nsubj} 1 0
{destroy,advmod} 0 1
{destroy,dobj} 0 1
[
C
The Great Storm of October 1987] [
?C
almost totally] de-
stroyed [
?C
the eighty year old pinetum at Nymans Garden in
Sussex].
Table 1: A knowledge base of verb frames. This
knowledge base is populated using the instances
of C and ?C labels given in this table.
g(v-np) ? KB and np identified as of class C
np
because we have already filtered the cases of np ?
?C
np
in section 3.2. x
3
(g(v-np), l) is a binary de-
cision variable set to 1 only if the label l ? L
1
is assigned to g(v-np) and only one label out of
|L
1
| choices can be assigned to g(v-np) (see con-
straints 10 and 11). We add information about verb
frames using constraints 12 and 13. These con-
straints enforce the predictions of the supervised
classifier of causality (section 3.1) to be consis-
tent with the predictions using tendencies of verb
frames (i.e., score S({v, gr}, l)). We maximize
objective function (9) subject to the above con-
straints. We remove those {v, gr} from KB which
have count({v, gr},C)+count({v, gr},?C) < 5
to avoid wrong predictions based on the small
counts of verb frames.
4.2 Data-driven Verb Semantics
In section 3.2 we considered that reporting events
belong to the class ?C
e
v
with the least tendency
to encode causation using the definition of these
events in the TimeBank corpus. Instead of re-
lying on definitions of events we now introduce
a data intensive approach to automatically iden-
tify the class ?C
e
v
of verbal events. In order
to identify this class we extract training instances
of verbal events encoding C and ?C relations.
Verbal events encode cause-effect relations using
verb-verb (e.g., Five shoppers were killed when a
car blew up.) and verb-noun linguistic construc-
tions. Therefore for the current purpose we use
the following two types of training instances: (A)
a training corpus of 240K instances of verb-verb
(v
i
-v
j
) pairs encoding C and ?C relations (named
as Training
v
i
-v
j
) (we refer the reader to Riaz and
Girju (2013) for the details of this training corpus)
and (B) the training corpus v-np instances intro-
duced in section 3.1 (named as Training
v-np
).
Following is the procedure to derive V
?C
?
V where V={Occurrence, Perception, Aspectual,
State, I State, I Action, Reporting} and the set
V
?C
contains the TimeBank?s semantic classes
165
with the least tendency to encode a cause relation.
1. Input: Training corpus, V
2. Output: Set V
?C
3. For each training instance k employ the supervised clas-
sifier of Bethard and Martin (2006) to do the following:
(a) if k ? Training
v
i
-v
j
then identify the semantic class
(sc) of both events represented by both verbs v
i
and
v
j
and add this information to a set i.e., T = T ?
(k
v
i
, sc
v
i
, l) ? (k
v
j
, sc
v
j
, l) where sc
v
i
is the se-
mantic class of event of the verb v
i
of instance k
and l ? {C,?C}.
(b) Else if k ? Training
v-np
then identify the semantic
class (sc) of event represented by the verb v and set
T = T ? (k
v
, sc
v
, l).
4. Using results of step 3, calculate tendency of each se-
mantic class sc ? V to encode non-causality (i.e.,
score(sc,?C)) as follows:
score(sc,?C) = score
1
(sc,?C)? score
2
(sc,?C)
score
1
(sc,?C) = (
count(sc,?C)
count(sc)
?
count(sc,C)
count(sc)
)
score
2
(sc,?C) = (
count(sc,?C)
count(?C)
?
count(sc,C)
count(C)
)
where count(m, n) is the number of instances of verbal
events with the labels m and n and count(m) is the number
of instances of verbal events with the label m.
5. Acquire a ranked list of semantic classes list
sc
= [sc
1
, sc
2
,
. . . sc
m
] s.t. score(sc
i
,?C) ? score(sc
i+1
,?C). From
this list we remove the class sc
i
if either score
1
(sc
i
, ?c)
< 0 or score
2
(sc
i
, ?c) < 0.
. The following steps are used to determine the cutoff
class sc
i
? list
sc
s.t. the semantic classes {sc
1
, sc
2
, . . .,
sc
i-1
} have the least tendency to encode causation.
6. result
sc
?1
= 0 and result
sc
0
= 0.
7. Remove sc
i
from the front of list
sc
and do the following:
(c) Predict the label (l) ?C for all tuples of form
(m, sc, l) ? T if sc ? {sc
1
, sc
2
, . . ., sc
i
} and pre-
dict C for the rest of the tuples.
(d) Using predictions from step (c), calculate the
result
sc
i
= F1-score ? accuracy for the label l ?
{C,?C}.
(e) If result
sc
i
?result
sc
i-1
< result
sc
i-1
?result
sc
i-2
then output {sc
1
, sc
2
, . . ., sc
i?1
}
(f) Else go to step 7.
Using the above procedure, we obtain the
sets {Aspectual} and {Reporting, I State} with
Training
v
i
-v
j
and Training
v-np
corpora. We con-
sider that the Aspectual, Reporting and I State
events of the TimeBank corpus belong to the class
?C
e
v
and rest of the events lie in C
e
v
. Using these
semantic classes we apply the constraints intro-
duced in section 3.2.
4.3 Metonymy Resolution:
Metonymy resolution is the task to determine if a
literal or non-literal reading is associated with a
{v, gr} count({v, gr},C
np
) count({v, gr},?C
np
)
{kill,nsubj} 1 0
{kill,dobj} 0 1
[
C
np
Pissed off Angelus] just kills [
?C
np
me]
Table 2: A knowledge base of verb frames. This
knowledge base is populated using the instances
of C
np
and ?C
np
labels given in this table.
natural language expression (Markert and Nissim,
2009). Consider the following example:
4. The United States has killed Osama bin Laden and has
custody of his body. (Cause (C))
In example (4) ?The United States? refers to a
non-literal reading i.e., the event of ?raid in Ab-
bottabad on May 2, 2011 by the United States?
rather than merely referring to a literal sense i.e.,
a country. The association of non-literal reading
with ?The United States? results in killing event.
Previously, researchers have worked with hand-
annotated selectional restrictions violation for this
task (Markert and Nissim, 2009). In the exam-
ple (4) a country cannot ?kill? someone and thus
a metonymic reading is associated with it. In this
work we identify association of metonymies with
noun phrases via verb frames and prepositions as
explained below in this section.
In the first part of our approach we employ
violations of tendencies of verb frames to iden-
tify if a non-literal reading is associated with a
noun phrases. Particularly, we build a knowledge
base of verb frames using C
np
and ?C
np
classes
as discussed in section 4.1. Consider the knowl-
edge base given in Table 2 populated using the
following FrameNet annotations ?[
Stimulus
Pissed
off Angelus] just kills [
Experiencer
me].? with as-
signments of labels C
np
and ?C
np
to the frame
elements. We populate the knowledge base using
only those FrameNet annotations in which a frame
element does not contain a verb.
Now we introduce our method to identify the
association of non-literal reading with the ?The
United States? in example (4). The supervised
classifier predicts the class ?C
np
for the np ?The
United States?. However, in the current state
of knowledge base (Table 2) P({destroy, nsubj},
C
np
) > P({destroy, nsubj}, ?C
np
) where P is
the probability. The prediction of ?C
np
for ?The
United States? violates the above probabilities.
Considering this violation, we predict the associa-
tion of metonymy with np.
In the second part of our approach we iden-
tify tendencies of prepositions to encode causation
166
and use violation of these tendencies to identify
metonymies. For this purpose, we use the training
corpus of v-np pairs with 4, 141 C and 77, 119 ?C
training instances (see section 3.1). We employ
only those training instances in which a preposi-
tion appears between v and np and there appears
no verb between them. From these instances, we
acquire a set of prepositions that appear between
v and np. Using this set of prepositions (PR) as
input to the following procedure, we acquire a set
of prepositions (PR
C
) with the highest tendency
to encode causation:
1. Input: Training Corpus of v-np pairs, PR
2. Output: PR
C
3. Calculate tendency of each preposition pr ? PR to en-
code causality (i.e., score(pr,C)) as follows:
score(pr,C) = score
1
(pr,C)? score
2
(pr,C)
score
1
(pr,C) = (
count(pr,C)
count(pr)
?
count(pr,?C)
count(pr)
)
score
2
(pr,C) = (
count(pr,C)
count(C)
?
count(pr,?C)
count(?C)
)
4. Acquire a ranked list of prepositions list
pr
= [pr
1
, pr
2
, . . .
pr
m
] s.t. score(pr
i
,C) ? score(pr
i+1
,C). From this
list we remove pr
i
if either score
1
(pr
i
, C) or score
2
(pr
i
,
C) < 0.
5. result
pr
?1
= 0, result
pr
0
= 0
6. Remove pr
i
from the front of the list
pr
and do the follow-
ing:
(a) Predict the label C for all v-np training instances
with pr ? {pr
1
, pr
2
, . . ., pr
i
} and assign the label
?C to the rest of the instances.
(b) Using predictions from step (a) calculate the
result
pr
i
= F1-score ? accuracy.
(c) If result
pr
i
-result
pr
i-1
< result
pr
i-1
-result
pr
i-2
then
output {pr
1
, pr
2
, . . ., pr
i?1
}.
(d) Else go to step 6.
The above procedure outputs the set PR
C
=
{for, by}. Now we introduce method to identify
association of non-literal reading for the example
?All weapon sites in Iraq were destroyed by the
United States? where ?the United States? ? ?C
np
as identified by the supervised classifier. However,
the preposition ?by? has a high tendency to en-
code causation and thus ?the United States? may
encode causation. Therefore, there is a possibil-
ity that this noun phrase has a non-literal sense
attached to it which results in encoding causality.
Using this method, we predict metonymies only
for the v-np instances where preposition appears
between v and np and there appears no verb be-
tween them. If any of two methods of metonymy
resolution predicts the association of metonymy
with np then we add v-np to the set M used in
ILP (see section 3.2).
5 Evaluation and Discussion
In this section we present experiments and discus-
sion on the performance achieved for the current
task. In order to evaluate our model, we generated
a test set of instances of v-np pairs. For this pur-
pose, we collected three wiki articles on the topics
of Hurricane Katrina, Iraq War and Egyptian Rev-
olution of 2011. We apply a part-of-speech tagger
and a dependency parser on all sentences of these
three articles (Toutanova et al., 2003; Marneffe et
al., 2006). We extracted all v-np pairs from each
sentence of these articles. For each of the these
three articles, we selected first 500 instances of v-
np pairs. Two annotators were asked to provide the
labels C and ?C to the instances of v-np pairs us-
ing the annotation guidelines from Riaz and Girju
(2010). We have achieved a 0.64 kappa score for
the human inter-annotator agreement on a total of
1,500 v-np instances. This results in a total of
1,365 instances of v-np pairs with 11.86% C pairs.
In this section, we present performance of the
following models (see Table 3):
1. Baseline: NB and MaxEnt (McCallum, 2002)
supervised classifiers using only the shallow
linguistic features (see section 3.1).
2. Basic noun and verb semantics: ILP with
the addition of semantic classes of nouns
without metonymy (denoted by +N
!M
) and
the addition of semantic classes of verbs
where ?C
e
v
={(R)eporting events} (denoted by
+N
!M
+V
{R}
). These models represent the
work proposed in Riaz and Girju (2014) (sec-
tion 3).
3. Noun semantics with metonymies: ILP
with the addition of noun semantics involv-
ing metonymies resolved via verb frames (de-
noted by +N
M
1
), metonymies resolved via verb
frames {v, gr} where gr ? GR = {csubj, csub-
jpass, nsubj, nsubjpass, xsubj, dobj, iobj, pobj,
agent} a set of core dependency relations of
subjects and objects (denoted by +N
M
1
GR
) and
metonymies resolved via both verb frames and
prepositions (denoted by +N
M
1
GR
+M
2
).
4. Verb frames and data-driven verb seman-
tics: ILP with the addition of information about
verb frames (denoted by +N
M
+VF where M =
M
1
GR
+M
2
), data-driven verb semantics i.e.,
?C
e
v
={(A)spectual, (R)eporting, (I) (S)tate
events} (denoted by +N
M
+V
{A,R,IS}
) and both
verb frames and data-driven verb semantics
(denoted by +N
M
+VF+V
{A,R,IS}
)
167
S B +N
!M
+N
!M
+V
{R}
+N
M
1
+N
M
1
GR
+N
M
1
GR
+M
2
+N
M
+VF +N
M
+V
{A,R,IS}
+N
M
+VF +V
{A,R,IS}
A 28.86 71.86 73.40 71.35 71.42 71.64 72.96 75.16 76.19
P 13.52 26.18 27.21 26.29 26.34 27.54 28.39 29.93 30.82
R 92.59 75.30 74.07 78.39 78.39 85.18 83.95 81.48 80.86
F 23.60 38.85 39.80 39.37 39.44 41.62 42.43 43.78 44.63
A 61.46 80.73 81.17 80.65 80.73 81.02 81.39 81.75 82.05
P 19.46 32.02 32.72 32.41 32.52 34.09 34.66 35.25 35.64
R 71.60 55.55 55.55 58.02 58.24 64.19 64.19 64.19 63.58
F 30.60 40.63 41.18 41.59 41.68 44.53 45.02 45.51 45.67
Table 3: Performance of (B)aseline, +N
!M
, +N
!M
+V
{R}
, +N
M
1
, +N
M
1
GR
, +N
M
1
GR
+M
2
, +N
M
+VF,
+N
M
+V
{A,R,IS}
and +N
M
+VF+V
{A,R,IS}
(see text for details) in terms of (S)cores of (A)ccuracy,
(P)recision, (R)ecall, (F)-score. The row 1 (2) of this table presents results over NB (MaxEnt) base-
line supervised classifier, respectively.
Table 3 shows that MaxEnt gives a very high ac-
curacy and F-score as compared with NB. Model
+N
!M
+V
{R}
with basic noun and verb seman-
tics introduced in section 3.2 results in more than
10% improvement in F-score over NB and Max-
Ent classifiers relying only on shallow linguis-
tic features. Model +N
M
+VF+V
{A,R,IS}
with en-
riched verb and noun semantics brings more than
4% improvement in F-score over +N
!M
+V
{R}
with MaxEnt as baseline. We perform statis-
tical significance test using bootstrap sampling
method given in Berg-Kirkpatrick et al. (2012)
(see Berg-Kirkpatrick et al. (2012) for the de-
tails). +N
M
+VF+V
{A,R,IS}
brings significant im-
provement in F-score over +N
!M
+V
{R}
with p-
value 0.0.
Though +N
!M
gives significantly better F-score
over baseline, it drops recall by more than 16%.
Metonymy resolution helps perform quite bet-
ter by recovering more than 8% recall with
+N
M
1
GR
+M
2
over +N
!M
. +N
M
1
GR
+M
2
also re-
sults in 3.9% improvement in F-score over +N
!M
with MaxEnt as baseline model (significant im-
provement with p-value 0.0). Metonymies re-
solved via verb frames with all and core grammat-
ical relations (i.e., set GR) recover more than 2%
recall and slightly improve F-score.
Model with the addition of information of verb
frames (i.e.,+N
M
+VF) brings 0.49% improve-
ment in F-score over +N
M
1
GR
+M
2
using Max-
Ent as baseline model (significant improvement
with p-value 0.027). Model with the addition of
data-driven verb semantics (i.e., +N
M
+V
{A,R,IS}
)
results in 0.98% improvement in F-score over
+N
M
1
GR
+M
2
using MaxEnt as baseline model
(significant improvement with p-value 0.0021).
Overall the model +N
M
+VF+V{A,R, IS} yields
more than 16% (20%) F-score (accuracy) over the
baseline models build via NB and MaxEnt.
5.1 Error Analysis
We performed error analysis for the model
+N
M
+VF+V
{A,R,IS}
by randomly selecting 50
False Positives (FP) and 50 False Negatives (FN).
For 32% FP instances information about verb
frames is not available in the knowledge base of
verb frames. To avoid this problem researchers
should exploit some abstractions e.g., {semantic
sense of v, gr} frames. Our model fails to iden-
tify the class ?C
np
for noun phrases of 29% FP
instances due to the lack of enough training data
for the semantic classes of nouns. In 21% FP
instances v and np are not even relevant to each
other. Our model first needs to determine rele-
vance between v and np before identifying causal-
ity. Remaining 18% instances have v and np in
temporal only sense, comparison relation or both
represent parts of same event. There is need to ex-
tract more knowledge sources to better distinguish
causality from any other type of relation.
77% FN instances are classified as non-causal
due to the lack of enough v-np training data and
require more sources of knowledge e.g., back-
ground knowledge. On remaining 23% FN in-
stances our model fails to identify C
np
class due
to the lack of enough training data for the seman-
tic classes of nouns.
6 Conclusion
This work has revealed that enriched semantics
of nouns and verbs help gain significant improve-
ment in performance over a baseline relying only
on shallow linguistic features. Through empiri-
cal evaluation and error analysis of our model we
have highlighted strengths and weaknesses of our
model for the current task. Our work has provided
a novel direction to exploit semantics of partici-
pants of causal relations to solve the challenge of
identifying causality.
168
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In proceed-
ings of COLING-ACL. Montreal, Canada.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statisti-
cal significance in NLP. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL)..
Steven Bethard and James H. Martin. 2006. Identifica-
tion of Event Mentions and their Semantic Class. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of ACL-08:
HLT.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04). Barcelona, Spain.
Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of EMNLP-2011.
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Association for
Computational Linguistics (ACL).
Roxana Girju and Dan Moldovan. 2002. Mining An-
swers for Causation Questions. In American Asso-
ciations of Artificial Intelligence (AAAI), 2002 Sym-
posium.
Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond 2003.
Katja Markert and Malvina Nissim 2009. Data and
models for metonymy resolution. Language Re-
sources and Evaluation Volume 43 Issue 2, Pages
123?138.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Andrew K. McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP,
2009.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP, 2009.
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics.
Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, WSDM ?13.
Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).
Mehwish Riaz and Roxana Girju 2013. Toward
a Better Understanding of Causality between Ver-
bal Events: Extraction and Analysis of the Causal
Power of Verb-Verb Associations. Proceedings of
the annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL).
Mehwish Riaz and Roxana Girju 2014. Recognizing
Causality in Verb-Noun Pairs via Noun and Verb Se-
mantics. Proceedings of the Workshop on Computa-
tional Approaches to Causality in Language EACL,
2014.
Dan Roth and Wen-tau Yih 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In Proceedings of the Annual Con-
ference on Computational Natural Language Learn-
ing (CoNLL).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3, July 2008
Pages 369?416.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Technol-
ogy and North American Chapter of the Association
for Computational Linguistics (HLT-NAACL).
169
Appendix A. Supervised Classifier
In this appendix, we provide a set of linguistic fea-
tures taken from Riaz and Girju (2014) to iden-
tify causality in v-np pairs employing a supervised
classifier (see section 3.1 for the details).
? Lexical Features: verb, lemma of verb,
noun phrase, lemmas of all words of
noun phrase, head noun of noun phrase,
lemmas of all words between verb and
noun phrase.
? Syntatic Features: part-of-speech tags of verb
and head noun of noun phrase.
? Semantic Features: We adopted this fea-
ture from Girju (2003) to capture semantics
of nouns. The 9 noun hierarchies of Word-
Net i.e., entity, psychological feature, abstrac-
tion, state, event, act, group, possession, phe-
nomenon are used as this feature. Each of these
hierarchies is set to 1 if any sense of head noun
of noun phrase lies in that hierarchy, otherwise
set to 0.
? Structural Features: This feature is applied
by considering both subject (i.e., sub in np)
and object (i.e., obj in np) of verb (v). For ex-
ample, for the pair v-np the variable sub in np
is set to 1 if the subject ? np, set to 0 if the
subject 6? np and set to -1 if the subject is not
available in an instance. The subject and object
of a verb are its core arguments and may some-
time be part of an event represented by a verb.
Therefore, these arguments may have high ten-
dency to encode non-causation with their verb.
? Pairs: The following pairs (verb, head noun
of noun phrase), (subject
verb
, head noun of
noun phrase) and (object
verb
, head noun of
noun phrase) are used to capture relations.
Appendix B. Noun Semantics
In this appendix, some examples of the frame ele-
ments of FrameNet and the WordNet senses be-
longing to the classes C
np
and ?C
np
are given
in Tables 4 and 5 (see section 3.2 for the de-
tails). We employ training instances acquired us-
ing the FrameNet annotations and WordNet senses
for building a supervised classifier for the classes
C
np
and ?C
np
. Following is the list of features we
use for this supervised classifier:
? Lexical Features: All words of noun phrase,
lemmas of all words of noun phrase, head noun
of noun phrase, first two (three) (four) letters
SC FrameNet Labels
c
np
Event, Goal, Purpose, Cause, Internal cause, External
cause, Result, Means, Reason, Phenomena, Coordi-
nated event, Action, Activity, Circumstances, Desired
goal, Explanation
?c
np
Artist, Performer, Duration, Time, Place, Distributor,
Area, Path, Direction, Sub-region Frequency, Body
part, Area, Degree, Angle, Fixed location, Path shape,
Addressee, Interval
Table 4: Some examples of the frame elements of
FrameNet to which we assign the semantic classes
C
np
and ?C
np
.
SC WordNet Senses
c
np
{act, deed, human action, human activity},
{phenomenon}, {state}, {psychological feature},
{event}, {causal agent, cause, causal agency}
?c
np
{time period, period of time, period}, {measure,
quantity, amount}, {group, grouping}, {organization,
organisation}, {time unit, unit of time}, {clock time,
time}
Table 5: This table shows our selected Word-
Net senses of nouns belonging to classes C
np
and
?C
np
. For example, using the information pro-
vided in this table we assume that any noun con-
cept whose all senses of WordNet lie in the seman-
tic hierarchy of the sense {time period, period of
time, period} is of class ?C
np
. We use English
Gigaword corpus to collect instances of noun (or
noun phrases) and label them with C
np
and ?C
np
according to their senses in WordNet.
of head noun of noun phrase, last two, (three)
(four) letters of head noun of noun phrase.
? Word Class Features: part-of-speech tags of
all words of noun phrase and head noun of
noun phrase.
? Semantic Features: Frequent sense of head
noun of noun phrase.
170
LAW VIII - The 8th Linguistic Annotation Workshop, pages 139?148,
Dublin, Ireland, August 23-24 2014.
Interactive Annotation for Event Modality in Modern Standard 
and Egyptian Arabic Tweets 
 
Rania Al-Sabbagh?, Roxana Girju?, Jana Diesner? 
?Department of Linguistics and Beckman Institute  
?School of Library and Information Science  
University of Illinois at Urbana-Champaign, USA 
{alsabba1, girju, jdiesner} @illinois.edu 
 
Abstract 
We present an interactive procedure to annotate a large-scale corpus of Modern Standard and 
Egyptian Arabic tweets for event modality that comprises obligation, permission, commitment, 
ability, and volition. The procedure splits up the annotation process into a series of simplified 
questions, dispenses with the requirement of expert linguistic knowledge, and captures nested 
modality triggers and their attributes semi-automatically.  
1  Introduction 
Event modality, according to Palmer (2001), describes events that are not actualized but are 
merely potential. It comprises obligation, permission, commitment, ability, and volition. Both 
obligation and permission emanate from an external authority such as the law; whereas 
commitments are the obligations placed by speakers on themselves as in promises. Ability is the 
(in)capacity to do something. Volition is broadly defined as intensions, desires, wishes, and 
preferences. Event modality is used for several NLP tasks, including sales and marketing 
analysis (Ramanand et al. 2010, Carlos and Yalamanchi 2012), sentiment analysis (Chardon et 
al. 2013), the automatic detection of request emails (Lampert et al. 2010), and the classification 
of animacy and writers' emotions (Liao and Liao 2009, Bowman and Chopra 2012). 
To-date, there are no large-scale Arabic corpora annotated for event modality compared to 
English (Baker et al. 2010, 2012; Rubinstein et al. 2013), Japanese (Matsuyoshi et al. 2010), 
Portuguese (Hendrickx et al. 2012), and Chinese (Cui and Chi 2013). One obstacle for the 
creation of modality-annotated corpora is the lack of consensus definitions of modality and its 
attributes to be rendered into annotation tasks and guidelines. Furthermore, most modality 
annotation schemes use sophisticated theoretical guidelines that need annotators with linguistic 
background; hence, annotation typically takes place in in-lab settings at small scales.  
In this paper, we present an interactive annotation procedure to annotate event modality and 
its attributes of sense, polarity, intensification, tense, holders, and scopes in Modern Standard 
and Egyptian Arabic tweets. The procedure depicts the following ideas: first, it defines each 
annotation task as a series of questions displayed1/hidden based on prior answers; second, it 
avoids lengthy theoretically-sophisticated definitions and uses the questions instead as 
simplified self-explanatory annotation prompts; and third, based on the elicited answers it 
automatically determines nested triggers and their attributes. The fact that our procedure does 
not require special linguistic background and consists of easy-to-administer questions makes it 
eligible for large-scale crowdsourcing annotation.  
Our corpus comprises 9949 unique tweets, annotated for 12134 tokens that map to 315 unique 
types of event modality triggers and their attributes of sense, polarity, intensification, tense, 
holders, and scopes. The reason to work on the genre of tweets is that our corpus is part of a 
larger project to incorporate linguistic features, such as modality, with network-based features 
to automatically identify the key players of political discourse on Twitter for countries with 
fast-changing politics such as Egypt. The fact that our corpus is harvested from the Arabic 
Egyptian Twitter entails that the corpus is diglossic for Modern Standard Arabic (MSA), the 
                                                            
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and 
proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 
139
formal Arabic variety, and Egyptian Arabic (EA), the native Arabic dialect of Egypt. We 
evaluate the annotation results with Krippendorff's alpha (Krippendorff 2011). Results show 
high inter-annotator reliability rates, indicating that our annotation scheme and procedure are 
effective. The contribution of this paper, therefore, is twofold: first, we create a novel annotated 
resource for Arabic NLP that is larger than existing corpora even for languages other than 
Arabic; and second, we present an efficient and easy-to-administer annotation procedure with 
interactive crowdsourcing potentials.  
The rest of this paper is organized as follows: Section 2 outlines the annotation scheme, 
guidelines and the interactive procedure; Section 3 gives examples for the final output 
representations; Section 4 describes corpus harvesting and sampling; Section 5 provides the 
annotation results and disagreement analysis; and Section 6 compares and contrasts our work 
with related work. 
2  Annotation Scheme: Tasks and Guidelines 
Our annotation scheme comprises six tasks to label sense, polarity, intensification, tense, 
holders, and scopes for each event modality. Prior to the beginning of the interactive procedure, 
we highlight all event modalities in each tweet using a string-match algorithm and the lexicons 
from Al-Sabbagh et al. (2013, 2014a). The algorithm finds all potential event modality triggers 
(i.e. words/phrases that convey event modality) within each tweet in our corpus and marks them 
as annotation units. A total of 12134 candidate triggers are highlighted in 9949 tweets.  
2.1 Task 1: Sense 
Sense annotation is to decide for each candidate trigger in context whether it actually conveys 
event modality given the tweet's context. The same present participle ???? HAbb in example 1 is 
a volition trigger meaning I want/desire; whereas in example 2 it is a non-modal present 
participle meaning like/prefer/respect. 
1. ???? ???? ????[ ???? ?? ??? ????[ 1 
TbEA >nA m$ HAbb [Emrw mwsY yksb] 
Definitely, I do not want [Amr Moussa to win]. 
2. ??? ????  ????????? ???????? ?? : ???? ????  #egypt #qalyoum 
Emrw >dyb: rsmyA AlktAtny m$ HAbb >bw HAmd #egypt #qalyoum 
Amr Adeeb: Alkatatny does not officially like Abu Hamed #egypt #qalyoum 
We define sense annotation as a synonymy judgment task, following Al-Sabbagh et al. (2013, 
2014b). Each event modality sense is represented by an exemplar set manually selected so that: 
(1) each exemplar is an unambiguous event modality trigger; (2) exemplars are in both MSA 
and EA; (3) exemplars comprise both simple words and multiword expressions; (4) exemplars 
are both affirmative and negative; and (5) exemplars are of different intensities. Presented with 
a pre-highlighted candidate trigger in context and the exemplar sets, annotations are to decide 
whether the candidate trigger is synonymous with any of the exemplar sets. If not, the trigger is 
then assumed as non-modal.   
If an annotator decides that a given candidate trigger is a non-modal, no further questions 
about polarity, intensification, tense, holders, or scopes are displayed. In order to guarantee that 
annotators do not select the non-modal option as an easy escape, they are not allowed to move 
forward without giving at least one synonym of their own to the candidate trigger.   
2.2 Task 2: Polarity 
Task 2 uses as input the candidates labeled as valid event modality triggers in Task 1 and label 
each as either affirmative (AFF) or negative (NEG). To decide, annotators are instructed to 
consider the absence/presence of: 
? Negation particles such as ?? m$ (not), ?? lA (not), and ??? gyr (not), among others. 
? Negation affixes, especially in EA, like the circumfix m...$ in ????? mqdr$ (I cannot).  
                                                            
1 Throughout the examples, modality triggers are marked in boldface, and scopes are in-between brackets. 
140
? Negative polarity items like ???? Emry (never) and ?? ??? lm yEd (no longer). 
? Negative auxiliaries where negation is placed on the past tense auxiliary as in ????? ???? 
mknt$ EAyz (I did not want).  
? Inherently-negative triggers that encode negation in their lexical meanings such as ???? 
EAjz (incapable) and ???? ymnE (prohibit). 
? Embedding under negated epistemic modality triggers as in ?? ????? ??? ??? lA >Etqd >nh 
yjb (I do not think it is necessary) which entails that the speaker is not actually setting 
an obligation.  
Annotators are instructed that using multiple negation markers results in an affirmative sense. 
Thus, ?? ???? lm yEjz (he was not unable to) means that he was actually able to. Annotators are 
required to give the reason for negation if they decide that a given trigger is negative. 
2.3 Task 3: Intensification 
Event modality triggers have different lexical intensities (i.e. intensities encoded in the lexical 
meaning of the word/phrase regardless of the context). In obligation triggers, for instance, even 
without a context, Arabic speakers know that ????? Drwry (necessary) expresses a higher 
necessity than ??????? AlmfrwD (should). When used in context, the trigger's lexical intensity 
can be maintained as is, or amplified/mitigated by such linguistic means as: 
? Modification: adverbs like ????? tmAmA (absolutely) amplify lexical intensity; whereas 
mitigation is invoked by such adverbs as ????? gAlbA (most probably).  
? Categorical negation typically amplifies lexical intensity as in ?? ??????? ???? m$ 
AlmfrwD >bdA (it should never be).  
? Emphatic expressions such as ?? qd (indeed), ????? wAllh (I swear), and ?? ?? ???? mn kl 
qlby (wholeheartedly), among others, lead to lexical intensity amplification.  
? Coordination of two or more triggers typically results in intensity amplification as in 
???? ?????? lAzm wDrwry (must and necessary). 
? Embedding under epistemic modality triggers can affect the lexical intensities of event 
modality triggers. In ????? ?? ??????? ?? >Etqd mn AlDrwry >n (I think it is necessary 
to) the strong obligation associated with ??????? AlDrwry (necessary) is mitigated by 
the moderate-intensity epistemic ????? >Etqd (I think), being embedded under it.  
The annotators' task for intensification annotation is to decide for each candidate labeled as a 
valid event modality trigger in Task 1 whether its lexical intensity is amplified (AMP), mitigated 
(MTG) or maintained (AS IS). During interactive annotation, annotators are asked to provide the 
reason for their selection; that is, whether the lexical intensity is affected by modification, 
coordination, negation, embedding or any other reason whether listed above or not. 
2.4 Task 4: Tense 
In this version of our event modality corpus, we work on the present and past tenses only. Thus, 
Task 4 is to decide for each valid event modality trigger from Task 1 whether it is present (PRS) 
or past (PST). Annotators are required to give their reasons for selecting either PRS or PST. 
2.5 Task 5: Holders  
Holder annotation identifies the source of the obligation, permission, commitment, ability, or 
volition. In example 3, the source that sets the obligation that Egyptians have to learn the 
meaning of democracy is the Twitter user. 
3. ???????? ??????? ???? ??? ?????????? ?????[ ????[  
lAzm [AlmSryyn ytElmwA yEny <yh dymwqrATyp Al>wl] 
[Egyptians have to learn what democracy is first] 
The holder is not always the Twitter user, however. In example 4, the Twitter user quotes 
Kamal Alganzoury - a former Egyptian Prime Minster - stating that he does not want to 
141
continue as the Prime Minister. Therefore, the holder of the negated volition trigger ??? ??? ???? 
lys ldy rgbp (not have a will) is Alganzoury not the Twitter user. This is an example of the 
nested holder notion first proposed by Wiebe et al. (2005) and Saur? and Pustejovsky (2009).  
4. ?????????[ ?? ??? ???? ???: ??????? ???? ????????[  #SCAF #Tahrir #Egypt 
Aldktwr kmAl Aljnzwry: lys ldy rgbp fy [AlAstmrAr] #SCAF #Tahrir #Egypt 
Dr. Kamal Alganzoury: I do not wish to [continue] #SCAF #Tahrir #Egypt 
Another example of nested holders is example 5. We know that the regime is incapable of 
maintaining security and protecting the people only because the Twitter user says so. Put 
differently, the best way to understand this tweet is that according to what the Twitter user holds 
as a true proposition, the regime is unable to maintain security and protect the people.  
5.  ????? ????? ?? ????? ?????????[ ??? ?????????? ???[  
AlnZAm gyr qAdr ElY [twfyr Al>mn >w HmAyp AlmwATnyn] 
The regime is not able to [maintain security and protect the people] 
We can have two or more nested holders. In example 4, the two holders are Alganzoury who 
expresses his unwillingness to continue as a Prime Minster and the Twitter user who is quoting 
Alganzoury. In example 5, the two holders are the regime that is incapable of marinating 
security and protecting its people and the Twitter user who holds this proposition as true. In 
example 6, we have three nested holders: the Iranians who are unwilling to confront the outside 
world, Obama who holds that as a true proposition about Iranians, and the Twitter user who is 
quoting Obama stating his proposition.  
6. ??? ??????????????? ?? ???[ ???? ?? ?? ??? ????? ????????: ??????[  
AwbAmA: Al$Eb AlAyrAny lm yEd yrgb fy [AlmwAjhp mE AlEAlm AlxArjy] 
Obama: the Iranians no longer want to [confront the other countries].  
During the interactive procedure, annotators are first asked whether the holder is the same as 
the Twitter user. If not, more questions are displayed to determine (1) who the real holder is; (2) 
whether the tweet is a(n) (in)direct quote; or it conveys the Twitter user's assumptions. 
When the holder is not the Twitter user, annotators are asked to mark the boundaries of the 
linguistic unit that corresponds to the holder in the tweet's text. Annotators are instructed to use 
the maximal length principle from Szarvas et al. (2008) so that they mark the largest possible 
meaningful linguistic unit. Thus, in example 4 the holder is ??????? ???? ???????? Aldktwr kmAl 
Aljnzwry (Dr. Kamal Alganzoury) not only Kamal Alganzoury.  
2.6 Task 6: Scopes  
Scopes are the events modified by the trigger, syntactically realized as clauses, verb phrases, 
deverbal nouns or to-infinitives, according to Al-Sabbagh et al. (2013). We use the same 
maximal length principle from Task 5 so that the marked scope segment corresponds to the 
largest meaningful linguistic unit that describes the event. Typically, scope segments are 
delimited by: (1) punctuation markers and (2) subordinate conjunctions. 
Annotators are instructed that: (1) a single trigger may have one or more scopes; (2) two or 
more triggers - especially conjoined by coordinating particles - can share the same scope; and 
(3) scopes are not necessarily adjacent to their triggers. Examples 7, 8 and 9 illustrate each of 
these guidelines, repecetively.  
7.  ?????? ????? ???????[?] ?????[ ???????? ?????? ????[  
lw AstbEd $fyq ystTyE [AlTEn] w[AlEwdp lsbAq Alr}Asp] 
If Shafiq is excluded, he can [appeal] and [run again for presidency].  
8.  [?????? ???????? ???? ??? ??? ???? ????? ?????? ???? [???? ??? ?? ??????? 
mlAyyn AlmSryyn Ally brh mSr lAzm wHtmA wDrwry wyjb [ybqY lhm Hq AltSwyt] 
It is necessary, it is a must, it is a need that [Egyptians abroad are given the right to vote]. 
9. [???? ????? ??? ??? ?? ???? [???? #??? ???? ????? ??? ??????? 
nfsy wAllh bjd qbl mA Amwt [A$wf #mSr AHsn wAHlY bld fAldnyA] 
I really wish before I die to [see #Egypt becoming one of the best counties in the world]. 
3 Final Output Representation   
All elicited answers during annotation are organized into the representations illustrated in the 
following examples. The representation of example 10 reads as: the Twitter USER strongly did 
142
not want Shafiq to win the presidential elections. The trigger ?????? Atmnyt (wished) is tagged as 
synonymous with the volition exemplar set; therefore, it denotes a DESIRE. It is then labeled as a 
past tense (PST), negative (NEG) trigger. Furthermore, its lexical intensity is labeled as amplified 
(AMP) because of the categorical negation ???? ?? Emry mA (never ever). Originally, ?????? 
Atmnyt (wished) is of moderate lexical intensity, being less intense than ?????? A$thyt (longed 
for) but more intense than ???? >rdt (wanted). Given the categorical negation, the lexical 
intensity of ?????? Atmnyt (wished) goes up the scale from moderate to strong (STRG). 
10. ????#????? ???? ???? ??????? ?? ???? . ]???? ????[??  ????????? ??  
Emry mAtmnyt An [$fyq yksb]. AlHmd Allh rbnA mHrmny$ mn HAjp #mrsy 
I have never ever wished [Shafiq to win]. Thank God! #Morsi. 
rep. USER, STRG PST NEG DESIRE ($fyq yksb)  
Example 11 reads as: the Twitter USER reports Hegazy stating that he has the ability to 
become the Muslim's caliphate. The trigger ???? >SlH (can) is labeled as synonymous with the 
ability exemplar set. It is also labeled as a present (PRS), affirmative (AFF) trigger whose lexical 
intensity is maintained (AS IS) in the context. Therefore, its lexical intensity is maintained to its 
original level which is moderate (MOD). 
11.  ?????? ???? ?????? ]??? ?????? ?????????[??  ??????? : ?????  #Ikhwan 
HjAzy: >nA >SlH >n [>kwn xlyfp llmslmyn] wsnkwn sAdp AlEAlm #Ikhwan 
Hegazy: I can [be the Muslims' caliphate] and we will become the world's masters. #Ikhwan 
rep. USER, report, (HjAzy, MOD PRS AFF ABLE, (>kwn xlyfp llmslmyn)) 
Example 12 shows a Twitter user who holds as true that the only thing Egypt needed was a 
wise politician to avoid the bloodshed. The trigger ????? tHtAj (needs) is labeled as an obligation 
trigger synonymous with ????? ttTlb (requires). It is also labeled as past tense (PST) given the 
preceding past tense auxiliary ??? tkn (was). The assigned strong (STRG) lexical intensity label is 
attributed to the fact that the original moderate intensity of ????? tHtAj (needs) is amplified by 
the categorical negation structure  ???... ??  lm ... <lA (nothing but). 
12. # ????????? ???? ???? ?? ??????? ???? ????? [???  ???????? ?? ???[  
#mSr lm tkn tHtAj AlA [rjl EAql yxrj mn AlAzmAt bdwn ArAqp AldmA'] 
#Egypt needed nothing but [a rational politician who solves crises without bloodshed] 
rep. USER, true, (mSr , STRG PST AFF REQUIRE (rjl EAql yxrj mn AlAzmAt bdwn ArAqp AldmA')) 
Example 13 illustrates the representation of three-level nested holders. It reads as: the USER 
reports Obama's assumption as the latter holds as true that the Iranians do not want to confront 
other countries.  
13. ???????? ?? ?????? ???????[ ???? ???? ??? ???????? ????? : ??????[  
AwbAmA: Al$Eb AlAyrAny lm yEd yrgb fy [AlmwAjhp mE AlEAlm AlxArjy] 
Obama: the Iranians no longer want to [confront other countries].  
rep. USER, report, (AwbAmA, true, (Al$Eb AlAyrAny, MOD PRS NEG DESIRE, (AlmwAjhp mE 
AlEAlm AlxArjy))) 
Example 14 shows how two conjoined triggers (i.e. ???? lAzm (must) and ????? Drwry 
(necessary)) that share the same holder and scope are merged into one representation, and the 
conjunction leads to amplifying the intensity of the obligation set by them both. 
14.  ???? ???? ???? ??? ???????? ?????? ???? ??????[ ??????????[  
lAzm wDrwry [klnA nkwn qdAm mqr AlmHAkmp wmEAnA Swrp Alr}ys]  
We must and it is necessary that [we go to the court with President's pictures]. 
rep. USER, STRG PRS AFF REQUIRE, (klnA nkwn qdAm mqr AlmHAkmp wmEAnA Swrp Alr}ys)  
4  Corpus Harvesting  
Tweets are harvested from the Arabic Egyptian Twitter provided that (1) each tweet has at least 
one trendy political English or Arabic hashtag; and (2) each tweet has at least one candidate 
event modality trigger from the Arabic modality lexicons (Al-Sabbagh et al. 2013, 2014a). We 
harvest tweets from a variety of users such as newspapers, TV stations, political and 
humanitarian campaigns, politicians, celebrities, and ordinary people. Thus, our corpus 
comprises both MSA, the formal Arabic variety, and EA, the native Arabic dialect of Egypt.  
The harvested corpus comprises 9949 unique tweets, with 12134 tokens of event modality 
triggers that map to 315 unique types. 
143
5 Annotation Results  
5.1 Evaluation Methodology and Metrics  
Our annotation tasks are of two types: (1) Tasks 1-4 are label-based where there is a pre-defined 
set of labels from which annotators choose; and (2) Tasks 5-6 are segmentation-based where the 
output of the annotation is a text segment. For the segmentation-based tasks, we use an all-or-
nothing method to measure inter-annotator reliability: for segments to be considered as 
agreement, they must share both the beginning and end boundaries. We use Krippendorff's 
alpha ? (Krippendorff 2011) as our inter-annotator reliability measure, following the most 
recent work on modality annotation for other languages including English (Rubinstein et al. 
2013) and Chinese (Cui and Chi 2013). For more details on Krippendorff's alpha and a, we refer 
the reader to Artstein and Poesio (2008).   
5.2 Results 
We use the surveygizmo survey services2 to implement our interactive annotation procedure 
given that their survey structure is one that uses conditional branching and skip logic. We 
distribute the survey on Twitter and we have three annotators participating. According to the 
short qualifying quiz given at the beginning of the survey, all three participants are native 
Egyptian Arabic (EA) speakers who have at least two-year experience with Twitter. They are 
also university graduates who, therefore, master MSA. None of the participants has a linguistics 
background. Table 1 shows alpha rates for each annotation task. 
 Sense Polarity Intensification Tense Holder Scope 
Obligation 0.890 0.893 0.892 0.978 0.829 0.744 
Permission 0.864 0.905 0.821 0.983 0.800 0.739 
Commitment 0.760 0.794 0.783 0.947 0.702 0.654 
Ability 0.895 0.914 0.905 0.950 0.828 0.763 
Volition 0.921 0.921 0.867 0.982 0.858 0.779 
Averages 0.866 0.885 0.854 0.968 0.803 0.736 
Table 1: Krippendorff's alpha rates for inter-annotator reliability 
5.3 Discussion and Disagreement Analysis   
Among the factors that lead to high inter-annotator reliability are that: (1) the vast majority of 
negation is explicitly marked by negation particles that are easy to detect by human annotators; 
(2) the vast majority of triggers are used without any amplification or mitigation markers; and 
(3) punctuation markers are surprisingly informative for marking scope boundaries and direct 
quotations; and hence, holders. 
Sense-related disagreement is attributed to: (1) nominal triggers, (2) highly-polysemous 
triggers, and (3) different interpretations invoked by the ?RATIONAL (i.e. non-human) holders. 
Typically, event modality triggers are adjunct constituents that add an extra-layer of meaning 
and can be removed without disturbing the syntactic structure. Yet, in example 15, ???? wAjb (a 
must) and ???? >wjb (a more important must) have main grammatical functions as the 
predicates of the phrases they modify. Most of the exemplars from Section 2.1 are adjuncts; 
and, thus, none can substitute ???? wAjb (a must) or ???? >wjb (a more important must) in such 
a context.   
15. ]???? ]?????? ??? ?????[???  ???? ]?????? ?? ?????? ??????  
[AltHfZ mn AxtTAf Alvwrp] wAjb lkn [AltwHd xlf m$rwE] >wjb 
[Being cautious about manipulating the revolution] is a must but [getting united for one project] 
is a more important must.   
Highly-polysemous triggers invoke disagreement because in many cases even the context is 
ambiguous. In example 16, ???? >qsm (I swear) has two eligible interpretations: an epistemic 
trigger interpretation I assure (you) that and a commitment trigger interpretation I promise (you) 
                                                            
2 http://www.surveygizmo.com/ 
144
that. Even the context is not enough to disambiguate the two interpretations and annotators go 
by the most common sense for the trigger according to their own opinions.  
16. ????: ???? ????  T??? ????? ????? ??? ????? 90? ???? ??? ]???#?? ???? [??  
Emrw >dyb: >qsm bAllh [ln tsqT #mSr], AHnA $Eb 90 mlywn wm$ <$Arp htsqT bld 
Amr Adeeb: I promise/assure (you) by God that [#Egypt will not collapse]. We are 90 million 
Egyptians and we will not be defeated by a sign.   
Non-human or ?RATIONAL holders invoke disagreement, especially for obligation versus 
volition triggers.  The most common sense of such triggers as ????? EAyzp (want) is volition. 
Yet, when the holder is ?RATIONAL like ?????????? AlAntxAbAt (the elections) in example 17, 
annotators disagree as to whether ????? EAyzp means want (i.e. a volition trigger) or need (i.e. an 
obligation trigger). 
17.  ????? ??????? ???? ???????? ]??????[ ???????????????  
AlAntxAbAt EAyzp [mr$Hyn] wHmlAt Al>HzAb tyjy brAHthA 
Elections want/need [candidates] and later we can establish the political parties.  
Intensity-related disagreement is attributed mostly to progressive verb aspect. Some 
annotators consider progressive verb aspect as indicated by the EA prefix b as a marker for 
lexical intensity amplification. Thus they tag the volition trigger ????? btmnY (I wish) in example 
18 as amplified, especially it is modified by ?? ??? kl ywm (everyday).   
18.  ????#???? ??? [ ??????? ???[  
kl ywm btmnY [sqwT Hkm #mrsy]  
Every day, I wish for [#Morsi's regime to fall].  
Polarity-related disagreement is mainly caused by (1) negated holders and (2) contextual 
negation. In ???? ?? ???? mfy$ Hd yqdr (no one can), annotators disagree as to whether ???? yqdr 
(can) should be labeled as affirmative or negative. By contextual negation we mean examples 
like ?? ????? ?? ????? ?? mn AlSEb >n ntmnY >n (it is hard to wish to), which entails negation 
due to the adjective ????? AlSEb (hard).  
Holder-related disagreement is attributed mainly to generic nouns and impersonal pronouns 
like ????? Al$Eb (the people) and ?????? AlwAHd (one), respectively. They are interpreted by 
some annotators as referring implicitly to the Twitter USER. Therefore, the annotators select the 
USER as the only holder with zero nesting. Other annotators interpret them as referring to people 
in general not necessarily the Twitter USER and thus they consider these as instances of nested 
holders.  
Scope-related disagreement is attributed to (1) ambiguous subordinate conjunctions, (2) 
triggers' modifiers, and (3) absent punctuation markers.  
Tense yields almost perfect inter-annotator reliability rates. Annotation disagreement does not 
show any particular pattern. Therefore, we attribute minor disagreement to random errors, 
resulting from fatigue.  
5.4 Majority Statistics  
Based on majority annotations, Table 2 gives the statistics for our corpus in terms of sense, 
polarity, intensification, and tense. As for holder annotations, approximately 60.5% of the 
triggers have zero-nested holders (i.e. the tweet's writer is the same as the holder).  
 Sense Polarity Intensification Tense 
 MD NMD AFF NEG AMP MTG ASIS PRS PST 
Ability 1729 920 1047 682 348 308 1073 1175 554 
Commitment 1048 495 599 449 221 220 607 639 409 
Obligation 1786 848 1059 727 369 399 1018 1018 768 
Permission 1699 980 1054 645 286 428 985 1053 646 
Volition 1622 1007 974 648 341 292 989 1038 584 
Totals 7884 4250 4733 3151 1565 1647 4672 4923 2961 
Table 2: Token statistics for each annotation task per event modality sense where MD is modal, NMD is 
non-modal, AFF is affirmative, NEG is negative, AMP is amplified, MTG is mitigated, ASIS is as is, PRS is 
present, and PST is past 
 
145
6 Related Work 
Event modality is the focus of many annotation projects. Matsuyoshi et al. (2010) annotate a 
corpus of English and Japanese blog posts for a number of modality senses including volition, 
wishes, and permission. They annotate sense, tense, polarity, holders as well as other attributes 
that we have not covered in our scheme such as grammatical mood. They report macro kappa 
inter-annotator agreement rates of 0.69, 0.70, 0.66 and 0.72 for holders, tense, sense, and 
polarity, respectively.  
Baker et al. (2010, 2012) simultaneously annotate modality and modality-based negation for 
Urdu-English machine translation systems. Among the modality senses they work on are 
requirement, permission, success, intention, ability, and desires. They report macro kappa inter-
annotator agreement rates of 0.82 for sense annotation and 0.76 for scopes. They, however, do 
not annotate holders and do not consider nested modalities.  
Hendrickx et al. (2012) annotate eleven modality senses in Portuguese, including necessity, 
capacity, permission, obligation, and volition, among others. They report a macro kappa inter-
annotator rate of 0.85 for sense annotation. 
Rubinstein et al. (2013) propose a linguistically-motivated annotation scheme for modalities 
in the MPQA English corpus. They annotate sense, polarity, holders, and scopes, among other 
annotation units. They work on obligation, ability, and volition among other modality senses. 
They attain macro alpha inter-annotator reliability rates of 0.89 and 0.65 for sense and scope, 
respectively.  
Cui and Chi (2013) apply the same scheme of Rubinstein et al. (2013) to the Chinese Penn 
Treebank and get alpha inter-annotator reliability rates of 0.81 and 0.39 for sense and scope 
annotation, respectively.  
Finally, Al-Sabbagh et al. (2013) annotate event modality in MSA and EA tweets. We attain 
kappa inter-annotator agreement rates of 0.90 and 0.93 for sense and scope annotation, 
respectively, for only 772 tokens of event modality triggers.  
Our annotation results, therefore, are comparable to the results in the literature. Furthermore, 
our annotation scheme and its tasks are orthogonal to most of the aforementioned schemes. 
However, the key differences between our work and related work are:  
? We use a standardized taxonomy of event modality - Palmer's (2001) - that has been 
proved valid for a variety of languages, including Arabic, according to Mitchell and 
Al-Hassan (1994), Brustad (2000), and Moshref (2012). 
? We annotate nested holders unlike some of the aforementioned studies  (e.g. Baker et 
al. 2010, 2012) and use a wider range of negation and intensification markers.  
? We use crowdsourcing with simplified guidelines implemented interactively to 
annotate a larger-scale corpus of 12134 tokens for event modality and its attributes.  
7 Conclusion and Outlook 
We presented a large-scale corpus annotated for event modality in MSA and EA tweets. We use 
a simplified annotation procedure that defines each annotation task as a series of questions, 
implemented interactively. Our scheme covers a wide range of the most common annotation 
units mentioned in the literature, including modality sense, polarity, intensification, tense, 
holders, and scopes. We deal with nested holders - which are crucial in a highly interactive 
genre such as tweets where users frequently quote others and make assumptions about them. 
We also automatically merge triggers with shared holders and scopes based on elicited 
annotators' answers. The annotation procedure yields reliable results and creates a novel 
resources for Arabic NLP. The current version of our corpus does not, however, cover a number 
of issues including: the future tense, grammatical moods other than the declarative, and 
modality entailment. By modality entailment, we mean, for example, when a tweet's user 
criticizes the obligation of another quoted person, this entails that the user does not consider 
such an event as required. For a future version of the corpus, we plan to cover such points. 
Furthermore, we will use the corpus to train and test a machine learning system for the 
automatic processing of Arabic event modality.  
146
References 
Rania Al-Sabbagh, Jana Diesner and Roxana Girju. 2013. Using the Semantic-Syntactic Interface for 
Reliable Arabic Modality Annotation. In Proceedings of IJCNLP'13, pages 410-418, October 14-18, 
2013, Nagoya, Japan.  
Rania Al-Sabbagh, Roxana Girju and Jana Diesner. 2014a. Unsupervised Construction of a Lexicon and a 
Pattern Repository of Arabic Modal Multiword Expressions. In Proceedings of the 10th Workshop of 
Multiword Expressions at EACL'14, April 26-27, 2014, Gothenburg, Sweden. 
Rania Al-Sabbagh, Roxana Girju and Jana Diesner. 2014b. 3arif: A Corpus of Modern Standard and 
Egyptian Arabic Tweets Annotated for Epistemic Modality Using Interactive Crowdsourcing. In 
Proceedings of the 25th International Conference on Computational Linguistics, August 23-29, 2014, 
Dublin, Ireland. 
Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. 
Computational Linguistics, volume 34, issue 4, pages 555-596. 
Kathrin Baker, Michael Bloodgood, Mona Diab, Bonnie Dorr, Nathaniel W. Filardo, Lori Levin and 
Christine Piatko. 2010. A Modality Lexicon and its Use in Automatic Tagging. In Proceedings of the 
7th International Conference on Language Resources and Evaluation (LREC'10), pages 1402-1405, 
May 19-21, 2010, Valetta, Malta.   
Kathryn Baker, Michael Bloodgood, Bonnie J. Dorr, Chris Callison-Burch, Nathaniel W. Filardo, 
Christine Piatko, Lori Levin and Scott Miller. 2012. Modality and Negation in SIMT. Computational 
Linguistics. volume 38, issue 2, pages 411-438. 
Samuel R. Bowman and Harshit Chopra. 2012. Automatic Animacy Classification. In Proceedings of the 
NAACL HTL 2012 Student Research Workshop, pages 7-10, June 3-8, 2012, Montreal, Canada.  
Kristen E. Brustad. 2000. The Syntax of Spoken Arabic: A Comparative Study of Moroccan, Egyptian, 
Syrian and Kuwaiti Dialects. Georgetown University Press, Washington DC, USA. 
Cohan Sujay Carlos and Madulika Yalamanchi. 2012. Intention Analysis for Sales, Marketing and 
Customer Service. In Processing of COLING 2012: Demonstration Papers, pages 33-40, December 
2012, Mumbai, India. 
Baptiste Chardon, Farah Benamara, Yannick Mathieu, Vladimir Popescu and Nicholas Asher. 2013. 
Sentiment Composition Using a Parabolic Model. In Proceedings of the 10th International Conference 
on Computational Semantics (IWCS 2013), pages 47-58, March 20-22, 2013, Potsdam, Germany.  
Yanyan Cui and Ting Chi. 2013. Annotating Modal Expressions in the Chinese Treebank. In Proceedings 
of the IWC 2013Workshop on Annotation of Modal Meaning in Natural Language (WAMM), pages 24-
32, March 2013, Potsdam, Germany. 
Iris Hendrickx, Am?lia Mendes and Silvia Mencarelti. 2012. Modality in Text: A Proposal for Corpus 
Annotation. In Proceedings of the 8th International Conference on Language Resources and Evaluation 
(LREC'12), pages 1805-1812, May 21-27, 2012, Istanbul, Turkey. 
Klaus Krippendorff. 2011. Computing Krippendorff's Alpha-Reliability. Annenberg School of 
Communication, Departmental Papers: University of Pennsylvania. 
Andrew Lampert, Robert Dale and Cecile Paris. 2010. Detecting Emails Containing Requests for Action. 
In Proceedings of Human Language Technologies: the 2010 Annual Conference of the North American 
Chapter of the ACL, pages 984-992, June 2010, Los Angles, California. 
Ying-Shu Liao and Ting-Gen Liao. 2009. Modal Verbs for the Advice Move in Advice Columns. In 
Proceedings of the 23rd Pacific Asia Conference on language, Information and Computation, pages 
307-316, December 3-5, 2009, Hong Kong, China. 
Suguru Matsuyoshi, Megumi Eguchi, Chitose Sao, Koji Murakami, Kentaro Inui and Yuji Matsumoto. 
2010. Annotating Event Mentions in Text with Modality, Focus and Source Information. In 
Proceedings of LREC'10, pages 1456-1463, May 19-21, 2010, Valletta, Malta.  
T. F. Mitchell and S. A. Al-Hassan. 1994. Modality, Mood and Aspect in Spoken Arabic with Special 
Reference to Egypt and the Levant. London and NY: Kegan Paul International. 
147
Ola Moshref. 2012. Corpus Study of Tense, Aspect, and Modality in Diglossic Speech in Cairene Arabic. 
PhD Thesis. University of Illinois at Urbana-Champaign. 
Frank R. Palmer. 2001. Mood and Modality. 2nd Edition. Cambridge University Press, Cambridge, UK. 
J. Ramanand, Krishna Bhavsar and Niranjan Pedanekar. 2010. Wishful Thinking: Finding Suggestions 
and "Buy" Wishes for Product Reviews. In Proceedings of the NAACL HLT 2010 Workshop on 
Computational Approaches to the Analysis and Generation of Emotion in Text, pages 54-61, June 
2010, Los Angeles, California. 
Aynat Rubinstein, Hillary Harner, Elizabeth Krawczyk, Daniel Simoson, Graham Katz and Paul Portner. 
2013. Toward Fine-Grained Annotation of Modality in Text. In Proceedings of the IWC 2013Workshop 
on Annotation of Modal Meaning in Natural Language (WAMM), pages 38-46, March 2013, Potsdam, 
Germany. 
Roser Saur? and James Pustejovsky. 2009. FactBank: A Corpus Annotated with Event Factuality. 
Language Resources and Evaluation, 43:227-268 
Gy?rgy Szarvas, Veronika Vincze, Rich?rd Farkas and J?nos Csirik. 2008. The BioScope Corpus: 
Annotation for Negation, Uncertainty and their Scope in Biomedical Texts. In Proceedings of BioNLP 
2008: Current Trends in Biomedical Natural Language Processing, pages 38-45, June 2008, 
Columbus, Ohio, USA. 
Janyce Wiebe, Theresa Wilson and Claire Cardie. 2005. Annotating Expressions of Opinions and 
Emotions in Language. Language Resources and Evaluation, volume 39, issue 203, pages 1663-210. 
148
