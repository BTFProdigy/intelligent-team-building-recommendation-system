Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 232?240,
Beijing, August 2010
Local lexical adaptation in Machine Translation through triangulation:
SMT helping SMT
Josep Maria Crego
LIMSI-CNRS
jmcrego@limsi.fr
Aur?lien Max
LIMSI-CNRS
Univ. Paris Sud
amax@limsi.fr
Fran?ois Yvon
LIMSI-CNRS
Univ. Paris Sud
yvon@limsi.fr
Abstract
We present a framework where auxiliary
MT systems are used to provide lexical
predictions to a main SMT system. In
this work, predictions are obtained by
means of pivoting via auxiliary languages,
and introduced into the main SMT sys-
tem in the form of a low order language
model, which is estimated on a sentence-
by-sentence basis. The linear combination
of models implemented by the decoder
is thus extended with this additional lan-
guage model. Experiments are carried out
over three different translation tasks using
the European Parliament corpus. For each
task, nine additional languages are used
as auxiliary languages to obtain the trian-
gulated predictions. Translation accuracy
results show that improvements in trans-
lation quality are obtained, even for large
data conditions.
1 Introduction
Important improvements are yet to come regard-
ing the performance of Statistical Machine Trans-
lation systems. Dependence on training data and
limited modelling expressiveness are the focus of
many research efforts, such as using monolingual
corpora for the former and syntactic models for
the latter.
Another promising approach consists in ex-
ploiting complementary sources of information
in order to build better translations, as done by
consensus-based system combination (e.g. (Ma-
tusov et al, 2008)). This, however, requires to
have several systems available for the same lan-
guage pair. Considering that the same training
data would be available to all systems, differences
in translation modelling are expected to produce
redundant and complementary hypotheses. Mul-
tisource translation (e.g. (Och and Ney, 2001;
Schwartz, 2008)) is a variant, involving source
texts available in several languages which can be
translated by systems for different language pairs
and whose outputs can be successfully combined
into better translations (Schroeder et al, 2009).
One theoretical expectation of multisource trans-
lation is that it can successfully reduce ambiguity
of the original source text, but does so under the
rare conditions of availability of existing (accu-
rate) translations. In contrast, pivot-based system
combination (e.g. (Utiyama and Isahara, 2007;
Wu and Wang, 2007)) aims at compensating the
lack of training data for a given language pair by
producing translation hypotheses obtained by piv-
oting via an intermediary language for which bet-
ter systems are available.
These techniques generally produce a search
space that differs from that of the direct transla-
tion systems. As such, they create a new transla-
tion system out of various systems for which di-
agnosis becomes more difficult.
This paper instead focusses on improving a sin-
gle system, which should be state-of-the-art as
regards data and models. We propose a frame-
work in which information coming from external
sources is used to boost lexical choices and guide
the decoder into making more informed choices.1
1We performed initial experiments where the comple-
mentary information was exploited during n-best list rerank-
ing (Max et al, 2010), but except for the multisource condi-
tion the list of hypotheses contained too little useful variation
232
Complementary sources can be of different na-
ture: they can involve other automatic systems
(for the same or different language pairs) and/or
human knowledge. Furthermore, complementary
information is injected at the lexical level, thus
making targeted fine-grained lexical predictions
useful. Importantly, those predictions are ex-
ploited at the sentence level2, so as to allow for
efficient use of source contextual information.
The second contribution of this paper is an in-
stantiation of the proposed framework. Auto-
matically pivoting via auxiliary languages is used
to make complementary predictions that are ex-
ploited through language model adaptation by the
decoder for a given language pair. For this appar-
ently difficult condition, where predictions result
from automatic translations involving two sys-
tems, we manage to report significant improve-
ments, measured with respect to the target and the
source text, under various configurations.
This paper is organized as follows. We first re-
view related work in section 2.1, and describe the
distinctive characteristics of our approach in Sec-
tion 2.2. Section 2.3 presents our instantiation of
the framework based on lexical boosting via aux-
iliary language triangulation. Experiments involv-
ing three language pairs of various complexity and
different amounts of training data are described in
Section 3. We finally conclude by discussing the
prospects offered by our proposed framework in
Section 4.
2 A framework for sentence-level lexical
boosting
2.1 Related work
The idea of using more than one translation sys-
tem to improve translation performance is not new
and has been implemented in many different ways
which we briefly review here.
System combination An often used strategy
consists in combining the output of several sys-
tems for a fixed language pair, and to rescore the
resulting set of hypotheses taking into account
all the available translations and scores. Various
to lead to measurable improvements.
2We plan to experiment next on using predictions at the
document level.
proposals have been made to efficiently perform
such a combination, using auxiliary data struc-
tures such as n-best lists, word lattices or con-
sensus networks (see for instance (Kumar and
Byrne, 2004; Rosti et al, 2007; Matusov et al,
2008; Hildebrand and Vogel, 2008; Tromble et al,
2008)). Theses techniques have proven extremely
effective and have allowed to deliver very signifi-
cant gains in several recent evaluation campaigns
(Callison-Burch et al, 2008).
Multisource translation A related, yet more re-
sourceful approach, consists in trying to combine
several systems providing translations from differ-
ent sources into the same target, provided such
multilingual sources are available. (Och and Ney,
2001) propose to select the most promising trans-
lation amongst the hypotheses produced by sev-
eral Foreign?English systems, where output se-
lection is based on the translation scores. The
intuition that if a system assigns a high figure
of merits to the translation of a particular sen-
tence, then this translation should be preferred,
is implemented in the MAX combination heuris-
tics, whose relative (lack of) success is discussed
in (Schwartz, 2008). A similar idea is explored in
(Nomoto, 2004), where the sole target language
model score is used to rank competing outputs.
(Schroeder et al, 2009) propose to combine the
available sources prior to translation, under the
form of a multilingual lattice, which is decoded
with a multisource phrase table. (Chen et al,
2008) integrate the available auxiliary information
in a different manner, and discuss how to improve
the translation model of the primary system: the
idea is to use the entries in the phrase table of
the auxiliary system to filter out those acciden-
tal correspondences that pollute the main transla-
tion model. The most effective implementation of
multisource translation to date however consists
in using mono-source system combination tech-
niques (Schroeder et al, 2009).
Translation through pivoting The use of aux-
iliary systems has also been proposed in another
common situation, as a possible remedy to the
lack of parallel data for a particular language pair,
or for a particular domain. Assume, for instance,
that one wishes to build a translation system for
233
the pair A ? B, for which the parallel data
is sparse; assuming further that such parallel re-
sources exist for pairs A ? C and for C ? B,
it is then tempting to perform the translation in-
directly through pivoting, by first translating from
A to C, then from C to B. Direct implementa-
tions of this idea are discussed e.g. in (Utiyama
and Isahara, 2007). Pivoting can also intervene
earlier in the process, for instance as a means
to automatically generate the missing parallel re-
source, an idea that has also been considered to
adapt an existing translation systems to new do-
mains (Bertoldi and Federico, 2009). Pivoting can
finally be used to fix or improve the translation
model: (Cohn and Lapata, 2007) augments the
phrase table for a baseline bilingual system with
supplementary phrases obtained by pivoting into
a third language.
Triangulation in translation Triangulation
techniques are somewhat more general and only
require the availabily of one auxiliary system (or
one auxiliary parallel corpus). For instance, the
authors of (Chen et al, 2008) propose to use the
translation model of an auxiliary C ? B system
to filter-out the phrase-table of a primary A ? B
system.
2.2 Our framework
As in other works, we propose to make use of sev-
eral MT systems (of any type) to improve trans-
lation performance, but contrarily to these works
we concentrate on improving one particular sys-
tem. Our framework is illustrated on Figure 1.
The main system (henceforth, direct system), cor-
responding to configuration 1, is a SMT system,
translating from German to English in the exam-
ple. Auxiliary information may originate from
various sources (2-6) and enter into the decoder.
A new model is dynamically built and is used to
guide the exploration of the search space to the
best hypothesis. Several auxiliary models can be
used at once and can be weighted by standard op-
timization techniques using development data, so
that bad sources are not used in practice, or by
exploiting a priori information. In the implemen-
tation described in section 2.3, this information is
updated by the auxiliary source at each sentence.
Figure 1: Lexical boosting framework with vari-
ous configurations for auxiliary predictions
We now briefly describe various possible con-
figurations to make some links to previous works
explicit. Configuration 2 translates the same
source text by means of another system for the
same language pair, as would be done in system
combination, except that here a new complete de-
coding is performed by the direct system. Con-
figuration 3, which will be detailed in section 2.3,
uses translations obtained by triangulating via an
auxiliary language (Spanish in the example). Us-
ing this two-step translation is common to pivot
approaches, but our approach is different in that
the result of the triangulation is only used as aux-
iliary information for the decoding of the direct
system. Configurations 4 and 5 are instances of
multisource translation, where a paraphrase or a
translation of the source text is available. Lastly,
configuration 6 illustrates the case where a human
translator, with knowledge of the target language
and at least of one of the available source lan-
guages, could influence the decoding by provid-
ing desired3 words (e.g. only for source words or
phrases that would be judged difficult to translate).
This human supervision through a feedback text in
real time is similar to the proposal of (Dymetman
et al, 2003).
Given this framework, several questions arise,
3The proposal as it is limits the hypotheses produced by
the system to those that are attainable given its training data.
It is conceivable, however, to find ways of introducing new
knowledge in this framework.
234
the most important underlying this work being
whether the performance of SMT systems can be
improved by using other SMT systems. Another
point of interest is whether improvements made
to auxiliary systems can yield improvement to the
direct system, without the latter undergoing any
modification.
2.3 Lexical boosting via triangulation
Auxiliary translations obtained by pivoting can be
viewed as a source of adaptation data for the target
language model of the direct system. Assuming
we have computed n-best translation hypotheses
of a sentence in the target language, we can then
boost the likeliness of the words and phrases oc-
curring in these hypotheses by deriving an auxil-
iary language model for each test sentence. This
allows us to integrate this auxiliary information
during the search and thus provides a tighter in-
tegration with the direct system. This idea has
successfully been used in speech recognition, us-
ing for instance close captions (Placeway and Laf-
ferty, 1996) or an imperfect translation (Paulik et
al., 2005) to provide auxiliary in-domain adap-
tation data for the recognizer?s language model.
(Simard and Isabelle, 2009) proposed a similar ap-
proach in Machine Translation in which they use
the target-side of an exact match in a translation
memory to build language models on a per sen-
tence basis used in their decoder.
This strategy can be implemented in a straight-
forward manner, by simply training a language
model using the n-best list as an adaptation cor-
pus. Being automatically generated, hypotheses
in the n-best list are not entirely reliable: in par-
ticular, they may contain very unlikely target se-
quences at the junction of two segments. It is how-
ever straightforward to filter these out using the
available phrase alignment information.
This configuration is illustrated on Figure 2: the
direct system (configuration 1) makes use of pre-
dictions from pivoting through an auxiliary lan-
guage (configuration 2), where n-best lists can be
used to produce several hypotheses. In order to
get a upper bound on the potential gains of this ap-
proach, we can run the artificial experiment (con-
figuration 3) where a reference in the target lan-
guage is used as a ?perfect? source of information.
Furthermore, we are interested in the performance
of the simple pivot system alone (configuration 4),
as it gives an indication of the quality of the data
used for LM adaptation.
Figure 2: Architecture of a German?English sys-
tem for lexical boosting via triangulation through
Spanish
3 Experiments and results
3.1 Translation engine
In this study, we used our own machine trans-
lation engine, which implements the n-gram-
based approach to statistical machine translation
(Mari?o et al, 2006). The translation model
is implemented as a stochastic finite-state trans-
ducer trained using a n-gram language model of
(source,target) pairs.
In addition to a bilingual n-gram model, our
SMT system uses six additional models which
are linearly combined following a discriminative
modeling framework: two lexicalized reorder-
ing (Tillmann, 2004) models,a target-language
model, two lexicon models, a ?weak? distance-
based distortion model, a word bonus model and
a translation unit bonus model. Coefficients in
this linear combination are tuned over develop-
ment data with the MERT optimization toolkit4,
slightly modified to use our decoder?s n-best lists.
For this study, we used 3-gram bilingual and
3-gram target language models built using modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1996); model estimation was performed with the
SRI language modeling toolkit.5 Target language
4http://www.statmt.org/moses
5http://wwww.speech.sri.com/projects/
srilm
235
models were trained on the target side of the bi-
text corpora.
After preprocessing the corpora with standard
tokenization tools, word-to-word alignments are
performed in both directions, source-to-target and
target-to-source. In our system implementation,
the GIZA++ toolkit6 is used to compute the word
alignments. Then, the grow-diag-final-and heuris-
tic is used to obtain the final alignments from
which translation units are extracted. Convergent
studies have showed that systems built accord-
ing to these principles typically achieve a per-
formance comparable to that of the widely used
MOSES phrase-based system for the language
pairs under study.
3.2 Corpora
We have used the Europarl corpus7 for our main
and auxiliary languages. The eleven languages
are: Danish (da), German (de), English (en),
Spanish (es), Finnish (fi), French (fr), Greek
(el), Italian (it), Dutch (nl), Portuguese (pt) and
Swedish (sv).
We focussed on three translation tasks: one
for which translation accuracy, as measured by
automatic metrics, is rather high (fr ? en),
and two for which translation accuracy is lower
(de ? en) and (fr ? de). This will allow us
to check whether the improvements provided by
our method carry over even in situations where the
baseline is strong; conversely, it will allow us to
assess whether the proposed techniques are appli-
cable when the baseline is average or poor.
In order to measure the contribution of each of
the auxiliary languages we used a subset of the
training corpus that is common to all language
pairs, hereinafter referred to as the intersection
data condition. We used the English side of all
training language pairs to collect the same sen-
tences in all languages, summing up to 320, 304
sentence pairs. Some statistics on the data used in
this study are reported in Table 1. Finally, in order
to assess the impact of the training data size over
the results obtained, we also considered a much
more challenging condition for the fr ? de pair,
where we used the entire Europarl data (V5) made
6http://www.fjoch.com/GIZA++.html
7http://www.statmt.org/europarl
available for the fifth Workshop on Statistical Ma-
chine Translation8 for training, and test our sys-
tem on out-of-domain news data. The training
corpus in this condition contains 43.6M French
words and 37.2M German words.
Development and test data for the first con-
dition (intersection) were obtained by leaving
out respectively 500 and 1000 sentences from
the common subset (same sentences for all lan-
guages), while the first 500 sentences of news-
test2008 and the entire newstest2009 official test
sets were used for the full data condition.
Train Dev Test
Words Voc. Words Voc. OOV Words Voc. OOV
da 8.5M 133.5k 13.4k 3.2k 104 25.9k 5.1k 226
de 8.5M 145.3k 13.5k 3.5k 120 26.0k 5.5k 245
en 8.9M 53.7k 14.0k 2.8k 39 27.2k 4.0k 63
es 9.3M 85.3k 14.6k 3.3k 56 28.6k 5.0k 88
fi 6.4M 274.9k 10.1k 4.3k 244 19.6k 7.1k 407
fr 10.3M 67.8k 16.1k 3.2k 47 31.5k 4.8k 87
el 8.9M 128.3k 14.1k 3.9k 72 27.2k 6.2k 159
it 9.0M 78.9k 14.3k 3.4k 61 28.1k 5.1k 99
nl 8.9M 105.0k 14.2k 3.1k 76 27.5k 4.8k 162
pt 9.2M 87.3k 14.5k 3.4k 49 28.3k 5.2k 118
sv 8.0M 140.8k 12.7k 3.3k 116 24.5k 5.2k 226
Table 1: Statistics for the training, development
and test sets of the intersection data condition
3.3 Results
In this section, we report on the experiments car-
ried out to assess the benefits of introducing an
auxiliary language model to the linear combina-
tion of models implemented in our SMT system.
Table 2 reports translation accuracy (BLEU) re-
sults for the main translation tasks considered in
this work (fr ? de), (fr ? en) and (de ? en),
as well as for multiple intermediate tasks needed
for pivoting via auxiliary systems.
For each triplet of languages (src, aux, trg),
columns 4th to 6th show BLEU scores for systems
performing (src ? aux), (aux ? trg) and pivot
translations using aux as the bridge language.
The last two columns display BLEU scores for
the main translation tasks (fr ? de), (fr ? en)
and (de? en). Column src-trg refers to the base-
line (direct) systems, for which no additional lan-
8http://www.statmt.org/wmt10
236
src aux trg src-aux aux-trg pivot src-trg +auxLM
Intersection data condition
fr - de - - - 18.02
da 22.78 20.02 16.27 +0.44
el 24.54 18.51 15.86 +0.76
en 29.53 17.31 15.69 +0.50
es 34.94 18.31 16.76 +0.96
fi 10.71 14.15 11.39 +0.65
it 31.60 16.86 16.54 -0.05
nl 22.71 21.44 16.76 +0.55
pt 33.61 17.47 16.34 -0.12
sv 20.73 19.59 13.73 -0.14
average +0.39
- - ref - - - - +6.46
fr - en - - - 29.53
da 22.78 29.54 25.48 +0.02
de 18.02 24.66 23.50 +0.05
el 24.54 29.37 25.31 +0.07
es 34.94 31.05 27.76 +0.61
fi 10.71 20.56 19.15 +0.44
it 31.60 25.75 25.79 +0.32
nl 22.71 24.49 25.15 +0.01
pt 33.61 29.44 27.27 +0.01
sv 20.73 30.98 23.74 +0.50
average +0.22
- - ref - - - - +11.30
de - en - - - 24.66
da 24.59 29.54 22.73 +0.96
el 19.72 29.37 20.88 +1.02
es 25.48 31.05 21.23 +0.77
fi 12.42 20.56 18.02 +0.94
fr 25.93 29.53 21.55 +0.19
it 18.82 25.75 18.05 +0.19
nl 24.97 24.49 22.62 +0.64
pt 23.15 29.44 21.93 +0.87
sv 19.80 30.98 21.35 +0.69
average +0.69
- - ref - - - - +9.53
Full data condition
fr - de - - - 19.94
es 38.76 20.18 19.36 +0.61
Table 2: Translation accuracy (BLEU) results.
guage model is used; column +auxLM refers to
the same system augmented with the additional
language model. Additional language models are
built from hypotheses obtained by means of pivot
translations, using aux as auxiliary language. The
last score is shown in the form of the difference
(improvement) with respect to the score of the
baseline system.
This table additionally displays the BLEU re-
sults obtained when building the additional lan-
guage models directly from the English reference
translations (see last row of each translation task).
These numbers provide an upper-bound of the ex-
pected improvements. Note finally that numbers
in boldface correspond to the best numbers in their
column for a given language pair.
As detailed above, the additional language
models are built using trg hypotheses obtained by
pivoting via an auxiliary language: (src ? aux)
+ (aux ? trg). Hence, column pivot shows the
quality (measured in terms of BLEU) of the hy-
potheses used to estimate the additional model.
Note that we did not limit the language model to
be estimated from the 1-best pivot hypotheses. In-
stead, we uses n-best translation hypotheses of the
(src ? aux) system and m-best hypotheses of
the (aux ? trg) system. Hence, n ? m target
hypotheses were used as training data to estimate
the additional models. Column +auxLM shows
BLEU scores over the test set after performing
four system optimizations on the development set
to select the best combination of values used for n
and m among: (1, 1), (10, 1), (10, 1) and (10, 10).
All hypotheses used to estimate a language model
are considered equally likely. Language models
are learnt using Witten-Bell discounting. Approx-
imately?1.0 point must be added to BLEU scores
shown in the last 2 columns for 95% confidence
levels.
As expected, pivot translations yield lower
quality scores than the corresponding direct trans-
lations hypotheses. However, pivot hypotheses
may contain better lexical predictions, that the ad-
ditional model helps transfer into the baseline sys-
tem, yielding translations with a higher quality, as
shown in many cases the +auxLM systems results.
The case of using Finnish as an auxiliary language
is particularly remarkable. Even though pivot hy-
potheses obtained through Finnish have the low-
est scores9, they help improve the baseline perfor-
mance as additional language models.
As expected, the translation results of the pair
9Given the agglutinative nature of morphological pro-
cesses in Finnish, reflected in a much lower number of words
per sentence, and a higher number of types (see Table 1),
BLEU scores for this language do not compare directly with
the ones obtained for other languages.
237
with a highest baseline (fr ? en) were on av-
erage less improved than those of the pairs with
lower baselines.
As can also be seen, the contribution of each
auxiliary language varies for each of the three
translation tasks. For instance, Danish (da) pro-
vides a clear improvement to (de ? en) transla-
tions, while no gain is observed for (fr ? en).
No clear patterns seems to emerge, though, and
the correlation between the quality of the pivot
translation and the boost provided by using these
pivot hypotheses remains to be better analyzed.
In order to assess whether the improvements
obtained carry over larger data conditions, we
trained our (fr ? de), (fr ? es) and (es? de)
systems over the entire EPPS data. Results are re-
ported in the bottom part of Table 2. As can be
seen, the (fr ? de) system is still improved by
using the additional language model. However,
the absolute value of the gain under the full condi-
tion (+0.61) is lower than that of the intersection
data condition (+0.96).
3.4 Contrastive evaluation of lexical
translation
In some cases, automatic metrics such as BLEU
cannot show significant differences that can be re-
vealed by fine-grained focussed human evaluation
(e.g. (Vilar et al, 2006)). Furthermore, comput-
ing some similarity between a system?s hypothe-
ses and gold standard references puts a strong
focus on the target side of translation, and does
not allow evaluating translation performance from
the source words that were actually translated.
We therefore use the evaluation methodology de-
scribed in (Max et al, 2010) for a complementary
measure of translation performance that focuses
on the contrastive ability of two systems to ade-
quately translate source words.
Source words from the test corpus were first
aligned with target words in the reference, by au-
tomatically aligning the union of the training and
test corpus using GIZA++.10 The test corpus was
analyzed by the TREETAGGER11 so as to identify
10The obtained alignments are thus strongly influenced by
alignments from the training corpus. It could be noted that
alignments could be manually corrected.
11http://www.ims.uni-stuttgart.de/
Source words? part-of-speech
aux ADJ ADV NOM PRO VER all +Bleu
el - 27 21 114 25 99 286 +0.07+ 62 29 136 27 114 368
es - 33 25 106 26 110 300 +0.61+ 64 38 136 22 117 377
fi - 44 40 106 20 92 302 +0.44+ 49 31 120 23 106 329
it - 55 39 128 35 119 376 +0.32+ 55 39 145 36 121 396
sv - 40 30 138 29 109 346 +0.50+ 69 46 144 23 134 416
Table 3: Contrastive lexical evaluation re-
sults per part-of-speech between the baseline
French?English system and our systems using
various auxiliary languages. ?-? (resp. ?+?) val-
ues indicate numbers of words that only the base-
line system (resp. our system) correctly translated
with respect to the reference translation.
content words, which have a more direct impact
on translation adequacy. When source words are
aligned to several target words, each target word
should be individually searched for in the candi-
date translation, and words from the reference can
only be matched once.
Table 3 shows contrastive results per part-of-
speech between the baseline fr?en system and
systems using various auxiliary languages. Val-
ues in the ?-? row indicate the number of words
that only the baseline system translated as in the
reference translation, and values in the ?+? row
the number of words that only our corresponding
system translated as in the reference. The most
striking result is the contribution of Greek, which,
while giving no gain in terms of BLEU, improved
the translation of 82 content words. This could
be explained, in addition to the lower Bleu3 and
Bleu4 precision, by the fact that the quality of
the translation of grammatical words may have
decreased. On the contrary, Italian brings little
improvement for content words save for nouns.
The mostly negative results on the translation of
pronouns were expected, because this depends on
their antecedent in English and is not the object of
specific modelling from the systems. The trans-
lation of nouns and adjectives benefits the most
from auxiliary translations.
projekte/corplex/TreeTagger
238
Figure 3 illustrates this evaluation by means of
two examples. It should be noted that a recurrent
type of improvement was that of avoiding missing
words, which is here a direct result of their being
boosted in the auxiliary hypotheses.
4 Conclusions and future work
We have presented a framework where auxiliary
MT systems are used to provide useful informa-
tion to a main SMT system. Our experiments
on auxiliary language triangulation have demon-
strated its validity on a difficult configuration and
have shown that improvements in translation qual-
ity could be obtained even under large training
data conditions.
The fact that low quality sources such as pivot
translation can provide useful complementary in-
formation calls for a better understanding of the
phenomena at play. It is very likely that, look-
ing at our results on the contribution of auxiliary
languages, improving the quality of an auxiliary
source can also be achieved by identifying what
a source is good for. For example, in the stud-
ied language configurations predictions of transla-
tions for pronouns in the source text by auxiliary
triangulation does not give access to useful infor-
mation. On the contrary, triangulation with Greek
when translating from French to English seems to
give useful information regarding the translation
of adjectives, a result which was quite unexpected.
Also, it would be interesting to use richer pre-
dictions than short n-grams, such as syntactic
dependencies, but this would require significant
changes on the decoders used. Using dynamic
models at the discourse level rather than only at
the sentence level would also be a useful improve-
ment. Besides the improvements just mentioned,
our future work includes working on several con-
figurations of the framework described in sec-
tion 2.2, in particular investigating the new type
of system combination.
Acknowledgements
This work has been partially funded by OSEO un-
der the Quaero program.
References
Bertoldi, Nicola and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In Proceedings of
WMT, Athens, Greece.
Callison-Burch, Chris, Cameron Shaw Fordyce,
Philipp Koehn, Christof Monz, and Josh Schroeder.
2008. Further meta-evaluation of machine transla-
tion. In Proceedings of WMT, Columbus, USA.
Chen, Stanley F. and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of ACL, Santa
Cruz, USA.
Chen, Yu, Andreas Eisele, and Martin Kay. 2008. Im-
proving statistical machine translation efficiency by
triangulation. In Proceedings of LREC, Marrakech,
Morocco.
Cohn, Trevor and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use
of multi-parallel corpora. In Proceedings of ACL,
Prague, Czech Republic.
Dymetman, Marc, Aur?lien Max, and Kenji Yamada.
2003. Towards interactive text understanding. In
Proceedings of ACL, short paper session, Sapporo,
Japan.
Hildebrand, Almut Silja and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
Proceedings of AMTA, Honolulu, USA.
Kumar, Shankar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of NAACL-HLT, Boston, USA.
Mari?o, Jos?, Rafael E. Banchs, Josep Maria Crego,
Adria de Gispert, Patrick Lambert, J.A.R. Fonol-
losa, and Martha Costa-juss?. 2006. N-gram based
machine translation. Computational Linguistics,
32(4):527?549.
Matusov, Evgeny, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose Mari?o,
Matthias Paulik, Salim Roukos, Holger Schwenk,
and Hermann Ney. 2008. System combination for
machine translation of spoken and written language.
IEEE Transactions on Audio, Speech and Language
Processing, 16(7):1222?1237, September.
Max, Aur?lien, Josep M. Crego, and Fran?ois Yvon.
2010. Contrastive Lexical Evaluation of Machine
Translation. In Proceedings of LREC, Valletta,
Malta.
239
ref #357 this concession to the unions ignores the reality that all airlines have different safety procedures which even differ
between aircrafts within each airline .
bas this concession unions ignores the fact that all airlines have different safety procedures which are even within each
of the companies in accordance with the types of equipment .
w.r.t. src cette concession aux syndicats ignore la r?alit? selon laquelle toutes les compagnies a?riennes ont des proc?dures de s?curit?
diff?rentes qui diff?rent m?me au sein de chacune des compagnies en fonction des types d ? appareils .
+aux this concession to the trade unions ignores the reality according to which all the airlines have different safety pro-
cedures which differ even within each of the companies in accordance with the types of equipment .
w.r.t. src cette concession aux syndicats ignore la r?alit? selon laquelle toutes les compagnies a?riennes ont des proc?dures de s?curit?
diff?rentes qui diff?rent m?me au sein de chacune des compagnies en fonction des types d ? appareils .
Figure 3: Example of automatic translations from French to English for the baseline system and when
using Spanish as the auxiliary language. Bold marking indicates source/target words which were cor-
rectly translated according to the reference translation.
Nomoto, Tadashi. 2004. Multi-engine machine trans-
lation with voted language model. In Proceedings
of ACL, Barcelona, Catalunya, Spain.
Och, Franz Josef and Hermann Ney. 2001. Statisti-
cal multi-source translation. In Proceedings of MT
Summit, Santiago de Compostela, Spain.
Paulik, Matthias, Christian F?gen, Thomas Schaaf,
Tanja Schultz, Sebastian St?ker, and Alex Waibel.
2005. Document driven machine translation en-
hanced automatic speech recognition. In Proceed-
ings of InterSpeech, Lisbon, Portugal.
Placeway, Paul and John Lafferty. 1996. Cheating
with imperfect transcripts. In Proceedings of IC-
SLP, Philadelphia, USA.
Rosti, Antti-Veikko, Necip Fazil Ayan, Bin Xiang,
Spyros Matsoukas, Richard Schwatz, and Bonnie J.
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Proceedings of
NAACL-HTL, Rochester, USA.
Schroeder, Josh, Trevor Cohn, and Philipp Koehn.
2009. Word lattices for multi-source translation. In
Proceedings of EACL, Athens, Greece.
Schwartz, Lane. 2008. Multi-source translation meth-
ods. In Proceedings of AMTA, Honolulu, USA.
Simard, Michel and Pierre Isabelle. 2009. Phrase-
based machine translation in a computer-assisted
translation environment. In Proceedings of Machine
Translation Summit XII, Ottawa, Canada.
Tillmann, Christoph. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of NAACL-HLT, Boston, USA.
Tromble, Roy, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk decoding for statistical machine translation. In
Proceedings of EMNLP, Honolulu, USA.
Utiyama, Masao and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statisti-
cal machine translation. In Proceedings of NAACL-
HLT, Rochester, USA.
Vilar, David, Jia Xu, Luis Fernando d?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In Proceedings of LREC,
Genoa, Italy.
Wu, Hua and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine trans-
lation. In Proceedings of ACL, Prague, Czech Re-
public.
240
Coling 2010: Poster Volume, pages 197?205,
Beijing, August 2010
Improving Reordering with Linguistically Informed Bilingual n-grams
Josep Maria Crego
LIMSI-CNRS
jmcrego@limsi.fr
Franc?ois Yvon
LIMSI-CNRS & Univ. Paris Sud
yvon@limsi.fr
Abstract
We present a new reordering model es-
timated as a standard n-gram language
model with units built from morpho-
syntactic information of the source and
target languages. It can be seen as a
model that translates the morpho-syntactic
structure of the input sentence, in contrast
to standard translation models which take
care of the surface word forms. We take
advantage from the fact that such units
are less sparse than standard translation
units to increase the size of bilingual con-
text that is considered during the trans-
lation process, thus effectively account-
ing for mid-range reorderings. Empirical
results on French-English and German-
English translation tasks show that our
model achieves higher translation accu-
racy levels than those obtained with the
widely used lexicalized reordering model.
1 Introduction
Word ordering is one of the major issues in statis-
tical machine translation (SMT), due to the many
word order peculiarities of each language. It is
widely accepted that there is a need for struc-
tural information to account for such differences.
Structural information, such as Part-of-speech
(POS) tags, chunks or constituency/dependency
parse trees, offers a greater potential to learn
generalizations about relationships between lan-
guages than models based on word surface forms,
because such ?surfacist? models fail to infer gen-
eralizations from the training data.
The word ordering problem is typically decom-
posed in a number of related problems which can
be further explained by a variety of linguistic phe-
nomena. Accordingly, we can sort out the re-
ordering problems into three categories based on
the kind of linguistic units involved and/or the
typical distortion distance they imply. Roughly
speaking, we face short-range reorderings when
single words are reordered within a relatively
small window distance. It consist of the easi-
est case as typically, the use of phrases (in the
sense of translation units of the phrase-based ap-
proach to SMT) is believed to adequately perform
such reorderings. Mid-range reorderings involve
reorderings between two or more phrases (trans-
lation units) which are closely positioned, typi-
cally within a window of about 6 words. Many
alternatives have been proposed to tackle mid-
range reorderings through the introduction of lin-
guistic information in MT systems. To the best
of our knowledge, the authors of (Xia and Mc-
Cord, 2004) were the first to address this prob-
lem in the statistical MT paradigm. They auto-
matically build a set of linguistically grounded
rewrite rules, aimed at reordering the source sen-
tence so as to match the word order of the target
side. Similarly, (Collins, et al2005) and (Popovic
and Ney, 2006) reorder the source sentence us-
ing a small set of hand-crafted rules for German-
English translation. (Crego and Marin?o, 2007)
show that the ordering problem can be more accu-
rately solved by building a source-sentence word
lattice containing the most promising reordering
hypotheses, allowing the decoder to decide for the
best word order hypothesis. Word lattices are built
by means of rewrite rules operating on POS tags;
such rules are automatically extracted from the
training bi-text. (Zhang, et al2007) introduce
shallow parse (chunk) information to reorder the
source sentence, aiming at extending the scope of
their rewrite rules, encoding reordering hypothe-
ses in the form of a confusion network that is
then passed to the decoder. These studies tackle
mid-range reorderings by predicting more or less
accurate reordering hypotheses. However, none
197
of them introduce a reordering model to be used
in decoding time. Nowadays, most of SMT sys-
tems implement the well known lexicalized re-
ordering model (Tillman, 2004). Basically, for
each translation unit it estimates the probability
of being translated monotone, swapped or placed
discontiguous with respect to its previous trans-
lation unit. Integrated within the Moses (Koehn,
et al2007) decoder, the model achieves state-of-
the-art results for many translation tasks. One
of the main reasons that explains the success of
the model is that it considers information of the
source- and target-side surface forms, while the
above mentionned approaches attempt to hypoth-
esize reorderings relying only on the information
contained on the source-side words.
Finally, long-range reorderings imply reorder-
ings in the structure of the sentence. Such re-
orderings are necessary to model the translation
for pairs like Arabic-English, as English typically
follows the SVO order, while Arabic sentences
have different structures. Even if several attempts
exist which follow the above idea of making the
ordering of the source sentence similar to the tar-
get sentence before decoding (Niehues and Kolss,
2009), long-range reorderings are typically better
addressed by syntax-based and hierarchical (Chi-
ang, 2007) models. In (Zollmann et al, 2008),
an interesting comparison between phrase-based,
hierarchical and syntax-augmented models is car-
ried out, concluding that hierarchical and syntax-
based models slightly outperform phrase-based
models under large data conditions and for suf-
ficiently non-monotonic language pairs.
Encouraged by the work reported in (Hoang
and Koehn, 2009), we tackle the mid-range re-
ordering problem in SMT by introducing a n-
gram language model of bilingual units built from
POS information. The rationale behind such a
model is double: on the one hand we aim at in-
troducing morpho-syntactic information into the
reordering model, as we believe it plays an im-
portant role for predicting systematic word or-
dering differences between language pairs; at the
same time that it drastically reduces the sparse-
ness problem of standard translation units built
from surface forms. On the other hand, n-gram
language modeling is a robust approach, that en-
ables to account for arbitrary large sequences of
units. Hence, the proposed model takes care of
the translation adequacy of the structural informa-
tion present in translation hypotheses, here intro-
duced in the form of POS tags. We also show how
the new model compares to a widely used lexical-
ized reordering model, which we have also im-
plemented in our particular bilingual n-gram ap-
proach to SMT, as well as to the widely known
Moses SMT decoder, a state-of-the-art decoder
performing lexicalized reordering.
The remaining of this paper is as follows. In
Section 2 we briefly describe the bilingual n-gram
SMT system. Section 3 details the bilingual n-
gram reordering model, the main contribution of
this paper, and introduces additional well known
reordering models. In Section 4, we analyze the
reordering needs of the language pairs considered
in this work and we carry out evaluation experi-
ments. Finally, we conclude and outline further
work in Section 5.
2 Bilingual n-gram SMT
Our SMT system defines a translation hypothesis
t given a source sentence s, as the sentence which
maximizes a linear combination of feature func-
tions:
t?I1 = argmaxtI1
{ M?
m=1
?mhm(sJ1 , tI1)
}
(1)
where ?m is the weight associated with the fea-
ture hm(s, t). The main feature is the log-score of
the translation model based on bilingual n-grams.
This model constitutes a language model of a par-
ticular bi-language composed of bilingual units
which are typically referred to as tuples (Marin?o et
al., 2006). In this way, the translation model prob-
abilities at the sentence level are approximated by
using n-grams of tuples:
p(sJ1 , tI1) =
K?
k=1
p((s, t)k|(s, t)k?1 . . . (s, t)k?n+1)
where s refers to source t to target and (s, t)k to
the kth tuple of the given bilingual sentence pairs,
sJ1 and tI1. It is important to notice that, since
both languages are linked up in tuples, the context
198
information provided by this translation model is
bilingual. As for any standard n-gram language
model, our translation model is estimated over a
training corpus composed of sentences of the lan-
guage being modeled, in this case, sentences of
the bi-language previously introduced. Transla-
tion units consist of the core elements of any SMT
system. In our case, tuples are extracted from a
word aligned corpus in such a way that a unique
segmentation of the bilingual corpus is achieved,
allowing to estimate the n-gram model. Figure 1
presents a simple example illustrating the unique
tuple segmentation for a given word-aligned pair
of sentences (top).
Figure 1: Tuple extraction from an aligned sen-
tence pair.
The resulting sequence of tuples (1) is further
refined to avoid NULL words in source side of the
tuples (2). Once the whole bilingual training data
is segmented into tuples, n-gram language model
probabilities can be estimated. Notice from the
example that the English source words perfect and
translations have been reordered in the final tu-
ple segmentation, while the French target words
are kept in their original order. During decoding,
sentences to be translated are encoded in the form
of word lattices containing the most promising re-
ordering hypotheses, so as to reproduce the word
order modifications introduced during the tuple
extraction process. Hence, at decoding time, only
those reordering hypotheses encoded in the word
lattice are examined. Reordering hypotheses are
introduced following a set of reordering rules au-
tomatically learned from the bi-text corpus word
alignments.
Following on the previous example, the rule
perfect translations ; translations perfect pro-
duces the swap of the English words that is ob-
served for the French and English pair. Typically,
POS information is used to increase the general-
ization power of such rules. Hence, rewrite rules
are built using POS instead of surface word forms.
See (Crego and Marin?o, 2007) for details on tuples
extraction and reordering rules.
3 Reordering Models
In this section, we detail three different reordering
models implemented in our SMT system. As pre-
viously outlined, the purpose of reordering mod-
els is to accurately learn generalizations for the
word order modifications introduced on the source
side during the tuple extraction process.
3.1 Source n-gram Language Model
We employ a n-gram language model estimated
over the source words of the training corpus af-
ter being reordered in the tuple extraction process.
Therefore, the model scores a given source-side
reordering hypothesis according to the reorder-
ings performed in the training sentences.
POS tags are used instead of surface forms
in order to improve generalization and to reduce
sparseness. The model is estimated as any stan-
dard n-gram language model, described by the
following equation:
p(sJ1 , tI1) =
J?
j=1
p(stj |stj?1, . . . , stj?n+1) (2)
where stj relates to the POS tag used for the jth
source word.
The main drawback of this model is the lack
of knowledge of the hypotheses on the target-
side. The probability assigned to a sequence of
source words is only conditioned to the sequence
of source words.
3.2 Lexicalized Reordering Model
A broadly used reordering model for phrase-based
systems is lexicalized reordering (Tillman, 2004).
It introduces a probability distribution for each
phrase pair that indicates the likelihood of being
199
translated monotone, swapped or placed discon-
tiguous to its previous phrase. The ordering of
the next phrase with respect to the current phrase
is typically also modeled. In our implementa-
tion, we modified the three orientation types and
consider: a consecutive type, where the original
monotone and swap orientations are lumped to-
gether, a forward type, specifying discontiguous
forward orientation, and a backward type, spec-
ifying discontiguous backward orientation. Em-
pirical results showed that in our case, the new
orientations slightly outperform the original ones.
This may be explained by the fact that the model
is applied over tuples instead of phrases.
Counts of these three types are updated for
each unit collected during the training process.
Given these counts, we can learn probability dis-
tributions of the form pr(orientation|(st)) where
orientation ? {c, f, b} (consecutive, forward
and backward) and (st) is a translation unit.
Counts are typically smoothed for the estimation
of the probability distribution. A major weakness
of the lexicalized reordering model is due to the
fact that it does not considers phrase neighboring,
i.e. a single probability is learned for each phrase
pair without considering its context. An additional
concern is the problem of sparse data: translation
units may occur only a few times in the training
data, making it hard to estimate reliable probabil-
ity distributions.
3.3 Linguistically Informed Bilingual
n-gram Language Model
The bilingual n-gram LM is estimated as a stan-
dard n-gram LM over translation units built from
POS tags represented as:
p(sJ1 , tI1) =
K?
k=1
p((st)tk|(st)tk?1 . . . (st)tk?n+1)
where (st)tk relates to the kth translation unit,
(st)k, built from POS tags instead of words.
This model aims at alleviating the drawbacks of
the previous two reordering models. On the one
hand it takes into account bilingual information
to model reordering. On the other hand it con-
siders the phrase neighboring when estimating the
reordering probability of a given translation unit.
Figure 2 shows the sequence of translation units
built from POS tags, used in our previous exam-
ple.
Figure 2: Sequence of POS-tagged units used to
estimate the bilingual n-gram LM.
POS-tagged units used in our model are ex-
pected to be much less sparse than those built from
surface forms, allowing to estimate higher order
language models. Therefore, larger bilingual con-
text are introduced in the translation process. This
model can also be seen as a translation model of
the sentence structure. It models the adequacy of
translating sequences of source POS tags into tar-
get POS tags.
Note that the model is not limited to using
POS information. Rather, many other informa-
tion sources could be used (supertags, additional
morphology features, etc.), allowing to model dif-
ferent translation properties. However, we must
take into account that the degree of sparsity of the
model units, which is directly related to the in-
formation they contain, affects the level of bilin-
gual context finally introduced in the translation
process. Since more informed units may yield
more accurate predictions, more informed units
may also force the model to fall to lower n-grams.
Hence, the degree of accuracy and generalization
power of the model units must be carefully bal-
anced to allow good reordering predictions for
contexts as large as possible.
As any standard language model, smoothing is
needed. Empirical results showed that Kneser-
Ney smoothing (Kneser and Ney, 1995) achieved
the best performance among other options (mea-
sured in terms of translation accuracy).
3.4 Decoding Issues
A straightforward implementation of the three
models is carried out by extending the log-linear
combination of equation (1) with the new features.
Note that no additional decoding complexity is
introduced in the baseline decoding implementa-
tion. Considering the bilingual n-gram language
model, the decoder must know the POS tags for
200
each tuple. However, each tuple may be tagged
differently, as words with same surface form may
have different POS tags.
We have implemented two solutions for this sit-
uation. Firstly, we assume that each tuple has a
single POS-tagged version. Accordingly, we se-
lect a single POS-tagged version out of the mul-
tiple choices (the most frequent). Secondly, all
POS-tagged versions of each tuple are allowed.
The second choice implies using more accurate
POS-tagged tuples to model reordering, however,
it overpopulates the search space with spurious
hypotheses, as multiple identical units (with dif-
ferent POS tags) are considered.
Our first empirical findings showed no differ-
ences in translation accuracy for both configura-
tions. Hence, in the remaining of this paper we
only consider the first solution (a single POS-
tagged version of each tuple). The training cor-
pus composed of tagged units out of which our
new model is estimated is accordingly modified to
contain only those tagged units considered in de-
coding. Note that most of the ambiguity present in
word tagging is resolved by the fact that transla-
tion units may contain multiple source and target
side words.
4 Evaluation Framework
In this section, we perform evaluation experi-
ments of our novel reordering model. First, we
give details of the corpora and baseline system
employed in our experiments and analyze the re-
ordering needs of the translation tasks, French-
English and German-English (in both directions).
Finally, we evaluate the performance of our model
and contrast results with other reordering models
and translation systems.
4.1 Corpora
We have used the fifth version of the EPPS and the
News Commentary corpora made available in the
context of the Fifth ACL Workshop on Statistical
Machine Translation. Table 1 presents the basic
statistics for the training and test data sets. Our
test sets correspond to news-test2008 and new-
stest2009 file sets, hereinafter referred to as Tune
and Test respectively.
French, German and English Part-of-speech
tags are computed by means of the TreeTagger 1
toolkit. Additional German tags are obtained us-
ing the RFTagger 2 toolkit, which annotates text
with fine-grained part-of-speech tags (Schmid and
Laws, 2008) with a vocabulary of more than 700
tags containing rich morpho-syntactic information
(gender, number, case, tense, etc.).
Lang. Sent. Words Voc. OOV Refs
Train
French 1.75 M 52.4 M 137 k ? ?
English 1.75 M 47.4 M 138 k ? ?
Tune
French 2, 051 55.3 k 8, 957 1, 282 1
English 2, 051 49.2 k 8, 359 1, 344 1
Test
French 2, 525 72.8 k 10, 832 1, 749 1
English 2, 525 65.1 k 9, 568 1, 724 1
Train
German 1, 61 M 42.2 M 381 k ? ?
English 1, 61 M 44.2 M 137 k ? ?
Tune
German 2, 051 47, 8 k 10, 994 2, 153 1
English 2, 051 49, 2 k 8, 359 1, 491 1
Test
German 2, 525 62, 8 k 12, 856 2, 704 1
English 2, 525 65, 1 k 9, 568 1, 810 1
Table 1: Statistics for the training, tune and test
data sets.
4.2 System Details
After preprocessing the corpora with standard tok-
enization tools, word-to-word alignments are per-
formed in both directions, source-to-target and
target-to-source. In our system implementation,
the GIZA++ toolkit3 is used to compute the
word alignments. Then, the grow-diag-final-and
(Koehn et al, 2005) heuristic is used to obtain the
alignments from which tuples are extracted.
In addition to the tuple n-gram translation
model, our SMT system implements six addi-
tional feature functions which are linearly com-
1www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
2www.ims.uni-stuttgart.de/projekte/corplex/RFTagger
3http://www.fjoch.com/GIZA++.html
201
bined following a discriminative modeling frame-
work (Och and Ney, 2002): a target-language
model which provides information about the tar-
get language structure and fluency; two lexicon
models, which constitute complementary trans-
lation models computed for each given tuple;
a ?weak? distance-based distortion model; and
finally a word-bonus model and a tuple-bonus
model which are used in order to compensate for
the system preference for short translations.
All language models used in this work are
estimated using the SRI language modeling
toolkit4. According to our experience, Kneser-
Ney smoothing (Kneser and Ney, 1995) and in-
terpolation of lower and higher n-grams options
are used as they typically achieve the best per-
formance. Optimization work is carried out by
means of the widely used MERT toolkit5 which
has been slightly modified to perform optimiza-
tions embedding our decoder. The BLEU (Pap-
ineni et al, 2002) score is used as objective func-
tion for MERT and to evaluate test performance.
4.3 Reordering in German-English and
French-English Translation
Two factors are found to greatly impact the overall
translation performance: the morphological mis-
match between languages, and their reordering
needs. The vocabulary size is strongly influenced
by the number of word forms for number, case,
tense, mood, etc., while reordering needs refer to
the difference in their syntactic structure. In this
work, we are primarily interested on the reorder-
ing needs of each language pair. Figure 3 displays
a quantitative analysis of the reordering needs for
the language pairs under study.
Figure 3 displays the (%) distribution of the
reordered sequences, according to their size, ob-
served for the training bi-texts of both translation
tasks. Word alignments are used to determine re-
orderings. A reordering sequence can also be seen
as the sequence of words implied in a reorder-
ing rule. Hence, we used the reordering rules ex-
tracted from the training corpus to account for re-
ordering sequences. Coming back to the example
of Figure 1, a single reordering sequence is found,
4http://www.speech.sri.com/projects/srilm/
5http://www.statmt.org/moses/
which considers the source words perfect transla-
tions.
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
2 3 4 5 6 7 >=8
Size (words)
fr-en
de-en
Figure 3: Size (in words) of reorderings (%) ob-
served in training bi-texts.
As can be seen, the French-English and
German-English pairs follow a different distribu-
tion of reorderings according to their size. A
lower number of short-range reorderings are ob-
served for the German-English task while a higher
number of long-range reorderings. Considering
mid-range reorderings (from 5 to 7 words), the
French-English pair shows a lower percentage (?
14%) than the German-English (? 22%). A simi-
lar performance is expected when considering the
opposite translation directions. Note that reorder-
ings are extracted from word-alignments, an au-
tomatic process which is far notoriously error-
prone. The above statistics must be accordingly
considered.
4.4 Results
Translation accuracy (BLEU) results are given in
table 2 for the same baseline system performing
different reordering models: source 6-gram LM
(sLM); lexicalized reordering (lex); bilingual 6-
gram LM (bLM) assuming a single POS-tagged
version of each tuple. In the case of the German-
English translation task we also report results for
the bilingual 5-gram LM built from POS tags ob-
tained from RFTagger containing a richer vocab-
ulary tag set (b+LM). For comparison purposes,
we also show the scores obtained by the Moses
phrase-based system performing lexicalized re-
ordering. Models of both systems are built sharing
the same training data and word alignments.
202
The worst results are obtained by the sLM
model. The fact that it only considers source-
language information results clearly relevant to
accurately model reordering. A very similar
performance is shown by our bilingual n-gram
system and Moses under lexicalized reordering
(bLM and Moses), slightly lower results are
obtained by the n-gram system under French-
English translation.
Config Fr;En En;Fr De;En En;De
sLM 22.32 21.97 17.11 12.23
lex 22.46 22.09 17.31 12.38
bLM 23.03 22.32 17.37 12.58
b+LM ? ? 17.57 12.92
Moses 22.81 22.33 17.22 12.45
Table 2: Translation accuracy (BLEU) results.
When moving from lex to bLM, our system
increases its accuracy results for both tasks and
translation directions. In this case, results are
slightly higher than those obtained by Moses
(same results for English-to-French). Finally, re-
sults for translations performed with the bilingual
n-gram reordering model built from rich German
POS tags (b+LM) achieve the highest accuracy
results for both directions of the German-English
task. Even though results are consistent for all
translation tasks and directions they fall within
the statistical confidence margin. Add ?2.36
to French-English results and ?1.25 to German-
English results for a 95% confidence level. Very
similar results were obtained when estimating our
model for orders from 5 to 7.
In order to better understand the impact of the
proposed reordering model, we have measured the
accuracy of the reordering task. Hence, isolat-
ing the reordering problem from the more general
translation problem. We use BLEU to account the
n-gram matching between the sequence of source
words aligned to the 1-best translation hypothe-
sis, i.e. the permutation of the source words out-
put by the decoder, and the permutation of source
words that monotonizes the word alignments with
respect to the target reference. Note that in or-
der to obtain the word alignments of the test sets
we re-aligned the entire corpus after including the
test set. Table 3 shows the BLEU results of the
reordering task. Bigram, trigram and 4gram pre-
cision scores are also given.
Pair Config BLEU (2g/3g/4g)
Fr;En lex 71.69 (75.0/63.4/55.6)
bLM 71.98 (75.3/63.7/56.0)
En;Fr lex 72.92 (75.5/65.0/57.6)
bLM 73.25 (75.8/65.4/58.1)
De;En lex 62.12 (67.3/52.1/42.5)
b+LM 63.29 (68.3/53.5/44.0)
En;De lex 62.72 (67.9/52.8/43.1)
b+LM 63.36 (68.6/53.6/43.8)
Table 3: Reordering accuracy (BLEU) results.
As can be seen, the bilingual n-gram reordering
model shows higher results for both translation
tasks and directions than lexicalized reordering,
specially for German-English translation. Our
model also obtains higher values of n-gram pre-
cision for all values of n.
Next, we validate the introduction of additional
bilingual context in the translation process. Fig-
ure 4 shows the average size of the translation
unit n-grams used for the test set according to dif-
ferent models (German-English), the surface form
3-gram language model (main translation model),
and the new reordering model when built from the
reduced POS tagset (POS) and using the rich POS
tagset (POS+).
 0
 5
 10
 15
 20
 25
 30
 35
 40
0 1 2 3 4 5 6 7 8 9
Size (units)
word-based bilingual units
POS-based bilingual units
POS+-based bilingual units
Figure 4: Size of translation unit n-grams (%)
seen in test for different n-gram models.
As expected, translation units built from the re-
duced POS tagset are less sparse, enabling us to
203
introduce larger n-grams in the translation pro-
cess. However, the fact that they achieve lower
translation accuracy scores (see Table 2) indicates
that the probabilities associated to these large n-
grams are less accurate. It can also be seen that
the model built from the rich POS tagset uses a
higher number of large n-grams than the language
model built from surface forms.
The availability of mid-range n-grams validates
the introduction of additional bilingual context
achieved by the new model, leading to effec-
tively modeling mid-range reorderings. Notice
additionally that considering the language model
built from surface forms, only a few 4-grams of
the test set are seen in the training set, which
explains the small reduction in performance ob-
served when translating with a bilingual 4-gram
language model (internal results). Similarly, the
results shown in Figure 4 validates the choice of
using bilingual 5-grams for b+LM and 6-grams
for bLM .
Finally, we evaluate the mismatch between the
reorderings collected on the training data, and
those output by the decoder. Table 4 shows the
percentage of reordered sequences found for the
1-best translation hypothesis of the test set ac-
cording to their size. The French-to-English and
German-to-English tasks are considered.
Pair Config 2 3 4 5 6 7 ? 8
Fr;En lex 58 23 10 5 2 1 1
bLM 57 23 11 4 2.5 1.5 1
De;En lex 33 24 22 14 5 1.5 0.5
b+LM 35 25 19 13 5 2.5 0.5
Table 4: Size (%) of the reordered sequences ob-
served when translating the test set.
Very similar distributions are observed for both
reordering models. In parallel, distributions are
also comparable to those presented in Figure 3
for reorderings collected from the training bi-text,
with the exception of long-range and very short-
range reorderings. This may be explained by the
fact that system models, in special the distortion
penalty model, typically prefer monotonic trans-
lations, while the system lacks a model to support
large-range reorderings.
5 Conclusions and Further Work
We have presented a new reordering model based
on bilingual n-grams with units built from lin-
guistic information, aiming at modeling the struc-
tural adequacy of translations. We compared our
new reordering model to the widely used lexical-
ized reordering model when implemented in our
bilingual n-gram system as well as using Moses,
a state-of-the-art phrase-based SMT system.
Our model obtained slightly higher transla-
tion accuracy (BLEU) results. We also analysed
the quality of the reorderings output by our sys-
tem when performing the new reordering model,
which also outperformed the quality of those out-
put by the system performing lexicalized reorder-
ing. The back-off procedure used by standard
language models allows to dynamically adapt the
scope of the context used. Therefore, in the case
of our reordering model, back-off allows to con-
sider always as much bilingual context (n-grams)
as possible. The new model was straightfor-
ward implemented in our bilingual n-gram sys-
tem by extending the log-linear combination im-
plemented by our decoder. No additional decod-
ing complexity was introduced in the baseline de-
coding implementation.
Finally, we showed that mid-range reorder-
ings are present in French-English and German-
English translations and that our reordering model
effectively tackles such reorderings. However, we
saw that long-range reorderings, also present in
these tasks, are yet to be addressed.
We plan to further investigate the use of differ-
ent structural information, such as supertags, and
tags conveying different levels of morphology in-
formation (gender, number, tense, mood, etc.) for
different language pairs.
Acknowledgments
This work has been partially funded by OSEO un-
der the Quaero program.
References
F. Xia and M. McCord. Improving a Statistical MT
System with Automatically Learned Rewrite Pat-
terns. In Proc. of the COLING 2004, 508?514,
Geneva, Switzerland, August 2004.
204
D. Chiang. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228, June
2007.
H. Hoang and Ph. Koehn. Improving Mid-Range Re-
Ordering Using Templates of Factors. In Proc. of
the EACL 2009, 372?379, Athens, Greece, March
2009.
J. M. Crego and J. B. Marin?o. Improving statistical
MT by coupling reordering and decoding. In Ma-
chine Translation, 20(3):199?215, July 2007.
Marin?o, Jose? and Banchs, Rafael E. and Crego, Josep
Maria and de Gispert, Adria and Lambert, Patrick
and Fonollosa, J.A.R. and Costa-jussa`, Marta N-
gram Based Machine Translation. In Computa-
tional Linguistics, 32(4):527?549, 2006
Ch. Tillman. A Unigram Orientation Model for Sta-
tistical Machine Translation. In Proc. of the HLT-
NAACL 2004, 101?104, Boston, MA, USA, May
2004.
M. Collins, Ph. Koehn and I. Kucerova. Clause Re-
structuring for Statistical Machine Translation. In
Proc. of the ACL 2005, 531?540, Ann Arbor, MI,
USA, June 2005.
Ph. Koehn, H. Hoang, A. Birch, Ch. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, Ch.
Moran, R. Zens, Ch. Dyer, O. Bojar, A. Constantin
and E. Herbst. Moses: Open Source Toolkit for Sta-
tistical Machine Translation. In Proc. of the ACL
2007, demonstration session, prague, Czech Repub-
lic, June 2007.
Y. Zhang, R. Zens and H. Ney Improved Chunk-level
Reordering for Statistical Machine Translation. In
Proc. of the IWSLT 2007, 21?28, Trento, Italy, Oc-
tober 2007.
H. Schmid and F. Laws. Estimation of Conditional
Probabilities with Decision Trees and an Applica-
tion to Fine-Grained POS Tagging. In Proc. of the
COLING 2008, 777?784, Manchester, UK, August
2008.
F.J. Och and H. Ney. Improved statistical alignment
models. In Proc. of the ACL 2000, 440?447, Hong
Kong, China, October 2000.
Ph. Koehn, A. Axelrod, A. Birch, Ch. Callison-Burch,
M. Osborne and D. Talbot. Edinburgh System De-
scription for the 2005 IWSLT Speech Translation
Evaluation. In Proc of the IWSLT 2005, Pittsburgh,
PA, October 2005.
F. J. Och and H. Ney. Discriminative Training and
Maximum Entropy Models for Statistical Machine
Translation. In Proc. of the ACL 2002. 295?302,
Philadelphia, PA, July 2002.
A. Stolcke. SRLIM: an extensible language model-
ing toolkit. Proc. of the INTERSPEECH 2002. 901?
904, Denver, CO, September 2008.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. Bleu:
a method for automatic evaluation of machine trans-
lation. In Proc. of the ACL 2002, 311?318, Philadel-
phia, PA, July 2002.
R. Kneser and H. Ney. Improved backing-off for m-
gram language modeling. In Proc. of the ICASSP
1995. 181?184, Detroit, MI, May 1995.
A. Zollmann, A. Venugopal, F. J. Och and J. Ponte.
A Systematic Comparison of Phrase-Based, Hierar-
chical and Syntax-Augmented Statistical MT. In
Proc. of the COLING 2008. 1145?1152, Manch-
ester, UK, August 2008.
M. Popovic and H. Ney. POS-based Word Reorderings
for Statistical Machine Translation. In Proc. of the
LREC 2006. 1278?1283, Genoa, Italy, May 2006.
J. Niehues and M. Kolss. A POS-Based Model for
Long-Range Reorderings in SMT. In Proc. of the
WMT 2009. 206?214, Athens Greece, March 2009.
205
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIMSI @ WMT11
Alexandre Allauzen
He?le`ne Bonneau-Maynard
Hai-Son Le
Aure?lien Max
Guillaume Wisniewski
Franc?ois Yvon
Univ. Paris-Sud and LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Gilles Adda
Josep M. Crego
Adrien Lardilleux
Thomas Lavergne
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Abstract
This paper describes LIMSI?s submissions to
the Sixth Workshop on Statistical Machine
Translation. We report results for the French-
English and German-English shared transla-
tion tasks in both directions. Our systems
use n-code, an open source Statistical Ma-
chine Translation system based on bilingual
n-grams. For the French-English task, we fo-
cussed on finding efficient ways to take ad-
vantage of the large and heterogeneous train-
ing parallel data. In particular, using a sim-
ple filtering strategy helped to improve both
processing time and translation quality. To
translate from English to French and Ger-
man, we also investigated the use of the
SOUL language model in Machine Trans-
lation and showed significant improvements
with a 10-gram SOUL model. We also briefly
report experiments with several alternatives to
the standard n-best MERT procedure, leading
to a significant speed-up.
1 Introduction
This paper describes LIMSI?s submissions to the
Sixth Workshop on Statistical Machine Translation,
where LIMSI participated in the French-English and
German-English tasks in both directions. For this
evaluation, we used n-code, our in-house Statistical
Machine Translation (SMT) system which is open-
source and based on bilingual n-grams.
This paper is organized as follows. Section 2 pro-
vides an overview of n-code, while the data pre-
processing and filtering steps are described in Sec-
tion 3. Given the large amount of parallel data avail-
able, we proposed a method to filter the French-
English GigaWord corpus (Section 3.2). As in our
previous participations, data cleaning and filtering
constitute a non-negligible part of our work. This
includes detecting and discarding sentences in other
languages; removing sentences which are also in-
cluded in the provided development sets, as well as
parts that are repeated (for the monolingual news
data, this can reduce the amount of data by a fac-
tor 3 or 4, depending on the language and the year);
normalizing the character set (non-utf8 characters
which are aberrant in context, or in the case of the
GigaWord corpus, a lot of non-printable and thus in-
visible control characters such as EOT (end of trans-
mission)1).
For target language modeling (Section 4), a stan-
dard back-off n-gram model is estimated and tuned
as described in Section 4.1. Moreover, we also in-
troduce in Section 4.2 the use of the SOUL lan-
guage model (LM) (Le et al, 2011) in SMT. Based
on neural networks, the SOUL LM can handle an
arbitrary large vocabulary and a high order marko-
vian assumption (up to 10-gram in this work). Fi-
nally, experimental results are reported in Section 5
both in terms of BLEU scores and translation edit
rates (TER) measured on the provided newstest2010
dataset.
2 System Overview
Our in-house n-code SMT system implements the
bilingual n-gram approach to Statistical Machine
Translation (Casacuberta and Vidal, 2004). Given a
1This kind of characters was used for Teletype up to the sev-
enties or early eighties.
309
source sentence sJ1, a translation hypothesis t?
I
1 is de-
fined as the sentence which maximizes a linear com-
bination of feature functions:
t?I1 = argmax
tI1
{
M
?
m=1
?mhm(sJ1, tI1)
}
(1)
where sJ1 and t
I
1 respectively denote the source and
the target sentences, and ?m is the weight associated
with the feature function hm. The translation fea-
ture is the log-score of the translation model based
on bilingual units called tuples. The probability as-
signed to a sentence pair by the translation model is
estimated by using the n-gram assumption:
p(sJ1, t
I
1) =
K
?
k=1
p((s, t)k|(s, t)k?1 . . .(s, t)k?n+1)
where s refers to a source symbol (t for target) and
(s, t)k to the kth tuple of the given bilingual sentence
pair. It is worth noticing that, since both languages
are linked up in tuples, the context information pro-
vided by this translation model is bilingual. In ad-
dition to the translation model, eleven feature func-
tions are combined: a target-language model (see
Section 4 for details); four lexicon models; two lex-
icalized reordering models (Tillmann, 2004) aim-
ing at predicting the orientation of the next transla-
tion unit; a ?weak? distance-based distortion model;
and finally a word-bonus model and a tuple-bonus
model which compensate for the system preference
for short translations. The four lexicon models are
similar to the ones used in a standard phrase-based
system: two scores correspond to the relative fre-
quencies of the tuples and two lexical weights are
estimated from the automatically generated word
alignments. The weights associated to feature func-
tions are optimally combined using a discriminative
training framework (Och, 2003) (Minimum Error
Rate Training (MERT), see details in Section 5.4),
using the provided newstest2009 data as develop-
ment set.
2.1 Training
Our translation model is estimated over a training
corpus composed of tuple sequences using classi-
cal smoothing techniques. Tuples are extracted from
a word-aligned corpus (using MGIZA++2 with de-
fault settings) in such a way that a unique segmenta-
tion of the bilingual corpus is achieved, allowing to
estimate the n-gram model. Figure 1 presents a sim-
ple example illustrating the unique tuple segmenta-
tion for a given word-aligned pair of sentences (top).
Figure 1: Tuple extraction from a sentence pair.
The resulting sequence of tuples (1) is further re-
fined to avoid NULL words in the source side of the
tuples (2). Once the whole bilingual training data is
segmented into tuples, n-gram language model prob-
abilities can be estimated. In this example, note that
the English source words perfect and translations
have been reordered in the final tuple segmentation,
while the French target words are kept in their orig-
inal order.
2.2 Inference
During decoding, source sentences are encoded
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, at decoding time,
only those encoded reordering hypotheses are trans-
lated. Reordering hypotheses are introduced using
a set of reordering rules automatically learned from
the word alignments.
In the previous example, the rule [perfect transla-
tions ; translations perfect] produces the swap of
the English words that is observed for the French
and English pair. Typically, part-of-speech (POS)
information is used to increase the generalization
power of such rules. Hence, rewriting rules are built
using POS rather than surface word forms. Refer
2http://geek.kyloo.net/software
310
to (Crego and Marin?o, 2007) for details on tuple ex-
traction and reordering rules.
3 Data Pre-processing and Selection
We used all the available parallel data allowed in
the constrained task to compute the word align-
ments, except for the French-English tasks where
the United Nation corpus was not used to train our
translation models. To train the target language
models, we also used all provided data and mono-
lingual corpora released by the LDC for French
and English. Moreover, all parallel corpora were
POS-tagged with the TreeTagger (Schmid, 1994).
For German, the fine-grained POS information used
for pre-processing was computed by the RFTag-
ger (Schmid and Laws, 2008).
3.1 Tokenization
We took advantage of our in-house text process-
ing tools for the tokenization and detokenization
steps (De?chelotte et al, 2008). Previous experi-
ments have demonstrated that better normalization
tools provide better BLEU scores (Papineni et al,
2002). Thus all systems are built in ?true-case.?
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which poses a number of diffi-
culties both at training and decoding time. Thus,
to translate from German to English, the German
side was normalized using a specific pre-processing
scheme (described in (Allauzen et al, 2010)), which
aims at reducing the lexical redundancy and splitting
complex compounds.
Using the same pre-processing scheme to trans-
late from English to German would require to post-
process the output to undo the pre-processing. As in
our last year?s experiments (Allauzen et al, 2010),
this pre-processing step could be achieved with a
two-step decoding. However, by stacking two de-
coding steps, we may stack errors as well. Thus, for
this direction, we used the German tokenizer pro-
vided by the organizers.
3.2 Filtering the GigaWord Corpus
The available parallel data for English-French in-
cludes a large Web corpus, referred to as the Giga-
Word parallel corpus. This corpus is very noisy, and
contains large portions that are not useful for trans-
lating news text. The first filter aimed at detecting
foreign languages based on perplexity and lexical
coverage. Then, to select a subset of parallel sen-
tences, trigram LMs were trained for both French
and English languages on a subset of the available
News data: the French (resp. English) LM was used
to rank the French (resp. English) side of the cor-
pus, and only those sentences with perplexity above
a given threshold were selected. Finally, the two se-
lected sets were intersected. In the following exper-
iments, the threshold was set to the median or upper
quartile value of the perplexity. Therefore, half (or
75%) of this corpus was discarded.
4 Target Language Modeling
Neural networks, working on top of conventional
n-gram models, have been introduced in (Bengio
et al, 2003; Schwenk, 2007) as a potential means
to improve conventional n-gram language models
(LMs). However, probably the major bottleneck
with standard NNLMs is the computation of poste-
rior probabilities in the output layer. This layer must
contain one unit for each vocabulary word. Such a
design makes handling of large vocabularies, con-
sisting of hundreds thousand words, infeasible due
to a prohibitive growth in computation time. While
recent work proposed to estimate the n-gram dis-
tributions only for the most frequent words (short-
list) (Schwenk, 2007), we explored the use of the
SOUL (Structured OUtput Layer Neural Network)
language model for SMT in order to handle vocabu-
laries of arbitrary sizes.
Moreover, in our setting, increasing the order of
standard n-gram LM did not show any significant
improvement. This is mainly due to the data spar-
sity issue and to the drastic increase in the number of
parameters that need to be estimated. With NNLM
however, the increase in context length at the input
layer results in only a linear growth in complexity
in the worst case (Schwenk, 2007). Thus, training
longer-context neural network models is still feasi-
ble, and was found to be very effective in our system.
311
4.1 Standard n-gram Back-off Language
Models
To train our language models, we assumed that the
test set consisted in a selection of news texts dat-
ing from the end of 2010 to the beginning of 2011.
This assumption was based on what was done for
the 2010 evaluation. Thus, for each language, we
built a development corpus in order to optimize the
vocabulary and the target language model.
Development set and vocabulary In order to
cover different periods, two development sets were
used. The first one is newstest2008. This corpus is
two years older than the targeted time period; there-
fore, a second development corpus named dev2010-
2011 was collected by randomly sampling bunches
of 5 consecutive sentences from the provided news
data of 2010 and 2011.
To estimate such large LMs, a vocabulary
was first defined for each language by including
all tokens observed in the Europarl and News-
Commentary corpora. For French and English, this
vocabulary was then expanded with all words that
occur more than 5 times in the French-English Gi-
gaWord corpus, and with the most frequent proper
names taken from the monolingual news data of
2010 and 2011. As for German, since the amount
of training data was smaller, the vocabulary was ex-
panded with the most frequent words observed in the
monolingual news data of 2010 and 2011. This pro-
cedure resulted in a vocabulary containing around
500k words in each language.
Language model training All the training data al-
lowed in the constrained task were divided into sev-
eral sets based on dates or genres (resp. 9 and 7
sets for English and French). On each set, a stan-
dard 4-gram LM was estimated from the 500k words
vocabulary using absolute discounting interpolated
with lower order models (Kneser and Ney, 1995;
Chen and Goodman, 1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 were first linearly interpolated.
The associated coefficients were estimated so as to
minimize the perplexity evaluated on dev2010-2011.
The resulting LM and the 2010-2011 LM were fi-
naly interpolated with newstest2008 as development
data. This procedure aims to avoid overestimating
the weight associated to the 2010-2011 LM.
4.2 The SOUL Model
We give here a brief overview of the SOUL LM;
refer to (Le et al, 2011) for the complete training
procedure. Following the classical work on dis-
tributed word representation (Brown et al, 1992),
we assume that the output vocabulary is structured
by a clustering tree, where each word belongs to
only one class and its associated sub-classes. If wi
denotes the i-th word in a sentence, the sequence
c1:D(wi) = c1, . . . ,cD encodes the path for the word
wi in the clustering tree, with D the depth of the tree,
cd(wi) a class or sub-class assigned to wi, and cD(wi)
the leaf associated with wi (the word itself). The
n-gram probability of wi given its history h can then
be estimated as follows using the chain rule:
P(wi|h) = P(c1(wi)|h)
D
?
d=2
P(cd(wi)|h,c1:d?1)
Figure 2 represents the architecture of the NNLM
to estimate this distribution, for a tree of depth
D = 3. The SOUL architecture is the same as for
the standard model up to the output layer. The
main difference lies in the output structure which in-
volves several layers with a softmax activation func-
tion. The first softmax layer (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class proba-
bilities P(cd(wi)|h,c1:d?1). Finally, the word layers
estimate the word probabilities P(cD(wi)|h,c1:D?1).
Words in the short-list are a special case since each
of them represents its own class without any sub-
classes (D = 1 in this case).
5 Experimental Results
The experimental results are reported in terms of
BLEU and translation edit rate (TER) using the
newstest2010 corpus as evaluation set. These auto-
matic metrics are computed using the scripts pro-
vided by the NIST after a detokenization step.
5.1 English-French
Compared with last year evaluation, the amount of
available parallel data has drastically increased with
about 33M of sentence pairs. It is worth noticing
312
wi-1
w
i-2
w
i-3
R
R
R
W
ih
 shared context space
input layer
hidden layer:
tanh activation
word layers
sub-class 
layers
}
short list
Figure 2: Architecture of the Structured Output Layer
Neural Network language model.
that the provided corpora are not homogeneous, nei-
ther in terms of genre nor in terms of topics. Never-
theless, the most salient difference is the noise car-
ried by the GigaWord and the United Nation cor-
pora. The former is an automatically collected cor-
pus drawn from different websites, and while some
parts are indeed relevant to translate news texts, us-
ing the whole GigaWord corpus seems to be harm-
ful. The latter (United Nation) is obviously more
homogeneous, but clearly out of domain. As an il-
lustration, discarding the United Nation corpus im-
proves performance slightly.
Table 1 summarizes some of our attempts at deal-
ing with such a large amount of parallel data. As
stated above, translation models are trained with
the news-commentary, Europarl, and GigaWord cor-
pora. For this last data set, results show the reward of
sentence pair selection as described in Section 3.2.
Indeed, filtering out 75% of the corpus yields to
a significant BLEU improvement when translating
from English to French and of 1 point in the other
direction (line upper quartile in Table 1). More-
over, a larger selection (50% in the median line) still
increases the overall performance. This shows the
room left for improvement by a more accurate data
selection process such as a well optimized thresh-
old in our approach, or a more sophisticated filtering
strategy (see for example (Foster et al, 2010)).
Another issue when using such a large amount
System en2fr fr2en
BLEU TER BLEU TER
All 27.4 56.6 26.8 55.0
Upper quartile 27.8 56.3 28.4 53.8
Median 28.1 56.0 28.6 53.5
Table 1: English-French translation results in terms of
BLEU score and TER estimated on newstest2010 with
the NIST script. All means that the translation model is
trained on news-commentary, Europarl, and the whole
GigaWord. The rows upper quartile and median corre-
spond to the use of a filtered version of the GigaWord.
of data is the mismatch between the target vocab-
ulary derived from the translation model and that of
the LM. The translation model may generate words
which are unknown to the LM, and their probabili-
ties could be overestimated. To avoid this behaviour,
the probability of unknown words for the target LM
is penalized during the decoding step.
5.2 English-German
For this translation task, we compare the impact of
two different POS-taggers to process the German
part of the parallel data. The results are reported
in Table 2. Results show that to translate from En-
glish to German, the use of a fine-grained POS infor-
mation (RFTagger) leads to a slight improvement,
whereas it harms the source reordering model in the
other direction. It is worth noticing that to translate
from German to English, the RFTagger is always
used during the data pre-processing step, while a dif-
ferent POS tagger may be involved for the source
reordering model training.
System en2de de2en
BLEU TER BLEU TER
RFTagger 22.8 60.1 16.3 66.0
TreeTagger 23.1 59.4 16.2 66.0
Table 2: Translation results in terms of BLEU score
and translation edit rate (TER) estimated on newstest2010
with the NIST scoring script.
5.3 The SOUL Model
As mentioned in Section 4.2, the order of a con-
tinuous n-gram model such as the SOUL LM can
be raised without a prohibitive increase in complex-
ity. We summarize in Table 3 our experiments with
313
SOUL LMs of orders 4, 6, and 10. The SOUL LM
is introduced in the SMT pipeline by rescoring the
n-best list generated by the decoder, and the asso-
ciated weight is tuned with MERT. We observe for
the English-French task: a BLEU improvement of
0.3, as well as a similar trend in TER, when intro-
ducing a 4-gram SOUL LM; an additional BLEU
improvement of 0.3 when increasing the order from
4 to 6; and a less important gain with the 10-gram
SOUL LM. In the end, the use of a 10-gram SOUL
LM achieves a 0.7 BLEU improvement and a TER
decrease of 0.8. The results on the English-German
task show the same trend with a 0.5 BLEU point
improvement.
SOUL LM en2fr en2de
BLEU TER BLEU TER
without 28.1 56.0 16.3 66.0
4-gram 28.4 55.5 16.5 64.9
6-gram 28.7 55.3 16.7 64.9
10-gram 28.8 55.2 16.8 64.6
Table 3: Translation results from English to French and
English to German measured on newstest2010 using a
100-best rescoring with SOUL LMs of different orders.
5.4 Optimization Issues
Along with MIRA (Margin Infused Relaxed Al-
gorithm) (Watanabe et al, 2007), MERT is the
most widely used algorithm for system optimiza-
tion. However, standard MERT procedure is known
to suffer from instability of results and very slow
training cycle with approximate estimates of one de-
coding cycle for each training parameter. For this
year?s evaluation, we experimented with several al-
ternatives to the standard n-best MERT procedure,
namely, MERT on word lattices (Macherey et al,
2008) and two differentiable variants to the BLEU
objective function optimized during the MERT cy-
cle. We have recast the former in terms of a spe-
cific semiring and implemented it using a general-
purpose finite state automata framework (Sokolov
and Yvon, 2011). The last two approaches, hereafter
referred to as ZHN and BBN, replace the BLEU
objective function, with the usual BLEU score on
expected n-gram counts (Rosti et al, 2010) and
with an expected BLEU score for normal n-gram
counts (Zens et al, 2007), respectively. All expecta-
tions (of the n-gram counts in the first case and the
BLEU score in the second) are taken over all hy-
potheses from n-best lists for each source sentence.
Experiments with the alternative optimization
methods achieved virtually the same performance in
terms of BLEU score, but 2 to 4 times faster. Neither
approach, however, showed any consistent and sig-
nificant improvement for the majority of setups tried
(with the exception of the BBN approach, that had
almost always improved over n-best MERT, but for
the sole French to English translation direction). Ad-
ditional experiments with 9 complementary transla-
tion models as additional features were performed
with lattice-MERT, but neither showed any substan-
tial improvement. In the view of these rather incon-
clusive experiments, we chose to stick to the classi-
cal MERT for the submitted results.
6 Conclusion
In this paper, we described our submissions to
WMT?11 in the French-English and German-
English shared translation tasks, in both directions.
For this year?s participation, we only used n-code,
our open source Statistical Machine Translation sys-
tem based on bilingual n-grams. Our contributions
are threefold. First, we have shown that n-gram
based systems can achieve state-of-the-art perfor-
mance on large scale tasks in terms of automatic
metrics such as BLEU. Then, as already shown by
several sites in the past evaluations, there is a signifi-
cant reward for using data selection algorithms when
dealing with large heterogeneous data sources such
as the GigaWord. Finally, the use of a large vocab-
ulary continuous space language model such as the
SOUL model has enabled to achieve significant and
consistent improvements. For the upcoming evalua-
tion(s), we would like to suggest that the important
work of data cleaning and pre-processing could be
shared among all the participants instead of being
done independently several times by each site. Re-
ducing these differences could indeed help improve
the reliability of SMT systems evaluation.
Acknowledgment
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
314
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
P.F. Brown, P.V. de Souza, R.L. Mercer, V.J. Della Pietra,
and J.C. Lai. 1992. Class-based n-gram models of nat-
ural language. Computational Linguistics, 18(4):467?
479.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Josep Maria Crego and Jose? Bernardo Marin?o. 2007. Im-
proving statistical MT by coupling reordering and de-
coding. Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2011), Prague (Czech Republic),
22-27 May.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. of the Conf. on EMNLP, pages 725?734.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 321?326,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Holger Schwenk. 2007. Continuous space language
models. Computer, Speech & Language, 21(3):492?
518.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the 15th
Annual Conference of the European Association for
Machine Translation, EAMT?2011, May.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 524?
532.
315
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358?364,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joint WMT Submission of the QUAERO Project
?Markus Freitag, ?Gregor Leusch, ?Joern Wuebker, ?Stephan Peitz, ?Hermann Ney,
?Teresa Herrmann, ?Jan Niehues, ?Alex Waibel,
?Alexandre Allauzen, ?Gilles Adda,?Josep Maria Crego,
?Bianka Buschbeck, ?Tonio Wandmacher, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2011 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems. Then RWTH system combination
combines these translations to a better one. In
this paper, we describe the single systems of
each group. Before we present the results of
the system combination, we give a short de-
scription of the RWTH Aachen system com-
bination approach.
1 Overview
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
1.1 Data Sets
For WMT 2011 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned on
the newstest2009 dev set. The newstest2008 dev set
was used to train the system combination parame-
ters. Finally the newstest2010 dev set was used to
compare the results of the different system combi-
nation approaches and settings.
2 Translation Systems
2.1 RWTH Aachen Single Systems
For the WMT 2011 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
bilingual corpus, the translation probabilities are es-
timated by relative frequencies. The standard feature
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. Parameters are optimized with the Downhill-
Simplex algorithm (Nelder and Mead, 1965) on the
word graph.
358
2.1.2 Hierarchical System
For the hierarchical setups described in this pa-
per, the open source Jane toolkit (Vilar et al, 2010)
is employed. Jane has been developed at RWTH
and implements the hierarchical approach as intro-
duced by Chiang (2007) with some state-of-the-art
extensions. In hierarchical phrase-based translation,
a weighted synchronous context-free grammar is in-
duced from parallel text. In addition to contiguous
lexical phrases, hierarchical phrases with up to two
gaps are extracted. The search is typically carried
out using the cube pruning algorithm (Huang and
Chiang, 2007). The model weights are optimized
with standard MERT (Och, 2003) on 100-best lists.
2.1.3 Phrase Model Training
For some PBT systems a forced alignment pro-
cedure was applied to train the phrase translation
model as described in Wuebker et al (2010). A
modified version of the translation decoder is used
to produce a phrase alignment on the bilingual train-
ing data. The phrase translation probabilities are es-
timated from their relative frequencies in the phrase-
aligned training data. In addition to providing a sta-
tistically well-founded phrase model, this has the
benefit of producing smaller phrase tables and thus
allowing more rapid and less memory consuming
experiments with a better translation quality.
2.1.4 Final Systems
For the German?English task, RWTH conducted
experiments comparing the standard phrase extrac-
tion with the phrase training technique described in
Section 2.1.3. Further experiments included the use
of additional language model training data, rerank-
ing of n-best lists generated by the phrase-based sys-
tem, and different optimization criteria.
A considerable increase in translation quality can
be achieved by application of German compound
splitting (Koehn and Knight, 2003). In comparison
to standard heuristic phrase extraction techniques,
performing force alignment phrase training (FA)
gives an improvement in BLEU on newstest2008
and newstest2009, but a degradation in TER. The
addition of LDC Gigaword corpora (+GW) to the
language model training data shows improvements
in both BLEU and TER. Reranking was done on
1000-best lists generated by the the best available
system (PBT (FA)+GW). Following models were
applied: n-gram posteriors (Zens and Ney, 2006),
sentence length model, a 6-gram LM and IBM-1 lex-
icon models in both normal and inverse direction.
These models are combined in a log-linear fashion
and the scaling factors are tuned in the same man-
ner as the baseline system (using TER?4BLEU on
newstest2009).
The final table includes two identical Jane sys-
tems which are optimized on different criteria. The
one optimized on TER?BLEU yields a much lower
TER.
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogeneous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation. Op-
timization with regard to the BLEU score is done
using Minimum Error Rate Training as described
by Venugopal et al (2005). The translation model
is trained on the Europarl and News Commentary
Corpus and the phrase table is based on a GIZA++
Word Alignment. We use two 4-gram SRI language
models, one trained on the News Shuffle corpus and
one trained on the Gigaword corpus. Reordering is
performed based on continuous and non-continuous
POS rules to cover short and long-range reorder-
ings. The long-range reordering rules were also ap-
plied to the training corpus and phrase extraction
was performed on the resulting reordering lattices.
Part-of-speech tags are obtained using the TreeTag-
1http://hunspell.sourceforge.net/
359
ger (Schmid, 1994). In addition, the system applies
a bilingual language model to extend the context of
source language words available for translation. The
individual models are described briefly in the fol-
lowing.
2.2.3 POS-based Reordering Model
We use a reordering model that is based on parts-
of-speech (POS) and learn probabilistic rules from
the POS tags of the words in the training corpus and
the alignment information. In addition to continu-
ous reordering rules that model short-range reorder-
ing (Rottmann and Vogel, 2007), we apply non-
continuous rules to address long-range reorderings
as typical for German-English translation (Niehues
and Kolss, 2009). The reordering rules are applied
to the source sentences and the reordered sentence
variants as well as the original sequence are encoded
in a word lattice which is used as input to the de-
coder.
2.2.4 Lattice Phrase Extraction
For the test sentences, the POS-based reordering
allows us to change the word order in the source sen-
tence so that the sentence can be translated more eas-
ily. If we apply this also to the training sentences, we
would be able to extract also phrase pairs for origi-
nally discontinuous phrases and could apply them
during translation of reordered test sentences.
Therefore, we build reordering lattices for all
training sentences and then extract phrase pairs from
the monotone source path as well as from the re-
ordered paths. To limit the number of extracted
phrase pairs, we extract a source phrase only once
per sentence, even if it is found in different paths and
we only use long-range reordering rules to generate
the lattices for the training corpus.
2.2.5 Bilingual Language Model
In phrase-based systems the source sentence is
segmented by the decoder during the search pro-
cess. This segmentation into phrases leads to the
loss of context information at the phrase boundaries.
The language model can make use of more target
side context. To make also source language context
available we use a bilingual language model, an ad-
ditional language model in the phrase-based system
in which each token consist of a target word and all
source words it is aligned to. The bilingual tokens
enter the translation process as an additional target
factor.
2.3 LIMSI-CNRS Single System
2.3.1 System overview
The LIMSI system is built with n-code2, an open
source statistical machine translation system based
on bilingual n-grams.
2.3.2 n-code Overview
In a nutshell, the translation model is im-
plemented as a stochastic finite-state transducer
trained using a n-gram model of (source,target)
pairs (Casacuberta and Vidal, 2004). Training this
model requires to reorder source sentences so as to
match the target word order. This is performed by a
stochastic finite-state reordering model, which uses
part-of-speech information3 to generalize reordering
patterns beyond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a weak
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
use in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 data as development
set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2http://www.limsi.fr/Individu/jmcrego/n-code
3Part-of-speech information for English and German is com-
puted using the TreeTagger.
360
2.3.3 Data Preprocessing
Based on previous experiments which have
demonstrated that better normalization tools provide
better BLEU scores (K. Papineni and Zhu, 2002),
all the English texts are tokenized and detokenized
with in-house text processing tools (De?chelotte et
al., 2008). For German, the standard tokenizer sup-
plied by evaluation organizers is used.
2.3.4 Target n-gram Language Models
The English language model is trained assuming
that the test set consists in a selection of news texts
dating from the end of 2010 to the beginning of
2011. This assumption is based on what was done
for the 2010 evaluation. Thus, a development cor-
pus is built in order to create a vocabulary and to
optimize the target language model.
Development Set and Vocabulary In order to
cover different period, two development sets are
used. The first one is newstest2008. However, this
corpus is two years older than the targeted time pe-
riod. Thus a second development corpus is gath-
ered by randomly sampling bunches of 5 consecu-
tive sentences from the provided news data of 2010
and 2011.
To estimate a LM, the English vocabulary is first
defined by including all tokens observed in the Eu-
roparl and news-commentary corpora. This vocabu-
lary is then expanded with all words that occur more
that 5 times in the French-English giga-corpus, and
with the most frequent proper names taken from the
monolingual news data of 2010 and 2011. This pro-
cedure results in a vocabulary around 500k words.
Language Model Training All the training data
allowed in the constrained task are divided into 9
sets based on dates on genres. On each set, a
standard 4-gram LM is estimated from the 500k
word vocabulary with in-house tools using abso-
lute discounting interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 are first linearly interpolated.
The associated coefficients are estimated so as to
minimize the perplexity evaluated on the dev2010-
2011. The resulting LM and the 2010-2011 LM are
finally interpolated with newstest2008 as develop-
ment data. This two steps interpolation aims to avoid
an overestimate of the weight associated to the 2010-
2011 LM.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
the SYSTRAN baseline system in combination with
a statistical post editing (SPE) component.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k ?
800k entries per language pair).
The basic setup of the SPE component is identi-
cal to the one described in (L. Dugast and Koehn,
2007). A statistical translation model is trained on
the rule-based translation of the source and the tar-
get side of the parallel corpus. This is done sepa-
rately for each parallel corpus. Language models are
trained on each target half of the parallel corpora and
also on additional in-domain corpora. Moreover, the
following measures ? limiting unwanted statistical
effects ? were applied:
? Named entities are replaced by special tokens
on both sides. This usually improves word
alignment, since the vocabulary size is signif-
icantly reduced. In addition, entity translation
is handled more reliably by the rule-based en-
gine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus (whose target is identical
to the source). This was added to the parallel
text in order to improve word alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
361
? Phrase pairs appearing less than 2 times were
pruned.
The SPE language model was trained 15M
phrases from the news/europarl corpora, provided
as training data for WMT 2011. Weights for these
separate models were tuned by the MERT algorithm
provided in the Moses toolkit (P. Koehn et al, 2007),
using the provided news development set.
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical mod-
els is selected as consensus translation. A deeper
description will be also given in the WMT11 sys-
tem combination paper of RWTH Aachen Univer-
sity. For this task only the A2L framework has been
used.
4 Experiments
We tried different system combinations with differ-
ent sets of single systems and different optimiza-
tion criteria. As RWTH has two different transla-
tion systems, we put the output of both systems into
system combination. Although both systems have
the same preprocessing, their hypotheses differ. Fi-
nally, we added for both RWTH systems two addi-
tional hypotheses to the system combination. The
two hypotheses of Jane were optimized on differ-
ent criteria. The first hypothesis was optimized on
BLEU and the second one on TER?BLEU. The first
RWTH phrase-based hypothesis was generated with
force alignment, the second RWTH phrase-based
hypothesis is a reranked version of the first one as
described in 2.1.4. Compared to the other systems,
the system by SYSTRAN has a completely different
approach (see section 2.4). It is mainly based on a
rule-based system. For the German?English pair,
SYSTRAN achieves a lower BLEU score in each
test set compared to the other groups. But since the
SYSTRAN system is very different to the others, we
still obtain an improvement when we add it also to
system combination.
We obtain the best result from system combina-
tion of all seven systems, optimizing the parameters
on BLEU. This system was the system we submitted
to the WMT 2011 evaluation.
For each dev set we obtain an improvement com-
pared to the best single systems. For newstest2008
and newstest2009 we get an improvement of 0.5
points in BLEU and 1.8 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. For newstest2010 we get an improvement
of 1.8 points in BLEU and 2.7 points in TER com-
pared to the best single system of RWTH. The sys-
tem combination weights optimized for the best run
are listed in Table 2. We see that although the single
system of SYSTRAN has the lowest BLEU scores,
it gets the second highest system weight. This high
value shows the influence of a completely different
system. On the other hand, all RWTH systems are
very similar, because of their same preprocessing
and their small variations. Therefor the system com-
bination parameter of all four systems by themselves
are relatively small. The summarized ?RWTH ap-
proach? system weight, though, is again on par with
the other systems.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU
compared to each single system. If the system
combination implementation can handle enough sin-
gle systems we would recommend to add all single
systems to the system combination. Although the
single system of SYSTRAN has the lowest BLEU
scores and the RWTH single systems are similar we
achieved the best result in using all single systems.
362
newstest2008 newstest2009 newstest2010 description
BLEU TER BLEU TER BLEU TER
22.73 60.73 22.50 59.82 25.26 57.37 sc (all systems) BLEU opt
22.61 60.60 22.28 59.39 25.07 56.95 sc (all systems - (1)) TER?BLEU opt
22.50 60.41 22.52 59.61 25.23 57.40 sc (all systems) TER?BLEU opt
22.19 60.09 22.05 59.31 24.74 56.89 sc (all systems - (4)) TER?BLEU opt
22.21 60.71 21.89 59.95 24.72 57.58 sc (all systems - (4,7)) TER?BLEU opt
22.22 60.45 21.79 59.72 24.32 57.59 sc (all systems - (3,4)) TER?BLEU opt
22.27 60.60 21.75 59.92 24.35 57.64 sc (all systems - (3,4)) BLEU opt
22.10 62.59 22.01 61.64 23.34 60.35 (1) Karlsruhe Institute of Technology
21.41 62.77 21.12 61.91 23.44 60.06 (2) RWTH PBT (FA) rerank +GW
21.11 62.96 21.06 62.16 23.29 60.26 (3) RWTH PBT (FA)
21.47 63.89 21.00 63.33 22.93 61.71 (4) RWTH jane + GW BLEU opt
20.89 61.05 20.36 60.47 23.42 58.31 (5) RWTH jane + GW TER?BLEU opt
20.33 64.50 19.79 64.91 21.97 61.44 (6) Limsi-CNRS
17.06 69.48 17.52 67.34 18.68 66.37 (7) SYSTRAN Software
Table 1: All systems for the WMT 2011 German?English translation task (truecase). BLEU and TER results are in
percentage. FA denotes systems with phrase training, +GW the use of LDC data for the language model. sc denotes
system combination.
system weight
Karlsruhe Institute of Technology 0.350
RWTH PBT (FA) rerank +GW 0.001
RWTH PBT (FA) 0.046
RWTH jane + GW BLEU opt 0.023
RWTH jane + GW TER?BLEU opt 0.034
Limsi-CNRS 0.219
SYSTRAN Software 0.328
Table 2: Optimized systems weights for each system of the best system combination result.
Acknowledgments
This work was achieved as part of the QUAERO
Programme, funded by OSEO, French State agency
for innovation.
References
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
S.F. Chen and J.T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
T. Ward K. Papineni, S. Roukos and W. Zhu. 2002. Bleu:
363
a method for automatic evaluation of machine transla-
tion. In ACL ?02: Proc. of the 40th Annual Meeting
on Association for Computational Linguistics, pages
311?318. Association for Computational Linguistics.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
J.A. Nelder and R. Mead. 1965. The Downhill Simplex
Method. Computer Journal, 7:308.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
J. Wuebker, A. Mauser, and H. Ney. 2010. Training
Phrase Translation Models with Leaving-One-Out. In
Proceedings of the 48th Annual Meeting of the Assoc.
for Computational Linguistics, pages 475?484, Upp-
sala, Sweden, July.
R. Zens and H. Ney. 2006. N-gram Posterior Proba-
bilities for Statistical Machine Translation. In Human
Language Technology Conf. / North American Chap-
ter of the Assoc. for Computational Linguistics Annual
Meeting (HLT-NAACL), Workshop on Statistical Ma-
chine Translation, pages 72?77, New York City, June.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
364
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 542?553,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
From n-gram-based to CRF-based Translation Models
Thomas Lavergne Josep Maria Crego
LIMSI/CNRS
BP 133
F-91 403 Orsay Ce?dex
{lavergne,jmcrego}@limsi.fr
Alexandre Allauzen Franc?ois Yvon
LIMSI/CNRS & Uni. Paris Sud
BP 133
F-91 403 Orsay Ce?dex
{allauzen,yvon}@limsi.fr
Abstract
A major weakness of extant statistical ma-
chine translation (SMT) systems is their lack
of a proper training procedure. Phrase extrac-
tion and scoring processes rely on a chain of
crude heuristics, a situation judged problem-
atic by many. In this paper, we recast the ma-
chine translation problem in the familiar terms
of a sequence labeling task, thereby enabling
the use of enriched feature sets and exact train-
ing and inference procedures. The tractabil-
ity of the whole enterprise is achieved through
an efficient implementation of the conditional
random fields (CRFs) model using a weighted
finite-state transducers library. This approach
is experimentally contrasted with several con-
ventional phrase-based systems.
1 Introduction
A weakness of existing phrase-based SMT systems,
that has been repeatedly highlighted, is their lack
of a proper training procedure. Attempts to de-
sign probabilistic models of phrase-to-phrase align-
ments (e.g. (Marcu and Wong, 2002)) have thus far
failed to overcome the related combinatorial prob-
lems (DeNero and Klein, 2008) and/or to yield im-
proved training heuristics (DeNero et al, 2006).
Phrase extraction and scoring thus rely on a chain
of heuristics see (Koehn et al, 2003), which evolve
phrase alignments from ?symmetrized? word-to-
word alignments obtained with IBM models (Brown
et al, 1990) and the like (Liang et al, 2006b; Deng
and Byrne, 2006; Ganchev et al, 2008). Phrase
scoring is also mostly heuristic and relies on an op-
timized interpolation of several simple frequency-
based scores. Overall, the training procedure of
translation models within conventional phrase-based
(or hierarchical) systems is generally considered un-
satisfactory and the design of better estimation pro-
cedures remains an active research area (Wuebker et
al., 2010).
To overcome the NP-hard problems that derive
from the need to consider all possible permutations
of the source sentence, we make here a radical
simplification and consider training the translation
model given a fixed segmentation and reordering.
This idea is not new, and is one of the grounding
principle of n-gram-based approaches (Casacuberta
and Vidal, 2004; Marin?o et al, 2006) in SMT. The
novelty here is that we will use this assumption to re-
cast machine translation (MT) in the familiar terms
of a sequence labeling task.
This reformulation allows us to make use of the
efficient training and inference tools that exists for
such tasks, most notably linear CRFs (Lafferty et
al., 2001; Sutton and McCallum, 2006). It also en-
ables to easily integrate linguistically informed (de-
scribing morphological or morpho-syntactical prop-
erties of phrases) and/or contextual features into the
translation model. In return, in addition to having
a better trained model, we also expect (i) to make
estimation less sensible to data sparsity issues and
(ii) to improve the ability of our system to make
the correct lexical choices based on the neighbor-
ing source words. As explained in Section 2, this
reformulation borrows much from the general ar-
chitecture of n-gram MT systems and implies to
solve several computational challenges. In our ap-
542
proach, the tractability of the whole enterprise is
achieved through an efficient reimplementation of
CRFs using a public domain library for weighted
finite-state transducers (WFSTs) (see details in Sec-
tion 3). This approach is experimentally contrasted
with more conventional n-gram based and phrase-
based approaches on a standard benchmark in Sec-
tion 4, where we also evaluate the benefits of various
feature sets and training regimes. We finally relate
our new system with alternative proposals for train-
ing discriminatively SMT systems in Section 5, be-
fore drawing some lessons and discussing possible
extensions of this work.
The main contribution of this work are thus (i) a
detailed presentation of the CRF in translation in-
cluding all necessary implementation details and (ii)
an experimental study of various feature functions
and of various ways to integrate target side LM in-
formation.
2 MT as sequence labeling
In this section, we briefly review the n-gram based
approach to SMT, originally introduced in (Casacu-
berta and Vidal, 2004; Marin?o et al, 2006), which
constitutes our starting point. We then describe our
new proposal, which, in essence, consists in replac-
ing the modeling of compound source-target trans-
lation units by a conditional model where the prob-
ability of each target side phrase is conditioned on
the source sentence.
2.1 The n-gram based approach in SMT
The n-gram based approach of (Marin?o et al, 2006)
is a variation of the standard phrase-based model,
characterized by the peculiar form of the translation
model. In this approach, the translation model is
based on bilingual units called tuples. Tuples are
the analogous of phrase pairs, as they represent a
matching u = (e, f) between a source f and a tar-
get e word sequence. The probability of a sequence
of tuples is computed using a conventional n-gram
model as:
p(u1 . . . uI) =
I?
i=1
p(ul|ui?1 . . . ui?n+1).
The probability of a sentence pair (f , e) is then ei-
ther recovered by marginalization, or approximated
by maximization, over all possible joint segmenta-
tions of f and e into tuples.
As for any n-gram model, the parameters are es-
timated using statistics collected in a training corpus
made of sequences of tuples derived from the par-
allel sentences in a two step process. First, a word
alignment is computed using a standard alignment
pipeline1 based on the IBM models. Source words
are then reordered so as to disentangle the align-
ment links and to synchronize the source and tar-
get texts. Special care has to be paid to non-aligned
source words, which have to be collapsed with their
neighbor words. A byproduct of this process is a de-
terministic joint segmentation of parallel sentences
into minimal bilingual units, the tuples, that consti-
tute the basic elements in the model. This process is
illustrated on Figure 1, where the unfolding process
enables the extraction of tuples such as: (demanda,
said ) or (de nouveau, again).
f : demanda de nouveau la femme voile?e
e: the veiled dame said again
f? : la voile?e femme demanda de nouveau
Figure 1: The tuple extraction process
The original (top) and reordered (bottom) French
sentence aligned with its translation.
At test time, the source text is reordered so as
to match the reordering implied by the disentangle-
ment procedure. Various proposals has been made
to perform such source side reordering (Collins et
al., 2005; Xia and McCord, 2004), or even learn-
ing reordering rules based on syntactic or morpho-
syntactic information (Crego and Marin?o, 2007).
The latter approach amounts to accumulate reorder-
ing patterns during the training; test source sen-
tences are then non-deterministically reordered in
all possible ways yielding a word graph. This graph
is then monotonously decoded, where the score of
a translation hypothesis combines information from
the translation models as well as from other infor-
mation sources (lexicalized reordering model, target
1Here, using the MGIZA++ package (Gao and Vogel, 2008).
543
side language model (LM), word and phrase penal-
ties, etc).
2.2 Translating with CRFs
A discriminative version of the n-gram approach
consists in modeling P (e|f) instead of P (e, f),
which can be efficiently performed with CRFs (Laf-
ferty et al, 2001; Sutton and McCallum, 2006). As-
suming matched sequences of observations (x =
xL1 ) and labels (y = y
L
1 ), CRFs express the con-
ditional probability of labels as:
P (yL1 |x
L
1 ) =
1
Z(xL1 ; ?)
exp(?TG(xL1 , y
L
1 )),
where ? is a parameter vector and G denotes a vec-
tor of feature functions testing various properties of
x and y. In the linear-chain CRF, each compo-
nent Gk(xI1, y
I
1) of G is decomposed as a sum of
local features: Gk(xI1, y
I
1) =
?
i gk(x
I
1, yi?1, yi)
2.
CRFs are trained by maximizing the (penalized) log-
likelihood of a corpus containing observations and
their labels.
In principle, the data used to train n-gram trans-
lation models provide all the necessary information
required to train a CRF3. It suffices to consider that
the alphabet of possible observations ranges over all
possible source side fragments, and that each tar-
get side of a tuple is a potential label. The model
thus defines the probability of a segmented target
e? = e?I1 given the segmented and reordered source
sentence f? = f? I1 . To complete the model, one just
needs to define a distribution over source segmen-
tations P (f? |f). Given the deterministic relationship
between e and e? expressed by the ?unsegmentation?
function ? which maps e? with e = ?(e?), we then
have:
P (e|f) =
?
f? ,e|?(e)=e
P (e?, f? |f)
=
?
f? ,e|?(e)=e
P (e?, |f? , f)P (f? |f)
=
?
f? ,e|?(e)=e
P (e?, |f?)P (f? |f)
2Assuming first order dependencies.
3This is a significant difference with (Blunsom et al, 2008),
as we do not need to introduce latent variables during training.
In practice, we will only consider a restricted
number of possible segmentation/reorderings of the
source, denoted L(f), and compute the best transla-
tion e? as ?(e??), where:
e?? = arg max
e
P (e?|f)
? arg max
f??L(f),e
P (e?, |f? , f)P (f? |f) (1)
Even with these simplifying assumptions, this
approach raises several challenging computational
problems. First, training a CRF is quadratic in the
number of labels, of which we will have plenty (typ-
ically hundreds of thousands). A second issue is de-
coding: as we need to consider at test time a combi-
natorial number of possible source reorderings and
segmentations, we can no longer dispense with the
computation of the normalizer Z(f? ; ?) which is re-
quired to compute P (e?, f? |f) as P (f? |f)P (e?|f?) and to
compare hypotheses associated with different values
of f? . We discuss our solutions to these problems in
the next section.
3 Implementation issues
3.1 Training
Basic training The main difficulties in training are
caused by the unusually large number of labels, each
of which corresponds to a (small) sequence of target
words. Hopefully, each observation (source side tu-
ple) occurs with a very small number of different
labels. A first simplification is thus to consider that
the set of possible ?labels? e? for a source sequence
f? is limited to those that are seen in training: all
the other associations (f? , e?) are deemed impossible,
which amounts to setting the corresponding param-
eter value to ??.
A second speed-up is to enforce sparsity in the
model, through the use of a `1 regularization term
(Tibshirani, 1996): on the one hand, this greatly re-
duces the memory usage; furthermore, sparse mod-
els are also prone to various optimization of the
forward-backward computations (Lavergne et al,
2010). As discussed in (Ng, 2004; Turian et al,
2007), this feature selection strategy is well suited
to the task at hand, where the number of possible
features is extremely large. Optimization is per-
544
formed using the Rprop algorithm4 (Riedmiller and
Braun, 1993), which provides the memory efficiency
needed to cope with the very large feature sets con-
sidered here.
Training with a target language model One of
the main strength of the phrase-based ?log-linear?
models is their ability to make use of powerful
target side language models trained on very large
amounts of monolingual texts. This ability is crucial
to achieve good performance and has to be preserved
no matter the difficulties that occur when one moves
away from conventional phrase-based systems (Chi-
ang, 2005; Huang and Chiang, 2007; Blunsom and
Osborne, 2008; Ka?a?ria?inen, 2009). It thus seems
appropriate to include a LM feature function in our
model or alternatively to define:
P (e?|f?) =
1
Z(f? ; ?)
PLM (e?) exp(?
TG(f? , e?)),
where PLM is the target language model and
Z(f? ; ?) =
?
e PLM (e?) exp(?
TG(f? , e?)). Imple-
menting this approach implies to deal with the lack
of synchronization between the units of the trans-
lation models, which are variable-length (possibly
empty) tuples, and the units of the language models,
which are plain words.
In practice, this extension is implemented by per-
forming training and inference over a graph whose
nodes are not only indexed by their position and the
left target context, but also by the required n-gram
(target) history. In most cases, for small values of
n such as considered in this study, the n-gram his-
tory can be deduced from the left target tuple. The
most problematic case is when the left target tuple
is NULL, which require to copy the history from the
previous states. As a consequence, for the values of
n considered here, the impact of this extension on
the total training time is limited.
Reference reachability A recurring problem for
discriminative training approaches is reference un-
reachability (Liang et al, 2006a): this happens when
the model cannot predict the reference translation,
which means in our case that the probability of the
reference cannot be computed. In our implementa-
tion, this only happens when the reference involves
4Adapted to handle a locally non-differentiable objective.
a tuple (f? ,e?) that is too rare to be included in the
model. As a practical workaround, when this hap-
pens for a given training sentence, we make sure
to ?locally? augment the tuple dictionary with the
missing part of the reference, which is then removed
for processing the rest of the training corpus.
3.2 Inference
Our decoder is implemented as a cascade of
weighted finite-state transducers (WFSTs) using the
functionalities of the OpenFst library (Allauzen et
al., 2007). This library provides many basic opera-
tion for WFSTs, notably the left (pi1) and right (pi2)
projections as well as the composition operation (?).
The related notions and algorithms are presented in
detail in (Mohri, 2009), to which we refer the reader.
In essence, our decoder is implemented of a finite-
state cascade involoving the following steps: (i)
source reordering and segmentation (ii) application
of the translation model and (optionally) (iii) com-
position with a target side language model, an ar-
chitecture that is closely related to the proposal of
(Kumar et al, 2006). A more precise account of
these various steps is given below, where we de-
scribe the main finite-state transducers involved in
our decoder:
? S, the acceptor for the source sentence f ;
? R, which implements segmentation and re-
ordering rules;
? T , the tuple dictionary, associating source side
sequences with possible translations based on
the inventory of tuples;
? F , the feature matcher, mapping each feature
with the corresponding parameter value;
Source reordering The computation of R mainly
follows the approach of (Crego and Marin?o, 2007)
and uses a part-of-speech tagged version of the re-
ordered training data. Each reordering pattern seen
in training is generalized as a non-deterministic re-
ordering rule which expresses a possible rearrange-
ment of some subpart of the source sentence. Each
rule is implemented as an elementary finite-state
transducer, and the set of possible word reorderings
is computed as the composition of these transducers.
R is finally obtained by composing the result with a
545
transducer computing all the possible segmentations
of its input into sequences of source side tuples5.
The output of S ? R are sequences of source side
tuples f? ; each path in this transducer is addition-
ally weighted with a simplistic n-tuple segmentation
model, estimated using the source side of the paral-
lel training corpus. Note that these scores are nor-
malized, so that the weight of each path labelled f? in
S ?R is logP (f? |f).
The feature matcher F The feature matcher is
also implemented as a series of elementary weighted
transducers, each transducer being responsible for a
given class of feature functions. The simplest trans-
ducer in this family deals with the class of unigram
feature functions, ie. feature functions that only test
the current observation and label. It is represented
on the left part of Figure 3.2, where for the sake of
readability we only display one example for each
test pattern (here: an unconditional feature that al-
ways returns true for a given label, a test on the
source word, and a test on the source POS label).
As long as dependencies between source and/or tar-
get symbols remain local, they can be captured by
finite-state transducers such as the ones on the mid
and right part of Figure 3.2, which respectively com-
pute bigram target features, and joint bigram source
and target features.
The feature matcher F is computed as the com-
position of these elementary transducers, where we
only include source and target labels that can occur
given the current input sentence. Weights in F are
interpreted in the tropical semiring. exp(F ) is ob-
tained by replacing weights w in F with exp(w) in
the real semiring.
Decoding a word graph If the input segmentation
and reordering were deterministically set, meaning
that the automaton I = pi1(S ? R ? T ) would only
contain one path, decoding would amount to finding
the best path in S ?R ? T ?F . However, we need to
compute:
arg max
e
P (e?|f) = arg max
e
?
f?
P (e?, f? |f)
= arg max
e
?
f?
P (e?|f?)P (f? |f).
5When none is found, we also consider a maximal segmen-
tation into isolated words.
This requires to compare model scores for mul-
tiple source segmentations and reorderings f? , hence
to compute P (f? |f) and P (e?|f?), rather than just the
non-normalized value that is usually used in CRFs.
Computing the normalizer Z(f? ; ?) for all se-
quences in S ?R is performed efficiently using stan-
dard finite-state operations as :
D = det(pi1(pi2(S ?R) ? T ? exp(F ))).
In fact, determinization (in the real semiring) has the
effect of accumulating for each f? the corresponding
normalizer Z(f? ; ?). Replacing each weight w in D
by ? log(w) and using the log semiring enables to
compute? log(Z(f? ; ?)). The best translation is then
obtained as: bestpath(pi2(S?R)??log(D)?T ?F )
in the tropical semiring.
Decoding and Rescoring with a target language
model An alternative manner of using a (large)
target side language model is to use it for rescoring
purposes. The consistent use of finite-state machines
and operations makes it fairly easy to include one
during decoding : it suffices to perform the search in
pi2(S?R)?? log(D)?T ?F ?L, where L represents
a n-gram language model. When combining several
models, notably a source segmentation model and/or
a target language model for rescoring, we have made
sure to rescale the (log)probabilities so as to balance
the language model scores with the CRF scores, and
to use a fixed word bonus to make hypotheses of dif-
ferent length more comparable. All these parameters
are tuned as part of the decoder development pro-
cess. It is finally noteworthy that, in our architecture,
alternative decoding strategies, such as MBR (Ku-
mar and Byrne, 2004) are also readily implemented.
4 Experiments
4.1 Corpora and metrics
For these experiments, we have used a medium size
training corpus, extracted from the datasets made
available for WMT 20116 evaluation campaign, and
have focused on one translation direction, from
French to English7.
Translation model training uses the entire News-
Commentary subpart of the WMT?2011 training
6statmt.org/wmt11
7Results in the other direction suggest similar conclusions.
546
0le : the/?le,the
DET : the/?DET,the
? : the/?the 0 1
? : the/0
? : cat/?the,cat
0 1
? : the/0
chat : cat/?chat,cat
Figure 2: Feature matchers. The star symbol (*) matches any possible observation.
French English
sent? token types token types
train 115 K 3 339 K 60 K 2 816 K 58 K
test 2008 2.0 K 55 K 9 K 49 K 8 K
test 2009 2.5 K 72 K 11 K 65 K 10 K
test 2010 2.5 K 69 K 10 K 61 K 9 K
Table 1: Corpora used for the experiments
data; for language models, we have considered two
approaches (i) a ?large? bigram model highly opti-
mized using all the available monolingual data and
(ii) a ?small? trigram language model trained on
just the English side of the NewsCommentary cor-
pus. The regularization parameters used in training
are tuned using the WMT 2009 test set; the various
parameters implied in the decoding are tuned (for
BLEU) on WMT 2008 test set; the internal tests re-
ported below are performed on the 2010 test lines
(see Table 1) using the best parameters found during
tuning. Various statistics regarding these corpora are
reproduced on Table 1.
All the training corpora were aligned using
MGIZA++ with standard parameters8, and pro-
cessed in the standard tuple extraction pipeline. The
development and test corpora were also processed
analogously. For the sake of comparison, we also
trained a standard n-gram-based and a Moses sys-
tem (Koehn et al, 2007) with default parameters
and a 3-gram target LM trained using only the tar-
get side of our parallel corpus. The development set
(test 2009) was used to tune these two systems. All
performance are measured using BLEU (Papineni et
al., 2002).
8As part of a much larger batch of texts.
4.2 Features
The baseline system is composed only of transla-
tion features [trs] and target bigram features [t2g].
The former correspond to functions of the form
gus,t(f? , e?, i) = I(f?i = s ? e?i = t), where s
and t respectively denote source and target phrases
and I() is the indicator function. These are also
generalized to part-of-speech and also to any pos-
sible source phrase, giving rise to features such as
gu?,t = (f? , e?, i) = I(e?i = t). Target bigram features
correspond to functions of the form gbt,t?(f? , e?, i) =
I(e?i?1 = t? e?i = t?). The last baseline feature is the
copy feature, which fires whenever the source and
target segments are identical.
Supplementary groups of features are considered
in further stages:
? suffix/prefix features [ix]. These features allow
to generalize baseline features on the source
side to fixed length prefixes and suffixes, thus
smoothing the parameters.
? context features [ctx]. These features are sim-
ilar to unigram features, but also test the left
source tuple and the corresponding part-of-
speech.
? segmentation features [seg]. These features are
meant to express a preference for longer tuples
and to regulate the number of target words per
source word. We consider the following feature
functions (|e| denotes the length of e):
? target length features :
gl?,l(f? , e?, i) = I(|e?i| = l)
? source-target length features :
gll,l?(f? , e?, i) = I(|f?i| = l ? |e?i| = l
?)
? source-target length ratio :
gll(f? , e?, i) = I(round(
| efi|
|ei|
) = l)
547
Note that all these features are further condi-
tioned on the target label.
? reordering features [ord]. These features are
meant to model preferences for specific lo-
cal reordering patterns and take into account
neighbor source fragments in e? together with
the current label. Each source side segment
f?i is made of some source words that, prior
to source reordering, were located at indices
i1 . . . il, so that f?i = fi1 . . . fil . The high-
est (resp. lowest) index in this sequence is df?ie
(resp. bf?ic). The leftmost (resp. rightmost) in-
dex is [f?i[ (resp. ]f?i]).
Using these notations, our model includes the
following patterns:
? distortion features, measuring the gaps be-
tween consecutive source fragments :
gol,t(f? , e?, i)=I(?(f?i, e?i)= l ? e?i= t),
where ?(f?i, e?i) ={
bf?ic ? df?i?1e if (df?i?1e ? bf?ic)
df?ie ? bf?i?1c otherwise .
? lexicalized reordering, identifying mono-
tone, swap and discontinuous configura-
tions (Tillman, 2004). The monotonous
test is defined as: gom(f? , e?, i) =
I(]ei?1] = [ei[); the swap and discon-
tinuous configurations are defined analo-
gously.
? ?gappiness? test : this feature is activated
whenever the source indices i1...il contain
one or several gaps.
4.3 Experiments and lessons learned
Training time The first lesson learned is that
training can be performed efficiently. Our baseline
system, which only contains trs and trg contains ap-
proximately 87 million features, out of which a lit-
tle bit more than 600K are selected. Adding up all
supplementary features raises the number of param-
eters to about 130M features, out of which 1.5M are
found useful. All these systems require between 3
and 5 hours to train9. These numbers are obtained
with a `1 penalty term ? 1, which offers a good bal-
ance between accuracy and sparsity.
9All experiments run on a server with 64G of memory and
two Xeon processors with 4 cores at 2.27 Ghz.
Test conditions In order to better assess the
strengths and weaknesses of our approach, we com-
pare several test settings: the most favorable con-
siders only one possible segmentation/reordering f?
for each f , obtained through forced alignment with
the reference; we then consider the more challeng-
ing case where the reordering is fixed, but several
segmentations are considered; then the regular de-
coding task, where both segmentation and reorder-
ing are unknown and where the entire space of all
segmentations and reordering is searched. For each
condition, we also vary (i) the set of features used
and (ii) the target language model used, if any.
Wherever applicable, we also report contrasts with
n-gram-based systems subject to the same input and
comparable resources, varying the order of the tuple
language model, as well as with Moses. Results are
in Table 2.
dev test # feat.
decoding with optimal segmentation/reordering
CRF (trs,trg) 23.8 25.1 660K
CRF +ctx 24.1 25.4 1.5M
CRF +ix,ord,seg 24.3 25.6 1.5M
decoding with optimal reordering
n-gram (2g,3g) 20.6 24.1 755K
n-gram (3g,3g) 21.5 25.2 755K
CRF trs,trg - 22.8 660K
CRF +ctx - 23.1 1.5M
CRF +ix,ord,seg - 23.5 1.5M
regular decoding
Moses (3g) 21.2 20.5
n-gram (2g,3g) 20.6 20.2 755K
n-gram (3g,3g) 21.5 21.2 755K
CRF (trs,trg) - 18.3 660K
CRF +ctx - 18.8 1.5M
CRF +ix,ord,seg - 19.1 1.5M
CRF +ix,ord,seg+3g - 19.1 1.5M
Table 2: Translation performance
Extending the feature set As expected, the use
of increasingly complex feature sets seems benefi-
cial in all experimented conditions. It is noteworthy
that throwing in reordering and contextual features
is helping, even when decoding one single segmen-
tation and reordering. This is because these features
do not help to select the best input reordering, but
548
help choose the best target phrase.
Searching a larger space Going from the sim-
pler to the more difficult conditions yields signif-
icant degradations in the model, as our best score
drops down from 25.6 to 23.5 (with known reorder-
ing) then to 19.1 (regular decoding). This is a clear
indication that our current segmentation/reordering
model is not delivering very useful scores. A similar
loss is incurred by the n-gram system, which loses
4 bleu points between the two conditions.
LM rescoring Our results to date with target side
language models have proven inconclusive, which
might explain why our best results remain between
one and two BLEU points behind the n-gram based
system using comparable information. Note also
that preliminary experiments with incorporating a
large bigram during training have also failed to date
to provide us with improvements over the baseline.
Summary In sum, the results accumulated during
this first round of experiments tend to show that our
CRF model is still underperforming the more es-
tablished baseline by approximately 1 to 1.5 BLEU
point, when provided with comparable resources.
Sources of improvements that have been clearly
identified is the scoring of reordering and segmen-
tations, and the use of a target language model in
training and/or decoding.
5 Related work
Discriminative learning approaches have proven
successful for many NLP tasks, notably thanks to
their ability to cope with flexible linguistic repre-
sentations and to accommodate potentially redun-
dant descriptions. This is especially appealing for
machine translation, where the mapping between
a source word or phrase and its target correlate(s)
seems to involve an large array of factors, such as its
morphology, its syntactic role, its meaning, its lexi-
cal context, etc. (see eg. (Och et al, 2004; Gimpel
and Smith, 2008; Chiang et al, 2009), for inspira-
tion regarding potentially useful features in SMT).
Discriminative learning requires (i) a parameter-
ized scoring function and (ii) a training objective.
The scoring function is usually assumed to be linear
and ranks candidate outputs y for input x accord-
ing to ?TG(x, y), where ? is the parameter vector. ?
andG deterministically imply the input/output map-
ping as x ? arg maxy ?
TG(x, y). Given a set of
training pairs {xi, yi, i = 1 . . . N}, parameters are
learned by optimizing some regularized loss func-
tion of ?, so as to make the inferred input/output
mapping faithfully replicate the observed instances.
Machine translation, like most NLP tasks, does
not easily lend itself to that approach, due to the
complexity of the input/output objects (word or la-
bel strings, parse trees, dependency structures, etc).
This complexity makes inference and learning in-
tractable, as both steps imply the resolution of
the arg max problem over a combinatorially large
space of candidates y. Structured learning tech-
niques (Bakir et al, 2007), developed over the last
decade, rely on decompositions of these objects into
sub-parts as part of a derivation process, and use
conditional independence assumptions between sub-
parts to render the learning and inference problem
tractable. For machine translation, this only pro-
vides part of the solution, as the training data only
contain pairs of word aligned sentences (f , e), but
lack the explicit derivation h from f to e that is re-
quired to train the model in a fully supervised way.
The approach of (Liang et al, 2006a) circumvents
the issue by assuming that the hidden derivation h
can be approximated through forced decoding. As-
suming that h is in fact observed as the optimal
(Viterbi) derivation h? from f to e given the cur-
rent parameter value10, it is straightforward to re-
cast the training of a phrase-based system as a stan-
dard structured learning problem, thus amenable to
training algorithms such as the averaged perceptron
of (Collins, 2002). This approximation is however
not genuine, and the choice of the most appropriate
derivation seems to raises intriguing issues (Watan-
abe et al, 2007; Chiang et al, 2008).
The authors of (Blunsom et al, 2008; Blunsom
and Osborne, 2008) consider models for which it is
computationally possible to marginalize out all pos-
sible derivations of a given translation. As demon-
strated in these papers, this approach is tractable
even when the derivation process is a based on syn-
chronous context-free grammars, rather that finite-
state devices. However, the computational cost as-
10If one actually exists in the model, thus raising the issue of
reference reachability, see discussion in Section 3.
549
sociated with training and inference remains very
high, especially when using a target side language
model, which seems to preclude the application to
large-scale translation tasks11. The recent work of
(Dyer and Resnik, 2010) proceeds from a similar
vein: translation is however modeled as a two step
process, where a set of possible source reorderings,
represented as a parse forest, are associated with
possible target sentences, using, as we do, a finite-
state translation model. This translation model is
trained discriminatively by marginalizing out the
(unobserved) reordering variables; inference can be
performed effectively by intersecting the input parse
forest with a transducer representing translation op-
tions.
A third strategy is to consider a simpler class of
derivation process, which only partly describe the
mapping between f and e. This is, for instance,
the approach of (Bangalore et al, 2007), where a
simple bag-of-word representation of the target sen-
tence is computed using a battery of boolean clas-
sifiers (one for each target word). In this approach,
discriminative training is readily applicable, as the
required supervision is overtly present in example
source-target pairs (f , e); however, a complemen-
tary reshaping/reordering step is necessary to turn
the bag-of-word into a full-fledged translation. This
work was recently revisited in (Mauser et al, 2009),
where a conditional model predicting the presence
of each target phrase provides a supplementary score
for the standard ?log-linear? model.
This line of research has been continued notably
in (Ka?a?ria?inen, 2009), which introduces an exponen-
tial model of bag of phrases (allowing some over-
lap), that enables to capture localized dependencies
between target words, while preserving (to some ex-
tend) the efficiency of training and inference. Su-
pervision is here indirectly provided by word align-
ment and correlated phrase extraction processes
implemented in conventional phrase-based systems
(Koehn et al, 2003). If this model seems to deliver
state-of-the-art performance on large-scale tasks, it
does so at a very high computational cost. More-
over, for lack of an internal modeling of reordering
processes, this approach, like the bag-of-word ap-
11For instance, the experiments reported in (Blunsom and Os-
borne, 2008) use the English-Chinese BTEC, where the average
sentence length is lesser than 10.
proach, seems only appropriate for language pairs
with similar or related word ordering.
The approach developed in this paper fills a gap
between the hierarchical model of (Blunsom et
al., 2008) and the phrase-based model (Ka?a?ria?inen,
2009), with whom we share several important as-
sumptions, such as the use of alignment information
to provide supervision, and the resort to a an ?ex-
ternal?, albeit a more powerful, reordering compo-
nent. Using a finite-state model enables to process
reasonably large corpora, and gives some hopes as to
the scalability of the whole enterprise; it also makes
the integration of a target side language model much
easier than in hierarchical models.
6 Discussion and future work
In this paper, we have given detailed description of
an original phrase-based system implementing a dis-
criminative version of the n-gram model, where the
translation model probabilities are computed with
conditional random fields. We have showed how
to implement this approach using a memory effi-
cient implementation of the optimization algorithms
needed for training: in our approach, training a mid-
scale translation system with hundred of thousands
sentence pairs and millions of features only takes a
couple of hours on a standalone desktop machine.
Using `1 regularization has enabled to assess the
usefulness of various families of features.
We have also detailed a complete decoder im-
plemented as a pipeline of finite-state transducers,
which allows to efficiently combine several models,
to produce n-best lists and word lattices.
The results obtained in a series of preliminary ex-
periments show that our system is already deliver-
ing competitive translations, as acknowledged by a
comparison with two strong phrase-based baselines.
We have already started to implement various opti-
mizations and to experiment with somewhat larger
datasets (up to 500K sentence pairs) and larger fea-
ture sets, notably incorporating word sense disam-
biguation features: this work needs to be contin-
ued. In addition, we intend to explore a number
of extensions of this architecture, such as imple-
menting MBR decoding (Kumar and Byrne, 2004)
or adapting the translation model to new domains
and conditions, using, for instance, the proposal of
550
(Daume III, 2007)12.
One positive side effect of experimenting with
new translation models is that they help reevalu-
ate the performance of the whole translation system
pipeline: in particular, discriminative training seems
to be more sensible to alignments errors than the cor-
responding n-gram system, which suggests to pay
more attention to possible errors in the training data;
we have also seen that the current reordering model
defines a too narrow search space and delivers in-
sufficiently discriminant scores: we will investigate
various ways to further improve the computation and
scoring of hypothetical source reorderings.
Acknowledgements
The authors wish to thank the reviewers for com-
ments and suggestions. This work was achieved as
part of the Quaero Programme, funded by OSEO,
French State agency for innovation.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-
jciech Skut, and Mehryar Mohri. 2007. OpenFst:
A general and efficient weighted finite-state trans-
ducer library. In Proceedings of the Ninth Interna-
tional Conference on Implementation and Application
of Automata, (CIAA 2007), volume 4783 of Lecture
Notes in Computer Science, pages 11?23. Springer.
http://www.openfst.org.
Go?khan Bakir, Thomas Hofmann, Bernhard Scho?lkopf,
Alexander J.Smola, Ben Taskar, and S.V.N. Vish-
wanathan. 2007. Predicting structured output. MIT
Press.
Srinivas Bangalore, Patrick Haffner, and Stephan Kan-
thak. 2007. Statistical machine translation through
global lexical selection and sentence reconstruction.
In Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 152?159,
Prague, Czech Republic.
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings of
the 2008 Conference on Empirical Methods in Natu-
ral Language Processing, pages 215?223, Honolulu,
Hawaii.
12In a nutshell, this proposal amounts to having three dif-
ferent parameters for each feature; one parameter is trained
as usual; the other two parameters are updated conditionally,
depending whether the training instance comes from the in-
domain or from the out-domain training dataset.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proceedings of ACL-08: HLT,
pages 200?208, Columbus, Ohio.
Peter F. Brown, John Cocke, Stephen Della Pietra, Vin-
cent J. Della Pietra, Frederick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics, 16(2):79?85.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 224?233, Honolulu, Hawaii.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
218?226. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics (ACL?05), pages 263?270, Ann
Arbor, Michigan.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 531?540, Ann Arbor, Michigan.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1?8. Association for
Computational Linguistics, July.
Josep M. Crego and Jose? B. Marin?o. 2007. Improving
SMT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Hal Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?
263, Prague, Czech Republic. Association for Compu-
tational Linguistics.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of ACL-
08: HLT, Short Papers, pages 25?28, Columbus, Ohio.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
551
surface heuristics. In Proceedings of the ACL work-
shop on Statistical Machine Translation, pages 31?38,
New York City, NY.
Yonggang Deng and William Byrne. 2006. MTTK: An
alignment toolkit for statistical machine translation. In
Proceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Demon-
strations, pages 265?268, New York City, USA.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 858?866, Los Angeles,
California. Association for Computational Linguistics.
Kuzman Ganchev, Joa?o V. Grac?a, and Ben Taskar. 2008.
Better alignments = better translations ? In Pro-
ceedings of ACL-08: HLT, pages 986?993, Columbus,
Ohio.
Qin Gao and Stephan Vogel. 2008. Parallel implementa-
tions of word alignment tool. In SETQA-NLP ?08.
Kevin Gimpel and Noah A. Smith. 2008. Rich source-
side context for statistical machine translation. In Pro-
ceedings of the Third Workshop on Statistical Machine
Translation, pages 9?17, Columbus, Ohio, June.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144?151,
Prague, Czech Republic.
Matti Ka?a?ria?inen. 2009. Sinuhe ? statistical machine
translation using a globally trained conditional expo-
nential family translation model. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 1027?1036, Singapore.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistic, pages 127?133, Edmond-
ton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
Annual Meeting of the Association for Computational
Linguistics (ACL), demonstration session, pages 177?
180, Prague, Czech Republic.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Daniel Marcu Susan Dumais and Salim Roukos,
editors, HLT-NAACL 2004: Main Proceedings, pages
169?176, Boston, Massachusetts, USA. Association
for Computational Linguistics.
Shankar Kumar, Yonggang Deng, and William Byrne.
2006. A weighted finite state transducer transla-
tion template model for statistical machine translation.
Natural Language Engineering, 12(1):35?75.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the International Conference on Ma-
chine Learning, pages 282?289. Morgan Kaufmann,
San Francisco, CA.
Thomas Lavergne, Olivier Capp, and Franois Yvon.
2010. Practical very large scale crfs. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 504?513, Uppsala,
Sweden.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006a. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia.
Percy Liang, Ben Taskar, and Dan Klein. 2006b. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL, Main
Conference, pages 104?111, New York City, USA.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 133?139.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending statistical machine translation with discrimi-
native and trigger-based lexicon models. In Proceed-
ings of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 210?218, Singa-
pore.
Mehryar Mohri. 2009. Weighted automata algorithms.
In Manfred Droste, Werner Kuich, and Heiko Vogler,
editors, Handbook of Weighted Automata, chapter 6,
pages 213?254. Springer Verlag.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regular-
ization, and rotational invariance. In Proceedings of
the twenty-first international conference on Machine
learning, pages 78?86.
Franz J. Och, Daniel Gildea, Sanjeev Khudanpur, Anoop
Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,
552
Libin Shen, David Smith, Katherine Eng, Viren Jain,
Zhen Jin, and Dragomir Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL 2004: Main Proceedings, pages 161?168,
Boston, Massachusetts, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Martin Riedmiller and Heinrich Braun. 1993. A direct
adaptive method for faster backpropagation learning:
The RPROP algorithm. In Proceedings of the IEEE
International Conference on Neural Networks, pages
586?591, San Francisco, USA.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning, Cam-
bridge, MA. The MIT Press.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. J.R.Statist.Soc.B, 58(1):267?288.
Christoph Tillman. 2004. A unigram orientation model
for statistical machine translation. In Susan Du-
mais, Daniel Marcu, and Salim Roukos, editors, HLT-
NAACL 2004: Short Papers, pages 101?104, Boston,
Massachusetts, USA.
J. Turian, B. Wellington, and I.D. Melamed. 2007. Scal-
able discriminative learning for natural language pars-
ing and translation. In Proc. Neural Information Pro-
cessing Systems (NIPS), volume 19, pages 1409?1417.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
475?484, Uppsala, Sweden.
Fei Xia and Michael McCord. 2004. Improving a statis-
tical mt system with automatically learned rewrite pat-
terns. In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING), pages
508?514, Geneva, Switzerland.
553
