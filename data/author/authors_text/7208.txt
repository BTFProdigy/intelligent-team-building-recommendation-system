Cluster Stopping Rules for Word Sense Discrimination 
Guergana Savova, Terry Therneau and Christopher Chute, 
Mayo Clinic, Rochester, MN, USA 
[Savova.Guergana;therneau;chute]@mayo.edu
Abstract
As text data becomes plentiful, unsuper-
vised methods for Word Sense Disam-
biguation (WSD) become more viable. A 
problem encountered in applying WSD 
methods is finding the exact number of 
senses an ambiguity has in a training cor-
pus collected in an automated manner. 
That number is not known a priori; rather 
it needs to be determined based on the 
data itself. We address that problem us-
ing cluster stopping methods. Such tech-
niques have not previously applied to 
WSD. We implement the methods of 
Calinski and Harabasz (1975) and Harti-
gan (1975) and our adaptation of the Gap 
statistic (Tibshirani, Walter and Hastie, 
2001). For evaluation, we use the WSD 
Test Set from the National Library of 
Medicine, whose sense inventory is the 
Unified Medical Language System. The 
best accuracy for selecting the correct 
number of clusters is 0.60 with the C&H 
method. Our error analysis shows that the 
cluster stopping methods make finer-
grained sense distinctions by creating ad-
ditional clusters. The highest F-scores 
(82.89), indicative of the quality of clus-
ter membership assignment, are compa-
rable to the baseline majority sense 
(82.63) and point to a path towards accu-
racy improvement via additional cluster 
pruning. The importance and significance 
of the current work is in applying cluster 
stopping rules to WSD. 
1 Introduction 
The dominant approach in word sense disam-
biguation (WSD) is based on supervised learning 
from manually sense-tagged text. While this is 
effective, it is quite difficult to get a sufficient 
number of manually sense-tagged examples to 
train a system. Mihalcea (2003) estimates that 
80-person years of annotation would be needed 
to create training corpora for 20,000 ambiguous 
English words, given 500 instances per word. 
For that reason, we are developing unsupervised 
knowledge-lean methods that avoid the bottle-
necks created by sense-tagged text. Unsupervised 
clustering methods utilize only raw corpora as 
their source of information, and there are grow-
ing amounts of general and specialized domain 
corpora available, e.g. biomedical domain cor-
pora.
Improvements in WSD methods would be of 
immediate value in indexing and retrievals of 
biomedical text given the explosion of biomedi-
cal literature as well as the rapid deployment of 
electronic medical records. Semantic/conceptual 
indexing and retrieval in that domain is often 
done in regard to the Unified Medical Language 
System (UMLS) developed at the National Li-
brary of Medicine (NLM) at the United States 
National Institutes of Health (NIH)1. It is impor-
tant to understand that the UMLS is significantly 
different than a dictionary, which is often the 
source of the sense inventory. Rather, the UMLS 
integrates more than 100 medical domain con-
trolled vocabularies such as SNOMED-CT2 and 
the International Classification of Diseases 
(ICD)3. UMLS has three main components. The 
first component, the Metathesaurus, includes all 
terms from the controlled vocabularies and is 
organized by concept, which is a cluster of terms 
representing the same meaning. Each concept is 
assigned a concept unique identifier (CUI), 
which is inherited by each term in the cluster. 
UMLS-based semantic indexing is based on CUI 
assignments. The second component, the Seman-
tic Network, groups the concepts into 134 types 
of categories and indicates the relationships be-
tween them. The Semantic Network is a coarse 
ontology of the concepts. The third component, 
the SPECIALIST lexicon, contains syntactic in-
formation for the Metathesaurus terms. 
1 http://www.nlm.nih.gov/pubs/factsheets/umls.html
2 http://www.snomed.org/ 
3 http://www.who.int/classifications/help/icdfaq/en/
9
MeSH, an ontology within UMLS, is heavily 
used for indexing biomedical scientific publica-
tions, e.g. Medline4. Hospitals, medical practices 
and biomedical research increasingly rely on the 
UMLS, or a subset ontology within it, to index 
and retrieve relevant information. It is estimated 
that approximately 7400 UMLS terms map to 
multiple concepts which creates ambiguity 
(Weeber, Mork and Aronson., 2001). Term am-
biguity has been pointed out to be one of the ma-
jor challenges for UMLS-based semantic index-
ing and retrieval (Weeber et al, 2001). For ex-
ample, ?cold? has the following six UMLS 
meanings, each with its own UMLS CUI: cold 
temperature, common cold, cold sensation, cold 
therapy, chronic obstructive lung disease 
(COLD), and Cold brand of chlorpheniramine-
phenylpropanolamine.  
The problem we are addressing in this paper is 
discovering the number of senses an ambiguous 
word has in a given corpus, which is a compo-
nent within a completely unsupervised WSD sys-
tem. For example, if a corpus of 1000 instances 
containing the word ?cold? has been compiled 
from patients medical records, how many ?cold? 
senses are in that corpus? This is a challenge any 
NLP system implementing WSD faces. To ad-
dress this problem, we apply cluster stopping 
rules in an automated way. 
The paper is organized as follows. Section 2 
overviews the related work on cluster stopping 
rules. Section 3 outlines our methods, tools, fea-
tures selection, test set and evaluation metrics. 
Section 4 presents the results and discusses them. 
Section 5 is the conclusions. 
2 Background and Related Work 
Our work is based on cluster analysis. Cluster 
analysis is often performed to discover the 
groups that the data naturally fall into. The num-
ber of groups is not known a priori; rather, it 
needs to be determined based on the data itself. 
Such methods or ?cluster stopping rules? usually 
rely on within-cluster dissimilarity/error (W(k))
metrics which in general exhibit a decline when 
the number of clusters increases. Splitting a natu-
ral group into subgroups reduces the criterion 
less than when well-separated clusters are dis-
covered. In those cases, the W(k) will not have a 
sharp decline as the instances are close. This 
phenomenon has been described in statistical 
literature as the ?elbow? effect as illustrated in 
4 http://www.nlm.nih.gov/pubs/factsheets/medline.html
Figure 1. Methods for locating the ?elbow have 
been the goal of many research studies (Hartigan, 
1975; Calinski and Harabasz, 1975; Milligan and 
Cooper, 1987; Tibshirani, Walter and Hastie, 
2001 among many). 
Milligan and Cooper (1985) offer the most 
comprehensive comparative study of the per-
formance of 30 stopping rules. They carry out 
their study on ?mildly truncated data from multi-
variate normal distributions, and one would not 
expect their ranking of the set of stopping rules 
to be reproduced exactly if a different cluster-
generating strategy were adopted.? (Gordon, 
1999, p. 61). The five rules which were the top 
performance in the Milligan and Cooper study  
are Calinski and Harabasz (1974) a.k.a. C&H, 
Goodman and Kruskal (1954), C index (Hubert 
and Schultz, 1976), Duda and Hart (1973) and 
Beale (1969). Tibshirani et al (2001) introduce 
the Gap statistic and compare its performance to 
the methods of Calinski and Harabasz (1974), 
Krzanowski and Lai (1985), Hartigan (1975), 
and the Silhouette method (Kaufman and 
Rousseeuw, 1990). On the simulated and DNA 
microarray data Tibshirani and colleagues used 
for their experiments, the Gap statistic yields the 
best result. 
In general, stopping rules fall into two catego-
ries ? global and local (Gordon, 1999; Tibshirani 
et al, 2001). Global rules take into account a 
combination of within-cluster and between-
cluster similarity measures over the entire data. 
Global rules choose such k where that combined 
metric is optimal. Global rules, however, in most 
cases do not work for k=1, that is they do not 
make predictions of when the data should not be 
partitioned at all. Global rules look at the entire 
data over k number of clusters. Local rules, on 
the other hand, are based only on a given k solu-
tion or individual pairs of clusters and test 
whether they should be grouped together. They 
need a threshold value or a significance level, 
which depends on the specific data and in most 
cases have to be empirically determined. 
3 Methodology
3.1 Overview
In this study, we explore three cluster stopping 
methods as applied to unsupervised WSD ? Har-
tigan (1975), Calinski and Harabasz (1974), and 
the Gap statistic (Tibshirani et al, 2001). The 
data to be clustered is instances of context sur-
rounding each ambiguity. Each instance is con-
verted into a feature vector where the features are 
10
ngrams (unigrams or bigrams) and each cell is 
the frequency of occurrence of a unigram or bi-
gram or the log-likelihood of a bigram occurring
in that particular instance after applying a feature
selection method. The clustering algorithm for 
this set of experiments is agglomerative cluster-
ing (see Section 3.5 for a more detailed descrip-
tion).
Our goal is to group contexts into separate
clusters based on the underlying sense of the am-
biguous word. Thus, the observations are con-
texts and the features are the identified lexical 
features (i.e. significant word(s)) that represent
the contexts. Our observed data matrix generally
shows the following characteristics ?
1) it is discrete
2) it is high dimensional/multivariate
3) it can be real valued or integer, or binary
4) it is sparse; while the number of features
can be in few hundreds, contexts have a length 
limit (ignoring the commonly occurring ?closed
class words? like ?the?, ?an?, ?on? etc.) 
5) it represents a distribution of contexts that
is generally skewed.
Following is our motivation for choosing the 
three cluster stopping rules. Hartigan (1975) and
Calinski and Harabasz (1974) have been consis-
tently used as baselines in a number of studies, 
e.g. Tibshirani et al (2001). The Hartigan
method is computationally simple and efficient
and unlike C&H, it is defined for k=1. The C&H 
method was ranked the top among 30 stopping
rules in the comprehensive study conducted by
Milligan and Cooper (1975). The Gap statistic 
(Tibshirani et al, 2001) is a fairly recent method
that has gained popularity by showing excellent
results when applied to the bio domain, e.g. clus-
tering DNA mircoarray data. None of the meth-
ods, however, have been applied or adapted to
WSD.
3.2 Calinski and Harabasz (1975) Method
The C&H method is reported to perform the best 
among 30 stopping rules (Milligan and Cooper, 
1985). C&H is a global method. The Variance
Ratio Criteria C&H uses is 
kn
kWGSS
k
kBGSS
kVRC

 )(1
)(
)(
where BGSS (between group sum of squares) is
the sum of the dispersions between the k cluster
centroids and the general centroid; WGSS
(within-group sum of squares) is the sum of each
cluster?s dispersion of its cluster members
(measured by the sum of squared distances be-
tween each member and the cluster centroid) 
weighed by the number of cluster members; k is 
the number of clusters and n is the number of 
instances. The distance used is the Euclidean dis-
tance. As Calinski and Harabasz point out, VRC
?is analogous to the F-statistic in univariate 
analysis? (Calinski and Harabasz, 1975, p. 10).
C&H seeks to maximize VRC.
3.3 Hartigan (1975) Method 
Hartigan (1975) proposes a cluster stopping rule: 
)1(1
)1(
)()( ??
???
?  knkWGSS
kWGSSkH
where n is the total number of instances to be 
clustered, k is the number of clusters and
WGSS(k) is the total sum of squared distances of
cluster members from their cluster centroid in all 
clusters when clustered in k clusters. 
H(k) is used to decide when k+1 clusters are
needed rather than k clusters. Its distribution ap-
proximates the F distribution. A large value of 
H(k) would indicate that the addition of a cluster 
is warranted. Hartigan suggests that as a crude 
rule of thumb, values exceeding 10 justify in-
creasing the number of clusters from k to k+1 
(Hartigan, 1975, p. 91). Thus, a solution is the 
smallest k ? 1 such that H(k) ? 10. The method
can return 1 cluster as the optimal solution. Har-
tigan (1975) is a local method.
3.4 Gap Statistic Method 
In general, the ?gap? method compares the dif-
ference/gap between the within-cluster disper-
sion measure for the observed distribution and
that for an appropriate null distribution of the
data. Tibshirani and colleagues (Tibshirani et al, 
2001) start with the assumption of a single clus-
ter null model which is to be rejected in favor of 
a k-component model (k>1) if the observed data 
supports it. Tibshirani and colleagues use a uni-
form distribution as the null distribution of the
data to standardize the comparison between all
the W(k) over the various values of k where W(k)
is the pooled within cluster sum of squares 
around the cluster means (distance is squared 
Euclidean distance).  The uniform distribution is
the least favorable distribution and the most
likely to produce spurious clusters. However,
11
Tibshirani and colleagues also point out that the 
choice of an appropriate null distribution de-
pends on the data. Tibshirani and colleagues
compare the curve of log(W(k)) to the log(W*(k))
curve obtained from the reference uniformly dis-
tributed over the data. The estimated optimal
number of clusters is the k value where the gap 
between the two curves is the largest. Figure 1 is
an example of log(W(k)) to the log(W*(k)) curves
used in the computation of the Gap statistic. 
The two main advantages of the Gap statistic 
over various previously proposed ?stopping
rules? are its ability to work with data created by
almost any type of clustering and its ability to 
accurately estimate the optimal number of clus-
ters even for data that naturally falls into just one 
cluster. The Gap statistic is an application of pa-
rametric bootstrap methods to the clustering
problem. Unlike non-parametric methods, para-
metric techniques represent the observed data 
distribution. The basic strategy is to create multi-
ple random data sets over the observed distribu-
tion for which there are no clusters, apply the 
chosen clustering method to them, and tabulate 
the apparent decrease in within-cluster variation 
that ensues.  This gives a measure of optimism
with which to compare the clustering of the ob-
served data. 
The complete methodology can be broadly
classified into two important components namely
the reference distribution and the algorithm
which uses the reference distribution. We de-
scribe each of the two components below.
Figure 1: The functions log(W(k)) (observed) 
and log(W*(k)) (reference) used for computing 
the Gap statistic 
Reference Distribution Generation for an 
NLP Task
Here, we describe how we extend the generation
of the reference distribution over the observed
data to retain the characteristics mentioned at end
of section 3.1. We will use the observed data
shown in Table 1 as a running example. To simu-
late the structure of the observed data, the fol-
lowing features are to be emulated:
(a) Context length is the number of features 
that can occur in a context. Contexts can be sen-
tences, paragraphs, entire documents or just any
specified window size. In general, the number of
available features will be at least in the hundreds,
however, only a few might occur in a given con-
text, especially if the context is limited to the 
sentence the target ambiguity occurs in. Addi-
tionally, context length is influenced by the fea-
ture selection method ? if only very frequent 
lexical units are retained as features, then only 
those units will represent the context. Thus, a
context length could be very small compared to
the size of the feature set. In the example from
Table 1, context length is captured by the row 
marginals, e.g. the context length for Context1 is 
3, which means that overall there are only three
features for that context. 
(b) Sparsity is a consequence of relatively
small context length. Currently, our assumption
is that contexts are derived from small discourse 
units (sentences or abstracts at the most). For 
bigger discourse units, e.g. several paragraphs or
entire documents, our proposed generation of the 
reference distribution should be modified to re-
flect feature occurrences over those units. In the 
example from Table 1, for instance in Context1, 
there are 3 features that are present ? Feature1,
Feature4 and Feature5 ? the rest are absent.
Sparsity can be viewed as the number of ab-
sent/zero-valued features for each row. 
(c) Feature distribution is the frequency of oc-
currence of each feature across all contexts. It is 
captured by the column marginals of the ob-
served data matrix. For example, in Table 1 Fea-
ture1 occurs twice over the entire data; similarly
Feature2 occurs twice and so on. Feature distri-
bution can be viewed as the number of occur-
rences of each feature in the entire corpus. 
Now we describe how we do the reference
generation to stay faithful to the characteristics
described above. We use the uniform and the 
proportional methods. The uniform method gen-
erates data that realizes (a) and (b) characteristics
of the data and is the used originally in Tibshi-
rani et al (2001). The proportional method cap-
tures (a), (b), and (c) and is our adaptation of the 
Gap method.
The data is constructed as follows. To retain
the context lengths of the observed data in the
12
Feature1 Feature2 Feature3 Feature4 Feature5 ?FeatureP Total number of
non-zero value cells
Context1 1 0 0 1 1 ??? 3
Context2 0 1 1 0 1 ??? 3
Context3 1 0 0 0 1 ??? 2
Context4 0 1 1 1 1 ??? 4
?ContextN ??? ??? ??? ??? ??? ??? ???
2 2 2 2 4 ??? 12
Table 1: Observed data (sample)
reference data, the row marginals of the refer-
ence data are fixed to be equal to those of the 
observed data. In Table 1, the row marginals for 
the reference data will be {3, 3, 2, 4}. Carrying
the observed marginals to the reference data ap-
plies to both the uniform and proportional meth-
ods. Note that currently we fix only the row mar-
ginals. Due to the current assumption of binary
feature frequency, the generated reference data is 
binary too and this is true for both methods.
The main difference between the uniform
and proportional methods lies in whether the fea-
ture distribution is maintained in the simulation.
The uniform method does not weigh the features; 
rather, all features are given equal probability of 
occurring in the generated data. A uniform ran-
dom number r over the range [1, featureSetSize]
is drawn. The cell corresponding to the rth col-
umn (i.e. feature) in the current row under con-
sideration (i.e. context) is assigned ?1?. For ex-
ample, in our running example let?s say we are 
generating reference data for the 3rd row from
Table 1. We first generate a random number over 
the range [1, p]. Let?s assume that the generated
number is 4. Then, the cell [3, 4] is assigned
value ?1?. This procedure is repeated twice since
the row marginal for this row of the reference
data is 2. The proportional method factors in the
distribution of the column marginals of the ob-
served data while generating the random data. 
Unlike the uniform method, it takes into account 
the weight of each feature. In other words, the 
features by their frequency assign themselves a
range. For example, the features in the Table 1
will be assigned the following ranges: Feature1 -
[1, 2]; Feature2 - [3, 4]; Feature3 ? [5, 6]; Fea-
ture4 ? [7, 8]; Feature5 ? [9, 12]. A random
number is generated over the range [1, total
number of feature occurrences]. For the data in
Table 1, a random number is generated over the 
range [1, 12]. The feature corresponding to the 
range in which the random number falls is as-
signed ?1?. For example, if we are generating the 
reference for Context3 and the generated random
number over the range [1, 12] is 5, then a look-
up determines that 5 falls in the range for Fea-
ture3. Hence, the cell in Context3 corresponding
to Feature3 is assigned ?1?. Similar to the uni-
form method we would repeat this procedure 
twice to achieve the row marginal total of 2. 
Currently we proceed with the binary refer-
ence data created by the procedure described
above. Note that this binary reference matrix can 
be converted to a strength-of-association matrix
by multiplying it with a diagonal matrix that con-
tains the strength-of-association scores, e.g. log 
likelihood ratio, Mutual Information, Pointwise
mutual information, Chi-squared to name a few. 
Algorithm
The complete algorithm of the Gap Statistics 
which the reference distribution is a part of is: 
1.Cluster the observed data, varying the total 
number of clusters from k = 1, 2, ?., K, giving
within dispersion measures W(k), k = 1, 2, 
?.K.
2.Generate B reference datasets using the uni-
form or the proportional methods as described
above, and cluster each one giving within dis-
persion measures W*(kb), b = 1, 2, ? B, k = 
1, 2,? K. Compute the estimated Gap statistic:
))(log())(*log()/1()( kWkbWBkGap
b
6 
3. Let ? 
B
kbWBl ))(*log()/1( , compute the stan-
dard deviation
2/12]))(*(log()/1[()( lkbWBksd
B
 ?  and define
Bksdks /11)()(  . Finally choose the number of
clusters via 
)1()1()(_? t kskGapkGapuchThatsmallestKsk
The final step is the criterion for selecting the 
optimal k value. It says to choose the smallest k
value for which the gap is greater than the gap
for the earlier k value by the significance test of
?one standard error?. The ?one standard error?
calculations are modified to account for the
simulation error. Tibshirani and colleagues also
advise to use a multiplier to the s(k) for better 
rejection of the null hypothesis.
13
3.5 Tools, Feature Selection and Method
Parameters
rus strings (Weeber et al, 2001). Each ambiguity
has 100 manually sense-tagged instances. All
instances were randomly chosen from Medline 
abstracts. Each ambiguity instance is provided
with the sentence it occurred in and the Medline 
abstract text it was derived from. The senses for
every ambiguity are the UMLS senses plus a
?none of the above? category which captures all 
instances not fitting the available UMLS senses. 
For feature representation, selection, context rep-
resentation and clustering, we used Sense-
Clusters0.69 (http://senseclusters.sourceforge.net). It 
offers a variety of lexical features (ngrams, col-
locations, etc.) and feature selection methods 
(frequency, log likelihood, etc.). The contexts 
can then be represented with those features in
vector space using first or second order vectors 
which are then clustered. A detailed description
can be found in Purandare and Pedersen (2004)
and http://www.d.umn.edu/~tpederse/senseclusters.html.
SenseClusters links to CLUTO for the clustering
part (http://www-users.cs.umn.edu/~karypis/
cluto/download.html). CLUTO implements in a
fast and efficient way the main clustering algo-
rithms ? agglomerative, partitional and repeated
bisections.
For the current study, we modified the NLM
WSD by excluding instances sense-tagged with 
the ?none of the above? category. This is moti-
vated by the fact that that category is a catch-all
category for all senses that do not fit the current
UMLS inventory. First, we excluded words 
whose majority category was ?none of the 
above?. Secondly, from the instances of the re-
maining words, we removed those marked with 
?none of the above?. That subset of the original 
NLM WSD set we refer to as the ?modified
NLM WSD set? (Table 2).We chose the following methods for featurerepresentation and selection. Method1 uses bi-
grams as features, average link clustering in 
similarity space and the abstract as the context to 
derive the features from. The method is de-
scribed in Purandare and Pedersen (2004). It is 
based on first order context vectors, which repre-
sent features that occur in that context. A similar-
ity matrix is clustered using the average link ag-
glomerative method. Purandare and Pedersen
(2004) report that this method generally per-
formed better where there was a reasonably large
amount of data available (i.e., several thousand 
contexts). The application of that method to the 
biomedical domain is described in a technical
report (Savova, Pedersen, Kulkarni and Puran-
dare, 2005). Method2 uses unigrams which occur 
at least 5 times in the corpus. The context is the 
abstract. The choice of those features is moti-
vated by Joshi, Pedersen and Maclin (2005)
study which achieves best results with unigram
features.
3.7 Evaluation
Our evaluation of the performance of the cluster
stopping rules is two-fold. Accuracy is a direct
evaluation measuring the correctly recognized
number of senses:
words with correctly predicted number of senses
    all words 
Accuracy evaluates how well the methods dis-
cover the exact number of senses in the test cor-
pus. The F-score of the WSD is an indirect
evaluation for the quality of the cluster assign-
ment:
callecision?
callecision)(?scoreF
RePr2
RePr12_

 
Precision is the number of correctly clustered
instances divided by the number of clustered in-
stances; Recall is the number of correctly clus-
tered instances divided by all instances. There
may be some number of contexts that the cluster-
ing algorithm declines to process, which leads to 
the difference in precision and recall. 
For the Hartigan cluster stopping method, the
threshold is set to 10 which is the recommenda-
tion in the original algorithm. For the Gap cluster 
stopping method, we experiment with B=100, 
and the uniform and proportional reference gen-
eration methods. Our baseline is a simple clustering algorithm that
assigns all instances of a target word to a single
cluster.
3.6 Test Set
Our test set is the NLM WSD5 set which com-
prises 5000 disambiguated instances for 50 
highly frequent ambiguous UMLS Metathesau-
4 Results and Discussion
Table 3 presents the results for the three methods
5http://wsd.nlm.nih.gov/Restricted/Reviewed_Results/index.
shtml
14
Word, instances, senses after removal 
of ?none of the above? sense 
Word, instances, senses after removal 
of ?none of the above? sense 
Word, instances, senses after re-
moval of ?none of the above? sense 
Adjustment, 93, 3 Frequency, 94, 1 Radiation, 98, 2 
Blood pressure, 100, 3 Growth, 100, 2 Repair, 68, 2 
Cold, 95, 4 Immunosuppression, 100, 2 Scale, 65, 1 
Condition, 92, 2 Implantation, 98, 2 Secretion, 100, 1 
Culture, 100, 2 Inhibition, 99, 2 Sex, 100, 3 
Degree, 65, 2 Japanese, 79, 2 Single, 100, 2 
Depression, 85, 1 Ganglion, 100, 2 Strains, 93, 2 
Determination, 79, 1 Glucose, 100, 2 Surgery, 100, 2 
Discharge, 75, 2 Man, 92, 4 Transient, 100, 2 
Energy, 100, 2 Mole, 84, 2 Transport, 94, 2 
Evaluation, 100, 2 Mosaic, 97, 2 Ultrasound, 100, 2 
Extraction, 87, 2 Nutrition, 89, 4 Variation, 100, 2 
Fat, 73, 2 Pathology, 99, 2 White, 90, 2 
Fluid, 100, 1 Pressure, 96, 1 
Table 2: Modified NLM WSD set 
In terms of accuracy (Table 3, column 3), the 
C&H method has the best results (p<0.01 with t-
test). Note that the modified NLM WSD set con-
tains seven words with one sense ? depression, 
pressure, determination, fluid, frequency, scale, 
secretion ? for which the C&H method is at a 
disadvantage as it cannot return one cluster solu-
tion.
In terms of predicted number of senses (Table 
3, column 5), the Hartigan method tends to un-
derestimate the number of senses (overcluster), 
thus making coarser sense distinctions. The 
adapted Gap and C&H methods tend to overes-
timate them (undercluster), thus making finer 
grained sense distinctions. 
In terms of cluster member assignment as 
demonstrated by the F-scores (Table 3, column 
4), our adapted Gap method and the Hartigan 
method perform better than the C&H method 
(p<0.05 with t-test). The Hartigan method F-
scores  along with Gap uniform with Method 1 
feature selection are not significantly different 
from the baseline (p>0.05 with t-test); the rest 
are significantly lower than the majority sense 
baseline (p<0.05 with t-test). 
The high F-scores point to a path for improv-
ing accuracy results. Singleton clusters could be 
pruned as they could be insignificant to sense 
discrimination. As it was pointed out, the best 
performing algorithms (C&H and Gap propor-
tional) tend to create too many clusters (Table 3, 
column 5). Another way of dealing with single-
ton or smaller clusters is to present them for hu-
man review as they might represent new sense 
distinctions not included in the sense inventory. 
One explanation for the performance of the 
stopping rules (overclustering in particular) 
might be that some senses are very similar, e.g. 
?cold temperature? and ?cold sensation? for the 
?cold? ambiguity in instances like ?Her feet are 
cold.? Another explanation is that the stopping 
rules rely on the clustering algorithm used. In our 
current study, the experiments were run with 
only agglomerative clustering as implemented in 
CLUTO. The distance measure that we used is 
Euclidean distance, which is only one of many 
choices. Yet another explanation is in the feature 
sets we experimented with. They performed very 
similarly on both the accuracy and F-scores. Fu-
ture work we plan to do is aimed at experiment-
ing with different features, clustering algorithms, 
distance measures as well as applying Singular 
Value Decomposition (SVD) to the reference 
distribution matrix for our adapted Gap method. 
We are actively pursuing reference generation 
with fixed column and row marginals. The work 
of Pedersen, Kayaalp and Bruce (1996) uses this 
technique to find significant lexical relationships. 
They use the CoCo (Badsberg, 1995) package 
which implements the Patefield (1981) algorithm 
for I x J tables. Another venue is in the combina-
tion of several stopping rules which will take 
advantage of each rule?s strengths. Yet another 
component that needs to be addressed towards 
the path of completely automated WSD is cluster 
labeling.
5 Conclusions
In this work, we explored the problem of discov-
ering the number of the senses in a given target 
ambiguity corpus by studying three cluster stop-
ping rules. We implemented the original algo-
rithms of Calinski and Harabasz (1975) and Har-
tigan (1975) and adapted the reference genera-
tion of the Gap algorithm (Tibshirani et al, 
2001) to our task. The best accuracy for selecting 
the correct number of clusters is 0.60 with the
15
Feature 
Selection 
Stopping Rule Accuracy F-score                                  
(baseline majority sense = 82.63) 
Average number of senses (true 
average number of senses = 2.19) 
Method1 C&H 0.49 80.71 2.90 (overestimates) 
Hartigan 0.10 82.15 1.27 (underestimates) 
Gap (uniform) 0.02 82.00 1.49 (underestimates)     
Gap (proportional) 0.24 81.31 2.51 (overestimates) 
Method2 C&H 0.60 80.27 3.36 (overestimates) 
Hartigan 0.02 82.89 1.10 (underestimates) 
Gap (uniform) 0.05 81.63 2.44 (overestimates) 
Gap (proportional) 0.12 81.15 2.59 (overestimates) 
Table 3: Results ? accuracy, F-score and predicted average number of sense 
C&H method. Our error analysis shows that the 
cluster stopping methods make finer-grained 
sense distinctions by creating additional single-
ton clusters. The F-scores, indicative of the qual-
ity of cluster membership assignment, are in the 
80?s and point to a path towards accuracy im-
provement via additional cluster pruning. 
Acknowledgements 
The Perl modules of our implementations of the 
algorithms can be downloaded from 
http://search.cpan.org/dist/Statistics-CalinskiHarabasz/,
http://search.cpan.org/dist/Statistics-Hartigan/,
http://search.cpan.org/dist/Statistics-Gap/. We are 
greatly indebted to Anagha Kulkarni and Ted 
Pedersen for their participation in this research. 
We would also like to thank Patrick Duffy, 
James Buntrock and Philip Ogren for their sup-
port and collegial feedback, and the Mayo Clinic 
for funding the work.  
References 
A. D. Gordon. 1999. Classification (Second Edition). 
Chapman & Hall, London 
A. Purandare and T. Pedersen. 2004. Word Sense 
Discrimination by Clustering Contexts in Vector 
and Similarity Spaces. Proceedings of the Confer-
ence on Computational Natural Language Learning 
(CoNLL): 41-48, May 6-7, 2004, Boston, MA 
E. M.L. Beale.  1969. Euclidean cluster analysis. Bul-
letin of the International Statistical Institute:92?94, 
1969 
G. Savova, T. Pedersen, A. Kulkarni and A. Puran-
dare. 2005. Resolving Ambiguities in the Biomedi-
cal Domain. Technical Report. Minnesota Super-
computing Institute. 
G. W. Milligan and M.C. Cooper. 1985. An examina-
tion of procedures for determining the number of 
clusters in a data set. Psychometrika 50:159-179. 
J. H. Badsberg. 1995. An environment for graphical 
models. PhD dissertation, Aalborg University. 
J. Hartigan. 1975. Clustering Algorithms, Wiley, New 
York. 
L. A. Goodman and W.H. Kruskal. 1954. Measures of 
association for cross classifications. J. of Amer. 
Stat. Assoc., 49:732--764, 1954.
L. Hubert and J. Schultz. 1976. Quadratic assignment 
as a general data-analysis strategy. British Journal 
of Mathematical and Statistical Psychologie. 
29:190-241 
L. Kaufman and P. Rowsseeuw. 1990. Finding groups 
in data: an introduction to cluster analysis. New 
York. Wiley. 
M. Joshi, T. Pedersen and R. Maclin. 2005. A com-
parative study of support vector machines applied 
to the supervised word sense disambiguation prob-
lem in the medical domain. IICAI. India. 
M. Weeber, J. Mork and A. Aronson. 2001. Develop-
ing a test collection for biomedical word sense dis-
ambiguation. Proc. AMIA 
R. B. Calinski and J. Harabasz. 1974. A dendrite 
method for cluster analysis. Communications in 
statistics 3:1-27. 
R. Mihalcea. 2003. The role of non-ambiguous words 
in natural language disambiguation. RANLP-2003, 
Borovetz, Bulgaria 
R. O. Duda and P. E. Hart. 1973. Pattern Classifica-
tion and Scene Analysis. Wiley, New York, 1973
R. Tibshirani, G. Walther and T. Hastie. 2001. Esti-
mating the number of clusters in a dataset via the 
Gap statistic. Journal of the Royal Statistics Soci-
ety (Series B). 
T. Pedersen, M. Kayaalp and R. Bruce. 1996. Signifi-
cant lexical relationships. Proc. of the 13th National 
Conference on Artificial Intelligence, August 1996, 
Portland, Oregon. 
W. J. Krzanowski and Y. T. Lai. 1985. A criterion for 
determining the number of groups in a data set us-
ing the sum of squares clustering. Biometrics 
44:23-34. 
W. Patefield. 1981. An efficient method of generating 
random R x C tables with given row and column 
totals. Applied Statistics 30:91-97. 
16
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 94?95,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics 1
Conditional Random Fields and Support Vector Machines for Disorder 
Named Entity Recognition in Clinical Texts 
Dingcheng Li Karin Kipper-Schuler Guergana Savova 
University of Minnesota Mayo Clinic College of Medicine Mayo Clinic College of Medicine 
Minneapolis, Minnesota, USA Rochester, Minnesota, USA Rochester, Minnesota, USA 
lixxx345@umn.edu schuler.karin@mayo.edu savova.guergana@mayo.edu 
 
Abstract 
We present a comparative study between 
two machine learning methods, Conditional 
Random Fields and Support Vector Ma-
chines for clinical named entity recognition. 
We explore their applicability to clinical 
domain. Evaluation against a set of gold 
standard named entities shows that CRFs 
outperform SVMs. The best F-score with 
CRFs is 0.86 and for the SVMs is 0.64 as 
compared to a baseline of 0.60. 
1 Introduction and background 
Named entity recognition (NER) is the discovery 
of named entities (NEs), or textual mentions that 
belong to the same semantic class. In the biomedi-
cal domain NEs are diseases, signs/symptoms, ana-
tomical signs, and drugs. NER performance is high 
as applied to scholarly text and newswire narra-
tives (Leaman et al, 2008). Clinical free-text, on 
the other hand, exhibits characteristics of both in-
formal and formal linguistic styles which, in turn, 
poses challenges for clinical NER. Conditional 
Random Fields (CRFs) (Lafferty et al, 2001) and 
and Support Vector Machines (SVMs) (Cortes and 
Vapnik, 1995) are machine learning techniques 
which can handle multiple features during learn-
ing. CRFs? main strength lies in their ability to in-
clude various unrelated features, while SVMs? in 
the inclusion of overlapping features.  Our goal is 
to compare CRFs and SVMs performance for 
clinical NER with focus on disease/disorder NEs. 
2 Dataset and features 
Our dataset is a gold standard corpus of 1557 sin-
gle- and multi-word disorder annotations (Ogren et 
al., 2008). For training and testing the CRF and 
SVM models the IOB (inside-outside-begin) nota-
tion (Leaman, 2008) was applied. In our project, 
we used 1265 gold standard annotations for train-
ing and 292 for testing. The features used for the 
learning process are described as follows. Diction-
ary look-up is a binary value feature that represents 
if the NE is in the dictionary (SNOMED-CT). Bag 
of Words (BOW) is a representation of the context 
by the unique words in it. Part-of-speech tags 
(POS) of BOW is the pos tags of the context 
words. Window size is the number of tokens repre-
senting context surrounding the target word. Ori-
entation(left or right) is the location of the feature 
in regard to the target word. Distance is the prox-
imity of the feature in regard to the target word 
Capitalization has one of the four token-based val-
ues: all upper case, all lower case, mixed_case and 
initial upper case. Number features refer to the 
presence or absence of related numbers. Feature 
sets are in Table 1. 
3 Results and discussion 
Figure 1 shows the CRF results. The F-scores, re-
call and precision for the baseline dictionary look-
up are 0.604, 0.468 and 0.852 respectively. When 
BOW is applied in feature combination 2 results 
improve sharply adding 0.15, 0.17 and 0.08 points 
respectively. The F-score, recall and precision im-
prove even further with the capitalization feature to 
0.858, 0.774 and 0.963 respectively. Figure 2 
shows SVM results. The addition of more features 
to the model did not show an upward trend. The 
best results are with feature combination 1 and 3. 
The F-score reaches 0.643, which although an im-
provement over the baseline greatly underperforms 
CRF results. BOW features seem not discrimina-
tive with SVMs. When the window size increases 
to 5, performance decreases as demonstrated in 
feature combinations 2, 4 and 8. Results with fea-
ture combination 4, in particular, has a pronounced 
downward trend. Its F-score is 0.612, a decrease by 
0.031 compared with Test 1 or Test 3. Its recall 
and precision are 0.487 and 0.822 respectively, a 
decrease by 0.036 and 0.01 respectively. This sup-
ports the results achieved with CRFs where a 
smaller window size yields better performance. 
 
94
 2
No Features 
1 dictionary look-up (baseline) 
2 dictionary look-up+BOW+Orientation+distance (Win-
dow 5) 
3 dictionary look-up + BOW + Orientation + distance 
(Window 3) 
4 dictionary look-up + BOW  + POS + Orientation + 
distance (Window 5) 
5 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) 
6 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) + bullet number 
7 dictionary look-up + BOW + POS + Orientation + 
distance(Window 3) + measurement 
8 dictionary look-up + BOW + POS + Orientation + 
distance  (Window 5) + neighboring number 
9 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) + neighboring number 
10 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3)+neighboring number+measurement 
11 dictionary look-up+BOW+POS+Orientation (Window 
3)+neighboring number+bullet number + measurement 
12 dictionary look-up + BOW +POS + Orientation 
+distance (Window 3) + neighboring number + bullet 
number + measurement + capitalization 
Table 1: Feature combinations 
 
 
Figure 1: CRF evaluation results 
 
Figure 2: SVM evaluation results 
 
As the results show, context represented by the 
BOW feature plays an important role indicating the 
importance of the words surrounding NEs. On the 
other hand, POS tag features did not bring much 
improvement, which perhaps hints at a hypothesis 
that grammatical roles are not as important as con-
text in clinical text. Thirdly, a small window size is 
more discriminative. Clinical notes are unstruc-
tured free text with short sentences. If a larger win-
dow size is used, many words will share similar 
features. Fourthly, capitalization is highly dis-
criminative. Fifthly, as a finite state machine de-
rived from HMMs, CRFs can naturally consider 
state-to-state dependences and feature-to-state de-
pendences. On the other hand, SVMs do not con-
sider such dependencies. SVMs separate the data 
into categories via a kernel function. They imple-
ment this by mapping the data points onto an opti-
mal linear separating hyperplane. Finally, SVMs 
do not behave well for large number of feature 
values. For large number of feature values, it 
would be more difficult to find discriminative lines 
to categorize the labels. 
4 Conclusion and future work 
We investigated the use of CRFs and SVMs for 
disorder NER in clinical free-text. Our results 
show that, in general, CRFs outperformed SVMs. 
We demonstrated that well-chosen features along 
with dictionary-based features tend to improve the 
CRF model?s performance but not the SVM?s.  
Acknowledgements 
The work was partially supported by a Biomedical 
Informatics and Computational Biology scholar-
ship from the University of Minnesota. 
References 
Corinna Cortes and Vladimir Vapnik. Support-vector 
network. Machine Learning, 20:273-297, 1995. 
John Lafferty, Andrew McCallum and Fernando 
Pereira. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
In Proceedings of the Eighteenth International Con-
ference on Machine Learning (ICML-2001), 2001. 
Robert Leaman and Graciela Gonzalez. BANNER: an 
Executable Survey of Advances in Biomedical 
Named Entity Recognition. Pacific Symposium on 
Biocomputing 13:652-663. 2008. 
Philip Ogren, Guergana Savova and Christopher G 
Chute. Constructing evaluation corpora for auto-
mated clinical named entity recognition. Proc LREC 
2008. 
95
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 81?86,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Descending-Path Convolution Kernel for Syntactic Structures
Chen Lin
1
, Timothy Miller
1
, Alvin Kho
1
, Steven Bethard
2
,
Dmitriy Dligach
1
, Sameer Pradhan
1
and Guergana Savova
1
,
1
Children?s Hospital Boston Informatics Program and Harvard Medical School
{firstname.lastname}@childrens.harvard.edu
2
Department of Computer and Information Sciences, University of Alabama at Birmingham
bethard@cis.uab.edu
Abstract
Convolution tree kernels are an efficient
and effective method for comparing syntac-
tic structures in NLP methods. However,
current kernel methods such as subset tree
kernel and partial tree kernel understate the
similarity of very similar tree structures.
Although soft-matching approaches can im-
prove the similarity scores, they are corpus-
dependent and match relaxations may be
task-specific. We propose an alternative ap-
proach called descending path kernel which
gives intuitive similarity scores on compa-
rable structures. This method is evaluated
on two temporal relation extraction tasks
and demonstrates its advantage over rich
syntactic representations.
1 Introduction
Syntactic structure can provide useful features for
many natural language processing (NLP) tasks
such as semantic role labeling, coreference resolu-
tion, temporal relation discovery, and others. How-
ever, the choice of features to be extracted from a
tree for a given task is not always clear. Convolu-
tion kernels over syntactic trees (tree kernels) offer
a potential solution to this problem by providing
relatively efficient algorithms for computing sim-
ilarities between entire discrete structures. These
kernels use tree fragments as features and count
the number of common fragments as a measure of
similarity between any two trees.
However, conventional tree kernels are sensitive
to pattern variations. For example, two trees in Fig-
ure 1(a) sharing the same structure except for one
terminal symbol are deemed at most 67% similar
by the conventional tree kernel (PTK) (Moschitti,
2006). Yet one might expect a higher similarity
given their structural correspondence.
The similarity is further attenuated by trivial
structure changes such as the insertion of an ad-
jective in one of the trees in Figure 1(a), which
would reduce the similarity close to zero. Such
an abrupt attenuation would potentially propel a
model to memorize training instances rather than
generalize from trends, leading towards overfitting.
In this paper, we describe a new kernel over
syntactic trees that operates on descending paths
through the tree rather than production rules as
used in most existing methods. This representation
is reminiscent of Sampson?s (2000) leaf-ancestor
paths for scoring parse similarities, but here it is
generalized over all ancestor paths, not just those
from the root to a leaf. This approach assigns more
robust similarity scores (e.g., 78% similarity in the
above example) than other soft matching tree ker-
nels, is faster than the partial tree kernel (Moschitti,
2006), and is less ad hoc than the grammar-based
convolution kernel (Zhang et al, 2007).
2 Background
2.1 Syntax-based Tree Kernels
Syntax-based tree kernels quantify the similarity
between two constituent parses by counting their
common sub-structures. They differ in their defini-
tion of the sub-structures.
Collins and Duffy (2001) use a subset tree (SST)
representation for their sub-structures. In the SST
representation, a subtree is defined as a subgraph
with more than one node, in which only full pro-
duction rules are expanded. While this approach is
widely used and has been successful in many tasks,
the production rule-matching constraint may be un-
necessarily restrictive, giving zero credit to rules
that have only minor structural differences. For
example, the similarity score between the NPs in
Figure 1(b) would be zero since the production rule
is different (the overall similarity score is above-
zero because of matching pre-terminals).
The partial tree kernel (PTK) relaxes the defi-
nition of subtrees to allow partial production rule
81
a)
NP
DT
a
NN
cat
NP
DT
a
NN
dog
b)
NP
DT
a
NN
cat
NP
DT
a
JJ
fat
NN
cat
c)
S
ADVP
RB
here
NP
PRP
she
VP
VBZ
comes
S
NP
PRP
she
VP
VBZ
comes
ADVP
RB
here
Figure 1: Three example tree pairs.
matching (Moschitti, 2006). In the PTK, a subtree
may or may not expand any child in a production
rule, while maintaining the ordering of the child
nodes. Thus it generates a very large but sparse
feature space. To Figure 1(b), the PTK generates
fragments (i) [NP [DT a] [JJ fat]]; (ii) [NP [DT a]
[NN cat]]; and (iii) [NP [JJ fat] [NN cat]], among
others, for the second tree. This allows for partial
matching ? substructure (ii) ? while also generating
some fragments that violate grammatical intuitions.
Zhang et al (2007) address the restrictiveness
of SST by allowing soft matching of production
rules. They allow partial matching of optional
nodes based on the Treebank. For example, the
rule NP ? DT JJ NN indicates a noun phrase
consisting of a determiner, adjective, and common
noun. Zhang et al?s method designates the JJ as
optional, since the Treebank contains instances of
a reduced version of the rule without the JJ node
(NP ? DT NN ). They also allow node match-
ing among similar preterminals such as JJ, JJR, and
JJS, mapping them to one equivalence class.
Other relevant approaches are the spectrum tree
(SpT) (Kuboyama et al, 2007) and the route kernel
(RtT) (Aiolli et al, 2009). SpT uses a q-gram
? a sequence of connected vertices of length q ?
as their sub-structure. It observes grammar rules
by recording the orientation of edges: a?b?c is
different from a?b?c. RtT uses a set of routes as
basic structures, which observes grammar rules by
NP
DT
a
NN
cat
l=0: [NP],[DT],[NN]
l=1: [NP-DT],[NP-NN],
[DT-a],[NN-cat]
l=2: [NP-DT-a],[NP-NN-cat]
Figure 2: A parse tree (left) and its descending
paths according to Definition 1 (l - length).
recording the index of a neighbor node.
2.2 Temporal Relation Extraction
Among NLP tasks that use syntactic informa-
tion, temporal relation extraction has been draw-
ing growing attention because of its wide applica-
tions in multiple domains. As subtasks in Tem-
pEval 2007, 2010 and 2013, multiple systems
were built to create labeled links from events
to events/timestamps by using a variety of fea-
tures (Bethard and Martin, 2007; Llorens et al,
2010; Chambers, 2013). Many methods exist for
synthesizing syntactic information for temporal
relation extraction, and most use traditional tree
kernels with various feature representations. Mir-
roshandel et al (2009) used the path-enclosed tree
(PET) representation to represent syntactic informa-
tion for temporal relation extraction on the Time-
Bank (Pustejovsky et al, 2003) and the AQUAINT
TimeML corpus
1
. The PET is the smallest subtree
that contains both proposed arguments of a relation.
Hovy et al (2012) used bag tree structures to rep-
resent the bag of words (BOW) and bag of part of
speech tags (BOP) between the event and time in
addition to a set of baseline features, and improved
the temporal linking performance on the TempEval
2007 and Machine Reading corpora (Strassel et
al., 2010). Miller at al. (2013) used PET tree, bag
tree, and path tree (PT, which is similar to a PET
tree with the internal nodes removed) to represent
syntactic information and improved the temporal
relation discovery performance on THYME data
2
(Styler et al, 2014). In this paper, we also use
syntactic structure-enriched temporal relation dis-
covery as a vehicle to test our proposed kernel.
3 Methods
Here we decribe the Descending Path Kernel
(DPK).
1
http://www.timeml.org
2
http://thyme.healthnlp.org
82
Definition 1 (Descending Path): Let T be a
parse tree, v any non-terminal node in T , dv a
descendant of v, including terminals. A descending
path is the sequence of indexes of edges connecting
v and dv, denoted by [v ? ? ? ? ? dv]. The length l
of a descending path is the number of connecting
edges. When l = 0, a descending path is the non-
terminal node itself, [v]. Figure 2 illustrates a parse
tree and its descending paths of different lengths.
Suppose that all descending paths of a tree T are
indexed 1, ? ? ? , n, and path
i
(T ) is the frequency
of the i-th descending path in T . We represent T as
a vector of frequencies of all its descending paths:
?(T ) = (path
1
(T ), ? ? ? , path
n
(T )).
The similarity between any two trees T
1
and T
2
can be assessed via the dot product of their respec-
tive descending path frequency vector representa-
tions: K(T
1
, T
2
) = ??(T
1
),?(T
2
)?.
Compared with the previous tree kernels, our
descending path kernel has the following advan-
tages: 1) the sub-structures are simplified so that
they are more likely to be shared among trees,
and therefore the sparse feature issues of previous
kernels could be alleviated by this representation;
2) soft matching between two similar structures
(e.g., NP?DT JJ NN versus NP?DT NN) have
high similarity without reference to any corpus or
grammar rules;
Following Collins and Duffy (2001), we derive
a recursive algorithm to compute the dot product
of the descending path frequency vector represen-
tations of two trees T
1
and T
2
:
K(T
1
, T
2
) = ??(T
1
),?(T
2
)?
=
?
i
path
i
(T
1
) ? path
i
(T
2
)
=
?
n
1
?N
1
?
n
2
?N
2
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
=
?
n
1
?N
1
n
2
?N
2
C(n
1
, n
2
)
(1)
where N
1
and N
2
are the sets of nodes in T
1
and
T
2
respectively, i indexes the set of possible paths,
I
path
i
(n) is an indicator function that is 1 iff the
descending path
i
is rooted at node n or 0 other-
wise. C(n
1
, n
2
) counts the number of common
descending paths rooted at nodes n
1
and n
2
:
C(n
1
, n
2
) =
?
i
I
path
i
(n
1
) ? I
path
i
(n
2
)
C(n
1
, n
2
) can be computed in polynomial time by
the following recursive rules:
Rule 1: If n
1
and n
2
have different labels (e.g.,
?DT? versus ?NN?), then C(n1, n2) = 0;
Rule 2: Else if n
1
and n
2
have the same labels
and are both pre-terminals (POS tags), then
C(n
1
, n
2
) = 1 +
{
1 if term(n
1
) = term(n
2
)
0 otherwise.
where term(n) is the terminal symbol under n;
Rule 3: Else if n
1
and n
2
have the same labels
and they are not both pre-terminals, then:
C(n
1
, n
2
) = 1 +
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
where children(m) are the child nodes of m.
As in other tree kernel approaches (Collins and
Duffy, 2001; Moschitti, 2006), we use a discount
parameter ? to control for the disproportionately
large similarity values of large tree structures.
Therefore, Rule 2 becomes:
C(n
1
, n
2
) = 1 +
{
? if term(n
1
) = term(n
2
)
0 otherwise.
and Rule 3 becomes:
C(n
1
, n
2
) = 1 + ?
?
n
i
?children(n
1
)
n
j
?children(n
2
)
C(n
i
, n
j
)
Note that Eq. (1) is a convolution kernel under
the kernel closure properties described in Haus-
sler (1999). Rules 1-3 show the equivalence be-
tween the number of common descending paths
rooted at nodes n
1
and n
2
, and the number of
matching nodes below n
1
and n
2
.
In practice, there are many non-matching nodes,
and most matching nodes will have only a few
matching children, so the running time, as in SST,
will be approximated by the number of matching
nodes between trees.
3.1 Relationship with other kernels
For a given tree, DPK will generate significantly
fewer sub-structures than PTK, since it does not
consider all ordered permutations of a production
rule. Moreover, the fragments generated by DPK
are more likely to be shared among different trees.
For the number of corpus-wide fragments, it is
83
Kernel ID #Frag Sim N(Sim)
SST a 9 3 0.50
O
(
?|N
1
||N
2
|
)
b 15 2 0.25
c 63 7 0.20
DPK a 11 7 0.78
O
(
?
2
|N
1
||N
2
|
)
b 13 9 0.83
c 31 22 0.83
PTK a 20 10 0.67
O
(
?
3
|N
1
||N
2
|
)
b 36 15 0.65
c 127 34 0.42
Table 1: Comparison of the worst case computa-
tional complexicity (? - the maximum branching
factor) and kernel performance on the 3 examples
from Figure 1. #Frag is the number of fragments,
N(Sim) is the normalized similarity. Please see
the online supplementary note for detailed frag-
ments of example (a).
possible that DPK? SST? PTK. In Table 1, given
? = 1, we compare the performance of 3 kernels
on the three examples in Figure 1. Note that for
more complicated structures, i.e., examples b and
c, DPK generates fewer fragments than SST and
PTK, with more shared fragments among trees.
The complexity for all three kernels are at least
O
(
|N
1
||N
2
|
)
since they share the pairwise summa-
tion at the end of Equation 1. SST, due to its re-
quirement of exact production rule matching, only
takes one pass in the inner loop which adds a factor
of ? (the maximum branching factor of any pro-
duction rule). DPK does a pairwise summation
of children, which adds a factor of ?
2
to the com-
plexity. Finally, the efficient algorithm for PTK
is proved by Moschitti (2006) to contain a con-
stant factor of ?
3
. Table 1 orders the tree kernels
according by their listed complexity.
It may seem that the value of DPK is strictly in its
ability to evaluate all paths, which is not explicitly
accounted for by other kernels. However, another
view of the DPK is possible by thinking of it as
cheaply calculating rule production similarity by
taking advantage of relatively strict English word
ordering. Like SST and PTK, the DPK requires
the root category of two subtrees to be the same
for the similarity to be greater than zero. Unlike
SST and PTK, once the root category comparison
is successfully completed, DPK looks at all paths
that go through it and accumulates their similarity
scores independent of ordering ? in other words, it
will ignore the ordering of the children in its pro-
duction rule. This means, for example, that if the
rule production NP? NN JJ DT were ever found
in a tree, to DPK it would be indistinguishable from
the common production NP? DT JJ NN, despite
having inverted word order, and thus would have
a maximal similarity score. SST and PTK would
assign this pair a much lower score for having com-
pletely different ordering, but we suggest that cases
such as these are very rare due to the relatively
strict word ordering of English. In most cases, the
determiner of a noun phrase will be at the front, the
nouns will be at the end, and the adjectives in the
middle. So with small differences in production
rules (one or two adjectives, extra nominal modifier,
etc.) the PTK will capture similarity by compar-
ing every possible partial rule completion, but the
DPK can obtain higher and faster scores by just
comparing one child at a time because the ordering
is constrained by the language. This analysis does
lead to a hypothesis for the general viability of the
DPK, suggesting that in languages with freer word
order it may give inflated scores to structures that
are syntactically dissimilar if they have the same
constituent components in different order.
Formally, Moschitti (2006) showed that SST is
a special case of PTK when only the longest child
sequence from each tree is considered. On the other
end of the spectrum, DPK is a special case of PTK
where the similarity between rules only considers
child subsequences of length one.
4 Evaluation
We applied DPK to two published temporal relation
extraction systems: (Miller et al, 2013) in the
clinical domain and Cleartk-TimeML (Bethard,
2013) in the general domain respectively.
4.1 Narrative Container Discovery
The task here as described by Miller et al (2013) is
to identify the CONTAINS relation between a time
expression and a same-sentence event from clinical
notes in the THYME corpus, which has 78 notes
of 26 patients. We obtained this corpus from the
authors and followed their linear composite kernel
setting:
K
C
(s
1
, s
2
) = ?
P
?
p=1
K
T
(t
p
1
, t
p
2
)+K
F
(f
1
, f
2
) (2)
where s
i
is an instance object composed of flat fea-
tures f
i
and a syntactic tree t
i
. A syntactic tree t
i
84
can have multiple representations, as in Bag Tree
(BT), Path-enclosed Tree (PET), and Path Tree
(PT). For the tree kernel K
T
, subset tree (SST) ker-
nel was applied on each tree representation p. The
final similarity score between two instances is the
? -weighted sum of the similarities of all representa-
tions, combined with the flat feature (FF) similarity
as measured by a feature kernel K
F
(linear or poly-
nomial). Here we replaced the SST kernel with
DPK and tested two feature combinations FF+PET
and FF+BT+PET+PT. To fine tune parameters, we
used grid search by testing on the default develop-
ment data. Once the parameters were tuned, we
tested the system performance on the testing data,
which was set up by the original system split.
4.2 Cleartk-TimeML
We tested one sub-task from TempEval-2013 ?
the extraction of temporal relations between an
event and time expression within the same sen-
tence. We obtained the training corpus (Time-
Bank + AQUAINT) and testing data from the au-
thors (Bethard, 2013). Since the original features
didn?t contain syntactic features, we created a PET
tree extractor for this system. The kernel setting
was similar to equation (2), while there was only
one tree representation, PET tree, P=1. A linear
kernel was used as K
F
to evaluate the exact same
flat features as used by the original system. We
used the built-in cross validation to do grid search
for tuning the parameters. The final system was
tested on the testing data for reporting results.
4.3 Results and Discussion
Results are shown in Table 2. The top section
shows THYME results. For these experiments,
the DPK is superior when a syntactically-rich PET
representation is used. Using the full feature set of
Miller et al (2013), SST is superior to DPK and
obtains the best overall performance. The bottom
section shows results on TempEval-2013 data, for
which there is little benefit from either tree kernel.
Our experiments with THYME data show that
DPK can capture something in the linguistically
richer PET representation that the SST kernel can-
not, but adding BT and PT representations decrease
the DPK performance. As a shallow representation,
BT does not have much in the way of descending
paths for DPK to use. PT already ignores the pro-
duction grammar by removing the inner tree nodes.
DPK therefore cannot get useful information and
may even get misleading cues from these two rep-
Features K
T
P R F
THYME
FF+PET DPK 0.756 0.667 0.708
SST 0.698 0.630 0.662
FF+BT+ DPK 0.759 0.626 0.686
PET+PT SST 0.754 0.711 0.732
TempEval
FF+PET DPK 0.328 0.263 0.292
SST 0.325 0.263 0.290
FF - 0.309 0.266 0.286
Table 2: Comparison of tree kernel performance
for temporal relation extraction on THYME and
TempEval-2013 data.
resentations. These results show that, while DPK
should not always replace SST, there are represen-
tations in which it is superior to existing methods.
This suggests an approach in which tree representa-
tions are matched to different convolution kernels,
for example by tuning on held-out data.
For TempEval-2013 data, adding syntactic fea-
tures did not improve the performance significantly
(comparing F-score of 0.290 with 0.286 in Ta-
ble 3). Probably, syntactic information is not a
strong feature for all types of temporal relations on
TempEval-2013 data.
5 Conclusion
In this paper, we developed a novel convolution
tree kernel (DPK) for measuring syntactic similar-
ity. This kernel uses a descending path represen-
tation in trees to allow higher similarity scores on
partially matching structures, while being simpler
and faster than other methods for doing the same.
Future work will explore 1) a composite kernel
which uses DPK for PET trees, SST for BT and PT,
and feature kernel for flat features, so that different
tree kernels can work with their ideal syntactic rep-
resentations; 2) incorporate dependency structures
for tree kernel analysis 3) applying DPK to other
relation extraction tasks on various corpora.
6 Acknowledgements
Thanks to Sean Finan for technically supporting the
experiments. The project described was supported
by R01LM010090 (THYME) from the National
Library Of Medicine.
85
References
Fabio Aiolli, Giovanni Da San Martino, and Alessan-
dro Sperduti. 2009. Route kernels for trees. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 17?24. ACM.
Steven Bethard and James H Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and
semantic features. In Proceedings of the 4th Inter-
national Workshop on Semantic Evaluations, pages
129?132. Association for Computational Linguis-
tics.
Steven Bethard. 2013. Cleartk-timeml: A minimalist
approach to TempEval 2013. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), volume 2, pages 10?14.
Nate Chambers. 2013. Navytime: Event and time or-
dering from raw text. In Second Joint Conference
on Lexical and Computational Semantics (*SEM),
Volume 2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval 2013),
pages 73?77, Atlanta, Georgia, USA, June. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Neural Information
Processing Systems.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Califor-
nia in Santa Cruz.
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Pat-
wardhan, and Chris Welty. 2012. When did that
happen?: linking events and relations to timestamps.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 185?193. Association for Compu-
tational Linguistics.
Tetsuji Kuboyama, Kouichi Hirata, Hisashi Kashima,
Kiyoko F Aoki-Kinoshita, and Hiroshi Yasuda.
2007. A spectrum tree kernel. Information and Me-
dia Technologies, 2(1):292?299.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. Tipsem (english and spanish): Evaluating
CRFs and semantic roles in TempEval-2. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 284?291. Association for
Computational Linguistics.
Timothy Miller, Steven Bethard, Dmitriy Dligach,
Sameer Pradhan, Chen Lin, and Guergana Savova.
2013. Discovering temporal narrative containers
in clinical text. In Proceedings of the 2013 Work-
shop on Biomedical Natural Language Processing,
pages 18?26, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for
classifying temporal relations between events. Proc.
of the PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The TimeBank corpus. In Cor-
pus linguistics, volume 2003, page 40.
Geoffrey Sampson. 2000. A proposal for improving
the measurement of parse accuracy. International
Journal of Corpus Linguistics, 5(1):53?68.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger,
Heather Simpson, Robert Schrag, and Jonathan
Wright. 2010. The DARPA machine reading
program-encouraging linguistic and reasoning re-
search with a series of reading tasks. In LREC.
William Styler, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet de Groen, Brad Er-
ickson, Timothy Miller, Lin Chen, Guergana K.
Savova, and James Pustejovsky. 2014. Temporal
annotations in the clinical domain. Transactions
of the Association for Computational Linguistics,
2(2):143?154.
Min Zhang, Wanxiang Che, Ai Ti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007. A
grammar-driven convolution tree kernel for seman-
tic role classification. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 200?207.
86
Temporal Annotation in the Clinical Domain
William F. Styler IV1, Steven Bethard2, Sean Finan3, Martha Palmer1,
Sameer Pradhan3, Piet C de Groen4, Brad Erickson4, Timothy Miller3,
Chen Lin3, Guergana Savova3 and James Pustejovsky5
1 Department of Linguistics, University of Colorado at Boulder
2 Department of Computer and Information Sciences, University of Alabama at Birmingham
3 Children?s Hospital Boston Informatics Program and Harvard Medical School
4 Mayo Clinic College of Medicine, Mayo Clinic, Rochester, MN
5 Department of Computer Science, Brandeis University
Abstract
This article discusses the requirements of
a formal specification for the annotation of
temporal information in clinical narratives.
We discuss the implementation and extension
of ISO-TimeML for annotating a corpus of
clinical notes, known as the THYME cor-
pus. To reflect the information task and the
heavily inference-based reasoning demands
in the domain, a new annotation guideline
has been developed, ?the THYME Guidelines
to ISO-TimeML (THYME-TimeML)?. To
clarify what relations merit annotation, we
distinguish between linguistically-derived and
inferentially-derived temporal orderings in the
text. We also apply a top performing Temp-
Eval 2013 system against this new resource to
measure the difficulty of adapting systems to
the clinical domain. The corpus is available to
the community and has been proposed for use
in a SemEval 2015 task.
1 Introduction
There is a long-standing interest in temporal reason-
ing within the biomedical community (Savova et al.,
2009; Hripcsak et al., 2009; Meystre et al., 2008;
Bramsen et al., 2006; Combi et al., 1997; Keravnou,
1997; Dolin, 1995; Irvine et al., 2008; Sullivan et
al., 2008). This interest extends to the automatic ex-
traction and interpretation of temporal information
from medical texts, such as electronic discharge sum-
maries and patient case summaries. Making effective
use of temporal information from such narratives is
a crucial step in the intelligent analysis of informat-
ics for medical researchers, while an awareness of
temporal information (both implicit and explicit) in a
text is also necessary for many data mining tasks.
It has also been demonstrated that the temporal in-
formation in clinical narratives can be usefully mined
to provide information for some higher-level tempo-
ral reasoning (Zhao et al., 2005). Robust temporal
understanding of such narratives, however, has been
difficult to achieve, due to the complexity of deter-
mining temporal relations among events, the diver-
sity of temporal expressions, and the interaction with
broader computational linguistic issues.
Recent work on Electronic Health Records (EHRs)
points to new ways to exploit and mine the informa-
tion contained therein (Savova et al., 2009; Roberts
et al., 2009; Zheng et al., 2011; Turchin et al., 2009).
We target two main use cases for extracted data. First,
we hope to enable interactive displays and summaries
of the patient?s records to the physician at the time of
visit, making a comprehensive review of the patient?s
history both faster and less prone to oversights. Sec-
ond, we hope to enable temporally-aware secondary
research across large databases of medical records
(e.g., ?What percentage of patients who undergo pro-
cedure X develop side-effect Y within Z months??).
Both of these applications require the extraction of
time and date associations for critical events and the
relative ordering of events during the patient?s period
of care, all from the various records which make up a
patient?s EHR. Although we have these two specific
applications in mind, the schema we have developed
is generalizable and could potentially be embedded
in a wide variety of biomedical use cases.
Narrative texts in EHRs are temporally rich doc-
uments that frequently contain assertions about the
timing of medical events, such as visits, laboratory
values, symptoms, signs, diagnoses, and procedures
(Bramsen et al., 2006; Hripcsak et al., 2009; Zhou
et al., 2008). Temporal representation and reason-
ing in the medical record are difficult due to: (1) the
diversity of time expressions; (2) the complexity of
determining temporal relations among events (which
are often left to inference); (3) the difficulty of han-
dling the temporal granularity of an event; and (4)
143
Transactions of the Association for Computational Linguistics, 2 (2014) 143?154. Action Editor: Ellen Riloff.
Submitted 9/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
general issues in natural language processing (e.g.,
ambiguity, anaphora, ellipsis, conjunction). As a re-
sult, the signals used for reconstructing a timeline can
be both domain-specific and complex, and are often
left implicit, requiring significant domain knowledge
to accurately detect and interpret.
In this paper, we discuss the demands on accurately
annotating such temporal information in clinical
notes. We describe an implementation and extension
of ISO-TimeML (Pustejovsky et al., 2010), devel-
oped specifically for the clinical domain, which we
refer to as the ?THYME Guidelines to ISO-TimeML?
(?THYME-TimeML?), where THYME stands for
?Temporal Histories of Your Medical Events?. A sim-
plified version of these guidelines formed the basis
for the 2012 i2b2 medical-domain temporal relation
challenge (Sun et al., 2013a).
This is being developed in the context of the
THYME project, whose goal is to both create ro-
bust gold standards for semantic information in clini-
cal notes, as well as to develop state-of-the-art algo-
rithms to train and test on this dataset.
Deriving timelines from news text requires the con-
crete realization of context-dependent assumptions
about temporal intervals, orderings and organization,
underlying the explicit signals marked in the text
(Pustejovsky and Stubbs, 2011). Deriving patient
history timelines from clinical notes also involves
these types of assumptions, but there are special de-
mands imposed by the characteristics of the clinical
narrative. Due to both medical shorthand practices
and general domain knowledge, many event-event
relations are not signaled in the text at all, and rely
on a shared understanding and common conceptual
models of the progressions of medical procedures
available only to readers familiar with language use
in the medical community.
Identifying these implicit relations and temporal
properties puts a heavy burden on the annotation
process. As such, in the THYME-TimeML guideline,
considerable effort has gone into both describing and
proscribing the annotation of temporal orderings that
are inferable only through domain-specific temporal
knowledge.
Although the THYME guidelines describe a num-
ber of departures from the ISO-TimeML standard for
expediency and ease of annotation, this paper will
focus on those differences specifically motivated by
the needs of the clinical domain, and on the conse-
quences for systems built to extract temporal data in
both the clinical and general domain.
2 The Nature of Clinical Documents
In the THYME corpus, we have been examining
1,254 de-identified1 notes from a large healthcare
practice (the Mayo Clinic), representing two distinct
fields within oncology: brain cancer, and colon can-
cer. To date, we have principally examined two dif-
ferent general types of clinical narrative in our EHRs:
clinical notes and pathology reports.
Clinical notes are records of physician interactions
with a patient, and often include multiple, clearly
delineated sections detailing different aspects of the
patient?s care and present illness. These notes are
fairly generic across institutions and specialities, and
although some terms and inferences may be specific
to a particular type of practice (such as oncology),
they share a uniform structure and pattern. The ?His-
tory of Present Illness?, for example, summarizes the
course of the patient?s chief complaint, as well as the
interventions and diagnostics which have been thus
far attempted. In other sections, the doctor may out-
line her current plan for the patient?s treatment, then
later describe the patient?s specific medical history,
allergies, care directives, and so forth.
Most critically for temporal reasoning, each clin-
ical note reflects a single time in the patient?s treat-
ment history at which all of the doctor?s statements
are accurate (the DOCTIME), and each section tends
to describe events of a particular timeframe. For
example, ?History of Present illness? predominantly
describes events occuring before DOCTIME, whereas
?Medications? provides a snapshot at DOCTIME and
?Ongoing Care Orders? discusses events which have
not yet occurred.2
Clinical notes contain rich temporal information
and background, moving fluidly from prior treat-
ments and symptoms to present conditions to future
interventions. They are also often rich with hypo-
thetical statements (?if the tumor recurs, we can...?),
each of which can form its own separate timeline.
By constrast, pathology notes are quite different.
Such notes are generated by a medical pathologist
1Although most patient information was removed, dates
and temporal information were not modified according to this
project?s specific data use agreement.
2One complication is the propensity of doctors and automated
systems to later update sections in a note without changing the
timestamp or metadata. We have added a SECTIONTIME to keep
these updated sections from affecting our overall timeline.
144
upon receipt and analysis of specimens (ranging from
tissue samples from biopsy to excised portions of
tumor or organs). Pathology notes provide crucial
information to the patient?s doctor confirming the
malignancy (cancer) in samples, describing surgi-
cal margins (which indicate whether a tumor was
completely excised), and classifying and ?staging? a
tumor, describing the severity and spread of the can-
cer. Because the information in such notes pertains
to samples taken at a single moment in time, they are
temporally sparse, seldom referring to events before
or after the examination of the specimen. However,
they contain critical information about the state of
the patient?s illness and about the cancer itself, and
must be interpreted to understand the history of the
patient?s illness.
Most importantly, in all EHRs, we must contend
with the results of a fundamental tension in mod-
ern medical records: hyper-detailed records provide
a crucial defense against malpractice litigation, but
including such detail takes enormous time, which
doctors seldom have. Given that these notes are writ-
ten by and for medical professionals (who form a
relatively insular speech community), a great many
non-standard expressions, abbreviations, and assump-
tions of shared knowledge are used, which are simul-
taneously concise and detail-rich for others who have
similar backgrounds.
These time-saving devices can range from tempo-
rally loaded acronyms (e.g., ?qid?, Latin for quater in
die, ?four times daily?), to assumed orderings (a diag-
nostic test for a disorder is assumed to come before
the procedure which treats it), and even to completely
implicit events and temporal details. For example,
consider the sentence in (1).
(1) Colonoscopy 3/12/10, nodule biopsies negative
We must understand that during the colonoscopy,
the doctor obtained biopsies of nodules, which were
packaged and sent to a pathologist, who reviewed
them and determined them to be ?negative? (non-
cancerous).
In such documents, we must recover as much tem-
poral detail as possible, even though it may be ex-
pressed in a way which is not easily understood out-
side of the medical community, let alone by linguists
or automated systems. We must also be aware of the
legal relevance of some events (e.g., ?We discussed
the possible side effects?), even when they may not
seem relevant to the patient?s actual care.
Finally, each specialty and note type has separate
conventions. Within colon cancer notes, the Amer-
ican Joint Committee on Cancer (AJCC) Staging
Codes (e.g., T4N1, indicating the nature of the tumor,
lymph node and metastasis involvement) are metic-
ulously recorded, but are largely absent in the brain
cancer notes which make up the second corpus in
our project. So, although clinical notes share many
similarities, annotators without sufficient domain ex-
pertise may require additional training to adapt to the
inferences and nuances of a new clinical subdomain.
3 Interpreting ?Event? and Temporal
Expressions in the Clinical Domain
Much prior work has been done on standardizing
the annotation of events and temporal expressions
in text. The most widely used approach is the ISO-
TimeML specification (Pustejovsky et al., 2010), an
ISO standard that provides a common framework for
annotating and analyzing time, events, and event rela-
tions. As defined by ISO-TimeML, an EVENT refers
to anything that can be said ?to obtain or hold true, to
happen or to occur?. This is a broad notion of event,
consistent with Bach?s use of the term ?eventuality?
(Bach, 1986) as well as the notion of fluents in AI
(McCarthy, 2002).
Because the goals of the THYME project involve
automatically identifying the clinical timeline for
a patient from clincal records, the scope of what
should be admitted into the domain of events is inter-
preted more broadly than in ISO-TimeML3. Within
the THYME-TimeML guideline, an EVENT is any-
thing relevant to the clinical timeline, i.e., anything
that would show up on a detailed timeline of the pa-
tient?s care or life. The best single-word syntactic
head for the EVENT is then used as its span. For
example, a diagnosis would certainly appear on such
a timeline, as would a tumor, illness, or procedure.
On the other hand, entities that persist throughout
the relevant temporal period of the clinical timeline
(endurants in ontological circles) would not be con-
sidered as event-like. This includes the patient, other
humans mentioned (the patient?s mother-in-law or
the doctor), organizations (the emergency room),
non-anatomical objects (the patient?s car), or indi-
vidual parts of the patient?s anatomy (an arm is not
an EVENT unless missing or otherwise notable).
To meet our explicit goals, the THYME-TimeML
guideline introduces two additional levels of interpre-
3Our use of the term ?EVENT? corresponds with the less
specific ISO-TimeML term ?Eventuality?
145
tation beyond that specified by ISO-TimeML: (i) a
well-defined task; and (ii) a clearly identified domain.
By focusing on the creation of a clinical timeline
from clinical narrative, the guideline imposes con-
straints that cannot be assumed for a broadly defined
and domain independent annotation schema.
Some EVENTs annotated under our guideline are
considered meaningful and eventive mostly by virtue
of a specific clinical or legal value. For example,
AJCC Staging Codes (discussed in Section 2) are
eventive only in the sense of the code being assigned
to a tumor at a given moment in the patient?s care.
However, they are of such critical importance and
informative value to doctors that we have chosen to
annotate them specifically so that they will show up
on the patient?s timeline in a clinical setting.
Similarly, because of legal pressures to establish in-
formed consent and patient knowledge of risk, entire
paragraphs of clinical notes are dedicated to docu-
menting the doctor?s discussion of risks, plans, and
alternative strategies. As such, we annotate verbs of
discussion (?We talked about the risks of this drug?),
consent (?She agreed with the current plan?), and
comprehension (?Mrs. Larsen repeated the potential
side effects back to me?), even though they are more
relevant to legal defense than medical treatment.
It is also because of this grounding in clinical lan-
guage that entities and other non-events are often
interpreted in terms of their associated eventive prop-
erties. There are two major types for which this is a
significant shift in semantic interpretation:
(2) a Medication as Event:
Orders: Lariam twice daily.
b Disorder as Event:
Tumor of the left lung.
In both these cases, entities which are not typically
marked as events are identified as such, because they
contribute significant information to the clinical time-
line being constructed. In (2a), for example, the
TIMEX3 ?twice daily? is interpreted as scoping over
the eventuality of the patient taking the medication,
not the prescription event. In sentence (2b), the ?tu-
mor? is interpreted as a stative eventuality of the
patient having a tumor located within an anatomical
region, rather than an entity within an entity.
Within the medical domain, these eventive inter-
pretations of medications, growths and status codes
are unambiguous and consistent. Doctors in clini-
cal notes (unlike in biomedical research texts) do
not discuss medications without an associated (im-
plicit) administering EVENT (though some mentions
may be hypothetical, generic or negated). Similarly,
mentions of symptoms or disorders reflect occur-
rences in a patient?s life, rather than abstract entities.
With these interpretations in mind, we can safely in-
fer, for instance, that all UMLS (Unified Medical
Language System, (Bodenreider, 2004)) entities of
the types Disorder, Chemical/Drug, Procedure and
Sign/Symptom will be EVENTs.
In general, in the medical domain, it is essential to
read ?between the lines? of the shorthand expressions
used by the doctors, and recognize implicit events
that are being referred to by specific anatomical sites
or medications.
4 Modifications to ISO-TimeML for the
Clinical Domain
Overall, we have found that the specification required
for temporal annotation in the clinical domain does
not require substantial modification from existing
specifications for the general domain. The clinical
domain includes no shortage of inferences, short-
hands, and unusual use of language, but the structure
of the underlying timeline is not unique.
As a result of this, we have been able to adopt most
of the framework from ISO-TimeML, adapting the
guidelines where needed, as well as reframing the
focus of what gets annotated. This is reflected in a
comprehensive guideline, incorporating the specific
patterns and uses of events and temporal expressions
as seen in clinical data. This approach allows the
resulting annotations to be interoperable with exist-
ing solutions, while still accommodating the major
differences in the nature of the texts. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org4
Our extensions of the ISO-TimeML specification
to the clinical domain are intended to address specific
constructions, meanings, and phenomena in medical
texts. Our schema differs from ISO-TimeML in a
few notable ways.
EVENT Properties We have both simplified the
ISO-TimeML coding of EVENTs, and extended it to
meet the needs of the clinical domain and the specific
language goals of the clinical narrative.
4Access to the corpus will require a data use agreement.
More information about this process is available from the corpus
website.
146
Consider, for example, how modal subordination is
handled in ISO-TimeML. This involves the semantic
characterization of an event as ?likely?, ?possible?, or
as presented by observation, evidence, or hearsay. All
of these are accounted for compositionally in ISO-
TimeML within the SLINK (Subordinating Link)
relation (Pustejovsky et al., 2005). While accept-
ing ISO-TimeML?s definition of event modality, we
have simplified the annotation task within the cur-
rent guideline, so that EVENTs now carry attributes
for ?contextual modality?, ?contextual aspect? and
?permanence?.
Contextual modality allows the values ACTUAL,
HYPOTHETICAL, HEDGED, and GENERIC. ACTUAL
covers EVENTs which have actually happened, e.g.,
?We?ve noted a tumor?. HYPOTHETICAL covers con-
ditionals and possibilities, e.g., ?If she develops a
tumor?. HEDGED is for situations where doctors
proffer a diagnosis, but do so cautiously, to avoid
legal liability for an incorrect diagnosis or for over-
looking a correct one. For example:
(3) a. The signal in the MRI is not inconsistent
with a tumor in the spleen.
b. The rash appears to be measles, awaiting
antibody test to confirm.
These HEDGED EVENTs are more real than a hypo-
thetical diagnosis, and likely merit inclusion on a
timeline as part of the diagnostic history, but must
not be conflated with confirmed fact. These (and
other forms of uncertainty in the medical domain)
are discussed extensively in (Vincze et al., 2008). In
contrast, GENERIC EVENTs do not refer to the pa-
tient?s illness or treatment, but instead discuss illness
or treatment in general (often in the patient?s specific
demographic). For example:
(4) In other patients without significant comor-
bidity that can tolerate adjuvant chemother-
apy, there is a benefit to systemic adjuvant
chemotherapy.
These sections would be true if pasted into any pa-
tient?s note, and are often identical chunks of text
repeatedly used to justify a course of action or treat-
ment as well as to defend against liability.
Contextual Aspect (to distinguish from grammati-
cal aspect), allows the clinically-necessary category,
INTERMITTENT. This serves to distinguish intermit-
tent EVENTs (such as vomiting or seizures) from
constant, more stative EVENTs (such as fever or sore-
ness). For example, the bolded EVENT in (5a) would
be marked as INTERMITTENT, while that in (5b)
would not:
(5) a She has been vomiting since June.
b She has had swelling since June.
In the first case, we assume that her vomiting has
been intermittent, i.e., there were several points since
June in which she was not actively vomiting. In the
second case, unless made otherwise explicit (?she has
had occasional swelling?), we assume that swelling
was a constant state. This property is also used when
a particular instance of an EVENT is intermittent,
even though it generally would not be:
(6) Since starting her new regime, she has had occa-
sional bouts of fever, but is feeling much better.
The permanence attribute has two values, FINITE
and PERMANENT. Permanence is a property of dis-
eases themselves, roughly corresponding to the med-
ical concept of ?chronic? vs. ?acute? disease, which
marks whether a disease is persistent following diag-
nosis. For example, a (currently) uncurable disease
like Multiple Sclerosis would be classed as PERMA-
NENT, and thus, once mentioned in a patient?s note,
will be assumed to persist through the end of the
patient?s timeline. This is compared with FINITE
disorders like ?Influenza? or ?fever?, which, if not
mentioned in subsequent notes, should be considered
cured and no longer belongs on the patient?s time-
line. Because it requires domain-specific knowledge,
although present in the specification, Permanence
is not currently annotated. However, annotators are
trained on the basic idea and told about subsequent
axiomatic assignment. The addition of this property
to our schema is designed to relieve annotators of any
feeling of obligation to express this inferred informa-
tion in some other way.
TIMEX3 Types Temporal expressions (TIMEX3s)
in the clinical domain function the same as in the gen-
eral linguistic community, with two notable excep-
tions. ISO-TimeML SETs (statements of frequency)
occur quite frequently in the medical domain, par-
ticularly with regard to medications and treatments.
Medication sections within notes often contain long
lists of medications, each with a particular associated
set (?Claritin 30mg twice daily?), and further tempo-
ral specification is not uncommon (e.g., ?three times
per day at meals?, ?once a week at bedtime?).
The second major change for the medical domain
is a new type of TIMEX3 which we call PREPOS-
TEXP. This covers temporally complex terms like
147
?preoperative?, ?postoperative?, and ?intraoperative?.
These temporal expressions designate a span of time
bordered, usually only on one side, by the incorpo-
rated event (an operation, in the previous EVENTs).
In many cases, the referent is clear:
(7) She underwent hemicolectomy last week, and
had some postoperative bleeding.
Here we understand that ?postoperative? refers to
?the period of time following the hemicolectomy?. In
these cases, the PREPOSTEXP makes explicit a tempo-
ral link between the bleeding and the hemicolectomy.
In other cases, no clear referent is present:
(8) Patient shows some post-procedure scarring.
In these situations, where no procedure is mentioned
(or the reference is never explicitly resolved), we
treat the PREPOSTEXP as a narrative container (see
Section 5), covering the span of time following the
unnamed procedure.
Finally, it is worth noting that the process of nor-
malizing those TIMEX3s is significantly more com-
plex relative to the general domain, because many
temporal expressions are anchored not to dates or
times, but to other EVENTs (whose dates are often
not mentioned or not known by the physician). As
we move towards a complete system, we are working
to expand the ISO-TimeML system for TIMEX3 nor-
malization to allow some value to be assigned to a
phrase like ?in the months after her hemicolectomy?
when no referent date is present. ISO-TimeML, in
discussion with ISO TC 37SC 4, plans to reference
to such TIMEX3s in a future release of the standard.
5 Temporal Ordering and Narrative
Containers
The semantic content and informational impact of
a timeline is encoded in the ordering relations that
are identified between the temporal and event expres-
sions present in clinical notes. ISO-TimeML speci-
fies the standard thirteen ?Allen relations? from the
interval calculus (Allen, 1983), which it refers to as
TLINK values. For unguided, general-purpose annota-
tion, the number of relations that could be annotated
grows quadratically with the number of events and
times, and the task quickly becomes unmanageable.
There are, however, strategies that we can adopt to
make this labeling task more tractable. Temporal
ordering relations in text are of three kinds:
1. Relations between two events
2. Relations between two times
3. Relations between a time and an event.
ISO-TimeML, as a formal specification of the tem-
poral information conveyed in language, makes no
distinction between these ordering types. Humans,
however, do make distinctions, based on local tempo-
ral markers and the discourse relations established in
a narrative (Miltsakaki et al., 2004; Poesio, 2004).
Because of the difficulty of humans capturing ev-
ery relationship present in the note (and the disagree-
ment which arises when annotators attempt to do so),
it is vital that the annotation guidelines describe an
approach that reduces the number of relations that
must be considered, but still results in maximally in-
formative temporal links. We have found that many
of the weaknesses in prior annotation approaches
stem from interaction between two competing goals:
? The guideline should specify certain types of an-
notations that should be performed;
? The guideline should not force annotations to be
performed when they need not be.
Failing in the first goal will result in under-annotation
and the neglect of relations which provide necessary
information for inference and analysis. Failure in the
second goal results in over-annotation, creating com-
plex webs of temporal relations which yield mostly
inferable information, but which complicate annota-
tion and adjudication considerably.
Our method of addressing both goals in tempo-
ral relations annotation is that of the narrative con-
tainer, discussed in Pustejovsky and Stubbs (2011).
A narrative container can be thought of as a temporal
bucket into which an EVENT or series of EVENTs
may fall, or a natural cluster of EVENTs around a
given time or situation. These narrative containers
are often represented (or ?anchored?) by dates or
other temporal expressions (within which a variety
of different EVENTs occur), although they can also
be anchored to more abstract concepts (?recovery?
which might involve a variety of EVENTs) or even
durative EVENTs (many other EVENTs can occur dur-
ing a surgery). Rather than marking every possible
TLINK between each EVENT, we instead try to link
all EVENTs to their narrative containers, and then
link those containers so that the contained EVENTs
can be linked by inference.
First, annotators assign each event to one of four
broad narrative containers: before the DOCTIME, be-
fore and overlapping the DOCTIME, just overlapping
the DOCTIME or after the DOCTIME. This narrative
148
container is identified by the EVENT attribute Doc-
TimeRel. After the assignment of DocTimeRel, the
remainder of the narrative container relations must
be specified using temporal links (TLINKs). There
are five different temporal relations used for such
TLINKs: BEFORE, OVERLAP, BEGINS-ON, ENDS-ON
and CONTAINS5. Due to our narrative container ap-
proach, CONTAINS is the most frequent relation by a
large margin.
EVENTs serving as narrative container anchors are
not tagged as containers per-se. Instead, annotators
use the narrative container idea to help them visu-
alize the temporal relations within a document, and
then make a series of CONTAINS TLINK annotations
which establish EVENTs and TIMEX3s as anchors,
and specify their contents. If the annotators do their
jobs correctly, properly implementing DocTimeRel
and creating accurate TLINKs, a good understanding
of the narrative containers present in a document will
naturally emerge from the annotated text.
The major advantage introduced with narrative
containers is this: a narrative event is placed within a
bounding temporal interval which is explicitly men-
tioned in the text. This allows EVENTs within sep-
arate containers to be linked by post-hoc inference,
temporal reasoning, and domain knowledge, rather
than by explicit (and time-consuming) one-by-one
temporal relations annotation.
A secondary advantage is that this approach works
nicely with the general structure of story-telling in
both the general and clinical domains, and provides a
compelling and useful metaphor for interpreting time-
lines. Often, especially in clinical histories, doctors
will cluster discussions of symptoms, interventions
and diagnoses around a given date (e.g. a whole para-
graph starting ?June 2009:?), a specific hospitaliza-
tion (?During her January stay at Mercy?), or a given
illness or treatment (?While she underwent Chemo?).
Even when specific EVENTs are not explicitly or-
dered within a cluster (often because the order can be
easily inferred with domain knowledge), it is often
quite easy to place the EVENTs into containers, and
just a few TLINKs can order the containers relative to
one another with enough detail to create a clinically
useful understanding of the overall timeline.
Narrative containers also allow the inference of re-
lations between sub-events within nested containers:
5This is a subset of the ISO-TimeML TLINK types, excluding
those seldom occurring in medical records, like ?simultaneous?
as well as inverse relations like ?during? or ?after?.
(9) December 19th: The patient underwent an MRI
and EKG as well as emergency surgery. Dur-
ing the surgery, the patient experienced mild
tachycardia, and she also bled significantly
during the initial incision.
1. December 19th CONTAINS MRI
2. December 19th CONTAINS EKG
3. December 19th CONTAINS surgery
a. surgery CONTAINS tachycardia
b. surgery CONTAINS incision
c. incision CONTAINS bled
Through our container nesting, we can automatically
infer that ?bled? occurred on December 19th (because
?19th? CONTAINS ?surgery? which CONTAINS ?inci-
sion? which CONTAINS ?bled?). This also allows the
capture of EVENT/sub-event relations, and the rapid
expression of complex temporal interactions.
6 Explicit vs. Inferable Annotation
Given a specification language, there are essentially
two ways of introducing the elements into the docu-
ment (data source) being annotated:6
? Manual annotation: Elements are introduced into
the document directly by the human annotator fol-
lowing the guideline.
? Automatic (inferred) annotation: Elements are cre-
ated by applying an automated procedure that in-
troduces new elements that are derivable from the
human annotations.
As such, there is a complex interaction between spec-
ification and guideline, and we focus on how the
clinical annotation task has helped shape and refine
the annotation guidelines. It is important to note that
an annotation guideline does not necessarily force
the markup of certain elements in a text, even though
the specification language (and the eventual goal of
the project) might require those annotations to exist.
In some cases, these added annotations are derived
logically from human annotations. Explicitly marked
temporal relations can be used to infer others that are
not marked but exist implicitly through closure. For
instance, given EVENTs A, B and C and TLINKs ?A
BEFORE B? and ?B BEFORE C?, the TLINK ?A BE-
FORE C? can be automatically inferred. Repeatedly
applying such inference rules allows all inferable
6We ignore the application of automatic techniques, such as
classifiers trained on external datasets, as our focus here is on
the preparation of the gold standard used for such classifiers.
149
TLINKs to be generated (Verhagen, 2005). We can
use this idea of closure to show our annotators which
annotations need not be marked explicitly, saving
time and effort. We have also incorporated these clo-
sure rules into our inter-annotator agreement (IAA)
calculation for temporal relations, described further
in Section 7.2.
The automatic application of rules following the
annotation of the text is not limited to the marking
of logically inferable relations or EVENTs. In the
clinical domain, the combination of within-group
shared knowledge and pressure towards concise writ-
ing leads to a number of common, inferred relations.
Take, for example, the sentence:
(10) Jan 2013: Colonoscopy, biopsies. Pathology
showed adenocarcinoma, resected at Mercy.
Diagnosis T3N1 Adenocarcinoma.
In this sentence, only the CONTAINS relations be-
tween ?Jan 2013? and the EVENTs (in bold) are
explicitly stated. However, based on the known
progression-of-care for colon cancer, we can infer
that the colonoscopy occurs first, biopsies occur dur-
ing the colonoscopy, pathology happens afterwards,
a diagnosis (here, adenocarcinoma) is returned after
pathology, and resection of the tumor occurs after
diagnosis. The presence of the AJCC staging infor-
mation in the final sentence (along with the confir-
mation of the adenocarcinoma diagnosis) implies a
post-surgical pathology exam of the resected spec-
imen, as the AJCC staging information cannot be
determined without this additional examination.
These inferences come naturally to domain ex-
perts but are largely inaccessible to people outside
the medical community without considerable anno-
tator training. Making explicit our understanding of
these ?understood orderings? is crucial; although they
are not marked by human annotators in our schema,
the annotators often found it initially frustrating to
leave these (purely inferential) relations unstated. Al-
though many of our (primarily linguistically trained)
annotators learned to see these patterns, we chose to
exclude them from the manual task since newer an-
notators with varying degrees of domain knowledge
may struggle if asked to manually annotate them.
Similar unspoken-but-understood orderings are
found throughout the clinical domain. As mentioned
in Section 3, both Permanence and Contextual As-
pect:Intermittent are properties of symptoms and dis-
eases themselves, rather than of the patient?s particu-
lar situation. As such, these properties could easily
Annotation Type Raw Count
EVENT 15,769
TIMEX3 1,426
LINK 7935
Total 25,130
Table 1: Raw Frequency of Annotation Types
TLINK Type Raw Count % of TLINKs
CONTAINS 5,112 64.42%
OVERLAP 1,205 15.19%
BEFORE 1,004 12.65%
BEGINS-ON 488 6.15%
ENDS-ON 126 1.59%
Total 7,935 100.00%
Table 2: Relative Frequency of TLINK types
be identified and marked across a medical ontology,
and then be automatically assigned to EVENTs rec-
ognized as specific medical named entities.
Finally, due to the peculiarities of EHR systems,
some annotations must be done programatically. Ex-
act dates of patient visit (or of pathology/radiology
consult) are often recorded as metadata on the EHR
itself, rather than within the text, making the canoni-
cal DOCTIME (or time of automatic section modifi-
cations) difficult to access in de-identified plaintext
data, but easy to find automatically.
7 Results
We report results on the annotations from the here-
released subset of the THYME colon cancer corpus,
which includes clinical notes and pathology reports
for 35 patients diagnosed with colon cancer for a
total of 107 documents. Each note was annotated
by a pair of graduate or undergraduate students in
Linguistics at the University of Colorado, then adju-
dicated by a domain expert. These clinical narratives
were sampled from the EHRs of a major healthcare
center (the Mayo Clinic). They were deidentified for
all patient-sensitive information; however, original
dates were retained.
7.1 Descriptive Statistics
Table 1 presents the raw counts for events, temporal
expressions and links in the adjudicated gold anno-
tations. Table 2 presents the number and percentage
of TLINKs by type in the adjudicated relations gold
annotations.
150
Annotation Type F1-Score Alpha
EVENT 0.8038 0.7899
TIMEX3 0.8047 0.6705
LINK: Participants only 0.5012 0.4999
LINK: Participants+type 0.4506 0.4503
LINK: CONTAINS 0.5630 0.5626
Table 3: IAA (F1-Score and Alpha) by annotation type
EVENT Property F1-Score Alpha
DocTimeRel 0.7189 0.6889
Cont.Aspect 0.9947 0.9930
Cont.Modality 0.9547 0.9420
Table 4: IAA (F1-Score and Alpha) for EVENT properties
7.2 Inter-annotator Agreement
We report inter-annotator agreement (IAA) results
on the THYME corpus. Each note was annotated by
two independent annotators. The final gold standard
was produced after disagreement adjudication by a
third annotator was performed.
We computed the IAA as F1-score and Krippen-
dorff?s Alpha (Krippendorff, 2012) by applying clo-
sure, using explicitly marked temporal relations to
identify others that are not marked but exist implicitly.
In the computation of the IAA, inferred-only TLINKs
do not contribute to the score, matched or unmatched.
For instance, if both annotators mark A BEFORE B
and B BEFORE C, to prevent artificially inflating the
agreement score, the inferred A BEFORE C is ignored.
Likewise, if one annotator marked A BEFORE B and
B BEFORE C and the other annotator did not, the
inferred A BEFORE C is not counted. However, if
one annotator did explicitly mark A BEFORE C, then
an equivalent inferred TLINK would be used to match
it. EVENT and TIMEX3 IAA was generated based
on exact and overlapping spans, respectively. These
results are reported in Table 3.
The THYME corpus also differs from ISO-
TimeML in terms of EVENT properties, with the
addition of DocTimeRel, ContextualModality and
ContextualAspect. IAA for these properties is in
Table 4.
7.3 Baseline Systems
To get an idea of how much work will be neces-
sary to adapt existing temporal information extrac-
tion systems to the clinical domain, we took the freely
available ClearTK-TimeML system (Bethard, 2013),
TempEval 2013 THYME Corpus
P R F1 P R F1
TIMEX3 83.2 71.7 77.0 59.3 42.8 49.7
EVENT 81.4 76.4 78.8 78.9 23.9 36.6
DocTimeRel - - - 47.4 47.4 47.4
LINK7 28.6 30.9 26.6 22.7 18.6 20.4
EVENT-TIMEX3 - - - 32.3 60.7 42.1
EVENT-EVENT - - - 7.0 3.0 4.2
Table 5: Performance of ClearTK-TimeML models, as
reported in the TempEval 2013 competition, and as applied
to the THYME Corpus development set.
which was among the top performing systems in
TempEval 2013 (UzZaman et al., 2013), and eval-
uated its performance on the THYME corpus.
ClearTK-TimeML uses support vector machine
classifiers trained on the TempEval 2013 training
data, employing a small set of features including
character patterns, tokens, stems, part-of-speech tags,
nearby nodes in the constituency tree, and a small
time word gazetteer. For EVENTs and TIMEX3s,
the ClearTK-TimeML system could be applied di-
rectly to the THYME corpus. For DocTimeRels, the
relation for an EVENT was taken from the TLINK
between that EVENT and the document creation time,
after mapping INCLUDES to OVERLAP. EVENTs
with no such TLINK were assumed to have a Doc-
TimeRel of OVERLAP. For other temporal relations,
INCLUDES was mapped to CONTAINS.
Results of this system on TempEval 2013 and the
THYME corpus are shown in Table 5. For time ex-
pressions, performance when moving to the clinical
data degrades about 25%, from F1 of 77.0 to 49.7.
For events, the degradation is much larger, about
40%, from 78.8 to 36.6, most likely because of the
large number of clinical symptoms, diseases, disor-
ders, etc. which have never been observed by the
system during training. Temporal relations are a bit
more difficult to compare because TempEval lumped
DocTimeRel and other temporal relations together
and had several differences in their evaluation met-
ric7. However, we at least can see that performance
of the ClearTK-TimeML system on temporal rela-
tions is low on clinical text, achieving only F1 of
20.4.
These results suggest that clinical narratives do
7The TempEval 2013 evaluation metric penalized systems
for parts of the text that were not examined by annotators, and
used different variants of closure-based precision and recall.
151
indeed present new challenges for temporal informa-
tion extraction systems, and that having access to
domain specific training data will be crucial for ac-
curate extraction in the clinical domain. At the same
time, it is encouraging that we were able to apply
existing ISO-TimeML-based systems to our corpus,
despite the several extensions to ISO-TimeML that
were necessary for clinical narratives.
8 Discussion
CONTAINS plays a large role in the THYME cor-
pus, representing 66% of TLINK annotations made,
compared with only 14.6% for OVERLAP, the second
most frequent type. We also see that BEFORE links
are relatively less common than OVERLAP and CON-
TAINS, illustrating that much of the temporal ordering
on the timeline is accomplished by using many ver-
tical links (CONTAINS, OVERLAP) to build contain-
ers, and few horizontal links (BEFORE, BEGINS-ON,
ENDS-ON) to order them.
IAA on EVENTs and Temporal Expressions is
strong, although differentiating implicit EVENTs
(which should not be marked) from explicit, mark-
able EVENTs remains one of the biggest sources of
disagreement. When compared to the data from the
2012 i2b2 challenge (Sun et al., 2013b), our IAA
figures are quite similar. Even with our more com-
plex schema, we achieved an F1-score of 0.8038 for
EVENTs (compared to the i2b2 score of 0.87 for par-
tial match). For TIMEX3s, our F1-score was 0.8047,
compared to an F1-score of 0.89 for i2b2.
TLINKing medical EVENTs remains a very diffi-
cult task. By using our narrative container approach
to constrain the number of necessary annotations and
by eliminating often-confusing inverse relations (like
?after? and ?during?) (neither of which were done for
the i2b2 data), we were able to significantly improve
on the i2b2 TLINK span agreement F1-score of 0.39,
achieving an agreement score of 0.5012 for all LINKs
across our corpus. The majority of remaining an-
notator disagreement comes from different opinions
about whether any two EVENTs require an explicit
TLINK between them or an inferred one, rather than
what type of TLINK it would be (e.g. BEFORE vs.
CONTAINS). Although our results are still signifi-
cantly higher than the results reported for i2b2, and
in line with previously reported general news figures,
we are not satisfied. Improving IAA is an important
goal for future work, and with further training, speci-
fication, experience, and standardization, we hope to
clarify contexts for explicit TLINKS.
News-trained temporal information extraction sys-
tems see a significant drop in performance when ap-
plied to the clinical texts of the THYME corpus. But
as the corpus is an extension of ISO-TimeML, future
work will be able to train ISO-TimeML compliant
systems on the annotations of the THYME corpus to
reduce or eliminate this performance gap.
Some applications that our work may enable in-
clude (1) better understanding of event semantics,
such as whether a disease is chronic or acute and
its usual natural history, (2) typical event duration
for these events, (3) the interaction of general and
domain-specific events and their importance in the fi-
nal timeline, and, more generally, (4) the importance
of rough temporality and narrative containers as a
step towards finer-grained timelines.
We have several avenues of ongoing and future
work. First, we are working to demonstrate the utility
of the THYME corpus for training machine learning
models. We have designed support vector machine
models with constituency tree kernels that were able
to reach an F1-score of 0.737 on an EVENT-TIMEX3
narrative container identification task (Miller et al.,
2013), and we are working on training models to
identify events, times and the remaining types of
temporal relations. Second, as per our motivating
use cases, we are working to integrate this annotation
data with timeline visualization tools and to use these
annotations in quality-of-care research. For example,
we are using temporal reasoning built on this work to
investigate the liver toxicity of methotrexate across
a large corpus of EHRs (Lin et al., under review)].
Finally, we plan to explore the application of our
notion of an event (anything that should be visible on
a domain-appropriate timeline) to other domains. It
should transfer naturally to clinical notes about other
(non-cancer) conditions, and even to other types of
clinical notes, as certain basic events should always
be included in a patient?s timeline. Applying our
notion of event to more distant domains, such as legal
opinions, would require first identifying a consensus
within the domain about which events must appear
on a timeline.
9 Conclusion
Much of the information in clinical notes critical to
the construction of a detailed timeline is left implicit
by the concise shorthand used by doctors. Many
events are referred to only by a term such as ?tu-
152
mor?, while properties of the event itself, such as
?intermittent?, may not be specified. In addition, the
ordering of events on a timeline is often left to the
reader to infer, based on domain-specific knowledge.
It is incumbent upon the annotation guideline to in-
dicate that only informative event orderings should
be annotated, while leaving domain-specific order-
ings to post-annotation inference. This document
has detailed our approach to adapting the existing
ISO-TimeML standard to this recovery of implicit
information, and defining guidelines that support an-
notation within this complex domain. Our guide-
lines, as well as the annotated data, are available at
http://thyme.healthnlp.org, and the full
corpus has been proposed for use in a SemEval 2015
shared task.
Acknowledgments
The project described is supported by Grant Num-
ber R01LM010090 and U54LM008748 from the Na-
tional Library Of Medicine. The content is solely the
responsibility of the authors and does not necessarily
represent the official views of the National Library
Of Medicine or the National Institutes of Health.
We would also like to thank Dr. Piet C. de Groen
and Dr. Brad Erickson at the Mayo Clinic, as well as
Dr. William F. Styler III, for their contributions to the
schema and to our understanding of the intricacies of
clinical language.
References
James F Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Emmon Bach. 1986. The algebra of events. Linguistics
and philosophy, 9(1):5?16.
Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-
proach to tempeval 2013. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10?14, Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Olivier Bodenreider. 2004. The Unified Medical
Language System (UMLS): integrating biomedical
terminology. Nucleic acids research, 32(Database
issue):D267?D270, January.
Philip Bramsen, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Finding temporal order
in discharge summaries. In AMIA Annual Symposium
Proceedings, volume 2006, page 81. American Medical
Informatics Association.
Carlo Combi, Yuval Shahar, et al. 1997. Temporal reason-
ing and temporal data maintenance in medicine: issues
and challenges. Computers in biology and medicine,
27(5):353?368.
Robert H Dolin. 1995. Modeling the temporal complex-
ities of symptoms. Journal of the American Medical
Informatics Association, 2(5):323?331.
George Hripcsak, Nicholas D Soulakis, Li Li, Frances P
Morrison, Albert M Lai, Carol Friedman, Neil S Cal-
man, and Farzad Mostashari. 2009. Syndromic surveil-
lance using ambulatory electronic health records. Jour-
nal of the American Medical Informatics Association,
16(3):354?361.
Ann K Irvine, Stephanie W Haas, and Tessa Sullivan.
2008. Tn-ties: A system for extracting temporal infor-
mation from emergency department triage notes. In
AMIA Annual Symposium proceedings, volume 2008,
page 328. American Medical Informatics Association.
Elpida T Keravnou. 1997. Temporal abstraction of med-
ical data: Deriving periodicity. In Intelligent Data
Analysis in Medicine and Pharmacology, pages 61?79.
Springer.
Klaus H. Krippendorff. 2012. Content Analysis: An
Introduction to Its Methodology. SAGE Publications,
Inc, third edition edition, April.
Chen Lin, Elizabeth Karlson, Dmitriy Dligach, Mon-
ica Ramirez, Timothy Miller, Huan Mo, Natalie
Braggs, Andrew Cagan, Joshua Denny, and Guer-
gana. Savova. under review. Automatic identification
of methotrexade-induced liver toxicity in rheumatoid
arthritis patients from the electronic medical records.
Journal of the Medical Informatics Association.
John McCarthy. 2002. Actions and other events in sit-
uation calculus. In Proceedings of the International
conference on Principles of Knowledge Representation
and Reasoning, pages 615?628. Morgan Kaufmann
Publishers; 1998.
Ste?phane M Meystre, Guergana K Savova, Karin C Kipper-
Schuler, John F Hurdle, et al. 2008. Extracting infor-
mation from textual documents in the electronic health
record: a review of recent research. Yearb Med Inform,
35:128?44.
Timothy Miller, Steven Bethard, Dmitriy Dligach, Sameer
Pradhan, Chen Lin, and Guergana Savova. 2013. Dis-
covering temporal narrative containers in clinical text.
In Proceedings of the 2013 Workshop on Biomedical
Natural Langua ge Processing, pages 18?26, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
153
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and Bon-
nie Webber. 2004. The penn discourse treebank. In In
Proceedings of LREC 2004.
Massimo Poesio. 2004. Discourse annotation and seman-
tic annotation in the gnome corpus. In In Proceedings
of the ACL Workshop on Discourse Annotation.
James Pustejovsky and Amber Stubbs. 2011. Increasing
informativeness in temporal annotation. In Proceedings
of the 5th Linguistic Annotation Workshop, pages 152?
160. Association for Computational Linguistics.
James Pustejovsky, Robert Knippen, Jessica Littman, and
Roser Sauri. 2005. Temporal and event information in
natural language text. Language Resources and Evalu-
ation, 39(2-3):123?164.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Laurent
Romary. 2010. Iso-timeml: An international standard
for semantic annotation. In Proceedings of the Seventh
International Conference on Language Resources and
Evaluation (LREC 2010), Valletta, Malta.
Angus Roberts, Robert Gaizauskas, Mark Hepple, George
Demetriou, Yikun Guo, and Ian Roberts. 2009. Build-
ing a semantically annotated corpus of clinical texts.
Journal of biomedical informatics, 42(5):950?966.
Guergana Savova, Steven Bethard, Will Styler, James Mar-
tin, Martha Palmer, James Masanz, and Wayne Ward.
2009. Towards temporal relation discovery from the
clinical narrative. In AMIA Annual Symposium Pro-
ceedings, volume 2009, page 568. American Medical
Informatics Association.
Tessa Sullivan, Ann Irvine, and Stephanie W Haas. 2008.
It?s all relative: usage of relative temporal expressions
in triage notes. Proceedings of the American Society
for Information Science and Technology, 45(1):1?8.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013a.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013b.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association, 20(5):806?813.
Alexander Turchin, Maria Shubina, Eugene Breydo,
Merri L Pendergrass, and Jonathan S Einbinder. 2009.
Comparison of information content of structured and
narrative text data sources on the example of medica-
tion intensification. Journal of the American Medical
Informatics Association, 16(3):362?370.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. Semeval-2013 task 1: Tempeval-3: Evaluating
time expressions, events, and temporal relations. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 1?9, Atlanta, Georgia, USA,
June. Association for Computational Linguistics.
Marc Verhagen. 2005. Temporal Closure in an Annota-
tion Environment. Language Resources and Evalua-
tion, 39(2):211?241.
Veronika Vincze, Gyrgy Szarvas, Richrd Farkas, Gyrgy
Mra, and Jnos Csirik. 2008. The bioscope corpus:
biomedical texts annotated for uncertainty, negation
and their scopes. BMC Bioinformatics, 9(Suppl 11):1?
9.
Ying Zhao, George Karypis, and Usama M. Fayyad.
2005. Hierarchical clustering algorithms for docu-
ment datasets. Data Mining and Knowledge Discovery,
10:141?168.
Jiaping Zheng, Wendy W Chapman, Rebecca S Crowley,
and Guergana K Savova. 2011. Coreference resolution:
A review of general methodologies and applications in
the clinical domain. Journal of biomedical informatics,
44(6):1113?1122.
Li Zhou, Simon Parsons, and George Hripcsak. 2008. The
evaluation of a temporal reasoning system in processing
clinical discharge summaries. Journal of the American
Medical Informatics Association, 15(1):99?106.
154
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 54?62,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 7: Analysis of Clinical Text
Sameer Pradhan
1
, No
?
emie Elhadad
2
, Wendy Chapman
3
,
Suresh Manandhar
4
and Guergana Savova
1
1
Harvard University, Boston, MA,
2
Columbia University, New York, NY
3
University of Utah, Salt Lake City, UT,
4
University of York, York, UK
{sameer.pradhan,guergana.savova}@childrens.harvard.edu, noemie.elhadad@columbia.edu,
wendy.chapman@utah.edu, suresh@cs.york.ac.uk
Abstract
This paper describes the SemEval-2014,
Task 7 on the Analysis of Clinical Text
and presents the evaluation results. It fo-
cused on two subtasks: (i) identification
(Task A) and (ii) normalization (Task B)
of diseases and disorders in clinical reports
as annotated in the Shared Annotated Re-
sources (ShARe)
1
corpus. This task was
a follow-up to the ShARe/CLEF eHealth
2013 shared task, subtasks 1a and 1b,
2
but
using a larger test set. A total of 21 teams
competed in Task A, and 18 of those also
participated in Task B. For Task A, the
best system had a strict F
1
-score of 81.3,
with a precision of 84.3 and recall of 78.6.
For Task B, the same group had the best
strict accuracy of 74.1. The organizers
have made the text corpora, annotations,
and evaluation tools available for future re-
search and development at the shared task
website.
3
1 Introduction
A large amount of very useful information?both
for medical researchers and patients?is present
in the form of unstructured text within the clin-
ical notes and discharge summaries that form a
patient?s medical history. Adapting and extend-
ing natural language processing (NLP) techniques
to mine this information can open doors to bet-
ter, novel, clinical studies on one hand, and help
patients understand the contents of their clini-
cal records on the other. Organization of this
1
http://share.healthnlp.org
2
https://sites.google.com/site/shareclefehealth/
evaluation
3
http://alt.qcri.org/semeval2014/task7/
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
shared task helps establish state-of-the-art bench-
marks and paves the way for further explorations.
It tackles two important sub-problems in NLP?
named entity recognition and word sense disam-
biguation. Neither of these problems are new to
NLP. Research in general-domain NLP goes back
to about two decades. For an overview of the
development in the field through roughly 2009,
we refer the refer to Nadeau and Sekine (2007).
NLP has also penetrated the field of bimedical
informatics and has been particularly focused on
biomedical literature for over the past decade. Ad-
vances in that sub-field has also been documented
in surveys such as one by Leaman and Gonza-
lez (2008). Word sense disambiguation also has
a long history in the general NLP domain (Nav-
igli, 2009). In spite of word sense annotations in
the biomedical literature, recent work by Savova
et al. (2008) highlights the importance of annotat-
ing them in clinical notes. This is true for many
other clinical and linguistic phenomena as the var-
ious characteristics of the clinical narrative present
a unique challenge to NLP. Recently various ini-
tiatives have led to annotated corpora for clini-
cal NLP research. Probably the first comprehen-
sive annotation performed on a clinical corpora
was by Roberts et al. (2009), but unfortunately
that corpus is not publicly available owing to pri-
vacy regulations. The i2b2 initiative
4
challenges
have focused on such topics as concept recog-
nition (Uzuner et al., 2011), coreference resolu-
tion (Uzuner et al., 2012), temporal relations (Sun
et al., 2013) and their datasets are available to the
community. More recently, the Shared Annotated
Resources (ShARe)
1
project has created a corpus
annotated with disease/disorder mentions in clini-
cal notes as well as normalized them to a concept
unique identifier (CUI) within the SNOMED-CT
subset of the Unified Medical Language System
5
4
http://www.i2b2.org
5
https://uts.nlm.nih.gov/home.html
54
Train Development Test
Notes 199 99 133
Words 94K 88K 153K
Disorder mentions 5,816 5,351 7,998
CUI-less mentions 1,639 (28%) 1,750 (32%) 1,930 (24%)
CUI-ied mentions 4,117 (72%) 3,601 (67%) 6,068 (76%)
Contiguous mentions 5,165 (89%) 4,912 (92%) 7,374 (92%)
Discontiguous mentions 651 (11%) 439 (8%) 6,24 (8%)
Table 1: Distribution of data in terms of notes and disorder mentions across the training, development
and test sets. The disorders are further split according to two criteria ? whether they map to a CUI or
whether they are contiguous.
(UMLS) (Campbell et al., 1998). The task of nor-
malization is a combination of word/phrase sense
disambiguation and semantic similarity where a
phrase is mapped to a unique concept in an on-
tology (based on the description of that concept in
the ontology) after disambiguating potential am-
biguous surface words, or phrases. This is espe-
cially true with abbreviations and acronyms which
are much more common in clinical text (Moon et
al., 2012). The SemEval-2014 task 7 was one of
nine shared tasks organized at the SemEval-2014.
It was designed as a follow up to the shared tasks
organized during the ShARe/CLEF eHealth 2013
evaluation (Suominen et al., 2013; Pradhan et al.,
2013; Pradhan et al., 2014). Like the previous
shared task, we relied on the ShARe corpus, but
with more data for training and a new test set. Fur-
thermore, in this task, we provided the options to
participants to utilize a large corpus of unlabeled
clinical notes. The rest of the paper is organized as
follows. Section 2 describes the characteristics of
the data used in the task. Section 3 describes the
tasks in more detail. Section 4 explains the evalu-
ation criteria for the two tasks. Section 5 lists the
participants of the task. Section 6 discusses the re-
sults on this task and also compares them with the
ShARe/CLEF eHealth 2013 results, and Section 7
concludes.
2 Data
The ShARe corpus comprises annotations over
de-identified clinical reports from a US intensive
care department (version 2.5 of the MIMIC II
database
6
) (Saeed et al., 2002). It consists of
discharge summaries, electrocardiogram, echocar-
diogram, and radiology reports. Access to data
was carried out following MIMIC user agreement
requirements for access to de-identified medical
6
http://mimic.physionet.org ? Multiparameter Intelligent
Monitoring in Intensive Care
data. Hence, all participants were required to reg-
ister for the evaluation, obtain a US human sub-
jects training certificate
7
, create an account to the
password-protected MIMIC site, specify the pur-
pose of data usage, accept the data use agree-
ment, and get their account approved. The anno-
tation focus was on disorder mentions, their var-
ious attributes and normalizations to an UMLS
CUI. As such, there were two parts to the annota-
tion: identifying a span of text as a disorder men-
tion and normalizing (or mapping) the span to a
UMLS CUI. The UMLS represents over 130 lex-
icons/thesauri with terms from a variety of lan-
guages and integrates resources used world-wide
in clinical care, public health, and epidemiology.
A disorder mention was defined as any span of text
which can be mapped to a concept in SNOMED-
CT and which belongs to the Disorder semantic
group
8
. It also provided a semantic network in
which every concept is represented by its CUI
and is semantically typed (Bodenreider and Mc-
Cray, 2003). A concept was in the Disorder se-
mantic group if it belonged to one of the follow-
ing UMLS semantic types: Congenital Abnormal-
ity; Acquired Abnormality; Injury or Poisoning;
Pathologic Function; Disease or Syndrome; Men-
tal or Behavioral Dysfunction; Cell or Molecu-
lar Dysfunction; Experimental Model of Disease;
Anatomical Abnormality; Neoplastic Process; and
Signs and Symptoms. The Finding semantic type
was left out as it is very noisy and our pilot study
showed lower annotation agreement on it. Follow-
ing are the salient aspects of the guidelines used to
7
The course was available free of charge on the Internet, for example,
via the CITI Collaborative Institutional Training Initiative at
https://www.citiprogram.org/Default.asp
or, the US National Institutes of Health (NIH) at
http://phrp.nihtraining.com/users.
8
Note that this definition of Disorder semantic group did not include the
Findings semantic type, and as such differed from the one of UMLS Seman-
tic Groups, available at http://semanticnetwork.nlm.nih.gov/
SemGroups
55
annotate the data.
? Annotations represent the most specific dis-
order span. For example, small bowel ob-
struction is preferred over bowel obstruction.
? A disorder mention is a concept in the
SNOMED-CT portion of the Disorder se-
mantic group.
? Negation and temporal modifiers are not con-
sidered part of the disorder mention span.
? All disorder mentions are annotated?even
the ones related to a person other than the pa-
tient and including acronyms and abbrevia-
tions.
? Mentions of disorders that are coreferen-
tial/anaphoric are also annotated.
Following are a few examples of disorder men-
tions from the data.
Patient found to have lower extremity DVT. (E1)
In example (E1), lower extremity DVT is marked
as the disorder. It corresponds to CUI C0340708
(preferred term: Deep vein thrombosis of lower
limb). The span DVT can be mapped to CUI
C0149871 (preferred term: Deep Vein Thrombo-
sis), but this mapping would be incorrect because
it is part of a more specific disorder in the sen-
tence, namely lower extremity DVT.
A tumor was found in the left ovary. (E2)
In example (E2), tumor ... ovary is annotated as a
discontiguous disorder mention. This is the best
method of capturing the exact disorder mention
in clinical notes and its novelty is in the fact that
either such phenomena have not been seen fre-
quently enough in the general domain to gather
particular attention, or the lack of a manually
curated general domain ontology parallel to the
UMLS.
Patient admitted with low blood pressure. (E3)
There are some disorders that do not have a rep-
resentation to a CUI as part of the SNOMED CT
within the UMLS. However, if they were deemed
important by the annotators then they were anno-
tated as CUI-less mentions. In example (E3), low
blood pressure is a finding and is normalized as
a CUI-less disorder. We constructed the annota-
tion guidelines to require that the disorder be a
reasonable synonym of the lexical description of a
SNOMED-CT disorder. There are a few instances
where the disorders are abbreviated or shortened
in the clinical note. One example is w/r/r, which
is an abbreviation for concepts wheezing (CUI
C0043144), rales (CUI C0034642), and ronchi
(CUI C0035508). This abbreviation is also some-
times written as r/w/r and r/r/w. Another is gsw for
gunshot wound and tachy for tachycardia. More
details on the annotation scheme is detailed in the
guidelines
9
and in a forthcoming manuscript. The
annotations covered about 336K words. Table 1
shows the quantity of the data and the split across
the training, development and test sets as well as
in terms of the number of notes and the number of
words.
2.1 Annotation Quality
Each note in the training and development set was
annotated by two professional coders trained for
this task, followed by an open adjudication step.
By the time we reached annotating the test data,
the annotators were quite familiar with the anno-
tation and so, in order to save time, we decided
to perform a single annotation pass using a senior
annotator. This was followed by a correction pass
by the same annotator using a checklist of frequent
annotation issues faced earlier. Table 2 shows the
inter-annotator agreement (IAA) statistics for the
adjudicated data. For the disorders we measure the
agreement in terms of the F
1
-score as traditional
agreement measures such as Cohen?s kappa and
Krippendorf?s alpha are not applicable for measur-
ing agreement for entity mention annotation. We
computed agreements between the two annotators
as well as between each annotator and the final ad-
judicated gold standard. The latter is to give a
sense of the fraction of corrections made in the
process of adjudication. The strict criterion con-
siders two mentions correct if they agree in terms
of the class and the exact string, whereas the re-
laxed criteria considers overlapping strings of the
9
http://goo.gl/vU8KdW
Disorder CUI
Relaxed Strict Relaxed Strict
F
1
F
1
Acc. Acc.
A1-A2 90.9 76.9 77.6 84.6
A1-GS 96.8 93.2 95.4 97.3
A2-GS 93.7 82.6 80.6 86.3
Table 2: Inter-annotator (A1 and A2) and gold
standard (GS) agreement as F
1
-score for the Dis-
order mentions and their normalization to the
UMLS CUI.
56
Institution User ID Team ID
University of Pisa, Italy attardi UniPI
University of Lisbon, Portugal francisco ULisboa
University of Wisconsin, Milwaukee, USA ghiasvand UWM
University of Colorado, Boulder, USA gung CLEAR
University of Guadalajara, Mexico herrera UG
Taipei Medical University, Taiwan hjdai TMU
University of Turku, Finland kaewphan UTU
University of Szeged, Hungary katona SZTE-NLP
Queensland University of Queensland, Australia kholghi QUT AEHRC
KU Leuven, Belgium kolomiyets KUL
Universidade de Aveiro, Portugal nunes BioinformaticsUA
University of the Basque Country, Spain oronoz IxaMed
IBM, India parikh ThinkMiners
easy data intelligence, India pathak ezDI
RelAgent Tech Pvt. Ltd., India ramanan RelAgent
Universidad Nacional de Colombia, Colombia riveros MindLab-UNAL
IIT Patna, India sikdar IITP
University of North Texas, USA solomon UNT
University of Illinois at Urbana Champaign, USA upadhya CogComp
The University of Texas Health Science Center at Houston, USA wu UTH CCB
East China Normal University, China yi ECNU
Table 3: Participant organization and the respective User IDs and Team IDs.
same class as correct. The reason for checking
the class is as follows. Although we only use the
disorder mention in this task, the corpus has been
annotated with some other UMLS types as well
and therefore there are instances where a differ-
ent UMLS type is assigned to the same character
span in the text by the second annotator. If exact
boundaries are not taken into account then the IAA
agreement score is in the mid-90s. For the task of
normalization to CUIs, we used accuracy to assess
agreement. For the relaxed criterion, all overlap-
ping disorder spans with the same CUI were con-
sidered correct. For the strict criterion, only disor-
der spans with identical spans and the same CUI
were considered correct.
3 Task Description
The participants were evaluated on the following
two tasks:
? Task A ? Identification of the character spans
of disorder mentions.
? Task B ? Normalizing disorder mentions to
SNOMED-CT subset of UMLS CUIs.
For Task A, participants were instructed to develop
a system that predicts the spans for disorder men-
tions. For Tasks B, participants were instructed
to develop a system that predicts the UMLS CUI
within the SNOMED-CT vocabulary. The input to
Task B were the disorder mention predictions from
Task A. Task B was optional. System outputs ad-
hered to the annotation format. Each participant
was allowed to submit up to three runs. The en-
tire set of unlabeled MIMIC clinical notes (exclud-
ing the test notes) were made available to the par-
ticipants for potential unsupervised approaches to
enhance the performance of their systems. They
were allowed to use additional annotations in their
systems, but this counted towards the total allow-
able runs; systems that used annotations outside
of those provided were evaluated separately. The
evaluation for all tasks was conducted using the
blind, withheld test data. The participants were
provided a training set containing clinical text as
well as pre-annotated spans and named entities for
disorders (Tasks A and B).
4 Evaluation Criteria
The following evaluation criteria were used:
? Task A ? The system performance was eval-
uated against the gold standard using the
F
1
-score of the Precision and Recall values.
There were two variations: (i) Strict; and (ii)
Relaxed. The formulae for computing these
metrics are mentioned below.
Precision = P =
D
tp
D
tp
+ D
fp
(1)
Recall = R =
D
tp
D
tp
+ D
fn
(2)
Where, D
tp
= Number of true positives dis-
order mentions; D
fp
= Number of false pos-
itives disorder mentions; D
fn
= Number of
false negative disorder mentions. In the strict
case, a span was counted as correct if it was
identical to the gold standard span, whereas
57
Task A
Strict Relaxed
Team ID User ID Run P R F
1
P R F
1
Data
(%) (%) (%) (%) (%) (%)
UTH CCB wu 0 84.3 78.6 81.3 93.6 86.6 90.0 T+D
UTH CCB wu 1 80.8 80.5 80.6 91.6 90.7 91.1 T+D
UTU kaewphan 1 76.5 76.7 76.6 88.6 89.9 89.3 T+D
UWM ghiasvand 0 78.7 72.6 75.5 91.1 85.6 88.3 T+D
UTH CCB wu 2 68.0 84.9 75.5 83.8 93.5 88.4 T+D
UTU kaewphan 0 77.3 72.4 74.8 90.1 85.6 87.8 T
IxaMed oronoz 1 68.1 78.6 73.0 87.2 89.0 88.1 T+D
UWM ghiasvand 0 77.5 67.9 72.4 90.9 81.2 85.8 T
RelAgent ramanan 0 74.1 70.1 72.0 89.5 84.0 86.7 T+D
IxaMed oronoz 0 72.9 70.1 71.5 88.5 80.8 84.5 T+D
ezDI pathak 1 75.0 68.2 71.4 91.5 82.7 86.9 T
CLEAR gung 0 80.7 63.6 71.2 92.0 72.3 81.0 T
ezDI pathak 0 75.0 67.7 71.2 91.4 81.9 86.4 T
ULisboa francisco 0 75.3 66.3 70.5 91.4 81.5 86.2 T
ULisboa francisco 1 75.2 66.0 70.3 90.9 80.6 85.5 T
ULisboa francisco 2 75.2 66.0 70.3 90.9 80.6 85.5 T
BioinformaticsUA nunes 0 81.3 60.5 69.4 92.9 69.3 79.4 T+D
ThinkMiners parikh 0 73.4 65.0 68.9 89.2 80.2 84.4 T
ThinkMiners parikh 1 74.9 61.7 67.7 90.7 75.8 82.6 T
ECNU yi 0 75.4 61.1 67.5 89.8 72.2 80.0 T+D
UniPI attardi 2 71.2 60.1 65.2 89.7 76.6 82.6 T+D
UNT solomon 0 64.7 62.8 63.8 81.5 79.9 80.7 T+D
UniPI attardi 1 65.9 61.2 63.5 90.2 77.5 83.4 T+D
BioinformaticsUA nunes 2 75.3 53.8 62.8 86.5 62.1 72.3 T+D
BioinformaticsUA nunes 1 60.0 62.1 61.0 69.8 72.3 71.0 T+D
UniPI attardi 0 53.9 68.4 60.2 77.8 88.5 82.8 T+D
CogComp upadhya 1 63.9 52.9 57.9 82.3 68.3 74.6 T+D
CogComp upadhya 2 64.1 52.0 57.4 82.9 67.5 74.4 T+D
CogComp upadhya 0 63.6 51.5 56.9 81.9 66.5 73.4 T+D
TMU hjdai 0 52.4 57.6 54.9 91.4 76.5 83.3 T+D
MindLab-UNAL riveros 2 56.1 53.4 54.7 76.9 67.7 72.0 T
MindLab-UNAL riveros 1 57.8 51.5 54.5 77.7 65.4 71.0 T
TMU hjdai 1 62.2 42.9 50.8 89.9 65.2 75.6 T+D
IITP sikdar 0 50.0 47.9 48.9 81.5 79.7 80.6 T+D
IITP sikdar 1 47.3 45.8 46.5 78.9 77.6 78.2 T+D
IITP sikdar 2 45.0 48.1 46.5 76.9 82.6 79.6 T+D
MindLab-UNAL riveros 0 32.1 56.5 40.9 43.9 72.5 54.7 T
SZTE-NLP katona 1 54.7 25.2 34.5 88.4 40.1 55.1 T
SZTE-NLP katona 2 54.7 25.2 34.5 88.4 40.1 55.1 T
QUT AEHRC kholghi 0 38.7 29.8 33.7 90.6 70.9 79.5 T+D
SZTE-NLP katona 0 57.1 20.5 30.2 91.8 32.5 48.0 T
KUL kolomiyets 0 65.5 17.8 28.0 72.1 19.6 30.8 P
UG herrera 0 11.4 23.4 15.3 25.9 49.0 33.9 P
Table 4: Performance on test data for participating systems on Task A ? Identification of disorder men-
tions.
Task A
Strict Relaxed
Team ID User ID Run P R F
1
P R F
1
Data
(%) (%) (%) (%) (%) (%)
hjdai TMU 1 0.687 0.922 0.787 0.952 1.000 0.975 T
wu UTH CCB 0 0.877 0.710 0.785 0.962 0.789 0.867 T
wu UTH CCB 1 0.828 0.747 0.785 0.941 0.853 0.895 T
Best ShARe/CLEF-2013 performance 0.800 0.706 0.750 0.925 0.827 0.873 T
ghiasvand UWM 0 0.827 0.675 0.743 0.958 0.799 0.871 T
pathak ezDI 0 0.813 0.670 0.734 0.954 0.800 0.870 T
pathak ezDI 1 0.809 0.667 0.732 0.954 0.801 0.871 T
wu UTH CCB 2 0.657 0.790 0.717 0.806 0.893 0.847 T
francisco ULisboa 1 0.803 0.646 0.716 0.954 0.781 0.858 T
francisco ULisboa 2 0.803 0.646 0.716 0.954 0.781 0.858 T
francisco ULisboa 0 0.796 0.642 0.711 0.959 0.793 0.868 T
oronoz IxaMed 0 0.766 0.650 0.703 0.936 0.752 0.834 T
oronoz IxaMed 1 0.660 0.721 0.689 0.899 0.842 0.870 T
hjdai TMU 0 0.667 0.414 0.511 0.912 0.591 0.717 T
sikdar IITP 0 0.525 0.430 0.473 0.862 0.726 0.788 T
sikdar IITP 2 0.467 0.440 0.453 0.812 0.775 0.793 T
sikdar IITP 1 0.493 0.410 0.448 0.828 0.706 0.762 T
Table 5: Performance on development data for participating systems on Task A ? Identification of disor-
der mentions.
58
in the relaxed case, a span overlapping with
the gold standard span was also considered
correct.
? Task B ? Accuracy was used as the perfor-
mance measure for Task 1b. It was defined as
follows:
Accuracy
strict
=
D
tp
?N
correct
T
g
(3)
Accuracy
relaxed
=
D
tp
?N
correct
D
tp
(4)
Where, D
tp
= Number of true positive disor-
der mentions with identical spans as in the
gold standard; N
correct
= Number of cor-
rectly normalized disorder mentions; and T
g
= Total number of disorder mentions in the
gold standard. For Task B, the systems were
only evaluated on annotations they identified
in Task A. Relaxed accuracy only measured
the ability to normalize correct spans. There-
fore, it was possible to obtain very high val-
ues for this measure by simply dropping any
mention with a low confidence span.
5 Participants
A total of 21 participants from across the world
participated in Task A and out of them 18 also par-
ticipated in Task B. Unfortunately, although inter-
ested, the ThinkMiners team (Parikh et al., 2014)
could not participate in Task B owing to some
UMLS licensing issues. The participating organi-
zations along with the contact user?s User ID and
their chosen Team ID are mentioned in Table 3.
Eight teams submitted three runs, six submitted
two runs and seven submitted just one run. Out
of these, only 13 submitted system description pa-
pers. We based our analysis on those system de-
scriptions.
6 System Results
Tables 4 and 6 show the performance of the sys-
tems on Tasks A and B. None of the systems used
any additional annotated data so we did not have
to compare them separately. Both tables mention
performance of all the different runs that the sys-
tems submitted. Given the many variables, we de-
liberately left the decision on how many and how
to define these runs to the individual participant.
They used various different ways to differentiate
their runs. Some, for example, UTU (Kaewphan et
al., 2014), did it based on the composition of train-
ing data, i.e., whether they used just the training
data or both the training and the development data
for training the final system, which highlighted
the fact that adding development data to training
bumped the F
1
-score on Task A by about 2 percent
points. Some participants, however, did not make
use of the development data in training their sys-
tems. This was partially due to the fact that we had
not explicitly mentioned in the task description
that participants were allowed to use the develop-
ment data for training their final models. In order
to be fair, we allowed some users an opportunity
to submit runs post evaluation where they used the
exact same system that they used for evaluation
but used the development data as well. We added
a column to the results tables showing whether the
participant used only the training data (T) or both
training and development data (T+D) for training
their system. It can be seen that even though the
addition of development data helps, there are still
systems that perform in the lower percentile who
have used both training and development data for
training, indicating that both the features and the
machine learning classifier contribute to the mod-
els. A novel aspect of the SemEval-2014 shared
task that differentiates it from the ShARE/CLEF
task?other than the fact that it used more data and
a new test set?is the fact that SemEval-2014 al-
lowed the use of a much larger set of unlabeled
MIMIC notes to inform the models. Surprisingly,
only two of the systems (ULisboa (Leal et al.,
2014) and UniPi (Attardi et al., 2014)) used the
unlabeled MIMIC corpus to generalize the lexical
features. Another team?UTH CCB(Zhang et al.,
2014)?used off-the-shelf Brown clusters
10
as op-
posed to training them on the unlabeled MIMIC
II data. For Task B, the accuracy of a system
using the strict metric was positively correlated
with its recall on the disorder mentions that were
input to it (i.e., recall for Task A), and did not
get penalized for lower precision. Therefore one
could essentially gain higher accuracy in Task B
by tuning a system to provide the highest men-
tion recall in Task A potentially at the cost of pre-
cision and the overall F
1
-score and using those
mentions as input for Task B. This can be seen
from the fact that the run 2 for UTH CCB (Zhang
et al., 2014) system with the lowest F
1
-score has
10
Personal conversation with the participants as it was not
very clear in the system description paper.
59
Task B
Strict Relaxed
Team ID User ID Run Acc. Acc. Data
(%) (%)
UTH CCB wu 2 74.1 87.3 T+D
UTH CCB wu 1 70.8 88.0 T+D
UTH CCB wu 0 69.4 88.3 T+D
UWM ghiasvand 0 66.0 90.9 T+D
RelAgent ramanan 0 63.9 91.2 T+D
UWM ghiasvand 0 61.7 90.8 T
IxaMed oronoz 0 60.4 86.2 T+D
UTU kaewphan 1 60.1 78.3 T+D
ezDI pathak 1 59.9 87.8 T
ezDI pathak 0 59.2 87.4 T
UTU kaewphan 0 57.7 79.7 T
BioinformaticsUA nunes 1 53.1 85.5 T+D
BioinformaticsUA nunes 0 52.7 87.0 T+D
CLEAR gung 0 52.5 82.5 T
TMU hjdai 0 48.9 84.9 T+D
UNT solomon 0 47.0 74.8 T+D
UniPI attardi 0 46.7 68.3 T+D
BioinformaticsUA nunes 2 46.3 86.1 T+D
MindLab-UNAL riveros 2 46.1 86.3 T
IxaMed oronoz 1 43.9 55.8 T+D
MindLab-UNAL riveros 0 43.5 77.1 T
UniPI attardi 1 42.8 69.9 T+D
UniPI attardi 2 41.7 69.3 T+D
MindLab-UNAL riveros 1 41.1 79.7 T
ULisboa francisco 2 40.5 61.5 T
ULisboa francisco 1 40.4 61.2 T
ULisboa francisco 0 40.2 60.6 T
ECNU yi 0 36.4 59.5 T+D
TMU hjdai 1 35.8 83.4 T+D
IITP sikdar 0 33.3 69.6 T+D
IITP sikdar 2 33.2 69.1 T+D
IITP sikdar 1 31.9 69.6 T+D
CogComp upadhya 1 25.3 47.9 T+D
CogComp upadhya 2 24.8 47.7 T+D
CogComp upadhya 0 24.4 47.3 T+D
KUL kolomiyets 0 16.5 92.8 P
UG herrera 0 12.5 53.4 P
Table 6: Performance on test data for participat-
ing systems on Task B ? Normalization of disorder
mentions to UMLS (SNOMED-CT subset) CUIs.
Task B
Strict Relaxed
Team ID User ID Run Acc. Acc. Data
(%) (%)
TMU hjdai 0 0.716 0.777 T
TMU hjdai 1 0.716 0.777 T
UTH CCB wu 2 0.713 0.903 T
UTH CCB wu 1 0.680 0.910 T
UTH CCB wu 0 0.647 0.910 T
UWM ghiasvand 0 0.623 0.923 T
ezDI pathak 0 0.603 0.900 T
ezDI pathak 1 0.600 0.899 T
Best ShARe/CLEF-2013 performance 0.589 0.895 T
IxaMed oronoz 0 0.556 0.855 T
IxaMed oronoz 1 0.421 0.584 T
ULisboa francisco 2 0.388 0.601 T
ULisboa francisco 1 0.385 0.596 T
ULisboa francisco 0 0.377 0.588 T
IITP sikdar 2 0.318 0.724 T
IITP sikdar 0 0.312 0.725 T
IITP sikdar 1 0.299 0.730 T
Table 7: Performance on development data
for some participating systems on Task B ?
Normalization of disorder mentions to UMLS
(SNOMED-CT subset) CUIs.
the best accuracy for Task B and vice-versa for
run 0 with run 1 in between the two. In order to
fairly compare the performance between two sys-
tems one would have to provide perfect mentions
as input to Task B. One of the systems?UWM
Ghiasvand and Kate (2014)?did run some abla-
tion experiments using gold standard mentions as
input to Task B and obtained a best performance
of 89.5F
1
-score (Table 5 of Ghiasvand and Kate
(2014)) as opposed to 62.3 F
1
-score (Table 7) in
the more realistic setting which is a huge differ-
ence. In the upcoming SemEval-2014 where this
same evaluation is going to carried out under Task
14, we plan to perform supplementary evaluation
where gold disorder mentions would be input to
the system while attempting Task B. An inter-
esting outcome of planning a follow-on evalua-
tion to the ShARe/CLEF eHealth 2013 task was
that we could, and did, use the test data from the
ShARe/CLEF eHealth 2013 task as the develop-
ment set for this evaluation. After the main eval-
uation we asked participants to provide the sys-
tem performance on the development set using the
same number and run convention that they submit-
ted for the main evaluation. These results are pre-
sented in Tables 5 and 7. We have inserted the best
performing system score from the ShARe/CLEF
eHealth 2013 task in these tables. For Task A, re-
ferring to Tables 4 and 5, there is a boost of 3.7
absolute percent points for the F
1
-score over the
same task (Task 1a) in the ShARe/CLEF eHealth
2013. For Task B, referring to Tables 6 and 7, there
is a boost of 13.7 percent points for the F
1
-score
over the same task (Task 1b) in the ShARe/CLEF
eHealth 2013 evaluation. The participants used
various approaches for tackling the tasks, rang-
ing from purely rule-based/unsupervised (RelA-
gent (Ramanan and Nathan, 2014), (Matos et
al., 2014), KUL
11
) to a hybrid of rules and ma-
chine learning classifiers. The top performing sys-
tems typically used the latter. Various versions
of the IOB formulation were used for tagging the
disorder mentions. None of the standard varia-
tions on the IOB formulation were explicitly de-
signed or used to handle discontiguous mentions.
Some systems used novel variations on this ap-
proach. Probably the simplest variation was ap-
plied by the UWM team (Ghiasvand and Kate,
2014). In this formulation the following labeled
sequence ?the/O left/B atrium/I is/O moderately/O
11
Personal communication with participant.
60
dilated/I? can be used to represent the discontigu-
ous mention left atrium...dilated, and can be con-
structed as such from the output of the classifica-
tion. The most complex variation was the one used
by the UTH CCB team (Zhang et al., 2014) where
they used the following set of tags?B, I, O, DB,
DI, HB, HI. This variation encodes discontiguous
mentions by adding four more tags to the I, O and
B tags. These are variations of the B and I tags
with either a D or a H prefix. The prefix H indi-
cates that the word or word sequence is the shared
head, and the prefix D indicates otherwise. An-
other intermediate approach used by the ULisboa
team (Leal et al., 2014) with the tagset?S, B, I,
O, E and N. Here, S represents the single token
entity to be recognized, E represents the end of an
entity (which is part of one of the prior IOB vari-
ations) and an N tag to identify non-contiguous
mentions. They don?t provide an explicit exam-
ple usage of this tag set in their paper. Yet another
variation was used by the SZTE-NLP team (Ka-
tona and Farkas, 2014). This used tags B, I, L, O
and U. Here, L is used for the last token similar to
E earlier, and U is used for a unit-token mention,
similar to S earlier. We believe that the only ap-
proach that can distinguish between discontiguous
disorders that share the same head word/phrase is
the one used by the UTH CCB team (Zhang et
al., 2014). The participants used various machine
learning classifiers such as MaxEnt, SVM, CRF in
combination with rich syntactic and semantic fea-
tures to capture the disorder mentions. As men-
tioned earlier, a few participants used the avail-
able unlabeled data and also off-the-shelf clusters
to better generalize features. The use of vector
space models such as cosine similarities as well
as continuous distributed word vector representa-
tions was useful in the normalization task. They
also availed of tools such as MetaMap and cTakes
to generate features as well as candidate CUIs dur-
ing normalizations.
7 Conclusion
We have created a reference standard with high
inter-annotator agreement and evaluated systems
on the task of identification and normalization
of diseases and disorders appearing in clinical
reports. The results have demonstrated that an
NLP system can complete this task with reason-
ably high accuracy. We plan to annotate another
evaluation using the same data as part of the in
the SemEval-2015, Task 14
12
adding another task
of template filling where the systems will iden-
tify and normalize ten attributes the identified dis-
ease/disorder mentions.
Acknowledgments
We greatly appreciate the hard work and feed-
back of our program committee members and an-
notators David Harris, Jennifer Green and Glenn
Zaramba. Danielle Mowery, Sumithra Velupillai
and Brett South for helping prepare the manuscript
by summarizing the approaches used by various
systems. This shared task was partially sup-
ported by Shared Annotated Resources (ShARe)
project NIH 5R01GM090187 and Temporal His-
tories of Your Medical Events (THYME) project
(NIH R01LM010090 and U54LM008748).
References
Giuseppe Attardi, Vitoria Cozza, and Daniele Sartiano.
2014. UniPi: Recognition of mentions of disorders
in clinical text. In Proceedings of the International
Workshop on Semantic Evaluations, Dublin, Ireland,
August.
Olivier Bodenreider and Alexa McCray. 2003. Ex-
ploring semantic groups through visual approaches.
Journal of Biomedical Informatics, 36:414?432.
Keith E. Campbell, Diane E. Oliver, and Edward H.
Shortliffe. 1998. The Unified Medical Language
System: Towards a collaborative approach for solv-
ing terminologic problems. J Am Med Inform Assoc,
5(1):12?16.
Omid Ghiasvand and Rohit J. Kate. 2014. UWM: Dis-
order mention extraction from clinical text using crfs
and normalization using learned edit distance pat-
terns. In Proceedings of the International Workshop
on Semantic Evaluations, Dublin, Ireland, August.
Suwisa Kaewphan, Kai Hakaka1, and Filip Ginter.
2014. UTU: Disease mention recognition and nor-
malization with crfs and vector space representa-
tions. In Proceedings of the International Workshop
on Semantic Evaluations, Dublin, Ireland, August.
Melinda Katona and Rich?ard Farkas. 2014. SZTE-
NLP: Clinical text analysis with named entity recog-
nition. In Proceedings of the International Work-
shop on Semantic Evaluations, Dublin, Ireland, Au-
gust.
Andr?e Leal, Diogo Gonc?alves, Bruno Martins, and
Francisco M. Couto. 2014. ULisboa: Identifica-
tion and classification of medical concepts. In Pro-
ceedings of the International Workshop on Semantic
Evaluations, Dublin, Ireland, August.
12
http://alt.qcri.org/semeval2015/task14
61
Robert Leaman and Graciela Gonzalez. 2008. Ban-
ner: an executable survey of advances in biomedical
named entity recognition. In Pacific Symposium on
Biocomputing, volume 13, pages 652?663.
S?ergio Matos, Tiago Nunes, and Jos?e Lu??s Oliveira.
2014. BioinformaticsUA: Concept recognition in
clinical narratives using a modular and highly ef-
ficient text processing framework. In Proceedings
of the International Workshop on Semantic Evalua-
tions, Dublin, Ireland, August.
Sungrim Moon, Serguei Pakhomov, and Genevieve B
Melton. 2012. Automated disambiguation of
acronyms and abbreviations in clinical texts: Win-
dow and training size considerations. In AMIA Annu
Symp Proc, pages 1310?1319.
David Nadeau and Satoshi Sekine. 2007. A sur-
vey of named entity recognition and classification.
Lingvisticae Investigationes, 30(1):3?26.
Roberto Navigli. 2009. Word sense disambiguation.
ACM Computing Surveys, 41(2):1?69, February.
Ankur Parikh, Avinesh PVS, Joy Mustafi, Lalit Agar-
walla, and Ashish Mungi. 2014. ThinkMiners:
SemEval-2014 task 7: Analysis of clinical text. In
Proceedings of the International Workshop on Se-
mantic Evaluations, Dublin, Ireland, August.
Sameer Pradhan, No?emie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2013. Task 1: ShARe/CLEF eHealth
Evaluation Lab 2013. In Working Notes of CLEF
eHealth Evaluation Labs.
Sameer Pradhan, No?emie Elhadad, Brett South, David
Martinez, Lee Christensen, Amy Vogel, Hanna
Suominen, Wendy W. Chapman, and Guergana
Savova. 2014. Evaluating the state of the art in
disorder recognition and normalization of the clin-
ical narrative. In Journal of the American Medical
Informatics Association (to appear).
S. V. Ramanan and P. Senthil Nathan. 2014. RelA-
gent: Entity detection and normalization for diseases
in clinical records: a linguistically driven approach.
In Proceedings of the International Workshop on Se-
mantic Evaluations, Dublin, Ireland, August.
Angus Roberts, Robert Gaizauskas, Mark Hepple,
George Demetriou, Yikun Guo, Ian Roberts, and
Andrea Setzer. 2009. Building a semantically an-
notated corpus of clinical texts. J Biomed Inform,
42(5):950?66.
Mohammed Saeed, C. Lieu, G. Raber, and R.G. Mark.
2002. MIMIC II: a massive temporal ICU patient
database to support research in intelligent patient
monitoring. Comput Cardiol, 29.
Guergana K. Savova, A. R. Coden, I. L. Sominsky,
R. Johnson, P. V. Ogren, P. C. de Groen, and C. G.
Chute. 2008. Word sense disambiguation across
two domains: Biomedical literature and clinical
notes. J Biomed Inform, 41(6):1088?1100, Decem-
ber.
Weiyi Sun, Anna Rumshisky, and
?
Ozlem Uzuner.
2013. Evaluating temporal relations in clinical text:
2012 i2b2 Challenge. Journal of the American Med-
ical Informatics Association, 20(5):806?13.
Hanna Suominen, Sanna Salanter?a, Sumithra Velupil-
lai, Wendy W. Chapman, Guergana Savova,
Noemie Elhadad, Sameer Pradhan, Brett R. South,
Danielle L. Mowery, Gareth J. F. Jones, Johannes
Leveling, Liadh Kelly, Lorraine Goeuriot, David
Martinez, and Guido Zuccon. 2013. Overview of
the ShARe/CLEF eHealth evaluation lab 2013. In
Working Notes of CLEF eHealth Evaluation Labs.
?
Ozlem Uzuner, Brett R South, Shuying Shen, and
Scott L DuVall. 2011. 2010 i2b2/VA challenge on
concepts, assertions, and relations in clinical text.
Journal of the American Medical Informatics Asso-
ciation, 18(5):552?556.
?
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Jour-
nal of American Medical Informatics Association,
19(5):786?791, September.
Yaoyun Zhang, Jingqi Wang, Buzhou Tang, Yonghui
Wu, Min Jiang, Yukun Chen, and Hua Xu. 2014.
UTH CCB: A report for SemEval 2014 task 7 anal-
ysis of clinical text. In Proceedings of the Interna-
tional Workshop on Semantic Evaluations, Dublin,
Ireland, August.
62
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 73?81,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Active Learning for Coreference Resolution
Timothy A. Miller and Dmitriy Dligach and Guergana K. Savova
Children?s Hospital Boston
and Harvard Medical School
300 Longwood Ave.
Enders 141
Boston, MA 02115, USA
{Timothy.Miller,Dmitriy.Dligach,Guergana.Savova}@childrens.harvard.edu
Abstract
Active learning can lower the cost of anno-
tation for some natural language processing
tasks by using a classifier to select informa-
tive instances to send to human annotators. It
has worked well in cases where the training in-
stances are selected one at a time and require
minimal context for annotation. However,
coreference annotations often require some
context and the traditional active learning ap-
proach may not be feasible. In this work we
explore various active learning methods for
coreference resolution that fit more realisti-
cally into coreference annotation workflows.
1 Introduction
Coreference resolution is the task of deciding which
entity mentions in a text refer to the same entity.
Solving this problem is an important part of the
larger task of natural language understanding in gen-
eral. The clinical domain offers specific tasks where
it is easy to see that correctly resolving coreference
is important. For example, one important task in the
clinical domain is template filling for the Clinical El-
ements Model (CEM).1 This task involves extracting
various pieces of information about an entity and fit-
ting the information into a standard data structure
that can be reasoned about. An example CEM tem-
plate is that for Disease with attributes for Body Lo-
cation, Associated Sign or Symptom, Subject, Nega-
tion, Uncertainty, and Severity. Since a given entity
may have many different attributes and relations, it
1http://intermountainhealthcare.org/cem
may be mentioned multiple times in a text. Coref-
erence resolution is important for this task because
it must be known that all the attributes and relations
apply to the same entity so that a single CEM tem-
plate is filled in for an entity, rather than creating a
new template for each mention of the entity.
2 Background
2.1 Coreference Resolution
Space does not permit a thorough review of coref-
erence resolution, but recent publications covered
the history and current state of the art for both the
general domain and the clinical domain (Ng, 2010;
Pradhan et al, 2011; Zheng et al, 2011).
The system used here (Zheng et al, 2012) is
an end-to-end coreference resolution system, mean-
ing that the algorithm receives no gold standard in-
formation about mentions, named entity types, or
any linguistic information. The coreference res-
olution system is a module of the clinical Tex-
tual Analysis and Knowledge Extraction System
(cTAKES) (Savova et al, 2010) that is trained on
clinical data. It takes advantage of named entity
recognition (NER) and categorization to detect en-
tity mentions, and uses several cTAKES modules
as feature generators, including the NER module,
a constituency parser module, and a part of speech
tagging module.
The system architecture is based on the pairwise
discriminative classification approach to the coref-
erence resolution problem. In that paradigm, pairs
of mentions are classified as coreferent or not, and
then some reconciliation must be done on all of the
73
links so that there are no conflicts in the clusters.
The system uses support vector machines (SVMs)
as the pairwise classifiers, and conflicts are avoided
by only allowing an anaphor to link with one an-
tecedent, specifically that antecedent the classifier
links with the highest probability.
There are separate pairwise classifiers for named
entity and pronominal anaphor types. In the domain
of clinical narratives, person mentions and personal
pronouns in particular are not especially challeng-
ing ? the vast majority of person mentions are the
patient. In addition, pronoun mentions, while im-
portant, are relatively rare. Thus we are primarily
interested in named entity coreference classification,
and we use that classifier as the basis of the work de-
scribed here.
The feature set of this system is similar to that
used by Ng and Cardie (2002). That system in-
cludes features based on surface form of the men-
tions, shallow syntactic information, and lexical se-
mantics from WordNet. The system used here has
a similar feature set but uses Unified Medical Lan-
guage System (UMLS)2 semantic features as it is
intended for clinical text, and also incorporates sev-
eral syntactic features extracted from constituency
parses extracted from cTAKES.
To generate training data for active learning simu-
lations, mention detection is run first (cTAKES con-
tains a rule-based NER system) to find named en-
tities and a constituency parser situates entities in
a syntax tree). For each entity found, the system
works backwards through all other mentions within
a ten sentence window. For each candidate anaphor-
antecedent pair, a feature vector is extracted using
the features briefly described above.
2.2 Active Learning
Active Learning (AL) is a popular approach to se-
lecting unlabeled data for annotation (Settles, 2010)
that can potentially lead to drastic reductions in the
amount of annotation that is necessary for train-
ing an accurate statistical classifier. Unlike passive
learning, where the data is sampled for annotation
randomly, AL delegates data selection to the clas-
sifier. AL is an iterative process that operates by
first training a classifier on a small sample of the
2http://www.nlm.nih.gov/research/umls/
data known as the seed examples. The classifier
is subsequently applied to a pool of unlabeled data
with the purpose of selecting additional examples
the classifier views as informative. The selected data
is annotated and the cycle is repeated, allowing the
learner to quickly refine the decision boundary be-
tween classes. One common approach to assessing
the informativeness is uncertainty sampling (Lewis
and Gale, 1994; Schein and Ungar, 2007), in which
the learner requests a label for the instance it is most
uncertain how to label. In this work, we base our
instance selection on the distance to the SVM de-
cision boundary (Tong and Koller, 2002), assuming
that informative instances tend to concentrate near
the boundary.
Most AL work focuses on instance selection
where the unit of selection is one instance repre-
sented as a feature vector. In this paper we also
attempt document selection, where the unit of se-
lection is a document, typically containing multi-
ple coreference pairs each represented as a feature
vector. The most obvious way to extend a sin-
gle instance informativeness metric to the document
scenario is to aggregate the informativeness scores.
Several uncertainty metrics have been proposed that
follow that route to adapt single instance selection
to multiple instance scenarios (Settles et al, 2008;
Tomanek et al, 2009). We borrow some of these
metrics and propose several new ones.
To the best of our knowledge only one work
exists that explores AL for coreference resolution.
Gasperin (2009) experiments with an instance based
approach in which batches of anaphoric pairs are se-
lected on each iteration of AL. In these experiments,
AL did not outperform the passive learning baseline,
probably due to selecting batches of large size.
3 Active Learning Configurations
3.1 Instance Selection
The first active learning model we considered selects
individual training instances ? putatively coreferent
mention pairs. This method is quite easy to simu-
late, and follows naturally from most of the theo-
retical active learning literature, but it has the draw-
back of being seemingly unrealistic as an annotation
paradigm. That is, since coreference can span across
an entire document, it is probably not practical to
74
have a human expert annotate only a single instance
at a time when a given instance may require many
sentences of reading in order to contextualize the in-
stance and properly label it. Moreover, even if such
an annotation scheme proved viable, it may result
in an annotated corpus that is only valuable for one
type of coreference system architecture.
Nonetheless, active learning for coreference at the
instance level is still useful. First, since this method
most closely follows the successful active learning
literature by using the smallest discrete problems, it
can serve as a proof of concept for active learning
in the coreference task ? if it does not work well at
this level, it probably will not work at the document
level. Previous results (Gasperin, 2009) have shown
that certain multiple instance methods do not work
for coreference resolution, so testing on smaller se-
lection sizes first can ensure that active learning is
even viable at that scale. In addition, though in-
stance selection may not be feasible for real world
annotations, individual instances and metrics for se-
lecting them are usually used as building blocks for
more complex methods. In order for this to be pos-
sible it must be shown that the instances themselves
have some value.
3.2 Document Selection
Active learning with document selection is a much
more realistic representation of conventional anno-
tation methods. Conventionally, a set of documents
is selected, and each document is annotated exhaus-
tively for coreference (Pradhan et al, 2011; Savova
et al, 2011). Document selection fits into this work-
flow very naturally, by selecting the next document
to annotate exhaustively based on some metric of
which document has the best instances. In theory,
this method can save annotation time by only anno-
tating the most valuable documents.
Document selection is somewhat similar to the
concept of batch-mode active learning, wherein
multiple instances are selected at once, though
batch-mode learning is usually intended to solve a
different problem, that of an asymmetry between
classifier training speed and annotation speed (Set-
tles, 2010). A more important difference is that doc-
ument selection requires that all of the instances in
the batch must come from the same document. Thus,
one might expect a priori that document selection
for active learning will not perform as well as in-
stance selection. However, it is possible that even
smaller gains will be valuable for improving annota-
tion time, and the more robust nature of a corpus an-
notated in such a way will make the long term bene-
fits worthwhile.
In this work, we propose several metrics for se-
lecting documents to annotate, all of which are
based on instance level uncertainty. In the fol-
lowing descriptions, D is the set of documents, d
is a single document, d? is the selected document,
Instances(d) is a function which returns the set of
pair instances in document d, i is an instance, dist(i)
is a function which returns the distance of instance i
from the classification boundary, and I is the indica-
tor function, which takes the value 1 if its argument
is true and 0 otherwise. Note that high uncertainty
occurs when Abs(dist(i)) approaches 0.
? Best instance ? This method uses the un-
certainty sampling criteria on instances, and
selects the document containing the in-
stance the classifier is least certain about.
d? = argmin
d?D
[mini?Instances(d)Abs(dist(i))]
? Highest average uncertainty ? This method
computes the average uncertainty of all
instances in a document, and selects the
document with the highest average uncertainty.
d? = argmin
d?D
1
|Instances(d)|
?
i?Instances(d)Abs(dist(i))
? Least bad example ? This method uses
uncertainty sampling criteria to find the
document whose most certain example is
least certain, in other words the document
whose most useless example is least useless.
d? = argmin
d?D
maxi?Instances(d)Abs(dist(i))
? Narrow band ? This method creates an un-
certainty band around the discriminating
boundary and selects the document with
the most examples inside that narrow band.
d? = argmax
d?D
?
i?Instances(d) I(Abs(dist(i) < 0.2))
? Smallest spread ? This method computes the
distance between the least certain and most
certain instances and selects the document
minimizing that distance.
75
d? = argmin
d?D
[maxi?Instances(d)(Abs(dist(i)))?
mini?Instances(d)(Abs(dist(i)))]
? Most positives ? This method totals the
number of positive predicted instances
in each document and selects the doc-
ument with the most positive instances.
d? = argmax
d?D
?
i?Instances(d) I(dist(i) > 0)
? Positive ratio ? This method calculates
the percentage of positive predicted in-
stances in each document and selects the
document with the highest percentage.
d? = argmax
d?D
?
i?Instances(d) I(dist(i)>0)
|Instances(d)|
Many of these are straightforward adaptations of
the instance uncertainty criteria, but others deserve
a bit more explanation. The most positives and pos-
itive ratio metrics are based on the observation that
the corpus is somewhat imbalanced ? for every posi-
tive instance there are roughly 20 negative instances.
These metrics try to account for the possibility that
instance selection focuses on positive instances. The
average uncertainty is an obvious attempt to turn in-
stance metrics into document metrics, but narrow
band and smallest spread metrics attempt to do the
same thing while accounting for skew in the distri-
bution of ?good? and ?bad? instances.
3.3 Document-Inertial Instance Selection
One of the biggest impracticalities of instance se-
lection is that labeling any given instance may re-
quire reading a fair amount of the document, since
the antecedent and anaphor can be quite far apart.
Thus, any time savings accumulated by only anno-
tating an instance is reduced since the reading time
per instance is probably increased.
It is also possible that document selection goes
too far in the other direction, and requires too
many useless instances to be annotated to achieve
gains. Therefore, we propose a hybrid method of
document-inertial instance selection which attempts
to combine aspects of instance selection and docu-
ment selection.
This method uses instance selection criteria to se-
lect new instances, but will look inside the current
document for a new instance within an uncertainty
threshold rather than selecting the most uncertain in-
stance in the entire training set. Sticking with the
same document for several instances in a row can
potentially solve the real world annotation problem
that marking up each instance requires some knowl-
edge of the document context. Instead, the context
learned by selecting one instance can be retained if
useful for annotating the next selected instance from
the same document.
This also preserves one of the biggest advantages
of instance selection, that of re-training the model
after every selected instance. In batch-mode selec-
tion and document selection, many instances are se-
lected according to criteria based on the same model
starting point. As a result, the selected instances
may be redundant and document scores based on
accumulated instance scores may not reflect reality.
Re-training the model between selected instances
prevents redundant instances from being selected.
4 Evaluation
Evaluations of the active learning models described
above took place in a simulation context. In active
learning simulations, a labeled data set is used, and
the unlabeled pool is simulated by ignoring or ?cov-
ering? the labels for part of the data until the selec-
tion algorithm selects a new instance for annotation.
After selection the next data point is simply put into
the training data and its label is uncovered.
The data set used was the Ontology Development
and Information Extraction (ODIE) corpus (Savova
et al, 2011) used in the 2011 i2b2/VA Challenge on
coreference resolution.3 We used a set of 64 docu-
ments from the training set of the Mayo Clinic notes
for our simulations.
Instances were created by using the training
pipeline from the coreference system described in
Section 2.1. As previously mentioned, this work
uses the named entity anaphor classifier as it con-
tains the most data points. This training set resulted
in 6820 instances, with 311 positive instances and
6509 negative instances. Baseline ten-fold cross val-
idation performance on this data set using an SVM
with RBF kernel is an F-score of 0.48.
Simulations are performed using ten fold cross-
validation. First, each data point is assigned to one
3https://www.i2b2.org/NLP/Coreference/
76
of ten folds (this is done randomly to avoid any auto-
correlation issues). Then, for each iteration, one fold
is made the seed data, another fold is the validation
data, and the remainder are the unlabeled pool. Ini-
tially the labeled training data contains only the seed
data set. The model is trained on the labeled train-
ing data, tested on the validation set, then used to
select the next data point from the pool data set. The
selected data point is then removed from the pool
and added to the training data with its gold stan-
dard label(s), and the process repeats until the pool
of unlabeled data is empty. Performance is averaged
across folds to minimize the effects of randomness
in seed and validation set selection. Typically, active
learning is compared to a baseline of passive learn-
ing where the next data point to be labeled is selected
from the unlabeled pool data set randomly.
4.1 Instance Selection Experiments
Instance selection simulations follow the general
template above, with each instance (representing
a putative antecedent-anaphor pair) randomly as-
signed to a fold. After scoring on the validation set,
uncertainty sampling is used to select a single in-
stance from the unlabeled pool, and that instance is
added to the training set.
Figure 1 shows the results of active learning using
uncertainty selection on instances versus using pas-
sive learning (random selection). This makes it clear
that if the classifier is allowed to choose the data, top
performance can be achieved much faster than if the
data is presented in random order. Specifically, the
performance for uncertainty selection levels off at
around 500 instances into the active learning, out of
a pool set of around 5500 instances. In contrast, the
passive learning baseline takes basically the entire
dataset to reach the same performance.
This is essentially a proof of concept that there is
such a thing as a ?better? or ?worse? instance when
it comes to training a classifier for coreference. We
take this as a validation for attempting a document
selection experiment, with many metrics using in-
stance uncertainty as a building block.
4.2 Document Selection Experiments
Document selection follows similarly to the instance
selection above. The main difference is that instead
of assigning pair vectors to folds, we assign docu-
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Active vs. Passive Learning on Pairwise Named Entity Coreference
Number of instances
F?
sc
or
e
 
 
Random (Passive)
Uncertainty Sampling
Figure 1: Instance selection simulation results. The x-
axis is number of instances and the y-axis is ten-fold av-
eraged f-score of the pairwise named entity classifier.
ments to folds. To make a selection, each instance is
labeled according to the model, document level met-
rics described in Section 3.2 are computed per docu-
ment, and the document is selected which optimizes
the metric being evaluated. All of that document?s
instances and labels are added to the training data,
and the process repeats as before.
The results of these experiments are divided into
two plots for visual clarity. Figure 2 shows the
results of these experiments, roughly divided into
those that work as well as a random baseline (left)
and those that seem to work worse than a random
baseline (right). The best performing metrics (on
the left side of the figure) are Positive Ratio, Least
Worst,Highest Average, and Narrow Band, although
none of these performs noticeably better than ran-
dom. The remaining metrics (on the right) seem
to do worse than random, taking more instances to
reach the peak performance near the end.
The performance of document selection suggests
that it may not be a viable means of active learn-
ing. This may be due to a model of data distribution
in which useful instances are distributed very uni-
formly throughout the corpus. In this case, an aver-
age document will only have 8?10 useful instances
and many times as many that are not useful.
This was investigated by follow-up experiments
on the instance selection which kept track of which
77
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Least worst
Highest average
Pos/neg ratio
Narrow Band
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Best example
Most positives
Smallest spread
Figure 2: Two sets of document selection experiments.
document each instance came from. The experi-
ments tracked the first 500 instances only, which is
roughly the number of instances shown in Figure 1
to reach peak performance. Figure 3 (left) shows
a histogram with document indices on the x-axis
and normalized instance counts on the y-axis. The
counts are normalized by total number of document
vectors. In other words, we wanted to show whether
there was a distinction between ?good? documents
containing lots of good instances and ?bad? docu-
ments with few good instances.
The figure shows a few spikes, but most docu-
ments have approximately 10% of their instances
sampled, and all but one document has at least one
instance selected. Further investigation shows that
the spikes in the figure are from shorter documents.
Since shorter documents have few instances overall
but always at least one positive instance, they will be
biased to have a higher ratio of positive to negative
instances. If positive instances are more uncertain
(which may be the case due to the class imbalance),
then shorter documents will have more selected in-
stances per unit length.
We performed another follow-up experiment
along these lines using the histogram as a measure
of document value. In this experiment, we took the
normalized histogram, selected documents from it in
order of normalized number of items selected, and
used that as a document selection technique. Ob-
viously this would be ?cheating? if used as a metric
for document selection, but it can serve as a check on
the viability of document selection. If the results are
better than passive document selection, then there is
some hope that a document level metric based on the
uncertainty of its instances can be successful.
In fact, the right plot on Figure 3 shows that the
?cheating? method of document selection still does
not look any better than random document selection.
4.3 Document-Inertial Instance Selection
Experiments
The experiments for document-inertial instance se-
lection were patterned after the instance selection
paradigm. However, each instance was bundled with
metadata representing the document from which it
came. In the first selection, the algorithm selects the
most uncertain instance, and the document it comes
from is recorded. For subsequent selections, the
document which contained the previously selected
instance is given priority when looking for a new
instance. Specifically, each instance in that docu-
ment is classified, and the confidence is compared
against a threshold. If the document contains in-
stances meeting the threshold, the most uncertain in-
stance was selected. After each instance, the model
is retrained as in normal instance selection, and the
new model is used in the next iteration of the selec-
tion algorithm. For these experiments, the threshold
is set at 0.75, where the distance between the classi-
fication boundary and the margin is 1.0.
Figure 4 shows the performance of this algorithm
compared to passive and uncertainty sampling. Per-
78
0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Normalized document selection counts
Document index
%
 o
f v
ec
to
rs
 s
el
ec
te
d
0 1000 2000 3000 4000 5000 6000 7000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Number of instances
F?
sc
or
e
Document?level active learning
 
 
Passive
Cheating
Figure 3: Left: Percentage of instances selected from each document. Right: Performance of a document selection
algorithm that can ?cheat? and select the document with the highest proportion of good instances.
0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Active vs. Passive Learning on Pairwise Named Entity Coreference
Number of instances
F?
sc
or
e
 
 
Random (Passive)
Uncertainty Sampling
Sticky Instance Sampling
Figure 4: Document-inertial instance selection results.
formance using this algorithm is clearly better than
passive learning and is similar to standard uncer-
tainty selection ignoring document constraints.
5 Discussion and Conclusion
The results of these experiments paint a complex
picture of the way active learning works for this do-
main and model combination. The first experiments
with uncertainty selection indicate that the number
of instances required to achieve classifier perfor-
mance can be compressed. Selecting and training
on all the good instances first leads to much faster
convergence to the asymptotic performance of the
classifier given the features and data set.
Attempting to extend this result to document se-
lection met with mediocre results. Even the best per-
forming of seven attempted algorithms seems to be
about the same as random document selection. One
can interpret these results in different ways.
The most pessimistic interpretation is that docu-
ment selection simply requires too many useless in-
stances to be annotated, good instances are spread
too evenly, and so document selection will never be
meaningfully faster than random selection. This in-
terpretation seems to be supported by experiments
showing that even if document selection uses a
?cheating? algorithm to select the documents with
the highest proportion of good instances it still does
not beat a passive baseline.
One can also interpret these results to inspire fur-
ther work, first by noting that all of the selection
techniques attempt to build on the instance selec-
tion metrics. While our document selection metrics
were more sophisticated than simply taking the n-
best instances, Settles (2010) notes that some suc-
cessful batch mode techniques explicitly account for
diversity in the selections, which we do not. In ad-
dition, one could argue that our experiments were
unduly constrained by the small number of docu-
ments available in the unlabeled pool, and that with
a larger unlabeled pool, one would eventually en-
counter documents with many good instances. This
may be true, but may be difficult in practice as clin-
ical notes often need to be manually de-identified
79
before any research use, and so it is not simply a
matter of querying all records in an entire electronic
medical record system.
The document-inertial instance selection showed
that the increase in training speed can be main-
tained without switching documents for every in-
stance. This suggests that while good training in-
stances may be uniformly distributed, it is usually
possible to find multiple good enough instances in
the current document, and they can be found despite
not selecting instances in the exact best order that
plain instance selection would suggest.
Future work is mainly concerned with real world
applicability. Document level active learning can
probably be ruled out as being non-beneficial despite
being the easiest to work into annotation work flows.
Instance level selection is very efficient in achieving
classifier performance but the least practical.
Document-inertial seems to provide some com-
promise. It does not completely solve the prob-
lems of instance selection, however, as annotation
will still not be complete if done exactly as simu-
lated here. In addition, the assumption of savings
is based on a model that each instance takes a con-
stant amount of time to annotate. This assumption is
probably true for tasks like word sense disambigua-
tion, where an annotator can be presented one in-
stance at a time with little context. However, a better
model of annotation for tasks like coreference is that
there is a constant amount of time required for read-
ing and understanding the context of a document,
then a constant amount of time on top of that per
instance.While modeling annotation time may pro-
vide some insight, it will probably be most effective
to undertake empirical annotation experiments to in-
vestigate whether document-inertial instance selec-
tion actually provides a valuable time savings.
The final discussion point is that of producing
complete document annotations. For coreference
systems following the pairwise discriminative ap-
proach as in that described in Section 2.1, a corpus
annotated instance by instance is useful. However,
many recent approaches do some form of document-
level clustering or explicit coreference chain build-
ing, and are not natively able to handle incompletely
annotated documents.4
4Other recent unsupervised graphical model approaches us-
Future work will investigate this issue by quan-
tifying the value of complete gold standard annota-
tions versus the partial annotations that may be pro-
duced using document-inertial instance selection.
One way of doing this is in simulation, by training
a model on the 500 good instances that document-
inertial instance selection selects, and then classify-
ing the rest of the training instances using that model
to create a ?diluted? gold standard. Then, a model
trained on the diluted gold standard will be used
to classify the validation set and performance com-
pared to the version trained on the full gold standard
corpus. Similar experiments can be performed using
other systems. The logic here is that if an instance
was not in the top 10% of difficult instances it can be
classified with high certainty. The fact that positive
instances are rare and tend to be most uncertain is a
point in favor of this approach ? after all, high accu-
racy can be obtained by guessing in favor of negative
once the positive instances are labeled. On the other
hand, if document-inertial instance selection simply
amounts to labeling of positive instances, it may not
result in substantial time savings.
In conclusion, this work has shown that instance
selection works for coreference resolution, intro-
duced several metrics for document selection, and
proposed a hybrid selection approach that preserves
the benefits of instance selection while offering the
potential of being applicable to real annotation. This
work can benefit the natural language processing
community by providing practical methods for in-
creasing the speed of coreference annotation.
Acknowledgments
The project described was supported by award
number NLM RC1LM010608, the Strategic Health
IT Advanced Research Projects (SHARP) Program
(90TR002) administered by the Office of the Na-
tional Coordinator for Health Information Technol-
ogy, and Integrating Informatics and Biology to the
Bedside (i2b2) NCBO U54LM008748. The content
is solely the responsibility of the authors and does
not necessarily represent the official views of the
NLM/NIH/ONC.
ing Gibbs sampling (Haghighi and Klein, 2007) may be able to
incorporate partially annotated documents in semi-supervised
training.
80
References
Caroline Gasperin. 2009. Active learning for anaphora
resolution. In Proceedings of the NAACL HLT Work-
shop on Active Learning for Natural Language Pro-
cessing, pages 1?8.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
848?855.
David D. Lewis andWilliam A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 3?12.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL).
Vincent Ng. 2010. Supervised noun phrase coreference
research: The first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the 15th Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27.
Guergana K. Savova, James J. Masanz, Philip V. Ogren,
Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-
Schuler, and Christopher G. Chute. 2010. Mayo
clinical text analysis and knowledge extraction sys-
tem (cTAKES): architecture, component evaluation
and applications. J Am Med Inform Assoc, 17(5):507?
513.
Guergana K. Savova, Wendy W. Chapman, Jiaping
Zheng, and Rebecca S. Crowley. 2011. Anaphoric
relations in the clinical narrative: corpus creation. J
Am Med Inform Assoc, 18:459?465.
A.I. Schein and L.H. Ungar. 2007. Active learning for
logistic regression: an evaluation. Machine Learning,
68(3):235?265.
B. Settles, M. Craven, and S. Ray. 2008. Multiple-
instance active learning. Advances in Neural Informa-
tion Processing Systems (NIPS), 20:1289?1296.
Burr Settles. 2010. Active learning literature survey.
Technical report, University of Wisconsin?Madison.
Katrin Tomanek, Florian Laws, Udo Hahn, and Hinrich
Schu?tze. 2009. On proper unit selection in active
learning: co-selection effects for named entity recog-
nition. In HLT ?09: Proceedings of the NAACL HLT
2009 Workshop on Active Learning for Natural Lan-
guage Processing, pages 9?17, Morristown, NJ, USA.
Association for Computational Linguistics.
S. Tong and D. Koller. 2002. Support vector machine
active learning with applications to text classification.
The Journal of Machine Learning Research, 2:45?66.
Jiaping Zheng, Wendy Webber Chapman, Rebecca S.
Crowley, and Guergana K. Savova. 2011. Coreference
resolution: A review of general methodologies and ap-
plications in the clinical domain. Journal of Biomedi-
cal Informatics, 44:1113?1122.
Jiaping Zheng, Wendy W Chapman, Timothy A Miller,
Chen Lin, Rebecca S Crowley, and Guergana K
Savova. 2012. A system for coreference resolution for
the clinical narrative. Journal of the American Medi-
cal Informatics Association.
81
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 18?26,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discovering Narrative Containers in Clinical Text
Timothy A. Miller1, Steven Bethard2, Dmitriy Dligach1,
Sameer Pradhan1, Chen Lin1, and Guergana K. Savova1
1 Children?s Hospital Informatics Program, Boston Children?s Hospital and Harvard Medical School
firstname.lastname@childrens.harvard.edu
2 Center for Computational Language and Education Research, University of Colorado Boulder
steven.bethard@colorado.edu
Abstract
The clinical narrative contains a great deal
of valuable information that is only under-
standable in a temporal context. Events,
time expressions, and temporal relations
convey information about the time course
of a patient?s clinical record that must be
understood for many applications of inter-
est. In this paper, we focus on extracting
information about how time expressions
and events are related by narrative con-
tainers. We use support vector machines
with composite kernels, which allows for
integrating standard feature kernels with
tree kernels for representing structured
features such as constituency trees. Our
experiments show that using tree kernels
in addition to standard feature kernels im-
proves F1 classification for this task.
1 Introduction
Clinical narratives are a rich source of unstruc-
tured information that hold great potential for im-
pacting clinical research and clinical care. These
narratives consist of unstructured natural language
descriptions of various stages of clinical care,
which makes them information dense but chal-
lenging to use computationally. Information ex-
tracted from these narratives is already being used
for clinical research tasks such as automatic phe-
notype classification for collecting disease cohorts
retrospectively (Ananthakrishnan et al, 2013),
which can in turn be used for a variety of studies,
including pharmacogenomics (Lin et al, 2012;
Wilke et al, 2011). Future applications may use
information extracted from the clinical narrative at
the point of care to assist physicians in decision-
making in a real time fashion.
One of the most interesting and challenging as-
pects of clinical text is the pervasiveness of tempo-
rally grounded information. This includes a num-
ber of clinical concepts which are events with fi-
nite time spans (e.g., surgery or x-ray), time ex-
pressions (December, postoperatively), and links
that relate events to times or other events. For ex-
ample, surgery last May relates the time last May
with the event surgery via the CONTAINS relation,
while Vicodin after surgery relates the medication
event Vicodin with the procedure event surgery via
the AFTER relation. There are many potential ap-
plications of clinical information extraction that
are only possible with an understanding of the or-
dering and duration of the events in a clinical en-
counter.
In this work we focus on extracting a particu-
lar temporal relation, CONTAINS, that holds be-
tween a time expression and an event expression.
This level of representation is based on the compu-
tational discourse model of narrative containers
(Pustejovsky and Stubbs, 2011), which are time
expressions or events which are central to a sec-
tion of a text, usually manifested by being rela-
tive hubs of temporal relation links. We argue that
containment relations are useful as an intermediate
level of granularity between full temporal relation
extraction and ?coarse? temporal bins (Raghavan
et al, 2012) like before admission, on admission,
and after admission. Correctly extracting CON-
TAINS relations will, for example, allow for more
accurate placement of events on a timeline, to
the resolution possible by the number of time ex-
pressions in the document. We suspect that this
finer grained information will also be more useful
for downstream applications like coreference, for
which coarse information was found to be useful.
The approach we develop is a supervised machine
18
learning approach in which pairs of time expres-
sions and events are classified as CONTAINS or
not. The specific approach is a support vector ma-
chine using both standard feature kernels and tree
kernels, a novel approach to this problem in this
domain that has shown promise on other relation
extraction tasks.
This work makes use of a new corpus we devel-
oped as part of the THYME1 project (Temporal
History of Your Medical Events) focusing on tem-
poral events and relations in clinical text. This cor-
pus consists of clinical and pathology notes on col-
orectal cancer from Mayo Clinic. Gold standard
annotations include Penn Treebank-style phrase
structure in addition to clinically relevant temporal
annotations like clinical events, temporal expres-
sions, and various temporal relations.
2 Background and Related Work
2.1 Annotation Methodology
The THYME annotation guidelines2 detail the ex-
tension of TimeML (Pustejovsky et al, 2003b)
to the annotations of events, temporal expres-
sions and temporal relations in the clinical do-
main. In summary, an EVENT is anything that is
relevant to the clinical timeline. Temporal expres-
sions (TIMEX3s) in the clinical domain are simi-
lar to those in the general domain with two excep-
tions. First, TimeML sets and frequencies occur
much more often in the clinical domain, especially
with regard to medications and treatments (Clar-
itin 30mg twice daily). The second deviation is a
new type of TIMEX3 ? PREPOSTEXP which covers
temporally complex terms like preoperative, post-
operative, and intraoperative.
EVENTs and TIMEX3s are ordered on a timeline
through temporal TLINKs which range from fairly
coarse (the relation to document time creation) to
fairly granular (the explicit pairwise TLINKs be-
tween EVENTs and/or TIMEX3s). Of note for this
work, the CONTAINS relation between a TIMEX3
and an EVENT means that the span of the EVENT
is completely within the span of the TIMEX3. The
interannotator agreement F1-score for CONTAINS
for the set of documents used here was 0.60.
2.2 Narrative Containers
One relatively new concept for marking temporal
relations is that of narrative containers, as in Puste-
1http://clear.colorado.edu/TemporalWiki
2Annotation guidelines are posted on the THYME wiki.
jovsky and Stubbs (2011). Narrative containers
are time spans which are central to the discourse
and often subsume multiple events and time ex-
pressions. They are often anchored by a time ex-
pression, though more abstract events may also act
as anchors. Using the narrative container frame-
work significantly reduces the number of explicit
TLINK annotations yet retains a relevant degree of
granularity enabling inferencing.
Consider the following clinical text example
with DocTime of February 8.
The patient recovered well after her ini-
tial first surgery on December 16th to
remove the adenocarcinoma, although
on the evening of January 3rd she was
admitted with a fever and treated with
antibiotics.
There are three narrative containers in this snip-
pet ? (1) the broad period leading up to the docu-
ment creation time which includes the events of re-
covered and adenocarcinoma, (2) December 16th,
which includes the events of surgery and remove,
and (3) January 3rd, which includes the events of
admitted, fever, and treated.
Using only the relation to the document creation
time would provide too coarse of a timeline result-
ing in collapsing the three narrative containers (the
coarse time bins of Raghavan et al (2012) would
collapse all events into the before admission cat-
egory). On the other hand, marking explicit links
between every pair of events and temporal expres-
sions would be tedious and redundant. In this ex-
ample, there is no need to explicitly mark that, for
instance, fever was AFTER surgery, because we
know that the fever happened on January 3rd and
that the surgery happened on December 16th, and
that January 3rd is AFTER December 16th. With
the grouping of EVENTs in this way, we can infer
the links between them and reduce annotator ef-
fort. Narrative containers strike the right balance
between parsimony and expressiveness.
2.3 Related Work
Of course, the possibility of annotating temporal
containment relations was allowed by even the ear-
liest versions of the TimeML specification using
TLINKs with the relation type INCLUDES. How-
ever, TimeML is a specification not a guideline,
and as such, the way in which temporal relations
have been annotated has varied widely and no
19
corpus has previously been annotated with narra-
tive containers in mind. In the TimeBank corpus
(Pustejovsky et al, 2003a), annotators annotated
only a sparse, mostly disconnected graph of the
temporal relations that seemed salient to them. In
TempEval 2007 and 2010 (Verhagen et al, 2007;
Verhagen et al, 2010), annotators annotated only
relations in specific constructions ? e.g. all pairs
of events and times in a sentence ? and used a re-
stricted set of relation types that excluded the IN-
CLUDES relation. TempEval 2013 (UzZaman et
al., 2013) allowed INCLUDES relations, but again
only in particular constructions or when the rela-
tion seemed salient to the annotators. The 2012
i2b2 Challenge3, which provided TimeML anno-
tations on clinical data, annotated the INCLUDES
relation, but merged it with other relations for the
evaluation due to low inter-annotator agreement.
Since no narrative container-annotated corpora
exist, there are also no existing models for extract-
ing narrative container relations. However, we
can draw on the various methods applied to re-
lated temporal relation tasks. Most relevant is the
work on linking events to timestamps. This was
one of the subtasks in TempEval 2007 and 2010,
and systems used a variety of features including
words, part-of-speech tags, and the syntactic path
between the event and the time (Bethard and Mar-
tin, 2007; Llorens et al, 2010). Syntactic path
features were also used in the 2012 i2b2 Chal-
lenge, where they provided gains especially for
intra-sentential temporal links (Xu et al, 2013).
Recent research has also looked to syntac-
tic tree kernels for temporal relation extraction.
Mirroshandel et al (2009) used a path-enclosed
tree (i.e., selecting only the sub-tree containing
the event and time), and used various weighting
scheme variants of this approach on the Time-
Bank (Pustejovsky et al, 2003a) and Opinion4
corpora. Hovy et al (2012) used a flat tree struc-
ture for each event-time pair, including only token-
based information (words, part of speech tags) be-
tween the event and time, and found that adding
such tree kernels on top of a baseline set of fea-
tures improved event-time linking performance on
the TempEval 2007 and Machine Reading cor-
pora (Strassel et al, 2010). While Mirroshandel et
al. saw improvements using a representation with
syntactic structure, Hovy et al used the flat tree
3http://i2b2.org/NLP/TemporalRelations
4Also known as the AQUAINT TimeML corpus ?
http://www.timeml.org
structure because they found that ?using a full-
parse syntactic tree as input representation did not
help performance.? Thus, it remains an open ques-
tion exactly where and when syntactic tree kernels
will help temporal relation extraction.
3 Methods
Inspired by this prior work, we treat the narrative
container extraction task as a within-sentence rela-
tion extraction task between time and event men-
tions. For each sentence, this approach iterates
over every gold standard annotated EVENT, pair-
ing it with each TIMEX3 in the sentence, and uses
a supervised machine learning algorithm to clas-
sify each pair as related by the CONTAINS relation
or not. Training examples are generated in the
same way, with pairs corresponding to annotated
links marked as positive examples and all others
marked as negative. We investigate a variety of
features for the classifier as well as a variety of
tree kernel combinations.
This straightforward approach does not address
all relation pairs, setting aside event-event rela-
tions and inter-sentential relations, which are both
likely to require different approaches.
3.1 SVM with Tree Kernels
The machine learning approach we use is support
vector machine (SVM) with standard feature ker-
nels, tree kernels, and composite kernels that com-
bine the two. SVMs are used extensively for clas-
sification tasks in natural language processing, due
to robust performance and widely available soft-
ware packages. We take advantage of the ability
in SVMs to represent structured features such as
trees using convolution kernels (Collins and Duffy,
2001), also known as tree kernels. This kernel
computes similarity between two tree structures
by computing the number of common sub-trees,
with a weight parameter to discount the influence
of larger structural similarities. The specific for-
malism we use is sometimes called a subset tree
kernel (Moschitti, 2006), which checks for simi-
larity on subtrees of all sizes, as long as each sub-
tree has its production rule completely expanded.
A useful property of kernels is that a linear com-
bination of two kernels is guaranteed to be a ker-
nel (Cristianini and Shawe-Taylor, 2000). In ad-
dition, the product of two kernels is also a ker-
nel. This means that it is simple to combine tradi-
tional feature-based kernels used in SVMs (linear,
20
polynomial, radial basis function) with tree ker-
nels representing structural information. This ap-
proach of using composite kernels has been widely
used in the task of relation extraction where syn-
tactic information is presumed to be useful, but is
hard to represent as traditional numeric features.
We investigate a few different composite ker-
nels here, including a linear combination:
KC(o1, o2) = ? ?KT (t1, t2) +KF (f1, f2) (1)
where a composite kernel KC operates on objects
oj composed of features fj and tree tj , by adding
a tree kernel KT weighted by ? to a feature kernel
KF . We also use a composite kernel that takes the
product of kernels:
KC(o1, o2) = KT (t1, t2) ?KF (f1, f2) (2)
Sometimes it is beneficial to make use of multi-
ple syntactic ?views? of the same instance. Below
we will describe many different tree representa-
tions, and the tree kernel framework allows them
to all be used simultaneously, by simply summing
the similarities of the different representations and
taking the combined sum as the tree kernel value:
KT ({t
1
1, t
2
1 . . . , t
N
1 }, {t
1
2, t
2
2, . . . , t
N
2 }) =
N?
i=1
KT (t
i
1, t
i
2) (3)
where i indexes the N different tree views. In all
kernel combinations we compute the normalized
version of both the feature and tree kernels so that
they can be combined on an even footing.
The actual implementations we use for train-
ing are the SVM-LIGHT-TK package (Mos-
chitti, 2006), which is a tree kernel extension to
SVMlight (Joachims, 1999). At test time, we
use the SVM-LIGHT-TK bindings of the ClearTK
toolkit (Ogren et al, 2009) in a module built on
top of Apache cTAKES (Savova et al, 2010), to
take advantage of the pre-processing stages.
3.2 Flat Features
The flat features developed for the standard fea-
ture kernel include the text of each argument as a
whole, the tokens of each argument represented as
a bag of words, the first and last word of each ar-
gument, and the preceding and following words of
each argument as bags of words. The token con-
text between arguments is also represented using
the text span as a whole, the first and last words,
the set of words represented as a bag of words, and
the distance between the arguments. In addition,
part of speech (POS) tag features are extracted for
each mention, with separate bag of POS tag fea-
tures for each argument. The POS features are
generated by the cTAKES POS tagger.
We also include semantic features of each argu-
ment. For event mentions, we include a feature
marking the contextual modality, which can take
on the possible values Actual, Hedged, Hypothet-
ical, or Generic, which is part of the gold stan-
dard annotations. This feature was included as it
was presumed that actual events are more likely
to have definite time spans, and thus be related
to times, than hypothetical or generic mentions of
events. For time mentions we include a feature for
the time class, with possible values of Date, Time,
Duration, Quantifier, Set, or Prepostexp. The time
class feature was used as it was hypothesized that
dates and times are more likely to contain events
than sets (e.g., once a month).
3.3 Tree Kernel Representations
We leverage existing tree kernel representations
for this work, using some directly and others as
starting point to a domain-specific representation.
First, we take advantage of the (relatively) flat
structured tree kernel representations of Hovy et
al. (2012). This representation uses lexical items
such as POS tags rather than constituent struc-
ture, but places them into an ordered tree struc-
ture, which allows tree kernels to use them as a
bag of items while also taking advantage of order-
ing structure when it is useful. Figure 1 shows an
example tree for an event-time pair for which a re-
lation exists, where the lexical information used is
POS tag information for each term (the represen-
tation that Hovy et al found most useful). We also
used a version of this representation where the sur-
face form is used instead of the POS tag.
While Hovy et al showed positive results using
this representation over just standard features, it is
still somewhat constrained in its ability to repre-
sent long distance relations. This is because the
subset tree kernel compares only complete rule
productions, and with long distance relations a flat
tree structure will have a production that is too big
to learn. Alternatively, tree kernel representations
can be based on constituent structure, as is com-
mon in the relation extraction literature. This will
21
BOP
Event-Actual
TOK
VBN
TOK
TO
TOK
VB
TOK
NN
Timex-Date
TOK
JJ
TOK
NN
BOW
Event-Actual
TOK
scheduled
TOK
to
TOK
undergo
TOK
surgery
Timex-Date
TOK
next
TOK
week
Figure 1: Two trees indicating the flat tree kernel
representation. Above is the bag of POS tags ver-
sion; below is the bag of words version.
hopefully allow for the representation of longer
distance relations by taking advantage of syntactic
sub-structure with smaller productions. The rep-
resentations used here are known as Feature Trees
(FT), Path Trees (PT) and Path-Enclosed Trees
(PET).
The Feature Tree representation takes the en-
tire syntax tree for the sentence containing both
arguments and inserts semantic information about
those arguments. That information includes the ar-
gument type (EVENT or TIMEX) as an additional
tree node above the constituent enclosing the argu-
ment. We also append semantic class information
to the argument (contextual modality for events,
time class for times), as in the flat features.
The Feature Tree representation is not com-
monly used, as it includes an entire sentence
around the arguments of interest, and that may in-
clude a great deal of unrelated structure that adds
noise to the classifier. Here we include it in an at-
tempt to get to the root of an apparent discrepancy
in the tree kernel literature, as explained in Sec-
tion 2, in which Hovy et al (2012) report a nega-
tive result and Mirroshandel et al (2009) report a
positive result for using constituency structure in
tree kernels for temporal relation extraction.
The Path Tree representation uses a sub-tree of
the whole constituent tree, but removes all nodes
that are not along the path between the two argu-
ments. Path information has been used in standard
feature kernels (Pradhan et al, 2008), with each
individual path being a possible boolean feature.
VP
Arg1-Event-Actual
arg1
S
VP
VP
Arg2-Timex-Date
arg2
Figure 2: Path Tree (PT) representation
Another representation making use of the path tree
takes contiguous subsections of the path tree, or
?path n-grams,? in an attempt to combat the spar-
sity of using the whole path (Zheng et al, 2012).
By using the path representation with a tree ker-
nel, the model should get the benefit of all different
sizes of path n-grams, up to the size of the whole
path. This representation is augmented by adding
in argument nodes with event and time features, as
in the Feature Tree. Unlike the Feature Tree and
the PET below, the Path Tree representation does
not include word nodes, because the important as-
pect of this representation is the labels of the nodes
on the path between arguments. Figure 2 shows an
example of what this representation looks like.
The Path-Enclosed Tree representation is based
on the smallest sub-tree that encloses the two pro-
posed arguments. This is a representation that has
shown value in other work using tree kernels for
relation extraction (Zhang et al, 2006; Mirroshan-
del et al, 2009). The information contained in
the PET representation is a superset of that con-
tained in the Path Tree representation, since it in-
cludes the full path between arguments as well
as the structure between arguments and the ar-
gument text. This means that it can take into
account path information while also considering
constituent structure between arguments that may
play a role in determining whether the two ar-
guments are related. For example, temporal cue
words like after or during may occur between ar-
guments and will not be captured by Path Trees.
Like the PT representation, the PET representa-
tion is augmented with the semantic information
specified above in the Feature Tree representation.
Figure 3 shows an example of this representation.
22
VP
Arg1-Event-Actual
VBN
scheduled
VP
TO
to
VP
VB
undergo
NP
NN
surgery
Arg2-Timex-Date
NP
JJ
next
NN
week
Figure 3: Path-Enclosed Tree representation
4 Evaluation
The corpus we used for evaluations was described
in Section 2. There are 78 total notes in the corpus,
with three notes for each of 26 patients. The data
is split into training (50%), development (25%),
and test (25%) sections based on patient number,
so that each patient?s notes are all in the same
section. The combined training and development
set used for final training consists of 4378 sen-
tences with 49,050 tokens, and 7372 events, 849
time expressions, and 2287 CONTAINS relations.
There were 774 positive instances of CONTAINS
in the training data, with 1513 negative instances.
For constituent structure and features we use the
gold standard treebank and event and time features
from our corpus. Preliminary work suggests that
automatic parses from cTAKES do not harm per-
formance very much, but the focus of this work is
on the relation extraction so we use gold standard
parses. All preliminary experiments were done us-
ing the development set for testing.
We designed a set of experiments to exam-
ine several hypotheses regarding extraction of the
CONTAINS relation and the efficacy of different
tree kernel representations. The first two config-
urations test simple rule-based baseline systems,
CLOSEST-P and CLOSEST-R, for distance-related
decision rule systems meant to optimize precision
and recall, respectively. CLOSEST-P hypothesizes
a CONTAINS link between every TIMEX3 and the
closest annotated EVENT, which will make few
links overall. CLOSEST-R hypothesizes a CON-
TAINS link between every EVENT and the closest
TIMEX3, which will make many more links.
The next configuration, Flat Features, uses the
token and part of speech features along with ar-
gument semantics features, as described in Sec-
tion 3. While this feature set may not seem ex-
haustive, in preliminary work many traditional re-
lation extraction features were tried and found to
not have much effect. This particular configura-
tion was tested because it is most comparable to
the bag of word and bag of POS kernels from
Hovy et al (2012), and should help show whether
the tree kernel is providing anything over an equiv-
alent set of basic features.
We then examine several composite kernels, all
using the same feature kernel, but using different
tree kernel-based representations. First, we use a
composite kernel which uses the bag of word and
bag of POS tree views, as in Hovy et al (2012).
Next, we add in two additional tree views to the
tree kernel, Path-Enclosed Tree and Path Tree,
which are intended to examine the effect of using
traditional syntax, and the long distance features
that they enable. The final experimental config-
uration replaces the PET and PT representations
from the last configuration with the Feature Tree
representation. This tests the hypothesis that the
difference between positive results for tree kernels
in this task (as in, say, Mirroshandel et al (2009))
and negative results reported by Hovy et al (2012)
is the difference between using a full-parse tree
and using standard sub-tree representations.
For the rule-based systems, there are no param-
eters to tune. Our machine-learning systems are
based on support vector machines (SVM), which
require tuning of several parameters, including
kernel type (linear, polynomial, and radial basis
function), the parameters for each kernel, and c,
the cost of misclassification. Tree kernels intro-
duce an additional parameter ? for weighting large
structures, and the use of a composite kernel in-
troduces parameters for which kernel combination
operator to use, and how to weight the different
kernels for the sum operator.
For each machine learning configuration, we
performed a large grid search over the combined
parameter space, where we trained on the train-
ing set and tested on the development set. For
the final experiments, the parameters were chosen
that optimized the F1 score on the development
set. Qualitatively, the parameter tuning strongly
favored configurations which combined the ker-
nels using the sum operator, and recall and pre-
cision were strongly correlated with the SVM pa-
rameter c. Using these parameters, we then trained
23
on the combined training and development sets
and tested on the official test set.
4.1 Evaluation Metrics
The state of evaluating temporal relations has been
evolving over the past decade. This is partially
due to the inferential properties of temporal rela-
tions, because it is possible to define the same set
of relations using different set of axioms. To take
a very simple example, given a gold set of rela-
tions A<B and B<C, and given the system output
A<B, A<C and B<C, if one were to compute a
plain precision/recall metric, then the axiom A<C
would be counted against the system, when one
can easily infer from the gold set of relations that
it is indeed correct. With more relations the infer-
ence process becomes more complex.
Recently there has been some work trying
to address the shortcomings of the plain F1
score (Muller and Tannier, 2004; Setzer et al,
2006; UzZaman and Allen, 2011; Tannier and
Muller, 2008; Tannier and Muller, 2011). How-
ever, the community has not yet come to a consen-
sus on the best evaluation approach. Two recent
evaluations, TempEval-3 (UzZaman et al, 2013)
and the 2012 i2b2 Challenge (Sun et al, 2013),
used an implementation of the proposal by (Uz-
Zaman and Allen, 2011). However, as described
in Cherry et al (2013), this algorithm, which uses
a greedy graph minimization approach, is sensi-
tive to the order in which the temporal relations
are presented to the scorer. In addition, the scorer
is not able to give credit for non-redundant, non-
minimum links (Cherry et al, 2013) as with the
the case of the relation A<C mentioned earlier.
Considering that the measures for evaluating
temporal relations are still evolving, we decided to
use plain F-score, with recall and precision scores
also reported. This score is computed across all
intra-sentential EVENT-TIMEX3 pairs in the gold
standard, where precision = # correct predictions# predictions ,
recall = # correct predictions# gold standard relations , and F1 score =
2?precision?recall
precision+recall .
4.2 Experimental Results
Results are shown in Table 1. Rule-based base-
lines perform reasonably well, but are heavily bi-
ased in terms of precision or recall. The ma-
chine learning baseline cannot even obtain the
same performance as the CLOSEST-R rule-based
system, though it is more balanced in terms of pre-
System Precision Recall F1
CLOSEST-P 0.754 0.537 0.627
CLOSEST-R 0.502 0.947 0.656
Flat Features (FF) 0.705 0.593 0.645
FF+Bag Trees (BT) 0.649 0.728 0.686
FF+BT+PET+PT 0.770 0.707 0.737
FF+BT+FT 0.691 0.691 0.691
Table 1: Table of results of main experiments.
cision and recall. Using a composite kernel which
adds in the flat token-based tree kernels improves
performance over the standard feature kernel by
4.1 points. Adding in the Path Tree and Path-
Enclosed Tree constituency-based trees along with
the flat trees improves F1 score to our best result
of 73.7. Finally, replacing PT and PET representa-
tions with the Feature Tree representation does not
offer any performance improvement over the Flat
Features + Bag Trees configuration.
4.3 Error Analysis
We performed error analysis on the outputs of the
best-performing system (FF+BT+PET+PT in Ta-
ble 1). First, we note that the parameter search
was optimized for F1. This resulted in the highest-
scoring configuration using a composite kernel
with the sum operator, polynomial kernel for the
secondary kernel, ? = 0.5, tree kernel weight (T )
of 0.1, and c = 10.0. This high value of c and low
value of T results in higher precision and lower
recall, but there were configurations with lower c
and higher T which made the opposite tradeoff,
with only marginally worse F1-score. For the pur-
poses of error analysis, however, this configuration
leads to a focus on false negatives.
First, the false positives contained many rela-
tions that were legitimately ambiguous or possible
annotator errors. An example ambiguous case is
She is currently being treated on the Surgical Ser-
vice for..., in which the system generates the re-
lation CONTAINS(currently, treated), but the gold
standard labels as OVERLAP. This example is am-
biguous because it is not clear from just the lin-
guistic context whether the treatment is wholly
contained in the small time window denoted by
currently, or whether it started a while ago or will
continue into the future. There are many similar
cases where the event is a disease/disorder type,
and the specific nature of the disease is impor-
tant to understanding whether this is a CONTAINS
24
or OVERLAP relation, specifically understanding
whether the disease is chronic or more acute.
Another source of false positives were where
the event and time were clearly related, but not
with CONTAINS. In the example reports that she
has been having intermittent bleeding since May
of 1998, the term since clearly indicates that this
is a BEGINS-ON relation between bleeding and
May of 1998. This is a case where having other
temporal relation classifiers may be useful, as they
can compete and the relation can be assigned to
whichever classifier is more confident.
False negatives frequently occurred in contexts
where the event and time were far apart. Syn-
tactic tree kernels were introduced to help im-
prove recall on longer-distance relations, and were
successful up to a limit. However, certain ex-
amples are so far apart that the algorithm may
have had difficulty sorting noise from important
structure. For example, the system did not find
the CONTAINS(October 27, 2010, oophorectomy)
relation in the sentence:
October 27, 2010, Dr. XXX performed
exploratory laparotomy with an trans-
verse colectomy and Dr. YYY performed
a total abdominal hysterectomy with a
bilateral salpingo-oophorectomy.
Here, while the date may be part of the same sen-
tence as the event, the syntactic relation between
the pair is not what makes the relation; the date is
acting as a kind of discourse marker that indicates
that the following events are contained. This sug-
gests that discourse-level features may be useful
even for the intra-sentential classification task.
Other false negatives occurred where there was
syntactic complexity, even on shorter examples.
The subset tree kernel used here matches com-
plete rule productions, and across complex struc-
ture with large productions, the chances of finding
similarity decreases substantially. Thus, events
within coordination or separated from the time by
clause breaks are more difficult to relate to the
time due to the multiple different ways of relating
these different syntactic elements.
Finally, there are some examples where the an-
chor of a narrative container is an event with mul-
tiple sub-events. In these cases, the system per-
forms well at relating a time expression to the an-
chor event, but may miss the sub-events that are
farther away. This is a case where having an event-
event TLINK classifier, then applying determinis-
tic closure rules, would allow a combined system
to link the sub-events to the time expression.
5 Discussion and Conclusion
In this paper we have developed a system for auto-
matically identifying CONTAINS relations in clini-
cal text. The experiments show first that a machine
learning approach that intelligently integrates con-
stituency information can greatly improve perfor-
mance over rule-based baselines. We also show
that the tree kernel approach, which can model se-
quence better than a bag of tokens-style approach,
is beneficial even when it uses the same features.
Finally, the experiments show that choosing the
correct representation is important for tree kernel
approaches, and specifically that using a full parse
tree may give inferior performance compared to
sub-trees focused on the structure of interest.
In general, there is much work to be done in the
area of representing temporal information in clin-
ical records. Many of the inputs to the algorithm
described in this paper need to be extracted auto-
matically, including time expressions and events.
Work on relations will focus on adding features
to represent discourse information and richer rep-
resentation of event semantics. Discourse infor-
mation may help with the longer-distance errors,
where the time expression acts almost as a topic
for an extended description of events. Better un-
derstanding of event semantics, such as whether
a disease is chronic or acute, or typical duration
for a treatment, may help constrain relations. In
addition, we will explore the effectiveness of us-
ing dependency tree structure, which has been use-
ful in the domain of extracting relations from the
biomedical literature (Tikk et al, 2013).
Acknowledgements
The work described was supported by Tempo-
ral History of Your Medical Events (THYME)
NLM R01LM010090 and Integrating Informat-
ics and Biology to the Bedside (i2b2) NCBO
U54LM008748. Thanks to the anonymous re-
viewers for thorough and insightful comments.
References
Naushad UzZaman, Hector Llorens, et al 2013. Semeval-
2013 task 1: Tempeval-3: Evaluating time expressions,
events, and temporal relations. In Second Joint Confer-
ence on Lexical and Computational Semantics (*SEM),
25
Volume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages 1?9,
Atlanta, Georgia, USA, June. Association for Computa-
tional Linguistics.
Ashwin N Ananthakrishnan, Tianxi Cai, et al 2013. Improv-
ing case definition of Crohn?s disease and ulcerative col-
itis in electronic medical records using natural language
processing: a novel informatics approach. Inflammatory
bowel diseases.
Steven Bethard and James H. Martin. 2007. CU-TMP: Tem-
poral relation classification using syntactic and semantic
features. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007), pages
129?132.
Colin Cherry, Xiaodan Zhu, et al 2013. A la recherche du
temps perdu: extracting temporal relations from medical
text in the 2012 i2b2 NLP challenge. Journal of the Amer-
ican Medical Informatics Association, March.
Michael Collins and Nigel Duffy. 2001. Convolution kernels
for natural language. In Neural Information Processing
Systems.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Dirk Hovy, James Fan, et al 2012. When did that happen?:
linking events and relations to timestamps. In Proceedings
of the 13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 185?193.
Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large scale svm learning
practical. In B. Schlkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector Learn-
ing. Universita?t Dortmund.
Chen Lin, Helena Canhao, et al 2012. Feature engineering
and selection for rheumatoid arthritis disease activity clas-
sification using electronic medical records. In Proceed-
ings of ICML Workshop on Machine Learning for Clinical
Data.
Hector Llorens, Estela Saquete, and Borja Navarro. 2010.
TIPSem (english and spanish): Evaluating crfs and seman-
tic roles in tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 284?291.
Association for Computational Linguistics.
Seyed Abolghasem Mirroshandel, M Khayyamian, and
GR Ghassem-Sani. 2009. Using tree kernels for clas-
sifying temporal relations between events. Proc. of the
PACLIC23, pages 355?364.
Alessandro Moschitti. 2006. Efficient convolution kernels
for dependency and constituent syntactic trees. In Ma-
chine Learning: ECML 2006, pages 318?329. Springer.
Philippe Muller and Xavier Tannier. 2004. Annotating and
measuring temporal relations in texts. In Proceedings of
the 20th international conference on Computational Lin-
guistics, COLING ?04, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Philip V. Ogren, Philipp G. Wetzler, and Steven J. Bethard.
2009. ClearTK: a framework for statistical natural lan-
guage processing. In Unstructured Information Manage-
ment Architecture Workshop at the Conference of the Ger-
man Society for Computational Linguistics and Language
Technology, 9.
Sameer S Pradhan, Wayne Ward, and James H Martin. 2008.
Towards robust semantic role labeling. Computational
Linguistics, 34(2):289?310.
James Pustejovsky and Amber Stubbs. 2011. Increasing in-
formativeness in temporal annotation. In Proceedings of
the 5th Linguistic Annotation Workshop, pages 152?160.
James Pustejovsky, Patrick Hanks, et al 2003a. The time-
bank corpus. In Corpus linguistics, volume 2003, page 40.
James Pustejovsky, Jose? Casta no, et al 2003b. Timeml:
Robust specification of event and temporal expressions in
text. In Fifth International Workshop on Computational
Semantics (IWCS-5).
Preethi Raghavan, Eric Fosler-Lussier, and Albert M Lai.
2012. Temporal classification of medical events. In Pro-
ceedings of the 2012 Workshop on Biomedical Natural
Language Processing, pages 29?37. Association for Com-
putational Linguistics.
Guergana K. Savova, James J. Masanz, et al 2010. Mayo
clinical text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and appli-
cations. J Am Med Inform Assoc, 17(5):507?513.
Andrea Setzer, Robert Gaizauskas, and Mark Hepple. 2006.
The role of inference in the temporal annotation and anal-
ysis of text. Language Resources and Evaluation, 39(2-
3):243?265, February.
Stephanie Strassel, Dan Adams, et al 2010. The DARPA
machine reading program - encouraging linguistic and rea-
soning research with a series of reading tasks. In Proceed-
ings of the Seventh International Conference on Language
Resources and Evaluation (LREC?10), Valletta, Malta.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informatics
Association, April.
Xavier Tannier and Philippe Muller. 2008. Evaluation met-
rics for automatic temporal annotation of texts. Proceed-
ings of the Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08).
Xavier Tannier and Philippe Muller. 2011. Evaluating tem-
poral graphs built from texts via transitive reduction. J.
Artif. Int. Res., 40(1):375413, January.
Domonkos Tikk, Ille?s Solt, et al 2013. A detailed error anal-
ysis of 13 kernel methods for protein-protein interaction
extraction. BMC bioinformatics, 14(1):12.
Naushad UzZaman and James Allen. 2011. Temporal eval-
uation. In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Lan-
guage Technologies, page 351?356.
Marc Verhagen, Robert Gaizauskas, et al 2007. Semeval-
2007 task 15: Tempeval temporal relation identification.
In Proceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 75?80.
Marc Verhagen, Roser Sauri, et al 2010. Semeval-2010 task
13: Tempeval-2. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 57?62, Uppsala,
Sweden, July. Association for Computational Linguistics.
RA Wilke, H Xu, et al 2011. The emerging role of electronic
medical records in pharmacogenomics. Clinical Pharma-
cology & Therapeutics, 89(3):379?386.
Yan Xu, Yining Wang, et al 2013. An end-to-end system to
identify temporal relation in discharge summaries: 2012
i2b2 challenge. Journal of the American Medical Infor-
matics Association : JAMIA.
Min Zhang, Jie Zhang, and Jian Su. 2006. Exploring syn-
tactic features for relation extraction using a convolution
tree kernel. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of the
ACL, pages 288?295.
Jiaping Zheng, Wendy W Chapman, et al 2012. A sys-
tem for coreference resolution for the clinical narrative.
Journal of the American Medical Informatics Association,
19:660?667.
26
