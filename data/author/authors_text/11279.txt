Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1041?1048
Manchester, August 2008
Measuring and Predicting Orthographic Associations:
Modelling the Similarity of Japanese Kanji
Lars Yencken and Timothy Baldwin
{lljy,tim}@csse.unimelb.edu.au
NICTA Research Lab
University of Melbourne
Abstract
As human beings, our  mental  processes
for  recognising  linguistic  symbols  gen-
erate  perceptual  neighbourhoods  around
such symbols where confusion errors oc-
cur. Such  neighbourhoods  also  pro-
vide  us  with  conscious  mental  associa-
tions between symbols. This  paper  for-
malises orthographic models for similarity
of Japanese kanji, and provides a proof-
of-concept dictionary extension leveraging
the mental associations provided by ortho-
graphic proximity.
1 Introduction
Electronic dictionary interfaces have evolved from
mere digitised forms of their paper ancestors. They
now enhance accessibility by addressing the sep-
arate needs of language consumers and language
producers, of learners from non-speakers to native
speakers, and by targeting the specific difficulties
presented by individual languages.
For languages with logographic orthographies,
such  as  Japanese  and  Chinese, accessibility  re-
mains poor due to the difficulties in looking up
an unknown character in the dictionary. The tra-
ditional method of character lookup in these lan-
guages involves identifying the primary compo-
nent (or ?radical?), counting its strokes, looking it
up in the index, counting the remainder of strokes
in the original character, then finding the character
in a sub-index. This presents several opportunities
for error, but fortunately improvements have been
made, as we discuss in Section 2.
We are interested in the perceptual process of
identifying characters, in particular the behaviour
of perception within dense visual neighbourhoods.
Within the dictionary accessibility space, we are
?2008. Licensed  under  the Creative  Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
motivated by the potential to correct confusion er-
rors, but also to leverage the mental associations
provided by visual proximity to allow advanced
learners  to  find  unknown characters  faster. As
proof-of-concept, we propose a method for look-
ing up unknown words with unfamiliar characters,
based on similarity with known characters.
In  essence, our  method  is  based  on  the  user
plausibly ?mistyping? the word based on closely-
matching kanji they are familiar with (and hence
can readily access via a standard input method ed-
itor), from which we predict the correct kanji com-
bination based on kanji similarity and word fre-
quency. For example, given the input ??, the
system could suggest the word?? [hosa] ?help?
based on similarity between the high-frequency?
and the graphically-similar but low-frequency?.
The  proposed  method  is  combined  with  the
FOKS lookup strategy proposed by Bilac (2002)
for looking up unknown words via plausibly incor-
rect readings.
The contributions of this paper are the proposal
of a range of character similarity models for logo-
graphic scripts, a novel evaluation method for lo-
gographic character confusability, and the incorpo-
ration of kanji similarity into a word-level lookup
model.
The remainder of this paper is structured as fol-
lows. Firstly, we review related lookup systems
(Section 2), and go on to discuss how we measure
and model kanji similarity, including an evalua-
tion of the methods (Section 3). We then focus on
the conversion of similarity models into confusion
models, and their integration into a search interface
(Section 4). Examining both our models and the
interface itself, we discuss our findings (Section 5)
before finally concluding (Section 6).
2 A review of related systems
2.1 Associative lookup systems
Associative  lookup  systems  are  based  on  the
premise that characters and words form a highly
1041
connected lexical network. They focus on finding
and making accessible the mental links provided
by proximity within this network. In contrast, sys-
tems which correct for confusion model plausible
errors in order to recover from them. Examples of
associative systems are as follows:
Semantic Orthographic
(for producers) (for consumers)
Monolingual
Visual WordNet
this paper
Bilingual
standard  bilingual
dictionaries
Pinyomi  dictio-
nary interface
Ferret and Zock (2006) introduce the distinction
between language producers as encoders of seman-
tic  information, and language consumers  as  de-
coders of orthographic (or phonetic) information.
We first consider systems to aid production of lan-
guage.
Systems for production give form and sound to
known semantics. The most common such systems
are bilingual dictionaries which associate words in
one language with their near-synonyms in a second
language. Even within a monolingual context, the
problem of selecting the right word can be difficult,
whether the difficulty is one of limited knowledge
or simply one of access, as in the case of the tip-
of-the-tongue problem. Work in this area (Zock,
2002; Zock and Bilac, 2004) has more recently fo-
cused on extending WordNet with syntagmatic re-
lationships (Ferret and Zock, 2006). Access could
take the form of the Visual WordNet Project.
1
For language consumers, the challenge is to find
the meaning or sound of a word with known form.
For logographic languages, where characters are
entered phonetically
2
using an input method ed-
itor, computer  input  of  an  unknown word  with
known form remains difficult, since the reading is
unknown.
In  a  bilingual  context, the  Pinyomi  Chinese-
Japanese dictionary interface overcomes this ob-
stacle by allowing Japanese speakers to look up
a Chinese word via the Japanese-equivalent char-
acters based on orthographic associations between
similar characters (Yencken et al, 2007).
Our proposed extension to the FOKS dictionary
is functionally similar to Pinyomi, but in a mono-
lingual Japanese context. In our case, a Japanese
word containing unknown characters is found by
1http://kylescholz.com/projects/wordnet/
2
A notable exception is the Wubizixing lookup method for
Chinese.
querying with known characters that are visually
similar. Unlike Pinyomi, which uses an ideogram
transliteration table to determine associations, we
use direct models of character similarity to deter-
mine associations.
2.2 Kanji lookup systems
We next provide a brief review of five kanji lookup
systems in order to situate our proposed interface
appropriately.
The SKIP (System of Kanji Indexing by Pat-
terns)  system  of  lookup  provides  an  indexing
scheme based on a kanji?s overall shape rather than
its primary radical (Halpern, 1999). For example,
? [aka] ?bright? has skip code 1-4-4, with the first
number indicating it is horizontally split into two
parts, and the second and third numbers represent-
ing the respective stroke counts of the two parts.
The Kansuke dictionary simplifies the method of
counting strokes, to form a three-number code rep-
resenting the horizontal, vertical and other strokes
that make up a character (Tanaka-Ishii and Godon,
2006). Characters can also be looked up from their
components. For our earlier example ? consists
of? with code 3-2-0 and? with code 3-1-1.
The  Kanjiru  dictionary  (Winstead, 2006)  at-
tempts  to  interactively  assemble  a  character  by
shape and stroke via mouse movements, providing
the user with structural ways of building up com-
ponents until the desired character is found.
Finally, hand-writing interfaces attempt to cir-
cumvent the computer input problem altogether,
but still suffer from several issues: the awkward-
ness of mouse input for drawing characters; sensi-
tivity to both stroke order and connectivity of com-
ponents; and the difference in hand-writing styles
between learners and native speakers.
These lookup methods contrast  with our pro-
posed similarity-based search in several ways.
Firstly, our  method  combines  word-  and
character-level information directly, yet provides
the means to lookup words with unknown charac-
ters without the use of wildcards. The downside to
this is that the user needs to use kanji in the search
query, limiting potential users to intermediate and
advanced learners with some knowledge of kanji.
Secondly, we are able to cater to both intentional
similarity-based searches, and unintentional input
errors, increasing the accessibility of the base dic-
tionary. This approach shares much with the FOKS
dictionary interface (Bilac, 2002), which provides
1042
error-correcting lookup for reading-based dictio-
nary queries. Suppose, for example, a user wishes
to look up the word ?? ?festival float?, but is
unsure of its pronunciation. FOKS allows them
to guess the pronunciation based on readings they
know for each character in other contexts. In this
case, they might combine ? [yama] ?mountain?
and ? [kuruma] ?car? and guess the word read-
ing as [yamakuruma]. The correct reading [dashi]
cannot be guessed from the word?s parts, but our
educated guess would lead the user to the word
and provide access to both the correct reading and
meaning.
This  approach  is  complementary  to  our  pro-
posed method. Suppose, analogously, that the user
wishes to look up the word?? but is unfamiliar
with the first kanji. A query for ?? would trig-
ger an inference based on the similarity between?
and?, and provide the desired word in the results,
allowing the user to determine both its pronuncia-
tion [h?moN] and its meaning ?visit?.
3 Modelling similarity
3.1 Metric space models
There has been little work on methods for measur-
ing or predicting the similarity between two kanji.
While there have been many psycholinguistic stud-
ies  on  various  specific  aspects  of  perception  of
Chinese and Japanese logographic characters, few
touch directly on orthographic confusion. For a
brief discussion, see Yencken and Baldwin (2006).
Broadly, current  literature  suggests  that  kanji
recognition may be hierarchical, building radicals
from strokes, and whole characters from radicals.
Each point of recognition and combination sug-
gests a potential site for misrecognition or confu-
sion with an orthographic or semantic neighbour.
The most directly relevant study involved two
experiments  by Yeh  and  Li  (2002). In  a  sort-
ing task, subjects tended to categorise characters
by their structure, rather than their shared compo-
nents. In a subsequent search task, presence of
shared structure between target and distractors was
the dominant factor in subjects? response times.
We previously proposed two naive kanji similar-
ity measures: a cosine similarity metric operating
on boolean radical vectors, and the l
1
norm (Man-
hattan distance) between rendered images of kanji
(Yencken and Baldwin, 2006). Evaluating on a
set of human similarity judgements, we determined
that the cosine similarity method outperformed the
d
stroke
d
tree
d
radical
?
?
????
????
l
1
?
?
?
?
3, 11a, 2a, 2a
3, 11a, 2a, 2a, 2a
?
? ?
? ?
? ? ?
? ? ? ? ? ?
?
? ?
? ?
? ? ?
? ? ? ? ? ?
? ?
Figure 1: A summary of our kanji distance metrics
l
1
norm, although it had lower precision for high-
similarity pairs.
3.1.1 Bag of radicals with shape
When learners of Japanese study a new charac-
ter, they do not study its strokes in isolation, but
instead build on prior knowledge of its component
radicals. For example, ? [aka] ?bright? could be
analysed as being made up of the? [sun] ?hi? and
? [moon] ?tsuki? radicals.
Radicals are useful in several ways. The num-
ber of radicals in any kanji is much smaller than
the number of strokes for any kanji, making such
kanji easier to chunk and recall in memory. Fur-
thermore, radicals can provide cues to the mean-
ing and pronunciation of characters which contain
them.
3
The original metric used in Yencken and Bald-
win (2006) simply calculates the cosine similarity
between radical vectors. This ignores the position
of radicals, which is known to be important in sim-
ilarity judgements, and also the number of times
each radical occurs within a kanji. Hence, ?, ?
and ? are all considered identical (radical = ?),
as are? and? (radical =?). The metric is cal-
3
For example, kanji containing the radical ?, such as?
[mune] ?chest? and? [ude] ?arm?, are reliably body parts.
Kanji containing the radical ?, as in ? [d?] ?copper? and
? [d?] ?body?, often have the Chinese or on reading [d?]
amongst their valid pronunciations.
1043
culated by:
d
radical
(x,y) = 1? rx ? ry
|rx||ry|
(1)
To address radical multiplicity, and the findings
of Yeh and Li?s study, we set the above metric to
unit distance whenever the two characters differ
in their basic shape. To approximate shape, we
use the first part of each kanji?s 3-part SKIP code.
which can take values horizontal, vertical, contain-
ment or other. SKIP codes for each kanji are pro-
vided in Kanjidic,
4
and radical membership in the
Radkfile.
5
This change allows the metric to distinguish be-
tween examples with repeated components. The
altered metric aims to capture the visual and se-
mantic salience of radicals in kanji perception, and
to also take into account some basic shape similar-
ity.
3.1.2 Distance of rendered images
In contrast to the previous approach, we can con-
sider kanji as arbitrary symbols rendered in print or
on screen, and then attempt to measure their sim-
ilarity. The simplest way to do this is to simply
render each kanji to an image of fixed size, and to
then use some distance metric over images.
A common and simple distance metric is the l
1
norm, which simply sums the difference in lumi-
nance between pixels of the two images for some
alignment. Fortunately, all kanji are intended to
occupy an identically sized block, so alignment is
via a grid, constant across all kanji. Considering
px(i, j) to be the luminance of the pixel at position
(i, j) of rendered kanji x, we evaluate the l
1
norm
as follows:
l
1
(x,y) =
?
i,j
|px(i, j)?py(i, j)| (2)
This calculation depends on the image representa-
tion chosen, and could differ slightly across fonts,
image sizes and rasterisation methods. We used the
MS Gothic font, rendering to 80x80 images, with
anti-aliasing.
This metric is  aimed at  capturing the general
overlap  of  strokes  between  the  two  characters,
along with the overlap of whitespace, which gives
useful structure information. This metric is known
to be noisy for low-to-medium similarity pairs, but
is very useful in distinguishing near neighbours.
4http://www.csse.monash.edu.au/~jwb/kanjidic.html
5http://www.csse.monash.edu.au/~jwb/kradinf.html
3.1.3 Stroke edit distance
A third possibility is to reduce kanji to the very
strokes used to write them. Two features of the or-
thography make this possible: (1) kanji are not ar-
bitrary symbols, but configurations of strokes cho-
sen from within a finite and limited set; and (2)
each kanji has a precise stroke order which is con-
sistent for reused kanji components, such that if
two or more arbitrary components were combined
to form a new pseudo-character, native speakers
would largely agree on the stroke order.
To define a metric based on strokes, we need
both  a  source  of  stroke  data  and  a  comparison
method. For stroke data, we look to a hierarchi-
cal data set for Japanese kanji created by Apel and
Quint (2004). Each kanji is specified by its strokes,
grouped into common stroke groups (components),
and broken down in a hierarchical manner into rel-
ative positions within the kanji (for example: left
and right, top and bottom). The strokes themselves
are based on a taxonomy of some 26 stroke types
(46 including sub-variants).
For any given kanji, we can flatten its hierarchy
to generate an ordered sequence of strokes: a sig-
nature for that character. The natural distance met-
ric across such sequences is the string edit distance.
This forms our d
stroke
metric.
Much  useful  information  is  preserved  within
stroke signatures. Since radicals are written in se-
quence, they form contiguous blocks in the signa-
ture. The edit distance will thus align shared radi-
cals when their position is similar enough. Since
components are usually drawn in a left-to-right,
top-to-bottom order, the order of components in
a signature also reflects their position as part of
the larger character. Finally, it provides a smooth
blending from stroke similarity to radical similar-
ity, and can recognise the similarity between pairs
like? [hi] ?sun? and? [me] ?eye?.
3.1.4 Tree edit distance
In our previous approach, we discarded much of
the hierarchical information available, relying on
stroke order to approximate it. We can instead use
the full data, and calculate the ordered tree edit dis-
tance between kanji XML representations. Tree
edit distance is defined as the length of the short-
est sequence of inserts, deletions and relabellings
required to convert  one tree into another (Bille,
2005). Though a cost function between labels can
be specified, we gave inserts/deletions and rela-
bellings unit cost.
1044
Figure 1 provides an overview of the structure
of each kanji?s representation. Actual trees also
contain  phonetic  elements, radicals, and  stroke
groups  whose  strokes  are  spread  across  several
non-contiguous blocks. Another motivation for in-
cluding tree edit distance is to determine if this ad-
ditional information is useful in determining kanji
similarity.
3.2 Evaluation
We evaluate our distance metrics over three data
sets.
The first data set is the human similarity judge-
ments from Yencken and Baldwin (2006). This
data set is overly broad in that it weights the abil-
ity to distinguish low and medium similarity pairs
equally with distinguishing medium and high sim-
ilarity pairs. It is clear that for most applications,
determining the high similarity pairs with high pre-
cision is most important. Nevertheless, this data set
is useful for comparing our metrics with those pro-
posed in previous research.
In order to better measure performance on high-
similarity pairs, which we expect to form the basis
of incorrect kanji inputs, we need a set of human-
selected confusion data. The second data set is
drawn from the White Rabbit JLPT Level 3
6
kanji
flashcards. Each flashcard contains either one or
two highly-similar neighbours which might be con-
fused with a given kanji. We use this set to deter-
mine our likely performance in a search task.
Our third data set is based on human confusabil-
ity judgements for kanji pairings.
3.2.1 Similarity experiment data
The first data consists of human similarity judge-
ments to pairs of kanji, scored on a 5 point scale
(Yencken and Baldwin, 2006). The experiment
had 179 participants, covering a broad range of
Japanese proficiency. The key participant group-
ings are: (1) non-speakers of Chinese, Japanese or
Korean (Non-CJK); (2) Japanese second-language
learners  (JSL);  and  (3)  Japanese  first-language
speakers (JFL). Figure 2 gives the rank correlation
? between each metric and a rater, averaged over
all raters in each proficiency group.
For  each  metric, the  mean  rank  correlation
increased  with  the  participants?  knowledge  of
6
Japanese Language Proficiency Test: the standard gov-
ernment test for foreigners learning Japanese.
Non-CJK JSL JFL
0
0.175
0.350
0.525
0.700
Metric agreement within rater groups
d
radical
?SHAPE
d
radical
+SHAPE
l
1
d
tree
d
stroke
Figure 2: Mean value of Spearman?s rank correlation ? over
rater groups for each metric (d
radical
(?shape) is the original
metric, and d
radical
(+shape) is our augmented version)
Japanese (from Non-CJK to JSL to JFL), indicat-
ing that the raters made more motivated and consis-
tent similarity judgements. The d
radical
(+shape)
metric dominates the other metrics, including the
original d
radical
(?shape), at all levels of knowl-
edge. This confirms the salience of radicals and the
tendency for individuals to classify kanji by their
broad shape, as suggested by Yeh and Li (2002).
l
1
, d
stroke
and d
tree
perform poorly in comparison.
Interestingly, these three metrics have large per-
formance differences for non-speakers, but not for
native-speakers.
Despite overall poor performance from our new
metrics, we were able to improve on the original
d
radical
(?shape). We now evaluate over the flash-
card data set for comparison.
3.2.2 Flashcard data set
The flashcard data differs greatly from the previ-
ous experimental data, as it consists of only human-
selected  high-similarity  pairs. Accordingly, we
took two approaches to evaluation.
Firstly, for  each  high-similarity  pair  (a pivot
kanji and its distractor), we randomly select a third
kanji from the j?y? character set
7
and combine it
with the pivot to form a second pair which is highly
likely to be low similarity. We then compare how
well each metric can classify the two pairs by im-
posing the correct ordering on them, in the form of
classification accuracy. The results of this evalua-
tion are shown in Table 1. We include a theoretical
random baseline of 0.500, since any decision has a
7
The  ?common  use?  government  kanji  set, containing
1945 characters.
1045
Metric Accuracy
d
tree
0.979
d
stroke
0.968
l
1
0.957
d
radical
0.648
random baseline 0.500
Table 1: Accuracy at detecting which of two pairs (flashcard
vs. random) has high similarity
Metric MAP p@1 p@5 p@10
d
stroke
0.594 0.313 0.151 0.100
d
tree
0.560 0.313 0.149 0.094
l
1
0.503 0.257 0.139 0.089
d
radical
0.356 0.197 0.087 0.063
Table 2: The mean average precision (MAP), and precision at
N ? {1,5,10} over the flashcard data
50% a priori chance of being successful.
The performance of d
radical
? close to our ran-
dom baseline, despite performing best in the pre-
vious task ? is suggestive of the different charac-
teristics of this task. In particular, a metric which
orders well across the broad spectrum of similarity
pairs may not be well suited to identifying high-
similarity pairs, and vice-versa.
The other  three  metrics  have  accuracy above
0.95 on this task, indicating the ease with which
they can identify high-similarity pairs. However,
this does not guarantee that the neighbourhoods
they generate will  be free from noise, since the
real-world prevalence of highly similar characters
is likely to be very low.
To better determine what dictionary search re-
sults  might  be  like, we consider  each flashcard
kanji as a query, and its high-similarity distractors
as relevant documents (and implicitly all remaining
kanji as irrelevant documents, i.e. dissimilar char-
acters). We can then calculate the Mean Average
Precision (MAP, i.e. the mean area under the preci-
sion?recall curve for a query set) and the precision
at N neighbours, for varied N . The results of this
approach are presented in Table 2.
The precision statistics confirm the ranking of
metrics found in the earlier classification task. The
d
stroke
metric outperforms l
1
by a greater margin in
the MAP statistic and precision at N = 1, but nar-
rows again for greater N . This suggests that it is
more reliable in the upper similarity ranking.
3.2.3 Distractor pool experiment
The flashcard data provides good examples of
high-similarity pairs, but suffers from several prob-
lems. Firstly, the constraints of the flashcard for-
mat limit the number of high-similarity neighbours
that can be presented on each flashcard to at most
two; in some cases we might expect more. Sec-
ondly, the  methodology behind  the  selection  of
these high-similarity neighbours is unclear.
For these reasons, we conducted an experiment
to attempt to replicate the flashcard data. 100 kanji
were randomly chosen from the JLPT 3 set (here-
after pivots). For each pivot kanji, we generated a
pool of possible high-similarity neighbours in the
following way. Firstly, we seeded the pool with
the neighbours from the flashcard data set. We then
added the highest similarity neighbour as given by
each of our similarity metrics. Since these could
overlap, we iteratively continued adding an addi-
tional neighbour from all of our metrics until our
pool contained at least four neighbours.
Native or native-like speakers of Japanese were
solicited as participants. After a dry run, each par-
ticipant was presented with a series of pivot kanji.
For each pivot kanji, they were asked to select from
its pool of neighbours which (if any) might be con-
fused for that kanji based on their graphical simi-
larity. The order of pivots was randomised for each
rater, as was the order of neighbours for each pivot.
Kanji were provided as images using MS Gothic
font for visual consistency across browsers.
Three  participants  completed  the  experiment,
selecting 1.32 neighbours per pivot on average, less
than 1.86 per pivot provided by the flashcard data.
Inter-rater agreement was quite low, with a mean
? of 0.34 across rater pairings, suggesting that par-
ticipants found the task difficult. This is unsurpris-
ing, since as native speakers the participants are
experts at discriminating between characters, and
are unlikely to make the same mistakes as learners.
Comparing their judgements to the flashcard data
set yields a mean ? of 0.37.
Ideally, this data generates a frequency distribu-
tion over potential neighbours based on the num-
ber of times they were rated as similar. However,
since the number of participants was small, we sim-
ply combined the neighbours with high-similarity
judgements for each pivot, yielding an average of
2.45 neighbours per pivot. Re-evaluating our met-
rics on this data gives the figures in Table 3.
1046
Metric MAP p@1 p@5 p@10
d
stroke
1.046 0.530 0.228 0.146
d
tree
1.028 0.540 0.228 0.136
l
1
0.855 0.480 0.200 0.117
d
radical
0.548 0.270 0.122 0.095
Table 3: The mean average precision (MAP), and precision at
N ? {1,5,10} over the distractor data
Compared to the flashcard data set, the ordering
and relative performance of metrics is similar, with
d
stroke
marginally improving on d
tree
, but both sig-
nificantly outperforming l
1
and d
radical
. The near-
doubling of high similarity neighbours from 1.32
to 2.45 is reflected by a corresponding increase in
MAP and precision@N scores, though the effect
is somewhat reduced as N increases.
4 From similarity to search
Having examined several character distance met-
rics, and evaluated them over our three data sets,
we now consider  their  application  to  dictionary
word search.
4.1 Overall model
Our broad probability model for looking up words
based on similar kanji  is  identical  to the FOKS
model for search based on readings, save that we
substitute readings for kanji in our query. A uni-
gram approximation leads us to Equation 3 below,
where q = q
0
. . . qn is the query given by the user,
w = w
0
. . .wn is the desired word, and each qi and
wi is a kanji character:
Pr(w|q) ? Pr(w)Pr(q|w)
= Pr(w)
?
i
Pr(qi|w,q0 . . . qi?1)
? Pr(w)
?
i
Pr(qi|wi) (3)
The final line of Equation 3 requires two models
to be supplied. The first, Pr(w), is the probability
that a word will be looked up. Here we approxi-
mate using corpus frequency over the Nikkei news-
paper data, acknowledging that a newspaper cor-
pus is skewed differently to learner data. The sec-
ond model is our confusion model Pr(qi|wi), inter-
preted either as the probability of confusing kanji
wi with kanji qi, or of the user intentionally select-
ing qi to query for wi. It is this model that we now
focus on.
4.2 Confusion model
Although we can construct a confusion model us-
ing our distance metric alone, it is clear that fre-
quency effects will occur. For example, the like-
lihood of confusion is increased if the target wi is
rare and unknown, but qi is a highly-similar high-
frequency neighbour; certainly this is a typical use
case for intentional similarity-based querying. We
thus propose a generic confusion model based a
similarity measure between kanji:
Pr(qi|wi)?
Pr(qi)s(qi,wi)
?
j Pr(qi,j)s(qi,j ,wi)
(4)
The  confusion  model  uses  a  similarity  function
s(qi,wi) and a kanji frequency model Pr(qi) to de-
termine the relative probability  of  confusing wi
with qi amongst  other  candidates. We convert
the desired distance metric d into s according to
s(x,y) = 1? d(x,y) if the range of d is [0,1], or
s(x,y) = 1
1+d(x,y) if the range of d is [0,?).
To maximise the accessibility of this form of
search, we must find the appropriate trade-off be-
tween providing sufficient candidates and limiting
the noise. We use a thresholding method borrowed
from Clark and Curran (2004), where our thresh-
old is set as a proportion of the first candidate?s
score. For example, using 0.9 as our threshold, if
the first candidate has a similarity score of 0.7 with
the target kanji, we would then accept any neigh-
bours with a similarity greater than 0.63. Using
the d
stroke
metric with a ratio of 0.9, there are on
average 2.65 neighbours for each kanji in the j?y?
character set.
4.3 Evaluating search
Search by similar grapheme has an advantage to
search by word reading: reading results are natu-
rally ambiguous due to homophony in Japanese,
and attempts to perform error correction may in-
terfere with exact matches in the results ranking.
Grapheme-based search may have only one exact
match, so additional secondary candidates are not
in direct competition with existing search practices.
We can estimate the accessibility improvement
given by this form of search as follows. Let us
assume that learners study kanji in frequency or-
der. For each kanji learned, one or more high-
similarity neighbours also become accessible. Tak-
ing all pairings of kanji within the JIS X 0208-
1990 character set, using the d
stroke
metric with a
cutoff ratio of 0.9, and assuming full precision on
the neighbour graph this generates, we get the ac-
cessibility curve found in Figure 3. Our baseline
is a single kanji accessible for each kanji learned.
1047
01500
3000
4500
6000
50 400 800 12501750225027503250375042504750
Accessibility of similarity search
#
 
k
a
n
j
i
 
a
c
c
e
s
s
i
b
l
e
# kanji known
baseline accessible by search
Figure 3: The accessibility improvement of kanji similarity
search
Our actual precision makes the proportion of us-
able neighbours smaller; we will thus need to ex-
pose the user to a larger set of candidates to get this
level of improvement. Improvements in precision
and recall are still needed to reduce noise.
5 Discussion and future work
A current  difficulty  in  evaluating  this  form  of
search is the lack of available query data to objec-
tively evaluate the search before deployment. This
restricts evaluation to longer-term post-hoc analy-
sis based on query logs. Such logs will also provide
additional real-world similarity and confusion data
to improve our metrics.
This form of search is directly extensible to Chi-
nese, and is limited only by the availability of char-
acter data. Indeed, preliminary similarity models
for Chinese already exist (Liu and Lin, 2008). Our
similarity modelling may also suggest approaches
for more general symbol systems that lack ade-
quate indexing schemes, for example heraldry.
There is much potential in the adaption of dic-
tionaries as drill tutors in the context of language
learning (Zock and Quint, 2004). The models pre-
sented in this paper could provide dynamic kanji
drills, to aid early learners to distinguish similar
kanji and provide challenge more advanced learn-
ers.
6 Conclusion
We have proposed a method of searching the dictio-
nary for Japanese words containing unknown kanji,
based on their visual similarity to familiar kanji.
In order to achieve this, we have considered sev-
eral metrics over characters, improved on existing
baselines and evaluated further over a flashcard set.
Of these metrics, the edit distance taken over stroke
descriptions performed the best for high-similarity
cases, and was used to construct similarity-based
search at the word level.
References
[Apel and Quint2004] Apel, Ulrich and Julien Quint. 2004.
Building a graphetic dictionary for Japanese kanji ? char-
acter look up based on brush strokes or stroke groups, and
the display of kanji as path data. In Proc. COLING 2004,
Geneva, Switzerland.
[Bilac2002] Bilac, Slaven. 2002. Intelligent dictionary inter-
face for learners of Japanese. Master?s thesis, Tokyo Insti-
tute of Technology.
[Bille2005] Bille, Philip. 2005. A survey on tree edit dis-
tance and related problems. Theoretical Computer Sci-
ence, 337(1-3):217?239.
[Clark and Curran2004] Clark, Stephen and James R. Curran.
2004. The importance of supertagging for wide-coverage
CCG parsing. In Proc. COLING 2004, page 282?288,
Geneva, Switzerland.
[Ferret and Zock2006] Ferret, Olivier  and  Michael  Zock.
2006. Enhancing electronic dictionaries with an index
based on associations. In Proc. COLING/ACL 2006, pages
281?288, Sydney, Australia.
[Halpern1999] Halpern, Jack, editor. 1999. The  Kodan-
sha Kanji Learner?s Dictionary. Kodansha International,
Tokyo.
[Liu and Lin2008] Liu, Chao-Lin and Jen-Hsiang Lin. 2008.
Using structural information for identifying similar Chi-
nese characters. In Proc. ACL 2008: HLT, Short Papers
(Companion Volume), pages 93?96, Columbus, Ohio.
[Tanaka-Ishii and Godon2006] Tanaka-Ishii, Kumiko and Ju-
lian Godon. 2006. Kansuke: A kanji look-up system based
on a few stroke prototype. In Proc. ICCPOL 2006, Singa-
pore.
[Winstead2006] Winstead, Chris. 2006. Electronic kanji dic-
tionary based on ?Dasher?. Proc. IEEE SMCals 2006 ,
pages 144?148, Logan, USA.
[Yeh and Li2002] Yeh, Su-Ling  and  Jing-Ling  Li. 2002.
Role  of  structure  and  component  in  judgements  of  vi-
sual similarity of Chinese characters. Journal of Experi-
mental Psychology: Human Perception and Performance,
28(4):933?947.
[Yencken and Baldwin2006] Yencken, Lars  and  Timothy
Baldwin. 2006. Modelling  the  orthographic  neigh-
bourhood for  Japanese kanji. In Proc.  ICCPOL 2006,
Singapore.
[Yencken et al2007] Yencken, Lars, Zhihui Jin, and Kumiko
Tanaka-Ishii. 2007. Pinyomi - dictionary lookup via ortho-
graphic associations. In Proc. PACLING 2007, Melbourne,
Australia.
[Zock and Bilac2004] Zock, Michael and Slaven Bilac. 2004.
Word lookup on the basis of associations: from an idea to
a roadmap. In Proc. COLING 2004, pages 89?95, Geneva,
Switzerland.
[Zock and Quint2004] Zock, Michael and Julien Quint. 2004.
Why have them work for peanuts, when it is so easy to pro-
vide reward? Motivations for converting a dictionary into
a drill tutor. In Proc. PAPILLON 2004, Grenoble, France.
[Zock2002] Zock, Michael. 2002. Sorry, what was your name
again, or how to overcome the tip-of-the tongue problem
with the help of a computer? In Proc. COLING 2002,
pages 1?6, Taipei, Taiwan.
1048
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 266?270,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Relation Guided Bootstrapping of Semantic Lexicons
Tara McIntosh? Lars Yencken? James R. Curran? Timothy Baldwin?
? NICTA, Victoria Research Lab ? School of Information Technologies
Dept. of Computer Science and Software Engineering The University of Sydney
The University of Melbourne
nlp@taramcintosh.org james@it.usyd.edu.au
lars@yencken.org tb@ldwin.net
Abstract
State-of-the-art bootstrapping systems rely on
expert-crafted semantic constraints such as
negative categories to reduce semantic drift.
Unfortunately, their use introduces a substan-
tial amount of supervised knowledge. We
present the Relation Guided Bootstrapping
(RGB) algorithm, which simultaneously ex-
tracts lexicons and open relationships to guide
lexicon growth and reduce semantic drift.
This removes the necessity for manually craft-
ing category and relationship constraints, and
manually generating negative categories.
1 Introduction
Many approaches to extracting semantic lexicons
extend the unsupervised bootstrapping framework
(Riloff and Shepherd, 1997). These use a small set
of seed examples from the target lexicon to identify
contextual patterns which are then used to extract
new lexicon items (Riloff and Jones, 1999).
Bootstrappers are prone to semantic drift, caused
by selection of poor candidate terms or patterns
(Curran et al, 2007), which can be reduced by
semantically constraining the candidates. Multi-
category bootstrappers, such as NOMEN (Yangar-
ber et al, 2002) and WMEB (McIntosh and Curran,
2008), reduce semantic drift by extracting multiple
categories simultaneously in competition.
The inclusion of manually-crafted negative cate-
gories to multi-category bootstrappers achieves the
best results, by clarifying the boundaries between
categories (Yangarber et al, 2002). For exam-
ple, female names are often bootstrapped with
the negative categories flowers (e.g. Rose, Iris)
and gem stones (e.g. Ruby, Pearl) (Curran et al,
2007). Unfortunately, negative categories are dif-
ficult to design, introducing a substantial amount
of human expertise into an otherwise unsupervised
framework. McIntosh (2010) made some progress
towards automatically learning useful negative cate-
gories during bootstrapping.
In this work we identify an unsupervised source
of semantic constraints inspired by the Coupled Pat-
tern Learner (CPL, Carlson et al (2010)). In CPL,
relation bootstrapping is coupled with lexicon boot-
strapping in order to control semantic drift in the
target relation?s arguments. Semantic constraints
on categories and relations are manually crafted in
CPL. For example, a candidate of the relation IS-
CEOOF will only be extracted if its arguments can
be extracted into the ceo and company lexicons
and a ceo is constrained to not be a celebrity
or politician. Negative examples such as IS-
CEOOF(Sergey Brin, Google) are also introduced to
clarify boundary conditions. CPL employs a large
number of these manually-crafted constraints to im-
prove precision at the expense of recall (only 18 IS-
CEOOF instances were extracted). In our approach,
we exploit open relation bootstrapping to minimise
semantic drift, without any manual seeding of rela-
tions or pre-defined category lexicon combinations.
Orthogonal to these seeded and constraint-based
methods is the relation-independent Open Informa-
tion Extraction (OPENIE) paradigm. OPENIE sys-
tems, such as TEXTRUNNER (Banko et al, 2007),
define neither lexicon categories nor predefined re-
lationships. They extract relation tuples by exploit-
266
ing broad syntactic patterns that are likely to indi-
cate relations. This enables the extraction of inter-
esting and unanticipated relations from text. How-
ever these patterns are often too broad, resulting in
the extraction of tuples that do not represent rela-
tions at all. As a result, heavy (supervised) post-
processing or use of supervised information is nec-
essary. For example, Christensen et al (2010) im-
prove TEXTRUNNER precision by using deep pars-
ing information via semantic role labelling.
2 Relation Guided Bootstrapping
Rather than relying on manually-crafted category
and relation constraints, Relation Guided Bootstrap-
ping (RGB) automatically detects, seeds and boot-
straps open relations between the target categories.
These relations anchor categories together, e.g. IS-
CEOOF and ISFOUNDEROF anchor person and
company, preventing them from drifting into other
categories. Relations can also identify new terms.
We demonstrate that this relation guidance effec-
tively reduces semantic drift, with performance ap-
proaching manually-crafted constraints.
RGB can be applied to any multi-category boot-
strapper, and in these experiments we use WMEB
(McIntosh and Curran, 2008), as shown in Figure 1.
RGB alternates between two phases of WMEB, one
for terms and the other for relations, with a one-off
relation discovery phase in between.
Term Extraction
The first stage of RGB follows the term extraction
process of WMEB. Each category is initialised by a
set of hand-picked seed terms. In each iteration, a
category?s terms are used to identify candidate pat-
terns that can match the terms in the text. Seman-
tic drift is reduced by forcing the categories to be
mutually exclusive (i.e. patterns must be nominated
by only one category). The remaining patterns are
ranked according to reliability and relevance, and
the top-n patterns are then added to the pattern set.1
The reliability of a pattern for a given category is
the number of extracted terms in the category?s lex-
icon that match the pattern. A pattern?s relevance
weight is defined as the sum of the ?2 values be-
tween the pattern (p) and each of the lexicon terms
1In this work, n is set to 5.
WMEB
WMEB
lexicon
Person
get patterns
get terms
lexicon
Company
get patterns
get terms
relation
get patterns
get tuples
? ?
? ?
arg ? 
arg ? 
relation 
discovery
Lee Scott, Walmart
Sergey Brin, Google
Joe Bloggs, Walmart
T
e
r
m
 
e
x
t
r
a
c
t
i
o
n
R
e
l
a
t
i
o
n
 
e
x
t
r
a
c
t
i
o
n
Figure 1: Relation Guided Bootstrapping framework
(t): weight(p) =
?
t?T ?
2(p, t). These metrics are
symmetrical for both candidate terms and pattern.
In WMEB?s term selection phase, a category?s pat-
tern set is used to identify candidate terms. Like the
candidate patterns, terms matching multiple cate-
gories are excluded. The remaining terms are ranked
and the top-n terms are added to the lexicon.
Relation Discovery
In CPL (Carlson et al, 2010), a relation is instanti-
ated with manually-crafted seed tuples and patterns.
In RGB, the relations and their seeds are automati-
cally identified in relation discovery. Relation dis-
covery is only performed once after the first 20 iter-
ations of term extraction, which ensures the lexicons
have adequate coverage to form potential relations.
Each ordered pair of categories (C1, C2) = R1,2
is checked for open (not pre-defined) relations be-
tween C1 and C2. This check removes all pairs of
terms, tuples (t1, t2) ? C1 ? C2 with freq(t1, t2) <
5 and a cooccurrence score ?2(t1, t2) ? 0.2 If R1,2
has fewer than 10 remaining tuples, it is discarded.
The tuples for R1,2 are then used to find its ini-
tial set of relation patterns. Each pattern must match
more than one tuple and must be mutually exclusive
between the relations. If fewer than n relation pat-
terns are found forR1,2, it is discarded. At this stage
2This cut-off is used as the ?2 statistic is sensitive to low
frequencies.
267
TYPE 5gm 5gm + 4gm 5gm + DC
Terms 1 347 002
Patterns 4 090 412
Tuples 2 114 243 3 470 206 14 369 673
Relation Patterns 5 523 473 10 317 703 31 867 250
Table 1: Statistics of three filtered MEDLINE datasets
we have identified the open relations that link cate-
gories together and their initial extraction patterns.
Using the initial relation patterns, the top-n mu-
tually exclusive seed tuples are identified for the re-
lation R1,2. In CPL, these tuple seeds are manually
crafted. Note that R1,2 can represent multiple rela-
tions betweenC1 andC2, which may not apply to all
of the seeds, e.g. isCeoOf and isEmployedBy.
We discover two types of relations, inter-category
relations where C1 6= C2, and intra-category rela-
tions where C1 = C2.
Relation Extraction
The relation extraction phase involves running
WMEB over tuples rather than terms. If multiple re-
lations are found, e.g. R1,2 and R2,3, these are boot-
strapped simultaneously, competing with each other
for tuples and relation patterns. Mutual exclusion
constraints between the relations are also forced.
In each iteration, a relation?s set of tuples is used
to identify candidate relation patterns, as for term
extraction. The top-n non-overlapping patterns are
extracted for each relation, and are used to identify
the top-n candidate tuples. The tuples are scored
similarly to the relation patterns, and any tuple iden-
tified by multiple relations is excluded.
For tuple extraction, a relation R1,2 is constrained
to only consider candidates where either t1 or t2
has previously been extracted into C1 or C2, respec-
tively. To extract a candidate tuple with an unknown
term, the term must also be a valid candidate of its
associated category. That is, the term must match
at least one pattern assigned to the category and not
match patterns assigned to another category.
This type-checking anchors relations to the cat-
egories they link together, limiting their drift into
other relations. It also provides guided term growth
in the categories they link. The growth is ?guided?
because the relations define, semantically coher-
ent subregions of the category search spaces. For
example, ISCEOOF defines the subregion ceo
CAT DESCRIPTION
ANTI Antibodies: MAb IgG IgM rituximab infliximab
CELL Cells: RBC HUVEC BAEC VSMC SMC
CLNE Cell lines: PC12 CHO HeLa Jurkat COS
DISE Diseases: asthma hepatitis tuberculosis HIV malaria
DRUG Drugs: acetylcholine carbachol heparin penicillin
tetracyclin
FUNC Molecular functions and processes:
kinase ligase acetyltransferase helicase binding
MUTN Mutations: Leiden C677T C282Y 35delG null
PROT Proteins and genes: p53 actin collagen albumin IL-6
SIGN Signs and symptoms: anemia cough fever
hypertension hyperglycemia
TUMR Tumors: lymphoma sarcoma melanoma
neuroblastoma osteosarcoma
Table 2: The MEDLINE semantic categories
within person. This guidance reduces semantic
drift.
3 Experimental Setup
To compare the effectiveness of RGB we consider
the task of extracting biomedical semantic lexi-
cons, building on the work of McIntosh and Curran
(2008). Note however the method is equally appli-
cable to any corpus and set of semantic categories.
The corpus consists of approximately 18.5 mil-
lion MEDLINE abstracts (up to Nov 2009). The text
was tokenised and POS-tagged using bio-specific
NLP tools (Grover et al, 2006), and parsed using
the biomedical C&C CCG parser (Rimell and Clark,
2009; Clark and Curran, 2007).
The term extraction data is formed from the raw
5-grams (t1, t2, t3, t4, t5), where the set of candi-
date terms correspond to the middle tokens (t3) and
the patterns are formed from the surrounding tokens
(t1, t2, t4, t5). The relation extraction data is also
formed from the 5-grams. The candidate tuples cor-
respond to the tokens (t1, t5) and the patterns are
formed from the intervening tokens (t2, t3, t4).
The second relation dataset (5gm + 4gm), also in-
cludes length 2 patterns formed from 4-grams. The
final relation dataset (5gm + DC) includes depen-
dency chains up to length 5 as the patterns between
terms (Greenwood et al, 2005). These chains are
formed using the Stanford dependencies generated
by the Rimell and Clark (2009) parser. All candi-
dates occurring less than 10 times were filtered. The
sizes of the resulting datasets are shown in Table 1.
268
1-500 501-1000 1-1000
WMEB 76.1 56.4 66.3
+negative 86.9 68.7 77.8
intra-RGB 75.7 62.7 69.2
+negative 87.4 72.4 79.9
inter-RGB 80.5 69.9 75.1
+negative 87.7 76.4 82.0
mixed-RGB 74.7 69.9 72.3
+negative 87.9 73.5 80.7
Table 3: Performance comparison of WMEB and RGB
We follow McIntosh and Curran (2009) in us-
ing the 10 biomedical semantic categories and
their hand-picked seeds in Table 2, and manu-
ally crafted negative categories: amino acid,
animal, body part and organism. Our eval-
uation process involved manually judging each ex-
tracted term and we calculate the average precision
of the top-1000 terms over the 10 target categories.
We do not calculate recall, due to the open-ended
nature of the categories.
4 Results and Discussion
Table 3 compares the performance of WMEB and
RGB, with and without the negative categories. For
RGB, we compare intra-, inter- and mixed relation
types, and use the 5gm format of tuples and relation
patterns. In WMEB, drift dominates in the later iter-
ations with ?19% precision drop between the first
and last 500 terms. The manually-crafted negative
categories give a substantial boost in precision on
both the first and last 500 terms (+11.5% overall).
Over the top 1000 terms, RGB significantly out-
performs the corresponding WMEB with and with-
out negative categories (p < 0.05).3 In particu-
lar, inter-RGB significantly improves upon WMEB
with no negative categories (501-1000: +13.5%,
1-1000: +8.8%). In similar experiments, NEG-
FINDER, used during bootstrapping, was shown to
increase precision by ?5% (McIntosh, 2010). Inter-
RGB without negatives approaches the precision of
WMEB with the negatives, trailing only by 2.7%
overall. This demonstrates that RGB effectively re-
duces the reliance on manually-crafted negative cat-
egories for lexicon bootstrapping.
The use of intra-category relations was far less
3Significance was tested using intensive randomisation tests.
INTER-RGB 1-500 501-1000 1-1000
5gm 80.5 69.9 75.1
+negative 87.7 76.4 82.0
5gm + 4gm 79.6 71.5 75.5
+negative 87.7 76.1 81.9
5gm + DC 77.2 70.1 73.5
+negative 86.6 80.2 83.5
Table 4: Comparison of different relation pattern types
effective than inter-category relations, and the com-
bination of intra- and inter- was less effective than
just using inter-category relations. In intra-RGB the
categories are more susceptible to single-category
drift. The additional constraints provided by anchor-
ing two categories appear to make inter-RGB less
susceptible to drift. Many intra-category relations
represent listings commonly identified by conjunc-
tions. However, these patterns are identified by mul-
tiple intra-category relations and are excluded.
Through manual inspection of inter-RGB?s tuples
and patterns, we identified numerous meaningful re-
lations, such as isExpressedIn(prot, cell).
Relations like this helped to reduce semantic drift
within the CELL lexicon by up to 23%.
Table 4 compares the effect of different relation
pattern representations on the performance of inter-
RGB. The 5gm+4gm data, which doubles the num-
ber of possible candidate relation patterns, performs
similarly to the 5gm representation. Adding depen-
dency chains decreased and increased precision de-
pending on whether negative categories were used.
In Wu and Weld (2010), the performance of an
OPENIE system was significantly improved by us-
ing patterns formed from dependency parses. How-
ever in our DC experiments, the earlier bootstrap-
ping iterations were less precise than the simple
5gm+4gm and 5gm representations. Since the
chains can be as short as two dependencies, some
of these patterns may not be specific enough. These
results demonstrate that useful open relations can be
represented using only n-grams.
5 Conclusion
In this paper, we have proposed Relation Guided
Bootstrapping (RGB), an unsupervised approach to
discovering and seeding open relations to constrain
semantic lexicon bootstrapping.
269
Previous work used manually-crafted lexical and
relation constraints to improve relation extraction
(Carlson et al, 2010). We turn this idea on its head,
by using open relation extraction to provide con-
straints for lexicon bootstrapping, and automatically
discover the open relations and their seeds from the
expanding bootstrapped lexicons.
RGB effectively reduces semantic drift delivering
performance comparable to state-of-the-art systems
that rely on manually-crafted negative constraints.
Acknowledgements
We would like to thank Dr Cassie Thornley, our sec-
ond evaluator, and the reviewers for their helpful
feedback. NICTA is funded by the Australian Gov-
ernment as represented by the Department of Broad-
band, Communications and the Digital Economy
and the Australian Research Council through the
ICT Centre of Excellence program. This work has
been supported by the Australian Research Council
under Discovery Project DP1097291 and the Capital
Markets Cooperative Research Centre.
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676, Hyderabad, India.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining,
pages 101?110, New York, USA.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2010. Semantic role labeling for
open information extraction. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, pages 52?60, Los Angeles, California, USA, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 172?180, Melbourne, Australia.
Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk
Harkema, and Angus Roberts. 2005. Automatically
acquiring a linguistically motivated genic interaction
extraction system. In Proceedings of the 4th Learn-
ing Language in Logic Workshop, pages 46?52, Bonn,
Germany.
Claire Grover, Michael Matthews, and Richard Tobin.
2006. Tools to address the interdependence between
tokenisation and standoff annotation. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 19?26, Trento, Italy.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop, pages 97?105, Hobart, Australia.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional similar-
ity. In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, pages 396?404, Suntec, Singapore, Au-
gust.
Tara McIntosh. 2010. Unsupervised discovery of neg-
ative categories in lexicon bootstrapping. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 356?365,
Boston, USA.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of the 16th National Conference
on Artificial Intelligence and the 11th Innovative Ap-
plications of Artificial Intelligence Conference, pages
474?479, Orlando, USA.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?124,
Providence, USA.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, pages 852?865.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the 48th
Annual Meeting of the Association of Computational
Linguistics, pages 118?127, Uppsala, Sweden.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names. In
Proceedings of the 19th International Conference on
Computational Linguistics, pages 1135?1141, Taipei,
Taiwan.
270
