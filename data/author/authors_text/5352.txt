An Annotation System for Enhancing Quality of Natural Language
Processing
Hideo Watanabe*, Katashi Nagao**, Michael C. McCord*** and Arendse Bernth***
* IBM Research,
Tokyo Research Laboratory
1623-14 Shimotsuruma, Yamato,
Kanagawa 242-8502, Japan
hiwat@jp.ibm.com
** Dept. of Information Engineering
Nagoya University
Furo-cho, Chikusa-ku,
Nagoya 464-8603, Japan
nagao@nuie.nagoya-u.ac.jp
*** IBM T. J. Watson
Research Center
Route 134, Yorktown Heights,
NY 10598, USA
mcmccord@us.ibm.com,
arendse@us.ibm.com
Abstract
Natural language processing (NLP) programs are
confronted with various diculties in processing
HTML and XML documents, and have the po-
tential to produce better results if linguistic infor-
mation is annotated in the source texts. We have
therefore developed the Linguistic Annotation Lan-
guage (or LAL), which is an XML-compliant tag set
for assisting natural language processing programs,
and NLP tools such as parsers and machine trans-
lation programs which can accept LAL-annotated
input. In addition, we have developed a LAL-
annotation editor which allows users to annotate
documents graphically without seeing tags. Fur-
ther, we have conducted an experiment to check
the translation quality improvement by using LAL
annotation.
1 Introduction
Recently there has been increasing interest in
applying natural language processing (NLP) sys-
tems, such as keyword extraction, automatic text
summarization, and machine translation, to Inter-
net documents. However, there are various ob-
stacles that make it dicult for them to produce
good results. It is true that NLP technologies are
not perfect, but some of the diculties result from
problems in HTML. Further, in general, if linguis-
tic information is added to source texts, it greatly
helps NLP programs to produce better results. In
what follows, we would like to show some examples
related to machine translation.
In general, it is very helpful for machine transla-
tion programs to know boundaries on many levels
(such as sentence, phrases, and words) and to know
word-to-word dependency relations. For instance,
in the following example, since \St." has two possi-
ble meanings, \street" and \saint," it is dicult to
determine whether the following example consists
of one or two sentences.
I went to Newark St. Paul lived there
two years ago.
As another example, the following sentence has
two interpretations; one interpretation is that what
he likes is people and the other interpretation is
that what he likes is accommodating.
He likes accommodating people.
If there are tags indicating the direct-object mod-
ier of the word \like," then the correct interpreta-
tion is possible. NLP may be able to resolve these
ambiguities eventually by using advanced context
processing techniques, but current NLP technology
generally needs a hint from the author for these
sorts of ambiguities.
Further, there are issues in HTML/XML. When
MT systems are applied to Web pages, most of the
errors are generated by the linguistic incomplete-
ness of MT technology, but some are generated by
problems in HTML and XML tag usage. For in-
stance, writers often use <br> tag to sentence ter-
mination. Sometimes writers intend that a <br>
tag should terminate the sentence (even without
terminating punctuation such as a period), and in
other cases writers intend <br> only as a format-
ting device. In the HTML <table> shown in Figure
1, the writer intends each line of a cell to express
one linguistic unit. The MT program cannot tell
whether each line is a unit for translation, or, in-
stead, the two lines form one unit. In this example,
some MT programs would try to produce a transla-
tion of a unit \NetVista Models ThinkPad News."
As shown in the above examples, NLP appli-
cations do not achieve their full potential, on ac-
count of problems unrelated to the essential NLP
processes. If tags expressing linguistic information
<table><tr><td>
<a href="...">NetVista Models</a><br>
<a href="...">ThinkPad News</a><br>
</td></tr></table>
Figure 1: An example of using hbri tags in a table
are inserted into source documents, they help NLP
programs recognize document and linguistic struc-
tures properly, allowing the programs to produce
much better results. At the same time, it is true
that NLP technologies are incomplete, but their de-
ciencies can sometimes be circumvented through
the use of such tags. Therefore, this paper proposes
a set of tags for helping NLP programs, called Lin-
guistic Annotation Language (or LAL).
2 Linguistic Annotation Language
LAL is an XML-compliant tag set and its XML
namespace prex is lal.
The LAL tag set is designed to be as simple as
possible for the following reasons: (1) A simple tag
set is easier for developers to check manually. (2)
An easy-to-use annotation tool is mandatory for
this annotation scheme. Simplicity is important
for making an easy-to-use annotation tool, since if
we use a feature-rich tag set, the user must check
many annotation items.
2.1 Basic Tags
The sentence tag s is used to delimit a sentence.
<lal:s>This is the first sentence.</lal:s>
<lal:s>This is the second sentence.</lal:s>
The attribute type="hdr" means that the sen-
tence is a title or header.
The word tag w is used to delimit a word. It
can have attributes for additional information such
as base form (lex), part-of-speech (pos), features
(ftrs), and sense (sense) of a word. The values of
these attributes are language-dependent, and are
not described in this paper because of space limi-
tations. The following example illustrates some of
these tags and attributes.
<lal:s>
<lal:w lex="this" pos="det">This</lal:w>
<lal:w lex="be" pos="verb" ftr="sg,3rd">
is</lal:w>
<lal:w lex="a" pos="det">a</lal:w>
<lal:w lex="pen" pos="noun" ftr="sg,count">
pen</lal:w>
</lal:s>
The dependency (or word-to-word modication)
relationship can be expressed by using the id and
mod attributes of a word tag; that is, a word can
have the ID value of its modiee in a mod attribute.
The ID value of a mod attribute must be an ID value
of a word tag or a segment tag. For instance, the
following example contains attributes showing that
the word \with" modies the word \saw," meaning
that \she" has a telescope.
She <lal:w id="w1" lex="see" pos="v"
sense="see1">saw</lal:w> a man
<lal:w mod="w1">with</lal:w>
a telescope.
The phrase (or segment) tag seg is used to spec-
ify a phrase scope on any level. In addition, you
can specify the syntactic category for a phrase by
using an optional attribute cat. The following ex-
ample species the scope of a noun phrase \a man
... a telescope," and it is a noun phrase. This also
implies that the prepositional phrase \with a tele-
scope" modies the noun phrase \a man."
She saw <lal:seg cat="np">a man with a
telescope</lal:seg>.
The attribute para="yes" means that the seg-
ment is a coordinated segment. The following ex-
ample shows that the word \software" and the word
\hardware" are coordinated.
This company deals with <lal:seg cat="np"
para="yes">software and hardware</lal:seg>
for networking.
The ref attribute has the ID value of the refer-
ent of the current word. This can be used to specify
a pronoun referent, for instance:
<lal:s>He bought <lal:seg id="w1">a
new car</lal:seg> yesterday.</lal:s>
<lal:s>She was very surprised to
learn that <lal:w ref="w1">it</lal:w>
was very expensive.</lal:s>
2.2 Expressing Multiple Parses
As mentioned earlier, since natural language con-
tains ambiguities, it is useful for LAL annotation
to have a mechanism for expressing syntactic am-
biguities.
We have introduced a parse identier (or PID)
in attribute values for distinguishing parses. An
attribute value which may be changed according
to parses can be allowed to be expressed as space-
separated multiple values, each of which consists of
a PID prex followed by a colon and an attribute
value.
<lal:s>
<lal:w id="1" mod="2">He</lal:w>
<lal:w id="2" mod="0">likes</lal:w>
<lal:w id="3" mod="p1:2 p2:4">
accommodating</lal:w>
<lal:w id="4" mod="p1:3 p2:2">people
</lal:w>.</lal:s>
This example shows that there are two interpre-
tations whose PIDs are p1 and p2, and that the p1
interpretation is \He likes people" and p2 is \He
likes accommodating."
3 LAL-Aware NLP Programs
We have modied certain NLP systems to be
LAL-aware. ESG [5, 6] is an English parsing sys-
tem developed by the IBM Watson Research Cen-
ter, and updated to accept and generate LAL-annotated
English. We have also developed a Japanese pars-
ing system with LAL output functionality. These
LAL-aware versions of parsers are used as a back-
end process to show users the system's default in-
terpretation for a given sentence in the LAL-annotation
editor described below.
Further, the English to German, French, Span-
ish, Italian and Portuguese translation engines [6,
7] and English to Japanese translation engine [9]
are modied to accept LAL-annotated English HTML
input.
1
4 The LAL-Annotation Editor
Since inserting tags into documents manually is
not generally an easy task for end users, it is impor-
tant to provide an easy-to-use GUI-based editing
environment. In developing such an environment,
we took into consideration the following points: (1)
Users should not have to see any tags. (2) Users
should not have to see internal representations ex-
pressing linguistic information. (3) Users should be
able to view and modify linguistic information such
as feature values, but only if they want to.
Considering these points, we have found that
most of the errors made by NLP programs result
from their failure to recognize the phrasal struc-
tures of sentences. Therefore, we have decided to
1
In addition, Watanabe [11] reported on an algorithm
for accelerating CFG-parsing by using LAL tag informa-
tion, and it is implemented in the above English-to-Japanese
translation engine.
show only a structural view of a sentence in the ini-
tial screen; other information is shown only if the
user requests it.
The important issue here is how to represent the
syntactic structure of a sentence to the user. NLP
programs normally deal with a linguistic structure
by means of a syntactic tree, but such a structure
is not necessarily easy for end users to understand.
For instance, Figure 2 shows the dependency struc-
ture of the English sentence \IBM announced a new
computer system for children with voice function."
This dependency structure is dicult for end users,
partly because a dependency tree does not keep the
surface word order, so that it is dicult to map it
to the original sentence quickly.
2
Therefore, an im-
portant property for the linguistic structural view
is that users can easily reconstruct the original sur-
face sentence string.
The next important issue is how easily a user
can understand the overall linguistic structure. If
a user is, at rst, presented with detailed linguistic
structure at the word level, then it is dicult to
grasp the important linguistic skeleton of a sen-
tence. Therefore, another necessary property is
to give users a view in which the overall sentence
structure is easily recognized.
Figure 2: An example of tree structure of an En-
glish sentence
With these requirements in mind, we have devel-
oped a GUI tool called the LAL Editor. To satisfy
the last requirement, this editor has two presenta-
tion modes: the reduced presentation view and the
expanded presentation view. In the reduced pre-
sentation view, a main verb and its modiers are
basic units for presenting dependencies, and they
are located on dierent lines, keeping the surface
order. Figure 3 shows an example of this reduced
presentation view. In this view, since dependen-
cies that are obvious for native speakers (e.g. \a"
and \computer" ) are not displayed explicitly, the
user can concentrate on dependencies between key
2
You must perform an inorder tree walk to reconstruct a
surface sentence string.
Figure 3: Screen Images of LAL Editor - Reduced
View
units (or phrases). If the user nds any depen-
dency errors in the reduced view, he or she can
enter the expanded view mode in which all words
are basic units for presenting dependencies. Fig-
ure 4 (a, b) shows examples of this expanded view.
In these views, to satisfy the former requirement,
dependencies between basic units are expressed by
using indentation. Therefore you can easily recon-
struct the surface sentence string by just looking at
words from top to bottom and from left to right,
and easily know dependencies of words by looking
at words located in the same column. For details
of the algorithm, see [12].
In Figure 3, you can easily grasp the overall
structure. In this case, since the dependencies be-
tween \for" and \announced," and \with" and \an-
nounced" are wrong, the user can change the mode
to the expanded view (as shown in Figure 4 (a)).
In this view, the user can change dependencies by
dragging a modier to the correct modiee using
a mouse. The corrected dependency structure is
shown in Figure 4 (b).
In addition, the LAL Editor has the capability of
testing translation by using LAL annotation. Fig-
ure 5 shows a window in which the top pane shows
the input sentence, the second pane shows the LAL-
annotation of the input, the third pane shows the
translation result using the LAL annotation, and
the fourth pane shows the default translation with-
out using the LAL annotation. The user can easily
check whether the current annotation can improve
translations.
5 Experiment
We have conducted a small experiment for eval-
uating LAL annotation to our English-to-Japanese
machine translation system[9]. We gathered about
60 sentences from Web pages in the computer do-
main, and added LAL annotation to these sen-
(a) Expanded View (before correction)
(b) Expanded View (after correction)
Figure 4: Screen Images of LAL Editor - Expanded
View
tences with the LAL annotation editor. In this
experiment, only word-to-word modications were
corrected. Due to severe parsing errors and glitches
of the annotation editor, 53 of the 60 sentences
were used in this experiment. The average sentence
length for this test set was 21 words. Two evalu-
ators assigned a quality evaluation ranging from 1
(worst) to 5 (best) for each translation, with and
without use of annotation.
Translation results for 18 sentences (about 34%)
were better for the annotated case than the non-
annotated case. These better sentences were 1.16
Figure 5: Translation test window of LAL Editor
points better (27% better in quality score). On
the other hand, 26 sentences (about 49%) were not
changed, and 9 sentences (about 17%) were worse.
The main reason why these 9 sentences were worse
was the structural mismatch between the output
of the LAL Editor and the expected structure of
EtoJ translation system, since the LAL Editor and
the EtoJ MT system use dierent parsing systems.
We have developed a structure conversion routine
from LAL editor output to EtoJ input, but it does
not yet cover all situations. This is the reason why
these 9 sentences become worse.
Note that this experiment only uses word-to-
word modication corrections, so there is room for
producing better translations if we use other types
of annotation such as part-of-speech, and word sense.
6 Discussion
There have been several eorts to dene tags
for describing language resources, such as TEI [10],
OpenTag [8], CES [1], EAGLES [2], GDA [3]. The
main focus of these eorts other than GDA has
been to share linguistic resources by expressing them
in a standard tag set, and therefore they dene very
detailed levels of tags for expressing linguistic de-
tails. GDA has almost the same purposes but it
has also dened a very complex tag set. This com-
plexity discourages people from using these tag sets
when writing documents, and it also becomes dif-
cult to make an annotation tool for these tags.
LAL is not opposed to these previous eorts, but
attempts to strike a useful balance between expres-
siveness and simplicity, so that annotation can be
used widely.
As mentioned in the discussion of the experi-
ment, there is an issue when the parsing system
of LAL editor and the parsing system of a NLP
tool which accepts the output of LAL editor are
dierent. As mentioned before, we used the ESG
parser for producing LAL-annotated English, and
Japanese-to-EnglishMT system for accepting LAL-
annotated English. Since these systems have been
independently developed based on dierent approaches
by dierent developers, we found there are some
structural dierences. For instance, given a prepo-
sitional phrase Prep N, ESG's head word of the
prepositional phrase is Prep, but EtoJ MT engine's
head is N. In most cases, we can make systematic
conversion routines for dierent structures. In fact,
for most of sentences whose translation is worse
when annotation is used, we can provide struc-
tural conversion routines for linguistic structures
included in them. The basic idea of LAL-awareness
for NLP tools is that an NLP tool uses LAL infor-
mation as much as possible, but if LAL information
produces a severe conict with the internal process-
ing, then such information should not be used. Our
EtoJ MT program was basically implemented this
way based on the algorithm described in [11], but
we seem to need more research on this issue.
7 Conclusion
In this paper, we have proposed an XML-compliant
tag set called Linguistic Annotation Language (or
LAL), which helps NLP programs perform their
tasks more correctly. LAL is designed to be as
simple as possible so that humans can use it with
minimal help from assisting tools. We have also de-
veloped a GUI-based LAL annotation editor, and
have shown in an experiment that use of LAL anno-
tation enhances translation quality. We hope that
wide acceptance of LAL will make it possible to use
more intelligent Internet tools and services.
References
[1] CES, \Corpus Encoding Standard (CES),"
(http://www.cs.vassar.edu/CES/)
[2] EAGLES, \Expert Advisory Group on Language Engi-
neering Standards,"
(http://www.ilc.pi.cnr.it/EAGLES/home.html)
[3] GDA, \Global Document Annotation,"
(http://www.etl.go.jp/etl/nl/gda/)
[4] Koichi Hashida, Katashi Nagao, et. al, \Progress
and Prospect of Global Document Annotation," (in
Japanese) Proc. of 4th Annual Meeting of the Asso-
ciation of Natural Language Processing, pp. 618{621,
1998
[5] McCord, M. C., \Slot Grammars," Computational Lin-
guistics, Vol. 6, pp. 31{43, 1980.
[6] McCord, M. C., \Slot Grammar: A System for Sim-
pler Construction of Practical Natural Language Gram-
mars," in (ed) R. Studer, Natural Language and Logic:
International Scientic Symposium, Lecture Notes in
Computer Science, pp. 118{145, Springer Verlag, 1990.
[7] McCord, M. C., and Bernth, A., \The LMT Transfor-
mational System," Proc. of Proceedings of AMTA-98,
pp. 344{355, 1998.
[8] OpenTag, \A Standard Extraction/Abstraction Text
Format for Translation and NLP Tools,"
(http://www.opentag.org/)
[9] Takeda, K., \Pattern-Based Machine Translation,"
Proc. of 16th COLING, Vol. 2, pp. 1155{1158, August
1996.
[10] TEI, \Text Encoding Initiative (TEI),"
(http://www.uic.edu:80/orgs/tei/)
[11] Watanabe, H., \A Method for Accelerating CFG-
Parsing by Using Dependency Information," Proc. of
18th COLING, 2000.
[12] Watanabe, H., Nagao, K., McCord, M. C., and Bernth,
A., \Improving Natural Language Processing by Lin-
guistic Document Annotation," Proc. of COLING 2000
Workshop for Semantic Annotation and Intelligent Con-
tent, pp. 20{27, 2000.
 
	ffProceedings of the 5th Workshop on Important Unresolved Matters, pages 81?88,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
An Arabic Slot Grammar Parser 
Michael C. McCord 
 
IBM T. J. Watson Research Center 
P.O.B. 704 
Hawthorne, NY 10532 
mcmccord@us.ibm.com 
Violetta Cavalli-Sforza 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213 
violetta@cs.cmu.edu 
 
 
Abstract 
We describe a Slot Grammar (SG) parser 
for Arabic, ASG, and new features of SG 
designed to accommodate Arabic as well as 
the European languages for which SGs 
have been built.  We focus on the integra-
tion of BAMA with ASG, and on a new, 
expressive SG grammar formalism, SGF, 
and illustrate how SGF is used to advan-
tage in ASG. 
1 Introduction 
In this paper we describe an initial version of a Slot 
Grammar parser, ASG, for Arabic.  Slot Grammar 
(SG) (McCord, 1980. 1993) is dependency-
oriented, and has the feature that deep structure 
(via logical predicate arguments) and surface struc-
ture are both shown in parse trees. 
    A new formalism SGF (Slot Grammar Formal-
ism) for SG syntax rules has been developed 
(McCord, 2006), and the ASG syntax rules are 
written in SGF.  SGF is largely declarative, and 
can be called ?object-oriented? in a sense we will 
explain.  The rules in SGF all have to do with slot 
filling.   
ASG uses BAMA (Buckwalter, 2002), in a ver-
sion from Qamus, as its morphological analyzer.  
All the internal processing of ASG is done with the 
Buckwalter Arabic transliteration ? though of 
course ASG can take real Arabic script (in UTF-8 
form) as input.  We use BAMA features in the 
processing (and parse trees), but augmented with 
other features more unique to ASG.  The Penn 
Arabic Treebank (ATB), which also uses BAMA 
features, has served as a development guide in the 
work.  But SG is a rule-based system, and there is 
no automatic training from the ATB. 
Prior to this work, SGs had been written for 
English (McCord), German (Claudia Gdaniec), and 
for the Romance languages (Esm?ralda Manandise) 
Spanish, French, Italian and Portuguese.  For han-
dling Arabic, there have been two main new adap-
tations of SG.   
One adaptation is in the treatment of features in 
the form that BAMA delivers.  This treatment in-
cludes a feature lexicon in ASG, which can specify 
two kinds of relations among features, which we 
will describe below.  We also take steps to handle 
the large number of analyses returned by BAMA.  
Special treatment of features appears as well in the 
SGF syntax rules.  The other main adaptation is in 
the treatment of clitics, where special things hap-
pen in Arabic for proclitics. 
Although the basic ideas of SG have not 
changed in treating Arabic, ASG has been serving 
as a test bed for the new syntax rule formalism 
SGF.   
Overall, the design of the SG system has be-
come neater by including Arabic as well as the 
European languages.  For instance, the new treat-
ment of features generalizes the existing treatment 
in the German SG.  And the new treatment of cli-
tics will make the treatment of clitics for the Ro-
mance languages neater. 
In Section 2, we discuss the ASG feature system.  
Section 3 briefly describes the ASG slot frame 
lexicon.  Sections 4 and 5 deal with syntactic 
analysis.  In Section 6, we discuss current perform-
ance of ASG  (coverage and speed), and in Section 
7, related work. 
81
2 The Feature System 
Features for an SG parser for language X are speci-
fied externally as character strings, listed by the 
grammar writer in the feature lexicon Xfeas.lx (Ar-
feas.lx for Arabic).  Internally, features are repre-
sented in two ways, for efficient processing:  (1) 
The features themselves are ?atoms?, represented 
by integers.  (2) The set of features for a parse 
node is represented by a bit string, where each fea-
ture atom is assigned a bit position.  For ASG, 
these bit strings are currently of length 512.  But 
these internal representations are invisible to the 
grammar writer. 
In the set of features for a node, some subsets 
can be viewed disjunctively.  For instance if a noun 
is ambiguously singular or plural, it might have 
both features sg and pl.  This situation occurs 
very much for Arabic text input because of the 
ambiguity due to unvocalized script.  In order not 
to choke the parse space, the  SG-BAMA interface 
combines some BAMA analyses, basically ones 
that have the same stem and POS, so that nodes 
have disjunctive BAMA features.  But agreement 
rules or slot filler constraints often reduce the 
ambiguity.  Such rules, specified in a  perspicuous 
way in SGF, as we will see below, are 
implemented internally by intersecting the bit 
string representations of relevant feature sets. 
For ASG, there are two categories of features. 
One category consists of BAMA compound 
features like  
 
NOUN+NSUFF_FEM_PL+CASE_DEF_ACC  
 
(indicating a feminine plural definite accusative 
noun).  Although such features are compound in 
intent, they are treated as atomic symbols by ASG 
(as are all features specified in Xfeas.lx).   
Features of the other category are more special 
to ASG.  Some of them have to do with syntactic 
structure (like presence of an overt subject), and 
others are morphological.  Typical morphological 
features are standard, simple ones that appear in 
sets of values for attributes like case, number, 
gender, and definiteness ? for example: 
 
nom, acc, gen 
sg, dual, pl 
m, f, 
def, indef 
   
Besides declaring features, Xfeas.lx can specify 
relations between features.  One way is to specify 
simple hierarchical relations.  An entry of the form      
   
x  <  y ? z ? 
 
specifies that feature x implies features y ?  z.  This 
means for instance that if the feature x is marked 
on a node, then a test in the grammar for feature y 
will succeed.  Hierarchical information like this is 
stored internally in bit string arrays and allows ef-
ficient processing.     
If an entry is of the form 
 
x  < ?  >   u ?  v 
 
then we say that x extends the feature set {u ... v}, 
and x is an extending feature.  The basic idea is that 
x is a kind of abbreviation for the disjunction of the 
set {u ... v}, but x may appear on a node independ-
ently of that set.  We will explain the exact mean-
ing in the section below on the syntax rules.  A 
typical example of an extending feature rule in Ar-
feas.lx is as follows: 
 
gen < > 
  NOUN+NSUFF_FEM_DU_GEN 
  NOUN+NSUFF_FEM_PL+CASE_DEF_GEN 
  NOUN+NSUFF_FEM_PL+CASE_INDEF_GEN    
    ...  
 
where we list all BAMA compound features that 
include a genitive subfeature.  Rules in the syntax 
component can test simply for extending features 
like gen, as we will see below.  The syntax com-
ponent does not even mention BAMA features.  
But this representational scheme allows us to keep 
BAMA compound features as units -- and this is 
important, because the morphological analysis 
(with ambiguities shown) requires such groupings.  
The internal representation of an extending feature 
relationship of x to {u ... v} associates with the 
atom for x the disjunction of the bit strings for u ... 
v, and the processing is quite efficient.   
Although the features in Xfeas.lx are generally 
morphosyntactic, and have internal atom and bit 
string position representations in limited storage 
areas, SG also allows open-ended features, which 
may be used in the SG lexicon and tested for in the 
syntax component.  These are typically semantic 
features. 
82
3 The SG Lexicon 
Although BAMA contains lexicons for doing 
Arabic morphological analysis, an SG needs its SG 
lexicon to drive syntactic analysis and help pro-
duce parse trees that show (deep) predicate argu-
ment structure.  The main ingredients associated 
with index words in an SG lexicon are sense 
frames.  A sense frame can specify a part of speech 
(POS), features (typically semantic features), a slot 
frame, and other ingredients.  The most important 
ingredient is the slot frame, which consists of an 
ordered list of (complement) slots.  Slots can be 
thought of as grammatical relations, but also as 
names for logical arguments for word sense predi-
cates.  An example from the ASG lexicon, called  
Ar.lx,  is: 
 
Eoniy < v (obj n fin) 
 
This says that Eoniy () is a verb (stem) with a 
direct object slot (obj) which can be filled by ei-
ther an NP (indicated by the n) or a finite VP (in-
dicated by the fin).  A slot can be either an atomic 
symbol or a list of the form 
 
(SlotName Option1 ? Optionn) 
 
where the options are terms that specify conditions 
on the fillers of the slot.  If no options are specified, 
then defaults are used.  The Eoniy () example 
shows no subject slot, but the default is that every 
verb has a subject slot (even though it may not be 
overtly filled).  One can specify a subject slot 
(subj) if it needs non-default options.   
For the index words for ASG, we are currently 
using vocalized stems ? stems as in the ATB, or as 
produced by BAMA.  To produce a starter for 
Ar.lx, we extracted stems from the ATB, listed by 
frequency, and associated default sense frames 
based on the BAMA features in the ATB.  Using 
vocalized stems entails some repetition of sense 
frames, since there can be more than one vocalized 
stem for a given word sense. 
Index words in the SG lexicon can also be mul-
tiwords.  Some multiword entries occur in Ar.lx. 
Morpholexical analysis for ASG combines 
BAMA analysis with look-up in Ar.lx.  BAMA 
provides morphological features (BAMA com-
pound features) associated with vocalized stems.  
Also, an algorithm in ASG separates clitics out of 
the BAMA analyses and represents them in a form 
convenient for the parser.  The vocalized stems are 
looked up in Ar.lx, and the sense frames found 
there (if look-up is successful) are merged with 
compatible analyses from BAMA.  If look-up in 
Ar.lx fails, then the BAMA analyses can still be 
used, with default slot frames assigned.  In the 
other direction, look-up in BAMA may fail, and 
special entries in Ar.lx can cover such words 
(specifying morphological features as well as slot 
frames). 
4 The Parsing Algorithm 
The SG parser is a bottom-up chart parser.  Ini-
tial chart elements are one-word (or one-multiword) 
phrases that arise from morpholexical analysis.  All 
further chart elements arise from binary combina-
tions of a modifier phrase M with a higher phrase 
H, where M fills a slot S in H.  The slot S could be 
a complement slot which is stored with H, having 
arisen from the lexical slot frame of the word sense 
head of H.  Or S could be an adjunct slot associated 
with the POS of M in the syntax rule component 
X.gram.  In both cases, the conditions for filling S 
are specified in X.gram.  The parser attaches post-
modifiers first, then premodifiers.     
   Normally, M and H will be existing adjacent 
phrases in the chart.  But there is an interesting 
treatment of clitics that is especially relevant for 
Arabic.  The SG data structure for a phrase P in-
cludes two fields for clitics associated with the 
head word of P ? a list of proclitics, and a list of 
enclitics. Each clitic is itself a (one-word) phrase 
data structure, ready to be used for slot filling.  So 
the parsing algorithm can combine not only adja-
cent phrases in the chart in the normal way, but can 
also combine a phrase with one of its clitics.  For 
Arabic, all enclitics (typically pronouns) for a 
phrase P are attached to P (by postmodification) 
before P enters into any other slot filling.  On the 
other side, proclitics (typically conjunctions and 
prepositions) of P are used only as higher phrases 
where P is the modifier.  But a proclitic can get 
?passed upwards? before it is treated as a higher 
phrase.  A non-deterministic option in the parser is 
that a phrase M becomes a premodifier of an adja-
cent phrase H in the chart, and the proclitic list of 
M is passed up to become the proclitic list of H.  
For instance a conjunction like ?w?/?wa? [ , ?and?] 
might be attached as a proclitic to the first word in 
83
a (premodifying) subject of a clause C, and the 
conjunction proclitic gets passed upwards until it 
finally takes C as a postconjunct modifier. 
    Although SG is a rule-based system, it does use 
a numerical scoring system for phrases during 
parsing.  Real numbers are attached to phrases, 
indicating, roughly, how likely it is that the phrase 
is a good analysis of what it spans.  Partial analy-
ses (phrases) can be pruned out of the chart if their 
scores are too bad.  Also, final parses get ranked by 
their scores.  Scores can arise from rules in the 
syntax component, in the lexicon, or in the shell.  
A general rule in the shell is that complement slots 
are preferred over adjunct slots.  The specific val-
ues of scores are normally determined by the 
grammar writer, with regression testing.     
5 The ASG Syntax Rule Component 
In an SG syntax rule component X.gram  
(Ar.gram for Arabic), the rules are written in the 
formalism SGF (McCord, 2006).  Each rule deals 
with slot filling, and is either a complement slot 
rule or an adjunct slot rule.  Each rule is of the 
form 
 
S  <  Body 
 
where S is the index, which is a complement slot 
for a complement slot rule, or a POS for an adjunct 
slot rule.  The Body is basically a logical expres-
sion (in a form we will describe) which is true iff 
the corresponding slot filling can succeed.  The 
rules can be viewed largely declaratively, even 
though there are some operators that look like 
commands.    
    The rule system is applied by the parsing algo-
rithm when it is looking at specific phrases M and 
H that are adjacent or have a clitic relationship, and 
asking whether M can fill a slot in H.  For a yet 
unfilled complement slot S of H, with a chosen slot 
option, the parser looks for the complement slot 
rule in X.gram indexed by S, and applies its body, 
requiring that to be true before doing the slot fill-
ing.  And the parser also looks at the POS of M, 
finds the corresponding adjunct slot rule indexed 
by that POS, and applies its body.  In this case, the 
body determines what the adjunct slot and option 
are; and it can do so non-deterministically:  The 
body may be a disjunction, with operator ||, of sev-
eral sub-bodies, which are all tried for insertion of 
the filled version of H into the chart.  Complement 
slot rules can also use the infix operator || for dis-
junctions of the body on the top level, but in this 
case the || behaves deterministically ? as in an if-
then-else. 
    A simple example of a complement slot rule is 
the following, for the object of a preposition: 
 
objprep < 
   ri 
   (opt n) 
   (mpos noun) 
   (extmf gen) 
   (removemf nom acc) 
   satisfied 
 
The body is a sequence of tests which are viewed 
conjunctively.  The first test, ri, means that the 
filler M is on the ? right?  of H (a postmodifier).  
The opt test checks that the slot option is n, re-
quiring an NP.  The next test requires that the filler 
M has POS noun.  In SGF rules, the letter m in 
operators indicates the filler M as an implicit oper-
and, and h indicates the higher phrase H.   
    The term (extmf gen) is an extending feature 
test on M for the feature gen (genitive).  This will 
succeed iff either gen is marked explicitly on M or 
M has at least one of the BAMA features associ-
ated with gen in the extending feature rule for gen 
in Arfeas.lx (see Section 2).  The test (removemf 
nom acc) always succeeds, and it will remove  
explicit occurrences of  nom or acc on M, as well 
as any BAMA features associated with those fea-
tures by extending feature rules. 
    Finally, the test satisfied succeeds iff M has 
no unfilled obligatory complement slots. 
    The syntax of the SGF formalism is Cambridge 
Polish (Lisplike), except for the uses of the binary 
operators < and ||.  There are quite a number of 
? built-in?  operators in SGF, and many of them can 
take any number of arguments.   
    Tests in SGF can be nested; some operators, in-
cluding all the logical operators, can contain other 
tests as arguments.  We mentioned that SGF is 
? object-oriented?  in a certain sense.  In any given 
test, however much embedded, there is always a 
phrase in focus, which is an implicit argument of 
the test.  The phrase in focus can be considered 
like this in object-oriented languages.  The de-
fault phrase in focus on top-level tests is M (the 
modifier).  But some operators can shift the focus 
84
to another phrase, and this can happen an unlimited 
number of times in nested tests.  For example, a 
test of the form 
 
       (rmod Test1 ... Testn)      
      
searches the postmodifiers of the current phrase in 
focus and succeeds iff, for one of them as a new 
phrase in focus, all of the test arguments are satis-
fied.  This scheme allows for quite compact ex-
pressions for searching and testing parse trees. 
    Now let us look at (a modified form of) an ad-
junct slot rule in Ar.gram, for adjectives that post-
modify nouns: 
 
adj < 
   ri 
   (hf noun) 
   (agreef nom acc gen) 
   (agreef def indef) 
   (if (& (exthf pl) (nhf h)) 
       /* then */ 
       (extmf sg f) 
       /* else */ 
       (& (agreef sg pl dual) 
          (agreef m f) ) ) 
   satisfied 
   (setslot nadj) 
   (setopt aj) 
 
    So the filler M should be an adjective phrase.  
The first two tests check that M postmodifies H, 
and H is a noun phrase.  The main operator here is  
agreef, which works with a list of extending fea-
tures.  The list of features should consist of the 
possible values of an attribute like case, number, 
gender, etc.  The agreef test will succeed iff M 
and H agree along this dimension.  For at least one 
of the argument features, both M and H should 
have this feature (as an extending feature).  Fur-
thermore, agreef takes care of reducing feature 
ambiguity in M and H (if it succeeds):  If x is an 
argument feature such that one of M and H has x 
(as an extending feature) but the other does not, 
then x is removed from the other (as an extending 
feature). 
    For the adj rule at hand, the if statement can 
be interpreted as follows:  If H (the noun) is plural 
and not human, then M (the adjective) must be sin-
gular and feminine; otherwise M and H must agree 
in number and gender.  The actual current rule in 
Ar.gram skips the agreement test for plural non-
human nouns, because we do not currently have 
enough marking of the human (h) features. 
    For subject-verb agreement, we have the situa-
tion that verbs do not use the same extending fea-
ture names as nouns do.  (This has to do with cor-
responding BAMA features.)  To handle this, 
agreef can take as arguments pairs of features, 
like (sg vsg), where the first element is checked 
for M (the subj noun), and the second is checked 
for H (the verb).  Here is a shortened form of the 
subject slot rule of ASG, which contains the cur-
rent subject-verb agreement rule for ASG: 
 
subj < 
  (opt n) 
  (mpos noun) 
  (if (mf pron) 
   /* then */ 
    (& (agreef (m vm) (f vf)) 
       (agreef (sg vsg)  
               (pl vpl)  
               (dual vdual)) 
       (agreef (pers1 vpers1) 
               (pers2 vpers2) 
               (pers3 vpers3)) ) 
    /* else */ 
    (& (exthf vpers3) 
      (if (| (^ (extmf pl)) (mf h)) 
         (& 
           (agreef (m vm) (f vf)) 
           (if le 
              /* subj before verb */ 
              (agreef (sg vsg)  
                      (pl vpl) 
                      (dual vdual)) 
              /*subj after verb: */ 
              (exthf vsg) ) ) ) ) 
) 
 
The agreement part is the outer if test, and can be 
interpreted as follows:   
  
1. If  M is a pronoun, then M agrees with H 
in gender, number and person; 
2. else H must be 3rd-person and if M is 
non-plural or human, then: 
a. M agrees with H  in gender and 
b. if M premodifies H then it 
agrees with H in number, 
c. else H is singular. 
 
This formulation shows the way we are currently 
ignoring agreement for plural non-human nouns, 
until we get human markings on nouns. 
85
    Now let us illustrate how an adjunct slot rule can 
overcome a seeming problem for dependency 
grammars when there is a ? missing head word?  for 
a phrase.  Consider n the sentence shown in Figure 
1, along with its ASG parse tree. 
 
 	


Abductive Reasoning with a Large Knowledge Base
for Discourse Processing
Ekaterina Ovchinnikova
University of Osnabru?ck
eovchinn@uos.de
Niloofar Montazeri
USC ISI
niloofar@isi.edu
Theodore Alexandrov
University of Bremen
theodore@uni-bremen.de
Jerry R. Hobbs
USC ISI
hobbs@isi.edu
Michael C. McCord
IBM Research
mcmccord@us.ibm.com
Rutu Mulkar-Mehta
USC ISI
me@rutumulkar.com
Abstract
This paper presents a discourse processing framework based on weighted abduction. We elabo-
rate on ideas described in Hobbs et al (1993) and implement the abductive inference procedure in a
system called Mini-TACITUS. Particular attention is paid to constructing a large and reliable knowl-
edge base for supporting inferences. For this purpose we exploit such lexical-semantic resources as
WordNet and FrameNet. We test the proposed procedure and the obtained knowledge base on the
Recognizing Textual Entailment task using the data sets from the RTE-2 challenge for evaluation. In
addition, we provide an evaluation of the semantic role labeling produced by the system taking the
Frame-Annotated Corpus for Textual Entailment as a gold standard.
1 Introduction
In this paper, we elaborate on a semantic processing framework based on a mode of inference called
abduction, or inference to the best explanation. In logics, abduction is a kind of inference which arrives
at an explanatory hypothesis given an observation. Hobbs et al (1993) describe how abductive reasoning
can be applied to the discourse processing problem viewing the process of interpreting sentences in
discourse as the process of providing the best explanation of why the sentence would be true. In this
framework, interpreting a sentence means 1) proving its logical form, 2) merging redundancies where
possible, and 3) making assumptions where necessary. As the reader will see later in this paper, abductive
reasoning as a discourse processing technique helps to solve many pragmatic problems such as reference
resolution, the interpretation of noun compounds, the resolution of some kinds of syntactic, and semantic
ambiguity as a by-product. We adopt this approach. Specifically, we use a system we have built called
Mini-TACITUS1 (Mulkar et al, 2007) that provides the expressivity of logical inference but also allows
probabilistic, fuzzy, or defeasible inference and includes measures of the ?goodness? of abductive proofs
and hence of interpretations of texts and other situations.
The success of a discourse processing system based on inferences heavily depends on a knowledge
base. The main contribution of this paper is in showing how a large and reliable knowledge base can be
obtained by exploiting existing lexical semantic resources and can be successfully applied to reasoning
tasks on a large scale. In particular, we experiment with axioms extracted from WordNet, see Fellbaum
(1998), and FrameNet, see Ruppenhofer et al (2006). In axiomatizing FrameNet we rely on the study
described in Ovchinnikova et al (2010).
We evaluate our inference system and the obtained knowledge base in recognizing textual entailment
(RTE). As the reader will see in the following sections, inferences carried out by Mini-TACITUS are
fairly general and not tuned for a particular application. We decided to test our approach on RTE because
this is a well-defined task that captures major semantic inference needs across many natural language
1http://www.rutumulkar.com/download/TACITUS/tacitus.php
225
processing applications, such as question answering, information retrieval, information extraction, and
document summarization. For evaluation, we have chosen the RTE-2 data set (Bar-Haim et al, 2006),
because besides providing text-hypothesis pairs and a gold standard this data set has been annotated with
FrameNet frame and role labels (Burchardt and Pennacchiotti, 2008) which gives us the possibility of
evaluating our frame and role labeling based on the axioms extracted from FrameNet.
2 NL Pipeline and Abductive Reasoning
Our natural language pipeline produces interpretations of texts given the appropriate knowledge base. A
text is first input to the English Slot Grammar (ESG) parser (McCord, 1990, 2010). For each segment,
the parse produced by ESG is a dependency tree that shows both surface and deep structure. The deep
structure is exhibited via a word sense predication for each node, with logical arguments. These logical
predications form a good start on a logical form (LF) for the whole segment. An add-on to ESG converts
the parse tree into a LF in the style of Hobbs (1985). The LF is a conjunction of predications, which have
generalized entity arguments that can be used for showing relationships among the predications. These
LFs are used by the downstream components.
The interpretation of the text is carried out by an inference system called Mini-TACITUS using
weighted abduction as described in detail in Hobbs et al (1993). Mini-TACITUS tries to prove the logical
form of the text, allowing assumptions where necessary. Where the system is able to prove parts of the
LF, it is anchoring it in what is already known from the overall discourse or from a knowledge base.
Where assumptions are necessary, it is gaining new information. Obviously, there are many possible
proofs in this procedure. A cost function on proofs enables the system to chose the ?best? (the cheapest)
interpretation. The key factors involved in assigning a cost are the following: 1) proofs with fewer
assumptions are favored, 2) short proofs are favored over long ones, 3) plausible axioms are favored over
less plausible axioms, and 4) proofs are favored that exploit the inherent implicit redundancy in text.
Let us illustrate the procedure with a simple example. Suppose that we want to construct the best
interpretation of the sentence John composed a sonata. As a by-product, the procedure will disambiguate
between two readings of compose, namely between the ?form? reading instantiated for example in the
sentence Three representatives composed a committee, and the ?create art? meaning instantiated in the
given sentence. After being processed by the parser, the sentence will be assigned the following logical
form where the numbers (20) after every proposition correspond to the default costs of these proposi-
tions.2 The total cost of this logical form is equal to 60.
John(x1):20 & compose(e1,x1,x2):20 & sonata(x2):20
Suppose our knowledge base contains the following axioms:
1) form(e0,x1,x2):90 ? compose(e0,x1,x2)
2) create art(e0,x1,x2):50 & art piece(x2):40 ? compose(e0,x1,x2)
3) art piece(x1):90 ? sonata(x1)
Unlike deductive axioms, abductive axioms should be read ?right to left?. Thus, the propositions on
the right hand side (compose, sonata) correspond to an input, whereas the left hand side propositions
will be assumed given the input. The number assigned to each proposition on the left hand side shows
what percentage of the total input cost the assumption of this proposition will cost.3 For example, if the
proposition compose costs 20 then the assumption of form will cost 18.
Two interpretations can be constructed for the given logical form. The first one is the result of the
application of axioms 1 and 3. Note that the costs of the backchained propositions (compose, sonata) are
2The actual value of the default costs of the input propositions does not matter, because, as the reader will see in this section,
the axiom weights which affect the costs of the resulting interpretations are given as percentages of the input proposition costs.
The only heuristic we use here concerns setting all costs of the input propositions to be equal (all propositions cost 20 in the
discussed example). This heuristic needs a further investigation to be approved or modified.
3The axiom weights in the given example are arbitrary.
226
set to 0, because their costs are now carried by the newly introduces assumptions (form, art piece). The
total cost of the first interpretation I1 is equal to 56.
I1: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & form(e1,x1,x2):18 & art piece(x2):18
The second interpretation is constructed in two steps. First, axioms 2 and 3 are applied as follows.
I21: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 &
create art(e1,x1,x2):10 & art piece(x2):8 & art piece(x2):18
The total cost of I21 is equal to 56. This interpretation is redundant, because it contains the propo-
sition art piece twice. The procedure will merge propositions with the same predicate, setting the cor-
responding arguments of these propositions to be equal and assigning the minimum of the costs to the
result of merging. The idea behind such mergings is that if an assumption has already been made then
there is no need to make it again. The final form of the second interpretation I22 with the cost of 38
is as follows. The ?create art? meaning of compose has been brought forward because of the implicit
redundancy in the sentence which facilitated the disambiguation.
I22: John(x1):20 & compose(e1,x1,x2):0 & sonata(x2):0 & create art(e1,x1,x2):10 &
art piece(x2):8
Thus, on each reasoning step the procedure 1) applies axioms to propositions with non-zero costs
and 2) merges propositions with the same predicate, assigning the lowest cost to the result of merging.
Reasoning terminates when no more axioms can be applied.4 The procedure favors the cheapest inter-
pretations. Among them, the shortest proofs are favored, i.e. if two interpretations have the same cost
then the one which has been constructed with fewer axiom application steps is considered to be ?better?.
It is easy to see that changing weights of axioms can crucially influence the reasoning process. Axiom
weights can help to propagate more frequent and reliable inferences and to distinguish between ?real?
abduction and deduction. For example, an axiom backchaining from dog to animal should in the general
case have a weight below 100, because it is cheap to assume that there is an animal if there is a dog; it is
a reliable deduction. On the contrary, assuming dog given animal should have a weight above 100.
In order to avoid undesirable mergings, we introduce non-merge constraints. For example, in the
sentence John reads a book and Bill reads a book the two read propositions should not be merged
because they refer to different actions. This is ensured by the following non-merge constraint: if not all
arguments of two propositions (which are not nouns) with the same predicate can be merged, then these
propositions cannot be merged. The constraint implies that in the sentence above two read propositions
cannot be merged, because John being the first argument of the first read cannot be merged with Bill.5
This constraint is a heuristic; it corresponds to the intuition that it is unlikely that the same noun refers to
different objects in a short discourse, while for other parts of speech it is possible. An additional corpus
study is needed in order to prove or disprove it.
The described procedure provides solutions to a whole range of natural language pragmatics prob-
lems, such as resolving ambiguity, discovering implicit relations in nouns compounds, prepositional
phrases, or discourse structure. Moreover, this account of interpretation solves the problem of where to
stop drawing inferences, which could easily be unlimited in number; an inference is appropriate if it is
part of the lowest-cost proof of the logical form.
Adapting Mini-TACITUS to a Large-Scale Knowledge Base
Mini-TACITUS (Mulkar et al, 2007) began as a simple backchaining theorem-prover intended to be a
more transparent version of the original TACITUS system, which was based on Stickel?s PTTP system
(Stickel, 1988). Originally, Mini-TACITUS was not designed for treating large amounts of data. A clear
and clean reasoning procedure rather than efficiency was in the focus of its developers. In order to make
the system work with the large-scale knowledge base, we had to perform several optimization steps and
add a couple of new features.
4In practice, we use the depth parameter d and do not allow an inference chain with more that d steps.
5Recall that only propositions with the same predicate can be merged, therefore John and Bill cannot be merged.
227
For avoiding the reasoning complexity problem, we have introduced two parameters. The time pa-
rameter t is used to restrict the processing time. After the processing time exceeds t the reasoning
terminates and the best interpretation so far is output. The time parameter ensures that an interpretation
will be always returned by the procedure even if reasoning could not be completed in a reasonable time.
The depth parameter d restricts the depth of the inference chain. Suppose that a proposition p occurring
in the input has been backchained and a proposition p? has been introduced as a result. Then, p? will be
backchained and so on. The number of such iterations cannot exceed d. The depth parameter reduces
the number of reasoning steps.
Since Mini-TACITUS processing time increases exponentially with the input size (sentence length
and number of axioms), making such a large set of axioms work was an additional issue. For speeding
up reasoning it was necessary to reduce both the number of the input propositions and the number of
axioms. In order to reduce the number of axioms, a two-step reduction of the axiom set is performed.
First, only the axioms which could be evoked by the input propositions or as a result of backchaining
from the input are selected for each reasoning task. Second, the axioms which could never lead to any
merging are filtered out. Concerning the input propositions, those which could never be merged with the
others (even after backchaining) are excluded from the reasoning process.
3 Knowledge Base
As described in the previous section, the Mini-TACITUS inferences are based on a knowledge base (KB)
consisting of a set of axioms. In order to obtain a reliable KBwith a sufficient coverage we have exploited
existing lexical-semantic resources.
First, we have extracted axioms from WordNet (Fellbaum, 1998), version 3.0, which has already
proved itself to be useful in knowledge-intensive NLP applications. The central entity in WordNet is
called a synset. Synsets correspond to word senses, so that every lexeme can participate in several
synsets. For every word sense, WordNet indicates the frequency of this particular word sense in the
WordNet annotated corpora. We have used the lexeme-synset mapping for generating axioms, with the
corresponding frequencies of word senses converted into the axiom weights. For example, in the axioms
below, the verb compose is mapped to its sense 2 in WordNet which participates in synset-X.
compose-2(e1,x1,x2):80 ? compose(e1,x1,x2)
synset-X(e0,e1):100 ? compose-2(e1,x1,x2)
Moreover, we have converted the following WordNet relations defined on synsets into axioms: hy-
pernymy, instantiation, entailment, similarity, meronymy. Hypernymy and instantiation relations pre-
suppose that the related synsets refer to the same entity (the first axiom below), whereas other types of
relations relate synsets referring to different entities (the second axiom below). All axioms based on
WordNet relations have the weights equal to 100.
synset-1(e0,e1):100 ? synset-2(e0,e1)
synset-1(e0,e1):100 ? synset-2(e2,e3)
WordNet alo provides morphosemantic relations which relate verbs and nouns, e.g., buy-buyer.
WordNet distinguishes between 14 types of such relations.We use relation types in order to define the
direction of the entailment and map the arguments. For example, the ?agent? relation (buy-buyer) stands
for a bi-directional entailment such that the noun is the first (agentive) argument of the verb:
buy-1(e0,x1,x2):100 ? buyer-1(x1)
buyer-1(x1):100 ? buy-1(e0,x1,x2)
Additionally, we have exploited the WordNet synset definitions. In WordNet the definitions are given
in natural language form. We have used the extended WordNet resource6 which provides logical forms
for the definition in WordNet version 2.0. We have adapted logical forms from extended WordNet to our
6http://xwn.hlt.utdallas.edu/
228
representation format and converted them into axioms; for example the following axiom represents the
meaning of the synset containing such lexemes as horseback. These axioms have the total weight of 100.
on(e2,e1,x2):25 & back(e3,x2):25 & of (e4,x2,x1):25 & horse(e5,x1):25 ? synset-X(e0,x0)
The second resource which we have used as a source of axioms is FrameNet, release 1.5, see Rup-
penhofer et al (2006). FrameNet has a shorter history in NLP applications thanWordNet, but lately more
and more researchers have been demonstrating its potential to improve the quality of question answering
(Shen and Lapata, 2007) and recognizing textual entailment (Burchardt et al, 2009). The lexical mean-
ing of predicates in FrameNet is represented in terms of frames which describe prototypical situations
spoken about in natural language. Every frame contains a set of roles corresponding to the participants of
the described situation. Predicates with similar semantics are assigned to the same frame; e.g. both give
and hand over refer to the GIVING frame. For most of the lexical elements FrameNet provides syntactic
patterns showing the surface realization of these lexical elements and their arguments. Syntactic patterns
also contain information about their frequency in the FrameNet annotated corpora. We have used the
patterns and the frequencies for deriving axioms such as for example the following.
GIVING(e1,x1,x2,x3):70 & DONOR(e1,x1):0 & RECIPIENT(e1,x2):0 & THEME(e1,x3):0 ?
give(e1,x1,x3) & to(e2,e1,x2)
HIRING(e1,x1,x3):90 & EMPLOYER(e1,x1) & EMPLOYEE(e1,x3) ?
give(e1,x1,x2,x3):10 & job(x2)
The first pattern above corresponds to the phrases like John gave a book to Mary and the second ?
less frequent ? to phrases like John gave Mary a job. It is interesting to note that application of such
axioms provides a solution to the problem of semantic role labeling as a by-product. As in the statis-
tical approaches, more frequent patterns will be favored. Moreover, patterns helping to detect implicit
redundancy will be brought forward.
FrameNet alo introduces semantic relations defined on frames such as inheritance, causation or
precedence; for example the GIVING and GETTING frames are connected with the causation relation.
Roles of the connected frames are also linked, e.g. DONOR in GIVING is linked with SOURCE in GETTING.
Frame relations have no formal semantics in FrameNet. In order to generate corresponding axioms, we
have used the previous work on axiomatizing frame relations and extracting new relations from corpora
(Ovchinnikova et al, 2010). Weights of the axioms derived from frame relations depend on corpus-based
similarity of the lexical items assigned to the corresponding frames. An example of an axiomatized
relation is given below.7
GIVING(e0,x1,x2,x3):120 & DONOR(e0,x1):0 & RECIPIENT(e0,x2):0 & THEME(e0,x3):0 &
causes(e0,e1):0 ? GETTING(e1,x2,x3,x1) & SOURCE(e1,x1) & RECIPIENT(e1,x2) & THEME(e1,x3)
Both WordNet and FrameNet are manually created resources which ensures a relatively high quality
of the resulting axioms as well as the possibility of exploiting the linguistic information provided for
structuring the axioms. Although manual creation of resources is a very time-consuming task, WordNet
and FrameNet, being long-term projects, have an extensive coverage of English vocabulary. The cover-
age of WordNet is currently larger than that of FrameNet (155 000 vs. 12 000 lexemes). However, the
fact that FrameNet introduces complex argument structures (roles) for frames and provides mappings of
these structures makes FrameNet especially valuable for reasoning.
The complete list of axioms we have extracted from these resources is given in table 1.
4 Recognizing Textual Entailment
As the reader can see from the previous sections, the discourse processing procedure we have presented
is fairly general and not tuned for any particular type of inferences. We have evaluated the procedure and
7The ?causes? predicate is supposed to be linked to an underlying causation theory, see for example
http://www.isi.edu/?hobbs/bgt-cause.text. However, in the described experimental settings we have left the abstract theories
out and evaluated only the axioms extracted from the lexical-semantic resources.
229
Table 1: Statistics for extracted axioms
Axiom type Source Numb. of axioms
Lexeme-synset mappings WN 3.0 422,000
Lexeme-synset mappings WN 2.0 406,000
Synset relations WN 3.0 141,000
Derivational relations WN 3.0 (annotated) 35,000
Synset definitions WN 2.0 (parsed, annotated) 120,500
Lexeme-frame mappings FN 1.5 50,000
Frame relations FN 1.5 + corpora 6,000
the KB derived from WordNet and FrameNet on the Recognizing Textual Entailment (RTE) task, which
is a generic task that seems to capture major semantic inference needs across many natural language
processing applications. In this task, the system is given a text and a hypothesis and must decide whether
the hypothesis is entailed by the text plus commonsense knowledge.
Our approach is to interpret both the text and the hypothesis using Mini-TACITUS, and then see
whether adding information derived from the text to the knowledge base will reduce the cost of the best
abductive proof of the hypothesis as compared to using the original knowledge base only. If the cost
reduction exceeds a threshold determined from a training set, then we predict entailment.
A simple example would be the text John gave a book to Mary and the hypothesis Mary got a book.
Our pipeline constructs the following logical forms for these two sentences.
T: John(x1):20 & give(e1,x1,x2):20 & book(x3):20 & to(e2,e1,x3):20 & Mary(x3):20
H: Mary(x1):20 & get(e1,x1,x2):20 & book(x2):20
These logical forms constitute the Mini-TACITUS input. Mini-TACITUS applies the axioms from
the knowledge base to the input logical forms in order to reduce the overall cost of the interpretations.
Suppose that we have three FrameNet axioms in our knowledge base. The first one maps give to to the
GIVING frame, the second one maps get to GETTING and the third one relates GIVING and GETTING with
the causation relation. The first two axioms have the weights of 90 and the third 120. As a result of the
application of the axioms the following best interpretations will be constructed for T and H.
I(T): John(x1):20 & give(e1,x1,x2):0 & book(x3):20 & to(e2,e1,x3):0 & Mary(x3):20 &
GIVING(e0,x1,x2,x3):18
I(H): Mary(x1):20 & get(e1,x1,x2):0 & book(x2):20 & GETTING(e0,x1,x2):18
The total cost of the best interpretation for H is equal to 58. Now the best interpretation of T will
be added to H with the zero costs (as if T has been totally proven) and we will try to prove H once
again. First of all, merging of the propositions with the same names will result in reducing costs of the
propositions Mary and book to 0, because they occur in T:
I(T+H): John(x1):0 & give(e1,x1,x2):0 & book(x3):0 & to(e2,e1,x3):0 & Mary(x3):0 &
GIVING(e0,x1,x2,x3):0 & get(e1,x1,x2):0 & GETTING(e0,x1,x2):18
The only proposition left to be proved is GETTING. Using the GETTING-GIVING relation as described
in the previous section, this proposition can be backchained on to GIVING which will merge with GIVING
coming from the T sentence. H appears to be proven completely with respect to T; the total cost of its
best interpretation given T is equal to 0. Thus, using knowledge from T helped to reduce the cost of the
best interpretation of H from 58 to 0.
The approach presented does not have any special account for logical connectors such as if, not, or
etc. Given a text If A then B and a hypothesis A and B our procedure will most likely predict entailment.
At the moment our RTE procedure mainly accounts for the informational content of texts, being able to
detect the ?aboutness? overlap of T and H. In our framework, a fuller treatment of the logical structure
230
of the natural language would presuppose a more complicated strategy of merging redundancies.
5 Evaluation Results
We have evaluated our procedure on the RTE-2 dataset 8, see Bar-Haim et al (2006) . The RTE-2
dataset contains the development and the test set, both including 800 text-hypothesis pairs. Each dataset
consists of four subsets, which correspond to typical success and failure settings in different applications:
information extraction (IE), information retrieval (IR), question answering (QA), and summarization
(SUM). In total, 200 pairs were collected for each application in each dataset.
As a baseline we have processed the datasets with an empty knowledge base. Then we have done 2
runs, first, using axioms extracted fromWordNet 3.0 plus FrameNet, and, second, using axioms extracted
from the WordNet 2.0 definitions. In both runs the depth parameter was set to 3. The development
set was used to train the threshold as described in the previous section.9 Table 2 contains results of
our experiments.10 Accuracy was calculated as the percentage of pairs correctly judged. The results
suggest that the proposed method seems to be promising as compared to the other systems evaluated
on the same task. Our best run gives 63% accuracy. Two systems participating the RTE-2 Challenge
had 73% and 75% accuracy, two systems achieved 62% and 63%, while most of the systems achieved
55%-61%, cf. Bar-Haim et al (2006). For our best run (WN 3.0 + FN), we present the accuracy data
for each application separately (table 2). The distribution of the performance of Mini-TACITUS on the
four datasets corresponds to the average performance of systems participating in RTE-2 as reported by
Garoufi (2007). The most challenging task in RTE-2 appeared to be IE. QA and IR follow, and finally,
SUM was titled the ?easiest? task, with a performance significantly higher than that of any other task.11
It is worth noting that the performance of Mini-TACITUS increases with the increasing time of pro-
cessing. This is not surprising. We use the time parameter t for restricting the processing time. The
smaller t is, the fewer chances Mini-TACITUS has for applying all relevant axioms. The experiments
carried out suggest that optimizing the system computationally could lead to producing significantly bet-
ter results. Tracing the reasoning process, we found out that given a long sentence and a short processing
time Mini-TACITUS had time to construct only a few interpretations, and the real best interpretation was
not always among them.
The lower performance of the system using the KB based on axioms extracted from extended Word-
Net can be easily explained. At the moment we define non-merge constraints (see section 2) for the input
propositions only. The axioms extracted from the synset definitions introduce a lot of new lexemes into
the logical form, since these axioms define words with the help of other words rather than abstract con-
cepts. These new lexemes, especially those which are frequent in English, result in undesired mergings
(e.g., mergings of frequent prepositions), since no non-merge constraints are defined for them. In order
to fix this problem, we will need to implement dynamic non-merge constraints which will be added on
the fly if a new lexeme is introduced during reasoning. The WN 3.0 + FN axiom set does not fall into
this problem, because these axioms operate on frames and synsets rather than on lexemes.
In addition, for the run using axioms derived from FrameNet, we have evaluated how well we do
in assigning frames and frame roles. For Mini-TACITUS, semantic role labeling is a by-product of
constructing the best interpretation. But since this task is considered to be important as such in the NLP
community, we provide an additional evaluation for it. As a gold standard we have used the Frame-
Annotated Corpus for Textual Entailment, FATE, see Burchardt and Pennacchiotti (2008). This corpus
provides frame and semantic role label annotations for the RTE-2 challenge test set.12 It is important to
8http://pascallin.ecs.soton.ac.uk/Challenges/RTE2/
9Interpretation costs were normalized to the number of propositions in the input.
10?Time? stands for the value of the time parameter ? processing time per sentence, in minutes; ?Numb. of ax.? stands for
the average number of axioms per sentence.
11In order to get a better understanding of which parts of our KB are useful for computing entailment and for which types of
entailment, in future, we are planning to use the detailed annotation of the RTE-2 dataset describing the source of the entailment
which was produced by Garoufi (2007). We would like to thank one of our reviewers for giving us this idea.
12FATE was annotated with the FrameNet 1.3 labels, while we have been using 1.5 version for extracting axioms. However,
231
Table 2: Evaluation results for the RTE-2 test set
KB Accuracy Time
Numb. of ax.
T H
No KB 57% 1 0 0
WN 3.0 + FN 62% 20 533 237
WN 3.0 + FN 63% 30 533 237
Ext. WN 2.0 60% 20 3700 1720
Ext. WN 2.0 61% 30 3700 1720
Task Accuracy
SUM 75%
IR 64%
QA 62%
IE 50%
Table 3: Evaluation of frames/roles labeling towards FATE
System
Frame match
Recall
Role match
Precision Recall
Shalmaneser 0.55 0.54 0.37
Shalmaneser + Detour 0.85 0.52 0.36
Mini-TACITUS 0.65 0.55 0.30
note that FATE annotates only those frames which are relevant for computing entailment. Since Mini-
TACITUS makes all possible frame assignments for a sentence, we provide only the recall measure for
the frame match and leave the precision out.
The FATE corpus was also used as a gold standard for evaluating the Shalmaneser system (Erk and
Pado, 2006) which is a state-of-the-art system for assigning FrameNet frames and roles. In table 2 we
replicate results for Shalmaneser alone and Shalmaneser boosted with the WordNet Detour to FrameNet
(Burchardt et al, 2005). The WN-FN Detour extended the frame labels assigned by Shalmaneser with
the labels related via the FrameNet hierarchy or by the WordNet inheritance relation, cf. Burchardt et al
(2009). In frame matching, the number of frame labels in the gold standard annotation that can also be
found in the system annotation (recall) was counted. Role matching was evaluated only on the frames
that are correctly annotated by the system. The number of role labels in the gold standard annotation
that can also be found in the system annotation (recall) as well as the number of role labels found by
the system which also occur in the gold standard (precision) were counted.13 Table 3 shows that given
FrameNet axioms, the performance of Mini-TACITUS on semantic role labeling is compatible with those
of the system specially designed to solve this task.
6 Conclusion and Future Work
This paper presents a discourse processing framework underlying the abductive reasoner called Mini-
TACITUS. We have shown that interpreting texts using weighted abduction helps solve pragmatic prob-
lems in discourse processing as a by-product. In this paper, particular attention was paid to the construc-
tion of a large and reliable knowledge base populated with axioms extracted from such lexical-semantic
resources as WordNet and FrameNet. The reasoning procedure as well as the knowledge base were eval-
uated in the Recognizing Textual Entailment task. The data for evaluation were taken from the RTE-2
Challenge. First, we have evaluated the accuracy of the entailment prediction. Second, we have eval-
in the new FN version the number of frames and roles increases and there is no message about removed frames in the General
Release Notes R1.5, see http://framenet.icsi.berkeley.edu. Therefore we suppose that most of the frames and roles used for the
FATE annotation are still present in FN 1.5.
13We do not compare filler matching, because the FATE syntactic annotation follows different standards as the one produced
by the ESG parser, which makes aligning fillers non-trivial.
232
uated frame and role labeling using the Frame-Annotated Corpora for Textual Entailment as the gold
standard. In both tasks our system showed performance compatible with those of the state-of-the art
systems. Since the inference procedure and the axiom set are general and not tuned for a particular task,
we consider the results of our experiments to be promising concerning possible manifold applications of
Mini-TACITUS.
The experiments we have carried out have shown that there is still a lot of space for improving the
procedure. First, for successful application of Mini-TACITUS on a large scale the system needs to be
computationally optimized. In its current state, Mini-TACITUS requires too much time for producing
satisfactory results. As our experiments suggest (cf. table 2), speeding up reasoning may lead to signif-
icant improvements in the system performance. Since Mini-TACITUS was not originally designed for
large-scale processing, its implementation is in many aspects not effective enough. We hope to improve
it by changing the data structure and re-implementing some of the main algorithms.
Second, in the future we plan to elaborate our treatment of natural language expressions standing for
logical connectors such as implication if, negation not, disjunction or and others. Quantifiers such as
all, each, some also require a special treatment. This advance is needed in order to achieve more precise
entailment inferences, which are at the moment based in our approach on the core information content
(?aboutness?) of texts. Concerning the heuristic non-merge constraints preventing undesired mergings
as well as the heuristic for assigning default costs (see section 2), in the future we would like to perform
a corpus study for evaluating and possibly changing these heuristics.
Another future direction concerns the enlargement of the knowledge base. Hand-crafted lexical-
semantic resources such as WordNet and FrameNet provide both an extensive lexical coverage and a
high-value semantic labeling. However, such resources still lack certain features essential for captur-
ing some of the knowledge required for linguistic inferences. First of all, manually created resources
are static; updating them with new information is a slow and time-consuming process. By contrast,
commonsense knowledge and the lexicon undergo daily updates. In order to accommodate dynamic
knowledge, we plan to make use of the distributional similarities of words in a large Web-corpus such
as for example Wikipedia. Many researchers working on RTE have already been using word similarity
for computing similarity between texts and hypotheses, e.g., Mehdad et al (2010). In our approach, we
plan to incorporate word similarities into the reasoning procedure making them affect proposition costs
so that propositions implied by the context (similar to other words in the context) will become cheaper
to prove. This extension might give us a performance improvement in RTE, because it will help to relate
those propositions from H for which there are no appropriate axioms in the KB to propositions in T.
Lexical-semantic resources as knowledge sources for reasoning have another shortcoming: They
imply too little structure. WordNet and FrameNet enable some argument mappings of related synsets or
frames, but they cannot provide a more detailed concept axiomatization. We are engaged in two types of
efforts to obtain more structured knowledge. The first effort is the manual encoding of abstract theories
explicating concepts that pervade natural language discourse, such as causality, change of state, and
scales, and the manual encoding of axioms linking lexical items to these theories. A selection of the core
theories can be found at http://www.isi.edu/ hobbs/csk.html. The second effort concerns making use of
the existing ontologies. The recent progress of the Semantic Web technologies has stimulated extensive
development of the domain-specific ontologies as well as development of inference machines specially
designed to reason with these ontologies.14 In practice, domain-specific ontologies usually represent
detailed and structured knowledge about particular domains (e.g. geography, medicine etc.). We intend
to make Mini-TACITUS able to use this knowledge through querying an externally stored ontology with
the help of an existing reasoner. This extension will give us a possibility to access elaborated domain-
specific knowledge which might be crucial for interpretation of domain-specific texts.
We believe that implementation of the mentioned improvements and extensions will make Mini-
TACITUS a powerful reasoning system equipped with enough knowledge to solve manifold NLP tasks on
a large scale. In our view, the experiments with the axioms extracted from the lexical-semantic resources
presented in this paper show the potential of weighted abduction for natural language reasoning and open
14www.w3.org/2001/sw/,http://www.cs.man.ac.uk/ sattler/reasoners.html
233
new ways for its application.
References
Bar-Haim, R., I. Dagan, B. Dolan, L. Ferro, D. Giampiccolo, B. Magnini, and I. Szpektor (2006). The
second PASCAL recognising textual entailment challenge. In Proc. of the Second PASCAL Challenges
Workshop on Recognising Textual Entailment.
Burchardt, A., K. Erk, and A. Frank (2005). A WordNet Detour to FrameNet. In Sprachtechnologie,
mobile Kommunikation und linguistische Resourcen, Volume 8.
Burchardt, A. and M. Pennacchiotti (2008). FATE: a FrameNet-Annotated Corpus for Textual Entail-
ment. In Proc. of LREC?08.
Burchardt, A., M. Pennacchiotti, S. Thater, and M. Pinkal (2009). Assessing the impact of frame seman-
tics on textual entailment. Natural Language Engineering 15(4), 527?550.
Erk, K. and S. Pado (2006). Shalmaneser - a flexible toolbox for semantic role assignment. In Proc. of
LREC?06, Genoa, Italy.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database (First ed.). MIT Press.
Garoufi, K. (2007). Towards a better understanding of applied textual entailment: Annotation and eval-
uation of the rte-2 dataset. Master?s thesis, Saarland University.
Hobbs, J. R. (1985). Ontological promiscuity. In Proceedings, 23rd Annual Meeting of the Association
for Computational Linguistics, Chicago, Illinois, pp. 61?69.
Hobbs, J. R., M. Stickel, and P. Martin (1993). Interpretation as abduction. Artificial Intelligence 63,
69?142.
McCord, M. C. (1990). Slot grammar: A system for simpler construction of practical natural language
grammars. In Natural Language and Logic: International Scientific Symposium, Lecture Notes in
Computer Science, pp. 118?145. Springer Verlag.
McCord, M. C. (2010). Using Slot Grammar. Technical report, IBM T. J. Watson Research Center. RC
23978Revised.
Mehdad, Y., A. Moschitti, and F. M. Zanzotto (2010). Syntactic/semantic structures for textual entailment
recognition. In Proc. of HLT ?10: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pp. 1020?1028.
Mulkar, R., J. R. Hobbs, and E. Hovy (2007). Learning from Reading Syntactically Complex Biol-
ogy Texts. In Proc.of the 8th International Symposium on Logical Formalizations of Commonsense
Reasoning. Palo Alto.
Ovchinnikova, E., L. Vieu, A. Oltramari, S. Borgo, and T. Alexandrov (2010). Data-Driven and Onto-
logical Analysis of FrameNet for Natural Language Reasoning. In Proc. of LREC?10, Valletta, Malta.
Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson, and J. Scheffczyk (2006). FrameNet II: Extended
Theory and Practice. International Computer Science Institute.
Shen, D. and M. Lapata (2007). Using Semantic Roles to Improve Question Answering. In Proc. of
EMNLP-CoNLL, pp. 12?21.
Stickel, M. E. (1988). A prolog technology theorem prover: Implementation by an extended prolog
compiler. Journal of Automated Reasoning 4(4), 353?380.
234
