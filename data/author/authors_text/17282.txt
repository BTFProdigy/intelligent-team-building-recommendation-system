Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 153?157, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
IIITH: A Corpus-Driven Co-occurrence Based Probabilistic Model for 
Noun Compound Paraphrasing  
 
Nitesh Surtani, Arpita Batra, Urmi Ghosh and Soma Paul 
Language Technologies Research Centre 
IIIT Hyderabad 
Hyderabad, Andhra Pradesh-500032 
{nitesh.surtaniug08, arpita.batra, urmi.ghosh}@students.iiit.ac.in, soma@iiit.ac.in 
  
Abstract 
This paper presents a system for automatically 
generating a set of plausible paraphrases for a 
given noun compound and rank them in de-
creasing order of their usage represented by 
the confidence value provided by the human 
annotators. Our system implements a corpus-
driven probabilistic co-occurrence based 
model for predicting the paraphrases, that uses 
a seed list of paraphrases extracted from cor-
pus to predict other paraphrases based on their 
co-occurrences. The corpus study reveals that 
the prepositional paraphrases for the noun 
compounds are quite frequent and well cov-
ered but the verb paraphrases, on the other 
hand, are scarce, revealing the unsuitability of 
the model for standalone corpus-driven ap-
proach. Therefore, to predict other paraphras-
es, we adopt a two-fold approach: (i) 
Prediction based on Verb-Verb co-
occurrences, in case the seed paraphrases are 
greater than threshold; and (ii) Prediction 
based on Semantic Relation of NC, otherwise. 
The system achieves a comparabale score of 
0.23 for the isomorphic system while main-
taining a score of 0.26 for the non-isomorphic 
system. 
1 Introduction 
Semeval 2013 Task 4 (Hendrickx et. al., 2013), 
?Free Paraphrases of Noun Compounds? is a pa-
raphrase generation task that requires the system to 
generate multiple paraphrases for a given noun 
compound and rank them to the best approxima-
tion of the human rankings, represented by the cor-
responding confidence value. The task is an 
extension of Semeval 2010 Task 9 (Butnariu et al, 
2010), where the participants were asked to rank 
the set of given paraphrases for each noun com-
pound. Although the ranking task is quite distinct 
from the task of generating paraphrases, however, 
we have taken many insights from the systems de-
veloped for the ranking task, and have reported 
them appropriately in our system description. 
This paper describes a system for generating a 
ranked set of paraphrases for a given NC. A pa-
raphrase can be Prepositional, Verb or Verb + Pre-
positional. Since the prepositional paraphrases are 
easily available in the corpus while the occurrences 
of verb or verb+prep paraphrases is scarce, the task 
of paraphrasing becomes significant in finding out 
a method for predicting reliable paraphrases with 
verbs for a given NC. Our system implements a 
model that is based on co-occurrences of the pa-
raphrases and selects those paraphrases that have a 
higher probability of co-occurring with a set of 
extracted paraphrases which are referred to as Seed 
Paraphrases. Keeping the verb-paraphrase scarcity 
issue in mind, we develop a two-way model: (i) 
Model 1 is used when the seed paraphrases are 
considerable in number i.e., greater than the thre-
shold value. In this case, other verb paraphrases are 
predicted based on their co-occurrence with the set 
of extracted verb paraphrases. (ii) Model 2 is used 
when the size of the seed list falls below the thre-
shold value, in which case, we make use of the 
prepositional paraphrases to predict the relation of 
the noun compound and select verbs that mostly 
co-occur with that relation. Our system achieves an 
isomorphic score of 0.23 with a non-isomorphic of 
0.26 with the human generated paraphrases. The 
next section discusses the system.  
2 System Description 
This section of the paper describes each module of 
the system in detail. The first module of the system 
153
talks about the Seed data extraction using corpus 
search. The next module uses the seed data for 
predicting more verbs that would be used in pa-
raphrasing. The third module uses these predicted 
verbs in template generation for generating NC 
Paraphrasing and the generated paraphrases are 
ranked in the last module. 
2.1 Seed Data Extraction Module 
We have relied mostly on the Google N-gram Cor-
pus for extracting the seed paraphrases. Google has 
publicly released their web data as n-grams, also 
known as Web-1T corpus, via the Linguistic Data 
Consortium (Brants and Franz, 2006). It contains 
sequences of n-terms that occur more than 40 times 
on the web. Since the corpus consists of raw data 
from the web, certain pre-processing steps are es-
sential before it can be used. We extract a set of 
POS templates from the training data, and general-
ize them enough to accommodate the legitimate 
paraphrases extracted from the corpus. The follow-
ing templates are used for extracting n-gram data: 
Head-Mod N-gram: This template includes both 
the head and the modifier in the same regular ex-
pression. A corresponding 5-gram template for a 
NC Amateur-Championship is shown in Table 1. 
Head <*> <*> 
<*>Mod 
championship conducted for the 
amateurs 
Head <*><*>  
Mod <*> 
championship for all amateur 
players 
Head <*>Mod 
<*><*> 
championship where amateur is 
competing 
Table 1: Templates for paraphrase extraction 
The paraphrases obtained from the above template 
are quite useful, but scarce. To overcome the issue 
of coverage of verb paraphrases, a loosely coupled 
analysis and representation of compounds can be 
employed, as suggested by (Li et.al, 2010). We 
retrieve the partial triplets from the n-gram corpus 
in the form of ?Head Para? and ?Para Modifier?. 
 
 
 
Head Template: Head <*> <*> 
Mod Template: <*> <*> Mod; <*> Mod <*> 
But the process of generating paraphrases from 
head and the modifier n-gram incorporates a huge 
amount of noise and produces a lot of irrelevant 
paraphrases. Therefore, these partial paraphrases 
are not directly used for generating the paraphrases 
but are instead used to diagnose the compatibility 
of the selected verb with the head and the modifier 
of the given NC in Section 2.2.2. We also extract 
paraphrases from ANC and BNC corpus. 
2.2 Verb Prediction Module 
This module is the heart of our system. It imple-
ments two models for predicting the verb paraph-
rases: a Verb Co-occurrence model and a Relation 
Prediction model. The decision of selection of 
model for verb prediction is based on the size of 
the seed list. If the number of seed paraphrases is 
above the threshold value, the verb co-occurrence 
model is used whereas the relation prediction mod-
el is used if it is below the threshold value. 
2.2.1 Verb Co-occurrence Model 
This model uses the seed paraphrases extracted 
from the corpus to predict other verb paraphrases 
by computing their co-occurrences. The model 
gains insights from the UCD-PN system (Nulty 
and Costello, 2010) which tries to identify a more 
general paraphrase by computing the co-
occurrence of a paraphrase with other paraphrases. 
But the task of generating paraphrases has two sub-
tle but significant differences: (i) The list of seed 
verb paraphrases for a given NC is usually small, 
with each seed verb having a corresponding proba-
bility of occurrence; and (ii) Not all the seed verbs 
have legitimate representation of the noun com-
pound. Our system incorporates these distinctions 
in the co-occurrence model discussed below. 
Using the training data at hand, we build a Verb-
Verb co-occurrence matrix, a 2-D matrix where 
each cell (i,j) represents the probability of occur-
rence of Vj when Vi has already occurred.  
? ??  ?? =
?(?? ,?? )
?(??)
=
?????(?? ,?? )
?????(??)
 
The verbs used in co-occurrence matrix are stored 
in a List A. Now, for a given test NC, the model 
extracts the seed list of verb paraphrases (referred 
as List B) from the corpus with their corresponding 
probabilities. The above model calculates a score 
for each verb in List A, by computing its co-
occurrence with the verbs in List B. 
???????? ?? =  ? ??  ?? ? ?(??)
???
 
(Head, Para, ?)  
(?, Para, Mod)  
(Head, Para, Mod)  
154
The term ?(??) in the above equation represents 
the relative occurrence of the verb ??  with the giv-
en NC. The relevance of this term becomes evident 
in the next model. The verbs achieving higher 
score are selected, suggesting a higher probability 
of co-occurrence with the seed verbs.  
2.2.2 Semantic Relation Prediction Model 
This module describes the second model of the 
two-way model, and is used by the system when 
the verbs extracted from the corpus are less than 
the threshold. In this model, we use prepositional 
paraphrases, having a pretty good coverage in the 
corpus, to predict the semantic relation of the com-
pound which helps us in predicting the other pa-
raphrases. The intuition behind using semantic 
class for predicting paraphrases is that they tend to 
capture the behavior of the noun compound and 
can be represented by general paraphrases.  
Noun Compound Relation Paraphrase Sel. 
Prep Verb 
Garden Party Location In, At Held 
Community Life Theme Of, In Made 
Advertising Agency Purpose For, Of, In Doing 
Table 2: Occurrence of Prepositional Paraphrases 
Relation Annotation: Since a supervised ap-
proach is used for identifying the semantic relation 
of the noun compound, we manually annotate the 
noun compounds with a semantic relation. We tag 
each noun compound with one semantic relation 
from the set used in (Moldovan et. al. 2004).  
Prep-Rel and Verb-Rel Co-occurrence: A Prep-
Rel co-occurrence matrix similar to Verb-Verb co-
occurrence matrix discussed in last subsection. 
This 2-D matrix consists of co-occurrence proba-
bilities between the prepositional paraphrases and 
the semantic relation of the compound, where each 
cell (i,j) represents the probability of occurrence of 
preposition Pj with relation Ri. This matrix is used 
as a model to identify semantic relation using pre-
positional paraphrases extracted from the corpus. 
The Verb-Relation co-occurrence matrix is used to 
predict the most co-occurring verbs with the identi-
fied relation. Each cell (i,j) in the matrix represents 
the probability of the verb Vj co-occurring with 
relation Ri. 
Relation Extraction: Research focusing on se-
mantic relation extraction has followed two direc-
tions: (i) Statistical approaches to using very large 
corpus (Berland and Charniak (1999); Hearst 
(1998)); and (ii) Ontology based approaches using 
hierarchical structure of wordnet (Moldovan et. al., 
2004). We employ a statistical model based on the 
Preposition-Relation co-occurrence for identifying 
the relation. The model is quite similar to the one 
used in Section 2.2, but it is here that the model 
reveals its actual power. Since two or more rela-
tions can be represented by same set of preposi-
tional paraphrases, as Theme and Purpose in Table 
2, it is important to take into account the probabili-
ties with which the extracted prepositions occur in 
the corpus. In Table 2, the NC Community Life 
(Theme) occurs frequently with preposition ?of? 
whereas the NC Advertising Agency (Purpose) is 
mostly represented by preposition ?for? in the cor-
pus. The term ?(??) in the equation below cap-
tures this phenomenon and classifies these two 
NCs in their respective classes. 
???????? ? =  ? ? ?? ? ?(??)
???
 
The relation with the highest score is selected as 
the semantic class of the noun compound. A set of 
verbs highly co-occurring with that class are se-
lected, and their compatibility with the correspond-
ing noun compound is judged from their 
occurrences with the partial head and the modifier 
paraphrases as discussed in Section 2.1. The above 
classifier performs moderately and classifies a giv-
en NC with 42.5% accuracy. We have also tried 
the Wordnet based Semantic Scattering model 
(Moldovan et. al., 2004), trained on a set of 400 
instances, but achieved an accuracy of 38%, the 
reason for which can be attributed to the small 
training set. Since the accuracy of identifying the 
correct relation is low, we select some paraphrases 
from the 2nd most probable relation, as assigned by 
the probabilistic classifier.  
2.3 Paraphrase Generator Module  
After predicting a set of verb for a test noun com-
pound, we use the following templates to generate 
the paraphrases: 
a) Head VP Mod 
b) Head VP PP Mod 
c) Head [that|which] VP PP Mod 
The paraphrases that are extracted from the corpus 
are also cleaned using the POS templates extracted 
from the training data. 
155
2.4 Paraphrase Ranker Module  
Motivated by the observations from Nulty and 
Costello (2010) that ?people tend to use general, 
semantically light paraphrases more often than de-
tailed, semantically heavy ones?, we perform rank-
ing of the paraphrases in two steps: (i) Assigning 
different weights to different type of paraphrases, 
i.e. a light weight prepositional paraphrases achiev-
ing higher score than the verb paraphrases; and (ii) 
Ranking a more general paraphrase with the same 
category higher. A paraphrase A is more general 
that paraphrase B (Nulty and Costello, 2010) if 
? ?|? > ?(?|?) 
For a list of paraphrases A generated for a given 
compound, each paraphrase b in that list is scored 
using the below eq., where more general paraph-
rase achieves a high score and is ranked higher. 
????? ? =  ? ? ? 
???
 
The seed paraphrases extracted from the corpus are 
ranked higher than the predicted paraphrases. 
3 Algorithm  
This section presents the implementation of the 
overall system.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
4 Results 
The set of generated paraphrases are evaluated on 
two metrics: a) Isomorphic; b) Non-isomorphic. In 
the isomorphic setting, the test paraphrase is 
matched to the closest reference paraphrases, but 
the reference paraphrase is removed from the set 
whereas in non-isomorphic setting, the reference 
paraphrase which is mapped to a test paraphrase 
can still be used for matching other test paraphras-
es. Table 3 presents the scores of the 3 participat-
ing teams who have submitted total of 4 systems.  
Systems Isomorphic Non-Isomorphic 
SFS 0.2313 0.1794 
IIITH 0.2309 0.2583 
MELODI-Pri 0.1298 0.5484 
MELODI-Cont 0.1357 0.536 
Table 3: Results of the submitted systems 
Our system achieves an isomorphic score of 0.23, 
just below the SFS system maintaining a score of 
0.26 for the non-isomorphic system. The two va-
riants of MELODI system get a high score for the 
non-isomorphic metric but low scores for isomor-
phic metric as compared to other systems. 
5 Conclusion 
We have described a system for automatically ge-
nerating a set of paraphrases for a given noun 
compound, based on the co-occurrences of the pa-
raphrases. The system describes an approach for 
handling those 38% cases (calculated for optimum 
threshold value) of NCs where it is not convenient 
to predict the verbs using their co-occurrences with 
the seed verbs, because the size of the seed list is 
below a threshold value. For other cases, the verb 
co-occurrence model is used to predict the verbs 
for NC paraphrasing. The optimum value of thre-
shold parameter investigated from experiments is 
found to be 3, showing that atleast 3 verb paraph-
rases are necessary to capture the concept of a NC. 
// Training Phase ? Build Co-occurrence Matrices 
Verb_Co-occur = 2-D Matrix  
Prep-Rel_Co-occur = 2-D Matrix  
Verb-Rel_Co-occur = 2-D Matrix  
Verb_List = Verb List extracted from training corpus 
// Testing ? Extract paraphrases with probabilities 
Ext_Verb = List of extracted verb paraphrase  
VProb = Probability of each Ext_Verb 
Ext_Prep = List of extracted prepositional paraphrases 
PProb = Probability of each Ext_Prep 
Prob_Verb = List // Verbs with their selection score 
Prob_Rel = List // Relations with their selection score 
Threshold = 3 // Verb threshold for two-way model 
if count( Ext_Verb ) > Threshold  
    Candidate_Verbs = {Verb_List } - { Ext_Verbs }      
    foreach Candidate_Verbs Vi : 
        Prob_Verb[Vi] = 0 
        foreach Ext_Verb Vj : 
            Prob_Verb[Vi] += Verb_Co-occur [Vi][Vj] *   
   VProb[Vj] 
else       
    foreach Prep-Rel_Co-occur as rel : 
        Prob_Rel[rel] = 0 
             
            
       foreach Ext_Prep as prep : 
           Prob_Rel[rel] += Prep-Rel_Co-occur[rel][prep]
              * PProb[prep]              
           Rel=select highestProb(Prob_Rel) 
           Prob_Verb = Verb-Rel_Co-occur[Rel] 
sort(Prob_Verb)  
Verb_Predicted = select top(N)   
Paraphrase = generate_paraphrase(verb_predicted) 
rank(Paraphrase) 
156
References  
Matthew Berland and Eugene Charniak. 1999. Finding 
parts in very large. In Proceeding of ACL 1999 
T. Brants and A. Franz. 2006. Web 1T 5-gram Version1. 
Linguistic Data Consortium 
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid O S? eaghdha, Stan Szpakowicz, and Tony-
Veale. 2010. Semeval-2 task 9: The interpreta-tion of 
noun compounds using paraphrasing verbs and pre-
positions. In Proceedings of the 5th SIGLEX Work-
shop on Semantic Evaluation 
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid O S?eaghdha, Stan Szpakowicz, and Tony-
Veale. 2013. Semeval?13 task 4: Free Paraphrases of 
Noun Compounds. In Proceedings of the Internation-
al Workshop on Semantic Evaluation, Atlanta, Geor-
gia 
Marti Hearst. 1998. Automated Discovery of Word-Net 
relations. In An Electronic Lexical Database and-
Some of its Applications. MIT Press, Cambridge MA 
Mark Lauer. 1995. Designing Statistical Language-
Learners: Experiments on Noun Compounds. Ph.D. 
Thesis, Macquarie University 
Guofu Li, Alejandra Lopez-Fernandez and Tony Veale. 
2010. UCD-Goggle: A Hybrid System for Noun 
Compound Paraphrasing. In Proceedings of the 5th 
International Workshop on Semantic Evaluation 
(SemEval-2), Uppsala, Sweden 
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel 
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on 
Computational Lexical Semantics, pages 60?67, Bos-
ton, MA 
Paul Nulty and Fintan Costello. 2010. UCD-PN: Select-
ing general paraphrases using conditional probabili-
ty. In Proceedings of the 5th International Workshop 
on Semantic Evaluation (SemEval-2), Uppsala, Swe-
den 
 
157
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 15?21,
Dublin, Ireland, August 23, 2014.
A Two-Stage Approach for Computing Associative Responses to a Set of
Stimulus Words
Urmi Ghosh, Sambhav Jain and Soma Paul
Language Technologies Research Center
IIIT-Hyderabad, India
{urmi.ghosh, sambhav.jain}@research.iiit.ac.in,
soma@iiit.ac.in
Abstract
This paper describes the system submitted by the IIIT-H team for the CogALex-2014 shared task
on multiword association. The task involves generating a ranked list of responses to a set of
stimulus words. The two-stage approach combines the strength of neural network based word
embeddings and frequency based association measures. The system achieves an accuracy of
34.9% over the test set.
1 Introduction
Research in psychology gives evidence that word associations reveal the respondents? perception, learn-
ing and verbal memories and thus determine language production. Hence, it is possible to simulate
human derived word associations by analyzing the statistical distribution of words in a corpus. Church
and Hanks (1990) and Wettler and Rapp (1989) were amongst the first to devise association measures by
utilizing frequencies and co-occurrences from large corpora. Wettler and Rapp (1993) demonstrate that
corpus-based computations of word associations are similar to association norms collected from human
subjects.
The CogALex-2014 shared task on multi-word association involves generating a ranked list of re-
sponse words for a given set of stimulus words. For example, the stimulus word bank can invoke as-
sociative responses such as river, loan, finance and money. Priming
1
bank with bed and bridge, results
in strengthening association with the word river and it emerges as the best response amongst the afore-
mentioned response choices. This task is motivated by the tip-of-the-tongue problem, where associated
concepts from the memory can help recall the target word. Other practical applications include query ex-
pansion for information retrieval and natural language generation where missing words can be predicted
from their context.
The participating systems are distinguished into two categories - Unrestricted systems that allows
usage of any kind of data and Restricted systems that can only make use of the ukWaC (Baroni et al.,
2009) corpus, consisting of two billion tokens. Our proposed system falls in the restricted track since
we only used ukWaC for extracting information on word associations. It follows a two-staged approach:
Candidate Response Generation, which involves selection of words that are semantically similar to the
primes and Re-ranking by Association Measure, that re-ranks the responses using a proposed weighted
Pointwise Mutual Information (wPMI) measure. Our system was evaluated on test-datasets derived
from the Edinburgh Associative Thesaurus (Kiss et al., 1972) and it achieved an accuracy of 34.9%.
When ignoring the inflectional variations of the response word, an accuracy of 39.55% was achieved.
2 Observations on Training Data
The training set consists of 2000 sets of five words (multiword stimuli or primes) and the word that is
most closely associated to all of them (associative response). For example, a set of primes such as wheel,
driver, bus, drive and lorry are given along with the expected associative response - car.
In this section, our initial observations on the given training data are enlisted.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
The phenomenon of providing multiple stimulus words is called priming.
15
2.1 Relation between the Associative Response and the Prime Words
It is observed that a response largely exhibits two kind of relations with a priming word.
Primes Associative Response
presents, Christmas, birthday, shops, present gifts
butterfly, light, ball, fly, insect moth
mouse, cat, catcher, race, tail rat
Table 1: Some examples of primes and their associative responses from the training set
Type A relation depicts a synonymous/antonymous behavior or ?of the same kind? nature. Word pairs
with paradigmatic relation are highly semantically related and belong to the same part of speech. And
hence, they tend to show a substitutive nature amongst themselves without affecting the grammar of the
sentence. From Table - 1, we observe that present/presents , butterfly/insect and mouse/cat can be substi-
tuted in place of gifts, moth and rat respectively. Type B relation depicts contextual co-occurrence, where
the words tend to occur together or form a collocation. This kind of relationship can be demonstrated by
taking examples from Table - 1, such as Christmas gifts, gift shops, birthday gifts, moth ball, rat catcher,
rat race and rat tail. In theory, the above have been formally categorized as paradigmatic (Type A) and
syntagmatic (Type B ) relations by De Saussure et al. (1916) and we will be referring to them accordingly
in rest of the paper.
Type C relation, depicting associations based on the phonological component of the words was also
observed. According to McCarthy (1990), responses can be affected by phonological shapes and or-
thographic patterns especially when instantaneous paradigmatic or syntagmatic association is difficult.
Examples from the training data set include ajar-Ajax, hypothalamus-hippopotamus and cravat-caravan.
Such examples were very few and hence, have not been dealt with in this paper.
2.2 Context Window Size
Words exhibiting syntagmatic associations often occur in close proximity in the corpus. We tested this
phenomenon on 500 randomly chosen sets of primes by calculating the distance of each prime from the
associative responses in the corpus. Figure - 1 testifies that a majority of primes occur within a context
window size of ?2 from the associative response.
1 2 3 4 5 6 7 8 9 10
0
500
1,000
1,500
2,000
d
f
Figure 1: Co-occurrence frequency f of an association at distance d from the response, averaged over the
2500 stimulus word and response word pairs from randomly chosen 500 training datasets
Next, a mechanism to interpret the above associations in a quantitative manner is required.
16
3 Word Representation
In order to have a quantitative comparison of association, first we need a representation for words in
a context. Traditionally co-occurrence vectors serve as a simple mechanism for such a representation.
However, such vectors are unable to effectively capture deeper semantics of words and also tend to suffer
from sparsity due to high dimensional space (equal to the vocabulary size). Several efforts have been
made to represent word vectors in a lower dimensional space. Largely, these can be categorized into:
1. Clustering: Clustering algorithms like Brown et al. (1992), are used to form clusters and derive
a vector based representation for each cluster, where semantically similar clusters are closer in
distance.
2. Topic Modeling: In this approach a word (or a document) is represented as a distribution of topics.
Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dutnais, 1997) , which falls
in this category, utilizes SVD (Singular Value Decomposition) to produce a low rank representation
of a word. Latent Dirichlet Allocation (Blei et al., 2003) is an improvement with dirichlet priors
over the probabilistic version of LSA (Hofmann, 1999).
3. Neural Network based Word Embeddings: Here, a neural network is trained to output a vector
corresponding to a word which effectively signifies its position in the semantic space. There has
been different suggestions on the nature of the neural-net and how the context needs to be fed to
the neural-net. Some notable works include Collobert and Weston (2008), Mnih and Hinton (2008),
Turian et al. (2010) and Mikolov et al. (2013a).
4 Methodology
Our system follows a two-staged approach, where we first generate response candidates which are seman-
tically similar to prime words, followed by a re-ranking step where we give weightage to the responses
likely to occur in proximity.
4.1 Candidate Response Generation
The complete vocabulary (of ukWaC Corpus) is represented in a semantic space by generating word
embeddings induced by the algorithm described in Mikolov et al. (2013a). Our choice is motivated by
the fact that this approach models semantic similarity and outperforms other approaches in terms of
accuracy as well as computational efficiency(Mikolov et al., 2013a; Mikolov et al., 2013c).
The word2vec
2
utility is used to learn this model and thereby create 300-dimensional word embed-
dings. word2vec implements two classification networks - the Skip-gram architecture and the Continuous
Bag-of-words (CBOW) architecture. We applied CBOW architecture as it works better on large corpora
and is significantly faster than Skip-gram(Mikolov et al., 2013b). The CBOW architecture predicts the
current word based on its context. The architecture employs a feed forward neural network, which con-
sists of:
1. An input layer, where the context words are fed to the network.
2. A projection layer, which projects words onto continuous space and reduces number of parameters
that are needed to be estimated.
3. An output layer.
This log-linear classifier learns to predict words based on its neighbors in a window of ?5. We also
applied a minimum word count of 25 so that infrequent words are filtered out.
With the vector representation available, a response r to a set of primes S, is searched in the vocabulary
by measuring its cosine similarity with each prime x
i
in S. The overall similarity of the response r, with
the prime word set S, is defined as the average of these similarities.
2
word2vec : https://code.google.com/p/word2vec/
17
sim(r, S) =
1
|S|
?
|S|
?
i=1
x
i
.r
|x
i
|.|r|
Using the best similarity score as the selection criterion for response, the approach resulted in an
accuracy of 20.8% over the test set. Error analysis revealed that the above approach is biased towards
finding a paradigmatic candidate. However, it is further observed that much of the correct answers
(> 80%) exist in a k-best(k=500) list but with a relatively lower similarity score. This confirmed that our
broader selection is correct but a better re-ranking approach is required.
4.2 Re-ranking by Association Measures
To give due weightage to responses with high syntagmatic associativity, we utilize word co-occurrences
from the corpus. Since we are dealing with semantically related candidates, applying even a basic lexical
association measure like Pointwise Mutual Information (PMI) (Church and Hanks, 1990) tend to improve
the results.
PMI
For each prime word, we calculate co-occurrence frequency information for its neighbors within a win-
dow of ?2 as mentioned in Section 2. Also, a threshold of 3 is set to the observed frequency measures
as PMI tends to give very high association score to infrequent words.
For each candidate response r, we calculate its PMI
i
with each of the primes (x
i
) in the set S. The
total association score Score
PMI
for a candidate is defined as the average of the individual measures.
PMI
i
=
p(x
i
r)
p(x
i
)p(r)
Score
PMI
=
1
|S|
?
?
i?S
PMI
i
Ranking the candidates based on PMI improved the results to 30.45%
Weighted PMI
It should be duly noted that only some primes exhibit a syntagmatic relation with the response, while
the rest exhibit a paradigmatic relation. For example, the expected response for primes Avenue, column,
dimension, sixth, fourth is fifth. The first three words share a syntagmatic relation with the response
while the last two words share a paradigmatic relation with the response. As PMI deals with word
co-occurrences, ideally, only primes exhibiting syntagmatic associations should be considered for re-
ranking. However, a clear distinction between the two categories of primes is a difficult task as the target
response is unknown.
In order to take effective contribution of each prime, we propose a weighed extension of PMI which
gives more weightage to syntagmatic primes as to the paradigmatic ones. Since, primes sharing a
paradigmatic relation with the response word are highly semantically related, they are expected to be
closer in the semantic space too. On the other hand, the primes showcasing syntagmatic relations are
expected to be distant.
Using the vector representation described in Section 4.1, we calculate an average vector of the five
primes, p
avg
, and compute its cosine distance from individual primes. The cosine distance thus obtained
is used as the weight w for the PMI associativity of a prime. In a nutshell, larger the distance of a
prime from p
avg
, the greater is its contribution in the PMI based re-ranking score. This ranking schema
assumes that the prime set consists of at least two words demonstrating paradigmatic relation with the
target response. Table - 2 displays the primes along with their distance from p
avg
.
Score
wPMI
=
1
|S|
?
?
i?S
w
i
PMI
i
Next, a ranked list of candidate responses for each set is generated by sorting the previously ranked
list according to the new score. The new ranking scheme based on weighted PMI (wPMI) improves the
results to 34.9%. Table -3 displays some sets which show improvement upon implementing the wPMI
18
Primes Cosine Distance
Avenue 0.612
column 0.422
dimension 0.390
sixth 0.270
fourth 0.212
Table 2: An example demonstrating Cosine Distance between the primes and the p
avg
of the prime set
ranking scheme. Taking a case from Table - 3, we observe that the correct response skeleton is generated
for primes cupboard, body, skull, bone and bones when ranked according to the wPMI scheme. This is
due to larger weights being assigned to primes cupboard and body which have a closer proximity to the
word skeleton than the word vertebral which is generated by the simple PMI ranking scheme.
cupboard 0.615 pit 0.553 boat 0.499
Primes(with weights) body 0.410 band 0.549 sailing 0.476
skull 0.248 hand 0.426 drab 0.338
bone 0.244 limb 0.0.340 dark 0.318
bones 0.172 leg 0.270 dull 0.307
PMI vertebral amputated drizzly
wPMI skeleton arm dingy
Expected Response skeleton arm dingy
Table 3: Comparison between results from PMI and wPMI re-ranking approaches
5 Results and Evaluation
The system was evaluated on the test set derived from the Edinburgh Associative Thesaurus (EAT) which
lists the associations to thousands of English stimulus words as collected from native speakers. For
example, for the stimulus word visual the top associations are aid, eyes, aids, see, eye, seen and sight.
For the shared task, top five associations for 2000 randomly selected stimulus words were provided as
prime sets and the system was evaluated based on its ability to predict the corresponding stimulus word
for each set. Table - 4 displays the top ten responses generated by our system for some prime sets and
their corresponding stimulus word.
Primes
knight, plate, soldier,
protection, sword
ants, flies, fly,
bees, bite
babies, baby, rash,
wet, washing
butterfly, moth, caterpillar,
cocoon, insect
Top 10
Responses
armor
armour
helmet
shield
guard
bulletproof
guards
warrior
enemy
gallant
mosquitoes
wasps
beetles
insects
spiders
sting
moths
butterflies
arachnids
bedbugs
nappy
shaving
nappies
clothes
skin
bathing
dry
eczema
bedding
dirty
larva
larvae
pupa
species
pests
beetle
silkworm
wings
pupate
pollinated
Target armour insects nappies chrysalis
Table 4: Top ten responses for some prime sets and their corresponding target response
19
As we have considered exact string match(ignoring capitalization), the evaluation does not account
for spelling variations. For example, the response output armor instead of the expected response armour
results in counting it as incorrect.
We achieved an accuracy of 34.9% by considering the top response for each list of ranked responses.
However, it was observed that the correct response was present within the top ten responses in 59.8%
of the cases. For example, the primes ants, flies, fly, bees, bite generate the response output mosquitoes.
The expected output insects ranks 4
th
in our list of responses.
For primes babies, baby, rash, wet, washing, our system outputs nappy while the expected response is
nappies. Such inflected forms of the responses are challenging to predict and hence, another evaluation
is presented which ignores the inflectional variation of the response word. Under this evaluation, we
achieved an accuracy of 39.55% for the best response and 63.15% if the expected response occurs in the
top ten responses. Table - 5 displays accuracy of our system when the target response lies within the
top-n responses for both evaluation methods.
Exact Match Ignoring Inflections
n=1 34.9 39.55
n=3 48.15 49.65
n=5 53.2 55.45
n=10 59.8 63.15
Table 5: Evaluation results in %
6 Conclusion
There exist some word associations that are asymmetric in nature. Rapp (2013) observed that the primary
response of a given stimulus word may have stronger association with another word and need not gen-
erate the stimulus word back. For example, the strongest association to bitter is sweet but the strongest
association to sweet is sour. Therefore, the EAT data set chosen for evaluation, may not be the best judge
for certain cases. Taking a case from our test data, for primes butterfly, moth, caterpillar, cocoon, insect,
our system outputs larva instead of the original stimulus word chrysalis which does not feature even in
the top ten responses (Refer Table - 4).
In this work, we proposed a system to generate a ranked list of responses for multiple stimulus words.
Candidate responses were generated by computing its semantic similarity with the stimulus words and
then re-ranked using a lexical association measure, PMI. This system scored 34.9% when the top ranked
response was considered and 59.8% when the top ten responses were taken into account. When ignoring
inflectional variations, the accuracy improved to 39.55% and 63.15% for the two evaluation methods
respectively.
In future, a more sophisticated re-ranking approach in place of PMI measure can be used such as
product-of-rank algorithm (Rapp, 2008). Since, the re-ranking methodologies discussed by far, take
into account word co-occurrences, it is biased towards syntagmatic responses. A better trade-off can be
worked out to give due weightage to paradigmatic responses too.
References
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection
of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209?
226.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational linguistics, 18(4):467?479.
20
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Comput. Linguist., 16(1):22?29, March.
Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine learning,
pages 160?167. ACM.
Ferdinand De Saussure, Charles Bally, Albert Sechehaye, and Albert Riedlinger. 1916. Cours de linguistique
g?en?erale: Publi?e par Charles Bally et Albert Sechehaye avec la collaboration de Albert Riedlinger. Libraire
Payot & Cie.
Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by latent semantic analysis. JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCI-
ENCE, 41(6):391?407.
Thomas Hofmann. 1999. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth conference on
Uncertainty in artificial intelligence, pages 289?296. Morgan Kaufmann Publishers Inc.
George R. Kiss, Christine A. Armstrong, and Robert Milroy. 1972. An associative thesaurus of English. Medical
Research Council, Speech and Communication Unit, University of Edinburgh, Scotland.
Thomas K. Landauer and Susan T. Dutnais. 1997. A solution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211?240.
Michael McCarthy. 1990. Vocabulary. Oxford University Press.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. CoRR, abs/1301.3781.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine
translation. arXiv preprint arXiv:1309.4168.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word
representations. Proceedings of NAACL-HLT, pages 746?751.
Andriy Mnih and Geoffrey E. Hinton. 2008. A scalable hierarchical distributed language model. In NIPS, pages
1081?1088.
Reinhard Rapp. 2008. The computation of associative responses to multiword stimuli. In Proceedings of the
workshop on Cognitive Aspects of the Lexicon, pages 102?109. Association for Computational Linguistics.
Reinhard Rapp. 2013. From stimulus to associations and back. Natural Language Processing and Cognitive
Science, page 78.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394. Association for Computational Linguistics.
Manfred Wettler and Reinhard Rapp. 1989. A connectionist system to simulate lexical decisions in information
retrieval. Pfeifer, R., Schreter, Z., Fogelman, F. Steels, L.(eds.), Connectionism in perspective. Amsterdam:
Elsevier, 463:469.
Manfred Wettler and Reinhard Rapp. 1993. Computation of word associations based on the co-occurrences of
words in large corpora.
21
