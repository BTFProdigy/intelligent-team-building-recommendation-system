Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1318?1327,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Human-competitive tagging using automatic keyphrase extraction   Olena Medelyan, Eibe Frank, Ian H. Witten Computer Science Department University of Waikato {olena,eibe,ihw}@cs.waikato.ac.nz     Abstract This paper connects two research areas: auto-matic tagging on the web and statistical key-phrase extraction. First, we analyze the quality of tags in a collaboratively created folksonomy using traditional evaluation techniques. Next, we demonstrate how documents can be tagged automatically with a state-of-the-art keyphrase extraction algorithm, and further improve per-formance in this new domain using a new al-gorithm, ?Maui?, that utilizes semantic infor-mation extracted from Wikipedia. Maui out-performs existing approaches and extracts tags that are competitive with those assigned by the best performing human taggers. 1 Introduction Tagging is the process of labeling web resources based on their content. Each label, or tag, corre-sponds to a topic in a given document. Unlike metadata assigned by authors, or by professional indexers in libraries, tags are assigned by end-users for organizing and sharing information that is of interest to them. The organic system of tags assigned by all users of a given web platform is called a folksonomy.  In contrast to traditional taxonomies painstak-ingly constructed by experts, a user can add any tags to a folksonomy. This leads to the greatest downside of tagging, inconsistency, which origi-nates in the synonymy and polysemy of human language, as well as in the varying degrees of specificity used by taggers (Golder and Huber-man, 2006). In traditional libraries, consistency is the primary evaluation criterion of indexing (Rolling, 1981). Much work has been done on describing the statistical properties of folksono-mies, such as tag distribution and co-occurrences (Halpin et al, 2007; Sigurbj?rnsson et al, 2008; Sood et al, 2007), but to our knowledge there has been none on assessing the actual quality of 
tags. How well do human taggers perform? How consistent are they with each other?  One potential solution to inconsistency in folksonomies is to use suggestion tools that automatically compute tags for new documents (e.g. Mishne, 2006; Sood et al, 2007; Heymann et al, 2008). Interestingly, the blooming research on automatic tagging has so far not been con-nected to work on keyphrase extraction (e.g. Frank et al, 1999; Turney, 2003; Hulth, 2004), which can be used as a tool for the same task (note: we use tag and keyphrase as synonyms). Instead of simple heuristics based on term fre-quencies and co-occurrence of tags, keyphrase extraction methods apply machine learning to determine typical distributions of properties common to manually assigned phrases, and can include analysis of semantic relations between candidate tags (Turney, 2003). How well do state-of-the-art keyphrase extraction systems per-form compared to simple tagging techniques? How consistent are they with human taggers? These are questions we address in this paper. Until now, keyphrase extraction methods have primarily been evaluated using a single set of keyphrases for each document, thereby largely ignoring the subjective nature of the task. Collaboratively tagged documents, on the other hand, offer multiple tag assignments by inde-pendent users, a unique basis for evaluation that we capitalize upon in this paper.  The experiments reported in this paper fill these gaps in the research on automatic tagging and keyphrase extraction. First, we analyze tag-ging consistency on the CiteULike.org platform for organizing academic citations. Methods tradi-tionally used for the evaluation of professional indexing will provide insight into the quality of this folksonomy. Next, we extract a high quality corpus from CiteULike, containing documents that have been tagged consistently by the best human taggers.  
1318
Following that, our goal is to build a system that matches the performance of these taggers. We first apply an existing approach proposed by Brooks and Montanez (2006) and compare it to the keyphrase extraction algorithm Kea (Frank et al, 1999). Next we create a new algorithm, called Maui, that enhances Kea?s successful ma-chine learning framework with semantic knowl-edge retrieved from Wikipedia, new features, and a new classification model. We evaluate Maui using tag sets assigned to the same documents by several users and show that it is as consistent with CiteULike users as they are with each other.  Most of the computation required for auto-matic tagging with this method can be performed offline. In practice, it can be used as a tag sug-gestion tool that provides users with tags describ-ing the main topics of newly added documents, which can then be corrected or enhanced by per-sonal tags if required. This will improve consis-tency in the folksonomy without compromising its flexibility. 2 Collaboratively-tagged Data  CiteULike.org is a bookmarking service that re-sembles the popular del.icio.us, but concentrates on scholarly papers. Rather than replicating the full text of tagged papers it simply points to them on the web (e.g. PubMed, CiteSeer, ScienceDi-rect, Amazon) or in journals (e.g. HighWire, Na-ture). This avoids violating copyright but means that the full text of articles is not necessarily available. When entering new resources, users are encouraged to assign tags describing their content or reflecting their own grouping of the information. However, the system does not sug-gest tags. Moreover, users do not see other users? tags and are thus not biased in their tag choices. 
2.1 Extracting a high quality tagged corpus The CiteULike data set is freely available and contains information about which documents were tagged with what tags by which users (al-though identities are not provided).  CiteULike?s 22,300 users have tagged 713,600 documents with 2.4M ?tag assignments?? sin-gle applications of a tag by a user to a document. The two most popular tags, bibtex-import and no-tag, indicate an information source and a missing tag respectively. Most of the remainder describe particular concepts relevant to the documents. We exclude non-content tags from our experiments, e.g. personal tags like to-read or todo. Note that spam entries have been elimi-nated from the data set.  Because CiteULike taggers are not profes-sional indexers, high quality of the assigned top-ics cannot be guaranteed. In fact, manual as-sessment of users? tags by human evaluators shows precision of 59% (Mishne, 2006) and 49% (Sood et al, 2006). However, why is the opinion of human evaluators valued more than the opin-ion of taggers? We propose an alternative way of determining ground truth using an automatic ap-proach to determine reliable tags: We concen-trate on a subset of CiteULike containing docu-ments that have been indexed with at least three tags on which at least two users have agreed.  In order to be able to measure the tagging con-sistency between the users, and then compare it to the algorithm?s consistency, we need taggers who have tagged documents that some others had tagged. We say that two users are ?co-taggers? if they have both tagged at least one common document. As well as restricting the document set, we only include taggers who have at least two co-taggers.  Figure 1 shows the proportions of CiteULike documents that are discarded in order to produce our high quality data set. The final set contains only 2,100 documents (0.3% of the original). Unfortunately, many of these are unavailable for download?for example, books at Amazon.com and ArXiv.org references cannot be crawled. We further restrict attention to two sources: High-Wire and Nature, both of which provide easily-accessible PDFs of the full text.  The result is a set of 180 documents indexed by 332 taggers. A total of 4,638 tags were as-signed by all taggers to documents in this set; however, the number of tags on which at least two users agreed is significantly smaller, namely 946. Still, this results in accurate tag sets that contain an average of five tags per document. 
 Figure 1. Quality control of CiteULike data  
1319
Note that traditionally much smaller data sets are used to assess consistency of human indexers, because such sets need to be created specifically for the experiment. Collaborative tagging plat-forms like CiteULike can be mined for large col-lections of this kind in natural settings.  Most documents in the extracted set relate to the area of bioinformatics. To give an example, a document entitled Initial sequencing and com-parative analysis of the mouse genome was tagged by eight users with a total of 22 tags. Four of them agreed on the tag mouse, but one used the broader term rodents. Three agreed on the tag genome, but one added genome paper, and an-other used the more specific comparative genom-ics. There are also cases when tags are written together, e.g. genomepaper, or with a prefix key genome, or in a different grammatical form: se-quence vs. sequencing. This example shows that many inconsistencies in tags are not caused by 
personalized tag choices as Chirita et al (2007) suggest, but rather stem from the lack of guide-lines and uniform tag suggestions that a book-marking service could provide. 2.2 Measuring tagging consistency Traditional indexers aim for consistency, on the basis that this will enhance document retrieval (Leonard, 1975). Consistency is measured using experiments in which several people index the same documents?usually a small set of a few dozen documents. It is computed for pairs of in-dexers, by formulae such as Rolling?s (1981):   , where C is the number of tags (index terms) in-dexers I1 and I2 have in common and A and B is the size of their tag sets respectively.  In our experiments, before computing the number of terms in common, we stem each tag with the Porter (1980) stemmer. For example, the overlap C between the tag sets {complex systems, network, small world} and {theoretical, small world, networks, dynamics} consist of the two tags {network, small world}, and the consistency is 2?2/(3+4) = 0.57.  To compute the overall consistency of a par-ticular indexer, this figure is averaged over all documents and co-indexers. There were no cases where the same user reassigned tags to the same articles, so computing intra-tagger consistency, although interesting, was not impossible. To our knowledge, traditional indexing consis-tency metrics have not yet been applied to col-laboratively tagged data. However, experiments on determining tagging quality do follow the same idea. For example, Xu et al (2006) define an authority metric that assigns high scores to those users who match other users? choices on the same documents, in order to eliminate spammers. 2.3 Consistency of CiteULike taggers In the collection of 180 documents tagged by 332 users described in Section 3.1, each tagger has 18 co-taggers on average, ranging from 2 to 129, and has indexed 1 to 25 documents. For each user we compute the consistency with all other users who tagged the same document. Consis-tency is then averaged across documents. We found that the distribution of per-user consis-tency resembles a power law with a few users achieving high consistency values and a long tail of inconsistent taggers. The maximum consis-
tagger co-taggers documents consistency 1 1 5 71.4 2 1 5 71.4 3 6 5 57.9 4 6 6 51.0 5 11 12 50.4 6 2 5 50.1 7 4 6 48.3 8 8 8 47.1 9 13 16 45.4 10 12 8 44.4 11 7 6 43.5 12 7 6 41.7 13 8 5 40.9 14 7 6 39.7 15 9 13 38.8 16 4 5 38.4 17 12 9 37.3 18 4 14 36.1 19 9 8 35.9 20 10 11 33.7 21 7 6 33.1 22 6 5 33.0 23 7 10 32.1 24 11 16 31.7 25 8 13 30.6 26 6 8 30.6 27 9 6 29.8 28 10 12 29.0 29 8 6 28.8 30 9 10 27.9 31 10 8 26.7 32 8 7 26.3 33 10 5 25.6 34 8 7 21.0 35 9 9 18.3 36 3 6 7.9 average 7.5 8.1 37.7 Table 1. Consistency of the most prolific and most consistent taggers  
1320
tency in this group is 92.3% and the average is 18.5%. The average consistency of the most pro-lific 70 indexers?those who have indexed at least five documents?is in the same range, namely 18.4%. The consistency of traditional approaches to free indexing is reported to be be-tween 4% and 67%, with an average of 27% de-pending on what aids are used (Leininger, 2000).  It is instructive to consider the group of best taggers. We define these as the ones who (a) ex-hibit greater than average consistency with all others, and (b) are sufficiently prolific, i.e. have tagged at least five documents. There are 36 such taggers; Table 1 lists their consistency within this group. The average consistency they achieve as a group is 37.7%, which is the similar to the aver-age consistency of professionals (Leininger, 2000). The above consistency analysis provides in-sight into the tagging quality of the best CiteULike users, based on HighWire and Nature articles. For the purposes of this paper, it shows how the tagging community can be restricted to a best-performing group of taggers by measuring their consistency. This is helpful for testing the performance of automatic tagging (Section 4.4).  3 Automatic tagging with Maui  Maui is a general algorithm for automatic topical indexing based on the Kea system (Frank et al, 1999).1 It works in two stages: candidate selec-tion and machine learning based filtering. In this paper, we apply it to automatic tagging. In the candidate selection stage, Maui first determines textual sequences defined by orthographic boundaries and splits these sequences into to-kens. Then all n-grams up to a maximum length of 3 words that do not begin or end with a stop-word are extracted as candidate tags. To reduce the number of candidates, all those that appear only once are discarded. This speeds up the train-ing and the extraction process without impacting the results. In the filtering stage several features are computed for each candidate, which are then input to a machine learning model to obtain the probability that the candidate is indeed a tag.  Maui?s architecture resembles that of many other supervised keyphrase extraction systems (Turney, 2000; Hulth 2004; Medelyan et al, 2008). However, this architecture has not previ-ously been applied to the task of automatic tag-ging.                                                   1 Maui is open-source and available for download  at http://maui-indexer.googlecode.com 
3.1 Features indicating significance We now describe the features used in the classi-fication model to determine whether a phrase is likely to be a tag. We begin with three baseline features used in Kea (Frank et al, 1999), and extend the set with three features that have been found useful in previous work. We also add three new features that have not been evaluated before: spread, semantic relatedness and inverse Wikipedia linkage. All Wikipedia-based features are computed using the WikipediaMiner toolkit.2  1. TF?IDF combines the frequency of a phrase in a particular document with its inverse occurrence frequency in general use (Salton and McGill, 1983). This score is high for rare phrases that appear frequently in a document and there-fore are more likely to be significant. 2. Position of the first occurrence is com-puted as the relative distance of the first occur-rence of the candidate tag from the beginning of the document. Candidates with very high or very low values are likely to be tags, because they appear either in the opening document parts such as title, abstract, table of contents, and introduc-tion, or in the document?s final sections such as conclusion and reference lists. 3. Keyphraseness quantifies how often a can-didate phrase appears as a tag in the training cor-pus. Automatic tagging approaches utilize the same information: Mishne (2006) and Sood et al (2006) automatically suggest tags previously as-signed to similar documents. However, in Maui (as in Kea) this feature is just one component of the overall model. Thus if a candidate never ap-pears as a keyphrase in the training corpus, it can still be extracted if its other feature values are significant enough.  4. Phrase length is measured in words. Gen-erally speaking, the longer the phrase, the more specific it is. Training captures and quantifies the specificity preference in a given training corpus.  5. Node degree quantifies the semantic relat-edness of a candidate tag to other candidates. Turney (2003) computes semantic relatedness using search engine statistics. Instead, following Medelyan et al (2008), we utilize Wikipedia hyperlinks for this task. We first map each can-didate phrase to its most common Wikipedia page. For example, the word Jaguar appears as a link anchor in Wikipedia 927 times. In 466 cases it links to the article Jaguar cars, thus the com-monness of this mapping is 0.5. In 203 cases it links to the animal description, a commonness of                                                  2 http://wikipedia-miner.sourceforge.net/ 
1321
0.22. We compute the node degree of the corre-sponding Wikipedia article as the number of hyperlinks that connect it to other Wikipedia pages that have been identified for other candi-date tags from the same document. A document that describes a particular topic will cover many related concepts, so high node degree?which indicates strong connectivity to other phrases in the same document?means that a candidate is more likely to be significant. 6. Wikipedia-based keyphraseness is the likelihood of a phrase being a link in the Wikipedia corpus. It divides the number of Wikipedia pages in which the phrase appears in the anchor text of a link by the total number of Wikipedia pages containing it. We multiply this number by the phrase?s document frequency. The new features proposed in this paper are the following: 7. Spread of a phrase is the distance between its first and last occurrences in a document. Both values are computed relative to the length of the document (see feature 2). High values help to determine phrases that are mentioned both in the beginning and at the end of a document.  8. Semantic relatedness of a phrase has al-ready been captured as the node degree (see fea-ture 5). However, recent research allows us to compute semantic relatedness with better tech-niques than mere hyperlink counts. Milne and Witten (2008) propose an efficient Wikipedia based approach that is nearly as accurate as hu-man subjects at quantifying the relationship be-tween two given concepts. Given a set of candi-date phrases we determine the most likely Wikipedia articles that describe them (as ex-plained in feature 5), and then determine the total relatedness of a given phrase to all other candi-dates. The higher the value, the more likely is the phrase to be a tag. 9. Inverse Wikipedia linkage is another fea-ture that utilizes Wikipedia as a source of lan-guage usage statistics. Here, again given the most likely Wikipedia article for a given phrase, we count the number of other Wikipedia articles that link to it and normalize this value as in in-verse document frequency:    where linksTo(AP) is the number of incoming links to the article A representing the candidate phrase P, and N is the total number of links in our Wikipedia snapshot (52M). This feature highlights those phrases that refer to concepts commonly used to describe other concepts.  
3.2 Machine learning in Maui In order to build the model, we use the subset of the CiteULike collection described in Section 3.1. For each document we know a set of tags that at least two users have agreed on. This is used as ground truth for building the model. For each training document, candidate phrases (i.e. n-grams) are identified and their feature values are calculated as described above.  Each candidate is then marked as a positive or negative example, depending on whether users have assigned it as a tag to the corresponding document. The machine-learning model is con-structed automatically from these labeled train-ing examples using the WEKA machine learning workbench. Kea (Frank et al, 1999) uses the Na?ve Bayes classifier, which implicitly assumes that the features are independent of each other given the classification. However, Kea uses only two or three features, whereas Maui combines nine features amongst which there are many ob-vious relationships, e.g. first occurrence and spread, or node degree and semantic relatedness. Consequently, we also consider bagged decision trees, which can model attribute interactions and do not require parameter tuning to yield good results. Bagging learns an ensemble of classifiers and uses them in combination, thereby often achieving significantly better results than the in-dividual classifiers (Breiman, 1996). Different trees are generated by sampling from the original dataset with replacement. Like Na?ve Bayes, bagged trees yield probability estimates that can be used to rank candidates. To select tags from a new document, Maui de-termines candidate phrases and their feature val-ues, and then applies the classifier built during training. This classifier determines the probabil-ity that a candidate is a tag based on relative fre-quencies observed from the training data.  4 Evaluation Here we describe the data used in the experi-ments and the results obtained, addressing the following questions:  1. How does a state-of-the-art keyphrase ex-traction method perform on collaboratively tagged data, compared to a baseline auto-matic tagging method?  2. What is the performance of Maui with old and new features?  3. How consistent are Maui?s tags compared to those assigned by human taggers? 
1322
4.1 Evaluation method The evaluation was performed using a set of 180 documents, described in Section 3.1, each tagged with at least three tags on which two users have agreed. In the following, unless explicitly stated otherwise, these are the only tags we use. We consider them to be ground truth. There are on average five such tags per document, and our goal is to extract tag sets that contain them all.  We regard a predicted tag as ?correct? if it matches one of the ground truth tags after using the Porter stemmer. We measure performance by computing Precision (the percentage of correct extracted tags out of all extracted), Recall (the percentage of correct extracted tags out of all correct) and F-Measure (the harmonic mean of the two). Given the set {yeast (4), network (3), regulation (2), metabolic (2)} of ground truth tags, where the numbers in parenthesis show how many users have assigned each one, and the set {network, metabolic, regulatory, ChIP-chip, transcription} of predicted tags, three out of five predicted terms are correct, yielding a precision of 60%, and three out of four ground-truth terms are extracted, a recall of 75%. The F-measure combining the two values is 67%. The reported precision and recall values are averaged over all test documents. We use 10-fold cross-validation for evaluation, which allows us to use all 180 documents as test documents with-out introducing optimistic bias in the perform-ance measures obtained. The results obtained in Sections 4.2 and 4.3 using this evaluation provide answers to the first two questions above. To answer the third we compare the indexing consistency of Maui to that of CiteULike users in Section 4.4. Here, we con-sider the assigned tag sets individually and com-pute the consistency of Maui with each tagger as described in Section 3.2. We compare Maui both to all 332 users who tagged these documents, and to the 36 best taggers identified in Section 3.3. 4.2 Keyphrase extraction vs. auto-tagging  As noted earlier, Brooks and Montanez (2006) automatically determine tags by extracting terms with the highest TF?IDF values for each post and argue that their quality is perhaps better than 
that of manual tags. Note that they only use one-word tags. We evaluate this approach using our 180 test documents and cross-validation, and compare the top five extracted tags with those assigned manually. Comparing the first two rows of Table 2 shows that using multi-word phrases as candidate tags (Section 4) is less accurate than using single words, which gives an overall F-Measure of 17%. Multi-words have higher TF?IDF values, but single words are the majority among the users? tags. The length feature applied in the next section helps to capture this character-istic, without compromising Maui?s ability to assign correct multi-words tags. Adding a second feature, the position of the first occurrence, and using Kea?s Na?ve Bayes model to learn their conditional distribution, im-proves the results by 5 percentage points (row 3). Adding the keyphraseness feature (row 4) nearly doubles the F-Measure, from 21.3 to 42.1%. This shows that CiteULike users tend to re-assign ex-isting tags.  4.3 Maui with additional features To evaluate Maui let us first consider the indi-vidual performance of old and new features, as shown in Table 3. Rows 1 to 3 evaluate the stan-dard features used by Frank et al (1999); Rows 4 to 6 evaluate features that were previously used in Kea for controlled indexing (Medelyan et al, 2006) and which we have adapted in Maui for free indexing. Rows 7 to 9 evaluate the three new features of Maui. The values can be compared to keyphrase extraction by chance (F-Measure = 1%) and to the multi-word TF?IDF baseline in Table 2, row 2 (F-Measure = 15.2%). The strength of these features varies from 2.1 to 25.5% (F-Measure). The strongest ones are key-phraseness, Wikipedia keyphraseness, TF?IDF and spread.  Table 4 demonstrates Maui?s performance when the features are combined and shows how the two different classifiers, Na?ve Bayes (left) and bagged decision trees (right), compare to 
   P R F 1 Top words based on TF?IDF  16.8 17.3 17.0 2 Top phrases based on TF?IDF  14.4 16.0 15.2 3 Kea (TF?IDF, 1st occur) 20.4 22.3 21.3 4 Kea (+keyphraseness) 41.1 43.1 42.1 Table 2. Baseline auto-tagging approach vs. Kea      P R F 1 TFxIDF 14.4 16 15.2 2 1st occurrence 5.4 5.4 5.4 3 Keyphraseness 25.2 26.3 25.5 4 Length 2.1 2.1 2.1 5 Node degree 8.3 9.0 8.6 6 Wikipedia keyphraseness 16.9 18.3 17.6 7 Spread 12.1 13.0 12.5 8 Semantic relatedness 7.1 7.3 7.2 9 Inverse Wikipedia linkage 7.3 6.8 7.0 Table 3. Evaluation of individual features  
1323
each other. The baseline in row 1 (left) shows Kea?s performance, using TF?IDF, first occur-rence, keyphraseness and Na?ve Bayes to com-bine them (same as row 4 in Table 2). Using de-cision trees with these three features does not improve the performance (row 1, right). The fol-lowing row combines the three original features with length, node degree and Wikipedia-based keyphraseness. In contrast to previous research (Medelyan et al, 2008), in this setting we do not observe an improvement with either Na?ve Bayes or bagged decision trees. In row 3 we combine the three original features with the three new ones introduced in this work. While Na?ve Bayes? values are lower than the baseline, with bagged decision trees Maui?s F-Measure im-proves from 41.2 to 44.9%. The best results are obtained by combining all nine features, again using bagged decision trees, giving in row 4 (right) a notably improved F-Measure of 47.1%. The recall of 48.6% shows that we match nearly half of all tags on which at least two human tag-gers have agreed.  Given this best combination of features, we eliminate each feature one by one starting from the individually weakest feature, in order to de-termine the contribution of each feature to this overall result. Table 5 compares the values and only bagged decision trees are used this time. The ?Difference? column quantifies the differ-ence between the best F-Measure achieved with all 9 features and excluding the one that is exam-ined in that row. Interestingly, one of the strong-est features, TF?IDF, is the one that contributes the least when all features are combined, while 
the contribution of the strongest feature?keyphraseness?is, as expected, the highest, add-ing 16.9 points. The second most important fea-ture is Wikipedia keyphraseness, contributing 4 percentage points to the overall result. Since some of the features in the best perform-ing combination rely on Wikipedia as a knowl-edge source, it is interesting to determine Wikipedia?s exact contribution. The last row of Table 5 combines the following features: TF?IDF, first occurrence, keyphraseness, length and spread. The F-Measure is 5.4 points lower than that of Maui with all 9 features combined. Therefore, the contribution of Wikipedia-based features is significant. 4.4 Maui?s consistency with human taggers In Section 2.3 we discussed the indexing consis-tency of CiteULike users on our data. There are a total of 332 taggers and their consistency with each other is 18.5%. Now, we use results ob-tained with Maui during the cross-validation, when all 9 features and bagged decision trees are used (Table 4, row 4, right; see examples in Ta-ble 5), and compute how consistent Maui is with each human user, based on whatever document this user has tagged. Then we average the results to obtain the overall consistency with all 332 users. Maui?s consistency with the 332 human tag-gers ranges from 0 to 80%, with an average of 23.8%. The only cases where very low consis-tency was achieved are those where the human has only assigned a few tags per document (one to three), or has some idiosyncratic tagging be-havior (for example, one tagger adds the word key in front of most tags). Still, with an average of 23.8%, Maui?s performance is over 5 points higher than that of an average CiteULike tagger (18.5%)?and note this group only includes tag-gers who have at least two co-taggers. In Section 2.3 we were also able to determine a smaller group of users who perform best and are most prolific. This group consists of 36 tag-gers whose consistency exceeds the average of the original 332 users. These 36 taggers have tagged a total of 143 documents with an average consistency of 37.6%. Maui?s consistency with 
  Na?ve Bayes Bagged decision trees   P R F P R F 1 Features 1 ? 3 41.1 43.1 42.1 40.3 42.2 41.2 2 Features 1 ? 6 38.9 41.1 40.0 40.3 42.6 41.4 3 Features 1 ? 3, 7 ? 9 39.3 41.1 40.2 43.7 46.2 44.9 4 Features 1 ? 9 37.6 39.6 38.6 45.7 48.6 47.1 Table 4. Combining all features in Maui   
Features F-Measure Difference All 9 Features 47.1  ? Length 45 2.1 ? 1st occurrence 45.6 1.5 ? Inverse Wikip linkage 45.1 2 ? Semantic relatedness 45.4 1.7 ? Node degree 46 1.1 ? Spread 46.4 0.7 ? TFxIDF 46.8 0.3 ? Wikip keyphraseness 43.1 4 ? Keyphraseness 30.2 16.9 Non-Wikip features 41.7 5.4 Table 5. Evaluation using feature elimination    
1324
these taggers ranges from 11.5% to 56%, with an average of 35%. This places it only 2.6 percent-age points behind the average performance of the best CiteULike taggers. In fact, it outperforms 17 of them (cf. Table 1). 4.5 Examples Table 6 compares Maui with some of CiteULike?s best human taggers on four ran-domly chosen test documents. Boldface in the taggers? row indicates a tag that has been chosen by at least two other human taggers; the remain-ing tags have been chosen by just one human. Boldface in Maui?s row shows tags that match human tags. For each document Maui extracts several tags assigned by at least two humans. The other tags it chooses are generally chosen by at least one human tagger, and even if not, they are still related to the main theme of the docu-ment. 5 Discussion and related work It is possible to indirectly compare the results of several previously published automatic tagging approaches with Maui?s. For each paper, we compute Maui?s results in settings closest to the reported ones. 
Brooks and Montanez (2006) extract terms with the highest TF?IDF values as tags for posts on technorati.com. They do not report precision and recall values for their system, but our re-implementation resulted in precision of 16.8% and recall of 17.3% for the top five assigned tags, compared to those agreed to by at least two CiteULike users on 180 documents. Adding eight additional features and combining them using machine learning gives a clear improve-ment?Maui achieves 45.7% and 48.7% preci-sion and recall respectively. Mishne (2006) uses TF?IDF-weighted terms as full-text queries to retrieve posts similar to the one being analyzed. Tags assigned to these posts are analyzed to retrieve the best ones using clus-tering and heuristic ranking; tags assigned by the given user receive extra weight. Mishne per-forms manual evaluation on 30 short articles and reports precision and recall for the top ten tags of 38% and 47% respectively. We matched Maui?s top ten terms to all tags assigned to 180 docu-ments automatically and obtained precision and recall of 44% and 29% respectively. (We believe that manual rather than automatic evaluation would be likely to give a far more favorable as-sessment of our system.) Chirita et al (2007) aim to extract personal-ized tags. Given a web page, they first retrieve 
Document 86865. Neural correlates of decision variables in parietal cortex. Platt and Glimcher. Nature 400,15 (1999) 44. Exploring complex networks. Strogatz. Nature 410, 8 (2001) 353537. Computational roles for dopamine in behavioural control. Mon-tague et al Nature 431, 14 (2004) 101. Network motifs: simple building blocks of complex networks. Milo et al Science 298, 824 (2002)  Tags assigned by CiteULike taggers decision making decisionmaking lip monkey neurophysiology reward Idiosyncratic: brain, choice, cortex, decision, electrophysiology, eye-movements, limitations, monkeys, neuroecono-mics, neurons, neuro-science, other, ppc, quals, reinforcementlearning complex complexity complex networks graph networks review small world social networks survey Idiosyncratic: 2001, adap-tive systems, bistability, coupled oscillator, graph mining, graphs, explorig, network biological, neu-rons, strogatz dopamine neuroscience reinforcement learning review  Idiosyncratic: action selection, attention, behavior, behavioral con-trol, cognitive control, learning, network, rein-forcementlearning, re-ward, td model applied math combinatorics  complexity motifs network original  sub graph pattern Idiosyncratic: 2002, datamining, data min-ing, graphs, link analy-sis, modularity, net paper, patterns, protein, science, sysbio, web characterization, web graph Tags assigned by Maui cortex decision lip monkey visual complex networks  networks review synchronization graph dopamine learning neuroscience review reward  complex networks network motifs gene complex Table 6. Tags assigned by CiteULike taggers and Maui to four sample documents  
1325
similar documents stored on the user?s desktop and then determine keywords for these docu-ments. They evaluate different term scoring techniques, such as term and document fre-quency, lexical dispersion, sentence scoring, and term co-occurrence. Like the Kea algorithm, the best formula combines term frequency with the position of the first occurrence of the term, nor-malized by page length. It yields a precision of 80% for the top four tags assigned to 30 large websites (32Kbytes), again evaluated manually. Our documents are considerably longer (47Kbytes) and thus more difficult to work with, nevertheless Maui achieves only slightly lower values, from 66% to 80%, when evaluating automatically against user-assigned tags. (The above caveat regarding automatic and manual assessment applies here too.) Budura et al (2008) develop a scoring for-mula that combines three features (tag frequency, tag co-occurrence and document similarity) and manually evaluate it on ten CiteULike docu-ments. Their precision for the top three to five tags ranges from 66% to 77%, slightly worse than in our paper (66% to 80%). The only reported automatic evaluation of tags was found in Sood et al (2006), where TagAssist was tested on 1000 blog posts. This algorithm is similar to Mishne?s (2006), but uses centroid-based clustering. Exact matching of TagAssist?s tags against existing ones yielded precision and recall of 13.1% and 22.8% respectively. This is substantially lower than Maui?s 45.75% and 48.7% obtained with best settings (Section 4.3). Note that this indirect comparison does not re-veal the true ranking of approaches, because their task definitions and test sets are slightly differ-ent. It would be interesting to compare other sys-tems on the multiple tagger set described in this paper, as we believe this would more objectively reflect the performance of humans and algo-rithms. 6 Conclusions This paper has introduced a systematic way of evaluating automatic tagging techniques without the need for manual inspection. We have shown how documents with multiple tag sets can be used in conjunction with a standard consistency measure to identify a robust test corpus for these techniques. Based on the evaluation methodol-ogy developed, we have shown that machine-learning-based automatic keyphrase extraction produces tag sets that exhibit consistency on a 
par with that achieved by the best human taggers. Our results also show a substantial improvement on an existing automatic tagging approach based on TF?IDF, and the results compare well to other systems.  The success of automatic keyphrase extraction depends primarily on the quality of the features that are provided to the machine learning algo-rithm involved. In this paper we have evaluated nine different features, including two novel Wikipedia-based semantic features, and found that their combination used in conjunction with bagged decision trees produces the best perform-ance. References  Breiman, L. 1996. Bagging predictors. Machine Learning 24(2): 123?140. Brooks, C. H. and N. Montanez. 2006. Improved an-notation of the blogosphere via autotagging and hierarchical clustering. In Proc. Int. Conf. on World Wide Web, Edinburgh, UK. pp. 625?632. New York, NY, USA. ACM Press. Budura, A., S. Michel, P. Cudre-Mauroux, and K. Aberer. 2008. To tag or not to tag - harvesting ad-jacent metadata in large-scale tagging systems. In Proc. Int. ACM SIGIR Conf. on Research and De-velopment in Information Retrieval, Singapore. pp. 733?734. New York, NY, USA: ACM Press. Chirita, P. A., S. Costache, W. Nejdl, and S. Hand-schuh. 2007. P-tag: large scale automatic genera-tion of personalized annotation tags for the web. In Proc. Int. Conf. on World Wide Web, Banff, Can-ada. pp. 845?854. New York, NY, USA: ACM Press. Frank, E., G. W. Paynter, I. H. Witten, C. Gutwin, and C. G. Nevill-Manning. 1999. Domain-specific keyphrase extraction. In Proc. of the 16th Interna-tional Joint Conference on Artificial Intelligence, Stockholm, Sweden. pp. 668?673. San Francisco, CA: Morgan Kaufmann Publishers.  Golder, S. A. and B. A. Huberman. 2006. Usage pat-terns of collaborative tagging systems. Journal of Information Science, 32(2): 198?208. Halpin, H., V. Robu, and H. Shepherd. 2007. The complex dynamics of collaborative tagging. In Proc. Int. Conf. on World Wide Web, pp. 211?220. New York, NY, USA: ACM Press. Heymann, P., D. Ramage, and H. Garcia-Molina. 2008. Social tag prediction. In Proc. Int. ACM SIGIR Conf. on Research and Development in In-formation Retrieval, Singapore. pp. 531?538. New York, NY, USA: ACM Press. 
1326
Hulth, A. 2004. Combining machine learning and natural language processing for automatic key-word extraction. Ph.D. thesis, Dep. of Computer and Systems Sciences, Stockholm University.  Leonard, L. E. 1975. Inter-indexer consistency and retrieval effectiveness: measurement of relation-ships. Ph.D. thesis, Grad. School of Library Sci-ence, Univ. of Illinois, Urbana-Champaign, IL. Leininger, K. 2000. Interindexer consistency in Psyc-Info. Journal of Librarianship and Information Science 32(1): 4?8. Medelyan, O., I. H. Witten and D. Milne. 2008. Topic indexing with Wikipedia. In Proc. of AAAI?08 Workshop on Wikipedia and Artificial Intelligence: an Evolving Synergy, Chicago, USA. pp. 19?24. Mishne, G. 2006. Autotag: a collaborative approach to automated tag assignment for weblog posts. In Proc. Int. Conf. on World Wide Web, Edinburgh, UK. pp. 953?954. New York, NY, USA. ACM Press Porter, M. F. 1980. An algorithm for suffix stripping, Program, 14(3): 130?137.   
Rolling, L. 1981. Indexing Consistency, Quality and Efficiency. Information Processing & Management 17(2): 69?76. Salton, G. and M. J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill New York. Sigurbj?rnsson, B. and R. van Zwol. 2008. Flickr tag recommendation based on collective knowledge. In Proc. Int. Conf. on World Wide Web, Beijing, China. pp. 327?336. New York, NY, USA: ACM Press. Sood, S., K. Hammond, S. Owsley, and L. Birnbaum. 2007. TagAssist: Automatic tag suggestion for blog posts. of Int. Conf. on Weblogs and Social Media, Boulder, Colorado. Menlo Park, CA.  Turney, P. D. 2003. Coherent keyphrase extraction via web mining. In Proc. of the 18th Int. Joint Conf. on Artificial Intelligence, Acapulco, Mexico. pp. 434?439. San Francisco, CA: Morgan Kauf-mann Publishers. Xu, Z., Fu, Y., Mao, J., and D. Su. 2006. Towards the Semantic Web: Collaborative tag suggestions. In Proc. Collaborative Web Tagging Workshop at the Int. Joint Conf. on Artificial Intelligence, Stock-holm, Sweden.   
1327
Proceedings of the ACL 2007 Student Research Workshop, pages 85?90,
Prague, June 2007. c?2007 Association for Computational Linguistics
Computing Lexical Chains with Graph Clustering
Olena Medelyan
Computer Science Department
The University of Waikato
New Zealand
olena@cs.waikato.ac.nz
Abstract
This paper describes a new method for 
computing lexical chains. These are 
sequences of semantically related words 
that reflect a text?s cohesive structure. In 
contrast to previous methods, we are able 
to select chains based on their cohesive
strength. This is achieved by analyzing the 
connectivity in graphs representing the 
lexical chains. We show that the generated 
chains significantly improve performance 
of automatic text summarization and 
keyphrase indexing. 
1 Introduction
Text understanding tasks such as topic detection, 
automatic summarization, discourse analysis and 
question answering require deep understanding of 
the text?s meaning. The first step in determining 
this meaning is the analysis of the text?s concepts
and their inter-relations. Lexical chains provide a 
framework for such an analysis. They combine
semantically related words across sentences into 
meaningful sequences that reflect the cohesive 
structure of the text. 
Lexical chains, introduced by Morris and Hirst 
(1991), have been studied extensively in the last 
decade, since large lexical databases are available 
in digital form. Most approaches use WordNet or 
Roget?s thesaurus for computing the chains and 
apply the results for text summarization.
We present a new approach for computing 
lexical chains by treating them as graphs, where 
nodes are document terms and edges reflect
semantic relations between them. In contrast to 
previous methods, we analyze the cohesive 
strength within a chain by computing the diameter 
of the chain graph. Weakly cohesive chains with a 
high graph diameter are decomposed by a graph 
clustering algorithm into several highly cohesive 
chains. We use WordNet and alternatively a 
domain-specific thesaurus for obtaining semantic 
relations between the terms.
We first give an overview of existing methods 
for computing lexical chains and related areas. 
Then we discuss the motivation behind the new 
approach and describe the algorithm in detail. Our 
evaluation demonstrates the advantages of using
extracted lexical chains for the task of automatic 
text summarization and keyphrase indexing, 
compared to a simple baseline approach. The 
results are compared to annotations produced by a 
group of humans.
2 Related Work
Morris and Hirst (1991) provide the theoretical 
background behind lexical chains and demonstrate 
how they can be constructed manually from
Roget?s thesaurus. The algorithm was re-
implemented as soon as digital WordNet and 
Roget?s became available (Barzilay and Elhadad, 
1997) and its complexity was improved (Silber and 
McCoy, 2002; Galley and McKeown, 2003). All 
these algorithms perform explicit word sense 
disambiguation while computing the chains. For 
each word in a document the algorithm chooses 
only one sense, the one that relates to members of 
existing lexical chains. Reeve et al (2006)
85
compute lexical chains with a medical thesaurus 
and suggest an implicit disambiguation: once the 
chains are computed, weak ones containing
irrelevant senses are eliminated. We also follow 
this approach.
One of the principles of building lexical chains 
is that each term must belong to exactly one chain. 
If several chains are possible, Morris and Hirst
(1991) choose the chain to whose overall score the
term contributes the most. This score is a sum over
weights of semantic relations between chain 
members. This approach produces different lexical 
chains depending on the order of words in the 
document. This is not justified, as the same content 
can be expressed with different sequences of 
statements. We propose an alternative order 
independent approach, where a graph clustering 
algorithm calculates the chain to which a term 
should belong.
3 Lexical Chains
The following notation is used throughout the 
paper. A lexical chain is a graph G = (V,E) with 
nodes vi?V being terms and edges (vi, vj, wij)?E
representing semantic relations between them, 
where wij is a weight expressing the strength of the 
relation. 1 A set of terms and semantic relations
building a graph is a valid lexical chain if the graph
is connected, i.e. there are no unconnected nodes 
and no isolated groups of nodes.
The graph distance d(vi, vj) between two nodes 
vi and vj is the minimum length of the path
connecting them. And the graph diameter is the 
?longest shortest distance? between any two nodes 
in a graph, defined as:
(1) ),(max , jivv vvdm ji? .
                                                
1 The initial experiments presented in this paper use an 
unweighted graph with wi,j = 1 for any semantic relation.
Because semantic relations are either bi-
directional or inverse, we treat lexical chains as 
undirected graphs.
3.1 The Cohesive Strength
Lexical cohesion is the property of lexical 
entities to ?stick together? and function as a whole 
(Morris and Hirst, 1991). How strongly the 
elements of a lexical chain ?stick together,? that is
the cohesive strength of the chain, has been 
defined as the sum of semantic relations between 
every pair of chain members (e.g. Morris and Hirst, 
1991; Silber and McCoy, 2002). This number 
increases with the length of a chain, but longer 
lexical chains are not necessarily more cohesive 
than shorter ones.
Instead, we define the cohesive strength as the 
diameter of the chain graph. Depending on their 
diameter we propose to group lexical chains as
follows: 
1. Strongly cohesive lexical chains (Fig. 1a) 
build fully connected graphs where each term is 
related to all other chain members and m = 1.
2. Weakly cohesive lexical chains (Fig. 1b) 
connect terms without cycles and with a diameter 
m = |V| ? 1.
3. Moderately cohesive lexical chains (Fig. 1c) 
are in-between the above cases with m ?[1, |V|? 1]. 
To detect individual topics in texts it is more 
useful to extract strong lexical chains. For 
example, Figure 1a describes ?physiographic 
features? and 1c refers to ?seafood,? while it is 
difficult to summarize the weak chain 1b with a 
single term. The goal is to compute lexical chains 
with the highest possible cohesion. Thus, the 
algorithm must have a way to control the selection. 
physiographic 
features
valleys    lowland           plains    lagoons
(a) strong m = 1
symptoms eyes
vision
senses
pain
(b) weak m = 4
shelfish
seafoods
squids
foods
fish
(c) average m = 2
Semantic relation:            broader term               sister term                related term
Figure 1. Lexical chains of different cohesive strength.
86
3.2 Computing Lexical Chains
The algorithm consists of two stages. First, we 
compute lexical chains in a text with only one 
condition: to be included into a chain a term needs
to be related to at least one of its members. Then, 
we apply graph clustering on the resulting weak 
chains to determine their strong subchains.
I. Determining all chains. First, the documents? 
n-grams are mapped onto terms in the thesaurus. 
To improve conflation we ignore stopwords and 
sort the remaining stemmed words alphabetically. 
Second, for each thesaurus term t that was found in 
the document we search for an appropriate lexical 
chain. We iterate over the list L containing
previously created chains and check whether term t
is related to any of the members of each chain. The 
following cases are possible:
1. No lexical chains were found. 
A new lexical chain with the term t as a 
single element is created and included in L.
2. One lexical chain was found. 
This chain is updated with the term t.
3. Two or more lexical chains were found.
We merge these chains into a single new 
chain, and remove the old chains from L.
II. Clustering within the weak chains.
Algorithms for graph clustering divide sparsely 
connected graphs into dense subgraphs with a 
similar diameter. We consider each lexical chain in 
L with diameter 3?m as a weak chain and apply 
graph clustering to identify highly cohesive 
subchains within this chain. The list L is updated 
with the newly generated chains and the original 
chain is removed. 
A popular graph clustering algorithm, Markov 
Clustering (MCL) is based on the idea that ?a 
random walk that visits a dense cluster will likely 
not leave the cluster until many of its vertices have 
been visited? (van Dongen, 2000). MCL is
implemented as a sequence of iterative operations
on a matrix representing the graph. We use 
ChineseWhispers (Biemann, 2006), a special case 
of MCL that performs the iteration in a more 
aggressive way, with an optimized linear 
complexity with the number of graph edges. 
Figure 2 demonstrates how an original weakly 
cohesive lexical chain has been divided by 
ChineseWhispers into five strong chains.
4 Lexical Chains for Text Summarization
Lexical chains are usually evaluated in terms of their 
performance on the automatic text summarization 
task, where the most significant sentences are 
extracted from a document into a summary of a 
predefined length. The idea is to use the cohesive 
information about sentence members stored in 
lexical chains. We first describe the summarization
approach and then compare results to manually 
created summaries.
4.1 Identifying the Main Sentences
The algorithm takes one document at a time and 
computes its lexical chains as described in Section 
3.2, using the lexical database WordNet. First, we 
consider all semantic senses of each document 
term. However, after weighting the chains we 
eliminate senses appearing in low scored chains.
Doran et al (2004) state that changes in 
weighting schemes have little effect on summaries.
We have observed significant differences between 
reported functions on our data and achieved best 
results with the formula produced by Barzilay and 
Elhadad (1997):
(2) ?? ?
?
???
LCt
LCt
tfreqtfreq
LCLCScore )())(
||1()(
Here, |LC| is the length of the chain and freq(t) is 
the frequency of the term t in the document. All 
lexical chains with score lower than a threshold 
contain irrelevant word senses and are eliminated.
Next we identify the main sentences for the final 
summary of the document. Different heuristics
have been proposed for sentence extraction based 
on the information in lexical chains. For each top
scored chain, Barzilay and Elhadad (1997) extract
econometrics
statistsical 
methods
economic
analysis
case
studies
methods
measurement
evaluation
statistical
data
data
analysis cartography
data
collection
surveys
censures
Figure 2. Clustering of a weak chain 
with ChineseWhispers.
87
  Rater 2
Positive Negative
Positive a bRater 1 Negative c d
Table 1. Possible choices for any two raters
that sentence which contains the first appearance 
of a chain member. Doran et al (2004) sum up the 
weights all words in the sentence, which 
correspond to the chain weights in which these 
words occur. We choose the latter heuristic 
because it significantly outperforms the former 
method in our experiments. 
The highest scoring sentences from the 
document, presented in their original order, form
the automatically generated summary. How many 
sentences are extracted depends on the requested 
summary length, which is defined as the
percentage of the document length.
4.2 Experimental Settings
For evaluation we used a subset of a manually 
annotated corpus specifically created to evaluate
text summarization systems (Hasler et al 2003). 
We concentrate only on documents with at least 
two manually produced summaries: 11 science and 
29 newswire articles with two summaries each, and
7 articles additionally annotated by a third person. 
This data allows us to compare the consistency of 
the system with humans to their consistency with 
each other. 
The results are evaluated with the Kappa 
statistic ?, defined for Table 1 as follows:
(3) ))(()9)((
)(2
badbcca
bcab
?????
???
It takes into account the probability of chance 
agreement and is widely used to measure inter-
rater agreement (Hripcsak and Rothshild, 2005). 
The ideal automatic summarization algorithm 
should have as high agreement with human 
subjects as they have with each other.
We also use a baseline approach (BL) to 
estimate the advantage of using the proposed
lexical chaining algorithm (LCA). It extracts text
summaries in exactly the manner described in 
Section 4.1, with the exception of the lexical 
chaining stage. Thus, when weighting sentences, 
the frequencies of all WordNet mappings are taken 
into account without the implicit word sense 
disambiguation provided by lexical chains.
Humans BL LCA
S1 0.19 0.2029 newswire 
articles S2 0.32 0.20 0.24
S1 0.08 0.1311 science
articles S2 0.34 0.13 0.22
Table 2. Kappa agreement on 40 summaries
vs. human 
2,3 and 1 vs. BL vs. LCA
human 1 0,41 0,30 0,30
human 2 0,38 0,22 0,24
human 3 0,28 0,17 0,24
average 0,36 0,23 0,26
Table 3. Kappa agreement on 7 newswire articles
4.3 Results
Table 2 compares the agreement among the human 
annotators and their agreement with the baseline 
approach BL and the lexical chain algorithm LCA. 
The agreement between humans is low, which 
confirms that sentence extraction is a highly 
subjective task. The lexical chain approach LCA 
significantly outperforms the baseline BL, 
particularly on the science articles.
While the average agreement of the LCA with 
humans is still low, the picture changes when we 
look at the agreement on individual documents.
Human agreement varies a lot (stdev = 0.24), while 
results produced by LCA are more consistent
(stdev = 0.18). In fact, for over 50% of documents 
LCA has greater or the same agreement with one 
or both human annotators than they with each 
other. The overall superior performance of humans 
is due to exceptionally high agreement on a few 
documents, whereas on another couple of 
documents LCA failed to produce a consistent 
summary with both subjects. This finding is similar 
to the one mentioned by Silber and McCoy (2002). 
Table 3 shows the agreement values for 7
newswire articles that were summarized by three 
human annotators. Again, LCA clearly 
outperforms the baseline BL. Interestingly, both 
systems have a greater agreement with the first 
subject than the first and the third human subjects 
with each other. 
5 Lexical Chains for Keyphrase Indexing
Keyphrase indexing is the task of identifying the 
main topics in a document. The drawback of 
conventional indexing systems is that they analyze
88
Professional Indexers
1 2 3 4 5 6 Avg
1 61 51 64 57 57 58
2 61 48 53 60 52 55
3 51 48 54 44 61 51
4 64 53 54 51 57 56
5 57 60 44 51 49 52
6 57 52 61 57 49 55
BL 42 39 37 39 39 35 39
LCA 43 42 40 40 39 40 41
Table 4. Topic consistency over 30 documents
document terms individually. Lexical chains enable
topical indexing, where first highly cohesive terms 
are organized into larger topics and then the main 
topics are selected. Properties of chain members 
help to identify terms that represent each 
keyphrases. To compute lexical chains and assign 
keyphrases this time we use a domain-specific 
thesaurus instead of WordNet.
5.1 Finding Keyphrases in Lexical Chains
The ranking of lexical chains is essential for 
determining the main topics of a document. Unlike 
in summarization, it should capture the specificity 
of the individual chains. Also, for some topics, e.g. 
proper nouns, the number of terms to express it can 
be limited; therefore we average frequencies over 
all chain members. Our measure of chain 
specificity combines TFIDFs and term length, 2
which boosts chains containing specific terms that 
are particularly frequent in a given document:
(4)
LC
tlengthtTFIDF
LCScore LCtLCt
??
??
?
?
)()(
)(
We assume that the top ranked weighted lexical 
chains represent the main topics in a document. To 
determine the keyphrases, for each lexical chain 
we need to choose a term that describes this chain 
in the best way, just as ?seafood? is the best 
descriptor for the chain in Figure 1c. 
Each member of the chain t is scored as follows: 
(5) )()()()( tlengthtNDtTFIDFtScore ???
where ND(t) is the node degree, or the number of 
edges connecting term t to other chain members. 
The top scored term is chosen as a keyphrase. 
                                                
2 Term length, measured in words, gives an indirect but 
simple measure of its specificity. E.g., ?tropical rain 
forests? is more specific than ?forests?.
Professional indexers tend to choose more than 
one term for a document?s most prominent topics. 
Thus, we extract the top two keyphrases from the 
top two lexical chains with |LC| ? 3. If the second 
keyphrase is a broader or a narrower term of the 
first one, this rule does not apply. 
5.2 Evaluation of the Extracted Keyphrases
This approach is evaluated on 30 documents 
indexed each by 6 professional indexers from the 
UN?s Food and Agriculture Organization. The 
keyphrases are driven from the agricultural 
thesaurus Agrovoc3 with around 40,000 terms and 
30,000 semantic relations between them.
The effectiveness of the lexical chains is shown 
in comparison to a baseline approach, which given 
a document simply defines keyphrases as Agrovoc 
terms with top TFIDF values. 
Indexing consistency is computed with the F-
Measure F, which can be expressed in terms of 
Table 1 (Section 4.1) as following:4
(6) cba
aF ??? 2
2
The overlap between two keyphrase sets a is 
usually computed by exact matching of keyphrases. 
However, discrepancies between professional 
human indexers show that there are no ?correct? 
keyphrases. Capturing main topics rather than 
exact term choices is more important. Lexical 
chains provide a way of measuring this so called 
topical consistency. Given a set of lexical chains 
extracted from a document, we first compute 
chains that are covered in its keyphrase set and 
then compute consistency in the usual manner.
5.3 Results
Table 4 shows topical consistency between each 
pair of professional human indexers, as well as 
between the indexers and the two automatic 
approaches, baseline BL and the lexical chain 
algorithm LCA, averaged over 30 documents.
The overall consistency between the human 
indexers is 55%. The baseline BL is 16 percentage 
points less consistent with the 6 indexers, while 
                                                
3 http://www.fao.org/agrovoc/
4 When vocabulary is large, the consistency is the same,
whether it is computed with the Kappa statistic or the F-
Measure (Hripcsak and Rothshild, 2005).
89
LCA is 1 to 5 percentage points more consistent 
with each indexer than the baseline. 
6 Discussion
Professional human indexers first perform 
conceptual analysis of a document and then 
translate the discovered topics into keyphrases. We 
show how these two indexing steps are realized 
with lexical chain approach that first builds an
intermediate semantic representation of a 
document and then translates chains into 
keyphrases. Conceptual analysis with lexical 
chains in text summarization helps to identify
irrelevant word senses. 
The initial results show that lexical chains 
perform better than baseline approaches in both 
experiments. In automatic summarization, lexical 
chains produce summaries that in most cases have 
higher consistency with human annotators than 
they with each other, even using a simplified 
weighting technique. Integrating lexical chaining 
into existing keyphrase indexing systems is a 
promising step towards their improvement. 
The lexical chaining does not require any 
resources other than a controlled vocabulary. We 
have shown that it performs well with a general 
lexical database and with a domain-specific 
thesaurus. We use the Semantic Knowledge 
Organization Standard 5 which allows easy inter-
changeability of thesauri. Thus, this approach is
domain and language independent.
7 Conclusions
We have shown a new method for computing 
lexical chains based on graph clustering. While 
previous chaining algorithms did not analyze the 
lexical cohesion within each chain, we force our 
algorithm to produce highly cohesive lexical 
chains based on the minimum diameter of the chain 
graph. The required cohesion can be controlled by 
increasing the diameter value and adjusting 
parameters of the graph clustering algorithm.
Experiments on text summarization and key-
phrase indexing show that the lexical chains
approach produces good results. It combines
symbolic analysis with statistical features and 
                                                
5 http://www.w3.org/2004/02/skos/
outperforms a purely statistical baseline. The 
future work will be to further improve the lexical 
chaining technique and integrate it into a more 
complex topical indexing system.
8 Acknowledgements
I would like to thank my PhD supervisors 
Ian H. Witten and Eibe Frank, as well as Gordon 
Paynter and Michael Poprat and the anonymous 
reviewers of this paper for their valuable comments. 
This work is supported by a Google Scholarship.
References
Chris Biemann 2006. Chinese Whispers?an Efficient 
Graph Clustering Algorithm and its Application to 
Natural Language Processing Problems. In Proc of 
the HLT-NAACL-06 Workshop on Textgraphs, pp. 
73-80.
Regina Barzilay and Michael Elhadad. 1997. Using 
Lexical Chains for Text Summarization, In Proc of 
the ACL Intelligent Scalable Text Summarization 
Workshop, pp. 10-17.
Stijn M. van Dongen. 2000. Graph Clustering by Flow 
Simulation. PhD thesis, University of Utrecht.
William P. Doran, Nicola Stokes, Joe Carthy and John
Dunnion. 2004. Assessing the Impact of Lexical 
Chain Scoring Methods on Summarization. In Proc of 
CICLING?04, pp. 627-635.
Laura Hasler, Constantin Orasan and Ruslan Mitkov. 
2003. Building Better Corpora for Summarization. In 
Proc of Corpus Linguistics CL?03, pp. 309-319.
George Hripcsak and Adam S. Rothschild. 2005. 
Agreement, the F-Measure, and Reliability in IR. 
JAMIA, (12), pp. 296-298.
Jane Morris and Graeme Hirst. 1991. Lexical Cohesion 
Computed by Thesaural Relations as an Indicator of 
the Structure of Text. Computational Linguistics, 
17(1), pp. 21-48.
Lawrence H. Reeve, Hyoil Han and Ari D. Brooks. 
2006.  BioChain: Using Lexical Chaining for 
Biomedical Text Summarization.  In Proc of the ACM 
Symposium on Applied Computing, pp. 180-184.
Gregory Silber and Kathleen McCoy, 2002. Efficiently 
Computed Lexical Chains as an Intermediate 
Representation for Automatic Text Summarization.
Computational Linguistics, vol. 28, pp. 487-496.
90
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 21?26,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 5: Automatic Keyphrase Extraction from Scientific
Articles
Su Nam Kim,? Olena Medelyan,? Min-Yen Kan? and Timothy Baldwin?
? Dept of Computer Science and Software Engineering, University of Melbourne, Australia
? Pingar LP, Auckland, New Zealand
? School of Computing, National University of Singapore, Singapore
sunamkim@gmail.com, medelyan@gmail.com,
kanmy@comp.nus.edu.sg, tb@ldwin.net
Abstract
This paper describes Task 5 of the
Workshop on Semantic Evaluation 2010
(SemEval-2010). Systems are to automat-
ically assign keyphrases or keywords to
given scientific articles. The participating
systems were evaluated by matching their
extracted keyphrases against manually as-
signed ones. We present the overall rank-
ing of the submitted systems and discuss
our findings to suggest future directions
for this task.
1 Task Description
Keyphrases
1
are words that capture the main top-
ics of a document. As they represent these key
ideas, extracting high-quality keyphrases can ben-
efit various natural language processing (NLP) ap-
plications such as summarization, information re-
trieval and question-answering. In summariza-
tion, keyphrases can be used as a form of se-
mantic metadata (Barzilay and Elhadad, 1997;
Lawrie et al, 2001; D?Avanzo and Magnini,
2005). In search engines, keyphrases can supple-
ment full-text indexing and assist users in formu-
lating queries.
Recently, a resurgence of interest in keyphrase
extraction has led to the development of several
new systems and techniques for the task (Frank
et al, 1999; Witten et al, 1999; Turney, 1999;
Hulth, 2003; Turney, 2003; Park et al, 2004;
Barker and Corrnacchia, 2000; Hulth, 2004; Mat-
suo and Ishizuka, 2004; Mihalcea and Tarau,
2004; Medelyan and Witten, 2006; Nguyen and
Kan, 2007; Wan and Xiao, 2008; Liu et al, 2009;
Medelyan, 2009; Nguyen and Phan, 2009). These
1
We use ?keyphrase? and ?keywords? interchangeably to
refer to both single words and phrases.
?
Min-Yen Kan?s work was funded by National Research
Foundation grant ?Interactive Media Search? (grant # R-252-
000-325-279).
have showcased the potential benefits of keyphrase
extraction to downstream NLP applications.
In light of these developments, we felt that this
was an appropriate time to conduct a shared task
for keyphrase extraction, to provide a standard as-
sessment to benchmark current approaches. A sec-
ond goal of the task was to contribute an additional
public dataset to spur future research in the area.
Currently, there are several publicly available
data sets.
2
For example, Hulth (2003) contributed
2,000 abstracts of journal articles present in In-
spec between the years 1998 and 2002. The data
set contains keyphrases (i.e. controlled and un-
controlled terms) assigned by professional index-
ers ? 1,000 for training, 500 for validation and
500 for testing. Nguyen and Kan (2007) col-
lected a dataset containing 120 computer science
articles, ranging in length from 4 to 12 pages.
The articles contain author-assigned keyphrases
as well as reader-assigned keyphrases contributed
by undergraduate CS students. In the general
newswire domain, Wan and Xiao (2008) devel-
oped a dataset of 308 documents taken from DUC
2001 which contain up to 10 manually-assigned
keyphrases per document. Several databases, in-
cluding the ACM Digital Library, IEEE Xplore,
Inspec and PubMed provide articles with author-
assigned keyphrases and, occasionally, reader-
assigned ones. Medelyan (2009) automatically
generated a dataset using tags assigned by the
users of the collaborative citation platform CiteU-
Like. This dataset additionally records how many
people have assigned the same keyword to the
same publication. In total, 180 full-text publi-
cations were annotated by over 300 users.
3
De-
spite the availability of these datasets, a standard-
ized benchmark dataset with a well-defined train-
2
All data sets listed below are available for
download from http://github.com/snkim/
AutomaticKeyphraseExtraction
3
http://bit.ly/maui-datasets
21
ing and test split is needed to maximize compara-
bility of results.
For the SemEval-2010 Task 5, we have
compiled a set of 284 scientific articles with
keyphrases carefully chosen by both their authors
and readers. The participants? task was to develop
systems which automatically produce keyphrases
for each paper. Each team was allowed to sub-
mit up to three system runs, to benchmark the
contributions of different parameter settings and
approaches. Each run consisted of extracting a
ranked list of 15 keyphrases from each docu-
ment, ranked by their probability of being reader-
assigned keyphrases.
In the remainder of the paper, we describe
the competition setup, including how data collec-
tion was managed and the evaluation methodol-
ogy (Section 2). We present the results of the
shared task, and discuss the immediate findings of
the competition in Section 3. In Section 4 we as-
sess the human performance by comparing reader-
assigned keyphrases to those assigned by the au-
thors. This gives an approximation of an upper-
bound performance for this task.
2 Competition Setup
2.1 Data
We collected trial, training and test data from
the ACM Digital Library (conference and work-
shop papers). The input papers ranged from 6
to 8 pages, including tables and pictures. To en-
sure a variety of different topics was represented
in the corpus, we purposefully selected papers
from four different research areas for the dataset.
In particular, the selected articles belong to the
following four 1998 ACM classifications: C2.4
(Distributed Systems), H3.3 (Information Search
and Retrieval), I2.11 (Distributed Artificial In-
telligence ? Multiagent Systems) and J4 (Social
and Behavioral Sciences ? Economics). All three
datasets (trial, training and test) had an equal dis-
tribution of documents from among the categories
(see Table 1). This domain specific information
was provided with the papers (e.g. I2.4-1 or H3.3-
2), in case participant systems wanted to utilize
this information. We specifically decided to strad-
dle different areas to see whether participant ap-
proaches would work better within specific areas.
Participants were provided with 40, 144, and
100 articles, respectively, in the trial, training and
test data, distributed evenly across the four re-
search areas in each case. Note that the trial data is
a subset of the training data. Since the original for-
mat for the articles was PDF, we converted them
into (UTF-8) plain text using pdftotext, and sys-
tematically restored full words that were originally
hyphenated and broken across two lines. This pol-
icy potentially resulted in valid hyphenated forms
having their hyphen (-) removed.
All collected papers contain author-assigned
keyphrases, part of the original PDF file. We addi-
tionally collected reader-assigned keyphrases for
each paper. We first performed a pilot annotation
task with a group of students to check the stabil-
ity of the annotations, finalize the guidelines, and
discover and resolve potential issues that may oc-
cur during the actual annotation. To collect the ac-
tual reader-assigned keyphrases, we then hired 50
student annotators from the Computer Science de-
partment of the National University of Singapore.
We assigned 5 papers to each annotator, esti-
mating that assigning keyphrases to each paper
should take about 10-15 minutes. Annotators were
explicitly told to extract keyphrases that actually
appear in the text of each paper, rather than to cre-
ate semantically-equivalent phrases, but could ex-
tract phrases from any part of the document (in-
cluding headers and captions). In reality, on av-
erage 15% of the reader-assigned keyphrases did
not appear in the text of the paper, but this is still
less than the 19% of author-assigned keyphrases
that did not appear in the papers. These values
were computed using the test documents only. In
other words, the maximum recall that the partici-
pating systems can achieve on these documents is
85% and 81% for the reader- and author-assigned
keyphrases, respectively.
As some keyphrases may occur in multiple
forms, in our evaluation we accepted two differ-
ent versions of genitive keyphrases: A of B ? B
A (e.g. policy of school = school policy) and A?s
B ? A B (e.g. school?s policy = school pol-
icy). In certain cases, such alternations change the
semantics of the candidate phrase (e.g., matter of
fact vs. ?fact matter). We judged borderline cases
by committee and do not include alternations that
were judged to be semantically distinct.
Table 1 shows the distribution of the trial, train-
ing and test documents over the four different re-
search areas, while Table 2 shows the distribution
of author- and reader-assigned keyphrases.
Interestingly, among the 387 author-assigned
22
Dataset Total Document Topic
C H I J
Trial 40 10 10 10 10
Training 144 34 39 35 36
Test 100 25 25 25 25
Table 1: Number of documents per topic in the
trial, training and test datasets, across the four
ACM document classifications
Dataset Author Reader Combined
Trial 149 526 621
Training 559 1824 2223
Test 387 1217 1482
Table 2: Number of author- and reader-assigned
keyphrases in the different datasets
keywords, 125 keywords match exactly with
reader-assigned keywords, while many more near-
misses (i.e. partial matches) occur.
2.2 Evaluation Method and Baseline
Traditionally, automatic keyphrase extraction sys-
tems have been assessed using the proportion of
top-N candidates that exactly match the gold-
standard keyphrases (Frank et al, 1999; Witten et
al., 1999; Turney, 1999). In some cases, inexact
matches, or near-misses, have also been consid-
ered. Some have suggested treating semantically-
similar keyphrases as correct based on simi-
larities computed over a large corpus (Jarmasz
and Barriere, 2004; Mihalcea and Tarau, 2004),
or using semantic relations defined in a the-
saurus (Medelyan and Witten, 2006). Zesch and
Gurevych (2009) compute near-misses using an n-
gram based approach relative to the gold standard.
For our shared task, we follow the traditional ex-
act match evaluation metric. That is, we match the
keyphrases in the answer set with those the sys-
tems provide, and calculate micro-averaged preci-
sion, recall and F-score (? = 1). In the evaluation,
we check the performance over the top 5, 10 and
15 candidates returned by each system. We rank
the participating systems by F-score over the top
15 candidates.
Participants were required to extract ex-
isting phrases from the documents. Since
it is theoretically possible to retrieve author-
assigned keyphrases from the original PDF arti-
cles, we evaluate the participating systems over
the independently-generated and held-out reader-
assigned keyphrases, as well as the combined set
of keyphrases (author- and reader-assigned).
All keyphrases in the answer set are stemmed
using the English Porter stemmer for both the
training and test dataset.
4
We computed a TF?IDF n-gram based baseline
using both supervised and unsupervised learning
systems. We use 1, 2, 3-grams as keyphrase can-
didates, used Na??ve Bayes (NB) and Maximum
Entropy (ME) classifiers to learn two supervised
baseline systems based on the keyphrase candi-
dates and gold-standard annotations for the train-
ing documents. In total, there are three baselines:
two supervised and one unsupervised. The per-
formance of the baselines is presented in Table 3,
where R indicates reader-assigned keyphrases and
C indicates combined (both author- and reader-
assigned) keyphrases.
3 Competition Results
The trial data was downloaded by 73 different
teams, of which 36 teams subsequently down-
loaded the training and test data. 21 teams partici-
pated in the final competition, of which two teams
withdrew their systems.
Table 4 shows the performance of the final 19
submitted systems. 5 teams submitted one run,
6 teams submitted two runs and 8 teams sub-
mitted the maximum number of three runs. We
rank the best-performing system from each team
by micro-averaged F-score over the top 15 can-
didates. We also show system performance over
reader-assigned keywords in Table 5, and over
author-assigned keywords in Table 6. In all these
tables, P, R and F denote precision, recall and F-
score, respectively.
The best results over the reader-assigned and
combined keyphrase sets are 23.5% and 27.5%,
respectively, achieved by the HUMB team. Most
systems outperformed the baselines. Systems also
generally did better over the combined set, as the
presence of a larger gold-standard answer set im-
proved recall.
In Tables 7 and 8, we ranked the teams by F-
score, computed over the top 15 candidates for
each of the four ACM document classifications.
The numbers in brackets are the actual F-scores
4
Using the Perl implementation available at http://
tartarus.org/
?
martin/PorterStemmer/; we in-
formed participants that this was the stemmer we would be
using for the task, to avoid possible stemming variations be-
tween implementations.
23
Method Top 5 candidates Top 10 candidates Top 15 candidates
by P R F P R F P R F
TF?IDF R 17.8% 7.4% 10.4% 13.9% 11.5% 12.6% 11.6% 14.5% 12.9%
C 22.0% 7.5% 11.2% 17.7% 12.1% 14.4% 14.9% 15.3% 15.1%
NB R 16.8% 7.0% 9.9% 13.3% 11.1% 12.1% 11.4% 14.2% 12.7%
C 21.4% 7.3% 10.9% 17.3% 11.8% 14.0% 14.5% 14.9% 14.7%
ME R 16.8% 7.0% 9.9% 13.3% 11.1% 12.1% 11.4% 14.2% 12.7%
C 21.4% 7.3% 10.9% 17.3% 11.8% 14.0% 14.5% 14.9% 14.7%
Table 3: Baseline keyphrase extraction performance for one unsupervised (TF?IDF) and two supervised
(NB and ME) systems
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 39.0% 13.3% 19.8% 32.0% 21.8% 26.0% 27.2% 27.8% 27.5%
WINGNUS 2 40.2% 13.7% 20.5% 30.5% 20.8% 24.7% 24.9% 25.5% 25.2%
KP-Miner 3 36.0% 12.3% 18.3% 28.6% 19.5% 23.2% 24.9% 25.5% 25.2%
SZTERGAK 4 34.2% 11.7% 17.4% 28.5% 19.4% 23.1% 24.8% 25.4% 25.1%
ICL 5 34.4% 11.7% 17.5% 29.2% 19.9% 23.7% 24.6% 25.2% 24.9%
SEERLAB 6 39.0% 13.3% 19.8% 29.7% 20.3% 24.1% 24.1% 24.6% 24.3%
KX FBK 7 34.2% 11.7% 17.4% 27.0% 18.4% 21.9% 23.6% 24.2% 23.9%
DERIUNLP 8 27.4% 9.4% 13.9% 23.0% 15.7% 18.7% 22.0% 22.5% 22.3%
Maui 9 35.0% 11.9% 17.8% 25.2% 17.2% 20.4% 20.3% 20.8% 20.6%
DFKI 10 29.2% 10.0% 14.9% 23.3% 15.9% 18.9% 20.3% 20.7% 20.5%
BUAP 11 13.6% 4.6% 6.9% 17.6% 12.0% 14.3% 19.0% 19.4% 19.2%
SJTULTLAB 12 30.2% 10.3% 15.4% 22.7% 15.5% 18.4% 18.4% 18.8% 18.6%
UNICE 13 27.4% 9.4% 13.9% 22.4% 15.3% 18.2% 18.3% 18.8% 18.5%
UNPMC 14 18.0% 6.1% 9.2% 19.0% 13.0% 15.4% 18.1% 18.6% 18.3%
JU CSE 15 28.4% 9.7% 14.5% 21.5% 14.7% 17.4% 17.8% 18.2% 18.0%
LIKEY 16 29.2% 10.0% 14.9% 21.1% 14.4% 17.1% 16.3% 16.7% 16.5%
UvT 17 24.8% 8.5% 12.6% 18.6% 12.7% 15.1% 14.6% 14.9% 14.8%
POLYU 18 15.6% 5.3% 7.9% 14.6% 10.0% 11.8% 13.9% 14.2% 14.0%
UKP 19 9.4% 3.2% 4.8% 5.9% 4.0% 4.8% 5.3% 5.4% 5.3%
Table 4: Performance of the submitted systems over the combined author- and reader-assigned keywords,
ranked by F-score
for each team. Note that in the case of a tie in
F-score, we ordered teams by descending F-score
over all the data.
4 Discussion of the Upper-Bound
Performance
The current evaluation is a testament to the gains
made by keyphrase extraction systems. The sys-
tem performance over the different keyword cat-
egories (reader-assigned and author-assigned) and
numbers of keyword candidates (top 5, 10 and 15
candidates) attest to this fact.
The top-performing systems return F-scores in
the upper twenties. Superficially, this number is
low, and it is instructive to examine how much
room there is for improvement. Keyphrase extrac-
tion is a subjective task, and an F-score of 100% is
infeasible. On the author-assigned keyphrases in
our test collection, the highest a system could the-
oretically achieve was 81% recall
5
and 100% pre-
cision, which gives a maximum F-score of 89%.
However, such a high value would only be possi-
ble if the number of keyphrases extracted per doc-
ument could vary; in our task, we fixed the thresh-
olds at 5, 10 and 15 keyphrases.
5
The remaining 19% of keyphrases do not actually appear
in the documents and thus cannot be extracted.
Another way of computing the upper-bound
performance would be to look into how well peo-
ple perform the same task. We analyzed the
performance of our readers, taking the author-
assigned keyphrases as the gold standard. The au-
thors assigned an average of 4 keyphrases to each
paper, whereas the readers assigned 12 on average.
These 12 keyphrases cover 77.8% of the authors?
keyphrases, which corresponds to a precision of
21.5%. The F-score achieved by the readers on the
author-assigned keyphrases is 33.6%, whereas the
F-score of the best-performing system on the same
data is 19.3% (for top 15, not top 12 keyphrases,
see Table 6).
We conclude that there is definitely still room
for improvement, and for any future shared tasks,
we recommend against fixing any threshold on the
number of keyphrases to be extracted per docu-
ment. Finally, as we use a strict exact matching
metric for evaluation, the presented evaluation fig-
ures are a lower bound for performance, as se-
mantically equivalent keyphrases are not counted
as correct. For future runs of this challenge, we
believe a more semantically-motivated evaluation
should be employed to give a more accurate im-
pression of keyphrase acceptability.
24
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 30.4% 12.6% 17.8% 24.8% 20.6% 22.5% 21.2% 26.4% 23.5%
KX FBK 2 29.2% 12.1% 17.1% 23.2% 19.3% 21.1% 20.3% 25.3% 22.6%
SZTERGAK 3 28.2% 11.7% 16.6% 23.2% 19.3% 21.1% 19.9% 24.8% 22.1%
WINGNUS 4 30.6% 12.7% 18.0% 23.6% 19.6% 21.4% 19.8% 24.7 22.0%
ICL 5 27.2% 11.3% 16.0% 22.4% 18.6% 20.3% 19.5% 24.3% 21.6%
SEERLAB 6 31.0% 12.9% 18.2% 24.1% 20.0% 21.9% 19.3% 24.1% 21.5%
KP-Miner 7 28.2% 11.7% 16.5% 22.0% 18.3% 20.0% 19.3% 24.1% 21.5%
DERIUNLP 8 22.2% 9.2% 13.0% 18.9% 15.7% 17.2% 17.5% 21.8% 19.5%
DFKI 9 24.4% 10.1% 14.3% 19.8% 16.5% 18.0% 17.4% 21.7% 19.3%
UNICE 10 25.0% 10.4% 14.7% 20.1% 16.7% 18.2% 16.0% 19.9% 17.8%
SJTULTLAB 11 26.6% 11.1% 15.6% 19.4% 16.1% 17.6% 15.6% 19.4% 17.3%
BUAP 12 10.4% 4.3% 6.1% 13.9% 11.5% 12.6% 14.9% 18.6% 16.6%
Maui 13 25.0% 10.4% 14.7% 18.1% 15.0% 16.4% 14.9% 18.5% 16.1%
UNPMC 14 13.8% 5.7% 8.1% 15.1% 12.5% 13.7% 14.5% 18.0% 16.1%
JU CSE 15 23.4% 9.7% 13.7% 18.1% 15.0% 16.4% 14.4% 17.9% 16.0%
LIKEY 16 24.6% 10.2% 14.4% 17.9% 14.9% 16.2% 13.8% 17.2% 15.3%
POLYU 17 13.6% 5.7% 8.0% 12.6% 10.5% 11.4% 12.0% 14.9% 13.3%
UvT 18 20.4% 8.5% 12.0% 15.6% 13.0% 14.2% 11.9% 14.9% 13.2%
UKP 19 8.2% 3.4% 4.8% 5.3% 4.4% 4.8% 4.7% 5.8% 5.2%
Table 5: Performance of the submitted systems over the reader-assigned keywords, ranked by F-score
System Rank Top 5 candidates Top 10 candidates Top 15 candidates
P R F P R F P R F
HUMB 1 21.2% 27.4% 23.9% 15.4% 39.8% 22.2% 12.1% 47.0% 19.3%
KP-Miner 2 19.0% 24.6% 21.4% 13.4% 34.6% 19.3% 10.7% 41.6% 17.1%
ICL 3 17.0% 22.0% 19.2% 13.5% 34.9% 19.5% 10.5% 40.6% 16.6%
Maui 4 20.4% 26.4% 23.0% 13.7% 35.4% 19.8% 10.2% 39.5% 16.2%
SEERLAB 5 18.8% 24.3% 21.2% 13.1% 33.9% 18.9% 10.1% 39.0% 16.0%
SZTERGAK 6 14.6% 18.9% 16.5% 12.2% 31.5% 17.6% 9.9% 38.5% 15.8%
WINGNUS 7 18.6% 24.0% 21.0% 12.6% 32.6% 18.2% 9.3% 36.2% 14.8%
DERIUNLP 8 12.6% 16.3% 14.2% 9.7% 25.1% 14.0% 9.3% 35.9% 14.7%
KX FBK 9 13.6% 17.6% 15.3% 10.0% 25.8% 14.4% 8.5% 32.8% 13.5%
BUAP 10 5.6% 7.2% 6.3% 8.1% 20.9% 11.7% 8.3% 32.0% 13.2%
JU CSE 11 12.0% 15.5% 13.5% 8.5% 22.0% 12.3% 7.5% 29.0% 11.9%
UNPMC 12 7.0% 9.0% 7.9% 7.7% 19.9% 11.1% 7.1% 27.4% 11.2%
DFKI 13 12.8% 16.5% 14.4% 8.5% 22.0% 12.3% 6.6% 25.6% 10.5%
SJTULTLAB 14 9.6% 12.4% 10.8% 7.8% 20.2% 11.3% 6.2% 24.0% 9.9%
Likey 15 11.6% 15.0% 13.1% 7.9% 20.4% 11.4% 5.9% 22.7% 9.3%
UvT 16 11.4% 14.7% 12.9% 7.6% 19.6% 11.0% 5.8% 22.5% 9.2%
UNICE 17 8.8% 11.4% 9.9% 6.4% 16.5% 9.2% 5.5% 21.5% 8.8%
POLYU 18 3.8% 4.9% 4.3% 4.1% 10.6% 5.9% 4.1% 16.0% 6.6%
UKP 19 1.6% 2.1% 1.8% 0.9% 2.3% 1.3% 0.8% 3.1% 1.3%
Table 6: Performance of the submitted systems over the author-assigned keywords, ranked by F-score
5 Conclusion
This paper has described Task 5 of the Workshop
on Semantic Evaluation 2010 (SemEval-2010), fo-
cusing on keyphrase extraction. We outlined the
design of the datasets used in the shared task and
the evaluation metrics, before presenting the offi-
cial results for the task and summarising the im-
mediate findings. We also analyzed the upper-
bound performance for this task, and demon-
strated that there is still room for improvement
over the task. We look forward to future advances
in automatic keyphrase extraction based on this
and other datasets.
References
Ken Barker and Nadia Corrnacchia. Using noun
phrase heads to extract document keyphrases. In
Proceedings of BCCSCSI : Advances in Artificial In-
telligence. 2000, pp.96?103.
Regina Barzilay and Michael Elhadad. Using lexi-
cal chains for text summarization. In Proceedings
of ACL/EACL Workshop on Intelligent Scalable Text
Summarization. 1997, pp. 10?17.
Ernesto D?Avanzo and Bernado Magnini. A
Keyphrase-Based Approach to Summarization: the
LAKE System at DUC-2005. In Proceedings of
DUC. 2005.
Eibe Frank and Gordon W. Paynter and Ian H. Witten
and Carl Gutwin and Craig G. Nevill-Manning. Do-
main Specific Keyphrase Extraction. In Proceedings
of IJCAI. 1999, pp.668?673.
Annette Hulth. Improved automatic keyword extrac-
tion given more linguistic knowledge. In Proceed-
ings of EMNLP. 2003, 216?223.
Annette Hulth. Enhancing Linguistically Oriented
Automatic Keyword Extraction. In Proceedings of
HLT/NAACL. 2004, pp. 17?20.
Mario Jarmasz and Caroline Barriere. Using semantic
similarity over tera-byte corpus, compute the perfor-
mance of keyphrase extraction. In Proceedings of
CLINE. 2004.
Dawn Lawrie and W. Bruce Croft and Arnold Rosen-
berg. Finding Topic Words for Hierarchical Summa-
rization. In Proceedings of SIGIR. 2001, pp. 349?
357.
25
Rank Group C Group H Group I Group J
1 HUMB(28.3%) HUMB(30.2%) HUMB(24.2%) HUMB(27.4%)
2 ICL(27.2%) WINGNUS(28.9%) SEERLAB(24.2%) WINGNUS(25.4%)
3 KP-Miner(25.5%) SEERLAB(27.8%) KP-Miner(22.8%) ICL(25.4%)
4 SZTERGAK(25.3%) KP-Miner(27.6%) KX FBK(22.8%) SZTERGAK(25.17%)
5 WINGNUS(24.2%) SZTERGAK(27.6%) WINGNUS(22.3%) KP-Miner(24.9%)
6 KX FBK(24.2%) ICL(25.5%) SZTERGAK(22.25%) KX FBK(24.6%)
7 DERIUNLP(23.6%) KX FBK(23.9%) ICL(21.4%) UNICE(23.5%)
8 SEERLAB(22.0%) Maui(23.9%) DERIUNLP(20.1%) SEERLAB(23.3%)
9 DFKI(21.7%) DERIUNLP(23.6%) DFKI(19.3%) DFKI(22.2%)
10 Maui(19.3%) UNPMC(22.6%) BUAP(18.5%) Maui(21.3%)
11 BUAP(18.5%) SJTULTLAB(22.1%) SJTULTLAB(17.9%) DERIUNLP(20.3%)
12 JU CSE(18.2%) UNICE(21.8%) JU CSE(17.9%) BUAP(19.7%)
13 Likey(18.2%) DFKI(20.5%) Maui(17.6%) JU CSE(18.6%)
14 SJTULTLAB(17.7%) BUAP(20.2%) UNPMC(17.6%) UNPMC(17.8%)
15 UvT(15.8%) UvT(20.2%) UNICE(14.7%) Likey(17.2%)
16 UNPMC(15.2%) Likey(19.4%) Likey(11.3%) SJTULTLAB(16.7%)
17 UNIC(14.3%) JU CSE(17.3%) POLYU(13.6%) POLYU(14.3%)
18 POLYU(12.5%) POLYU(15.8%) UvT(10.3%) UvT(12.6%)
19 UKP(4.4%) UKP(5.0%) UKP(5.4%) UKP(6.8%)
Table 7: System ranking (and F-score) for each ACM classification: combined keywords
Rank Group C Group H Group I Group J
1 ICL(23.3%) HUMB(25.0%) HUMB(21.7%) HUMB(24.7%)
2 KX FBK(23.3%) WINGNUS(23.5%) KX FBK(21.4%) WINGNUS(24.4%)
3 HUMB(22.7%) SEERLAB(23.2%) SEERLAB(21.1%) SZTERGAK(24.4%)
4 SZTERGAK(22.7%) KP-Miner(22.4%) WINGNUS(19.9%) KX FBK(24.4%)
5 DERIUNLP(21.5%) SZTERGAK(21.8%) KP-Miner(19.6%) UNICE(23.8%)
6 KP-Miner(21.2%) KX FBK(21.2%) SZTERGAK(19.6%) ICL(23.5%)
7 WINGNUS(20.0%) ICL(20.1%) ICL(19.6%) KP-Miner(22.6%)
8 SEERLAB(19.4%) DERIUNLP(20.1%) DFKI(18.5%) SEERLAB(22.0%)
9 DFKI(19.4%) DFKI(19.5%) SJTULTLAB(17.6%) DFKI(21.7%)
10 JU CSE(17.0%) SJTULTLAB(19.5%) DERIUNLP(17.3%) BUAP(19.6%)
11 Likey(16.4%) UNICE(19.2%) JU CSE(16.7%) DERIUNLP(19.0%)
12 SJTULTLAB(15.8%) Maui(18.1%) BUAP(16.4%) Maui(17.8%)
13 BUAP(15.5%) UNPMC(18.1%) UNPMC(16.1%) JU CSE(17.9%)
14 Maui(15.2%) Likey(16.9%) Maui(14.9%) Likey(17.5%)
15 UNICE(14.0%) UvT(16.4%) UNICE(14.0%) UNPMC(16.6%)
16 UvT(14.0%) POLYU(15.5%) POLYU(11.9%) SJTULTLAB(16.3%)
17 UNPMC(13.4%) BUAP(14.9%) Likey(10.4%) POLYU(13.3%)
18 POLYU(12.5%) JU CSE(12.6%) UvT(9.5%) UvT(13.0%)
19 UKP(4.5%) UKP(4.3%) UKP(5.4%) UKP(6.9%)
Table 8: System ranking (and F-score) for each ACM classification: reader-assigned keywords
Zhiyuan Liu and Peng Li and Yabin Zheng and Sun
Maosong. Clustering to Find Exemplar Terms for
Keyphrase Extraction. In Proceedings of EMNLP.
2009, pp. 257?266.
Yutaka Matsuo and Mitsuru Ishizuka. Keyword Ex-
traction from a Single Document using Word Co-
occurrence Statistical Information. International
Journal on Artificial Intelligence Tools. 2004, 13(1),
pp. 157?169.
Olena Medelyan and Ian H. Witten. Thesaurus based
automatic keyphrase indexing. In Proceedings of
ACM/IEED-CS JCDL. 2006, pp. 296?297.
Olena Medelyan. Human-competitive automatic topic
indexing. PhD Thesis. University of Waikato. 2009.
Rada Mihalcea and Paul Tarau. TextRank: Bringing
Order into Texts. In Proceedings of EMNLP. 2004,
pp. 404?411.
Thuy Dung Nguyen and Min-Yen Kan. Key phrase
Extraction in Scientific Publications. In Proceed-
ings of ICADL. 2007, pp. 317?326.
Chau Q. Nguyen and Tuoi T. Phan. An ontology-based
approach for key phrase extraction. In Proceedings
of the ACL-IJCNLP. 2009, pp. 181?184.
Youngja Park and Roy J. Byrd and Branimir Bogu-
raev. Automatic Glossary Extraction Beyond Termi-
nology Identification. In Proceedings of COLING.
2004, pp. 48?55.
Peter Turney. Learning to Extract Keyphrases from
Text. In National Research Council, Institute for In-
formation Technology, Technical Report ERB-1057.
1999.
Peter Turney. Coherent keyphrase extraction via Web
mining. In Proceedings of IJCAI. 2003, pp. 434?
439.
Xiaojun Wan and Jianguo Xiao. CollabRank: to-
wards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING.
2008, pp. 969?976.
Ian H. Witten and Gordon Paynter and Eibe
Frank and Car Gutwin and Graig Nevill-Manning.
KEA:Practical Automatic Key phrase Extraction.
In Proceedings of ACM conference on Digital li-
braries. 1999, pp. 254?256.
Torsten Zesch and Iryna Gurevych. Approximate
Matching for Evaluating Keyphrase Extraction. In
Proceedings of RANLP. 2009.
26
