Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 78?86,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Inductive Detection of Language Features via Clustering Minimal Pairs:
Toward Feature-Rich Grammars in Machine Translation
Jonathan H. Clark, Robert Frederking, Lori Levin
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jhclark,ref,lsl}@cs.cmu.edu
Abstract
Syntax-based Machine Translation systems
have recently become a focus of research
with much hope that they will outperform
traditional Phrase-Based Statistical Machine
Translation (PBSMT). Toward this goal, we
present a method for analyzing the mor-
phosyntactic content of language from an
Elicitation Corpus such as the one included in
the LDC?s upcoming LCTL language packs.
The presented method discovers a mapping
between morphemes and linguistically rele-
vant features. By providing this tool that
can augment structure-based MT models with
these rich features, we believe the discrimina-
tive power of current models can be improved.
We conclude by outlining how the resulting
output can then be used in inducing a mor-
phosyntactically feature-rich grammar for AV-
ENUE, a modern syntax-based MT system.
1 Introduction
Recent trends in Machine Translation have begun
moving toward the incorporation of syntax and
structure in translation models in hopes of gaining
better translation quality. In fact, some structure-
based systems have already shown that they can out-
perform phrase-based SMT systems (Chiang, 2005).
Still, even the best-performing data-driven systems
have not fully explored the depth of such linguistic
features as morphosyntax.
Certainly, many have brought linguistically moti-
vated features into their models in the past. Huang
and Knight (2006) explored relabeling of non-
terminal symbols to embed more information di-
rectly into the backbone of the grammar. Bonneau-
Maynard et al (2007) argue that incorporation of
morphosyntax in the form of a part of speech (POS)
language model can improve translation. While
these approaches do make use of various linguis-
tic features, we have only begun to scratch the sur-
face of what actually occurs in the languages of the
world. We wish to address such issues as case mark-
ing, subject-verb agreement, and numeral-classifier
agreement by providing models with information
about which morphemes correspond to which gram-
matical meanings.
2 Task Overview
Feature Detection is the process of determining from
a corpus annotated with feature structures (Figure 2)
which feature values (Figure 1) have a distinct rep-
resentation in a target language in terms of mor-
phemes (Figure 3). By leveraging knowledge from
the field of language typology, we know what types
of phenomena are possible across languages and,
thus, which features to include in our feature speci-
fication.
But not every language will display each of these
phenomena. Our goal is to determine which fea-
ture values (e.g. singular, dual, plural) have a dis-
tinct encoding in a given target language. Viewed
differently, we can ask which feature values can be
clustered by similarity. For instance, in Chinese, we
would expect singular, plural and dual to be mem-
bers of the same cluster (since they are typically not
explicitly expressed), while for Arabic we should
place each of these into separate clusters to indicate
they are each grammaticalized differently. Similarly,
78
Feature Name Feature Value Comment
np-gen m ,f, n Biological Gender
np-def +, - Definiteness
np-num sg, dl, pl Number
c-ten past, pres, fut Tense
np-function act, und Actor and undergoer participant roles
c-function main, rel Main and relative clause roles
Figure 1: An example feature specification.
ID Source Language Target Language Lexical Cluster Feature Structure
s1 He loves her. El ama a ella. `1 ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s2 She loves her. Ella ama a ella. `1 ((act (np-gen f) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s3 He loved her. El *ama a ella. `1 ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten past))
s4 The boy eats. El nin?o come. `2 ((act (np-gen m) (np-num sg) (np-def +)) (c-ten pres))
s5 The girl eats. La nin?a come. `2 ((act (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
s6 A girl eats. Una nin?a come. `2 ((act (np-gen f) (np-num sg) (np-def -)) (c-ten pres))
s7 The girls eat. Las nin?as comen. `2 ((act (np-gen f) (np-num pl) (np-def +)) (c-ten pres))
s8 The girls eat. Las nin?as comen. `2 ((act (np-gen f) (np-num dl) (np-def +)) (c-ten pres))
s9 Girls eat. Unas nin?as comen. `2 ((act (np-gen f) (np-num pl) (np-def -)) (c-ten pres))
Figure 2: An example of sentences that might be found in an elicitation corpus. Notice that each sentence differs from
some other sentence in the corpus by exactly one feature value. This enables us to see how the written form of the
language changes (or does not change) when the grammatical meaning changes.
English would have two clusters for the feature num-
ber: (singular) and (dual, plural). Further, we would
like to determine which morphemes express each of
these values (or value clusters). For example, En-
glish expresses negation with the morphemes no and
not, whereas questions are expressed by reordering
of the auxiliary verb or the addition of a wh-word.
Though many modern corpora contain feature-
annotated utterances, these corpora are often not
suitable for feature detection. For this purpose, we
use an Elicitation Corpus (see Figure 2), a corpus
that has been carefully constructed to provide a large
number of minimal pairs of sentences such as He
sings and She sings so that only a single feature (e.g.
gender) differs between the two sentences. Also, no-
tice that the feature structures are sometimes more
detailed than the source language sentence. For ex-
ample, English does not express dual number, but
we might want to include this feature in our Elicita-
tion Corpus (especially for a language such as Ara-
bic). For these cases, we include a context field for
the translator with an instruction such as ?Translate
this sentence as if there are two girls.?
In the past, we proposed deductive (rule-based)
methods for feature detection (Clark et al, 2008).
In this paper, we propose the use of inductive fea-
ture detection, which operates directly on the feature
set that the corpus has been annotated with, remov-
ing the need for manually written rules. We define
inductive feature detection as a recall-oriented task
since its output is intended to be analyzed by a Mor-
phosyntactic Lexicon Generator, which will address
the issue of precision. This, in turn, allows us to in-
form a rule learner about which language features
can be clustered and handled by a single set of rules
and which must be given special attention. How-
ever, due to the complexity of this component, de-
scribing it is beyond the scope of this paper. We also
note that future work will include the integration of a
morphology analysis system such as ParaMor (Mon-
son et al, 2007) to extract and annotate the valuable
morphosyntactic information of inflected languages.
An example of this processing pipeline is given in
Figure 4.
79
Feature Value Candidate Morphemes
np-gen m el, nin?o
np-gen f ella, nin?a
np-gen n *unobserved*
np-def + el, la, las
np-def - una, unas
np-num sg el, ella, la, una, come, nin?o, nin?a
np-num dl-pl las, unas, comen, nin?as
c-ten past-pres ?
c-ten fut *unobserved*
Figure 3: An example of the output of our system for the above corpus: a list of feature-morpheme pairings.
Elicitation
Corpus
Inductive
Feature
Detection
Morphosyntactic
Lexicon
Generator
Unsupervised
Morphology
Induction
Grammar
Rule
Learner
Decoder
Figure 4: An outline of the steps from an input Elicitation Corpus to the application of a morphosyntactically feature
rich grammar in a MT decoder. This paper discusses the highlighted inductive feature detection component. Note that
this is just one possible configuration for integrating inductive feature detection into system training.
3 The Need to Observe Real Data
One might argue that such information could be ob-
tained from a grammatical sketch of a language.
However, these sketches often focus on the ?inter-
esting? features of a language, rather than those that
are most important for machine translation. Fur-
ther, not all grammatical functions are encoded in
the elements that most grammatical sketches focus
on. According to Construction Grammar, such in-
formation is also commonly found in constructions
(Kay, 2002). For example, future tense is not gram-
maticalized in Japanese according to most reference
sources, yet it may be expressed with a construction
such as watashi wa gakoo ni iku yode desu (lit. ?I
have a plan to go to school.?) for I will go to school.
Feature detection informs us of such constructional-
ized encodings of language features for use in im-
proving machine translation models.
Recognizing the need for this type of data, the
LDC has included our Elicitation Corpus in their
Less Commonly Taught Languages (LCTL) lan-
guage packs (Simpson et al, 2008). Already, these
language packs have been translated into Thai, Ben-
gali, Urdu, Hungarian, Punjabi, Tamil, and Yoruba.
With structured elicitation corpora already being
produced on a wide scale, there exists plenty of data
that can be exploited via feature detection. Some of
these language packs have already been released for
use in MT competitions and they will start being re-
leased to the general research community this year
through LDC?s catalog.
4 Applications
4.1 Induction of Feature-Rich Grammars
Given these outputs, a synchronous grammar in-
duction system can then use these feature-annotated
morphemes and the knowledge of which features are
expressed to create a feature rich grammar. Consider
the example in Figure 5, which shows Urdu subject-
verb agreement taking place while being separated
by 12 words. Traditional n-gram Language Mod-
els (LM?s) would not be able to detect any disagree-
ments more than n words away, which is the nor-
mal case for a trigram LM. Even most syntax-based
systems would not be able to detect this problem
without using a huge number of non-terminals, each
marked for all possible agreements. A syntax-based
system might be able to check this sort of agree-
80
ek talb alm arshad jo mchhlyoN ke liye pani maiN aata phink raha tha . . .
a.SG student named Irshad who fish for water in flour throw PROG.SG.M be.PAST.SG.M
?A student named Irshad who was throwing flour in the water for the fish . . . ?
Figure 5: A glossed example from parallel text in LDC?s Urdu-English LCTL language pack showing subject-verb
agreement being separated by 12 words.
ment if it produced a target-side dependency tree as
in Ding and Palmer (2005). However, we are not
aware of any systems that attempt this. Therefore,
the correct hypotheses, which have correct agree-
ment, will likely be produces as hypotheses of tra-
ditional beam-search MT systems, but their features
might not be able to discern the correct hypothe-
sis, allowing it to fall below the 1-best or out of the
beam entirely. By constructing a feature-rich gram-
mar in a framework that allows unification-based
feature constraints such as AVENUE (Carbonell et
al., 2002), we can prune these bad hypotheses lack-
ing agreement from the search space.
Returning to the example of subject-verb agree-
ment, consider the following Urdu sentences taken
from the Urdu-English Elicitation Corpus in LDC?s
LCTL language pack:
Danish ne Amna ko sza di
Danish ERG Amna DAT punish give.PERF
?Danish punished Amna.?
Danish Amna ko sza dita hai
Danish Amna DAT punish give.HAB be.PRES
?Danish punishes Amna.?
These examples show the split-ergativity of Urdu
in which the ergative marker ne is used only for
the subject of transitive, perfect aspect verbs. In
particular, since these sentences have the perfect
aspect marked on the light verb di, a closed-class
word (Poornima and Koenig, 2008), feature detec-
tion will allow the induction of a grammar that per-
colates a feature up from the VP containing di in-
dicating that its aspect is perfect. Likewise, the NP
containing Danish ne will percolate a feature up in-
dicating that the use of ne requires perfect aspect.
If, during translation, a hypothesis is proposed that
does not meet either of these conditions, unification
will fail and the hypothesis will be pruned 1.
Certainly, unification-based grammars are not the
1If the reader is not familiar with Unification Grammars, we
recommend Kaplan (1995)
only way in which this rich source of linguistic infor-
mation could be used to augment a structure-based
translation system. One could also imagine a system
in which the feature annotations are simply used to
improve the discriminative power of a model. For
example, factored translation models (Koehn and
Hoang, 2007) retain the simplicity of phrase-based
SMT while adding the ability to incorporate addi-
tional features. Similarly, there exists a continuum
of degrees to which this linguistic information can
be used in current syntax-based MT systems. As
modern systems move toward integrating many fea-
tures (Liang et al, 2006), resources such as this will
become increasingly important in improving trans-
lation quality.
5 System Description
In the following sections, we will describe the pro-
cess of inductive feature detection by way of a run-
ning example.
5.1 Feature Specification
The first input to our system is a feature specification
(Figure 1). The feature specification used for this ex-
periment was written by an expert in language typol-
ogy and is stored in a human-readable XML format.
It is intended to cover a large number of phenom-
ena that are possible in the languages of the world.
Note that features beginning with np- are partici-
pant (noun) features while features beginning with
c- are clause features. The feature specification al-
lows us to know which values are unobserved during
elicitation (that is, no sentence having that feature
value was given to the bilingual person to translate).
This is the case for the first four features and their
values in Figure 1. The last two function features
and their values tell us what possible roles partici-
pants and clauses can take in sentences.
81
5.2 Elicitation Corpus
As outlined in Section 3, feature detection uses an
Elicitation Corpus (see Figure 2), a corpus that has
been carefully constructed to provide a large num-
ber of minimal pairs of sentences such as He sings
and She sings so that only a single feature (e.g. gen-
der) differs between the two sentences (Levin et al,
2006; Alvarez et al, 2006). If two features had var-
ied at once (e.g. It sang) or lexical choice varied
(e.g. She reads), then making assertions about which
features the language does and does not express be-
comes much more difficult.
Notice that each input sentence has been tagged
with an identifier for a lexical cluster as a pre-
processing step. Specifying lexical clusters ensures
that we don?t compare sentences with different con-
tent just because their feature structures match. For
example, we would not want to compare Dog bites
man and Man bites dog nor The student snored
and The professor snored. Note that bag-of-words
matching is insufficient for this purpose.
Though any feature-annotated corpus can be used
in feature detection, the amount of useful informa-
tion extracted from the corpus is directly dependent
on how many minimal pairs can be formed from the
corpus. For instance, one might consider using a
morphologically annotated corpus or even an auto-
matically parsed corpus in place of the elicitation
corpus. Even though these resources are likely to
suffer from having very sparse minimal pairs due to
their uncontrolled usage of vocabulary, they might
still contain some amount of useful information.
However, since we seek both to apply these methods
to language for which there are currently no man-
ually annotated corpora and to investigate features
that existing parsers generally cannot identify (e.g.
generic nouns and evidentiality), we will not men-
tion these types of resources any further.
5.3 Minimal Pair Clustering
Minimal pair clustering is the process of grouping
all possible sets of minimal pairs, those pairs of sen-
tences that have exactly one difference between their
feature structures. We use wildcard feature struc-
tures to represent each minimal pair cluster. We de-
fine a wildcard feature as any feature whose value
is *, which denotes that the value matches another *
rather than its original feature value. Similarly, we
define the feature context of the wildcard feature be
the enclosing participant and clause type for a np-
feature or the enclosing clause for a c- type fea-
ture. Then, for each sentence s in the corpus, we
substitute a wildcard feature for each of the values v
in its feature structure, and we append s to the list
of sentences associated with this wildcard feature
structure. A sample of some of the minimal pairs
for our running example are shown in Figure 6.
Here, we show minimal pairs for just one wild-
card, though multiple wildcards may be created if
one wishes to examine how features interact with
one another. This could be useful in cases such as
Hindi where the perfective verb aspect interacts with
the past verb tense and the actor NP function to add
the case marker ne (for split ergativity of Urdu, see
Section 4.1). That said, a downstream component
such as a Morphosyntactic Lexicon Generator would
perhaps be better suited for the analysis of feature in-
teractions. Also, note that the feature context is not
used when there is only one wildcard feature. The
feature context becomes useful when multiple wild-
cards are added in that it may also act as a wildcard
feature.
The next step is to organize the example sentences
into a table that helps us decide which examples can
be compared and stores information that will inform
our comparison. Briefly, any two sentences belong-
ing to the same minimal pair cluster and lexical clus-
ter will eventually get compared. As specified in Al-
gorithm 1, we create a table like that in Figure 7.
Having collected this information, we are now ready
to begin clustering feature values.
Algorithm 1 Organize()
Require: Minimal pairs, lexical clusters, and the
feature specification.
Ensure: A table T of comparable examples.
for all pair m ? minimalPairs do
for all sentence s ? m do
f? wildcardFeature(s, m)
v? featureValue(s, f)
c? featureContext(m)
`? lexCluster(s)
T[f,m, c, `, v]? T[f,m, c, `, v]? s
return T
82
ID Set Members Feature Feature Context Feature Structure
m1 {s1, s2} np-gen ((act)) ((act (np-gen *) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten pres))
m2 {s1, s3} np-ten () ((act (np-gen m) (np-num sg) (np-def +))
(und (np-gen f) (np-num sg) (np-def +)) (c-ten *))
m3 {s4, s5, s7, s8} np-gen ((act)) ((act (np-gen *) (np-num sg) (np-def +)) (c-ten pres))
m4 {s5, s7, s8} np-num ((act)) ((act (np-gen f) (np-num *) (np-def +)) (c-ten pres))
m5 {s6, s9} np-num ((act)) ((act (np-gen f) (np-num *) (np-def -)) (c-ten pres))
m6 {s5, s6} np-def ((act)) ((act (np-gen f) (np-num sg) (np-def *)) (c-ten pres))
m7 {s7, s9} np-def ((act)) ((act (np-gen f) (np-num pl) (np-def *)) (c-ten pres))
Figure 6: An example subset of minimal pairs that can be formed from the corpus in Figure 2.
Feature Min. Pair Feat. Context Lex. Cluster Feat. Value. Sentence
np-gen m1 ((act)) `1 m s1
np-gen m1 ((act)) `1 f s2
np-ten m2 () `1 pres s1
np-ten m2 () `1 past s3
np-num m4 ((act)) `2 sg s5
np-num m4 ((act)) `2 pl s7
np-num m4 ((act)) `2 dl s8
np-num m5 ((act)) `2 sg s6
np-num m5 ((act)) `2 pl s9
Figure 7: An example subset of the organized items that can be formed from the minimal pairs in Figure 6. Each item
that has a matching minimal pair ID, feature context, and lexical cluster ID can be compared during feature detection.
5.4 Feature Value Clustering
During the process of feature value clustering, we
collapse feature values that do not have a distinct
encoding in the target language into a single group.
This is helpful both as information to components
using the output of inductive feature detection and
later as a method of reducing data sparseness when
creating morpheme-feature pairings. We represent
the relationship between the examples we have gath-
ered for each feature as a feature expression graph.
We define a feature expression graph (FEG) for a
feature f to be a graph on |v| vertices where v is
the number of possible values of f (though for most
non-trivial cases, it is more conveniently represented
as a triangular matrix).
Each vertex of the FEG corresponds to a feature
value (e.g. singular, dual) while each arc contains
the list of examples that are comparable according
to the table from the previous step. The examples at
each arc are organized into those that had the same
target language string, indicating that the feature val-
ues are not distinctly expressed, and those that had
a different target language string, indicating that the
change in grammatical meaning represented in the
feature structure has a distinct encoding in the tar-
get language. Algorithm 2 more formally specifies
the creation of a FEG. The FEG?s for our running
example are shown in Figure 8. From these statis-
tics generated from these graphs, we then estimate
the maximum likelihood probability of each feature
value pair being distinctly encoded as shown in Fig-
ure 9.
The interpretation of these probabilities might not
be obvious. They estimate the likelihood of a lan-
guage encoding a feature given that the meaning of
that feature is intended to be conveyed. These proba-
bilities should not be interpreted as a traditional like-
lihood of encountering a given lexical item.
Finally, we cluster by randomly selecting a start-
ing vertex for a new cluster and adding vertices to
that cluster, following arcs out from the cluster that
have a weight lower than some threshold ?. When
no more arcs may be followed, a new start vertex is
selected and another cluster is formed. This is re-
peated until all feature values have been assigned to
a cluster. For our running example, we use ? = 0.6,
83
fm
n
{(s1, s2, NEQ), (s4, s5, NEQ), 
(s4, s7, NEQ), (s4, s8, NEQ)}
np-gen
{} {}
pls
dl
{(s5,s7, NEQ), (s6, s9, NEQ)}
{(s5, s8, NEQ)}
{(s7, s8, EQ)}
np-num
-+
{(s5, s6, NEQ), 
(s7, s9, NEQ))}
np-def
prespast
fut
{(s1, s2, NEQ)}
c-ten
{} {}
Figure 8: An example subset of the Feature Expression Graphs that are formed from the minimal pairs in Figure 7.
fm
n
| arcs[m,f] with (s
m
,s
f
,x,NEQ) |
| arcs[m,f] |
| arcs[m,n] with (s
m
,s
n
,x,NEQ) |
| arcs[m,n] |
| arcs[f,n] with (s
f
,s
n
,x,NEQ) |
| arcs[f,n] |
Figure 9: An example of how probabilities are estimated for each feature value pair in a Feature Expression Graph for
the feature np-gender.
Algorithm 2 Collecting statistics for each FEG.
Require: The table T from the previous step.
Ensure: A complete graph as an arc list with the
observed similarities and differences for each fea-
ture value.
for all si, sj ? T s.t. (mi, ci, `i) = (mj , cj , `j)
do
(vi, vj)? (featureValue(si), featureValue(sj))
if tgt(si) = tgt(sj) then
arcs[vi, vj ]? arcs[vi, vj ] ? (si, sj ,m,EQ)
else
arcs[vi, vj ]? arcs[vi, vj ] ? (si, sj ,m,NEQ)
return arcs
which results in the following clusters being formed:
np-gen: m, f
np-num: s, pl/dl
np-def: +, -
c-ten: past, pres
5.5 Morpheme-Feature Pairing
Finally, using the information from above about
which values should be examined as a group and
which sentence pairs exemplify an orthographic dif-
ference, we examine each pair of target language
sentences to determine which words changed to re-
flect the change in grammatical meaning. This pro-
cess is outlined in Algorithm 3. The general idea is
that for each arc going out of a feature value vertex
we examine all of the target language sentence pairs
that expressed a difference. We then take the words
that were in the vocabulary of the target sentence
for the current feature value, but not in the sentence
it was being compared to and add them to the list
of words that could be used to express this feature
value (Figure 3).
6 Evaluation and Results
We evaluated the output of feature detection with
one wildcard feature as applied to the Elicitation
Corpus from the LDC?s Urdu-English LCTL lan-
guage pack. Threshold parameters were set to small
values (? = 0.05). Note that an increase in precision
might be possible by tuning this value; however, as
stated, we are most concerned with recall.
An initial attempt was made to create a gold stan-
dard against which recall could be directly calcu-
lated. However, the construction of this gold stan-
dard was both noisier and more time consuming
than expected. That is, even though the task is
based on how a linguistic field worker might col-
84
Algorithm 3 Determine which morphemes are as-
sociated with which feature values.
Require: List of clusters C and list of FEGs F
Ensure: A list of morphemes associated with each
feature value
for all feature ? F do
for all vertex ? feature do
for all arc ? vertex do
for all (s1, s2,m,NEQ) ? arc do
v1 ? featureValue(s1,m)
v2 ? featureValue(s2,m)
if v1 6= v then (s1, v1)? (s2, v2)
w1 ? vocabulary(s1)
w2 ? vocabulary(s2)
? ?W1 ?W2
for all w ? freq do
freq[w]++
for all w ? freq do
p = freq[w] / ?w freq[w]
if p ? ?? then
morphemes[v]? morphemes[v]? w
return morphemes
lect data, it was more difficult for a human than
anticipated. Therefore, we instead produced a list
of hypothesized morpheme-feature pairs and had a
human trained in linguistics who was also bilingual
in Hindi/Urdu-English mark each pair as ?Correct,?
?Incorrect,? or ?Ambiguous.? The results of this
evaluation are summarized in Figure 10. The reader
may be surprised by how many incorrect hypothe-
ses were generated, given the controlled nature of
the Elicitation Corpus. However, there are two im-
portant factors to consider. First, features can in-
teract in complex and often unexpected ways. For
instance, in English, the only feature difference in
minimal pair Cats yawned and A cat yawned is the
number of the actor. However, this causes an in-
teraction with definiteness that would cause the pre-
sented algorithms to associate a with the number of
nouns even though it is canonically associated with
definiteness. Second, the bilingual people translat-
ing the Elicitation Corpus are prone to make errors.
Though a fair number of incorrect hypotheses
were produced, the number of correct hypotheses
is encouraging. We also note that the words be-
ing identified are largely function words and multi-
Judgment Morpheme-Feature Pairings
Correct 68
Ambiguous 29
Incorrect 109
TOTAL 206
Figure 10: The results of feature detection. Being a
recall-oriented approach, inductive feature detection is
geared toward overproduction of morpheme-feature pair-
ings as shown in the number of ambiguous and incorrect
pairings.
morpheme tokens from which closed-class func-
tional morphemes will be extracted. One might
think the counts extracted seem low when compared
to the typical MT vocabulary size, but these function
words that we extract cover a much larger probabil-
ity mass of the language than content words.
We are confident that the Morphosyntactic Lex-
icon Generator designed to operate directly down-
stream from this process will be sufficiently discrim-
inant to use these morpheme-feature pairings to cre-
ate a high precision lexicon. However, since this
component is, in itself, highly complex, its specifics
are beyond the scope of this paper and so we leave it
to be discussed in future work.
7 Conclusion
We have presented a method for inductive feature
detection of an annotated corpus, which determines
which feature values have a distinct representation
in a target language and what morphemes can be
used to express these grammatical meanings. This
method exploits the unique properties of an Elici-
tation Corpus, a resource which is becoming widely
available from the LDC. Finally, we have argued that
the output of feature detection is useful for exploit-
ing these linguistic features via a feature-rich gram-
mar for a machine translation system.
Acknowledgments
We would like to thank our colleagues Alon Lavie,
Vamshi Ambati, Abhaya Agarwal, and Alok Par-
likar for their insights. Thanks to Keisuke Kamataki
for the Japanese example and to Shakthi Poornima
for her help with the Urdu examples. This work was
supported by US NSF Grant Number 0713-292.
85
References
Alison Alvarez, Lori Levin, Robert Frederking, Simon
Fung, Donna Gates, and Jeff Good. 2006. The MILE
corpus for less commonly taught languages. In HLT-
NAACL, New York, New York, June.
H. Bonneau-Maynard, A. Allauzen, D. De?chelotte, and
H. Schwenk. 2007. Combining morphosyntactic en-
riched representation with n-best reranking in statis-
tical translation. In Proceedings of the Workshop on
Structure and Syntax in Statistical Translation (SSST)
at NAACL-HLT.
Jaime Carbonell, Kathrina Probst, Erik Peterson, Chris-
tian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource limited
MT. In Association for Machine Translation in the
Americas (AMTA), October.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Association for
Computational Linguistics (ACL).
Jonathan H. Clark, Robert Frederking, and Lori Levin.
2008. Toward active learning in corpus creation: Au-
tomatic discovery of language features during elicita-
tion. In Proceedings of the Language Resources and
Evaluation Conference (LREC).
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Meeting
of the Association for Computational Linguistics ACL.
Bryant Huang and Kevin Knight. 2006. Relabeling syn-
tax trees to improve syntax-based machine translation
quality. In Proceedings of (NAACL-HLT).
Ronald Kaplan. 1995. The formal architecture of lexi-
cal functional grammar. In Mary Dalrymple, Ronald
Kaplan, J. Maxwell, and A. Zaenen, editors, Formal
Issues in Lexical Functional Grammar. CSLI Publica-
tions.
Paul Kay. 2002. An informal sketch of a formal archi-
tecture for construction grammar. In Grammars.
Phillipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Lori Levin, Jeff Good, Alison Alvarez, and Robert Fred-
erking. 2006. Parallel reverse treebanks for the dis-
covery of morpho-syntactic markings. In Proceedings
of Treebanks and Linguistic Theory, Prague.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
44th Annual Meeting of the Association for Computa-
tional Linguistics, Sydney.
Christian Monson, Jaime Carbonell, Alon Lavie, and Lori
Levin. 2007. Paramor: Minimally supervised induc-
tion of paradigm structure and morphological analysis.
In Proceedings of the 9th ACL SIGMORPH.
Shakthi Poornima and Jean-Pierre Koenig. 2008. Re-
verse complex predicates in Hindi. In Proceedings of
the 24th Northwest Linguistic Conference.
Heather Simpson, Christopher Cieri, Kazuaki Maeda,
Kathryn Baker, and Boyan Onyshkevych. 2008. Hu-
man language technology resources for less commonly
taught languages: Lessons learned toward creation of
basic language resources. In Proceedings of the LREC
2008 Workshop on Collaboration: interoperability be-
tween people in the creation of language resources for
less-resourced langauges.
86
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 140?144,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
An Improved Statistical Transfer System for French?English
Machine Translation
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark, Alok Parlikar, Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema,vamshi,jhclark,aup,alavie}@cs.cmu.edu
Abstract
This paper presents the Carnegie Mellon
University statistical transfer MT system
submitted to the 2009 WMT shared task
in French-to-English translation. We de-
scribe a syntax-based approach that incor-
porates both syntactic and non-syntactic
phrase pairs in addition to a syntactic
grammar. After reporting development
test results, we conduct a preliminary anal-
ysis of the coverage and effectiveness of
the system?s components.
1 Introduction
The statistical transfer machine translation group
at Carnegie Mellon University has been devel-
oping a hybrid approach combining a traditional
rule-based MT system and its linguistically ex-
pressive formalism with more modern techniques
of statistical data processing and search-based de-
coding. The Stat-XFER framework (Lavie, 2008)
provides a general environment for building new
MT systems of this kind. For a given language
pair or data condition, the framework depends on
two main resources extracted from parallel data: a
probabilistic bilingual lexicon, and a grammar of
probabilistic synchronous context-free grammar
rules. Additional monolingual data, in the form of
an n-gram language model in the target language,
is also used. The statistical transfer framework op-
erates in two stages. First, the lexicon and gram-
mar are applied to synchronously parse and trans-
late an input sentence; all reordering is applied
during this stage, driven by the syntactic grammar.
Second, a monotonic decoder runs over the lat-
tice of scored translation pieces produced during
parsing and assembles the highest-scoring overall
translation according to a log-linear feature model.
Since our submission to last year?s Workshop
on Machine Translation shared translation task
(Hanneman et al, 2008), we have made numerous
improvements and extensions to our resource ex-
traction and processing methods, resulting in sig-
nificantly improved translation scores. In Section
2 of this paper, we trace our current methods for
data resource management for the Stat-XFER sub-
mission to the 2009 WMT shared French?English
translation task. Section 3 explains our tuning pro-
cedure, and Section 4 gives our experimental re-
sults on various development sets and offers some
preliminary analysis.
2 System Construction
Because of the additional data resources provided
for the 2009 French?English task, our system this
year is trained on nearly eight times as much
data as last year?s. We used three officially pro-
vided data sets to make up a parallel corpus for
system training: version 4 of the Europarl cor-
pus (1.43 million sentence pairs), the News Com-
mentary corpus (0.06 million sentence pairs), and
the pre-release version of the new Giga-FrEn cor-
pus (8.60 million sentence pairs)1. The combined
corpus of 10.09 million sentence pairs was pre-
processed to remove blank lines, sentences of 80
words or more, and sentence pairs where the ra-
tio between the number of English and French
words was larger than 5 to 1 in either direction.
These steps removed approximately 3% of the cor-
pus. Given the filtered corpus, our data prepara-
tion pipeline proceeded according to the descrip-
tions below.
1Because of data processing time, we were unable to use
the larger verions 1 or 2 of Giga-FrEn released later in the
evaluation period.
140
2.1 Parsing and Word Alignment
We parsed both sides of our parallel corpus with
independent automatic constituency parsers. We
used the Berkeley parser (Petrov and Klein, 2007)
for both English and French, although we obtained
better results for French by tokenizing the data
with our own script as a preprocessing step and
not allowing the parser to change it. There were
approximately 220,000 English sentences that did
not return a parse, which further reduced the size
of our training corpus by 2%.
After parsing, we re-extracted the leaf nodes
of the parse trees and statistically word-aligned
the corpus using a multi-threaded implementa-
tion (Gao and Vogel, 2008) of the GIZA++ pro-
gram (Och and Ney, 2003). Unidirectional align-
ments were symmetrized with the ?grow-diag-
final? heuristic (Koehn et al, 2005).
2.2 Phrase Extraction and Combination
Phrase extraction for last year?s statistical transfer
system used automatically generated parse trees
on both sides of the corpus as absolute constraints:
a syntactic phrase pair was extracted from a given
sentence only when a contiguous sequence of En-
glish words exactly made up a syntactic con-
stituent in the English parse tree and could also
be traced though symmetric word alignments to a
constituent in the French parse tree. While this
?tree-to-tree? extraction method is precise, it suf-
fers from low recall and results in a low-coverage
syntactic phrase table. Our 2009 system uses an
extended ?tree-to-tree-string? extraction process
(Ambati and Lavie, 2008) in which, if no suit-
able equivalent is found in the French parse tree
for an English node, a copy of the English node is
projected into the French tree, where it spans the
French words aligned to the yield of the English
node. This method can result in a 50% increase
in the number of extracted syntactic phrase pairs.
Each extracted phrase pair retains a syntactic cat-
egory label; in our current system, the node label
in the English parse tree is used as the category for
both sides of the bilingual phrase pair, although we
subsequently map the full set of labels used by the
Berkeley parser down to a more general set of 19
syntactic categories.
We also ran ?standard? phrase extraction on the
same corpus using Steps 4 and 5 of the Moses sta-
tistical machine translation training script (Koehn
et al, 2007). The two types of phrases were then
merged in a syntax-prioritized combination that
removes all Moses-extracted phrase pairs that have
source sides already covered by the tree-to-tree-
string syntactic phrase extraction. The syntax pri-
oritization has the advantage of still including a se-
lection of non-syntactic phrases while producing a
much smaller phrase table than a direct combina-
tion of all phrase pairs of both types. Previous ex-
periments we conducted indicated that this comes
with only a minor drop in automatic metric scores.
In our current submission, we modify the proce-
dure slightly by removing singleton phrase pairs
from the syntactic table before the combination
with Moses phrases. The coverage of the com-
bined table is not affected ? our syntactic phrase
extraction algorithm produces a subset of the non-
syntactic phrase pairs extracted from Moses, up to
phrase length constraints ? but the removal al-
lows Moses-extracted versions of some phrases to
survive syntax prioritization. In effect, we are lim-
iting the set of category-labeled syntactic transla-
tions we trust to those that have been seen more
than once in our training data. For a given syn-
tactic phrase pair, we also remove all but the most
frequent syntactic category label for the pair; this
removes a small number of entries from our lexi-
con in order to limit label ambiguity, but does not
affect coverage.
From our training data, we extracted 27.6 mil-
lion unique syntactic phrase pairs after single-
ton removal, reducing this set to 27.0 million en-
tries after filtering for category label ambiguity.
Some 488.7 million unique phrase pairs extracted
from Moses were reduced to 424.0 million after
syntax prioritization. (The remaining 64.7 mil-
lion phrase pairs had source sides already covered
by the 27.0 million syntactically extracted phrase
pairs, so they were thrown out.) This means non-
syntactic phrases outnumber syntactic phrases by
nearly 16 to 1. However, when filtering the phrase
table to a particular development or test set, we
find the syntactic phrases play a larger role, as this
ratio drops to approximately 3 to 1.
Sample phrase pairs from our system are shown
in Figure 1. Each pair includes two rule scores,
which we calculate from the source-side syntac-
tic category (cs), source-side text (ws), target-side
category (ct), and target-side text (wt). In the
case of Moses-extracted phrase pairs, we use the
?dummy? syntactic category PHR. Rule score rt|s
is a maximum likelihood estimate of the distri-
141
cs ct ws wt rt|s rs|t
ADJ ADJ espagnols Spanish 0.8278 0.1141
N N repre?sentants officials 0.0653 0.1919
NP NP repre?sentants de la Commission Commission officials 0.0312 0.0345
PHR PHR haute importance a` very important to 0.0357 0.0008
PHR PHR est charge? de has responsibility for 0.0094 0.0760
Figure 1: Sample lexical entries, including non-syntactic phrases, with rule scores (Equations 1 and 2).
bution of target-language translations and source-
and target-language syntactic categories given the
source string (Equation 1). The rs|t score is simi-
lar, but calculated in the reverse direction to give a
source-given-target probability (Equation 2).
rt|s =
#(wt, ct, ws, cs)
#(ws) + 1
(1)
rs|t =
#(wt, ct, ws, cs)
#(wt) + 1
(2)
Add-one smoothing in the denominators counter-
acts overestimation of the rule scores of lexical en-
tries with very infrequent source or target sides.
2.3 Syntactic Grammar
Syntactic phrase extraction specifies a node-to-
node alignment across parallel parse trees. If these
aligned nodes are used as decomposition points,
a set of synchronous context-free rules that pro-
duced the trees can be collected. This is our pro-
cess of syntactic grammar extraction (Lavie et al,
2008). For our 2009 WMT submission, we ex-
tracted 11.0 million unique grammar rules, 9.1
million of which were singletons, from our paral-
lel parsed corpus. These rules operate on our syn-
tactically extracted phrase pairs, which have cat-
egory labels, but they may also be partially lexi-
calized with explicit source or target word strings.
Each extracted grammar rule is scored according
to Equations 1 and 2, where now the right-hand
sides of the rule are used as ws and wt.
As yet, we have made only minimal use of the
Stat-XFER framework?s grammar capabilities, es-
pecially for large-scale MT systems. For the cur-
rent submission, the syntactic grammar consisted
of 26 manually chosen high-frequency grammar
rules that carry out some reordering between En-
glish and French. Since rules for high-level re-
ordering (near the top of the parse tree) are un-
likely to be useful unless a large amount of parse
structure can first be built, we concentrate our
rules on low-level reorderings taking place within
or around small constituents. Our focus for this
selection is the well-known repositioning of adjec-
tives and adjective phrases when translating from
French to English, such as from le Parlement eu-
rope?en to the European Parliament or from l? in-
tervention forte et substantielle to the strong and
substantial intervention. Our grammar thus con-
sists of 23 rules for building noun phrases, two
rules for building adjective phrases, and one rule
for building verb phrases.
2.4 English Language Model
We built a suffix-array language model (Zhang and
Vogel, 2006) on approximately 700 million words
of monolingual data: the unfiltered English side of
our parallel training corpus, plus the 438 million
words of English monolingual news data provided
for the WMT 2009 shared task. With the relatively
large amount of data available, we made the some-
what unusual decision of building our language
model (and all other data resources for our system)
in mixed case, which adds approximately 12.3%
to our vocabulary size. This saves us the need to
build and run a recaser as a postprocessing step
on our output. Our mixed-case decision may also
be validated by preliminary test set results, which
show that our submission has the smallest drop in
BLEU score (0.0074) between uncased and cased
evaluation of any system in the French?English
translation task.
3 System Tuning
Stat-XFER uses a log-linear combination of seven
features in its scoring of translation fragments:
language model probability, source-given-target
and target-given-source rule probabilities, source-
given-target and target-given-source lexical prob-
abilities, a length score, and a fragmentation score
based on the number of parsed translation frag-
ments that make up the output sentence. We tune
the weights for these features with several rounds
of minimum error rate training, optimizing to-
142
Primary Contrastive
Data Set METEOR BLEU TER METEOR BLEU TER
news-dev2009a-425 0.5437 0.2299 60.45 ? ? ?
news-dev2009a-600 ? ? ? 0.5134 0.2055 63.46
news-dev2009b 0.5263 0.2073 61.96 0.5303 0.2104 61.74
nc-test2007 0.6194 0.3282 51.17 0.6195 0.3226 51.49
Figure 2: Primary and contrastive system results on tuning and development test sets.
wards the BLEU metric. For each tuning itera-
tion, we save the n-best lists output by the sys-
tem from previous iterations and concatenate them
onto the current n-best list in order to present the
optimizer with a larger variety of translation out-
puts and score values.
From the provided ?news-dev2009a? develop-
ment set we create two tuning sets: one using the
first 600 sentences of the data, and a second using
the remaining 425 sentences. We tuned our sys-
tem separately on each set, saving the additional
?news-dev2009b? set as a final development test to
choose our primary and contrastive submissions2.
At run time, our full system takes on average be-
tween four and seven seconds to translate each in-
put sentence, depending on the size of the final
bilingual lexicon.
4 Evaluation and Analysis
Figure 2 shows the results of our primary and con-
trastive systems on four data sets. First, we report
final (tuned) performance on our two tuning sets
? the last 425 sentences of news-dev2009a for the
primary system, and the first 600 sentences of the
same set for the contrastive. We also include our
development test (news-dev2009b) and, for addi-
tional comparison, the ?nc-test2007? news com-
mentary test set from the 2007 WMT shared task.
For each, we give case-insensitive scores on ver-
sion 0.6 of METEOR (Lavie and Agarwal, 2007)
with all modules enabled, version 1.04 of IBM-
style BLEU (Papineni et al, 2002), and version 5
of TER (Snover et al, 2006).
From these results, we highlight two interest-
ing areas of analysis. First, the low tuning and
development test set scores bring up questions
about system coverage, given that the news do-
main was not strongly represented in our system?s
2Due to a data processing error, the choice of the primary
submission was based on incorrectly computed scores. In
fact, the contrastive system has better performance on our de-
velopment test set.
training data. We indeed find a significantly larger
proportion of out-of-vocabulary (OOV) words in
news-domain sets: the news-dev2009b set is trans-
lated by our primary submission with 402 of 6263
word types (6.42%) or 601 of 27,821 word tokens
(2.16%) unknown. The same system running on
the 2007 WMT ?test2007? set of Europarl-derived
data records an OOV rate of only 87 of 7514
word types (1.16%) or 105 of 63,741 word tokens
(0.16%).
Second, we turn our attention to the usefulness
of the syntactic grammar. Though small, we find
it to be both beneficial and precise. In the 1026-
sentence news-dev2009b set, for example, we find
351 rule applications ? the vast majority of them
(337) building noun phrases. The three most fre-
quently occurring rules are those for reordering the
sequence [DET N ADJ] to [DET ADJ N] (52 oc-
currences), the sequence [N ADJ] to [ADJ N] (51
occurrences), and the sequence [N1 de N2] to [N2
N1] (45 occurrences). We checked precision by
manually reviewing the 52 rule applications in the
first 150 sentences of news-dev2009b. There, 41
of the occurrences (79%) were judged to be cor-
rect and beneficial to translation output. Of the
remainder, seven were judged incorrect or detri-
mental and four were judged either neutral or of
unclear benefit.
We expect to continue to analyze the output and
effectiveness of our system in the coming months.
In particular, we would like to learn more about
the usefulness of our 26-rule grammar with the
view of using significantly larger grammars in fu-
ture versions of our system.
Acknowledgments
This research was supported in part by NSF grants
IIS-0121631 (AVENUE) and IIS-0534217 (LE-
TRAS), and by the DARPA GALE program. We
thank Yahoo! for the use of theM45 research com-
puting cluster, where we ran the parsing stage of
our data processing.
143
References
Vamshi Ambati and Alon Lavie. 2008. Improving
syntax driven translation models by re-structuring
divergent and non-isomorphic parse tree structures.
In Proceedings of the Eighth Conference of the As-
sociation for Machine Translation in the Americas,
pages 235?244, Waikiki, HI, October.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, OH,
June.
Greg Hanneman, Edmund Huber, Abhaya Agarwal,
Vamshi Ambati, Alok Parlikar, Erik Peterson, and
Alon Lavie. 2008. Statistical transfer systems
for French?English and German?English machine
translation. In Proceedings of the Third Workshop
on Statistical Machine Translation, pages 163?166,
Columbus, OH, June.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system description
for the 2005 IWSLT speech translation evaluation.
In Proceedings of IWSLT 2005, Pittsburgh, PA, Oc-
tober.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the ACL 2007 Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public, June.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Alon Lavie. 2008. Stat-XFER: A general search-based
syntax-driven framework for machine translation. In
Computational Linguistics and Intelligent Text Pro-
cessing, Lecture Notes in Computer Science, pages
362?375. Springer.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the As-
sociation for Machine Translation in the Americas,
pages 223?231, Cambridge, MA, August.
Ying Zhang and Stephan Vogel. 2006. Suffix array
and its applications in empirical natural language
processing. Technical Report CMU-LTI-06-010,
Carnegie Mellon University, Pittsburgh, PA, Decem-
ber.
144
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 409?419,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Word Alignment with Arbitrary Features
Chris Dyer Jonathan Clark Alon Lavie Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{cdyer,jhclark,alavie,nasmith}@cs.cmu.edu
Abstract
We introduce a discriminatively trained, glob-
ally normalized, log-linear variant of the lex-
ical translation models proposed by Brown
et al (1993). In our model, arbitrary, non-
independent features may be freely incorpo-
rated, thereby overcoming the inherent limita-
tion of generative models, which require that
features be sensitive to the conditional inde-
pendencies of the generative process. How-
ever, unlike previous work on discriminative
modeling of word alignment (which also per-
mits the use of arbitrary features), the param-
eters in our models are learned from unanno-
tated parallel sentences, rather than from su-
pervised word alignments. Using a variety
of intrinsic and extrinsic measures, including
translation performance, we show our model
yields better alignments than generative base-
lines in a number of language pairs.
1 Introduction
Word alignment is an important subtask in statis-
tical machine translation which is typically solved
in one of two ways. The more common approach
uses a generative translation model that relates bilin-
gual string pairs using a latent alignment variable to
designate which source words (or phrases) generate
which target words. The parameters in these models
can be learned straightforwardly from parallel sen-
tences using EM, and standard inference techniques
can recover most probable alignments (Brown et al,
1993). This approach is attractive because it only
requires parallel training data. An alternative to the
generative approach uses a discriminatively trained
alignment model to predict word alignments in the
parallel corpus. Discriminative models are attractive
because they can incorporate arbitrary, overlapping
features, meaning that errors observed in the predic-
tions made by the model can be addressed by engi-
neering new and better features. Unfortunately, both
approaches are problematic, but in different ways.
In the case of discriminative alignment mod-
els, manual alignment data is required for train-
ing, which is problematic for at least three reasons.
Manual alignments are notoriously difficult to cre-
ate and are available only for a handful of language
pairs. Second, manual alignments impose a commit-
ment to a particular preprocessing regime; this can
be problematic since the optimal segmentation for
translation often depends on characteristics of the
test set or size of the available training data (Habash
and Sadat, 2006) or may be constrained by require-
ments of other processing components, such parsers.
Third, the ?correct? alignment annotation for differ-
ent tasks may vary: for example, relatively denser or
sparser alignments may be optimal for different ap-
proaches to (downstream) translation model induc-
tion (Lopez, 2008; Fraser, 2007).
Generative models have a different limitation: the
joint probability of a particular setting of the ran-
dom variables must factorize according to steps in a
process that successively ?generates? the values of
the variables. At each step, the probability of some
value being generated may depend only on the gen-
eration history (or a subset thereof), and the possible
values a variable will take must form a locally nor-
malized conditional probability distribution (CPD).
While these locally normalized CPDs may be pa-
409
rameterized so as to make use of multiple, overlap-
ping features (Berg-Kirkpatrick et al, 2010), the re-
quirement that models factorize according to a par-
ticular generative process imposes a considerable re-
striction on the kinds of features that can be incor-
porated. When Brown et al (1993) wanted to in-
corporate a fertility model to create their Models 3
through 5, the generative process used in Models 1
and 2 (where target words were generated one by
one from source words independently of each other)
had to be abandoned in favor of one in which each
source word had to first decide how many targets it
would generate.1
In this paper, we introduce a discriminatively
trained, globally normalized log-linear model of lex-
ical translation that can incorporate arbitrary, over-
lapping features, and use it to infer word alignments.
Our model enjoys the usual benefits of discrimina-
tive modeling (e.g., parameter regularization, well-
understood learning algorithms), but is trained en-
tirely from parallel sentences without gold-standard
word alignments. Thus, it addresses the two limita-
tions of current word alignment approaches.
This paper is structured as follows. We begin by
introducing our model (?2), and follow this with a
discussion of tractability, parameter estimation, and
inference using finite-state techniques (?3). We then
describe the specific features we used (?4) and pro-
vide experimental evaluation of the model, showing
substantial improvements in three diverse language
pairs (?5). We conclude with an analysis of related
prior work (?6) and a general discussion (?8).
2 Model
In this section, we develop a conditional model
p(t | s) that, given a source language sentence s with
length m = |s|, assigns probabilities to a target sen-
tence t with length n, where each word tj is an el-
ement in the finite target vocabulary ?. We begin
by using the chain rule to factor this probability into
two components, a translation model and a length
model.
p(t | s) = p(t, n | s) = p(t | s, n)
? ?? ?
translation model
? p(n | s)
? ?? ?
length model
1Moore (2005) likewise uses this example to motivate the
need for models that support arbitrary, overlapping features.
In the translation model, we then assume that each
word tj is a translation of one source word, or a
special null token. We therefore introduce a latent
alignment variable a = ?a1, a2, . . . , an? ? [0,m]n,
where aj = 0 represents a special null token.
p(t | s, n) =
?
a
p(t, a | s, n)
So far, our model is identical to that of (Brown et
al., 1993); however, we part ways here. Rather than
using the chain rule to further decompose this prob-
ability and motivate opportunities to make indepen-
dence assumptions, we use a log-linear model with
parameters ? ? Rk and feature vector function H
that maps each tuple ?a, s, t, n? into Rk to model
p(t, a | s, n) directly:
p?(t, a | s, n) =
exp?>H(t, a, s, n)
Z?(s, n)
, where
Z?(s, n) =
?
t???n
?
a?
exp?>H(t?, a?, s, n)
Under some reasonable assumptions (a finite target
vocabulary ? and that all ?k < ?), the partition
function Z?(s, n) will always take on finite values,
guaranteeing that p(t, a | s, n) is a proper probability
distribution.
So far, we have said little about the length model.
Since our intent here is to use the model for align-
ment, where both the target length and target string
are observed, it will not be necessary to commit to
any length model, even during training.
3 Tractability, Learning, and Inference
The model introduced in the previous section is
extremely general, and it can incorporate features
sensitive to any imaginable aspects of a sentence
pair and their alignment, from linguistically in-
spired (e.g., an indicator feature for whether both
the source and target sentences contain a verb), to
the mundane (e.g., the probability of the sentence
pair and alignment under Model 1), to the absurd
(e.g., an indicator if s and t are palindromes of each
other).
However, while our model can make use of arbi-
trary, overlapping features, when designing feature
functions it is necessary to balance expressiveness
and the computational complexity of the inference
410
algorithms used to reason under models that incor-
porate these features.2 To understand this tradeoff,
we assume that the random variables being modeled
(t, a) are arranged into an undirected graph G such
that the vertices represent the variables and the edges
are specified so that the feature function H decom-
poses linearly over all the cliques C in G,
H(t, a, s, n) =
?
C
h(tC , aC , s, n) ,
where tC and aC are the components associated with
subgraph C and h(?) is a local feature vector func-
tion. In general, exact inference is exponential in
the width of tree-decomposition of G, but, given a
fixed width, they can be solved in polynomial time
using dynamic programming. For example, when
the graph has a sequential structure, exact infer-
ence can be carried out using the familiar forward-
backward algorithm (Lafferty et al, 2001). Al-
though our features look at more structure than this,
they are designed to keep treewidth low, meaning
exact inference is still possible with dynamic pro-
gramming. Figure 1 gives a graphical representation
of our model as well as the more familiar genera-
tive (directed) variants. The edge set in the depicted
graph is determined by the features that we use (?4).
3.1 Parameter Learning
To learn the parameters of our model, we select the
?? that minimizes the `1 regularized conditional log-
likelihood of a set of training data T :
L(?) =?
?
?s,t??T
log
?
a
p?(t, a | s, n) + ?
?
k
|?k| .
Because of the `1 penalty, this objective is not every-
where differentiable, but the gradient with respect to
the parameters of the log-likelihood term is as fol-
lows.
?L
??
=
?
?s,t??T
Ep?(a|s,t,n)[H(?)]? Ep?(t,a|s,n)[H(?)]
(1)
To optimize L, we employ an online method that
approximates `1 regularization and only depends on
2One way to understand expressiveness is in terms of inde-
pendence assumptions, of course. Research in graphical models
has done much to relate independence assumptions to the com-
plexity of inference algorithms (Koller and Friedman, 2009).
the gradient of the unregularized objective (Tsu-
ruoka et al, 2009). This method is quite attrac-
tive since it is only necessary to represent the active
features, meaning impractically large feature spaces
can be searched provided the regularization strength
is sufficiently high. Additionally, not only has this
technique been shown to be very effective for opti-
mizing convex objectives, but evidence suggests that
the stochasticity of online algorithms often results
in better solutions than batch optimizers for non-
convex objectives (Liang and Klein, 2009). On ac-
count of the latent alignment variable in our model,
L is non-convex (as is the likelihood objective of the
generative variant).
To choose the regularization strength ? and the
initial learning rate ?0,3 we trained several mod-
els on a 10,000-sentence-pair subset of the French-
English Hansards, and chose values that minimized
the alignment error rate, as evaluated on a 447 sen-
tence set of manually created alignments (Mihalcea
and Pedersen, 2003). For the remainder of the ex-
periments, we use the values we obtained, ? = 0.4
and ?0 = 0.3.
3.2 Inference with WFSAs
We now describe how to use weighted finite-state
automata (WFSAs) to compute the quantities neces-
sary for training. We begin by describing the ideal
WFSA representing the full translation search space,
which we call the discriminative neighborhood, and
then discuss strategies for reducing its size in the
next section, since the full model is prohibitively
large, even with small data sets.
For each training instance ?s, t?, the contribution
to the gradient (Equation 1) is the difference in two
vectors of expectations. The first term is the ex-
pected value of H(?) when observing ?s, n, t? and
letting a range over all possible alignments. The
second is the expectation of the same function, but
observing only ?s, n? and letting t? and a take on
any possible values (i.e., all possible translations
of length n and all their possible alignments to s).
To compute these expectations, we can construct
a WFSA representing the discriminative neighbor-
hood, the set ?n?[0,m]n, such that every path from
the start state to goal yields a pair ?t?, a? with weight
3For the other free parameters of the algorithm, we use the
default values recommended by Tsuruoka et al (2009).
411
a1
a
2
a
3
a
n
t
1
t
2
t
3
t
n
s
n
Fully directed model (Brown et al, 1993;
Vogel et al, 1996; Berg-Kirkpatrick et al, 2010)
Our model
...
a
1
a
2
a
3
a
n
t
1
t
2
t
3
t
n
s
n
...
......
s
s s s s
ss s
Figure 1: A graphical representation of a conventional generative lexical translation model (left) and our model with
an undirected translation model. For clarity, the observed node s (representing the full source sentence) is drawn in
multiple locations. The dashed lines indicate a dependency on a deterministic mapping of tj (not its complete value).
H(t?, a, s, n). With our feature set (?4), number of
states in this WFSA isO(m?n) since at each target
index j, there is a different state for each possible in-
dex of the source word translated at position j ? 1.4
Once the WFSA representing the discriminative
neighborhood is built, we use the forward-backward
algorithm to compute the second expectation term.
We then intersect the WFSA with an unweighted
FSA representing the target sentence t (because of
the restricted structure of our WFSA, this amounts
to removing edges), and finally run the forward-
backward algorithm on the resulting WFSA to com-
pute the first expectation.
3.3 Shrinking the Discriminative
Neighborhood
The WFSA we constructed requires m? |?| transi-
tions between all adjacent states, which is impracti-
cally large. We can reduce the number of edges by
restricting the set of words that each source word can
translate into. Thus, the model will not discriminate
4States contain a bit more information than the index of the
previous source word, for example, there is some additional in-
formation about the previous translation decision that is passed
forward. However, the concept of splitting states to guarantee
distinct paths for different values of non-local features is well
understood by NLP and machine translation researchers, and
the necessary state structure should be obvious from the feature
description.
among all candidate target strings in ?n, but rather
in ?ns , where ?s =
?m
i=1 ?si , and where ?s is the
set of target words that s may translate into.5
We consider four different definitions of ?s: (1)
the baseline of the full target vocabulary, (2) the set
of all target words that co-occur in sentence pairs
containing s, (3) the most probable words under
IBM Model 1 that are above a threshold, and (4) the
same Model 1, except we add a sparse symmetric
Dirichlet prior (? = 0.01) on the translation distri-
butions and use the empirical Bayes (EB) method to
infer a point estimate, using variational inference.
Table 1: Comparison of alternative definitions ?s (arrows
indicate whether higher or lower is better).
?s time (s) ?
?
s |?s| ? AER ?
= ? 22.4 86.0M 0.0
co-occ. 8.9 0.68M 0.0
Model 1 0.2 0.38M 6.2
EB-Model 1 1.0 0.15M 2.9
Table 1 compares the average per-sentence time
required to run the inference algorithm described
5Future work will explore alternative formulations of the
discriminative neighborhood with the goal of further improving
inference efficiency. Smith and Eisner (2005) show that good
performance on unsupervised syntax learning is possible even
when learning from very small discriminative neighborhoods,
and we posit that the same holds here.
412
above under these four different definitions of ?s on
a 10,000 sentence subset of the Hansards French-
English corpus that includes manual word align-
ments. While our constructions guarantee that all
references are reachable even in the reduced neigh-
borhoods, not all alignments between source and tar-
get are possible. The last column is the oracle AER.
Although EB variant of Model 1 neighborhood is
slightly more expensive to do inference with than
regular Model 1, we use it because it has a lower
oracle AER.6
During alignment prediction (rather than during
training) for a sentence pair ?s, t?, it is possible to
further restrict ?s to be just the set of words occur-
ring in t, making extremely fast inference possible
(comparable to that of the generative HMM align-
ment model).
4 Features
Feature engineering lets us encode knowledge about
what aspects of a translation derivation are useful in
predicting whether it is good or not. In this section
we discuss the features we used in our model. Many
of these were taken from the discriminative align-
ment modeling literature, but we also note that our
features can be much more fine-grained than those
used in supervised alignment modeling, since we
learn our models from a large amount of parallel
data, rather than a small number of manual align-
ments.
Word association features. Word association fea-
tures are at the heart of all lexical translation models,
whether generative or discriminative. In addition to
fine-grained boolean indicator features ?saj , tj? for
pair types, we have several orthographic features:
identity, prefix identity, and an orthographic simi-
larity measure designed to be informative for pre-
dicting the translation of named entities in languages
that use similar alphabets.7 It has the property that
source-target pairs of long words that are similar are
given a higher score than word pairs that are short
and similar (dissimilar pairs have a score near zero,
6We included all translations whose probability was within
a factor of 10?4 of the highest probability translation.
7In experiments with Urdu, which uses an Arabic-derived
script, the orthographic feature was computed after first ap-
plying a heuristic Romanization, which made the orthographic
forms somewhat comparable.
regardless of length). We also include ?global? asso-
ciation scores that are precomputed by looking at the
full training data: Dice?s coefficient (discretized),
which we use to measure association strength be-
tween pairs of source and target word types across
sentence pairs (Dice, 1945), IBM Model 1 forward
and reverse probabilities, and the geometric mean of
the Model 1 forward and reverse probabilities. Fi-
nally, we also cluster the source and target vocab-
ularies (Och, 1999) and include class pair indicator
features, which can learn generalizations that, e.g.,
?nouns tend to translate into nouns but not modal
verbs.?
Positional features. Following Blunsom and
Cohn (2006), we include features indicating
closeness to the alignment matrix diagonal,
h(aj , j,m, n) =
?
?
?
aj
m ?
j
n
?
?
?. We also conjoin this
feature with the source word class type indicator to
enable the model to learn that certain word types
are more or less likely to favor a location on the
diagonal (e.g. Urdu?s sentence-final verbs).
Source features. Some words are functional el-
ements that fulfill purely grammatical roles and
should not be the ?source? of a translation. For ex-
ample, Romance languages require a preposition in
the formation of what could be a noun-noun com-
pound in English, thus, it may be useful to learn not
to translate certain words (i.e. they should not par-
ticipate in alignment links), or to have a bias to trans-
late others. To capture this intuition we include an
indicator feature that fires each time a source vocab-
ulary item (and source word class) participates in an
alignment link.
Source path features. One class of particularly
useful features assesses the goodness of the align-
ment ?path? through the source sentence (Vogel et
al., 1996). Although assessing the predicted path
requires using nonlocal features, since each aj ?
[0,m] and m is relatively small, features can be sen-
sitive to a wider context than is often practical.
We use many overlapping source path features,
some of which are sensitive to the distance and di-
rection of the jump between aj?1 and aj , and oth-
ers which are sensitive to the word pair these two
points define, and others that combine all three el-
ements. The features we use include a discretized
413
jump distance, the discretized jump conjoined with
an indicator feature for the target length n, the dis-
cretized jump feature conjoined with the class of saj ,
and the discretized jump feature conjoined with the
class of saj and saj?1 . To discretize the features we
take a log transform (base 1.3) of the jump width and
let an indicator feature fire for the closest integer.
In addition to these distance-dependent features, we
also include indicator features that fire on bigrams
?saj?1 , saj ? and their word classes. Thus, this fea-
ture can capture our intuition that, e.g., adjectives
are more likely to come before or after a noun in
different languages.
Target string features. Features sensitive to mul-
tiple values in the predicted target string or latent
alignment variable must be handled carefully for the
sake of computational tractability. While features
that look at multiple source words can be computed
linearly in the number of source words considered
(since the source string is always observable), fea-
tures that look at multiple target words require ex-
ponential time and space!8 However, by grouping
the tj?s into coarse equivalence classes and looking
at small numbers of variables, it is possible to incor-
porate such features. We include a feature that fires
when a word translates as itself (for example, a name
or a date, which occurs in languages that share the
same alphabet) in position j, but then is translated
again (as something else) in position j ? 1 or j + 1.
5 Experiments
We now turn to an empirical assessment of our
model. Using various datasets, we evaluate the
performance of the models? intrinsic quality and
theirtheir alignments? contribution to a standard ma-
chine translation system. We make use of parallel
corpora from languages with very different typolo-
gies: a small (0.8M words) Chinese-English corpus
from the tourism and travel domain (Takezawa et al,
2002), a corpus of Czech-English news commen-
tary (3.1M words),9 and an Urdu-English corpus
(2M words) provided by NIST for the 2009 Open
MT Evaluation. These pairs were selected since
each poses different alignment challenges (word or-
8This is of course what makes history-based language model
integration an inference challenge in translation.
9http://statmt.org/wmt10
der in Chinese and Urdu, morphological complex-
ity in Czech, and a non-alphabetic writing system in
Chinese), and confining ourselves to these relatively
small corpora reduced the engineering overhead of
getting an implementation up and running. Future
work will explore the scalability characteristics and
limits of the model.
5.1 Methodology
For each language pair, we train two log-linear
translation models as described above (?3), once
with English as the source and once with English
as the target language. For a baseline, we use
the Giza++ toolkit (Och and Ney, 2003) to learn
Model 4, again in both directions. We symmetrize
the alignments from both model types using the
grow-diag-final-and heuristic (Koehn et al,
2003) producing, in total, six alignment sets. We
evaluate them both intrinsically and in terms of their
performance in a translation system.
Since we only have gold alignments for Czech-
English (Bojar and Prokopova?, 2006), we can re-
port alignment error rate (AER; Och and Ney, 2003)
only for this pair. However, we offer two further
measures that we believe are suggestive and that
do not require gold alignments. One is the aver-
age alignment ?fertility? of source words that occur
only a single time in the training data (so-called ha-
pax legomena). This assesses the impact of a typical
alignment problem observed in generative models
trained to maximize likelihood: infrequent source
words act as ?garbage collectors?, with many target
words aligned to them (the word dislike in the Model
4 alignment in Figure 2 is an example). Thus, we ex-
pect lower values of this measure to correlate with
better alignments. The second measure is the num-
ber of rule types learned in the grammar induction
process used for translation that match the transla-
tion test sets.10 While neither a decrease in the aver-
age singleton fertility nor an increase in the number
of rules induced guarantees better alignment quality,
we believe it is reasonable to assume that they are
positively correlated.
For the translation experiments in each language
pair, we make use of the cdec decoder (Dyer et al,
10This measure does not assess whether the rule types are
good or bad, but it does suggest that the system?s coverage is
greater.
414
2010), inducing a hierarchical phrase based trans-
lation grammar from two sets of symmetrized align-
ments using the method described by Chiang (2007).
Additionally, recent work that has demonstrated that
extracting rules from n-best alignments has value
(Liu et al, 2009; Venugopal et al, 2008). We
therefore define a third condition where rules are
extracted from the corpus under both the Model 4
and discriminative alignments and merged to form
a single grammar. We incorporate a 3-gram lan-
guage model learned from the target side of the
training data as well as 50M supplemental words
of monolingual training data consisting of sentences
randomly sampled from the English Gigaword, ver-
sion 4. In the small Chinese-English travel domain
experiment, we just use the LM estimated from the
bitext. The parameters of the translation model were
tuned using ?hypergraph? minimum error rate train-
ing (MERT) to maximize BLEU on a held-out de-
velopment set (Kumar et al, 2009). Results are
reported using case-insensitive BLEU (Papineni et
al., 2002), METEOR11 (Lavie and Denkowski, 2009),
and TER (Snover et al, 2006), with the number of
references varying by task. Since MERT is a non-
deterministic optimization algorithm and results can
vary considerably between runs, we follow Clark et
al. (2011) and report the average score and stan-
dard deviation of 5 independent runs, 30 in the case
of Chinese-English, since observed variance was
higher.
5.2 Experimental Results
Czech-English. Czech-English poses problems
for word alignment models since, unlike English,
Czech words have a complex inflectional morphol-
ogy, and the syntax permits relatively free word or-
der. For this language pair, we evaluate alignment
error rate using the manual alignment corpus de-
scribed by Bojar and Prokopova? (2006). Table 2
summarizes the results.
Chinese-English. Chinese-English poses a differ-
ent set of problems for alignment. While Chinese
words have rather simple morphology, the Chinese
writing system renders our orthographic features
useless. Despite these challenges, the Chinese re-
11Meteor 1.0 with exact, stem, synonymy, and paraphrase
modules and HTER parameters.
Table 2: Czech-English experimental results. ??sing. is the
average fertility of singleton source words.
AER ? ??sing. ? # rules ?
Model 4 e | f 24.8 4.1
f | e 33.6 6.6
sym. 23.4 2.7 993,953
Our model e | f 21.9 2.3
f | e 29.3 3.8
sym. 20.5 1.6 1,146,677
Alignment BLEU ? METEOR ? TER ?
Model 4 16.3?0.2 46.1?0.1 67.4?0.3
Our model 16.5?0.1 46.8?0.1 67.0?0.2
Both 17.4?0.1 47.7?0.1 66.3?0.5
sults in Table 3 show the same pattern of results as
seen in Czech-English.
Table 3: Chinese-English experimental results.
??sing. ? # rules ?
Model 4 e | f 4.4
f | e 3.9
sym. 3.6 52,323
Our model e | f 3.5
f | e 2.6
sym. 3.1 54,077
Alignment BLEU ? METEOR ? TER ?
Model 4 56.5?0.3 73.0?0.4 29.1?0.3
Our model 57.2?0.8 73.8?0.4 29.3?1.1
Both 59.1?0.6 74.8?0.7 27.6?0.5
Urdu-English. Urdu-English is a more challeng-
ing language pair for word alignment than the pre-
vious two we have considered. The parallel data is
drawn from numerous genres, and much of it was ac-
quired automatically, making it quite noisy. So our
models must not only predict good translations, they
must cope with bad ones as well. Second, there has
been no previous work on discriminative modeling
of Urdu, since, to our knowledge, no manual align-
ments have been created. Finally, unlike English,
Urdu is a head-final language: not only does it have
SOV word order, but rather than prepositions, it has
post-positions, which follow the nouns they modify,
meaning its large scale word order is substantially
415
different from that of English. Table 4 demonstrates
the same pattern of improving results with our align-
ment model.
Table 4: Urdu-English experimental results.
??sing. ? # rules ?
Model 4 e | f 6.5
f | e 8.0
sym. 3.2 244,570
Our model e | f 4.8
f | e 8.3
sym. 2.3 260,953
Alignment BLEU ? METEOR ? TER ?
Model 4 23.3?0.2 49.3?0.2 68.8?0.8
Our model 23.4?0.2 49.7?0.1 67.7?0.2
Both 24.1?0.2 50.6?0.1 66.8?0.5
5.3 Analysis
The quantitative results presented in this section
strongly suggest that our modeling approach pro-
duces better alignments. In this section, we try to
characterize how the model is doing what it does
and what it has learned. Because of the `1 regular-
ization, the number of active (non-zero) features in
the inferred models is small, relative to the number
of features considered during training. The num-
ber of active features ranged from about 300k for
the small Chinese-English corpus to 800k for Urdu-
English, which is less than one tenth of the available
features in both cases. In all models, the coarse fea-
tures (Model 1 probabilities, Dice coefficient, coarse
positional features, etc.) typically received weights
with large magnitudes, but finer features also played
an important role.
Language pair differences manifested themselves
in many ways in the models that were learned.
For example, orthographic features were (unsurpris-
ingly) more valuable in Czech-English, with their
largely overlapping alphabets, than in Chinese or
Urdu. Examining the more fine-grained features is
also illuminating. Table 5 shows the most highly
weighted source path bigram features on the three
models where English was the source language, and
in each, we may observe some interesting character-
istics of the target language. Left-most is English-
Czech. At first it may be surprising that words like
since and that have a highly weighted feature for
transitioning to themselves. However, Czech punc-
tuation rules require that relative clauses and sub-
ordinating conjunctions be preceded by a comma
(which is only optional or outright forbidden in En-
glish), therefore our model translates these words
twice, once to produce the comma, and a second
time to produce the lexical item. The middle col-
umn is the English-Chinese model. In the training
data, many of the sentences are questions directed to
a second person, you. However, Chinese questions
do not invert and the subject remains in the canon-
ical first position, thus the transition from the start
of sentence to you is highly weighted. Finally, Fig-
ure 2 illustrates how Model 4 (left) and our discrimi-
native model (right) align an English-Urdu sentence
pair (the English side is being conditioned on in both
models). A reflex of Urdu?s head-final word order
is seen in the list of most highly weighted bigrams,
where a path through the English source where verbs
that transition to end-of-sentence periods are predic-
tive of good translations into Urdu.
Table 5: The most highly weighted source path bigram
features in the English-Czech, -Chinese, and -Urdu mod-
els.
Bigram ?k
. ?/s? 3.08
like like 1.19
one of 1.06
? . 0.95
that that 0.92
is but 0.92
since since 0.84
?s? when 0.83
, how 0.83
, not 0.83
Bigram ?k
. ?/s? 2.67
? ? 2.25
?s? please 2.01
much ? 1.61
?s? if 1.58
thank you 1.47
?s? sorry 1.46
?s? you 1.45
please like 1.24
?s? this 1.19
Bigram ?k
. ?/s? 1.87
?s? this 1.24
will . 1.17
are . 1.16
is . 1.09
is that 1.00
have . 0.97
has . 0.96
was . 0.91
will ?/s? 0.88
6 Related Work
The literature contains numerous descriptions of dis-
criminative approaches to word alignment motivated
by the desire to be able to incorporate multiple,
overlapping knowledge sources (Ayan et al, 2005;
Moore, 2005; Taskar et al, 2005; Blunsom and
Cohn, 2006; Haghighi et al, 2009; Liu et al, 2010;
DeNero and Klein, 2010; Setiawan et al, 2010).
This body of work has been an invaluable source
of useful features. Several authors have dealt with
the problem training log-linear models in an unsu-
416
IBM Model 4 alignment Our model's alignment
Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model
4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model
does not exhibit these problems, and in fact, makes no mistakes in the alignment.
pervised setting. The contrastive estimation tech-
nique proposed by Smith and Eisner (2005) is glob-
ally normalized (and thus capable of dealing with ar-
bitrary features), and closely related to the model we
developed; however, they do not discuss the problem
of word alignment. Berg-Kirkpatrick et al (2010)
learn locally normalized log-linear models in a gen-
erative setting. Globally normalized discriminative
models with latent variables (Quattoni et al, 2004)
have been used for a number of language processing
problems, including MT (Dyer and Resnik, 2010;
Blunsom et al, 2008a). However, this previous
work relied on translation grammars constructed us-
ing standard generative word alignment processes.
7 Future Work
While we have demonstrated that this model can be
substantially useful, it is limited in some important
ways which are being addressed in ongoing work.
First, training is expensive, and we are exploring al-
ternatives to the conditional likelihood objective that
is currently used, such as contrastive neighborhoods
advocated by (Smith and Eisner, 2005). Addition-
ally, there is much evidence that non-local features
like the source word fertility are (cf. IBM Model 3)
useful for translation and alignment modeling. To be
truly general, it must be possible to utilize such fea-
tures. Unfortunately, features like this that depend
on global properties of the alignment vector, a, make
the inference problem NP-hard, and approximations
are necessary. Fortunately, there is much recent
work on approximate inference techniques for incor-
porating nonlocal features (Blunsom et al, 2008b;
Gimpel and Smith, 2009; Cromie`res and Kurohashi,
2009; Weiss and Taskar, 2010), suggesting that this
problem too can be solved using established tech-
niques.
8 Conclusion
We have introduced a globally normalized, log-
linear lexical translation model that can be trained
discriminatively using only parallel sentences,
which we apply to the problem of word alignment.
Our approach addresses two important shortcomings
of previous work: (1) that local normalization of
generative models constrains the features that can be
used, and (2) that previous discriminatively trained
word alignment models required supervised align-
ments. According to a variety of measures in a vari-
ety of translation tasks, this model produces superior
alignments to generative approaches. Furthermore,
the features learned by our model reveal interesting
characteristics of the language pairs being modeled.
Acknowledgments
This work was supported in part by the DARPA GALE
program; the U. S. Army Research Laboratory and the
U. S. Army Research Office under contract/grant num-
417
ber W911NF-10-1-0533; and the National Science Foun-
dation through grants IIS-0844507, IIS-0915187, IIS-
0713402, and IIS-0915327 and through TeraGrid re-
sources provided by the Pittsburgh Supercomputing Cen-
ter under grant number TG-DBS110003. We thank
Ondr?ej Bojar for providing the Czech-English alignment
data, and three anonymous reviewers for their detailed
suggestions and comments on an earlier draft of this pa-
per.
References
N. F. Ayan, B. J. Dorr, and C. Monz. 2005. NeurAlign:
combining word alignments using neural networks. In
Proc. of HLT-EMNLP.
T. Berg-Kirkpatrick, A. Bouchard-Co?te?, J. DeNero, and
D. Klein. 2010. Painless unsupervised learning with
features. In Proc. of NAACL.
P. Blunsom and T. Cohn. 2006. Discriminative word
alignment with conditional random fields. In Proc. of
ACL.
P. Blunsom, T. Cohn, and M. Osborne. 2008a. A dis-
criminative latent variable model for statistical ma-
chine translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2008b. Proba-
bilistic inference for machine translation. In Proc. of
EMNLP 2008.
O. Bojar and M. Prokopova?. 2006. Czech-English word
alignment. In Proc. of LREC.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
J. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Bet-
ter hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proc. of
ACL.
F. Cromie`res and S. Kurohashi. 2009. An alignment al-
gorithm using belief propagation and a structure-based
distortion model. In Proc. of EACL.
J. DeNero and D. Klein. 2010. Discriminative modeling
of extraction sets for machine translation. In Proc. of
ACL.
L. R. Dice. 1945. Measures of the amount of eco-
logic association between species. Journal of Ecology,
26:297?302.
C. Dyer and P. Resnik. 2010. Context-free reordering,
finite-state translation. In Proc. of NAACL.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL (demonstration session).
A. Fraser. 2007. Improved Word Alignments for Statis-
tical Machine Translation. Ph.D. thesis, University of
Southern California.
K. Gimpel and N. A. Smith. 2009. Cube summing, ap-
proximate inference with non-local features, and dy-
namic programming without semirings. In Proc. of
EACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing
schemes for statistical machine translation. In Proc. of
NAACL, New York.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL-IJCNLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
D. Koller and N. Friedman. 2009. Probabilistic Graphi-
cal Models: Principles and Techniques. MIT Press.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL-IJCNLP.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of ICML.
A. Lavie and M. Denkowski. 2009. The METEOR metric
for automatic evaluation of machine translation. Ma-
chine Translation Journal, 23(2?3):105?115.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
Y. Liu, T. Xia, X. Xiao, and Q. Liu. 2009. Weighted
alignment matrices for statistical machine translation.
In Proc. of EMNLP.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3):303?339.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proc. of the Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proc. of HLT-EMNLP.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. Och. 1999. An efficient method for determining bilin-
gual word classes. In Proc. of EACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
418
A. Quattoni, M. Collins, and T. Darrell. 2004. Condi-
tional random fields for object recognition. In NIPS
17.
H. Setiawan, C. Dyer, and P. Resnik. 2010. Discrimina-
tive word alignment with a function word reordering
model. In Proc. of EMNLP.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
training log-linear models on unlabeled data. In Proc.
of ACL.
M. Snover, B. J. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In Proc. of LREC.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. of HLT-EMNLP.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL-IJCNLP.
A. Venugopal, A. Zollmann, N. A. Smith, and S. Vogel.
2008. Wider pipelines: n-best alignments and parses
in MT training. In Proc. of AMTA.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING.
D. Weiss and B. Taskar. 2010. Structured prediction cas-
cades. In Proc. of AISTATS.
419
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176?181,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Better Hypothesis Testing for Statistical Machine Translation:
Controlling for Optimizer Instability
Jonathan H. Clark Chris Dyer Alon Lavie Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{jhclark,cdyer,alavie,nasmith}@cs.cmu.edu
Abstract
In statistical machine translation, a researcher
seeks to determine whether some innovation
(e.g., a new feature, model, or inference al-
gorithm) improves translation quality in com-
parison to a baseline system. To answer this
question, he runs an experiment to evaluate the
behavior of the two systems on held-out data.
In this paper, we consider how to make such
experiments more statistically reliable. We
provide a systematic analysis of the effects of
optimizer instability?an extraneous variable
that is seldom controlled for?on experimen-
tal outcomes, and make recommendations for
reporting results more accurately.
1 Introduction
The need for statistical hypothesis testing for ma-
chine translation (MT) has been acknowledged since
at least Och (2003). In that work, the proposed
method was based on bootstrap resampling and was
designed to improve the statistical reliability of re-
sults by controlling for randomness across test sets.
However, there is no consistently used strategy that
controls for the effects of unstable estimates of
model parameters.1 While the existence of opti-
mizer instability is an acknowledged problem, it is
only infrequently discussed in relation to the relia-
bility of experimental results, and, to our knowledge,
there has yet to be a systematic study of its effects on
1We hypothesize that the convention of ?trusting? BLEU
score improvements of, e.g., > 1, is not merely due to an ap-
preciation of what qualitative difference a particular quantita-
tive improvement will have, but also an implicit awareness that
current methodology leads to results that are not consistently
reproducible.
hypothesis testing. In this paper, we present a series
of experiments demonstrating that optimizer insta-
bility can account for substantial amount of variation
in translation quality,2 which, if not controlled for,
could lead to incorrect conclusions. We then show
that it is possible to control for this variable with a
high degree of confidence with only a few replica-
tions of the experiment and conclude by suggesting
new best practices for significance testing for ma-
chine translation.
2 Nondeterminism and Other
Optimization Pitfalls
Statistical machine translation systems consist of a
model whose parameters are estimated to maximize
some objective function on a set of development
data. Because the standard objectives (e.g., 1-best
BLEU, expected BLEU, marginal likelihood) are
not convex, only approximate solutions to the op-
timization problem are available, and the parame-
ters learned are typically only locally optimal and
may strongly depend on parameter initialization and
search hyperparameters. Additionally, stochastic
optimization and search techniques, such as mini-
mum error rate training (Och, 2003) and Markov
chain Monte Carlo methods (Arun et al, 2010),3
constitute a second, more obvious source of noise
in the optimization procedure.
This variation in the parameter vector affects the
quality of the model measured on both development
2This variation directly affects the output translations, and
so it will propagate to both automated metrics as well as human
evaluators.
3Online subgradient techniques such as MIRA (Crammer et
al., 2006; Chiang et al, 2008) have an implicit stochastic com-
ponent as well based on the order of the training examples.
176
data and held-out test data, independently of any ex-
perimental manipulation. Thus, when trying to de-
termine whether the difference between two mea-
surements is significant, it is necessary to control for
variance due to noisy parameter estimates. This can
be done by replication of the optimization procedure
with different starting conditions (e.g., by running
MERT many times).
Unfortunately, common practice in reporting ma-
chine translation results is to run the optimizer once
per system configuration and to draw conclusions
about the experimental manipulation from this sin-
gle sample. However, it could be that a particu-
lar sample is on the ?low? side of the distribution
over optimizer outcomes (i.e., it results in relatively
poorer scores on the test set) or on the ?high? side.
The danger here is obvious: a high baseline result
paired with a low experimental result could lead to a
useful experimental manipulation being incorrectly
identified as useless. We now turn to the question of
how to reduce the probability falling into this trap.
3 Related Work
The use of statistical hypothesis testing has grown
apace with the adoption of empirical methods in
natural language processing. Bootstrap techniques
(Efron, 1979; Wasserman, 2003) are widespread
in many problem areas, including for confidence
estimation in speech recognition (Bisani and Ney,
2004), and to determine the significance of MT re-
sults (Och, 2003; Koehn, 2004; Zhang et al, 2004;
Zhang and Vogel, 2010). Approximate randomiza-
tion (AR) has been proposed as a more reliable tech-
nique for MT significance testing, and evidence sug-
gests that it yields fewer type I errors (i.e., claiming
a significant difference where none exists; Riezler
and Maxwell, 2005). Other uses in NLP include
the MUC-6 evaluation (Chinchor, 1993) and pars-
ing (Cahill et al, 2008). However, these previous
methods assume model parameters are elements of
the system rather than extraneous variables.
Prior work on optimizer noise in MT has fo-
cused primarily on reducing optimizer instability
(whereas our concern is how to deal with optimizer
noise, when it exists). Foster and Kuhn (2009) mea-
sured the instability of held-out BLEU scores across
10 MERT runs to improve tune/test set correlation.
However, they only briefly mention the implications
of the instability on significance. Cer et al (2008)
explored regularization of MERT to improve gener-
alization on test sets. Moore and Quirk (2008) ex-
plored strategies for selecting better random ?restart
points? in optimization. Cer et al (2010) analyzed
the standard deviation over 5 MERT runs when each
of several metrics was used as the objective function.
4 Experiments
In our experiments, we ran the MERT optimizer to
optimize BLEU on a held-out development set many
times to obtain a set of optimizer samples on two dif-
ferent pairs of systems (4 configurations total). Each
pair consists of a baseline system (System A) and an
?experimental? system (System B), which previous
research has suggested will perform better.
The first system pair contrasts a baseline phrase-
based system (Moses) and experimental hierarchi-
cal phrase-based system (Hiero), which were con-
structed from the Chinese-English BTEC corpus
(0.7M words), the later of which was decoded with
the cdec decoder (Koehn et al, 2007; Chiang, 2007;
Dyer et al, 2010). The second system pair con-
trasts two German-English Hiero/cdec systems con-
structed from the WMT11 parallel training data
(98M words).4 The baseline system was trained on
unsegmented words, and the experimental system
was constructed using the most probable segmenta-
tion of the German text according to the CRF word
segmentation model of Dyer (2009). The Chinese-
English systems were optimized 300 times, and the
German-English systems were optimized 50 times.
Our experiments used the default implementation
of MERT that accompanies each of the two de-
coders. The Moses MERT implementation uses 20
random restart points per iteration, drawn uniformly
from the default ranges for each feature, and, at each
iteration, 200-best lists were extracted with the cur-
rent weight vector (Bertoldi et al, 2009). The cdec
MERT implementation performs inference over the
decoder search space which is structured as a hyper-
graph (Kumar et al, 2009). Rather than using restart
points, in addition to optimizing each feature inde-
pendently, it optimizes in 5 random directions per it-
eration by constructing a search vector by uniformly
sampling each element of the vector from (?1, 1)
and then renormalizing so it has length 1. For all
systems, the initial weight vector was manually ini-
tialized so as to yield reasonable translations.
4http://statmt.org/wmt11/
177
Metric System Avg ssel sdev stest
BTEC Chinese-English (n = 300)
BLEU ?
System A 48.4 1.6 0.2 0.5
System B 49.9 1.5 0.1 0.4
MET ?
System A 63.3 0.9 - 0.4
System B 63.8 0.9 - 0.5
TER ?
System A 30.2 1.1 - 0.6
System B 28.7 1.0 - 0.2
WMT German-English (n = 50)
BLEU ?
System A 18.5 0.3 0.0 0.1
System B 18.7 0.3 0.0 0.2
MET ?
System A 49.0 0.2 - 0.2
System B 50.0 0.2 - 0.1
TER ?
System A 65.5 0.4 - 0.3
System B 64.9 0.4 - 0.4
Table 1: Measured standard deviations of different au-
tomatic metrics due to test-set and optimizer variability.
sdev is reported only for the tuning objective function
BLEU.
Results are reported using BLEU (Papineni et
al., 2002), METEOR5 (Banerjee and Lavie, 2005;
Denkowski and Lavie, 2010), and TER (Snover et
al., 2006).
4.1 Extraneous variables in one system
In this section, we describe and measure (on the ex-
ample systems just described) three extraneous vari-
ables that should be considered when evaluating a
translation system. We quantify these variables in
terms of standard deviation s, since it is expressed
in the same units as the original metric. Refer to
Table 1 for the statistics.
Local optima effects sdev The first extraneous
variable we discuss is the stochasticity of the opti-
mizer. As discussed above, different optimization
runs find different local maxima. The noise due to
this variable can depend on many number of fac-
tors, including the number of random restarts used
(in MERT), the number of features in a model, the
number of references, the language pair, the portion
of the search space visible to the optimizer (e.g. 10-
best, 100-best, a lattice, a hypergraph), and the size
of the tuning set. Unfortunately, there is no proxy to
estimate this effect as with bootstrap resampling. To
control for this variable, we must run the optimizer
multiple times to estimate the spread it induces on
the development set. Using the n optimizer samples,
with mi as the translation quality measurement of
5METEOR version 1.2 with English ranking parameters and
all modules.
the development set for the ith optimization run, and
m is the average of all mis, we report the standard
deviation over the tuning set as sdev:
sdev =
?
?
?
?
n?
i=1
(mi ?m)
2
n? 1
A high sdev value may indicate that the optimizer is
struggling with local optima and changing hyperpa-
rameters (e.g. more random restarts in MERT) could
improve system performance.
Overfitting effects stest As with any optimizer,
there is a danger that the optimal weights for a tuning
set may not generalize well to unseen data (i.e., we
overfit). For a randomized optimizer, this means that
parameters can generalize to different degrees over
multiple optimizer runs. We measure the spread in-
duced by optimizer randomness on the test set met-
ric score stest, as opposed to the overfitting effect in
isolation. The computation of stest is identical to sdev
except that the mis are the translation metrics cal-
culated on the test set. In Table 1, we observe that
stest > sdev, indicating that optimized parameters are
likely not generalizing well.
Test set selection ssel The final extraneous vari-
able we consider is the selection of the test set it-
self. A good test set should be representative of
the domain or language for which experimental ev-
idence is being considered. However, with only a
single test corpus, we may have unreliable results
because of idiosyncrasies in the test set. This can
be mitigated in two ways. First, replication of ex-
periments by testing on multiple, non-overlapping
test sets can eliminate it directly. Since this is not
always practical (more test data may not be avail-
abile), the widely-used bootstrap resampling method
(?3) also controls for test set effects by resampling
multiple ?virtual? test sets from a single set, making
it possible to infer distributional parameters such as
the standard deviation of the translation metric over
(very similar) test sets.6 Furthermore, this can be
done for each of our optimizer samples. By averag-
ing the bootstrap-estimated standard deviations over
6Unlike actually using multiple test sets, bootstrap resam-
pling does not help to re-estimate the mean metric score due to
test set spread (unlike actually using multiple test sets) since the
mean over bootstrap replicates is approximately the aggregate
metric score.
178
optimizer samples, we have a statistic that jointly
quantifies the impact of test set effects and optimizer
instability on a test set. We call this statistic ssel.
Different values of this statistic can suggest method-
ological improvements. For example, a large ssel in-
dicates that more replications will be necessary to
draw reliable inferences from experiments on this
test set, so a larger test set may be helpful.
To compute ssel, assume we have n indepen-
dent optimization runs which produced weight vec-
tors that were used to translate a test set n times.
The test set has ` segments with references R =
?R1, R2, . . . , R`?. Let X = ?X1,X2, . . . ,Xn?
where each Xi = ?Xi1, Xi2, . . . , Xi`? is the list of
translated segments from the ith optimization run
list of the ` translated segments of the test set. For
each hypothesis output Xi, we construct k bootstrap
replicates by drawing ` segments uniformly, with re-
placement, from Xi, together with its corresponding
reference. This produces k virtual test sets for each
optimization run i. We designate the score of the jth
virtual test set of the ith optimization run with mij .
If mi = 1k
?k
j=1 mij , then we have:
si =
?
?
?
?
k?
j=1
(mij ?mi)
2
k ? 1
ssel =
1
n
n?
i=1
si
4.2 Comparing Two Systems
In the previous section, we gave statistics about
the distribution of evaluation metrics across a large
number of experimental samples (Table 1). Because
of the large number of trials we carried out, we can
be extremely confident in concluding that for both
pairs of systems, the experimental manipulation ac-
counts for the observed metric improvements, and
furthermore, that we have a good estimate of the
magnitude of that improvement. However, it is not
generally feasible to perform as many replications
as we did, so here we turn to the question of how
to compare two systems, accounting for optimizer
noise, but without running 300 replications.
We begin with a visual illustration how opti-
mizer instability affects test set scores when com-
paring two systems. Figure 1 plots the histogram
of the 300 optimizer samples each from the two
BTEC Chinese-English systems. The phrase-based
46 47 48 49 50 51
BLEU
0
5
10
15
20
25
30
35
40
Ob
se
rv
at
ion
 C
ou
nt
Figure 1: Histogram of test set BLEU scores for the
BTEC phrase-based system (left) and BTEC hierarchical
system (right). While the difference between the systems
is 1.5 BLEU in expectation, there is a non-trivial region
of overlap indicating that some random outcomes will re-
sult in little to no difference being observed.
0.6 0.3 0.0 0.3 0.6 0.9
BLEU difference
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Pr
ob
ab
ilit
y 
of
 o
bs
er
va
tio
n 1 sample
3 samples
5 samples
10 samples
50 samples
Figure 2: Relative frequencies of obtaining differences
in BLEU scores on the WMT system as a function of the
number of optimizer samples. The expected difference
is 0.2 BLEU. While there is a reasonably high chance of
observing a non-trivial improvement (or even a decline)
for 1 sample, the distribution quickly peaks around the
expected value given just a few more samples.
system?s distribution is centered at the sample
mean 48.4, and the hierarchical system is centered
at 49.9, a difference of 1.5 BLEU, correspond-
ing to the widely replicated result that hierarchi-
cal phrase-based systems outperform conventional
phrase-based systems in Chinese-English transla-
tion. Crucially, although the distributions are dis-
tinct, there is a non-trivial region of overlap, and
experimental samples from the overlapping region
could suggest the opposite conclusion!
To further underscore the risks posed by this over-
lap, Figure 2 plots the relative frequencies with
which different BLEU score deltas will occur, as a
function of the number of optimizer samples used.
When is a difference significant? To determine
whether an experimental manipulation results in a
179
statistically reliable difference for an evaluation met-
ric, we use a stratified approximate randomization
(AR) test. This is a nonparametric test that approxi-
mates a paired permutation test by sampling permu-
tations (Noreen, 1989). AR estimates the probability
(p-value) that a measured difference in metric scores
arose by chance by randomly exchanging sentences
between the two systems. If there is no significant
difference between the systems (i.e., the null hypoth-
esis is true), then this shuffling should not change
the computed metric score. Crucially, this assumes
that the samples being analyzed are representative
of all extraneous variables that could affect the out-
come of the experiment. Therefore, we must include
multiple optimizer replications. Also, since metric
scores (such as BLEU) are in general not compa-
rable across test sets, we stratify, exchanging only
hypotheses that correspond to the same sentence.
Table 2 shows the p-values computed by AR, test-
ing the significance of the differences between the
two systems in each pair. The first three rows illus-
trate ?single sample? testing practice. Depending on
luck with MERT, the results can vary widely from
insignificant (at p > .05) to highly significant.
The last two lines summarize the results of the test
when a small number of replications are performed,
as ought to be reasonable in a research setting. In
this simulation, we randomly selected n optimizer
outputs from our large pool and ran the AR test to
determine the significance; we repeated this proce-
dure 250 times. The p-values reported are the p-
values at the edges of the 95% confidence interval
(CI) according to AR seen in the 250 simulated com-
parison scenarios. These indicate that we are very
likely to observe a significant difference for BTEC
at n = 5, and a very significant difference by n = 50
(Table 2). Similarly, we see this trend in the WMT
system: more replications leads to more significant
results, which will be easier to reproduce. Based on
the average performance of the systems reported in
Table 1, we expect significance over a large enough
number of independent trials.
5 Discussion and Recommendations
No experiment can completely control for all pos-
sible confounding variables. Nor are metric scores
(even if they are statistically reliable) a substitute
for thorough human analysis. However, we believe
that the impact of optimizer instability has been ne-
p-value
n System A System B BTEC WMT
1 high low 0.25 0.95
1 median median 0.15 0.13
1 low high 0.0003 0.003
p-value (95% CI)
5 random random 0.001?0.034 0.001?0.38
50 random random 0.001?0.001 0.001?0.33
Table 2: Two-system analysis: AR p-values for three
different ?single sample? scenarios that illustrate differ-
ent pathological scenarios that can result when the sam-
pled weight vectors are ?low? or ?high.? For ?random,?
we simulate an experiments with n optimization replica-
tions by drawing n optimized system outputs from our
pool and performing AR; this simulation was repeated
250 times and the 95% CI of the AR p-values is reported.
glected by standard experimental methodology in
MT research, where single-sample measurements
are too often used to assess system differences. In
this paper, we have provided evidence that optimizer
instability can have a substantial impact on results.
However, we have also shown that it is possible to
control for it with very few replications (Table 2).
We therefore suggest:
? Replication be adopted as standard practice in
MT experimental methodology, especially in
reporting results;7
? Replication of optimization (MERT) and test
set evaluation be performed at least three times;
more replications may be necessary for experi-
mental manipulations with more subtle effects;
? Use of the median system according to a trusted
metric when manually analyzing system out-
put; preferably, the median should be deter-
mined based on one test set and a second test
set should be manually analyzed.
Acknowledgments
We thank Michael Denkowski, Kevin Gimpel, Kenneth
Heafield, Michael Heilman, and Brendan O?Connor for
insightful feedback. This research was supported in part
by the National Science Foundation through TeraGrid re-
sources provided by Pittsburgh Supercomputing Center
under TG-DBS110003; the National Science Foundation
under IIS-0713402, IIS-0844507, IIS-0915187, and IIS-
0915327; the DARPA GALE program, the U. S. Army
Research Laboratory, and the U. S. Army Research Of-
fice under contract/grant number W911NF-10-1-0533.
7Source code to carry out the AR test for multiple optimizer
samples on the three metrics in this paper is available from
http://github.com/jhclark/multeval.
180
References
A. Arun, B. Haddow, P. Koehn, A. Lopez, C. Dyer,
and P. Blunsom. 2010. Monte Carlo techniques
for phrase-based translation. Machine Translation,
24:103?121.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for mt evaluation with improved corre-
lation with human judgments. In Proc. of ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization.
N. Bertoldi, B. Haddow, and J.-B. Fouet. 2009. Im-
proved minimum error rate training in Moses. Prague
Bulletin of Mathematical Linguistics, No. 91:7?16.
M. Bisani and H. Ney. 2004. Bootstrap estimates for
confidence intervals in ASR performance evaluation.
In Proc. of ICASSP.
A. Cahill, M. Burke, R. O?Donovan, S. Riezler, J. van
Genabith, and A. Way. 2008. Wide-coverage deep
statistical parsing using automatic dependency struc-
ture annotation. Computational Linguistics, 34(1):81?
124.
D. Cer, D. Jurafsky, and C. D. Manning. 2008. Regular-
ization and search for minimum error rate training. In
Proc. of WMT.
D. Cer, C. D. Manning, and D. Jurafsky. 2010. The best
lexical metric for phrase-based statistical mt system
optimization. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 555?563. Proc. of ACL, June.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. of EMNLP.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
N. Chinchor. 1993. The statistical significance of the
MUC-5 results. Proc. of MUC.
K. Crammer, O. Dekel, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research,
7:551?585.
M. Denkowski and A. Lavie. 2010. Extending the
METEOR machine translation evaluation metric to the
phrase level. In Proc. of NAACL.
C. Dyer, J. Weese, A. Lopez, V. Eidelman, P. Blunsom,
and P. Resnik. 2010. cdec: A decoder, alignment,
and learning framework for finite-state and context-
free translation models. In Proc. of ACL.
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
B. Efron. 1979. Bootstrap methods: Another look at the
jackknife. The Annals of Statistics, 7(1):1?26.
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
P. Koehn, A. Birch, C. Callison-burch, M. Federico,
N. Bertoldi, B. Cowan, C. Moran, C. Dyer, A. Con-
stantin, and E. Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009.
Efficient minimum error rate training and minimum
Bayes-risk decoding for translation hypergraphs and
lattices. In Proc. of ACL-IJCNLP.
R. C. Moore and C. Quirk. 2008. Random restarts
in minimum error rate training for statistical machine
translation. In Proc. of COLING, Manchester, UK.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses: An Introduction. Wiley-
Interscience.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-j. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
MT. In Proc. of the Workshop on Intrinsic and Extrin-
sic Evaluation Methods for Machine Translation and
Summarization.
M. Snover, B. Dorr, C. Park, R. Schwartz, L. Micciulla,
and J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA.
L. Wasserman. 2003. All of Statistics: A Concise Course
in Statistical Inference. Springer.
Y. Zhang and S. Vogel. 2010. Significance tests of auto-
matic machine translation metrics. Machine Transla-
tion, 24:51?65.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
BLEU/NIST scores: How much improvement do we
need to have a better system? In Proc. of LREC.
181
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690?696,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Scalable Modified Kneser-Ney Language Model Estimation
Kenneth Heafield?,? Ivan Pouzyrevsky? Jonathan H. Clark? Philipp Koehn?
?University of Edinburgh
10 Crichton Street
Edinburgh EH8 9AB, UK
?Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
?Yandex
Zelenograd, bld. 455 fl. 128
Moscow 124498, Russia
heafield@cs.cmu.edu ivan.pouzyrevsky@gmail.com jhclark@cs.cmu.edu pkoehn@inf.ed.ac.uk
Abstract
We present an efficient algorithm to es-
timate large modified Kneser-Ney mod-
els including interpolation. Streaming
and sorting enables the algorithm to scale
to much larger models by using a fixed
amount of RAM and variable amount of
disk. Using one machine with 140 GB
RAM for 2.8 days, we built an unpruned
model on 126 billion tokens. Machine
translation experiments with this model
show improvement of 0.8 BLEU point
over constrained systems for the 2013
Workshop on Machine Translation task in
three language pairs. Our algorithm is also
faster for small models: we estimated a
model on 302 million tokens using 7.7%
of the RAM and 14.0% of the wall time
taken by SRILM. The code is open source
as part of KenLM.
1 Introduction
Relatively low perplexity has made modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1998) a popular choice for
language modeling. However, existing estima-
tion methods require either large amounts of RAM
(Stolcke, 2002) or machines (Brants et al, 2007).
As a result, practitioners have chosen to use
less data (Callison-Burch et al, 2012) or simpler
smoothing methods (Brants et al, 2007).
Backoff-smoothed n-gram language models
(Katz, 1987) assign probability to a word wn in
context wn?11 according to the recursive equation
p(wn|wn?11 ) =
{
p(wn|wn?11 ), if wn1 was seen
b(wn?11 )p(wn|wn2 ), otherwise
The task is to estimate probability p and backoff
b from text for each seen entry wn1 . This paper
Filesystem
Map
Reduce 1
Filesystem
Identity Map
Reduce 2
Filesystem ...
MapReduce Steps
Filesystem
Map
Reduce 1
Reduce 2...
Optimized
Figure 1: Each MapReduce performs three copies
over the network when only one is required. Ar-
rows denote copies over the network (i.e. to and
from a distributed filesystem). Both options use
local disk within each reducer for merge sort.
contributes an efficient multi-pass streaming algo-
rithm using disk and a user-specified amount of
RAM.
2 Related Work
Brants et al (2007) showed how to estimate
Kneser-Ney models with a series of five MapRe-
duces (Dean and Ghemawat, 2004). On 31 billion
words, estimation took 400 machines for two days.
Recently, Google estimated a pruned Kneser-Ney
model on 230 billion words (Chelba and Schalk-
wyk, 2013), though no cost was provided.
Each MapReduce consists of one layer of map-
pers and an optional layer of reducers. Mappers
read from a network filesystem, perform optional
processing, and route data to reducers. Reducers
process input and write to a network filesystem.
Ideally, reducers would send data directly to an-
other layer of reducers, but this is not supported.
Their workaround, a series of MapReduces, per-
forms unnecessary copies over the network (Fig-
ure 1). In both cases, reducers use local disk.
690
Writing and reading from the distributed filesys-
tem improves fault tolerance. However, the same
level of fault tolerance could be achieved by
checkpointing to the network filesystem then only
reading in the case of failures. Doing so would en-
able reducers to start processing without waiting
for the network filesystem to write all the data.
Our code currently runs on a single machine
while MapReduce targets clusters. Appuswamy et
al. (2013) identify several problems with the scale-
out approach of distributed computation and put
forward several scenarios in which a single ma-
chine scale-up approach is more cost effective in
terms of both raw performance and performance
per dollar.
Brants et al (2007) contributed Stupid Backoff,
a simpler form of smoothing calculated at runtime
from counts. With Stupid Backoff, they scaled to
1.8 trillion tokens. We agree that Stupid Backoff
is cheaper to estimate, but contend that this work
makes Kneser-Ney smoothing cheap enough.
Another advantage of Stupid Backoff has been
that it stores one value, a count, per n-gram in-
stead of probability and backoff. In previous work
(Heafield et al, 2012), we showed how to collapse
probability and backoff into a single value without
changing sentence-level probabilities. However,
local scores do change and, like Stupid Backoff,
are no longer probabilities.
MSRLM (Nguyen et al, 2007) aims to scal-
ably estimate language models on a single ma-
chine. Counting is performed with streaming algo-
rithms similarly to this work. Their parallel merge
sort also has the potential to be faster than ours.
The biggest difference is that their pipeline de-
lays some computation (part of normalization and
all of interpolation) until query time. This means
that it cannot produce a standard ARPA file and
that more time and memory are required at query
time. Moreover, they use memory mapping on en-
tire files and these files may be larger than physi-
cal RAM. We have found that, even with mostly-
sequential access, memory mapping is slower be-
cause the kernel does not explicitly know where
to read ahead or write behind. In contrast, we use
dedicated threads for reading and writing. Perfor-
mance comparisons are omitted because we were
unable to compile and run MSRLM on recent ver-
sions of Linux.
SRILM (Stolcke, 2002) estimates modified
Kneser-Ney models by storing n-grams in RAM.
Corpus
Counting
Adjusting Counts
DivisionSumming
Interpolation
Model
Figure 2: Data flow in the estimation pipeline.
Normalization has two threads per order: sum-
ming and division. Thick arrows indicate sorting.
It also offers a disk-based pipeline for initial steps
(i.e. counting). However, the later steps store
all n-grams that survived count pruning in RAM.
Without pruning, both options use the same RAM.
IRSTLM (Federico et al, 2008) does not imple-
ment modified Kneser-Ney but rather an approxi-
mation dubbed ?improved Kneser-Ney? (or ?mod-
ified shift-beta? depending on the version). Esti-
mation is done in RAM. It can also split the corpus
into pieces and separately build each piece, intro-
ducing further approximation.
3 Estimation Pipeline
Estimation has four streaming passes: counting,
adjusting counts, normalization, and interpolation.
Data is sorted between passes, three times in total.
Figure 2 shows the flow of data.
3.1 Counting
For a language model of order N , this step counts
all N -grams (with length exactly N ) by streaming
through the corpus. Words near the beginning of
sentence also formN -grams padded by the marker
<s> (possibly repeated multiple times). The end
of sentence marker </s> is appended to each sen-
tence and acts like a normal token.
Unpruned N -gram counts are sufficient, so
lower-order n-grams (n < N ) are not counted.
Even pruned models require unpruned N -gram
counts to compute smoothing statistics.
Vocabulary mapping is done with a hash table.1
Token strings are written to disk and a 64-bit Mur-
1This hash table is the only part of the pipeline that can
grow. Users can specify an estimated vocabulary size for
memory budgeting. In future work, we plan to support lo-
cal vocabularies with renumbering.
691
Suffix
3 2 1
Z B A
Z A B
B B B
Context
2 1 3
Z A B
B B B
Z B A
Figure 3: In suffix order, the last word is primary.
In context order, the penultimate word is primary.
murHash2 token identifier is retained in RAM.
Counts are combined in a hash table and spilled
to disk when a fixed amount of memory is full.
Merge sort also combines identical N -grams (Bit-
ton and DeWitt, 1983).
3.2 Adjusting Counts
The counts c are replaced with adjusted counts a.
a(wn1 ) =
{
c(wn1 ), if n = N or w1 = <s>
|v : c(vwn1 ) > 0|, otherwise
Adjusted counts are computed by streaming
through N -grams sorted in suffix order (Figure 3).
The algorithm keeps a running total a(wNi ) for
each i and compares consecutive N -grams to de-
cide which adjusted counts to output or increment.
Smoothing statistics are also collected. For each
length n, it collects the number tn,k of n-grams
with adjusted count k ? [1, 4].
tn,k = |{wn1 : a(wn1 ) = k}|
These are used to compute closed-form estimates
(Chen and Goodman, 1998) of discounts Dn(k)
Dn(k) = k ?
(k + 1)tn,1tn,k+1
(tn,1 + 2tn,2)tn,k
for k ? [1, 3]. Other cases are Dn(0) = 0 and
Dn(k) = Dn(3) for k ? 3. Less formally, counts
0 (unknown) through 2 have special discounts.
3.3 Normalization
Normalization computes pseudo probability u
u(wn|wn?11 ) =
a(wn1 )?Dn(a(wn1 ))?
x a(wn?11 x)
and backoff b
b(wn?11 ) =
?3
i=1Dn(i)|{x : a(wn?11 x) = i}|?
x a(wn?11 x)
2https://code.google.com/p/smhasher/
The difficulty lies in computing denominator?
x a(wn?11 x) for all wn?11 . For this, we sort in
context order (Figure 3) so that, for every wn?11 ,
the entries wn?11 x are consecutive. One pass col-
lects both the denominator and backoff3 terms
|{x : a(wn?11 x) = i}| for i ? [1, 3].
A problem arises in that denominator?
x a(wn?11 x) is known only after streaming
through all wn?11 x, but is needed immediately
to compute each u(wn|wn?11 ). One option is to
buffer in memory, taking O(N |vocabulary|) space
since each order is run independently in parallel.
Instead, we use two threads for each order. The
sum thread reads ahead to compute?x a(wn?11 x)
and b(wn?11 ) then places these in a secondary
stream. The divide thread reads the input and the
secondary stream then writes records of the form
(wn1 , u(wn|wn?11 ), b(wn?11 )) (1)
The secondary stream is short so that data read by
the sum thread will likely be cached when read by
the divide thread. This sort of optimization is not
possible with most MapReduce implementations.
Because normalization streams through wn?11 x
in context order, the backoffs b(wn?11 ) are com-
puted in suffix order. This will be useful later
(?3.5), so backoffs are written to secondary files
(one for each order) as bare values without keys.
3.4 Interpolation
Chen and Goodman (1998) found that perplex-
ity improves when the various orders within the
same model are interpolated. The interpolation
step computes final probability p according to the
recursive equation
p(wn|wn?11 ) = u(wn|wn?11 )+b(wn?11 )p(wn|wn?12 )
(2)
Recursion terminates when unigrams are interpo-
lated with the uniform distribution
p(wn) = u(wn) + b()
1
|vocabulary|
where  denotes the empty string. The unknown
word counts as part of the vocabulary and has
count zero,4 so its probability is b()/|vocabulary|.
3Sums and counts are done with exact integer arithmetic.
Thus, every floating-point value generated by our toolkit is
the result of O(N) floating-point operations. SRILM has nu-
merical precision issues because it uses O(N |vocabulary|)
floating-point operations to compute backoff.
4SRILM implements ?another hack? that computes
pSRILM(wn) = u(wn) and pSRILM(<unk>) = b() when-
ever p(<unk>) < 3? 10?6, as it usually is. We implement
both and suspect their motivation was numerical precision.
692
Probabilities are computed by streaming in suf-
fix lexicographic order: wn appears before wnn?1,
which in turn appears before wnn?2. In this way,
p(wn) is computed before it is needed to compute
p(wn|wn?1), and so on. This is implemented by
jointly iterating through N streams, one for each
length of n-gram. The relevant pseudo probability
u(wn|wn?11 ) and backoff b(wn?11 ) appear in the
input records (Equation 1).
3.5 Joining
The last task is to unite b(wn1 ) computed in ?3.3
with p(wn|wn?11 ) computed in ?3.4 for storage in
the model. We note that interpolation (Equation 2)
used the different backoff b(wn?11 ) and so b(wn1 )
is not immediately available. However, the back-
off values were saved in suffix order (?3.3) and in-
terpolation produces probabilities in suffix order.
During the same streaming pass as interpolation,
we merge the two streams.5 Suffix order is also
convenient because the popular reverse trie data
structure can be built in the same pass.6
4 Sorting
Much work has been done on efficient disk-based
merge sort. Particularly important is arity, the
number of blocks that are merged at once. Low
arity leads to more passes while high arity in-
curs more disk seeks. Abello and Vitter (1999)
modeled these costs and derived an optimal strat-
egy: use fixed-size read buffers (one for each
block being merged) and set arity to the number of
buffers that fit in RAM. The optimal buffer size is
hardware-dependent; we use 64 MB by default. To
overcome the operating system limit on file han-
dles, multiple blocks are stored in the same file.
To further reduce the costs of merge sort, we
implemented pipelining (Dementiev et al, 2008).
If there is enough RAM, input is lazily merged
and streamed to the algorithm. Output is cut into
blocks, sorted in the next step?s desired order, and
then written to disk. These optimizations elim-
inate up to two copies to disk if enough RAM
is available. Input, the algorithm, block sorting,
and output are all threads on a chain of producer-
consumer queues. Therefore, computation and
disk operations happen simultaneously.
5Backoffs only exist if the n-gram is the context of some
n+ 1-gram, so merging skips n-grams that are not contexts.
6With quantization (Whittaker and Raj, 2001), the quan-
tizer is trained in a first pass and applied in a second pass.
0
10
20
30
40
50
0 200 400 600 800 1000
RA
M
(G
B)
Tokens (millions)
SRI
SRI compact
IRST
This work
Figure 4: Peak virtual memory usage.
0
2
4
6
8
10
12
14
0 200 400 600 800 1000
CP
U
tim
e(
ho
urs
)
Tokens (millions)
SRI
SRI compact
IRST
This work
Figure 5: CPU usage (system plus user).
Each n-gram record is an array of n vocabu-
lary identifiers (4 bytes each) and an 8-byte count
or probability and backoff. At peak, records are
stored twice on disk because lazy merge sort is
not easily amenable to overwriting the input file.
Additional costs are the secondary backoff file (4
bytes per backoff) and the vocabulary in plaintext.
5 Experiments
Experiments use ClueWeb09.7 After spam filter-
ing (Cormack et al, 2011), removing markup, se-
lecting English, splitting sentences (Koehn, 2005),
deduplicating, tokenizing (Koehn et al, 2007),
and truecasing, 126 billion tokens remained.
7http://lemurproject.org/clueweb09/
693
1 2 3 4 5
393 3,775 17,629 39,919 59,794
Table 1: Counts of unique n-grams (in millions)
for the 5 orders in the large LM.
5.1 Estimation Comparison
We estimated unpruned language models in bi-
nary format on sentences randomly sampled from
ClueWeb09. SRILM and IRSTLM were run un-
til the test machine ran out of RAM (64 GB).
For our code, the memory limit was set to 3.5
GB because larger limits did not improve perfor-
mance on this small data. Results are in Figures
4 and 5. Our code used an average of 1.34?1.87
CPUs, so wall time is better than suggested in Fig-
ure 5 despite using disk. Other toolkits are single-
threaded. SRILM?s partial disk pipeline is not
shown; it used the same RAM and took more time.
IRSTLM?s splitting approximation took 2.5 times
as much CPU and about one-third the memory (for
a 3-way split) compared with normal IRSTLM.
For 302 million tokens, our toolkit used 25.4%
of SRILM?s CPU time, 14.0% of the wall time,
and 7.7% of the RAM. Compared with IRSTLM,
our toolkit used 16.4% of the CPU time, 9.0% of
the wall time, and 16.6% of the RAM.
5.2 Scaling
We built an unpruned model (Table 1) on 126 bil-
lion tokens. Estimation used a machine with 140
GB RAM and six hard drives in a RAID5 configu-
ration (sustained read: 405 MB/s). It took 123 GB
RAM, 2.8 days wall time, and 5.4 CPU days. A
summary of Google?s results from 2007 on differ-
ent data and hardware appears in ?2.
We then used this language model as an ad-
ditional feature in unconstrained Czech-English,
French-English, and Spanish-English submissions
to the 2013 Workshop on Machine Translation.8
Our baseline is the University of Edinburgh?s
phrase-based Moses (Koehn et al, 2007) submis-
sion (Durrani et al, 2013), which used all con-
strained data specified by the evaluation (7 billion
tokens of English). It placed first by BLEU (Pap-
ineni et al, 2002) among constrained submissions
in each language pair we consider.
In order to translate, the large model was quan-
tized (Whittaker and Raj, 2001) to 10 bits and
compressed to 643 GB with KenLM (Heafield,
8http://statmt.org/wmt13/
Source Baseline Large
Czech 27.4 28.2
French 32.6 33.4
Spanish 31.8 32.6
Table 2: Uncased BLEU results from the 2013
Workshop on Machine Translation.
2011) then copied to a machine with 1 TB RAM.
Better compression methods (Guthrie and Hepple,
2010; Talbot and Osborne, 2007) and distributed
language models (Brants et al, 2007) could reduce
hardware requirements. Feature weights were re-
tuned with PRO (Hopkins and May, 2011) for
Czech-English and batch MIRA (Cherry and Fos-
ter, 2012) for French-English and Spanish-English
because these worked best for the baseline. Un-
cased BLEU scores on the 2013 test set are shown
in Table 2. The improvement is remarkably con-
sistent at 0.8 BLEU point in each language pair.
6 Conclusion
Our open-source (LGPL) estimation code is avail-
able from kheafield.com/code/kenlm/
and should prove useful to the community. Sort-
ing makes it scalable; efficient merge sort makes
it fast. In future work, we plan to extend to the
Common Crawl corpus and improve parallelism.
Acknowledgements
Miles Osborne preprocessed ClueWeb09. Mo-
hammed Mediani contributed to early designs.
Jianfeng Gao clarified how MSRLM operates.
This work used the Extreme Science and Engi-
neering Discovery Environment (XSEDE), which
is supported by National Science Foundation grant
number OCI-1053575. We used Stampede and
Trestles under allocation TG-CCR110017. Sys-
tem administrators from the Texas Advanced
Computing Center (TACC) at The University of
Texas at Austin made configuration changes on
our request. This work made use of the resources
provided by the Edinburgh Compute and Data Fa-
cility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT ini-
tiative (http://www.edikt.org.uk/). The
research leading to these results has received fund-
ing from the European Union Seventh Framework
Programme (FP7/2007-2013) under grant agree-
ment 287658 (EU BRIDGE).
694
References
James M. Abello and Jeffrey Scott Vitter, editors.
1999. External memory algorithms. American
Mathematical Society, Boston, MA, USA.
Raja Appuswamy, Christos Gkantsidis, Dushyanth
Narayanan, Orion Hodson, and Antony Rowstron.
2013. Nobody ever got fired for buying a cluster.
Technical Report MSR-TR-2013-2, Microsoft Re-
search.
Dina Bitton and David J DeWitt. 1983. Duplicate
record elimination in large data files. ACM Trans-
actions on database systems (TODS), 8(2):255?265.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Language Learning, pages 858?867, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Ciprian Chelba and Johan Schalkwyk, 2013. Em-
pirical Exploration of Language Modeling for the
google.com Query Stream as Applied to Mobile
Voice Search, pages 197?229. Springer, New York.
Stanley Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Harvard
University, August.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Gordon V Cormack, Mark D Smucker, and Charles LA
Clarke. 2011. Efficient and effective spam filtering
and re-ranking for large web datasets. Information
retrieval, 14(5):441?465.
Jeffrey Dean and Sanjay Ghemawat. 2004. MapRe-
duce: Simplified data processing on large clusters.
In OSDI?04: Sixth Symposium on Operating Sys-
tem Design and Implementation, San Francisco, CA,
USA, 12.
Roman Dementiev, Lutz Kettner, and Peter Sanders.
2008. STXXL: standard template library for XXL
data sets. Software: Practice and Experience,
38(6):589?637.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the ACL 2013 Eighth Workshop on Sta-
tistical Machine Translation, Sofia, Bulgaria, Au-
gust.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, Brisbane, Australia.
David Guthrie and Mark Hepple. 2010. Storing the
web in memory: Space efficient language mod-
els with constant time retrieval. In Proceedings of
EMNLP 2010, Los Angeles, CA.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012. Language model rest costs and space-efficient
storage. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, Edin-
burgh, UK, July. Association for Computational Lin-
guistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, July.
Slava Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics,
Speech, and Signal Processing, ASSP-35(3):400?
401, March.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, pages
181?184.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), Prague, Czech Repub-
lic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit.
Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.
2007. MSRLM: a scalable language modeling
toolkit. Technical Report MSR-TR-2007-144, Mi-
crosoft Research.
695
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Sev-
enth International Conference on Spoken Language
Processing, pages 901?904.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine trans-
lation. In Proceedings of ACL, pages 512?519,
Prague, Czech Republic.
Edward Whittaker and Bhiksha Raj. 2001.
Quantization-based language model compres-
sion. In Proceedings of Eurospeech, pages 33?36,
Aalborg, Denmark, December.
696
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 82?87,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Improved Features and Grammar Selection for Syntax-Based MT
Greg Hanneman and Jonathan Clark and Alon Lavie
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213 USA
{ghannema, jhclark, alavie}@cs.cmu.edu
Abstract
We present the Carnegie Mellon Univer-
sity Stat-XFER group submission to the
WMT 2010 shared translation task. Up-
dates to our syntax-based SMT system
mainly fell in the areas of new feature for-
mulations in the translation model and im-
proved filtering of SCFG rules. Compared
to our WMT 2009 submission, we report
a gain of 1.73 BLEU by using the new
features and decoding environment, and a
gain of up to 0.52 BLEU from improved
grammar selection.
1 Introduction
From its earlier focus on linguistically rich ma-
chine translation for resource-poor languages, the
statistical transfer MT group at Carnegie Mellon
University has expanded in recent years to the in-
creasingly successful domain of syntax-based sta-
tistical MT in large-data scenarios. Our submis-
sion to the 2010 Workshop on Machine Transla-
tion is a syntax-based SMT system with a syn-
chonous context-free grammar (SCFG), where the
SCFG rules are derived from full constituency
parse trees on both the source and target sides of
parallel training sentences. We participated in the
French-to-English shared translation task.
This year, we focused our efforts on making
more and better use of syntactic grammar. Much
of the work went into formulating a more expan-
sive feature set in the translation model and a new
method of assigning scores to phrase pairs and
grammar rules. Following a change of decoder
that allowed us to experiment with systems using
much larger syntactic grammars than previously,
we also adapted a technique to more intelligently
pre-filter grammar rules to those most likely to be
useful.
2 System Overview
We built our system on a partial selection of
the provided French?English training data, us-
ing the Europarl, News Commentary, and UN
sets, but ignoring the Giga-FrEn data. After
tokenization and some pruning of our training
data, this left us with a corpus of approximately
8.6 million sentence pairs. We word-aligned the
corpus with MGIZA++ (Gao and Vogel, 2008),
a multi-threaded implementation of the standard
word alignment tool GIZA++ (Och and Ney,
2003). Word alignments were symmetrized with
the ?grow-diag-final-and? heuristic. We automati-
cally parsed the French side of the corpus with the
Berkeley parser (Petrov and Klein, 2007), while
we used the fast vanilla PCFG model of the Stan-
ford parser (Klein and Manning, 2003) for the
English side. These steps resulted in a parallel
parsed corpus from which to extract phrase pairs
and grammar rules.
Phrase extraction involves three distinct steps.
In the first, we perform standard (non-syntactic)
phrase extraction according to the heuristics of
phrase-based SMT (Koehn et al, 2003). In the
second, we obtain syntactic phrase pairs using
the tree-to-tree matching method of Lavie et al
(2008). Briefly, this method aligns nodes in par-
allel parse trees by projecting up from the word
alignments. A source-tree node s will be aligned
to a target-tree node t if the word alignments in the
yield of s all land within the yield of t, and vice
versa. This node alignment is similar in spirit to
the subtree alignment method of Zhechev and Way
(2008), except our method is based on the spe-
cific Viterbi word alignment links found for each
82
sentence rather than on the general word trans-
lation probabilities computed for the corpus as a
whole. This enables us to use efficient dynamic
programming to infer node alignments, rather than
resorting to a greedy search or the enumeration of
all possible alignments. Finally, in the third step,
we use the node alignments from syntactic phrase
pair extraction to extract grammar rules. Each
aligned node in a tree pair specifies a decompo-
sition point for breaking the parallel trees into a
series of SCFG rules. Like Galley et al (2006),
we allow ?composed? (non-minimal) rules when
they build entirely on lexical items. However, to
control the size of the grammar, we do not produce
composed rules that build on other non-terminals,
nor do we produce multiple possible rules when
we encounter unaligned words. Another differ-
ence is that we discard internal structure of com-
posed lexical rules so that we produce SCFG rules
rather than synchronous tree substitution grammar
rules.
The extracted phrase pairs and grammar rules
are collected together and scored according to a
variety of features (Section 3). Instead of decod-
ing with the very large complete set of extracted
grammar rules, we select only a small number of
rules meeting certain criteria (Section 4).
In contrast to previous years, when we used the
Stat-XFER decoder, this year we switched to the
the Joshua decoder (Li et al, 2009) to take advan-
tage of its more efficient architecture and imple-
mentation of modern decoding techniques, such as
cube pruning and multi-threading. We also man-
aged system-building workflows with LoonyBin
(Clark and Lavie, 2010), a toolkit for managing
multi-step experiments across different servers or
computing clusters. Section 5 details our experi-
mental results.
3 Translation Model Construction
One major improvement in our system this year
is the feature scores we applied to our grammar
and phrase pairs. Inspired largely by the Syntax-
Augmented MT system (Zollmann and Venu-
gopal, 2006), our translation model contains 22
features in addition to the language model. In con-
trast to earlier formulations of our features (Han-
neman and Lavie, 2009), our maximum-likelihood
features are now based on a strict separation be-
tween counts drawn from non-syntactic phrase ex-
traction heuristics and our syntactic rule extractor;
no feature is estimated from counts in both spaces.
We define an aggregate rule instance as a 5-
tuple r = (L,S, T,Cphr, Csyn) that contains a
left-hand-side label L, a sequence of terminals
and non-terminals for the source (S) and target
(T ) right-hand sides, and aggregated counts from
phrase-based SMT extraction heuristics Cphr and
the syntactic rule extractor Csyn.
In preparation for feature scoring, we:
1. Run phrase instance extraction using stan-
dard phrase-based SMT heuristics to obtain
tuples (PHR, S, T,Cphr, ?) where S and T
never contain non-terminals
2. Run syntactic rule instance extraction as de-
scribed in Section 2 above to obtain tuples
(L,S, T, ?, Csyn)
3. Share non-syntactic counts such that, for
any two tuples r1 = (PHR, S, T,Cphr, ?)
and r2 = (L2, S, T, ?, Csyn) with equiv-
alent S and T values, we produce r2 =
(L2, S, T,Cphr, Csyn)
Note that there is no longer any need to retain
PHR rules (PHR, S, T ) that have syntactic equiv-
alents (L 6= PHR, S, T ) since they have the same
features In addition, we assume there will be no
tuples where S and T contain non-terminals while
Cphr = 0 and Csyn > 0. That is, the syntactic
phrases are a subset of non-syntactic phrases.
3.1 Maximum-Likelihood Features
Our most traditional features are Pphr(T |S) and
Pphr(S |T ), estimated using only counts Cphr.
These features apply only to rules not con-
taining any non-terminals. They are equiva-
lent to the phrase P (T |S) and P (S |T ) fea-
tures from the Moses decoder, even when L 6=
PHR. In contrast, we used Psyn?phr(L,S |T ) and
Psyn?phr(L, T |S) last year, which applied to all
rules. The new features are no longer subject to
increased sparsity as the number of non-terminals
in the grammar increases.
We also have grammar rule probabili-
ties Psyn(T |S), Psyn(S |T ), Psyn(L |S),
Psyn(L |T ), and Psyn(L |S, T ) estimated using
Csyn; these apply only to rules where S and T
contain non-terminals. By no longer including
counts from phrase-based SMT extraction heuris-
tics in these features, we encourage rules where
L 6= PHR since the smaller counts from the rule
learner would have otherwise been overshadowed
83
by the much larger counts from the phrase-based
SMT heuristics.
Finally, we estimate ?not labelable? (NL) fea-
tures Psyn(NL |S) and Psyn(NL |T ). With R de-
noting the set of all extracted rules,
Psyn(NL |S) =
Csyn
?
r??R s.t. S?=S C ?syn
(1)
Psyn(NL |T ) =
Csyn
?
r??R s.t. T ?=T C ?syn
(2)
We use additive smoothing (with n = 1 for our ex-
periments) to avoid a probability of 0 when there
is no syntactic label for an (S, T ) pair. These fea-
tures can encourage syntactic rules when syntax
is likely given a particular string since probability
mass is often distributed among several different
syntactic labels.
3.2 Instance Features
We add several features that use sufficient statis-
tics local to each rule. First, we add three binary
low-count features that take on the value 1 when
the frequency of the rule is exactly 1, 2, or 3. There
are also two indicator features related to syntax:
one each that fires when L = PHR and when
L 6= PHR. Other indicator features analyze the
abstractness of grammar rules: AS = 1 when the
source side contains only non-terminals, AT = 1
when the target side contains only non-terminals,
TGTINSERTION = 1 when AS = 1, AT = 0,
SRCDELETION = 1 when AS = 0, AT = 1, and
INTERLEAVED = 1 when AS = 0, AT = 0.
Bidirectional lexical probabilities for each rule
are calculated from a unigram lexicon MLE-
estimated over aligned word pairs in the training
corpus, as is the default in Moses.
Finally, we include a glue rule indicator feature
that fires whenever a glue rule is applied during
decoding. In the Joshua decoder, these monotonic
rules stitch syntactic parse fragments together at
no model cost.
4 Grammar Selection
With extracted grammars typically reaching tens
of millions of unique rules ? not to mention
phrase pairs ? our systems clearly face an en-
gineering challenge when attempting to include
the full grammar at decoding time. Iglesias et al
(2009) classified SCFG rules according to the pat-
tern of terminals and non-terminals on the rules?
right-hand sides, and found that certain patterns
could be entirely left out of the grammar without
loss of MT quality. In particular, large classes of
monotonic rules could be removed without a loss
in automatic metric scores, while small classes of
reordering rules contributed much more to the suc-
cess of the system. Inspired by that approach, we
passed our full set of extracted grammar rule in-
stances through a filter after scoring. Using the
rule notation from Section 3, the filter retained
only those rules that matched one of the follow-
ing patterns:
S = X1 w, T = w X1
S = w X1, T = X1 w
S = X1 X2, T = X2 X1
S = X1 X2, T = X1 X2
where X represents any non-terminal and w rep-
resents any span of one or more terminals. The
choice of the specific reordering patterns above
captures our intuition that binary swaps are a fun-
damental ordering divergence between languages,
while the inclusion of the abstract monotonic pat-
tern (X1 X2,X1 X2) ensures that the decoder is
not disproportionately biased towards applying re-
ordering rules without supporting lexical evidence
merely because in-order rules are left out.
Orthogonally to the pattern-based pruning, we
also selected grammars by sorting grammar rules
in decreasing order of frequency count and using
the top n in the decoder. We experimented with
n = 0, 100, 1000, and 10,000. In all cases of
grammar selection, we disallowed rules that in-
serted unaligned target-side terminals unless the
inserted terminals were among the top 100 most
frequent unigrams in the target-side vocabulary.
5 Results and Analysis
5.1 Comparison with WMT 2009 Results
We performed our initial development work on
an updated version of our previous WMT sub-
mission (Hanneman et al, 2009) so that the ef-
fects of our changes could be directly compared.
Our 2009 system was trained from the full Eu-
roparl and News Commentary data available that
year, plus the pre-release version of the Giga-FrEn
data, for a total of 9.4 million sentence pairs. We
used the news-dev2009a set for minimum error-
rate training and tested system performance on
news-dev2009b. To maintain continuity with our
previously reported scores, we report new scores
here using the same training, tuning, and test-
ing sets, using the uncased versions of IBM-style
84
System Configuration METEOR BLEU
1. WMT ?09 submission 0.5263 0.2073
2. Joshua decoder 0.5231 0.2158
3. New TM features 0.5348 0.2246
Table 1: Dev test results (on news-dev2009b) from
our WMT 2009 system when updating decoding
environment and feature formulations.
System Configuration METEOR BLEU
1. n = 100 0.5314 0.2200
2. n = 100, filtered 0.5341 0.2242
3. n = 1000 0.5324 0.2206
4. n = 1000, filtered 0.5330 0.2233
5. n = 10,000 0.5332 0.2198
6. n = 10,000, filtered 0.5350 0.2250
Table 2: Dev test results (on news-dev2009b) from
our WMT 2009 system with and without pattern-
based grammar selection.
BLEU 1.04 (Papineni et al, 2002) and METEOR
0.6 (Lavie and Agarwal, 2007).
Table 1 shows the effect of our new scoring and
decoding environment. Line 2 uses the same ex-
tracted phrase pairs and grammar rules as line 1,
but the system is tuned and tested with the Joshua
decoder instead of Stat-XFER. For line 3, we re-
scored the extracted phrase pairs from lines 1 and
2 using the updated features discussed in Sec-
tion 3.1 The difference in automatic metric scores
shows a significant benefit from both the new de-
coder and the updated feature formulations: 0.8
BLEU points from the change in decoder, and 0.9
BLEU points from the expanded set of 22 transla-
tion model features.
Our next test was to examine the usefulness of
the pattern-based grammar selection described in
Section 4. For various numbers of rules n, Ta-
ble 2 shows the scores obtained with and without
filtering the grammar before the n most frequent
rules are skimmed off for use. We observe a small
but consistent gain in scores from the grammar se-
lection process, up to half a BLEU point in the
largest-grammar systems (lines 5 and 6).
1In line 2, we did not control for difference in formulation
of the translation length feature: Stat-XFER uses a length
ratio, while Joshua uses a target word count. Line 3 does
not include 26 manually selected grammar rules present in
lines 1 and 2; this is because our new feature scoring requires
information from the grammar rules that was not present in
our 2009 extracted resources.
Source Target
un ro?le AP1 ADJP1 roles
l? instabilite? AP1 ADJP1 instability
l? argent PP1 NP1 money
une pression AP1 ADJP1 pressure
la gouvernance AP1 ADJP1 governance
la concurrence AP1 ADJP1 competition
des preuves AP1 ADJP1 evidence
les outils AP1 ADJP1 tools
des changements AP1 ADJP1 changes
Table 3: Rules fitting the pattern (S = w X1, T =
X1 w) that applied on the news-test2010 test set.
5.2 WMT 2010 Results and Analysis
We built the WMT 2010 version of our system
from the training data described in Section 2. (The
system falls under the strictly constrained track:
we used neither the Giga-FrEn data for training
nor the LDC Gigaword corpora for language mod-
eling.) We used the provided news-test2008 set
for system tuning, while news-test2009 served
as our 2010 dev test set. Based on the results
in Table 2, our official submission to this year?s
shared task was constructed as in line 6, with
10,000 syntactic grammar rules chosen after a
pattern-based grammar selection step. On the
news-test2010 test set, this system scored 0.2327
on case-insensitive IBM-style BLEU 1.04, 0.5614
on METEOR 0.6, and 0.5519 on METEOR 1.0
(Lavie and Denkowski, 2009).
The actual application of grammar rules in the
system is quite surprising. Despite having a gram-
mar of 10,000 rules at its disposal, the decoder
chose to only apply a total of 20 unique rules
in 392 application instances in the 2489-sentence
news-test2010 set. On a per-sentence basis, this
is actually fewer rule applications than our sys-
tem performed last year with a 26-rule handpicked
grammar! The most frequently applied rules are
fully abstract, monotonic structure-building rules,
such as for stitching together compound noun
phrases with adverbial phrases or prepositional
phrases. Nine of the 20 rules, listed in Table 3,
demonstrate the effect of our pattern-based gram-
mar selection. These partially lexicalized rules fit
the pattern (S = w X1, T = X1 w) and han-
dle cases of lexicalized binary reordering between
French and English. Though the overall impact of
these rules on automatic metric scores is presum-
85
ably quite small, we believe that the key to effec-
tive syntactic grammars in our MT approach lies
in retaining precise rules of this type for common
linguistically motivated reordering patterns.
The above pattern of rule applications is also
observed in our dev test set, news-test2009, where
16 distinct rules apply a total of 352 times. Seven
of the fully abstract rules and three of the lexical-
ized rules that applied on news-test2009 also ap-
plied on news-test2010, while a further two ab-
stract and four lexicalized rules applied on news-
test2009 alone. We thus have a general trend of a
set of general rules applying with higher frequency
across test sets, while the set of lexicalized rules
used varies according to the particular set.
Since, overall, we still do not see as much gram-
mar application in our systems as we would like,
we plan to concentrate future work on further im-
proving this aspect. This includes a more detailed
study of grammar filtering or refinement to select
the most useful rules. We would also like to ex-
plore the effect of the features of Section 3 individ-
ually, on different language pairs, and using differ-
ent grammar types.
Acknowledgments
This research was supported in part by NSF grant
IIS-0534217 (LETRAS) and the DARPA GALE
program. We thank Yahoo! for the use of the M45
research computing cluster, where we ran many
steps of our experimental pipeline.
References
Jonathan Clark and Alon Lavie. 2010. LoonyBin:
Keeping language technologists sane through
automated management of experimental (hy-
per)workflows. In Proceedings of the Seventh
International Language Resources and Evaluation
(LREC ?10), Valletta, Malta, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the ACL, pages 961?968, Sydney, Australia,
July.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, OH,
June.
Greg Hanneman and Alon Lavie. 2009. Decoding
with syntactic and non-syntactic phrases in a syntax-
based machine translation system. In Proceedings of
the Third Workshop on Syntax and Structure in Sta-
tistical Translations, pages 1?9, Boulder, CO, June.
Greg Hanneman, Vamshi Ambati, Jonathan H. Clark,
Alok Parlikar, and Alon Lavie. 2009. An improved
statistical transfer systems for French?English ma-
chine translation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
140?144, Athens, Greece, March.
Gonzalo Iglesias, Adria` de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings
of the 12th Conference of the European Chapter of
the ACL, pages 380?388, Athens, Greece, March?
April.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15, pages 3?10. MIT Press,
Cambridge, MA.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003, pages 48?54, Ed-
monton, Alberta, May?June.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228?231, Prague, Czech
Republic, June.
Alon Lavie and Michael J. Denkowski. 2009. The ME-
TEOR metric for automatic evaluation of machine
translation. Machine Translation, 23(2?3):105?115.
Alon Lavie, Alok Parlikar, and Vamshi Ambati. 2008.
Syntax-driven learning of sub-sentential translation
equivalents and translation rules from parsed parallel
corpora. In Proceedings of the Second ACL Work-
shop on Syntax and Structure in Statistical Transla-
tion, pages 87?95, Columbus, OH, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit
for parsing-based machine translation. In Proceed-
ings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 135?139, Athens, Greece,
March.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evalution of machine translation. In Proceedings of
86
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
PA, July.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411, Rochester, NY, April.
Ventsislav Zhechev and Andy Way. 2008. Automatic
generation of parallel treebanks. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics, pages 1105?1112, Manchester,
England, August.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings of the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York, NY,
June.
87
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 337?343,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The CMU-ARK German-English Translation System
Chris Dyer Kevin Gimpel Jonathan H. Clark Noah A. Smith
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
{cdyer,kgimpel,jhclark,nasmith}@cs.cmu.edu
Abstract
This paper describes the German-English
translation system developed by the ARK re-
search group at Carnegie Mellon University
for the Sixth Workshop on Machine Trans-
lation (WMT11). We present the results of
several modeling and training improvements
to our core hierarchical phrase-based trans-
lation system, including: feature engineering
to improve modeling of the derivation struc-
ture of translations; better handing of OOVs;
and using development set translations into
other languages to create additional pseudo-
references for training.
1 Introduction
We describe the German-English translation system
submitted to the shared translation task in the Sixth
Workshop on Machine Translation (WMT11) by the
ARK research group at Carnegie Mellon Univer-
sity.1 The core translation system is a hierarchical
phrase-based machine translation system (Chiang,
2007) that has been extended in several ways de-
scribed in this paper.
Some of our innovations focus on modeling.
Since German and English word orders can diverge
considerably, particularly in non-matrix clauses,
we focused on feature engineering to improve the
modeling of long-distance relationships, which are
poorly captured in standard hierarchical phrase-
based translation models. To do so, we devel-
oped features that assess the goodness of the source
1http://www.ark.cs.cmu.edu
language parse tree under the translation grammar
(rather than of a ?linguistic? grammar). To train the
feature weights, we made use of a novel two-phase
training algorithm that incorporates a probabilistic
training objective and standard minimum error train-
ing (Och, 2003). These segmentation features were
supplemented with a 7-gram class-based language
model, which more directly models long-distance
relationships. Together, these features provide a
modest improvement over the baseline and suggest
interesting directions for future work. While our
work on parse modeling was involved and required
substantial changes to the training pipeline, some
other modeling enhancements were quite simple: for
example, improving how out-of-vocabulary words
are handled. We propose a very simple change, and
show that it provides a small, consistent gain.
On the training side, we had two improvements
over our baseline system. First, we were inspired
by the work of Madnani (2010), who showed that
when training to optimize BLEU (Papineni et al,
2002), overfitting is reduced by supplementing a sin-
gle human-generated reference translation with ad-
ditional computer-generated references. We gener-
ated supplementary pseudo-references for our de-
velopment set (which is translated into many lan-
guages, but once) by using MT output from a sec-
ondary Spanish-English translation system. Second,
following Foster and Kuhn (2009), we used a sec-
ondary development set to select from among many
optimization runs, which further improved general-
ization.
We largely sought techniques that did not require
language-specific resources (e.g., treebanks, POS
337
annotations, morphological analyzers). An excep-
tion is a compound segmentation model used for
preprocessing that was trained on a corpus of man-
ually segmented German. Aside from this, no fur-
ther manually annotated data was used, and we sus-
pect many of the improvements described here can
be had in other language pairs. Despite avoiding
language-specific resources and using only the train-
ing data provided by the workshop, an extensive
manual evaluation determined that the outputs pro-
duced were of significantly higher quality than both
statistical and rule-based systems that made use of
language-specific resources (Callison-Burch et al,
2011).
2 Baseline system and data
Our translation system is based on a hierarchical
phrase-based translation model (Chiang, 2007), as
implemented in the cdec decoder (Dyer et al,
2010). Since German is a language that makes
productive use of ?closed? compounds (compound
words written as a single orthographic token), we
use a CRF segmentation model of to evaluate the
probability of all possible segmentations, encoding
the most probable ones compactly in a lattice (Dyer,
2009). For the purposes of grammar induction, the
single most probable segmentation of each word in
the source side of the parallel training data under the
model was inferred.
The parallel data were aligned using the
Giza++ implementation of IBM Model 4 run
in both directions and then symmetrized using
the grow-diag-final-and heuristic (Och and
Ney, 2002; Brown et al, 1993; Koehn et al, 2003).
The aligned corpus was encoded as a suffix array
(Lopez, 2008) and lattice-specific grammars (con-
taining just the rules that are capable of matching
spans in the input lattice) were extracted for each
sentence in the test and development sets, using the
heuristics recommended by Chiang (2007).
A 4-gram modified Kneser-Ney language model
(Chen and Goodman, 1996) was constructed using
the SRI language modeling toolkit (Stolcke, 2002)
from the English side of the parallel text, the mono-
lingual English data, and the English version 4 Giga-
word corpus (Parker et al, 2009). Since there were
many duplicate segments in the training data (much
of which was crawled from the web), duplicate seg-
ments and segments longer than 100 words were re-
moved. Inference was carried out using the language
modeling library described by Heafield (2011).
The newstest-2009 set (with the 500 longest
segments removed) was used for development,2 and
newstest-2010 was used as a development test
set. Results in this paper are reported on the dev-
test set using uncased BLEU4 with a single refer-
ence translation. Minimum error rate training (Och,
2003) was used to optimize the parameters of the
system to maximize BLEU on the development data,
and inference was performed over a pruned hyper-
graph representation of the translation hypothesis
space (Kumar et al, 2009).
For the experiments reported in this paper, Viterbi
(max-derivation) decoding was used. The system
submitted for manual evaluation used segment-level
MBR decoding with 1 ? BLEU as the loss function,
approximated over a 500-best list for each sentence.
This reliably results in a small but consistent im-
provement in translation quality, but is much more
time consuming to compute (Kumar and Byrne,
2004).
3 Source parse structure modeling
Improving phrase-based translation systems is chal-
lenging in part because our intuitions about what
makes a ?good? phrase or translation derivation are
often poor. For example, restricting phrases and
rules to be consistent with syntactic constituents
consistently harms performance (Chiang, 2007; Gal-
ley et al, 2006; Koehn et al, 2003), although our
intuitions might suggest this is a reasonable thing
to do. On the other hand, it has been shown that
incorporating syntactic information in the form of
features can lead to improved performance (Chiang,
2010; Gimpel and Smith, 2009; Marton and Resnik,
2008). Syntactic features that are computed by as-
sessing the overlap of the translation parse with a
linguistic parse can be understood to improve trans-
lation because they lead to a better model of what a
?correct? parse of the source sentence is under the
translation grammar.
Like the ?soft syntactic features? used in pre-
2Removing long segments substantially reduces training
time and does not appear to negatively affect performance.
338
vious work (Marton and Resnik, 2008; Chiang et
al., 2008), we propose features to assess the tree
structure induced during translation. However, un-
like that work, we do not rely on linguistic source
parses, but instead only make use of features that
are directly computable from the source sentence
and the parse structure being considered in the de-
coder. In particular, we take inspiration from the
model of Klein and Manning (2002), which mod-
els constituency in terms of the contexts that rule
productions occur in. Additionally, we make use of
salient aspects of the spans being dominated by a
nonterminal, such as the words at the beginning and
end of the span, and the length of the span. Impor-
tantly, the features do not rely on the target words
being predicted, but only look at the structure of the
translation derivation. As such, they can be under-
stood as monolingual parse features.3
Table 1 lists the feature templates that were used.
Template Description
CTX:fi?1, fj context bigram
CTX:fi?1, fj , x context bigram + NT
CTX:fi?1, fj , x, (j ? i) context bigram + NT + len
LU:fi?1 left unigram
LB:fi?1, fi left bigram (overlapping)
RU:fj right unigram
RB:fj?1, fj right bigram (overlapping)
Table 1: Context feature templates for features extracted
from every translation rule used; i and j indicate hypothe-
sized constituent span, x is its nonterminal category label
(in our grammar, X or S), and fk is the kth word of the
source sentence, with f<1 = ?s? and f>|f| = ?/s?. If a
word fk is not among the 1000 most frequent words in
the training corpus, it is replaced by a special unknown
token. The SMALLCAPS prefixes prevent accidental fea-
ture collisions.
3.1 Two-phase discriminative learning
The parse features just introduced are numerous and
sparse, which means that MERT can not be used
to infer their weights. Instead, we require a learn-
ing algorithm that can cope with millions of fea-
tures and avoid overfitting, perhaps by eliminating
most of the features and keeping only the most valu-
able (which would also keep the model compact).
3Similar features have been proposed for use in discrimina-
tive monolingual parsing models (Taskar et al, 2004).
Furthermore, we would like to be able to still tar-
get the BLEU measure of translation quality during
learning. While large-scale discriminative training
for machine translation is a widely studied problem
(Hopkins and May, 2011; Li and Eisner, 2009; De-
vlin, 2009; Blunsom et al, 2008; Watanabe et al,
2007; Arun and Koehn, 2007; Liang et al, 2006), no
tractable algorithm exists for learning a large num-
ber of feature weights while directly optimizing a
corpus-level metric like BLEU. Rather than resorting
to a decomposable approximation, we have explored
a new two-phase training algorithm in development
of this system.
The two-phase algorithm works as follows. In
phase 1, we use a non-BLEU objective to train a
translation model that includes the large feature set.
Then, we use this model to compute a small num-
ber of coarse ?summary features,? which summa-
rize the ?opinion? of the first model about a trans-
lation hypothesis in a low dimensional space. Then,
in the second training pass, MERT is used to deter-
mine how much weight to give these summary fea-
tures together with the other standard coarse trans-
lation features. At test time, translation becomes a
multi-step process as well. The hypothesis space is
first scored using the phase-1 model, then summary
features are computed, then the hypothesis space is
rescored with the phase-2 model. As long as the fea-
tures used factor with the edges in the translation
space (which ours do), this can be carried out in lin-
ear time in the size of the translation forest.
3.1.1 Phase 1 training
For the first model, which includes the sparse parse
features, we learn weights in order to optimize pe-
nalized conditional log likelihood (Blunsom et al,
2008). We are specifically interested in modeling
an unobserved variable (i.e., the parse tree underly-
ing a translation derivation), this objective is quite
natural, since probabilistic models offer a principled
account of unobserved data. Furthermore, because
our features factor according to edges in the trans-
lation forest (they are ?stateless? in standard MT
terminology), there are efficient dynamic program-
ming algorithms that can be used to exactly compute
the expected values of the features (Lari and Young,
1990), which are necessary for computing the gradi-
ents used in optimization.
339
We are therefore optimizing the following objec-
tive, given a set T of parallel training sentences:
L = ?R(?)?
?
?f,e??T
log
?
d
p?(e,d | f)
where p?(e,d | f) =
exp ?>h(f, e,d)
Z(f)
,
where d is a variable representing the unobserved
synchronous parses giving rise to the pair of sen-
tences ?f, e?, and where R(?) is a penalty that favors
less complex models. Since we not only want to pre-
vent over fitting but also want a small model, we use
R(?) =
?
k |?k|, the `1 norm, which forces many
parameters to be exactly 0.
Although L is not convex in ? (on account of the
latent derivation variable), we make use of an on-
line stochastic gradient descent algorithm that im-
poses an `1 penalty on the objective (Tsuruoka et
al., 2009). Online algorithms are often effective for
non-convex objectives (Liang and Klein, 2009).
We selected 12,500 sentences randomly from the
news-commentary portion of the training data to use
to train the latent variable model. Using the stan-
dard rule extraction heuristics (Chiang, 2007), 9,967
of the sentence pairs could be derived.4 In addition
to the parse features describe above, the standard
phrase features (relative frequency and lexical trans-
lation probabilities), and a rule count feature were
included. Training was run for 48 hours on a sin-
gle machine, which resulted in 8 passes through the
training data, instantiating over 8M unique features.
The regularization strength ? was chosen so that ap-
proximately 10, 000 (of the 8M) features would be
non-zero.5
3.1.2 Summary features
As outlined above, the phase 1 model will be incor-
porated into the final translation model using a low
dimensional ?summary? of its opinion. Because we
are using a probabilistic model, posterior probabili-
ties (given the source sentence f) under the parsing
4When optimizing conditional log likeligood, it is necessary
to be able to exactly derive the training pair. See Blunsom et al
(2008) for more information.
5Ideally, ? would have been tuned to optimize held-out like-
lihood or BLEU; however, the evaluation deadline prevented us
from doing this.
model are easily defined and straightforward to com-
pute with dynamic programming. We made use of
four summary features: the posterior log probability
log p?(e,d|f); for every rule r ? d, the probability of
its span being a constituent under the parse model;
the probabilities that some span starts at the r?s start-
ing index, or that some rule ends at r?s ending index.
Once these summary features have been com-
puted, the sparse features are discarded, and the
summary features are reweighted using coefficients
learned by MERT, together with the standard MT
features (language model, word penalty, etc.). This
provides a small improvement over our already very
strong baseline, as the first two rows in Table 2 show.
Condition BLEU
baseline 25.0
+ parse features 25.2
+ parse features + 7-gram LM 25.4
Table 2: Additional features designed to improve model
of long-range reordering.
3.2 7-gram class-based LM
The parsing features above were intended to im-
prove long range reordering quality. To further sup-
port the modeling of larger spans, we incorporated
a 7-gram class-based language model. Automatic
word clusters are attractive because they can be
learned for any language without supervised data,
and, unlike part-of-speech annotations, each word
is in only a single class, which simplifies inference.
We performed Brown clustering (Brown et al, 1992)
on 900k sentences from our language modeling data
(including the news commentary corpus and a sub-
set of Gigaword). We obtained 1,000 clusters us-
ing an implementation provided by Liang (2005),6
as Turian et al (2010) found that relatively large
numbers clusters gave better performance for infor-
mation extraction tasks. We then replaced words
with their clusters in our language modeling data
and built a 7-gram LM with Witten-Bell smoothing
(Witten and Bell, 1991).7 The last two rows of Ta-
6http://www.cs.berkeley.edu/?pliang/
software
7The distributional assumptions made by the more com-
monly used Kneser-Ney estimator do not hold in the word-
340
ble 2 shows that in conjunction with the source parse
features, a slight improvement comes from includ-
ing the 7-gram LM.
4 Non-translating tokens
When two languages share a common alphabet (as
German and English largely do), it is often appro-
priate to leave some tokens untranslated when trans-
lating. Named entities, numbers, and graphical el-
ements such as emoticons are a few common ex-
amples of such ?non-translating? elements. To en-
sure that such elements are well-modeled, we aug-
ment our translation grammar so that every token
in the input can translate as itself and add a feature
that counts the number of times such self-translation
rules are used in a translation hypothesis. This is in
contrast to the behavior of most other decoders, such
as Moses, which only permit a token to translate as
itself if it is learned from the training data, or if there
is no translation in the phrase table at all.
Since many non-translating tokens are out-of-
vocabulary (OOV) in the target LM, we also add
a feature that fires each time the LM encounters a
word that is OOV.8 This behavior be understood as
discriminatively learning the unknown word penalty
that is part of the LM. Again, this is in contrast to
the behavior of other decoders, which typically add
a fixed (and very large) cost to the LM feature for
every OOV. Our multi-feature parameterization per-
mits the training algorithm to decide that, e.g., some
OOVs are acceptable if they occur in a ?good? con-
text rather than forcing the decoder to avoid them
at all costs. Table 3 shows that always providing
a non-translating translation option together with a
discriminative learned OOV feature improves the
quality of German-English translation.9
Condition BLEU
?OOV (baseline) 24.6
+OOV and non-translating rules 25.0
Table 3: Effect of discriminatively learned penalties for
OOV words.
classified corpus.
8When multiple LMs are used, there is an extra OOV feature
for each LM.
9Both systems were trained using the human+ES-EN refer-
ence set described below (?5).
5 Computer-generated references
Madnani (2010) shows that models learned by op-
timizing BLEU are liable to overfit if only a sin-
gle reference is used, but that this overfitting can
be mitigated by supplementing the single reference
with supplemental computer-generated references
produced by paraphrasing the human reference us-
ing a whole-sentence statistical paraphrase system.
These computer-generated paraphrases are just used
to compute ?better? BLEU scores, but not directly as
examples of target translations.
Although we did not have access to a paraphrase
generator, we took advantage of the fact that our de-
velopment set (newstest-2009) was translated
into several languages other than English. By trans-
lating these back into English, we hypothesized we
would get suitable pseudo-references that could be
used in place of computer-generated paraphrases.
Table 4 shows the results obtained on our held-out
test set simply by altering the reference translations
used to score the development data. These systems
all contain the OOV features described above.
Condition BLEU
1 human 24.7
1 human + ES-EN 25.0
1 human + FR-EN 24.0
1 human + ES-EN + FR-EN 24.2
Table 4: Effect of different sets of reference translations
used during tuning.
While the effect is somewhat smaller than Mad-
nani (2010) reports using a sentential paraphraser,
the extremely simple technique of adding the output
of a Spanish-English (ES-EN) system was found to
consistently improve the quality of the translations
of the held-out data. However, a comparable effect
was not found when using references generated from
a French-English (FR-EN) translation system, indi-
cating that the utility of this technique must be as-
sessed empirically and depends on several factors.
6 Case restoration
Our translation system generates lowercased out-
put, so we must restore case as a post-processing
step. We do so using a probabilistic transducer as
implemented in SRILM?s disambig tool. Each
341
lowercase token in the input can be mapped to a
cased variant that was observed in the target lan-
guage training data. Ambiguities are resolved us-
ing a language model that predicts true-cased sen-
tences.10 We used the same data sources to con-
struct this model as were used above. During devel-
opment, it was observed that many named entities
that did not require translation required some case
change, from simple uppercasing of the first letter,
to more idiosyncratic casings (e.g., iPod). To ensure
that these were properly restored, even when they
did not occur in the target language training data, we
supplement the true-cased LM training data and case
transducer training data with the German source test
set.
Condition BLEU (Cased)
English-only 24.1
English+test-set 24.3
Table 5: Effect of supplementing recasing model training
data with the test set source.
7 Model selection
Minimum error rate training (Och, 2003) is a
stochastic optimization algorithm that typically finds
a different weight vector each time it is run. Foster
and Kuhn (2009) showed that while the variance on
the development set objective may be narrow, the
held-out test set variance is typically much greater,
but that a secondary development set can be used to
select a system that will have better generalization.
We therefore replicated MERT 6 times and selected
the output that performed best on NEWSTEST-2010.
Since we had no additional blind test set, we can-
not measure what the impact is. However, the BLEU
scores we selected on varied from 25.4 to 26.1.
8 Summary
We have presented a summary of the enhancements
made to a hierarchical phrase-based translation sys-
tem for the WMT11 shared translation task. Some
of our results are still preliminary (the source parse
10The model used is p(y | x)p(y). While this model is some-
what unusual (the conditional probability is backwards from a
noisy channel model), it is a standard and effective technique
for case restoration.
model), but a number of changes we made were
quite simple (OOV handling, using MT output to
provide additional references for training) and also
led to improved results.
Acknowledgments
This research was supported in part by the NSF through
grant IIS-0844507, the U. S. Army Research Laboratory
and the U. S. Army Research Office under contract/grant
number W911NF-10-1-0533, and Sandia National Labo-
ratories (fellowship to K. Gimpel). We thank the anony-
mous reviewers for their thorough feedback.
References
A. Arun and P. Koehn. 2007. Online learning methods
for discriminative training of phrase based statistical
machine translation. In Proc. of MT Summit XI.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. F. Brown, P. V. de Souza, R. L. Mercer, V. J.
Della Pietra, and J. C. Lai. 1992. Class-based n-gram
models of natural language. Computational Linguis-
tics, 18:467?479.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and O. F. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of the Sixth Workshop
on Statistical Machine Translation.
S. F. Chen and J. Goodman. 1996. An empirical study of
smoothing techniques for language modeling. In Proc.
of ACL, pages 310?318.
D. Chiang, Y. Marton, and P. Resnik. 2008. Online large-
margin training of syntactic and structural translation
features. In Proc. EMNLP, pages 224?233.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
D. Chiang. 2010. Learning to translate with source and
target syntax. In Proc. of ACL, pages 1443?1452.
J. Devlin. 2009. Lexical features for statistical machine
translation. Master?s thesis, University of Maryland.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL (demonstration session).
C. Dyer. 2009. Using a maximum entropy model to build
segmentation lattices for MT. In Proc. of NAACL.
342
G. Foster and R. Kuhn. 2009. Stabilizing minimum error
rate training. Proc. of WMT.
M. Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,
W. Wang, and I. Thayer. 2006. Scalable inference and
training of context-rich syntactic translation models.
In Proc. of ACL, pages 961?968.
K. Gimpel and N. A. Smith. 2009. Feature-rich transla-
tion by quasi-synchronous lattice parsing. In Proc. of
EMNLP, pages 219?228.
K. Heafield. 2011. KenLM: Faster and smaller language
model queries. In Proc. of the Sixth Workshop on Sta-
tistical Machine Translation.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP.
D. Klein and C. D. Manning. 2002. A generative
constituent-context model for improved grammar in-
duction. In Proc. of ACL, pages 128?135.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Effi-
cient minimum error rate training and minimum bayes-
risk decoding for translation hypergraphs and lattices.
In Proc. of ACL-IJCNLP.
K. Lari and S. Young. 1990. The estimation of stochas-
tic context-free grammars using the inside-outside al-
gorithm. Computer Speech and Language.
Z. Li and J. Eisner. 2009. First- and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proc. of EMNLP,
pages 40?51.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL.
P. Liang. 2005. Semi-supervised learning for natural
language. Master?s thesis, Massachusetts Institute of
Technology.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING.
N. Madnani. 2010. The Circle of Meaning: From Trans-
lation to Paraphrasing and Back. Ph.D. thesis, De-
partment of Computer Science, University of Mary-
land College Park.
Y. Marton and P. Resnik. 2008. Soft syntactic constraints
for hierarchical phrased-based translation. In Proc. of
ACL, pages 1003?1011, Columbus, Ohio.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL, pages 295?302.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
R. Parker, D. Graff, J. Kong, K. Chen, and K. Maeda.
2009. English gigaword fourth edition.
A. Stolcke. 2002. SRILM ? an extensible language mod-
eling toolkit. In Intl. Conf. on Spoken Language Pro-
cessing.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. of EMNLP.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochas-
tic gradient descent training for l1-regularized log-
linear models with cumulative penalty. In Proc. of
ACL-IJCNLP.
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proc. of ACL, pages 384?394.
T. Watanabe, J. Suzuki, H. Tsukuda, and H. Isozaki.
2007. Online large-margin training for statistical ma-
chine translation. In Proc. of EMNLP.
I. H. Witten and T. C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events
in adaptive text compression. IEEE Trans. Informa-
tion Theory, 37(4).
343
