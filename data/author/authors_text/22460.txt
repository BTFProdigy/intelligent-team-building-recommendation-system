Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1465?1468,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Verifiably Effective Arabic Dialect Identification
Kareem Darwish, Hassan Sajjad, Hamdy Mubarak
Qatar Computing Research Institute
Qatar Foundation
{kdarwish,hsajjad,hmubarak}@qf.org.qa
Abstract
Several recent papers on Arabic dialect identi-
fication have hinted that using a word unigram
model is sufficient and effective for the task.
However, most previous work was done on a
standard fairly homogeneous dataset of dialec-
tal user comments. In this paper, we show
that training on the standard dataset does not
generalize, because a unigram model may be
tuned to topics in the comments and does not
capture the distinguishing features of dialects.
We show that effective dialect identification
requires that we account for the distinguishing
lexical, morphological, and phonological phe-
nomena of dialects. We show that accounting
for such can improve dialect detection accu-
racy by nearly 10% absolute.
1 Introduction
Modern Standard Arabic (MSA) is the lingua franca
of the so-called Arab world, which includes north-
ern Africa, the Arabian Peninsula, and Mesopotamia.
However, Arabic speakers generally use dramatically
different languages (or dialects) in daily interactions
and in social media. These dialects may differ in vocab-
ulary, morphology, and spelling from MSA and most
do not have standard spellings. There is often large
lexical overlap between dialects and MSA. Performing
proper Arabic dialect identification may positively im-
pact many Natural Language Processing (NLP) appli-
cation. For example, transcribing dialectal speech or
automatically translating into a particular dialect would
be aided by the use of targeted language models that are
trained on texts in that dialect.
This has led to recent interest in automatic identifi-
cation of different Arabic dialects (Elfardy et al., 2013;
Cotterell et al., 2014; Zaidan et al., 2014). Though pre-
vious work (Cotterell et al., 2014) have reported high
accuracies for dialect identification using word uni-
gram model, which implies that this is a solved prob-
lem, we argue that the problem is far from being solved.
The reason for this assertion stems from the fact that the
available dialectal data is drawn from singular sources,
namely online news sites, for each dialect. This is prob-
lematic because comments on singular news site are
likely to have some homogeneity in topics and jargon.
Such homogeneity has caused fairly simple classifica-
tion techniques that use word unigrams and character n-
grams to yield very high identification accuracies. Per-
haps, this can be attributed to topical similarity and not
just differences between dialects. To showcase this, we
trained a classifier using the best reported methods, and
we tested the classifier on a new test set of 700 tweets,
with dialectal Egyptian (ARZ) and MSA tweets, which
led to a low accuracy of 83.3%. We also sorted words
in the ARZ part from our training dataset by how much
they discriminate between ARZ and MSA (using mu-
tual information) and indeed many of the top words
were in fact MSA words.
There seems to be a necessity to identify lexical and
linguistic features that discriminate between MSA and
different dialects. In this paper, we highlight some
such features that help in separating between MSA
and ARZ. We identify common ARZ words that do
not overlap with MSA and identify specific linguistic
phenomena that exist in ARZ, and not MSA, such as
morphological patterns, word concatenations, and verb
negation constructs (Section 3). We also devise meth-
ods for capturing the linguistic phenomena, and we use
the appearance of such phenomena as features (Sec-
tion 4). Further, we show the positive impact of using
the new features in identifying ARZ (Section 5).
2 Previous Work
Previous work on Arabic dialect identification uses n-
gram based features at both word-level and character-
level to identify dialectal sentences (Elfardy et al.,
2013; Cotterell et al., 2014; Zaidan et al., 2011; Zaidan
et al., 2014). Zaidan et al. (2011) created a dataset of
dialectal Arabic. They performed cross-validation ex-
periments for dialect identification using word n-gram
based features. Elfardy et al. (2013) built a system to
distinguish between ARZ and MSA. They used word
n-gram features combined with core (token-based and
perplexity-based features) and meta features for train-
ing. Their system showed a 5% improvement over
the system of Zaidan et al. (2011). Later, Zaidan et
al. (2014) used several word n-gram based and char-
acter n-gram based features for dialect identification.
The system trained on word unigram-based feature per-
formed the best with character five-gram-based feature
being second best. A similar result is shown by Cot-
terell et al. (2014) where word unigram model performs
1465
the best.
All of the previous work except Cotterell et al.
(2014)
1
evaluate their systems using cross-validation.
These models heavily rely on the coverage of training
data to achieve better identification. This limits the ro-
bustness of identification to genres inline with the train-
ing data.
Language identification is a related area to dialect
identification. It has raised some of the issues which we
discussed in this paper in the context of dialect identi-
fication. Lui et al. (2011) showed that in-domain lan-
guage identification performs better than cross domain
language identification. Tiedemann et al. (2012) argued
that the linguistic understanding of the differences be-
tween languages can lead to a better language identi-
fication system. kilgarriff (2001) discussed the differ-
ences between datasets as a poor representation of dif-
ferences between dialects of English.
In this paper, we exploit the linguistic phenomena
that are specific to Arabic dialects to show that they
produce significant improvements in accuracy. We
show that this also helps in achieving high quality
cross-domain dialect identification system.
3 Dialectal Egyptian Phenomena
There are several phenomena in ARZ that set it apart
from MSA. Some of them are as follows:
Dialectal words: ARZ uses unique words that do
not overlap with MSA and may not overlap with other
dialects. Some of the common ARZ words are: ?zy?
(like), ?kdh? (like this), and ?Azyk? (how are you)
2
.
These dialectal terms stem from the following:
? Using proper Arabic words that are rarely used in
MSA such as ?$nTp? (bag) and ?n$wf? (we see).
? Fusing multiple words together by concatenating and
dropping letters such as the word ?mEl$? (no worry),
which is a fusion of ?mA Elyh $y? ?.
? Using non-standard spelling of words such as
?SAbE? (finger) instead of ?<sbE? in MSA. Conse-
quently, broken plurals may also be non-standard.
? using non-Arabic words such as ?<y$Arb? (scarf),
which is transliterated from the French ?echarpe.
? altering the forms of some pronouns such as the fem-
inine second person pronoun from ?k? to ?ky?, the sec-
ond person plural pronoun ?tm? to ?tw?, and the object
pronoun ?km? to ?kw?.
Morphological differences: ARZ makes use of par-
ticular morphological patterns that do not exist in MSA
and often alters some morphological constructs. Some
examples include:
? Adding the letter ?b? in front of verb in present tense.
Ex. MSA: ?ylEb? (he plays)? EG: ?bylEb?.
? Using the letters ?H? or ?h?, instead of ?s?, to indi-
cate future tense. Ex. MSA: ?sylEb? (he will play)?
EG: ?hylEb? or ?HylEb?.
1
Zaidan et al. (2014) applied their classifier to a different
genre but did not evaluate it?s performance.
2
Buckwalter encoding is used throughout the paper.
? Adding the letters ?At? to passive past tense verbs.
Ex. MSA: ?luEiba? (was played)? ?AtlaEab?.
? Adding the letters ?m? or ?mA? before the verb and
?$? or ?$y? after the verb to express negation. Ex.
MSA: ?lm ylEb? (he did not play)? ?mlEb$?.
? the merging of verbs and prepositional phrases of the
form (to-pronoun) that follow it. Ex. MSA: ?ylEb lh?
(he plays for/to him)? ?bylEblh?.
? Replacing a short vowel with a long vowel in im-
perative verbs that are derived from hollow roots. Ex.
MSA: ?qul? (say)? ?qwl?.
Letter substitution: in ARZ the following letter
substitutions are common:
? ?v?? ?t?. Ex. MSA: ?kvyr? (a lot)? EG: ?ktyr?.
? ?}?? ?y?. Ex. MSA: ?b}r? (well)? ?byr?.
? Trailing ?y?? ?Y?. Ex. MSA: ?Hqy? (my right)?
?HqY?.
? ?*?? ?d?. Ex. MSA: ?xu*? (take)? ?xud?.
? middle or trailing ?>? ? ?A?. Ex. MSA: ?f>r?
(mouse)? ?fAr?.
? ?D?? ?Z?. Ex. MSA: ?DAbT? (officer)? ?ZAbT?.
? ?Z?? ?D?. Ex. MSA: ?Zhr? (back)? ?Dhr?.
? Middle ?|? ? ?yA?. Ex. MSA: ?ml|n? (full) ?
?mlyAn?.
? Removal of trailing ? ? ?. Ex. MSA: ?AlsmA? ? (the
sky)? ?AlsmA?.
Syntactic differences: some of the following phe-
nomena are generally observed:
? Common use of masculine plural or singular noun
forms instead dual and feminine plural. Ex. MSA ?jny-
hyn? (two pounds)? EG: ?Atnyn jnyh?.
? Dropping some articles and preposition in some syn-
tactic constructs. For example, the preposition ?<lY?
(to) in ?>nA rAyH <lY Al$gl? (I am going to work)
is typically dropped. Also, the particle ?>n? (to) is
dropped in the sentence ?>nA mHtAj >n >nAm? (I
need to sleep).
? Using only one form of noun and verb suffixes such
as ?yn? instead of ?wn? and ?wA? instead of ?wn? re-
spectively. Also, so-called ?five nouns?, are used in
only one form (ex. ?>bw? (father of) instead of ?>bA?
or ?>by?).
4 Detecting Dialectal Peculiarities
ARZ is different from MSA lexically, morphologically,
phonetically, and syntactically. Here, we present meth-
ods to handle such peculiarities. We chose not to han-
dle syntactic differences, because they may be captured
using word n-gram models.
To capture lexical variations, we extracted and sorted
by frequency all the unigrams from the Egyptian side of
the LDC2012T09 corpus (Zbib et al., 2012), which has
? 38k Egyptian-English parallel sentences. A linguist
was tasked with manually reviewing the words from the
top until 1,300 dialectal words were found. Some of the
words on the list included dialectal words, commonly
used foreign words, words that exhibit morphological
variations, and others with letter substitution.
1466
For morphological phenomenon, we employed three
methods, namely:
? Unsupervised Morphology Induction: We em-
ployed the unsupervised morpheme segmentation tool,
Morfessor (Virpioja et al., 2013). It is a data driven
tool that automatically learns morphemes from data in
an unsupervised fashion. We used the trained model to
segment the training and test sets.
? Morphological Rules: In contrast to Morfessor, we
developed only 15 morphological rules (based on the
analysis proposed in Section 3) to segment ARZ text.
These rules would separate prefixes and suffixes like a
light stemmer. Example rules would segment a leading
?b? and segment a combination of a leading ?m? and
trailing ?$?.
?Morphological Generator: For morphological gen-
eration, we enumerated a list of ? 200 morphological
patterns that derive dialectal verbs from Arabic roots.
One such pattern is ytCCC that would generate the di-
alectal verb-form ytktb (to be written) from the root
?ktb?. We used the root list that is distributed with Se-
bawai (Darwish, 2002). We also expanded the list by
attaching negation affixes and pronouns. We retained
generated word forms that: a) exist in a large corpus of
63 million Arabic tweets from 2012 with more than 1
billion tokens; and b) don?t appear in a large MSA cor-
pus of 10 years worth of Aljazeera articles containing
114 million tokens
3
. The resulting list included 94k
verb surface forms such as ?mbyEmlhA$? (he does not
do it).
For phonological differences, we used a morpholog-
ical generator that makes use of the aforementioned
root list and an inventory of ? 605 morphological pat-
terns (with diacritization) to generate possible Arabic
stems. The generated stems with their diacritics were
checked against a large diacritized Arabic corpus con-
taining more than 200 million diacritized words
4
. If
generated words contained the letters ?v?, ?}?, ?*?, and
?D?, we used the aforementioned letter substitutions.
We retained words that exist in the large tweet corpus
but not in the Aljazeera corpus. The list contained 8k
surface forms.
5 Evaluation Setup
Dataset: We performed dialect identification exper-
iment for ARZ and MSA. For ARZ, we used the
Egyptian side of the LDC2012T09 corpus (Zbib et
al., 2012)
5
. For MSA, we used the Arabic side
of the English/Arabic parallel corpus from the Inter-
national Workshop on Arabic Language Translation
6
which consists of ? 150k sentences. For testing, we
constructed an evaluation set that is markedly different
3
http://aljazeera.net
4
http://www.sh.rewayat2.com
5
We did not use the Arabic Online Commentary data
(Zaidan et al., 2011) as annotations were often not reliable.
6
https://wit3.fbk.eu/mt.php?release=
2013-01
from the training set. We crawled Arabic tweets from
Twitter during March 2014 and selected those where
user location was set to Egypt or a geographic location
within Egypt, leading to 880k tweets. We randomly
selected 2k tweets, and we manually annotated them
as ARZ, MSA, or neither until we obtained 350 ARZ
and 350 MSA tweets. We used these tweets for testing.
We plan to release the tweet ID?s and our annotations.
We preprocessed the training and test sets using the
method described by Darwish et al. (2012), which in-
cludes performing letter and word normalizations, and
segmented all data using an open-source MSA word
segmentor (Darwish et al., 2012). We also removed
punctuations, hashtags, and name mentions from the
test set. We used a Random Forest (RF) ensemble clas-
sifier that generates many decision trees, each of which
is trained on a subset of the features.
7
We used the RF
implementation in Weka (Breiman, 2001).
5.1 Classification Runs
Baseline BL: In our baseline experiments, we used
word unigram, bigram, and trigram models and charac-
ter unigram to 5-gram models as features. We first per-
formed a cross-validation experiment using ARZ and
MSA training sets. The classifier achieved fairly high
results (+95%) which are much higher than the results
mentioned in the literature. This could be due in part
to the fact that we were doing ARZ-MSA classification
instead of multi-dialect classification and MSA data is
fairly different in genre from ARZ data. We did not fur-
ther discuss these results. This experiment was a sanity
check to see how does in-domain dialect identification
perform.
Later, we trained the RF classifier on the complete
training set using word n-gram features (WRD), char-
acter n-gram features (CHAR), or both (BOTH) and
tested it on the tweets test set. We referred to this sys-
tem as BL later on.
Dialectal Egyptian Lexicon S
lex
: As mentioned ear-
lier, we constructed three word lists containing 1,300
manually reviewed ARZ words (MAN), 94k dialectal
verbs (VERB), and 8k words with letter substitutions
(SUBT). Using the lists, we counted the number of
words in a tweet that exist in the word lists and used it
as a standalone feature for classifications. LEX refers
to concatenation of all three lists.
Morphological Features: For S
mrph
, we trained Mor-
fessor separately on the MSA and Egyptian training
data and applied to the same training data for segmen-
tation. For S
rule
, we segmented Egyptian part of the
training data using the morphological rules mentioned
in Section 4. For both, word and character n-gram fea-
tures were calculated from the segmented data and the
7
We tried also the multi-class Bayesian classifier and
SVM classifier. SVM classifier had comparable results with
Random Forest classifier. However, it was very slow. So, we
decided to go with Random Forest classifier for the rest of the
experiments.
1467
SYS WRD CHR BOTH BEST+LEX
BL 53.0 74.0 83.3 84.7
S
mrph
72.0 88.0 62.1 89.3
S
rule
53.9 85.9 85.9 90.1
Table 1: Dialect identification accuracy using
various classification settings: only word-based
(WRD), character-based (CHAR), and both features.
BEST+LEX is built on the best feature of that system
plus a feature built on the concatenation of all lists
SYS MAN +VERB +SUBT
S
lex
93.6 94.6 94.4
Table 2: Accuracy of the dialect identification system
with the addition of various types of lexicon
classifier was trained on them and tested on the tweet
test set.
5.2 Results
Table 1 summarizes the results. Unlike results in the lit-
erature, character-based n-gram features outperformed
word-based n-gram features, as they seemed to better
generalize to the new test set, where lexical overlap be-
tween the training and test sets was low. Except for
S
mrph
, adding both character and word n-gram fea-
tures led to improved results. We observed that Mor-
fessor over-segmented the text, which in turns created
small character segments and enabled the character-
based language model to learn the phenomenon inherit
in a word. The baseline system achieved an accuracy
of 84.7% when combined with the S
lex
feature. Com-
bining S
mrph
and S
rule
features with the S
lex
feature
led to further improvement. However, as shown in Ta-
ble 2, using the S
lex
feature alone with the MAN and
VERB lists led to the best results (94.6%), outperform-
ing using all other features either alone or in combina-
tion. This suggests that having a clean list of dialectal
words that cover common dialectal phenomena is more
effective than using word and character n-grams. It also
highlights the shortcomings of using a homogeneous
training set where word unigrams could be capturing
topical cues along with dialectal ones.
6 Conclusion
In this paper, we identified lexical, morphological,
phonological, and syntactic features that help distin-
guish between dialectal Egyptian and MSA. Given the
substantial lexical overlap between dialectal Egyptian
and MSA, targeting words that exhibit distinguishing
traits is essential to proper dialect identification. We
used some of these features for dialect detection lead-
ing to nearly 10% (absolute) improvement in classifi-
cation accuracy. We plan to extend our work to other
dialects.
References
Leo Breiman. 2001. Random Forests. Machine Learn-
ing. 45(1):5-32.
Ryan Cotterell, Chris Callison-Burch. 2014. A Multi-
Dialect, Multi-Genre Corpus of Informal Written
Arabic. LREC-2014, pages 241?245.
Kareem Darwish. 2002. Building a shallow morpho-
logical analyzer in one day. In Proceedings of the
ACL-2002 Workshop on Computational Approaches
to Semitic Languages.
Kareem Darwish, Walid Magdy, Ahmed Mourad.
2012. Language Processing for Arabic Microblog
Retrieval. CIKM-2012, pages 2427?2430.
Kareem Darwish, Ahmed Abdelali, Hamdy Mubarak.
2014. Using Stem-Templates to improve Arabic POS
and Gender/Number Tagging. LREC-2014.
Heba Elfardy, Mona Diab. 2013. Sentence Level Di-
alect Identification in Arabic. ACL-2013, pages
456?461.
Sami Virpioja, Peter Smit, Stig-Arne Grnroos, and
Mikko Kurimo. 2013. Morfessor 2.0: Python Im-
plementation and Extensions for Morfessor Base-
line. Aalto University publication series SCI-
ENCE + TECHNOLOGY, 25/2013. Aalto Univer-
sity, Helsinki, 2013.
Omar F. Zaidan, Chris Callison-Burch. 2011. The Ara-
bic Online Commentary Dataset: An Annotated
Dataset of Informal Arabic with High Dialectal Con-
tent. ACL-11, pages 37?41.
Omar F. Zaidan, Chris Callison-Burch. 2014. Arabic
Dialect Identification. CL-11, 52(1).
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz,
John Makhoul, Omar F. Zaidan, Chris Callison-
Burch. 2012. Machine translation of Arabic dialects.
NAACL-2012, pages 49?59.
Marco Lui and Timothy Baldwin. 2011. Cross-
domain feature selection for language identification.
IJCNLP-2011, page 553?561.
J?org Tiedemann and Nikola Ljubesic. 2012. Efficient
discrimination between closely related languages.
COLING-2012, 2619?2634.
Adam Kilgarriff. 2001. Comparing corpora. CL-01,
6(1).
1468
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 1?7,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Using Twitter to Collect a Multi-Dialectal Corpus of Arabic
Hamdy Mubarak, Kareem Darwish
Qatar Computing Research Institute
Qatar Foundation
{hmubarak,kdarwish}@qf.org.qa
Abstract
This paper describes the collection and clas-
sification of a multi-dialectal corpus of Ara-
bic based on the geographical information of
tweets. We mapped information of user lo-
cations to one of the Arab countries, and
extracted tweets that have dialectal word(s).
Manual evaluation of the extracted corpus
shows that the accuracy of assignment of
tweets to some countries (like Saudi Arabia
and Egypt) is above 93% while the accuracy
for other countries, such Algeria and Syria is
below 70%.
1 Introduction
Arabic is a morphologically complex lan-
guage (Holes, 2004). With more than 380
million people whose mother tongue is Arabic, it
is the fifth most widely spoken language. Mod-
ern Standard Arabic (MSA) is the lingua franca
amongst Arabic native speakers, and is used in
formal communications, such as newspaper, official
speeches, and news broadcasts. However, MSA is
rarely used in day to day communication. Nearly
all the Arabic speakers use dialectal Arabic (DA)
in everyday communication (Cotterell et al., 2014).
DA may differ from MSA in morphology and
phonology (Habash et al., 2012). These dialects
may differ also in vocabulary and spelling from
MSA and most do not have standard spellings.
There is often large lexical overlap between di-
alects and MSA. Performing proper Arabic dialect
identification may positively impact many Natural
Language Processing (NLP) application. For exam-
ple, transcribing dialectal speech or automatically
translating into a particular dialect would be aided
by the use of targeted language models that are
trained on texts in that dialect. This has led to recent
interest in the automatic collection large dialectal
corpora and the identification of different Arabic
dialects (Al-Mannai et al., 2014; Elfardy et al.,
2013; Cotterell et al., 2014; Zaidan et al., 2014).
There are many varieties of dialectal Arabic dis-
tributed over the 22 countries in the Arabic world.
There are often several variants of a dialect within
the same country. There is also the difference be-
tween Bedouin and Sedentary speech, which runs
across all Arabic countries. However, in natural
language processing, researchers have merged di-
alectal Arabic into five regional language groups,
namely: Egyptian, Maghrebi, Gulf (Arabian Penin-
sula), Iraqi, and Levantine (Cotterell et al., 2014; Al-
Sabbagh and Girju, 2012).
In this paper, we use geographical information in
user Twitter profiles to collect a dialectal corpus for
different Arab countries. The contributions of this
paper are:
1. We show that we can use Twitter as a source
for collecting dialectal corpra for specific Arab
countries with reasonable accuracy.
2. We show that most Arabic dialectal words are
used in more than one country, and cannot be
used separately to collect a dialectal corpus per
country.
The paper is organized as follows: Section 2 sur-
veys pervious work on dialect classification; Sec-
tion 3 describes dialectal Arabic and some of the
possible ways to breakdown Arabic dialects; sec-
tion 4 describes how tweets are collected and classi-
fied; section 4 shows how to extract dialectal words
and shows that many of them are used in more than
one country; Section 5 describes our evaluation ap-
proach and reports on evaluation results; and Sec-
tion 6 contains conclusion and future work.
1
2 Previous Work
Previous work on Arabic dialect identification uses
n-gram based features at both word-level and
character-level to identify dialectal sentences (El-
fardy et al., 2013; Cotterell et al., 2014; Zaidan et al.,
2011; Zaidan et al., 2014). Zaidan et al. (2011) cre-
ated a dataset of dialectal Arabic. They performed
cross-validation experiments for dialect identifica-
tion using word n-gram based features. Elfardy
et al. (2013) built a system to distinguish between
Egyptian and MSA. They used word n-gram features
combined with core (token-based and perplexity-
based features) and meta features for training. Their
system showed a 5% improvement over the system
of Zaidan and Callison-Burch (2011). Later, Zaidan
et al. (2014) used several word n-gram based and
character n-gram based features for dialect identifi-
cation. The system trained on word unigram-based
feature performed the best with character five-gram-
based feature being second best. A similar result is
shown by Cotterell et al. (2014) where word unigram
model performs the best. Recent work by Darwish
et al. (2014) indicates that using a dialectal word list
to identify dialectal Egyptian tweets is better than
training on one of the existing dialect corpora.
All of the previous work except Cotterell et
al. (2014)
1
evaluated their systems using cross-
validation. These models heavily rely on the cover-
age of training data to achieve better identification.
This limits the robustness of identification to gen-
res inline with the training data. In this paper, we
exploit geographic information supplied by users to
properly identify the dialect of tweets.
There is also increasing interest in the literature to
geotag tweets due to its importance for some appli-
cations such as event detection, local search, news
recommendation and targeted advertising. For ex-
ample, Mahmud et el. (2012) (Mahmud et al., 2012)
presented a new algoritm for inferring home loca-
tions of Twitter users by collecting tweets from the
top 100 US cities using the geo-tag filter option of
Twitter and latitude and longitude for each city us-
ing Googles geo-coding API. Bo Han et al. (2014)
(Han et al., 2014) presented an integrated geolo-
cation prediction framework and investigated what
1
Zaidan et al. (2014) applied their classifier to a different
genre but did not evaluate it?s performance.
factors impact on prediction accuracy. They ex-
ploited the tweets and profile information of a given
user to infer their primary city-level location.
3 Dialectal Arabic (DA)
DA refers to the spoken language used for
daily communication in Arab countries. There
are considerable geographical distinctions be-
tween DAs within countries, across country bor-
ders, and even between cities and villages as
shown in Figure 1. According to Ethnologue
(http://www.ethnologue.com/browse/names), there
are 34 variations of spoken Arabic or dialects in
Arabic countries in addition to the Modern Standard
Arabic (MSA).
Some recent works (Zbib et al., 2012; Cotterell
et al., 2014) are based on a coarser classification of
Arabic dialects into five groups namely: Egyptian
(EGY), Gulf (GLF), Maghrebi (MGR), Levantine
(LEV), and Iraqi (IRQ). Other dialects are classified
as OTHER.
Zaidan and Callison-Burch (2014) mentioned that
this is one possible breakdown but it is relatively
coarse and can be further divided into more dialect
groups, especially in large regions such as Maghreb.
The goal of this paper is to collect a large, clean cor-
pus for each country and study empirically if some
of these dialects can be merged together.
We found that there are very few dialectal words
that are used in a country and not used in any other
country. For example, we took the most frequent
Egyptian dialectal words in the Arabic Online Com-
mentary Dataset (AOCD) described in Zaidan and
Callison-Burch (2014) according to what they call
the dialectness factor, which is akin to mutual in-
formation. The AOCD contains comments from
newspapers from dialect groups and these comments
were classified into different dialects using crowd
souring. We examined whether they appear in dif-
ferent dialects or not. As shown in Table 1, most
Egyptian dialectal words are being used in different
dialects.
With this finding, we realized that unique dialec-
tal words for each country are not common in the
sense that they are few and in the sense that relying
on them to filter tweets would likely yield a small
number of tweets. Thus, we opted not to use such
2
Figure 1: Different Arabic Dialects in the Arab World (http://en.wikipedia.org/wiki/Arabic_dialects)
Word Word in Tweet Dialect
?


X (dy) ?Y? ?K


Y? ??'


A

K Q
	
??@

??A
	
?

? ?


X

HA?AK


B@ ??

??@ Sudan
?X (dh) ?


?A

JK
.
?


?J


J
.
??@ ?X ! ?
	
KA??
	
?Q? ?


	
?
	
?J



?J


K
.
	
??? ???

?

K QK


@X
Q




J? ?C? Sudan
	
?A

?? (E$An)

??C
	
m
?
'@ ?


? ?


	
Y?

?
	
Jm
.
?
'@ ?


	
?
	
?A?? A
	
J? ?


	
?J
.
	
K
	
?A

?? A
	
J

??
	
m
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 132?136,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Automatic Correction of Arabic Text: a Cascaded Approach
Hamdy Mubarak, Kareem Darwish
Qatar Computing Research Institute
Qatar Foundation
{hmubarak,kdarwish}@qf.org.qa
Abstract
This paper describes the error correction model that
we used for the Automatic Correction of Arabic Text
shared task. We employed two correction mod-
els, namely a character-level model and a case-
specific model, and two punctuation recovery mod-
els, namely a simple statistical model and a CRF
model. Our results on the development set suggest
that using a cascaded correction model yields the
best results.
1 Introduction
In This paper, we describe our system for auto-
matic Arabic error correction shared task (QALB-
2014 Shared Task on Automatic Correction of Ara-
bic) as part of the Arabic NLP workshop (Mohit
et al., 2014). Our system is composed of two main
steps. The first involves correcting word level er-
rors, and the second pertains to performing punctu-
ation recovery. For word level correction, we used
two approaches, namely: 1) a statistical character
level transformation model that is aided by a lan-
guage model (LM) to handle letter insertions, dele-
tions, and substitutions and word merges; and 2) a
case-specific system that is aided by a LM to han-
dle specific error types such as dialectal word sub-
stitutions and word splits. For punctuation recovery,
we used two approaches, namely a simple statistical
word-based system, and a conditional random fields
(CRF) sequence labeler (Lafferty et al., 2001) that
attempts to recover punctuation based on POS and
word sequences. We performed all experiments on
the QALB dataset (Zaghouani et al., 2014).
2 Word Error Correction
In this section we describe two approaches for word
correction. The first approach involves using a char-
acter level model, and the second handles specific
correction cases.
2.1 Character-level Correction Model
For the character level model, we treated correction
as a Transliteration Mining (TM) task. In TM, a
sequence in a source alphabet is used to find the
most similar sequence in a lexicon that is written
in a target alphabet. TM has been fairly well stud-
ied with multiple evaluation campaigns such as the
Named Entities Workshop (NEWS) (Zhang et al.,
2011; Zhang et al., 2012). In our work, we adopted
a TM system to find corrections appearing in a large
Arabic corpus. The system involved learning char-
acter (or character-sequence) level mappings be-
tween erroneous words and their correct counter-
parts. Given the character mappings between the
erroneous and correct words, we used a generative
model that attempts to generate all possible map-
pings of a source word while restricting the out-
put to words in the target language (El-Kahki et
al., 2011; Noeman and Madkour, 2010). Specifi-
cally, we used the baseline system of El-Kahky et
al. (2011). To train character-level mappings, we
extracted all the parallel word-pairs in the original
(uncorrected) and corrected versions in the training
set. If a word in the original version of the training
set was actually correct, the word would be mapped
to itself. We then aligned the parallel word pairs at
character level using GIZA++ (Och and Ney, 2003),
and symmetrized the alignments using grow-diag-
132
final-and heuristic (Koehn et al., 2007). In all, we
aligned a little over one million word pairs. As in the
baseline of El-Kahki et al. (2011), given a possibly
misspelled word w
org
, we produced all its possible
segmentations along with their associated mappings
that we learned during alignment. Valid target se-
quences were retained and sorted by the product of
the constituent mapping probabilities. The top n (we
picked n = 10) candidates, w
trg
1..n
with the highest
probability were generated. Using Bayes rule, we
computed:
argmax
w
trg
i?1..n
p(w
trg
i
|w
org
) = p(w
org
|w
trg
i
)p(w
trg
i
)
(1)
where p(w
org
|w
trg
i
) is the posterior probability of
mapping, which is computed as the product of the
mappings required to generate w
org
from w
trg
i
,
and p(w
trg
i
) is the prior probability of the word.
Then we used a trigram LM to pick the most likely
candidate in context. We used a linear combination
of the the character-level transformation probability
and the LM probability using the following formula:
score = ?log(Prob
LM
) + (1? ?)log(Prob
char
)
We built the lexicon from a set of 234,638 Aljazeera
articles
1
that span 10 years and all of Arabic
Wikipedia. We also built a trigram language
model on the same corpus. The combined corpus
contains 576 million tokens including 1.6 million
unique ones. Spelling mistakes in Aljazeera arti-
cles (Mubarak et al., 2010) and Wikipedia were
infrequent.
We varied the value of ? between 0 and 1 with in-
crements of 0.1 and found that the values 0.6 and 0.7
yielded the best results. This indicates that LM prob-
ability is more important than character-mapping
probability.
2.2 Case-specific Correction
In this method we attempted to address specific
types of errors that are potentially difficult for the
character-based model to handle. Some of these er-
rors include dialectal words and words that were er-
roneously split. Before applying any correction, we
consulted a bigram LM that was trained the afore-
mentioned set of Aljazeera articles. The following
1
http://www.aljazeera.net
cases are handled (in order):
? Switching from English punctuations to Arabic
ones, namely changing: ??? ? ??? and ?,?? ?,?.
? Handling common dialectal words and common
word-level mistakes. An example dialectal word is
?


?
?@ (Ally)
2
(meaning ?this? or ?that?) which could
be mapped to ?


	
Y ?@ (Al?y) , ?



? ?@ (Alty) or
	
?K


	
Y ? @
(Al?yn). An example of a common mistake is ZA

?
	
 @
??? @ (An$A? Allh) (meaning ?God willing?) which is
corrected to ? ? ? @ Z A

?
	
?@

(>n $A? Allh). The sen-
tence is scored with and without the word replace-
ment, and the replacement is done if it yields higher
LM probability.
?Handling errors pertaining to the different forms
of alef, alef maqsoura and ya, and ta marbouta
and ha (Nizar Habash, 2010). We reimplemented
the baseline system in (Moussa et al., 2012) where
words are normalized and the different possible de-
normalized forms are scored in context using the
LM. We also added the following cases, namely at-
tempting to replace:

?' (&) with ?

?' (&w) or ?

K'
(}w); and

?' (}) with Z?


(y?) or vice versa (ex:
?

?Q? (mr&s)? ??

?Q? (mr&ws)).
? Handling merges and splits. Often words are
concatenated erroneously. Thus, we attempted to
split all words that were at least 5 letters long af-
ter letters that don?t change their shapes when they
are connected to the letters following them, namely
different alef forms, X (d),
	
X (*), P (r),
	
P (z), ? (w),

?
(p), and ? (Y) (ex: A
	
JK
.
PAK


(yArbnA)? A
	
JK
.
P AK


(yA
rbnA)). If the bigram was observed in the LM and
the LM score was higher (in context) than when they
were concatenated, then the word was split. Con-
versely, some words were split in the middle. We
attempted to merge every two words in sequence.
If the LM score was higher (in context) after the
merge, then the two words would be merged (ex:
2
Buckwalter transiteration
133
H@ PA?

J
	
K @ (AntSAr At)?

H@PA?

J
	
K @ (AntSArAt)).
? Removing repeated letters. Often people repeat
letters, particularly long vowels, for emphasis as in
@ @ @
Q



J


J


J


	
k

@ (>xyyyyrAAA) (meaning ?at last?). We
corrected for elongation in a manner similar to that
of Darwish et al. (Darwish et al., 2012). When a
long vowel are repeated, we replaced it with a either
the vowel (ex. @
Q



	
g

@) (>xyrA) or the vowel with one
repetition (ex. @
Q



J


	
k

@) (>xyyrA) and scored using
the LM. If a repeated alef appeared in the beginning
of the word, we attempted to replace it with alef lam
(ex.

?PA
	
? k@@ (AAHDArp) ?

?PA
	
? m
?
'@ (AlHDArp)
(meaning ?civilization?)). A trailing alef-hamza-
alef sequence was replaced by alef-hamza (ex. @ Z A?
?
?
(smA?A)? ZA?
?
?
(smA?) (meaning ?sky?)).
? Correcting out-of-vocabulary words. For words
that were not observed in the LM, we attempted the
following corrections: 1) replacing phonetically or
visually confusable letters, namely
	
? (D) and
	
?
(Z), X (d) and
	
X (*), and
	
X (*) and
	
P; (z) (ex: ?
.
A
	
?
(ZAbT) ? ? 
.
A
	
? (DAbT)) 2) removing the letters
H
.
(b) and X (d) that are added to verbs in present
tense in some dialects (ex: I
.

J?J


K
.
(byktb)? I
.

J?K


(yktb)); 3) replacing the letters h (H) and ? (h),
which are added in some dialects to indicate future
tense, with ? (s) (ex: H
.
Q?

?J


k (Hy$rb)? H
.
Q?

?J


?
(sy$rb)); and 4) replacing a leading ?A? (hAl) with
either ?@ @
	
Y ? (h*A Al) or ?@ ?
	
Y ? (h*h Al) (ex.
H
.
A

J??A? (hAlktAb)? H
.
A

J??@ @
	
Y? (h*A AlktAb))
and the leading ?A? (EAl) with ?@
??
? (ElY Al) (ex.
	
?P

BA ? (EAl>rD) ?
	
?P

B@
??
? (ElY Al>rD)).
After replacement, the LM was always consulted.
2.3 Correction Results
Table 1 reports on the results of performing both cor-
rection methods on the development set. Also, since
Method F-measure
Character-level 0.574
Case-specific 0.587
Character-level? Case-specific 0.615
Case-specific? Character-level 0.603
Table 1: The correction results using the character-level
model, case-specific correction, or their cascades.
the case-specific corrections handle cases that were
not handled by the character-level model, we at-
tempted to cascade both methods together. It seems
that when applying the character-level model first
followed by the case-specific correction yielded the
best results.
3 Punctuation Recovery
In this section, we describe two methods for punc-
tuation recovery. The first is a simple word-based
model and the other is a CRF based model.
3.1 Simple Statistical Model
In this approach, we identified words that were pre-
ceded or followed by punctuations in the training
set. If a word was preceded or followed by a par-
ticular punctuation mark more than 40% of the time,
then we automatically placed the punctuation before
or after the word in the dev set. Also, if a sentence
did not have a period at the end of it, we added a
period.
3.2 CRF Model
In this approach we trained a CRF sequence labeling
to attempt to recover punctuation. CRF combines
state and transition level features making it a pos-
sibly better choice than an HMM or a simple clas-
sifier. We used the CRF++ implementation
3
of the
sequence labeler. We trained the labeler on the train-
ing part of the QALB dataset. We used the following
features:
Word features: the current word, the previous and
next words, and the two previous and two next
words.
Part-of-speech (POS) tags: the POS of the current
3
http://crfpp.googlecode.com/svn/trunk/
doc/index.html
134
Method Precision Recall F-measure
Stat model 0.306 0.153 0.204
CRF model 0.373 0.141 0.204
Table 2: The punctuation recovery results using the sim-
ple statistical model and the CRF model.
Method F-measure
Stat model (before correction) 0.593
Stat model (after correction) 0.614
CRF model (before correction) 0.607
CRF model (after correction) 0.615
Table 3: Cascaded correction (Character-level ? Case-
specific) combined with punctuation recovery.
word and the POS of the two previous and two fol-
lowing words.
3.3 Punctuation Recovery Results
Table 2 reports on the results of using the two differ-
ent methods for punctuation recovery. Note that no
other correction is applied.
4 Combining Correction with Punctuation
Recovery
Given that cascading both correction models yielded
the best results, we attempted to combine the cas-
caded correction model with the two punctuation re-
covery methods. We tried to put punctuation recov-
ery before and after correction. Table 3 summarizes
the results. As the results suggest, combining cor-
rection with punctuation recovery had a negative ef-
fect on overall F-measure. This requires further in-
vestigation.
5 Official Shared Task Experiments and
Results
For the official submissions to the shared task, we
submitted 3 runs as follows:
1. QCRI-1: character-level correction, then case-
based correction.
2. QCRI-2: case-based correction, then statistical
punctuation recovery
3. QCRI-3: exactly like 2, but preceded also by
statistical punctuation recovery
Run Precision Recall F-measure
QCRI-1 0.717 0.5686 0.6343
QCRI-2 0.6286 0.6032 0.6157
QCRI-3 0.6066 0.5928 0.5996
Table 4: Official Results.
Table 4 reports on the officially submitted results
against the test set. It seems that our attempts to add
punctuation recovery worsened results.
6 Conclusion
In this paper, we presented automatic approaches
for correcting Arabic text and punctuation recovery.
Our results on the development set shows that using
a cascaded approach that involves a character-level
model and another model that handles specific errors
yields the best results. Incorporating punctuation re-
covery did not improve correction.
References
Kareem Darwish, Walid Magdy, and Ahmed Mourad.
2012. Language processing for arabic microblog re-
trieval. Proceedings of the 21st ACM international
conference on Information and knowledge manage-
ment. ACM, 2012.
Ali El-Kahky, Kareem Darwish, Ahmed Saad Aldein,
Mohamed Abd El-Wahab, Ahmed Hefny, and Waleed
Ammar. 2001. Improved transliteration mining using
graph reinforcement. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pp. 1384-1393, 2011.
Nizar Habash. 2010. Introduction to Arabic natural lan-
guage processing. Synthesis Lectures on Human Lan-
guage Technologies 3.1 (2010): 1-187
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst, Moses: Open Source Toolkit
for Statistical Machine Translation, Annual Meeting of
the Association for Computational Linguistics (ACL),
demonstration session, Prague, Czech Republic, June
2007.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data, In Proc. of ICML,
pp.282-289, 2001.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi
Zaghouani, and Ossama Obeid, 2014. The First QALB
135
Shared Task on Automatic Text Correction for Arabic.
In Proceedings of EMNLP workshop on Arabic Natu-
ral Language Processing. Doha, Qatar.
Mohammed Moussa, Mohamed Waleed Fakhr, and Ka-
reem Darwish. 2012. Statistical denormalization for
Arabic Text. In Empirical Methods in Natural Lan-
guage Processing, pp. 228. 2012.
Hamdy Mubarak, Ahmed Metwali, Mostafa Ramadan.
2010. Spelling Mistakes in Arabic Newspapers. Arabic
Language and Scientific Researches conference, Fac-
ulty of Arts, Ain Shams University, Cairo, Egypt
Sara Noeman and Amgad Madkour. 2010. Language In-
dependent Transliteration Mining System Using Finite
State Automata Framework. ACL NEWS workshop
2010.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, Vol. 1(29), 2003.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large Scale Arabic Error Annotation: Guidelines and
Framework. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC14), Reykjavik, Iceland.
Min Zhang, A Kumaran, Haizhou Li. 2011. Whitepaper
of NEWS 2012 Shared Task on Machine Translitera-
tion. IJCNLP-2011 NEWS workshop.
Min Zhang, Haizhou Li, Ming Liu, A Kumaran. 2012.
Whitepaper of NEWS 2012 Shared Task on Machine
Transliteration. ACL-2012 NEWS workshop.
136
