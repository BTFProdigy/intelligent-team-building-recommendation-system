Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 177?184, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Inner-Outer Bracket Models for Word Alignment
using Hidden Blocks
Bing Zhao
School of Computer Science
Carnegie Mellon University
{bzhao}@cs.cmu.edu
Niyu Ge and Kishore Papineni
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598, USA
{niyuge, papineni}@us.ibm.com
Abstract
Most statistical translation systems are
based on phrase translation pairs, or
?blocks?, which are obtained mainly from
word alignment. We use blocks to infer
better word alignment and improved word
alignment which, in turn, leads to better
inference of blocks. We propose two new
probabilistic models based on the inner-
outer segmentations and use EM algorithms
for estimating the models? parameters. The
first model recovers IBM Model-1 as a spe-
cial case. Both models outperform bi-
directional IBM Model-4 in terms of word
alignment accuracy by 10% absolute on the
F-measure. Using blocks obtained from
the models in actual translation systems
yields statistically significant improvements
in Chinese-English SMT evaluation.
1 Introduction
Today?s statistical machine translation systems rely
on high quality phrase translation pairs to acquire
state-of-the-art performance, see (Koehn et al, 2003;
Zens and Ney, 2004; Och and Ney, 2003). Here,
phrase pairs, or ?blocks? are obtained automati-
cally from parallel sentence pairs via the underlying
word alignments. Word alignments traditionally are
based on IBM Models 1-5 (Brown et al, 1993) or on
HMMs (Vogel et al, 1996). Automatic word align-
ment is challenging in that its accuracy is not yet
close to inter-annotator agreement in some language
pairs: for Chinese-English, inter-annotator agree-
ment exceeds 90 on F-measure whereas IBM Model-
4 or HMM accuracy is typically below 80s. HMMs
assume that words ?close-in-source? are aligned to
words ?close-in-target?. While this locality assump-
tion is generally sound, HMMs do have limitations:
the self-transition probability of a state (word) is the
only control on the duration in the state, the length
of the phrase aligned to the word. Also there is no
natural way to control repeated non-contiguous vis-
its to a state. Despite these problems, HMMs remain
attractive for their speed and reasonable accuracy.
We propose a new method for localizing word
alignments. We use blocks to achieve locality in the
following manner: a block in a sentence pair is a
source phrase aligned to a target phrase. We assume
that words inside the source phrase cannot align to
words outside the target phrase and that words out-
side the source phrase cannot align to words inside
the target phrase. Furthermore, a block divides the
sentence pair into two smaller regions: the inner
part of the block, which corresponds to the source
and target phrase in the block, and the outer part of
the block, which corresponds to the remaining source
and target words in the parallel sentence pair. The
two regions are non-overlapping; and each of them is
shorter than the original parallel sentence pair. The
regions are thus easier to align than the original sen-
tence pairs (e.g., using IBM Model-1). While the
model uses a single block to split the sentence pair
into two independent regions, it is not clear which
block we should select for this purpose. Therefore,
we treat the splitting block as a hidden variable.
This proposed approach is far simpler than treat-
ing the entire sentence as a sequence of non-
overlapping phrases (or chunks) and considering such
sequential segmentation either explicitly or implic-
itly. For example, (Marcu and Wong, 2002) for a
joint phrase based model, (Huang et al, 2003) for
a translation memory system; and (Watanabe et
al., 2003) for a complex model of insertion, deletion
and head-word driven chunk reordering. Other ap-
proaches including (Watanabe et al, 2002) treat ex-
tracted phrase-pairs as new parallel data with limited
success. Typically, they share a similar architecture
of phrase level segmentation, reordering, translation
as in (Och and Ney, 2002; Koehn and Knight, 2002;
Yamada and Knight, 2001). The phrase level inter-
action has to be taken care of for the non-overlapping
sequential segmentation in a complicated way. Our
models model such interactions in a soft way. The
hidden blocks are allowed to overlap with each other,
177
while each block induced two non-overlapping re-
gions, i.e. the model brackets the sentence pair
into two independent parts which are generated syn-
chronously. In this respect, it resembles bilingual
bracketing (Wu, 1997), but our model has more lex-
ical items in the blocks with many-to-many word
alignment freedom in both inner and outer parts.
We present our localization constraints using
blocks for word alignment in Section 2; we detail our
two new probabilistic models and their EM train-
ing algorithms in Section 3; our baseline system, a
maximum-posterior inference for word alignment, is
explained in Section 4; experimental results of align-
ments and translations are in Section 5; and Section
6 contains discussion and conclusions.
2 Segmentation by a Block
We use the following notation in the remainder of
this paper: e and f denote the English and foreign
sentences with sentence lengthes of I and J , respec-
tively. ei is an English word at position i in e; fj is
a foreign word at position j in f . a is the alignment
vector with aj mapping the position of the English
word eaj to which fj connects. Therefore, we have
the standard limitation that one foreign word can-
not be connected to more than one English word. A
block ?[] is defined as a pair of brackets as follows:
?[] = (?e, ?f ) = ([il, ir], [jl, jr]), (1)
where ?e = [il, ir] is a bracket in English sentence de-
fined by a pair of indices: the left position il and the
right position ir, corresponding to a English phrase
eiril . Similar notations are for ?f = [jl, jr], which isone possible projection of ?e in f . The subscript l and
r are abbreviations of left and right, respectively.
?e segments e into two parts: (?e, e) = (?e?, ?e/?).The inner part ?e? = {ei, i ? [il, ir]} and the outer
part ?e/? = {ei, i /? [il, ir]}; ?f segments f similarly.
Thus, the block ?[] splits the parallel sentence pair
into two non-overlapping regions: the Inner ?[]? and
Outer ?[]/? parts (see Figure 1). With this segmen-tation, we assume the words in the inner part are
aligned to inner part only: ?[]? = ?e? ? ?f? : {ei, i ?
[il, ir]} ? {fj , j ? [jl, jr]}; and words in the outer
part are aligned to outer part only: ?[]/? = ?e/? ? ?f/? :{ei, i /? [il, ir]} ? {fj , j /? [jl, jr]}. We do not allow
alignments to cross block boundaries. Words inside
a block ?[] can be aligned using a variety of models
(IBM models 1-5, HMM, etc). We choose Model1 for
simplicity. If the block boundaries are accurate, we
can expect high quality word alignment. This is our
proposed new localization method.
Outer
Inner
li ri
rj
lj
e?
f?
Figure 1: Segmentation by a Block
3 Inner-Outer Bracket Models
We treat the constraining block as a hidden variable
in a generative model shown in Eqn. 2.
P (f |e) =
?
{?[]}
P (f , ?[]|e)
=
?
{?e}
?
{?f}
P (f , ?f |?e, e)P (?e|e), (2)
where ?[] = (?e, ?f ) is the hidden block. In the gen-
erative process, the model first generates a bracket
?e for e with a monolingual bracketing model of
P (?e|e). It then uses the segmentation of the En-
glish (?e, e) to generate the projected bracket ?f of f
using a generative translation model P (f , ?f |?e, e) =
P (?f/?, ?f?|?e/?, ?e?) ? the key model to implement ourproposed inner-outer constraints. With the hidden
block ?[] inferred, the model then generates word
alignments within the inner and outer parts sepa-
rately. We present two generating processes for the
inner and outer parts induced by ?[] and correspond-
ing two models of P (f , ?f |?e, e). These models are
described in the following secions.
3.1 Inner-Outer Bracket Model-A
The first model assumes that the inner part and the
outer part are generated independently. By the for-
mal equivalence of (f, ?f ) with (?f?, ?f/?), Eqn. 2 canbe approximated as:
P (f |e)?
?
{?e}
?
{?f}
P (?f?|?e?)P (?f/?|?e/?)P (?e|e)P (?f |?e),
(3)
where P (?f?|?e?) and P (?f/?|?e/?) are two independentgenerative models for inner and outer parts, respec-
178
tively and are futher decompsed into:
P (?f?|?e?) =
?
{aj??e?}
?
fj??f?
P (fj |eaj )P (eaj |?e?)
P (?f/?|?e/?) =
?
{aj??e/?}
?
fj??f/?
P (fj |eaj )P (eaj |?e/?), (4)
where {aJ1 } is the word alignment vector. Given the
block segmentation and word alignment, the genera-
tive process first randomly selects a ei according to
either P (ei|?e?) or P (ei|?e/?); and then generates fj in-dexed by word alignment aj with i = aj according to
a word level lexicon P (fj |eaj ). This generative pro-
cess using the two models of P (?f?|?e?) and P (?f/?|?e/?)must satisfy the constraints of segmentations induced
by the hidden block ?[] = (?e, ?f ). The English
words ?e? inside the block can only generate the words
in ?f? and nothing else; likewise ?e/? only generates
?f/?. Overall, the combination of P (?f?|?e?)P (?f/?|?e/?)in Eqn. 3 collaborates each other quite well in prac-
tice. For a particular observation ?f?, if ?e? is too
small (i.e., missing translations), P (?f?|?e?) will suf-
fer; and if ?e? is too big (i.e., robbing useful words
from ?e/?), P (?f/?|?e/?) will suffer. Therefore, our pro-posed model in Eqn. 3 combines the two costs and
requires both inner and outer parts to be explained
well at the same time.
Because the model in Eqn. 3 is essentially a two-
level (?[] and a) mixture model similar to IBM Mod-
els, the EM algorithm is quite straight forward as
in IBM models. Shown in the following are several
key E-step computations of the posteriors. The M-
step (optimization) is simply the normalization of
the fractional counts collected using the posteriors
through the inference results from E-step:
P?[]?(aj |?
f
?, ?e?) =
P (fj |eaj )?
ek??e? P (fj |ek)
P?[]/?(aj |?
f
/?, ?e/?) =
P (fj |eaj )?
ek??e/? P (fj |ek)
(5)
The posterior probability of P (aJ1 |f , ?f , ?e, e) =?J
j=1 P (aj |f , ?f , ?e, e), where P (aj |f , ?f , ?e, e) is ei-
ther P?[]?(aj |?
f
?, ?e?) when (fj , eaj ) ? ?[]?, or oth-
erwise P?[]/?(aj |?
f
/?, ?e/?) when (fj , eaj ) ? ?[]/?. As-
suming P (?e|e) to be a uniform distribution, the
posterior of selecting a hidden block given ob-
servations: P (?[] = (?e, ?f )|e, f) is proportional
to block level relative frequency Prel(?f?|?e?) up-
dated in each iteration; and can be smoothed
with P (?f |?e, f , e) = P (?f?|?e?)P (?f/?|?e/?)/
?
{??f}
P (??f? |?e?)P (?
?f
/? |?e/?) assuming Model-1 alignment inthe inner and outer parts independently to reduce
the risks of data sparseness in estimations.
In principle, ?e can be a bracket of any length
not exceeding the sentence length. If we restrict the
bracket length to that of the sentence length, we re-
cover IBM Model-1. Figure 2 summarizes the gener-
ation process for Inner-Outer Bracket Model-A.
f1 f2  f3  f4
e1 e2  e3
[e1] e2  e3 e1 [e2] e3 [e1 e2] e3 e1 [e2 e3]
?.
f1 f4
e1 e3
f2  f3
e2
f1 f3 f4
e1 e3
f2
e2
? ?
]3,2[=f? ]2,2[=f? [.,.]=f?
]2,2[=e?]1,1[=e? ]2,1[=e? ]3,2[=e?
innerouter innerouter
Figure 2: Illustration of generative Bracket Model-A
3.2 Inner-Outer Bracket Model-B
A block ?[] invokes both the inner and outer gener-
ations simultaneously in Bracket Model A (BM-A).
However, the generative process is usually more ef-
fective in the inner part as ?[] is generally small and
accurate. We can build a model focusing on gener-
ating only the inner part with careful inferences to
avoid errors from noisy blocks. To ensure that all
fJ1 are generated, we need to propose enough blocks
to cover each observation fj . This constraint can be
met by treating the whole sentence pair as one block.
The generative process is as follows: First the
model generates an English bracket ?e as before. The
model then generates a projection ?f in f to local-
ize all aj ?s for the given ?e according to P (?f |?e, e).
?e and ?f forms a hidden block ?[]. Given ?[], the
model then generates only the inner part fj ? ?f? via
P (f |?f , ?e, e) ' P (?f?|?f , ?e, e). Eqn. 6 summarizes
this by rewriting P (f , ?f |?e, e):
P (f , ?f |?e, e) = P (f |?f , ?e, e)P (?f |?e, e) (6)
= P (f |?f , ?e, e)P ([jl, jr]|?e, e)
' P (?f?|?f , ?e, e)P ([jl, jr]|?e, e).
P (?f?|?f , ?e, e) is a bracket level emission proba-
bilistic model which generates a bag of contiguous
words fj ? ?f? under the constraints from the given
hidden block ?[] = (?f , ?e). The model is simplified
in Eqn. 7 with the assumption of bag-of-words? inde-
pendence within the bracket ?f :
P (?f?|?f , ?e, e) =?
aJ1
?
j??f? P (fj |eaj )P (eaj |?
f , ?e, e). (7)
179
180
puting a pair (j, t)?:
(j, t)? = argmax
(j,t)
P (fj |et), (11)
that is, the point at which the posterior is maximum.
The pair (j, t) defines a word pair (fj , et) which is
then aligned. The procedure continues to find the
next maximum in the posterior matrix. Contrast
this with Viterbi alignment where one computes
f?T1 = argmax
{fT1 }
P (f1, f2, ? ? ? , fT |eT1 ), (12)
We observe, in parallel corpora, that when one
word translates into multiple words in another lan-
guage, it usually translates into a contiguous se-
quence of words. Therefore, we impose a conti-
guity constraint on word alignments. When one
word fj aligns to multiple English words, the En-
glish words must be contiguous in e and vice versa.
The algorithm to find word alignments using max-
posterior with contiguity constraint is illustrated in
Algorithm 1.
Algorithm 1 A maximum-posterior algorithm with
contiguity constraint
1: while (j, t) = (j, t)? (as computed in Eqn. 11)
do
2: if (fj , et) is not yet algned then
3: align(fj , et);
4: else if (et is contiguous to what fj is aligned)
or (fj is contiguous to what et is aligned) then
5: align(fj , et);
6: end if
7: end while
The algorithm terminates when there isn?t any
?next? posterior maximum to be found. By defi-
nition, there are at most JxT ?next? maximums in
the posterior matrix. And because of the contiguity
constraint, not all (fj , et) pairs are valid alignments.
The algorithm is sure to terminate. The algorithm
is, in a sense, directionless, for one fj can align to
multiple et?s and vise versa as long as the multiple
connections are contiguous. Viterbi, however, is di-
rectional in which one state can emit multiple obser-
vations but one observation can only come from one
state.
5 Experiments
We evaluate the performances of our proposed mod-
els in terms of word alignment accuracy and trans-
lation quality. For word alignment, we have 260
hand-aligned sentence pairs with a total of 4676 word
pair links. The 260 sentence pairs are randomly
selected from the CTTP1 corpus. They were then
word aligned by eight bilingual speakers. In this set,
we have one-to-one, one-to-many and many-to-many
alignment links. If a link has one target functional
word, it is considered to be a functional link (Ex-
amples of funbctional words are prepositions, deter-
miners, etc. There are in total 87 such functional
words in our experiments). We report the overall F-
measures as well as F-measures for both content and
functional word links. Our significance test shows
an overall interval of ?1.56% F-measure at a 95%
confidence level.
For training data, the small training set has 5000
sentence pairs selected from XinHua news stories
with a total of 131K English words and 125K Chi-
nese words. The large training set has 181K sentence
pairs (5k+176K); and the additional 176K sentence
pairs are from FBIS and Sinorama, which has in to-
tal 6.7 million English words and 5.8 million Chinese
words.
5.1 Baseline Systems
The baseline is our implementation of HMM with
the maximum-posterior algorithm introduced in sec-
tion 4. The HMMs are trained unidirectionally. IBM
Model-4 is trained with GIZA++ using the best re-
ported settings in (Och and Ney, 2003). A few pa-
rameters, especially the maximum fertility, are tuned
for GIZA++?s optimal performance. We collect bi-
directional (bi) refined word alignment by growing
the intersection of Chinese-to-English (CE) align-
ments and English-to-Chinese (EC) alignments with
the neighboring unaligned word pairs which appear
in the union similar to the ?final-and? approaches
(Koehn, 2003; Och and Ney, 2003; Tillmann, 2003).
Table 1 summarizes our baseline with different set-
tings. Table 1 shows that HMM EC-P gives the
F-measure(%) Func Cont Both
Small
HMM EC-P 54.69 69.99 64.78
HMM EC-V 31.38 53.56 55.59
HMM CE-P 51.44 69.35 62.69
HMM CE-V 31.43 63.84 55.45
Large
HMM EC-P 60.08 78.01 71.92
HMM EC-V 32.80 74.10 64.26
HMM CE-P 58.45 79.44 71.84
HMM CE-V 35.41 79.12 68.33
Small GIZA MH-bi 45.63 69.48 60.08GIZA M4-bi 48.80 73.68 63.75
Large GIZA MH-bi 49.13 76.51 65.67GIZA M4-bi 52.88 81.76 70.24
- Fully-Align 2 5.10 15.84 9.28
Table 1: Baseline: V: Viterbi; P: Max-Posterior
1LDC2002E17
181
best baseline, better than bidirectional refined word
alignments from GIZA M4 and the HMM Viterbi
aligners.
5.2 Inner-Outer Bracket Models
We trained HMM lexicon P (f |e) to initialize the
inner-outer Bracket models. Afterwards, up to 15?
20 EM iterations are carried out. Iteration starts
from the fully aligned2 sentence pairs, which give an
F-measure of 9.28% at iteration one.
5.2.1 Small Data Track
Figure 4 shows the performance of Model-A (BM-
A) trained on the small data set. For each English
bracket, Top-1 means only the fractional counts from
the Top-1 projection are collected, Top-all means
counts from all possible projections are collected. In-
side means the fractional counts are collected from
the inner part of the block only; and outside means
they are collected from the outer parts only. Using
the Top-1 projection from the inner parts of the block
(top-1-inside) gives the best performance: an F-
measure of 72.29%, or a 7.5% absolute improvement
over the best baseline at iteration 5. Figure 5 shows
BM-A with different settings on small data set
62
64
66
68
70
72
74
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16EM  Iterations
F-m
ea
su
re top-1 inside
top-all inside
top all
top-1
top-1 outside
top-all outside
Figure 4: BM-A with different settings on small data
the performance of Inner-Outer Bracket Model-B
(BM-B) over EM iterations. smoothing means when
collecting the fractional counts, we reweigh the up-
dated fractional count by 0.95 and give the remain-
ing 0.05 weight to original fractional count from the
links, which were aligned in the previous iteration.
w/null means we applied the proposed Null word
model in section 3.3 to infer null links. We also pre-
defined a list of 15 English function words, for which
there might be no corresponding Chinese words as
translations. These 15 English words are ?a, an, the,
of, to, for, by, up, be, been, being, does, do, did, -?.
In the drop-null experiments, the links containing
these predefined function words are simply dropped
2Every possible word pair is aligned
in the final word alignment (this means they are left
unaligned).
BM-B with different settings on small data set
65
67
69
71
73
75
77
1 2 3 4 5 6 7 8EM  Iterations
F-m
ea
su
re
top-1 smooth dropnulltop-1 smooth w/nulltop-1 smoothtop 1top all
Figure 5: BM-B with different settings on small data
Empirically we found that doing more than 5 it-
erations lead to overfitting. The peak performance
in our model is usually achieved around iteration
4?5. At iteration 5, setting ?BM-B Top-1? gives an
F-measure of 73.93% which is better than BM-A?s
best performance (72.29%). This is because Model
B leverages a local search for less noisy blocks and
hence the inner part is more accurately generated
(which in turn means the outer part is also more
accurate). From this point on, all of our experi-
ments are using Model B. With smoothing, BM-B
improves to 74.46%. After applying the null word
model, we get 75.20%. By simply dropping links
containing the 15 English functional words, we get
76.24%, which is significantly better than our best
baseline obtained from even the large training set
(HMM EC-P: 71.92%).
BM-B with different settings on large data set
69
71
73
75
77
79
81
83
1 2 3 4 5 6 7 8EM  Iterations
F-m
ea
su
re
top-1 smooth dropnull
top-1 smooth w/null
top-1 smooth
Figure 6: BM-B with different settings on large data
5.2.2 Large Data Track
Figure 6 shows performance pictures of model
BM-B on the large training set. Without dropping
English functional words, the best performance is
182
80.38% at iteration 4 using the Top-1 projection to-
gether with the null word models. By additionally
dropping the links containing the 15 functional En-
glish words, we get 81.47%. These results are all
significantly better than our strongest baseline sys-
tem: 71.92% F-measure using HMM EC-P (70.24%
using bidirectional Model-4 for comparisons).
On this data set, we experimented with different
maximum bracket length limits, from one word (un-
igram) to nine-gram. Results show that a maximum
bracket length of four is already optimal (79.3% with
top-1 projection), increased from 62.4% when maxi-
mum length is limited to one. No improvements are
observed using longer than five-gram.
5.3 Evaluate Blocks in the EM Iterations
Our intuition was that good blocks can improve word
alignment and, in turn, good word alignment can
lead to better block selection. The experimental re-
sults above support the first claim. Now we consider
the second claim that good word alignment leads to
better block selection.
Given reference human word alignment, we extract
reference blocks up to five-gram phrases on Chinese.
The block extraction procedure is based on the pro-
cedures in (Tillmann, 2003).
During EM, we output all the hidden blocks actu-
ally inferred at each iteration, then we evaluate the
precision, recall and F-measure of the hidden blocks
according to the extracted reference blocks. The re-
sults are shown in Figure 7. Because we extract all
10%
15%
20%
25%
30%
35%
40%
45%
F-m
ea
su
res
1 2 3 4 5 6 7 8EM  Iterations
A Direct Eval of blocks' accuracy in 'BM-B top-1 smooth w/null'
F-measure
Recall
Precision
Figure 7: A Direct Eval. of Blocks in BM-B
possible n-grams at each position in e, the precision
is low and the recall is relatively high as shown by
Figure 7. It also shows that blocks do improve, pre-
sumably benefiting from better word alignments.
Table 2 summarizes word alignment performances
of Inner-Outer BM-B in different settings. Overall,
without the handcrafted function word list, BM-B
gives about 8% absolute improvement in F-measure
on the large training set and 9% for the small set
F-measure(%) Func Cont Both
Small
Baseline 54.69 69.99 64.78
BM-B-drop 62.76 82.99 76.24
BM-B w/null 61.24 82.54 75.19
BM-B smooth 59.61 82.99 74.46
Large
Baseline 60.08 78.01 71.92
BM-B-drop 63.95 90.09 81.47
BM-B w/null 62.24 89.99 80.38
BM-B smooth 60.49 90.09 79.31
Table 2: BM-B with different settings
with a confidence interval of ?1.56%.
5.4 Translation Quality Evaluations
We also carried out the translation experiments using
the best settings for Inner-Outer BM-B (i.e. BM-B-
drop) on the TIDES Chinese-English 2003 test set.
We trained our models on 354,252 test-specific sen-
tence pairs drawn from LDC-supplied parallel cor-
pora. On this training data, we ran 5 iterations of
EM using BM-B to infer word alignments. A mono-
tone decoder similar to (Tillmann and Ney, 2003)
with a trigram language model3 is set up for trans-
lations. We report case sensitive Bleu (Papineni et
al., 2002) scoreBleuC for all experiments. The base-
line system (HMM ) used phrase pairs built from the
HMM-EC-P maximum posterior word alignment and
the corresponding lexicons. The baseline BleuC score
is 0.2276 ? 0.015. If we use the phrase pairs built
from the bracket model instead (but keep the HMM
trained lexicons), we get case sensitive BleuC score
0.2526. The improvement is statistically significant.
If on the other hand, we use baseline phrase pairs
with bracket model lexicons, we get a BleuC score
0.2325, which is only a marginal improvement. If we
use both phrase pairs and lexicons from the bracket
model, we get a case sensitive BleuC score 0.2750,
which is a statistically significant improvement. The
results are summarized in Table 3.
Settings BleuC
Baseline (HMM phrases and lexicon) 0.2276
Bracket phrases and HMM lexicon 0.2526
Bracket lexicon and HMM phrases 0.2325
Bracket (phrases and lexicon) 0.2750
Table 3: Improved case sensitive BleuC using BM-B
Overall, using Model-B, we improve translation
quality from 0.2276 to 0.2750 in case sensitive BleuC
score.
3Trained on 1-billion-word ViaVoice English data; the
same data is used to build our True Caser.
183
6 Conclusion
Our main contributions are two novel Inner-Outer
Bracket models based on segmentations induced by
hidden blocks. Modeling the Inner-Outer hidden seg-
mentations, we get significantly improved word align-
ments for both the small training set and the large
training set over the widely-practiced bidirectional
IBM Model-4 alignment. We also show significant
improvements in translation quality using our pro-
posed bracket models. Robustness to noisy blocks
merits further investigation.
7 Acknowledgement
This work is supported by DARPA under contract
number N66001-99-28916.
References
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation:
Parameter estimation. In Computational Linguis-
tics, volume 19(2), pages 263?331.
Niyu Ge. 2004. A maximum posterior method
for word alignment. In Presentation given at
DARPA/TIDES MT workshop.
J.X. Huang, W.Wang, and M. Zhou. 2003. A unified
statistical model for generalized translation mem-
ory system. In Machine Translation Summit IX,
pages 173?180, New Orleans, USA, September 23-
27.
Philipp Koehn and Kevin Knight. 2002. Chunkmt:
Statistical machine translation with richer linguis-
tic knowledge. Draft, Unpublished.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based machine transla-
tion. In Proc. of HLT-NAACL 2003, pages 48?54,
Edmonton, Canada, May-June.
Philipp Koehn. 2003. Noun phrase translation. In
Ph.D. Thesis, University of Southern California,
ISI.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical ma-
chine translation. In Proc. of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 133?139, Philadelphia, PA, July 6-7.
Franz J. Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for
statistical machine translation. In Proceedings of
the 40th Annual Meeting of ACL, pages 440?447.
Franz J. Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. In Computational Linguistics, volume 29,
pages 19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proc.
of the 40th Annual Conf. of the ACL (ACL 02),
pages 311?318, Philadelphia, PA, July.
Christoph Tillmann and Hermann Ney. 2003. Word
reordering and a dp beam search algorithm for
statistical machine translation. In Computational
Linguistics, volume 29(1), pages 97?133.
Christoph Tillmann. 2003. A projection extension
algorithm for statistical machine translation. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing.
Kristina Toutanova, H. Tolga Ilhan, and Christo-
pher D. Manning. 2002. Extensions to hmm-based
statistical word alignment models. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA, July 6-7.
S. Vogel, Hermann Ney, and C. Tillmann. 1996.
Hmm based word alignment in statistical machine
translation. In Proc. The 16th Int. Conf. on Com-
putational Lingustics, (Coling?96), pages 836?841,
Copenhagen, Denmark.
Taro Watanabe, Kenji Imamura, and Eiichiro
Sumita. 2002. Statistical machine translation
based on hierarchical phrases. In 9th International
Conference on Theoretical and Methodological Is-
sues, pages 188?198, Keihanna, Japan, March.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G.
Okuno. 2003. Chunk-based statistical transla-
tion. In In 41st Annual Meeting of the ACL (ACL
2003), pages 303?310, Sapporo, Japan.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. In Computational Linguistics, volume 23(3),
pages 377?403.
K. Yamada and Kevin. Knight. 2001. Syntax-based
statistical translation model. In Proceedings of the
Conference of the Association for Computational
Linguistics (ACL-2001).
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Pro-
ceedings of the Human Language Technology Con-
ference (HLT-NAACL)s, pages 257?264, Boston,
MA, May.
184
  
			TIPS: A Translingual Information Processing System
Y. Al-Onaizan, R. Florian, M. Franz, H. Hassan, Y. S. Lee, S. McCarley, K.
Papineni, S. Roukos, J. Sorensen, C. Tillmann, T. Ward, F. Xia
IBM T. J. Watson Research Center
Yorktown Heights
Abstract
Searching online information is
increasingly a daily activity for many
people. The multilinguality of online
content is also increasing (e.g. the
proportion of English web users, which
has been decreasing as a fraction the
increasing population of web users, dipped
below 50% in the summer of 2001). To
improve the ability of an English speaker
to search mutlilingual content, we built a
system that supports cross-lingual search
of an Arabic newswire collection and
provides on demand translation of Arabic
web pages into English. The cross-lingual
search engine supports a fast search
capability (sub-second response for typical
queries) and achieves state-of-the-art
performance in the high precision region
of the result list. The on demand statistical
machine translation uses the Direct
Translation model along with a novel
statistical Arabic Morphological Analyzer
to yield state-of-the-art translation quality.
The on demand SMT uses an efficient
dynamic programming decoder that
achieves reasonable speed for translating
web documents.
Overview
Morphologically rich languages like Arabic
(Beesley, K. 1996) present significant challenges
to many natural language processing applications
as the one described above because a word often
conveys complex meanings decomposable into
several morphemes (i.e. prefix, stem, suffix). By
segmenting words into morphemes, we can
improve the performance of natural language
systems including machine translation (Brown et
al. 1993) and information retrieval (Franz, M.
and McCarley, S. 2002). In this paper, we
present a cross-lingual English-Arabic search
engine combined with an on demand Arabic-
English statistical machine translation system
that relies on source language analysis for both
improved search and translation. We developed
novel statistical learning algorithms for
performing Arabic word segmentation (Lee, Y.
et al2003) into morphemes and morphological
source language (Arabic) analysis (Lee, Y. et al
2003b). These components improve both mono-
lingual (Arabic) search and cross-lingual
(English-Arabic) search and machine
translation. In addition, the system supports
either document translation or convolutional
models for cross-lingual search (Franz, M. and
McCarley, S. 2002).
The overall demonstration has the following
major components:
1. Mono-lingual search: uses Arabic word
segmentation and an okapi-like search
engine for document ranking.
2. Cross-lingual search: uses Arabic word
segmentation and morphological
analysis along with a statistical
morpheme translation matrix in a
convolutional model for document
ranking. The search can also use
document translation into English to
rank the Arabic documents. Both
approaches achieve similar precision in
the high precision region of retrieval.
The English query is also
morphologically analyzed to improve
performance.
3. OnDemand statistical machine
translation: this component uses both
analysis components along with a direct
channel translation model with a fast
dynamic programming decoder
(Tillmann, C. 2003). This system
                                                               Edmonton, May-June 2003
                                                              Demonstrations , pp. 1-2
                                                         Proceedings of HLT-NAACL 2003
achieves state-of-the-art Arabic-English
translation quality.
4. Arabic named entity detection and
translation: we have 31 categories of
Named Entities (Person, Organization,
etc.) that we detect and highlight in
Arabic text and provide the translation
of these entities into English. The
highlighted named entities help the user
to quickly assess the relevance of a
document.
All of the above functionality is available
through a web browser. We indexed the Arabic
AFP corpus about 330k documents for the
demonstration. The resulting search engine
supports sub-second query response. We also
provide an html detagging capability that allows
the translation of Arabic web pages while trying
to preserve the original layout as much as
possible in the on demand SMT component. The
Arabic Name Entity Tagger is currently run as an
offline process but we expect to have it online by
the demonstration time. We aslo include two
screen shots of the demonstration system.
Acknowledgments
This work was partially supported by the
Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position of
policy of the Government and no official
endorsement should be inferred.
References
Beesley, K. 1996. Arabic Finite-State
Morphological Analysis and Generation.
Proceedings of COLING-96, pages 89? 94.
Brown, P., Della Pietra, S., Della Pietra, V., and
Mercer, R. 1993. The mathematics of statistical
machine translation: Parameter Estimation.
Computational Linguistics, 19(2): 263?311.
Franz, M. and McCarley, S. 2002. Arabic
Information Retrieval at IBM. Proceedings
of TREC 2002, pages 402?405.
Lee, Y., Papineni, K., Roukos, S.,
Emam, O., and Hassan, H. 2003. Language
Model Based Arabic Word Segmentation.
Submitted for publication.
Lee, Y., Papineni, K., Roukos, S., Emam,
O., and Hassan, H. 2003b. Automatic
Induction of Morphological Analysis for
Statistical Machine Translation. Manuscript in
preparation.
Tillmann, C., 2003. Word Reordering and a
DP Beam Search Algorithm for Statistical
Machine Translation. Computational
Linguistics, 29(1): 97-133.
BLEU: a Method for Automatic Evaluation of Machine Translation
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598, USA
{papineni,roukos,toddward,weijing}@us.ibm.com
Abstract
Human evaluations of machine translation
are extensive but expensive. Human eval-
uations can take months to finish and in-
volve human labor that can not be reused.
We propose a method of automatic ma-
chine translation evaluation that is quick,
inexpensive, and language-independent,
that correlates highly with human evalu-
ation, and that has little marginal cost per
run. We present this method as an auto-
mated understudy to skilled human judges
which substitutes for them when there is
need for quick or frequent evaluations.1
1 Introduction
1.1 Rationale
Human evaluations of machine translation (MT)
weigh many aspects of translation, including ade-
quacy, fidelity , and fluency of the translation (Hovy,
1999; White and O?Connell, 1994). A compre-
hensive catalog of MT evaluation techniques and
their rich literature is given by Reeder (2001). For
the most part, these various human evaluation ap-
proaches are quite expensive (Hovy, 1999). More-
over, they can take weeks or months to finish. This is
a big problem because developers of machine trans-
lation systems need to monitor the effect of daily
changes to their systems in order to weed out bad
ideas from good ideas. We believe that MT progress
stems from evaluation and that there is a logjam of
fruitful research ideas waiting to be released from
1So we call our method the bilingual evaluation understudy,
BLEU.
the evaluation bottleneck. Developers would bene-
fit from an inexpensive automatic evaluation that is
quick, language-independent, and correlates highly
with human evaluation. We propose such an evalua-
tion method in this paper.
1.2 Viewpoint
How does one measure translation performance?
The closer a machine translation is to a professional
human translation, the better it is. This is the cen-
tral idea behind our proposal. To judge the quality
of a machine translation, one measures its closeness
to one or more reference human translations accord-
ing to a numerical metric. Thus, our MT evaluation
system requires two ingredients:
1. a numerical ?translation closeness? metric
2. a corpus of good quality human reference trans-
lations
We fashion our closeness metric after the highly suc-
cessful word error rate metric used by the speech
recognition community, appropriately modified for
multiple reference translations and allowing for le-
gitimate differences in word choice and word or-
der. The main idea is to use a weighted average of
variable length phrase matches against the reference
translations. This view gives rise to a family of met-
rics using various weighting schemes. We have se-
lected a promising baseline metric from this family.
In Section 2, we describe the baseline metric in
detail. In Section 3, we evaluate the performance of
BLEU. In Section 4, we describe a human evaluation
experiment. In Section 5, we compare our baseline
metric performance with human evaluations.
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 311-318.
                         Proceedings of the 40th Annual Meeting of the Association for
2 The Baseline BLEU Metric
Typically, there are many ?perfect? translations of a
given source sentence. These translations may vary
in word choice or in word order even when they use
the same words. And yet humans can clearly dis-
tinguish a good translation from a bad one. For ex-
ample, consider these two candidate translations of
a Chinese source sentence:
Example 1.
Candidate 1: It is a guide to action which
ensures that the military always obeys
the commands of the party.
Candidate 2: It is to insure the troops
forever hearing the activity guidebook
that party direct.
Although they appear to be on the same subject, they
differ markedly in quality. For comparison, we pro-
vide three reference human translations of the same
sentence below.
Reference 1: It is a guide to action that
ensures that the military will forever
heed Party commands.
Reference 2: It is the guiding principle
which guarantees the military forces
always being under the command of the
Party.
Reference 3: It is the practical guide for
the army always to heed the directions
of the party.
It is clear that the good translation, Candidate 1,
shares many words and phrases with these three ref-
erence translations, while Candidate 2 does not. We
will shortly quantify this notion of sharing in Sec-
tion 2.1. But first observe that Candidate 1 shares
"It is a guide to action" with Reference 1,
"which" with Reference 2, "ensures that the
military" with Reference 1, "always" with Ref-
erences 2 and 3, "commands" with Reference 1, and
finally "of the party" with Reference 2 (all ig-
noring capitalization). In contrast, Candidate 2 ex-
hibits far fewer matches, and their extent is less.
It is clear that a program can rank Candidate 1
higher than Candidate 2 simply by comparing n-
gram matches between each candidate translation
and the reference translations. Experiments over
large collections of translations presented in Section
5 show that this ranking ability is a general phe-
nomenon, and not an artifact of a few toy examples.
The primary programming task for a BLEU imple-
mentor is to compare n-grams of the candidate with
the n-grams of the reference translation and count
the number of matches. These matches are position-
independent. The more the matches, the better the
candidate translation is. For simplicity, we first fo-
cus on computing unigram matches.
2.1 Modified n-gram precision
The cornerstone of our metric is the familiar pre-
cision measure. To compute precision, one simply
counts up the number of candidate translation words
(unigrams) which occur in any reference translation
and then divides by the total number of words in
the candidate translation. Unfortunately, MT sys-
tems can overgenerate ?reasonable? words, result-
ing in improbable, but high-precision, translations
like that of example 2 below. Intuitively the prob-
lem is clear: a reference word should be considered
exhausted after a matching candidate word is iden-
tified. We formalize this intuition as the modified
unigram precision. To compute this, one first counts
the maximum number of times a word occurs in any
single reference translation. Next, one clips the to-
tal count of each candidate word by its maximum
reference count,2adds these clipped counts up, and
divides by the total (unclipped) number of candidate
words.
Example 2.
Candidate: the the the the the the the.
Reference 1: The cat is on the mat.
Reference 2: There is a cat on the mat.
Modified Unigram Precision = 2/7.3
In Example 1, Candidate 1 achieves a modified
unigram precision of 17/18; whereas Candidate
2 achieves a modified unigram precision of 8/14.
Similarly, the modified unigram precision in Exam-
ple 2 is 2/7, even though its standard unigram pre-
cision is 7/7.
2Countclip = min(Count,Max Re f Count). In other words,
one truncates each word?s count, if necessary, to not exceed the
largest count observed in any single reference for that word.
3As a guide to the eye, we have underlined the important
words for computing modified precision.
Modified n-gram precision is computed similarly
for any n: all candidate n-gram counts and their
corresponding maximum reference counts are col-
lected. The candidate counts are clipped by their
corresponding reference maximum value, summed,
and divided by the total number of candidate n-
grams. In Example 1, Candidate 1 achieves a mod-
ified bigram precision of 10/17, whereas the lower
quality Candidate 2 achieves a modified bigram pre-
cision of 1/13. In Example 2, the (implausible) can-
didate achieves a modified bigram precision of 0.
This sort of modified n-gram precision scoring cap-
tures two aspects of translation: adequacy and flu-
ency. A translation using the same words (1-grams)
as in the references tends to satisfy adequacy. The
longer n-gram matches account for fluency. 4
2.1.1 Modified n-gram precision on blocks of
text
How do we compute modified n-gram precision
on a multi-sentence test set? Although one typically
evaluates MT systems on a corpus of entire docu-
ments, our basic unit of evaluation is the sentence.
A source sentence may translate to many target sen-
tences, in which case we abuse terminology and re-
fer to the corresponding target sentences as a ?sen-
tence.? We first compute the n-gram matches sen-
tence by sentence. Next, we add the clipped n-gram
counts for all the candidate sentences and divide by
the number of candidate n-grams in the test corpus
to compute a modified precision score, pn, for the
entire test corpus.
pn =
?
C?{Candidates}
?
n-gram?C
Countclip(n-gram)
?
C ??{Candidates}
?
n-gram??C ?
Count(n-gram?) .
4BLEU only needs to match human judgment when averaged
over a test corpus; scores on individual sentences will often vary
from human judgments. For example, a system which produces
the fluent phrase ?East Asian economy? is penalized heavily on
the longer n-gram precisions if all the references happen to read
?economy of East Asia.? The key to BLEU?s success is that
all systems are treated similarly and multiple human translators
with different styles are used, so this effect cancels out in com-
parisons between systems.
2.1.2 Ranking systems using only modified
n-gram precision
To verify that modified n-gram precision distin-
guishes between very good translations and bad
translations, we computed the modified precision
numbers on the output of a (good) human transla-
tor and a standard (poor) machine translation system
using 4 reference translations for each of 127 source
sentences. The average precision results are shown
in Figure 1.
Figure 1: Distinguishing Human from Machine







	



   
Language Model Based Arabic Word Segmentation 
 
Young-Suk Lee     Kishore Papineni      Salim Roukos 
IBM T. J. Watson Research Center 
Yorktown Heights, NY 10598 
 
Ossama Emam    Hany Hassan 
IBM Cairo Technology Development Center 
P.O.Box 166, El-Ahram, Giza, Egypt
  
Abstract 
 
We approximate Arabic?s rich 
morphology by a model that a word 
consists of a sequence of morphemes in 
the pattern prefix*-stem-suffix* (* 
denotes zero or more occurrences of a 
morpheme). Our method is seeded by a 
small manually segmented Arabic corpus 
and uses it to bootstrap an unsupervised 
algorithm to build the Arabic word 
segmenter from a large unsegmented 
Arabic corpus. The algorithm uses a 
trigram language model to determine the 
most probable morpheme sequence for a 
given input. The language model is 
initially estimated from a small manually 
segmented corpus of about 110,000 
words. To improve the segmentation 
accuracy, we use an unsupervised 
algorithm for automatically acquiring 
new stems from a 155 million word 
unsegmented corpus, and re-estimate the 
model parameters with the expanded 
vocabulary and training corpus. The 
resulting Arabic word segmentation 
system achieves around 97% exact match 
accuracy on a test corpus containing 
28,449 word tokens. We believe this is a 
state-of-the-art performance and the 
algorithm can be used for many highly 
inflected languages provided that one can 
create a small manually segmented 
corpus of the language of interest.  
 
 
 
1   Introduction 
 
Morphologically rich languages like       
Arabic present significant challenges to many 
natural language processing applications 
because a word often conveys complex 
meanings decomposable into several 
morphemes (i.e. prefix, stem, suffix).   By 
segmenting words into morphemes, we can 
improve the performance of natural language 
systems including machine translation (Brown 
et al 1993) and information retrieval (Franz, 
M. and McCarley, S. 2002). In this paper, we 
present a general word segmentation algorithm 
for handling inflectional morphology capable 
of segmenting a word into a prefix*-stem-
suffix* sequence, using a small manually 
segmented corpus and a table of 
prefixes/suffixes of the language. We do not 
address Arabic infix morphology where many 
stems correspond to the same root with various 
infix variations; we treat all the stems of a 
common root as separate atomic units. The use 
of a stem as a morpheme (unit of meaning) is 
better suited than the use of a root for the 
applications we are considering in information 
retrieval and machine translation (e.g. different 
stems of the same root translate into different 
English words.) Examples of Arabic words and 
their segmentation into prefix*-stem-suffix* are 
given in Table 1, where '#' indicates a 
morpheme being a prefix, and '+' a suffix.1 As  
                                                          
1 Arabic is presented in both native and Buckwalter 
transliterated Arabic whenever possible. All native 
Arabic is to be read from right-to-left, and transliterated 
Arabic is to be read from left-to-right. The convention of 
shown in Table 1, a word may include multiple 
prefixes, as in   ???? (l: for, Al: the),  or multiple 
suffixes, as in   ????? (t: feminine singular, h: his).  
A word may also consist only of a stem, as in 
 ?????  (AlY, to/towards). 
  The algorithm implementation involves (i) 
language model training on a morpheme-
segmented corpus, (ii) segmentation of input 
text into a sequence of morphemes using the 
language model parameters, and (iii) 
unsupervised acquisition of new stems from a 
large unsegmented corpus. The only linguistic 
resources required include  a small manually 
segmented corpus ranging from 20,000 words 
to 100,000 words, a table of prefixes and 
suffixes of the language and  a large 
unsegmented corpus.   
  In Section 2, we discuss related work. In 
Section 3, we describe the segmentation 
algorithm.  In Section 4, we discuss the  
unsupervised algorithm for new stem 
acquisition. In Section 5, we present 
experimental results. In Section 6, we 
summarize the paper. 
 
2   Related Work 
 
Our work adopts major components of the 
algorithm from (Luo & Roukos 1996): 
language model (LM) parameter estimation 
from a segmented corpus and input 
segmentation on the basis of LM probabilities.  
However, our work diverges from their work 
in two crucial respects: (i) new technique of 
computing all possible segmentations of a 
word into prefix*-stem-suffix* for decoding, 
and  (ii) unsupervised algorithm for new stem 
acquisition based on a stem candidate's 
similarity to stems occurring in the training 
corpus. 
  (Darwish 2002) presents a  supervised 
technique which identifies the root of an 
Arabic word by stripping away the prefix and 
the suffix of the word on the basis of manually 
acquired dictionary of word-root pairs and the 
likelihood that a prefix and a suffix would 
occur with the template from which the root is 
derived. He reports 92.7% segmentation 
accuracy on a 9,606 word evaluation corpus.  
His technique pre-supposes at most one prefix 
and one suffix per stem regardless of the actual 
number and meanings of prefixes/suffixes 
associated with the stem.  (Beesley 1996)  
presents a finite-state morphological analyzer 
for Arabic, which displays the root, pattern, 
and prefixes/suffixes. The analyses are based 
on manually acquired lexicons and rules.  
Although his analyzer is comprehensive in the 
types of knowledge it presents, it has been 
criticized for their extensive development time 
and lack of robustness, cf. (Darwish 2002). 
                                                                                    
marking a prefix with '#" and a suffix with '+' will be 
adopted throughout the paper. 
  (Yarowsky and Wicentowsky 2000) 
presents a minimally supervised morphological 
analysis with a  performance of over 99.2% 
accuracy for the 3,888 past-tense test cases in 
English. The core algorithm lies in the 
estimation of a probabilistic alignment 
between inflected forms and root forms. The 
probability estimation is based on the lemma 
alignment by frequency ratio similarity among 
different inflectional forms derived from the 
same lemma, given a table of inflectional 
parts-of-speech, a list of the canonical suffixes 
for each part of speech, and a list of the 
candidate noun, verb and adjective roots of the 
language.  Their algorithm does not handle 
multiple affixes per word. 
  (Goldsmith 2000) presents an unsupervised 
technique based on the expectation-
maximization algorithm and minimum 
description length to segment exactly one 
suffix per word, resulting in an F-score of 81.8 
for suffix identification in English according to 
(Schone and Jurafsky 2001). (Schone and 
Jurafsky 2001) proposes an unsupervised 
algorithm capable of automatically inducing 
the morphology of inflectional languages using 
only text corpora. Their algorithm combines 
cues from orthography, semantics, and 
contextual information to induce 
morphological relationships in German, Dutch, 
and English, among others. They report F-
scores between 85 and 93 for suffix analyses 
and between 78 and 85 for circumfix analyses 
in these languages. Although their algorithm 
captures prefix-suffix combinations or 
circumfixes, it does not handle the multiple 
affixes per word we observe in Arabic.
 2
                Words            Prefixes                 Stems             Suffixes 
    Arabic    Translit.   Arabic  Translit.    Arabic    Translit.   Arabic   Translit. 
 ????????????? ?   AlwlAyAt  #??    Al#       ????       wlAy      ?? +    +At 
      ???????????    HyAth           ??????     HyA  ?  +? +    +t +h 
 ?????????????    llHSwl  #?#  ??     l# Al#    ??????     HSwl   
         ?????        AlY           ?????      AlY   
 Table 1  Segmentation of Arabic Words into Prefix*-Stem-Suffix* 
 
3  Morpheme Segmentation 
 
3.1 Trigram Language Model 
 
Given an Arabic sentence, we use a trigram 
language model on morphemes to segment it 
into a sequence of morphemes {m1, m2, ?,mn}. 
The input to the morpheme segmenter is a 
sequence of Arabic tokens ? we use a 
tokenizer that looks only at white space and 
other punctuation, e.g. quotation marks, 
parentheses, period, comma, etc.  A sample of 
a manually segmented corpus is given below2. 
Here multiple occurrences of prefixes and 
suffixes per word are marked with an 
underline. 
 
???? # ??? ??????? ???? ?? ?? ??# ?
??? # ???? ?? # ? ??+??? ?? ???? # ??
? ?????? ??? ?+???? ??? ???? # ?? #
 ??? ???+? +? ???? +???? ?? ???  #
??? #?# ??? # ????? ?# ?????? ?? ?? 
?? ??+???? # ? ??????# ??? ???? ? #
 ? ??? ?? ???? ???? ?????? +????? .
????? ?? ?????? #  ?? ???? ??#?# ?# ?
??????? ??????? ????? ???? # ??
??? # ???? ??? ??# ??????? ?? ??
 ?? ?+?? + ??? ???? ??? #? # ????? 
 ?? ?????????+???? ???? 
 
w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# 
Awl fy jA}z +p Al# nmsA Al# EAm Al# 
mADy Ely syAr +p fyrAry $Er b# AlAm fy 
bTn +h ADTr +t +h Aly Al# AnsHAb mn Al#  
tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# 
fHwS +At Al# Drwry +p Hsb mA A$Ar fryq  
 
                                                          
2 A manually segmented Arabic corpus containing about 
140K word tokens has been provided by LDC 
(http://www.ldc.upenn.edu). We divided this corpus into 
training and the development test sets as described in 
Section 5. 
 
 
jAgwAr. w# s# y# Hl sA}q Al# tjArb fy 
jAgwAr Al# brAzyly lwsyAnw bwrty mkAn 
AyrfAyn fy Al# sbAq gdA Al# AHd Al*y s# 
y# kwn Awly xTw +At +h fy EAlm sbAq +At 
AlfwrmwlA 
 
Many instances of prefixes and suffixes in 
Arabic are meaning bearing and correspond to 
a word in English such as pronouns and 
prepositions.  Therefore, we choose a 
segmentation into multiple prefixes and 
suffixes. Segmentation into one prefix  and one 
suffix per word, cf. (Darwish 2002), is not very 
useful for applications like statistical machine 
translation, (Brown et al 1993), for which an 
accurate word-to-word alignment between the 
source and the target languages is critical for 
high quality translations. 
  The trigram language model probabilities 
of morpheme sequences, p(mi|mi-1, mi-2), are 
estimated from the morpheme-segmented 
corpus. At token boundaries, the morphemes 
from previous tokens constitute the histories of 
the current morpheme in the trigram language 
model.  The trigram model is smoothed using 
deleted interpolation with the bigram and 
unigram models, (Jelinek 1997), as in (1): 
 
(1) p(m3 | m1 ,m2) =  ?3 p(m3 |m1 ,m2) + ?2 
p(m3 |m2) + ?3 p(m3), where ?1+?2 +?3 = 1. 
 
  A small morpheme-segmented corpus 
results in a relatively high out of vocabulary 
rate for the stems. We describe below an 
unsupervised acquisition of new stems from a 
large unsegmented Arabic corpus.  However, 
we first describe the segmentation algorithm.   
 
3.2  Decoder for Morpheme Segmentation 
 
 3
We take the unit of decoding to be a sentence 
that has been tokenized using white space and 
punctuation.  The task of a decoder is to find 
the morpheme sequence which maximizes the 
trigram probability of the input sentence, as in 
(2): 
 
(2)  SEGMENTATIONbest = Argmax IIi=1, N 
p(mi|mi-1mi-2), N = number of morphemes in 
the input. 
 
Search algorithm for (2) is informally 
described for each word token as follows: 
 
Step 1: Compute all possible segmentations of 
the token  (to be elaborated in 3.2.1). 
Step 2: Compute the trigram language model 
score of each segmentation.  For some 
segmentations of a token, the stem may be an 
out of vocabulary item. In that case, we use an 
?UNKNOWN? class in the trigram language 
model with the model probability given by 
p(UNKNOWN|mi-1, mi-2) * UNK_Fraction, where 
UNK_Fraction is 1e-9 determined on empirical 
grounds. This allows us to segment new words 
with a high accuracy even with a relatively 
high number of unknown stems in the 
language model vocabulary, cf. experimental 
results in Tables 5 & 6. 
Step 3: Keep the top N highest scored 
segmentations. 
 
3.2.1  Possible Segmentations of  a Word 
 
Possible segmentations of a word token are 
restricted to those derivable from a table of 
prefixes and suffixes of the language for 
decoder speed-up and improved accuracy.   
  Table 2 shows examples of atomic (e.g. ??, 
??) and multi-component (e.g.  ??????     ,???????) 
prefixes and suffixes, along with their 
component morphemes in native Arabic.3 
 
                                                          
3 We have acquired the prefix/suffix table from a 110K 
word manually segmented LDC corpus (51 prefixes & 72 
suffixes) and from IBM-Egypt (additional 14 prefixes & 
122 suffixes). The performance improvement by the 
additional prefix/suffix list ranges from 0.07% to 0.54% 
according to the manually segmented training corpus 
size. The smaller the manually segmented corpus size is, 
the bigger the performance improvement by adding 
additional prefix/suffix list is. 
         Prefixes          Suffixes 
      ??          ??       ??# ??+ 
    ??????        ?#  ??# ?????+   ???     ??+ 
 ???????     ?#  ?#   ??# ?????+?? + ??  
          Table 2  Prefix/Suffix Table 
 
Each token is assumed to have the structure 
prefix*-stem-suffix*, and is compared against 
the prefix/suffix table for segmentation. Given 
a word token, (i) identify all of the matching 
prefixes and suffixes from the table, (ii) further 
segment each matching prefix/suffix at each 
character position, and (iii) enumerate all 
prefix*-stem-suffix* sequences derivable from 
(i) and (ii).  
  Table 3 shows all of its possible 
segmentations of the token ???????  
(wAkrrhA; 'and I repeat it'),4 where ? indicates 
the null prefix/suffix and the Seg Score is the 
language model probabilities of each 
segmentation S1 ... S12. For this token, there 
are two matching prefixes #?(w#) and 
#??(wA#) from the prefix table, and two 
matching suffixes ?+(+A) and ??+(+hA)  
from the suffix table. S1, S2, & S3 are the 
segmentations given the null prefix ? and 
suffixes ?, +A, +hA. S4, S5, & S6 are the 
segmentations given the prefix w# and suffixes 
?, +A, +hA. S7, S8, & S9 are the 
segmentations given the prefix wA# and 
suffixes ?, +A, +hA. S10, S11, & S12 are the 
segmentations given the prefix sequence w# 
A# derived from the prefix wA# and  suffixes 
?, +A, +hA. As illustrated by S12, derivation 
of sub-segmentations of the matching 
prefixes/suffixes enables the system to identify 
possible segmentations which would have been 
missed otherwise. In this case, segmentation 
including the derived prefix sequence               
??+??? # ?# ? (w# A# krr +hA) happens to 
be the correct one.  
 
3.2.2. Prefix-Suffix Filter 
 
While the number of possible segmentations is 
maximized by sub-segmenting matching 
                                                          
4 A sentence in which the token occurs is as follows:  ?????
??????? ???????? ???? ?? ????? ????? ????? ?? ???????? ??????? 
(qlthA wAkrrhA fAlm$klp lyst fy AlfnT AlxAm wAnmA fy 
Alm$tqAt AlnfTyp.) 
 4
prefixes and suffixes, some of illegitimate sub-
segmentations are filtered out on the basis of 
the knowledge specific to the manually 
segmented corpus. For instance, sub-
segmentation of the suffix hA into +h +A is 
ruled out because there is no suffix sequence 
+h +A in the training corpus. Likewise, sub-
segmentation of the prefix Al into A# l# is 
filtered out. Filtering out improbable 
prefix/suffix sequences improves the 
segmentation accuracy, as shown in Table 5. 
 
 Prefix Stem Suffix Seg Scores 
S1 ? wAkrrhA ? 2.6071e-05 
S2 ? wAkrrh +A 1.36561e-06 
S3 ? wAkrr +hA 9.45933e-07 
S4 w# AkrrhA ? 2.72648e-06 
S5 w# Akrrh +A 5.64843e-07 
S6 w# Akrr +hA 4.52229e-05 
S7 wA# krrhA ? 7.58256e-10 
S8 wA# krrh +A 5.09988e-11 
S9 wA# krr +hA 1.91774e-08 
S10 w# A# krrhA ? 7.69038e-07 
S11 w# A# krrh +A 1.82663e-07 
S12 w# A# krr +hA 0.000944511 
Table 3 Possible Segmentations of  
??????? (wAkrrhA) 
 
4  Unsupervised Acquisition  of  New  
Stems 
 
Once the seed segmenter is developed on the 
basis of a manually segmented corpus,  the 
performance may be improved by iteratively 
expanding the stem vocabulary  and retraining 
the language model on a large automatically 
segmented Arabic corpus.  
  Given a small manually segmented corpus 
and a large unsegmented corpus, segmenter 
development proceeds as follows. 
 
Initialization: Develop the seed segmenter 
Segmenter0 trained on the manually segmented 
corpus Corpus0, using the language model 
vocabulary, Vocab0, acquired from Corpus0.  
Iteration: For i = 1 to N, N = the number of 
partitions of the unsegmented corpus 
 i. Use Segmenteri-1 to segment Corpusi. 
 ii.  Acquire new stems from the newly 
segmented Corpusi. Add the new stems to 
Vocabi-1, creating an expanded vocabulary 
Vocabi.  
 iii. Develop Segmenteri trained on Corpus0 
through Corpusi with Vocabi.   
Optimal Performance Identification:  
Identify the Corpusi and Vocabi, which result 
in the best performance, i.e. system training 
with Corpusi+1 and Vocabi+1 does not improve 
the performance any more. 
  Unsupervised acquisition of new stems 
from an automatically segmented new corpus 
is a three-step process: (i)  select new stem 
candidates on the basis of a frequency 
threshold, (ii) filter out new stem candidates  
containing a sub-string with a high likelihood 
of being a prefix, suffix, or prefix-suffix. The 
likelihood of a sub-string being a prefix, suffix, 
and prefix-suffix of a token is computed as in  
(5) to (7), (iii) further filter out new stem 
candidates on the basis of contextual 
information, as in (8). 
 
(5)  Pscore = number of tokens with prefix P / 
number of tokens starting with sub-string P 
(6)  Sscore = number of tokens with suffix S / 
number of tokens ending with sub-string S 
(7)  PSscore = number of tokens with prefix P 
and suffix S / number of tokens starting with 
sub-string P and ending with  sub-string S 
 
Stem candidates containing a sub-string with a 
high prefix, suffix, or prefix-suffix likelihood 
are filtered out. Example sub-strings with the 
prefix, suffix, prefix-suffix likelihood 0.85 or 
higher in a 110K word manually segmented 
corpus are given in Table 4. If a token starts 
with the sub-string ???  (sn), and end with  ???  
(hA), the sub-string's likelihood of being the 
prefix-suffix of the token is 1.  If a token starts 
with the sub-string  ????  (ll), the sub-string's 
likelihood of being the prefix of the token is 
0.945, etc. 
 
        Arabic Transliteration      Score 
 ??? +  stem # ???     sn# stem+hA      1.0 
     ?+ stem # ?????  Al# stem+p      0.984        
         stem # ????   ll# stem      0.945 
  ??+  stem         stem+At      0.889 
    Table 4 Prefix/Suffix Likelihood Score 
 
 5
(8) Contextual Filter: (i) Filter out stems co-
occurring with prefixes/suffixes not present in 
the training corpus. (ii) Filter out stems whose 
prefix/suffix distributions are highly 
disproportionate to those seen in the training 
corpus.  
   According to (8), if a stem is followed by 
a potential suffix +m, not present in the 
training corpus, then it is filtered out as an 
illegitimate stem. In addition, if a stem is 
preceded by a prefix and/or followed by a 
suffix with a significantly higher proportion 
than that observed in the training corpus, it is 
filtered out. For instance, the probability for 
the suffix +A to follow a stem is less than 50% 
in the training corpus regardless of the stem 
properties, and therefore, if a candidate stem is 
followed by +A with the probability of over 
70%, e.g. mAnyl +A, then it is filtered out as 
an illegitimate stem. 
 
5  Performance Evaluations 
 
We present experimental results illustrating the 
impact of three factors on segmentation error 
rate: (i) the base algorithm, i.e. language model 
training and decoding, (ii) language model 
vocabulary and training corpus size, and (iii) 
manually segmented training corpus size.  
Segmentation error rate is defined in (9). 
 
(9)  (number of incorrectly segmented tokens /  
       total number of tokens)  x  100 
 
  Evaluations have been performed on a 
development test corpus containing 28,449 
word tokens.  The test set is extracted from 
20001115_AFP_ARB.0060.xml.txt through 
20001115_AFP_ARB.0236.xml.txt of the 
LDC Arabic Treebank: Part 1 v 2.0 Corpus. 
Impact of the core algorithm and the 
unsupervised stem acquisition has been 
measured on segmenters developed from 4 
different sizes of manually segmented seed 
corpora: 10K, 20K, 40K, and 110K words.    
  The experimental results are shown in 
Table 5. The baseline performances are 
obtained by assigning each token the most 
frequently occurring segmentation in the 
manually segmented training corpus. The 
column headed by '3-gram LM' indicates the 
impact of the segmenter using only trigram 
language model probabilities for decoding. 
Regardless of the manually segmented training 
corpus size, use of  trigram language model 
probabilities reduces the word error rate of the 
corresponding baseline by approximately 50%. 
The column headed by '3-gram LM + PS 
Filter' indicates the impact of the core 
algorithm plus Prefix-Suffix Filter discussed in 
Section 3.2.2. Prefix-Suffix Filter reduces the 
word error rate ranging from 7.4% for the 
smallest (10K word) manually segmented 
corpus to 21.8% for the largest (110K word) 
manually segmented corpus ?- around 1% 
absolute reduction for all segmenters. The 
column headed by '3-gram LM + PS Filter + 
New Stems' shows the impact of unsupervised 
stem acquisition from a 155 million word 
Arabic corpus.  Word error rate reduction due 
to the unsupervised stem acquisition is 38% for 
the segmenter developed from the 10K word 
manually segmented corpus and 32% for the 
segmenter developed from 110K word 
manually segmented corpus. 
  Language model vocabulary size (LM VOC 
Size) and the unknown stem ratio (OOV ratio) 
of various segmenters is given in Table 6. For 
unsupervised stem acquisition, we have set the 
frequency threshold at 10 for every 10-15 
million word corpus, i.e. any new morphemes 
occurring more than 10 times in a 10-15 
million word corpus are considered to be new 
stem candidates. Prefix, suffix, prefix-suffix 
likelihood score to further filter out illegitimate 
stem candidates was set at 0.5 for the 
segmenters developed from 10K, 20K, and 
40K manually segmented corpora, whereas it 
was set at 0.85 for the segmenters developed 
from a 110K manually segmented corpus.  
Both the frequency threshold and the optimal 
prefix, suffix, prefix-suffix likelihood scores 
were determined on empirical grounds. 
Contextual Filter stated in (8) has been applied 
only to the segmenter developed from 110K 
manually segmented training corpus.5 
Comparison of Tables 5 and 6 indicates a high 
correlation between the segmentation error rate 
and the unknown stem ratio.  
                                                          
5 Without the Contextual Filter, the  error rate of the 
same segmenter is 3.1%. 
 6
   
 
Manually Segmented 
Training Corpus Size 
      Baseline  3-gram LM  3-gram LM +  
PS Filter 
3-gram LM + PS 
Filter + New Stems 
        10K Words    26.0%        14.7%            13.6%          8.5% 
        20K Words       19.7%        9.1%            8.0%          5.9% 
        40K Words        14.3%        7.6%            6.5%          5.1% 
      110K Words        11.0%        5.5%            4.3%           2.9% 
Table 5 Impact of Core Algorithm and LM Vocabulary Size on Segmentation Error Rate 
 
                       3-gram LM  3-gram LM + PS Filter + New Stems Manually Segmented 
Training Corpus Size     LM VOC Size      OOV Ratio    LM VOC Size      OOV Ratio 
         10K Words           2,496          20.4%          22,964           7.8% 
         20K Words           4,111          11.4%          25,237           5.3% 
         40K Words           5,531            9.0%          21,156           4.7% 
       110K Words           8,196            5.8%          25,306           1.9% 
             Table 6 Language Model Vocabulary Size and Out of Vocabulary Ratio 
  
                                  3-gram LM + PS Filter + New Stems Manually Segmented 
Training Corpus Size   Unknown Stem          Alywm     Other Errors  Total # of Errors 
         10 K Words    1,844  (76.9%)        98 (4.1%)     455 (19.0%)          2,397 
         20 K Words    1,174  (71.1%)        82 (5.0%)     395 (23.9%)          1,651 
         40 K Words    1,005  (69.9%)        81 (5.6%)     351 (24.4%)          1,437 
       110 K Words       333  (39.6%)        82 (9.8%)     426 (50.7%)             841 
Table 7 Segmentation Error Analyses
  
Table 7 gives the error analyses of four 
segmenters according to three factors: (i) 
errors due to unknown stems, (ii) errors 
involving  ?????????? (Alywm), and (iii) errors due to 
other factors. Interestingly, the segmenter 
developed from a 110K manually segmented 
corpus has the lowest percentage of ?unknown 
stem? errors at 39.6% indicating that our 
unsupervised acquisition of new stems is 
working well, as well as suggesting to use a 
larger unsegmented corpus for unsupervised 
stem acquisition.  
    ?????????? (Alywm) should be segmented 
differently depending on its part-of-speech to 
capture the semantic ambiguities. If it is an 
adverb or a proper noun, it is segmented as 
 ?????????? 'today/Al-Youm', whereas if it is a noun, 
it is segmented as ?? # ??????   'the day.'  Proper 
segmentation of   ?????????? primarily requires its 
part-of-speech information, and cannot be 
easily handled by morpheme trigram models 
alone. 
  Other errors include over-segmentation of  
foreign words such as  ???????????????  (bwtyn) as  ?# 
 ??????????  and  ?????????????  (lytr)  'litre' as ? # ?# ????? .  
These errors are attributed to the segmentation 
ambiguities of these tokens:  ??????????????? is 
ambiguous between ' ??????????????? (Putin)' and '?# 
 ?????????? (by aorta)'.   ?????????????  is ambiguous 
between ' ????????????? (litre)' and ' ? # ?# ?????  (for him 
to harm)'. These errors may also be corrected 
by incorporating part-of-speech information 
for disambiguation. 
  To address the segmentation ambiguity 
problem, as illustrated by ' ??????????????? (Putin)' vs. 
' ? # ??????????  (by aorta)', we have developed a 
joint model for segmentation and part-of-
speech tagging for which the best 
segmentation of an input sentence is obtained 
according to the formula (10), where ti is the 
part-of-speech of morpheme mi, and N is the 
number of morphemes in the input sentence. 
 
(10) SEGMENTATIONbest = Argmax ?i=1,N  
p(mi|mi-1 mi-2) p(ti|ti-1 ti-2) p(mi|ti) 
 
By using the joint model, the segmentation 
word error rate of the best performing 
segmenter has been reduced by about 10% 
 7
from 2.9% (cf. the last column of Table 5) to 
2.6%. 
   
5  Summary and Future Work 
 
We have presented a robust word segmentation 
algorithm which segments a word into a 
prefix*-stem-suffix* sequence, along with 
experimental results. Our Arabic word 
segmentation system implementing the 
algorithm achieves around 97% segmentation 
accuracy on a development test corpus 
containing 28,449 word tokens. Since the 
algorithm can identify any number of prefixes 
and suffixes of a given token, it is generally 
applicable to various language families 
including agglutinative languages (Korean, 
Turkish, Finnish), highly inflected languages 
(Russian, Czech) as well as semitic languages 
(Arabic, Hebrew). 
   Our future work includes (i) application 
of the current technique to other highly 
inflected languages, (ii) application of the 
unsupervised stem acquisition technique on 
about 1 billion word unsegmented Arabic 
corpus, and (iii) adoption of a novel 
morphological analysis technique to handle 
irregular morphology, as realized in Arabic 
broken plurals  ????????? (ktAb) 'book' vs.  ???????? 
(ktb) 'books'. 
 
Acknowledgment 
 
This work was partially supported by the 
Defense Advanced Research Projects Agency 
and monitored by SPAWAR under contract No. 
N66001-99-2-8916. The views and findings 
contained in this material are those of the 
authors and do not necessarily reflect the 
position of policy of the Government and no 
official endorsement should be inferred. We 
would like to thank Martin Franz for discussions 
on language model building, and his help with 
the use of ViaVoice language model toolkit. 
 
References 
 
Beesley, K. 1996. Arabic Finite-State 
 Morphological Analysis and Generation. 
 Proceedings of COLING-96, pages 89?  94. 
Brown, P., Della Pietra, S., Della Pietra, V., 
 and Mercer, R. 1993. The mathematics  of 
 statistical machine translation:  Parameter 
 Estimation. Computational  Linguistics, 
 19(2): 263?311. 
Darwish, K. 2002. Building a Shallow  Arabic 
 Morphological Analyzer in  One  Day. 
 Proceedings of the  Workshop on 
 Computational  Approaches to Semitic 
 Languages,  pages 47?54.  
Franz, M. and McCarley, S. 2002. Arabic 
 Information Retrieval at IBM.  Proceedings 
 of TREC 2002, pages 402? 405. 
Goldsmith, J. 2000. Unsupervised  learning 
 of  the morphology of a natural  language.   
 Computational Linguistics, 27(1). 
Jelinek, F. 1997. Statistical Methods for 
 Speech Recognition. The MIT Press. 
Luo, X. and Roukos, S. 1996. An Iterative 
 Algorithm to Build Chinese Language 
 Models. Proceedings of ACL-96, pages 
 139?143. 
Schone, P. and Jurafsky, D. 2001. 
 Knowledge-Free Induction of  Inflectional 
 Morphologies. Proceedings  of  North 
 American Chapter of  Association for 
 Computational  Linguistics. 
Yarowsky, D. and Wicentowski, R. 2000. 
 Minimally supervised morphological 
 analysis by multimodal alignment. 
 Proceedings of ACL-2000, pages 207? 216. 
Yarowsky, D, Ngai G. and Wicentowski, R. 
 2001. Inducting Multilingual Text  Analysis 
 Tools via Robust Projection  across Aligned 
 Corpora. Proceedings of  HLT 2001, pages 
 161?168. 
 
 
 
 8
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 529?536,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Distortion Models For Statistical Machine Translation
Yaser Al-Onaizan and Kishore Papineni
IBM T.J. Watson Research Center
1101 Kitchawan Road
Yorktown Heights, NY 10598, USA
{onaizan, papineni}@us.ibm.com
Abstract
In this paper, we argue that n-gram lan-
guage models are not sufficient to address
word reordering required for Machine Trans-
lation. We propose a new distortion model
that can be used with existing phrase-based
SMT decoders to address those n-gram lan-
guage model limitations. We present empirical
results in Arabic to English Machine Transla-
tion that show statistically significant improve-
ments when our proposed model is used. We
also propose a novel metric to measure word
order similarity (or difference) between any
pair of languages based on word alignments.
1 Introduction
A language model is a statistical model that gives
a probability distribution over possible sequences of
words. It computes the probability of producing a given
word w1 given all the words that precede it in the sen-
tence. An n-gram language model is an n-th order
Markov model where the probability of generating a
given word depends only on the last n ? 1 words im-
mediately preceding it and is given by the following
equation:
P (wk1 ) = P (w1)P (w2|w1) ? ? ? P (wn|wn?11 ) (1)
where k >= n.
N -gram language models have been successfully
used in Automatic Speech Recognition (ASR) as was
first proposed by (Bahl et al, 1983). They play an im-
portant role in selecting among several candidate word
realization of a given acoustic signal. N -gram lan-
guage models have also been used in Statistical Ma-
chine Translation (SMT) as proposed by (Brown et al,
1990; Brown et al, 1993). The run-time search pro-
cedure used to find the most likely translation (or tran-
scription in the case of Speech Recognition) is typically
referred to as decoding.
There is a fundamental difference between decoding
for machine translation and decoding for speech recog-
nition. When decoding a speech signal, words are gen-
erated in the same order in which their corresponding
acoustic signal is consumed. However, that is not nec-
essarily the case in MT due to the fact that different
languages have different word order requirements. For
example, in Spanish and Arabic adjectives are mainly
noun post-modifiers, whereas in English adjectives are
noun pre-modifiers. Therefore, when translating be-
tween Spanish and English, words must usually be re-
ordered.
Existing statistical machine translation decoders
have mostly relied on language models to select the
proper word order among many possible choices when
translating between two languages. In this paper, we
argue that a language model is not sufficient to ade-
quately address this issue, especially when translating
between languages that have very different word orders
as suggested by our experimental results in Section 5.
We propose a new distortion model that can be used
as an additional component in SMT decoders. This
new model leads to significant improvements in MT
quality as measured by BLEU (Papineni et al, 2002).
The experimental results we report in this paper are for
Arabic-English machine translation of news stories.
We also present a novel method for measuring word
order similarity (or differences) between any given pair
of languages based on word alignments as described in
Section 3.
The rest of this paper is organized as follows. Sec-
tion 2 presents a review of related work. In Section 3
we propose a method for measuring the distortion be-
tween any given pair of languages. In Section 4, we
present our proposed distortion model. In Section 5,
we present some empirical results that show the utility
of our distortion model for statistical machine trans-
lation systems. Then, we conclude this paper with a
discussion in Section 6.
2 Related Work
Different languages have different word order require-
ments. SMT decoders attempt to generate translations
in the proper word order by attempting many possible
529
word reorderings during the translation process. Trying
all possible word reordering is an NP-Complete prob-
lem as shown in (Knight, 1999), which makes search-
ing for the optimal solution among all possible permu-
tations computationally intractable. Therefore, SMT
decoders typically limit the number of permutations
considered for efficiency reasons by placing reorder-
ing restrictions. Reordering restrictions for word-based
SMT decoders were introduced by (Berger et al, 1996)
and (Wu, 1996). (Berger et al, 1996) allow only re-
ordering of at most n words at any given time. (Wu,
1996) propose using contiguity restrictions on the re-
ordering. For a comparison and a more detailed discus-
sion of the two approaches see (Zens and Ney, 2003).
A different approach to allow for a limited reorder-
ing is to reorder the input sentence such that the source
and the target sentences have similar word order and
then proceed to monotonically decode the reordered
source sentence.
Monotone decoding translates words in the same or-
der they appear in the source language. Hence, the
input and output sentences have the same word order.
Monotone decoding is very efficient since the optimal
decoding can be found in polynomial time. (Tillmann
et al, 1997) proposed a DP-based monotone search al-
gorithm for SMT. Their proposed solution to address
the necessary word reordering is to rewrite the input
sentence such that it has a similar word order to the de-
sired target sentence. The paper suggests that reorder-
ing the input reduces the translation error rate. How-
ever, it does not provide a methodology on how to per-
form this reordering.
(Xia and McCord, 2004) propose a method to auto-
matically acquire rewrite patterns that can be applied
to any given input sentence so that the rewritten source
and target sentences have similar word order. These
rewrite patterns are automatically extracted by pars-
ing the source and target sides of the training parallel
corpus. Their approach show a statistically-significant
improvement over a phrase-based monotone decoder.
Their experiments also suggest that allowing the de-
coder to consider some word order permutations in
addition to the rewrite patterns already applied to the
source sentence actually decreases the BLEU score.
Rewriting the input sentence whether using syntactic
rules or heuristics makes hard decisions that can not
be undone by the decoder. Hence, reordering is better
handled during the search algorithm and as part of the
optimization function.
Phrase-based monotone decoding does not directly
address word order issues. Indirectly, however, the
phrase dictionary1 in phrase-based decoders typically
captures local reorderings that were seen in the training
data. However, it fails to generalize to word reorder-
ings that were never seen in the training data. For ex-
ample, a phrase-based decoder might translate the Ara-
1Also referred to in the literature as the set of blocks or
clumps.
bic phrase AlwlAyAt AlmtHdp2 correctly into English
as the United States if it was seen in its training data,
was aligned correctly, and was added to the phrase dic-
tionary. However, if the phrase Almmlkp AlmtHdp is
not in the phrase dictionary, it will not be translated
correctly by a monotone phrase decoder even if the in-
dividual units of the phrase Almmlkp and AlmtHdp, and
their translations (Kingdom and United, respectively)
are in the phrase dictionary since that would require
swapping the order of the two words.
(Och et al, 1999; Tillmann and Ney, 2003) relax
the monotonicity restriction in their phrase-based de-
coder by allowing a restricted set of word reorderings.
For their translation task, word reordering is done only
for words belonging to the verb group. The context in
which they report their results is a Speech-to-Speech
translation from German to English.
(Yamada and Knight, 2002) propose a syntax-based
decoder that restrict word reordering based on reorder-
ing operations on syntactic parse-trees of the input
sentence. They reported results that are better than
word-based IBM4-like decoder. However, their de-
coder is outperformed by phrase-based decoders such
as (Koehn, 2004), (Och et al, 1999), and (Tillmann and
Ney, 2003) . Phrase-based SMT decoders mostly rely
on the language model to select among possible word
order choices. However, in our experiments we show
that the language model is not reliable enough to make
the choices that lead to a better MT quality. This obser-
vation is also reported by (Xia and McCord, 2004).We
argue that the distortion model we propose leads to a
better translation as measured by BLEU.
Distortion models were first proposed by (Brown et
al., 1993) in the so-called IBM Models. IBM Mod-
els 2 and 3 define the distortion parameters in terms of
the word positions in the sentence pair, not the actual
words at those positions. Distortion probability is also
conditioned on the source and target sentence lengths.
These models do not generalize well since their param-
eters are tied to absolute word position within sentences
which tend to be different for the same words across
sentences. IBM Models 4 and 5 alleviate this limita-
tion by replacing absolute word positions with relative
positions. The latter models define the distortion pa-
rameters for a cept (one or more words). This models
phrasal movement better since words tend to move in
blocks and not independently. The distortion is con-
ditioned on classes of the aligned source and target
words. The entire source and target vocabularies are
reduced to a small number of classes (e.g., 50) for the
purpose of estimating those parameters.
Similarly, (Koehn et al, 2003) propose a relative dis-
tortion model to be used with a phrase decoder. The
model is defined in terms of the difference between the
position of the current phrase and the position of the
previous phrase in the source sentence. It does not con-
2Arabic text appears throughout this paper in Tim Buck-
walter?s Romanization.
530
Arabic Ezp1 AbrAhym2 ystqbl3 ms&wlA4 AqtSAdyA5 sEwdyA6 fy7 bgdAd8
English Izzet1 Ibrahim2 Meets3 Saudi4 Trade5 official6 in7 Baghdad8
Word Alignment (Ezp1,Izzet1) (AbrAhym2,Ibrahim2) (ystqbl3,Meets3) ( ms&wlA4,official6)
(AqtSAdyA5,Trade5) (sEwdyA6,Saudi4) (fy7,in7) (bgdAd8,Baghdad8)
Reordered English Izzet1 Ibrahim2 Meets3 official6 Trade5 Saudi4 in7 Baghdad8
Table 1: Alignment-based word reordering. The indices are not part of the sentence pair, they are only used to
illustrate word positions in the sentence. The indices in the reordered English denote word position in the original
English order.
sider the words in those positions.
The distortion model we propose assigns a proba-
bility distribution over possible relative jumps condi-
tioned on source words. Conditioning on the source
words allows for a much more fine-grained model. For
instance, words that tend to act as modifers (e.g., adjec-
tives) would have a different distribution than verbs or
nouns. Our model?s parameters are directly estimated
from word alignments as we will further explain in Sec-
tion 4. We will also show how to generalize this word
distortion model to a phrase-based model.
(Och et al, 2004; Tillman, 2004) propose
orientation-based distortion models lexicalized on the
phrase level. There are two important distinctions be-
tween their models and ours. First, they lexicalize their
model on the phrases, which have many more param-
eters and hence would require much more data to esti-
mate reliably. Second, their models consider only the
direction (i.e., orientation) and not the relative jump.
We are not aware of any work on measuring word
order differences between a given language pair in the
context of statistical machine translation.
3 Measuring Word Order Similarity
Between Two Language
In this section, we propose a simple, novel method for
measuring word order similarity (or differences) be-
tween any given language pair. This method is based
on word-alignments and the BLEU metric.
We assume that we have word-alignments for a set
of sentence pairs. We first reorder words in the target
sentence (e.g., English when translating from Arabic
to English) according to the order in which they are
aligned to the source words as shown in Table 1. If
a target word is not aligned, then, we assume that it
is aligned to the same source word that the preceding
aligned target word is aligned to.
Once the reordered target (here English) sentences
are generated, we measure the distortion between the
language pair by computing the BLEU3 score between
the original target and reordered target, treating the
original target as the reference.
Table 2 shows these scores for Arabic-English and
3the BLEU scores reported throughout this paper are for
case-sensitive BLEU. The number of references used is also
reported (e.g., BLEUr1n4c: r1 means 1 reference, n4 means
upto 4-gram are considred, c means case sensitive).
Chinese-English. The word alignments we use are both
annotated manually by human annotators. The Arabic-
English test set is the NIST MT Evaluation 2003 test
set. It contains 663 segments (i.e., sentences). The
Arabic side consists of 16,652 tokens and the English
consists of 19,908 tokens. The Chinese-English test set
contains 260 segments. The Chinese side is word seg-
mented and consists of 4,319 tokens and the English
consists of 5,525 tokens.
As suggested by the BLEU scores reported in Ta-
ble 2, Arabic-English has more word order differences
than Chinese-English. The difference in n-gPrec is big-
ger for smaller values of n, which suggests that Arabic-
English has more local word order differences than in
Chinese-English.
4 Proposed Distortion Model
The distortion model we are proposing consists of three
components: outbound, inbound, and pair distortion.
Intuitively our distortion models attempt to capture the
order in which source words need to be translated. For
instance, the outbound distortion component attempts
to capture what is typically translated immediately after
the word that has just been translated. Do we tend to
translate words that precede it or succeed it? Which
word position to translate next?
Our distortion parameters are directly estimated
from word alignments by simple counting over align-
ment links in the training data. Any aligner such as
(Al-Onaizan et al, 1999) or (Vogel et al, 1996) can
be used to obtain word alignments. For the results
reported in this paper word alignments were obtained
using a maximum-posterior word aligner4 described in
(Ge, 2004).
We will illustrate the components of our model with
a partial word alignment. Let us assume that our
source sentence5 is (f10, f250, f300)6, and our target
sentence is (e410, e20), and their word alignment is
a = ((f10, e410), (f300, e20)). Word Alignment a can
4We also estimated distortion parameters using a Maxi-
mum Entropy aligner and the differences were negligible.
5In practice, we add special symbols at the start and end of
the source and target sentences, we also assume that the start
symbols in the source and target are aligned, and similarly
for the end symbols. Those special symbols are omitted in
our example for ease of presentation.
6The indices here represent source and target vocabulary
ids.
531
N-gram Precision Arabic-English Chinese-English
1-gPrec 1 1
2-gPrec 0.6192 0.7378
3-gPrec 0.4547 0.5382
4-gPrec 0.3535 0.3990
5-gPrec 0.2878 0.3075
6-gPrec 0.2378 0.2406
7-gPrec 0.1977 0.1930
8-gPrec 0.1653 0.1614
9-gPrec 0.1380 0.1416
BLEUr1n4c 0.3152 0.3340
95% Confidence ? 0.0180 0.0370
Table 2: Word order similarity for two language pairs: Arabic-English and Chinese-English. n-gPrec is the n-gram
precision as defined in BLEU.
be rewritten as a1 = 1 and a2 = 3 (i.e., the second tar-
get word is aligned to the third source word). From this
partial alignment we increase the counts for the follow-
ing outbound, inbound, and pair distortions: Po(? =
+2|f10), Pi(? = +2|f300). and Pp(? = +2|f10, f300).
Formally, our distortion model components are de-
fined as follows:
Outbound Distortion:
Po(?|fi) =
C(?|fi)
?
k
C(?k |fi)
(2)
where fi is a foreign word (i.e., Arabic in our case),
? is the step size, and C(?|fi) is the observed count of
this parameter over all word alignments in the training
data. The value for ?, in theory, ranges from ?max to
+max (where max is the maximum source sentence
length observed), but in practice only a small number
of those step sizes are observed in the training data,
and hence, have non-zero value).
Inbound Distortion:
Pi(?|fj) =
C(?|fj)
?
k
C(?k|fj)
(3)
Pairwise Distortion:
Pp(?|fi, fj) =
C(?|fi, fj)
?
k
C(?k|fi, fj)
(4)
In order to use these probability distributions in our
decoder, they are then turned into costs. The outbound
distortion cost is defined as:
Co(?|fi) = log {?Po(?|fi) + (1 ? ?)Ps(?)} (5)
where Ps(?) is a smoothing distribution 7 and ? is a
linear-mixture parameter 8.
7The smoothing we use is a geometrically decreasing dis-
tribution as the step size increases.
8For the experiments reported here we use ? = 0.1,
which is set empirically.
The inbound and pair costs (Ci(?|fi) and
Cp(?|fi, fj)) can be defined in a similar fashion.
So far, our distortion cost is defined in terms of
words, not phrases. Therefore, we need to general-
ize the distortion cost in order to use it in a phrase-
based decoder. This generalization is defined in terms
of the internal word alignment within phrases (we used
the Viterbi word alignment). We illustrate this with
an example: Suppose the last position translated in the
source sentence so far is n and we are to cover a source
phrase p=wlAyp wA$nTn that begins at position m in
the source sentence. Also, suppose that our phrase dic-
tionary provided the translation Washington State, with
internal word alignment a = (a1 = 2, a2 = 1) (i.e.,
a=(<Washington,wA$nTn>,<State,wlAyp>), then the
outbound phrase cost is defined as:
Co(p, n, m, a) =Co(? = (m ? n)|fn)+
l?1
?
i=1
Co(? = (ai+1 ? ai) |fai)
(6)
where l is the length of the target phrase, a is the
internal word alignment, fn is source word at position
n (in the sentence), and fai is the source word that is
aligned to the i-th word in the target side of the phrase
(not the sentence).
The inbound and pair distortion costs (i..e,
Ci(p, n, m, a) and Cp(p, n, m, a)) can be defined
in a similar fashion.
The above distortion costs are used in conjunction
with other cost components used in our decoder. The
ultimate word order choice made is influenced by both
the language model cost as well as the distortion cost.
5 Experimental Results
The phrase-based decoder we use is inspired by the de-
coder described in (Tillmann and Ney, 2003) and sim-
ilar to that described in (Koehn, 2004). It is a multi-
stack, multi-beam search decoder with n stacks (where
n is the length of the source sentence being decoded)
532
s 0 1 1 1 1 1 2 2 2 2
w 0 4 6 8 10 12 4 6 8 10
BLEUr1n4c 0.5617 0.6507 0.6443 0.6430 0.6461 0.6456 0.6831 0.6706 0.6609 0.6596
2 3 3 3 3 3 4 4 4 4 4
12 4 6 8 10 12 4 6 8 10 12
0.6626 0.6919 0.6751 0.6580 0.6505 0.6490 0.6851 0.6592 0.6317 0.6237 0.6081
Table 3: BLEU scores for the word order restoration task. The BLEU scores reported here are with 1 reference.
The input is the reordered English in the reference. The 95% Confidence ? ranges from 0.011 to 0.016
and a beam associated with each stack as described
in (Al-Onaizan, 2005). The search is done in n time
steps. In time step i, only hypotheses that cover ex-
actly i source words are extended. The beam search
algorithm attempts to find the translation (i.e., hypoth-
esis that covers all source words) with the minimum
cost as in (Tillmann and Ney, 2003) and (Koehn, 2004)
. The distortion cost is added to the log-linear mixture
of the hypothesis extension in a fashion similar to the
language model cost.
A hypothesis covers a subset of the source words.
The final translation is a hypothesis that covers all
source words and has the minimum cost among all pos-
sible 9 hypotheses that cover all source words. A hy-
pothesis h is extended by matching the phrase dictio-
nary against source word sequences in the input sen-
tence that are not covered in h. The cost of the new
hypothesis C(hnew) = C(h) + C(e), where C(e) is
the cost of this extension. The main components of
the cost of extension e can be defined by the following
equation:
C(e) = ?1CLM (e) + ?2CTM (e) + ?3CD(e)
where CLM (e) is the language model cost, CTM (e)
is the translation model cost, and CD(e) is the distor-
tion cost. The extension cost depends on the hypothesis
being extended, the phrase being used in the extension,
and the source word positions being covered.
The word reorderings that are explored by the search
algorithm are controlled by two parameters s and w as
described in (Tillmann and Ney, 2003). The first pa-
rameter s denotes the number of source words that are
temporarily skipped (i.e., temporarily left uncovered)
during the search to cover a source word to the right of
the skipped words. The second parameter is the win-
dow width w, which is defined as the distance (in num-
ber of source words) between the left-most uncovered
source word and the right-most covered source word.
To illustrate these restrictions, let us assume the
input sentence consists of the following sequence
(f1, f2, f3, f4). For s=1 and w=2, the permissi-
ble permutations are (f1, f2, f3, f4), (f2, f1, f3, f4),
9Exploring all possible hypothesis with all possible word
permutations is computationally intractable. Therefore, the
search algorithm gives an approximation to the optimal so-
lution. All possible hypotheses refers to all hypotheses that
were explored by the decoder.
(f2, f3, f1, f4), (f1, f3, f2, f4),(f1, f3, f4, f2), and
(f1, f2, f4, f3).
5.1 Experimental Setup
The experiments reported in this section are in the con-
text of SMT from Arabic into English. The training
data is a 500K sentence-pairs subsample of the 2005
Large Track Arabic-English Data for NIST MT Evalu-
ation.
The language model used is an interpolated trigram
model described in (Bahl et al, 1983). The language
model is trained on the LDC English GigaWord Cor-
pus.
The test set used in the experiments in this section
is the 2003 NIST MT Evaluation test set (which is not
part of the training data).
5.2 Reordering with Perfect Translations
In the experiments in this section, we show the util-
ity of a trigram language model in restoring the correct
word order for English. The task is a simplified transla-
tion task, where the input is reordered English (English
written in Arabic word order) and the output is English
in the correct order. The source sentence is a reordered
English sentence in the same manner we described in
Section 3. The objective of the decoder is to recover
the correct English order.
We use the same phrase-based decoder we use for
our SMT experiments, except that only the language
model cost is used here. Also, the phrase dictionary
used is a one-to-one function that maps every English
word in our vocabulary to itself. The language model
we use for the experiments reported here is the same
as the one used for other experiments reported in this
paper.
The results in Table 3 illustrate how the language
model performs reasonably well for local reorderings
(e.g., for s = 3 and w = 4), but its perfromance de-
teriorates as we relax the reordering restrictions by in-
creasing the reordering window size (w).
Table 4 shows some examples of original English,
English in Arabic order, and the decoder output for two
different sets of reordering parameters.
5.3 SMT Experiments
The phrases in the phrase dictionary we use in
the experiments reported here are a combination
533
Eng Ar Opposition Iraqi Prepares for Meeting mid - January in Kurdistan
Orig. Eng. Iraqi Opposition Prepares for mid - January Meeting in Kurdistan
Output1 Iraqi Opposition Meeting Prepares for mid - January in Kurdistan
Output2 Opposition Meeting Prepares for Iraqi Kurdistan in mid - January
Eng Ar Head of Congress National Iraqi Visits Kurdistan Iraqi
Orig. Eng. Head of Iraqi National Congress Visits Iraqi Kurdistan
Output1 Head of Iraqi National Congress Visits Iraqi Kurdistan
Output2 Head Visits Iraqi National Congress of Iraqi Kurdistan
Eng Ar House White Confirms Presence of Tape New Bin Laden
Orig. Eng. White House Confirms Presence of New Bin Laden Tape
Output1 White House Confirms Presence of Bin Laden Tape New
Output2 White House of Bin Laden Tape Confirms Presence New
Table 4: Examples of reordering with perfect translations. The examples show English in Arabic order (Eng Ar.),
English in its original order (Orig. Eng.) and decoding with two different parameter settings. Output1 is decoding
with (s=3,w=4). Output2 is decoding with (s=4,w=12). The sentence lengths of the examples presented here are
much shorter than the average in our test set (? 28.5).
s w Distortion Used? BLEUr4n4c
0 0 NO 0.4468
1 8 NO 0.4346
1 8 YES 0.4715
2 8 NO 0.4309
2 8 YES 0.4775
3 8 NO 0.4283
3 8 YES 0.4792
4 8 NO 0.4104
4 8 YES 0.4782
Table 5: BLEU scores for the Arabic-English machine translation task. The 95% Confidence ? ranges from 0.0158
to 0.0176. s is the number of words temporarily skipped, and w is the word permutation window size.
of phrases automatically extracted from maximum-
posterior alignments and maximum entropy align-
ments. Only phrases that conform to the so-called con-
sistent alignment restrictions (Och et al, 1999) are ex-
tracted.
Table 5 shows BLEU scores for our SMT decoder
with different parameter settings for skip s, window
width w, with and without our distortion model. The
BLEU scores reported in this table are based on 4 refer-
ence translations. The language model, phrase dictio-
nary, and other decoder tuning parameters remain the
same in all experiments reported in this table.
Table 5 clearly shows that as we open the search and
consider wider range of word reorderings, the BLEU
score decreases in the absence of our distortion model
when we rely solely on the language model. Wrong
reorderings look attractive to the decoder via the lan-
guage model which suggests that we need a richer
model with more parameter. In the absence of richer
models such as the proposed distortion model, our re-
sults suggest that it is best to decode monotonically and
only allow local reorderings that are captured in our
phrase dictionary.
However, when the distortion model is used, we see
statistically significant increases in the BLEU score as
we consider more word reorderings. The best BLEU
score achieved when using the distortion model is
0.4792 , compared to a best BLEU score of 0.4468
when the distortion model is not used.
Our results on the 2004 and 2005 NIST MT Evalua-
tion test sets using the distortion model are 0.4497 and
0.464610, respectively.
Table 6 shows some Arabic-English translation ex-
amples using our decoder with and without the distor-
tion model.
6 Conclusion and Future Work
We presented a new distortion model that can be in-
tegrated with existing phrase-based SMT decoders.
The proposed model shows statistically significant im-
provement over a state-of-the-art phrase-based SMT
decoder. We also showed that n-gram language mod-
10The MT05 BLEU score is the from the official NIST
evaluation. The MT04 BLEU score is only our second run
on MT04.
534
Input (Ar) kwryA Al$mAlyp mstEdp llsmAH lwA$nTn bAltHqq mn AnhA lA tSnE AslHp nwwyp
Ref. (En) North Korea Prepared to allow Washington to check it is not Manufacturing Nuclear
Weapons
Out1 North Korea to Verify Washington That It Was Not Prepared to Make Nuclear Weapons
Out2 North Korea Is Willing to Allow Washington to Verify It Does Not Make Nuclear Weapons
Input (Ar) wAkd AldblwmAsy An ?AnsHAb (kwryA Al$mAlyp mn AlmEAhdp) ybd> AEtbArA mn
Alywm?.
Ref. (En) The diplomat confirmed that ?North Korea?s withdrawal from the treaty starts as of today.?
Out1 The diplomat said that ? the withdrawal of the Treaty (start) North Korea as of today. ?
Out2 The diplomat said that the ? withdrawal of (North Korea of the treaty) will start as of
today ?.
Input (Ar) snrfE *lk AmAm Almjls Aldstwry?.
Ref. (En) We will bring this before the Constitutional Assembly.?
Out1 The Constitutional Council to lift it. ?
Out2 This lift before the Constitutional Council ?.
Input (Ar) wAkd AlbrAdEy An mjls AlAmn ?ytfhm? An 27 kAnwn AlvAny/ynAyr lys mhlp nhA}yp.
Ref. (En) Baradei stressed that the Security Council ?appreciates? that January 27 is not a final
ultimatum.
Out1 Elbaradei said that the Security Council ? understand ? that is not a final period January 27.
Out2 Elbaradei said that the Security Council ? understand ? that 27 January is not a final period.
Table 6: Selected examples of our Arabic-English SMT output. The English is one of the human reference trans-
lations. Output 1 is decoding without the distortion model and (s=4, w=8), which corresponds to 0.4104 BLEU
score. Output 2 is decoding with the distortion model and (s=3, w=8), which corresponds to 0.4792 BLEU score.
The sentences presented here are much shorter than the average in our test set. The average length of the arabic
sentence in the MT03 test set is ? 24.7.
els are not sufficient to model word movement in trans-
lation. Our proposed distortion model addresses this
weakness of the n-gram language model.
We also propose a novel metric to measure word or-
der similarity (or differences) between any pair of lan-
guages based on word alignments. Our metric shows
that Chinese-English have a closer word order than
Arabic-English.
Our proposed distortion model relies solely on word
alignments and is conditioned on the source words.
The majority of word movement in translation is
mainly due to syntactic differences between the source
and target language. For example, Arabic is verb-initial
for the most part. So, when translating into English,
one needs to move the verb after the subject, which is
often a long compounded phrase. Therefore, we would
like to incorporate syntactic or part-of-speech informa-
tion in our distortion model.
Acknowledgment
This work was partially supported by DARPA GALE
program under contract number HR0011-06-2-0001. It
was also partially supported by DARPA TIDES pro-
gram monitored by SPAWAR under contract number
N66001-99-2-8916.
References
Yaser Al-Onaizan, Jan Curin, Michael Jahr, Kevin
Knight, John Lafferty, Dan Melamed, Franz-
Josef Och, David Purdy, Noah Smith, and David
Yarowsky. 1999. Statistical Machine Translation:
Final Report, Johns Hopkins University Summer
Workshop (WS 99) on Language Engineering, Cen-
ter for Language and Speech Processing, Baltimore,
MD.
Yaser Al-Onaizan. 2005. IBM Arabic-to-English MT
Submission. Presentation given at DARPA/TIDES
NIST MT Evaluation workshop.
Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer.
1983. A Maximum Likelihood Approach to Con-
tinuous Speech Recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, PAMI-
5(2):179?190.
Adam L. Berger, Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, Andrew S. Kehler,
and Robert L. Mercer. 1996. Language Transla-
tion Apparatus and Method of Using Context-Based
Translation Models. United States Patent, Patent
Number 5510981, April.
Peter F Brown, John Cocke, Stephen A Della Pietra,
Vincent J Della Pietra, Frederick Jelinek, John D
Lafferty, Robert L Mercer, and Paul S Roossin.
1990. A Statistical Approach to Machine Transla-
tion. Computational Linguistics, 16(2):79?85.
535
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The Mathematics of Statistical Machine Translation:
Parameter Estimation. Computational Linguistics,
19(2):263?311.
Niyu Ge. 2004. Improvements in Word Alignments.
Presentation given at DARPA/TIDES NIST MT Eval-
uation workshop.
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational
Linguistics, 25(4):607?615.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Marti Hearst
and Mari Ostendorf, editors, HLT-NAACL 2003:
Main Proceedings, pages 127?133, Edmonton, Al-
berta, Canada, May 27 ? June 1. Association for
Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Trans-
lation Models. In Proceedings of the 6th Con-
ference of the Association for Machine Translation
in the Americas, pages 115?124, Washington DC,
September-October. The Association for Machine
Translation in the Americas (AMTA).
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved Alignment Models for Statis-
tical Machine Translation. In Joint Conf. of Empir-
ical Methods in Natural Language Processing and
Very Large Corpora, pages 20?28, College Park,
Maryland.
Franz Josef Och, Daniel Gildea, Sanjeev Khudan-
pur, Anoop Sarkar, Kenji Yamada, Alex Fraser,
Shankar Kumar, Libin Shen, David Smith, Kather-
ine Eng, Viren Jain, Zhen Jin, and Dragomir Radev.
2004. A Smorgasbord of Features for Statistical
Machine Translation. In Daniel Marcu Susan Du-
mais and Salim Roukos, editors, HLT-NAACL 2004:
Main Proceedings, pages 161?168, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of machine translation. In 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL 02), pages 311?318, Philadelphia, PA,
July.
Christoph Tillman. 2004. A unigram orienta-
tion model for statistical machine translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101?
104, Boston, Massachusetts, USA, May 2 - May 7.
Association for Computational Linguistics.
Christoph Tillmann and Hermann Ney. 2003. Word
Re-ordering and a DP Beam Search Algorithm for
Statistical Machine Translation. Computational Lin-
guistics, 29(1):97?133.
Christoph Tillmann, Stephan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-Based Search Using
Monotone Alignments in Statistical Translation. In
Proceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics and 8th Con-
ference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 289?296,
Madrid. Association for Computational Linguistics.
Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-BasedWord Alignment in Statisti-
cal Machine Translation. In Proc. of the 16th
Int. Conf. on Computational Linguistics (COLING
1996), pages 836?841, Copenhagen, Denmark, Au-
gust.
Dekai Wu. 1996. A Polynomial-Time Algorithm for
Statistical Machine Translation. In Proc. of the 34th
Annual Conf. of the Association for Computational
Linguistics (ACL 96), pages 152?158, Santa Cruz,
CA, June.
Fei Xia and Michael McCord. 2004. Improving a
Statistical MT System with Automatically Learned
Rewrite Patterns. In Proc. of the 20th International
Conference on Computational Linguistics (COLING
2004), Geneva, Switzerland.
Kenji Yamada and Kevin Knight. 2002. A Decoder for
Syntax-based Statistical MT. In Proc. of the 40th
Annual Conf. of the Association for Computational
Linguistics (ACL 02), pages 303?310, Philadelphia,
PA, July.
Richard Zens and Hermann Ney. 2003. A Compar-
ative Study on Reordering Constraints in Statistical
Machine Translation. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 144?151, Sapporo, Japan.
536
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 277?286, Prague, June 2007. c?2007 Association for Computational Linguistics
Hierarchical System Combination for Machine Translation
Fei Huang
IBM T.J. Watson Research Center
Yorktown Heights, NY 10562
huangfe@us.ibm.com
Kishore Papineni ?
Yahoo! Research
New York, NY 10011
kpapi@yahoo-inc.com
Abstract
Given multiple translations of the same
source sentence, how to combine them to
produce a translation that is better than any
single system output? We propose a hier-
archical system combination framework for
machine translation. This framework inte-
grates multiple MT systems? output at the
word-, phrase- and sentence- levels. By
boosting common word and phrase trans-
lation pairs, pruning unused phrases, and
exploring decoding paths adopted by other
MT systems, this framework achieves bet-
ter translation quality with much less re-
decoding time. The full sentence translation
hypotheses from multiple systems are addi-
tionally selected based on N-gram language
models trained on word/word-POS mixed
stream, which further improves the transla-
tion quality. We consistently observed sig-
nificant improvements on several test sets in
multiple languages covering different gen-
res.
1 Introduction
Many machine translation (MT) frameworks have
been developed, including rule-based transfer MT,
corpus-based MT (statistical MT and example-based
MT), syntax-based MT and the hybrid, statistical
MT augmented with syntactic structures. Different
MT paradigms have their strengths and weaknesses.
?This work was done when the author was at IBM Research.
Systems adopting the same framework usually pro-
duce different translations for the same input, due
to their differences in training data, preprocessing,
alignment and decoding strategies. It is beneficial
to design a framework that combines the decoding
strategies of multiple systems as well as their out-
puts and produces translations better than any single
system output. More recently, within the GALE1
project, multiple MT systems have been developed
in each consortium, thus system combination be-
comes more important.
Traditionally, system combination has been con-
ducted in two ways: glass-box combination and
black-box combination. In the glass-box combi-
nation, each MT system provides detailed decod-
ing information, such as word and phrase transla-
tion pairs and decoding lattices. For example, in the
multi-engine machine translation system (Nirenburg
and Frederking, 1994), target language phrases from
each system and their corresponding source phrases
are recorded in a chart structure, together with their
confidence scores. A chart-walk algorithm is used
to select the best translation from the chart. To com-
bine words and phrases from multiple systems, it is
preferable that all the systems adopt similar prepro-
cessing strategies.
In the black-box combination, individual MT sys-
tems only output their top-N translation hypothe-
ses without decoding details. This is particularly
appealing when combining the translation outputs
from COTS MT systems. The final translation may
be selected by voted language models and appropri-
ate confidence rescaling schemes ((Tidhar and Kuss-
1http://www.darpa.mil/ipto/programs/gale/index.htm
277
ner, 2000) and (Nomoto, 2004)). (Mellebeek et al,
2006) decomposes source sentences into meaning-
ful constituents, translates them with component MT
systems, then selects the best segment translation
and combine them based on majority voting, lan-
guage models and confidence scores.
(Jayaraman and Lavie, 2005) proposed another
black-box system combination strategy. Given sin-
gle top-one translation outputs from multiple MT
systems, their approach reconstructs a phrase lat-
tice by aligning words from different MT hypothe-
ses. The alignment is based on the surface form
of individual words, their stems (after morphology
analysis) and part-of-speech (POS) tags. Aligned
words are connected via edges. The algorithm finds
the best alignment that minimizes the number of
crossing edges. Finally the system generates a new
translation by searching the lattice based on align-
ment information, each system?s confidence scores
and a language model score. (Matusov et al, 2006)
and (Rosti et al, 2007) constructed a confusion net-
work from multiple MT hypotheses, and a consen-
sus translation is selected by redecoding the lattice
with arc costs and confidence scores.
In this paper, we introduce our hierarchical sys-
tem combination strategy. This approach allows
combination on word, phrase and sentence levels.
Similar to glass-box combination, each MT sys-
tem provides detailed information about the trans-
lation process, such as which source word(s) gener-
ates which target word(s) in what order. Such in-
formation can be combined with existing word and
phrase translation tables, and the augmented phrase
table will be significantly pruned according to reli-
able MT hypotheses. We select an MT system to re-
translate the test sentences with the refined models,
and encourage search along decoding paths adopted
by other MT systems. Thanks to the refined trans-
lation models, this approach produces better transla-
tions with a much shorter re-decoding time. As in
the black-box combination, we select full sentence
translation hypotheses from multiple system outputs
based on n-gram language models. This hierarchical
system combination strategy avoids problems like
translation output alignment and confidence score
normalization. It seamlessly integrates detailed de-
coding information and translation hypotheses from
multiple MT engines, and produces better transla-
tions in an efficient manner. Empirical studies in a
later section show that this algorithm improves MT
quality by 2.4 BLEU point over the best baseline de-
coder, with a 1.4 TER reduction. We also observed
consistent improvements on several evaluation test
sets in multiple languages covering different genres
by combining several state-of-the-art MT systems.
The rest of the paper is organized as follows: In
section 2, we briefly introduce several baseline MT
systems whose outputs are used in the system com-
bination. In section 3, we present the proposed hi-
erarchical system combination framework. We will
describe word and phrase combination and pruning,
decoding path imitation and sentence translation se-
lection. We show our experimental results in section
4 and conclusions in section 5.
2 Baseline MT System Overview
In our experiments, we take the translation out-
puts from multiple MT systems. These include
phrase-based statistical MT systems (Al-Onaizan
and Papineni, 2006) (Block) and (Hewavitharana et
al., 2005) (CMU SMT) , a direct translation model
(DTM) system (Ittycheriah and Roukos, 2007) and a
hierarchical phrased-based MT system (Hiero) (Chi-
ang, 2005). Different translation frameworks are
adopted by different decoders: the DTM decoder
combines different features (source words, mor-
phemes and POS tags, target words and POS tags)
in a maximum entropy framework. These features
are integrated with a phrase translation table for
flexible distortion model and word selection. The
CMU SMT decoder extracts testset-specific bilin-
gual phrases on the fly with PESA algorithm. The
Hiero system extracts context-free grammar rules
for long range constituent reordering.
We select the IBM block decoder to re-translate
the test set for glass-box system combination. This
system is a multi-stack, multi-beam search decoder.
Given a source sentence, the decoder tries to find
the translation hypothesis with the minimum trans-
lation cost. The overall cost is the log-linear combi-
nation of different feature functions, such as trans-
lation model cost, language model cost, distortion
cost and sentence length cost. The translation cost
278
between a phrase translation pair (f, e) is defined as
TM(e, f) =
?
i
?i?(i) (1)
where feature cost functions ?(i) includes:
? log p(f |e), a target-to-source word translation
cost, calculated based on unnormalized IBM model1
cost (Brown et al, 1994);
p(f |e) =
?
j
?
i
t(fj|ei) (2)
where t(fj|ei) is the word translation probabilities,
estimated based on word alignment frequencies over
all the training data. i and j are word positions in
target and source phrases.
? log p(e|f), a source-to-target word translation
cost, calculated similar to ? log p(f |e);
S(e, f), a phrase translation cost estimated ac-
cording to their relative alignment frequency in the
bilingual training data,
S(e, f) = ? log P (e|f) = ? log C(f, e)C(f) . (3)
??s in Equation 1 are the weights of different fea-
ture functions, learned to maximize development set
BLEU scores using a method similar to (Och, 2003).
The SMT system is trained with testset-specific
training data. This is not cheating. Given a test set,
from a large bilingual corpora we select parallel sen-
tence pairs covering n-grams from source sentences.
Phrase translation pairs are extracted from the sub-
sampled alignments. This not only reduces the size
of the phrase table, but also improves topic relevancy
of the extracted phrase pairs. As a results, it im-
proves both the efficiency and the performance of
machine translation.
3 Hierarchical System Combination
Framework
The overall system combination framework is
shown in Figure 1. The source text is translated
by multiple baseline MT systems. Each system pro-
duces both top-one translation hypothesis as well as
phrase pairs and decoding path during translation.
The information is shared through a common XML
file format, as shown in Figure 2. It demonstrates
how a source sentence is segmented into a sequence
of phrases, the order and translation of each source
phrase as well as the translation scores, and a vector
of feature scores for the whole test sentence. Such
XML files are generated by all the systems when
they translate the source test set.
We collect phrase translation pairs from each de-
coder?s output. Within each phrase pair, we iden-
tify word alignment and estimate word translation
probabilities. We combine the testset-specific word
translation model with a general model. We aug-
ment the baseline phrase table with phrase trans-
lation pairs extracted from system outputs, then
prune the table with translation hypotheses. We re-
translate the source text using the block decoder with
updated word and phrase translation models. Ad-
ditionally, to take advantage of flexible reordering
strategies of other decoders, we develop a word or-
der cost function to reinforce search along decod-
ing paths adopted by other decoders. With the re-
fined translation models and focused search space,
the block decoder efficiently produces a better trans-
lation output. Finally, the sentence hypothesis se-
lection module selects the best translation from each
systems? top-one outputs based on language model
scores. Note that the hypothesis selection module
does not require detailed decoding information, thus
can take in any MT systems? outputs.
3.1 Word Translation Combination
The baseline word translation model is too general
for the given test set. Our goal is to construct a
testset-specific word translation model, combine it
with the general model to boost consensus word
translations. Bilingual phrase translation pairs are
read from each system-generated XML file. Word
alignments are identified within a phrase pair based
on IBM Model-1 probabilities. As the phrase pairs
are typically short, word alignments are quite accu-
rate. We collect word alignment counts from the
whole test set translation, and estimate both source-
to-target and target-to-source word translation prob-
abilities. We combine such testset-specific transla-
tion model with the general model.
t??(e|f) = ?t?(e|f) + (1 ? ?)t(e|f); (4)
where t?(e|f) is the testset-specific source-to-target
word translation probability, and t(e|f) is the prob-
279
<tr engine="XXX"> 
<s id="0"> <w>  </w><w> 	
 </w><w>  </w><w> 	 </w><w>  
</w><w>  </w><w>  </w><w> Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 61?68,
ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
 
Multiple Reorderings in Phrase-based Machine Translation 
 
Niyu Ge,  Abe Ittycheriah 
IBM T.J.Watson Research 
1101 Kitchawan Rd.  
Yorktown Heights, NY 10598 
(niyuge, abei)@us.ibm.com 
 
Kishore Papineni 
Yahoo! Research 
45 West 18th St. 
New York, NY 10011 
kpapi@yahoo-inc.com 
 
 
Abstract  
This paper presents a method to integrate 
multiple reordering strategies in 
phrase-based statistical machine 
translation.  Recently there has been much 
research effort in reordering problems in 
machine translation.   State-of-the-art 
decoders incorporate sophisticated local 
reordering strategies, but there is little 
research on a unified approach to 
incorporate various kinds of reordering 
methods.  We present a phrase-based 
decoder which easily allows multiple 
reordering schemes.  We show how to use 
this framework to perform distance-based 
reordering and HIERO-style (Chiang 
2005) hierarchical reordering.  We also 
present two novel syntax-based reordering 
methods, one built on part-of-speech tags 
and the other based on parse trees.  We will 
give experimental results using these 
relatively easy to implement methods on 
standard tests.    
1 Introduction and Previous Work 
Given an input source sentence and guided by a 
translation model, language model, distortion 
model, etc., a machine translation decoder 
searches for a target sentence that is the best 
translation of the source.  There are usually two 
aspects of the search.  One tries to find target 
words for a given source segment.  The other 
searches for the order in which the source 
segments are to be translated.   A source segment 
here means a contiguous part of the source 
sentence. The former is largely controlled by 
language models and translation models and the 
latter by language models and distortion models.  
It is, in most cases, the latter, the search for the 
correct word order (which source segment to be 
translated next) that results in a large 
combinatorial search space.  State-of-the-art 
decoders use dynamic programming based 
beam-search with local reordering (Och 1999, 
Tillmann 2000).  Although local reordering to 
some degree is implicit in phrase-based 
decoding, the kind of reordering is very limited.   
The simplest distance-based reordering, from the 
current source position i, tries to defer the 
translation of the next n words (1 ? n ? N, N the 
maximum number of words to be delayed).  N is 
bounded by the computational requirements.    
 
Recent work on reordering has been on trying to 
find ?smart? ways to decide word order, using 
syntactic features such as POS tags (Lee and Ge 
2005) , parse trees (Zhang et.al, 2007, Wang et.al. 
2007,  Collins et.al. 2005, Yamada and Knight 
2001) to name just a few, and synchronized CFG 
(Wu 1997, Chiang 2005), again to name just a 
few.  These efforts have shown promising 
improvements in translation quality.  However, 
to use these features during decoding requires 
either a separate decoder to be written or some 
ad-hoc mechanisms to be invented to incorporate 
them into an existing decoder, or in some cases 
(Wang et. al. 2007) the input source is 
pre-ordered to be decoded monotonically.   
 
(Kanthak et. al. 2005) described a framework  in 
which different reordering methods are 
represented as search constraints to a finite state 
automata.   It is able to compute distance-based 
and ITG-style reordering automata.   We differ 
from that approach in a couple of ways.  One is 
that in (Kanthak et. al. 2005), an on-demand 
61
 reordering graph is pre-computed which is then 
taken as a input for monotonic decoding.    We 
compute the reordering as the sentence is being 
decoded.  The second is that it is not clear how to 
generate the permutation graphs under, say 
HIERO-type hierarchical constraints,  or other 
syntax-inspired reorderings such as those based 
on part-of-speech patterns.  Our approach differs 
in that we allow greater flexibility in capturing a 
wider range of reordering strategies. 
 
We will first give an overview of  the framework 
(?2).  We then describe how to implement four 
reordering methods in a single decoder in ?3.  ?4 
presents some Chinese-English results on the 
NIST MT test sets.  It also shows results on web 
log and broadcast news data.    
2 Reordering in Decoding 
2.1 Hypothesis 
The process of MT decoding can be thought of as 
a process of hypothesizing target translations.  
Given an input source sentence of length L, the 
decoding is done segment by segment.  A 
segment is simply an n-word source chunk, 
where 1 ? n ? L.  Decoding finishes when all 
source chunks are translated (some source words 
that have no target translations can be thought of 
as being translated into a special token NULL).   
The decoder at this point outputs its best 
hypothesis. 
 
2.2 Hypothesis with reorderings 
In order to facilitate various search strategies, a 
separation of duty is called for.    The decoder is 
composed of two major modules, a reordering 
module and a production module.  The reordering 
module decides which source segment to be 
translated next.   The production module 
produces the actual translations for a given 
segment.  Although most of the start-of-the-art 
decoders have these two modules, they are 
nevertheless tightly coupled.  Here they are 
separated.  This separation does not compromise 
the search space of the decoder.  Hypotheses that 
are explored in the traditional way are still 
explored in this framework.  This separation is 
essential if one were to design a decoder that 
incorporates phrase-based, syntax-based, and 
other types of decoding in a unified and 
disciplined way.   In the decoder, each hypothesis 
carries with it a sequence of source segments to 
be decoded at the current time step.   After the 
production module translates these segments and 
after beam pruning is applied to all the 
hypotheses produced at this time step, the 
hypotheses go back to the reordering module 
which determines the next source segments to be 
translated.  This process continues until all source 
words are translated.   
 
One can think of the reordering module as a black 
box whose sole responsibility is to determine the 
next sequence of source segments to be translated.   
Given this separation, the reordering module can 
be implemented in whichever way and the 
changes in it do not require changes to any other 
modules in the decoder.  There can be a suite of 
such modules, each exploring different features 
and implementing different search schemes.  A 
reordering module that implement basic 
distance-based reordering will take two 
parameters, the number of source words to be 
skipped and the window size that determines 
when the skipped words must be translated.  A 
reordering module that is based on HIERO rules 
will take the library of HIERO rules and select 
the subset that fire on a given input sentence.  The 
module will use this subset of rules to determine 
the source translation order.  A parse-inspired 
reordering module will take an input parse tree 
and based on either a trained model or 
hand-written rules  decide the next source 
sequence to be translated.  As long as all the 
reordering modules are written to a common 
interface,  they can be separately written and 
maintained.   
Figure 1 shows an example of how three 
reordering modules can be incorporated into a 
single decoder.  The input source is S1?Sn.   
Module
skip = 2
window = 3
S1 S2 X ?> T1 T2 X
S1
S2 S3
Sn?1 Sn
.....
Distance?based
Reordering
S1 X Sn ?> Tn X T1 HIERO?based
Reordering
Parse?based
Reordering
S1
S2
S3
Sn
S1
S1
S2
Sn?1
Production
 
Figure 1.  Reordering module example 
62
 Each reordering module has its own resources 
and parameters which are shown on the left side.  
Each reordering module produces a vector of 
next source positions.  The production module 
takes these positions and produces translations 
for them.  
3  Reordering Modules 
In this section, we describe four reordering 
modules implementing different reordering 
strategies.  The framework is not limited to these 
four methods.  We present these four to 
demonstrate the ability of the framework to 
incorporate a wide variety of reordering methods. 
 
3.1 Distance-based Skip Reordering 
 
This is the type of reordering first presented by 
(Brown et.al. 1993) and was briefly alluded to in 
the above Introduction section.  This method is 
controlled by 2 parameters: 
Skip = number of words whose 
translations are to be delayed.  Let us call these 
words skipped words. 
WindowWidth (ww) = maximum 
number of words allowed to be translated before 
translating the skipped words. 
 
This reordering module outputs all the possible 
next source words to be translated according to 
these two parameters.  For illustration purposes, 
let us use a bit vector B to represent which source 
words have been translated.  Thus those that have 
been translated have value 1 in the bit vector, and 
those un-translated have 0.   As an example, let 
skip = 2 and ww = 3, and an input sentence of 
length = 10.  Initially, all 10 entries of B are 0.  At 
the first time step, only the following are possible 
next positions: 
a) 1 0 0 0 0 0 0 0 0 0 :  translate 1st word 
b) 0 1 0 0 0 0 0 0 0 0 :  skip 1st word 
c) 0 0 1 0 0 0 0 0 0 0 :  skip 1st and 2nd words 
 
At the next time step,  if  we want to continue the 
path of c),  we have these choices: 
1) we can leave the first 2 words open and 
continue until we reach 3 words (because ww=3) 
c1) 0 0 1 1 0 0 0 0 0 0  
c2) 0 0 1 1 1 0 0 0 0 0 
2) or we can go back and translate either of the 
first 2 skipped words: 
 c3) 1 0 1 0 0 0 0 0 0 0 
 c4) 0 1 1 0 0 0 0 0 0 0 
 
It is clear that the search space easily blows up 
with large skip and window-width values.  
Therefore, a beam pruning step is performed after 
partial hypotheses are produced at every time 
step.   
 
3.2 HIERO Hierarchical Reordering 
 
In this section we show an example of how the 
Hiero decoding method (Chiang 2005) can be 
implemented as a reordering module in this 
framework.  This is not meant to show that our 
MT decoder is a synchronous CFG parser.  This 
is a conceptual demonstration of how the Hiero 
rules can be used in a reordering module to 
decide the source translation order and thus used 
in a traditional phrase-based decoder.  This 
module uses the Hiero rules to determine the next 
source segment to be translated.  The example is 
Chinese-English translation. Consider the 
following Chinese sentence (word position and 
English gloss are shown in parentheses): 
 
(1.Australia) (2. is)  (3. with) 
(4. North Korea) 	(5. have)  
(6. diplomatic 
relation)  (7. NULL)  (8. few)  (9. 
country) (10. one of) 
 
Suppose we have two following Hiero rules: 
 X ? Australia X  (1) 
 X  ? is one of X   (2) 
 
The left-hand-side of Hiero rules are source 
phrases and the right-hand-side is their English 
translation and the Xs are the non-terminals 
whose extent is determined by the source input 
against which the rules are tested for matching.  
A rule fires if its left-hand-side matches certain 
segments of the input. 
 
Given the above Chinese input and the two Hiero 
rules, the Hiero decoder as described in (Chiang 
2005) will produce a partial hypothesis 
?Australia is one of? by firing the two rules 
during parsing (see Chiang 2005 for decoding 
details).  We will show how to decode in the 
Hiero paradigm using the framework. 
63
  
The reordering module first decides a source 
segment based on rule (1).  Rule (1) generates a 
sequence of source segments in term of source 
ranges: <[1,1],[2,10]>.  This means the source 
segment spanning range [1,1] (word 1, 
/Australia) is to be translated first, and then the 
remaining segment spanning range [2,10] is to be 
translated next.  This is exactly what rule (1) 
dictates where  corresponds to source 
[1,1] in the reordering module?s output and the X 
is [2,10].  The range [1,1], after being given to the 
production module,  results in the production of a 
partial hypothesis where the target ?Australia? is 
produced.  The task now is to translate the next 
source range [2,10].  At this point,  the reordering 
module generates another source segment 
according to rule (2) where the left-hand-side ? 
X ? is matched against the input and three 
corresponding source ranges are found which are 
[2,2] (/is), [4,9] (X), and [10,10] (/one of).  
According to rule (2), this source sequence is to 
be translated in the order of [2,2] (is), [10,10] 
(one of), and then [4,9] (X).  Therefore the output 
of the reordering module at this stage is 
<[2,2],[10,10],[4,9]>.  This would then go on to 
be translated and results in a partial hypothesis to 
?Australia is one of?.  Thus ?Australia is one of? 
is a partial production which covers source 
segments [1,1] [2,2] and [10,10] in that order.  
Note that the source segments decoded so far are 
not contiguous and this is the effect of long-range 
reordering imposed by rule (2).  The next stage is 
<[4,9]> which is what the X in rule (2) 
corresponds to.  From here onwards, other rules 
will fire and the decoding sequence these rules 
dictate will be realized by the reordering module 
in the form of source ranges.  This process can 
also be viewed hierarchically in Figure 2. 
 
In Figure 2 the ranges (the bracketed numbers) 
are source segments and the leaves are English 
productions.  Initially we have the whole input 
sentence as one range [1,10].  According to rule 
(1), this initial range is refined to be 
<[1,1],[2,10]>,  the 2nd level in Figure 2.  The 
[2,10] is further refined by rule (2)  to generate  
the 3rd level ranges <[2,2],[10,10],[4,9]> and the 
process goes on.  Ranges that cannot be further 
refined go into the production module which 
 ...
[1,10]
[1,1]
Australia
[2,10]
[2,2] [10,10] [4,9]
is one of
 
 
Figure 2. Hiero-style decoding  
 
generates partial hypotheses which are the leaves 
in the figure.  In other words, the partial 
hypotheses are generated by traversing the tree in 
Figure 2 in a left-to-right depth-first fashion. 
 
3.3 Generalized Part-Of-Speech-based 
Reordering 
 
The aim of a generalized part-of-speech-based 
reordering method is to tackle the problem of 
long-range word movement.  Chinese is a 
pre-modification language in which the modifiers 
precede the head.  The following is an example 
with English gloss   in parentheses.  The 
prepositional modifier ?on the table'' follows the 
head ?the book'' in English (3.3b), but precedes it 
in Chinese (3.3a).  When the modifiers are long, 
word-based local reordering is inadequate to 
handle the movement. 
3.3a.  (table)  (on)  (NULL) (book) 
 3.3b.  the book on the table 
 
There have been several approaches to the 
problem some of which are mentioned in ?1.  
Compared to these methods, this approach is 
lightweight in that it requires only part-of-speech  
(POS) tagging on the source side. The idea is to 
capture general long-distance distortion 
phenomena by extracting reordering patterns 
using a mixture of words and part-of-speech tags 
on the source side.  The reordering patterns are 
extracted for every contiguously aligned source 
segment in the following form:  
source  sequence ? target sequence 
 
 
Both the source sequence and the target  
sequence are expressed using a combination of 
source words and POS tags.  The patterns are 
?generalized? not only because POS tags are used 
but also because variables or place-holders are 
64
 allowed.  Given a pair of source and target 
training sentences, their word alignments and 
POS tags on the source, we look for any 
contiguously aligned source segment and extract 
word reordering patterns around it.  Figure 3 
shows an example.   
 
Shown in Figure 3 are a pair of Chinese and 
English sentence, the Chinese POS tags and the 
word alignment indicated by the lines.  When 
multiple English words  are aligned to a single 
Chinese word, they are grouped by a rectangle for 
easy viewing.  Here we have a contiguously 
aligned source segment from position 3 to 8. 
Using the range notation, we say that source 
range [3,8] is aligned to target range [6, 14].  Let 
X denote the source  segment [3,8].   The source 
verb phrase (at positions 9 and 10) occur after X 
whereas the corresponding target verb phrase 
(target words 2,3, and 4) occur before the 
translation of X (which is target [6,14]). We thus 
extract the following pattern: 
  X V N ? V N  X       (1) 
 
where the left-hand side ? X V N? is the source 
word sequence and the right-hand side ?V N  X? 
is the target word sequence.   The X  in the pattern 
is meant to represent a variable, to be matched by 
a sequence of source words in the test data when 
this pattern fires during decoding.  Note that the 
pattern is a mixture of words and POS tags.  
Specifically, the word identity of the preposition 
 (position 2) is retained whereas the content 
words (the verb and the noun) are substituted by 
their POS tags.  This is because in general, for the 
reordering purpose the POS tags are good class 
representations for content words whereas 
different prepositions may have different word 
order patterns so that mapping them all to a single 
POS P masks the difference. Examples of 
patterns are shown in Table 1. 
 
In Chinese-English translation, the majority of 
the reorderings occur around verb modifiers 
(prepositions) and noun modifiers (usually 
around the Chinese part-of-speech DEG as in 
position 6).   Therefore we choose to extract only 
these 2 kinds of patterns that involve a 
preposition and/or a DEG.  In the example above, 
there are only 2 such patterns: 
    X V N ? V N  X              (1) 
X1 DEG X2 ? X2 DEG X1           (2)     
 
Figure 3. Chinese/English Alignment Example 
 
 
 
 Source Seq. Target Seq. Count P(tseq 
|sseq) 
1 X DEG NN X DEG NN 861 0.198 
2 X DEG NN X NN DEG 1322 0.305 
3 X DEG NN NN DEG X 2070 0.477 
4 X DEG NN NN X DEG 10 0.002 
5 X DEG NN DEG NN X 52 0.012 
6 X DEG NN DEG X NN 22 0.005 
7  X VV  X VV 15 0.118 
8  X VV VV  X 112 0.882 
9 X VV VV  X 2 0.041 
10 X VV X VV 47 0.959 
 
Table 1. Pattern examples 
 
 
In the table, we see that when the preposition is 
  (rows 7 and 8, translation: by), then the 
swapping is more likely (0.882 in row 8).  When 
the preposition is  (rows 9 and 10 translation: 
because), then the target most often stays the 
same order as the source (prob 0.959, last row). 
 
3.4 Parse-based Lexicalized Reordering 
  
Part-of-speech reordering patterns as described in 
?3.3 are crude approximation to the structure of 
the source sentence.  For example, in the source 
pattern ?X DEG NN?, the variable X can match a 
source segment of arbitrary length which is 
followed by ?DEG NN?.  Although it does 
capture very long range movement as a result of 
SrcPOS  Source              Target 
 
1.NNP 