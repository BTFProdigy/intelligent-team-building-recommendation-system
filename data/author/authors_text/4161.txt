Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
 
		 		Multilingual Summary Generation in a Speech-To-Speech 
Translation System for Multilingual Dialogues* 
J an  A lexandersson ,  Peter  Po l le r ,  M ichae l  K ipp ,  Ra l f  Enge l  
DFK I  GmbH 
Stuh lsatzenhausweg 3 
66123 Saarbr f i cken  
{alexanders son, poller, engel, kipp}@dfki, de 
Abst rac t  
This paper describes a novel functionality of the 
VERBMOBIL system, a large scale translation sys- 
tem designed for spontaneously spoken multilingual 
negotiation dialogues. The task is the on-demand 
generation of dialogue scripts and result summaries 
of dialogues. We focus on summary generation and 
show how the relevant data are selected from the 
dialogue memory and how they are packed into 
an appropriate abstract representation. Finally, we 
demonstrate how the existing generation module of 
VERBMOBIL was extended to produce multilingual 
and result summaries from these representations. 
1 I n t roduct ion  
In the last couple of years different methods for 
summarization have been developed. In this pa- 
per we report on a new system functionality within 
the scope of VERBMOBIL (Bub et al, 1997), a fully 
implemented speech-to-speech translation system, 
that generates German or English dialogue scripts 
(Alexandersson and Poller, 1998) as well as Ger- 
man or English summaries of a multilingual nego- 
tiation dialogue held with assistance of the system. 
By a script we mean a document hat reflects the 
domain-specific propositional contents of the indi- 
vidual turns of a dialogue as a whole, while a sum- 
mary gives a compact summarization of all negotia- 
tions the dialogue participants agreed on. 
The key idea behind our approach is to utilize 
as many existing resources as possible. Conceptu- 
ally we have added one module (although techni- 
cally realized in different already existing modules 
of the overall VERBMOBIL system) - the summary 
generator. Besides formatting, our new module gen- 
erates sequences of language specific (i.e., German) 
semantic representations for thegeneration of Sam: 
maries/seripts based on the content of the dialogue 
memory (Kipp et al, 1999). These descriptions are 
? The research within VERBMOBIL presented here is funded 
by the German Ministry of Research and Technology under 
grant 011V101K/1. The authors would like to thank Tilman 
Becker for comments on earlier drafts on this paper, and 
Stephan Lesch for invaluable help with programming. 
realized into text by the existing VERBMOBIL gen- 
erator (Becker et al, 1998). To produce multilingual 
summaries we utilize the transfer module of VERS- 
MOBIL (Dorna and Emele, 1996). 
The next section gives an overview of the VERB- 
MOBIL system focusing on the modules central for 
the production of summaries/scripts. It is followed 
by a section describing the extraction and mainte- 
nance of summary relevant data. We then describe 
the functionality of the summary generator in detail. 
An excerpt of the sample dialogue we refer to in the 
paper is given at the end of the paper. 
2 P rerequ is i tes  
VERBMOBIL is a speech-to-speech translation 
project, which at present is approaching its end and 
in which over 100 researchers 1 at academic and in- 
dustrial sites are developing a translation system 
for multilingual negotiation dialogues (held face to 
face or via telephone) using English, German, and 
Japanese. The main difference between VERBMO- 
BIL and, c.f., man-machine dialogue systems is that 
VERBMOBIL mediates the dialogue instead of con- 
trolling it. Consequently, the complete dialogue 
structure as well as almost the complete macro- 
planning is out of the system's control. 
The running system of today is complex, consist- 
ing of more than 75 separate modules. About one 
third of them concerns linguistic processing and the 
rest serves technical purposes. (For more informa- 
tion see for instance (Bub et al, 1997)). For the sake 
of this paper we concentrate on a small part of the 
system as shown in figure 1. 
A user contribution is called a turn which is di -  
vided into segments. A segment ideally resembles 
a complete sentence as we know it from traditional 
grammars,  However; because :of -the. spontaneity of 
the user input and because the turn is chunked by 
a statistical process, the input segments for the lin- 
guistic components are sometimes merely pieces of 
linguistic material. For the dialogue memory and 
one of the shallow translation components the dia- 
lSee http://verbmobil.dfki.de for the list of project 
partners. 
148 
Data  - ~ 
Figure 1: Part of the VERBMOBIL system 
logue act (Alexandersson et al, 1998) plays an im- 
portant role. The dialogue act represents the com- 
municative function of an utterance, which is an im- 
portant information for the translation as well as the 
modeling of the dialogue as a whole. Examples of il- 
locutionary acts are REQUEST and GREET. Other 
acts can carry propositional content, like SUGGEST 
and INFORM_FEATURE. 
To obtain a good translation and enhance the 
robustness of the overall system the translation is 
based on several competing translation tracks, each 
based on different paradigms. The deep translation 
track consists of an HPSG based analysis, semantic 
transfer and finally a TAG-based generator (VM- 
GECO). The linguistic information within this track 
is encoded in a so-called VIT 2 (Bos et al, 1996; 
Dorna, 1996) which is a formalism following DRT. 
It consists of a set of semantic onditions (i.e. predi- 
cates, roles, operators and quantifiers) and allows for 
underspecification with respect o scope and subor- 
dination or inherent underspecification. A graphical 
representation of the VIT for the English sentence 
"They will meet at the station" is shown in figure 2. 
Besides the deep translation track several shallow 
tracks have been developed. The main source of 
input for the generation of summaries comes from 
one of these shallow analysis components (described 
in section 3) which produces dialogue acts, topic 
suggestions and expressions in a new knowledge 
representation language called DIREX 3. These ex- 
pressions represent domain related information like 
source and destination-o!ties~ dates;-important hotel 
related data, and meeting points. This input is pro- 
cessed by the dialogue module which computes the 
relevant (accepted) objects of the negotiation (each 
consisting of dialogue act, topic, and a DIREX) 
Figure 3 shows the conceptual architecture, where 
2Verbmobil Interface Term 
aDomaln Represematioa EXpression 
. J.d.=C,.i;,hi3, h2) 
B Z I  ... II, ' " 
Figure 2: Graphical representation f VIT for "They 
will meet at the station" 
the summary generation process as a whole is indi- 
cated with thicker lines. It consists of the following 
steps: 
o Content  Select ion:  The relevant structures are 
selected from the dialogue memory. 
. ..o .Summary~ Generat ion :  These- Structures are 
converted into sequences of semantic descriptions 
(VITs) of full sentences for German (see section 4). 
o Transfer :  Depending on the target language, the 
German sentence VITs are sent through the transfer 
module. 
* Sentence Generat ion :  The VITs are generated 
by the existing VERBMOBIL generator (Becker et al, 
149 
Figure 3: Conceptual Architecture of the Summary Generation Process 
2000). . . . .  
? Presentat ion :  The sentences are incorporated 
into the final, e.g., HTML document. 
Throughout he paper we will refer to a German- 
English dialogue (see appendix for an excerpt). 
The information presented there is the spoken sen- 
tence(s) together with the information extracted as 
described in section 3. To save space we only present 
parts of it, namely those which give rise to the struc- 
tures in figure 4. 
3 Ext rac t ion  and  Ma intenance  o f  
Protocol Relevant Data 
The dialogue memory gets its input from one of 
the shallow translation components, which bases 
its translation on the dialogue act and Dll:tEX- 
expression extracted from the segment. The input 
is a triple consisting of: 
? D ia logue  Act  representing the intention of the 
segment. 
? Topic is one of the four topics scheduling, travel- 
ing, accommodation and entertainment. 
? D i rex  representing the propositional content of 
the segment. 
For the extraction of propositional content and in- 
tention we use a combination of knowledge based 
and statistical methods. To compute the propo- 
sitional content finite state transducers (FSTs) 
(Appelt et al, 1993) with built-in functions are 
used (Kipp et al, 1999). The intention (represented 
by a dialogue act) is computed statistically us- 
ing language models (Reithinger and Klesen, 1997). 
Both methods were chosen because of their robust- 
ness - since the speech recognizers have a word error 
rate of about 20%, we cannot expect sound input 
for the analysis. Also the segmentation of turns in 
utterances i stochastic and therefore sometimes de- 
livers suboptimal segments. Consider the input to 
be processed: 
I would  so we were  to leave Hamburg  on the 
f i rs t  
where the speech recognizer eplaced "good so we 
will" with "I would so we were to". The result of 
the extraction module looks like: 
..... """ "\[ITNFORMTtravel ing, he~s_move : \ [move, 
has_source_ locat  ion : \ [c ity,  has_name = 
' hamburg  ' \] , has_depar ture_ t ime : 
\ [date,  t ime= \ [day : i\] \] \] \] 
The result consists of the dialogue act INFORM, 
the topic suggestion t rave l ing ,  and and a DIREX. 
The top object is a move with two roles: A source 
location (which is a city - Hanover), and a departure 
time (which is a date - day 1). 
Dialog processing 
For each utterance, and hence each DIREX the di- 
alogue manager (1) estimates its relevance, and (2) 
enriches it with context. For summary generation, 
we are solely interested in the most specific, accepted 
objects. Therefore, we also (3) compute more spe- 
cific~general relations between objects: 
Relevance detection. Depending on the dialogue act 
of the current utterance different courses of action 
are taken. SUGGEST dialogue acts trigger the stor- 
age, completion, focusing and inter-object relation 
(see below) computation for the current structure. 
ACCEPT and REJECT acts let the system mark the 
focused object accepted/rejected. 
Object Completion. Suggestions in negotiation dia- 
logues are incomplete most of the time. E.g., the 
utterance "I would prefer to leave at five" is a sug- 
gestion referring to the departure time for a trip 
from Munich to Hanover on the 19. Jan. 2000 (see 
turn 1005 in the appendix). Most of the complete 
data has been mentioned in the preceding dialogue. 
Our completion algorithm uses the focused object 
(itself a completed suggestion) to complete the cur- 
rent structure. All non-conflicting information of tile 
focused object is copied onto the new object.  In our 
example the current temporal information "I would 
prefer to leave at five" would be completed with date 
(i.e., "19. Jan. 2000'" ) and other travel data ( " t r ip  
from-Munich to Hanover" ) .  Afterwards, it Will be  
put to focus. 
Object Relations. The processing results in a number 
of accepted and rejected objects. Normally, a nego- 
tiation produces a series of suggestions that become 
more specific over time. For each new object we cal- 
culate the relation to all other suggestions it\] terms 
of more specific/general or equal. A final inference 
150 
procedure then filters redundant objects and pro- representation to a semantic description (VIT) for 
duces a list of accepted objects with highest speci . . . . . .  each sentence (suitable foz.further processing by the 
ficity. Figure 4 shows two such objects extracted 
from the sample dialogue. Both structures have been 
completed from context data including situational 
data, i.e., current time and place of the negotiation. 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
Topic SCHEDULING 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
relations: 
( (MDRE_SPECIF IC_THAN.#~APPOINTMENT P2*>)) 
APPOINTMENT (Ph*+0) 
HAS_LOCATION --> CITY (P4*) 
HAS_NAME="hannover" 
HAS_MEETING --> MEETING (P3**) 
HAS_NAME="ges chae ft st re f fen" 
HAS_DATE --> DATE (Ph*) 
TEMPEX= \[year : 2000, 
month: j an, 
day : 20, 
part :am, 
time: ii :0\] 
relations : 
((MOKE_SPECIFIC_THAN . #<APPOINTMENT P26.>) 
(MORE_SPECIFIC_THAN . #<APPOINTMENT P30**+0>)) 
APPOINTMENT (P29.+0) 
HAS_LOCATION --> NONGEO_LOCATION (P30***) 
HAS_NAME="b~hnhof" 
HAS_DATE --> DATE (P29") 
TEMPEX=\[year:2000, 
month:jan, 
day:lg, 
time:9:30\] 
Figure 4: The scheduling part of the thematic struc- 
ture 
4 Generat ing  Summar ies  
Our system uses many of tim existing components 
of VERB~'IOBIL. However, we had to develop a new 
component, the summary generator, which is de- 
scribed below. It solves the task of mapping the 
DIREX structures elected in the dialogue nmmory 
into sequences of full fledged semant.ic sentence de- 
scriptions (VITs), thereby performing the following 
steps: 
* Document  P lann ing :  Extracting, preparing 
and dividing the content of the dialogue memory into 
a predefined format. -This includes, c.f., time/place 
of negotiation, participants, result of the negotia- 
tion. 
o Sentence  P lann ing :  Splitting the input into 
chunks suitable for a sentence. This process in- 
voh'es choosing an appropriate verb and arranging 
the parts of the chunk as arguments and/or a(l- 
.iuncts. The final step is the mapping of this internal 
existing VERBMOBIL components). 
? Generat ion :  Verbalizing the VITs by the exist- 
ing multilingual generator of VERBMOBIL. 
? Presentat ion :  Formatting of the complete doc- 
ument content o an, e.g., HTML-page. Finally, the 
document is displayed by an appropriate browser. 
Our approach as been mostly guided by robust- 
ness: our representation language (DIREX) was co- 
developed uring the course of the project. More- 
over, as the extraction component increased its vo: 
cabulary, we wanted to be able to generate new in- 
formation which had not been seen before. Hence 
we needed an approach which is fault tolerant. In- 
stead of failing when the representation changes or 
new type of objects were introduced we degrade in 
precision. Our two step approach as proven its use- 
fulness for this. 
4.1 Document  P lann ing  
The document itself contains two main parts. The 
top of the document includes general informa- 
tion about the dialogue (place, date, participants, 
theme). The body of the document contains the 
summary part which is divided into four paragraphs, 
each of them verbalizing the agreements for one ne- 
gotiation topic: scheduling, accommodation, travel- 
ing and entertainment. Therefore, our document 
planning is very straightforward. The four elements 
of the top document are processed in the following 
manner: 
o Place and Date: For place and date the informa- 
tion is simply retrieved from the dialogue memory. 
? Participants: The participants information are 
transformed into a VIT by the plan processor de- 
scribed below. In the absence of name/title infor- 
mation, a character, e.g., h, B, .. ? is used. 
? Theme: By a shallow examination of the result of 
the content extraction, a semantic description corre- 
sponding to a noun phrase mirroring the content of 
the document as a whole is construed. An example 
is Bus iness  tr ip  w i th  accommodat ion.  
? The summary." Finally, the summary relevant D1- 
REX objects are retrieved from the dialogue men> 
ory: First we compute the most specific suggestions 
by using the most specific/general nd equal rela- 
tions. The remaining suggestions are partitioned 
into equivalence classes which are filtered by com- 
puting the degree of acceptance. In case of conflict 
the most recent one is taken. The resulting set is par- 
titioned into the above mentioned topics the)' belong 
to. Finally these are processed by the plan processor 
as described below. 
4.2 Sentence P lann ing  
We now turn into the process of mapping the inter- 
esting part of the dialogue memory onto sequences 
151 
of VITs. An example of the content of one topic - 
scheduling - was shown in figure 4. O.ur two step 
approach consists of: 
* A p lan processor  whose task it is to split the 
objects selected into chunks suitable for a sentence. 
Possibly it contributes to the selection of verbs. 
o A semant ic  onst ructor  whose task it is to con- 
vert the output of the plan processor into full fledged 
semantic descriptions (VITs) for the sentences of the 
document. This second step can be viewed as a ro- 
bust fall-back: If the plan processor does not succeed 
in obtaining full Specifications of all sentence parts, 
this step secures a valid and complete specification. 
4.2.1 The  plan processor  
Input to the plan processor (Alexandersson and Rei- 
thinger, 1997) is the thematic structure partly shown 
in figure 4. The plan processor interprets (currently 
about 150) plan operators which are expanded in a 
top-down left to right fashion. 
For the overall structure of the text, the imposed 
topic structure of the thematic structure is kept. 
Within a topic we use a set of operators which are ca- 
pable of realizing (parts of) the structures to NPs, 
PPs and possibly verb information forming a high 
level specification of a sentence. 
P lan  operators  
A plan operator consists of a goal which is option- 
ally divided into subgoal(s). Its syntax contains the 
keywords :const ra in ts  and  :ac t ions  which can 
be any Lisp expression. Variables are indicated with 
question/exclamation marks (see figures 5 and 6). 
The goal of the operators uses an interface based 
on a triple with the following usage: 
o <descr ip t ion> This is the input position of the 
operator. It describes and binds the object which 
will be processed by this operator. 
o <context> This is the context - input/output. 
The context contains a stack for objects in focus, 
handled as described in (Grosz and Sidner, 1986). 
Additionally we put the generated information on a 
history list (Dale, 1995). The context supports the 
generation of, e.g., pronouns (see below). At present 
the context is only used local to each topic. 
o <output> The result of the operator. Tile possible 
output types are NP, PP and sentence(s). 
We the distinguish two types of operators; complex 
operators, responsible for complex objects, which 
can contain several roles, and simple operators, 
which can process imple objects (carrying only one 
role). The general design of a complex operator --see 
figure 5 for an operator esponsible for appointment 
objects - consists of three subgoals: 
o ( f ind - ro les  . . . )  Retrieve tile content of the 
object. "ghe operators responsible for soh'ing the 
f ind - ro les  goal optionally allow for an enumera- 
tion of the roles we want to use. 
e (sp l i t - ro les  . . . )  These  ro les (and values) will 
be partit ioned,into chunks, (which we, call a split) 
suitable for generating one sentence. 
? (generate -sp l i t s  . . . )  Finally the output - a 
sentence description - will be constructed. 
(defplan appointment 
:goal ((class (Vapp scheduling)) 
(?in-context ?out-context) 
?sentence) 
:constraints (appointment-p !app) 
:subgoals (:sequence 
(find-roles ?appZrels) 
(split-roles ?rels 
appointment ?l-of-splits) 
(generate-splits ?l-of-splits 
(Via-context ?out-context) 
appointment ?sentence))) 
Figure 5: An example of an operator for a "complex" 
object 
Behind the functionality of the sp l i t - ro les  goal 
we use pairs of operators (figure 6), where the first is 
a fact describing the roles of the split, and the second 
is a description for how to realize the sentence. In 
this example the selection of an appropriate verb is 
not performed by this operator but by the semantic 
constructor. 
The second type of operators are simple operators 
like the one for the generation of time expressions 
(tempex) or cities (see figure 4). 
Figure 7 shows a simplified plan processor output 
(building block) for one sentence. 
4.2.2 The Semant ic  Const ructor  
The task of the semantic onstructor is to map the 
information about sentences computed by the plan 
processor to full semantic representations (VITs). 
The knowledge source for this computational step 
is a declarative set of about 160 different semanti- 
cally oriented sentence patterns which are encoded 
in an easily extendable semantic/syntactic descrip- 
tion language. 
To obtain a complete semantic representation for 
a sentence we first select a sentence pattern. This 
pattern is then, together with tile output of the plan 
processor, interpreted to produce the VIT. The se- 
lection criteria for a sentence pattern are: 
All patterns are ordered topic-wise because 
the appropriateness of sentence patterns is topic- 
dependent (e.g., the insertion of topic-specific NPs 
or PPs into a sentence). 
-+ The int.entional state of the inforination to 
be verbalized highly restricts the set of appropriate 
verbs. 
Depending on the propositional content de- 
scribed within a DIat-:x-VIT - i.e., a VIT repre- 
senting one sentence part in a building block of the 
152 
; ;  - Das <Treffen> finder i n  <City> 
;;  am <tempex> statt 
? ;; - The <Meeting>takes place 
;; in <City> on the <tempex> 
(deffact sentence-split 
:goal (sentence-split 
((has_meeting ?has_name) 
(has_location ?has_location) 
(has_date ?has_date)) 
?_topic)) 
(defplan generate-split 
:goal (generate-split 
((has_meeting ?nmme) ......... ;;:meeting 
(has_location ?location) ;; city 
(has_date ?date)) ;; tempex 
(?in-context ?out-context) 
?topic 
?s) 
:subgoals 
(:seq ((class (?location ?scheduling pp)) 
7topic ?loc-pp) 
((class (?name ?scheduling)) 
?topic ?s-topic) 
(generate-full-tempex ?date ?tempex) 
(((generate-sentence decl) 
(subj ?topic has_topic) 
(obj ?l-pp has_location) 
(obj-add ?tempex has_date)) 
?in-context ?out-context ?s))) 
Figure 6: Example of sentence definition and gener- 
ation 
(ACCOMMODATION 
(ACCEPTED 
(HAS_SIZE VIT: <Einzelzimmer>) 
(HAS_PRICE VIT: <80-Euro-pro-Nacht>) 
)) 
Figure 7: Exmnple of a plan processor output 
plan processor output - it has to play different se- 
mantic roles in the sentence (e.g., verb-argument vs. 
verb-complement) 
Additionally, the number of DtREx-VITs given 
within a building block for a sentence, influences the 
distribution of them to appropriate semantic roles. 
Figure 8 shows a simplified sentence pattern that 
is selected for the building block in figure 7 to con- 
struct a VIT for, e.g., the German sentence Das 
Einzelzimmer kostet 80 Euro pro Nacht. ("The sin- 
gle room costs 80 euro per night."). According 
(( : verb kosten_v) 
( :subj  HAS_SIZE) 
(: obj HAS_PRICE) 
( : res t  DIREX_PPS)) 
Figure 8: Example of a sentence pattern 
to the above mentioned selection criteria, this pat- 
tern is selected only for building blocks within 
. ...the.~ accommodation:topi.c~ that-contain, a t  least ,val- 
ues for the roles HAS.SIZE and HAS.PRIZE, respec- 
tively. The sentence pattern contains the following 
"building instructions": The semantic verb predi- 
cate ( :verb) is kosten_v (to cost), its subject ar- 
gument ( :subj)  is to be filled by the DIREX-VIT 
associated to the DmEx-role HAS.SIZE while :obj 
means a similar instruction for the direct object. 
The robustness fallback ( : res t  DIREX._PPS) means 
.that.all_other DmEx=VITs are attached to the verb  
as PP  complement?. It i spah  ~/f a\]l 'Sen~df/6+ p i t -  
terns to ensure that even erroneous building blocks 
or erroneously selected sentence patterns produce a 
sentence VIT. 
Finally, the VIT is constructed by interpreting the 
sentence pattern. The interpreter walks through the 
sentence pattern and performs different actions de- 
pending on the keywords, e.g., :verb,  :subj and 
their values. 
4.2.3 Util izing Context 
During'the course of the generation, the plan proces- 
sor incrementally constructs a context (Dale, 1995), 
which allows for the generation of, c.f., anaphora or 
demonstratives for making the text fluent or con- 
trasting purposes. 
? Anaphora  If, e.g., a meeting is split into 
more than one sentence, the plan processor uses an 
anaphora to the meeting in the second sentence. 
? D iscourse Markers  In case of multiple, e.g., 
meetings we introduce the second with a discourse 
marker, e.g., "also". 
o Demonst ra t ives  In case of multiple meetings, we 
use a demonstrative to refer to the second meeting. 
In addition to the plan processor, the seman- 
tic constructor also takes care of coherence within 
the paragraphs produced for the individual topics 
hereby focusing on the generation of anaphora nd 
adverbial discourse markers. While the local con- 
text of the plan processor is based on the proposi- 
tional content at hand, the semantic onstructor uses 
a postprocessing module that is based oil the output  
\qTs  of the plan processor (DIREx-VITs) using its 
own semantically oriented local context memory. 
Anaphorization and insertion of discourse mark- 
ers within the semantic onstructor are based on a 
comparison of plan processor output VITs occur- 
ring within consecutive sentences of a paragraph. 
Identical verb arguments (NPs) in consecutive sen- 
., tences are replaced by .appropriate anaphoric pro- 
nouns while identical verbs themselves lead to the in- 
sertion of an appropriate adverbial discourse marker. 
5 Mu l t i l i ngua l i ty  
The generation of dialogue scripts and result sum- 
maries is fully implemented in VERB~VIoBIL for Ger- 
man and English. For the English smnmaries we 
153 
extracted, then the transfer module produces equiv- 
alent English VITs which are finally sent to the En- 
glish generation component for producing the En- 
glish text. 
Figure 9 shows the English result summary of the 
dialogue shown in the appendix. 
make use of the transfer component as follows. All o TN A feature was not part of the dialogue, and 
VITs from the German-document representation are . not included in. the..summary. 
The evaluation result is shown in figure 10. It uses 
the standard precision, recall and fallout as defined 
in (Mani et.al., 1998). 
Dialogue 1 2 3 4 aver 
Turns 33 33 31 32 32.25 
Corr 6 13 9 11 9.75 
Miss 6 3 5 4 4.5 
False 3 3 3 0 2.25 I 
TN 32 28 30 32 30.5 I 
Recall 0.5---0- 0.8-'--1- 0.6----4-- 0 .7 - ' - -3 - -~ 
10 I 
Fallout i 0.0__9 0.1___0_ 0.0____9_ _0"00 
Figure 10: Evaluation Results 
Figure 9: Example of an English result summary 
6 Eva luat ion  
We have performed a small evaluation of the overall 
system as described in this paper. Basis for the eval- 
uation were the transcripts of four German-English 
negotiation dialogues. For each dialogue the result- 
ing features of the negotiation (maximally 47, e.g., 
location, date for a meeting, speakers name and title, 
book agent) were annotated by a lmman, and then 
compared with the result of running the dialogues 
through the system and generating the summaries. 
The features in the summary were compared using 
the following classifications: 
? Cor r  The feature approximately corresponds to 
the human annotation. This means that the feature 
is either (1) a 100% match; (2) it was not sufficiently 
specified or (2) too specific. An example of (2) is 
when the correct date included a time, which was 
not captured. An example of (3) is when a date 
with time was annotated but the feature contained 
just a (late. 
o Miss A feature is not included in the summary. 
o False A feature was erroneously iimluded in the 
sumlnary, meaning that the feature was not part of 
the dialogue or it received a wrong value. 
Obviously, our approach tries to be on the safe 
side; the summary contains only those features that 
the system thinks both partners agreed on. The 
main reasons for not getting higher numbers is 
twofold. The recognition of dialogue acts, and thus 
the recognition of the intension behind the utter- 
ances reaches a 70% recall (Reithinger and Klesen, 
1997). We also still make errors during the content 
extraction. 
7 Conc lus ion  
We have presented an extension to existing modules 
allowing for the generation of summaries within the 
VERBMOBIL system. To our knowledge our system 
is the only one that uses semantic representation as
basis for summarizing. Other approaches use, e.g., 
statistical techniques or rhetorical parsing (Waibel 
et al, 1998; Hovy and Marcu, 1998) to obtain the 
summaries. Moreover, although our module is re- 
stricted to language specific processing, the use of 
semantics and the transfer module allow for the gen- 
eration of multilingual documents in a very straight- 
forward fashion. 
In the near future we will extend the system with 
respect o: 
o Sentence  Spl it  At present the first found sen- 
tence split is chosen. This is not necessarily the op- 
timal one. We are currently in the process of devel- 
oping criteria for ranking competing results. 
o Japanese  The VERBMOBIL system currently in- 
cludes German, English and Japanese. We intend 
to apply the same technique as for the English sum- 
maries to generate Japanese ones. 
References 
J. Alexandersson and P. Poller. 1998. Towards multilin- 
~oual protocol generation for spontaneous speech dia- 
gues. In Probeedings of INLG-98, Niagara-On-The- 
Lake. Ontario. Canada. 
154 
J. Alexandersson and N. Reithinger. 1997. Learning di- 
alogue structures from a corpus. In Proceedings of 
? EufoSpeech-97; pages' 2231-2235," Rhodes. 
Jan Alexandersson, Bianka Buschbeck-Wolf, Tsutomu 
Fujinami, Michael Kipp, 'S tephan Koch, Elisa- 
beth Maier, Norbert P~eithinger, Birte Schmitz, 
and Melanie Siegel. 1998. Dialogue Acts in 
VERBMOBIL-2 - Second Edition. Vergmobil-Report 
226, DFKI  Saarbrficken, Universitgt Stuttgart, Tech- 
nische Universit/it Berlin, Universit/it des Saarlandes. 
D. Appelt, J. Hobbs, J. Bear, and M. Tyson. 1993. FAS- 
TUS: A finite-state processor for information extrac- 
tion from real-world text. In IJCAL93. 
T. Becker, W. Finkler, A. Kilger, and P. Poller. 1998. An 
efficient kernel for multilingual generation in speech- 
to--speech dialogue -translation-.- In :Proceediiigs of 
COLING/ACL-98, Montreal, Quebec, Canada. 
T. Becket, A. Kilger, P. Lopez, and P. Poller. 2000. Mul- 
tilingual generation for translation in speech-to-speech 
dialoga.les and its realization in verbmobil. In Proceed- 
ings of ECAI-2000, Berlin, Germany. 
J. Bos, B. Gamb/ick, C. Lieske, Y. Mori, M. Pinkal, and 
K. Worm. 1996. Compositional semantics in verbmo- 
bil. In Proceedings of Coling '96, Copenhagen, Den- 
mark. 
T. Bub, W. Wahlster, and A. Waibel. 1997. Verbmo- 
bih The combination of deep and shallow processing 
for spontaneous speech translation. In Proceedings dr/ 
ICASSP-97, pages 71-74, Munich. 
R. Dale. 1995. An introduction to natural lan- 
guage generation. Technical report, Microsoft 
Research Institute (MRI), Macquarie Univer- 
sity. Presented at the 1995 European Summer 
School on Logic, Language and Information, Avail- 
able from http://www.mri.mq.edu.au/-rdale/nlg- 
textbook/ESSLLI95/. 
M. Dorna and M. Emele. 1996. Efficient Implementation 
of a Semantic-Based Transfer Approach. In Proceed- 
ings of ECAI-96, pages 567-571, Budapest, Hungary, 
August. 
M. Dorna. 1996. The ADT-Package for the VERBMOBIL 
Interface Term. Verbmobil Report 104, IMS, Univer- 
sit/it Stuttgart, Germany. 
B. Grosz and C. Sidner. 1986. Attention. Intentions and 
the Structure of Discourse. Journal o~ Computational 
Linguistics, 12(3). 
E. Hovy and D. Marcu. 1998. Coling/acl-98 tu- 
torial on automated text summarization. Avail- 
able from http://w~v.isi.edu/-marcu/coling-ac198- 
tutorial.html. 
M. Kipp, J. Alexandersson, and N. Reithinger. 1999. 
Understanding Spontaneous Negotiation Dialogue. In 
Workshop Proceedings 'Knowledge And Reasoning in 
Practica\[Dialogue Systems' of TJCAI '99, pages 57- 
64. 
I. Mani, D. House, G. Klein, L. Hirschman, L. 
Obrist, T. Firmin. M. Chrzanowski, and B. 
Sundheim. 1998. The tipster summac text sum- 
marization evaluation - final report. Technical 
reports The Mitre Corp. Available from http://www- 
24.nist.gov/related_projects/tipster_summac/finalxpt- 
.html. 
N. Reithinger and M. Klesen. 1997. Dialogue Act Clas- 
sification Using Language Models. In Proceedings of 
EuroSpeech-97, pages 2235-2238, Rhodes. 
A. Waibel, M. Bett, M. Finke, and R Stiefelhagen. 1998. 
Meeting Browser: Tracking and Summarizing Meet- 
ings. In Proceedings of the DARPA Broadcast News 
Workshop. 
Appendix 
Excerpt from our sample dialogue. 
\[...\] 
1002 
- j a  es  geht um das Geschftstreffen in 
Hannover ~lit.: Yes i t  is about the business 
meeting in Hanover) 
\[INIT,scheduling,has_appointment: 
\[appointment,has_meeting:\[meeting, 
has_name='geschaeftstreffen'\], 
has_location:\[city,has_name='hannover ' , 
has_loc_spec=in,has_det=nnknown\]\]\] 
- das  i s t  j a  am zwanzigsten Januar um elf 
Uhr vormittags 
\[SUGGEST,uncertain_scheduling,has_date: 
..\[date,tempex='.(ge_2920_O,\[from: 
\[dom:20,month:jan,tod:11:0, 
pod:morning_ger2\]\])'\]\] 
1003 
- so we have to leave Munich at six o'clock 
\[SUGGEST,traveling,has_move:\[move, 
has_source_location:\[city,has_name 
='muenchen'\],has_departure_time:\[date, 
tempex='(en_2920_O,\[from:tod:6:0\])'\]\]\] 
1004 
- vielleicht fahren wir lieber den Tag davor 
(lit.: maybe we better leave the day before) 
\[SUGGEST,traveling,has_move:\[move, 
has_departure_time:\[date,tempex = 
'(ge_2920_l,\[from: 
neg_shift(dur(l,days),ana_point)\])'\]\]\] 
- da gibt es einen Zug um zwei Uhr 
(lit.: there is a train at two o'clock) 
\[SUGGEST,traveling,has_move:\[move,has- 
_transportation:\[rail\],has_departure_time: 
\[date,tempex='(ge_2920_2,\[from:tod:2:0\])'\]\]\] 
1005 
I would prefer to leave at five 
\[SUGGEST,traveling,has_move:\[move, 
has_agent:\[speaker\],has_departure_time: 
\[date,tempex='(en_2920_l,\[from:tod:5:0\])'\]\]\] 
\[...\] 
I011 
- let us meet at the station on Wednesday 
\[SUGGEST,scheduling,has_appointment: 
\[appointment,has_location:\[nongeo_location, 
has_name='bahnhof',has_loc_spec=at, 
has_det=def\],has_date:\[date,tempex = 
'(en_2920_2,\[from:dow:wed\])'\]\]\] 
1012 
-um halb zehn am Bahnhof 
(lit.: at half past nine at the station) 
\[ACCEPT, uncert ain_s cheduling, has_date : \[date, 
tempex= ' (ge_2S20_3, \[fzom: rod : 9 : 30\] ) ' \] , 
has location: \[nongeo_location,has_name = 
' bahnhof ' \] \] 
\[...\] 
155 
