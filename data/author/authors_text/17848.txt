Proceedings of the EACL 2012 Student Research Workshop, pages 1?10,
Avignon, France, 26 April 2012. c?2012 Association for Computational Linguistics
Improving Pronoun Translation for Statistical Machine Translation
Liane Guillou
School of Informatics
University of Edinburgh
Edinburgh, UK, EH8 9AB
L.K.Guillou@sms.ed.ac.uk
Abstract
Machine Translation is a well?established
field, yet the majority of current systems
translate sentences in isolation, losing valu-
able contextual information from previ-
ously translated sentences in the discourse.
One important type of contextual informa-
tion concerns who or what a coreferring
pronoun corefers to (i.e., its antecedent).
Languages differ significantly in how they
achieve coreference, and awareness of an-
tecedents is important in choosing the cor-
rect pronoun. Disregarding a pronoun?s an-
tecedent in translation can lead to inappro-
priate coreferring forms in the target text,
seriously degrading a reader?s ability to un-
derstand it.
This work assesses the extent to which
source-language annotation of coreferring
pronouns can improve English?Czech Sta-
tistical Machine Translation (SMT). As
with previous attempts that use this method,
the results show little improvement. This
paper attempts to explain why and to pro-
vide insight into the factors affecting per-
formance.
1 Introduction
It is well-known that in many natural languages,
a pronoun that corefers must bear similar features
to its antecedent. These can include similar num-
ber, gender (morphological or referential), and/or
animacy. If a pronoun and its antecedent occur in
the same unit of translation (N-gram or syntactic
tree), these agreement features can influence the
translation. But this locality cannot be guaranteed
in either phrase-based or syntax-based Statistical
Machine Translation (SMT). If it is not within the
same unit, a coreferring pronoun will be trans-
lated without knowledge of its antecedent, mean-
ing that its translation will simply reflect local fre-
quency. Incorrectly translating a pronoun can re-
sult in readers/listeners identifying the wrong an-
tecedent, which can mislead or confuse them.
There have been two recent attempts to solve
this problem within the framework of phrase-
based SMT (Hardmeier & Federico, 2010; Le
Nagard & Koehn, 2010). Both involve anno-
tation projection, which in this context means
annotating coreferential pronouns in the source-
language with features derived from the transla-
tion of their aligned antecedents, and then build-
ing a translation model of the annotated forms.
When translating a coreferring pronoun in a new
source-language text, the antecedent is identified
and its translation used (differently in the two at-
tempts cited above) to annotate the pronoun prior
to translation.
The aim of this work was to better understand
why neither of the previous attempts achieved
more than a small improvement in translation
quality associated with coreferring pronouns.
Only by understanding this will it be possible to
ascertain whether the method of annotation pro-
jection is intrinsically flawed or the unexpectedly
small improvement is due to other factors.
Errors can arise when:
1. Deciding whether or not a third person pro-
noun corefers;
2. Identifying the pronoun antecedent;
3. Identifying the head of the antecedent, which
serves as the source of its features;
4. Aligning the source and target texts at the
phrase and word levels.
1
Factoring out the first two decisions would
show whether the lack of significant improvement
was simply due to imperfect coreference resolu-
tion. In order to control for these errors several
different manually annotated versions of the Penn
Wall Street Journal corpus were used, each pro-
viding different annotations over the same text.
The BBN Pronoun Coreference and Entity Type
corpus (Weischedel & Brunstein, 2005) was used
to provide coreference information in the source-
language and exclude non-referential pronouns.
It also formed the source-language side of the
parallel training corpus. The PCEDT 2.0 cor-
pus (Hajic? et al 2011), which contains a close
Czech translation of the Penn Wall Street Journal
corpus, provided reference translations for test-
ing and the target-language side of the parallel
corpus for training. To minimise (although not
completely eliminate) errors associated with an-
tecedent head identification (item 3 above), the
parse trees in the Penn Treebank 3.0 corpus (Mar-
cus et al 1999) were used. The gold stan-
dard annotation provided by these corpora al-
lowed me to assume perfect identification of core-
ferring pronouns and coreference resolution and
near?perfect antecedent head noun identification.
These assumptions could not be made if state-of-
the-art methods had been used as they cannot yet
achieve sufficiently high levels of accuracy.
The remainder of the paper is structured as fol-
lows. The use of pronominal coreference in En-
glish and Czech and the problem of anaphora res-
olution are described in Section 2. The works
of Le Nagard & Koehn (2010) and Hardmeier
& Federico (2010) are discussed in Section 3,
and the source-language annotation projection
method is described in Section 4. The results are
presented and discussed in Section 5 and future
work is outlined in Section 6.
2 Background
2.1 Anaphora Resolution
Anaphora resolution involves identifying the an-
tecedent of a referring expression, typically a pro-
noun or noun phrase that is used to refer to some-
thing previously mentioned in the discourse (its
antecedent). Where multiple referring expres-
sions refer to the same antecedent, they are said to
be coreferential. Anaphora resolution and the re-
lated task of coreference resolution have been the
subject of considerable research within Natural
Language Processing (NLP). Excellent surveys
are provided by Strube (2007) and Ng (2010).
Unresolved anaphora can add significant trans-
lation ambiguity, and their incorrect translation
can significantly decrease a reader?s ability to un-
derstand a text. Accurate coreference in trans-
lation is therefore necessary in order to produce
understandable and cohesive texts. This justifies
recent interest (Le Nagard & Koehn, 2010; Hard-
meier & Federico, 2010) and motivates the work
presented in this paper.
2.2 Pronominal Coreference in English
Whilst English makes some use of case, it lacks
the grammatical gender found in other languages.
For monolingual speakers, the relatively few dif-
ferent pronoun forms in English make sentences
easy to generate: Pronoun choice depends on the
number and gender of the entity to which they re-
fer. For example, when talking about ownership
of a book, English uses the pronouns ?his/her?
to refer to a book that belongs to a male/female
owner, and ?their? to refer to one with multi-
ple owners (irrespective of their gender). One
source of difficulty is that the pronoun ?it? has
both a coreferential and a pleonastic function. A
pleonastic pronoun is one that is not referential.
For example, in the sentence ?It is raining?, ?it?
does not corefer with anything. Coreference res-
olution algorithms must exclude such instances in
order to prevent the erroneous identification of an
antecedent when one does not exist.
2.3 Pronominal Coreference in Czech
Czech, like other Slavic languages, is highly in-
flective. It is also a free word order language, in
which word order reflects the information struc-
ture of the sentence within the current discourse.
Czech has seven cases and four grammatical gen-
ders: masculine animate (for people and animals),
masculine inanimate (for inanimate objects), fem-
inine and neuter. (With feminine and neuter gen-
ders, animacy is not grammatically marked.) In
Czech, a pronoun must agree in number, gender
and animacy with its antecedent. The morpho-
logical form of possessive pronouns depends not
only on the possessor but also the object in pos-
session. Moreover, reflexive pronouns (both per-
sonal and possessive) are commonly used. In ad-
dition, Czech is a pro-drop language, whereby an
2
explicit subject pronoun may be omitted if it is in-
ferable from other grammatical features such as
verb morphology. This is in contrast with En-
glish which exhibits relatively fixed Subject-Verb-
Object (SVO) order and only drops subject pro-
nouns in imperatives (e.g. ?Stop babbling?) and
coordinated VPs.
Differences between the choice of coreferring
expressions used in English and Czech can be
seen in the following simple examples:
1. The dog has a ball. I can see it playing out-
side.
2. The cow is in the field. I can see it grazing.
3. The car is in the garage. I will take it to work.
In each example, the English pronoun ?it?
refers to an entity that has a different gender in
Czech. Its correct translation requires identifying
the gender (and number) of its antecedent and en-
suring that the pronoun agrees. In 1 ?it? refers to
the dog (?pes?, masculine, animate) and should
be translated as ?ho?. In 2, ?it? refers to the cow
(?kra?va?, feminine) and should be translated as
?ji?. In 3, ?it? refers to the car (?auto?, neuter)
and should be translated as ?ho?.
In some cases, the same pronoun is used for
both animate and inanimate masculine genders,
but in general, different pronouns are used. For
example, with possessive reflexive pronouns in
the accusative case:
English: I admired my (own) dog
Czech: Obdivoval jsme sve?ho psa
English: I admired my (own) castle
Czech: Obdivoval jsme svu?j hrad
Here ?sve?ho? is used to refer to a dog (mascu-
line animate, singular) and ?svu?j? to refer to a cas-
tle (masculine inanimate, singular), both of which
belong to the speaker.
Because a pronoun may take a large number
of morphological forms in Czech and because
case is not checked in annotation projection, the
method presented here for translating coreferring
pronouns does not guarantee their correct form.
3 Related Work
Early work on integrating anaphora resolution
with Machine Translation includes the rule-based
approaches of Mitkov et al(1995) and Lappin &
Leass (1994) and the transfer-based approach of
Saggion & Carvalho (1994). Work in the 1990?s
culminated in the publication of a special issue
of Machine Translation on anaphora resolution
(Mitkov, 1999). Work then appears to have been
on hold until papers were published by Le Na-
gard & Koehn (2010) and Hardmeier & Federico
(2010). This resurgence of interest follows ad-
vances since the 1990?s which have made new ap-
proaches possible.
The work described in this paper resembles that
of Le Nagard & Koehn (2010), with two main dif-
ferences. The first is the use of manually anno-
tated corpora to extract coreference information
and morphological properties of the target trans-
lations of the antecedents. The second lies in the
choice of language pair. They consider English-
French translation, focussing on gender-correct
translation of the third person pronouns ?it? and
?they?. Coreference is more complex in Czech
with both number and gender influencing pronoun
selection. Annotating pronouns with both num-
ber and gender further exacerbates the problem of
data sparseness in the training data, but this can-
not be avoided if the aim is to improve their trans-
lation. This work also accommodates a wider
range of English pronouns.
In contrast, Hardmeier & Federico (2010) focus
on English-German translation and model coref-
erence using a word dependency module inte-
grated within the log-linear SMT model as an ad-
ditional feature function.
Annotation projection has been used elsewhere
in SMT. Gimpel & Smith (2008) use it to capture
long?distance phenomena within a single sen-
tence in the source-language text via the extrac-
tion of sentence-level contextual features, which
are used to augment SMT translation models and
better predict phrase translation. Projection tech-
niques have also been applied to multilingual
Word Sense Disambiguation whereby the sense
of a word may be determined in another language
(Diab, 2004; Khapra et al 2009).
4 Methodology
4.1 Overview
I have followed Le Nagard & Koehn (2010) in us-
ing a two-step approach to translation, with anno-
tation projection incorporated as a pre-processing
3
It stands on a hill.
The castle is old. Hrad je star?.
It stands on a hill.The castle is old.
Hrad je star?.
It.mascin.sg stands on a hill.
Masculine inanimate, singular
Translate:
Translate:
Input: 
The castle is old. It stands on a hill.
(1) Identification of 
coreferential pronoun
(2) Identification of 
antecedent head
(3) English ? Czech mapping 
of antecedent head
(4) Extraction of number 
and gender of Czech word
(5) Annotation of English pronoun with 
number and gender of Czech word
Figure 1: Overview of the Annotation Process
task. In the first step, pronouns are annotated in
the source-language text before the text is trans-
lated by a phrase-based SMT system in the second
step. This approach leaves the translation pro-
cess unaffected. In this work, the following pro-
nouns are annotated: third person personal pro-
nouns (except instances of ?it? that are pleonastic
or that corefer with clauses or VPs), reflexive per-
sonal pronouns and possessive pronouns, includ-
ing reflexive possessives. Relative pronouns are
excluded as they are local dependencies in both
English and Czech and this work is concerned
with the longer range dependencies typically ex-
hibited by the previously listed pronoun types.
Annotation of the English source-language
text and its subsequent translation into Czech is
achieved using two phrase-based translation sys-
tems. The first, hereafter called the Baseline sys-
tem, is trained using English and Czech sentence?
aligned parallel training data with no annotation.
The second system, hereafter called the Annotated
system, is trained using the same target data, but
in the source-language text, each coreferring pro-
noun has been annotated with number, gender and
animacy features. These are obtained from the
existing (Czech reference) translation of the head
of its English antecedent. Word alignment of En-
glish and Czech is obtained from the PCEDT 2.0
alignment file which maps English words to their
corresponding t-Layer (deep syntactic, tectogram-
matical) node in the Czech translation. Starting
with this t-Layer node the annotation layers of the
PCEDT 2.0 corpus are traversed and the number
and gender of the Czech word are extracted from
the morphological layer (m-Layer).
The Baseline system serves a dual purpose. It
forms the first stage of the two-step translation
process, and as described in Section 5, it provides
a baseline against which Annotated system trans-
lations are compared.
The annotation process used here is shown
in Figure 1. It identifies coreferential pronouns
and their antecedents using the annotation in the
BBN Pronoun Coreference and Entity Type cor-
pus, and obtains the Czech translation of the En-
glish antecedent from the translation produced
by the Baseline system. Because many an-
tecedents come from previous sentences, these
sentences must be translated before translating the
current sentence. Here I follow Le Nagard &
Koehn (2010) in translating the complete source-
language text using the Baseline system and then
extracting the (here, Czech) translations of the En-
glish antecedents from the output. This provides
a simple solution to the problem of obtaining the
Czech translation prior to annotation. In contrast
Hardmeier & Federico (2010) translate sentence
by sentence using a process which was deemed
to be more complex than was necessary for this
project.
The English text is annotated such that all
coreferential pronouns whose antecedents have an
identifiable Czech translation are marked with the
number and gender of that Czech word. The out-
put of the annotation process is thus the same En-
glish text that was input to the Baseline system,
with the addition of annotation of the coreferen-
tial pronouns. This annotated English text is then
translated using the Annotated translation system,
the output of which is the final translation.
4
Training Dev. Final
Parallel Sentences 47,549 280 540
Czech Words 955,018 5,467 10,110
English Words 1,024,438 6,114 11,907
Table 1: Sizes of the training and testing datasets
4.2 Baseline and Annotated systems
Both systems are phrase-based SMT models,
trained using the Moses toolkit (Hoang et al
2007). They share the same 3-gram language
model constructed from the target-side text of
the parallel training corpus and the Czech mono-
lingual 2010 and 2011 News Crawl corpora1.
The language model was constructed using the
SRILM toolkit (Stolcke, 2002) with interpolated
Kneser-Ney discounting (Kneser & Ney, 1995).
In addition, both systems are forced to use the
same word alignments (constructed using Giza++
(Och & Ney, 2003) in both language pair direc-
tions and using stemmed training data in which
words are limited to the first four characters) in
order to mitigate the effects of Czech word in-
flection on word alignment statistics. This helps
to ensure that the Czech translation of the head
of the antecedent remains constant in both steps
of the two-step process. If this were to change it
would defeat the purpose of pronoun annotation
as different Czech translations could result in dif-
ferent gender and/or number.
The Baseline system was trained using the
Penn Wall Street Journal corpus with no anno-
tation, while the Annotated system was trained
with an annotated version of the same text (see
Table 1), with the target-language text being the
same in both cases. The Penn Wall Street Journal
corpus was annotated using the process described
above, with the number and gender of the Czech
translation of the antecedent head obtained from
the PCEDT 2.0 alignment file.
4.3 Processing test files
Two test files were used (see Table 1) ? one called
?Final? and the other, ?Development? (Dev). A test
file is first translated using the Baseline system
with a trace added to the Moses decoder. Each
coreferential English pronoun is then identified
using the BBN Pronoun Coreference and Entity
Type corpus and the head of its antecedent is ex-
1Provided for the Sixth EMNLP Workshop on Statistical
Machine Translation (Callison-Burch et al 2011)
tracted from the annotated NPs in the Penn Tree-
bank 3.0 corpus. The sentence number and word
position of the English pronoun and its antecedent
head noun(s) are extracted from the input English
text and used to identify the English/Czech phrase
pairs that contain the Czech translations of the En-
glish words. Using this information together with
the phrase alignments (output by the Moses de-
coder) and the phrase-internal word alignments
in the phrase translation table, a Czech transla-
tion is obtained from the Baseline system. Num-
ber, gender and animacy (if masculine) features
of the Czech word identified as the translation
of the head of the antecedent are extracted from
a pre-built morphological dictionary of Czech
words constructed from the PCEDT 2.0 corpus
for the purpose of this work. A copy of the
original English test file is then constructed, with
each coreferential pronoun annotated with the ex-
tracted Czech features.
The design of this process reflects two assump-
tions. First, the annotation of the Czech words
in the m-Layer of the PCEDT 2.0 corpus is both
accurate and consistent. Second, as the Base-
line and Annotated systems were trained using the
same word alignments, the Czech translation of
the head of the English antecedent should be the
same in the output of both. Judging by the very
small number of cases in which the antecedent
translations differed (3 out of 458 instances), this
assumption was proved to be reasonable. These
differences were due to the use of different phrase
tables for each system as a result of training on
different data (i.e. the annotation of English pro-
nouns or lack thereof). This would not be an is-
sue for single-step translation systems such as that
used by Hardmeier & Federico (2010).
4.4 Evaluation
No standard method yet exists for evaluating pro-
noun translation in SMT. Early work focussed on
the development of techniques for anaphora reso-
lution and their integration within Machine Trans-
lation (Lappin & Leass, 1994; Saggion & Car-
valho, 1994; Mitkov et al 1995), with little men-
tion of evaluation. In recent work, evaluation
has become much more important. Both Le Na-
gard & Koehn (2010) and Hardmeier & Federico
(2010) consider and reject BLEU (Papineni et al
2002) as ill-suited for evaluating pronoun transla-
tion. While Hardmeier & Federico propose and
5
use a strict recall and precision based metric for
English?German translation, I found it unsuitable
for English?Czech translation, given the highly
inflective nature of Czech.
Given the importance of evaluation to the goal
of assessing the effectiveness of annotation pro-
jection for improving the translation of corefer-
ring pronouns, I carried out two separate types
of evaluation ? an automated evaluation which
could be applied to the entire test set, and an in-
depth manual assessment that might provide more
information, but could only be performed on a
subset of the test set. The automated evaluation
is based on the fact that a Czech pronoun must
agree in number and gender with its antecedent.
Thus one can count the number of pronouns in the
translation output for which this agreement holds,
rather than simply score the output against a sin-
gle reference translation. To obtain these figures,
the automated evaluation process counted:
1. Total pronouns in the input English test file.
2. Total English pronouns identified as corefer-
ential, as per the annotation of the BBN Pro-
noun Coreference and Entity Type corpus.
3. Total coreferential English pronouns that are
annotated by the annotation process.
4. Total coreferential English pronouns that are
aligned with any Czech translation.
5. Total coreferential English pronouns trans-
lated as any Czech pronoun.
6. Total coreferential English pronouns trans-
lated as a Czech pronoun corresponding to
a valid translation of the English pronoun.
7. Total coreferential English pronouns trans-
lated as a Czech pronoun (that is a valid
translation of the English pronoun) agreeing
in number and gender with the antecedent.
The representation of valid Czech translations
of English pronouns takes the form of a list pro-
vided by an expert in Czech NLP, which ignores
case and focusses solely on number and gender.
In contrast, the manual evaluation carried out
by that same expert, who is also a native speaker
of Czech, was used to determine whether devi-
ations from the single reference translation pro-
vided in the PCEDT 2.0 corpus were valid alter-
natives or simply poor translations. The following
judgements were provided:
1. Whether the pronoun had been translated
correctly, or in the case of a dropped pro-
noun, whether pro-drop was appropriate;
2. If the pronoun translation was incorrect,
whether a native Czech speaker would still
be able to derive the meaning;
3. For input to the Annotated system, whether
the pronoun had been correctly annotated
with respect to the Czech translation of its
identified antecedent;
4. Where an English pronoun was translated
differently by the Baseline and Annotated
systems, which was better. If both translated
an English pronoun to a valid Czech transla-
tion, equal correctness was assumed.
In order to ensure that the manual assessor
was directed to the Czech translations aligned
to the English pronouns, additional markup was
automatically inserted into the English and Czech
texts: (1) coreferential pronouns in both English
and Czech texts were marked with the head
noun of their antecedent (denoted by *), and
(2) coreferential pronouns in the English source
texts were marked with the Czech translation
of the antecedent head, and those in the Czech
target texts were marked with the original English
pronoun that they were aligned to:
English text input to the Baseline system: the u.s.
, claiming some success in its trade diplomacy , ...
Czech translation output by the Baseline system:
usa , tvrd?? ne?kter??? jej??(its) obchodn?? u?spe?ch v diplo-
macii , ...
English text input to the Annotated system: the
u.s.* , claiming some success in its(u.s.,usa).mascin.pl
trade diplomacy , ...
Czech translation output by the Annotated sys-
tem: usa ,* tvrd?? ne?kter??? u?spe?chu ve sve?(its.mascin.pl)
obchodn?? diplomacii , ...
5 Results and Discussion
5.1 Automated Evaluation
Automated evaluation of both ?Development?
and ?Final? test sets (see Table 2) shows that even
factoring out the problems of accurate identifica-
tion of coreferring pronouns, coreference resolu-
tion and antecedent head?finding, does not im-
prove performance of the Annotated system much
above that of the Baseline.
6
Dev. Final
Baseline Annotated Baseline Annotated
Total pronouns in English file 156 156 350 350
Total pronouns identified as coreferential 141 141 331 331
Annotated coreferential English pronouns ? 117 ? 278
Coreferential English pronouns aligned with any Czech translation 141 141 317 317
Coreferential English pronouns translated as Czech pronouns 71 75 198 198
Czech pronouns that are valid translations of the English pronouns 63 71 182 182
Czech pronouns that are valid translations of the English pronouns
and that match their antecedent in number and gender
44 46 142 146
Table 2: Automated Evaluation Results for both test sets
Criterion Baseline System Better Annotated System Better Systems Equal
Overall quality 9/31 (29.03%) 11/31 (35.48%) 11/31 (35.48%)
Quality when annotation is correct 3/18 (16.67%) 9/18 (50.00%) 6/18 (33.33%)
Table 3: Manual Evaluation Results: A direct comparison of pronoun translations that differ between systems
Taking the accuracy of pronoun translation to
be the proportion of coreferential English pro-
nouns having a valid Czech translation that agrees
in both number and gender with their antecedent,
yields the following on the two test sets:
Baseline system:
Development ? 44/141 (31.21%)
Final ? 142/331 (42.90%)
Annotated system:
Development ? 46/141 (32.62%)
Final ? 146/331 (44.10%)
There are, however, several reasons for not tak-
ing this evaluation as definitive. Firstly, it relies
on the accuracy of the word alignments output by
the decoder to identify the Czech translations of
the English pronoun and its antecedent. Secondly,
these results fail to capture variation between the
translations produced by the Baseline and Anno-
tated systems. Whilst there is a fairly high de-
gree of overlap, for approximately 1/3 of the ?De-
velopment? set pronouns and 1/6 of the ?Final?
set pronouns, the Czech translation is different.
Since the goal of this work was to understand
what is needed in order to improve the transla-
tion of coreferential pronouns, manual evaluation
was critical for understanding the potential capa-
bilities of source-side annotation.
5.2 Manual Evaluation
The sample files provided for manual evaluation
contained 31 pronouns for which the translations
provided by the two systems differed (differences)
and 72 for which the translation provided by the
systems was the same (matches). Thus, the sam-
ple comprised 103 of the 472 coreferential pro-
nouns (about 22%) from across both test sets. Of
this sample, it is the differences that indicate the
relative performance of the two systems. Of the
31 pronouns in this set, 16 were 3rd-person pro-
nouns, 2 were reflexive personal pronouns and 13
were possessive pronouns.
The results corresponding to evaluation crite-
rion 4 in Section 4.4 provide a comparison of the
overall quality of pronoun translation for both sys-
tems. These results for the ?Development? and
?Final? test sets (see Table 3) suggest that the per-
formance of the Annotated system is comparable
with, and even marginally better than, that of the
Baseline system, especially when the pronoun an-
notation is correct.
An example of where the Annotated system
produces a better translation than the Baseline
system is:
Annotated English: he said mexico could be one of the
next countries to be removed from the priority list because of
its.neut.sg efforts to craft a new patent law .
Baseline translation: r?ekl , z?e mexiko by mohl by?t jeden
z dals???ch zem?? , aby byl odvola?n z prioritou seznam , protoz?e
jej?? snahy podpor?it nove? patentovy? za?kon .
Annotated translation: r?ekl , z?e mexiko by mohl by?t je-
den z dals???ch zem?? , aby byl odvola?n z prioritou seznam ,
protoz?e jeho snahy podpor?it nove? patentovy? za?kon .
In this example, the English pronoun ?its?,
which refers to ?mexico? is annotated as neuter
and singular (as extracted from the Baseline trans-
lation). Both systems translate ?mexico? as
?mexiko? (neuter, singular) but differ in their
translation of the pronoun. The Baseline system
translates ?its? incorrectly as ?jej??? (feminine, sin-
gular), whereas the Annotated system produces
7
the more correct translation: ?jeho? (neuter, sin-
gular), which agrees with the antecedent in both
number and gender.
An analysis of the judgements on the remain-
ing three evaluation criteria (outlined in Section
4.4) for the 31 differences provides further infor-
mation. The Baseline system appears to be more
accurate, with 19 pronouns either correctly trans-
lated (in terms of number and gender) or appro-
priately dropped, compared with 17 for the An-
notated system. Of those pronouns, the meaning
could still be understood for 7/12 for the Baseline
system compared with 8/14 for the Annotated sys-
tem. On the surface this may seem strange but it
appears to be due to a small number of cases in
which the translations produced by both systems
were incorrect but those produced by the Anno-
tated system were deemed to be marginally better.
Due to the small sample size it is difficult to form
a complete picture of where one system may per-
form consistently better than the other. The anno-
tation of both number and gender was accurate for
18 pronouns. Whilst this accuracy is not particu-
larly high, the results (see Table 3) suggest that
translation is more accurate for those pronouns
that are correctly annotated.
Whilst pro-drop in Czech was not explicitly
handled in the annotation process, manual evalu-
ation revealed that both systems were able to suc-
cessfully ?learn? a few (local) scenarios in which
pro-drop is appropriate. This was unexpected but
found to be due to instances in which there are
short distances between the pronoun and verb in
English. For example, many of the occurrences
of ?she? in English appear in the context of ?she
said...? and are translated correctly with the verb
form ?...r?ekla...?.
An example of where the Annotated system
correctly drops a pronoun is:
Annotated English: ? this is the worst shakeout ever in
the junk market , and it could take years before it.fem.sg ?
s over , ? says mark bachmann , a senior vice president at
standard & poor ? s corp . , a credit rating company .
Baseline translation: ? je to nejhors??? krize , kdy na trhu
s rizikovy?mi obligacemi , a to mu?z?e trvat roky , nez? je to pryc?
, ? r???ka? mark bachmann , hlavn?? viceprezident spolec?nosti
standard & poor ?s corp . , u?ve?rovy? rating spolec?nosti .
Annotated translation: ? je to nejhors??? krize , kdy na
trhu s rizikovy?mi obligacemi , a to mu?z?e trvat roky , nez?
je !! pryc? , ? r???ka? mark bachmann , hlavn?? viceprezident
spolec?nosti standard & poor ?s corp . , u?ve?rovy? rating
spolec?nosti .
In this example, the Baseline system trans-
lates ?it? incorrectly as the neuter singular pro-
noun ?to?, whereas the Annotated system cor-
rectly drops the subject pronoun (indicated by !!)
? this is a less trivial example than ?she said?. In
the case of the Baseline translation ?to? could be
interpreted as referring to the whole event, which
would be correct, but poor from a stylistic point
of view.
An example of where the Annotated system
fails to drop a pronoun is:
Annotated English: taiwan has improved its.mascin.sg*
standing with the u.s. by initialing a bilateral copyright
agreement , amending its.mascin.sg** trademark law and
introducing legislation to protect foreign movie producers
from unauthorized showings of their.mascan.pl films .
Annotated translation: tchaj-wan zleps?en?? sve?
postaven?? s usa o initialing bilatera?ln??ch autorsky?ch pra?v na
jeho obchodn?? dohody , u?prava za?kona a zaveden?? za?kona
na ochranu zahranic?n?? filmove? producenty z neopra?vne?ne?
showings svy?ch filmu? .
Reference translation: tchaj-wan zleps?il svou reputaci
v usa , kdyz? podepsal bilatera?ln?? smlouvu o autorsky?ch
pra?vech , pozme?nil !! za?kon o ochranny?ch zna?mka?ch a
zavedl legislativu na ochranu zahranic?n??ch filmovy?ch produ-
centu? proti neautorizovane?mu prom??ta?n?? jejich filmu? .
In this example, the English pronoun ?its?,
which refers to ?taiwan? is annotated as mascu-
line inanimate and singular. The first occurrence
of ?its? is marked by * and the second occurrence
by ** in the annotated English text above. The
second occurrence should be translated either as
a reflexive pronoun (as the first occurrence is cor-
rectly translated) or it should be dropped as in the
reference translation (!! indicates the position of
the dropped pronoun).
In addition to the judgements, the manual as-
sessor also provided feedback on the evalua-
tion task. One of the major difficulties encoun-
tered concerned the translation of pronouns in
sentences which exhibit poor syntactic structure.
This is a criticism of Machine Translation as a
whole, but of the manual evaluation of pronoun
translation in particular, since the choice of core-
ferring form is sensitive to syntactic structure.
Also the effects of poor syntactic structure are
likely to introduce an additional element of sub-
jectivity if the assessor must first interpret the
structure of the sentences output by the transla-
tion systems.
5.3 Potential Sources of Error
Related errors that may have contributed to the
Annotated system not providing a significant im-
provement over the Baseline include: (1) incor-
8
rect identification of the English antecedent head
noun, (2) incorrect identification of the Czech
translation of the antecedent head noun in the
Baseline output due to errors in the word align-
ments, and (3) errors in the PCEDT 2.0 align-
ment file (affecting training only). While ?per-
fect? annotation of the BBN Pronoun Coreference
and Entity Type, the PCEDT 2.0 and the Penn
Treebank 3.0 corpora has been assumed, errors in
these corpora cannot be completely ruled out.
6 Conclusion and Future Work
Despite factoring out three major sources of er-
ror ? identifying coreferential pronouns, finding
their antecedents, and identifying the head of each
antecedent ? through the use of manually anno-
tated corpora, the results of the Annotated system
show only a small improvement over the Baseline
system. Two possible reasons for this are that the
statistics in the phrase translation table have been
weakened in the Annotated system as a result of
including both number and gender in the anno-
tation and that the size of the training corpus is
relatively small.
However, more significant may be the avail-
ability of only a single reference translation. This
affects the development and application of au-
tomated evaluation metrics as a single reference
cannot capture the variety of possible valid trans-
lations. Coreference can be achieved without ex-
plicit pronouns. This is true of both English and
Czech, with sentences that contain pronouns hav-
ing common paraphrases that lack them. For ex-
ample,
the u.s. , claiming some success in its trade
diplomacy , ...
can be paraphrased as:
the u.s. , claiming some success in trade diplo-
macy , ...
A target-language translation of the former
might actually be a translation of the latter, and
hence lack the pronoun shown in bold. Given the
range of variability in whether pronouns are used
in conveying coreference, the availability of only
a single reference translation is a real problem.
Improving the accuracy of coreferential pro-
noun translation remains an open problem in Ma-
chine Translation and as such there is great scope
for future work in this area. The investigation re-
ported here suggests that it is not sufficient to fo-
cus solely on the source-side and further opera-
tions on the target side (besides post-translation
application of a target-language model) need also
be considered. Other target?side operations could
involve the extraction of features to score multi-
ple candidate translations in the selection of the
?best? option ? for example, to ?learn? scenar-
ios in which pro-drop is appropriate and to select
translations that contain pronouns of the correct
morphological inflection. This requires identifica-
tion of features in the target side, their extraction
and incorporation in the translation process which
could be difficult to achieve within a purely sta-
tistical framework given that the antecedent of a
pronoun may be arbitrarily distant in the previous
discourse.
The aim of this work was to better understand
why previous attempts at using annotation projec-
tion in pronoun translation showed less than ex-
pected improvement. Thus it would be beneficial
to conduct an error analysis to show the frequency
of the errors described in Section 5.3 appear.
I will also be exploring other directions re-
lated to problems identified during the course of
the work completed to date. These include, but
are not limited to, handling pronoun dropping in
pro-drop languages, developing pronoun-specific
automated evaluation metrics and addressing the
problem of having only one reference translation
for use with such metrics. In this regard, I will be
considering the use of paraphrase techniques to
generate synthetic reference translations to aug-
ment an existing reference translation set. Ini-
tial efforts will focus on adapting the approach of
Kauchak & Barzilay (2006) and back?translation
methods for extracting paraphrases (Bannard &
Callison-Burch, 2005) to the more specific prob-
lem of pronoun variation.
Acknowledgements
I would like to thank Bonnie Webber (Univer-
sity of Edinburgh) who supervised this project
and Marke?ta Lopatkova? (Charles University) who
provided the much needed Czech language assis-
tance. I am very grateful to Ondr?ej Bojar (Charles
University) for his numerous helpful suggestions
and to the Institute of Formal and Applied Lin-
guistics (Charles University) for providing the
PCEDT 2.0 corpus. I would also like to thank
Wolodja Wentland and the three anonymous re-
viewers for their feedback.
9
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with Bilingual Parallel Corpora. In Pro-
ceedings of the 43rd Annual Meeting of the ACL,
pages 597?604.
Chris Callison-Burch, Philipp Koehn, Christof Monz
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64.
Mona Diab. 2004. An Unsupervised Approach for
Bootstrapping Arabic Sense Tagging. In Proceed-
ings of the Workshop on Computational Approaches
to Arabic Script-based Languages, pages 43?50.
Kevin Gimpel and Noah A. Smith. 2008. Rich
Source-Side Context for Statistical Machine Trans-
lation. In Proceedings of the Third Workshop on
Statistical Machine Translation, pages 9?17.
Barbara J. Grosz, Scott Weinstein and Aravind K.
Joshi. 1995. Centering: A Framework for Mod-
eling the Local Coherence Of Discourse. Computa-
tional Linguistics, 21(2):203?225.
Christian Hardmeier and Marcello Federico. 2010.
Modelling Pronominal Anaphora in Statistical Ma-
chine Translation. In Proceedings of the 7th In-
ternational Workshop on Spoken Language Trans-
lation, pages 283?290.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens. Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, pages 177?180.
Jerry R. Hobbs. 1978. Resolving Pronominal Refer-
ences. Lingua, 44:311?338.
Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?, Petr Sgall,
Silvie Cinkova?, Eva Fuc???kova?, Marie Mikulova?,
Petr Pajas, Jan Popelka, Jir??? Semecky?, Jana
S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova? and Zdene?k Z?abokrtsky?. 2011. Prague
Czech-English Dependency Treebank 2.0. Institute
of Formal and Applied Linguistics. Prague, Czech
Republic.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing For Automatic Evaluation. In Proceedings
of the Main Conference on Human Language Tech-
nology Conference of the NAACL, June 5?7, New
York, USA, pages 455?462.
Mitesh M. Khapra, Sapan Shah, Piyush Kedia and
Pushpak Bhattacharyya. 2009. Projecting Param-
eters for Multilingual Word Sense Disambiguation.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, Au-
gust 6?7, Singapore, pages 459?467.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved Backing-Off for M-gram Language Model-
ing. IEEE International Conference on Acoustics,
Speech, and Signal Processing, May 9?12, Detroit,
USA, 1:181?184.
Shalom Lappin and Herbert J. Leass. 1994. An Algo-
rithm for Pronominal Anaphora Resolution. Com-
putational Linguistics, 20:535?561.
Ronan Le Nagard and Philipp Koehn. 2010. Aid-
ing Pronoun Translation with Co-reference Resolu-
tion. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 252?261.
Vincent Ng. 2010. Supervised Noun Phrase Corefer-
ence Research: The first 15 years. In Proceedings
of the 48th Meeting of the ACL, pages 1396?1411.
Mitchell P. Marcus, Beatrice Santorini, Mary A.
Marcinkiewicz and Ann Taylor. 1999. Penn Tree-
bank 3.0 LDC Calalog No.: LDC99T42. Linguistic
Data Consortium.
Ruslan Mitkov, Sung-Kwon Choi and Randall Sharp.
1995. Anaphora Resolution in Machine Transla-
tion. In Proceedings of the Sixth International Con-
ference on Theoretical and Methodological Issues
in Machine Translation, July 5-7, Leuven, Belgium,
pages 5?7.
Ruslan Mitkov. 1999. Introduction: Special Issue on
Anaphora Resolution in Machine Translation and
Multilingual NLP. Machine Translation, 14:159?
161.
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318.
Horacio Saggion and Ariadne Carvalho. 1994.
Anaphora Resolution in a Machine Translation Sys-
tem. In Proceedings of the International Con-
ference on Machine Translation: Ten Years On,
November, Cranfield, UK, 4.1-4.14.
Andreas Stolcke. 2002. SRILM ? An Extensible
Language Modeling Toolkit. In Proceedings of In-
ternational Conference on Spoken Language Pro-
cessing, September 16-20, Denver, USA, 2:901?
904.
Michael Strube. 2007. Corpus-based and Ma-
chine Learning Approaches to Anaphora Resolu-
tion. Anaphors in Text: Cognitive, Formal and
Applied Approaches to Anaphoric Reference, John
Benjamins Pub Co.
Ralph Weischedel and Ada Brunstein. 2005. BBN
Pronoun Coreference and Entity Type Corpus LDC
Calalog No.: LDC2005T33. Linguistic Data Con-
sortium.
10
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 10?18,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Analysing Lexical Consistency in Translation
Liane Guillou
School of Informatics
University of Edinburgh
Scotland, United Kingdom
L.K.Guillou@sms.ed.ac.uk
Abstract
A number of approaches have been taken
to improve lexical consistency in Statis-
tical Machine Translation. However, lit-
tle has been written on the subject of
where and when to encourage consistency.
I present an analysis of human authored
translations, focussing on words belong-
ing to different parts-of-speech across a
number of different genres.
1 Introduction
Writers are often given mixed messages with re-
spect to word choice. On one hand they are en-
couraged to vary their use of words (in essay writ-
ing): ?It is also important that the words you use
are varied, so that you aren?t using the same words
again and again.?1. On the other hand they are
encouraged to use the same words (only chang-
ing the determiner) when referring to the same en-
tity a second time (in technical writing): ?The first
time a single countable noun is introduced, use a.
Thereafter, when referring to that same item, use
the.? 2.
Halliday and Hassan (1976) showed that well-
written documents exhibit lexical cohesion in
terms of what they call reiteration and colloca-
tion. Reiteration is achieved via repetition as well
as the use of synonyms and hypernyms. A collo-
cation is a sequence of words / terms that co-occur
regularly in text. Examples of collocated pairs of
words include ?fast food?, ?bright idea? and ?nu-
clear family?. Any source language document will
1Purdue University, Online Writing Lab: http:
//owl.english.purdue.edu/engagement/
index.php?category_id=2&sub_category_id=
2&article_id=66. Accessed 21/04/2013
2Monash University, Language and Learning On-
Line: http://monash.edu.au/lls/llonline/
grammar/engineering/articles/6.xml. Ac-
cessed 21/04/2013
therefore contain repeated instances of the same
words or lemmas (morphological variants of the
same words). This repeated use of words and lem-
mas is known as lexical consistency and the in-
stances can be grouped together to form lexical
chains (Morris and Hirst, 1991). Lexical chains
were proposed by Lotfipour-Saedi (1997) as one
feature of a text via which translational equiva-
lence between source and target could be mea-
sured.
While Statistical Machine Translation (SMT)
has gone from ignoring these properties of dis-
course by translating sentences independently, to
trying to impose lexical consistency at a universal
level, both approaches have given little consider-
ation to what might be standard practice among
human translators.
In order to discover what the standard practice
might be, and thus what an SMT system might bet-
ter aim to achieve, I have carried out a detailed
analysis of lexical consistency in human transla-
tion. For comparison, I also present an analysis of
translations produced by an SMT system. I have
considered a variety of genres, as genre correlates
with the function of a text, which in turn predicts
its important elements. A preliminary conclusion
of this analysis is that human translators use lex-
ical consistency to support what is important in a
text.
2 Related Work
2.1 Unique Terms and Lexical Consistency
Intuitively, it seems obvious that specialised,
?semantically heavy? words like ?genome? and
?hypochondria? will only have a single exact
translation into any given target language, and as
such will tend to be translated with greater consis-
tency than semantically ?light? words. Melamed
(1997) showed that this intuition could be quan-
tified using the concept of entropy, which the
10
author uses over a large corpus to show what
words and what parts-of-speech are more likely to
be translated consistently than others. However,
Melamed?s analysis ignores any segmentation of
the corpus by document, topic, speaker/writer or
translator, considering only overall translational
distributions. It is therefore similar to that which
can be gleaned from the phrase table in a modern
SMT system.
2.2 Enforcing and Encouraging Consistency
A number of approaches have been taken to both
encourage and enforce lexical consistency in SMT.
These range from the cache-based model ap-
proaches of Tiedemann (2010a; 2010b) and Gong
et al (2011), to the post-editing approach of Xiao
et al (2011) and discriminative learning approach
of Ma et al (2011) and He et al (2011).
Carpuat (2009) and Ture et al (2012) suggested
that the one sense per discourse constraint (Gale
et al, 1992) might apply as well to one sense per
translation. Both demonstrated that exploiting this
constraint in SMT led to better quality transla-
tions. Ture et al (2012) encourage consistency
themselves using soft constraints implemented as
additional features in a hierarchical phrase-based
translation model.
What has not been adequately addressed in the
available MT literature is where and when lexical
consistency is desirable in translation.
2.3 Measuring Consistency
In contrast with entropy following from lexical
properties of words (i.e. how many senses a word
has, and how many different possible ways there
are of translating each sense in a given target lan-
guage), as explored in (Melamed, 1997), Itagaki et
al (2007) developed a way to measure the termi-
nological consistency of a single document. They
define consistency as a measure of the number of
translation variations for a term and the frequency
for each variation. They adapted the Herfindahl-
Hirschman Index (HHI) measure, typically used
to measure market concentration, to measure the
consistency of a single term in a single document.
HHI is defined as:
HHI =
n?
i=1
s2i
Where i ranges over the n different ways that the
given term has been translated in the document,
and si is the ratio of the number of times the term
has been translated as i to the number of times it
has been translated. The lower the index, the more
variation there is in translation of the term, i.e. the
less consistent the translation. The maximum in-
dex is 10,000 (or 1 using the normalised scale) for
a completely consistent translation.
HHI is best illustrated with examples of dis-
tributions over a single document. An English
word with two French translations that are ob-
served with equal frequency will receive a score
of: 0.502 +0.502 = 0.5. A different English word
with two French translations observed 80% and
20% of the time will receive a score of: 0.902 +
0.102 = 0.82 representing a more consistent trans-
lation of the English word. When the number
of possible French translations increases, the HHI
score will likely decrease unless one translation is
much more frequent - see previous example. An
English word with three translations observed with
equal frequency (33.3% each) will have a score of:
0.332 +0.332 +0.332 = 0.33 representing a word
that is translated with lower consistency.
Itagaki et al incorporate these HHI scores (one
score per term, per document) in a wider calcula-
tion that measures inter-document consistency of
a set of documents that all use the same term. As
the analyses presented in this paper are concerned
with single documents and their translations, the
per term, per document HHI scores are sufficient.
3 Methodology
This section describes analyses of manual (hu-
man) translation and automated translation (by
a phrase-based SMT system). The data used
is described in Section 3.1 and the methods for
analysing consistency in human and automated
translation are described in Sections 3.2 and 3.3.
3.1 Data
As the focus of the analysis is lexical consistency,
it was important to select texts that were writ-
ten/translated by the same author. The typical
corpora used in training SMT systems were dis-
missed; Europarl as speakers change frequently
and news-crawl as the articles are typically too
short to exhibit much lexical repetition. Instead
I selected the INTERSECT corpus (Salkie, 2010)
which contains a collection of sentence-aligned
parallel texts from different genres. From this cor-
pus I extracted a number of texts from the English-
11
Title Genre Sentences Words En POS Count Fr POS Count
En Fr N A V N A V
English Source
Xerox ScanWorx Manual Instructions 2,573 38,698 44,841 14,060 2,308 6,555 15,206 2,528 8,822
On the Origin of Species Natural Science 1,702 62,454 68,016 13,774 6,868 9,857 17,452 6,291 12,895
Dracula Ch. 1-2 Novel 584 11,209 10,840 2,147 817 2,110 2,659 745 2,336
The Invisible Man Ch. 1-4 Novel 504 7,578 7,924 1,845 442 1,471 2,118 472 1,720
French Source
Nuclear Testing Public Info 613 13,127 13,563 3,918 1,412 1,808 4,261 1,344 2,253
French Revolution to 1945 Public Info 1530 34,038 33,187 11,217 3,119 4,279 11,008 3,025 4,632
The Immoralist Novel 1,377 29,323 24,942 5,299 2,049 5,888 5,813 1,513 6,138
News article 1 News 126 1,757 1,751 549 122 284 558 115 324
News article 2 News 126 2,306 2,254 590 150 430 673 125 459
News article 3 News 85 1,891 1,756 501 183 332 534 122 332
News article 4 News 97 2,236 1,974 641 157 367 609 120 356
Table 1: Documents taken from the English (En) - French (Fr) section of the INTERSECT corpus.
French collection (Table 1). The frequencies for
nouns (N), adjectives (A) and verbs (V) in this ta-
ble were extracted automatically using the Tree-
Tagger tool (Schmid, 1994).
Word alignments for the parallel documents
were computed using Giza++ (Och and Ney,
2003) run in both directions. In order to improve
the robustness of the word alignments the doc-
uments were concatenated into a single file, to-
gether with English-French parallel data from the
Europarl corpus (Koehn, 2005). The word align-
ments for the relevant documents were then ex-
tracted from the symmetrised alignment file.
3.2 Consistency in Human Translation
The motivation for this analysis was to assess the
extent to which a human translator maintained lex-
ical consistency when translating a document. In
other words, in those places where the author of a
source document makes consistent lexical choices,
do human translators do so as well? And if they
do, should we aim for the same in SMT?
For each document, the English and French
parallel texts were processed using TreeTagger
(Schmid, 1994). Using the language in which
the document was originally written (its born lan-
guage) as the source language, word alignments
were used to identify what each source word
aligned to in the (human) translation.
Since I wanted to establish not just the degree
of consistency, but where consistency was being
maintained, and because I felt that the Part-of-
Speech (POS) tags output by TreeTagger were
too fine-grained for this purpose, these tags were
mapped to a set of coarse-grained tags. The
Universal POS tagset mapping file (Petrov et al,
2011) was used for English and a comparable file
was constructed for French. In addition to this, I
also sub-divided the coarse-grained verb class into
three classes: light verbs (e.g. do, have, make),
mid-range verbs (e.g. build, read, speak) and rare-
verbs (e.g. revolutionise, obfuscate, perambulate).
This was to test the hypothesis that light verbs
will exhibit lower levels of consistency than other
verbs. A light verb is defined a verb with little se-
mantic content of its own that forms a predicate
with its argument (usually a noun). For example
the verb ?do? in ?do lunch? or ?make? in ?make
a request?. As no predefined lists of light, mid-
range and rare verbs are available, these groups
were approximated. An English verb?s category
is determined by its frequency in the British Na-
tional Corpus (BNC) (Clear, 1993). A verb with
a frequency count in the bottom 5% is deemed a
rare verb, in the top 5% is deemed a light verb
and anything in between, is deemed a mid-range
verb. A manual inspection of the resulting cat-
egory boundaries shows that these thresholds are
reasonable. For French, verb frequencies were ex-
tracted from the French Treebank (Abeille? et al,
2000).
Herfindahl-Hirschman Index (HHI) (Itagaki et
al., 2007) scores were calculated for each surface
word (one score per surface word) in the born
language document. The documents were treated
separately, and no inter-document scores are cal-
culated. These scores tell us how consistent the
translation is into the target language. For words
in the English documents I considered what words
and lemmas were present in the French transla-
tion. Lemmas are included as French verb inflec-
tions may otherwise skew the results. For com-
12
pleteness, lemmas in the English translation of the
French documents are also considered.
For each POS category, an average HHI score
is calculated by taking the sum of the HHI scores
per word and dividing it by the number of words
(for that POS category). Only those words that are
repeated (i.e. appear more than once in the source
document with the same coarse POS category) are
considered. (That is, a word that appeared once
as a mid-range verb, once as a noun and once as
something else, would not be included). A similar
average is calculated for lemmas.
HHI scores are normally presented in the range
of 0 to 10,000. However, for simplicity, the scores
presented in this paper are normalised to between
0 to 1.
3.3 Consistency in Automated Translation
The aim of this analysis was to assess how the con-
sistency in translations produced by an SMT sys-
tem would compare to those by a human translator.
The SMT system was an English-French phrase-
based system trained and tuned using (Moses and)
Europarl data. Its language model was constructed
from the French side of the parallel training cor-
pus. The system was used to translate the born
English source documents (Xerox Manual, On
the Origin of Species, Dracula and The Invisible
Man). Word alignments and a file containing a list
of Out of Vocabulary (OOV) words were also re-
quested from the decoder. Note that all of the doc-
uments are considered to be ?out of domain? with
respect to the training data used to build the SMT
system.
Using a similar process as described in Section
3.2, but omitting those words that are reported by
the decoder as OOV, average HHI scores are cal-
culated for each POS category. OOV words are
omitted as these will be ?carried through? by the
decoder, appearing untranslated in the translation
output. They therefore do not say anything about
the consistency of the translation.
The other major difference is that HHI scores
are calculated only at the word level, not at the
lemma level as it is expected that the TreeTagger
would perform poorly on SMT output and these
errors could lead to misleading results. In all other
respects, the process for analysing text is the same
as described in Section 3.2.
4 Results
4.1 Consistency in Human Translation
The results are presented in Table 2. Higher (aver-
age) HHI scores represent greater consistency.
For both English and French source documents,
nouns score highly, suggesting that in general hu-
man translators translate nouns rather consistently.
However, nouns don?t always receive the highest
average score. For verbs, the trend is that consis-
tency is irrelevant in translating light verbs, rare
verbs tend to be translated with the highest consis-
tency, and mid-range verbs are somewhere in be-
tween. This suggests that consistency in the trans-
lation of light verbs would be undesirable.
Looking at some of the texts in more detail it
may be possible to infer certain qualities of text
across different genres.
Novels: In all three texts, (Dracula, The
Invisible Man and The Immoralist), nouns receive
the highest average HHI score of all the POS
categories. An analysis of some of the most
frequent (and aligned) nouns in Dracula (Table
3) suggests that it is desirable to keep important
nouns constant - those that identify characters and
other entities central to the story. For example,
the Count is an important character and is never
referred to by any other name/title in the original
text. (N.B. ?count? is also a mid-range verb, but it
is used only as a noun in Dracula). The translation
to (le) comte in French is highly consistent. A
similar observation is made for horses which are
important in the story. Interestingly, the (same)
coach driver is referred to as (le) chauffer, (le)
conducteur and (le) cocher in French:
English: ...and the driver said in excellent German
French: Le conducteur me dit alors, en excellent allemand
English: Then the driver cracked his whip
French: Puis le chauffeur fit claquer son fouet
English: When the caleche stopped, the driver jumped down
French: La cale`che arre?te?e, le cocher sauta de son sie`ge
This perhaps reflects a stylistic choice made by the
translator to vary the terms used to refer to a char-
acter of lesser importance. It is worth noting that
the English text also contains several instances of
?coachman? to refer to the ?driver? but the varia-
tion is much less compared with the French trans-
lation.
Verbs, on the other hand, receive lower (aver-
age) HHI scores indicating that this may be an area
13
Title Noun Adj Verb
All Light Mid-Range Rare
English Source
Xerox ScanWorx Manual 0.6995 0.5900 0.5568 0.3256 0.5766 0.6485
Xerox ScanWorx Manual (Lemmas) 0.7126 0.7112 0.6612 0.4172 0.6902 0.7086
On the Origin of Species 0.6109 0.4390 0.4001 0.2339 0.4140 0.4592
On the Origin of Species (Lemmas) 0.6417 0.5722 0.5056 0.3355 0.5273 0.5098
Dracula 0.6182 0.4191 0.3631 0.2477 0.4175 0.5000
Dracula (Lemmas) 0.6294 0.4979 0.4113 0.2902 0.4711 0.5000
The Invisible Man 0.6290 0.5110 0.4159 0.3139 0.4797 0.4219
The Invisible Man (Lemmas) 0.6275 0.5743 0.4573 0.3723 0.5121 0.4219
French Source
Nuclear Testing 0.7388 0.8079 0.5616 0.3312 0.5279 0.6228
Nuclear Testing (Lemmas) 0.7521 0.8209 0.5972 0.4198 0.5599 0.6584
French Revolution to 1945 0.6346 0.6587 0.5054 0.3041 0.4404 0.5521
French Revolution to 1945 (Lemmas) 0.6509 0.6632 0.5266 0.3950 0.4710 0.5655
The Immoralist 0.6807 0.5732 0.4868 0.3106 0.4524 0.5046
The Immoralist (Lemmas) 0.7007 0.5856 0.5142 0.3821 0.4977 0.5236
News article 1 0.7278 0.6400 0.5424 0.4336 0.5608 0.5734
News article 1 (Lemmas) 0.7542 0.6400 0.5616 0.4943 0.5608 0.5911
News article 2 0.6745 0.7140 0.5345 0.3660 0.5395 0.6751
News article 2 (Lemmas) 0.6836 0.7140 0.5717 0.4083 0.5395 0.7778
News article 3 0.6991 0.7986 0.5024 0.3016 0.5794 0.5988
News article 3 (Lemmas) 0.7121 0.7986 0.5869 0.4801 0.6508 0.6204
News article 4 0.6734 0.6556 0.5073 0.2408 0.6667 0.6295
News article 4 (Lemmas) 0.6984 0.6333 0.6118 0.3790 0.6667 0.7545
Table 2: Human Translation: Average HHI scores for words in the source and their aligned words (and
lemmas) in the translations. Scores are provided in the range of 0 to 1 and the highest score for each
document is highlighted in bold text. The scores for rare verbs in Dracula and The Invisible Man are the
same for words and lemmas. These documents contain very few repeated rare verbs (far fewer than the
other English documents) and those that are repeated are very specific and diverse such that no difference
is seen between the two distributions.
Noun (word) HHI score Count
Count 0.9412 33
driver 0.2985 28
horses 0.9050 20
room 0.4000 20
time 0.1150 20
door 0.5986 17
place 0.6797 16
night 0.4667 15
Table 3: Dracula - most frequent noun words
in which some artistic license may be used.
These findings suggest that when aiming to en-
courage consistency in the translation of novels,
the focus should be on nouns. As for adjectives,
less frequent in novels than verbs and nouns (Table
1), further analysis may show whether consistency
varies depending on function (e.g. modifier, pred-
icate adjective) or frequency as well. The transla-
tion of pronouns also requires investigation.
Natural Science: The natural science text On
the Origin of Species exhibits a similar pattern of
translational consistency to novels. This is perhaps
not surprising as 19th century British natural sci-
ence texts would have had the same middle-class
audience as the novels of the same era. The trans-
lation of modern scientific texts may or may not
follow this pattern.
Instruction Manuals: In the Xerox Manual
nouns receive the highest average HHI score at
the word level. When considering what lemmas
the source words align to in the translation, nouns
again score the highest, closely followed by adjec-
tives and rare verbs. This overall pattern makes
sense as in an instruction manual it is important to
identify both the actions and entities involved at
each step. Adjectives will help the user correctly
identify the intended entities. The word-level HHI
scores for the most frequently used (and aligned)
rare verbs are given in Table 4.
The verb process has several translations in
French: traitement (?treatment?/?processing?),
traiter (?process?) and exe?cuter (?execute?).
(Note that traitement is in fact a noun, reflecting
a change in the structure of the sentence.) The
14
Rare Verb (word) HHI score Count
process 0.5729 109
previewing 0.5868 33
previewed 0.6399 19
verifying 0.5556 18
formatted 0.3244 15
scans 1.0000 13
formatting 0.4380 11
dithering 0.4380 11
Table 4: Xerox Manual - most frequent rare verb
words
resulting translations into French are all clear, so
this may simply be a reflection of a difference in
terminology between English and French, at least
as used by Xerox. For example:
English: Process the page and save the output as an image.
French: Traitement de la page et sauvegarde de la sortie
comme image.
English: Page Settings enable you to describe the pages that
the system is about to process.
French: Les Instructions de page vous permettent de de?crire
les pages que le syste`me va traiter.
English: Load Verification Data, Loads a named verification
data file to process a job.
French: Charger donne?es de ve?rification, Charge un fichier
nomme? de donne?es de ve?rification pour exe?cuter une ta?che.
What is also interesting is that in the English text,
the word process is used as both a noun and a rare
verb. However, it is translated more consistently
when used as a verb (HHI: 0.5729) compared with
its use as a noun (HHI: 0.2576).
In this genre, accuracy and readability are im-
portant and it is acceptable to produce a ?repeti-
tive? or ?boring? text. It may, therefore, be ap-
propriate to encourage translational consistency of
nouns, rare verbs and adjectives in instructions.
Unlike with novels, it would make sense that all
entities in an instruction manual are of importance.
Public Information: In the French Revolution
to 1945 and Nuclear Testing documents, adjectives
score highest, followed by nouns. Word-level HHI
scores for the most frequent (and aligned) adjec-
tives in the French Revolution to 1945 document
are presented in Table 5.
Using a manual inspection of those nouns that
appear next to (i.e. directly after) the adjective in
French, the possibility that these nouns were se-
mantically light was explored. Focussing on the
English translation, WordNet (Miller, 1995) was
used to ascertain the distance of the noun from the
root of the relevant hierarchy. The assumption is
Adjective (word) HHI score Count
nationale (national) 0.8233 75
europe?enne (European) 0.8232 64
e?conomique (economic) 0.8575 40
constitutionnel (constitutional) 0.9474 37
franc?aise (French) 0.4288 37
constitutionnelle (constitutional) 1.0000 31
franc?ais (French) 0.7899 26
autres (other) 0.8496 25
Table 5: French Revolution to 1945 - most fre-
quent adjective words
the semantically light nouns appear closer to the
root than other nouns. For all 82,115 noun synsets
in WordNet, the average minimum and maximum
depths to the root are 7.25 and 7.70 respectively.
Taking the adjective economic (e?conomique in
French) in the French Revolution to 1945 docu-
ment as an example, the nouns it is paired with
(e.g. expansion, cooperation, development, ac-
tion, council, etc.) typically have depths below
the average and therefore could be considered se-
mantically light. The adjectives used in the text
include constitutionnel / constitutionnelle (?con-
stitutional?), e?conomique (?economic?) and na-
tionale (?national?). These words are rather spe-
cific (or ?semantically heavy?), so there may be
few alternative valid translations to choose from.
This is supported by Melamed?s (1997) notion of
semantic entropy, in which more specific words re-
ceive lower entropy scores, reflecting greater con-
sistency in translation. For texts of this genre, it
may be appropriate to encourage the consistent
translation of adjectives and nouns, allowing for
more freedom in the translation of verbs.
News Articles: The pattern for news articles is
a little less predictable, although a similar pattern
(to other document types) can be seen for light,
mid-range and rare verbs. This may be due to the
short length of the texts (circa 2,000 words) which
may not be sufficient to establish a stable pattern.
Or it may be that there are different writing styles
within the news genre dependent on the type or
subject of the ?story?.
4.2 Consistency in Automated Translation
The results of a similar analysis of translational
consistency in phrase-based SMT are presented in
Table 6. Overall, consistency is much higher than
in translations produced by human translators. But
what does this mean? Is the problem of consis-
tency in SMT non-existent? In short, no; there are
15
POS Category Xerox Manual Origin of Species Dracula The Invisible Man
Automated Human Automated Human Automated Human Automated Human
Noun 0.8502 0.6995 0.8481 0.6109 0.8318 0.6182 0.8308 0.6290
Adj 0.6871 0.5900 0.6333 0.4390 0.6543 0.4191 0.6966 0.5110
Verb (all) 0.7131 0.5568 0.6023 0.4001 0.5764 0.3631 0.5829 0.4159
Light Verb 0.4919 0.3256 0.4538 0.2339 0.4310 0.2477 0.4873 0.3139
Mid-Range Verb 0.7160 0.5766 0.5927 0.4140 0.6301 0.4175 0.6271 0.4797
Rare Verb 0.8955 0.6485 0.8195 0.4592 0.8571 0.5000 0.8750 0.4218
Table 6: Automated Translation: Average HHI scores taken for words in automated translations as com-
pared with the scores from human translations. Scores are provided in the range of 0 to 1
still areas in which consistency is a real problem,
but one needs to look more closely at the data to
find the problems.
Any consistency in the output of an SMT sys-
tem will be accidental, and not by design. It is a
reflection of the data that the system was trained
with and represents the ?best? choice for translat-
ing a word or phrase, as determined by scores from
the phrase table and language model. Carpuat
and Simard (2012) suggest that consistency in the
source side local context may be sufficient to con-
strain the phrase table and language model to pro-
duce consist translations. It is also important to
note that the outcome is very much dependent
on the system used to perform the translation.
Carpuat and Simard (2012) suggest that weaker
SMT systems (i.e. those that report lower BLEU
scores) may be more consistent than their stronger
counterparts due to fewer translation options.
There are several possibilities. A word in the
source language may be translated:
? Completely consistently (HHI = 1);
? Very inconsistently (HHI ? 0);
? or anywhere in between
Additionally, a translation that is deemed to be
completely consistent may be either correct or in-
correct. With humans, we assume the translation
output to be of a high standard but we cannot as-
sume the same of an SMT system.
Examples of completely consistent translations
are horses as ?chevaux?, man as ?homme? and
nails as ?clous?. All are taken from Dracula.
While horses and man are translated correctly,
?clous? is an incorrect translation of nails which
the context of the novel refer to Dracula?s finger-
nails. ?ongles? would have been the correct trans-
lation. The word ?clous? is typically used in the
sense of nails used in construction. This is an ex-
ample of a translation that could result either from
lack of sufficient local context (for disambigua-
tion) or because ?ongles? is not present in the data
the SMT system was trained on.
Examples of inconsistent translations are for the
body parts arm and hand in the text of Drac-
ula. arm is translated either correctly as ?bras?
(arm, body part) or incorrectly as ?armer? (the
verb ?to arm?). hand is translated correctly as
?main? (?hand?) and incorrectly as co?te? (?side?)
and ?part? (?portion?). In both cases, the correct
translation was available to the system and a more
accurate translation could have been obtained had
the correct translation been identified and its con-
sistency encouraged.
Ambiguous words in particular can cause trou-
ble for SMT systems. There are many words that
can function as both a verb and a noun, e.g. pro-
cess and count. Local context might not always
be sufficient to provide the correct disambigua-
tion, resulting in opportunities for incorrect trans-
lations.
An example of where an ambiguous word
results in problems is in the translation of
count (i.e. Count Dracula) as: omitted (4),
?compter? (21), ?compatage? (2), ?comte? (1) and
?de?pouillement? (5). The only acceptable transla-
tion from this set is ?comte?. As for the reaming
options: ?compte? and ?comptage? are both verbs
meaning ?to count? and ?de?pouillement? is a noun
meaning ?starkness?, ?austerity? or ?analysis? (of
data).
5 Conclusion
The analysis of human translation presented in this
paper is a first attempt to understand where and
when it might be appropriate to encourage con-
sistency in an SMT system. I consider genre as
the where and parts-of-speech as the when, but
other interpretations are also possible. On the
whole, it seems reasonable to encourage the con-
16
sistent translation of nouns, across all genres. In
addition, encouraging consistency in the transla-
tion of rare verbs and adjectives for technical doc-
uments and of adjectives for public information
documents may also prove beneficial.
With respect to verbs, variation in verb consis-
tency has been shown to correlate with frequency
(as a proxy to identify light and rare verbs). Given
the low consistency with which humans translate
light verbs, encouraging their consistency in auto-
mated translation would be undesirable.
Automated translation may look very consis-
tent on the surface, but it is necessary to look be-
yond this to see the errors. While humans may
make inconsistent translations, we trust that these
inconsistencies will not confuse or mislead the
reader. SMT systems on the other hand gener-
ate their translations based on statistics that say
what the ?best choice? might be, both at the
word/phrase level (through the phrase table) and
overall (through the language model). Further-
more, they do nothing to guarantee consistency -
this occurs by chance, whether desirable or not.
As a result, inconsistencies may arise that make
the translations difficult to read. These inconsis-
tencies are not predictable and could occur in any
SMT system.
6 Future Work
The findings presented in this paper are sugges-
tive but only a small number of texts have been
included for each genre. The analysis could be
extended to include a larger set of documents and
different language pairs (the only requirement is
for a POS tagger for the source language). Multi-
ple translations of the same document could also
be considered to identify whether similar patterns
can be observed for different translators.
There are a number of possible ways in which
to use this information to inform the design of a
SMT system. I have shown that SMT systems
are capable of highly consistent translations but
this consistency cannot be guaranteed and there
is the possibility that the translations will be con-
sistent and incorrect. Also, Carpuat and Simard
(2012) have shown that inconsistent translations
in SMT often indicate translation errors. A sys-
tem which encourages translations which are both
consistent and correct (or at least acceptable) for
words that belong to a predefined set (e.g. by POS
tag) is desirable. This ?encouragement? could be
achieved using rewards delivered via feature func-
tions or within n-best list re-ranking ? hypothe-
ses which make re-use of the same translation(s)
for repetitions of the same source word would be
ranked higher than those that introduced incon-
sistencies. Revisiting the cache-based models of
(2010a; 2010b) and Gong et al (2011) could pro-
vide a possible starting point.
The initial focus could be on nouns, which are
translated by human translators with high consis-
tency for all genres. Many nouns are used either to
specify entities that are only mentioned once in a
text (essentially setting the scene for more promi-
nent entities), or as ?predicate nominals? on those
more prominent entities (e.g. in ?...is a horrific
story?). However, other nouns occur within the
Noun Phrases (NPs) that make up part of a corefer-
ence chain, of subsequent reference to prominent
entities.
As an extension to this work I will aim to inves-
tigate the consistency of translation of those nouns
that belong to coreference chains and ultimately,
to build a system that makes use of the resulting in-
formation. Work has already started to construct a
parallel corpus in which coreference chains are an-
notated so that the translation of coreference (both
NPs and pronouns) may be studied in more depth.
Another question worth considering is whether
it would be desirable to replicate aspects of low
consistency in human translation by encouraging
inconsistent (but still acceptable) translations of
certain words or word categories. My instinct is
that this could lead to translations that better ap-
proximate those produced by humans.
7 Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement 287658 (EU BRIDGE). Thanks to Pro-
fessor Bonnie Webber for her guidance and nu-
merous helpful suggestions and to the three anony-
mous reviewers for their feedback.
References
Anne Abeille?, Lionel Cle?ment, and Alexandra Kinyon.
2000. Building a treebank for french. In In Pro-
ceedings of the LREC 2000.
Marine Carpuat and Michel Simard. 2012. The trouble
with smt consistency. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, WMT
17
?12, pages 442?449, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions,
DEW ?09, pages 19?27, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Jeremy H. Clear. 1993. The digital word. chapter The
British national corpus, pages 163?187. MIT Press,
Cambridge, MA, USA.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural
Language, HLT ?91, pages 233?237, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 909?919, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Michael A.K. Halliday and Ruqaiya Hasan. 1976. Co-
hesion in English. Longman, London.
Yifan He, Yanjun Ma, Andy Way, and Josef van Gen-
abith. 2011. Rich linguistic features for transla-
tion memory-inspired consistent translatio. In Pro-
ceedings of Machine Translation Summit XIII, pages
456?463.
Masaki Itagaki, Takako Aikawa, and Xiaodon He.
2007. Automatic validation of terminology consis-
tency with statistical method. In Proceedings of Ma-
chine Translation Summit XI, pages 269?274. Euro-
pean Associaton for Machine Translation.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In Conference
Proceedings: the tenth Machine Translation Sum-
mit, pages 79?86. AAMT, AAMT.
Kazem Lotfipour-Saedi. 1997. Lexical cohesion and
translation equivalence. Meta: Journal des Traduc-
teurs / Meta: Translators? Journal, 42(1):185?192.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrim-
inative learning: a translation memory-inspired ap-
proach. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1,
HLT ?11, pages 1239?1248, Stroudsburg, PA, USA.
Association for Computational Linguistics.
I. Dan Melamed. 1997. Measuring semantic entropy.
In Proceedings of the SIGLEX Workshop on Tagging
Text with Lexical Semantics, pages 41?46.
George A. Miller. 1995. Wordnet: A lexical database
for english. Commun. ACM, 38(11):39?41, Novem-
ber.
Jane Morris and Graeme Hirst. 1991. Lexical cohesion
computed by thesaural relations as an indicator of
the structure of text. Comput. Linguist., 17(1):21?
48, March.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical align-
ment models. Computational Linguistics, 29:19?51,
March.
Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011. A universal part-of-speech tagset. In IN
ARXIV:1104.2086.
Raphael Salkie. 2010. The intersect trans-
lation corpus. Available on the web:
http://arts.brighton.ac.uk/staff/raf-salkie/portfolio-
of-major-works/intersect.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
Jo?rg Tiedemann. 2010a. Context adaptation in sta-
tistical machine translation using models with ex-
ponentially decaying cache. In Proceedings of the
2010 Workshop on Domain Adaptation for Natural
Language Processing, DANLP 2010, pages 8?15,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jo?rg Tiedemann. 2010b. To cache or not to cache?
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
Metrics MATR, pages 189?194, Uppsala, Sweden,
July. Association for Computational Linguistics.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 417?426, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tong Xiao, Jingbo Zhu, and Shujie Yao. 2011.
Document-level consistency verification in machine
translation. In Proceedings of MT summit XIII,
pages 131?138.
18
