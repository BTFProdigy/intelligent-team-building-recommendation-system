Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 74?78,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
The Effect of Cognitive Load on a Statistical Dialogue System
M. Gas?ic??, P. Tsiakoulis?, M. Henderson?, B. Thomson?, K. Yu?, E. Tzirkel?? and S. Young?
?Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{mg436, pt344, mh521, brmt2, ky219, sjy}@eng.cam.ac.uk
??General Motors Advanced Technical Centre, Israel
eli.tzirkel@gm.com
Abstract
In recent years statistical dialogue systems
have gained significant attention due to their
potential to be more robust to speech recogni-
tion errors. However, these systems must also
be robust to changes in user behaviour caused
by cognitive loading. In this paper, a statistical
dialogue system providing restaurant informa-
tion is evaluated in a set-up where the sub-
jects used a driving simulator whilst talking to
the system. The influences of cognitive load-
ing were investigated and some clear differ-
ences in behaviour were discovered. In partic-
ular, it was found that users chose to respond
to different system questions and use different
speaking styles, which indicate the need for an
incremental dialogue approach.
1 Introduction
A spoken dialogue system enables a user to obtain
information while using their hands to perform some
other task, which in many cases is the user?s primary
task. A typical example is an in-car spoken dialogue
system where the spoken interaction is secondary to
the main task of driving the car (Weng et al, 2004).
This domain is particularly challenging since it in-
volves dealing with the errors caused by the varying
noise levels and changes in user behaviour caused
by the cognitive load.
A statistical approach to dialogue modelling has
been proposed as a means of automatically optimis-
ing dialogue policies. In particular, the partially ob-
servable Markov decision process (POMDP) model
for dialogue provides a representation of varying
levels of uncertainty of the user input, yielding more
robust dialogue policies (Williams and Young, 2007;
Thomson and Young, 2010; Young et al, 2010).
Another thread of research deals with speech
interfaces for in-car applications, see (Baron and
Green, 2006) for a review. Past research has inves-
tigated the extent to which speaking is cognitively
less demanding than typing (Gartner et al, 2001;
Tsimhoni et al, 2004; Kun et al, 2007). In addi-
tion, considerable research has examined how driv-
ing safety is influenced by a dialogue system (Lai
et al, 2001; Lee et al, 2001; Nielsen et al, 2008).
However, to the best of our knowledge, little work
has been done to investigate the effect of the cog-
nitive load when interacting with a real conversa-
tional spoken dialogue system. The work presented
in (Mishra et al, 2004) suggests that the user speech
is more disfluent when the user is performing an-
other task. However, this work is based on a Wiz-
ard of Oz framework, where a human provides the
system?s responses. Also, a push-to-talk button was
used for every utterance which will have affected the
natural flow of the dialogue. It is important to know
if the change of cognitive load has an effect on the
speaking style and whether the system can alter its
behaviour to accommodate for this.
In this paper we try to answer these questions by
examining dialogues where users drove a car simu-
lator and talked to an open-microphone fully auto-
mated spoken dialogue system at the same time.
The rest of the paper is organised as follows. Sec-
tion 2 provides an overview of the dialogue system
used and section 3 describes the evaluation set-up.
The analysis of the results is given in Section 4. Sec-
tion 5 concludes the paper.
74
Table 1: Example dialogue task
You are looking for a cheap restaurant and it
should be in the east part of town. Make sure you
get the address of the venue.
2 System overview
The user speaks to the system, and the acoustic sig-
nal is converted by the speech recogniser into a set
of sentence hypotheses, which represents a proba-
bility distribution over all possible things that the
user might have said. The sentence hypotheses are
converted into an N-best list of dialogue acts by a
semantic decoder. Since the dialogue state cannot
be directly observed it maintains a probability dis-
tribution over all states, which is called the belief
state. The belief state is updated at every user turn
using Bayesian inference treating the input dialogue
acts as evidence. Based on belief state, the optimal
system act is selected using a policy and which is
trained automatically using reinforcement learning.
The abstract system dialogue act is converted to an
appropriate utterance by a natural language genera-
tor and then converted to speech by an HMM-based
speech synthesiser. To enable in-car speech inter-
action via mobile phone, a VoIP interface is imple-
mented. The domain is Cambridge restaurant infor-
mation with a database of about 150 venues and 7
slots that users can query.
3 Evaluation set-up
Our goal is to understand system performance
when driving. However, due to the safety restric-
tions, performance was tested using a driving simu-
lator. The following sections explain the set-up.
3.1 Car simulator
The car simulator used in the evaluation was the
same as in (Davies and Robinson, 2011). It con-
sists of a seat, a steering wheel and pedals, which
give a realistic cab-like environment for the par-
ticipants. There is also a projection screen which
largely fills the visual field of the driver. The sim-
ulation software is a modified version of Rockstar
Games? Grand Theft Auto: San Andreas, with over
500 km of roads. For the purpose of the evaluation,
the subjects were asked to drive on the main motor-
way, to keep the lane and not to drive over 70mph.
3.2 Subjects
For the study 28 subjects were recruited, 22 where
native speakers. Each subject had to complete three
scenarios: (1) to drive the car simulator for 10 min-
utes, (2) to talk to the system for 7 dialogues and (3)
to talk to the system for 7 dialogues while driving.
The scenarios were in counter-balanced order.
While they were driving, the speed and the road
position were recorded. If the scenario involved
talking to the system, the instructor read out the di-
alogue task (see an example in Table 1) and dialled
the phone number. In addition, the subject had the
dialogue task displayed on a small screen next to the
driving wheel. The subject talked to the system us-
ing loud speaker mode on the mobile phone.
4 Results
To examine the influence of cognitive load, the
following examinations were performed. First, we
investigate if the subjects felt any change in the cog-
nitive load (Section 4.1). Then, in Section 4.2, we
examine how the driving was influenced by the sub-
jects talking to the system. Finally, we investigate
how successfully the subjects were able to complete
the dialogue tasks while driving (Section 4.3). This
is followed with an examination of the conversa-
tional patterns that occurred when the subjects were
driving whilst talking to the system (Section 4.4).
4.1 Cognitive load
After each scenario the subjects were asked to an-
swer five questions based on the NASA-TLX self-
reporting scheme for workload measurement. They
answered by providing a rating from 1 (very easy)
to 5 (very hard). The averaged results are given
in Table 2. We performed a Kruskal test, followed
by pairwise comparisons for every scenario for each
answer and all differences are statistically signifi-
cant (p < 0.03) apart from the differences in the
frustration, the stress and the pace between talking
and talking and driving. This means that they were
clearly able to feel the change in cognitive load.
75
Table 2: Subjective evaluation of the cognitive load
Driving Talking Talking&Driving
How mentally demanding was the scenario?
1.61 2.21 2.89
How hurried was the pace of the scenario?
1.21 1.71 1.89
How hard did you have to work?
1.5 2.32 2.96
How frustrated did you feel during the task?
1.29 2.61 2.61
How stressed did you feel during the task?
1.29 2.0 2.32
Table 3: Analysis of driving speed to determine which
measures are larger for Talking&Driving than Driving
Measure Percentage of
users
Confidence in-
terval
Higher speed 8% [1%, 25%]
Larger std.dev 77% [56%, 91%]
Larger entropy 85% [65%, 95%]
4.2 Driving performance
For 26 subjects we recorded position on the road
and the speed. Since these measurements vary sig-
nificantly across the subjects, for each subject we
calculated the average speed, the standard deviation
and the entropy and similarly for the average posi-
tion in the lane. For the speed, we computed how
many subjects had a higher average speed when they
were talking and driving versus when they were just
talking and similarly for the standard deviation and
the entropy. The results are given in Table 3. It
can be seen that the user?s speed is lower when they
are driving and talking, however, the increase in the
standard deviation and the entropy suggest that their
driving is more erratic. No significant differences
were observed for the road position.
4.3 Dialogue task completion
Each participant performed 14 dialogues, 7 for each
scenario. In total, there were 196 dialogues per sce-
nario. After each dialogue they told the instruc-
tor if they thought the dialogue was successful, and
this information was used to compute the subjective
Table 4: Subjective and Objective Task completion (196
Dialogues per scenario)
Talking Talking&Driving
Subjective 78.6% 74.0%
Objective 68.4% 64.8%
Table 5: Percentage of turns that are in line with the pre-
defined task
Talking Talking&Driving
Percentage of turns
that follow the task
98.3% 96.79%
Number of turns 1354 1388
completion rate. In addition, all dialogues were tran-
scribed and analysed to see if the system provided
information the user asked for and hence calculate
an objective completion rate. The results are given
in Table 4. These differences are not statistically sig-
nificant due to the small sample size. However, it
can be seen that the trend is that the dialogues where
the subject was not performing another task at the
same time were more successful. Also, it is inter-
esting that the subjective scores are higher than the
objective ones. This can be explained by the fact that
the dialogue tasks were predefined and the subjects
do not always pay sufficient attention to their task
descriptions.
4.4 Conversational patterns
Given that the subjects felt the change of cognitive
load when they were talking to the system and op-
erating the car simulator at the same time, we were
interested to see if there are any changes in the dia-
logues which might suggest this.
First, we examine how well they follow the given
task on a turn-to-turn basis. For example, if the task
is to find a cheap restaurant and if at some point
in the dialogue the user says I?d like an expensive
restaurant that turn is not consistent with the task.
The results are given in Table 5 and they are statisti-
cally significant (p < 0.01).
We then examine the number of contradictions on
a turn-to-turn basis. For example, if the user says I?d
like a cheap restaurant and later on they say I?d like
76
Table 6: User obedience to system questions
1. system requests or confirms and requests
Samples Obedience
Talking 392 67.6%
Talking&Driving 390 63.9%
2. system confirms
Samples Obedience
Talking 91 73.6%
Talking&Driving 92 81.5%
an expensive restaurant the latter turn is clearly a
contradiction. The percentage of contradicting turns
is less than 1% and the difference between the sce-
narios is not statistically significant. This suggests
that while users tend to forget the task they are given
when they are driving, they still act rationally despite
the increase in the cognitive load.
The next analysis concerns the user obedience,
i.e. the extent to which subjects answer the sys-
tem questions. We grouped the system questions in
two classes. The first class represents the questions
where the system requests a value for a particular
slot, for instance What part of town are you looking
for? and the questions where the system confirms
and requests at the same time, for instance You are
looking for a cheap restaurant. What part of town
are you looking for? The second class correspond to
system confirmations, for example Did you say you
are looking for a cheap restaurant? The percent-
age of the obedient user turns per class is given in
Table 6. Due to the small sample size these results
are not statistically significant. Still, it is interest-
ing to see that when driving the subjects appear to
be more obedient to the system confirmations than
when they are just talking. When the system makes
a confirmation, the user can answer with simple yes
or no, whereas when the system requests the value
of a particular slot, the user needs to think more to
provide an answer.
The number of barge-ins, the number of filler
words and the average speech intensity vary con-
siderably among the subjects. Therefore, we aver-
age these statistics per user and examine the number
of users for which the particular measure is greater
for the scenario where they talked to the system and
drove the simulator at the same time. The results
Table 7: Analysis of measures related to the speaking
style which values are larger for Talking&Driving than
Talking
Measure % of users Conf. interval
More barge-ins 87% [69%, 96%]
More fillers 73% [54%, 88%]
Higher intensity 67% [47%, 83%]
(Table 7) show that the number of barge-ins and the
number of fillers is significantly greater for the sce-
nario when they are talking and driving and the in-
tensity on average tend to be greater.
5 Conclusion and Future work
There are several important observations arising
from this study. Firstly, dialogues with cognitively
loaded users tend to be less successful. This sug-
gests that the system should alter its behaviour to
match user behaviour and alleviate the cognitive
load in order to maintain the level of performance.
This necessitates rapid on-line adaptation of dia-
logue policies.
The second observation is that cognitively loaded
users tend to respond to some types of system ques-
tions more than others. This indicates that the user
model within a POMDP dialogue system should be
conditioned on a measure of cognitive load.
Finally, this study has found that users barge-in
and use filler words significantly more often when
they are cognitively loaded. This suggests the need
for a much richer turn-taking model which allows
the system to use back-channels and barge-in when
the user hesitates. An obvious candidate is the in-
cremental approach (Schlangen and Skantze, 2009;
DeVault et al, 2009) which allows the system to pro-
cess partial user inputs, back-channels, predict short
term user input and interrupt the user during hesita-
tions. While incremental dialogue is a growing area
of study, it has not so far been examined in the con-
text of dialogue for secondary tasks. We signpost
this as an important area for future work.
Acknowledgments
We would like to thank to Peter Robinson and Ian
Davies for their help with the experiments.
77
References
A Baron and P Green. 2006. Safety and Usability of
Speech Interfaces for In-Vehicle Tasks while Driving:
A Brief Literature Review. Technical Report UMTRI-
2006-5.
I Davies and P Robinson. 2011. Emotional investment
in naturalistic data collection. In International Con-
ference on Affective Computing and Intelligent Inter-
action.
D DeVault, K Sagae, and DR Traum. 2009. Can I fin-
ish? Learning when to respond to incremental inter-
pretation results in interactive dialogue. In 10th An-
nual SIGDIAL meeting on Discourse and Dialogue.
U Gartner, W Konig, and T Wittig. 2001. Evaluation of
Manual vs. Speech Input When Using a Driver Infor-
mation System in Real Traffic. In International Driv-
ing Symposium on Human Factors in Driving Assess-
ment, Training and Vehicle Design.
A Kun, T Paek, and Z? Medenica. 2007. The effect of
speech interface accuracy on driving performance. In
Interspeech.
J Lai, K Cheng, P Green, and O Tsimhoni. 2001. On the
Road and on the Web? Comprehension of synthetic
and human speech while driving. In SIGCHI.
JD Lee, B Caven, S Haake, and TL Brown. 2001.
Speech-based Interaction with In-vehicle Computers:
The Effect of Speech-based E-mail on Drivers? Atten-
tion to the Roadway. Human Factors, 43:631?640.
R Mishra, E Shriberg, S Upson, J Chen, F Weng, S Pe-
ters, L Cavedon, J Niekrasz, H Cheng, and H Bratt.
2004. A wizard of Oz framework for collecting spo-
ken human-computer dialogs. In Interspeech.
BS Nielsen, B Harsham, B Raj, and C Forlines. 2008.
Speech-Based UI Design for the Automobile. Hand-
book of Research on User Interface Design and Eval-
uation for Mobile Technology, pages 237?252.
David Schlangen and Gabriel Skantze. 2009. A general,
abstract model of incremental dialogue processing. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, EACL ?09, pages 710?718.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken di-
alogue systems. Computer Speech and Language,
24(4):562?588.
O Tsimhoni, D Smith, and P Green. 2004. Address Entry
While Driving: Speech Recognition Versus a Touch-
Screen Keyboard. Human Factors, 46:600?610.
F Weng, L Cavedon, B Raghunathan, D Mirkovic,
H Cheng, H Schmidt, H Bratt, R Mishra, S Peters,
L Zhao, S Upson, E Shriberg, and C Bergmann. 2004.
Developing a conversational dialogue system for cog-
nitively overloaded users. In Proceedings of the Inter-
national Congress on Intelligent Transportation Sys-
tems.
JD Williams and SJ Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,
B Thomson, and K Yu. 2010. The Hidden Information
State Model: a practical framework for POMDP-based
spoken dialogue management. Computer Speech and
Language, 24(2):150?174.
78
Proceedings of the SIGDIAL 2013 Conference, pages 154?156,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Parlance system: a data-driven,
incremental, spoken dialogue system for interactive search
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Gasic, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, Steve Young
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
The Parlance system for interactive
search processes dialogue at a micro-
turn level, displaying dialogue phe-
nomena that play a vital role in hu-
man spoken conversation. These di-
alogue phenomena include more nat-
ural turn-taking through rapid sys-
tem responses, generation of backchan-
nels, and user barge-ins. The Par-
lance demonstration system differen-
tiates from other incremental systems
in that it is data-driven with an infras-
tructure that scales well.
1 Introduction
The Parlance system provides interactive
search through a Spoken Dialogue System
(SDS). This SDS aims to be incremental to al-
low for more natural spoken interaction. Tra-
ditionally, the smallest unit of speech process-
ing for interactive systems has been a full ut-
terance with strict, rigid turn-taking. The
Parlance architecture, however, is an incre-
mental framework that allows for processing
of smaller ?chunks? of user input, which en-
ables one to model dialogue phenomena such
as barge-ins and backchannels. This work is
carried out under the FP7 EC project Par-
lance 1, the goal of which is to develop inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
San Francisco. An example dialogue is given
in Table 1.
?Authors are in alphabetical order
1http://www.parlance-project.eu
SYS Thank you for calling the Parlance Restaurant
system. You may ask for information by cuisine
type, price range or area. How may I help you?
USR I want to find an Afghan restaurant.........which is
in the cheap price range.
SYS .......................................................[uhuhh]........
The Helmand Palace is a cheerful setting for au-
thentic Afghan cuisine.
USR What is the address and phone number?
SYS The address 2424 Van Ness Ave ....
Table 1: Example dialogue excerpt for restaurant in-
formation in San Francisco
2 Background
Previous work includes systems that can deal
with ?micro-turns? (i.e. sub-utterance process-
ing units), resulting in dialogues that are more
fluid and responsive. This has been backed up
by a large body of psycholinguistic literature
that indicates that human-human interaction
is in fact incremental (Levelt, 1989).
It has been shown that incremental dia-
logue behaviour can improve the user experi-
ence (Skantze and Schlangen, 2009; Baumann
et al, 2011; Selfridge et al, 2011) and en-
able the system designer to model several di-
alogue phenomena that play a vital role in
human discourse (Levelt, 1989) but have so
far been absent from systems. These dialogue
phenomena that will be demonstrated by the
Parlance system include more natural turn-
taking through rapid system responses, gener-
ation of backchannels and user barge-ins. The
system differentiates from other incremental
systems in that it is entirely data-driven with
an infrastructure that potentially scales well.
3 System Architecture
Figure 1 gives an overview of the Par-
lance system architecture, which maintains
154
LOCAL SEARCH ENGINE
AUTOMATIC SPEECH RECOGNITION
NLG
AUDIO I/O
TTS
BACKCHANNEL GENERATOR
IM
MIM
HUB
KNOWLEDGE BASE
WavePackets
1-Best Words
Segmentlabel
N-Best Phrase List
WavePackets
Micro-Turn Dialogue Act
System Dialogue Act
String Packets
StringPackets
VoIP Interface (PJSIP)
N-best Dialogue Act Units
 API call ( + metadata)
Search Response
Partial Dialogue Act (in case of interruption)
PartialString(in case of interruption)SPOKEN LANGUAGE UNDERSTANDING Decode from t0 to t1
Figure 1: Overview of the Parlance system
architecture
the modularity of a traditional SDS while at
the same time allowing for complex interaction
at the micro-turn level between components.
Each component described below makes use
of the PINC (Parlance INCremental) dialogue
act schema. In this scheme, a complete dia-
logue act is made up of a set of primitive di-
alogue acts which are defined as acttype-item
pairs. The PINC dialogue act scheme supports
incrementality by allowing SLU to incremen-
tally output primitive dialogue acts whenever
a complete acttype-item pair is recognised with
sufficient confidence. The complete dialogue
act is then the set of these primitive acts out-
put during the utterance.
3.1 Recognition and Understanding
The Automatic Speech Recogniser (ASR) and
Spoken Language Understanding (SLU) com-
ponents operate in two passes. The audio in-
put is segmented by a Voice Activity Detec-
tor and then coded into feature vectors. For
the first pass of the ASR2, a fast bigram de-
coder performs continuous traceback generat-
ing word by word output. During this pass,
while the user is speaking, an SLU module
called the ?segment decoder? is called incre-
2http://mi.eng.cam.ac.uk/research/dialogue/
ATK_Manual.pdf
mentally as words or phrases are recognised.
This module incrementally outputs the set of
primitive dialogue acts that can be detected
based on each utterance prefix. Here, the ASR
only provides the single best hypothesis, and
SLU only outputs a single set of primitive dia-
logue acts, without an associated probability.
On request from the Micro-turn Interaction
Manager (MIM), a second pass can be per-
formed to restore the current utterance using a
trigram language model, and return a full dis-
tribution over the complete phrase as a con-
fusion network. This is then passed to the
SLU module which outputs the set of alter-
native complete interpretations, each with its
associated probability, thus reflecting the un-
certainty in the ASR-SLU understanding pro-
cess.
3.2 Interaction Management
Figure 1 illustrates the role of the Micro-turn
Interaction Manager (MIM) component in the
overall Parlance architecture. In order to
allow for natural interaction, the MIM is re-
sponsible for taking actions such as listening to
the user, taking the floor, and generating back-
channels at the micro-turn level. Given various
features from different components, the MIM
selects a micro-turn action and sends it to the
IM and back-channel generator component to
generate a system response.
Micro-turn Interaction Manager A
baseline hand-crafted MIM was developed
using predefined rules. It receives turn-taking
information from the TTS, the audio-output
component, the ASR and a timer, and updates
turn-taking features. Based on the current
features and predefined rules, it generates
control signals and sends them to the TTS,
ASR, timer and HUB. In terms of micro-turn
taking, for example, if the user interrupts
the system utterance, the system will stop
speaking and listen to the user. The system
also outputs a short back-channel and stays in
user turn state if the user utterance provides
limited information.
Interaction Manager Once the MIM has
decided when the system should take the floor,
it is the task of the IM to decide what to say.
The IM is based on the partially observable
155
Markov decision process (POMDP) frame-
work, where the system?s decisions can be op-
timised via reinforcement learning. The model
adopted for Parlance is the Bayesian Update
of Dialogue State (BUDS) manager (Thom-
son and Young, 2010). This POMDP-based
IM factors the dialogue state into condition-
ally dependent elements. Dependencies be-
tween these elements can be derived directly
from the dialogue ontology. These elements
are arranged into a dynamic Bayesian network
which allows for their marginal probabilities
to be updated during the dialogue, compris-
ing the belief state. The belief state is then
mapped into a smaller-scale summary space
and the decisions are optimised using the nat-
ural actor critic algorithm.
HUB The HUB manages the high level flow
of information. It receives turn change infor-
mation from the MIM and sends commands
to the SLU/IM/NLG to ?take the floor? in the
conversation and generate a response.
3.3 Generation and TTS
We aim to automatically generate language,
trained from data, that is (1) grammatically
well formed, (2) natural, (3) cohesive and (4)
rapidly produced at runtime. Whilst the first
two requirements are important in any dia-
logue system, the latter two are key require-
ments for systems with incremental processing,
in order to be more responsive. This includes
generating back-channels, dynamic content re-
ordering (Dethlefs et al, 2012), and surface
generation that models coherent discourse phe-
nomena, such as pronominalisation and co-
reference (Dethlefs et al, 2013). Incremen-
tal surfacce generation requires rich context
awareness in order to keep track of all that has
been generated so far. We therefore treat sur-
face realisation as a sequence labelling task and
use Conditional Random Fields (CRFs), which
take semantically annotated phrase structure
trees as input, in order to represent long dis-
tance linguistic dependencies. This approach
has been compared with a number of compet-
itive state-of-the art surface realisers (Deth-
lefs et al, 2013), and can be trained from
minimally labelled data to reduce development
time and facilitate its application to new do-
mains.
The TTS component uses a trainable HMM-
based speech synthesizer. As it is a paramet-
ric model, HMM-TTS has more flexibility than
traditional unit-selection approaches and is es-
pecially useful for producing expressive speech.
3.4 Local Search and Knowledge Base
The domain ontology is populated by the local
search component and contains restaurants in
5 regional areas of San Francisco. Restaurant
search results are returned based on their lon-
gitude and latitude for 3 price ranges and 52
cuisine types.
4 Future Work
We intend to perform a task-based evaluation
using crowd-sourced users. Future versions
will use a dynamic Knowledge Base and User
Model for adapting to evolving domains and
personalised interaction respectively.
Acknowledgements
The research leading to this work was funded by the EC
FP7 programme FP7/2011-14 under grant agreement
no. 287615 (PARLANCE).
References
T. Baumann, O. Buss, and D. Schlangen. 2011. Eval-
uation and Optimisation of Incremental Processors.
Dialogue and Discourse, 2(1).
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of INLG, Chicago, USA.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation Using Global Features. In Pro-
ceedings of ACL, Sofia, Bulgaria.
W. Levelt. 1989. Speaking: From Intenion to Articu-
lation. MIT Press.
E. Selfridge, I. Arizmendi, P. Heeman, and J. Williams.
2011. Stability and Accuracy in Incremental Speech
Recognition. In Proceedings of SIGDIAL, Portland,
Oregon.
G. Skantze and D. Schlangen. 2009. Incremental Dia-
logue Processing in a Micro-Domain. In Proceedings
of EACL, Athens, Greece.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
156
Proceedings of the SIGDIAL 2013 Conference, pages 214?222,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
POMDP-based dialogue manager adaptation to extended domains
M. Gas?ic?, C. Breslin, M. Henderson, D. Kim, M. Szummer, B. Thomson, P. Tsiakoulis and S. Young
Cambridge University Engineering Department
{mg436,cb404,mh521,dk449,mos25,brmt2,pt344,sjy}@eng.cam.ac.uk
Abstract
Existing spoken dialogue systems are typ-
ically designed to operate in a static and
well-defined domain, and are not well
suited to tasks in which the concepts and
values change dynamically. To handle dy-
namically changing domains, techniques
will be needed to transfer and reuse ex-
isting dialogue policies and rapidly adapt
them using a small number of dialogues in
the new domain. As a first step in this di-
rection, this paper addresses the problem
of automatically extending a dialogue sys-
tem to include a new previously unseen
concept (or slot) which can be then used
as a search constraint in an information
query. The paper shows that in the con-
text of Gaussian process POMDP optimi-
sation, a domain can be extended through
a simple expansion of the kernel and then
rapidly adapted. As well as being much
quicker, adaptation rather than retraining
from scratch is shown to avoid subjecting
users to unacceptably poor performance
during the learning stage.
1 Introduction
Existing spoken dialogue systems are typically de-
signed to operate in a static and well-defined do-
main, and are not well suited to tasks in which
the concepts and values change dynamically. For
example, consider a spoken dialogue system in-
stalled in a car, which is designed to provide in-
formation about nearby hotels and restaurants. In
this case, not only will the data change as the
car moves around, but the concepts (or slots) that
a user might wish to use to frame a query will
also change. For example, a restaurant system de-
signed to be used within cities might not have the
concept of ?al fresco? dining and could not there-
fore handle a query such as ?Find me a French
restaurant where I can eat outside?. In order to
make this possible, techniques will be needed to
extend and adapt existing dialogue policies.
Adaptation can be viewed as a process of im-
proving action selection in a different condition to
the one in which the policy was originally trained.
While adaptation has been extensively studied in
speech recognition (see an overview in (Gales and
Young, 2007)), in spoken dialogue systems it is
still relatively novel and covers a wide range of
possible research topics (Litman and Pan, 1999;
Litman and Pan, 2002; Georgila and Lemon, 2004;
Janarthanam and Lemon, 2010).
A recent trend in statistical dialogue modelling
has been to model dialogue as a partially ob-
servable Markov decision process (POMDP). This
provides increased robustness to errors in speech
understanding and automatic dialogue policy op-
timisation via reinforcement learning (Roy et al,
2000; Zhang et al, 2001; Williams and Young,
2007; Young et al, 2010; Thomson and Young,
2010). A POMDP-based dialogue manager main-
tains a distribution over every possible dialogue
state at every dialogue turn. This is called the
belief state. Based on that distribution the sys-
tem chooses the action that gives the highest ex-
pected reward, measured by the Q-function. The
Q-function for a belief state and an action is the
expected cumulative reward that can be obtained
if that action is taken in that belief state. The opti-
misation typically requires O(105) to O(106) di-
alogues, so is normally done in interaction with a
simulated user (Jurc???c?ek et al, 2011b).
In reinforcement learning, policy adaptation has
been addressed in the context of transfer learn-
ing (Taylor and Stone, 2009). The core idea is to
exploit expertise gained in one domain (source do-
main) to improve learning in another domain (tar-
get domain). A number of techniques have been
developed but they have not been previously ap-
plied to dialogue management.
214
Gaussian process (GP) based reinforcement
learning (Engel, 2005) has been recently applied
to POMDP dialogue policy optimisation in or-
der to exploit the correlations between different
belief states and thus reduce the number of dia-
logues needed for the learning process (Gas?ic? et
al., 2010).
An important feature of a Gaussian process is
that it can incorporate a prior mean and variance
for the function it estimates, in this case the Q-
function. Setting these appropriately can signif-
icantly speed up the process of learning. If the
mean or the variance are estimated in one envi-
ronment, for example a particular user type or a
particular domain, they can be used as a prior for
adaptation in a different environment, i.e. another
user type or another domain. A Gaussian process
does not depend on the belief state but on the cor-
relation between two belief states encoded by the
kernel function. Therefore, if one defines a kernel
function for two belief states in one domain, the
policy can be used in a different domain, provided
that the correlations between belief states follow a
similar pattern.
This paper explores the problem of extending an
existing domain by introducing a previously un-
seen slot. Specifically, a simple restaurant system
is considered which allows a user to search for
restaurants based on food-type and area. This do-
main is then extended by introducing an additional
price-range slot. The policy is trained for the basic
two-slot domain and then reused in the extended
domain by defining a modified kernel function and
using adaptation. This strategy not only allows for
the knowledge of a previously trained policy to be
reused but it also guards against poor performance
in the early stages of learning. This is particularly
useful in a real-world situation where the adapta-
tion is performed in direct interaction with users.
In addition, a potential application of this tech-
nique to reduce the number of training dialogues
is examined. The domain is decomposed into a
series of simple domains and the policy is grad-
ually adapted to the final domain with a smaller
number of dialogues than are normally needed for
training.
The rest of the paper is organised as follows. In
Section 2 the background on Gaussian processes
in POMDP optimisation is given. Then Section 3
gives a description of the Bayesian Update of Di-
alogue State dialogue manager, which is used as
a test-bed for the experiments. In Section 4, a
simple method of kernel modification is described
which allows a policy trained in the basic domain
to be used in an extended domain. Methods of
fast adaptation are investigated in Section 5 and
this adaptation strategy is then tested via interac-
tion with humans using the Amazon Mechanical
Turk service in Section 6. Finally, the use of re-
peated adaptation to speed up the process of policy
optimisation by learning gradually from simple to
more complex domains is explored in Section 7,
before presenting conclusions in Section 8.
2 Gaussian processes in POMDPs
The role of a dialogue policy pi is to map each be-
lief state b ? B into an action a ? A so as to
maximise the expected cumulative reward, a mea-
sure of how good the dialogue is.
The expected cumulative reward is defined by
the Q-function as:
Q(b, a) = Epi
( T?
?=t+1
???t?1r? |bt = b, at = a
)
,
(1)
where r? is the reward obtained at time ? , T is
the dialogue length and ? is the discount factor,
0 < ? ? 1. Optimising the Q-function is then
equivalent to optimising the policy pi.
A Gaussian process (GP) is a non-parametric
Bayesian probabilistic model that can be used
for function regression (Rasmussen and Williams,
2005). It is fully defined by a mean and a kernel
function which defines prior function correlations.
GP-Sarsa is an on-line RL algorithm that mod-
els the Q-function as a Gaussian process (Engel
et al, 2005), Q(b, a) ? GP (0, k((b, a), (b, a)))
where the kernel k(?, ?) is factored into separate
kernels over the belief state and action spaces
kC(b,b?)kA(a, a?). For a sequence of belief state-
action pairs Bt = [(b0, a0), . . . , (bt, at)]T visited
in a dialogue and the corresponding observed im-
mediate rewards rt = [r1, . . . , rt]T, the posterior
of the Q-function for any belief state-action pair
(b, a) is defined by the following:
215
Q(b, a)|rt,Bt ? N (Q(b, a), cov((b, a), (b, a))),
Q(b, a) = kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1rt,
cov((b, a), (b, a)) = k((b, a), (b, a))? kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1Htkt(b, a)
Ht =
?
????
1 ?? ? ? ? 0 0
0 1 ? ? ? 0 0
... . . . . . . ... ...
0 ? ? ? 0 1 ??
?
???? ,
kt(b, a) = [k((b0, a0), (b, a)), . . . , k((bt, at), (b, a))]T,
Kt = [kt((b0, a0)), . . . ,kt((bt, at))]
(2)
where Kt is the Gram matrix ? the matrix of the
kernel function values for visited points Bt, Ht is
a linear operator that captures the reward looka-
head from the Q-function (see Eq. 1) and ?2 is
an additive noise parameter which controls how
much variability in theQ-function estimate we ex-
pect during the process of learning.
If we assume that the Gaussian process
places a prior mean on the Q-function,
Q(b, a) ? GP (m(b, a), k((b, a), (b, a)))
then the posterior mean Q(b, a) is given by (Ras-
mussen and Williams, 2005):
Q(b, a) = m(b, a) + kt(b, a)THTt (HtKtHTt + ?2HtHTt )?1(rt ?mt), (3)
where mt = [m(b0, a0), . . . ,m(bt, at)]T. The
estimate of the variance is same as in Eq. 2.
The Q-function posterior in Eqs. 2 and 3
defines a Gaussian distribution for every be-
lief state-action pair. Thus, when a new be-
lief state b is encountered, for each action a ?
A, there is a Gaussian distribution Q(b, a) ?
N (Q(b, a), cov((b, a), (b, a)))). Sampling from
these Gaussian distributions gives a set of Q-
values for each action {Q(b, a) : a ? A} from
which the action with the highest sampledQ-value
can be selected:
pi(b) = argmax
a
{Q(b, a) : a ? A} . (4)
In this way, the stochastic model of theQ-function
is effectively transformed into a stochastic policy
model, which can be optimised to maximise the re-
ward (Geist and Pietquin, 2011; Gas?ic? et al, 2011;
Gas?ic? et al, 2012).
Due to the matrix inversion in Eq. 2, the compu-
tational complexity of calculating the Q-function
posterior is O(t3), where t is the number of data
points in Bt, and this poses a serious computa-
tional problem. The algorithm used here to ap-
proximate the Gaussian process is the kernel span
sparsification method described in (Engel, 2005).
In this case, only a set of representative data points
is retained ? called the dictionary of visited points.
3 BUDS dialogue manager
The Bayesian Update of Dialogue State (BUDS)
dialogue manager is a POMDP-based dialogue
manager (Thomson and Young, 2010) which fac-
torises the dialogue state into conditionally de-
pendent elements. These elements are arranged
into a dynamic Bayesian network, which allows
for their marginal probability distributions to be
updated during the dialogue. Thus, the belief
state of the BUDS dialogue manager consists of
the marginal posterior probability distribution over
hidden nodes in the Bayesian network. The hidden
nodes in the BUDS system consist of the history
nodes and the goal nodes for each concept in the
dialogue. For instance in a restaurant information
domain these include area, food-type, address.
The history nodes define possible dialogue histo-
ries for a particular concept, eg. system-informed,
user-requested. The goal nodes define possible
values for a particular concept, eg. Chinese, In-
dian. The role of the policy pi is then to map each
216
belief state into a summary action a from the sum-
mary action space A. Once a summary action is
found it is heuristically mapped into the master
action that the system finally takes (Gas?ic? et al,
2012). The master actions are composed of dia-
logue act type and list of slot value pairs. There are
15 dialogue act types in the BUDS system that fa-
cilitate not only simple information providing sce-
narios but also more complex dialogues where the
user can change their mind and ask for alterna-
tives.
To apply GP policy optimisation, a kernel func-
tion must be defined on both the belief state space
B and the action space A. The kernel function
over the belief state b is constructed from the sum
of individual kernels over the hidden node distri-
butions, such that the kernel function of two cor-
responding nodes is based on the expected likeli-
hood kernel (Jebara et al, 2004), which is also a
simple linear inner product:
kB(b,b?) =
?
h
?bh,b?h?, (5)
where bh is the probability distribution encoded
in the hth hidden node. This kernel gives the ex-
pectation of one belief state distribution under the
other.
For history nodes, the kernel is a simple inner
product between the corresponding node distribu-
tions. While it is possible to calculate the kernel
function for the goal nodes in the same way as for
the history nodes, in this case, the choice of sys-
tem action, such as confirm or inform, does not
depend on the actual values. It rather depends on
the shape of the distribution and, in particular, it
depends on the probability of the most likely value
compared to the rest. Therefore, to exploit the cor-
relations further, the kernel over two goal nodes
is calculated as the dot product of vectors, where
each vector represents the corresponding distribu-
tion sorted into order of probability. The only ex-
ceptions are the goal for the method node and the
discourse act node. The former defines whether
the user is searching for a venue by name or by
constraints and the latter defines which discourse
act the user used, eg. acknowledgement, thank you.
Their kernels are calculated in the same way as for
the history nodes.
For the action space kernel, the ?-kernel is used
defined by:
kA(a, a?) = ?a(a?). (6)
where ?a(a?) = 1 iff a = a?.
3.1 TopTable domain
The TopTable domain consists of restaurants in
Cambridge, UK automatically extracted from the
TopTable web service (TopTable, 2012). There are
about 150 restaurants and each restaurant has 7 at-
tributes ? slots. This results in a belief space that
consists of 25 concepts where each concept takes
from 3 to 150 values and each value has a proba-
bility in [0, 1]. The summary action space consists
of 16 summary actions.
3.2 The agenda-based simulated user
In training and testing a simulated user was used.
The agenda-based user simulator (Schatzmann,
2008; Keizer et al, 2010) factorises the user state
into an agenda and a goal. The goal ensures
that the user simulator exhibits consistent, goal-
directed behaviour. The role of the agenda is to
elicit the dialogue acts that are needed for the user
simulator to fulfil the goal. In addition, an er-
ror model adds confusions to the simulated user
input such that it resembles those found in real
data (Thomson et al, 2012). The length of the N-
best list was set to 10 and the confusion rate was
set to 15% during training and testing.1 This error
rate means that 15% of time the true hypothesis is
not in the N-best list. Intermediate experimenta-
tion showed that these confusion rates are typical
of real data.
The reward function was set to give a reward
of 20 for successful dialogues, zero otherwise. In
addition, 1 is deducted for each dialogue turn to
encourage shorter dialogues. The discount factor
? is set to 1 and the dialogue length is limited to
30 turns.
4 Extended domains
Transfer learning is a reinforcement learning tech-
nique which address three problems:
? given a target domain, how to select the
most appropriate source domain from a set of
source domains,
? given a target and a source domain how to
find the relationship between them, and
? given a target and a source domain and the
relationship between them, how to effectively
transfer knowledge between them.
1Except of course where the system is explicitly tested on
varying noise levels.
217
Here we assume that we are given a source and
a target domain and that the relationship between
them is defined by mapping the kernel function.
Knowledge transfer is then effected by adapting
the source domain policy for use in the target do-
main. For the latter, two forms of adaptation are
investigated: one simply continues to update the
set of source data dictionary points with new dic-
tionary points, the second uses the source domain
posterior as a prior for the new target domain.
In this case, the source is a basic restaurant do-
main with slots name, area, food-type, phone, ad-
dress, and postcode. The extended target domain
has an additional price-range slot. We are inter-
ested primarily in training the policy on the ba-
sic domain and testing it on the extended domain.
However, since real applications may also require
a slot to be forgotten, we also investigate the re-
verse where the policy is trained in the extended
domain and tested on the basic domain.
In order to enable the required cross domain
portability, a kernel function defining the correla-
tion between belief states from differing domains
is needed. Since the extended domain has an ex-
tra slot and thus extra hidden nodes, we need to
define the correlations between the extra hidden
nodes and the hidden nodes in the belief state of
the basic domain. This can be performed in vari-
ous ways, but the simplest approach is to specify
which slot from the basic domain is most similar
to the new slot in the extended domain and then
match their corresponding hidden nodes. In that
way the belief state kernel function between two
belief states bB, bE for the basic B and the ex-
tended E domain becomes:
kB(bB,bE) =
?
h?B
?bBh ,bEh?+
?
e/?B
?bBl(e),bEe ?, (7)
where h are the hidden nodes in the basic domain,
e are the hidden nodes in the extended domain and
function l : E? B for each hidden node that does
not exist in the basic domain finds its appropriate
replacement. In the particular case studied here,
the slot area is most similar to the new price-range
slot since they both have a relatively small number
of values, about 5. Hence, l(price-range)? area.
If the cardinality of the mapped slots differ, the
shorter is padded with zeros though other forms of
normalisation are clearly possible.
The (summary) action space for the extended
domain has more actions than the basic domain.
For example, one action that exists in the extended
domain and does not exist in the basic domain is
request(price-range). To define the kernel func-
tion between these sets of actions, one can specify
for each extra action in the extended domain its
most similar action in the basic domain:
kA(aB, aE) =
{
?aB(aE) aE ? AB,
?aB(L(aE)) aE /? AB,
(8)
where function L : AE ? AB for each action
that does not exist in the basic domain finds its
replacement action.
Functions L and l are here defined manually.
However, a simple but effective heuristic would be
to find for each new slot in the extended domain, a
slot in the basic domain with similar cardinality.
Porting in the reverse direction from the ex-
tended to the basic domain is easier since one can
simply disregard the extra hidden nodes and ac-
tions in the kernel calculation.
To experimentally examine the extent to which
this method supports cross domain portability, we
trained policies for both domains until conver-
gence, using 105 dialogues on the simulated user.
We then cross tested them on the mismatching do-
mains at varying user input error rates. The results
are given in Fig. 1.
0 10 20 30 40 50ErrorRate2
0
2
4
6
8
10
12
Rewa
rd
bsc-trn&tstextd-trn&tstextd-trn&bsc-tstbsc-trn&extd-tst
Figure 1: Cross testing policies trained on differ-
ent domains. bsc refers to the basic domain, extd is
the extended domain, trn is training and tst is test-
ing.
From the results it can be seen that the policy
trained for the basic domain has a better perfor-
mance than the policy trained on the extended do-
main, when tested on the matching domain (com-
218
pare bsc-trn&tst with extd-trn&tst). The extended do-
main has more slots so it is more difficult for the
system to fulfil the user request, especially in noisy
conditions. Secondly, the performance of the pol-
icy trained on the extended domain and tested on
the basic domain is close to optimal (compare bsc-
trn&tst with extd-trn&bsc-tst). However, the pol-
icy trained on the basic domain and tested on the
extended domain has much worse performance
(compare bsc-trn&extd-tst with extd-trn&tst). It is
hard for the policy to adequately extrapolate from
the basic to the extended domain. This difference
in performance, however, motivates the need for
adaptation and this is investigated in the next sec-
tion.
5 Adaptation
Adaptation of a policy trained on one domain to
another can be performed in several ways. Here
we examine two adaptation strategies similar to
the method described in (Taylor et al, 2007),
where every action-value for each state in the tar-
get domain is initialised with learned source do-
main values.
The first strategy is to take the policy trained in
the source domain and simply continue training it
in the target domain until convergence. In Gaus-
sian process reinforcement learning, this means
that we assume a zero-mean prior on the Gaussian
process for theQ-function and let the dictionary of
visited points Bt from Eq. 2 consist of both points
visited in the source domain and the extended tar-
get domain, making sure that the Gram matrix
Kt uses extended domain kernel function where
necessary. However, the estimate of the variance
decreases with the number of visited points (see
Eq. 2). The danger therefore when performing
adaptation in this way is that the estimate of vari-
ances obtained in the source domain will be very
small since the policy has already been trained un-
til convergence with a large number of dialogues.
As a consequence, the rate of exploration defined
by sampling in Eq. 4 will be reduced and thus lead
to the subsequent optimisation in the new target
domain falling prematurely into a local optimum.
As an alternative, we propose another adapta-
tion strategy. The estimate of the posterior of the
mean for the Q-function, Q in Eq. 2, from the pol-
icy trained on the basic domain can be taken to be
the prior of the mean when the policy is trained on
the extended domain as in Eq. 3. More precisely, if
Qbsc is the posterior mean of the policy trained on
the basic domain then mextd = Qbsc. In this case
it is also important to make sure that the kernel
function used to calculateQbsc is redefined for the
extended domain where necessary. The prior on
the variance is the original kernel function renor-
malised:
k((b, a), (b?, a?))? k((b,a),(b?,a?))?
k((b,a),(b,a))k((b?,a?),(b?,a?))
.
(9)
Given that the estimate of the mean provides rea-
sonable performance, it is not necessary to place
a flat prior on the variance of the Q-function and
therefore the kernel is normalised as in Eq. 9.
When comparing adaptation strategies, we are
interested in two aspects of performance. The first
is the performance of the policy during training.
The second is how quickly the policy reaches the
optimal performance. For that reason we adopt
the following evaluation scheme. After every 100
adaptation dialogues we test the partially opti-
mised policy with 1000 simulated dialogues, dif-
ferent to the ones used in adaptation. These 1000
dialogues are the same for every test point on the
graph. The results are given in Fig. 2.
0 200 400 600 800 1000 1200 1400 1600Training dialogues20
15
10
5
0
5
10
Rewa
rd
PRIORADAPTTRAINbsc-trn&extd-tstextd-trn&tst
Figure 2: Different adaptation strategies
The lower horizontal line represents the perfor-
mance of the policy trained on the basic source
domain and tested on the extended target domain.
This is the baseline. The upper horizontal line
represents the policy trained until convergence on
the extended domain and also tested on the ex-
tended domain. This provides the gold standard.
The adaptation strategy that takes both the mean
and variance of the policy trained on the basic do-
main and retrains the policy on the extended do-
219
main is denoted as ADAPT in Fig. 2. The adap-
tation strategy that uses the posterior mean of the
policy trained on the source domain as the prior
mean for adaptation is denoted as PRIOR in Fig. 2.
Finally, for comparison purposes we show the per-
formance of the policy that is trained from scratch
on the extended domain. This is denoted as TRAIN
on the graph. It can be seen that both adapta-
tion strategies significantly reduce the number of
training dialogues and, more importantly, main-
tain the level of performance during adaptation.
The adaptation strategy that places the prior on the
mean has slightly worse performance in the begin-
ning but provides the best performance after 1500
dialogues. As already noted, this could be due
to overly confident variances in the ADAPT case
leading to a local optimum.
6 Human experiments
In order to adapt and evaluate policies with hu-
mans, we used crowd-sourcing via the Ama-
zon Mechanical Turk service in a set-up similar
to (Jurc???c?ek et al, 2011a; Gas?ic? et al, 2013).
The BUDS dialogue manager was incorporated
in a live telephone-based spoken dialogue system.
The Mechanical Turk users were assigned spe-
cific tasks in the extended TopTable domain. They
were asked to find restaurants that have particu-
lar features as defined by the given task. To elicit
more complex dialogues, the users were some-
times asked to find more than one restaurant, and
in cases where such a restaurant did not exist they
were required to seek an alternative, for example
find a Chinese restaurant instead of a Vietnamese
one. After each dialogue the users filled in a feed-
back form indicating whether they judged the di-
alogue to be successful or not. Based on that bi-
nary rating, the subjective success was calculated
as well as the average reward. An objective rat-
ing can also be obtained by comparing the system
outputs with the predefined task.
During policy adaptation, at the end of each
call, users were asked to press 1 if they were satis-
fied (i.e. believed that they had been successful in
fulfilling the assigned task) and 0 otherwise. The
objective success was also calculated. The dia-
logue was then only used for adaptation if the user
rating agreed with the objective measure of suc-
cess as in (Gas?ic? et al, 2013). The performance
based on user ratings during adaptation for both
adaptation strategies is given in Table 1.
Table 1: Policy performance during adaptation
#Diags Reward Success (%)
ADAPT 251 11.7? 0.5 92.0? 1.7
PRIOR 329 12.1? 0.4 96.7? 1.0
We then evaluated four policies with real users:
the policy trained on the basic domain, the pol-
icy trained on the extended domain and the pol-
icy adapted to the extended domain using the prior
and the policy adapted to the extended domain via
interaction with real users using retraining. The
results are given in Table 2.
Table 2: Human evaluation of four systems in the
extended domain: trained in the basic domain,
trained in the extended domain, trained in the ba-
sic and adapted in the extended domain using both
ADAPT and PRIOR methods.
Training #Diags Reward Success(%)
Basic 246 11.0? 0.5 91.9? 1.7
Extended 250 12.1? 0.4 94.4? 1.5
ADAPT 268 12.6? 0.4 94.4? 1.4
PRIOR 252 12.4? 0.4 95.6? 1.3
The results show two important features of
these adaptation strategies. The first is that it is
possible to adapt the policy from one domain to
another with a small number of dialogues. Both
adaptation techniques achieve results statistically
indistinguishable from the matched case where the
policy was trained directly in the extended do-
main. The second important feature is that both
adaptation strategies guarantee a minimum level
of performance during training, which is better
than the performance of the basic policy tested on
the extended domain. This is particularly impor-
tant when training with real users so that they are
not exposed to poor performance at any time dur-
ing training.
7 Application to fast learning
The above results show that transfer learning
through policy adaptation can be relatively fast.
Since complex domains can be decomposed into a
series of domains with gradually increasing com-
plexity, an alternative to training a system to con-
vergence starting from an uninformative prior is
220
to train a system in stages iteratively adapting to
successively more complex domains (Taylor and
Stone, 2009).
We explored this idea by training the extended
system in three stages. The first has only one slot
that the user can specify: food-type and additional
slots phone, address and postcode that can be re-
quested (initial in Fig. 3). The second has an ad-
ditional area slot (intermediate in Fig. 3) and the
final domain has a the price-range slot added (final
on the graph).
A policy for each of these domains was trained
until convergence and the average rewards of these
policies are the horizontal lines on Fig. 3. In addi-
tion, the following adaptation schedule was imple-
mented. An initial policy was trained from scratch
for the one-slot initial system using only 1500 dia-
logues. The resulting policy was then retrained for
the intermediate two-slot system using again just
1500 dialogues. Finally, the required three-slot
system was trained using 1500 dialogues. At each
stage the policy was tested every 100 training dia-
logues, and the resulting performances are shown
by the three graphs initial-train, intermediate-adapt
and final-adapt in Fig. 3. The policies were tested
on the domains they are trained on or adapted to.
It can be seen that after just 500 dialogues of
the third stage (i.e. after just 3500 dialogues in to-
tal) the policy reaches optimal performance. It has
been shown previously that Gaussian process re-
inforcement learning for this task normally takes
104 dialogues (Gas?ic? et al, 2012) so this schedule
halves the number of dialogues needed for train-
ing. Also it is important to note that when training
from scratch the average reward is less than 5 for
300 dialogues (see TRAIN in Fig. 2), in this case
that only happens for about 100 dialogues (see
initial-train in Fig. 3).
8 Conclusions
This paper has investigated the problem of ex-
tending a dialogue system to handle new previ-
ously unseen concepts (i.e. slots) using adapta-
tion based transfer learning. It has been shown that
a GP kernel can be mapped to establish a relation-
ship between a basic and an extended domain and
that GP-based adaptation can restore a system to
optimal performance within 200 to 300 adaptation
dialogues. A major advantage of this technique is
that it allows a minimum level of performance to
be guaranteed and hence guards against subject-
0 200 400 600 800 1000 1200 1400 1600Training dialogues15
10
5
0
5
10
15
Rewa
rd
initial-trainintermediate-adaptfinal-adaptintermediateinitialfinal
Figure 3: Application of transfer learning to fast
training. The target is to achieve the performance
of the fully trained 3 slot system as shown by the
lower horizontal line final. This is achieved in three
stages, with the target being achieved part way
through the 3rd stage using just 3500 dialogues in
total.
ing the user to poor performance during the early
stages of adaptation.
Two methods of adaptation have been studied ?
one based on augmenting the training points from
the source domain with new points from the tar-
get domain, and a second which treats the source
policy as a prior for the target policy. Results us-
ing the prior method were consistently better. In a
further experiment, it was also shown that starting
with a simple system and successively extending
and adapting it slot by slot, can achieve optimal
performance faster than one trained directly from
scratch.
These results suggest that it should be feasi-
ble to construct dialogue systems which can dy-
namically update and extend their domains of dis-
course automatically during direct conversations
with users. However, further investigation of
methods for learning the relationship between the
new and the old domains is needed. Also, the
scalability of these results to large-scale domain
expansion remains a topic for future work.
Acknowledgments
This work was partly supported by PAR-
LANCE (www.parlance-project.eu), an EU Sev-
enth Framework Programme project (grant num-
ber 287615).
221
References
Y Engel, S Mannor, and R Meir. 2005. Reinforcement
learning with Gaussian processes. In Proceedings of
ICML.
Y Engel. 2005. Algorithms and Representations for
Reinforcement Learning. PhD thesis, Hebrew Uni-
versity.
M Gales and S Young. 2007. The application of hid-
den Markov models in speech recognition. Found.
Trends Signal Process., 1:195?304.
M Gas?ic?, F Jurc???c?ek, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, K Yu, and S Young. 2010.
Gaussian Processes for Fast Policy Optimisation of
POMDP-based Dialogue Managers. In Proceedings
of SIGDIAL.
M Gas?ic?, F Jurc???c?ek, B Thomson, K Yu, and S Young.
2011. On-line policy optimisation of spoken dia-
logue systems via live interaction with human sub-
jects. In Proceedings of ASRU.
M Gas?ic?, M Henderson, B Thomson, P Tsiakoulis, and
S Young. 2012. Policy optimisation of POMDP-
based dialogue systems without state space com-
pression. In Proceedings of SLT.
M Gas?ic?, C. Breslin, M. Henderson, Szummer M.,
B Thomson, P. Tsiakoulis, and S Young. 2013.
On-line policy optimisation of Bayesian Dialogue
Systems by human interaction. In Proceedings of
ICASSP.
M Geist and O Pietquin. 2011. Managing Uncertainty
within the KTD Framework. In Proceedings of the
Workshop on Active Learning and Experimental De-
sign, Sardinia (Italy).
K Georgila and O Lemon. 2004. Adaptive multimodal
dialogue management based on the information state
update approach. In W3C Workshop on Multimodal
Interaction.
S Janarthanam and O Lemon. 2010. Adaptive Re-
ferring Expression Generation in Spoken Dialogue
Systems: Evaluation with Real Users. In Proceed-
ings of SIGDIAL.
T Jebara, R Kondor, and A Howard. 2004. Probability
product kernels. J. Mach. Learn. Res., 5:819?844,
December.
F Jurc???c?ek, S Keizer, M Gas?ic?, F Mairesse, B Thomson,
K Yu, and S Young. 2011a. Real user evaluation of
spoken dialogue systems using Amazon Mechanical
Turk. In Proceedings of Interspeech.
F Jurc???c?ek, B Thomson, and S Young. 2011b. Natural
actor and belief critic: Reinforcement algorithm for
learning parameters of dialogue systems modelled as
POMDPs. ACM Transactions on Speech and Lan-
guage Processing.
S Keizer, M Gas?ic?, F Jurc???c?ek, F Mairesse, B Thomson,
K Yu, and S Young. 2010. Parameter estimation
for agenda-based user simulation. In Proceedings of
SIGDIAL.
DJ Litman and S Pan. 1999. Empirically evaluating
an adaptable spoken dialogue system. In Proceed-
ings of the seventh international conference on User
modelling.
DJ Litman and S Pan. 2002. Designing and evaluat-
ing an adaptive spoken dialogue system. User Mod-
elling and User-Adapted Interaction, 12:111?137.
CE Rasmussen and CKI Williams. 2005. Gaussian
Processes for Machine Learning. MIT Press, Cam-
bridge, Massachusetts.
N Roy, J Pineau, and S Thrun. 2000. Spoken dialogue
management using probabilistic reasoning. In Pro-
ceedings of ACL.
J Schatzmann. 2008. Statistical User and Error Mod-
elling for Spoken Dialogue Systems. Ph.D. thesis,
University of Cambridge.
ME Taylor and P Stone. 2009. Transfer learning for
reinforcement learning domains: A survey. J. Mach.
Learn. Res., 10:1633?1685, December.
ME Taylor, P Stone, and Y Liu. 2007. Transfer learn-
ing via inter-task mappings for temporal difference
learning. J. Mach. Learn. Res., 8:2125?2167, De-
cember.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken di-
alogue systems. Computer Speech and Language,
24(4):562?588.
B Thomson, M Gas?ic?, M Henderson, P Tsiakoulis, and
S Young. 2012. N-Best error simulation for training
spoken dialogue systems. In Proceedings of SLT.
TopTable. 2012. TopTable. https://www.
toptable.com.
JD Williams and SJ Young. 2007. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
S Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The Hid-
den Information State model: A practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
B Zhang, Q Cai, J Mao, E Chang, and B Guo.
2001. Spoken Dialogue Management as Planning
and Acting under Uncertainty. In Proceedings of
Eurospeech.
222
Proceedings of the SIGDIAL 2014 Conference, pages 260?262,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
The Parlance Mobile Application for Interactive Search in
English and Mandarin
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos,
Hugues Bouchard, Catherine Breslin, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Ga?i?, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Tim Potter, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay,
Boris Villazon-Terrazas, Majid Yazdani, Steve Young and Yanchao Yu
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
We demonstrate a mobile application in
English and Mandarin to test and eval-
uate components of the Parlance di-
alogue system for interactive search un-
der real-world conditions.
1 Introduction
With the advent of evaluations ?in the wild?,
emphasis is being put on converting re-
search prototypes into mobile applications that
can be used for evaluation and data col-
lection by real users downloading the ap-
plication from the market place. This is
the motivation behind the work demonstrated
here where we present a modular framework
whereby research components from the Par-
lance project (Hastie et al., 2013) can be
plugged in, tested and evaluated in a mobile
environment.
The goal of Parlance is to perform inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
Cambridge, UK for Mandarin and San Fran-
cisco, USA for English. The scenario is that
Mandarin speaking tourists would be able to
download the application and use it to learn
about restaurants in English speaking towns
and cities.
2 System Architecture
Here, we adopt a client-server approach as il-
lustrated in Figure 1 for Mandarin and Figure
2 for English. The front end of the demon-
stration system is an Android application that
calls the Google Automatic Speech Recogni-
tion (ASR) API and sends the recognized user
utterance to a server running the Interaction
?Authors are in alphabetical order
Manager (IM), Spoken Language Understand-
ing (SLU) and Natural Language Generation
(NLG) components.
Figure 1: Overview of the Parlance Man-
darin mobile application system architecture
Figure 2: Overview of the Parlance En-
glish mobile application system architecture
extended to use the Yahoo API to populate
the application with additional restaurant in-
formation
When the user clicks the Start button, a di-
alogue session starts. The phone application
first connects to the Parlance server (via
the Java Socket Server) to get the initial sys-
tem greeting which it speaks via the Google
260
Text-To-Speech (TTS) API. After the system
utterance finishes the recognizer starts to lis-
ten for user input to send to the SLU compo-
nent. The SLU converts text into a semantic
interpretation consisting of a set of triples of
communicative function, attribute, and (op-
tionally) value1. Probabilities can be associ-
ated with candidate interpretations to reflect
uncertainty in either the ASR or SLU. The
SLU then passes the semantic interpretation
to the IM within the same server.
Chinese sentences are composed of strings of
characters without any space to mark words as
other languages do, for example:
In order to correctly parse and understand
Chinese sentences, Chinese word segmenta-
tions must be performed. To do this segmen-
tation, we use the Stanford Chinese word seg-
mentor2, which relies on a linear-chain condi-
tional random field (CRF) model and treats
word segmentation as a binary decision task.
The Java Socket Server then sends the seg-
mented Chinese sentence to the SLU on the
server.
The IM then selects a dialogue act, accesses
the database and in the case of English passes
back the list of restaurant identification num-
bers (ids) associated with the relevant restau-
rants. For the English demonstration system,
these restaurants are displayed on the smart
phone as seen in Figures 4 and 5. Finally,
the NLG component decides how best to re-
alise the restaurant descriptions and sends the
string back to the phone application for the
TTS to realise. The example output is illus-
trated in Figure 3 for Mandarin and Figure 4
for English.
As discussed above, the Parlance mobile
application can be used as a test-bed for com-
paring alternative techniques for various com-
ponents. Here we discuss two such compo-
nents: IM and NLG.
1This has been implemented for English; Mandarin
uses the rule-based Phoenix parser.
2http://nlp.stanford.edu/projects/chinese-
nlp.shtml
Figure 3: Screenshot and translation of the
Mandarin system
Figure 4: Screenshot of dialogue and the list
of recommended restaurants shown on a map
and in a list for English
2.1 Interaction Management
The Parlance Interaction Manager is based
on the partially observable Markov decision
process (POMDP) framework, where the sys-
tem?s decisions can be optimised via reinforce-
ment learning. The model adopted for Par-
lance is the Bayesian Update of Dialogue
State (BUDS) manager (Thomson and Young,
2010). This POMDP-based IM factors the di-
alogue state into conditionally dependent ele-
ments. Dependencies between these elements
can be derived directly from the dialogue on-
tology. These elements are arranged into a dy-
namic Bayesian network which allows for their
marginal probabilities to be updated during
the dialogue, comprising the belief state. The
belief state is then mapped into a smaller-scale
summary space and the decisions are optimised
using the natural actor critic algorithm. In the
Parlance application, hand-crafted policies
261
Figure 5: Screenshot of the recommended
restaurant for the English application
can be compared to learned ones.
2.2 Natural Language Generation
As mentioned above, the server returns the
string to be synthesised by the Google TTS
API. This mobile framework allows for testing
of alternative approaches to NLG. In particu-
lar, we are interested in comparing a surface re-
aliser that uses CRFs against a template-based
baseline. The CRFs take semantically anno-
tated phrase structure trees as input, which it
uses to keep track of rich linguistic contexts.
Our approach has been compared with a num-
ber of competitive state-of-the art surface real-
izers (Dethlefs et al., 2013), and can be trained
from example sentences with annotations of se-
mantic slots.
2.3 Local Search and Knowledge Base
For the English system, the domain database is
populated by the search Yahoo API (Bouchard
and Mika, 2013) with restaurants in San Fran-
sisco. These restaurant search results are
returned based on their longitude and lati-
tude within San Francisco for 5 main areas, 3
price categories and 52 cuisine types contain-
ing around 1,600 individual restaurants.
The Chinese database has been partially
translated from an English database for restau-
rants in Cambridge, UK and search is based
on 3 price categories, 5 areas and 35 cuisine
types having a total of 157 restaurants. Due
to the language-agnostic nature of the Par-
lance system, only the name and address
fields needed to be translated.
3 Future Work
Investigating application side audio compres-
sion and audio streaming over a mobile in-
ternet connection would enable further assess-
ment of the ASR and TTS components used
in the original Parlance system (Hastie et
al., 2013). This would allow for entire research
systems to be plugged directly into the mobile
interface without the use of third party ASR
and TTS.
Future work also involves developing a feed-
back mechanism for evaluation purposes that
does not put undue effort on the user and put
them off using the application. In addition,
this framework can be extended to leverage
hyperlocal and social information of the user
when displaying items of interest.
Acknowledgements
The research leading to this work was funded
by the EC FP7 programme FP7/2011-14
under grant agreement no. 287615 (PAR-
LANCE).
References
H. Bouchard and P. Mika. 2013. Interactive hy-
perlocal search API. Technical report, Yahoo
Iberia, August.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and
O. Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation Using Global
Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (ACL), Sofia, Bulgaria.
H. Hastie, M.A. Aufaure, P. Alexopoulos,
H. Cuay?huitl, N. Dethlefs, M. Gasic,
J. Henderson, O. Lemon, X. Liu, P. Mika,
N. Ben Mustapha, V. Rieser, B. Thomson,
P. Tsiakoulis, Y. Vanrompay, B. Villazon-
Terrazas, and S. Young. 2013. Demonstration
of the PARLANCE system: a data-driven
incremental, spoken dialogue system for in-
teractive search. In Proceedings of the 14th
Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), Metz,
France, August.
B. Thomson and S. Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588.
262
