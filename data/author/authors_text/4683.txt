Coling 2008: Companion volume ? Posters and Demonstrations, pages 161?164
Manchester, August 2008
?Build Your Own? Spoken Dialogue Systems:
Automatically Generating ISU Dialogue Systems from Business User
Resources
Oliver Lemon, Xingkun Liu, and Helen Hastie
School of Informatics
University of Edinburgh
Informatics Forum
10 Crichton Street
Edinburgh, EH8 9AB
{olemon,xliu4,hhastie}@inf.ed.ac.uk
Abstract
Building effective spoken dialogue sys-
tems (SDS) is currently a complex task
requiring expert knowledge. Our tools
give control of SDS application develop-
ment to non-experts, who need only use
a Graphical User Interface or GUI to de-
velop state-of-the-art ?Information State
Update? (ISU) dialogue systems. Behind
the GUI is a set of Advanced Dialogue
Tools (ADT) that generate complete SDS
based on Business User Resources. These
resources include a database and a Pro-
cess Model that captures the structure of
an application, for example, banking or
restaurant information. Also generated
are speech recognition Language Models
and grammars for robust interpretation of
spontaneous speech. We will demonstrate
how our simple GUI allows developers to
easily and quickly create and modify SDS
without the need for expensive speech ap-
plication service providers. This demon-
stration shows the interface, the ADT com-
ponents, and discusses some of the re-
search issues involved. We also show an
example application built with the tools: a
tourist information system running on an
ultra-mobile PC.
1 Introduction
As automated call centres are becoming more and
more commonplace, new challenges are emerg-
ing such as having to rely on expensive service
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
providers to build systems, the inability to quickly
and easily modify live systems, and the time and
cost needed to create new SDS applications. This
paper describes a solution to these problems using
our Advanced Dialogue Tools (ADT). This pro-
totype system allows developers to take already
established business user resources such as Busi-
ness Process Models (BPM) and databases, and
use them to automatically generate spoken dia-
logue systems. Simple customisations can then
be made through the easy-to-use ADT interface or
GUI, which is example-driven. This radically new
way of creating spoken dialogue systems will put
control into the hands of the business user who is
familiar with customer needs and business goals,
thus improving usability and making spoken dia-
logue systems more widely and rapidly available.
Currently, VoiceXML is widely used for such
tasks. However, VoiceXML applications are dif-
ficult to build and maintain, because develop-
ers must anticipate and represent every possi-
ble dialogue path in the finite-state VoiceXML
model. ADT will generate VoiceXML dynami-
cally, but the easy-to-use interface allows devel-
opers to select, deploy, and monitor different ad-
vanced dialogue strategies without needing to code
VoiceXML directly. We apply the ?Information
State Update? (ISU) approach (Lemon, 2004) that
enables more robust, flexible and natural conver-
sations than VoiceXML. ISU uses a more concise
and maintainable representation of dialogue flow,
based on rules operating over dialogue contexts,
which can generalise to unforeseen states.
2 The ADT Architecture
Figure 1 shows the ADT architecture whereby the
main algorithm takes business user resources and
databases as input and uses these to automatically
161
generate the spoken dialogue system. Figure 2
shows part of one such resource, namely a BPM
for hotel bookings. First the caller will hear an
introduction, then they will be asked what price
range they want, and then whether they want a ho-
tel in the centre of town or not. Advantages of us-
ing BPMs include the fact that graphical interfaces
and authoring environments are widely available
for them, for example: Eclipse, IBM Websphere-
Process Server, BEA WeblogicWorkshop etc.. In
addition, Business User Resources can contain a
lot of additional information as well as call flows
including context, multi-media, and multiple cus-
tomer interactions.
Figure 1: The ADT Architecture
Figure 2: Part of an example Business Process
Model for searching for Hotels
The resulting spoken dialogue system deploys
the following main modules:
? Speech Recogniser module, e.g. ATK/HTK
(Young, 2007; Young, 1995) or Nuance (Nu-
ance, 2002)
? Spoken Language Understanding module,
e.g. Grammatical Framework (GF) parser
(Ranta, 2004)
? BPM and Database modules
? Speech synthesiser e.g. Festival (Taylor et al,
1998) or Cereproc (Aylett and Pidcock, 2007)
2.1 Generic Dialogue Modelling
Sophisticated research systems have been devel-
oped only for specific applications and cannot be
easily transferred to another, even very similar task
or domain. The problem of components being do-
main specific is especially prevalent in the core
area of dialogue management. For example MIT?s
Pegasus and Mercury systems (Seneff, 2002) have
dialogue managers (DM) that use approximately
350 domain-specific hand-coded rules each. The
sheer amount of labour required to construct sys-
tems prevents them from being more widely and
rapidly deployed. Our solution uses BPMs and
related authoring tools to specify domain-specific
dialogue interactions which are combined with a
domain-general dialogue manager. Specifically,
the DM consults the BPM to determine what task-
based steps to take next, such as asking for a cin-
ema name. General aspects of dialogue, such as
confirmation and clarification strategies, are han-
dled by the domain-general DM. Values for con-
straints on transitions and branching in the BPM,
for example ?present insurance option if the user is
business-class?, are compiled into domain-specific
parts of the DM?s update rules. XML format is
used for BPMs, and they are compiled into finite
state machines consulted by the spoken dialogue
system through the BPM module. The domain-
general DM was mostly abstracted from the TALK
system (Lemon et al, 2006).
2.2 Compiling Grammars for Business User
Resources and Databases
For Spoken Language Understanding, ADT cur-
rently uses Grammatical Framework (GF) (Ranta,
2004) which is a language for writing multilingual
grammars, on top of which various applications
such as machine translation and human-machine
interaction have been built. A GF grammar not
only defines syntactic well-formedness, but also
semantic content.
Using ADT, system developers do not have to
write a single line of GF grammar code. The sys-
162
tem compiles all database entries and their proper-
ties into the appropriate ?slot-filling? parts of the
GF grammar for each specific BPM.
For example, a generated GF rule is:
Bpm generalTypeRule 4:
town info hotels name->Utt=>{ s = np.s}
This rule was generated because ?name? is a
database field for the subtask hotels in the
?town info? BPM. It specifies that all hotel names
are valid utterances.
A core GF grammar has been developed to cover
basic information-seeking interactions. This is
combined with a domain-specific grammar which
is automatically generated from the BPM, database
and the example utterances provided by the devel-
oper in the GUI. Finally, GF is a robust parser ? it
skips all disfluencies and unknown words to pro-
duce an interpretation of the user input if one ex-
ists.
2.3 Speech Recognition and Text To Speech
The grammars for Spoken Language Understand-
ing generated by ADT are also compiled to
grammar-based language models (LM) for speech
recognition. ADT is plug-and-play and adheres to
industry standards such as GSML, GrXML. This
allows for greater flexibility since the application
developer is not tied to one recogniser or TTS en-
gine. For this demonstration, the speech recog-
niser is ATK (Young, 2007; Young, 1995) and
the speech synthesiser is Cereproc (Aylett and Pid-
cock, 2007). Future work will involve automati-
cally generating context sensitive language models
(Lemon and Gruenstein, 2004).
2.4 ADT GUI
As mentioned above, the prototype ADT GUI can
be used to define system prompts and add likely
user responses to the grammar. Figure 3 shows the
developer associating ?spotter? phrases with sub-
tasks in the BPM. Here the developer is associating
the phrases ?hotels, hotel, stay, room, night, sleep?
and ?rooms? with the hotels task. This means that,
for example, if the user says ?I need a place to
stay?, the hotel-booking BPM will be triggered.
Note that multi-word phrases may also be defined.
The defined spotters are automatically compiled
into the GF grammar for parsing and speech recog-
nition. By default all the lexical entries for answer-
types for the subtasks will already be present as
Figure 3: Example: using the ADT GUI to define
?spotter? phrases for different BPM subtasks
spotter phrases. ADT also checks for possible am-
biguities, for example whether ?pizza? is a spot-
ter for both cuisine type for a restaurant task and
food type for a shopping task, and it uses clarifica-
tion sub-dialogues to resolve them at runtime.
Figure 4 shows the developer?s overview of the
subtasks of a BPM, in this case hotel information.
The developer can navigate this representation and
edit it to define prompts and manipulate the asso-
ciated databases.
Figure 4: Sub-dialogue structure generated from
the Hotel booking BPM
Figure 5 shows the developer specifying the
required linguistic information to automate the
ask price subtask of the hotels BPM. Here the de-
veloper specifies the system prompt for the infor-
mation ?Do you want something cheap or expen-
sive??; a phrase for implicit confirmation of pro-
vided values ?a [X] hotel?, where [X] is the seman-
tics of the speech recognition hypothesis for the
user input; and a clarifying phrase for this subtask
163
Figure 5: Example: using ADT to define prompts,
answer sets, and database mappings for the
ask price subtask of the BPM in Figure 4
?Do you mean the hotel price?? for use when dis-
ambiguating between two or more tasks. The de-
veloper also specifies here the answer type that will
resolve the system prompt. There are many pre-
defined answer-types extracted from the databases
associated with the BPM, and the developer can
select and/or edit these. Optionally, they can give
additional example phrases that users might say
to answer the prompt, and these are automatically
added to the GF grammar.
2.5 Usability
Several demonstration systems have been built us-
ing ADT with an average development time of un-
der an hour. However, our planned evaluation will
test the ability of novice users, with some knowl-
edge of BPMs and databases, to iteratively develop
their own ISU dialogue systems.
3 Summary
This paper describes the Advanced Dialogue Tools
for creating Information State Update based dia-
logue systems automatically from Business User
Resources such as BPMs and databases. The tools
include automatic generation of grammars for ro-
bust interpretation of spontaneous speech, and uses
the application databases and BPMs to generate
lexical entries and grammar rules for speech recog-
nition language modelling. We also demonstrate
an easy-to-use prototype interface that allows the
user to easily and quickly modify aspects of the
dialogue, thus eliminating the need for third party
service providers. This paper describes ADT, its
main components, and some of the research issues
involved in its development.
4 Acknowledgement
This project is funded by a Scottish Enterprise
Proof of Concept Grant (project number 8-ELM-
004).
References
Aylett, Matthew P. and Christopher J. Pidcock. 2007.
The cerevoice characterful speech synthesiser sdk.
In AISB, pages 174?8.
Lemon, Oliver and Alexander Gruenstein. 2004. Mul-
tithreaded context for robust conversational inter-
faces: context-sensitive speech recognition and in-
terpretation of corrective fragments. ACM Trans-
actions on Computer-Human Interaction (ACM
TOCHI), 11(3):241? 267.
Lemon, Oliver, Kallirroi Georgila, James Henderson,
and Matthew Stuttle. 2006. An ISU dialogue system
exhibiting reinforcement learning of dialogue poli-
cies: generic slot-filling in the TALK in-car system.
In Proceedings of EACL, pages 119?122.
Lemon, Oliver. 2004. Context-sensitive speech recog-
nition in Information-State Update dialogue systems:
results for the grammar switching approach. In Pro-
ceedings of the 8th Workshop on the Semantics and
Pragmatics of Dialogue, CATALOG?04, pages 49?
55.
Nuance, 2002. http://www.nuance.com. As of 1 Feb
2002.
Ranta, A. 2004. Grammatical framework. a type-
theoretical grammar formalism. Journal of Func-
tional Programming, 14(2):145?189.
Seneff, Stephanie. 2002. Response Planning and Gen-
eration in the Mercury Flight Reservation System.
Computer Speech and Language, 16.
Taylor, P., A. Black, and R. Caley. 1998. The architec-
ture of the the Festival speech synthesis system. In
Third International Workshop on Speech Synthesis,
Sydney, Australia.
Young, Steve. 1995. Large vocabulary continuous
speech recognition: A review. In Proceedings of
the IEEE Workshop on Automatic Speech Recogni-
tion and Understanding, pages 3?28.
Young, Steve. 2007. ATK: An Application Toolkit
for HTK, Version 1.6. Technical report, Cambridge
University Engineering Department.
164
147
148
149
150
151
152
153
154
DUDE: a Dialogue and Understanding Development Environment,
mapping Business Process Models to Information State Update dialogue
systems
Oliver Lemon and Xingkun Liu
School of Informatics
University of Edinburgh
 
olemon,xliu4  @inf.ed.ac.uk
Abstract
We demonstrate a new development environ-
ment1 ?Information State Update? dialogue
systems which allows non-expert developers
to produce complete spoken dialogue sys-
tems based only on a Business Process Model
(BPM) describing their application (e.g. bank-
ing, cinema booking, shopping, restaurant in-
formation). The environment includes au-
tomatic generation of Grammatical Frame-
work (GF) grammars for robust interpretation
of spontaneous speech, and uses application
databases to generate lexical entries and gram-
mar rules. The GF grammar is compiled to
an ATK or Nuance language model for speech
recognition. The demonstration system allows
users to create and modify spoken dialogue
systems, starting with a definition of a Busi-
ness ProcessModel and ending with a working
system. This paper describes the environment,
its main components, and some of the research
issues involved in its development.
1 Introduction: Business Process
Modelling and Contact Centres
Many companies use ?business process models?
(BPMs) to specify communicative (andmany other) ac-
tions that must be performed in order to complete vari-
ous tasks (e.g. verify customer identity, pay a bill). See
for example BPEL4WS 2 (Andrews, 2003). These rep-
resentations specify states of processes or tasks, transi-
tions between the states, and conditions on transitions
(see e.g. the cinema booking example in figure 1). Typ-
ically, a human telephone operator (using a presenta-
tion of a BPM on a GUI) will step through these states
with a customer, during a telephone interaction (e.g. in
a contact centre), in order to complete a business pro-
cess. Note, however, that BPM representations do not
1This research is supported by Scottish Enterprise under
the Edinburgh-Stanford Link programme. We thank Graham
Technology for their collaboration.
2Business Process Execution Language for Web Services.
traditionally model dialogue context, so that (as well as
speech recognition, interpretation, and production) the
human operator is responsible for:
 contextual interpretation of incoming speech
 maintaining and updating dialogue context
 dialogue strategy (e.g. implicit/explicit confirma-
tion, initiative management).
Figure 1: Part of an example Business Process Model
(cinema booking) in the GT-X7 system (Graham Tech-
nology plc, 2005) (version 1.8.0).
A major advantage of current BPM systems (as well
as their support for database access and enterprise sys-
tem integration etc.) is their graphical development
and authoring environments. See for example figure
1 from the GT-X7 system (Graham Technology plc,
2005), version 1.8.0. This shows part of a BPM for a
cinema booking process. First (top left ?introduction?
node) the caller should hear an introduction, then (as
long as there is a ?ContinueEvent?) they will be asked
for the name of a cinema (?cinemaChoice?), and then
for the name of a film (?filmChoice?) and so on until
the correct cinema tickets are payed for.
These systems allow non-experts to construct, mod-
ify, and rapidly deploy process models and the result-
ing interactions, including interactions with back-end
99
databases. For example, a manager may decide (after
deployment of a banking application) that credit should
now only be offered to customers with a credit rating of
5 or greater, and this change can be made simply by re-
vising a condition on a state transition, presented as an
arc in a process diagram. Thus the modelling environ-
ment allows for easy specification and revision of in-
teractions. The process models are also hierarchical, so
that complex processes can be built from nested com-
binations of simple interactions. By using these sorts
of graphical tools, non-experts can deploy and man-
age complex business processes to be used by thou-
sands of human contact centre operatives. However,
many of these interactions are mundane and tedious for
humans, and can easily be carried out by automated
dialogue systems. We estimate that around 80% of
contact-centre interactions involve simple information-
gathering dialogues such as acquiring customer con-
tact details. These can be handled robustly by Infor-
mation State Update (ISU) dialogue systems (Larsson
and Traum, 2000; Bos et al, 2003). Our contribution
here is to allow non expert developers to build ISU sys-
tems using only the BPMs and databases that they are
already familiar with, as shown in figure 2.
Figure 2: The DUDE development process
1.1 Automating Contact Centres with DUDE
Automation of contact centre interactions is a realis-
tic aim only if state-of-the art dialogue management
technology is employed. Currently, several compa-
nies are attempting to automate contact centers via sim-
ple speech-recognition-based interfaces using Voice
XML. However, this is much like specification of dia-
logue managers using finite state networks, a technique
which is known to be insufficient for flexible dialogues.
The main problem is that most traditional BPM sys-
tems lack a representation of dialogue context.3 Here
we show how to elaborate business process models
with linguistic information of various types (e.g. how
to generate appropriate clarification questions), and we
show an ISU dialogue management component, which
tracks dialogue context and takes standard BPMs as in-
put to its discourse planner. Developers can now make
use of the dialogue context (Information State) using
DUDE to define process conditions that depend on IS
features (e.g. user answer, dialogue-length, etc.).
3Footnote: The manufacturer of the GT-X7 system (Gra-
ham Technology plc, 2005) has independently created the
agent247(TM) Dialogue Modelling component with dynamic
prompt and Grammar generation for Natural Language Un-
derstanding.
Customers are now able to immediately declare their
goals (?I want to change my address?) rather than hav-
ing to laboriously navigate a series of multiple-choice
options. This sort of ?How may I help you?? sys-
tem is easily within current dialogue system expertise
(Walker et al, 2000), but has not seen widespread com-
mercial deployment. Another possibility opened up by
the use of dialogue technology is the personalization
of the dialogue with the customer. By interacting with
a model of the customer?s preferences a dialogue in-
terface is able to recommend appropriate services for
the customer (Moore et al, 2004), as well as modify its
interaction style.
2 DUDE: a development environment
DUDE targets development of flexible and robust ISU
dialogue systems from BPMs and databases. Its main
components are:
 A graphical Business Process Modelling Tool
(Graham Technology plc, 2005) (java)
 DIPPER generic dialogue manager (Bos et al,
2003) (java or prolog)
 MySQL databases
 a development GUI (java), see section 2.2
The spoken dialogue systems produced by DUDE all
run using the Open Agent Architecture (OAA) (Cheyer
and Martin, 2001) and employ the following agents in
addition to DIPPER:
 Grammatical Framework (GF) parser (Ranta,
2004) (java)
 BPM agent (java) and Database agent (java)
 HTK speech recognizer (Young, 1995) using ATK
(or alternatively Nuance)
 Festival2 speech synthesizer (Taylor et al, 1998)
We now highlight generic dialogue management, the
DUDE developer GUI, and the use of GF.
2.1 DIPPER and generic dialogue management
Many sophisticated research systems are developed for
specific applications and cannot be transferred to an-
other, even very similar, task or domain. The prob-
lem of components being domain specific is espe-
cially severe in the core area of dialogue manage-
ment. For example MIT?s Pegasus and Mercury sys-
tems (Seneff, 2002) have dialogue managers which use
approximately 350 domain-specific hand-coded rules
each. The sheer amount of labor required to con-
struct systems prevents them from being more widely
and rapidly deployed. Using BPMs and related au-
thoring tools to specify dialogue interactions addresses
this problem and requires the development of domain-
general dialogue managers, where BPMs represent
application-specific information.
100
We have developed a generic dialogue manager
(DM) using DIPPER. The core DM rules cover mixed
initiative dialogue for multiple tasks (e.g. a BPM with
several sub-processes), explicit and implicit confirma-
tion, help, restart, repeat, and quit commands, and
presentation and refinement of database query results.
This is a domain-neutral abstraction of the ISU dia-
logue managers implemented for the FLIGHTS and
TALK systems (Moore et al, 2004; Lemon et al,
2006).
The key point here is that the DM consults the BPM
to determinewhat task-based steps to take next (e.g. ask
for cinema name), when appropriate. Domain-general
aspects of dialogue (e.g. confirmation and clarification
strategies) are handled by the core DM. Values for con-
straints on transitions and branching in the BPM (e.g.
present insurance option if the user is business-class)
are compiled into domain-specific parts of the Informa-
tion State. We use an XML format for BPMs, and com-
pile them into finite state machines (the BPM agent)
consulted by DIPPER for task-based dialogue control.
2.2 The DUDE developer GUI
Figures 3 to 5 show different screens from the DUDE
GUI for dialogue system development. Figure 3 shows
the developer associating ?spotter? phrases with sub-
tasks in the BPM. Here the developer is associating
the phrases ?hotels, hotel, stay, room, night, sleep? and
?rooms? with the hotels task. This means that, for
example, if the user says ?I need a place to stay?, the
hotel-booking BPM will be triggered. (Note that multi-
word phrases may also be defined). The defined spot-
ters are automatically compiled into the GF grammar
for parsing and speech recognition. By default all the
lexical entries for answer-types for the subtasks will al-
ready be present as spotter phrases. DUDE checks for
possible ambiguities (e.g. if ?sushi? is a spotter for both
cuisine type for a restaurant subtask and food type for
a shopping process) and uses clarification subdialogues
to resolve them at runtime.
Figure 3: Example: using DUDE to define ?spotter?
phrases for different BPM subtasks
Figure 4 shows the developer?s overview of the sub-
tasks of a BPM (here, hotel information). The devel-
oper can navigate this representation and edit it to de-
fine prompts and manipulate the associated databases.
Figure 4: A Business Process Model viewed by DUDE
Figure 5 shows the developer specifying the required
linguistic information to automate the ?ask price? sub-
task of the hotel-information BPM. Here the developer
specifies the system prompt for the information (?Do
you want something cheap or expensive??), a phrase
for implicit confirmation of provided values (here ?a
[X] hotel?, where [X] is the semantics of the ASR hy-
pothesis for the user input), and a clarifying phrase for
this subtask (e.g. ?Do you mean the hotel price??) for
use when disambiguating between 2 or more tasks. The
developer also specifies here the answer type that will
resolve the system prompt. There are many predefined
answer-types extracted from the databases associated
with the BPMs, and the developer can select and/or edit
these. They can also give additional (optional) example
phrases that users might employ to answer the prompt,
and these are automatically added to the GF grammar.
Figure 5: Example: using DUDE to define prompts,
answer sets, and database mappings for the ?ask price?
subtask of the BPM in figure 4
A similar GUI allows the developer to specify
101
database access and result presentation phases of the
dialogue, if they are present in the BPM.
2.3 The Grammatical Framework: compiling
grammars from BPMs, DBs, and example sets
GF (Ranta, 2004) is a language for writing multilin-
gual grammars, on top of which various applications
such as machine translation and human-machine inter-
action have been built. A GF grammar not only defines
syntactic well-formedness, but also semantic content.
Using DUDE, system developers do not have to
write a single line of GF grammar code. We have de-
veloped a core GF grammar for information-seeking
dialogues (this supports a large fragment of spoken En-
glish, with utterances such as ?Uh I think I think I want
a less expensive X and uhhh a Y on DATE please? and
so on). In addition, we compile all database entries and
their properties into the appropriate ?slot-filling? parts
of the GF grammar for each specific BPM.
For example, a generated GF rule is:
Bpm generalTypeRule 4:
town info hotels name->Utt=->   s = np.s  .
This means that all hotel names are valid utterances,
and it is generated because ?name? is a DB field for
the subtask ?hotels? in the ?town info? BPM.
Finally, we allow developers to give example sen-
tences showing how users might respond to system
prompts. If these are not already covered by the exist-
ing grammar we automatically generate rules to cover
them. Finally GF, is a robust parser ? it skips all dis-
fluencies and unknown words to produce an interpre-
tation of the user input if one exists. Note that the
GF grammars developed by DUDE can be compiled to
speech-recognition language models for both Nuance
and HTK/ATK (Young, 1995).
2.4 Usability
We have built several demonstration systems using
DUDE. We are able to build a new system in under
an hour, but our planned evaluation will test the abil-
ity of novice users (with some knowledge of BPMs
and databases) to iteratively develop their own ISU di-
alogue systems.
3 Summary
We demonstrate a development environment for ?Infor-
mation State Update? dialogue systems which allows
non-expert developers to produce complete spoken di-
alogue systems based only on Business Process Models
(BPM) describing their applications. The environment
includes automatic generation of Grammatical Frame-
work (GF) grammars for robust interpretation of spon-
taneous speech, and uses the application databases to
generate lexical entries and grammar rules. The GF
grammar is compiled to an ATK language model for
speech recognition (Nuance is also supported). The
demonstration system allows users to create and mod-
ify spoken dialogue systems, starting with a definition
of a Business Process Model (e.g. banking, cinema
booking, shopping, restaurant information) and ending
with a working system. This paper describes the en-
vironment, its main components, and some of the re-
search issues involved in its development.
References
Tony Andrews. 2003. Business process execution
language for web services, version 1.1, http://www-
106.ibm.com/developerworks/library/ws-bpel/.
Technical report, IBM developer works.
Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi
Oka. 2003. DIPPER: Description and Formalisation
of an Information-StateUpdate Dialogue SystemAr-
chitecture. In 4th SIGdial Workshop on Discourse
and Dialogue, pages 115?124, Sapporo.
Adam Cheyer and David Martin. 2001. The Open
Agent Architecture. Journal of Autonomous Agents
and Multi-Agent Systems, 4(1/2):143?148.
Graham Technology plc. 2005. GT-X7 v.1.8.0
from Graham Technology plc [without the
agent247(TM) Dialogue and NLP Engine].
www.grahamtechnology.com.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-
logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323?340.
Oliver Lemon, Kallirroi Georgila, James Henderson,
andMatthew Stuttle. 2006. An ISU dialogue system
exhibiting reinforcement learning of dialogue poli-
cies: generic slot-filling in the TALK in-car system.
In Proceedings of EACL, page to appear.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, compar-
ative descriptions in spoken dialogue. In The 17th
International FLAIRS Conference (Florida Artifical
Intelligence Research Society).
A. Ranta. 2004. Grammatical framework. a type-
theoretical grammar formalism. Journal of Func-
tional Programming, 14(2):145?189.
Stephanie Seneff. 2002. Response Planning and Gen-
eration in the Mercury Flight Reservation System.
Computer Speech and Language, 16.
P. Taylor, A. Black, and R. Caley. 1998. The architec-
ture of the the Festival speech synthesis system. In
Third International Workshop on Speech Synthesis,
Sydney, Australia.
M. A. Walker, I. Langkilde, J. Wright, A. Gorin, and
D. Litman. 2000. Learning to Predict Problematic
Situations in a Spoken Dialogue System: Experi-
ments with How May I Help You? In Proceedings
of the NAACL 2000, Seattle.
Steve Young. 1995. Large vocabulary continuous
speech recognition: A review. In Proceedings of
the IEEE Workshop on Automatic Speech Recogni-
tion and Understanding, pages 3?28.
102
An ISU Dialogue System Exhibiting Reinforcement Learning of Dialogue
Policies: Generic Slot-filling in the TALK In-car System
Oliver Lemon, Kallirroi Georgila, and James Henderson
School of Informatics
University of Edinburgh
olemon@inf.ed.ac.uk
Matthew Stuttle
Dept. of Engineering
University of Cambridge
mns25@cam.ac.uk
Abstract
We demonstrate a multimodal dialogue system
using reinforcement learning for in-car sce-
narios, developed at Edinburgh University and
Cambridge University for the TALK project1.
This prototype is the first ?Information State
Update? (ISU) dialogue system to exhibit rein-
forcement learning of dialogue strategies, and
also has a fragmentary clarification feature.
This paper describes the main components and
functionality of the system, as well as the pur-
poses and future use of the system, and surveys
the research issues involved in its construction.
Evaluation of this system (i.e. comparing the
baseline system with handcoded vs. learnt dia-
logue policies) is ongoing, and the demonstra-
tion will show both.
1 Introduction
The in-car system described below has been con-
structed primarily in order to be able to collect data
for Reinforcement Learning (RL) approaches to mul-
timodal dialogue management, and also to test and fur-
ther develop learnt dialogue strategies in a realistic ap-
plication scenario. For these reasons we have built a
system which:
  contains an interface to a dialogue strategy learner
module,
  covers a realistic domain of useful ?in-car? con-
versation and a wide range of dialogue phenom-
ena (e.g. confirmation, initiative, clarification, in-
formation presentation),
  can be used to complete measurable tasks (i.e.
there is a measure of successful and unsuccessful
dialogues usable as a reward signal for Reinforce-
ment Learning),
  logs all interactions in the TALK data collection
format (Georgila et al, 2005).
1This research is supported by the TALK project (Euro-
pean Community IST project no. 507802), http://www.talk-
project.org
In this demonstration we will exhibit the software
system that we have developed to meet these require-
ments. First we describe the domain in which the di-
alogue system operates (an ?in-car? information sys-
tem). Then we describe the major components of the
system and give examples of their use. We then discuss
the important features of the system in respect to the
dialogue phenomena that they support.
1.1 A System Exhibiting Reinforcement Learning
The central motivation for building this dialogue sys-
tem is as a platform for Reinforcement Learning (RL)
experiments. The system exhibits RL in 2 ways:
  It can be run in online learning mode with real
users. Here the RL agent is able to learn from suc-
cessful and unsuccessful dialogueswith real users.
Learning will be much slower than with simulated
users, but can start from an already learnt policy,
and slowly improve upon that.
  It can be run using an already learnt policy (e.g.
the one reported in (Henderson et al, 2005;
Lemon et al, 2005), learnt from COMMUNICA-
TOR data (Georgila et al, 2005)). This mode can
be used to test the learnt policies in interactions
with real users.
Please see (Henderson et al, 2005) for an expla-
nation of the techniques developed for Reinforcement
Learning with ISU dialogue systems.
2 System Overview
The baseline dialogue system is built around the DIP-
PER dialogue manager (Bos et al, 2003). This sys-
tem is initially used to conduct information-seeking di-
alogues with a user (e.g. find a particular hotel and
restaurant), using hand-coded dialogue strategies (e.g.
always use implicit confirmation, except when ASR
confidence is below 50%, then use explicit confirma-
tion). We have then modified the DIPPER dialogue
manager so that it can consult learnt strategies (for ex-
ample strategies learnt from the 2000 and 2001 COM-
MUNICATOR data (Lemon et al, 2005)), based on its
119
current information state, and then execute dialogue ac-
tions from those strategies. This allows us to compare
hand-coded against learnt strategies within the same
system (i.e. the other components such as the speech-
synthesiser, recogniser, GUI, etc. all remain fixed).
2.1 Overview of System Features
The following features are currently implemented:
  use of Reinforcement Learning policies or dia-
logue plans,
  multiple tasks: information seeking for hotels,
bars, and restaurants,
  overanswering/ question accommodation/ user-
initiative,
  open speech recognition using n-grams,
  confirmations - explicit and implicit based on
ASR confidence,
  fragmentary clarifications based on word confi-
dence scores,
  multimodal output - highlighting and naming en-
tities on GUI,
  simple user commands (e.g. ?Show me all the in-
dian restaurants?),
  dialogue context logging in ISU format (Georgila
et al, 2005).
3 Research Issues
The work presented here explores a number of research
themes, in particular: using learnt dialogue policies,
learning dialogue policies in online interaction with
users, fragmentary clarification, and reconfigurability.
3.1 Moving between Domains:
COMMUNICATOR and In-car Dialogues
The learnt policies in (Henderson et al, 2005) focussed
on the COMMUNICATOR system for flight-booking di-
alogues. There we reported learning a promising initial
policy for COMMUNICATOR dialogues, but the issue
arises of how we could transfer this policy to new do-
mains ? for example the in-car domain.
In the in-car scenarios the genre of ?information
seeking? is central. For example the SACTI corpora
(Stuttle et al, 2004) have driver information requests
(e.g. searching for hotels) as a major component.
One question we address here is to what extent di-
alogue policies learnt from data gathered for one sys-
tem, or family of systems, can be re-used or adapted
for use in other systems. We conjecture that the slot-
filling policies learnt from our experiments with COM-
MUNICATOR will also be good policies for other slot-
filling tasks ? that is, that we are learning ?generic?
slot-filling or information seeking dialogue policies. In
section 5 we describe how the dialogue policies learnt
for slot filling on the COMMUNICATOR data set can be
abstracted and used in the in-car scenarios.
3.2 Fragmentary Clarifications
Another research issue we have been able to explore
in constructing this system is the issue of generating
fragmentary clarifications. The system can be run with
this feature switched on or off (off for comparison with
COMMUNICATOR systems). Instead of a system sim-
ply saying ?Sorry, please repeat that? or some such sim-
ilar simple clarification request when there is a speech
recognition failure, we were able to use the word con-
fidence scores output by the ATK speech recogniser to
generate more intelligent fragmentary clarification re-
quests such as ?Did you say a cheap chinese restau-
rant??. This works by obtaining an ASR confidence
score for each recognised word. We are then able to
try various techniques for clarifying the user utterance.
Many possibilities arise, for example: explicitly clarify
only the highest scoring content word below the rejec-
tion threshold, or, implicitly clarify all content words
and explicitly clarify the lowest scoring content word.
The current platform enables us to test alternative
strategies, and develop more complex ones.
4 The ?In-car? Scenario
The scenario we have designed the system to cover is
that of information seeking about a town, for example
its hotels, restaurants, and bars. We imagine a driver
who is travelling towards this town, or is already there,
who wishes to accomplish relatively complex tasks,
such as finding an italian restaurant near their hotel,
or finding all the wine bars in town, and so on. The
driver/user should be able to specify queries using nat-
ural dialogue, and will receive system output that is a
mixture of spoken and graphical information (e.g. a de-
scription of an item and a map showing its location).
The example town is taken from the (Stuttle et al,
2004) corpus collection materials, and contains a num-
ber of hotels, bars, restaurants, and tourist attractions.
The user should be able to get information on a range
of locations in the town, and the dialogue system will
be used to specify and refine the user queries, as well
as to present information to the user. See the example
dialogue in table 1.
We now describe the dialogue system components.
5 Component-level Description
This section describes the components of the baseline
in-car dialogue system. Communication between com-
ponents is handled by OAA?s asynchronous hub archi-
tecture (Cheyer and Martin, 2001). The major compo-
nents are:
  DIPPER (Bos et al, 2003) ISU dialogue manager,
  Dialogue Policy Reinforcement Learner Agent2,
  ATK for speech recognition,
  Festival speech synthesiser,
2This is written in Python and has an OAA wrapper in C.
120
Utterance Feature
S How can I help you? Open initiative ini-
tial question
U I?m looking for a cheap chinese
restaurant.
User initiative
S Would you like something cen-
tral?
Intention recogni-
tion
U Yes.
S The Dragon Way is an af-
fordable cantonese restaurant
located on Main Street [dis-
played on map].
Multimodal presen-
tation
S Are you happy with those op-
tions?
U Yes, and I also want a double
room.
S Would you like something ex-
pensive, cheap, or reasonably
priced?
Intention recogni-
tion
U A luxury hotel, in the center of
town.
S There are 2 hotel options meet-
ing your requirements. The
Royal Hotel [displayed on
map] is a renovated nineteenth
century palace     The Hotel
Primus [displayed on map] has
big, bright, clean rooms    
Which hotel option would you
like?
Multimodal presen-
tation
U The second one please.
S The phone number of the Hotel
Primus is 2094-227.
S Would you like any further in-
formation?
U I want to find a jazz bar.
S I?m sorry, what kind of bar are
you looking for?
Intention recogni-
tion, Fragmentary
clarification
U A jazz bar.
S A jazz bar. Would you like
something central?
Implicit confirma-
tion
Table 1: Example dialogue, showing system features
  Multimodal Map interface (a java OAA agent),
  Database agent (java OAA wrapper to MySQL).
5.1 Dialogue Policy Learner Agent
This agent acts as an interface between the DIPPER
dialogue manager and the system simulation based on
RL. In particular it has the following solvable:
callRLsimulation(IS file name,
conversational domain, speech act, task,
result).
The first argument is the name of the file that contains
all information about the current information state,
which is required by the RL algorithm to produce
an action. The action returned by the RL agent is
a combination of conversational domain,
speech act, and task. The last argument shows
whether the learnt policy will continue to produce
more actions or release the turn. When run in online
learning mode the agent not only produces an action
when supplied with a state, but at the end of every
dialogue it uses the reward signal to update its learnt
policy. The reward signal is defined in the RL agent,
and is currently a linear combination of task success
metrics combined with a fixed penalty for dialogue
length (see (Henderson et al, 2005)).
This agent can be called whenever the system has
to decide on the next dialogue move. In the original
hand-coded system this decision is made by way of a
dialogue plan (using the ?deliberate? solvable). The
RL agent can be used to drive the entire dialogue pol-
icy, or can be called only in certain circumstances. This
makes it usable for whole dialogue strategies, but also,
if desired, it can be targetted only on specific dialogue
management decisions (e.g. implicit vs. explicit confir-
mation, as was done by (Litman et al, 2000)).
One important research issue is that of tranferring
learnt strategies between domains. We learnt a strat-
egy for the COMMUNICATOR flight booking dialogues
(Lemon et al, 2005; Henderson et al, 2005), but
this is generated by rather different scenarios than the
in-car dialogues. However, both are ?slot-filling? or
information-seeking applications. We defined a map-
ping (described below) between the states and actions
of both systems, in order to construct an interface be-
tween the learnt policies for COMMUNICATOR and the
in-car baseline system.
5.2 Mapping between COMMUNICATOR and
the In-car Domains
There are 2 main problems to be dealt with here:
  mapping between in-car system information states
and COMMUNICATOR information states,
  mapping between learnt COMMUNICATOR sys-
tem actions and in-car system actions.
The learnt COMMUNICATOR policy tells us, based
on a current IS, what the optimal system action
is (for example request info(dest city) or
acknowledgement). Obviously, in the in-car sce-
nario we have no use for task types such as ?destina-
tion city? and ?departure date?. Our method therefore
is to abstract away from the particular details of the
task type, but to maintain the information about dia-
loguemoves and the slot numbers that are under discus-
sion. That is, we construe the learnt COMMUNICATOR
policy as a policy concerning how to fill up to 4 (or-
dered) informational slots, and then access a database
and present results to the user. We also note that some
slots are more essential than others. For example, in
COMMUNICATOR it is essential to have a destination
city, otherwise no results can be found for the user.
Likewise, for the in-car tasks, we consider the food-
type, bar-type, and hotel-location to be more important
to fill than the other slots. This suggests a partial order-
ing on slots via their importance for an application.
In order to do this we define the mappings shown
in table 2 between COMMUNICATOR dialogue actions
and in-car dialogue actions, for each sub-task type of
the in-car system.
121
COMMUNICATOR action In-car action
dest-city food-type
depart-date food-price
depart-time food-location
dest-city hotel-location
depart-date room-type
depart-time hotel-price
dest-city bar-type
depart-date bar-price
depart-time bar-location
Table 2: Action mappings
Note that we treat each of the 3 in-car sub-tasks (ho-
tels, restaurants, bars) as a separate slot-filling dialogue
thread, governed by COMMUNICATOR actions. This
means that the very top level of the dialogue (?How
may I help you?) is not governed by the learnt policy.
Only when we are in a recognised task do we ask the
COMMUNICATOR policy for the next action. Since the
COMMUNICATOR policy is learnt for 4 slots, we ?pre-
fill? a slot3 in the IS when we send it to the Dialogue
Policy Learner Agent in order to retrieve an action.
As for the state mappings, these follow the same
principles. That is, we abstract from the in-car states to
form states that are usable by COMMUNICATOR . This
means that, for example, an in-car state where food-
type and food-price are filled with high confidence is
mapped to a COMMUNICATOR state where dest-city
and depart-date are filled with high confidence, and
all other state information is identical (modulo the task
names). Note that in a future version of the in-car sys-
tem where task switching is allowed we will have to
maintain a separate view of the state for each task.
In terms of the integration of the learnt policies with
the DIPPER system update rules, we have a system flag
which states whether or not to use a learnt policy. If
this flag is present, a different update rule fires when
the system determines what action to take next. For
example, instead of using the deliberate predicate
to access a dialogue plan, we instead call the Dialogue
Policy Learner Agent via OAA, using the current Infor-
mation State of the system. This will return a dialogue
action to the DIPPER update rule.
In current work we are evaluating howwell the learnt
policies work for real users of the in-car system.
6 Conclusions and Future Work
This report has described work done in the TALK
project in building a software prototype baseline ?In-
formation State Update? (ISU)-based dialogue system
in the in-car domain, with the ability to use dialogue
policies derived from machine learning and also to per-
form online learning through interaction. We described
the scenarios, gave a component level description of
the software, and a feature level description and exam-
3We choose ?orig city? because it is the least important
and is already filled at the start of many COMMUNICATOR
dialogues.
ple dialogue.
Evaluation of this system (i.e. comparing the sys-
tem with hand-coded vs. learnt dialogue policies) is
ongoing. Initial evaluation of learnt dialogue policies
(Lemon et al, 2005; Henderson et al, 2005) suggests
that the learnt policy performs at least as well as a rea-
sonable hand-coded system (the TALK policy learnt for
COMMUNICATOR dialogue management outperforms
all the individual hand-coded COMMUNICATOR sys-
tems).
The main achievements made in designing and con-
structing this baseline system have been:
  Combining learnt dialogue policies with an ISU
dialogue manager. This has been done for online
learning, as well as for strategies learnt offline.
  Mapping learnt policies between domains, i.e.
mapping Information States and system actions
between DARPA COMMUNICATOR and in-car in-
formation seeking tasks.
  Fragmentary clarification strategies: the combina-
tion of ATK word confidence scoring with ISU-
based dialogue management rules allows us to ex-
plore word-based clarification techniques.
References
J. Bos, E. Klein, O. Lemon, and T. Oka. 2003.
DIPPER: Description and Formalisation of an
Information-State Update Dialogue System Archi-
tecture. In 4th SIGdial Workshop on Discourse and
Dialogue, Sapporo.
A. Cheyer and D. Martin. 2001. The open agent archi-
tecture. Journal of Autonomous Agents and Multi-
Agent Systems, 4(1):143?148.
K. Georgila, O. Lemon, and J. Henderson. 2005. Au-
tomatic annotation of COMMUNICATOR dialogue
data for learning dialogue strategies and user sim-
ulations. In Ninth Workshop on the Semantics and
Pragmatics of Dialogue (SEMDIAL), DIALOR?05.
J. Henderson, O. Lemon, and K. Georgila. 2005.
Hybrid Reinforcement/Supervised Learning for Di-
alogue Policies from COMMUNICATOR data. In
IJCAI workshop on Knowledge and Reasoning in
Practical Dialogue Systems.
O. Lemon, K. Georgila, J. Henderson, M. Gabsdil,
I. Meza-Ruiz, and S. Young. 2005. D4.1: Inte-
gration of Learning and Adaptivity with the ISU ap-
proach. Technical report, TALK Project.
D. Litman, M. Kearns, S. Singh, and M. Walker. 2000.
Automatic optimization of dialoguemanagement. In
Proc. COLING.
M. Stuttle, J. Williams, and S. Young. 2004. A frame-
work for dialog systems data collection using a sim-
ulated ASR channel. In ICSLP 2004, Jeju, Korea.
122
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 505?513,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
User Simulations for context-sensitive speech recognition in Spoken
Dialogue Systems
Oliver Lemon
Edinburgh University
olemon@inf.ed.ac.uk
Ioannis Konstas
University of Glasgow
konstas@dcs.gla.ac.uk
Abstract
We use a machine learner trained on a
combination of acoustic and contextual
features to predict the accuracy of incom-
ing n-best automatic speech recognition
(ASR) hypotheses to a spoken dialogue
system (SDS). Our novel approach is to
use a simple statistical User Simulation
(US) for this task, which measures the
likelihood that the user would say each
hypothesis in the current context. Such
US models are now common in machine
learning approaches to SDS, are trained on
real dialogue data, and are related to the-
ories of ?alignment? in psycholinguistics.
We use a US to predict the user?s next dia-
logue move and thereby re-rank n-best hy-
potheses of a speech recognizer for a cor-
pus of 2564 user utterances. The method
achieved a significant relative reduction of
Word Error Rate (WER) of 5% (this is
44% of the possible WER improvement
on this data), and 62% of the possible se-
mantic improvement (Dialogue Move Ac-
curacy), compared to the baseline policy
of selecting the topmost ASR hypothesis.
The majority of the improvement is at-
tributable to the User Simulation feature,
as shown by Information Gain analysis.
1 Introduction
A crucial problem in the design of spoken dia-
logue systems (SDS) is to decide for incoming
recognition hypotheses whether a system should
accept (consider correctly recognized), reject (as-
sume misrecognition), or ignore (classify as noise
or speech not directed to the system) them.
Obviously, incorrect decisions at this point can
have serious negative effects on system usability
and user satisfaction. On the one hand, accept-
ing misrecognized hypotheses leads to misunder-
standings and unintended system behaviors which
are usually difficult to recover from. On the other
hand, users might get frustrated with a system that
behaves too cautiously and rejects or ignores too
many utterances. Thus an important feature in di-
alogue system engineering is the tradeoff between
avoiding task failure (due to misrecognitions) and
promoting overall dialogue efficiency, flow, and
naturalness.
In this paper, we investigate the use of machine
learning trained on a combination of acoustic fea-
tures and features computed from dialogue context
to predict the quality of incoming n-best recogni-
tion hypotheses to a SDS. These predictions are
then used to select a ?best? hypothesis and to de-
cide on appropriate system reactions. We evalu-
ate this approach in comparison with a baseline
system that works in the standard way: always
choosing the topmost hypothesis in the n-best list.
In such systems, complex repair strategies are re-
quired when the top hypothesis is incorrect.
The main novelty of this work is that we ex-
plore the use of predictions from simple statisti-
cal User Simulations to re-rank n-best lists of ASR
hypotheses. These User Simulations are now com-
monly used in statistical learning approaches to di-
alogue management (Williams and Young, 2003;
Schatzmann et al, 2006; Young, 2006; Young et
al., 2007; Schatzmann et al, 2007), but they have
not been used for context-sensitive ASR before.
In our model, the system?s ?belief? b(h) in a
recognition hypothesis h is factored in two parts:
the observation probability P (o|h) (approximated
by the ASR confidence score) and the User Simu-
lation probability P (h|us,C) of the hypothesis:
b(h) = P (o|h).P (h|us,C) (1)
where us is the state of the User Simulation in
context C. The context is simply a window of di-
505
alogue acts in the dialogue history, that the US is
sensitive to (see section 3).
The paper is organized as follows. After a short
relation to previous work, we describe the data
(Section 5) and derive baseline results (Section
6). Section 3 describes the User Simulations that
we use for re-ranking hypotheses. Section 7 de-
scribes our learning experiments for classifying
and selecting from n-best recognition hypotheses
and Section 9 reports our results.
2 Relation to Previous Work
In psycholinguistics, the idea that human dialogue
participants simulate each other to some extent is
gaining currency. (Pickering and Garrod, 2007)
write:
?if B overtly imitates A, then A?s com-
prehension of B?s utterance is facilitated
by A?s memory for A?s previous utter-
ance.?
We explore aspects of this idea in a computa-
tional manner. Similar work in the area of spoken
dialogue systems is described below.
(Litman et al, 2000) use acoustic-prosodic in-
formation extracted from speech waveforms, to-
gether with information derived from their speech
recognizer, to automatically predict misrecog-
nized turns in a corpus of train-timetable informa-
tion dialogues. In our experiments, we also use
recognizer confidence scores and a limited num-
ber of acoustic-prosodic features (e.g. amplitude
in the speech signal) for hypothesis classification,
but we also use User Simulation predictions.
(Walker et al, 2000) use a combination of fea-
tures from the speech recognizer, natural language
understanding, and dialogue manager/discourse
history to classify hypotheses as correct, partially
correct, or misrecognized. Our work is related to
these experiments in that we also combine con-
fidence scores and higher-level features for clas-
sification. However, both (Litman et al, 2000)
and (Walker et al, 2000) consider only single-best
recognition results and thus use their classifiers as
?filters? to decide whether the best recognition hy-
pothesis for a user utterance is correct or not. We
go a step further in that we classify n-best hypothe-
ses and then select among the alternatives. We also
explore the use of more dialogue and task-oriented
features (e.g. the dialogue move type of a recogni-
tion hypothesis) for classification.
(Gabsdil and Lemon, 2004) similarly perform
reordering of n-best lists by combining acoustic
and pragmatic features. Their study shows that di-
alogue features such as the previous system ques-
tion and whether a hypothesis is the correct answer
to a particular question contributed more to classi-
fication accuracy than the other attributes.
(Jonson, 2006) classifies recognition hypothe-
ses with labels denoting acceptance, clarification,
confirmation and rejection. These labels were
learned in a similar way to (Gabsdil and Lemon,
2004) and correspond to varying levels of con-
fidence, being essentially potential directives to
the dialogue manager. Apart from standard fea-
tures Jonson includes attributes that account for
the whole n-best list, i.e. standard deviation of
confidence scores.
As well as the use of a User Simulation, the
main difference between our approach and work
on hypothesis reordering (e.g. (Chotimongkol and
Rudnicky, 2001)) is that we make a decision re-
garding whether a dialogue system should accept,
clarify, reject, or ignore a user utterance. Like
(Gabsdil and Lemon, 2004; Jonson, 2006), our
approach is more generally applicable than pre-
ceding research, since we frame our methodology
in the Information State Update (ISU) approach
to dialogue management (Traum et al, 1999) and
therefore expect it to be applicable to a range of
related multimodal dialogue systems.
3 User Simulations
What makes this study different from the previous
work in the area of post-processing of the ASR hy-
potheses is the incorporation of a User Simulation
output as an additional feature. The history of a di-
alogue between a user and a dialogue system plays
an important role as to what the user might be ex-
pected to say next. As a result, most of the stud-
ies mentioned in the previous section make vari-
ous efforts to capture history by including relevant
features directly in their classifiers.
Various statistical User Simulations have been
trained on corpora of dialogue data in order to
simulate real user behaviour (Schatzmann et al,
2006; Young, 2006; Georgila et al, 2006; Young
et al, 2007; Schatzmann et al, 2007). We devel-
oped a simple n-gram User Simulation, using n-
grams of dialogue moves. It treats a dialogue as
a sequence of lists of consecutive user and system
turns in a high level semantic representation, i.e.
506
< SpeechAct >,< Task > pairs, for example
< provide info >,< music genre(punk) >.
It takes as input the n ? 1 most recent lists of
< SpeechAct >,< Task > pairs in the dialogue
history, and uses the statistics in the training set
to compute a distribution over the possible next
user actions. If no n-grams match the current his-
tory, the model can back-off to n-grams of lower
order. We use this model to assess the likelihood
of each candidate ASR hypothesis. Intuitively, this
is the likelihood that the user really would say the
hypothesis in the current dialogue situation. The
benefit of using n-gram models is that they are fast
and simple to train even on large corpora.
The main hypothesis that we investigate is that
by using the User Simulation model to predict the
next user utterance, we can effectively increase the
performance of the speech recogniser module.
4 Evaluation metrics
To evaluate performance we use Dialogue Move
Accuracy (DMA), a strict variant of Concept Er-
ror Rate (CER) as defined by (Boros et al, 1996),
which takes into account the semantic aspects of
the difference between the classified utterance and
the true transcription. CER is similar to WER,
since it takes into account deletions, insertions
and substitutions on the semantic (rather than the
word) level of the utterance. DMA is stricter than
CER in the sense that it does not allow for par-
tial matches in the semantic representation. In
other words, if the classified utterance corresponds
to the same semantic representation as the tran-
scribed then we have 100% DMA, otherwise 0%.
Sentence Accuracy (SA) is the alignment of a
single hypothesis in the n-best list with the true
transcription. Similarly to DMA, it accounts for
perfect alignment between the hypothesis and the
transcription, i.e. if they match perfectly we have
100% SA, otherwise 0%.
5 Data Collection
For our experiments, we use data collected in a
user study with the Town-Info spoken dialogue
system, using the HTK speech recognizer (Young,
2007). In this study 18 subjects had to solve 10
search/browsing tasks with the system, resulting in
180 complete dialogues and 2564 utterances (av-
erage 14.24 user utterances per dialogue).
For each utterance we have a series of files of
60-best lists produced by the speech recogniser,
namely the transcription hypotheses on a sentence
level along with the acoustic model score and the
equivalent transcriptions on a word level, with in-
formation such as the duration of each recognised
frame and the confidence score of the acoustic and
language model of each word.
5.1 Labeling
We transcribed all user utterances and parsed the
transcriptions offline using a natural language un-
derstanding component (a robust Keyword Parser)
in order to get a gold-standard labeling of the data.
We devised four labels with decreasing order of
confidence: ?opt? (optimal), ?pos? (positive), ?neg?
(negative), ?ign? (ignore). These are automatically
generated using two different modules: a key-
word parser that computes the < SpeechAct ><
Task > pair as described in the previous sec-
tion and a Levenshtein Distance calculator, for the
computation of the DMA and WER of each hy-
pothesis respectively. The reason for opting for a
more abstract level, namely the semantics of the
hypotheses rather than individual word recogni-
tion, is that in SDS it is usually sufficient to rely
on the meaning of message that is being conveyed
by the user rather than the precise words that they
used.
Similar to (Gabsdil and Lemon, 2004; Jonson,
2006) we ascribe to each utterance either of the
?opt?, ?pos?, ?neg?, ?ign? labels according to the
following schema:
? opt: The hypothesis is perfectly aligned and
semantically identical to the transcription
? pos: The hypothesis is not entirely aligned
(WER < 50) but is semantically identical to
the transcription
? neg: The hypothesis is semantically identical
to the transcription but does not align well
(WER > 50) or is semantically different to
the transcription
? ign: The hypothesis was not addressed to
the system (crosstalk), or the user laughed,
coughed, etc.
The 50% value for the WER as a threshold for
the distinction between the ?pos? and ?neg? cate-
gory is adopted from (Gabsdil, 2003), based on
the fact that WER is affected by concept accuracy
(Boros et al, 1996). In other words, if a hypothe-
sis is erroneous as far as its transcript is concerned
507
Transcript: I?d like to find a bar please
I WOULD LIKE TO FIND A BAR PLEASE pos
I LIKE TO FIND A FOUR PLEASE neg
I?D LIKE TO FIND A BAR PLEASE opt
WOULD LIKE TO FIND THE OR PLEASE ign
Table 1: Example hypothesis labelling
then it is highly likely that it does not convey the
correct message from a semantic point of view.
We always label conceptually equivalent hypothe-
ses to a particular transcription as potential candi-
date dialogue strategy moves, and total misrecog-
nitions as rejections. In table 5.1 we show exam-
ples of the four labels. Note that in the case of
silence, we give an ?opt? to the empty hypothesis.
6 The Baseline and Oracle Systems
The baseline for our experiments is the behavior
of the Town-Info spoken dialogue system that was
used to collect the experimental data. We evaluate
the performance of the baseline system by analyz-
ing the dialogue logs from the user study.
As an oracle for the system we defined the
choice of either the first ?opt? in the n-best list,
or if this does not exist the first ?pos? in the list.
In this way it is guaranteed that we always get as
output a perfect match to the true transcript as far
as its Dialogue Move is concerned, provided there
exists a perfect match somewhere in the list.
6.1 Baseline and Oracle Results
Table 2 summarizes the evaluation of the baseline
and oracle systems. We note that the Baseline sys-
tem already performs quite well on this data, when
we consider that in about 20% of n-best lists there
is no semantically correct hypothesis.
Baseline Oracle
WER 47.72% 42.16%
DMA 75.05% 80.20%
SA 40.48% 45.27%
Table 2: Baseline and Oracle results (statistically
significant at p < 0.001)
7 Classifying and Selecting N-best
Recognition Hypotheses
We use a threshold (50%) on a hypothesis? WER
as an indicator for whether hypotheses should be
clarified or rejected. This is adopted from (Gabs-
dil, 2003), based on the fact that WER correlates
with concept accuracy (CA, (Boros et al, 1996)).
7.1 Classification: Feature Groups
We represent recognition hypotheses as 13-
dimensional feature vectors for automatic classi-
fication. The feature vectors combine recognizer
confidence scores, low-level acoustic information,
and information from the User Simulation.
All the features used by the system are extracted
by the dialogue logs, the n-best lists per utterance
and per word and the audio files. The majority
of the features chosen are based on their success
in previous systems as described in the literature
(see section 2). The novel feature here is the User
Simulation score which may make redundant most
of the dialogue features used in other studies.
In order to measure the usefulness of each can-
didate feature and thus choose the most important
we use the metrics of Information Gain and Gain
Ratio (see table 3 in section 8.1) on the whole
training set, i.e. 93240 hypotheses.
In total 13 attributes were extracted, that can be
grouped into 4 main categories; those that concern
the current hypothesis to be classified, those that
concern low-level statistics of the audio files, those
that concern the whole n-best list, and finally the
User Simulation feature.
? Current Hypothesis Features (CHF) (6):
acoustic score, overall model confidence
score, minimum word confidence score,
grammar parsability, hypothesis length and
hypothesis duration.
? Acoustic Features (AF) (3): minimum, max-
imum and RMS amplitude
? List Features (LF) (3): n-best rank, deviation
of confidence scores in the list, match with
most frequent Dialogue Move
? User Simulation (US) (1): User Simulation
confidence score
The Current Hypothesis features (CHF) were
extracted from the n-best list files that contained
the hypotheses? transcription along with overall
acoustic score per utterance and from the equiv-
alent files that contained the transcription of each
word along with the start of frame, end of frame
and confidence score:
508
Acoustic score is the negative log likelihood as-
cribed by the speech recogniser to the whole hy-
pothesis, being the sum of the individual word
acoustic scores. Intuitively this is considered to
be helpful since it depicts the confidence of the
statistical model only for each word and is also
adopted in previous studies. Incorrect alignments
shall tend to adapt less well to the model and thus
have low log likelihood.
Overall model confidence score is the average
of the individual word confidence scores.
Minimum word confidence score is also com-
puted by the individual word transcriptions and ac-
counts for the confidence score of the word which
the speech recogniser is least certain of. It is ex-
pected to help our classifier distinguish between
poor overall hypothesis recognitions since a high
overall confidence score can sometimes be mis-
leading.
Grammar Parsability is the negative log
likelihood of the transcript for the current hy-
pothesis as produced by the Stanford Parser, a
wide-coverage Probabilistic Context-Free Gram-
mar (PCFG) (Klein and Manning, 2003) 1. This
feature seems helpful since we expect that a highly
ungrammatical hypothesis is likely not to match
with the true transcription semantically.
Hypothesis duration is the length of the hy-
pothesis in milliseconds as extracted from the n-
best list files with transcriptions per word that in-
clude the start and the end time of the recognised
frame. The reason for the inclusion of this fea-
ture is that it can help distinguish between short
utterances such as yes/no answers, medium-sized
utterances of normal answers and long utterances
caused by crosstalk.
Hypothesis length is the number of words in a
hypothesis and is considered to help in a similar
way as the above feature.
The Acoustic Features (AF) were extracted di-
rectly from the wave files using SoX: Minimum,
maximum and RMS amplitude are straightforward
features common in the previous studies men-
tioned in section 2.
The List Features (LF) were calculated based
on the n-best list files with transcriptions per utter-
ance and per word and take into account the whole
list:
N-best rank is the position of the hypothesis in
the list and could be useful in the sense that ?opt?
1http://nlp.stanford.edu/software/lex-parser.shtml
and ?pos? may be found in the upper part of the list
rather than the bottom.
Deviation of confidence scores in the list is
the deviation of the overall model confidence score
of the hypothesis from the mean confidence score
in the list. This feature is extracted in the hope
that it will indicate potential clusters of confidence
scores in particular positions in the list, i.e. group
hypotheses that deviate in a specific fashion from
the mean and thus indicating them being classified
with the same label.
Match with most frequent Dialogue Move is
the only boolean feature and indicates whether the
Dialogue Move of the current hypothesis, i.e. the
pair of < SpeechAct >< Task > coincides with
the most frequent one. The trend in n-best lists
is to have a majority of utterances that belong to
one or two labels and only one hypothesis belong-
ing to the ?opt? category and/or a few to the ?pos?
category. As a result, the idea behind this feature
is to extract such potential outliers which are the
desired goal for the re-ranker.
Finally, the User Simulation score is given as
an output from the User Simulation model and
adapted for the purposes of this study (see section
3 for more details). The model is operating with 5-
grams. Its input is given by two different sources:
the history of the dialogue, namely the 4 previous
Dialogue Moves, is taken from the dialogue log
and the current hypothesis? semantic parse which
is generated on the fly by the same keyword parser
used in the automatic labelling.
User Simulation score is the probability that
the current hypothesis? Dialogue Move has really
been said by the user given the 4 previous Dia-
logue Moves. The potential advantages of this fea-
ture have been discussed in section 3.
7.2 Learner and Selection Procedure
We use the memory based learner TiMBL (Daele-
mans et al, 2002) to predict the class of each of
the 60-best recognition hypotheses for a given ut-
terance.
TiMBL was trained using different parameter
combinations mainly choosing between number of
k-nearest neighbours (1 to 5) and distance metrics
(Weighted Overlap andModified Value Difference
Metric). In a second step, we decide which (if any)
of the classified hypotheses we actually want to
pick as the best result and how the user utterance
should be classified as a whole.
509
1. Scan the list of classified n-best recognition
hypotheses top-down. Return the first result
that is classified as ?opt?.
2. If 1. fails, scan the list of classified n-best
recognition hypotheses top-down. Return the
first result that is classified as ?pos?.
3. If 2. fails, count the number of negs and igns
in the classified recognition hypotheses. If
the number of negs is larger or equal than the
number of igns then return the first ?neg?.
4. Else return the first ?ign? utterance.
8 Experiments
Experiments were conducted in two layers: the
first layer concerns only the classifier, i.e. the abil-
ity of the system to correctly classify each hypoth-
esis to either of the four labels ?opt?, ?pos?, ?neg?,
?ign? and the second layer the re-ranker, i.e. the
ability of the system to boost the speech recog-
niser?s accuracy.
All results are drawn from the TiMBL classi-
fier trained with the Weighted Overlap metric and
k = 1 nearest neighbours settings. Both layers
are trained on 75% of the same Town-Info Corpus
of 126 dialogues containing 60-best lists for 1554
user utterances or a total of 93240 hypotheses. The
first layer was tested against a separate Town-Info
Corpus of 58 dialogues containing 510 user utter-
ances or a total of 30600 hypotheses, while the
second was tested on the whole training set with
10-fold cross-validation.
Using this corpus, a series of experiments was
carried out using different sets of features in order
to both determine and illustrate the increasing per-
formance of the classifier. These sets were deter-
mined not only by the literature but also by the In-
formation Gain measures that were calculated on
the training set using WEKA, as shown in table 3.
8.1 Information Gain
Quite surprisingly, we note that the rank given by
the Information Gain measure coincides perfectly
with the logical grouping of the attributes that was
initially performed (see table 3).
As a result, we chose to use this grouping for
the final 4 feature sets on which the classifier
experiments were performed, in the following
order:
Experiment 1: List Features (LF)
InfoGain Attribute
1.0324 userSimulationScore
0.9038 rmsAmp
0.8280 minAmp
0.8087 maxAmp
0.4861 parsability
0.3975 acousScore
0.3773 hypothesisDuration
0.2545 hypothesisLength
0.1627 avgConfScore
0.1085 minWordConfidence
0.0511 nBestRank
0.0447 standardDeviation
0.0408 matchesFrequentDM
Table 3: Information Gain
Experiment 2: List Features + Current Hypothe-
sis Features (LF+CHF)
Experiment 3: List Features + Current Hypothe-
sis Features + Acoustic Features (LF+CHF+AF)
Experiment 4: List Features + Current Hy-
pothesis Features + Acoustic Features + User
Simulation (LF+CHF+AF+US)
Note that the User Simulation score is a very
strong feature, scoring first in the Information
Gain rank, validating our central hypothesis.
The testing of the classifier using each of the
above feature sets was performed on the remain-
ing 25% of the Town-Info corpus comprising of 58
dialogues, consisting of 510 utterances and taking
the 60-best lists resulting in a total of 30600 vec-
tors. In each experiment we measured Precision,
Recall, F-measure per class and total Accuracy of
the classifier .
For the second layer, we used a trained instance
of the TiMBL classifier on the 4th feature set (List
Features + Current Hypothesis Features + Acous-
tic Features + User Simulation) and performed re-
ranking using the algorithm presented in section
7.2 on the same training set used in the first layer
using 10-fold cross validation.
9 Results and Evaluation
We performed two series of experiments in two
layers: the first corresponds to the training of the
classifier alone and the second to the system as a
whole measuring the re-ranker?s output.
510
Feature set (opt) Precision Recall F1
LF 42.5% 58.4% 49.2%
LF+CHF 62.4% 65.7% 64.0%
LF+CHF+AF 55.6% 61.6% 58.4%
LF+CHF+AF+US 70.5% 73.7% 72.1%
Table 4: Results for the ?opt? category
Feature set (pos) Precision Recall F1
LF 25.2% 1.7% 3.2%
LF+CHF 51.2% 57.4% 54.1%
LF+CHF+AF 51.5% 54.6% 53.0%
LF+CHF+AF+US 64.8% 61.8% 63.3%
Table 5: Results for the ?pos? category
9.1 First Layer: Classifier Experiments
In these series of experiments we measure preci-
sion, recall and F1-measure for each of the four
labels and overall F1-measure and accuracy of the
classifier. In order to have a better view of the
classifier?s performance we have also included the
confusion matrix for the final experiment with all
13 attributes. Tables 4 -7 show per class and per
attribute set measures, while Table 8 shows a col-
lective view of the results for the four sets of at-
tributes and the baseline being the majority class
label ?neg?. Table 9 shows the confusion matrix
for the final experiment.
In tables 4 - 8 we generally notice an increase
in precision, recall and F1-measure as we pro-
gressively add more attributes to the system with
the exception of the addition of the Acoustic Fea-
tures which seem to impair the classifier?s perfor-
mance. We also make note of the fact that in the
case of the 4th attribute set the classifier can dis-
tinguish very well the ?neg? and ?ign? categories
with 86.3% and 99.9% F1-measure respectively.
Most importantly, we observe a remarkable boost
in F1-measure and accuracy with the addition of
the User Simulation score. We find a 37.36% rel-
ative increase in F1-measure and 34.02% increase
Feature set (neg) Precision Recall F1
LF 54.2% 96.4% 69.4%
LF+CHF 70.7% 75.0% 72.8%
LF+CHF+AF 69.5% 73.4% 71.4%
LF+CHF+AF+US 85.6% 87.0% 86.3%
Table 6: Results for the ?neg? category
Feature set (ign) Precision Recall F1
LF 19.6% 1.3% 2.5%
LF+CHF 63.5% 48.7% 55.2%
LF+CHF+AF 59.3% 48.9% 53.6%
LF+CHF+AF+US 99.9% 99.9% 99.9%
Table 7: Results for the ?ign? category
Feature set F1 Accuracy
Baseline - 51.1%
LF 37.3% 53.1%
LF+CHF 64.1% 64.8%
LF+CHF+AF 62.6% 63.4%
LF+CHF+AF+US 86.0% 84.9%
Table 8: F1-Measure and Accuracy for the four
attribute sets
in the accuracy compared to the 3rd experiment,
which contains all but the User Simulation score
attribute and a 66.20% relative increase of the ac-
curacy compared to the Baseline. In table 7 we
make note of a rather low recall measure for the
?ign? category in the case of the LF experiment,
suggesting that the list features do not add extra
value to the classifier, partially validating the In-
formation Gain measure (Table 3).
Taking a closer look at the 4th experiment with
all 13 features we notice in table 9 that most er-
rors occur between the ?pos? and ?neg? category.
In fact, for the ?neg? category the False Positive
Rate (FPR) is 18.17% and for the ?pos? 8.9%, all
in all a lot larger than for the other categories.
9.2 Second Layer: Re-ranker Experiments
In these experiments we measure WER, DMA
and SA for the system as a whole. In order to
make sure that the improvement noted was re-
ally attributed to the classifier we computed the
p-values for each of these measures using the
Wilcoxon signed rank test for WER andMcNemar
chi-square test for the DMA and SA measures.
In table 10 we note that the classifier scores
opt pos neg ign
opt 232 37 46 0
pos 47 4405 2682 8
neg 45 2045 13498 0
ign 5 0 0 7550
Table 9: Confusion Matrix for LF+CHF+AF+US
511
Baseline Classifier Oracle
WER 47.72% 45.27% ** 42.16%***
DMA 75.05% 78.22% * 80.20% ***
SA 40.48% 42.26% 45.27%***
Table 10: Baseline, Classifier, and Oracle results
(*** = p < 0.001, ** = p < 0.01, * = p < 0.05)
Label Precision Recall F1
opt 74.0% 64.1% 68.7%
pos 76.3% 46.2% 57.6%
neg 81.9% 94.4% 87.7%
ign 99.9% 99.9% 99.9%
Table 11: Precision, Recall and F1: high-level fea-
tures
45.27% WER making a notable relative reduction
of 5.13% compared to the baseline and 78.22%
DMA incurring a relative improvement of 4.22%.
The classifier scored 42.26% on SA but it was
not considered significant compared to the base-
line (0.05 < p < 0.10). Comparing the classifier?s
performance with the Oracle it achieves a 44.06%
of the possible WER improvement on this data,
61.55% for the DMA measure and 37.16% for the
SA measure.
Finally, we also notice that the Oracle has a
80.20% for the DMA, which means that 19.80%
of the n-best lists did not include at all a hypothe-
sis that matched semantically to the true transcript.
10 Experiment with high-level features
We trained a Memory Based Classifier based only
on the higher level features of merely the User
Simulation score and the Grammar Parsability
(US + GP). The idea behind this choice is to try
and find a combination of features that ignores low
level characteristics of the user?s utterances as well
as features that heavily rely on the speech recog-
niser and thus by default are not considered to be
very trustworthy.
Quite surprisingly, the results taken from an ex-
periment with just the User Simulation score and
the Grammar Parsability are very promising and
comparable with those acquired from the 4th ex-
periment with all 13 attributes. Table 11 shows
the precision, recall and F1-measure per label and
table 12 illustrates the classifier?s performance in
comparison with the 4th experiment.
Table 12 shows that there is a somewhat consid-
Feature set F1 Accuracy Ties
LF+CHF+AF+US 86.0% 84.9% 4993
US+GP 85.7% 85.6% 115
Table 12: F1, Accuracy and number of ties cor-
rectly resolved for LF+CHF+AF+US and US+GP
feature sets
erable decrease in the recall and a corresponding
increase in the precision of the ?pos? and ?opt? cat-
egories compared to the LF + CHF + AF + US at-
tribute set, which account for lower F1-measures.
However, all in all the US + GP set manages to
classify correctly 207 more vectors and quite in-
terestingly commits far fewer ties and manages to
resolve more compared to the full 13 attribute set.
11 Conclusion
We used a combination of acoustic features and
features computed from dialogue context to pre-
dict the quality of incoming recognition hypothe-
ses to an SDS. In particular we use a score com-
puted from a simple statistical User Simulation,
which measures the likelihood that the user re-
ally said each hypothesis. The approach is novel
in combining User Simulations, machine learning,
and n-best processing for spoken dialogue sys-
tems. We employed a User Simulation model,
trained on real dialogue data, to predict the user?s
next dialogue move. This prediction was used to
re-rank n-best hypotheses of a speech recognizer
for a corpus of 2564 user utterances. The results,
obtained using TiMBL and an n-gram User Sim-
ulation, show a significant relative reduction of
Word Error Rate of 5% (this is 44% of the pos-
sible WER improvement on this data), and 62%
of the possible Dialogue Move Accuracy improve-
ment, compared to the baseline policy of selecting
the topmost ASR hypothesis. The majority of the
improvement is attributable to the User Simulation
feature. Clearly, this improvement would result in
better dialogue system performance overall.
Acknowledgments
We thank Helen Hastie and Kallirroi Georgila.
The research leading to these results has re-
ceived funding from the EPSRC (project no.
EP/E019501/1) and from the European Commu-
nity?s Seventh Framework Programme (FP7/2007-
2013) under grant agreement no. 216594 (CLAS-
SiC project www.classic-project.org)
512
References
M. Boros, W. Eckert, F. Gallwitz, G. Go?rz, G. Han-
rieder, and H. Niemann. 1996. Towards understand-
ing spontaneous speech: Word accuracy vs. concept
accuracy. In Proceedings ICSLP ?96, volume 2,
pages 1009?1012, Philadelphia, PA.
Ananlada Chotimongkol and Alexander I. Rudnicky.
2001. N-best Speech Hypotheses Reordering Using
Linear Regression. In Proceedings of EuroSpeech
2001, pages 1829?1832.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2002. TIMBL: Tilburg Mem-
ory Based Learner, version 4.2, Reference Guide. In
ILK Technical Report 02-01.
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recogni-
tion performance in spoken dialogue systems. In
Proceedings of ACL-04, pages 344?351.
Malte Gabsdil. 2003. Classifying Recognition Results
for Spoken Dialogue Systems. In Proceedings of the
Student Research Workshop at ACL-03.
Kallirroi Georgila, James Henderson, and Oliver
Lemon. 2006. User simulation for spoken dialogue
systems: Learning and evaluation. In Proceedings
of Interspeech/ICSLP, pages 1065?1068.
R. Jonson. 2006. Dialogue Context-Based Re-ranking
of ASR Hypotheses. In Proceedings IEEE 2006
Workshop on Spoken Language Technology.
D. Klein and C. Manning. 2003. Fast exact inference
with a factored model for natural language parsing.
Journal of Advances in Neural Information Process-
ing Systems, 15(2).
Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2000. Predicting Automatic Speech Recognition
Performance Using Prosodic Cues. In Proceedings
of NAACL.
M. Pickering and S. Garrod. 2007. Do people use lan-
guage production to make predictions during com-
prehension? Journal of Trends in Cognitive Sci-
ences, 11(3).
J Schatzmann, K Weilhammer, M N Stuttle, and S J
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. Knowledge En-
gineering Review, 21:97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proceedings of HLT/NAACL.
David Traum, Johan Bos, Robin Cooper, Staffan Lars-
son, Ian Lewin, Colin Matheson, and Massimo Poe-
sio. 1999. A Model of Dialogue Moves and In-
formation State Revision. Technical Report D2.1,
Trindi Project.
Marilyn Walker, Jerry Wright, and Irene Langkilde.
2000. Using Natural Language Processing and Dis-
course Features to Identify Understanding Errors
in a Spoken Dialogue System. In Proceedings of
ICML-2000.
Jason Williams and Steve Young. 2003. Using wizard-
of-oz simulations to bootstrap reinforcement-
learning-based dialog management systems. In
Proc. 4th SIGdial workshop.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
SJ Young. 2006. Using POMDPs for Dialog Manage-
ment. In IEEE/ACL Workshop on Spoken Language
Technology (SLT 2006), Aruba.
Steve Young. 2007. ATK: An Application Toolkit
for HTK, Version 1.6. Technical report, Cambridge
University Engineering Department.
513
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683?691,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Natural Language Generation as Planning Under Uncertainty for Spoken
Dialogue Systems
Verena Rieser
School of Informatics
University of Edinburgh
vrieser@inf.ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
olemon@inf.ed.ac.uk
Abstract
We present and evaluate a new model for
Natural Language Generation (NLG) in
Spoken Dialogue Systems, based on statis-
tical planning, given noisy feedback from
the current generation context (e.g. a user
and a surface realiser). We study its use in
a standard NLG problem: how to present
information (in this case a set of search re-
sults) to users, given the complex trade-
offs between utterance length, amount of
information conveyed, and cognitive load.
We set these trade-offs by analysing exist-
ing MATCH data. We then train a NLG pol-
icy using Reinforcement Learning (RL),
which adapts its behaviour to noisy feed-
back from the current generation context.
This policy is compared to several base-
lines derived from previous work in this
area. The learned policy significantly out-
performs all the prior approaches.
1 Introduction
Natural language allows us to achieve the same
communicative goal (?what to say?) using many
different expressions (?how to say it?). In a Spo-
ken Dialogue System (SDS), an abstract commu-
nicative goal (CG) can be generated in many dif-
ferent ways. For example, the CG to present
database results to the user can be realized as a
summary (Polifroni and Walker, 2008; Demberg
andMoore, 2006), or by comparing items (Walker
et al, 2004), or by picking one item and recom-
mending it to the user (Young et al, 2007).
Previous work has shown that it is useful to
adapt the generated output to certain features of
the dialogue context, for example user prefer-
ences, e.g. (Walker et al, 2004; Demberg and
Moore, 2006), user knowledge, e.g. (Janarthanam
and Lemon, 2008), or predicted TTS quality, e.g.
(Nakatsu and White, 2006).
In extending this previous work we treat NLG
as a statistical sequential planning problem, anal-
ogously to current statistical approaches to Dia-
logue Management (DM), e.g. (Singh et al, 2002;
Henderson et al, 2008; Rieser and Lemon, 2008a)
and ?conversation as action under uncertainty?
(Paek and Horvitz, 2000). In NLG we have
similar trade-offs and unpredictability as in DM,
and in some systems the content planning and DM
tasks are overlapping. Clearly, very long system
utterances with many actions in them are to be
avoided, because users may become confused or
impatient, but each individual NLG action will
convey some (potentially) useful information to
the user. There is therefore an optimization prob-
lem to be solved. Moreover, the user judgements
or next (most likely) action after each NLG action
are unpredictable, and the behaviour of the surface
realizer may also be variable (see Section 6.2).
NLG could therefore fruitfully be approached
as a sequential statistical planning task, where
there are trade-offs and decisions to make, such as
whether to choose another NLG action (and which
one to choose) or to instead stop generating. Re-
inforcement Learning (RL) allows us to optimize
such trade-offs in the presence of uncertainty, i.e.
the chances of achieving a better state, while en-
gaging in the risk of choosing another action.
In this paper we present and evaluate a new
model for NLG in Spoken Dialogue Systems as
planning under uncertainty. In Section 2 we argue
for applying RL to NLG problems and explain the
overall framework. In Section 3 we discuss chal-
lenges for NLG for Information Presentation. In
Section 4 we present results from our analysis of
the MATCH corpus (Walker et al, 2004). In Sec-
tion 5 we present a detailed example of our pro-
posed NLG method. In Section 6 we report on
experimental results using this framework for ex-
ploring Information Presentation policies. In Sec-
tion 7 we conclude and discuss future directions.
683
2 NLG as planning under uncertainty
We adopt the general framework of NLG as plan-
ning under uncertainty (see (Lemon, 2008) for the
initial version of this approach). Some aspects of
NLG have been treated as planning, e.g. (Koller
and Stone, 2007; Koller and Petrick, 2008), but
never before as statistical planning.
NLG actions take place in a stochastic environ-
ment, for example consisting of a user, a realizer,
and a TTS system, where the individual NLG ac-
tions have uncertain effects on the environment.
For example, presenting differing numbers of at-
tributes to the user, and making the user more or
less likely to choose an item, as shown by (Rieser
and Lemon, 2008b) for multimodal interaction.
Most SDS employ fixed template-based gener-
ation. Our goal, however, is to employ a stochas-
tic realizer for SDS, see for example (Stent et al,
2004). This will introduce additional noise, which
higher level NLG decisions will need to react
to. In our framework, the NLG component must
achieve a high-level Communicative Goal from
the Dialogue Manager (e.g. to present a number
of items) through planning a sequence of lower-
level generation steps or actions, for example first
to summarize all the items and then to recommend
the highest ranking one. Each such action has un-
predictable effects due to the stochastic realizer.
For example the realizer might employ 6 attributes
when recommending item i4, but it might use only
2 (e.g. price and cuisine for restaurants), depend-
ing on its own processing constraints (see e.g. the
realizer used to collect the MATCH project data).
Likewise, the user may be likely to choose an item
after hearing a summary, or they may wish to hear
more. Generating appropriate language in context
(e.g. attributes presented so far) thus has the fol-
lowing important features in general:
? NLG is goal driven behaviour
? NLG must plan a sequence of actions
? each action changes the environment state or
context
? the effect of each action is uncertain.
These facts make it clear that the problem of
planning how to generate an utterance falls nat-
urally into the class of statistical planning prob-
lems, rather than rule-based approaches such as
(Moore et al, 2004; Walker et al, 2004), or super-
vised learning as explored in previous work, such
as classifier learning and re-ranking, e.g. (Stent et
al., 2004; Oh and Rudnicky, 2002). Supervised
approaches involve the ranking of a set of com-
pleted plans/utterances and as such cannot adapt
online to the context or the user. Reinforcement
Learning (RL) provides a principled, data-driven
optimisation framework for our type of planning
problem (Sutton and Barto, 1998).
3 The Information Presentation Problem
We will tackle the well-studied problem of Infor-
mation Presentation in NLG to show the benefits
of this approach. The task here is to find the best
way to present a set of search results to a user
(e.g. some restaurants meeting a certain set of con-
straints). This is a task common to much prior
work in NLG, e.g. (Walker et al, 2004; Demberg
and Moore, 2006; Polifroni and Walker, 2008).
In this problem, there there are many decisions
available for exploration. For instance, which pre-
sentation strategy to apply (NLG strategy selec-
tion), how many attributes of each item to present
(attribute selection), how to rank the items and at-
tributes according to different models of user pref-
erences (attribute ordering), how many (specific)
items to tell them about (conciseness), how many
sentences to use when doing so (syntactic plan-
ning), and which words to use (lexical choice) etc.
All these parameters (and potentially many more)
can be varied, and ideally, jointly optimised based
on user judgements.
We had two corpora available to study some of
the regions of this decision space. We utilise the
MATCH corpus (Walker et al, 2004) to extract an
evaluation function (also known as ?reward func-
tion?) for RL. Furthermore, we utilise the SPaRKy
corpus (Stent et al, 2004) to build a high quality
stochastic realizer. Both corpora contain data from
?overhearer? experiments targeted to Information
Presentation in dialogues in the restaurant domain.
While we are ultimately interested in how hearers
engaged in dialogues judge different Information
Presentations, results from overhearers are still di-
rectly relevant to the task.
4 MATCH corpus analysis
The MATCH project made two data sets available,
see (Stent et al, 2002) and (Whittaker et al, 2003),
which we combine to define an evaluation function
for different Information Presentation strategies.
684
strategy example av.#attr av.#sentence
SUMMARY ?The 4 restaurants differ in food quality, and cost.?
(#attr = 2,#sentence = 1)
2.07?.63 1.56?.5
COMPARE ?Among the selected restaurants, the following offer
exceptional overall value. Aureole?s price is 71 dol-
lars. It has superb food quality, superb service and
superb decor. Daniel?s price is 82 dollars. It has su-
perb food quality, superb service and superb decor.?
(#attr = 4,#sentence = 5)
3.2?1.5 5.5?3.11
RECOMMEND ?Le Madeleine has the best overall value among the
selected restaurants. Le Madeleine?s price is 40 dol-
lars and It has very good food quality. It?s in Mid-
town West. ? (#attr = 3,#sentence = 3)
2.4?.7 3.5?.53
Table 1: NLG strategies present in the MATCH corpus with average no. attributes and sentences as found
in the data.
The first data set, see (Stent et al, 2002), com-
prises 1024 ratings by 16 subjects (where we only
use the speech-based half, n = 512) on the follow-
ing presentation strategies: RECOMMEND, COM-
PARE, SUMMARY. These strategies are realized
using templates as in Table 2, and varying num-
bers of attributes. In this study the users rate the
individual presentation strategies as significantly
different (F (2) = 1361, p < .001). We find that
SUMMARY is rated significantly worse (p = .05
with Bonferroni correction) than RECOMMEND
and COMPARE, which are rated as equally good.
This suggests that one should never generate
a SUMMARY. However, SUMMARY has different
qualities from COMPARE and RECOMMEND, as
it gives users a general overview of the domain,
and probably helps the user to feel more confi-
dent when choosing an item, especially when they
are unfamiliar with the domain, as shown by (Po-
lifroni and Walker, 2008).
In order to further describe the strategies, we ex-
tracted different surface features as present in the
data (e.g. number of attributes realised, number of
sentences, number of words, number of database
items talked about, etc.) and performed a step-
wise linear regression to find the features which
were important to the overhearers (following the
PARADISE framework (Walker et al, 2000)). We
discovered a trade-off between the length of the ut-
terance (#sentence) and the number of attributes
realised (#attr), i.e. its informativeness, where
overhearers like to hear as many attributes as pos-
sible in the most concise way, as indicated by
the regression model shown in Equation 1 (R2 =
.34). 1
score = .775?#attr + (?.301)?#sentence;
(1)
The second MATCH data set, see (Whittaker et
al., 2003), comprises 1224 ratings by 17 subjects
on the NLG strategies RECOMMEND and COM-
PARE. The strategies realise varying numbers of
attributes according to different ?conciseness? val-
ues: concise (1 or 2 attributes), average (3
or 4), and verbose (4,5, or 6). Overhearers
rate all conciseness levels as significantly different
(F (2) = 198.3, p < .001), with verbose rated
highest and concise rated lowest, supporting
our findings in the first data set. However, the rela-
tion between number of attributes and user ratings
is not strictly linear: ratings drop for #attr = 6.
This suggests that there is an upper limit on how
many attributes users like to hear. We expect this
to be especially true for real users engaged in ac-
tual dialogue interaction, see (Winterboer et al,
2007). We therefore include ?cognitive load? as a
variable when training the policy (see Section 6).
In addition to the trade-off between length and
informativeness for single NLG strategies, we are
interested whether this trade-off will also hold for
generating sequences of NLG actions. (Whittaker
et al, 2002), for example, generate a combined
strategy where first a SUMMARY is used to de-
scribe the retrieved subset and then they RECOM-
MEND one specific item/restaurant. For example
?The 4 restaurants are all French, but differ in
1For comparison: (Walker et al, 2000) report on R2 be-
tween .4 and .5 on a slightly lager data set.
685
Figure 1: Possible NLG policies (X=stop generation)
food quality, and cost. Le Madeleine has the best
overall value among the selected restaurants. Le
Madeleine?s price is 40 dollars and It has very
good food quality. It?s in Midtown West.?
We therefore extend the set of possible strate-
gies present in the data for exploration: we allow
ordered combinations of the strategies, assuming
that only COMPARE or RECOMMEND can follow a
SUMMARY, and that only RECOMMEND can fol-
low COMPARE, resulting in 7 possible actions:
1. RECOMMEND
2. COMPARE
3. SUMMARY
4. COMPARE+RECOMMEND
5. SUMMARY+RECOMMEND
6. SUMMARY+COMPARE
7. SUMMARY+COMPARE+RECOMMEND
We then analytically solved the regression
model in Equation 1 for the 7 possible strategies
using average values from the MATCH data. This is
solved by a system of linear inequalities. Accord-
ing to this model, the best ranking strategy is to
do all the presentation strategies in one sequence,
i.e. SUMMARY+COMPARE+RECOMMEND. How-
ever, this analytic solution assumes a ?one-shot?
generation strategy where there is no intermediate
feedback from the environment: users are simply
static overhearers (they cannot ?barge-in? for ex-
ample), there is no variation in the behaviour of the
surface realizer, i.e. one would use fixed templates
as in MATCH, and the user has unlimited cogni-
tive capabilities. These assumptions are not real-
istic, and must be relaxed. In the next Section we
describe a worked through example of the overall
framework.
5 Method: the RL-NLG model
For the reasons discussed above, we treat the
NLG module as a statistical planner, operat-
ing in a stochastic environment, and optimise
it using Reinforcement Learning. The in-
put to the module is a Communicative Goal
supplied by the Dialogue Manager. The CG
consists of a Dialogue Act to be generated,
for example present items(i1, i2, i5, i8),
and a System Goal (SysGoal) which is
the desired user reaction, e.g. to make the
user choose one of the presented items
(user choose one of(i1, i2, i5, i8)). The
RL-NLG module must plan a sequence of lower-
level NLG actions that achieve the goal (at lowest
cost) in the current context. The context consists
of a user (who may remain silent, supply more
constraints, choose an item, or quit), and variation
from the sentence realizer described above.
Now let us walk-through one simple ut-
terance plan as carried out by this model,
as shown in Table 2. Here, we start
with the CG present items(i1, i2, i5, i8)&
user choose one of(i1, i2, i5, i8) from the
system?s DM. This initialises the NLG state (init).
The policy chooses the action SUMMARY and this
transitions us to state s1, where we observe that
4 attributes and 1 sentence have been generated,
and the user is predicted to remain silent. In this
state, the current NLG policy is to RECOMMEND
the top ranked item (i5, for this user), which takes
us to state s2, where 8 attributes have been gener-
ated in a total of 4 sentences, and the user chooses
an item. The policy holds that in states like s2 the
686
init s1 s2
summarise recommend
end
stop
atts=4user=silent atts=8user=chooseENVIRONMENT:
ACTIONS:
GOAL
Reward
Figure 2: Example RL-NLG action sequence for Table 4
State Action State change/effect
init SysGoal: present items(i1, i2, i5, i8)& user choose one of(i1, i2, i5, i8) initialise state
s1 RL-NLG: SUMMARY(i1, i2, i5, i8) att=4, sent=1, user=silent
s2 RL-NLG: RECOMMEND(i5) att=8, sent=4, user=choose(i5)
end RL-NLG: stop calculate Reward
Table 2: Example utterance planning sequence for Figure 2
best thing to do is ?stop? and pass the turn to the
user. This takes us to the state end, where the total
reward of this action sequence is computed (see
Section 6.3), and used to update the NLG policy
in each of the visited state-action pairs via back-
propagation.
6 Experiments
We now report on a proof-of-concept study where
we train our policy in a simulated learning envi-
ronment based on the results from the MATCH cor-
pus analysis in Section 4. Simulation-based RL
allows to explore unseen actions which are not in
the data, and thus less initial data is needed (Rieser
and Lemon, 2008b). Note, that we cannot directly
learn from the MATCH data, as therefore we would
need data from an interactive dialogue. We are
currently collecting such data in a Wizard-of-Oz
experiment.
6.1 User simulation
User simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al, 2007). A user simulation for
NLG is very similar, in that it is a predictive model
of the most likely next user act. However, this user
act does not actually change the overall dialogue
state (e.g. by filling slots) but it only changes the
generator state. In other words, the NLG user sim-
ulation tells us what the user is most likely to do
next, if we were to stop generating now. It also
tells us the probability whether the user chooses
to ?barge-in? after a system NLG action (by either
choosing an item or providing more information).
The user simulation for this study is a simple
bi-gram model, which relates the number of at-
tributes presented to the next likely user actions,
see Table 3. The user can either follow the goal
provided by the DM (SysGoal), for example
choosing an item. The user can also do some-
thing else (userElse), e.g. providing another
constraint, or the user can quit (userQuit).
For simplification, we discretise the number of
attributes into concise-average-verbose,
reflecting the conciseness values from the MATCH
data, as described in Section 4. In addition, we
assume that the user?s cognitive abilities are lim-
ited (?cognitive load?), based on the results from
the second MATCH data set in Section 4. Once the
number of attributes is more than the ?magic num-
ber 7? (reflecting psychological results on short-
term memory) (Baddeley, 2001)) the user is more
likely to become confused and quit.
The probabilities in Table 3 are currently man-
ually set heuristics. We are currently conducting a
Wizard-of-Oz study in order to learn these proba-
687
bilities (and other user parameters) from real data.
SysGoal userElse userQuit
concise 20.0 60.0 20.0
average 60.0 20.0 20.0
verbose 20.0 20.0 60.0
Table 3: NLG bi-gram user simulation
6.2 Realizer model
The sequential NLG model assumes a realizer,
which updates the context after each generation
step (i.e. after each single action). We estimate
the realiser?s parameters from the mean values we
found in the MATCH data (see Table 1). For this
study we first (randomly) vary the number of at-
tributes, whereas the number of sentences is fixed
(see Table 4). In current work we replace the re-
alizer model with an implemented generator that
replicates the variation found in the SPaRKy real-
izer (Stent et al, 2004).
#attr #sentence
SUMMARY 1 or 2 2
COMPARE 3 or 4 6
RECOMMEND 2 or 3 3
Table 4: Realizer parameters
6.3 Reward function
The reward function defines the final goal of the
utterance generation sequence. In this experiment
the reward is a function of the various data-driven
trade-offs as identified in the data analysis in Sec-
tion 4: utterance length and number of provided
attributes, as weighted by the regression model
in Equation 1, as well as the next predicted user
action. Since we currently only have overhearer
data, we manually estimate the reward for the
next most likely user act, to supplement the data-
driven model. If in the end state the next most
likely user act is userQuit, the learner gets a
penalty of ?100, userElse receives 0 reward,
and SysGoal gains +100 reward. Again, these
hand coded scores need to be refined by a more
targeted data collection, but the other components
of the reward function are data-driven.
Note that RL learns to ?make compromises?
with respect to the different trade-offs. For ex-
ample, the user is less likely to choose an item
if there are more than 7 attributes, but the real-
izer can generate 9 attributes. However, in some
contexts it might be desirable to generate all 9 at-
tributes, e.g. if the generated utterance is short.
Threshold-based approaches, in contrast, cannot
(easily) reason with respect to the current content.
6.4 State and Action Space
We now formulate the problem as a Markov De-
cision Process (MDP), relating states to actions.
Each state-action pair is associated with a transi-
tion probability, which is the probability of mov-
ing from state s at time t to state s? at time t+1 af-
ter having performed action awhen in state s. This
transition probability is computed by the environ-
ment model (i.e. user and realizer), and explic-
itly captures noise/uncertainty in the environment.
This is a major difference to other non-statistical
planning approaches. Each transition is also as-
sociated with a reinforcement signal (or reward)
rt+1 describing how good the result of action a
was when performed in state s.
The state space comprises 9 binary features rep-
resenting the number of attributes, 2 binary fea-
tures representing the predicted user?s next ac-
tion to follow the system goal or quit, as well as
a discrete feature reflecting the number of sen-
tences generated so far, as shown in Figure 3.
This results in 211 ? 6 = 12, 288 distinct genera-
tion states. We trained the policy using the well
known SARSA algorithm, using linear function ap-
proximation (Sutton and Barto, 1998). The policy
was trained for 3600 simulated NLG sequences.
In future work we plan to learn lower level deci-
sions, such as lexical adaptation based on the vo-
cabulary used by the user.
6.5 Baselines
We derive the baseline policies from Informa-
tion Presentation strategies as deployed by cur-
rent dialogue systems. In total we utilise 7 differ-
ent baselines (B1-B7), which correspond to single
branches in our policy space (see Figure 1):
B1: RECOMMEND only, e.g. (Young et al, 2007)
B2: COMPARE only, e.g. (Henderson et al, 2008)
B3: SUMMARY only, e.g. (Polifroni and Walker,
2008)
B4: SUMMARY followed by RECOMMEND, e.g.
(Whittaker et al, 2002)
B5: Randomly choosing between COMPARE and
RECOMMEND, e.g. (Walker et al, 2007)
688
??
?
?
?
?
?
?
?
?
action:
?
?
?
?
?
SUMMARY
COMPARE
RECOMMEND
end
?
?
?
?
?
state:
?
?
?
?
?
?
?
?
?
?
attributes |1 |-|9 |:
{
0,1
}
sentence:
{
1-11
}
userGoal:
{
0,1
}
userQuit:
{
0,1
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 3: State-Action space for RL-NLG
B6: Randomly choosing between all 7 outputs
B7: Always generating whole sequence, i.e.
SUMMARY+COMPARE+RECOMMEND, as
suggested by the analytic solution (see
Section 4).
6.6 Results
We analyse the test runs (n=200) using an ANOVA
with a PostHoc T-Test (with Bonferroni correc-
tion). RL significantly (p < .001) outperforms all
baselines in terms of final reward, see Table 5. RL
is the only policy which significantly improves the
next most likely user action by adapting to features
in the current context. In contrast to conventional
approaches, RL learns to ?control? its environment
according to the estimated transition probabilities
and the associated rewards.
The learnt policy can be described as follows:
It either starts with SUMMARY or COMPARE af-
ter the init state, i.e. it learnt to never start with a
RECOMMEND. It stops generating after COMPARE
if the userGoal is (probably) reached (e.g. the
user is most likely to choose an item in the next
turn, which depends on the number of attributes
generated), otherwise it goes on and generates a
RECOMMEND. If it starts with SUMMARY, it al-
ways generates a COMPARE afterwards. Again, it
stops if the userGoal is (probably) reached, oth-
erwise it generates the full sequence (which corre-
sponds to the analytic solution B7).
The analytic solution B7 performs second best,
and significantly outperforms all the other base-
lines (p < .01). Still, it is significantly worse
(p < .001) than the learnt policy as this ?one-shot-
strategy? cannot robustly and dynamically adopt to
noise or changes in the environment.
In general, generating sequences of NLG ac-
tions rates higher than generating single actions
only: B4 and B6 rate directly after RL and B7,
while B1, B2, B3, B5 are all equally bad given
our data-driven definition of reward and environ-
ment. Furthermore, the simulated environment
allows us to replicate the results in the MATCH
corpus (see Section 4) when only comparing sin-
gle strategies: SUMMARY performs significantly
worse, while RECOMMEND and COMPARE per-
form equally well.
policy reward (?std)
B1 99.1 (?129.6)
B2 90.9 (?142.2)
B3 65.5 (?137.3)
B4 176.0 (?154.1)
B5 95.9 (?144.9)
B6 168.8 (?165.3)
B7 229.3 (?157.1)
RL 310.8 (?136.1)
Table 5: Evaluation Results (p < .001 )
7 Conclusion
We presented and evaluated a new model for Nat-
ural Language Generation (NLG) in Spoken Dia-
logue Systems, based on statistical planning. After
motivating and presenting the model, we studied
its use in Information Presentation.
We derived a data-driven model predicting
users? judgements to different information presen-
tation actions (reward function), via a regression
analysis on MATCH data. We used this regression
model to set weights in a reward function for Re-
inforcement Learning, and so optimize a context-
adaptive presentation policy. The learnt policy was
compared to several baselines derived from previ-
ous work in this area, where the learnt policy sig-
nificantly outperforms all the baselines.
There are many possible extensions to this
model, e.g. using the same techniques to jointly
optimise choosing the number of attributes, aggre-
gation, word choice, referring expressions, and so
on, in a hierarchical manner.
689
We are currently collecting data in targeted
Wizard-of-Oz experiments, to derive a fully data-
driven training environment and test the learnt
policy with real users, following (Rieser and
Lemon, 2008b). The trained NLG strategy
will also be integrated in an end-to-end statis-
tical system within the CLASSiC project (www.
classic-project.org).
Acknowledgments
The research leading to these results has received
funding from the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement no. 216594 (CLASSiC project
project: www.classic-project.org) and
from the EPSRC project no. EP/E019501/1.
References
A. Baddeley. 2001. Working memory and language:
an overview. Journal of Communication Disorder,
36(3):189?208.
Vera Demberg and Johanna D. Moore. 2006. Infor-
mation presentation in spoken dialogue systems. In
Proceedings of EACL.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement / supervised
learning of dialogue policies from fixed datasets.
Computational Linguistics (to appear).
Srinivasan Janarthanam and Oliver Lemon. 2008. User
simulations for online adaptation and knowledge-
alignment in Troubleshooting dialogue systems. In
Proc. of SEMdial.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as planning. In Proceedings of ACL.
Oliver Lemon. 2008. Adaptive Natural Language
Generation in Dialogue using Reinforcement Learn-
ing. In Proceedings of SEMdial.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, com-
parative descriptions in spoken dialogue. In Proc.
FLAIRS.
Crystal Nakatsu and Michael White. 2006. Learning
to say it well: Reranking realizations by predicted
synthesis quality. In Proceedings of ACL.
Alice Oh and Alexander Rudnicky. 2002. Stochastic
natural language generation for spoken dialog sys-
tems. Computer, Speech & Language, 16(3/4):387?
407.
Tim Paek and Eric Horvitz. 2000. Conversation as
action under uncertainty. In Proc. of the 16th Con-
ference on Uncertainty in Artificial Intelligence.
Joseph Polifroni and Marilyn Walker. 2008. Inten-
sional Summaries as Cooperative Responses in Di-
alogue Automation and Evaluation. In Proceedings
of ACL.
Verena Rieser and Oliver Lemon. 2008a. Does this
list contain what you were searching for? Learn-
ing adaptive dialogue strategies for Interactive Ques-
tion Answering. J. Natural Language Engineering,
15(1):55?72.
Verena Rieser and Oliver Lemon. 2008b. Learn-
ing Effective Multimodal Dialogue Strategies from
Wizard-of-Oz data: Bootstrapping and Evaluation.
In Proceedings of ACL.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with Reinforce-
ment Learning: Experiments with the NJFun sys-
tem. JAIR, 16:105?133.
Amanda Stent, Marilyn Walker, Steve Whittaker, and
Preetam Maloor. 2002. User-tailored generation for
spoken dialogue: an experiment. In In Proc. of IC-
SLP.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards developing general mod-
els of usability with PARADISE. Natural Language
Engineering, 6(3).
Marilyn Walker, S. Whittaker, A. Stent, P. Maloor,
J. Moore, M. Johnston, and G. Vasireddy. 2004.
User tailored generation in the match multimodal di-
alogue system. Cognitive Science, 28:811?840.
MarilynWalker, Amanda Stent, Franc?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413?456.
Steve Whittaker, Marilyn Walker, and Johanna Moore.
2002. Fish or Fowl: A Wizard of Oz evaluation
of dialogue strategies in the restaurant domain. In
Proc. of the International Conference on Language
Resources and Evaluation (LREC).
Stephen Whittaker, Marilyn Walker, and Preetam Mal-
oor. 2003. Should i tell all? an experiment on
conciseness in spoken dialogue. In Proc. European
Conference on Speech Processing (EUROSPEECH).
690
Andi Winterboer, Jiang Hu, Johanna D. Moore, and
Clifford Nass. 2007. The influence of user tailoring
and cognitive load on user performance in spoken
dialogue systems. In Proc. of the 10th International
Conference of Spoken Language Processing (Inter-
speech/ICSLP).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
691
Hybrid Reinforcement/Supervised Learning
of Dialogue Policies from Fixed Data Sets
James Henderson?
University of Geneva
Oliver Lemon??
University of Edinburgh
Kallirroi Georgila??
University of Edinburgh
We propose a method for learning dialogue management policies from a fixed data set. The method
addresses the challenges posed by Information State Update (ISU)-based dialogue systems, which
represent the state of a dialogue as a large set of features, resulting in a very large state space
and a huge policy space. To address the problem that any fixed data set will only provide
information about small portions of these state and policy spaces, we propose a hybrid model that
combines reinforcement learning with supervised learning. The reinforcement learning is used
to optimize a measure of dialogue reward, while the supervised learning is used to restrict the
learned policy to the portions of these spaces for which we have data. We also use linear function
approximation to address the need to generalize from a fixed amount of data to large state spaces.
To demonstrate the effectiveness of this method on this challenging task, we trained this model
on the COMMUNICATOR corpus, to which we have added annotations for user actions and In-
formation States. When tested with a user simulation trained on a different part of the same data
set, our hybrid model outperforms a pure supervised learning model and a pure reinforcement
learning model. It also outperforms the hand-crafted systems on the COMMUNICATOR data,
according to automatic evaluation measures, improving over the average COMMUNICATOR
system policy by 10%. The proposed method will improve techniques for bootstrapping and
automatic optimization of dialogue management policies from limited initial data sets.
1. Introduction
In the practical development of dialogue systems it is often the case that an initial
corpus of task-oriented dialogues is collected, either using ?Wizard of Oz? methods
or a prototype system deployment. This data is usually used to motivate and inspire
a new hand-built dialogue system or to modify an existing one. However, given the
? Universite? de Gene`ve, De?partement d?Informatique, Battelle-ba?timent A, 7 route de Drize, 1227 Carouge,
Switzerland. E-mail: james.henderson@cui.unige.ch.
?? University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK. E-mail: {olemon, kgeorgil}@
inf.ed.ac.uk.
Submission received: 18 November 2005; revised submission received: 18 October 2006; accepted for
publication: 21 September 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
existence of such data, it should be possible to exploit machine learning methods to
automatically build and optimize a new dialogue system. This objective poses two
questions: what machine learning methods are effective for this problem? and how
can we encode the task in a way which is appropriate for these methods? For the
latter challenge, we exploit the Information State Update (ISU) approach to dialogue
systems (Bohlin et al 1999; Larsson and Traum 2000), which provides the kind of rich
and flexible feature-based representations of context that are used with many recent
machine learning methods, including the linear function approximation method we
use here. For the former challenge, we propose a novel hybrid method that combines
reinforcement learning (RL) with supervised learning (SL).
The focus of this article is to establish effective methods for using fixed corpora
of dialogues to automatically optimize complex dialogue systems. To avoid the need
for extensive hand-crafting, we allow rich representations of context that include all
the features that might be relevant to dialogue management decisions, and we allow
a broad set of dialogue management decisions with very few constraints on when a
decision is applicable. This flexibility simplifies system design, but it leads to a huge
space of possible dialogue management policies, which poses severe difficulties for
existing approaches to machine learning for dialogue systems (see Section 1.1). Our pro-
posed method addresses these difficulties without the use of user simulations, feature
engineering, or further data collections.
We demonstrate the effectiveness of the proposed method on the COMMUNICA-
TOR corpora of flight-booking dialogues. Our method (?hybrid learning? with linear
function approximation) can learn dialogue strategies that are better than those learned
by standard learning methods, and that are better than the (in this case hand-coded)
strategies present in the original corpora, according to a variety of metrics. To evaluate
learned strategies we run them with simulated users that are also trained on (differ-
ent parts of) the COMMUNICATOR corpora, and automatically score the simulated
dialogues based on how many information ?slots? they manage to collect from users
(?filled slots?), whether those slots were confirmed (?confirmed slots?), and how many
dialogue turns were required to do so. Later work has shown these metrics to correlate
strongly with task completion for real users of the different policies (Lemon, Georgila,
and Henderson 2006).
The main contributions of the work are therefore in empirically demonstrating that:
 limited initial data sets can be used to train complex dialogue policies,
using a novel combination of supervised and reinforcement learning; and
 large, feature-based representations of dialogue context can be used
in tractable learning of dialogue policies, using linear function
approximation.
In this article, after a discussion of related work, we outline the annotations we have
added to the COMMUNICATOR data, then present the proposed learning method, and
describe our evaluation method. Finally, we present the evaluation results and discuss
their implications.
1.1 Related Work
As in previous work on learning for dialogue systems, in this article we focus on
learning dialogue management policies. Formally, a dialogue management policy is a
488
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
mapping from a dialogue context (a.k.a. a state) to an action that the system should
take in that context. Because most previous work on dialogue systems has been done in
the context of hand-crafted systems, we use representations of the dialogue context and
the action set based on previous work on hand-crafted dialogue systems. Our main
novel contribution is in the area of learning, where we build on previous work on
automatically learning dialogue management policies, discussed subsequently.
The ISU approach to dialogue (Bohlin et al 1999; Larsson and Traum 2000) employs
rich representations of dialogue context for flexible dialogue management. Information
States are feature structures intended to record all the information about the preceding
portion of the dialogue that is relevant to making dialogue management decisions. An
example of some of the types of information recorded in our Information States is shown
in Figure 1, including filled slots, confirmed slots, and previous speech acts. Previous
work has raised the question of whether dialogue management policies can be learned
(Levin and Pieraccini 1997) for systems that have only a limited view of the dialogue
context, for example, not including prior speech act history (see the following).
One prominent representation of the set of possible system actions is the DATE
scheme (Walker and Passonneau 2001). In particular, this representation is used in the
COMMUNICATOR corpus annotation (Walker, Passonneau, and Boland 2001), discussed
herein. The DATE scheme classifies system actions in terms of their Conversational
Domain, Speech Act, and Task. For example, one possible system action is ?about task,
Figure 1
Example fields from an Information State annotation. User-provided information is in
square brackets.
489
Computational Linguistics Volume 34, Number 4
request info, dest city?, which corresponds to a system utterance such as What is
your destination city? The specific instantiation of this scheme, and our extensions to it,
are discussed in Section 1.2.
Machine-learning approaches to dialogue management attempt to learn optimal
dialogue policies from corpora of simulated or real dialogues, or by generating such
data during automatic trial-and-error exploration of possible policies. Automatic op-
timization is desirable because of the high cost of developing and maintaining hand-
coded dialogue managers, and because there is no guarantee that hand-coded dialogue
management strategies are good. Several research groups have developed reinforce-
ment learning approaches to dialogue management, starting with Levin and Pieraccini
(1997) and Walker, Fromer, and Narayanan (1998). Previous work has been restricted
to limited dialogue context representations and limited sets of actions to choose among
(Walker, Fromer, and Narayanan 1998; Goddeau and Pineau 2000; Levin, Pieraccini, and
Eckert 2000; Roy, Pineau, and Thrun 2000; Scheffler and Young 2002; Singh et al 2002;
Williams and Young 2005; Williams, Poupart, and Young 2005a).
Much of the prior work in RL for dialogue management focuses on the problem
of choosing among a particular limited set of actions (e.g., confirm, don?t confirm)
in specific problematic states (see, e.g., Singh et al 2000a). This approach augments,
rather than replaces, hand-crafted dialogue systems, because the vast majority of deci-
sions, which are not learned, need to be specified by hand. In contrast, we tackle the
problem of learning to choose among any possible dialogue actions for almost every
possible state.
In addition, all prior work has used only a limited representation of the dialogue
context, often consisting only of the states of information slots (e.g., destination city
filled with high confidence) in the application (Goddeau and Pineau 2000; Levin,
Pieraccini, and Eckert 2000; Singh et al 2000a, 2000b, 2002; Young 2000; Scheffler and
Young 2002; Williams, Poupart, and Young 2005a, 2005b; Williams and Young 2005;
Pietquin and Dutoit 2006b), with perhaps some additional low-level information (such
as acoustic features [Pietquin 2004]). Only recently have researchers experimented with
using enriched representations of dialogue context (Gabsdil and Lemon 2004; Lemon
et al 2005; Frampton and Lemon 2006; Rieser and Lemon 2006c), as we do in this
article. From this work it is known that adding context features leads to better dialogue
strategies, compared to, for example, simply using the status of filled or confirmed
information slots as has been studied in all prior work (Frampton and Lemon 2006).
In this article we explore methods for scalable, tractable learning when using all the
available context features.
Reinforcement Learning requires estimating how good different actions will be
in different dialogue contexts. Because most previous work has only differentiated
between a small number of possible dialogue contexts, they have been able to per-
form these estimates for each state independently (e.g., Singh et al 2002; Pietquin
2004). In contrast, we use function approximation to allow generalization to states
that were not in the training data. Function approximation was also applied to RL by
Denecke, Dohsaka, and Nakano (2005), but they still use a relatively small state space
(6 features, 972 possible states). They also only exploit data for the 50 most frequent
states, using what is in effect a Gaussian kernel to compute estimates for the remaining
states from these 50 states. This is a serious limitation to their method, because a large
percentage of the data is likely to be from less frequent states, and thus would be
ignored. In our data set, we found that state frequencies followed a Zipfian (i.e., large-
tailed) distribution, with 61% of the system turns having states that only occurred once
in the data.
490
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Another source of variation between learning approaches is the extent to which they
train on data from simulated users of different kinds, rather than train on data gathered
from real user interactions (as is done in this article). Simulated users are generally
preferred due to the much smaller development effort involved, and the fact that trial-
and-error training with humans is tedious for the users. However, the issues of how
to construct and then evaluate simulated users are open problems. Clearly there is a
dependency between the accuracy of the simulation used for training and the eventual
dialogue policy that is learned (Schatzmann et al 2005). Current research attempts to
develop metrics for user simulation that are predictive of the overall quality of the
final learned dialogue policy (Schatzmann, Georgila, and Young 2005; Schatzmann
et al 2005; Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon
2006; Rieser and Lemon 2006a; Schatzmann et al 2006; Williams 2007). Furthermore,
several approaches use simple probabilistic simulations encoded by hand, using intu-
itions about reasonable user behaviors (e.g., Pietquin 2004; Frampton and Lemon 2005;
Pietquin and Dutoit 2006a), whereas other work (e.g., Scheffler and Young 2001, 2002;
Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon 2006; Rieser
and Lemon 2006a) builds simulated users from dialogue corpora. We use the latter
approach, but only in the evaluation of our learned policies.
No matter which method is chosen for user simulation, a simulated user is still
clearly different from a human user. Therefore, it is important to learn as much as possi-
ble from the data we have from human users. In addition, the huge policy space makes
policy exploration with simulated users intractable, unless we can initialize the system
with a good policy and constrain the policy exploration. This also requires learning as
much as possible from the initial set of data. Therefore, in this article we investigate
using a fixed corpus of dialogues to automatically optimize dialogue systems. No user
simulation is involved in training, thus avoiding the issue of dependency on the quality
and availability of user simulations.
Previous work on RL has made use of policy exploration (Sutton and Barto 1998),
where new data is generated for each policy that is considered during the course
of learning (for example using simulated users). Indeed, this is often considered an
integral part of RL. In contrast, we choose to learn from a fixed data set, without
policy exploration. This is motivated by the fact that real dialogue corpora are very
expensive to produce, and it is often not practical to produce new real dialogues during
the course of learning. Singh et al (2002) manage to perform one iteration of policy
exploration with real data, but most work on RL requires many thousands of iterations.
As discussed previously, this motivates using simulated data for training, but even if
accurate dialogues can be automatically generated with simulated users, training on
simulated dialogues does not replace the need to fully exploit the real data, and does
not solve the sparse data problems that we address here. With a very large state space,
it will never be tractable for policy exploration to test a new policy on even a reasonable
proportion of the states. Thus we will inevitably need to stop policy exploration with
a policy that has not been sufficiently tested. In this sense, we will be in a very similar
situation to learning from a fixed data set, where we don?t have the option of generating
new data for new states. For this reason, the solution we propose for learning from fixed
data sets is also useful for learning with policy exploration.
There have been some proposals in RL for learning a policy that is different from
that used to generate the data (called ?off-policy? learning), but these methods have
been found not to work well with linear function approximation (Sutton and Barto
1998). They also do not solve the problem of straying from the region of state space
that has been observed in the data, discussed subsequently.
491
Computational Linguistics Volume 34, Number 4
1.2 The COMMUNICATOR Domain and Data Annotation
To empirically evaluate our proposed learning method, we apply it to the COMMU-
NICATOR domain using the COMMUNICATOR corpora. The COMMUNICATOR corpora
(2000 [Walker et al 2001] and 2001 [Walker et al 2002b]) consist of human?machine
dialogues (approximately 2,300 dialogues in total). The users always try to book a flight,
but they may also try to select a hotel or car rental. The dialogues are primarily ?slot-
filling? dialogues, with some information being presented to the user after the system
thinks it has filled (or confirmed) the relevant slots. These corpora have been previously
annotated using the DATE scheme, for the Conversational Domain, Speech Act, and
Task of each system utterance (Walker and Passonneau 2001; Walker, Passonneau, and
Boland 2001). In addition, the results of user questionnaires are available, but only for
the 2001 corpus.
Table 1 shows some statistics for the two collections. In the 2000 collection each turn
contains only one utterance but in the 2001 corpus a turn may contain more than one
utterance. More details about the COMMUNICATOR corpora can be found in Walker,
Passonneau, and Boland (2001) and Walker et al (2001, 2002a).
We used a hand-crafted automatic system (Georgila, Lemon, and Henderson 2005;
Georgila et al, submitted) to assign Speech Acts and Tasks to the user utterances, and to
compute state representations for each point in the dialogue (i.e., after every utterance).
Although we annotated the whole 2000 and 2001 corpora, because we need the results
of user questionnaires (as discussed subsequently), we only make use of the 2001 data
for the experiments reported here. The 2001 data has eight systems, 1,683 dialogues, and
125,388 total states, two thirds of which result from system actions and one third from
user actions. The annotation system is implemented using DIPPER (Bos et al 2003) and
OAA (Cheyer and Martin 2001), using several OAA agents (see Georgila, Lemon, and
Henderson, 2005, and Georgila et al, submitted, for more details). Following the ISU
approach, we represented states using Information States, which are feature structures
intended to record all the information about the preceding portion of the dialogue that
is relevant to making dialogue management decisions. An example of some of the types
of information recorded in an Information State is shown in Figure 1, including filled
slots, confirmed slots, and previous speech acts.
Given this corpus, we need to learn a dialogue management policy that maps these
state representations to effective system actions. As the example in Figure 1 illustrates,
there are a large number of features in dialogue states that are potentially relevant to
Table 1
Statistics for the 2000 and 2001 COMMUNICATOR data.
Year
2000 2001 Total
Number of dialogues 648 1683 2331
Number of turns 24,728 78,718 103,446
Number of system turns 13,013 39,419 52,432
Number of user turns 11,715 39,299 51,014
Number of utterances 24,728 89,666 114,394
Number of system utterances 13,013 50,159 63,172
Number of user utterances 11,715 39,507 51,222
Number of system dialogue acts 22,752 85,881 108,633
492
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
dialogue management, and thus should not be excluded from the state representations
we use in learning. This leads to a very large space of possible states (over 10386 states are
theoretically possible in our model), with a very high chance that a state encountered
in testing will not be exactly the same as any state encountered in training. This fact
motivates, if not requires, the use of approximation methods.
The complexity of the COMMUNICATOR domain is alsomanifested in the large num-
ber of system actions that the dialogue management policy needs to choose between.
The DATE scheme representation of system actions implies that each possible triple of
values for the Conversational Domain, Speech Act, and Task is a different action. In
addition, we have added release turn and end dialogue actions. There are a total of
74 system actions that occur in the annotated COMMUNICATOR data.
2. Reinforcement Learning with a Fixed Data Set
We use the annotated COMMUNICATOR data to train a Reinforcement Learning system.
In RL, the objective of the system is to maximize the reward it gets during entire dia-
logues. Rewards are defined to reflect how successful a dialogue was, so by maximizing
the total reward the system optimizes the quality of dialogues. The difficulty is that, at
any point in the dialogue, the system cannot be sure what will happen in the remainder
of the dialogue, and thus cannot be sure what effect its actions will have on the total
reward at the end of the dialogue. Thus the system must choose an action based on the
average reward it has observed previously after it has performed that action in states
similar to the current one. This average is the expected future reward.
The core component of any RL system is the estimation of the expected future
reward (called the Q-function). Given a state and an action that could be taken in that
state, the Q-function tells us what total reward, on average, we can expect between
taking that action and the end of the dialogue.1 Once we have this function, the optimal
dialogue management policy reduces to simply choosing the action that maximizes the
expected future reward for the current state.
Our proposal for RL with fixed data sets uses two main techniques. The first is the
use of function approximation to estimate the expected future reward. We claim that
linear function approximation is an effective way to generalize from a limited data set
to a large space of state?action pairs. The second technique is a novel hybrid learning
method that combines RL with supervised learning (SL). SL is used to characterize how
much data we have for each area of the state?action space (also using linear function
approximation). Our hybrid policy uses SL to avoid state?action pairs for which we do
not have enough data, while using RL to maximize reward within the parts of the space
where we do have enough data. We claim that this is an effective solution to the problem
of learning complex tasks from fixed data sets.
2.1 Defining Dialogue Reward
To apply RL to the COMMUNICATOR data, we first have to define a mapping r(d, i) from
a dialogue d and a position in that dialogue i to a reward value. This reward function is
computed using the reward level of annotation in the COMMUNICATOR data, which was
1 The expected future reward also depends on the dialogue management policy that the system will use in
the future. This self-referential nature of RL is the topic of much RL research, and will be discussed more
in the following.
493
Computational Linguistics Volume 34, Number 4
extracted from user questionnaires and task completion measures. For all states other
than the final state, we provide a reward of ?1 if the state follows a system action, and
0 otherwise. This encodes the idea that, all other things being equal, short dialogues
are better than long ones. For the final state we provide a reward that is the sum of
the rewards for each feature in the reward annotation. ?Actual Task Completion? and
?Perceived Task Completion? are both worth a reward of 100 if they are non-zero (i.e.,
true), and 0 otherwise. The remaining reward features have values ranging from 1 to 5
in the annotation (where 5 is the best), which we rescale to the range 0 to 1 (1 converts
to 0, 5 converts to 1). Their reward is their rescaled value times the weight shown in
Table 2. The relative values of these later weights were determined by the empirical
analysis reported in Walker et al (2001) within the PARADISE evaluation framework
(Walker, Kamm, and Litman 2000).
2.2 Estimating the Expected Future Reward
Given this definition of reward, we want to find an estimate Q(si, a) of the expected
future reward, which is the expected value (?E[ ]? in Equation 1) of the total reward
between taking action a in state si and the end of the dialogue. This expectation is a sum
over all possible future dialogues d, weighted by the probability of the dialogue given
that we have performed action a in state si.
Q(si, a) ? Ed|si,a[
?
j>i
r(d, j)] =
?
d
(P(d|si, a)
?
j>i
r(d, j)) (1)
Given that the number of possible future dialogues d = ?si+1, . . . , snd? is exponential in
the length of the sequences, it is not surprising that estimating the expected reward over
these sequences can be very difficult.
The ISU framework is significantly different from the frameworks used in previous
work on reinforcement learning for dialogue management, in that the rich context
representation makes the number of possible states extremely large. Having a large
number of states is a more realistic scenario for practical, flexible, and generic dialogue
systems, but it also makes many RL approaches intractable. In particular, with a large
number of states it is not possible to learn estimates of the expected future reward for
every state, unless we can exploit commonalities between different states. The feature-
based nature of ISU state representations expresses exactly these commonalities be-
tween states through the features that the states share. There are a number of techniques
that could be used for RL with feature-based representations of states, but the simplest
and most efficient is linear function approximation.
Table 2
The weights used to compute a dialogue?s final reward value, multiplied by values between 0
and 1 computed from user responses.
Actual task completion 100
Perceived task completion 100
Task ease 36
Comprehension ease 28
System behaved as expected 32
Future use 36
494
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
We use linear function approximation to map from a vector of real valued features
f (s) for the state s to a vector of estimates Q(s, a), one estimate for each a. The trained
parameters of the linear function are a vector of weights wa for each action a. Given
weights trained on a given data set, an estimate Qdata(s, a) of the expected future reward
given a state s and an action a is the inner product of the state vector f (s) and the weight
vector wa.
2
Qdata(s, a) = f (s)
Twa =
?
i
fi(s)wai (2)
This approximation method has the effect of treating two states as similar if they
share features. During learning, updating the estimate Qdata(s, a) for one observed state
s will also update the estimate Qdata(s
?, a) for all other states s? to the extent that s?
shares features with s. This updating happens via the weights wa; if s has feature i then
updating the estimate Qdata(s, a) will change wai, which will in turn change Qdata(s
?, a) for
any s? that also has feature i. Thus each feature represents a dimension with respect to
which two states can be similar or different. This similarity measure is known as a linear
kernel.
This is the first time that linear function approximation has been used for learning
dialogue strategies. Denecke, Dohsaka, and Nakano (2005) also use function approxi-
mation, but there the notion of similarity used during learning is Euclidean distance,
rather than shared features. In effect, Denecke, Dohsaka, and Nakano use a Gaussian
kernel, whereas we use a linear kernel.
To train the weights of the linear approximation Qdata(s, a), we employed a standard
RL learning method called SARSA(?) (Sutton and Barto 1998). This method learns
based on two criteria, with a parameter ? used to weight their relative influence. The
first criterion comes from temporal-difference learning: the current estimate for the
Q-function should (on average) equal the reward from the next state plus the estimate
for the expected future reward at the next state. The second criterion comes directly
from the observed reward: The current estimate for the Q-function should (on average)
equal the reward observed for the remainder of the dialogue. The combination of these
two criteria makes learning faster than using either one alone. Gradient descent learning
is applied to the weights; at each step of learning, the weights are updated so as to make
the Q-function better fit this combined criterion.
Whereas the weights wa are learned from data, the mapping f (s) from states to vec-
tors must be specified beforehand. Because each value fi(s) in these vectors represents a
possible commonality between states, it is through the definition of f (s) that we control
the notion of similarity that will be used by the linear function approximation. The
definition of f (s) we are currently using is a straightforward mapping from attribute?
value pairs in the Information State s to values in the vector f (s).
The state vector mapping f (s) was computed using the first four levels of our state
annotations for the COMMUNICATOR data (i.e., the Dialogue, Task, Low, and History
levels shown in Figure 1). The values of the attributes in these annotations were con-
verted to features of three types. For attributes that take numbers as values, we used
a simple function to map these numbers to a real number between 0 and 1, with the
absence of any value being mapped to 0 (resulting in six features, e.g., StateNumber).
2 We will use the notation xTy to denote the inner product between vectors x and y (i.e., ?x transpose
times y?). wai is the ith element of the vector wa.
495
Computational Linguistics Volume 34, Number 4
For attributes that can have arbitrary text as their values, we used 1 to represent the
presence of text and 0 to represent no value (resulting in two features, e.g., AsrInput).
The remaining attributes all have either a finite set of possible values, or a list of such
values.
The vast majority of our features are constructed from this third set of attributes.
First, to reflect the importance of speech act?task pairs (which we use to define both
system and user actions), we construct a new SpeechAct-Task attribute whose value
is the concatenation of the values for the SpeechAct and Task attributes. The same is
done for the SpeechActsHist and TasksHist attributes. Second, attributes with a list
value (i.e., the . . .Hist and . . .Status attributes, plus user actions3) are converted to a
set of attribute?value pairs consisting of the attribute and each value in the list (result-
ing in 509 features, e.g., FilledSlotsStatus:[orig city]). Note that this conversion
loses the ordering between the values in the list. In the case of SpeechAct, Task, and
SpeechAct-Task attributes that have list values (which result from turns in which a
user performs more than one action), we also include the whole list as a value for
the attribute4 (resulting in 364 features, e.g., SpeechAct:[no answer,provide info]).
Finally, attributes with single values are assigned features (which result in 401 features,
e.g., Speaker:user).
From this set of potential features, we only use those that occur in the data at least
five times.5 (Only these features are included in the feature counts given previously.)
Each feature is assigned an element of the vector f (s) that is 1 if that feature is present in
the state and 0 if it is not. In total there are 1,282 features.
One advantage of using linear function approximation is that the learning method
can be kept fairly simple, while still incorporating domain knowledge in the design
of the mapping to feature vectors. One area of future research is to investigate more
complicated mappings to feature vectors f (s). This would involve making use of kernel-
based methods. Kernels are used to compensate for the oversimplicity of linear func-
tions, and can be used to express more complicated notions of commonality between
states (Shawe-Taylor and Cristianini 2004).
2.3 Pure RL and SL Policies
Given the estimate of the expected future reward Qdata(s, a) discussed in the previous
section, one obvious approach would use this estimate to define the dialogue policy.
This ?pure RL? policy simply selects the action a with the highest Qdata(s, a) given the
state s. As demonstrated by the evaluation in Section 3, this policy performs very badly.
Inspection of the actions chosen by the pure RL policy indicates that this policy is
very different from the policy observed in the COMMUNICATOR data; the pure RL policy
almost never chose the same action as was in the data. This means that the actions that
have been learned to have the best future reward for a state are not the ones that were
3 Because in the 2001 COMMUNICATOR data users may perform more than one action in a single turn, a
user?s action is potentially a list of speech act?task pairs. These are annotated as lists of speech acts plus
lists of tasks, to which we add lists of speech act?task pairs. Histories of these lists (i.e., lists of lists) are
first flattened and then treated like other lists.
4 These ?list? values are more accurately described as set values, because we do not encode the ordering
of the values in the list.
5 We also do not include the . . .Value. . . attributes, such as FilledSlotValue, which specify the actual
fillers for slots.
496
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
typically chosen by the COMMUNICATOR systems in that state. This difference results in
two problems:
 such atypical actions then lead to states unlike anything observed in the
data,
 the policy that the system will use for future actions is different from that
observed in the data, and
The first problem makes the Qdata(s, a) estimates for the visited states highly unreliable,
because we don?t have data for these states. Because the future reward depends on the
policy that the systemwill use in the future, the second problemmeans that the estimate
Qdata(s, a) is not even relevant to the expected future reward of the pure RL policy.Wewill
return to these problems when we develop our proposed method in Section 2.4.
These problems are a result of the fact that we are training on a fixed data set,
and therefore cannot generate new data that is appropriate for the new policy. The
solution to these problems that is typically used in RL research is to generate new
data as learning progresses and the policy changes, as discussed in Section 1.1. The RL
system can thus explore the space of possible policies and states, generating new data
that is relevant to each explored policy and its states. The problem with learning with
policy exploration, even when using simulated users, is that it is not tractable with a
large state space and action set. Consider that with 10386 states and 74 actions, there are
7410
386
possible policies. If we were able to explore policies at a rate of 1 policy a second,
after 1 year we would have visited only one policy in every 7410
385.6
policies. Policy
exploration algorithms are only partially random, so to some extent they can make
accurate choices about which parts of the policy space to explore and which to ignore,
but these numbers are indicative of the scale of the problem faced by policy exploration.
In addition, experiments with a random policy achieved an average score of ?66,
showing that the vast majority of policies are very bad. This indicates that starting
policy exploration with a random policy would require an extremely large amount of
exploration to move from there to a policy which is as good as the policy found with
the proposal discussed herein (which achieved a score of 140, out of a maximum 197).
Therefore it is crucial that exploratory learning at least be initialized with a policy that
we already know to be good. The method proposed in this article for learning a policy
from a pre-existing corpus of dialogues can be used to find such an initial policy.
Given these problems with using RL with a fixed data set, an obvious alternative
would be to simply train a policy to mimic the policies of the systems used to generate
the data. One reason for training a policy, rather than using one of the original policies,
is that learning allows us to merge the policies from all the different systems, which
can lead to a better policy than any one system (as we will show in Section 3). Another
reason is that learning results in a policy that generalizes from the original policies in
interesting ways. Most notably, our learning method can be used to define a probabilis-
tic policy, not just the (presumably) deterministic policies used to generate the data. A
third reason could be (as in our case) that we do not have access to any of the original
systems that generated the data. In some sense we can use learning to reverse engineer
the systems.
We train a policy to mimic the policy observed in the data using supervised learning
with linear function approximation. This ?pure SL? policy simply selects the action
a with the highest probability P(a|s) of being chosen given the state s. We estimate
P(a|s) with linear function approximation, just as for Qdata(s, a), except that a normalized
497
Computational Linguistics Volume 34, Number 4
exponential function (a.k.a. ?softmax?) is used so that the result is a probability distri-
bution over actions a.
P(a|s) ? Sdata(s, a) =
exp( f (s)Tw?a)
?
a? exp( f (s)
Tw?
a?
)
(3)
This gives us a log-linear model, also known as a maximum entropy model. The
parameters of this model (thew?a) are trained using supervised learning on the COMMU-
NICATOR data. As with the Q-function, the use of linear function approximation means
that we have estimates for P(a|s) even for states s that have never occurred in the data,
based on similar states that did occur.
2.4 A Hybrid Approach to RL
In this work we focus on solving the first of the two problems we have discussed,
namely, preventing the system from straying into portions of the state space for which
we do not have sufficient data. To do this, we propose a novel hybrid approach that
combines RL with supervised learning. SL is used to model which actions will take the
system into a portion of the state space for which we don?t have sufficient data. RL
is used to choose between the remaining actions. A discriminant function Qhybrid(s, a) is
derived that combines these two criteria in a principled way. The resulting policy can be
adjusted to be as similar as necessary to the policy in the data, thereby also addressing
the second problem discussed previously.
As with the pure SL policy, supervised learning is used to model the policy that
the systems in the data actually use. Because in general multiple policies were used,
we model the data?s policy as a probabilistic policy, using the estimate Sdata(s, a) of P(a|s)
presented in the previous section. Sdata(s, a) is an estimate of the probability that a random
system selected from those that generated the data would choose action a given that it is
in state s. Because we are using function approximation to learn Sdata(s, a) from the data,
it will generalize (or ?smooth?) the policies actually used to generate the data so that
similar states will allow similar sets of actions.6
The hybrid approach we have investigated is based on the assumption that the Q-
function trained on the data is a poor model of the expected future reward for states
in the portion of the state space not covered by the data. Thus we need an alternative
method for estimating the future reward for these unobserved states. We have exper-
imented with two such methods. The first method simply specifies a fixed reward U
for these states. By setting this fixed reward to a low value, it amounts to a penalty for
straying from the observed portion of the state space.
The second method estimated the reward for unobserved states by adding a fixed
reward offset UO to the reward estimates for ending the dialogue immediately. This
method compensates for the use of a dialogue-final reward scheme, where many things
that the dialogue has already accomplished aren?t reflected in the reward given so far.
For example, in our reward scheme, filling a slot does not result in immediate reward,
but instead results in reward at the end of the dialogue if it leads to a successful
dialogue. The estimated reward for ending the dialogue immediately reflects howmuch
6 For this reason, we will get a probabilistic policy even if only a single deterministic policy is used
to generate the data. This makes this method applicable even for data sets generated with a single
deterministic prototype system.
498
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
reward is stored up in the state in this way. If the fixed reward added to this estimate is
set to negative, then we can be sure that the reward estimated for unobserved states is
always less than that for the best observed state, so this method also results in a penalty
for straying from the observed portion of the state space.
Given an estimated reward u for unobserved states, the expected future reward is
then the average between u for the cases where performing a in s leads to an unobserved
state and the expected reward Qdata(s, a) for the cases where it leads to an observed state.
Formally, this average is a mixture of the estimate u with the estimate Qdata(s, a), where
the mixture coefficient is the probability Pobserved(s, a) that performing a in swill lead to an
observed state.
Ed|si,a[
?
j>i r(d, j)]
? Qdata(s, a)Pobserved(s, a)+ u(1? Pobserved(s, a))
(4)
Because this estimate of the expected future reward is only needed for choosing
the next action given the current state s, we only need to estimate a function that dis-
criminates between different actions in the same way as this estimate. To derive such a
discriminant function, we first approximate Pobserved(s, a) with a first-order approximation
in terms of the probability distribution in the data P(s, a) and the size of the data set N,
under the assumption that the number of possible state?action pairs is much larger than
the size of the data set (so P(s, a)N 	 1).
Pobserved(s, a) = 1? (1? P(s, a))N
? P(s, a)N ? Sdata(s, a)P(s)N
(5)
Given this approximation, the discriminant function needs to order two actions a1, a2 in
the same way as this estimate of the expected future reward.
Qdata(s, a1)Sdata(s, a1)P(s)N + u(1? Sdata(s, a1)P(s)N)
? Qdata(s, a2)Sdata(s, a2)P(s)N + u(1? Sdata(s, a2)P(s)N)
if and only if
Sdata(s, a1)(Qdata(s, a1)? u) ? Sdata(s, a2)(Qdata(s, a2)? u)
(6)
We call this discriminant function Qhybrid(s, a).
Qhybrid(s, a) = Sdata(s, a)(Qdata(s, a)? u) (7)
We use thisQhybrid(s, a) function to choose the actions for our hybrid policy. By adjust-
ing the value of the unobserved state penalty u, we can adjust the extent to which this
model follows the supervised policy defined by Sdata(s, a) or the reinforcement learning
policy defined by Qdata(s, a). In particular, if u is very low, then maximizing Qhybrid(s, a) is
equivalent to maximizing Sdata(s, a). Thus a very low u is equivalent to the policy that
always chooses the most probable action, which we will call the ?SL policy.?
The procedure for trainingQhybrid(s, a) is simply to trainQdata(s, a) with RL and Sdata(s, a)
with SL. These two models are then combined using Equation (7), given a value for u
computed with one of the two methods presented previously. Both of these methods
involve setting a constant that determines the relative importance of RL versus SL. In
the next section we will empirically investigate good values for these constants.
499
Computational Linguistics Volume 34, Number 4
3. Empirical Evaluation
We evaluate the trained dialogue management policies by running them against trained
user simulations. The policies and the user simulations were trained using different
parts of the annotated COMMUNICATOR data (using two-fold and five-fold cross val-
idation). We compare our results against each other and against the performance of
the eight COMMUNICATOR systems, using an evaluation metric discussed subsequently.
The Information States for the simulated dialogues were computed with the same rules
used to compute the Information States for the annotated data.
3.1 The Testing Setup
For these experiments, we restrict our attention to users who only want single-leg
and return flight bookings. This allows us to do the evaluation using only the four
essential slots included in both these types of bookings: origin city, destination city,
departure date, and departure time. To achieve this restriction, we first selected all those
COMMUNICATOR dialogues that consisted only of single-leg or return flight bookings.
This subset contained 217 ATT dialogues, 116 BBN dialogues, 126 CMU dialogues, 159
Colorado dialogues, 77 IBM dialogues, 192 Lucent dialogues, 180 MIT dialogues, and
185 SRI dialogues, for a total of 1,252 dialogues (out of 1,683). This subset was used for
evaluating the COMMUNICATOR systems and for training the user models. The system
models were trained on the full set of dialogues, because they should not know the
user?s goals in advance. So, for each fold of the data, the user model was trained on only
the single-leg and return dialogues from that fold and the system model was trained on
the full set of dialogues from a subset of the remaining folds (one fold for the two-fold
experiments and three folds for the five-fold experiment, as discussed subsequently).
The user models were trained in the same way as the Sdata(s, a) function for the
pure SL model discussed in Section 2.3, using linear function approximation and a
normalized exponential output function. The states that precede user actions are input
as vectors of features virtually identical to those used for the system. However, unlike
the action set for the system, the user only chooses one action per turn, and that action
can include multiple ?Speech Act, Task? pairs. The output of the model is a probability
distribution over these actions. The user simulation selects an action randomly accord-
ing to this distribution. We also trained a user model based on n-grams of user and
system actions, which produced similar results in our testing (Georgila, Henderson,
and Lemon 2006).
In our initial experiments with the hybrid policy, we found that it never closed
the dialogue. We think that this was due to the system action (annotated in DATE)
meta greeting goodbye, which is used both as the first action and as the last action
of a dialogue. The hybrid policy expects this action to be chosen before it will close
the dialogue, but the system never chooses this action at the end of a dialogue because
it is so strongly associated with the beginning of the dialogue. This is an example of
the limitations of linear function approximation, and our dependence on the previous
COMMUNICATOR annotations. We could address this problem by splitting this action
into two actions, one for ?greeting? and one for ?goodbye.? But because we do not want
to embark on the task of feature engineering at this stage, we have instead augmented
the hybrid policy with a rule that closes the dialogue after the system chooses the action
offer, to offer the user a flight. After this first flight offer, the user has one turn to
reply, and then the dialogue is ended. For practical reasons we have also added rules
500
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
that close the dialogue after 100 states (i.e., total of user and system actions), and that
release the turn if the system has done 10 actions in a row without releasing the turn.
3.2 The Evaluation Metrics
To evaluate the success of a dialogue, we take the final state of the dialogue and use
it to compute a scoring function. We want the scoring function to be similar to the
reward we compute from the quality measures provided with the COMMUNICATOR
data (e.g., the user questionnaires), but because we do not have these quality measures
for the simulated dialogues, we cannot use the exact same reward function. When we
compare the hybrid policy against the COMMUNICATOR systems, we apply the same
scoring function to both types of dialogues so that we have a comparable evaluation
metric for both.
Because currently we are only considering users who only want single-leg or return
flight bookings, the scoring function only looks at the four essential slots for these
bookings: origin city, destination city, departure date, and departure time. We give
25 points for each slot that is filled, plus another 25 points for each slot that is also
confirmed. We also deduct 1 point for each action performed by the system, to penalize
longer dialogues. Thus the maximum possible score is 197 (i.e., 200 minus 3 system
actions: ask for all the user information in one action, then confirm all the four slots in
one action and offer a flight).
The motivation behind this evaluation metric is that confirmed slots are more likely
to be correct than slots that are just filled. If we view the score as proportional to the
probability that a slot is filled correctly, then this scoring assumes that confirmed slots
are twice as likely to be correct. Although other scoring metrics are clearly possible, this
one is a simple and reasonable approximation of the relative expected correctness of
confirmed versus non-confirmed information in dialogue systems. On the other hand,
none of our conclusions depend on this exact scoring function, as indicated by results for
the ?no-conf? version of our scoring function (discussed subsequently), which ignores
confirmations.
When combining the scores for different slots, we do not try to model the all-or-
nothing nature of the COMMUNICATOR task-completion quality measures, but instead
sum the scores for the individual slots. This summakes our scoring metric value partial
completions more highly, but inspection of the distributions of scores indicates that
this difference does not favor either the hybrid policy or the original COMMUNICATOR
systems.
Although this evaluation metric could reflect the relative quality of individual
dialogues more accurately, we believe it provides a good measure of the relative quality
of the systems we wish to compare. First, the exact same metric is applied to every
system. Additional information that we have for some systems, but not all, is not used
(e.g., the COMMUNICATOR user questionnaires, which we do not have for simulated
dialogues). Second, the systems are being run against approximately equivalent users.
The user simulation is trained on exactly the same user actions that are used to evaluate
the COMMUNICATOR systems, so the user simulations mimic exactly these users. In
particular, the simulation is able to mimic the effects of speech recognition errors,
because it is just as likely as the real users to disagree with a confirmation or provide a
new value for a previously filled slot. The nature of the simulation model may make it
systematically different from real users in some way, but we know of no argument for
why this would bias our results in favor of one system or another.
501
Computational Linguistics Volume 34, Number 4
One concern about this evaluation metric is that it does not reflect the quality of the
speech recognizer being used by the system. If a system has a good speech recognizer,
then it may not be necessary for it to confirm a slot value, but our scoring function will
still penalize it for not confirming. This would certainly be a problem if this metric were
to be used to compare different systems within the COMMUNICATOR data set. However,
the intention of the metric is simply to facilitate comparisons between different versions
of our proposed system, and between our proposed systems and those in the data.
Because the user simulations are trained on the COMMUNICATOR data, they simulate
speech recognition errors at the same rate as the data, thereby controlling for the quality
of the speech recognizer.
Nonetheless, it is worth considering another evaluation metric that does not penal-
ize for missing confirmations. For this reason we also evaluate the different systems
based on their scores for only filled slots and length, which we call the ?no-conf? score.
3.3 The Influence of Reinforcement Learning
In our first set of experiments, we evaluated the success of our hybrid policy relative
to the performance of the pure reinforcement learning policy and the pure supervised
learning policy. We also investigated how to best set the parameters for combining the
supervised and reinforcement learning policies in a hybrid policy.
We first compared the two proposed hybrid methods using two-fold cross valida-
tion. We trained models of both Qdata(s, a) and Sdata(s, a), and then used them to define
policies. We trained both models for 100 iterations through the training portion of the
data, at which point there was little change in the training error. We trained Qdata(s, a)
using SARSA(?) with ? = 0.9. This training was repeated twice, once for each fold of the
complete data set. The reinforcement learning policy uses only Qdata(s, a), the SL policy
uses only Sdata(s, a), and the hybrid policies combine the two using Equation (7). For the
hybrid policies, we used the two methods for estimating the unobserved state penalty
u and various values for the fixed reward U or reward offset UO.
During testing, each policy was run for 2,000 dialogues against a linear function
approximation user model trained on the opposite half of the data. The final state for
each one of these dialogues was then fed through the scoring function and averaged
across dialogues and across data halves. The results are plotted in Figure 2. To allow
direct comparisons between the different values of U and UO, these scores are plotted
against the proportion of decisions that are different from that which the pure SL policy
would choose. Thus the SL policy (average reward 139.8) is plotted at 0 (which is
Figure 2
Average dialogue score plotted against the proportion of decisions that diverge from the SL
policy, for different values of the unobservable state reward U and reward offset UO. Averages
over two folds, 2,000 dialogues per fold.
502
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Figure 3
Average dialogue score plotted against the proportion of decisions that diverge from the SL
policy, for different values of the unobservable state reward offset UO. Averages over five folds,
2,000 dialogues per fold.
equivalent to a large negative U or UO). Additional points for the hybrid policies are
shown for (from left to right, respectively) U = 0, 40, 80, and 100, and UO = ?300, ?100,
?60, ?40, ?20, ?10, and 0. The pure reinforcement learning policy is not shown because
its average score falls well below the bottom of the graph, at 44.4.
Figure 2 indicates that, for both hybrid methods, adding some influence from RL
increases performance over pure SL, but too much RL results in degradation. Using a
reward offsetUO for u generally does better than a fixed rewardU, and allows a greater
influence from RL before degradation.
We found that the results for our two folds were very different,7 so we repeated
the experiments using five-fold cross validation, where the dialogues from each system
were split randomly (rather than chronologically).8 For each fold, we trained models
of both Qdata(s, a) and Sdata(s, a) on three of the folds, using a fourth fold to decide when
to stop training. The fifth fold was then used to train a linear function approximation
user model, which was used to generate 2,000 simulated dialogues. Combining the five
folds, this gave us 10,000 dialogues per model. Because, in the previous experiments,
using a reward offset UO performed better than using a fixed reward U, we only tested
models using different values of the reward offset UO.
The validation performance of the trained models for Qdata(s, a) and Sdata(s, a) per-
formed similarly across the different splits. Taken together, the models of Sdata(s, a) had a
perplexity of 4.4. Intuitively, this means that the supervised models were able to narrow
down the list of possible actions from 74 to about 4 choices, on average. This suggests
that the ISU representation of state is doing a good job of representing the information
being used by the systems to make dialogue management decisions, but that there is
still a good amount of uncaptured variability. Presumably most of this variability is due
to differences between the policies for the different systems. The models of Qdata(s, a)
had a mean squared error of 8,242, whose square root is 91. This measure is harder to
interpret because it is dominated by large errors, but suggests that the expected future
reward is rather hard to predict, as is to be expected.
Figure 3 shows the average scores for the pure SL policy (at 0) and for hybrid
policies (from left to right) with UO = ?300, ?100, ?60, ?40, ?20, ?10, ?5 and 0. The
7 For the two-fold experiments, the data were split by putting the first half of the dialogues for each system
in one fold and the second half in the other, under the constraint that no user had dialogues in more than
one fold. It appears that the users that were run at the beginning of the 2001 COMMUNICATOR data
collection were very different from those run at the end.
8 To be more precise, for each system we split the set of users randomly into five groups. Then all the
dialogues for a given group of users were put in the same fold.
503
Computational Linguistics Volume 34, Number 4
hybrid policies perform consistently better than the SL policy. The difference between
the hybrid policy and the SL policy is statistically significant at the 5% level for the
three best hybrid policies tested (p < 0.01 for UO = ?40, p < 0.001 for UO = ?10, and
p < 0.007 for UO = ?5). If we combine all the tested hybrid policies together, then their
average score (139.4) is also significantly better than the SL policy (p < 0.014). All these
results are significantly better than the average score of the pure RL policy (34.9).
3.4 Comparisons with COMMUNICATOR Systems
In our second set of experiments, we evaluated the success of our learned policies
relative to the performance of the COMMUNICATOR systems that they were trained on.
To evaluate the performance of the COMMUNICATOR systems, we extracted final states
from all the dialogues that only contain single-leg or return flight bookings and fed
them through the scoring function. The average scores are shown in Tables 3 and 4,
along with the average scores for the pure SL policy, the pure RL policy, and the best
hybrid policy (UO = ?10). The total score, the score excluding confirmations, and the
three components of the total score are shown.
Table 3 shows the results computed from the complete dialogues. These results
show a clear advantage for the hybrid policy over the average across the COMMUNI-
CATOR systems, as well as over each individual COMMUNICATOR system. In particular,
the hybrid policy uses fewer steps. Because the number of steps is doubtless affected
by the hybrid policy?s built-in strategy of stopping the dialogue after the first flight
offer, we also evaluated the performance of the COMMUNICATOR systems if we also
stopped these dialogues after the first flight offer, shown in Table 4. The COMMUNI-
CATOR systems do better when stopped at the first flight offer, but still their average
(?all COMMUNICATOR?) is not nearly as good as the hybrid or SL policies, under all
measures.
Although the average score of the COMMUNICATOR systems in Table 4 is well below
those of the hybrid and SL policies, under this measure the single best system (BBN)
beats our proposed system. Also, if we ignore confirmations (the ?no-conf? measure),
Table 3
The average scores from the different systems for single-leg and return dialogues, the score
excluding confirmations, and the three components of these scores.
System Total score No-conf Filled Confirmed Length
score slots slots penalty
hybrid RL/SL 140.3 70.3 88.0 70.0 ?17.7
pure SL 138.3 69.2 89.2 69.1 ?20.0
pure RL 34.9 25.6 56.9 8.3 ?31.3
all COMMUNICATOR 103.6 40.6 85.0 63.0 ?44.4
SRI 115.3 50.5 83.4 64.9 ?32.9
MIT 114.3 43.2 87.1 71.1 ?43.9
LUC 110.3 36.1 91.1 74.1 ?55.0
COL 105.9 47.0 90.6 59.0 ?43.6
BBN 102.4 27.1 82.5 75.2 ?55.4
ATT 94.0 38.7 78.3 55.3 ?39.6
CMU 92.1 24.0 81.7 68.1 ?57.7
IBM 77.0 61.8 85.4 15.3 ?23.6
504
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Table 4
The average scores after the first flight offer for single-leg and return dialogues, the score
excluding confirmations, and the three components of these scores.
System Total score No-conf Filled Confirmed Length
score slots slots penalty
hybrid RL/SL 140.3 70.3 88.0 70.0 ?17.7
pure SL 138.3 69.2 89.2 69.1 ?20.0
pure RL 34.9 25.6 56.9 8.3 ?31.3
all COMMUNICATOR 127.1 63.2 84.5 63.9 ?21.3
BBN 148.9 73.2 88.6 75.6 ?15.4
LUC 138.5 59.1 91.1 79.4 ?32.1
MIT 136.4 66.9 82.8 69.4 ?15.9
COL 132.9 71.4 89.9 61.5 ?18.6
SRI 128.2 61.7 84.2 66.5 ?22.5
CMU 123.8 58.7 77.2 65.1 ?18.5
ATT 109.1 53.6 78.3 55.4 ?24.7
IBM 86.4 71.2 85.1 15.3 ?13.9
then three of the individual systems beat our proposed system by small amounts. How-
ever, as discussed in Section 3.2, our evaluation methodology is not really appropriate
for comparing against individual COMMUNICATOR systems, due to likely differences in
speech recognition performance across systems. To test this explanation, we looked at
the word error rates for the speech recognition outputs for the different systems. BBN
has the highest percentage of user utterances with no speech recognition errors (79%,
versus an average of 66%), and the second lowest average word error rate (12.1 versus
an average of 22.1). Because our simulated users simulate speech recognition errors at
the average rate, the difference in performance between BBN and our systems could
easily be explained simply by differences in the speech recognizers, and not differences
in the dialogue management policies.
3.5 Discussion
The most obvious conclusion to draw from these results is not a surprising one: Pure
reinforcement learning with such a huge state space and such limited data does not
performwell. Given the pure RL policy?s score of 34.9, all the policies in Figure 3 and all
the COMMUNICATOR systems in Tables 3 and 4 perform better by quite a large margin.
Inspection of the dialogues indicates that the pure RL policy does not result in a coherent
sequence of actions. This policy tends to choose actions that are associated with the end
of the dialogue, even at the beginning of the dialogue. Perhaps this is because these
actions are only chosen by the COMMUNICATOR systems during relatively successful
dialogues. This policy also tends to repeat the same actions many times, for example
repeatedly requesting information even after the user has supplied this information.
These phenomena are examples of the problemwe used tomotivate our hybrid learning
method, in that they both involve state?action pairs that the learner would never have
seen in the COMMUNICATOR training data.
Given the disappointing performance of the pure RL policy, it is surprising that our
hybrid policies outperform the pure SL policy, as shown in Figures 2 and 3. Though the
increase in performance is small, it is statistically significant, and consistent across the
505
Computational Linguistics Volume 34, Number 4
two hybrid methods and across a range of degrees of influence from RL.9 This indicates
that our hybrid policies are succeeding in getting useful information from the results of
reinforcement learning, even under these extremely difficult circumstances. Perhaps,
under less severe circumstances for RL, a greater gain can be achieved with hybrid
policies. For the second hybrid policy (unobserved state reward offset), the fact that the
best result was achievedwith aUO value (UO = ?10) that is very close to the theoretical
limit of this method (UO = 0) suggests that future improvements to this method could
result in even more useful information being extracted from the RL policy.
The different components of the scoring function give some indication of how the
hybrid policies differ from the SL policy. As indicated in the top two rows of Table 4,
the hybrid policies mostly improve over the SL policy in dialogue length, with a slight
increase in confirmed slots and a slight decrease in filled slots.
One striking conclusion from the results comparing the learned policies to the poli-
cies of the COMMUNICATOR systems, shown in Tables 3 and 4, is that the learned policies
score better than the policies they were trained on. This is particularly surprising for
the pure SL policy, given that this policy is simply trying to mimic the behavior of
these same systems. This can be explained by the fact that the SL policy is the result
of merging all policies of the COMMUNICATOR systems. Thus it can be thought of as
a form of multi-version system, where decisions are made based on what the majority
of systems would do.10 Multi-version systems are well known to perform better than
their component systems, because the mistakes tend to be different across the different
component systems. They remove errors made by any one system that are not shared
by most of the other systems.
The good performance of the SL policy compared to the COMMUNICATOR systems
makes the better performance of the hybrid policies even more impressive. As shown
on the x axis of Figure 3, the best hybrid systems choose a different action from the
SL policy about one action out of four. Despite the good performance of the action
chosen by the SL policy, RL is able to (on average) find a better action by looking at the
rewards achieved by the systems in the data when they chose those actions in similar
states. By following different systems? choices at different points in the dialogue, the
learned policy can potentially perform better than any individual system. Although our
current evaluation methodology is not fine-grained enough to determine if this is being
achieved, the most promising aspect of applying RL to fixed data sets is in learning to
combine the best aspects of each system in the data set.
Although we believe that these results provide an accurate picture of the relative
strengths of the different types of systems we compare, it should be noted that the
reliance on evaluation with simulated dialogues inevitably leads to some lack of pre-
cision in the evaluation. All these results are computed with users who have the same
goal (booking a return flight) and with an evaluation metric that only looks at dialogue
length and whether the four main slots were filled and (optionally) confirmed. On the
9 We previously reported results that showed that adding influence from reinforcement learning always
degraded performance slightly compared to the pure SL policy (Henderson, Lemon, and Georgila 2005).
However, these results were obtained with a preliminary version of the data annotation, which gave
a less accurate indication of when slots were filled and confirmed. The scores we are achieving with
the new data annotation (Georgila et al submitted) are all higher than those reported in Henderson,
Lemon, and Geogila (2005), including the scores calculated from the data for the COMMUNICATOR
systems themselves.
10 To be more technically accurate, we can think of the SL policy as in effect asking each COMMUNICATOR
system for a probability distribution over state?action pairs for the current state, summing these
probabilities across systems, and choosing the action with the highest probability.
506
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
other hand, all the systems were trained to handle a more complicated task than this,
including multi-leg flights, hotel bookings, and rental-car bookings. They were also
designed or trained to complete the task, rather than to fill the slots. Therefore the
evaluation does not reflect all the capabilities or behaviors of the systems. However,
there is no apparent reason to believe that this fact biases our results towards one type
of system or another. This claim is easiest to support for the comparisons between
the hybrid method and the two trained baselines, pure RL and pure SL. For all these
systems, the same data was used to train the systems, the same user models were
used to generate simulated dialogues, and the same evaluation metric was applied to
these simulated dialogues. For the comparisons between the hybrid method and the
COMMUNICATOR systems, only the evaluation metric is exactly the same, but the user
models used for testing the hybrid method were trained to mimic exactly the users
in the dialogues used to evaluate the COMMUNICATOR systems. Because we know of
no evaluation bias introduced when moving from real users to their simulation, we
conclude that this comparison is also indicative of the relative performance of these two
types of systems (particularly given the size of the improvement).
A more general methodological objection could be raised against any evaluation
that uses simulated users. Despite the substantial amount of dialogue system work that
has relied on simulated users (e.g., Scheffler and Young 2002; Pietquin 2004; Georgila,
Henderson, and Lemon 2006; Schatzmann et al 2006), to date there has not been a
systematic experiment that validates this methodology against results from human
users. However, in related work (Lemon, Georgila, and Henderson 2006), we have
demonstrated that a hybrid policy learned as proposed in this article performs better
than a state-of-the-art hand-coded system in experiments with human users. The exper-
iments were done using the ?Town Information?multimodal dialogue system of Lemon
et at. (2006) and Lemon, Georgila, and Stuttle (2005). The hybrid policy reported here
(trained on the COMMUNICATOR data) was ported to this domain, and then evaluated
with human subjects. The learned policy achieved an average gain in perceived task
completion of 14.2% (from 67.6% to 81.8% at p < 0.03) compared to a state-of-the-art
hand-coded system (Lemon, Georgila, and Henderson 2006). This demonstrates that a
policy that performs well in simulation also performs well in real dialogues.11
These experiments demonstrate improvements given an initial fixed data set which
has been generated from existing systems. For applications where there are no existing
systems, an alternative would be to generate the initial data with a Wizard-of-Oz
experiment, where a human plays the part of the system, as explored by Williams and
Young (2003) and Rieser and Lemon (2006b). The methods proposed in this article can
be used to train a policy from such data without having to first build an initial system.
4. Conclusions
In this article, we have investigated how reinforcement learning can be applied to learn
dialogue management policies with large action sets and very large state spaces given
only a fixed data set of dialogues. Under a variety of metrics, our proposed hybrid re-
inforcement learning method outperforms both a policy trained with standard RL and a
11 Future work is to port the hand-coded policy back to the COMMUNICATOR domain for use in simulation.
This will investigate whether a relative improvement in simulated dialogues translates into a relative
improvement in real dialogues.
507
Computational Linguistics Volume 34, Number 4
policy trained with supervised learning, as well as the COMMUNICATOR systems which
generated the data it was trained on. This performance is achieved despite the extremely
challenging task, with 74 actions to choose between, over 10386 possible states, and
very few hand-coded policy decisions. The two main features of our model that make
this possible are the incorporation of supervised learning into a reinforcement learning
model, and the use of linear function approximation with state features provided by the
Information State Update approach to dialogue management. The supervised learning
is used to avoid states not covered by the data set, and the linear function approximation
is used to handle the very large state spaces.
With such a large space of possible state?action pairs, and therefore a huge policy
space, pure reinforcement learning would require an enormous amount of data to find
good policies. We have succeeded in using RL with fairly small data sets of only around
1,000 dialogues (in the portion used for training). This is achieved by using supervised
learning to model when an action would lead to a state for which we do not have
enough data. We proposed two methods for estimating a default value for these unseen
states, and derived a principled way to combine this value with the value estimated by
RL, using the probability provided by SL to weight this combination. This gave us two
hybrid RL/SL methods, both of which outperform both the RL and SL policies alone.
The best hybrid policy performs 302% better than the standard RL policy, and 1.4%
better than the SL policy, according to our automatic evaluation method. In addition,
according to our automatic evaluation method, the hybrid RL/SL policy outperforms
the systems used to generate the data. The best hybrid policy improves over the average
COMMUNICATOR system policy by 10% on our metric. This good performance has
been corroborated in separate experiments with human subjects (Lemon, Georgila, and
Henderson 2006), where the learned policy outperforms a state-of-the-art hand-coded
system.
The success of the hybrid method (and of pure supervised learning) on this chal-
lenging task indicates that linear function approximation is a viable approach to the very
large state spaces produced by the ISU framework. It also demonstrates the utility of a
feature-based representation of states, such as that used in the ISU approach. Further
improvement should be possible by tailoring the representation of states and actions
based on our experience so far (e.g., by including information about specific sequences
of moves), and by using automatic feature selection techniques. We should also be able
to get some improvement from more sophisticated function approximation methods,
such as kernel-based methods.
The next step is to better exploit the advantages of reinforcement learning. One
promising approach is to apply RL while running the learned policy against simulated
users, thereby allowing RL to explore parts of the policy and state spaces that are
not included in the COMMUNICATOR data. The hybrid policy we have learned on the
COMMUNICATOR data is a good starting point for this exploration. Also, the supervised
component within the hybrid system can be used to constrain the range of policies
that need to be explored when training the RL component. All of these advances will
improve techniques for bootstrapping and automatic optimization of dialogue manage-
ment policies from limited initial data sets.
Acknowledgments
This work was partially supported by the
European Commission under the FP6 project
?TALK: Talk and Look, Tools for Ambient
Linguistic Knowledge? (507802) and the FP7
project ?CLASSIC: Computational
Learning in Adaptive Systems for Spoken
Conversation? (216594), by the EPSRC
under grant EP/E019501/1, and by SHEFC
HR04016?Wellcome Trust VIP Award.
508
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
We thank Johanna Moore for proposing
the use of the COMMUNICATOR data set for
this work.
References
Bohlin, Peter, Robin Cooper, Elisabet
Engdahl, and Staffan Larsson. 1999.
Information states and dialog move
engines. Electronic Transactions in AI,
3(9). Available at www.ep.liu.se/ej/
etai/1999/D/.
Bos, Johan, Ewan Klein, Oliver Lemon, and
Tetsushi Oka. 2003. DIPPER: Description
and formalisation of an information-state
update dialogue system architecture. In
Proceedings of the 4th SIGdial Workshop on
Discourse and Dialogue, pages 115?124,
Sapporo.
Cheyer, Adam and David Martin. 2001.
The open agent architecture. Journal of
Autonomous Agents and Multi-Agent
Systems, 4(1/2):143?148.
Denecke, Matthias, Kohji Dohsaka, and
Mikio Nakano, 2005. Fast reinforcement
learning of dialogue policies using stable
function approximation. In K. Y. Su,
J. Tsujii, J.-H. Lee, and O. Y. Kwong,
Natural Language Processing, IJCNLP 2004.
Springer, Berlin, pages 1?11.
Frampton, Matthew and Oliver Lemon.
2005. Reinforcement learning of dialogue
strategies using the user?s last dialogue
act. In Proceedings of the 4th Workshop on
Knowledge and Reasoning in Practical Dialog
Systems, International Joint Conference on
Artificial Intelligence (IJCAI), pages 83?90,
Edinburgh.
Frampton, Matthew and Oliver Lemon.
2006. Learning more effective dialogue
strategies using limited dialogue move
features. In Proceedings of the 44th Meeting
of the Association for Computational
Linguistics, pages 185?192, Sydney.
Gabsdil, Malte and Oliver Lemon. 2004.
Combining acoustic and pragmatic
features to predict recognition
performance in spoken dialogue systems.
In Proceedings of the 42nd Meeting of the
Association for Computational Linguistics,
pages 344?351, Barcelona.
Georgila, Kallirroi, James Henderson, and
Oliver Lemon. 2005. Learning user
simulations for Information State
Update dialogue systems. In Proceedings
of the 9th European Conference on
Speech Communication and Technology
(Interspeech ? Eurospeech), pages 893?896,
Lisbon.
Georgila, Kallirroi, James Henderson, and
Oliver Lemon. 2006. User simulation
for spoken dialogue systems: Learning
and evaluation. In Proceedings of the
9th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 1065?1068, Pittsburgh, PA.
Georgila, Kallirroi, Oliver Lemon, and
James Henderson. 2005. Automatic
annotation of COMMUNICATOR
dialogue data for learning dialogue
strategies and user simulations.
In Proceedings of the Ninth Workshop
on the Semantics and Pragmatics of
Dialogue (SEMDIAL), pages 61?68,
Nancy.
Georgila, Kallirroi, Oliver Lemon, James
Henderson, and Johanna Moore.
(submitted). Automatic annotation of
context and speech acts for dialogue
corpora.
Goddeau, D. and J. Pineau. 2000. Fast
reinforcement learning of dialog strategies.
In Proceedings of the IEEE International
Conference on Acoustics Speech and Signal
Processing (ICASSP), pages II?1233?1236,
Istanbul.
Henderson, James, Oliver Lemon, and
Kallirroi Georgila. 2005. Hybrid
reinforcement/supervised learning for
dialogue policies from COMMUNICATOR
data. In Proceedings of the 4th Workshop on
Knowledge and Reasoning in Practical Dialog
Systems, International Joint Conference on
Artificial Intelligence (IJCAI), pages 68?75,
Edinburgh.
Larsson, Staffan and David Traum. 2000.
Information state and dialogue
management in the TRINDI Dialogue
Move Engine Toolkit. Natural Language
Engineering, 6(3?4):323?340.
Lemon, Oliver, Kallirroi Georgila, and James
Henderson. 2006. Evaluating effectiveness
and portability of reinforcement learned
dialogue strategies with real users: the
TALK TownInfo evaluation. In Proceedings
of the IEEE/ACL 2006 Workshop on Spoken
Language Technology, pages 178?181,
Aruba.
Lemon, Oliver, Kallirroi Georgila, James
Henderson, Malte Gabsdil, Ivan
Meza-Ruiz, and Steve Young. 2005.
Integration of learning and adaptivity with
the ISU approach. Technical Report D4.1,
TALK Project.
Lemon, Oliver, Kallirroi Georgila, James
Henderson, and Matthew Stuttle. 2006.
An ISU dialogue system exhibiting
reinforcement learning of dialogue
policies: generic slot-filling in the TALK
509
Computational Linguistics Volume 34, Number 4
in-car system. In Proceedings of the
Demonstrations of EACL, pages 119?122,
Trento.
Lemon, Oliver, Kallirroi Georgila, and
Matthew Stuttle. 2005. Showcase
exhibiting reinforcement learning for
dialogue strategies in the in-car domain.
Technical Report D4.2, TALK Project.
Levin, Esther and Roberto Pieraccini. 1997.
A stochastic model of computer-human
interaction for learning dialogue strategies.
In Proceedings of the 5th European Conference
on Speech Communication and Technology
(Interspeech ? Eurospeech), pages 1883?1886,
Rhodes.
Levin, Esther, Roberto Pieraccini, and
Wieland Eckert. 2000. A stochastic model
of human-machine interaction for learning
dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1):11?23.
Pietquin, Olivier. 2004. A Framework for
Unsupervised Learning of Dialogue Strategies.
Presses Universitaires de Louvain,
SIMILAR Collection.
Pietquin, Olivier and Thierry Dutoit. 2006a.
Dynamic Bayesian networks for NLU
simulation with application to dialog
optimal strategy learning. In Proceedings
of the IEEE International Conference on
Acoustics Speech and Signal Processing
(ICASSP), pages 49?52, Toulouse.
Pietquin, Olivier and Thierry Dutoit. 2006b.
A probabilistic framework for dialog
simulation and optimal strategy learning.
IEEE Transactions on Speech and Audio
Processing, 14(2):589?599.
Rieser, Verena and Oliver Lemon. 2006a.
Cluster-based user simulations for
learning dialogue strategies and the
SUPER evaluation metric. In Proceedings
of the 9th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 1766?1769, Pittsburgh, PA.
Rieser, Verena and Oliver Lemon. 2006b.
Using logistic regression to initialise
reinforcement-learning-based dialogue
systems. In Proceedings of the IEEE/ACL
2006 Workshop on Spoken Language
Technology, pages 190?193, Aruba.
Rieser, Verena and Oliver Lemon. 2006c.
Using machine learning to explore
human multimodal clarification
strategies. In Proceedings of the Poster
Session of the 44th Meeting of the
Association for Computational Linguistics,
pages 659?666, Sydney.
Roy, Nicholas, Joelle Pineau, and Sebastian
Thrun. 2000. Spoken dialog management
for robots. In Proceedings of the 38th Meeting
of the Association for Computational
Linguistics, pages 93?100, Hong Kong.
Schatzmann, Jost, Kallirroi Georgila,
and Steve Young. 2005. Quantitative
evaluation of user simulation techniques
for spoken dialogue systems. In
Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue, pages 45?54,
Lisbon.
Schatzmann, Jost, Matthew N. Stuttle,
Karl Weilhammer, and Steve Young.
2005. Effects of the user model on
simulation-based learning of dialogue
strategies. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 220?225,
San Juan, Puerto Rico.
Schatzmann, Jost, Karl Weilhammer,
Matthew N. Stuttle, and Steve Young.
2006. A survey of statistical user
simulation techniques for
reinforcement-learning of dialogue
management strategies. The Knowledge
Engineering Review, 21:97?126.
Scheffler, Konrad and Steve Young. 2001.
Corpus-based dialogue simulation
for automatic strategy learning and
evaluation. In Proceedings of the NAACL
Workshop on Adaptation in Dialogue
Systems, pages 64?70, Pittsburgh, PA.
Scheffler, Konrad and Steve Young.
2002. Automatic learning of dialogue
strategy using dialogue simulation
and reinforcement learning. In Proceedings
of the Human Language Technology
Conference, pages 12?19, San Diego, CA.
Shawe-Taylor, John and Nello Cristianini.
2004. Kernel Methods for Pattern Analysis.
Cambridge University Press.
Singh, Satinder, Michael Kearns, Diane
Litman, and Marilyn Walker. 2000a.
Empirical evaluation of a reinforcement
learning dialogue system. In Proceedings
of the AAAI, pages 645?651, Whistler.
Singh, Satinder, Michael Kearns, Diane
Litman, and Marilyn Walker. 2000b.
Reinforcement learning for spoken
dialogue systems. In Advances in Neural
Information Processing Systems, 12:956?962.
Singh, Satinder, Diane Litman, Michael
Kearns, and Marilyn Walker. 2002.
Optimizing dialogue management with
reinforcement learning: Experiments
with the NJFun system. Journal of
Artificial Intelligence Research (JAIR),
16:105?133.
Sutton, Richard and Andrew Barto. 1998.
Reinforcement Learning. MIT Press,
Cambridge, MA.
510
Henderson, Lemon, and Georgila Hybrid Reinforcement/Supervised Learning
Walker, M., J. Aberdeen, J. Boland, E. Bratt,
J. Garofolo, L. Hirschman, A. Le, S. Lee,
S. Narayanan, K. Papineni, B. Pellom,
B. Polifroni, A. Potamianos, P. Prabhu,
A. Rudnicky, G. Sanders, S. Seneff,
D. Stallard, and S. Whittaker. 2001.
DARPA communicator dialog travel
planning systems: The June 2000 data
collection. In Proceedings of the 7th European
Conference on Speech Communication and
Technology (Interspeech ? Eurospeech),
pages 1371?1374, Aalborg.
Walker, M. and R. Passonneau. 2001. DATE:
A dialogue act tagging scheme for
evaluation of spoken dialogue systems.
In Proceedings of the Human Language
Technology Conference, pages 1?8, San
Diego, CA.
Walker, M., A. Rudnicky, J. Aberdeen,
E. Bratt, J. Garofolo, H. Hastie, A. Le,
B. Pellom, A. Potamianos, R. Passonneau,
R. Prasad, S. Roukos, G. Sanders, S. Seneff,
D. Stallard, and S. Whittaker. 2002a.
DARPA Communicator Evaluation:
Progress from 2000 to 2001. In Proceedings
of the 7th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 273?276, Denver, CO.
Walker, M., A. Rudnicky, R. Prasad,
J. Aberdeen, E. Bratt, J. Garofolo,
H. Hastie, A. Le, B. Pellom, A. Potamianos,
R. Passonneau, S. Roukos, G. Sanders,
S. Seneff, and D. Stallard. 2002b. DARPA
Communicator: Cross-system results
for the 2001 evaluation. In Proceedings of
the 7th International Conference on Spoken
Language Processing (Interspeech ? ICSLP),
pages 269?272, Denver, CO.
Walker, Marilyn A., Jeanne C. Fromer, and
Shrikanth Narayanan. 1998. Learning
optimal dialogue strategies: A case study
of a spoken dialogue agent for email.
In Proceedings of the 17th International
Conference on Computational Linguistics,
pages 1345?1351, Montreal.
Walker, Marilyn A., Candace A. Kamm,
and Diane J. Litman. 2000. Towards
developing general models of usability
with PARADISE. Natural Language
Engineering, 6(3):363?377.
Walker, Marilyn A., Rebecca J. Passonneau,
and Julie E. Boland. 2001. Quantitative
and qualitative evaluation of DARPA
Communicator spoken dialogue systems.
In Proceedings of the 39th Meeting of the
Association for Computational Linguistics,
pages 515?522, Toulouse.
Williams, Jason. 2007. A method for
evaluating and comparing user
simulations: The Cramer-von Mises
divergence. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 508?513,
Kyoto.
Williams, Jason, Pascal Poupart, and
Steve Young. 2005a. Factored partially
observable Markov decision processes for
dialogue management. In Proceedings of the
4th Workshop on Knowledge and Reasoning in
Practical Dialog Systems, International Joint
Conference on Artificial Intelligence (IJCAI),
pages 76?82, Edinburgh.
Williams, Jason, Pascal Poupart, and Steve
Young. 2005b. Partially observable Markov
decision processes with continuous
observations for dialogue management.
In Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue, pages 25?34,
Lisbon.
Williams, Jason and Steve Young. 2003. Using
Wizard-of-Oz simulations to bootstrap
reinforcement-learning-based dialog
management systems. In Proceedings of the
4th SIGdial Workshop on Discourse and
Dialogue, pages 135?139, Sapporo.
Williams, Jason and Steve Young. 2005.
Scaling up POMDPs for dialog
management: The ?Summary POMDP?
method. In Proceedings of the IEEE
Automatic Speech Recognition and
Understanding Workshop, pages 177?182,
San Juan, Puerto Rico.
Young, Steve. 2000. Probabilistic methods in
spoken dialogue systems. Philosophical
Transactions of the Royal Society (Series A),
358(1769):1389?1402.
511

Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 173?176,
New York, June 2006. c?2006 Association for Computational Linguistics
Evolving optimal inspectable strategies for spoken dialogue systems
Dave Toney
School of Informatics
Edinburgh University
2 Buccleuch Place
Edinburgh EH8 9LW
dave@cstr.ed.ac.uk
Johanna Moore
School of Informatics
Edinburgh University
2 Buccleuch Place
Edinburgh EH8 9LW
jmoore@inf.ed.ac.uk
Oliver Lemon
School of Informatics
Edinburgh University
2 Buccleuch Place
Edinburgh EH8 9LW
olemon@inf.ed.ac.uk
Abstract
We report on a novel approach to gener-
ating strategies for spoken dialogue sys-
tems. We present a series of experiments
that illustrate how an evolutionary rein-
forcement learning algorithm can produce
strategies that are both optimal and easily
inspectable by human developers. Our ex-
perimental strategies achieve a mean per-
formance of 98.9% with respect to a pre-
defined evaluation metric. Our approach
also produces a dramatic reduction in
strategy size when compared with conven-
tional reinforcement learning techniques
(87% in one experiment). We conclude
that this algorithm can be used to evolve
optimal inspectable dialogue strategies.
1 Introduction
Developing a dialogue management strategy for a
spoken dialogue system is often a complex and time-
consuming task. This is because the number of
unique conversations that can occur between a user
and the system is almost unlimited. Consequently,
a system developer may spend a lot of time antic-
ipating how potential users might interact with the
system before deciding on the appropriate system re-
sponse.
Recent research has focused on generating dia-
logue strategies automatically. This work is based
on modelling dialogue as a markov decision process,
formalised by a finite state space S, a finite action
set A, a set of transition probabilities T and a re-
ward function R. Using this model an optimal dia-
logue strategy pi? is represented by a mapping be-
tween the state space and the action set. That is, for
each state s ? S this mapping defines its optimal ac-
tion a?s . How is this mapping constructed? Previous
approaches have employed reinforcement learning
(RL) algorithms to estimate an optimal value func-
tion Q? (Levin et al, 2000; Frampton and Lemon,
2005). For each state this function predicts the fu-
ture reward associated with each action available in
that state. This function makes it easy to extract the
optimal strategy (policy in the RL literature).
Progress has been made with this approach but
some important challenges remain. For instance,
very little success has been achieved with the large
state spaces that are typical of real-life systems.
Similarly, work on summarising learned strategies
for interpretation by human developers has so far
only been applied to tasks where each state-action
pair is explicitly represented (Lec?uche, 2001).
This tabular representation severely limits the size
of the state space.
We propose an alternative approach to finding op-
timal dialogue policies. We make use of XCS, an
evolutionary reinforcement learning algorithm that
seeks to represent a policy as a compact set of state-
action rules (Wilson, 1995). We suggest that this al-
gorithm could overcome both the challenge of large
state spaces and the desire for strategy inspectability.
In this paper, we focus on the issue of inspectabil-
ity. We present a series of experiments that illustrate
how XCS can be used to evolve dialogue strategies
that are both optimal and easily inspectable.
173
2 Learning Classifier Systems and XCS
Learning Classifier Systems were introduced by
John Holland in the 1970s as a framework for learn-
ing rule-based knowledge representations (Holland,
1976). In this model, a rule base consists of a popu-
lation of N state-action rules known as classifiers.
The state part of a classifier is represented by a
ternary string from the set {0,1,#} while the action
part is composed from {0,1}. The # symbol acts as
a wildcard allowing a classifier to aggregate states;
for example, the state string 1#1 matches the states
111 and 101. Classifier systems have been applied
to a number of learning tasks, including data mining,
optimisation and control (Bull, 2004).
Classifier systems combine two machine learning
techniques to find the optimal rule set. A genetic
algorithm is used to evaluate and modify the popu-
lation of rules while reinforcement learning is used
to assign rewards to existing rules. The search for
better rules is guided by the strength parameter as-
sociated with each classifier. This parameter serves
as a fitness score for the genetic algorithm and as a
predictor of future reward (payoff ) for the RL algo-
rithm. This evolutionary learning process searches
the space of possible rule sets to find an optimal pol-
icy as defined by the reward function.
XCS (X Classifier System) incorporates a num-
ber of modifications to Holland?s original frame-
work (Wilson, 1995). In this system, a classifier?s
fitness is based on the accuracy of its payoff predic-
tion instead of the prediction itself. Furthermore, the
genetic algorithm operates on actions instead of the
population as a whole. These aspects of XCS result
in a more complete map of the state-action space
than would be the case with strength-based classi-
fier systems. Consequently, XCS often outperforms
strength-based systems in sequential decision prob-
lems (Kovacs, 2000).
3 Experimental Methodology
In this section we present a simple slot-filling sys-
tem based on the hotel booking domain. The goal of
the system is to acquire the values for three slots: the
check-in date, the number of nights the user wishes
to stay and the type of room required (single, twin
etc.). In slot-filling dialogues, an optimal strategy is
one that interacts with the user in a satisfactory way
while trying to minimise the length of the dialogue.
A fundamental component of user satisfaction is the
system?s prevention and repair of any miscommuni-
cation between it and the user. Consequently, our
hotel booking system focuses on evolving essential
slot confirmation strategies.
We devised an experimental framework for mod-
elling the hotel system as a sequential decision task
and used XCS to evolve three behaviours. Firstly,
the system should execute its dialogue acts in a log-
ical sequence. In other words, the system should
greet the user, ask for the slot information, present
the query results and then finish the dialogue, in that
order (Experiment 1). Secondly, the system should
try to acquire the slot values as quickly as possible
while taking account of the possibility of misrecog-
nition (Experiments 2a and 2b). Thirdly, to increase
the likelihood of acquiring the slot values correctly,
each one should be confirmed at least once (Experi-
ments 3 and 4).
The reward function for Experiments 1, 2a and
2b was the same. During a dialogue, each non-
terminal system action received a reward value of
zero. At the end of each dialogue, the final reward
comprised three parts: (i) -1000 for each system
turn; (ii) 100,000 if all slots were filled; (iii) 100,000
if the first system act was a greeting. In Experiments
3 and 4, an additional reward of 100,000 was as-
signed if all slots were confirmed.
The transition probabilities were modelled using
two versions of a handcoded simulated user. A very
large number of test dialogues are usually required
for learning optimal dialogue strategies; simulated
users are a practical alternative to employing human
test users (Scheffler and Young, 2000; Lopez-Cozar
et al, 2002). Simulated user A represented a fully
cooperative user, always giving the slot information
that was asked. User B was less cooperative, giving
no response 20% of the time. This allowed us to
perform a two-fold cross validation of the evolved
strategies.
For each experiment we allowed the system?s
strategy to evolve over 100,000 dialogues with each
simulated user. Dialogues were limited to a maxi-
mum of 30 system turns. We then tested each strat-
egy with a further 10,000 dialogues. We logged the
total reward (payoff) for each test dialogue. Each
experiment was repeated ten times.
174
In each experiment, the presentation of the query
results and closure of the dialogue were combined
into a single dialogue act. Therefore, the dialogue
acts available to the system for the first experi-
ment were: Greeting, Query+Goodbye, Ask(Date),
Ask(Duration) and Ask(RoomType). Four boolean
variables were used to represent the state of the di-
alogue: GreetingFirst, DateFilled, DurationFilled,
RoomFilled.
Experiment 2 added a new dialogue act: Ask(All).
The goal here was to ask for all three slot values
if the probability of getting the slot values was rea-
sonably high. If the probability was low, the sys-
tem should ask for the slots one at a time as be-
fore. This information was modelled in the sim-
ulated users by 2 variables: Prob1SlotCorrect and
Prob3SlotsCorrect. The values for these variables
in Experiments 2a and 2b respectively were: 0.9 and
0.729 (=0.93); 0.5 and 0.125 (=0.53).
Experiment 3 added three new dialogue acts: Ex-
plicit Confirm(Date), Explicit Confirm(Duration),
Explicit Confirm(RoomType) and three new state
variables: DateConfirmed, DurationConfirmed,
RoomConfirmed. The goal here was for the sys-
tem to learn to confirm each of the slot val-
ues after the user has first given them. Experi-
ment 4 sought to reduce the dialogue length fur-
ther by allowing the system to confirm one slot
value while asking for another. Two new di-
alogue acts were available in this last experi-
ment: Implicit Confirm(Date)+Ask(Duration) and
Implicit Confirm(Duration)+Ask(RoomType).
4 Experimental Results
Table 1 lists the total reward (payoff) averaged over
the 10 cross-validated test trials for each experiment,
expressed as a percentage of the maximum payoff.
In these experiments, the maximum payoff repre-
sents the shortest possible successful dialogue. For
example, the maximum payoff for Experiment 1 is
195,000: 100,000 for filling the slots plus 100,000
for greeting the user at the start of the dialogue mi-
nus 5000 for the minimum number of turns (five)
taken to complete the dialogue successfully. The av-
erage payoff for the 10 trials trained on simulated
user A and tested on user B was 193,877 ? approxi-
mately 99.4% of the maximum possible. In light of
Exp. Training/Test Users Payoff (%)
1 A, B 99.4B, A 99.8
2a A, B 99.1B, A 99.4
2b A, B 96.8B, A 97.2
3 A, B 98.8B, A 99.3
4 A, B 99.3B, A 99.7
Table 1: Payoff results for the evolved strategies.
these results and the stochastic user responses, we
suggest that these evolved strategies would compare
favourably with any handcoded strategies.
It is instructive to compare the rate of convergence
for different strategies. Figure 1 shows the average
payoff for the 100,000 dialogues trained with sim-
ulated user A in Experiments 3 and 4. It shows
that Experiment 3 approached the optimal policy
after approximately 20,000 dialogues whereas Ex-
periment 4 converged after approximately 5000 dia-
logues. This is encouraging because it suggests that
XCS remains focused on finding the shortest suc-
cessful dialogue even when the number of available
actions increases.
0 25,000 50,000 75,000 100,000
0
0.5
1
1.5
2
2.5
3
x 105
Dialogues
Av
er
ag
e 
Pa
yo
ff
Exp. 3 
Exp. 4 
Figure 1: Convergence towards optimality during
training in Experiments 3 and 4 (simulated user A).
Finally, we look at how to represent an optimal
strategy. From the logs of the test dialogues we ex-
tracted the state-action rules (classifiers) that were
executed. For example, in Experiment 4, the op-
175
State Action
Gre
etin
gFi
rst
Da
teF
ille
d
Du
rat
ion
Fill
ed
Roo
mF
ille
d
Da
teC
onfir
med
Du
rat
ion
Con
firm
ed
Roo
mC
onfir
med
0 0 # # # # # Greeting
1 0 0 0 # # # Ask(Date)
1 1 # # 0 # # Implicit Confirm(Date) + Ask(Duration)
1 1 1 # 1 0 0 Implicit Confirm(Duration) + Ask(RoomType)
1 1 1 1 1 1 0 Explicit Confirm(RoomType)
1 1 1 1 1 1 1 Query + Goodbye
Table 2: A summary of the optimal strategy for Experiment 4.
timal strategy is represented by 17 classifiers. By
comparison, a purely RL-based strategy would de-
fine an optimal action for every theoretically pos-
sible state (i.e. 128). In this example, the evolu-
tionary approach has reduced the number of rules
from 128 to 17 (a reduction of 87%) and is therefore
much more easily inspectable. In fact, the size of the
optimal strategy can be reduced further by select-
ing the most general classifier for each action (Table
2). These rules are sufficient since they cover the 60
states that could actually occur while following the
optimal strategy.
5 Conclusions and future work
We have presented a novel approach to generating
spoken dialogue strategies that are both optimal and
easily inspectable. The generalizing ability of the
evolutionary reinforcement learning (RL) algorithm,
XCS, can dramatically reduce the size of the opti-
mal strategy when compared with conventional RL
techniques. In future work, we intend to exploit this
generalization feature further by developing systems
that require much larger state representations. We
also plan to investigate other approaches to strategy
summarisation. Finally, we will evaluate our ap-
proach against purely RL-based methods.
References
Larry Bull, editor. 2004. Applications of Learning Clas-
sifier Systems. Springer.
Matthew Frampton and Oliver Lemon. 2005. Reinforce-
ment learning of dialogue strategies using the user?s
last dialogue act. In IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Edin-
burgh, UK, July.
John Holland. 1976. Adaptation. In Rosen R.
and F. Snell, editors, Progress in theoretical biology.
Plenum, New York.
Tim Kovacs. 2000. Strength or accuracy? Fitness cal-
culation in learning classifier systems. In Pier Luca
Lanzi, Wolfgang Stolzmann, and Stewart Wilson, edi-
tors, Learning Classifier Systems. From Foundations to
Applications, Lecture Notes in Artificial Intelligence
1813, pages 143?160. Springer-Verlag.
Renaud Lec?uche. 2001. Learning optimal dialogue
management rules by using reinforcement learning
and inductive logic programming. In 2nd Meeting
of the North American Chapter of the Association of
Computational Linguistics, Pittsburgh, USA, June.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine inter-
action for learning dialogue strategies. IEEE Transac-
tions on Speech and Audio Processing, 8(1):11?23.
R. Lopez-Cozar, A. De la Torre, J. Segura, A. Rubio, and
V. Sa?nchez. 2002. Testing dialogue systems by means
of automatic generation of conversations. Interacting
with Computers, 14(5):521?546.
Konrad Scheffler and Steve Young. 2000. Probabilis-
tic simulation of human-machine dialogues. In Inter-
national Conference on Acoustics, Speech and Signal
Processing, pages 1217?1220, Istanbul, Turkey, June.
Stewart Wilson. 1995. Classifier fitness based on accu-
racy. Evolutionary Computation, 3(2):149?175.
176
Combining Acoustic and Pragmatic Features to Predict Recognition
Performance in Spoken Dialogue Systems
Malte Gabsdil
Department of Computational Linguistics
Saarland University
Germany
gabsdil@coli.uni-sb.de
Oliver Lemon
School of Informatics
Edinburgh University
Scotland
olemon@inf.ed.ac.uk
Abstract
We use machine learners trained on a combina-
tion of acoustic confidence and pragmatic plausi-
bility features computed from dialogue context to
predict the accuracy of incoming n-best recogni-
tion hypotheses to a spoken dialogue system. Our
best results show a 25% weighted f-score improve-
ment over a baseline system that implements a
?grammar-switching? approach to context-sensitive
speech recognition.
1 Introduction
A crucial problem in the design of spoken dialogue
systems is to decide for incoming recognition hy-
potheses whether a system should accept (consider
correctly recognized), reject (assume misrecogni-
tion), or ignore (classify as noise or speech not di-
rected to the system) them. In addition, a more so-
phisticated dialogue system might decide whether
to clarify or confirm certain hypotheses.
Obviously, incorrect decisions at this point can
have serious negative effects on system usability
and user satisfaction. On the one hand, accepting
misrecognized hypotheses leads to misunderstand-
ings and unintended system behaviors which are
usually difficult to recover from. On the other hand,
users might get frustrated with a system that be-
haves too cautiously and rejects or ignores too many
utterances. Thus an important feature in dialogue
system engineering is the tradeoff between avoiding
task failure (due to misrecognitions) and promoting
overall dialogue efficiency, flow, and naturalness.
In this paper, we investigate the use of machine
learners trained on a combination of acoustic confi-
dence and pragmatic plausibility features (i.e. com-
puted from dialogue context) to predict the qual-
ity of incoming n-best recognition hypotheses to
a spoken dialogue system. These predictions are
then used to select a ?best? hypothesis and to de-
cide on appropriate system reactions. We evalu-
ate this approach in comparison with a baseline
system that combines fixed recognition confidence
rejection thresholds with dialogue-state dependent
recognition grammars (Lemon, 2004).
The paper is organized as follows. After a short
relation to previous work, Section 3 introduces the
WITAS multimodal dialogue system, which we use
to collect data (Section 4) and to derive baseline re-
sults (Section 5). Section 6 describes our learning
experiments for classifying and selecting from n-
best recognition hypotheses and Section 7 reports
our results.
2 Relation to Previous Work
(Litman et al, 2000) use acoustic-prosodic infor-
mation extracted from speech waveforms, together
with information derived from their speech recog-
nizer, to automatically predict misrecognized turns
in a corpus of train-timetable information dialogues.
In our experiments, we also use recognizer con-
fidence scores and a limited number of acoustic-
prosodic features (e.g. amplitude in the speech sig-
nal) for hypothesis classification. (Walker et al,
2000) use a combination of features from the speech
recognizer, natural language understanding, and di-
alogue manager/discourse history to classify hy-
potheses as correct, partially correct, or misrecog-
nized. Our work is related to these experiments in
that we also combine confidence scores and higher-
level features for classification. However, both (Lit-
man et al, 2000) and (Walker et al, 2000) con-
sider only single-best recognition results and thus
use their classifiers as ?filters? to decide whether the
best recognition hypothesis for a user utterance is
correct or not. We go a step further in that we clas-
sify n-best hypotheses and then select among the al-
ternatives. We also explore the use of more dialogue
and task-oriented features (e.g. the dialogue move
type of a recognition hypothesis) for classification.
The main difference between our approach and
work on hypothesis reordering (e.g. (Chotimongkol
and Rudnicky, 2001)) is that we make a decision re-
garding whether a dialogue system should accept,
clarify, reject, or ignore a user utterance. Fur-
thermore, our approach is more generally applica-
ble than preceding research, since we frame our
methodology in the Information State Update (ISU)
approach to dialogue management (Traum et al,
1999) and therefore expect it to be applicable to a
range of related multimodal dialogue systems.
3 The WITAS Dialogue System
The WITAS dialogue system (Lemon et al, 2002)
is a multimodal command and control dialogue sys-
tem that allows a human operator to interact with
a simulated ?unmanned aerial vehicle? (UAV): a
small robotic helicopter. The human operator is pro-
vided with a GUI ? an interactive (i.e. mouse click-
able) map ? and specifies mission goals using nat-
ural language commands spoken into a headset, or
by using combinations of GUI actions and spoken
commands. The simulated UAV can carry out dif-
ferent activities such as flying to locations, follow-
ing vehicles, and delivering objects. The dialogue
system uses the Nuance 8.0 speech recognizer with
language models compiled from a grammar (written
using the Gemini system (Dowding et al, 1993)),
which is also used for parsing and generation.
3.1 WITAS Information States
The WITAS dialogue system is part of a larger
family of systems that implement the Information
State Update (ISU) approach to dialogue manage-
ment (Traum et al, 1999). The ISU approach has
been used to formalize different theories of dia-
logue and forms the basis of several dialogue sys-
tem implementations in domains such as route plan-
ning, home automation, and tutorial dialogue. The
ISU approach is a particularly useful testbed for
our technique because it collects information rele-
vant to dialogue context in a central data structure
from which it can be easily extracted. (Lemon et al,
2002) describe in detail the components of Informa-
tion States (IS) and the update procedures for pro-
cessing user input and generating system responses.
Here, we briefly introduce parts of the IS which are
needed to understand the system?s basic workings,
and from which we will extract dialogue-level and
task-level information for our learning experiments:
? Dialogue Move Tree (DMT): a tree-structure,
in which each subtree of the root node repre-
sents a ?thread? in the conversation, and where
each node in a subtree represents an utterance
made either by the system or the user. 1
? Active Node List (ANL): a list that records all
?active? nodes in the DMT; active nodes indi-
1A tree is used in order to overcome the limitations of stack-
based processing, see (Lemon and Gruenstein, 2004).
cate conversational contributions that are still
in some sense open, and to which new utter-
ances can attach.
? Activity Tree (AT): a tree-structure represent-
ing the current, past, and planned activities that
the back-end system (in this case a UAV) per-
forms.
? Salience List (SL): a list of NPs introduced in
the current dialogue ordered by recency.
? Modality Buffer (MB): a temporary store that
registers click events on the GUI.
The DMT and AT are the core components of In-
formation States. The SL and MB are subsidiary
data-structures needed for interpreting and generat-
ing anaphoric expressions and definite NPs. Finally,
the ANL plays a crucial role in integrating new user
utterances into the DMT.
4 Data Collection
For our experiments, we use data collected in a
small user study with the grammar-switching ver-
sion of the WITAS dialogue system (Lemon, 2004).
In this study, six subjects from Edinburgh Univer-
sity (4 male, 2 female) had to solve five simple tasks
with the system, resulting in 30 complete dialogues.
The subjects? utterances were recorded as 8kHz
16bit waveform files and all aspects of the Informa-
tion State transitions during the interactions were
logged as html files. Altogether, 303 utterances
were recorded in the user study (? 10 user utter-
ances/dialogue).
4.1 Labeling
We transcribed all user utterances and parsed the
transcriptions offline using WITAS? natural lan-
guage understanding component in order to get a
gold-standard labeling of the data. Each utter-
ance was labeled as either in-grammar or out-of-
grammar (oog), depending on whether its transcrip-
tion could be parsed or not, or as crosstalk: a spe-
cial marker that indicated that the input was not di-
rected to the system (e.g. noise, laughter, self-talk,
the system accidentally recording itself). For all
in-grammar utterances we stored their interpreta-
tions (quasi-logical forms) as computed by WITAS?
parser. Since the parser uses a domain-specific se-
mantic grammar designed for this particular appli-
cation, each in-grammar utterance had an interpre-
tation that is ?correct? with respect to the WITAS
application.
4.2 Simplifying Assumptions
The evaluations in the following sections make two
simplifying assumptions. First, we consider a user
utterance correctly recognized only if the logical
form of the transcription is the same as the logical
form of the recognition hypothesis. This assump-
tion can be too strong because the system might re-
act appropriately even if the logical forms are not
literally the same. Second, if a transcribed utter-
ance is out-of-grammar, we assume that the system
cannot react appropriately. Again, this assumption
might be too strong because the recognizer can ac-
cidentally map an utterance to a logical form that is
equivalent to the one intended by the user.
5 The Baseline System
The baseline for our experiments is the behavior of
the WITAS dialogue system that was used to col-
lect the experimental data (using dialogue context
as a predictor of language models for speech recog-
nition, see below). We chose this baseline because it
has been shown to perform significantly better than
an earlier version of the system that always used the
same (i.e. full) grammar for recognition (Lemon,
2004).
We evaluate the performance of the baseline by
analyzing the dialogue logs from the user study.
With this information, it is possible to decide how
the system reacted to each user utterance. We dis-
tinguish between the following three cases:
1. accept: the system accepted the recognition
hypothesis of a user utterance as correct.
2. reject: the system rejected the recognition hy-
pothesis of a user utterance given a fixed con-
fidence rejection threshold.
3. ignore: the system did not react to a user utter-
ance at all.
These three classes map naturally to the gold-
standard labels of the transcribed user utterances:
the system should accept in-grammar utterances, re-
ject out-of-grammar input, and ignore crosstalk.
5.1 Context-sensitive Speech Recognition
In the the WITAS dialogue system, the ?grammar-
switching? approach to context-sensitive speech
recognition (Lemon, 2004) is implemented using
the ANL. At any point in the dialogue, there is a
?most active node? at the top of the ANL. The dia-
logue move type of this node defines the name of a
language model that is used for recognizing the next
user utterance. For instance, if the most active node
is a system yes-no-question then the appropriate
language model is defined by a small context-free
grammar covering phrases such as ?yes?, ?that?s
right?, ?okay?, ?negative?, ?maybe?, and so on.
The WITAS dialogue system with context-
sensitive speech recognition showed significantly
better recognition rates than a previous version of
the system that used the full grammar for recogni-
tion at all times ((Lemon, 2004) reports a 11.5%
reduction in overall utterance recognition error
rate). Note however that an inherent danger with
grammar-switching is that the system may have
wrong expectations and thus might activate a lan-
guage model which is not appropriate for the user?s
next utterance, leading to misrecognitions or incor-
rect rejections.
5.2 Results
Table 1 summarizes the evaluation of the baseline
system.
System behavior
User utterance accept reject ignore
in-grammar 154/22 8 4
out-of-grammar 45 43 4
crosstalk 12 9 2
Accuracy: 65.68%
Weighted f-score: 61.81%
Table 1: WITAS dialogue system baseline results
Table 1 should be read as follows: looking at the
first row, in 154 cases the system understood and
accepted the correct logical form of an in-grammar
utterance by the user. In 22 cases, the system ac-
cepted a logical form that differed from the one for
the transcribed utterance.2 In 8 cases, the system re-
jected an in-grammar utterance and in 4 cases it did
not react to an in-grammar utterance at all. The sec-
ond row of Table 1 shows that the system accepted
45, rejected 43, and ignored 4 user utterances whose
transcriptions were out-of-grammar and could not
be parsed. Finally, the third row of the table shows
that the baseline system accepted 12 utterances that
were not addressed to it, rejected 9, and ignored 2.
Table 1 shows that a major problem with the base-
line system is that it accepts too many user utter-
ances. In particular, the baseline system accepts the
wrong interpretation for 22 in-grammar utterances,
45 utterances which it should have rejected as out-
of-grammar, and 12 utterances which it should have
2For the computation of accuracy and weighted f-scores,
these were counted as wrongly accepted out-of-grammar ut-
terances.
ignored. All of these cases will generally lead to
unintended actions by the system.
6 Classifying and Selecting N-best
Recognition Hypotheses
We aim at improving over the baseline results by
considering the n-best recognition hypotheses for
each user utterance. Our methodology consists of
two steps: i) we automatically classify the n-best
recognition hypotheses for an utterance as either
correctly or incorrectly recognized and ii) we use a
simple selection procedure to choose the ?best? hy-
pothesis based on this classification. In order to get
multiple recognition hypotheses for all utterances
in the experimental data, we re-ran the speech rec-
ognizer with the full recognition grammar and 10-
best output and processed the results offline with
WITAS? parser, obtaining a logical form for each
recognition hypothesis (every hypothesis has a log-
ical form since language models are compiled from
the parsing grammar).
6.1 Hypothesis Labeling
We labeled all hypotheses with one of the follow-
ing four classes, based on the manual transcriptions
of the experimental data: in-grammar, oog (WER ?
50), oog (WER > 50), or crosstalk. The in-grammar
and crosstalk classes correspond to those described
for the baseline. However, we decided to divide up
the out-of-grammar class into the two classes oog
(WER? 50) and oog (WER > 50) to get a more fine-
grained classification. In order to assign hypotheses
to the two oog classes, we compute the word er-
ror rate (WER) between recognition hypotheses and
the transcription of corresponding user utterances.
If the WER is ? 50%, we label the hypothesis as
oog (WER ? 50), otherwise as oog (WER > 50).
We also annotate all misrecognized hypotheses of
in-grammar utterances with their respective WER
scores.
The motivation behind splitting the out-of-
grammar class into two subclasses and for anno-
tating misrecognized in-grammar hypotheses with
their WER scores is that we want to distinguish be-
tween different ?degrees? of misrecognition that can
be used by the dialogue system to decide whether
it should initiate clarification instead of rejection.3
We use a threshold (50%) on a hypothesis? WER
as an indicator for whether hypotheses should be
3The WITAS dialogue system currently does not support
this type of clarification dialogue; the WER annotations are
therefore only of theoretical interest. However, an extended
system could easily use this information to decide when clari-
fication should be initiated.
clarified or rejected. This is adopted from (Gabs-
dil, 2003), based on the fact that WER correlates
with concept accuracy (CA, (Boros et al, 1996)).
The WER threshold can be set differently according
to the needs of an application. However, one would
ideally set a threshold directly on CA scores for this
labeling, but these are currently not available for our
data.
We also introduce the distinction between out-of-
grammar (WER ? 50) and out-of-grammar (WER
> 50) in the gold standard for the classification
of (whole) user utterances. We split the out-of-
grammar class into two sub-classes depending on
whether the 10-best recognition results include at
least one hypothesis with a WER ? 50 compared
to the corresponding transcription. Thus, if there is
a recognition hypothesis which is close to the tran-
scription, an utterance is labeled as oog (WER ?
50). In order to relate these classes to different sys-
tem behaviors, we define that utterances labeled as
oog (WER ? 50) should be clarified and utterances
labeled as oog (WER > 50) should be rejected by
the system. The same is done for all in-grammar
utterances for which only misrecognized hypothe-
ses are available.
6.2 Classification: Feature Groups
We represent recognition hypotheses as 20-
dimensional feature vectors for automatic classifica-
tion. The feature vectors combine recognizer con-
fidence scores, low-level acoustic information, in-
formation from WITAS system Information States,
and domain knowledge about the different tasks in
the scenario. The following list gives an overview
of all features (described in more detail below).
1. Recognition (6): nbestRank, hypothe-
sisLength, confidence, confidenceZScore,
confidence-StandardDeviation, minWordCon-
fidence
2. Utterance (3): minAmp, meanAmp, RMS-amp
3. Dialogue (9): currentDM, currentCommand,
mostActiveNode, DMBigramFrequency, qa-
Match, aqMatch, #unresolvedNPs, #unre-
solvedPronouns, #uniqueIndefinites
4. Task (2): taskConflict, #taskConstraintCon-
flict
All features are extracted automatically from the
output of the speech recognizer, utterance wave-
forms, IS logs, and a small library of plan operators
describing the actions the UAV can perform. The
recognition (REC) feature group includes the posi-
tion of a hypothesis in the n-best list (nbestRank),
its length in words (hypothesisLength), and five fea-
tures representing the recognizer?s confidence as-
sessment. Similar features have been used in the
literature (e.g. (Litman et al, 2000)). The minWord-
Confidence and standard deviation/zScore features
are computed from individual word confidences in
the recognition output. We expect them to help the
machine learners decide between the different WER
classes (e.g. a high overall confidence score can
sometimes be misleading). The utterance (UTT)
feature group reflects information about the ampli-
tude in the speech signal (all features are extracted
with the UNIX sox utility). The motivation for
including the amplitude features is that they might
be useful for detecting crosstalk utterances which
are not directly spoken into the headset microphone
(e.g. the system accidentally recognizing itself).
The dialogue features (DIAL) represent informa-
tion derived from Information States and can be
coarsely divided into two sub-groups. The first
group includes features representing general co-
herence constraints on the dialogue: the dialogue
move types of the current utterance (currentDM)
and of the most active node in the ANL (mostAc-
tiveNode), the command type of the current utter-
ance (currentCommand, if it is a command, null
otherwise), statistics on which move types typi-
cally follow each other (DMBigramFrequency), and
two features (qaMatch and aqMatch) that explic-
itly encode whether the current and the previous
utterance form a valid question answer pair (e.g.
yn-question followed by yn-answer). The second
group includes features that indicate how many def-
inite NPs and pronouns cannot be resolved in the
current Information State (#unresolvedNP, #unre-
solvedPronouns, e.g. ?the car? if no car was men-
tioned before) and a feature indicating the number
of indefinite NPs that can be uniquely resolved in
the Information State (#uniqueIndefinites, e.g. ?a
tower? where there is only one tower in the do-
main). We include these features because (short)
determiners are often confused by speech recogniz-
ers. In the WITAS scenario, a misrecognized deter-
miner/demonstrative pronoun can lead to confusing
system behavior (e.g. a wrongly recognized ?there?
will cause the system to ask ?Where is that??).
Finally, the task features (TASK) reflect conflict-
ing instructions in the domain. The feature taskCon-
flict indicates a conflict if the current dialogue move
type is a command and that command already ap-
pears as an active task in the AT. #taskConstraint-
Conflict counts the number of conflicts that arise
between the currently active tasks in the AT and the
hypothesis. For example, if the UAV is already fly-
ing somewhere the preconditions of the action op-
erator for take off (altitude = 0) conflict with
those for fly (altitude 6= 0), so that ?take off?
would be an unlikely command in this context.
6.3 Learners and Selection Procedure
We use the memory based learner TiMBL (Daele-
mans et al, 2002) and the rule induction learner
RIPPER (Cohen, 1995) to predict the class of each
of the 10-best recognition hypotheses for a given ut-
terance. We chose these two learners because they
implement different learning strategies, are well es-
tablished, fast, freely available, and easy to use. In a
second step, we decide which (if any) of the classi-
fied hypotheses we actually want to pick as the best
result and how the user utterance should be classi-
fied as a whole. This task is decided by the follow-
ing selection procedure (see Figure 1) which imple-
ments a preference ordering accept > clarify > re-
ject > ignore.4
1. Scan the list of classified n-best recognition
hypotheses top-down. Return the first result
that is classified as accept and classify the
utterance as accept.
2. If 1. fails, scan the list of classified n-best
recognition hypotheses top-down. Return
the first result that is classified as clarify and
classify the utterance as clarify.
3. If 2. fails, count the number of rejects and
ignores in the classified recognition hypothe-
ses. If the number of rejects is larger or equal
than the number of ignores classify the utter-
ance as reject.
4. Else classify the utterance as ignore.
Figure 1: Selection procedure
This procedure is applied to choose from the clas-
sified n-best hypotheses for an utterance, indepen-
dent of the particular machine learner, in all of the
following experiments.
Since we have a limited amount experimental
data in this study (10 hypotheses for each of the 303
user utterances), we use a ?leave-one-out? crossval-
idation setup for classification. This means that we
classify the 10-best hypotheses for a particular ut-
terance based on the 10-best hypotheses of all 302
other utterances and repeat this 303 times.
4Note that in a dialogue application one would not always
need to classify all n-best hypotheses in order to select a result
but could stop as soon as a hypothesis is classified as correct,
which can save processing time.
7 Results and Evaluation
The middle part of Table 2 shows the classifica-
tion results for TiMBL and RIPPER when run with
default parameter settings (the other results are in-
cluded for comparison). The individual rows show
the performance when different combinations of
feature groups are used for training. The results for
the three-way classification are included for com-
parison with the baseline system and are obtained
by combining the two classes clarify and reject.
Note that we do not evaluate the performance of the
learners for classifying the individual recognition
hypotheses but the classification of (whole) user ut-
terances (i.e. including the selection procedure to
choose from the classified hypotheses).
The results show that both learners profit from
the addition of more features concerning dialogue
context and task context for classifying user speech
input appropriately. The only exception from this
trend is a slight performance decrease when task
features are added in the four-way classification for
RIPPER. Note that both learners already outperform
the baseline results even when only recognition fea-
tures are considered. The most striking result is the
performance gain for TiMBL (almost 10%) when
we include the dialogue features. As soon as dia-
logue features are included, TiMBL also performs
slightly better than RIPPER.
Note that the introduction of (limited) task fea-
tures, in addition to the DIAL and UTT features, did
not have dramatic impact in this study. One aim for
future work is to define and analyze the influence of
further task related features for classification.
7.1 Optimizing TiMBL Parameters
In all of the above experiments we ran the machine
learners with their default parameter settings.
However, recent research (Daelemans and Hoste,
2002; Marsi et al, 2003) has shown that machine
learners often profit from parameter optimization
(i.e. finding the best performing parameters on
some development data). We therefore selected
40 possible parameter combinations for TiMBL
(varying the number of nearest neighbors, feature
weighting, and class voting weights) and nested a
parameter optimization step into the ?leave-one-
out? evaluation paradigm (cf. Figure 2).5
Note that our optimization method is not as so-
phisticated as the ?Iterative Deepening? approach
5We only optimized parameters for TiMBL because it per-
formed better with default settings than RIPPER and because
the findings in (Daelemans and Hoste, 2002) indicate that
TiMBL profits more from parameter optimization.
1. Set aside the recognition hypotheses for one
of the user utterances.
2. Randomly split the remaining data into an
80% training and 20% test set.
3. Run TiMBL with all possible parameter set-
tings on the generated training and test sets
and store the best performing settings.
4. Classify the left-out hypotheses with the
recorded parameter settings.
5. Iterate.
Figure 2: Parameter optimization
described by (Marsi et al, 2003) but is similar in the
sense that it computes a best-performing parameter
setting for each data fold.
Table 3 shows the classification results when we
run TiMBL with optimized parameter settings and
using all feature groups for training.
System Behavior
User Utterance accept clarify reject ignore
in-grammar 159/2 11 16 0
out-of-grammar 0 25 5 0(WER ? 50)
out-of-grammar 6 6 50 0(WER > 50)
crosstalk 2 5 0 16
Acc/wf-score (3 classes): 86.14/86.39%
Acc/wf-score (4 classes): 82.51/83.29%
Table 3: TiMBL classification results with opti-
mized parameters
Table 3 shows a remarkable 9% improvement for
the 3-way and 4-way classification in both accuracy
and weighted f-score, compared to using TiMBL
with default parameter settings. In terms of WER,
the baseline system (cf. Table 1) accepted 233 user
utterances with a WER of 21.51%, and in contrast,
TiMBL with optimized parameters (Ti OP) only ac-
cepted 169 user utterances with a WER of 4.05%.
This low WER reflects the fact that if the machine
learning system accepts an user utterance, it is al-
most certainly the correct one. Note that although
the machine learning system in total accepted far
fewer utterances (169 vs. 233) it accepted more cor-
rect utterances than the baseline (159 vs. 154).
7.2 Evaluation
The baseline accuracy for the 3-class problem is
65.68% (61.81% weighted f-score). Our best re-
sults, obtained by using TiMBL with parameter op-
System or features used Acc/wf-score Acc/wf-score Acc/wf-score Acc/wf-score
for classification (3 classes) (4 classes) (3 classes) (4 classes)
Baseline 65.68/61.81%
TiMBL RIPPER
REC 67.66/67.51% 63.04/63.03% 69.31/69.03% 66.67/65.14%
REC+UTT 68.98/68.32% 64.03/63.08% 72.61/72.33% 70.30/68.61%
REC+UTT+DIAL 77.56/77.59% 72.94/73.70% 74.92/75.34% 71.29/71.62%
REC+UTT+DIAL+TASK 77.89/77.91% 73.27/74.12% 75.25/75.61% 70.63/71.54%
TiMBL (optimized params.) 86.14/86.39% 82.51/83.29%
Oracle 94.06/94.17% 94.06/94.18%
Table 2: Classification Results
timization, show a 25% weighted f-score improve-
ment over the baseline system.
We can compare these results to a hypothetical
?oracle? system in order to obtain an upper bound
on classification performance. This is an imagi-
nary system which performs perfectly on the ex-
perimental data given the 10-best recognition out-
put. The oracle results reveal that for 18 of the
in-grammar utterances the 10-best recognition hy-
potheses do not include the correct logical form at
all and therefore have to be classified as clarify or
reject (i.e. it is not possible to achieve 100% accu-
racy on the experimental data). Table 2 shows that
our best results are only 8%/12% (absolute) away
from the optimal performance.
7.2.1 Costs and ?2 Levels of Significance
We use the ?2 test of independence to statistically
compare the different classification results. How-
ever, since ?2 only tells us whether two classifica-
tions are different from each other, we introduce a
simple cost measure (Table 4) for the 3-way classi-
fication problem to complement the ?2 results.6
System behavior
User utterance accept reject ignore
in-grammar 0 2 2
out-of-grammar 4 2 2
crosstalk 4 2 0
Table 4: Cost measure
Table 4 captures the intuition that the correct be-
havior of a dialogue system is to accept correctly
recognized utterances and ignore crosstalk (cost 0).
The worst a system can do is to accept misrec-
ognized utterances or utterances that were not ad-
dressed to the system. The remaining classes are as-
6We only evaluate the 3-way classification problem because
there are no baseline results for the 4-way classification avail-
able.
signed a value in-between these two extremes. Note
that the cost assignment is not validated against user
judgments. We only use the costs to interpret the ?2
levels of significance (i.e. as an indicator to compare
the relative quality of different systems).
Table 5 shows the differences in cost and ?2 lev-
els of significance when we compare the classifica-
tion results. Here, Ti OP stands for TiMBL with op-
timized parameters and the stars indicate the level of
statistical significance as computed by the ?2 statis-
tics (??? indicates significance at p = .001, ?? at
p = .01, and ? at p = .05).7
Baseline RIPPER TiMBL Ti OP
Oracle ?232??? ?116??? ?100??? ?56
Ti OP ?176??? ?60? ?44
TiMBL ?132??? ?16
RIPPER ?116???
Table 5: Cost comparisons and ?2 levels of signifi-
cance for 3-way classification
The cost measure shows the strict ordering: Or-
acle < Ti OP < TiMBL < RIPPER < Baseline.
Note however that according to the ?2 test there is
no significant difference between the oracle system
and TiMBL with optimized parameters. Table 5 also
shows that all of our experiments significantly out-
perform the baseline system.
8 Conclusion
We used a combination of acoustic confidence and
pragmatic plausibility features (i.e. computed from
dialogue context) to predict the quality of incom-
ing recognition hypotheses to a multi-modal dia-
logue system. We classified hypotheses as accept,
(clarify), reject, or ignore: functional categories that
7Following (Hinton, 1995), we leave out categories with ex-
pected frequencies < 5 in the ?2 computation and reduce the
degrees of freedom accordingly.
can be used by a dialogue manager to decide appro-
priate system reactions. The approach is novel in
combining machine learning with n-best processing
for spoken dialogue systems using the Information
State Update approach.
Our best results, obtained using TiMBL with op-
timized parameters, show a 25% weighted f-score
improvement over a baseline system that uses a
?grammar-switching? approach to context-sensitive
speech recognition, and are only 8% away from the
optimal performance that can be achieved on the
data. Clearly, this improvement would result in bet-
ter dialogue system performance overall. Parameter
optimization improved the classification results by
9% compared to using the learner with default set-
tings, which shows the importance of such tuning.
Future work points in two directions: first, inte-
grating our methodology into working ISU-based
dialogue systems and determining whether or not
they improve in terms of standard dialogue eval-
uation metrics (e.g. task completion). The ISU
approach is a particularly useful testbed for our
methodology because it collects information per-
taining to dialogue context in a central data struc-
ture from which it can be easily extracted. This av-
enue will be further explored in the TALK project8.
Second, it will be interesting to investigate the im-
pact of different dialogue and task features for clas-
sification and to introduce a distinction between
?generic? features that are domain independent and
?application-specific? features which reflect proper-
ties of individual systems and application scenarios.
Acknowledgments
We thank Nuance Communications Inc. for the use
of their speech recognition and synthesis software
and Alexander Koller and Dan Shapiro for read-
ing draft versions of this paper. Oliver Lemon was
partially supported by Scottish Enterprise under the
Edinburgh-Stanford Link programme.
References
M. Boros, W. Eckert, F. Gallwitz, G. Go?rz, G. Han-
rieder, and H. Niemann. 1996. Towards Under-
standing Spontaneous Speech: Word Accuracy
vs. Concept Accuracy. In Proc. ICSLP-96.
Ananlada Chotimongkol and Alexander I. Rud-
nicky. 2001. N-best Speech Hypotheses Re-
ordering Using Linear Regression. In Proceed-
ings of EuroSpeech 2001, pages 1829?1832.
William W. Cohen. 1995. Fast Effective Rule In-
duction. In Proceedings of the 12th International
Conference on Machine Learning.
8EC FP6 IST-507802, http://www.talk-project.org
Walter Daelemans and Ve?ronique Hoste. 2002.
Evaluation of Machine Learning Methods for
Natural Language Processing Tasks. In Proceed-
ings of LREC-02.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot,
and Antal van den Bosch. 2002. TIMBL: Tilburg
Memory Based Learner, version 4.2, Reference
Guide. In ILK Technical Report 02-01.
John Dowding, Jean Mark Gawron, Doug Appelt,
John Bear, Lynn Cherny, Robert Moore, and
Douglas Moran. 1993. GEMINI: a natural lan-
guage system for spoken-language understand-
ing. In Proceedings of ACL-93.
Malte Gabsdil. 2003. Classifying Recognition Re-
sults for Spoken Dialogue Systems. In Proceed-
ings of the Student Research Workshop at ACL-
03.
Perry R. Hinton. 1995. Statistics Explained ? A
Guide For Social Science Students. Routledge.
Oliver Lemon and Alexander Gruenstein. 2004.
Multithreaded context for robust conversational
interfaces: context-sensitive speech recognition
and interpretation of corrective fragments. ACM
Transactions on Computer-Human Interaction.
(to appear).
Oliver Lemon, Alexander Gruenstein, and Stanley
Peters. 2002. Collaborative activities and multi-
tasking in dialogue systems. Traitement Automa-
tique des Langues, 43(2):131?154.
Oliver Lemon. 2004. Context-sensitive speech
recognition in ISU dialogue systems: results for
the grammar switching approach. In Proceedings
of the 8th Workshop on the Semantics and Prag-
matics of Dialogue, CATALOG?04.
Diane J. Litman, Julia Hirschberg, and Marc Swerts.
2000. Predicting Automatic Speech Recognition
Performance Using Prosodic Cues. In Proceed-
ings of NAACL-00.
Erwin Marsi, Martin Reynaert, Antal van den
Bosch, Walter Daelemans, and Ve?ronique Hoste.
2003. Learning to predict pitch accents and
prosodic boundaries in Dutch. In Proceedings of
ACL-03.
David Traum, Johan Bos, Robin Cooper, Staffan
Larsson, Ian Lewin, Colin Matheson, and Mas-
simo Poesio. 1999. A Model of Dialogue Moves
and Information State Revision. Technical Re-
port D2.1, Trindi Project.
Marilyn Walker, Jerry Wright, and Irene Langkilde.
2000. Using Natural Language Processing and
Discourse Features to Identify Understanding Er-
rors in a Spoken Dialogue System. In Proceed-
ings of ICML-2000.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 185?192,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning More Effective Dialogue Strategies Using Limited Dialogue
Move Features
Matthew Frampton and Oliver Lemon
HCRC, School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, UK
M.J.E.Frampton@sms.ed.ac.uk, olemon@inf.ed.ac.uk
Abstract
We explore the use of restricted dialogue
contexts in reinforcement learning (RL)
of effective dialogue strategies for infor-
mation seeking spoken dialogue systems
(e.g. COMMUNICATOR (Walker et al,
2001)). The contexts we use are richer
than previous research in this area, e.g.
(Levin and Pieraccini, 1997; Scheffler and
Young, 2001; Singh et al, 2002; Pietquin,
2004), which use only slot-based infor-
mation, but are much less complex than
the full dialogue ?Information States? ex-
plored in (Henderson et al, 2005), for
which tractabe learning is an issue. We
explore how incrementally adding richer
features allows learning of more effective
dialogue strategies. We use 2 user simu-
lations learned from COMMUNICATOR
data (Walker et al, 2001; Georgila et al,
2005b) to explore the effects of differ-
ent features on learned dialogue strategies.
Our results show that adding the dialogue
moves of the last system and user turns
increases the average reward of the auto-
matically learned strategies by 65.9% over
the original (hand-coded) COMMUNI-
CATOR systems, and by 7.8% over a base-
line RL policy that uses only slot-status
features. We show that the learned strate-
gies exhibit an emergent ?focus switch-
ing? strategy and effective use of the ?give
help? action.
1 Introduction
Reinforcement Learning (RL) applied to the prob-
lem of dialogue management attempts to find op-
timal mappings from dialogue contexts to sys-
tem actions. The idea of using Markov Deci-
sion Processes (MDPs) and reinforcement learn-
ing to design dialogue strategies for dialogue sys-
tems was first proposed by (Levin and Pierac-
cini, 1997). There, and in subsequent work such
as (Singh et al, 2002; Pietquin, 2004; Scheffler
and Young, 2001), only very limited state infor-
mation was used in strategy learning, based al-
ways on the number and status of filled informa-
tion slots in the application (e.g. departure-city is
filled, destination-city is unfilled). This we refer to
as low-level contextual information. Much prior
work (Singh et al, 2002) concentrated only on
specific strategy decisions (e.g. confirmation and
initiative strategies), rather than the full problem
of what system dialogue move to take next.
The simple strategies learned for low-level def-
initions of state cannot be sensitive to (sometimes
critical) aspects of the dialogue context, such as
the user?s last dialogue move (DM) (e.g. request-
help) unless that move directly affects the status of
an information slot (e.g. provide-info(destination-
city)). We refer to additional contextual infor-
mation such as the system and user?s last di-
alogue moves as high-level contextual informa-
tion. (Frampton and Lemon, 2005) learned full
strategies with limited ?high-level? information
(i.e. the dialogue move(s) of the last user utter-
ance) and only used a stochastic user simulation
whose probabilities were supplied via common-
sense and intuition, rather than learned from data.
This paper uses data-driven n-gram user simula-
tions (Georgila et al, 2005a) and a richer dialogue
context.
On the other hand, increasing the size of the
state space for RL has the danger of making
the learning problem intractable, and at the very
least means that data is more sparse and state ap-
proximation methods may need to be used (Hen-
derson et al, 2005). To date, the use of very
large state spaces relies on a ?hybrid? super-
vised/reinforcement learning technique, where the
reinforcement learning element has not yet been
shown to significantly improve policies over the
purely supervised case (Henderson et al, 2005).
185
The extended state spaces that we propose are
based on theories of dialogue such as (Clark, 1996;
Searle, 1969; Austin, 1962; Larsson and Traum,
2000), where which actions a dialogue participant
can or should take next are not based solely on
the task-state (i.e. in our domain, which slots are
filled), but also on wider contextual factors such
as a user?s dialogue moves or speech acts. In
future work we also intend to use feature selec-
tion techniques (e.g. correlation-based feature sub-
set (CFS) evaluation (Rieser and Lemon, 2006))
on the COMMUNICATOR data (Georgila et al,
2005a; Walker et al, 2001) in order to identify ad-
ditional context features that it may be effective to
represent in the state.
1.1 Methodology
To explore these issues we have developed a Re-
inforcement Learning (RL) program to learn di-
alogue strategies while accurate simulated users
(Georgila et al, 2005a) converse with a dialogue
manager. See (Singh et al, 2002; Scheffler and
Young, 2001) and (Sutton and Barto, 1998) for a
detailed description of Markov Decision Processes
and the relevant RL algorithms.
In dialogue management we are faced with the
problem of deciding which dialogue actions it is
best to perform in different states. We use (RL) be-
cause it is a method of learning by delayed reward
using trial-and-error search. These two proper-
ties appear to make RL techniques a good fit with
the problem of automatically optimising dialogue
strategies, because in task-oriented dialogue of-
ten the ?reward? of the dialogue (e.g. successfully
booking a flight) is not obtainable immediately,
and the large space of possible dialogues for any
task makes some degree of trial-and-error explo-
ration necessary.
We use both 4-gram and 5-gram user sim-
ulations for testing and for training (i.e. train
with 4-gram, test with 5-gram, and vice-versa).
These simulations also simulate ASR errors since
the probabilities are learned from recognition hy-
potheses and system behaviour logged in the
COMMUNICATOR data (Walker et al, 2001) fur-
ther annotated with speech acts and contexts by
(Georgila et al, 2005b). Here the task domain is
flight-booking, and the aim for the dialogue man-
ager is to obtain values for the user?s flight infor-
mation ?slots? i.e. departure city, destination city,
departure date and departure time, before making
a database query. We add the dialogue moves of
the last user and system turns as context features
and use these in strategy learning. We compare
the learned strategies to 2 baselines: the original
COMMUNICATOR systems and an RL strategy
which uses only slot status features.
1.2 Outline
Section 2 contains a description of our basic ex-
perimental framework, and a detailed description
of the reinforcement learning component and user
simulations. Sections 3 and 4 describe the experi-
ments and analyse our results, and in section 5 we
conclude and suggest future work.
2 The Experimental Framework
Each experiment is executed using the DIPPER
Information State Update dialogue manager (Bos
et al, 2003) (which here is used to track and up-
date dialogue context rather than deciding which
actions to take), a Reinforcement Learning pro-
gram (which determines the next dialogue action
to take), and various user simulations. In sections
2.3 and 2.4 we give more details about the rein-
forcement learner and user simulations.
2.1 The action set for the learner
Below is a list of all the different actions that the
RL dialogue manager can take and must learn to
choose between based on the context:
1. An open question e.g. ?How may I help you??
2. Ask the value for any one of slots 1...n.
3. Explicitly confirm any one of slots 1...n.
4. Ask for the nth slot whilst implicitly confirm-
ing1 either slot value n? 1 e.g. ?So you want
to fly from London to where??, or slot value
n + 1
5. Give help.
6. Pass to human operator.
7. Database Query.
There are a couple of restrictions regarding
which actions can be taken in which states: an
open question is only available at the start of the
dialogue, and the dialogue manager can only try
to confirm non-empty slots.
2.2 The Reward Function
We employ an ?all-or-nothing? reward function
which is as follows:
1. Database query, all slots confirmed: +100
2. Any other database query: ?75
1Where n = 1 we implicitly confirm the final slot andwhere n = 4 we implicitly confirm the first slot. This action
set does not include actions that ask the nth slot whilst im-plicitly confirming slot value n ? 2. These will be added infuture experiments as we continue to increase the action andstate space.
186
3. User simulation hangs-up: ?100
4. DIPPER passes to a human operator: ?50
5. Each system turn: ?5
To maximise the chances of a slot value be-
ing correct, it must be confirmed rather than just
filled. The reward function reflects the fact that
a successful dialogue manager must maximise its
chances of getting the slots correct i.e. they must
all be confirmed. (Walker et al, 2000) showed
with the PARADISE evaluation that confirming
slots increases user satisfaction.
The maximum reward that can be obtained for
a single dialogue is 85, (the dialogue manager
prompts the user, the user replies by filling all four
of the slots in a single utterance, and the dialogue
manager then confirms all four slots and submits a
database query).
2.3 The Reinforcement Learner?s Parameters
When the reinforcement learner agent is initial-
ized, it is given a parameter string which includes
the following:
1. Step Parameter: ? = decreasing
2. Discount Factor: ? = 1
3. Action Selection Type = softmax (alternative
is ?-greedy)
4. Action Selection Parameter: temperature =
15
5. Eligibility Trace Parameter: ? = 0.9
6. Eligibility Trace = replacing (alternative is
accumulating)
7. Initial Q-values = 25
The reinforcement learner updates its Q-values
using the Sarsa(?) algorithm (see (Sutton and
Barto, 1998)). The first parameter is the step-
parameter ? which may be a value between 0 and
1, or specified as decreasing. If it is decreasing, as
it is in our experiments, then for any given Q-value
update ? is 1k where k is the number of times thatthe state-action pair for which the update is be-
ing performed has been visited. This kind of step
parameter will ensure that given a sufficient num-
ber of training dialogues, each of the Q-values will
eventually converge. The second parameter (dis-
count factor) ? may take a value between 0 and 1.
For the dialogue management problem we set it to
1 so that future rewards are taken into account as
strongly as possible.
Apart from updating Q-values, the reinforce-
ment learner must also choose the next action
for the dialogue manager and the third parameter
specifies whether it does this by ?-greedy or soft-
max action selection (here we have used softmax).
The fifth parameter, the eligibility trace param-
eter ?, may take a value between 0 and 1, and the
sixth parameter specifies whether the eligibility
traces are replacing or accumulating. We used re-
placing traces because they produced faster learn-
ing for the slot-filling task. The seventh parameter
is for supplying the initial Q-values.
2.4 N-Gram User Simulations
Here user simulations, rather than real users, inter-
act with the dialogue system during learning. This
is because thousands of dialogues may be neces-
sary to train even a simple system (here we train
on up to 50000 dialogues), and for a proper explo-
ration of the state-action space the system should
sometimes take actions that are not optimal for the
current situation, making it a sadistic and time-
consuming procedure for any human training the
system. (Eckert et al, 1997) were the first to
use a user simulation for this purpose, but it was
not goal-directed and so could produce inconsis-
tent utterances. The later simulations of (Pietquin,
2004) and (Scheffler and Young, 2001) were to
some extent ?goal-directed? and also incorporated
an ASR error simulation. The user simulations in-
teract with the system via intentions. Intentions
are preferred because they are easier to generate
than word sequences and because they allow er-
ror modelling of all parts of the system, for exam-
ple ASR error modelling and semantic errors. The
user and ASR simulations must be realistic if the
learned strategy is to be directly applicable in a
real system.
The n-gram user simulations used here (see
(Georgila et al, 2005a) for details and evaluation
results) treat a dialogue as a sequence of pairs of
speech acts and tasks. They take as input the n?1
most recent speech act-task pairs in the dialogue
history, and based on n-gram probabilities learned
from the COMMUNICATOR data (automatically
annotated with speech acts and Information States
(Georgila et al, 2005b)), they then output a user
utterance as a further speech-act task pair. These
user simulations incorporate the effects of ASR er-
rors since they are built from the user utterances
as they were recognized by the ASR components
of the original COMMUNICATOR systems. Note
that the user simulations do not provide instanti-
ated slot values e.g. a response to provide a des-
tination city is the speech-act task pair ?[provide
info] [dest city]?. We cannot assume that two such
responses in the same dialogue refer to the same
187
destination cities. Hence in the dialogue man-
ager?s Information State where we record whether
a slot is empty, filled, or confirmed, we only up-
date from filled to confirmed when the slot value
is implicitly or explicitly confirmed. An additional
function maps the user speech-act task pairs to a
form that can be interpreted by the dialogue man-
ager. Post-mapping user responses are made up of
one or more of the following types of utterance:
(1) Stay quiet, (2) Provide 1 or more slot values,
(3) Yes, (4) No, (5) Ask for help, (6) Hang-up, (7)
Null (out-of-domain or no ASR hypothesis).
The quality of the 4 and 5-gram user sim-
ulations has been established through a variety
of metrics and against the behaviour of the ac-
tual users of the COMMUNICATOR systems, see
(Georgila et al, 2005a).
2.4.1 Limitations of the user simulations
The user and ASR simulations are a fundamen-
tally important factor in determining the nature of
the learned strategies. For this reason we should
note the limitations of the n-gram simulations used
here. A first limitation is that we cannot be sure
that the COMMUNICATOR training data is suffi-
ciently complete, and a second is that the n-gram
simulations only use a window of n moves in
the dialogue history. This second limitation be-
comes a problem when the user simulation?s cur-
rent move ought to take into account something
that occurred at an earlier stage in the dialogue.
This might result in the user simulation repeating a
slot value unnecessarily, or the chance of an ASR
error for a particular word being independent of
whether the same word was previously recognised
correctly. The latter case means we cannot sim-
ulate for example, a particular slot value always
being liable to misrecognition. These limitations
will affect the nature of the learned strategies. Dif-
ferent state features may assume more or less im-
portance than they would if the simulations were
more realistic. This is a point that we will return to
in the analysis of the experimental results. In fu-
ture work we will use the more accurate user sim-
ulations recently developed following (Georgila et
al., 2005a) and we expect that these will improve
our results still further.
3 Experiments
First we learned strategies with the 4-gram user
simulation and tested with the 5-gram simula-
tion, and then did the reverse. We experimented
with different feature sets, exploring whether bet-
ter strategies could be learned by adding limited
context features. We used two baselines for com-
parison:
? The performance of the original COMMUNI-
CATOR systems in the data set (Walker et al,
2001).
? An RL baseline dialogue manager learned
using only slot-status features i.e. for each
of slots 1? 4, is the slot empty, filled or con-
firmed?
We then learned two further strategies:
? Strategy 2 (UDM) was learned by adding the
user?s last dialogue move to the state.
? Strategy 3 (USDM) was learned by adding
both the user and system?s last dialogue
moves to the state.
The possible system and user dialogue moves
were those given in sections 2.1 and 2.4 respec-
tively, and the reward function was that described
in section 2.2.
3.1 The COMMUNICATOR data baseline
We computed the scores for the original hand-
coded COMMUNICATOR systems as was done
by (Henderson et al, 2005), and we call this the
?HLG05? score. This scoring function is based
on task completion and dialogue length rewards as
determined by the PARADISE evaluation (Walker
et al, 2000). This function gives 25 points for
each slot which is filled, another 25 for each that
is confirmed, and deducts 1 point for each sys-
tem action. In this case the maximum possible
score is 197 i.e. 200 minus 3 actions, (the sys-
tem prompts the user, the user replies by filling all
four of the slots in one turn, and the system then
confirms all four slots and offers the flight). The
average score for the 1242 dialogues in the COM-
MUNICATOR dataset where the aim was to fill
and confirm only the same four slots as we have
used here was 115.26. The other COMMUNICA-
TOR dialogues involved different slots relating to
return flights, hotel-bookings and car-rentals.
4 Results
Figure 1 tracks the improvement of the 3 learned
strategies for 50000 training dialogues with the 4-
gram user simulation, and figure 2 for 50000 train-
ing dialogues with the 5-gram simulation. They
show the average reward (according to the func-
tion of section 2.2) obtained by each strategy over
intervals of 1000 training dialogues.
Table 1 shows the results for testing the strate-
gies learned after 50000 training dialogues (the
baseline RL strategy, strategy 2 (UDM) and strat-
egy 3 (USDM)). The ?a? strategies were trained
with the 4-gram user simulation and tested with
188
Features Av. Score HLG05 Filled Slots Conf. Slots Length
4 ? 5 gram = (a)
RL Baseline (a) Slots status 51.67 190.32 100 100 ?9.68
RL Strat 2, UDM (a) + Last User DM 53.65** 190.67 100 100 ?9.33
RL Strat 3, USDM (a) + Last System DM 54.9** 190.98 100 100 ?9.02
5 ? 4 gram = (b)
RL Baseline (b) Slots status 51.4 190.28 100 100 ?9.72
RL Strat 2, UDM (b) + Last User DM 54.46* 190.83 100 100 ?9.17
RL Strat 3, USDM (b) + Last System DM 56.24** 191.25 100 100 ?8.75
RL Baseline (av) Slots status 51.54 190.3 100 100 ?9.7
RL Strat 2, UDM (av) + Last User DM 54.06** 190.75 100 100 ?9.25
RL Strat 3, USDM (av) + Last System DM 55.57** 191.16 100 100 ?8.84
COMM Systems 115.26 84.6 63.7 ?33.1
Hybrid RL *** Information States 142.6 88.1 70.9 ?16.4
Table 1: Testing the learned strategies after 50000 training dialogues, average reward achieved per dia-
logue over 1000 test dialogues. (a) = strategy trained using 4-gram and tested with 5-gram; (b) = strategy
trained with 5-gram and tested with 4-gram; (av) = average; * significance level p < 0.025; ** signifi-
cance level p < 0.005; *** Note: The Hybrid RL scores (here updated from (Henderson et al, 2005))
are not directly comparable since that system has a larger action set and fewer policy constraints.
the 5-gram, while the ?b? strategies were trained
with the 5-gram user simulation and tested with
the 4-gram. The table also shows average scores
for the strategies. Column 2 contains the average
reward obtained per dialogue by each strategy over
1000 test dialogues (computed using the function
of section 2.2).
The 1000 test dialogues for each strategy were
divided into 10 sets of 100. We carried out t-tests
and found that in both the ?a? and ?b? cases, strat-
egy 2 (UDM) performs significantly better than
the RL baseline (significance levels p < 0.005
and p < 0.025), and strategy 3 (USDM) performs
significantly better than strategy 2 (UDM) (signif-
icance level p < 0.005). With respect to average
performance, strategy 2 (UDM) improves over the
RL baseline by 4.9%, and strategy 3 (USDM) im-
proves by 7.8%. Although there seem to be only
negligible qualitative differences between strate-
gies 2(b) and 3(b) and their ?a? equivalents, the
former perform slightly better in testing. This sug-
gests that the 4-gram simulation used for testing
the ?b? strategies is a little more reliable in filling
and confirming slot values than the 5-gram.
The 3rd column ?HLG05? shows the average
scores for the dialogues as computed by the re-
ward function of (Henderson et al, 2005). This is
done for comparison with that work but also with
the COMMUNICATOR data baseline. Using the
HLG05 reward function, strategy 3 (USDM) im-
proves over the original COMMUNICATOR sys-
tems baseline by 65.9%. The components making
up the reward are shown in the final 3 columns
of table 1. Here we see that all of the RL strate-
gies are able to fill and confirm all of the 4 slots
when conversing with the simulated COMMUNI-
CATOR users. The only variation is in the aver-
age length of dialogue required to confirm all four
slots. The COMMUNICATOR systems were of-
ten unable to confirm or fill all of the user slots,
and the dialogues were quite long on average. As
stated in section 2.4.1, the n-gram simulations do
not simulate the case of a particular user goal ut-
terance being unrecognisable for the system. This
was a problem that could be encountered by the
real COMMUNICATOR systems.
Nevertheless, the performance of all the learned
strategies compares very well to the COMMUNI-
CATOR data baseline. For example, in an average
dialogue, the RL strategies filled and confirmed all
four slots with around 9 actions not including of-
fering the flight, but the COMMUNICATOR sys-
tems took an average of around 33 actions per di-
alogue, and often failed to complete the task.
With respect to the hybrid RL result of (Hen-
derson et al, 2005), shown in the final row of the
table, Strategy 3 (USDM) shows a 34% improve-
ment, though these results are not directly compa-
rable because that system uses a larger action set
and has fewer constraints (e.g. it can ask ?how may
I help you?? at any time, not just at the start of a
dialogue).
Finally, let us note that the performance of the
RL strategies is close to optimal, but that there is
some room for improvement. With respect to the
HLG05 metric, the optimal system score would be
197, but this would only be available in rare cases
where the simulated user supplies all 4 slots in the
189
-120
-100
-80
-60
-40
-20
 0
 20
 40
 0  5  10 15 20 25 30 35 40 45 50
Av
er
ag
e 
Re
wa
rd
Number of Dialogues (Thousands)
Training With 4-gram
Baseline
Strategy 2
Strategy 3
Figure 1: Training the dialogue strategies with the
4-gram user simulation
first utterance. With respect to the metric we have
used here (with a ?5 per system turn penalty), the
optimal score is 85 (and we currently score an av-
erage of 55.57). Thus we expect that there are
still further improvments that can be made to more
fully exploit the dialogue context (see section 4.3).
4.1 Qualitative Analysis
Below are a list of general characteristics of the
learned strategies:
1. The reinforcement learner learns to query the
database only in states where all four slots
have been confirmed.
2. With sufficient exploration, the reinforce-
ment learner learns not to pass the call to a
human operator in any state.
3. The learned strategies employ implicit confir-
mations wherever possible. This allows them
to fill and confirm the slots in fewer turns than
if they simply asked the slot values and then
used explicit confirmation.
4. As a result of characteristic 3, which slots
can be asked and implicitly confirmed at the
same time influences the order in which the
learned strategies attempt to fill and confirm
each slot, e.g. if the status of the third slot is
?filled? and the others are ?empty?, the learner
learns to ask for the second or fourth slot
-120
-100
-80
-60
-40
-20
 0
 20
 40
 0  5  10 15 20 25 30 35 40 45 50
Av
er
ag
e 
Re
wa
rd
Number of Dialogues (Thousands)
Training With 5-gram
Baseline
Strategy 2
Strategy 3
Figure 2: Training the dialogue strategies with the
5-gram user simulation
rather than the first, since it can implicitly
confirm the third while it asks for the second
or fourth slots, but it cannot implicitly con-
firm the third while it asks for the first slot.
This action is not available (see section 2.1).
4.2 Emergent behaviour
In testing the UDM strategy (2) filled and con-
firmed all of the slots in fewer turns on aver-
age than the RL baseline, and strategy 3 (USDM)
did this in fewer turns than strategy 2 (UDM).
What then were the qualitative differences be-
tween the three strategies? The behaviour of the
three strategies only seems to really deviate when
a user response fails to fill or confirm one or more
slots. Then the baseline strategy?s state has not
changed and so it will repeat its last dialogue
move, whereas the state for strategies 2 (UDM)
and 3 (USDM) has changed and as a result, these
may now try different actions. It is in such circum-
stances that the UDM strategy seems to be more
effective than the baseline, and strategy 3 (USDM)
more effective than the UDM strategy. In figure 3
we show illustrative state and learned action pairs
for the different strategies. They relate to a sit-
uation where the first user response(s) in the di-
alogue has/have failed to fill a single slot value.
NB: here ?emp? stands for ?empty? and ?fill? for
?filled? and they appear in the first four state vari-
ables, which stand for slot states. For strategy 2
(UDM), the fifth variable represents the user?s last
190
dialogue move, and the for strategy 3 (USDM), the
fifth variable represents the system?s last dialogue
move, and the sixth, the user?s last dialogue move.
BASELINE STRATEGY
State:
[emp,emp,emp,emp]
Action: askSlot2
STRATEGY 2 (UDM)
State:
[emp,emp,emp,emp,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,user(null)]
Action: askSlot1
STRATEGY 3 (USDM)
State:
[emp,emp,emp,emp,askSlot3,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,askSlot3,user(null)]
Action: giveHelp
State:
[emp,emp,emp,emp,giveHelp,user(quiet)]
Action: askSlot3
State:
[emp,emp,emp,emp,giveHelp,user(null)]
Action: askSlot3
Figure 3: Examples of the different learned strate-
gies and emergent behaviours: focus switching
(for UDM) and giving help (for USDM)
Here we can see that should the user responses
continue to fail to provide a slot value, the base-
line?s state will be unchanged and so the strategy
will simply ask for slot 2 again. The state for strat-
egy 2 (UDM) does change however. This strategy
switches focus between slots 3 and 1 depending on
whether the user?s last dialogue move was ?null? or
?quiet? NB. As stated in section 2.4, ?null? means
out-of-domain or that there was no ASR hypothe-
sis. Strategy 3 (USDM) is different again. Knowl-
edge of the system?s last dialogue move as well
as the user?s last move has enabled the learner to
make effective use of the ?give help? action, rather
than to rely on switching focus. When the user?s
last dialogue move is ?null? in response to the sys-
tem move ?askSlot3?, then the strategy uses the
?give help? action before returning to ask for slot 3
again. The example described here is not the only
example of strategy 2 (UDM) employing focus
switching while strategy 3 (USDM) prefers to use
the ?give help? action when a user response fails
to fill or confirm a slot. This kind of behaviour in
strategies 2 and 3 is emergent dialogue behaviour
that has been learned by the system rather than ex-
plicitly programmed.
4.3 Further possibilities for improvement
over the RL baseline
Further improvements over the RL baseline might
be possible with a wider set of system actions.
Strategies 2 and 3 may learn to make more ef-
fective use of additional actions than the baseline
e.g. additional actions that implicitly confirm one
slot whilst asking another may allow more of the
switching focus described in section 4.1. Other
possible additional actions include actions that ask
for or confirm two or more slots simultaneously.
In section 2.4.1, we highlighted the fact that the
n-gram user simulations are not completely real-
istic and that this will make certain state features
more or less important in learning a strategy. Thus
had we been able to use even more realistic user
simulations, including certain additional context
features in the state might have enabled a greater
improvement over the baseline. Dialogue length
is an example of a feature that could have made
a difference had the simulations been able to sim-
ulate the case of a particular goal utterance being
unrecognisable for the system. The reinforcement
learner may then be able to use the dialogue length
feature to learn when to give up asking for a par-
ticular slot value and make a partially complete
database query. This would of course require a
reward function that gave some reward to partially
complete database queries rather than the all-or-
nothing reward function used here.
5 Conclusion and Future Work
We have used user simulations that are n-gram
models learned from COMMUNICATOR data to
explore reinforcement learning of full dialogue
strategies with some ?high-level? context infor-
mation (the user and and system?s last dialogue
moves). Almost all previous work (e.g. (Singh
et al, 2002; Pietquin, 2004; Scheffler and Young,
2001)) has included only low-level information
in state representations. In contrast, the explo-
ration of very large state spaces to date relies on a
?hybrid? supervised/reinforcement learning tech-
nique, where the reinforcement learning element
has not been shown to significantly improve poli-
cies over the purely supervised case (Henderson et
al., 2005).
We presented our experimental environment,
the reinforcement learner, the simulated users,
and our methodology. In testing with the sim-
ulated COMMUNICATOR users, the new strate-
gies learned with higher-level (i.e. dialogue move)
information in the state outperformed the low-
level RL baseline (only slot status information)
191
by 7.8% and the original COMMUNICATOR sys-
tems by 65.9%. These strategies obtained more
reward than the RL baseline by filling and con-
firming all of the slots with fewer system turns on
average. Moreover, the learned strategies show
interesting emergent dialogue behaviour such as
making effective use of the ?give help? action and
switching focus to different subtasks when the cur-
rent subtask is proving problematic.
In future work, we plan to use even more realis-
tic user simulations, for example those developed
following (Georgila et al, 2005a), which incorpo-
rate elements of goal-directed user behaviour. We
will continue to investigate whether we can main-
tain tractability and learn superior strategies as we
add incrementally more high-level contextual in-
formation to the state. At some stage this may
necessitate using a generalisation method such as
linear function approximation (Henderson et al,
2005). We also intend to use feature selection
techniques (e.g. CFS subset evaluation (Rieser and
Lemon, 2006)) on in order to determine which
contextual features this suggests are important.
We will also carry out a more direct comparison
with the hybrid strategies learned by (Henderson
et al, 2005). In the slightly longer term, we will
test our learned strategies on humans using a full
spoken dialogue system. We hypothesize that the
strategies which perform the best in terms of task
completion and user satisfaction scores (Walker et
al., 2000) will be those learned with high-level di-
alogue context information in the state.
Acknowledgements
This work is supported by the ESRC and the TALK
project, www.talk-project.org.
References
John L. Austin. 1962. How To Do Things With Words.Oxford University Press.
Johan Bos, Ewan Klein, Oliver Lemon, and TetsushiOka. 2003. Dipper: Description and formalisationof an information-state update dialogue system ar-
chitecture. In 4th SIGdial Workshop on Discourse
and Dialogue, Sapporo.
Herbert H. Clark. 1996. Using Language. CambridgeUniversity Press.
Weiland Eckert, Esther Levin, and Roberto Pieraccini.1997. User modeling for spoken dialogue system
evaluation. In IEEE Workshop on Automatic Speech
Recognition and Understanding.
Matthew Frampton and Oliver Lemon. 2005. Rein-forcement Learning Of Dialogue Strategies UsingThe User?s Last Dialogue Act. In IJCAI workshop
on Knowledge and Reasoning in Practical Dialogue
Systems.
Kallirroi Georgila, James Henderson, and OliverLemon. 2005a. Learning User Simulations for In-formation State Update Dialogue Systems. In In-
terspeech/Eurospeech: the 9th biennial conference
of the International Speech Communication Associ-
ation.
Kallirroi Georgila, Oliver Lemon, and James Hender-son. 2005b. Automatic annotation of COMMUNI-
CATOR dialogue data for learning dialogue strate-gies and user simulations. In Ninth Workshop on the
Semantics and Pragmatics of Dialogue (SEMDIAL:
DIALOR).
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2005. Hybrid Reinforcement/SupervisedLearning for Dialogue Policies from COMMUNI-CATOR data. In IJCAI workshop on Knowledge and
Reasoning in Practical Dialogue Systems,.
Staffan Larsson and David Traum. 2000. Information
state and dialogue management in the TRINDI Dia-logue Move Engine Toolkit. Natural Language En-
gineering, 6(3-4):323?340.
Esther Levin and Roberto Pieraccini. 1997. Astochastic model of computer-human interactionfor learning dialogue strategies. In Eurospeech,
Rhodes,Greece.
Olivier Pietquin. 2004. A Framework for Unsuper-
vised Learning of Dialogue Strategies. Presses Uni-versitaires de Louvain, SIMILAR Collection.
Verena Rieser and Oliver Lemon. 2006. Using ma-chine learning to explore human multimodal clarifi-cation strategies. In Proc. ACL.
Konrad Scheffler and Steve Young. 2001. Corpus-based dialogue simulation for automatic strategy
learning and evaluation. In NAACL-2001 Work-
shop on Adaptation in Dialogue Systems, Pittsburgh,USA.
John R. Searle. 1969. Speech Acts. Cambridge Uni-versity Press.
Satinder Singh, Diane Litman, Michael Kearns, andMarilyn Walker. 2002. Optimizing dialogue man-
agement with reinforcement learning: Experimentswith the NJFun system. Journal of Artificial Intelli-
gence Research (JAIR).
Richard Sutton and Andrew Barto. 1998. Reinforce-
ment Learning. MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J.Litman. 2000. Towards Developing General Mod-els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
Marilyn A. Walker, Rebecca J. Passonneau, and
Julie E. Boland. 2001. Quantitative and Qualita-tive Evaluation of Darpa Communicator Spoken Di-alogue Systems. In Meeting of the Association for
Computational Linguistics, pages 515?522.
192
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 659?666,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using Machine Learning to Explore Human Multimodal Clarification
Strategies
Verena Rieser
Department of Computational Linguistics
Saarland University
Saarbru?cken, D-66041
vrieser@coli.uni-sb.de
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
olemon@inf.ed.ac.uk
Abstract
We investigate the use of machine learn-
ing in combination with feature engineer-
ing techniques to explore human multi-
modal clarification strategies and the use
of those strategies for dialogue systems.
We learn from data collected in a Wizard-
of-Oz study where different wizards could
decide whether to ask a clarification re-
quest in a multimodal manner or else use
speech alone. We show that there is a
uniform strategy across wizards which is
based on multiple features in the context.
These are generic runtime features which
can be implemented in dialogue systems.
Our prediction models achieve a weighted
f-score of 85.3% (which is a 25.5% im-
provement over a one-rule baseline). To
assess the effects of models, feature dis-
cretisation, and selection, we also conduct
a regression analysis. We then interpret
and discuss the use of the learnt strategy
for dialogue systems. Throughout the in-
vestigation we discuss the issues arising
from using small initial Wizard-of-Oz data
sets, and we show that feature engineer-
ing is an essential step when learning from
such limited data.
1 Introduction
Good clarification strategies in dialogue systems
help to ensure and maintain mutual understand-
ing and thus play a crucial role in robust conversa-
tional interaction. In dialogue application domains
with high interpretation uncertainty, for example
caused by acoustic uncertainties from a speech
recogniser, multimodal generation and input leads
to more robust interaction (Oviatt, 2002) and re-
duced cognitive load (Oviatt et al, 2004). In this
paper we investigate the use of machine learning
(ML) to explore human multimodal clarification
strategies and the use of those strategies to decide,
based on the current dialogue context, when a di-
alogue system?s clarification request (CR) should
be generated in a multimodal manner.
In previous work (Rieser and Moore, 2005)
we showed that for spoken CRs in human-
human communication people follow a context-
dependent clarification strategy which systemati-
cally varies across domains (and even across Ger-
manic languages). In this paper we investigate
whether there exists a context-dependent ?intu-
itive? human strategy for multimodal CRs as well.
To test this hypothesis we gathered data in a
Wizard-of-Oz (WOZ) study, where different wiz-
ards could decide when to show a screen output.
From this data we build prediction models, using
supervised learning techniques together with fea-
ture engineering methods, that may explain the un-
derlying process which generated the data. If we
can build a model which predicts the data quite re-
liably, we can show that there is a uniform strategy
that the majority of our wizards followed in certain
contexts.
Figure 1: Methodology and structure
The overall method and corresponding structure
of the paper is as shown in figure 1. We proceed
659
as follows. In section 2 we present the WOZ cor-
pus from which we extract a potential context us-
ing ?Information State Update? (ISU)-based fea-
tures (Lemon et al, 2005), listed in section 3. We
also address the question how to define a suit-
able ?local? context definition for the wizard ac-
tions. We apply the feature engineering methods
described in section 4 to address the questions of
unique thresholds and feature subsets across wiz-
ards. These techniques also help to reduce the
context representation and thus the feature space
used for learning. In section 5 we test different
classifiers upon this reduced context and separate
out the independent contribution of learning al-
gorithms and feature engineering techniques. In
section 6 we discuss and interpret the learnt strat-
egy. Finally we argue for the use of reinforcement
learning to optimise the multimodal clarification
strategy.
2 The WOZ Corpus
The corpus we are using for learning was col-
lected in a multimodal WOZ study of German
task-oriented dialogues for an in-car music player
application, (Kruijff-Korbayova? et al, 2005) . Us-
ing data from a WOZ study, rather than from real
system interactions, allows us to investigate how
humans clarify. In this study six people played the
role of an intelligent interface to an MP3 player
and were given access to a database of informa-
tion. 24 subjects were given a set of predefined
tasks to perform using an MP3 player with a mul-
timodal interface. In one part of the session the
users also performed a primary driving task, us-
ing a driving simulator. The wizards were able
to speak freely and display the search results or
the playlist on the screen by clicking on vari-
ous pre-computed templates. The users were also
able to speak, as well as make selections on the
screen. The user?s utterances were immediately
transcribed by a typist. The transcribed user?s
speech was then corrupted by deleting a varying
number of words, simulating understanding prob-
lems at the acoustic level. This (sometimes) cor-
rupted transcription was then presented to the hu-
man wizard. Note that this environment introduces
uncertainty on several levels, for example multiple
matches in the database, lexical ambiguities, and
errors on the acoustic level, as described in (Rieser
et al, 2005). Whenever the wizard produced a
CR, the experiment leader invoked a questionnaire
window on a GUI, where the wizard classified
their CR according to the primary source of the
understanding problem, mapping to the categories
defined by (Traum and Dillenbourg, 1996).
2.1 The Data
The corpus gathered with this setup comprises
70 dialogues, 1772 turns and 17076 words. Ex-
ample 1 shows a typical multimodal clarification
sub-dialogue, 1 concerning an uncertain reference
(note that ?Venus? is an album name, song title,
and an artist name), where the wizard selects a
screen output while asking a CR.
(1) User: Please play ?Venus?.
Wizard: Does this list contain the song?
[shows list with 20 DB matches]
User: Yes. It?s number 4. [clicks on item 4]
For each session we gathered logging informa-
tion which consists of e.g., the transcriptions of
the spoken utterances, the wizard?s database query
and the number of results, the screen option cho-
sen by the wizard, classification of CRs, etc. We
transformed the log-files into an XML structure,
consisting of sessions per user, dialogues per task,
and turns.2
2.2 Data analysis:
Of the 774 wizard turns 19.6% were annotated
as CRs, resulting in 152 instances for learning,
where our six wizards contributed about equal
proportions. A ?2 test on multimodal strategy
(i.e. showing a screen output or not with a CR)
showed significant differences between wizards
(?2(1) = 34.21, p < .000). On the other hand, a
Kruskal-Wallis test comparing user preference for
the multimodal output showed no significant dif-
ference across wizards (H(5)=10.94, p > .05). 3
Mean performance ratings for the wizards? multi-
modal behaviour ranged from 1.67 to 3.5 on a five-
point Likert scale. Observing significantly differ-
ent strategies which are not significantly different
in terms of user satisfaction, we conjecture that the
wizards converged on strategies which were ap-
propriate in certain contexts. To strengthen this
1Translated from German.
2Where a new ?turn? begins at the start of each new user
utterance after a wizard utterance, taking the user utterance as
a most basic unit of dialogue progression as defined in (Paek
and Chickering, 2005).
3The Kruskal-Wallis test is the non-parametric equivalent
to a one-way ANOVA. Since the users indicated their satis-
faction on a 5-point likert scale, an ANOVA which assumes
normality would be invalid.
660
hypothesis we split the data by wizard and and per-
formed a Kruskal-Wallis test on multimodal be-
haviour per session. Only the two wizards with the
lowest performance score showed no significant
variation across session, whereas the wizards with
the highest scores showed the most varying be-
haviour. These results again indicate a context de-
pendent strategy. In the following we test this hy-
pothesis (that good multimodal clarification strate-
gies are context-dependent) by building a predic-
tion model of the strategy an average wizard took
dependent on certain context features.
3 Context/Information-State Features
A state or context in our system is a dialogue in-
formation state as defined in (Lemon et al, 2005).
We divide the types of information represented
in the dialogue information state into local fea-
tures (comprising low level and dialogue features),
dialogue history features, and user model fea-
tures. We also defined features reflecting the ap-
plication environment (e.g. driving). All fea-
tures are automatically extracted from the XML
log-files (and are available at runtime in ISU-
based dialogue systems). From these features we
want to learn whether to generate a screen out-
put (graphic-yes), or whether to clarify using
speech only (graphic-no). The case that the
wizard only used screen output for clarification did
not occur.
3.1 Local Features
First, we extracted features present in the ?lo-
cal? context of a CR, such as the number
of matches returned from the data base query
(DBmatches), how many words were deleted
by the corruption algorithm4 (deletion), what
problem source the wizard indicated in the pop-
up questionnaire (source), the previous user
speech act (userSpeechAct), and the delay be-
tween the last wizard utterance and the user?s reply
(delay). 5
One decision to take for extracting these local
features was how to define the ?local? context of
a CR. As shown in table 1, we experimented with
a number of different context definitions. Context
1 defined the local context to be the current turn
only, i.e. the turn containing the CR. Context 2
4Note that this feature is only an approximation of the
ASR confidence score that we would expect in an automated
dialogue system. See (Rieser et al, 2005) for full details.
5We introduced the delay feature to handle clarifications
concerning contact.
id Context (turns) acc/ wf-
score ma-
jority(%)
acc/ wf-score
Na??ve Bayes
(%)
1 only current turn 83.0/54.9 81.0/68.3
2 current and next 71.3/50.4 72.01/68.2
3 current and previous 60.50/59.8 76.0*/75.3
4 previous, current, next 67.8/48.9 76.9*/ 74.8
Table 1: Comparison of context definitions for lo-
cal features (* denotes p < .05)
also considered the current turn and the turn fol-
lowing (and is thus not a ?runtime? context). Con-
text 3 considered the current turn and the previous
turn. Context 4 is the maximal definition of a lo-
cal context, namely the previous, current, and next
turn (also not available at runtime). 6
To find the context type which provides the rich-
est information to a classifier, we compared the ac-
curacy achieved in a 10-fold cross validation by
a Na??ve Bayes classifier (as a standard) on these
data sets against the majority class baseline, us-
ing a paired t-test, we found that that for context
3 and context 4, Na??ve Bayes shows a significant
improvement (with p < .05 using Bonferroni cor-
rection). In table 1 we also show the weighted
f-scores since they show that the high accuracy
achieved using the first two contexts is due to over-
prediction. We chose to use context 3, since these
features will be available during system runtime
and the learnt strategy could be implemented in an
actual system.
3.2 Dialogue History Features
The history features account for events in the
whole dialogue so far, i.e. all information gath-
ered before asking the CR, such as the number of
CRs asked (CRhist), how often the screen output
was already used (screenHist), the corruption
rate so far (delHist), the dialogue duration so
far (duration), and whether the user reacted to
the screen output, either by verbally referencing
(refHist) , e.g. using expressions such as ?It?s
item number 4?, or by clicking (clickHist) as
in example 1.
3.3 User Model Features
Under ?user model features? we consider features
reflecting the wizards? responsiveness to the be-
6Note that dependent on the context definition a CR
might get annotated differently, since placing the question
and showing the graphic might be asynchronous events.
661
haviour and situation of the user. Each session
comprised four dialogues with one wizard. The
user model features average the user?s behaviour
in these dialogues so far, such as how responsive
the user is towards the screen output, i.e. how of-
ten this user clicks (clickUser) and how fre-
quently s/he uses verbal references (refUser);
how often the wizard had already shown a screen
output (screenUser) and how many CRs were
already asked (CRuser); how much the user?s
speech was corrupted on average (delUser), i.e.
an approximation of how well this user is recog-
nised; and whether this user is currently driving or
not (driving). This information was available
to the wizard.
LOCAL FEATURES
DBmatches: 20
deletion: 0
source: reference resolution
userSpeechAct: command
delay: 0
HISTORY FEATURES
[CRhist, screenHist, delHist,
refHist, clickHist]=0
duration= 10s
USER MODEL FEATURES
[clickUser,refUser,screenUser,
CRuser]=0
driving= true
Figure 2: Features in the context after the first turn
in example 1.
3.4 Discussion
Note that all these features are generic over
information-seeking dialogues where database re-
sults can be displayed on a screen; except for
driving which only applies to hands-and-eyes-
busy situations. Figure 2 shows a context for ex-
ample 1, assuming that it was the first utterance by
this user.
This potential feature space comprises 18 fea-
tures, many of them taking numeric attributes as
values. Considering our limited data set of 152
training instances we run the risk of severe data
sparsity. Furthermore we want to explore which
features of this potential feature space influenced
the wizards? multimodal strategy. In the next
two sections we describe feature engineering tech-
niques, namely discretising methods for dimen-
sionality reduction and feature selection methods,
which help to reduce the feature space to a sub-
set which is most predictive of multimodal clarifi-
cation. For our experiments we use implementa-
tions of discretisation and feature selection meth-
ods provided by the WEKA toolkit (Witten and
Frank, 2005).
4 Feature Engineering
4.1 Discretising Numeric Features
Global discretisation methods divide all contin-
uous features into a smaller number of distinct
ranges before learning starts. This has two advan-
tages concerning the quality of our data for ML.
First, discretisation methods take feature distribu-
tions into account and help to avoid sparse data.
Second, most of our features are highly positively
skewed. Some ML methods (such as the standard
extension of the Na??ve Bayes classifier to handle
numeric features) assume that numeric attributes
have a normal distribution. We use Proportional
k-Interval (PKI) discretisation as a unsupervised
method, and an entropy-based algorithm (Fayyad
and Irani, 1993) based on the Minimal Description
Length (MDL) principle as a supervised discreti-
sation method.
4.2 Feature Selection
Feature selection refers to the problem of select-
ing an optimum subset of features that are most
predictive of a given outcome. The objective of se-
lection is two-fold: improving the prediction per-
formance of ML models and providing a better un-
derstanding of the underlying concepts that gener-
ated the data. We chose to apply forward selec-
tion for all our experiments given our large fea-
ture set, which might include redundant features.
We use the following feature filtering methods:
correlation-based subset evaluation (CFS) (Hall,
2000) and a decision tree algorithm (rule-based
ML) for selecting features before doing the actual
learning. We also used a wrapper method called
Selective Na??ve Bayes, which has been shown to
perform reliably well in practice (Langley and
Sage, 1994). We also apply a correlation-based
ranking technique since subset selection models
inner-feature relations at the expense of saying
less about individual feature performance itself.
4.3 Results for PKI and MDL Discretisation
Feature selection and discretisation influence one-
another, i.e. feature selection performs differently
on PKI or MDL discretised data. MDL discreti-
sation reduces our range of feature values dra-
matically. It fails to discretise 10 of 14 nu-
meric features and bars those features from play-
ing a role in the final decision structure because
the same discretised value will be given to all
instances. However, MDL discretisation cannot
replace proper feature selection methods since
662
Table 2: Feature selection on PKI-discretised data (left) and on MDL-discretised data (right)
it doesn?t explicitly account for redundancy be-
tween features, nor for non-numerical features.
For the other 4 features which were discretised
there is a binary split around one (fairly low)
threshold: screenHist (.5), refUser (.375),
screenUser (1.0), CRUser (1.25).
Table 2 shows two figures illustrating the dif-
ferent subsets of features chosen by the feature
selection algorithms on discretised data. From
these four subsets we extracted a fifth, using all
the features which were chosen by at least two
of the feature selection methods, i.e. the features
in the overlapping circle regions shown in figure
2. For both data sets the highest ranking fea-
tures are also the ones contained in the overlapping
regions, which are screenUser, refUser
and screenHist. For implementation dialogue
management needs to keep track of whether the
user already saw a screen output in a previous in-
teraction (screenUser), or in the same dialogue
(screenHist), and whether this user (verbally)
reacted to the screen output (refUser).
5 Performance of Different Learners and
Feature Engineering
In this section we evaluate the performance of fea-
ture engineering methods in combination with dif-
ferent ML algorithms (where we treat feature op-
timisation as an integral part of the training pro-
cess). All experiments are carried out using 10-
fold cross-validation. We take an approach similar
to (Daelemans et al, 2003) where parameters of
the classifier are optimised with respect to feature
selection. We use a wide range of different multi-
variate classifiers which reflect our hypothesis that
a decision is based on various features in the con-
text, and compare them against two simple base-
line strategies, reflecting deterministic contextual
behaviour.
5.1 Baselines
The simplest baseline we can consider is to always
predict the majority class in the data, in our case
graphic-no. This yields a 45.6% wf-score.
This baseline reflects a deterministic wizard strat-
egy never showing a screen output.
A more interesting baseline is obtained by us-
ing a 1-rule classifier. It chooses the feature
which produces the minimum error (which is
refUser for the PKI discretised data set, and
screenHist for the MDL set). We use the im-
plementation of a one-rule classifier provided in
the WEKA toolkit. This yields a 59.8% wf-score.
This baseline reflects a deterministic wizard strat-
egy which is based on a single feature only.
5.2 Machine Learners
For learning we experiment with five different
types of supervised classifiers.We chose Na??ve
Bayes as a joint (generative) probabilistic model,
using the WEKA implementation of (John and Lan-
gley, 1995)?s classifier; Bayesian Networks as a
graphical generative model, again using the WEKA
implementation; and we chose maxEnt as a dis-
criminative (conditional) model, using the Max-
imum Entropy toolkit (Le, 2003). As a rule in-
duction algorithm we used JRIP, the WEKA imple-
mentation of (Cohen, 1995)?s Repeated Incremen-
tal Pruning to Produce Error Reduction (RIPPER).
And for decision trees we used the J4.8 classi-
fier (WEKA?s implementation of the C4.5 system
(Quinlan, 1993)).
5.3 Comparison of Results
We experimented using these different classifiers
on raw data, on MDL and PKI discretised data,
and on discretised data using the different fea-
ture selection algorithms. To compare the clas-
sification outcomes we report on two measures:
accuracy and wf-score, which is the weighted
663
Feature transformation/
(acc./ wf-score (%))
1-rule
baseline
Rule
Induction
Decision
Tree
maxEnt Na??ve
Bayes
Bayesian
Network
Average
raw data 60.5/59.8 76.3/78.3 79.4/78.6 70.0/75.3 76.0/75.3 79.5/72.0 73.62/73.21
PKI + all features 60.5/ 64.6 67.1/66.4 77.4/76.3 70.7/76.7 77.5/81.6 77.3/82.3 71.75/74.65
PKI+ CFS subset 60.5/64.4 68.7/70.7 79.2/76.9 76.7/79.4 78.2/80.6 77.4/80.7 73.45/75.45
PKI+ rule-based ML 60.5/66.5 72.8/76.1 76.0/73.9 75.3/80.2 80.1/78.3 80.8/79.8 74.25/75.80
PKI+ selective Bayes 60.5/64.4 68.2/65.2 78.4/77.9 79.3/78.1 84.6/85.3 84.5/84.6 75.92/75.92
PKI+ subset overlap 60.5/64.4 70.9/70.7 75.9/76.9 76.7/78.2 84.0/80.6 83.7/80.7 75.28/75.25
MDL + all features 60.5/69.9 79.0/78.8 78.0/78.1 71.3/76.8 74.9/73.3 74.7/73.9 73.07/75.13
MDL + CFS subset 60.5/69.9 80.1/78.2 80.6/78.2 76.0/80.2 75.7/75.8 75.7/75.8 74.77/76.35
MDL + rule-based ML 60.5/75.5 80.4/81.6 78.7/80.2 79.3/78.8 82.7/82.9 82.7/82.9 77.38/80.32
MDL + select. Bayes 60.5/75.5 80.4/81.6 78.7/80.8 79.3/80.1 82.7/82.9 82.7/82.9 77.38/80.63
MDL + overlap 60.5/75.5 80.4/81.6 78.7/80.8 79.3/80.1 82.7/82.9 82.7/82.9 77.38/80.63
average 60.5/68.24 74.9/75.38 78.26/78.06 75.27/78.54 79.91/79.96 80.16/79.86
Table 3: Average accuracy and wf-scores for models in feature engineering experiments .
sum (by class frequency in the data; 39.5%
graphic-yes, 60.5% graphic-no) of the f-
scores of the individual classes. In table 3 we
see fairly stable high performance for Bayesian
models with MDL feature selection. However, the
best performing model is Na??ve Bayes using wrap-
per methods (selective Bayes) for feature selection
and PKI discretisation. This model achieves a wf-
score of 85.3%, which is a 25.5% improvement
over the 1-rule baseline.
We separately explore the models and feature
engineering techniques and their impact on the
prediction accuracy for each trial/cross-validation.
In the following we separate out the independent
contribution of models and features. To assess
the effects of models, feature discretisation and
selection on performance accuracy, we conduct
a hierarchical regression analysis. The models
alone explain 18.1% of the variation in accuracy
(R2 = .181) whereas discretisation methods only
contribute 0.4% and feature selection 1% (R2 =
.195). All parameters, except for discretisation
methods have a significant impact on modelling
accuracy (P < .001), indicating that feature selec-
tion is an essential step for predicting wizard be-
haviour. The coefficients of the regression model
lead us to the following hypotheses which we ex-
plore by comparing the group means for models,
discretisation, and features selection methods. Ap-
plying a Kruskal-Wallis test with Mann-Whitney
tests as a post-hoc procedure (using Bonferroni
correction for multiple comparisons), we obtained
the following results: 7
? All ML algorithms are significantly better
than the majority and one-rule baselines. All
7We cannot report full details here. Supplementary
material is available at www.coli.uni-saarland.de/
?vrieser/acl06-supplementary.html
except maxEnt are significantly better than
the Rule Induction algorithm. There is no
significant difference in the performance of
Decision Tree, maxEnt, Na??ve Bayes, and
Bayesian Network classifiers. Multivariate
models being significantly better than the
two baseline models indicates that we have
a strategy that is based on context features.
? For discretisation methods we found that the
classifiers were performing significantly bet-
ter on MDL discretised data than on PKI or
continuous data. MDL being significantly
better than continuous data indicates that all
wizards behaved as though using thresholds
to make their decisions, and MDL being bet-
ter than PKI supports the hypothesis that de-
cisions were context dependent.
? All feature selection methods (except for
CFS) lead to better performance than using
all of the features. Selective Bayes and rule-
based ML selection performed significantly
better than CFS. Selective Bayes, rule-based
ML, and subset-overlap showed no signifi-
cant differences. These results show that wiz-
ards behaved as though specific features were
important (but they suggest that inner-feature
relations used by CFS are less important).
Discussion of results: These experimental re-
sults show two things. First, the results indi-
cate that we can learn a good prediction model
from our data. We conclude that our six wiz-
ards did not behave arbitrarily, but selected their
strategy according to certain contextual features.
By separating out the individual contributions of
models and feature engineering techniques, we
have shown that wizard behaviour is based on
multiple features. In sum, Decision Tree, max-
664
Ent, Na??ve Bayes, and Bayesian Network clas-
sifiers on MDL discretised data using Selective
Bayes and Rule-based ML selection achieved
the best results. The best performing feature
subset was screenUser,screenHist, and
userSpeechAct. The best performing model
uses the richest feature space including the feature
driving.
Second, the regression analysis shows that us-
ing these feature engineering techniques in combi-
nation with improved ML algorithms is an essen-
tial step for learning good prediction models from
the small data sets which are typically available
from multimodal WOZ studies.
6 Interpretation of the learnt Strategy
For interpreting the learnt strategies we discuss
Rule Induction and Decision Trees since they are
the easiest to interpret (and to implement in stan-
dard rule-based dialogue systems). For both we
explain the results obtained by MDL and selective
Bayes, since this combination leads to the best per-
formance.
Rule induction: Figure 3 shows a reformula-
tion of the rules from which the learned classifier
is constructed. The feature screenUser plays
a central role. These rules (in combination with
the low thresholds) say that if you have already
shown a screen output to this particular user in
any previous turn (i.e. screenUser > 1), then
do so again if the previous user speech act was
a command (i.e. userSpeechAct=command)
or if you have already shown a screen out-
put in a previous turn in this dialogue (i.e.
screenHist>0.5). Otherwise don?t show
screen output when asking a clarification.
Decision tree: Figure 4 shows the decision tree
learnt by the classifier J4.8. The five rules
contained in this tree also heavily rely on the
user model as well as the previous screen his-
tory. The rules constructed by the first two nodes
(screenUser, screenHist) may lead to a
repetitive strategy since the right branch will result
in the same action (graphic-yes) in all future
actions. The only variation is introduced by the
speech act, collapsing the tree to the same rule set
as in figure 3. Note that this rule-set is based on
domain independent features.
Discussion: Examining the classifications made
by our best performing Bayesian models we found
that the learnt conditional probability distribu-
tions produce similar feature-value mappings to
the rules described above. The strategy learnt
by the classifiers heavily depends on features ob-
tained in previous interactions, i.e. user model fea-
tures. Furthermore these strategies can lead to
repetitive action, i.e. if a screen output was once
shown to this user, and the user has previously
used or referred to the screen, the screen will be
used over and over again.
For learning a strategy which varies in context
but adapts in more subtle ways (e.g. to the user
model), we would need to explore many more
strategies through interactions with users to find
an optimal one. One way to reduce costs for build-
ing such an optimised strategy is to apply Rein-
forcement Learning (RL) with simulated users. In
future work we will begin with the strategy learnt
by supervised learning (which reflects sub-optimal
average wizard behaviour) and optimise it for dif-
ferent user models and reward structures.
Figure 4: Five-rule tree from J4.8 (?inf? = ?)
7 Summary and Future Work
We showed that humans use a context-dependent
strategy for asking multimodal clarification re-
quests by learning such a strategy from WOZ data.
Only the two wizards with the lowest performance
scores showed no significant variation across ses-
sions, leading us to hypothesise that the better wiz-
ards converged on a context-dependent strategy.
We were able to discover a runtime context based
on which all wizards behaved uniformly, using
feature discretisation methods and feature selec-
tion methods on dialogue context features. Based
on these features we were able to predict how
an ?average? wizard would behave in that context
with an accuracy of 84.6% (wf-score of 85.3%,
which is a 25.5% improvement over a one rule-
based baseline). We explained the learned strate-
gies and showed that they can be implemented in
665
IF screenUser>1 AND (userSpeechAct=command OR screenHist>0.5) THEN graphic=yes
ELSE graphic=no
Figure 3: Reformulation of the rules learnt by JRIP
rule-based dialogue systems based on domain in-
dependent features. We also showed that feature
engineering is essential for achieving significant
performance gains when using large feature spaces
with the small data sets which are typical of di-
alogue WOZ studies. By interpreting the learnt
strategies we found them to be sub-optimal. In
current research, RL is applied to optimise strate-
gies and has been shown to lead to dialogue strate-
gies which are better than those present in the orig-
inal data (Henderson et al, 2005). The next step
towards a RL-based system is to add task-level and
reward-level annotations to calculate reward func-
tions, as discussed in (Rieser et al, 2005). We
furthermore aim to learn more refined clarifica-
tion strategies indicating the problem source and
its severity.
Acknowledgements
The authors would like thank the ACL reviewers,
Alissa Melinger, and Joel Tetreault for help and dis-
cussion. This work is supported by the TALK project,
www.talk-project.org, and the International Post-
Graduate College for Language Technology and Cognitive
Systems, Saarbru?cken.
References
William W. Cohen. 1995. Fast effective rule induction.
In Proceedings of the 12th ICML-95.
Walter Daelemans, Ve?ronique Hoste, Fien De Meul-
der, and Bart Naudts. 2003. Combined optimization
of feature selection and algorithm parameter interac-
tion in machine learning of language. In Proceed-
ings of the 14th ECML-03.
Usama Fayyad and Keki Irani. 1993. Multi-
interval discretization of continuousvalued attributes
for classification learning. In Proc. IJCAI-93.
Mark Hall. 2000. Correlation-based feature selection
for discrete and numeric class machine learning. In
Proc. 17th Int Conf. on Machine Learning.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2005. Hybrid Reinforcement/Supervised
Learning for Dialogue Policies from COMMUNI-
CATOR data. In IJCAI workshop on Knowledge and
Reasoning in Practical Dialogue Systems,.
George John and Pat Langley. 1995. Estimating con-
tinuous distributions in bayesian classifiers. In Pro-
ceedings of the 11th UAI-95. Morgan Kaufmann.
Ivana Kruijff-Korbayova?, Nate Blaylock, Ciprian Ger-
stenberger, Verena Rieser, Tilman Becker, Michael
Kaisser, Peter Poller, and Jan Schehl. 2005. An ex-
periment setup for collecting data for adaptive out-
put planning in a multimodal dialogue system. In
10th European Workshop on NLG.
Pat Langley and Stephanie Sage. 1994. Induction of
selective bayesian classifiers. In Proceedings of the
10th UAI-94.
Zhang Le. 2003. Maximum entropy modeling toolkit
for Python and C++.
Oliver Lemon, Kallirroi Georgila, James Henderson,
Malte Gabsdil, Ivan Meza-Ruiz, and Steve Young.
2005. Deliverable d4.1: Integration of learning and
adaptivity with the ISU approach.
Sharon Oviatt, Rachel Coulston, and Rebecca
Lunsford. 2004. When do we interact mul-
timodally? Cognitive load and multimodal
communication patterns. In Proceedings of the 6th
ICMI-04.
Sharon Oviatt. 2002. Breaking the robustness bar-
rier: Recent progress on the design of robust mul-
timodal systems. In Advances in Computers. Aca-
demic Press.
Tim Paek and David Maxwell Chickering. 2005.
The markov assumption in spoken dialogue manage-
ment. In Proceedings of the 6th SIGdial Workshop
on Discourse and Dialogue.
Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Verena Rieser and Johanna Moore. 2005. Implica-
tions for Generating Clarification Requests in Task-
oriented Dialogues. In Proceedings of the 43rd ACL.
Verena Rieser, Ivana Kruijff-Korbayova?, and Oliver
Lemon. 2005. A corpus collection and annota-
tion framework for learning multimodal clarification
strategies. In Proceedings of the 6th SIGdial Work-
shop on Discourse and Dialogue.
David Traum and Pierre Dillenbourg. 1996. Mis-
communication in multi-modal collaboration. In
Proceedings of the Workshop on Detecting, Repair-
ing, and Preventing Human-Machine Miscommuni-
cation. AAAI-96.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques
(Second Edition). Morgan Kaufmann.
666
Proceedings of ACL-08: HLT, pages 638?646,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Effective Multimodal Dialogue Strategies from Wizard-of-Oz
data: Bootstrapping and Evaluation
Verena Rieser
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
vrieser@inf.ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh, EH8 9LW, GB
olemon@inf.ed.ac.uk
Abstract
We address two problems in the field of au-
tomatic optimization of dialogue strategies:
learning effective dialogue strategies when no
initial data or system exists, and evaluating the
result with real users. We use Reinforcement
Learning (RL) to learn multimodal dialogue
strategies by interaction with a simulated envi-
ronment which is ?bootstrapped? from small
amounts of Wizard-of-Oz (WOZ) data. This
use of WOZ data allows development of op-
timal strategies for domains where no work-
ing prototype is available. We compare the
RL-based strategy against a supervised strat-
egy which mimics the wizards? policies. This
comparison allows us to measure relative im-
provement over the training data. Our results
show that RL significantly outperforms Super-
vised Learning when interacting in simulation
as well as for interactions with real users. The
RL-based policy gains on average 50-times
more reward when tested in simulation, and
almost 18-times more reward when interacting
with real users. Users also subjectively rate
the RL-based policy on average 10% higher.
1 Introduction
Designing a spoken dialogue system is a time-
consuming and challenging task. A developer may
spend a lot of time and effort anticipating the po-
tential needs of a specific application environment
and then deciding on the most appropriate system
action (e.g. confirm, present items,. . . ). One of the
key advantages of statistical optimisation methods,
such as Reinforcement Learning (RL), for dialogue
strategy design is that the problem can be formu-
lated as a principled mathematical model which can
be automatically trained on real data (Lemon and
Pietquin, 2007; Frampton and Lemon, to appear). In
cases where a system is designed from scratch, how-
ever, there is often no suitable in-domain data. Col-
lecting dialogue data without a working prototype
is problematic, leaving the developer with a classic
chicken-and-egg problem.
We propose to learn dialogue strategies by
simulation-based RL (Sutton and Barto, 1998),
where the simulated environment is learned from
small amounts of Wizard-of-Oz (WOZ) data. Us-
ing WOZ data rather than data from real Human-
Computer Interaction (HCI) allows us to learn op-
timal strategies for domains where no working di-
alogue system already exists. To date, automatic
strategy learning has been applied to dialogue sys-
tems which have already been deployed using hand-
crafted strategies. In such work, strategy learning
was performed based on already present extensive
online operation experience, e.g. (Singh et al, 2002;
Henderson et al, 2005). In contrast to this preced-
ing work, our approach enables strategy learning in
domains where no prior system is available. Opti-
mised learned strategies are then available from the
first moment of online-operation, and tedious hand-
crafting of dialogue strategies is omitted. This inde-
pendence from large amounts of in-domain dialogue
data allows researchers to apply RL to new appli-
cation areas beyond the scope of existing dialogue
systems. We call this method ?bootstrapping?.
In a WOZ experiment, a hidden human operator,
the so called ?wizard?, simulates (partly or com-
638
pletely) the behaviour of the application, while sub-
jects are left in the belief that they are interacting
with a real system (Fraser and Gilbert, 1991). That
is, WOZ experiments only simulate HCI. We there-
fore need to show that a strategy bootstrapped from
WOZ data indeed transfers to real HCI. Further-
more, we also need to introduce methods to learn
useful user simulations (for training RL) from such
limited data.
The use of WOZ data has earlier been proposed
in the context of RL. (Williams and Young, 2004)
utilise WOZ data to discover the state and action
space for MDP design. (Prommer et al, 2006)
use WOZ data to build a simulated user and noise
model for simulation-based RL. While both stud-
ies show promising first results, their simulated en-
vironment still contains many hand-crafted aspects,
which makes it hard to evaluate whether the suc-
cess of the learned strategy indeed originates from
the WOZ data. (Schatzmann et al, 2007) propose to
?bootstrap? with a simulated user which is entirely
hand-crafted. In the following we propose an en-
tirely data-driven approach, where all components
of the simulated learning environment are learned
from WOZ data. We also show that the resulting
policy performs well for real users.
2 Wizard-of-Oz data collection
Our domains of interest are information-seeking di-
alogues, for example a multimodal in-car interface
to a large database of music (MP3) files. The corpus
we use for learning was collected in a multimodal
study of German task-oriented dialogues for an in-
car music player application by (Rieser et al, 2005).
This study provides insights into natural methods
of information presentation as performed by human
wizards. 6 people played the role of an intelligent
interface (the ?wizards?). The wizards were able
to speak freely and display search results on the
screen by clicking on pre-computed templates. Wiz-
ards? outputs were not restricted, in order to explore
the different ways they intuitively chose to present
search results. Wizard?s utterances were immedi-
ately transcribed and played back to the user with
Text-To-Speech. 21 subjects (11 female, 10 male)
were given a set of predefined tasks to perform, as
well as a primary driving task, using a driving simu-
lator. The users were able to speak, as well as make
selections on the screen. We also introduced artifi-
cial noise in the setup, in order to closer resemble
the conditions of real HCI. Please see (Rieser et al,
2005) for further detail.
The corpus gathered with this setup comprises 21
sessions and over 1600 turns. Example 1 shows a
typical multimodal presentation sub-dialogue from
the corpus (translated from German). Note that the
wizard displays quite a long list of possible candi-
dates on an (average sized) computer screen, while
the user is driving. This example illustrates that even
for humans it is difficult to find an ?optimal? solu-
tion to the problem we are trying to solve.
(1) User: Please search for music by Madonna .
Wizard: I found seventeen hundred and eleven
items. The items are displayed on the screen.
[displays list]
User: Please select ?Secret?.
For each session information was logged, e.g. the
transcriptions of the spoken utterances, the wizard?s
database query and the number of results, the screen
option chosen by the wizard, and a rich set of con-
textual dialogue features was also annotated, see
(Rieser et al, 2005).
Of the 793 wizard turns 22.3% were annotated
as presentation strategies, resulting in 177 instances
for learning, where the six wizards contributed about
equal proportions.
Information about user preferences was obtained,
using a questionnaire containing similar questions to
the PARADISE study (Walker et al, 2000). In gen-
eral, users report that they get distracted from driv-
ing if too much information is presented. On the
other hand, users prefer shorter dialogues (most of
the user ratings are negatively correlated with dia-
logue length). These results indicate that we need
to find a strategy given the competing trade-offs be-
tween the number of results (large lists are difficult
for users to process), the length of the dialogue (long
dialogues are tiring, but collecting more information
can result in more precise results), and the noise in
the speech recognition environment (in high noise
conditions accurate information is difficult to ob-
tain). In the following we utilise the ratings from the
user questionnaires to optimise a presentation strat-
egy using simulation-based RL.
639
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
acquisition action:
?
?
?
?
askASlot
implConfAskASlot
explConf
presentInfo
?
?
?
?state:
?
?
?
?
?
?
filledSlot 1 |2 |3 |4 |:
{
0,1
}
confirmedSlot 1 |2 |3 |4 |:
{
0,1
}
DB:
{
1--438
}
?
?
?
?
?
?
presentation action:
[
presentInfoVerbal
presentInfoMM
]
state:
?
?
?
?
?
?
DB low:
{
0,1
}
DB med:
{
0,1
}
DB high
{
0,1
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: State-Action space for hierarchical Reinforcement Learning
3 Simulated Learning Environment
Simulation-based RL (also know as ?model-free?
RL) learns by interaction with a simulated environ-
ment. We obtain the simulated components from the
WOZ corpus using data-driven methods. The em-
ployed database contains 438 items and is similar in
retrieval ambiguity and structure to the one used in
the WOZ experiment. The dialogue system used for
learning comprises some obvious constraints reflect-
ing the system logic (e.g. that only filled slots can be
confirmed), implemented as Information State Up-
date (ISU) rules. All other actions are left for opti-
misation.
3.1 MDP and problem representation
The structure of an information seeking dialogue
system consists of an information acquisition phase,
and an information presentation phase. For informa-
tion acquisition the task of the dialogue manager is
to gather ?enough? search constraints from the user,
and then, ?at the right time?, to start the information
presentation phase, where the presentation task is to
present ?the right amount? of information in the right
way? either on the screen or listing the items ver-
bally. What ?the right amount? actually means de-
pends on the application, the dialogue context, and
the preferences of users. For optimising dialogue
strategies information acquisition and presentation
are two closely interrelated problems and need to
be optimised simultaneously: when to present in-
formation depends on the available options for how
to present them, and vice versa. We therefore for-
mulate the problem as a Markov Decision Process
(MDP), relating states to actions in a hierarchical
manner (see Figure 1): 4 actions are available for
the information acquisition phase; once the action
presentInfo is chosen, the information presen-
tation phase is entered, where 2 different actions
for output realisation are available. The state-space
comprises 8 binary features representing the task for
a 4 slot problem: filledSlot indicates whether a
slots is filled, confirmedSlot indicates whether
a slot is confirmed. We also add features that hu-
man wizards pay attention to, using the feature se-
lection techniques of (Rieser and Lemon, 2006b).
Our results indicate that wizards only pay attention
to the number of retrieved items (DB). We there-
fore add the feature DB to the state space, which
takes integer values between 1 and 438, resulting in
28 ? 438 = 112, 128 distinct dialogue states. In to-
tal there are 4112,128 theoretically possible policies
for information acquisition. 1 For the presentation
phase the DB feature is discretised, as we will further
discuss in Section 3.6. For the information presenta-
tion phase there are 22
3
= 256 theoretically possible
policies.
3.2 Supervised Baseline
We create a baseline by applying Supervised Learn-
ing (SL). This baseline mimics the average wizard
behaviour and allows us to measure the relative im-
provements over the training data (cf. (Henderson et
al., 2005)). For these experiments we use the WEKA
toolkit (Witten and Frank, 2005). We learn with the
decision tree J4.8 classifier, WEKA?s implementation
of the C4.5 system (Quinlan, 1993), and rule induc-
1In practise, the policy space is smaller, as some of combi-
nations are not possible, e.g. a slot cannot be confirmed before
being filled. Furthermore, some incoherent action choices are
excluded by the basic system logic.
640
baseline JRip J48
timing 52.0(? 2.2) 50.2(? 9.7) 53.5(?11.7)
modality 51.0(? 7.0) 93.5(?11.5)* 94.6(? 10.0)*
Table 1: Predicted accuracy for presentation timing and
modality (with standard deviation ?), * denotes statisti-
cally significant improvement at p < .05
tion JRIP, the WEKA implementation of RIPPER (Co-
hen, 1995). In particular, we learn models which
predict the following wizard actions:
? Presentation timing: when the ?average? wizard
starts the presentation phase
? Presentation modality: in which modality the
list is presented.
As input features we use annotated dialogue con-
text features, see (Rieser and Lemon, 2006b). Both
models are trained using 10-fold cross validation.
Table 1 presents the results for comparing the ac-
curacy of the learned classifiers against the major-
ity baseline. For presentation timing, none of the
classifiers produces significantly improved results.
Hence, we conclude that there is no distinctive pat-
tern the wizards follow for when to present informa-
tion. For strategy implementation we therefore use a
frequency-based approach following the distribution
in the WOZ data: in 0.48 of cases the baseline policy
decides to present the retrieved items; for the rest of
the time the system follows a hand-coded strategy.
For learning presentation modality, both classifiers
significantly outperform the baseline. The learned
models can be rewritten as in Algorithm 1. Note that
this rather simple algorithm is meant to represent the
average strategy as present in the initial data (which
then allows us to measure the relative improvements
of the RL-based strategy).
Algorithm 1 SupervisedStrategy
1: if DB ? 3 then
2: return presentInfoVerbal
3: else
4: return presentInfoMM
5: end if
3.3 Noise simulation
One of the fundamental characteristics of HCI is an
error prone communication channel. Therefore, the
simulation of channel noise is an important aspect of
the learning environment. Previous work uses data-
intensive simulations of ASR errors, e.g. (Pietquin
and Dutoit, 2006). We use a simple model simulat-
ing the effects of non- and misunderstanding on the
interaction, rather than the noise itself. This method
is especially suited to learning from small data sets.
From our data we estimate a 30% chance of user
utterances to be misunderstood, and 4% to be com-
plete non-understandings. We simulate the effects
noise has on the user behaviour, as well as for the
task accuracy. For the user side, the noise model de-
fines the likelihood of the user accepting or rejecting
the system?s hypothesis (for example when the sys-
tem utters a confirmation), i.e. in 30% of the cases
the user rejects, in 70% the user agrees. These prob-
abilities are combined with the probabilities for user
actions from the user simulation, as described in the
next section. For non-understandings we have the
user simulation generating Out-of-Vocabulary utter-
ances with a chance of 4%. Furthermore, the noise
model determines the likelihood of task accuracy as
calculated in the reward function for learning. A
filled slot which is not confirmed by the user has a
30% chance of having been mis-recognised.
3.4 User simulation
A user simulation is a predictive model of real user
behaviour used for automatic dialogue strategy de-
velopment and testing. For our domain, the user
can either add information (add), repeat or para-
phrase information which was already provided at
an earlier stage (repeat), give a simple yes-no an-
swer (y/n), or change to a different topic by pro-
viding a different slot value than the one asked for
(change). These actions are annotated manually
(? = .7). We build two different types of user
simulations, one is used for strategy training, and
one for testing. Both are simple bi-gram models
which predict the next user action based on the pre-
vious system action (P (auser|asystem)). We face
the problem of learning such models when train-
ing data is sparse. For training, we therefore use
a cluster-based user simulation method, see (Rieser
641
and Lemon, 2006a). For testing, we apply smooth-
ing to the bi-gram model. The simulations are evalu-
ated using the SUPER metric proposed earlier (Rieser
and Lemon, 2006a), which measures variance and
consistency of the simulated behaviour with respect
to the observed behaviour in the original data set.
This technique is used because for training we need
more variance to facilitate the exploration of large
state-action spaces, whereas for testing we need sim-
ulations which are more realistic. Both user simula-
tions significantly outperform random and majority
class baselines. See (Rieser, 2008) for further de-
tails.
3.5 Reward modelling
The reward function defines the goal of the over-
all dialogue. For example, if it is most important
for the dialogue to be efficient, the reward penalises
dialogue length, while rewarding task success. In
most previous work the reward function is manu-
ally set, which makes it ?the most hand-crafted as-
pect? of RL (Paek, 2006). In contrast, we learn the
reward model from data, using a modified version
of the PARADISE framework (Walker et al, 2000),
following pioneering work by (Walker et al, 1998).
In PARADISE multiple linear regression is used to
build a predictive model of subjective user ratings
(from questionnaires) from objective dialogue per-
formance measures (such as dialogue length). We
use PARADISE to predict Task Ease (a variable ob-
tained by taking the average of two questions in the
questionnaire) 2 from various input variables, via
stepwise regression. The chosen model comprises
dialogue length in turns, task completion (as manu-
ally annotated in the WOZ data), and the multimodal
user score from the user questionnaire, as shown in
Equation 2.
TaskEase = ? 20.2 ? dialogueLength+
11.8 ? taskCompletion+ 8.7 ?multimodalScore; (2)
This equation is used to calculate the overall re-
ward for the information acquisition phase. Dur-
ing learning, Task Completion is calculated online
according to the noise model, penalising all slots
which are filled but not confirmed.
2?The task was easy to solve.?, ?I had no problems finding
the information I wanted.?
For the information presentation phase, we com-
pute a local reward. We relate the multimodal score
(a variable obtained by taking the average of 4 ques-
tions) 3 to the number of items presented (DB) for
each modality, using curve fitting. In contrast to
linear regression, curve fitting does not assume a
linear inductive bias, but it selects the most likely
model (given the data points) by function interpo-
lation. The resulting models are shown in Figure
3.5. The reward for multimodal presentation is a
quadratic function that assigns a maximal score to
a strategy displaying 14.8 items (curve inflection
point). The reward for verbal presentation is a linear
function assigning negative scores to all presented
items ? 4. The reward functions for information
presentation intersect at no. items=3. A comprehen-
sive evaluation of this reward function can be found
in (Rieser and Lemon, 2008a).
-80
-70
-60
-50
-40
-30
-20
-10
 0
 10
 0  10  20  30  40  50  60  70
us
er
 sc
or
e
no. items
reward function for information presentation
intersection point
turning point:14.8
multimodal presentation: MM(x)
verbal presentation: Speech(x)
Figure 2: Evaluation functions relating number of items
presented in different modalities to multimodal score
3.6 State space discretisation
We use linear function approximation in order to
learn with large state-action spaces. Linear func-
tion approximation learns linear estimates for ex-
pected reward values of actions in states represented
as feature vectors. This is inconsistent with the idea
3?I liked the combination of information being displayed on
the screen and presented verbally.?, ?Switching between modes
did not distract me.?, ?The displayed lists and tables contained
on average the right amount of information.?, ?The information
presented verbally was easy to remember.?
642
of non-linear reward functions (as introduced in the
previous section). We therefore quantise the state
space for information presentation. We partition
the database feature into 3 bins, taking the first in-
tersection point between verbal and multimodal re-
ward and the turning point of the multimodal func-
tion as discretisation boundaries. Previous work
on learning with large databases commonly quan-
tises the database feature in order to learn with large
state spaces using manual heuristics, e.g. (Levin et
al., 2000; Heeman, 2007). Our quantisation tech-
nique is more principled as it reflects user prefer-
ences for multi-modal output. Furthermore, in pre-
vious work database items were not only quantised
in the state-space, but also in the reward function,
resulting in a direct mapping between quantised re-
trieved items and discrete reward values, whereas
our reward function still operates on the continuous
values. In addition, the decision when to present a
list (information acquisition phase) is still based on
continuous DB values. In future work we plan to en-
gineer new state features in order to learn with non-
linear rewards while the state space is still continu-
ous. A continuous representation of the state space
allows learning of more fine-grained local trade-offs
between the parameters, as demonstrated by (Rieser
and Lemon, 2008b).
3.7 Testing the Learned Policies in Simulation
We now train and test the multimodal presentation
strategies by interacting with the simulated learn-
ing environment. For the following RL experiments
we used the REALL-DUDE toolkit of (Lemon et al,
2006b). The SHARSHA algorithm is employed for
training, which adds hierarchical structure to the
well known SARSA algorithm (Shapiro and Langley,
2002). The policy is trained with the cluster-based
user simulation over 180k system cycles, which re-
sults in about 20k simulated dialogues. In total, the
learned strategy has 371 distinct state-action pairs
(see (Rieser, 2008) for details).
We test the RL-based and supervised baseline
policies by running 500 test dialogues with a
smoothed user simulation (so that we are not train-
ing and testing on the same simulation). We then
compare quantitative dialogue measures performing
a paired t-test. In particular, we compare mean val-
ues of the final rewards, number of filled and con-
firmed slots, dialog length, and items presented mul-
timodally (MM items) and items presented ver-
bally (verbal items). RL performs signifi-
cantly better (p < .001) than the baseline strategy.
The only non-significant difference is the number
of items presented verbally, where both RL and SL
strategy settled on a threshold of less than 4 items.
The mean performance measures for simulation-
based testing are shown in Table 2 and Figure 3.
The major strength of the learned policy is that
it learns to keep the dialogues reasonably short (on
average 5.9 system turns for RL versus 8.4 turns
for SL) by presenting lists as soon as the number
of retrieved items is within tolerance range for the
respective modality (as reflected in the reward func-
tion). The SL strategy in contrast has not learned the
right timing nor an upper bound for displaying items
on the screen. The results show that simulation-
based RL with an environment bootstrapped from
WOZ data allows learning of robust strategies which
significantly outperform the strategies contained in
the initial data set.
One major advantage of RL is that it allows us
to provide additional information about user pref-
erences in the reward function, whereas SL simply
mimics the data. In addition, RL is based on de-
layed rewards, i.e. the optimisation of a final goal.
For dialogue systems we often have measures indi-
cating how successful and/or satisfying the overall
performance of a strategy was, but it is hard to tell
how things should have been exactly done in a spe-
cific situation. This is what makes RL specifically
attractive for dialogue strategy learning. In the next
section we test the learned strategy with real users.
4 User Tests
4.1 Experimental design
For the user tests the RL policy is ported to a work-
ing ISU-based dialogue system via table look-up,
which indicates the action with the highest expected
reward for each state (cf. (Singh et al, 2002)). The
supervised baseline is implemented using standard
threshold-based update rules. The experimental con-
ditions are similar to the WOZ study, i.e. we ask the
users to solve similar tasks, and use similar ques-
tionnaires. Furthermore, we decided to use typed
user input rather than ASR. The use of text input
643
Measure SL baseline RL Strategy
SIM REAL SIM REAL
av. turns 8.42(?3.04) 5.86(?3.2) 5.9(?2.4)*** 5.07(?2.9)***
av. speech items 1.04(?.2) 1.29(?.4) 1.1(?.3) 1.2(?.4)
av. MM items 61.37(?82.5) 52.2(?68.5) 11.2(?2.4)*** 8.73(?4.4)***
av. reward -1741.3(?566.2) -628.2(?178.6) 44.06(?51.5)*** 37.62(?60.7)***
Table 2: Comparison of results obtained in simulation (SIM) and with real users (REAL) for SL and RL-based strate-
gies; *** denotes significant difference between SL and RL at p < .001
Figure 3: Graph comparison of objective measures: SLs
= SL policy in simulation; SLr = SL policy with real
users; RLs = RL policy in simulation; RLr = RL policy
with real users.
allows us to target the experiments to the dialogue
management decisions, and block ASR quality from
interfering with the experimental results (Hajdinjak
and Mihelic, 2006). 17 subjects (8 female, 9 male)
are given a set of 6?2 predefined tasks, which they
solve by interaction with the RL-based and the SL-
based system in controlled order. As a secondary
task users are asked to count certain objects in a driv-
ing simulation. In total, 204 dialogues with 1,115
turns are gathered in this setup.
4.2 Results
In general, the users rate the RL-based significantly
higher (p < .001) than the SL-based policy. The re-
sults from a paired t-test on the user questionnaire
data show significantly improved Task Ease, better
presentation timing, more agreeable verbal and mul-
timodal presentation, and that more users would use
the RL-based system in the future (Future Use). All
the observed differences have a medium effects size
(r ? |.3|).
We also observe that female participants clearly
favour the RL-based strategy, whereas the ratings by
male participants are more indifferent. Similar gen-
der effects are also reported by other studies on mul-
timodal output presentation, e.g. (Foster and Ober-
lander, 2006).
Furthermore, we compare objective dialogue per-
formance measures. The dialogues of the RL strat-
egy are significantly shorter (p < .005), while fewer
items are displayed (p < .001), and the help func-
tion is used significantly less (p < .003). The mean
performance measures for testing with real users are
shown in Table 2 and Figure 3. However, there is
no significant difference for the performance of the
secondary driving task.
5 Comparison of Results
We finally test whether the results obtained in sim-
ulation transfer to tests with real users, following
(Lemon et al, 2006a). We evaluate the quality of
the simulated learning environment by directly com-
paring the dialogue performance measures between
simulated and real interaction. This comparison en-
ables us to make claims regarding whether a policy
which is ?bootstrapped? from WOZ data is transfer-
able to real HCI. We first evaluate whether objective
dialogue measures are transferable, using a paired
t-test. For the RL policy there is no statistical dif-
ference in overall performance (reward), dialogue
length (turns), and the number of presented items
(verbal and multimodal items) between simulated
644
Measure WOZ SL RL
av. Task Ease .53?.14 .63?.26 .79?.21***
av. Future Use .56?.16 .55?.21 .67?.20***
Table 3: Improved user ratings over the WOZ study
where *** denotes p < .001
and real interaction (see Table 2, Figure 3). This in-
dicates that the learned strategy transfers well to real
settings. For the SL policy the dialogue length for
real users is significantly shorter than in simulation.
From an error analysis we conclude that real users
intelligently adapt to poor policies, e.g. by changing
topic, whereas the simulated users do not react in
this way.
Furthermore, we want to know whether the sub-
jective user ratings for the RL strategy improved
over the WOZ study. We therefore compare the user
ratings from the WOZ questionnaire to the user rat-
ings of the final user tests using a independent t-test
and a Wilcoxon Signed Ranks Test. Users rate the
RL-policy on average 10% higher. We are especially
interested in the ratings for Task Ease (as this was
the ultimate measure optimised with PARADISE) and
Future Use, as we believe this measure to be an im-
portant indicator of acceptance of the technology.
The results show that only the RL strategy leads to
significantly improved user ratings (increasing av-
erage Task Ease by 49% and Future Use by 19%),
whereas the ratings for the SL policy are not signifi-
cantly better than those for the WOZ data, see Table
3. 4 This indicates that the observed difference is in-
deed due to the improved strategy (and not to other
factors like the different user population or the em-
bedded dialogue system).
6 Conclusion
We addressed two problems in the field of automatic
optimization of dialogue strategies: learning effec-
tive dialogue strategies when no initial data or sys-
tem exists, and evaluating the result with real users.
We learned optimal strategies by interaction with a
simulated environment which is bootstrapped from
4The ratings are normalised as some of the questions were
on different scales.
a small amount of Wizard-of-Oz data, and we evalu-
ated the result with real users. The use of WOZ data
allows us to develop optimal strategies for domains
where no working prototype is available. The de-
veloped simulations are entirely data driven and the
reward function reflects real user preferences. We
compare the Reinforcement Learning-based strategy
against a supervised strategy which mimics the (hu-
man) wizards? policies from the original data. This
comparison allows us to measure relative improve-
ment over the training data. Our results show that
RL significantly outperforms SL in simulation as
well as in interactions with real users. The RL-based
policy gains on average 50-times more reward when
tested in simulation, and almost 18-times more re-
ward when interacting with real users. The human
users also subjectively rate the RL-based policy on
average 10% higher, and 49% higher for Task Ease.
We also show that results obtained in simulation are
comparable to results for real users. We conclude
that a strategy trained from WOZ data via boot-
strapping is transferable to real Human-Computer-
Interaction.
In future work will apply similar techniques to
statistical planning for Natural Language Generation
in spoken dialogue (Lemon, 2008; Janarthanam and
Lemon, 2008), (see the EC FP7 CLASSiC project:
www.classic-project.org).
Acknowledgements
The research leading to these results has re-
ceived funding from the European Community?s
7th Framework Programme (FP7/2007-2013) un-
der grant agreement no. 216594 (CLASSiC project
www.classic-project.org), the EC FP6
project ?TALK: Talk and Look, Tools for Am-
bient Linguistic Knowledge (IST 507802, www.
talk-project.org), from the EPSRC, project
no. EP/E019501/1, and from the IRTG Saarland
University.
645
References
W. W. Cohen. 1995. Fast effective rule induction. In
Proc. of the 12th ICML-95.
M. E. Foster and J. Oberlander. 2006. Data-driven gen-
eration of emphatic facial displays. In Proc. of EACL.
M. Frampton and O. Lemon. (to appear). Recent re-
search advances in Reinforcement Learning in Spoken
Dialogue Systems. Knowledge Engineering Review.
N. M. Fraser and G. N. Gilbert. 1991. Simulating speech
systems. Computer Speech and Language, 5:81?99.
M. Hajdinjak and F. Mihelic. 2006. The PARADISE
evaluation framework: Issues and findings. Computa-
tional Linguistics, 32(2):263?272.
P. Heeman. 2007. Combining reinforcement learn-
ing with information-state update rules. In Proc. of
NAACL.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hy-
brid Reinforcement/Supervised Learning for Dialogue
Policies from COMMUNICATOR data. In Proc. of IJ-
CAI workshop on Knowledge and Reasoning in Prac-
tical Dialogue Systems, pages 68?75.
S. Janarthanam and O. Lemon. 2008. User simula-
tions for online adaptation and knowledge-alignment
in Troubleshooting dialogue systems. In Proc. of the
12th SEMDIAL Workshop (LONdial).
O. Lemon and O. Pietquin. 2007. Machine learning for
spoken dialogue systems. In Proc. of Interspeech.
O. Lemon, K. Georgila, and J. Henderson. 2006a.
Evaluating Effectiveness and Portability of Reinforce-
ment Learned Dialogue Strategies with real users: the
TALK TownInfo Evaluation. In Proc. of IEEE/ACL
workshop on Spoken Language Technology (SLT).
O. Lemon, X. Liu, D. Shapiro, and C. Tollander. 2006b.
Hierarchical reinforcement learning of dialogue poli-
cies in a development environment for dialogue sys-
tems: REALL-DUDE. In Proc. of the 10th SEMDIAL
Workshop (BRANdial).
O. Lemon. 2008. Adaptive Natural Language Gener-
ation in Dialogue using Reinforcement Learning. In
Proc. of the 12th SEMDIAL Workshop (LONdial).
E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochas-
tic model of human-machine interaction for learning
dialog strategies. IEEE Transactions on Speech and
Audio Processing, 8(1).
T. Paek. 2006. Reinforcement learning for spoken dia-
logue systems: Comparing strengths and weaknesses
for practical deployment. In Proc. Dialog-on-Dialog
Workshop, Interspeech.
O. Pietquin and T. Dutoit. 2006. A probabilistic
framework for dialog simulation and optimal strategy
learnin. IEEE Transactions on Audio, Speech and
Language Processing, 14(2):589?599.
T. Prommer, H. Holzapfel, and A. Waibel. 2006. Rapid
simulation-driven reinforcement learning of multi-
modal dialog strategies in human-robot interaction. In
Proc. of Interspeech/ICSLP.
R. Quinlan. 1993. C4.5: Programs for Machine Learn-
ing. Morgan Kaufmann.
V. Rieser and O. Lemon. 2006a. Cluster-based user sim-
ulations for learning dialogue strategies. In Proc. of
Interspeech/ICSLP.
V. Rieser and O. Lemon. 2006b. Using machine learning
to explore human multimodal clarification strategies.
In Proc. of ACL.
V. Rieser and O. Lemon. 2008a. Automatic learning
and evaluation of user-centered objective functions for
dialogue system optimisation. In LREC.
V. Rieser and O. Lemon. 2008b. Does this list con-
tain what you were searching for? Learning adaptive
dialogue strategies for interactive question answering.
Journal of Natural Language Engineering (special is-
sue on Interactive Question answering, to appear).
V. Rieser, I. Kruijff-Korbayova?, and O. Lemon. 2005. A
corpus collection and annotation framework for learn-
ing multimodal clarification strategies. In Proc. of the
6th SIGdial Workshop.
V. Rieser. 2008. Bootstrapping Reinforcement Learning-
based Dialogue Strategies from Wizard-of-Oz data (to
appear). Ph.D. thesis, Saarland University.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based user simulation for
bootstrapping a POMDP dialogue system. In Proc. of
HLT/NAACL.
D. Shapiro and P. Langley. 2002. Separating skills from
preference: Using learning to program by reward. In
Proc. of the 19th ICML.
S. Singh, D. Litman, M. Kearns, and M. Walker. 2002.
Optimizing dialogue management with reinforcement
learning: Experiments with the NJFun system. JAIR,
16.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
MIT Press.
M. Walker, J. Fromer, and S. Narayanan. 1998. Learn-
ing optimal dialogue strategies: A case study of a
spoken dialogue agent for email. In Proceedings of
ACL/COLING.
M. Walker, C. Kamm, and D. Litman. 2000. Towards de-
veloping general models of usability with PARADISE.
Journal of Natural Language Engineering, 6(3).
J. Williams and S. Young. 2004. Using Wizard-of-Oz
simulations to bootstrap reinforcement-learning-based
dialog management systems. In Proc. of the 4th SIG-
dial Workshop.
I. Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques (2nd Edi-
tion). Morgan Kaufmann.
646
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 73?76,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mixture Model POMDPs for Efficient Handling of Uncertainty
in Dialogue Management
James Henderson
University of Geneva
Department of Computer Science
James.Henderson@cui.unige.ch
Oliver Lemon
University of Edinburgh
School of Informatics
olemon@inf.ed.ac.uk
Abstract
In spoken dialogue systems, Partially Observ-
able Markov Decision Processes (POMDPs)
provide a formal framework for making di-
alogue management decisions under uncer-
tainty, but efficiency and interpretability con-
siderations mean that most current statistical
dialogue managers are only MDPs. These
MDP systems encode uncertainty explicitly in
a single state representation. We formalise
such MDP states in terms of distributions
over POMDP states, and propose a new di-
alogue system architecture (Mixture Model
POMDPs) which uses mixtures of these dis-
tributions to efficiently represent uncertainty.
We also provide initial evaluation results (with
real users) for this architecture.
1 Introduction
Partially Observable Markov Decision Processes
(POMDPs) provide a formal framework for mak-
ing decisions under uncertainty. Recent research
in spoken dialogue systems has used POMDPs for
dialogue management (Williams and Young, 2007;
Young et al, 2007). These systems represent the
uncertainty about the dialogue history using a prob-
ability distribution over dialogue states, known as
the POMDP?s belief state, and they use approxi-
mate POMDP inference procedures to make dia-
logue management decisions. However, these infer-
ence procedures are too computationally intensive
for most domains, and the system?s behaviour can be
difficult to predict. Instead, most current statistical
dialogue managers use a single state to represent the
dialogue history, thereby making them only Markov
Decision Process models (MDPs). These state rep-
resentations have been fine-tuned over many devel-
opment cycles so that common types of uncertainty
can be encoded in a single state. Examples of such
representations include unspecified values, confi-
dence scores, and confirmed/unconfirmed features.
We formalise such MDP systems as compact encod-
ings of POMDPs, where each MDP state represents
a probability distribution over POMDP states. We
call these distributions ?MDP belief states?.
Given this understanding of MDP dialogue man-
agers, we propose a new POMDP spoken dialogue
system architecture which uses mixtures of MDP be-
lief states to encode uncertainty. A Mixture Model
POMDP represents its belief state as a probability
distribution over a finite set of MDP states. This
extends the compact representations of uncertainty
in MDP states to include arbitrary disjunction be-
tween MDP states. Efficiency is maintained because
such arbitrary disjunction is not needed to encode
the most common forms of uncertainty, and thus the
number of MDP states in the set can be kept small
without losing accuracy. On the other hand, allow-
ing multiple MDP states provides the representa-
tional mechanism necessary to incorporate multiple
speech recognition hypotheses into the belief state
representation. In spoken dialogue systems, speech
recognition is by far the most important source of
uncertainty. By providing a mechanism to incorpo-
rate multiple arbitrary speech recognition hypothe-
ses, the proposed architecture leverages the main ad-
vantage of POMDP systems while still maintaining
the efficiency of MDP-based dialogue managers.
2 Mixture Model POMDPs
A POMDP belief state bt is a probability distribution
P (st|Vt?1, ut) over POMDP states st given the dia-
73
logue history Vt?1 and the most recent observation
(i.e. user utterance) ut. We formalise the meaning
of an MDP state representation rt as a distribution
b(rt) = P (st|rt) over POMDP states. We represent
the belief state bt as a list of pairs ?rit, p
i
t? such that?
i p
i
t = 1. This list is interpreted as a mixture of
the b(rit).
bt =
?
i
pitb(r
i
t) (1)
State transitions in MDPs are specified with an
update function, rt = f(rt?1, at?1, ht), which maps
the preceding state rt?1, system action at?1, and
user input ht to a new state rt. This function is in-
tended to encode in rt all the new information pro-
vided by at?1 and ht. The user input ht is the result
of automatic speech recognition (ASR) plus spoken
language understanding (SLU) applied to ut. Be-
cause there is no method for handling ambiguity in
ht, ht is computed from the single best ASR-SLU
hypothesis, plus some measure of ASR confidence.
In POMDPs, belief state transitions are done by
changing the distribution over states to take into ac-
count the new information from the system action
at?1 and an n-best list of ASR-SLU hypotheses h
j
t .
This new belief state can be estimated as
bt = P (st|Vt?1, ut)
=
?
hjt
?
st?1
P (st?1|Vt?1)P (h
j
t |Vt?1, st?1)
P (ut|Vt?1, st?1, h
j
t )
P (st|Vt?1, st?1, h
j
t , ut)
P (ut|Vt?1)
?
?
hjt
?
st?1
P (st?1|Vt?2, ut?1)P (h
j
t |at?1, st?1)
P (hjt |ut)P (st|at?1, st?1, h
j
t )
P (hjt )Z(Vt)
where Z(Vt) is a normalising constant.
P (st?1|Vt?2, ut?1) is the previous belief state.
P (hjt |ut) reflects the confidence of ASR-SLU in
hypothesis hjt . P (st|at?1, st?1, h
j
t ) is normally 1
for st = st?1, but can be used to allow users to
change their mind mid-dialogue. P (hjt |at?1, st?1)
is a user model. P (hjt ) is a prior over ASR-SLU
outputs.
Putting these two approaches together, we get the
following update equation for our mixture of MDP
belief states:
bt = P (st|Vt?1, ut)
?
?
hjt
?
rit?1
pit?1P (h
j
t |at?1, r
i
t?1)
P (hjt |ut)b(f(r
i
t?1, at?1, h
j
t ))
P (hjt )Z(Vt)
(2)
=
?
i?
pi
?
t b(r
i?
t )
where, for each i? there is one pair i, j such that
ri
?
t = f(r
i
t?1, at?1, h
j
t )
pi
?
t =
pit?1P (h
j
t |at?1,r
i
t?1)P (h
j
t |ut)
P (hjt )Z(Vt)
.
(3)
For equation (2) to be true, we require that
b(f(rit?1, at?1, h
j
t )) ? P (st|at?1, r
i
t?1, h
j
t ) (4)
which simply ensures that the meaning assigned to
MDP state representations and the MDP state tran-
sition function are compatible.
From equation (3), we see that the number
of MDP states will grow exponentially with the
length of the dialogue, proportionately to the num-
ber of ASR-SLU hypotheses. Some of the state-
hypothesis pairs rit?1, h
j
t may lead to equivalent
states f(rit?1, at?1, h
j
t ), but in general pruning is
necessary. Pruning should be done so as to min-
imise the change to the belief state distribution, for
example by minimising the KL divergence between
the pre- and post- pruning belief states. We use two
heuristic approximations to this optimisation prob-
lem. First, if two states share the same core features
(e.g. filled slots, but not the history of user inputs),
then the state with the lower probability is pruned,
and its probability is added to the other state. Sec-
ond, a fixed beam of the k most probable states is
kept, and the other states are pruned. The probabil-
ity pit from a pruned state r
i
t is redistributed to un-
pruned states which are less informative than rit in
their core features.1
The interface between the ASR-SLU module and
the dialogue manager is a set of hypotheses hjt paired
with their confidence scores P (hjt |ut). These pairs
are analogous to the state-probability pairs rit, p
i
t
within the dialogue manager, and we can extend our
mixture model architecture to cover these pairs as
well. Interpreting the set of hjt , P (h
j
t |ut) pairs as a
1In the current implementation, these pruned state probabil-
ities are simply added to an uninformative ?null? state, but in
general we could check for logical subsumption between states.
74
mixture of distributions over more specific hypothe-
ses becomes important when we consider pruning
this set before passing it to the dialogue manager. As
with the pruning of states, pruning should not sim-
ply remove a hypothesis and renormalise, it should
redistribute the probability of a pruned hypothesis to
similar hypotheses. This is not always computation-
ally feasible, but all interfaces within the Mixture
Model POMDP architecture are sets of hypothesis-
probability pairs which can be interpreted as finite
mixtures in some underlying hypothesis space.
Given an MDP state representation, this formali-
sation allows us to convert it into a Mixture Model
POMDP. The only additional components of the
model are the user model P (hjt |at?1, r
i
t?1), the
ASR-SLU prior P (hjt ), and the ASR-SLU confi-
dence score P (hjt |ut). Note that there is no need
to actually define b(rit), provided equation (4) holds.
3 Decision Making with MM POMDPs
Given this representation of the uncertainty in the
current dialogue state, the spoken dialogue system
needs to decide what system action to perform.
There are several approaches to POMDP decision
making which could be adapted to this representa-
tion, but to date we have only considered a method
which allows us to directly derive a POMDP policy
from the policy of the original MDP.
Here again we exploit the fact that the most fre-
quent forms of uncertainty are already effectively
handled in the MDP system (e.g. by filled vs. con-
firmed slot values). We propose that an effective di-
alogue management policy can be created by sim-
ply computing a mixture of the MDP policy applied
to the MDP states in the belief state list. More
precisely, we assume that the original MDP system
specifies a Q function QMDP(at, rt) which estimates
the expected future reward of performing action at
in state rt. We then estimate the expected future re-
ward of performing action at in belief state bt as the
mixture of these MDP estimates.
Q(at, bt) ?
?
i
pitQMDP(at, r
i
t) (5)
The dialogue management policy is to choose the
action at with the largest value for Q(at, bt). This is
known as a Q-MDP model (Littman et al, 1995), so
we call this proposal a Mixture Model Q-MDP.
4 Related Work
Our representation of POMDP belief states using a
set of distributions over POMDP states is similar to
the approach in (Young et al, 2007), where POMDP
belief states are represented using a set of partitions
of POMDP states. For any set of partitions, the mix-
ture model approach could express the same model
by defining one MDP state per partition and giving
it a uniform distribution inside its partition and zero
probability outside. However, the mixture model ap-
proach is more flexible, because the distributions in
the mixture do not have to be uniform within their
non-zero region, and these regions do not have to
be disjoint. A list of states was also used in (Hi-
gashinaka et al, 2003) to represent uncertainty, but
no formal semantics was provided for this list, and
therefore only heuristic uses were suggested for it.
5 Initial Experiments
We have implemented a Mixture Model POMDP ar-
chitecture as a multi-state version of the DIPPER
?Information State Update? dialogue manager (Bos
et al, 2003). It uses equation (3) to compute belief
state updates, given separate models for MDP state
updates (for f(rit?1, at?1, h
j
t )), statistical ASR-SLU
(for P (hjt |ut)/P (h
j
t )), and a statistical user model
(for P (hjt |at?1, r
i
t?1)). The state list is pruned as
described in section 2, where the ?core features?
are the filled information slot values and whether
they have been confirmed. For example, the sys-
tem will merge two states which agree that the user
only wants a cheap hotel, even if they disagree on
the sequence of dialogue acts which lead to this in-
formation. It also never prunes the ?null? state, so
that there is always some probability that the system
knows nothing.
The system used in the experiments described
below uses the MDP state representation and up-
date function from (Lemon and Liu, 2007), which
is designed for standard slot-filling dialogues. For
the ASR model, it uses the HTK speech recogniser
(Young et al, 2002) and an n-best list of three ASR
hypotheses on each user turn. The prior over user in-
puts is assumed to be uniform. The ASR hypotheses
are passed to the SLU model from (Meza-Ruiz et al,
2008), which produces a single user input for each
ASR hypothesis. This SLU model was trained on
75
TC % Av. length (std. deviation)
Handcoded 56.0 7.2 (4.6)
MDP 66.6 7.2 (4.0)
MM Q-MDP 73.3 7.3 (3.7)
Table 1: Initial test results for human-machine dialogues,
showing task completion and average length.
the TownInfo corpus of dialogues, which was col-
lected using the TownInfo human-machine dialogue
systems of (Lemon et al, 2006), transcribed, and
hand annotated. ASR hypotheses which result in the
same user input are merged (summing their proba-
bilities), and the resulting list of at most three ASR-
SLU hypotheses are passed to the dialogue manager.
Thus the number of MDP states in the dialogue man-
ager grows by up to three times at each step, before
pruning. For the user model, the system uses an n-
gram user model, as described in (Georgila et al,
2005), trained on the annotated TownInfo corpus.2
The system?s dialogue management policy is a
Mixture Model Q-MDP (MM Q-MDP) policy. As
with the MDP states, the MDP Q function is from
(Lemon and Liu, 2007). It was trained in an MDP
system using reinforcement learning with simulated
users (Lemon and Liu, 2007), and was not modified
for use in our MM Q-MDP policy.
We tested this system with 10 different users, each
attempting 9 tasks in the TownInfo domain (search-
ing for hotels and restaurants in a fictitious town),
resulting in 90 test dialogues. The users each at-
tempted 3 tasks with the MDP system of (Lemon
and Liu, 2007), 3 tasks with a state-of-the-art hand-
coded system (see (Lemon et al, 2006)), and 3 tasks
with the MM Q-MDP system. Ordering of sys-
tems and tasks was controlled, and 3 of the users
were not native speakers of English. We collected
the Task Completion (TC), and dialogue length for
each system, as reported in table 1. Task Comple-
tion is counted from the system logs when the user
replies that they are happy with their chosen option.
Such a small sample size means that these results are
not statistically significant, but there is a clear trend
showing the superiority of the the MM Q-MDP sys-
tem, both in terms of more tasks being completed
and less variability in overall dialogue length.
2Thanks to K. Georgilla for training this model.
6 Conclusions
Mixture Model POMDPs combine the efficiency of
MDP spoken dialogue systems with the ability of
POMDP models to make use of multiple ASR hy-
potheses. They can also be constructed from MDP
models without additional training, using the Q-
MDP approximation for the dialogue management
policy. Initial results suggest that, despite its sim-
plicity, this approach does lead to better spoken dia-
logue systems than MDP and hand-coded models.
Acknowledgments
This research received funding from UK EPSRC
grant EP/E019501/1 and the European Community?s
FP7 under grant no 216594 (CLASSIC project:
www.classic-project.org).
References
J Bos, E Klein, O Lemon, and T Oka. 2003. DIPPER:
Description and Formalisation of an Information-State
Update Dialogue System Architecture. In Proc. SIG-
dial Workshop on Discourse and Dialogue, Sapporo.
K Georgila, J Henderson, and O Lemon. 2005. Learning
User Simulations for Information State Update Dia-
logue Systems. In Proc. Eurospeech.
H Higashinaka, M Nakano, and K Aikawa. 2003.
Corpus-based discourse understanding in spoken dia-
logue systems. In Proc. ACL, Sapporo.
O Lemon and X Liu. 2007. Dialogue policy learning
for combinations of noise and user simulation: transfer
results. In Proc. SIGdial.
O Lemon, K Georgila, and J Henderson. 2006. Evalu-
ating Effectiveness and Portability of Reinforcement
Learned Dialogue Strategies with real users: the
TALK TownInfo Evaluation. In Proc. ACL/IEEE SLT.
ML Littman, AR Cassandra, and LP Kaelbling. 1995.
Learning policies for partially observable environ-
ments: Scaling up. In Proc. ICML, pages 362?370.
I Meza-Ruiz, S Riedel, and O Lemon. 2008. Accurate
statistical spoken language understanding from limited
development resources. In Proc. ICASSP. (to appear).
JD Williams and SJ Young. 2007. Partially Observ-
able Markov Decision Processes for Spoken Dialog Systems.
Computer Speech and Language, 21(2):231?422.
S Young, G Evermann, D Kershaw, G Moore, J Odell,
D Ollason, D Povey, V Valtchev, and P Woodland.
2002. The HTK Book. Cambridge Univ. Eng. Dept.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to Di-
alog Management. In Proc. ICASSP, Honolulu.
76
Managing Dialogue Interaction: A Multi-Layered Approach
Oliver Lemon
School of Informatics
University of Edinburgh
2 Buccleugh Place
Edinburgh EH8 9LW, UK
olemon@inf.ed.ac.uk
Lawrence Cavedon
CSLI
Stanford University
220 Panama St
Stanford, CA 94306, USA
lcavedon@csli.stanford.edu
Barbara Kelly
Department of Linguistics
UCSB
Santa Barbara
CA 93106-3100, USA
bfk0@umail.ucsb.edu
Keywords: dialogue management architecture, in-
teraction, communication channel management
Abstract
We present evidence for the importance
of low-level phenomena in dialogue in-
teraction and use this to motivate a
multi-layered approach to dialogue pro-
cessing. We describe an architecture
that separates content-level communica-
tive processes from interaction-level phe-
nomena (such as feedback, grounding,
turn-management), and provide details of
specific implementations of a number of
such phenomena.
1 Introduction
Real dialogue between human participants involves
phenomena that do not so much contribute to the
content of communication as relate directly to the
interactive process between the participants. This
includes turn management, providing feedback, ut-
terance fillers, error and false-start management, and
utterance timing.
Recent work on dialogue and natural language
processing in general has acknowledged the pres-
ence of such phenomena in natural speech, and in
some cases the importance of its role in dialogue in-
teraction. However, treatment of such phenomena
has generally been part of the standard processing
model; for example, some parsers are able to han-
dle fillers such as ?um?, while recent versions of the
TRIPS system (Allen et al, 2001) uses incremental
parsing and other techniques to handle a range of re-
lated phenomena.
We believe that greater focus on ?interaction
level? phenomena is appropriate and will lead to
benefits in building dialogue systems for more ro-
bust natural interaction. In this paper, we outline a
two-layer architecture for dialogue systems, where
one layer uses a range of ?shallow? processing tech-
niques to maintain a smooth interaction between the
dialogue participants.
1.1 Managing interaction
The inspiration for a clean separation into a two-
layer architecture comes from two sources. Clark
(1996) distinguishes between two separate commu-
nication tracks, which he calls communicative and
meta-communicative. These are simultaneously oc-
curring communications, the first dealing with the
information at hand, and the other relating to the
performance itself. Dialogue participants use what
Clark refers to as signals to refer to the performance
itself: e.g. timing, delays, re-phrasing, mistakes, re-
pairs, etc.1
A second motivation is work on architectures for
robots and autonomous agents embedded in com-
plex, dynamic, unpredictable environments. Sev-
eral researchers in this area have argued for multi-
layered architectures for agents that plan action se-
quences to achieve some goal or task, but need to
react quickly to change in the environment (e.g.
1Clark?s distinction does not necessarily carry over directly
to the design of a dialogue system architecture, but it motivates
focus on the low-level communication channel.
(Firby, 1994; Mu?ller, 1996)). In such architectures,
the role of the bottom layer is to monitor the envi-
ronment and initiate appropriate actions within the
broader context of the goal-directed plan, which is
provided by the higher layer of the architecture. The
layers operate independently and asynchronously,
but communicate as necessary: e.g. goals and plans
are passed down to the execution layer, while obser-
vations or problems (which may trigger replanning)
are passed up to the planning layer.
We view the process of natural interaction with
a dialogue participant as analogous to the interac-
tion with a dynamic environment: dialogue phenom-
ena arise which need to be negotiated (as a new
obstacle must be avoided by a robot). In the case
of a human user involved in activity-oriented dia-
logue, timeliness is particularly important in order
to keep the user engaged and focussed?otherwise,
performance of the joint activity may be adversely
affected. In particular, dialogic interaction is a con-
tinuous process which cannot be broken without the
risk of some breakdown: signal-level phenomena
must be handled as smoothly as possible, without
necessarily resorting to content-level processes, in
order to maintain a tight interaction between the par-
ticipants.
1.2 A multi-layered architecture
Motivated partially by some of the same issues we
discuss here, Allen et al (2001) describe a new ar-
chitecture for their TRIPS system that breaks dia-
logue management into multiple asynchronous com-
ponents. We concur with their concerns but focus on
a different architectural shift.
We outline below an architecture that sepa-
rates interaction-focussed techniques from context-
management and conversation planning. An initial
version of the architecture has been implemented at
the Center for the Study of Language and Informa-
tion (CSLI) at Stanford University.
This breakdown into separate architectural levels
is analogous to the multi-level agent/robot architec-
tures. However, many of the same motivations per-
tain, especially those related to design considera-
tions (e.g. separating different types of phenomena
into different layers) and performance (e.g. high-
level planning from low-level execution and mon-
itoring running in parallel2). Further, the manner
in which Mu?ller and Firby?s systems handle reac-
tive tasks (e.g. obstacle avoidance, object tracking,
etc.) completely at the low-level whenever possible
reflects our view of how certain dialogue interaction
phenomena are best handled. Much like these sys-
tems, dialogue communicative goals are produced
at the higher level and imposed as constraints on
the lower-level. Environment-level processes fill in
the detail of these goals and handle contingencies
which may otherwise prevent the achievement of
these goals.
A number of interaction-management techniques
are present in the current implementation, including:
  A back-up recognition pass, using statistical
processing to extend grammar-based coverage
and provide immediate user ?help? feedback
for unrecognized utterances (Hockey et al,
2003);
  Turn management?timing of system output is
governed by monitoring the speech channel and
the (prioritized) agenda of speech outputs. If
the system need to take the turn, it grabs it using
only low-level processing;
  Handling user barge-in?user speech interrupts
system output and automatically grabs the turn;
  Immediate Grounding of recognized com-
mands (e.g. system says ?OK? immediately af-
ter recognizing the user: ?fly to the tower?);
  NP selection ? choosing anaphoric or salient
noun-phrases at the point of generation;
  Incremental aggregation of system-generated
utterances ? appropriately condensing and
forming elliptical system output at the point of
generation.
While this accounts for only a small number of
signals that arise during natural dialogue, the ar-
chitecture provides a framework for incorporat-
ing further techniques?in particular, using shallow
2Note: we are talking about very different parallel threads
here than those which occur in multi-modal fusion, such as oc-
curs in the SmartKom (Wahlster, 2002) system.
processing?for making use of such signals to pro-
vide more natural and robust interactions between
dialogue systems and human participants.
In the next section, we describe work from the lin-
guistic and psychology literature that demonstrates
the importance of asynchronous interaction-level
processing. In Section 3, we propose a specific ar-
chitecture that provides a framework for integrat-
ing various processes for channel-management. In
Sections 4 and 5, we describe specifics of the CSLI
implementation, outlining first the more abstract di-
alogue management layer, followed by techniques
employed at the interaction layer. In Section 6, we
discuss further possibilities and in Section 7 we con-
clude.
2 The Importance of Channel Phenomena
The standard processing model for dialogue systems
involves a sequence of modules from speech recog-
nition to speech synthesis, as illustrated in Figure 1,
which essentially illustrates (a simplification of) the
original TRIPS architecture, as described in (Fergu-
son and Allen, 1998). Typically, each module is self-
contained and relatively independent of other mod-
ules.
Recent findings in the psycholinguistic literature
have suggested various shortcomings of this mod-
ular approach. For example, work on alignment
indicates that conversation participants? processing
interacts on multiple levels, contravening the strict
modular model (Pickering and Garrod, 2003). This
is one of the considerations we address below, but
we are primarily concerned with other interaction-
level phenomena.
One of our prime motivations for an interaction
level processing layer is to ensure timely response
to interaction. Parsing and processing takes time?
this can be alleviated by incremental parsing tech-
niques, but meta-communication signals typically
do not need to be interpreted and processed to the
same extent as communicative utterances, and in-
stead require immediate attention that precludes full
processing.
For example, researchers have looked at the use
of um and uh in conversation and found that these
are often used as place-holders for a speaker who
wants to maintain their speaking turn (Clark and Fox
Tree, 2002). The detection of fillers such as these
generally acts to inhibit (to some extent) the listener
from interrupting or taking the turn from the current
speaker. Hence, not only should such discontinu-
ities not be ignored but they must also be processed
immediately in order to maintain the ongoing inter-
action.
Conversely, listeners also use what is known as
back-channel feedback to indicate to the speaker that
they are listening and paying attention. For En-
glish, back-channels include uh-huh, mhm and yeah.
Back-channels differ from other devices used to
keep a conversation flowing, such as repetitions and
collaborative finishes, in that they tend to be non-
specific to the current utterance. Moreover, back-
channel feedback is often produced without think-
ing, in response to simple prosodic clues such as a
speaker pause, a lowering of speaker pitch, or a rise
in speaker intonation (Ward and Tsukahara, 1999).
Most importantly, however, back-channel feed-
back is important to the speaker.3 Bavelas et al
(2000) investigated how a speaker in a conversation
(in this case someone narrating a story) is affected
when listener responses are inhibited. They found
that speakers with distracted and unresponsive lis-
teners did not finish their stories effectively, measur-
ably faltering at what should have been the dramatic
ending. Speakers needed an interlocutor?s feedback
to be able to maintain fluency and continue the dia-
logue effectively. Bavelas et alalso found that re-
sponse latency in one-on-one conversations is ex-
tremely short and may be simultaneous: listeners
can provide back-channels without fully listening to
the conversation partner and without being respon-
sible for taking up a speaking turn.
These results indicate that the nature of interac-
tion between participants is crucial to the collabora-
tive act of dialogue?signals and feedback that carry
effectively no communicative content are still im-
portant for keeping the interaction smooth and to en-
sure that the participants stay attentive and focussed
on the task at hand. When the dialogue task involves,
say, a human user being guided through a safety-
critical activity by an automated system, then such
issues are of particular importance.
3Allwood (1995) refers to such feedback morphemes as the
most important cohesion device in spoken language.
Speech Input Speech Output
Speech Recog GenerationNL Parser Speech SynthDialogueManager
Problem-Solving
Manager
Discourse
context
Figure 1: Traditional Dialogue System Architecture
Conversely, communicative behavior contains
signals regarding a participant?s attention, and in
particular may indicate a loss of focus. In tuto-
rial settings?one of the dialogue applications we
are specifically concerned with?this can be used
to determine students? confidence in their responses.
For example, phenomena such as timing between re-
sponses, hesitance markers, and intonation can all
be implicit clues that a student is having a problem
(Graesser et al, 1995).
3 A Two-Level Architecture
The traditional architecture for dialogue systems ba-
sically involves a linear approach to processing, as
illustrated in Figure 1. In this standard architecture,
modules tend to be self-contained and only loosely
dependent. Evidence outlined above, particularly
that related to alignment, suggests that this tightly
encapsulated approach will deal poorly with the in-
teractive nature of real dialogue. Allen et als (2001)
revised TRIPS architecture introduces a more non-
linear approach to dialogue processing, with asyn-
chronous processes managing interpretation, gener-
ation, and interface to behavioral aspects.
We augment the TRIPS approach by combining
multiple processes for interpreting utterances (e.g.
structured parsing versus statistical techniques) and
for generating responses (e.g. generation from se-
mantic representation versus template-based). More
fundamental to the architectural distinction we pro-
pose, the processing of an utterance and generat-
ing an appropriate response may proceed without
full processing by the Dialogue Management com-
ponent: information gleaned from an utterance will
always be passed up to the Dialogue Manager, but to
ensure timely response, an appropriate response may
be produced directly from a low-level component.
Other processes included at the interaction layer de-
tect non-communicative information, such as gaps
or delays in the user?s speech.
Figure 2 illustrates various aspects of the specific
two-level architecture we are developing. The lower
level interfaces directly with the user and, impor-
tantly, is driven by this interaction. For example the
low level includes a Turn Manager which manipu-
lates the speech channel to ensure that:
  user inputs are respected without interruption
(except when necessary);
  turn passes to the appropriate participant, based
on the highest priority Agenda item and the di-
alogue move that generated it;
  generated outputs are natural and timely;
  recognized user inputs are acknowledged
quickly using simple feedback utterances.
The upper level is responsible for modeling other
aspects of the conversational context, as well as
communicative goals and intentions. The con-
tent (i.e. logical forms) of user utterances are pro-
cessed using the dialogue model (e.g. updates and
adding nodes to the Dialogue Move Tree (Lemon et
al., 2002b)), and system utterances are constructed
which are in line with the system?s communicative
Dialogue
Move
Tree
Activity
Model
Context
Mgr ConversationPlanner
Agent
- intentions
- goals
- plans
- observations
Content layer:
- utterance planning
- communicative intentions
- grounding
- content management
- interaction with agent arch
Speech
recogition
and
Parsing
Backup
Shallow
Processor
(Helper)
Speech channel
Turn
Mgr
TTS
Generation
ModuleOutput
Agenda
Attention
Monitor
Interaction layer
- timing
- form
- engagement
- acknowledgement
Generation:
- anaphora
- pronouns
- aggregation
- echoing
ack
Figure 2: System Architecture
goals and intentions, whether they be imparting in-
formation to the user or requesting clarification or
further information.
The higher level also interacts with the rest of the
agent architecture, mediated by an Activity Model
(i.e. a representation of the agent activities about
which dialogue may occur (Gruenstein, 2002)). The
agent may wish to communicate its own goals, the
progress of its activities, or report on any observa-
tions it makes regarding its environment.
As with multi-layered agent architectures, the
two levels operate semi-autonomously and asyn-
chronously: the lower level is driven by tight in-
teraction with the user, while the upper level is
driven by longer-range communicative goals from
its activities and responses to user utterances. How-
ever, various types of information exchange connect
the two levels. For instance, user utterances rec-
ognized at the lower level must clearly be passed
to the content-management level to be parsed and
then incorporated into the dialogue context, while
high-level communication goals must be passed to
the lower level?s Output Agenda for generation and
speech-synthesis.
The Output Agenda plays a crucial role in medi-
ating utterances to be communicated, whether they
be system-initiated or responses, and generated from
the planner or a low-level component. The Output
Agenda is a prioritized list, where an utterance?s pri-
ority is influenced by a number of factors, such as:
whether it is in response to an error or misunder-
standing (i.e. ?Pardon?); the importance of the com-
municative content (i.e. an urgent observation); and
the dialogue move that generated it (e.g. answering a
question). The Agenda runs asynchronously, aggre-
gating multiple utterances when appropriate as well
as influencing speaker turn (see below).
Of perhaps greater interest, the interaction level
can be used to monitor user engagement and at-
tention in other ways ? e.g. time between utter-
ances, speaking rate, use of speech fillers ? to de-
tect potential problems as soon as possible, and to
provide early warning to the content layer that the
user may have, for example, misunderstood some
instruction. This can be used to generate a clarifi-
cation or grounding sub-dialogue, in order to estab-
lish mutual understanding before proceeding (thus
improving robustness of the system as a whole).
Conversely, expectations at the upper-layer can
influence processing at the interaction layer: for ex-
ample, open points of attachment on the Dialogue
Move Tree represent types of utterances the system
expects from the user, and these are used to prime
the recognition of incoming utterances for faster
processing, as well as influencing the turn.
In engineering terms, this division of labour is
attractive in that the clarity and modularity of dia-
logue management is enhanced. Rather than conflat-
ing, for example, turn-management with utterance
planning in a single generation component of a dia-
logue system, the separation into multiple levels of
processing allows different turn-taking and utterance
planning strategies to be developed independently,
and various combinations to be experimented with.
In the rest of the paper, we discuss our dialogue
management architecture and, in particular, the tech-
niques employed so far at each of the two levels de-
scribed here to enhance user experience and improve
overall system performance. The current implemen-
tation based on the above architecture is still being
refined; we focus on the features that have already
been implemented.
4 Top-Level Context Management
The approach to dialogue modeling we have imple-
mented is based on the theory of dialogue games
(Carlson, 1983; Power, 1979), and, for task-oriented
dialogues, discourse segments (Grosz and Sidner,
1986). These accounts rely on the observation that
answers generally follow questions, commands are
usually acknowledged, and so on, so that dialogues
can be partially described as consisting of adjacency
pairs of such dialogue moves. The notion of ?attach-
ment? of dialogue moves on a Dialogue Move Tree
(DMT) (Lemon et al, 2002b) embodies this idea.
An Activity Tree represents hierarchical and tem-
poral information about the task-state of the dia-
logue. Activities are the joint tasks managed by the
dialogue: e.g. booking a flight or moving a robot?
again, see (Lemon et al, 2002b) for details. Nodes
on the Activity Tree can be in various states (active,
complete, failed,  ), and any change in the state of
a node (typically because of an action by the agent)
is placed onto the system?s Output Agenda for po-
tential verbal report to the user, via the low-level
message selection and generation module.
This level of the architecture is where conversa-
tion planning and generation of system-initiated top-
ics occur. Any planned communication (whether it
be system-initiated or in response to a user utter-
ance) is put on to the Output Agenda, where it is
scheduled for generation.4 Conversely, true ground-
ing ? i.e. acknowledging that an utterance is un-
derstood within the context of the rest of the dia-
logue ? only occurs after the utterance has been in-
terpreted with respect to the DMT. Since a simple
acknowledgment may already have been generated
4The order in which outputs are generated, or even whether
they end up generated at all, depends on the priority of the cor-
responding information as well other interactions with the user.
after recognition, output after interpretation is only
needed if a response is required (e.g. the user asked
a question), or if a problem is detected (e.g. an am-
biguity must be resolved).
Since system communication is planned here, this
layer is also the one that interacts with the rest of the
agent architecture: any goals, state-changes, or ob-
servations that the agent may wish to communicate
are added as communicative goals, typically via the
Activity Model. For command-and-control applica-
tions (e.g. guiding a robot or UAV), system-initiated
utterances tend to be fairly short and simple and
conversation-planning is minimal; however, for our
dialogue-enabled tutorial application (Clark et al,
2001), conversation-planning is quite complex and
the system may generate multiple, relatively long ut-
terances on its own initiative.
5 Low-level Conversation Management:
Maintaining the Communication
Channel
We currently employ a range of shallow processing
techniques to maintain a smooth interaction with the
human dialogue participant. By ?shallow process-
ing? we mean processing that does not necessarily
result in or concern itself with the semantic repre-
sentation or pragmatic interpretation of the utterance
in the context of the dialogue. In particular, informa-
tion at this level is not processed in the context of the
Dialogue Move Tree or the Activity Tree.
In the following, we describe a number of the low-
level processing techniques currently implemented
in our system. Future work will address more of the
interaction phenomena described earlier.
5.1 Case study 1: Helper Feedback
In cases where a user utterance is not recognized, the
input is passed to a statistical recognizer of wider
coverage. This recognizer is often able to detect
lexical items and grammatical structures in the in-
put that are not covered by the first (grammar-based)
recognizer. In these cases, the results of the second
recognition pass are used to inform the user of the
system?s shortcomings, for example: ?The system
heard you say ?Look around for a red car?, but the
system does not know the word ?around?. You could
say ?Look for a red car? ?.
None of these utterances is planned or represented
at the top level of dialogue management. They are
produced simply to inform the user of a communi-
cation breakdown and to try to keep the communi-
cation flowing. If the user were to indulge in meta-
dialogue about the help message, then that message
would need to be represented in the high-level con-
text. However, we present the help message as being
generated by a different ?helper? agent, which dis-
appears (from the GUI) as soon as the help message
is produced, thus discouraging the user from engag-
ing it in dialogue.
User tests have shown that the use of this low level
module (which can be installed independently of the
high-level dialogue manager) significantly improves
task completion (both percentage of tasks completed
and time taken). By the fifth task, 100% of users
with the helper completed the task as compared with
80% of those without, and those without the helper
took on average 53% longer to complete the tasks.
For full details of the evaluation see (Hockey et al,
2003).
5.2 Case study 2: Turn Taking
Here we use a turn-marker at the low-level of dia-
logue processing. The turn can be marked as user,
system or none, and is set in a variety of ways. If
the user begins to speak (start-of-speech signal is re-
ceived from the recognizer) the turn becomes user
and any system audio output is stopped. If the sys-
tem needs to take the turn (e.g. if it has urgent in-
formation it needs to communicate), but turn is set
to user, and the user is not speaking, the system will
output ?Just a moment? and so take the turn before
generating its required utterance. Again, note that
this turn-grabbing utterance is not planned or repre-
sented at the top-level of dialogue moves. It does
not need to enter into such high-level plans or rep-
resentations because it is required only in order to
manipulate and maintain the channel, and does not
carry any content of its own.
The demonstration system displays a turn marker
on the GUI, allowing observers to monitor the
changing possession of the turn.
5.3 Case study 3: Incremental aggregation
Aggregation (Appelt, 1985) combines and com-
presses utterances to make them more concise, avoid
repetitious language structure, and make the sys-
tem?s speech more natural and understandable over-
all. In our system, this process is carried out not
at the level of content planning, but at the lower-
level of processing, where content logical forms are
manipulated (possibly combined) and converted into
strings for speech synthesis. Indeed, it is impor-
tant that aggregation functions at this lower level,
because the process needs access to:
  the message to be uttered (A),
  what has just been said (B),
  what is to be said next (C),
and the precise surface form of B is only represented
at the low-level. High-level processing only plans
the content of the utterance to be generated, and
passes it down, and so cannot determine the details
of the eventual surface form of the generated utter-
ance.
Aggregation techniques on a prewritten body of
text combine and compress sentences that have al-
ready been determined and ordered. In a complex
dialogue system however, aggregation should pro-
duce similarly natural output, but must function in-
crementally because utterances are generated on the
fly. In fact, when constructing an utterance we often
have no information about the utterances that will
follow it, and thus the best we can do is to com-
press it or ?retro-aggregate? it with utterances that
preceded it (see the example below). Only occasion-
ally does the Output Agenda contain enough unsaid
utterances to perform reasonable ?pre-aggregation?.
At the low-level of processing, the generator re-
ceives an item (on the Output Agenda) to be con-
verted into synthesized speech. This item consists
of a dialogue move type along with some content
(e.g. wh-answer, location(tower)).
Each dialogue move type (e.g. report, wh-
question, wh-answer) has its own aggregation rules,
stored in the class for that logical form (LF) type. In
each type, rules specify which other dialogue move
types can aggregate with it, and exactly how ag-
gregation works. The rules note identical portions
of LFs and unify them, and then combine the non-
identical portions appropriately.
For example, the LF that represents the phrase ?I
will fly to the tower and I will land at the parking
lot?, will be converted to one representing ?I will fly
to the tower and land at the parking lot? according
to the compression rules. Similarly, ?I will fly to the
tower and fly to the hospital? gets converted to ?I
will fly to the tower and the hospital?.
In contrast, the ?retro-aggregation? rules result in
sequences of system utterances such as,
Sys: I have cancelled flying to the base
Sys: and the tower
Sys: and landing at the school
Again, this process happens only at the low-level
processing stage of content realization, and needs
no access to the high-level representations of di-
alogue structure, history, and plans. A separate
thread running in the Output Agenda component
asynchronously performs aggregation as needed and
appropriate.
5.4 Case study 4: Choosing NPs
Another low-level process in utterance realization is
choosing appropriate NPs ? anaphoric expressions
such as ?it? or ?there?, or NPs which ?echo? those
already used by the human operator. Again, this rou-
tine does not need access to the high-level dialogue
management representations, but only to the list of
NPs employed in the dialogue thus far (the Salience
List).
Echoing is achieved by accessing the Salience
List whenever generating referential terms, and us-
ing whatever noun-phrase (if any) the user has pre-
viously employed to refer to the object in question.
Anaphoric phrases are generated whenever the ref-
erence object is the same as the one at the top of the
Salience List.
As in the case of aggregation, the top level content
generation algorithm does not manage the details of
utterance realization ? this is better handled at the
instant that the content logical form is to be trans-
lated into a string for the speech synthesizer. Other-
wise the top level would have to replan utterances af-
ter every intervening dialogue move. This example
shows how respecting the multi-level architecture is
desirable from an engineering point of view.
6 Current Implementation and Further
Possibilities
An initial version of the CSLI dialogue system based
on the described architecture has been implemented,
and is able to engage in conversations such as illus-
trated in Figure 3.
The system has been applied to both command-
and-control and tutorial applications; this is of inter-
est since the former generally involves user-initiated
conversations while in the latter, conversation tends
to be system-initiated. The Output Agenda me-
diates by handling both standard logical forms or
generation-templates.
Only a small number of the interaction-level phe-
nomena that arise in human-human dialogue have
been implemented, but we believe a number of them
could be treated within our framework. For in-
stance, processes at the lower level could detect mis-
communication and channel breakdown, and send a
request to the top level to replan the long-range dia-
logue strategy. This is particularly relevant in the tu-
torial setting, where low-level processes could detect
problems with user attention and responsiveness,
and prompt a switch to a different high-level strat-
egy. Particularly important for safety-critical appli-
cations, but of general use, would be low-level moni-
toring of channel noise and other environmental fac-
tors such as user gestures and gaze. Again, certain
combinations of these inputs would have high-level
consequences for interpretation and dialogue plan-
ning.
Recent work makes use of Icarus (Shapiro, 2001),
a reactive planning system that learns and adapts to
user behavior, to cover timing and realization of sys-
tem turns as well as handling delays in the user in-
put. In future, we anticipate that this will allow,
for instance, turn-taking facilities to be more eas-
ily adapted as personalities or situations require: for
example, after noticing a particular event the system
may be more likely to interrupt a speaker, or may
adapt to become less prone to interruption when in-
teracting with a speaker who responds poorly to sys-
tem barge-in.
7 Conclusion
We have presented evidence from the psycholinguis-
tic literature that demonstrates the importance of
Figure 3: Demonstration system, November 2002
Utterance Dialogue Move
Operator (O): Fly around the school Command
Helper (H): I heard you say ?Fly around the school? but the
system does not know the word ?around?. You could say ?y
to the school?
Helper feedback
O: Fly to the school Command
Robot (R): OK. Confirmation
O: Then pick up the medical supplies at the hospital Take turn, Command
R: OK. Wait a minute Confirmation, System grab turn
R: I am supposed to always y high Report Global Constraint
R: Shall I y to the hospital at low altitude anyway? Negotiate constraint relaxation, Re-
lease Turn
O: Yes. Do it anyway Yn-Answer
R: OK. Wait a minute Confirmation, System grab turn
R: There is a re at the tower Report observation
R: I am ying to the hospital Report Current Activity
O: Why? Why-question, Release Turn
R: Because I am picking up the medical sup  Why-Answer, User barge-in
O: Stop that. Command (Barge-in)
R: OK. I have cancelled ying there Confirmation, Report Activity State,
Anaphoric NP
O: Fight the re at the tower Command
R: OK. I am ying to the lake Confirmation, Report Activity State
O: Why are you doing that? Why-question, Release Turn
R: Because I am picking up water there. Why-Answer, Anaphoric NP
interaction-level phenomena in dialogue. Phenom-
ena such as fillers, timing, and feedback must be
dealt with properly and in a timely manner; other-
wise, the interaction and even the collaborative ac-
tivity being performed may be adversely affected.
Our approach is to cleanly separate the architec-
ture of dialogue systems to provide for an asyn-
chronous layer that is designed to handle interaction
signals, analogously to agent/robot architectures that
include a layer to manage interaction with a dynamic
environment. This allows processing outside the full
context of a dialogue history when required for pro-
cessing speed, while allowing the context to still in-
fluence such processes when able.
A system has been implemented based on this
architecture, containing a range of low-level pro-
cesses, which we have described here in some detail:
shallow-helper feedback; turn-management; aggre-
gation; NP selection. Current work is directed to-
wards incorporating techniques to manage further
phenomena?such as predictors of uncertainty and
loss of attention?in both command-and-control and
tutoring applications.
Acknowledgements
This research was partially supported by the Wallen-
berg Foundation?s WITAS project, Linko?ping Uni-
versity, Sweden, and by grant number N00014-02-
1-0417 from the Department of the US Navy. The
dialogue system was implemented while the first au-
thor was employed at CSLI, Stanford University.
References
James F. Allen, Bradford W. Miller, Eric K. Ringger, and
Teresa Sikorski. 1996. A robust system for natural
spoken dialogue. In Proceedings of ACL.
James Allen, George Ferguson, and Amanda Stent. 2001.
An architecture for more realistic conversational sys-
tems. In Proceedings of Intelligent User Interfaces
2001, Santa Fe, NM.
Jens Allwood. 1995. An activity based approach to prag-
matics. In Gothenburg Papers in Theoretical Linguis-
tics 76, Dept. of Linguistics, Uni. of Go?teborg.
Douglas E. Appelt. 1985. Planning english referring ex-
pressions. Artificial Intelligence, 26(1):1 ? 33.
J. B. Bavelas, L. Coates, and T. Johnson. 2000. Listeners
and co-narrators. Journal of Personality and Social
Psychology, 79:941?952.
Lauri Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. D. Reidel.
Herbert H. Clark and Jean E. Fox Tree. 2002. Using uh
and um in spontaneous speaking. Cognition, 84:73?
111.
Brady Clark, John Fry, Matt Ginzton, Stanley Pe-
ters, Heather Pon-Barry, and Zachary Thomsen-Gray.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proceedings of SIGdial
2001.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
George Ferguson and James Allen. 1998. TRIPS: An in-
telligent integrated problem-solving assistant. In Pro-
ceedings 15th National Conference on Artificial Intel-
ligence (AAAI-98), pages 567?573, Madison, WI.
James Firby. 1994. Task networks for controlling con-
tinuous processes. In Proceedings 2nd Int?l Conf. on
AI Planning Systems, pages 49?54.
A. C. Graesser, N. K. Person, and J. P. Magliano. 1995.
Collaborative dialogue patterns in naturalistic one-to-
one tutoring. Applied Cognitive Psychology, 9:1?28.
Barbara Grosz and Candace Sidner. 1986. Attentions,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Alexander H. Gruenstein. 2002. Conversational inter-
faces: A domain-independent architecture for task-
oriented dialogues. Masters thesis, Computer Science
Department, Stanford University.
Beth-Ann Hockey, Oliver Lemon, Ellen Campana, Laura
Hiatt, Gregory Aist, Jim Hieronymus, Alexander Gru-
enstein, and John Dowding. 2003. Targeted help for
spoken dialogue systems: intelligent feed back im-
proves naive users? performance. In Proceedings Eu-
ropean Assoc. for Computational Linguistics (EACL
03).
Oliver Lemon, Alexander Gruenstein, Alexis Battle, and
Stanley Peters. 2002a. Multi-tasking and collabo-
rative activities in dialogue systems. In Proceedings
of 3rd SIGdial Workshop on Discourse and Dialogue,
pages 113 ? 124, Philadelphia.
Oliver Lemon, Alexander Gruenstein, and Stanley Pe-
ters. 2002b. Collaborative activities and multi-tasking
in dialogue systems. Traitement Automatique des
Langues (TAL), 43(2):131 ? 154. Special Issue on Di-
alogue.
Jorge P. Mu?ller. 1996. The Design of Intelligent Agents?
A Layered Approach. Springer Verlag, Heidelberg,
Germany.
Martin Pickering and Simon Garrod. 2003. Toward a
mechanistic psychology of dialogue. Brain and Be-
havioral Science. to appear.
Richard Power. 1979. The organization of purposeful
dialogues. Linguistics, 17:107?152.
Daniel Shapiro. 2001. Value-driven agents. Ph.D. thesis,
Department of Management Science and Engineering,
Stanford University.
Jan van Kuppevelt, Ulrich Heid, and Hans Kamp. 2000.
Best practice in spoken language dialogue system en-
gineering. Natural Language Engineering, 6.
Wolfgang Wahlster. 2002. SmartKom: fusion and fission
of speech, gestures, and facial expressions. In Pro-
ceedings of the 1st International Workshop on Man-
Machine Symbiotic Systems, pages 213?225, Kyoto,
Japan.
N. Ward and W. Tsukahara. 1999. A responsive dialog
system. In Y. Wilks, editor, Machine Conversations,
pages 169?174. Kluwer.
DIPPER: Description and Formalisation of an
Information-State Update Dialogue System Architecture
Johan Bos, Ewan Klein, Oliver Lemon, Tetsushi Oka
ICCS, School of Informatics
University of Edinburgh
2 Buccleuch Place, Edinburgh EH8 9LW
Scotland, United Kingdom
{jbos,ewan,olemon,okat}@inf.ed.ac.uk
Abstract
The DIPPER architecture is a collection
of software agents for prototyping spoken
dialogue systems. Implemented on top
of the Open Agent Architecture (OAA),
it comprises agents for speech input and
output, dialogue management, and fur-
ther supporting agents. We define a for-
mal syntax and semantics for the DIP-
PER information state update language.
The language is independent of particular
programming languages, and incorporates
procedural attachments for access to ex-
ternal resources using OAA.
1 Introduction
Spoken dialogue systems are complex frameworks,
involving the integration of speech recognition,
speech synthesis, natural language understanding
and generation, dialogue management, and interac-
tion with domain-specific applications. These com-
ponents might be written in different programming
languages or running on different platforms. Fur-
thermore, with current developments in speech tech-
nology, many components for a dialogue system
can be obtained ?off-the-shelf?, particularly those
involving speech recognition and speech synthesis,
and to a lesser extent those for parsing and genera-
tion. The overall behaviour of a dialogue system is
controlled by the dialogue management component,
where interaction between the different components
is managed in a flexible way. Allowing for plug-
and-play and easy adaptation to new domains is a
challenging task for dialogue system architectures.
This paper presents DIPPER, an architecture
tailored for prototyping spoken dialogue systems,
based on the Open Agent Architecture (OAA). Al-
though DIPPER supports many off-the-shelf com-
ponents useful for spoken dialogue systems, it
comes with its own dialogue management compo-
nent, based on the information-state approach to di-
alogue modelling (Traum et al, 1999; Larsson and
Traum, 2000).
The TrindiKit (Larsson et al, 1999; Larsson,
2002) is regarded as the first implementation of the
information-state approach. However impressive it
is, on many occasions the TrindiKit tends to give
the impression of a ?Rube Goldberg? machine for
what is a relatively straightforward task: updating
the information state of the dialogue with the help of
declaratively stated update rules. What should be a
transparent operation is often obscured by the com-
plexity of the TrindiKit framework. The dialogue
management component of DIPPER borrows many
of the core ideas of the TrindiKit, but is stripped
down to the essentials, uses a revised update lan-
guage (independent of Prolog), and is more tightly
integrated with OAA. We argue that the resulting
formalism offers several advantages for developing
flexible spoken dialogue systems.
We will first introduce OAA and DIPPER agents
for building spoken dialogue systems, and explain
how dialogue management interfaces with compo-
nents in a flexible way (Section 2). Then we review
the information-state approach to dialogue mod-
elling, introduce the DIPPER update language (Sec-
tion 3), and compare it to the TrindiKit (Section 4).
Finally, we list some practical results obtained using
the DIPPER framework (Section 5).
2 The DIPPER Environment
This section gives an overview of DIPPER. First
we introduce the Open Agent Architecture, then we
present the various agents that play a role in spoken
dialogue systems. We focus on the dialogue move
engine in particular.
2.1 The Open Agent Architecture
The Open Agent Architecture, OAA for short, is a
framework for integrating several software agents,
possibly coded in different programming languages
(C/C++, Java, Prolog) and running on different plat-
forms (Unix, Linux, Windows), in a distributed en-
vironment (Martin et al, 1999). Because dialogue
systems are typically built out of a set of indepen-
dent components performing particular tasks (where
in many cases some of them are ?out-of-the-box?
packages, such as speech recognition or speech syn-
thesis), the OAA framework forms an ideal medium
to allow easy integration of software agents for di-
alogue systems in a prototyping development envi-
ronment.
The term ?agent? within OAA refers to a software
process meeting the conventions of the OAA frame-
work. Basically, this means providing services to
other agents in a particular form, using the Inter-
agent Communication Language (ICL). Within the
community of agents, service requests can be sub-
mitted to the ?facilitator?. This is a special agent
with knowledge of available agents and their ca-
pabilities. It mediates all interactions between the
agents involved in submitting and fulfilling a re-
quest.
A prototypical spoken dialogue system built on
top of OAA consists of an agent for speech recog-
nition, an agent for dialogue management, an agent
for speech synthesis, and several supporting agents
for specific tasks such as parsing, semantic interpre-
tation, and generation. A distributed agent architec-
ture allows the implementation of flexible and adapt-
able dialogue systems, where individual agents can
easily be added (or substituted by others) to extend
functionality of the overall system. It also allows
the integration of multi-modal input or output in a
straightforward way.
The current collection of DIPPER agents consists
of the following: (1) agents for input/output modali-
ties, (2) agents for the dialogue move engine, and (3)
supporting agents. We will describe the functional-
ity of the DIPPER agents in the remainder of this
section in terms of the services they provide. We will
use the OAA term ?solvable? to describe the services
offered by agents. The solvables of an agent are reg-
istered with the facilitator, and are implemented by
function calls (in C++ and Java) or predicate defi-
nitions (in Prolog) by the agents that provide them.
We will use + and - in front of arguments to indicate
passing or returning values.
2.2 Input/Output Agents
DIPPER supports agents for Nuance speech recog-
nition software (www.nuance.com) by providing
wrappers written in C++ or Java. The speech
recognition agent can be used in two different
modes: continuous speech recognition, calling
the solvable apply effects(+Effects) and
thereby updating the information state of the dia-
logue (see Section 3); and in callback mode, where
the solvable recognize(+Grammar,+Time,-
Input) starts recognition using the speech gram-
mar Grammar and returns Input, within a time
specified by Time. The value of Input is deter-
mined by the grammar used as language model for
speech recognition. Callback mode makes it easy to
plug in new grammars during different stages of the
dialogue so as to increase speech recognition perfor-
mance.
On the output side, DIPPER provides
agents for the speech synthesisers Fes-
tival (Taylor et al, 1998) and rVoice
(www.rhetorical.com). The solvables for these
output agents are text2speech(+Text) and
sable2speech(+Sable). The latter can be
used to synthesise strings marked up in SABLE,
an XML schema for text-to-speech (Sproat et al,
1998). A further agent is available to control Greta,
a three-dimensional talking head (Pasquariello and
Pelachaud, 2001).
2.3 Dialogue Management Agents
The dialogue manager forms the heart of a dialogue
system, reading the input modalities, updating the
current state of the dialogue, deciding what to do
next, and generating output. In terms of interac-
tion with other agents, it is the most complex com-
ponent. In fact, the DIPPER dialogue manager is
implemented as two cooperating OAA agents: the
dialogue move engine (DME), and a DME server.
The DME does the real work by dealing
with input from other agents (normally the in-
put modalities, such as speech recognition),
updating its internal state, and calling other
agents (normally the output modalities, such as
speech synthesis). The solvables of the DME
are check conds(+Conditions) and ap-
ply effects(+Effects). The former is used
for other agents to check the current state of the di-
alogue, the latter is used to change the state (for in-
stance by integrating results of speech recognition).
(At this point these services might seem fairly ab-
stract, but they will be made more concrete in Sec-
tion 3.)
The DME server is an agent mediating between
the DME and other agents. It collects requests
submitted by the DME, waits for the results, and
posts these back to the DME. The DME server
enables the DME to manage information-state up-
dates in an asynchronous way. Because the DME
server is implemented as a multi-threaded system, it
is able to cope with multiple requests at the same
time. The solvable that the DME server supports is
dme(+Call,+Effects). On receiving this call,
the DME server posts the solvable Call to the fa-
cilitator, waits for the result, and subsequently re-
turns the results to the DME using its solveable ap-
ply effects(+Effects).
Let?s illustrate this with an example. Suppose
that the dialogue system just asked the user a yes-no
question, and is ready to accept a yes-no answer. It
will need to tell the speech recognition agent to load
the grammar for yes/no-answers and return a result
(say, within 7 seconds) at the is?inputfield of the
dialogue state (see Section 3 for more details). This
is done by posting the solvable:
dme(recognize(?.YesNo?,7,X),
[set(is?input,X)])
To summarise the functionality of the DME,
there are three ways it is able to communi-
cate with other agents in a dialogue system:
(1) agents can call the DME agent directly, us-
ing check conds(+Conditions) and ap-
ply effects(+Effects); (2) the DME agent
can call other agents directly, in particular if it is
not interested in the results of those requests; (3) the
DME agent can use the DME server as a mediating
agent, normally when the results are needed for up-
dating the information state of the DME.
The advantage of this architecture is the flexibil-
ity imposed by it, while at the same time allow-
ing asynchronous interaction of the input/output and
supporting agents with the dialogue move engine.
2.4 Supporting Agents
OAA itself comes with agents for parsing and gen-
erating based on the Gemini system (Dowding et
al., 1993). DIPPER provides a further set of agents
to deal with natural language understanding, based
on Discourse Representation Theory (Kamp and
Reyle, 1993). There is an ambiguity resolution
agent that resolves underspecified DRSs into fully
resolved DRSs, and there is an inference agent that
checks consistency of DRSs, using standard first-
order theorem proving techniques, including the the-
orem prover SPASS (Weidenbach et al, 1999) and
the model builder MACE (McCune, 1998). DIPPER
also includes a high-level dialogue planning compo-
nent using O-Plan (Currie and Tate, 1991) which can
be used to build domain-specific content plans.
3 The Information-state Approach
In this section we will briefly review the
information-state approach and then introduce
a revised version of the TrindiKit?s dialogue move
engine (Traum et al, 1999), including a new update
language for information states.
3.1 Some History
Traditional approaches to dialogue modelling can
roughly be classified as dialogue state approaches
or plan-based approaches. In the former the dia-
logue dynamics are specified by a set of dialogue
states, each state representing the results of perform-
ing a dialogue move in some previous state. The lat-
ter are used for more complex tasks requiring flex-
ible dialogue behaviour. The information-state ap-
proach (Traum et al, 1999) is intended to combine
the strengths of each paradigm, using aspects of dia-
logue state as well as the potential to include detailed
semantic representations and notions of obligation,
commitment, beliefs, and plans.
The information-state approach allows a declara-
tive representation of dialogue modelling. It is char-
acterised by the following components:
1. a specification of the contents of the informa-
tion state of the dialogue,
2. the datatypes used to structure the information
state,
3. a set of update rules covering the dynamic
changes of the information state, and
4. a control strategy for information state updates.
As mentioned earlier, the first fully fledged imple-
mentation of the information-state approach was the
TrindiKit (Larsson et al, 1999). Written in Prolog,
the TrindiKit implements dialogue systems by defin-
ing information states, update and selection rules,
and control algorithms governing the rules to be ap-
plied to the information state. The DIPPER dialogue
move engine builds on the TrindiKit by adopting its
record structure and datatypes to define information
states. However, there are some fundamental dif-
ferences, the most important being that there are no
update algorithms in the DIPPER DME, there is no
separation between update and selection rules, and
the update rules are abstracted away from Prolog.
We will consider these differences in more detail in
Section 4.
3.2 Specifying Information States
The information state of a dialogue ?represents the
information necessary to distinguish it from other di-
alogues, representing the cumulative additions from
previous actions in the dialogue, and motivating fu-
ture action? (Traum et al, 1999). The term informa-
tion state is very abstract, and concepts such as men-
tal model, discourse context, state of affairs, conver-
sational score, and other variations on this theme can
be seen as instances of an information state.
Like TrindiKit, DIPPER defines information
states using a rich set of datatypes, including
records, stacks, and queues.1 The TrindiKit allows
developers to define specific information states, tai-
lored to a particular theory or a special task. An
information state is normally defined as a recursive
structure of the form Name:Type, where Name is an
identifier, and Type a datatype. Here is a simple ex-
ample:
Example 1 Information State Definition
is:record([grammar:atomic,
input:queue(atomic),
sem:stack(record([int:atomic,
context:drs]))]).
This example defines an information state as a
record named is, consisting of the fields grammar,
input, and sem. The field input is itself defined
as a queue of atomic typed structures, and the field
sem is defined as a stack of records containing the
fields int and context.
As in the TrindiKit, DIPPER uses a system of ref-
erences to anchor conditions and actions in the infor-
mation state. Each record consists of a set of fields.
Following the convention of the TrindiKit, we use
the operator ?, where a?b refers to the value of field
b in record a, and call these paths. For instance, the
path is?input in the above example refers to a
queue of terms of type atomic. Note that paths can
be arbitrarily long and may be used in conjunction
with functions defined in the update language, which
we will introduce in the next section.
3.3 The DIPPER Update Language
We will present the DIPPER update language here
in a rather informal way, merely by using examples.
(The reader is referred to the appendix for a precise
definition of the update language.) The update lan-
guage defines the core of the formalism underlying
the information state approach: the update rules.
An update rule is a triple ?name, conditions, ef-
fects?, with name a rule identifier, conditions a set of
tests on the current information state, and effects an
ordered set of operations on the information state.
Update rules specify the information state change
potential in a declarative way: applying an update
1For the purpose of this paper, we restrict ourselves to a
small number of datatypes, although the implementation sup-
ports further types including sets, ordered sets, numbers, and
discourse representation structures.
rule to an information state (assuming a shared vo-
cabulary of fields) results in a new state.
The conditions and effects of update rules are both
recursively defined over terms. The terms allow one
to refer to a specific value within the information
state, either for testing a condition, or for applying
an effect. There are two kinds of terms: standard
terms and anchored terms. The standard terms de-
fine the data structures for the types (atomic types,
queue, stack, records, and so on), whereas the an-
chored terms allow us to refer to sub-structures of
the information state (such as first and last to
refer to the first respectively last item of a queue).
A particularly useful anchored term is of the form
T?f, referring to a field f in a record T.
As we saw earlier the information state itself is a
structure of type record. We refer to the information
state object with the unique fixed name is (which
belongs to the anchored terms). To illustrate refer-
ence of terms with respect to a certain information
state, consider the following example, using the def-
inition as given in Example 1.
Example 2 Information State
is: grammar: ?.YesNo?
input: <>
sem: < int: model(...)
drs: drs([X,Y],[...]) >
As defined in the Appendix, we will use the
interpretation function [[.]]
s
for (standard and an-
chored) terms with respect to an information state
s. Now, with respect to the information state in
Example 2, the value of [[is?grammar]]
s
denotes
?.YesNo?, whereas the value of [[grammar]]
s
denotes grammar, because the term is not an-
chored. Similarly, [[top(is?sem)?drs]]
s
yields
drs([X,Y],[...]). However, note that
[[top(sem)?drs]]
s
is undefined. This term is not
well-formed since sem is of type atomic and not of
type record.
This example (and the ones that follow) illustrates
the power and ease with which we can refer to spe-
cific attributes of the information state, and thereby
specify the conditions and effects of update rules.
The crucial property of conditions is that they must
not change the content of the information state, and
are only used to inspect values denoted by paths in
the record defining the information state (such as
checking identity of terms or whether a queue is
empty or not), in order to trigger the effects of an up-
date rule. Effects, on the other hand are responsible
for changing the information state. There are two
kinds of effects: operations (defined over terms),
and solvables. The former include assignments of
values to information state attributes and operations
on datatypes such as stacks and queues. The latter
are OAA-solvables that allow us to fulfil requests
by supporting agents or input/output agents of the
dialogue system, which is a useful way of incorpo-
rating procedural attachment using the functionality
provided by OAA as described in Section 2. As a re-
sult, external actions are able to update the informa-
tion state, giving the properties of an asynchronous
architecture while maintaining a central unit for data
processing.
3.4 A simple example
The following (extremely simple) example illus-
trates the DIPPER architecture and the information
state update language. The example implements a
?parrot?, where the system simply repeats what the
user says. Four OAA agents are involved: one agent
for the speech recogniser, one for the synthesiser,
and an agent each for the DME and the DME server.
We will use the following information structure:
is:record([input:queue(basic),
listening:basic,
output:queue(basic)]).
That is, there are three fields: a queue containing
the input of the speech recogniser (we?re assuming
that the objects returned by the speech recogniser are
strings), an auxiliary field keeping track of whether
speech recognition is active or not, and an output
field for the text-to-speech synthesiser.
There are four update rules. The first rule,
timeout, deals with the situation where the
speech recognition returned ?timeout? (no speech
was recognised in the given time). In that case we
simply remove it from the queue.
urule(timeout,
[first(is?input)=timeout],
[dequeue(is?input)]).
By virtue of the second rule, process, we sim-
ply move the string from the input queue to the out-
put queue. (This is just done for the sake of the ex-
ample, we could have directly sent it to the synthe-
siser).
urule(process,
[non-empty(is?input)],
[enqueue(is?output,first(is?input)),
dequeue(is?input)]).
The third rule, synthesise, gives the string to
the synthesiser, by posting an OAA solvable. We are
not interested in any result that could be yielded by
the solvable, so the set of effects is empty here.
urule(synthesise,
[non-empty(is?output)],
[solve(text2speech(first(is?output)),[]),
dequeue(is?output)]).
A slightly more complicated rule is recog-
nise. It activates the speech recognition agent
(with the grammar ?.Simple?) when the system
is currently not listening, then sets the listening flag
to yes (to prevent application of the update rule
again). The results of speech recognition will be in-
tegrated by the effects stated as the third argument
of solve: the results will be placed in the input
field, and the flag listening is set to no again.
urule(recognise,
[is?listening=no],
[solve(X,recognise(?.Simple?,10),
[enqueue(is?input,X),
assign(is?listening,no)]),
assign(is?listening,yes)]).
Finally, we would like to make a remarks about
the dynamics of effects in update rules. The effects
are ordered, because the information state is updated
after each single effect, and hence the order in which
the effects are applied to the information state mat-
ters. Conditions in update rules, however, are not
ordered.
4 Comparison with TrindiKit
Now that we have introduced the DIPPER informa-
tion state update language, we are in a good position
to compare DIPPER?s approach to dialogue man-
agement that of the TrindiKit. We will consider the
use of variables, controlling update rules, and dis-
tributed processing.
4.1 Use of Variables
The DIPPER update language is essentially a
variable-free language (apart from the variables that
are used in solve/3 to return answers which are
then substituted for the variable?s occurrences in the
effects). In the TrindiKit, Prolog variables are used
for references to objects in the information state.
The scope of such variables includes the conditions
and effects of the update rule. The system of refer-
ence in DIPPER is functional rather than relational,
which we will illustrate with two examples.
Example 3 In DIPPER, pushing the top el-
ement of stack is?a on another stack is?b,
and consequently pop the first stack, the effects
[push(is?b,top(is?a)), pop(is?a)]
will be the way to achieve this. In the TrindiKit,
one would need the effects [is::fst(a,X),
is::pop(a), is::push(b,X)] to get the
same result, where X denotes a Prolog variable.
Example 4 Given the information state struc-
ture presented at the beginning of this section,
the term assign(top(is?sem)?int,m)
picks the first record out of a stack, and refers
to one of its fields (here, the field int).
In the TrindiKit, this needs to be coded as
[is::fst(sem,X),X::set(int,m)],
where again X denotes a Prolog variable.
In both examples the TrindiKit relies on Prolog
unification to obtain the correct results. As a con-
sequence, the order of conditions in the TrindiKit
is crucial. Furthermore, in the TrindiKit it is com-
mon practice to use variables in the conditions to re-
fer to values in the effects of update rules. Unifica-
tion combined with Prolog?s backtracking can some-
times lead to unexpected behaviour, causing errors
that are difficult to debug (Burke et al, 2002). The
DIPPER update language does not rely on Prolog,
and therefore poses no such problems for dialogue
system developers unfamiliar with Prolog.
4.2 Control in DIPPER
In contrast to the TrindiKit, which comes with a spe-
cial language to define the update control algorithm,
the control strategy used in DIPPER to select up-
date rules is simple and completely determined by
the update rules. Furthermore, there is no distinc-
tion between update and selection rules (used for
selecting a new dialogue move to be made by the
system) which the TrindiKit makes. The DIPPER
update algorithm is characterised by the following
pseudo-code:
1 WHILE running
2 deal with OAA-events;
3 IF there is a rule whose condi-
tions are satisfied by the informa-
tion state
4 THEN apply its effects;
5 ENDWHILE
Line 2 deals with external OAA agents requesting
a service from the DME, in this case the solvable
apply effects(+Effects). If there are any
such requests, the information state gets updated,
and the algorithm proceeds with line 3. Here we
simply choose the first rule in the database whose
conditions are satisfied by the information state and
apply its effects to the information state (line 4).
If there is no such rule, no updates take place and
only an external event can change the information
state. Note that the effects of at most one rule will
be applied before proceeding to the end of the while-
loop, ensuring that incoming OAA-events are regu-
larly checked.
4.3 OAA Integration
Allowing OAA-solvables in the effects of update
rules, a facility that the TrindiKit lacks, is an intu-
itive way of interfacing other components of a dia-
logue system (see the example update rules in Sec-
tion 3.4). This feature allows components to be eas-
ily replaced by others with the same functionality,
which is defined purely in terms of the OAA solv-
ables. For instance, changing the synthesiser does
not affect the dialogue management component.
The direct handle on OAA technology further al-
lows one to implement advanced functionality for
dialogue systems such as dealing with barge-in and
multi-modal input. Most spoken dialogue systems
exhibit a pipelined architecture with the following
components: automatic speech recognition ? nat-
ural language understanding ? dialogue manage-
ment ? natural language generation ? speech syn-
thesis. Because DIPPER builds on the OAA frame-
work, it allows developers to design asynchronous
dialogue systems in a relatively straightforward way.
5 Practical Results
5.1 Prototyping
As the example in the previous section demon-
strated, relatively little effort is required to build the
core of a new dialogue system. First of all, the de-
veloper needs to select the OAA agents. A skeleton
for a spoken dialogue system could consists of the
Nuance speech recognition agent, the DME, and a
synthesiser. Further work involves defining the in-
formation state, and the update rules. Once a core
system has been built, it is often easy to switch to
new domains, using a similar configuration as in pre-
viously implemented systems.
5.2 Debugging
A disadvantage of the information-state approach is
that it makes testing and debugging of dialogue sys-
tems notoriously difficult. The more advanced ap-
plications require at least a couple of dozen update
rules, and even for a relatively small set of rules de-
velopers tend to lose the overview of the intended
behaviour of their system.
Formal testing is one possibility, where intended
effects of update rules could be verified by future in-
formation states, or testing whether the conditions
of an update rule guarantee that its effects can be ap-
plied to any information state defined over the same
vocabulary. Given the formal specification of con-
ditions and effects, an interesting topic for future
research would be to apply model checking tech-
niques to dialogue system development. Most of
the model checking tools do not work on the more
complex datatypes required by the information-state
approach, although these probably can be translated
into some kind of propositional representation.
Practically, the DIPPER environment offers a
graphical user interface that assists during develop-
ment (Figure 1). This GUI starts and stops the DME
and keeps a history of updates. In addition, the de-
veloper is able to engage in ?time-travelling?, by
backtracking in the dialogue and restarting the di-
alogue from any point in the past.
Further functionality of the GUI includes the
?Step? function, which applies just one update rule
before returning control to the GUI. This function
is particularly helpful in verifying the intended ef-
fect of an update rule. Finally, the ?Spy? function
displays all rules that are satisfied by the current in-
formation state.
5.3 DIPPER Prototypes
The number of successful spoken dialogue proto-
types implemented using DIPPER is a convincing
proof-of-concept. Applications include conversa-
Figure 1: The Graphical User Interface of the DIP-
PER DME, showing the current information state,
the last applied update rule, and system messages.
tion with domestic appliances, as initiated by the
EU project D?Homme (Bos and Oka, 2002), ex-
plaining route descriptions to a mobile robot in a
miniature town, an EPSRC-funded project (Lauria
et al, 2001), and meaningful conversation with a
mobile robot in the basement of our department
(Theobalt et al, 2002). Currently we are work-
ing on a prototype dialogue system including the
Greta three-dimensional talking head (Pasquariello
and Pelachaud, 2001) as part of the EU project Mag-
iCster.
6 Conclusion
We presented the DIPPER framework for build-
ing spoken dialogue systems, based on the infor-
mation state theory of dialogue management. In
comparison to TrindiKit, we showed that DIPPER
provides a transparent and elegant way of declar-
ing update rules?independent of any particular pro-
gramming language, and with the ability to use ar-
bitrary procedural attachment via OAA. The sys-
tem incorporates many off-the-shelf OAA agents,
which we described, as well as a variety of sup-
port agents. The DIPPER resources are available at
http://www.ltg.ed.ac.uk/dipper.
We also presented the formal syntax and seman-
tics of our information-state update language. Al-
though it is up to the developer to ensure the va-
lidity of update rules, this formalisation could form
the basis of implementing an interpreter that proves
validity of update rules. This is an attractive task
for future work, and similar directions have been
suggested by (Ljunglo?f, 2000; Ferna?ndez, 2003) for
proving generic properties of dialogue systems.
Acknowledgements
Part of this work was supported by the EU Project
MagiCster (IST 1999-29078). We thank Nuance for
permission to use their software and tools.
References
J. Bos and T. Oka. 2002. An Inference-based Ap-
proach to Dialogue System Design. In COLING 2002.
Proceedings of the 19th International Conference on
Computational Linguistics, pages 113?119, Taipei.
C. Burke, L. Harper, and D. Loehr. 2002. A Dialogue
Architecture for Multimodal Control of Robots. In In-
ternational CLASS Workshop on Natural, Intelligent
and Effective Interaction in Multimodal Dialogue Sys-
tems.
K. Currie and A. Tate. 1991. O-Plan: the open planning
architecture. Artificial Intelligence, 52:49?86.
J. Dowding, M. Gawron, D. Appelt, L. Cherny, R. Moore,
and D. Moran. 1993. Gemini: A natural language sys-
tem for spoken language understanding. In Proceed-
ings of the Thirty-First Annual Meeting of the Associ-
ation for Computational Linguistics.
R. Ferna?ndez. 2003. A dynamic logic formalisation of
the dialogue gameboard. In Proceedings of the 10th
Conference of the European Chapter of the ACL. Stu-
dent Research Workshop, pages 17?24, Budapest.
H. Kamp and U. Reyle. 1993. From Discourse to Logic;
An Introduction to Modeltheoretic Semantics of Natu-
ral Language, Formal Logic and DRT. Kluwer, Dor-
drecht.
S. Larsson and D. Traum. 2000. Information state and di-
alogue management in the trindi dialogue move engine
toolkit. Natural Language Engineering, 5(3?4):323?
340.
S. Larsson, A. Berman, J. Bos, L. Gro?nqvist, P. Ljunglo?f,
and D. Traum. 1999. A model of dialogue moves
and information state revision. Technical Report D5.1,
Trindi (Task Oriented Instructional Dialogue).
S. Larsson. 2002. Issue-based Dialogue Management.
Ph.D. thesis, Goteborg University.
S. Lauria, G. Bugmann, T. Kyriacou, J.Bos, and E. Klein.
2001. Training Personal Robots Using Natural
Language Instruction. IEEE Intelligent Systems,
16(5):38?45, Sept./Oct.
P. Ljunglo?f. 2000. Formalizing the dialogue move en-
gine. In Go?talog workshop on semantics and prag-
matics of dialogue.
D. L. Martin, A. J. Cheyer, and D. B. Moran. 1999. The
open agent architecture: A framework for building dis-
tributed software systems. Applied Artificial Intelli-
gence, 13.
W. McCune. 1998. Automatic Proofs and Counterex-
amples for Some Ortholattice Identities. Information
Processing Letters, 65(6):285?291.
S. Pasquariello and C. Pelachaud. 2001. Greta: A simple
facial animation engine. In 6th Online World Confer-
ence on Soft Computing in Industrial Appications.
R. Sproat, A. Hunt, M. Ostendorf, P. Taylor, A. Black,
and K. Lenzo. 1998. Sable: A standard for tts markup.
In ICSLP98, pages 1719?1724.
P. A. Taylor, A. Black, and R. Caley. 1998. The archi-
tecture of the festival speech synthesis system. In The
Third ESCA Workshop in Speech Synthesis.
C. Theobalt, J. Bos, T. Chapman, A. Espinosa-Romero,
M. Fraser, G. Hayes, E. Klein, T. Oka, and R. Reeve.
2002. Talking to Godot: Dialogue with a Mobile
Robot. In Proceedings of IROS 2002.
D. Traum, J. Bos, R. Cooper, S. Larsson, I. Lewin,
C. Matheson, and M. Poesio. 1999. A model of dia-
logue moves and information state revision. Technical
Report D2.1, Trindi.
C. Weidenbach, B. Afshordel, U. Brahm, C. Cohrs, T. En-
gel, E. Keen, C. Theobalt, and D. Topic. 1999. System
description: Spass version 1.0.0. In Harald Ganzinger,
editor, 16th International Conference on Automated
Deduction, CADE-16, volume 1632 of LNAI, pages
314?318. Springer-Verlag, Berlin.
Appendix: Syntax and Semantics of the
DIPPER Update Language
The terms of the update language refer to a specific
value within the information state, either for testing
a condition, or for applying an effect. There are two
kinds of terms: standard terms and anchored terms.
Definition: Standard Terms.
1. All constants are standard terms of type atomic.
2. If T
1
, . . . ,T
n
are standard terms of type ? , then
?T
1
, . . . ,T
n
? is a standard term of type stack(? ).
3. If T
1
, . . . ,T
n
are standard terms of type ? , then
(T
1
, . . .T
n
) is a standard term of type queue(? ).
4. If f
1
, . . . ,f
n
are record fields, T
1
, . . . ,T
n
are terms of type ?
1
, . . . , ?
n
, then
[f
1
:T
1
, . . . ,f
n
:T
n
] is a standard term of
type record(f
1
:?
1
,. . .,f
n
:?
n
).
5. Standard Terms are only defined on the basis of
(1)?(4).
Definition: Anchored Terms.
1. is is an anchored term of type
record(f
1
:?
1
,. . .,f
n
:?
n
).
2. If T is an anchored term of type
record(. . .,f:? ,. . .), then T?f is an anchored
term of type ? .
3. If T is an anchored term of type queue(? ), then
first(T) and last(T) are anchored terms of
type ? .
4. If T is an anchored term of type stack(? ), then
top(T) is an anchored term of type ? .
5. If T is an anchored term of type queue(? ) or
stack(? ), then member(T) is an anchored term
of type ? .
6. Anchored terms are only defined on the basis
of (1)?(5).
The interpretation function [[.]]
s
for (standard and an-
chored) terms with respect to an information state s
is defined as follows.
Definition: Reference of Terms.
1. [[T]]
s
= T iff T is a standard term.
2. [[is]]
s
= s.
3. [[T?f]]
s
= the value of field f in [[T]]
s
.
4. [[top(T)]]
s
= the top member of [[T]]
s
iff T is of
type stack().
5. [[first(T)]]
s
= the first member of [[T]]
s
iff T
is of type queue().
6. [[last(T)]]
s
= the last member of [[T]]
s
iff T is
of type queue().
7. [[member(T)]]
s
= a member of [[T]]
s
iff T is of
type stack() or of type queue().
Now we define the syntax and semantics of update
rule conditions in DIPPER. For the interpretation
of conditions we use a truth-conditional semantics
mapping conditions to one of the values 1 (?true?) or
0 (?false?), defined with the help of an interpretation
function I with respect to an information state s.
Definition: Syntax of Conditions.
1. If T
1
and T
2
are (standard or anchored) terms
of the same type, then T
1
=T
2
and T
1
6=T
2
are
conditions.
2. If T is a (standard or anchored) term of type
stack(? ), or queue(? ), then empty(T) and
non empty(T) are conditions.
3. Conditions are only defined on the basis of (1)
and (2).
Definition: Semantics of Conditions.
1. I
s
(T
1
=T
2
) = 1 iff [[T
1
]]
s
= [[T
2
]]
s
2. I
s
(T
1
6=T
2
) = 1 iff [[T
1
]]
s
6= [[T
2
]]
s
3. I
s
(empty(T)) = 1 iff [[T]]
s
denotes a stack or
queue containing no elements.
4. I
s
(non empty(T)) = 1 iff [[T]]
s
denotes a stack
or queue containing at least one element.
Definition: Information State Satisfaction.
An information state s satisfies a set of conditions C
iff ?c : c ? C ? [[c]]
s
= 1.
The effects in an update rule are responsible for
changing the information state. There are two kinds
of effects: operations defined over terms, and solv-
ables.
Definition: Syntax of Effects.
1. If T
1
is an anchored term of type ? and T
2
a (standard or anchored) term of type ? , then
assign(T
1
,T
2
) is an effect.
2. If T
1
is an anchored term of type stack(? ) and
T
2
a (standard or anchored) term of type ? ,
then clear(T
1
), pop(T
1
), and push(T
1
,T
2
)
are effects.
3. If T
1
is an anchored term of type queue(? )
and T
2
a (standard or anchored) term of type
? , then clear(T
1
), dequeue(T
1
), and en-
queue(T
1
,T
2
) are effects.
4. If the term S is an n-place OAA-solvable,
T
1
,. . .,T
n
are (standard or anchored) terms,
E(x) an ordered (possibly empty) set of
effects with free occurrences of x, then
solve(x,S(T
1
,. . .,T
n
),E) is an effect.
5. Effects are only defined on the basis of (1)?(4).
The semantics of the effects are defined with the
help of the function U: s ? E?s from an informa-
tion state and an effect to a new information state.
(Some notational conventions: We will use the nota-
tion s[T]s0 to mean that the information states s and
s
0 are the same except for the value of [[T]]
s
. We will
use E[t/u] to mean substituting t for u in E).
Definition: Semantics of Effects.
1. U(s,assign(T,T0)) = s0 if s[T]s0 and [[T]]
s
? =
[[T0]]
s
.
2. U(s,clear(T)) = s0 if s[T]s0 and [[T]]
s
? = ??.
3. U(s,pop(T)) = s0 if s[T]s0 and [[T]]
s
=
?t
1
, t
2
, . . . , t
n
? and [[T]]
s
? = ?t
2
, . . . , t
n
?.
4. U(s,push(T,T0)) = s0 if s[T]s0 and [[T]]
s
=
?t
1
, . . . , t
n
? and [[T]]
s
? = ?[[T0]]
s
, t
1
, . . . , t
n
?.
5. U(s,dequeue(T)) = s0 if s[T]s0 and [[T]]
s
=
(t
1
, t
2
, . . . , t
n
) and [[T]]
s
? = (t
2
, . . . , t
n
).
6. U(s,enqueue(T,T0)) = s0 if s[T]s0 and [[T]]
s
=
(t
1
, . . . , t
n
) and [[T]]
s
? = (t
1
, . . . , t
n
, [[T0]]
s
).
7. U(s,solve(x,S(T
1
,. . .,T
n
),E)) = s if for all an-
swers a returned by solve(S([[T
1
]]
s
,. . .,[[T
n
]]
s
))
there is an s0 such that the effects E[a/x] are ap-
plied to s0.
Definition: Update.
An ordered set of effects {e
1
, . . . , e
n
} are success-
fully applied to an information state s, resulting an
information state s0 if U(e
1
,s)=s
1
,. . ., U(e
i
,s
i?1
)=s
i
,. . ., U(e
n
,s
n?1
)=s0.
Proceedings of the 12th European Workshop on Natural Language Generation, pages 74?81,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
Learning Lexical Alignment Policies for Generating
Referring Expressions in Spoken Dialogue Systems
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
s.janarthanam@ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
olemon@inf.ed.ac.uk
Abstract
We address the problem that different
users have different lexical knowledge
about problem domains, so that automated
dialogue systems need to adapt their gen-
eration choices online to the users? domain
knowledge as it encounters them. We ap-
proach this problem using policy learning
in Markov Decision Processes (MDP). In
contrast to related work we propose a new
statistical user model which incorporates
the lexical knowledge of different users.
We evaluate this user model by showing
that it allows us to learn dialogue poli-
cies that automatically adapt their choice
of referring expressions online to differ-
ent users, and that these policies are sig-
nificantly better than adaptive hand-coded
policies for this problem. The learned
policies are consistently between 2 and
8 turns shorter than a range of different
hand-coded but adaptive baseline lexical
alignment policies.
1 Introduction
In current ?troubleshooting? spoken dialogue sys-
tems (SDS)(Williams, 2007), the major part of the
conversation is directed by the system, while the
user follows the system?s instructions. Once the
system decides what instruction to give the user
(at the dialogue management level), it faces sev-
eral decisions to be made at the natural language
generation (NLG) level. These include, deciding
which concepts to include in the utterance, decid-
ing the referring expressions (RE) to use in the ut-
terance and so on. A little-studied problem is to
what extent a system could automatically align to
the user?s lexical knowledge by adapting its RE
choices, in particular based on his domain exper-
tise, and how this can be modelled and optimised
computationally.
(Issacs and Clark, 1987) show how two in-
terlocutors adapt their language in a conversa-
tion by assessing each other?s domain expertise
during dialogue, by observing how they react to
each other?s RE choices. This is called align-
ment through Audience Design (Clark, 1996; Bell,
1984). Using inappropriate REs in instructions has
been identified as a serious problem affecting a
system?s usability (Molich and Nielsen, 1990).
In this paper, we treat NLG within a com-
putational learning paradigm (Lemon, 2008;
Rieser and Lemon, 2009; Janarthanam and
Lemon, 2008). We examine whether a SDS can
automatically learn a lexical alignment policy
for audience design, which enables it to choose
appropriate REs by predicting the user?s lexical
knowledge dynamically during the course of the
dialogue. This can avoid clarification requests
from the users and keep the dialogues short. The
example given below describes the kind of lexical
alignment behaviour that we want the system to
learn. The system chooses ?small white box?
instead of ?ADSL filter? and ?monitor symbol?
instead of ?network icon?, because it learnt that
the user is a novice based on their clarification
requests. However, it switches to using technical
terms like ?browser?, when it learns that the
user is not a complete novice (as he verifies the
description for the network icon in Usr 4).
Example 1.
Sys 1>> Is your router connected
to the computer?
Usr 1>> Uh. What?s a router?
Sys 2>> It?s the big black box.
Usr 2>> Ok.. yes.
Sys 3>> Do you see a small white
box connected to the router?
Usr 3>> Yes.
Sys 4>> Ok. Is there a flashing
monitor symbol at the bottom
74
right of the screen?
Usr 4>> the network icon?
Sys 5>> Yes. Is it flashing?
Usr 5>> Yes. It is flashing.
Sys 6>> Ok. Please open your
browser.
We study lexical alignment in the context of
troubleshooting dialogues ? where users wish to
mend their broken internet connection. This task
demands a new type of User Simulation.
In this paper, after a review of related work
(section 2), we first present (section 3) a statisti-
cal User Simulation which supports different do-
main knowledge profiles and reacts accordingly to
questions or instructions from an SDS. We then
present a reinforcement learning model of lexical
alignment due to audience design (in sections 4 &
5). We then evaluate the User Simulation (section
6), testing whether a simulation that is sensitive to
a system?s RE choices can be used to learn good
lexical alignment policies. Finally, we compare
policies learned in interaction with the User Sim-
ulation with hand-coded policies, and present the
results in section 7.
2 Related work
Several statistical user simulation models that
model a user?s behaviour in a conversation have
been proposed (Georgila et al, 2005; Schatzmann
et al, 2006; Schatzmann et al, 2007). These mod-
els issue task specific dialogue acts like inform-
ing their search constraints, confirming values, re-
jecting misrecognised values, etc. However, they
do not model a user population with varying do-
main expertise. Also, none of these models seek
clarification at conceptual or lexical levels that oc-
cur naturally in conversations between real users.
(Komatani et al, 2003) proposed using user mod-
els with features like skills, domain knowledge
and hastiness as a part of the dialogue manager
to produce adaptive responses. (Janarthanam and
Lemon, 2008) presented a user simulation model
that simulates a variety of users with different do-
main knowledge profiles. Although this model
incorporated clarification acts at the conceptual
level, these users ignore the issues concerning the
user?s understanding of the REs used by the sys-
tem. In this work, in contrast to the above, we
present a User Simulation model which explicitly
encodes the user?s lexical knowledge of the do-
main, understands descriptive expressions, and is-
sues clarification requests at the lexical level.
3 User Simulation
Our User Simulation module simulates dialogue
behaviour of different users, and interacts with the
dialogue system by exchanging both dialogue acts
and REs. It produces users with different knowl-
edge profiles. The user population produced by
the simulation comprises a spectrum from com-
plete novices to experts in the domain. Simulated
users behave differently from one another because
of differences in their knowledge profiles. Simu-
lated users are also able to learn new REs during
interaction with the SDS. These new expressions
are held in the user simulation?s short term mem-
ory for later use in the conversation. Simulated
users interact with the environment using an in-
teractive mechanism that allows them to observe
and manipulate the states of various domain ob-
jects. The interaction between the user and the
other components is given in figure 1 (notations
explained in later sections).
Figure 1: Experimental setup
3.1 Domain knowledge model
Domain experts know most of the technical terms
that are used to refer to domain objects whereas
novice users can only reliably identify them when
descriptive expressions are used. While in the
model of (Janarthanam and Lemon, 2008) knowl-
edge profiles were presented only at conceptual
levels (e.g. does the user know what a modem is?),
we present them in a more granular fashion. In
this model, the user?s domain knowledge profile
is factored into lexical (LKu,t), factual (FKu,t)
and procedural knowledge (PKu,t) components.
75
Lexical knowledge LKu,t
vocab([modem, router], dobj1)
vocab([wireless, WiFi], dobj3)
vocab([modem power light], dobj7)
Factual knowledge FKu,t
location(dobj1)
location(dobj7)
Procedural knowledge PKu,t
procedure(replace filter)
procedure(refresh page)
Table 1: Knowledge profile - Intermediate user.
A user?s lexical knowledge is encoded in the for-
mat:
vocab(referring expressions, domain object)
where referring expressions can be a list of ex-
pressions that the user knows can be used to talk
about each domain object.
Whether the user knows facts like the location
of the domain objects (location(domain object)) is
encoded in the factual component. Similarly, the
procedural component encodes the user?s knowl-
edge of how to find or manipulate domain objects
(procedure(domain action)). Table 1 shows an ex-
ample user knowledge profile.
In order to create a knowledge spectrum, a
Bayesian knowledge model is used. The current
model incorporates patterns of only the lexical
knowledge among the users. For instance, peo-
ple who know the word ?router? most likely also
know ?DSL light? and ?modem? and so on. These
dependencies between REs are encoded as condi-
tional probabilities in the Bayesian model. Figure
2 shows the dependencies between knowledge of
REs.
Figure 2: Bayes Net for User Lexical Knowledge
Using this Bayesian model, we instantiate dif-
ferent knowledge profiles for different users. The
current conditional probabilities were set by hand
based on intuition. In future work, these values
will be populated based on simple knowledge sur-
veys performed on real users (Janarthanam and
Lemon, 2009). This method creates a spectrum of
users from ones who have no knowledge of tech-
nical terms to ones who know all the technical
jargon, though every profile will have a different
frequency of occurrence. This difference in fre-
quency reflects that expert users are less common
than novice users.
The user?s domain knowledge can be dynami-
cally updated. The new REs, both technical and
descriptive, presented by the system through clar-
ification moves are stored in the user?s short term
memory. Exactly how long (in terms of dialogue
turns) to retain the newly acquired knowledge is
given by a retention index RIu. At the end of RIu
turns, the lexical item is removed from user?s short
term memory.
3.2 User Dialogue Action set
Apart from environment-directed acts, simulated
users issue a number of dialogue acts. The list of
dialogue actions that the user can perform in this
model is given in Table 2. It consists of default
moves like provide info and acknowledge as well
as some clarification moves. Request description
is issued when the SDS uses technical terms that
the simulated user does not know, e.g. ?What is
a router??. Request verification is issued when
the SDS uses descriptive lexical items for do-
main objects that the user knows more techni-
cal terms for, e.g. System: ?Is the black box
plugged in?? User: ?Do you mean the router??.
Request disambiguation is issued when the user
faces an underspecified and ambiguous descrip-
tive expression, e.g.?User: I have two black boxes
here - one with lights and one without. Which
one is it??. These clarification strategies have
been modeled based on (Schlangen, 2004). The
user simulation also issues request location and
request procedure dialogue acts, when it does not
know the location of domain objects or how to ma-
nipulate them, respectively.
3.3 Environment simulation
The environment simulation includes both physi-
cal objects, such as the computer, modem, ADSL
filter, etc and virtual objects, such as the browser,
control panel, etc in the user?s environment. Phys-
ical and virtual connections between these objects
76
report problem
provide info(dobj, info)
acknowledge
request verification(x, y)
request description(x)
request disambiguation(x, [y1,y2])
request location(dobj)
request procedure(daction)
thank system
Table 2: User Dialogue Acts.
are also simulated. At the start of every dialogue,
the environment is initiated to a faulty condition.
Following a system instruction or question, the
user issues two kinds of environment acts. It is-
sues an observation act Ou,t to observe the status
of a domain object and a manipulation act Mu,t
to change the state of the environment (Se,t). The
simulation also includes task irrelevant objects in
order to confuse the users with underspecified de-
scriptive expressions. For instance, we simulate
two domain objects that are black in colour - an
external hard disk and a router. So, the users may
get confused when the system uses the expression,
?black box?.
3.4 User Action Selection
User Action selection has several steps. The user?s
dialogue behaviour is described in the action se-
lection algorithm (Table 3). Firstly, the user must
identify all the RE choices (RECs,t) that are used
to refer to different domain objects (dobj) and
domain actions (daction) in the system instruc-
tion (step 1). Secondly, the user?s knowledge of
the prerequisite factual (FKprereq) and procedural
(PKprereq) knowledge components connected to
the observation or manipulation action is checked.
If the user does not satisfy the knowledge re-
quirements, the user simulation issues an appro-
priate clarification request (steps 2 & 3). After
the knowledge requirements are satisfied, the user
issues environment directed actions and responds
to system instruction As,t (steps 4 & 5). When
the system provides the user specific information,
they are added to the user?s short term memory
(steps 6-8). Although, the action selection process
is deterministic at this level, it is dependent on
the users? diverse knowledge profiles, which en-
sures stochastic dialogue behaviour amongst dif-
ferent users created by the module.
greet the user
request status(x)
request action(x)
give description(x)
accept verification(x,y)
give location(dobj)
give procedure(daction)
close dialogue
Table 4: System Dialogue acts.
4 Dialogue System Model
The dialogue system is modeled as a reinforce-
ment learning agent in a Markov Decision Pro-
cess framework (Levin et al, 1997). At every
turn, it interacts with the Simulated User by issu-
ing a System Dialogue Act (As,t) along with a set
of REs, called the System RE Choices (RECs,t).
RECs,t contains the REs that refer to various do-
main objects in the dialogue act As,t. First, the
system decides the dialogue act to issue using a
hand-coded dialogue strategy. Troubleshooting in-
structions are coded in the troubleshooting deci-
sion tree1. Dialogue repair moves include select-
ing clarification moves in response to user?s re-
quest. The list of system dialogue acts is given
Table 4.
The system issues various repair moves when
the users are unable to carry out the system?s in-
structions due to ignorance, non-understanding or
the ambiguous nature of the instructions. The
give description act is used to give the user a de-
scription of the domain object previously referred
to using a technical term. It is also used when
the user requests disambiguation. Similarly, ac-
cept verification is given when the user wants to
verify whether the system is referring to a certain
domain object y using the expression x.
After selecting the dialogue act As,t, a set
of REs must be chosen to refer to each of
the domain objects/actions used in the dia-
logue act. For instance, the dialogue act re-
quest status(router dsl light) requires references
to be made to domain objects ?router? and ?DSL
light?. For each of these references, the system
chooses a RE, creating the System RE Choice
RECs,t. In this study, we have 7 domain objects
and they can either be referred to using technical
1The Troubleshooting decision tree was hand-built using
guidelines from www.orange.co.uk and is similar to the one
used by their Customer Support personnel
77
Input: System Dialogue Act As,t, System Referring Expressions Choice RECs,t
and User State Su,t: LKu,t, FKu,t, PKu,t
Step 1. ? x ? RECs,t
Step 1a. if (vocab(x, dobj)? LKu,t) then next x.
Step 1b. else if (description(x, dobj) & ? j ((is jargon(j) & vocab(j, dobj) /? LKu,t))) then next x.
Step 1c. else if (is jargon(x) & (vocab(x, dobj) /? LKu,t)) then return request description(x).
Step 1d. else if (is ambiguous(x)) then return request disambiguation(x).
Step 1e. else if (description(x, dobj) & ? j ((is jargon(j) & vocab(j, dobj) ? LKu,t)))
then return request verification(x, j).
Step 2. if (?dobj location(dobj) ? FKprereq & location(dobj) /? FKu,t)
then return request location(dobj).
Step 3. else if (?daction procedure(daction) ? PKprereq & procedure(daction) /? PKu,t)
then return request procedure(daction).
Step 4. else if (As,t = request status(dobj))
then observe env(dobj, status), return provide info(dobj, status)
Step 5. else if (As,t = request action(daction))
then manipulate env(daction), return acknowledge.
Step 6. else if (As,t = give description(j, d) & description(d, dobj))
then add to short term memory(vocab(j, dobj)), return acknowledge.
Step 7. else if (As,t = give location(dobj))
then add to short term memory(location(dobj)), return acknowledge.
Step 8. else if (As,t = give procedure(daction))
then add to short term memory(procedure(daction)), return acknowledge.
Table 3: Algorithm: Simulated User Action Selection
terms or descriptive expressions. For instance, the
DSL light on the router can be descriptively re-
ferred to as the ?second light on the panel? or us-
ing the technical term, ?DSL light?. Sometimes
the system has to choose between a lesser known
technical term and a well-known one. Some de-
scriptive expressions may be underspecified and
therefore can be ambiguous to the user (for ex-
ample, ?the black box?). Choosing inappropri-
ate expressions can make the conversation longer
with lots of clarification and repair episodes. This
can lead to long frustrating dialogues, affecting the
task success rate. Therefore, the dialogue system
must learn to use appropriate REs in its utterances.
The RE choices available to the system are given
in Table 5.
The system?s RE choices are based on a part
of the dialogue state that records which of the
technical terms the user knows. These variables
are initially set to unknown (u). During the di-
alogue, they are updated to user knows (y) or
user doesnot know (n) states. We therefore record
the user?s lexical knowledge during the course of
the dialogue and let the system learn the statistical
usage patterns by itself. Part of the dialogue state
1. router / black box / black box with lights
2. power light / first light on the panel
3. DSL light / second light on the panel
4. online light / third light on the panel
5. network icon / flashing computer symbol
6. network connections / earth with plug
7. WiFi / wireless
Table 5: System RE choices.
relevant to system?s RE choices is given in Table 6.
The state can be extended to include other rele-
vant information like the usage of various REs by
the user as well to enable alignment with the user
through priming (Pickering and Garrod, 2004) and
personal experience (Clark, 1996). However they
are not yet implemented in the present work.
5 Reward function
The reward function calculates the reward
awarded to the reinforcement learning agent at
the end of each dialogue session. Successful
task completion is rewarded with 1000 points.
Dialogues running beyond 50 turns are deemed
78
Feature Values
user knows router y/n/u
user knows power light y/n/u
user knows dsl light y/n/u
user knows online light y/n/u
user knows network icon y/n/u
user knows network connections y/n/u
user knows wifi y/n/u
Table 6: (Part of) Dialogue state for Lexical Align-
ment.
unsuccessful and are awarded 0 points. The
number of turns in each dialogue varies according
to the system?s RE choices and the simulated
user?s response moves. Each turn costs 10 points.
The final reward is calculated as follows:
TaskCompletionReward(TCR) = 1000
TurnCost(TC) = 10
TotalTurnCost(TTC) = #(Turns) ? TC
FinalReward = TCR? TTC
The reward function therefore gives high re-
wards when the system produces shorter dia-
logues, which is possible by adaptively using ap-
propriate REs for each user.
6 Training
The system was trained to produce an adaptive
lexical alignment policy, which can adapt to users
with different lexical knowledge profiles. Ideally,
the system must interact with a number of dif-
ferent users in order to learn to align with them.
However, with a large number of distinct Bayesian
user profiles (there are 90 possible user profiles),
the time taken for learning to converge is exorbi-
tantly high. Hence the system was trained with
selected profiles from the distribution. It was
initially trained using two user profiles from the
very extremes of the knowledge spectrum pro-
duced by the Bayesian model - complete experts
and complete novices. In this study, we cali-
brated all users to know all the factual and proce-
dural knowledge components, because the learn-
ing exercise was targeted only at the lexical level.
With respect to the lexical knowledge, complete
experts knew all the technical terms in the do-
main. Complete novices, on the other hand, knew
only one: power light. We set the RIu to 10,
so that the users do not forget newly learned lexi-
cal items for 10 subsequent turns. Ideally, we ex-
pected the system to learn to use technical terms
with experts and to use descriptive expressions
with novices and a mixture for intermediates. The
system was trained using SARSA reinforcement
learning algorithm (Sutton and Barto, 1998), with
linear function approximation, for 50000 cycles.
It produced around 1500 dialogues and produced
an alignment policy (RL1) that adapted to users
after the first turn which provides evidence about
the kind of user the system is dealing with.
The system learns to get high reward by pro-
ducing shorter dialogues. By learning to choose
REs by adapting to the lexical knowledge of the
user, it avoids unnecessary clarification and repair
episodes. It learns to choose descriptive expres-
sions for novice users and jargon for expert users.
It also learns to use technical terms when all users
know them (for instance, ?power light?). Due to
the user?s high retention (10 turns), the system
learned to use newly learned items later in the di-
alogue.
We also trained another alignment policy (RL2)
with two other intermediate high frequency user
lexical profiles. These profiles (Int1 and Int2)
were chosen from either ends of the knowledge
spectrum close to the extremes. Int1 is a knowl-
edge profile that is close to the novice end. It
only knows two technical terms: ?power light?
and ?WiFi?. On the other hand, Int2 is profile
that is close to the expert end and knows all tech-
nical terms except: ?dsl light? and ?online light?
(which are the least well-known technical terms
in the user population). With respect to the other
knowledge components - factual and procedural,
both users know every component equally. We
trained the system for 50000 cycles following the
same procedure as above. This produced an align-
ment policy (RL2) that learned to optimize the
moves, similar to RL1, but with respect to the
given distinct intermediate users.
Figure 3 shows the overall dialogue reward for
the 2 policies during training.
Both policies RL1 and RL2, apart from learn-
ing to adapt to the users, also learned not to use
ambiguous expressions. Ambiguous expressions
lead to confusion and the system has to spend ex-
tra turns for clarification. Therefore both policies
learnt to avoid using ambiguous expressions.
Figure 4 shows the dialogue length variation for
the 2 policies during training.
79
7 Evaluation and baselines
We evaluated both the learned policies using a test-
ing simulation and compared the results to other
baseline hand-coded policies. Unlike the train-
ing simulation, the testing simulation used the
Bayesian knowledge model to produce all differ-
ent kinds of user knowledge profiles. It produced
around 90 different profiles in varying distribution,
resembling a realistic user population. The tests
were run over 250 simulated dialogues each.
Several rule-based baseline policies were man-
ually created for the sake of comparison:
1. Random - Choose REs at random.
2. Descriptive only - Only choose descriptive
expressions. If there is more than one de-
scriptive expression it picks one randomly.
3. Jargon only - Chooses the technical terms.
4. Adaptive 1 - It starts with a descriptive ex-
pression. If the user asks for verification, it
Figure 3: Final reward for RL1 & RL2.
Figure 4: Dialogue length for RL1 & RL2.
Policy Avg. Reward Avg. Length
RL2 830.4 16.98
RL1 812.3 18.77
Adaptive 1 809.6 19.04
Adaptive 2 792.1 20.79
Adaptive 3 780.2 21.98
Random 749.8 25.02
Desc only 796.6 20.34
Jargon only 762.0 23.8
Table 7: Rewards and Dialogue Length.
switches to technical terms for the rest of the
dialogue.
5. Adaptive 2 - It starts with a technical term
and switches to descriptive expressions if the
user does not understand in the first turn.
6. Adaptive 3 - This rule-based policy adapts
continuously based on the previous expres-
sion. For instance, if the user did not un-
derstand the technical reference to the current
object, it uses a descriptive expression for the
next object in the dialogue.
The first three policies (random, descriptive
only and jargon only) are equivalent to policies
learned using user simulations that are not sensi-
tive to system?s RE choices. In such cases, the
learned policies will not have a well-defined strat-
egy to choose REs based on user?s lexical knowl-
edge. Table 7 shows the comparative results for
the different policies. RL (1 & 2) are significantly
better than all the hand-coded policies. Also, RL2
is significantly better than RL1 (p < 0.05).
Ideally the system with complete knowledge of
the user would be able to finish the dialogue in
13 turns. Similarly, if it got it wrong every time
it would take 28 turns. From table 7 we see that
RL2 performs better than other policies, with an
average dialogue length of around 17 turns. The
learned policies were able to discover the hid-
den dependencies between lexical items that were
encoded in the Bayesian knowledge model. Al-
though trained only on two knowledge profiles, the
learned policies adapt well to unseen users, due to
the generalisation properties of the linear function
approximation method. Many unseen states arise
when interacting with users with new profiles and
both the learned policies generalise very well in
such situations, whereas the baseline policies do
not.
80
8 Conclusion
In this paper, we have shown that by using a sta-
tistical User Simulation that is sensitive to RE
choices we are able to learn NLG policies that
adaptively decide which REs to use based on audi-
ence design. We have shown that the lexical align-
ment policies learned with this type of simulation
are better than a range of hand-coded policies.
Although lexical alignment policies could be
hand-coded, the designers would need to invest
significant resources every time the list of referring
expressions is revised or the conditions of the dia-
logue change. Using reinforcement learning, near-
optimal lexical alignment policies can be learned
quickly and automatically. This model can be used
in any task where interactions need to be tailored
to different users? lexical knowledge of the do-
main.
8.1 Future work
Lexical alignment in dialogue also happens due
to priming (Pickering and Garrod, 2004) and per-
sonal experience (Clark, 1996). We will examine
trade-offs in various conditions, like ?instruct? ver-
sus ?teach? and low versus high retention users.
Using Wizard-of-Oz studies and knowledge sur-
veys, we plan to make the model more data-driven
and realistic (Janarthanam and Lemon, 2009). We
will also evaluate the learned policies with real
users.
Acknowledgements
The research leading to these results has re-
ceived funding from the European Community?s
Seventh Framework (FP7) under grant agree-
ment no. 216594 (CLASSiC Project www.classic-
project.org), EPSRC project no. EP/E019501/1,
and the British Council (UKIERI PhD Scholar-
ships 2007-08).
References
A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145?204.
H. H. Clark. 1996. Using Language. Cambridge Uni-
versity Press, Cambridge.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proceedings of Eu-
rospeech/Interspeech.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26?37.
S. Janarthanam and O. Lemon. 2008. User simulations
for online adaptation and knowledge-alignment in
Troubleshooting dialogue systems. In Proc. SEM-
dial?08.
S. Janarthanam and O. Lemon. 2009. A Wizard-of-Oz
environment to study Referring Expression Genera-
tion in a Situated Spoken Dialogue Task. In Proc.
ENLG?09.
K. Komatani, S. Ueno, T. Kawahara, and H. G. Okuno.
2003. Flexible Guidance Generation using User
Model in Spoken Dialogue Systems. In Proc.
ACL?03.
O. Lemon. 2008. Adaptive Natural Language Genera-
tion in Dialogue using Reinforcement Learning. In
Proc. SEMdial?08.
E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proceedings of ASRU97.
R. Molich and J. Nielsen. 1990. Improving a Human-
Computer Dialogue. Communications of the ACM,
33-3:338?348.
M. J. Pickering and S. Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and
Brain Sciences, 27:169?225.
V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL?09.
J. Schatzmann, K. WeilHammer, M. N. Stuttle, and
S. J. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proceedings of HLT/NAACL 2007.
D. Schlangen. 2004. Causes and strategies for request-
ing clarification in dialogue. Proceedings of the 5th
SIGdial Workshop on Discourse and Dialogue (SIG-
DIAL 04), Boston.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
J. Williams. 2007. Applying POMDPs to Dialog
Systems in the Troubleshooting Domain. In Proc
HLT/NAACL Workshop on Bridging the Gap: Aca-
demic and Industrial Research in Dialog Technol-
ogy.
81
Proceedings of the 12th European Workshop on Natural Language Generation, pages 94?97,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
A Wizard-of-Oz environment to study Referring Expression Generation
in a Situated Spoken Dialogue Task
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
s.janarthanam@ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
Edinburgh EH8 9AB
olemon@inf.ed.ac.uk
Abstract
We present a Wizard-of-Oz environment
for data collection on Referring Expres-
sion Generation (REG) in a real situated
spoken dialogue task. The collected data
will be used to build user simulation mod-
els for reinforcement learning of referring
expression generation strategies.
1 Introduction
In this paper, we present a Wizard-of-Oz (WoZ)
environment for data collection in a real situated
spoken dialogue task for referring expression gen-
eration (REG). Our primary objective is to study
how participants (hereafter called users) with dif-
ferent domain knowledge and expertise interpret
and resolve different types of referring expressions
(RE) in a situated dialogue context. We also study
the effect of the system?s lexical alignment due
to priming (Pickering and Garrod, 2004) by the
user?s choice of REs. The users follow instruc-
tions from an implemented dialogue manager and
realiser to perform a technical but realistic task ?
setting up a home Internet connection. The dia-
logue system?s utterances are manipulated to con-
tain different types of REs - descriptive, technical,
tutorial or lexically aligned REs, to refer to various
domain objects in the task. The users? responses
to different REs are then logged and studied.
(Janarthanam and Lemon, 2009) presented a
framework for reinforcement learning of optimal
natural language generation strategies to choose
appropriate REs to users with different domain
knowledge expertise. For this, we need user sim-
ulations with different domain knowledge profiles
that are sensitive to the system?s choice of REs. A
WoZ environment is an ideal tool for data collec-
tion to build data-driven user simulations. How-
ever, our study requires a novel WoZ environment.
In section 2, we present prior related work. Sec-
tion 3 describes the task performed by partici-
pants. In section 4, we describe the WoZ envi-
ronment in detail. Section 5 describes the data
collected in this experiment and section 6 presents
some preliminary results from pilot studies.
2 Related Work
(Whittaker et al, 2002) present a WoZ environ-
ment to collect data concerning dialogue strate-
gies for presenting restaurant information to users.
This study collects data on strategies used by users
and human expert wizards to obtain and present in-
formation respectively. (van Deemter et al, 2006)
present methods to collect data (the TUNA cor-
pus) for REG using artificially constructed pic-
tures of furniture and photographs of real people.
(Arts, 2004) presents a study choosing between
technical and descriptive expressions for instruc-
tion writing.
In contrast to the above studies, our study is
novel in that it collects data from users having dif-
ferent levels of expertise in a real situated task do-
main, and for spontaneous spoken dialogue. Our
focus is on choosing between technical, descrip-
tive, tutorial, and lexically aligned expressions
rather than selecting different attributes for gen-
erating descriptions.
3 The Domain Task
In this experiment, the task for each user is to lis-
ten to and follow the instructions from the WoZ
system and set up their home broadband Internet
connection. We provide the users with a home-
like environment with a desktop computer, phone
socket and a Livebox package from Orange con-
taining cables and components such as the mo-
dem, broadband filters and a power adaptor. Dur-
ing the experiment, they set up the Internet con-
nection by connecting these components to each
other. Prior to the task, the users are informed that
they are interacting with a spoken dialogue system
94
that will give them instructions to set up the con-
nection. However, their utterances are intercepted
by a human wizard. The users are requested to
have a conversation as if they were talking to a hu-
man operator, asking for clarifications if they are
confused or fail to understand the system?s utter-
ances. The system?s utterances are converted au-
tomatically to speech using the Cereproc Speech
Synthesiser and played back to the user. The user
follows the instructions and assembles the compo-
nents. The setup is examined by the wizard at the
end of the experiment to measure the percentage of
task success. The user also fills in questionnaires
prior to and after the task answering questions on
his background, quality of the system during the
task and the knowledge gained during the task.
4 The Wizard-of-Oz environment
The Wizard-of-Oz environment facilitates the en-
tire experiment as described in the section above.
The environment consists of the Wizard Interac-
tion Tool, the dialogue system and the wizard. The
users wear a headset with a microphone. Their ut-
terances are relayed to the wizard who then anno-
tates it using the Wizard Interaction Tool (shown
in figure 1) and sends it to the dialogue system.
The system responds with a natural language ut-
terance which is automatically converted to speech
and is played back to the user and the wizard.
4.1 Wizard Interaction Tool (WIT)
The Wizard Interaction Tool (WIT) (shown in fig-
ure 1) allows the wizard to interact with the dia-
logue system and the user. The GUI is divided in
to several panels.
a. System Response Panel - This panel displays
the dialogue system?s utterances and RE choices
for the domain objects in the utterance. It also dis-
plays the strategy adopted by the system currently
and a visual indicator of whether the system?s ut-
terance is being played to the user.
b. Confirmation Request Panel - This panel lets
the wizard handle issues in communication (for
e.g. noise). The wizard can ask the user to repeat,
speak louder, confirm his responses, etc using ap-
propriate pre-recorded messages or build his own
custom messages.
c. Confirmation Panel - This panel lets the wiz-
ard handle confirmation questions from the user.
The wizard can choose ?yes? or ?no? or build a cus-
tom message.
yes ?Yes it is on?
no ?No, its not flashing?
ok ?Ok. I did that?
req description ?Whats an ethernet cable??
req location ?Where is the filter??
req verify jargon ?Is it the ethernet cable??
req verify desc ?Is it the white cable??
req repeat ?Please repeat?
req rephrase ?What do you mean??
req wait ?Give me a minute??
Table 1: User Dialogue Acts.
d. Annotation panel - This panel lets the wizard
annotate the content of participant?s utterances.
User responses (dialogue acts and example utter-
ances) that can be annotated using this panel are
given in Table 1. In addition to these, other be-
haviours, like remaining silent or saying irrelevant
things are also accommodated.
e. User?s RE Choice panel - The user?s choice
of REs to refer to the domain objects are annotated
by the wizard using this panel.
4.2 The Instructional Dialogue Manager
The dialogue manager drives the conversation by
giving instructions to the users. It follows a deter-
ministic dialogue management policy so that we
only study variation in the decisions concerning
the choice of REs. It should be noted that typi-
cal WoZ environments (Whittaker et al, 2002) do
not have dialogue managers and the strategic de-
cisions will be taken by the wizard. Our dialogue
system has three main responsibilities - choosing
the RE strategy, giving instructions and handling
clarification requests.
The dialogue system, initially randomly
chooses the RE strategy at the start of the
dialogue. The list of strategies are as follows.
1. Jargon: Choose technical terms for every ref-
erence to the domain objects.
2. Descriptive: Choose descriptive terms for ev-
ery reference to the domain objects.
3. Tutorial: Use technical terms, but also aug-
ment the description for every reference.
The above three strategies are also augmented
with an alignment feature, so that the system can
either align or not align with the user?s prior choice
of REs. In aligned strategies, the system abandons
the existing strategy (jargon, descriptive or tuto-
rial) for a domain object reference when the user
95
Figure 1: Wizard Interaction Tool
uses a different expression from that of the system
to refer to the domain object. For instance, under
the descriptive strategy, the ethernet cable is re-
ferred to as ?the thick cable with red ends?. But
if the user refers to it as ?ethernet cable?, then the
system uses ?ethernet cable? in subsequent turns
instead of the descriptive expression. In case of
non-aligned strategies, the system simply ignores
user?s use of novel REs and continues to use its
own strategy.
The step-by-step instructions to set up the
broadband connection are hand-coded as a dia-
logue script. The script is a simple determinis-
tic finite state automaton, which contains execu-
tion instruction acts(e.g. Plug in the cable in to
the socket) and observation instruction acts(e.g. Is
the ethernet light flashing?) for the user. Based
on the user?s response, the system identifies the
next instruction. However, the script only con-
tains the dialogue acts. The dialogue acts are then
processed by a built-in realiser component to cre-
ate the system utterances. The realiser uses tem-
plates in which references to domain objects are
changed based on the selected strategy to create
final utterances. By using a fixed dialogue man-
agement policy and by changing the REs, we only
explore users? reactions to various RE strategies.
The utterances are finally converted to speech and
are played back to the user.
The dialogue system handles two kinds of clar-
ification requests - open requests and closed re-
quests. With open CRs, users request the sys-
tem for location of various domain objects (e.g.
?where is the ethernet cable??) or to describe
them. With closed CRs, users verify the intended
reference, in case of ambiguity (e.g. ?Do you
mean the thin white cable with grey ends??, ?Is
it the broadband filter??, etc.). The system han-
dles these requests using a knowledge base of the
domain objects.
4.3 Wizard Activities
The primary responsibility of the wizard is to un-
derstand the participant?s utterance and annotate
it as one of the dialogue acts in the Annotation
panel, and send the dialogue act to the dialogue
system for response. In addition to the primary
responsibility, the wizard also requests confirma-
tion from the user (if needed) and also responds to
confirmation requests from the user. The wizard
also observes the user?s usage of novel REs and
records them in the User?s RE Choice panel. As
mentioned earlier, our wizard neither decides on
which strategy to use to choose REs nor chooses
96
the next task instruction to give the user.
5 Data collected
Several different kinds of data are collected before,
during and after the experiment. This data will be
used to build user simulations and reward func-
tions for learning REG strategies and language
models for speech recognition.
1. WIT log - The WIT logs the whole conversa-
tion as an XML file. The log contains system and
user dialogue acts, time of system utterance, sys-
tem?s choice of REs and its utterance at every turn.
It also contains the dialogue start time, total time
elapsed, total number of turns, number of words
in system utterances, number of clarification re-
quests, number of technical, descriptive and tuto-
rial REs, number of confirmations etc.
2. Background of the user - The user is asked to fill
in a pre-task background questionnaire containing
queries on their experience with computers, Inter-
net and dialogue systems.
3. User satisfaction survey - The user is re-
quested to fill in a post-task questionnaire contain-
ing queries on the performance of the system dur-
ing the task. Each question is answered in a four
point Likert scale on how strongly the user agrees
or disagrees with the given statement. Statements
like, ?Conversation with the system was easy?,
?I would use such a system in future?, etc are
judged by the user which will be used to build re-
ward functions for reinforcement learning of REG
strategies.
4. Knowledge pre-test - Users? initial domain
knowledge is tested by asking them to match a list
of technical terms to their respective descriptive
expressions.
5. Knowledge gain post-test - Users? knowledge
gain during the dialogue task is measured by ask-
ing them to redo the matching task.
6. Percentage of task completion - The wizard
examines the final set up on the user?s table to
determine the percentage of task success using a
form containing declarative statements describing
the ideal broadband set up (for e.g. ?the broad-
band filter is plugged in to the phone socket on
the wall?). The wizard awards one point to every
statement that is true of the user?s set up.
7. User?s utterances WAV file - The user?s ut-
terances are recorded in WAV format for build-
ing language models for automatic speech recog-
nition.
6 Results from pilot studies
We are currently running pilot studies (with 6 par-
ticipants so far) and have collected around 60 min-
utes of spoken dialogue data. We found that in
the jargon strategy, some users take a lot longer to
finish the task than others (max 59 turns, min 26
turns). We found that besides requesting clarifi-
cations, sometimes novice users assume incorrect
references to some domain objects, affecting their
task completion rates.
7 Conclusion
We have presented a novel Wizard-of-Oz environ-
ment to collect spoken data in a real situated task
environment, and to study user reactions to a va-
riety of REG strategies, including system align-
ment. The data will be used for training user sim-
ulations for reinforcement learning of REG strate-
gies to choose between technical, descriptive, tu-
torial, and aligned REs based on a user?s expertise
in the task domain.
Acknowledgements
The research leading to these results has re-
ceived funding from the European Community?s
Seventh Framework (FP7) under grant agree-
ment no. 216594 (CLASSiC Project www.classic-
project.org), EPSRC project no. EP/E019501/1,
and the British Council (UKIERI PhD Scholar-
ships 2007-08).
References
A. Arts. 2004. Overspecification in Instructive Text.
Ph.D. thesis, Tilburg University, The Netherlands.
S. Janarthanam and O. Lemon. 2009. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG?09.
M. J. Pickering and S. Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and
Brain Sciences, 27:169?225.
K. van Deemter, I. van der Sluis, and A. Gatt.
2006. Building a semantically transparent corpus
for the generation of referring expressions. In Proc.
INLG?06.
S. Whittaker, M. Walker, and J. Moore. 2002. Fish
or Fowl: A Wizard of Oz Evaluation of Dialogue
Strategies in the Restaurant Domain. In Language
Resources and Evaluation Conference.
97
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 120?123,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
A Two-tier User Simulation Model for Reinforcement Learning of
Adaptive Referring Expression Generation Policies
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
s.janarthanam@ed.ac.uk
Oliver Lemon
School of Informatics
University of Edinburgh
olemon@inf.ed.ac.uk
Abstract
We present a new two-tier user simula-
tion model for learning adaptive referring
expression generation (REG) policies for
spoken dialogue systems using reinforce-
ment learning. Current user simulation
models that are used for dialogue pol-
icy learning do not simulate users with
different levels of domain expertise and
are not responsive to referring expres-
sions used by the system. The two-
tier model displays these features, that
are crucial to learning an adaptive REG
policy. We also show that the two-tier
model simulates real user behaviour more
closely than other baseline models, using
the dialogue similarity measure
based on Kullback-Leibler divergence.
1 Introduction
We present a new user simulation model for
learning adaptive referring expression generation
(REG) policies for spoken dialogue systems us-
ing reinforcement learning methods. An adap-
tive REG policy equips a dialogue system to dy-
namically modify its utterances in order to adapt
to user?s domain knowledge level. For instance,
to refer to domain objects, the system might use
simple descriptive expressions with novices and
technical jargon with experts. Such adaptations
help grounding between the dialogue partners (Is-
sacs and Clark, 1987). Since the user?s knowl-
edge level is unknown, the system must be able to
adapt dynamically during the conversation. Hand-
coding such a policy could be extremely difficult.
(Janarthanam and Lemon, 2009b) have shown
that such policies can be learned using simula-
tion based reinforcement learning (RL) methods.
The quality of such learned policies is directly de-
pendent on the performance of the user simula-
tions used to train them. So far, only hand-coded
user simulations have been employed. In contrast,
we now present a data driven two-tier user sim-
ulation model trained on dialogue data collected
from real users. We also show that the two-tier
model simulates real users more faithfully than
other data driven baseline n-gram models (Eckert
et al, 1997).
In section 2 we briefly discuss other work re-
lated to user simulations for dialogue policy learn-
ing using RL. In section 3 we describe the data
used to build the simulation. Section 4 describes
the simulation models in detail. In section 5 and
6 we present the evaluation metrics used and the
results.
2 Related work
Several user simulation models have been pro-
posed for dialogue management policy learning
(Schatzmann et al, 2006; Schatzmann et al,
2007). However, these models cannot be directly
used for REG policy learning because they inter-
act with the dialogue system only using high-level
dialogue acts. Also, they do not simulate differ-
ent user groups like experts, novices, etc. In order
to learn adaptive REG policies, user simulations
need to respond to the system?s choice of referring
expressions and simulate user groups with differ-
ent knowledge levels. We propose a two-tier simu-
lation which simulates users with different knowl-
edge levels and is sensitive to the system?s choice
of referring expressions.
120
3 Corpus
The ?Wizard-of-Oz? (WOZ) methodology is a
widely accepted way of collecting dialogue data
for user simulation modeling (Whittaker et al,
2002). In this setup, real users interact with a hu-
man wizard disguised as a dialogue system. The
wizard interprets the users responses and passes
them on to the dialogue system. The dialogue sys-
tem updates the dialogue state and decides the re-
sponses to user?s moves. The task of the partici-
pant is to interact with the dialogue system to get
instructions to setup a broadband Internet connec-
tion. The referring expression generation strategy
is chosen before the dialogue starts and stays the
same for the whole session. The strategies used
were ?jargon?, ?descriptive? and ?tutorial?. In the
jargon strategy the system instructs the user us-
ing technical terms (e.g. ?Plug the broadband
filter into the phone socket.?). In the de-
scriptive strategy, it uses descriptive terms (e.g.
?Plug the small white box into the square
white box on the wall.?). In the tutorial
strategy, the system uses both jargon and descrip-
tive terms together. The system provides clari-
fications on referring expressions when users re-
quest them. The participant?s domain knowledge
is also recorded during the task. Please refer to (Ja-
narthanam and Lemon, 2009a) for a more details
on our Wizard-of-Oz environment for data collec-
tion. The dialogues were collected from 17 par-
ticipants (one dialogue each) with around 24 to 35
turns per dialogue depending on the strategy and
user?s domain knowledge.
4 User Simulation models
The dialogue data and knowledge profiles were
used to build user simulation models. These mod-
els take as input the system?s dialogue act As,t (at
turn t) and choice of referring expressions RECs,t
and output the user?s dialogue Au,t and environ-
ment EAu,t acts. User?s observation and manipu-
lation of the domain objects is represented by the
environment act.
4.1 Advanced n-gram model
A simple approach to model real user behaviour
is to model user responses (dialogue act and
environment act) as advanced n-gram models
(Georgila et al, 2006) based on many context vari-
ables - all referring expressions used in the utter-
ance (RECs,t), the user?s knowledge of the REs
(DKu), history of clarification requests on the
REs (H), and the system?s dialogue act (As,t), as
defined below:
P (Au,t|As,t, RECs,t, DKu,H)
P (EAu,t|As,t, RECs,t, DKu,H)
Although this is an ideal model of the real user
data, it covers only a limited number of contexts
owing to the limited size of the corpus. Therefore,
it cannot be used for training as there may be a
large number of unseen contexts which the model
needs to respond to. For example, this model can-
not respond when the system uses a mix of jar-
gon and descriptive expressions in its utterance be-
cause such a context does not exist in our corpus.
4.2 A Two-tier model
Instead of using a complex context model, we di-
vide the large context in to several sub-contexts
and model the user?s response based on them. We
propose a two-tier model, in which the simulation
of a user?s response is divided into two steps. First,
all the referring expressions used in the system?s
utterance are processed as below:
P (CRu,t|REs,t, DKRE,u,HRE , As,t)
This step is repeated for each expression REs,t
separately. The above model returns a clarifi-
cation request based on the referring expression
REs,t used, the user?s knowledge of the expres-
sion DKRE,u, and previous clarification requests
on the expression HRE and the system dialogue
act As,t. A clarification request is highly likely in
case of the jargon strategy and less likely in other
strategies. Also, if a clarification has already been
issued, the user is less likely to issue another re-
quest for clarification. In such cases, the clarifica-
tion request model simply returns none.
In the next step, the model returns a user di-
alogue act Au,t and an environment act EAu,t
based on the system dialogue act As,t and the clar-
ification request CRu,t, as follows:
P (Au,t|As,t, CRu,t)
P (EAu,t|As,t, CRu,t)
By dividing the complex context into smaller
sub-contexts, the two-tier model simulates real
users in contexts that are not directly observed in
the dialogue data. The model will therefore re-
spond to system utterances containing a mix of
REG strategies (for example, one jargon and one
descriptive expression in the same utterance).
121
4.3 Baseline Bigram model
A bigram model was built using the dialogue data
by conditioning the user responses only on the sys-
tem?s dialogue act (Eckert et al, 1997).
P (Au,t|As,t)
P (EAu,t|As,t)
Since it ignores all the context variables except
the system dialogue act, it can be used in contexts
that are not observed in the dialogue data.
4.4 Trigram model
The trigram model is similar to the bigram model,
but with the previous system dialogue act As,t?1
as an additional context variable.
P (Au,t|As,t, As,t?1)
P (EAu,t|As,t, As,t?1)
4.5 Equal Probability model baseline
The equal probability model is similar to the bi-
gram model, except that it is not trained on the
dialogue data. Instead, it assigns equal probabil-
ity to all possible responses for the given system
dialogue act.
4.6 Smoothing
We used Witten-Bell discounting to smooth all
our models except the equal probability model,
in order to account for unobserved but possible
responses in dialogue contexts. Witten-Bell dis-
counting extracts a small percentage of probability
mass, i.e. number of distinct responses observed
for the first time (T ) in a context, out of the to-
tal number of instances (N ), and redistributes this
mass to unobserved responses in the given context
(V ? T ) (where V is the number of all possible
responses) . The discounted probabilities P ? of
observed responses (C(ei) > 0) and unobserved
responses (C(ei) = 0) are given below.
P ?(ei) = C(ei)N+T if(C(ei) > 0)
P ?(ei) = t(N+T )(V?T ) if(C(ei) = 0)
On analysis, we found that the Witten-Bell
discounting assigns greater probability to unob-
served responses than to observed responses, in
cases where the number of responses per con-
text is very low. For instance, in a partic-
ular context, the possible responses, their fre-
quencies and their original probabilities were -
provide info (3, 0.75), other (1, 0.25),
request clarification (0, 0). After dis-
counting, the revised probabilities P ? are 0.5,
0.167 and 0.33. request clarification
gets the whole share of extracted probability as
it is the only unobserved response in the context
and is more than the other responses actually
observed in the data. This is counter-intuitive for
our application. Therefore, we use a modified ver-
sion of Witten-Bell discounting (given below) to
smooth our models, where the extracted proba-
bility is equally divided amongst all possible re-
sponses. Using the modified version, the revised
probabilities for the illustrated example are 0.61,
0.28 and 0.11 respectively.
P ?(ei) = C(ei)N+T + T(N+T )V
5 Metrics for evaluation of simulations
While there are many proposed measures to rank
user simulation models with respect to real user
data (Schatzmann et al, 2005; Georgila et al,
2006; Rieser and Lemon, 2006a; Williams, 2008),
we use the Dialogue Similarity measure
based on Kullback-Leibler (KL) (Cuayahuitl et
al., 2005; Cuayahuitl, 2009) divergence to mea-
sure how similar the probability distributions of
the simulation models are to the original real hu-
man data.
5.1 Dialogue Similarity
Dialogue Similarity is a measure of divergence be-
tween real and simulated dialogues and can mea-
sure how similar a model is to real data. The mea-
sure is based on Kullback-Leibler (KL) divergence
and is defined as follows:
DS(P ||Q) = 1N
?N
i=1
DKL(P ||Q)+DKL(Q||P )
2
DKL(P ||Q) =
?M
i=1 pi ? log(piqi )
The metric measures the divergence between
distributions P and Q in N different contexts
with M responses per context. Ideally, the dia-
logue similarity between two similar distributions
is close to zero.
6 Evaluation results
We consider the Advanced N-gram model to be
a realistic model of the real human dialogue cor-
pus, as it takes into account all context variables
and is reasonably smoothed to account for unob-
served user responses. Therefore, we compare the
probability distributions of all the other models to
122
Model Au,t EAu,t
Two-tier 0.078 0.018
Bigram 0.150 0.139
Trigram 0.145 0.158
Equal Probability 0.445 0.047
Table 1: Dialogue Similarity with Modified
Witten-Bell discounting w.r.t Advanced N-gram
model
the advanced n-gram model using the dialogue
similarity measure. The results of the evalu-
ation are given in table 1.
The results show that the two-tier model is
much closer (0.078, 0.018) to the Advanced N-
gram model than the other models. This is due to
the fact that the bigram and trigram models don?t
take into account factors like the user?s knowl-
edge, the strategy used, and the dialogue history.
By effectively dividing the RE processing and the
environment interaction, the two-tier simulation
model is not only realistic in observed contexts but
also usable in unobserved contexts (unlike the Ad-
vanced N-gram model).
7 Conclusion
We have presented a data driven user simulation
model called the two-tier model for learning REG
policies using reinforcement learning. We have
also shown that the two-tier model is much closer
to real user data than the other baseline models.
We will now train REG policies using the two-tier
model and test them on real users in the future.
Acknowledgements
The research leading to these results has re-
ceived funding from the EPSRC (project no.
EP/E019501/1) and from the European Commu-
nity?s Seventh Framework Programme (FP7/2007-
2013) under grant agreement no. 216594 (CLAS-
SiC project www.classic-project.org),
and from the British Council?s UKERI pro-
gramme.
References
H. Cuayahuitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2005. Human-Computer Dialogue Sim-
ulation Using Hidden Markov Models. In Proc. of
ASRU 2005.
H. Cuayahuitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. the-
sis, University of Edinburgh, UK.
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
Modeling for Spoken Dialogue System Evaluation.
In Proc. of ASRU97.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
Simulation for Spoken Dialogue System: Learning
and Evaluation. In Proc of ICSLP 2006.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26?37.
S. Janarthanam and O. Lemon. 2009a. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG?09.
S. Janarthanam and O. Lemon. 2009b. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG?09.
V. Rieser and O. Lemon. 2006a. Cluster-based User
Simulations for Learning Dialogue Strategies. In
Proc. Interspeech/ICSLP.
J. Schatzmann, K. Georgila, and S. J. Young. 2005.
Quantitative Evaluation of User Simulation Tech-
niques for Spoken Dialogue Systems. In Proc. SIG-
dial workshop on Discourse and Dialogue ?05.
J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.
S. Whittaker, M. Walker, and J. Moore. 2002. Fish
or Fowl: A Wizard of Oz Evaluation of Dialogue
Strategies in the Restaurant Domain. In Language
Resources and Evaluation Conference.
J. Williams. 2008. Evaluating User Simulations with
the Cramer-von Mises Divergence. Speech Commu-
nication, 50:829?846.
123
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 148?151,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Automatic Generation of Information State Update Dialogue Systems that
Dynamically Create Voice XML, as Demonstrated on the iPhone
Helen Hastie, Xingkun Liu and Oliver Lemon
School of Informatics
University of Edinburgh
{hhastie,xliu4,olemon}@inf.ed.ac.uk
Abstract
We demonstrate DUDE1 (Dialogue
and Understanding Development En-
vironment), a prototype development
environment that automatically generates
dialogue systems from business-user
resources and databases. These generated
spoken dialogue systems (SDS) are then
deployed on an industry standard Voice
XML platform. Specifically, the deployed
system works by dynamically generating
context-sensitive Voice XML pages. The
dialogue move of each page is determined
in real time by the dialogue manager,
which is an Information State Update
engine. Firstly, we will demonstrate the
development environment which includes
automatic generation of speech recogni-
tion grammars for robust interpretation of
spontaneous speech, and uses the appli-
cation database to generate lexical entries
and grammar rules. A simple graphical
interface allows users (i.e. developers) to
easily and quickly create and the modify
SDS without the need for expensive
service providers. Secondly, we will
demonstrate the deployed system which
enables participants to call up and speak
to the SDS recently created. We will also
show a pre-built application running on
the iPhone and Google Android phone for
searching for places such as restaurants,
hotels and museums.
1Patent Pending
1 Introduction
With the advent of new mobile platforms such as
the iPhone and Google Android, there is a need for
a new way to interact with applications and search
for information on the web. Google Voice Search
is one such example. However, we believe that
this simple ?one-shot? search using speech recog-
nition is not optimal for the user. A service that
allows the user to have a dialogue via their phone
opens up a wider set of possibilities. For exam-
ple, the user may be visiting a foreign city and
would like to have a discussion about the types
of restaurants, their cuisine, their price-range and
even ask for recommendations from the system or
their friends on social networking sites. The Di-
alogue Understanding Development Environment
or DUDE makes this possible by providing a flex-
ible, natural, mixed initiative dialogue using an in-
formation state update dialogue engine (Bos et al,
2003).
Currently, if a company wishes to deploy such
a spoken dialogue system, they have to employ
a costly service provider with a long turn around
time for any changes to the system, even minor
ones such as a special promotion offer. In addi-
tion, there is steep competition on application sites
such as Google Market Place and Apple App Store
which are populated with very cheap applications.
DUDE?s Development Environment takes existing
business-user resources and databases and auto-
matically generates the dialogue system. This re-
duces development time and, therefore, costs and
opens up the technology to a wider user-base. In
addition, the DUDE environment is so easy to use
that it gives the control back into the business-user
and away from independent services providers.
In this paper, we describe the architecture and
148
technology of the DUDE Development Environ-
ment and then discuss how the deployed system
works on a mobile platform.
2 The DUDE Development Environment
Figure 1 shows the DUDE Development Envi-
ronment architecture whereby the main algorithm
takes the business-user resources and databases as
input and uses these to automatically generate the
spoken dialogue system which includes a Voice
XML generator. Advantages of using business-
user resources such as Business Process Mod-
els (BPM) (Williams, 1967) include the fact that
graphical interfaces and authoring environments
are widely available (e.g. Eclipse). In addition,
business-user resources can contain a lot of addi-
tional information as well as call flow including
context, multi-media and multiple customer inter-
actions.
Figure 1: The DUDE Architecture
2.1 Spoken Dialogue System Generation
Many sophisticated research systems are devel-
oped for specific applications and cannot be eas-
ily transferred to another, even very similar task or
domain. The problem of components being do-
main specific is especially prevalent in the core
area of dialogue management. For example MIT?s
Pegasus and Mercury systems (Seneff, 2002) have
dialogue managers (DM) that use approximately
350 domain-specific hand-coded rules each. The
sheer amount of labour required to construct sys-
tems prevents them from being more widely and
rapidly deployed. We present a solution whereby
BPMs and related authoring tools are used to spec-
ify domain-specific dialogue interactions which
are combined with domain-general dialogue man-
agers. Specifically, the DM consults the BPM to
determine what task-based steps to take next, such
as asking for price range after establishing pre-
ferred cuisine type. General aspects of dialogue,
such as confirmation and clarification strategies,
are handled by the domain-general DM. Values
for constraints on transitions and branching in the
BPM, for example ?present insurance offer if the
user is business-class?, are compiled into domain-
specific parts of the Information State. XML for-
mat is used for BPMs, and they are compiled into
finite state machines consulted by the spoken dia-
logue system. The domain-general dialogue man-
ager was mostly abstracted from the TALK system
(Lemon et al, 2006).
Using DUDE, developers do not have to write
a single line of grammar code. There are three
types of grammars: (1) a core grammar, (2) a
grammar generated from the database and BPM,
and (3) dynamically generated grammars created
during the dialogue. The core grammar (1) was
developed to cover basic information-seeking in-
teractions. In addition (2), the system com-
piles relevant database entries and their proper-
ties into the appropriate ?slot-filling? parts of a
SRGS GRXML (Speech Recognition Grammar
Specification) grammar for each specific BPM
node. Task level grammars are used to allow a
level of mixed initiative, for example, if the sys-
tem asks ?what type of cuisine?? the user can
reply with cuisine and also any other slot type,
such as, ?cheap Italian?. The dynamically gen-
erated grammars (3), such as for restaurants cur-
rently being recommended, minimizes grammar
size and makes the system more efficient. In ad-
dition to the above-mentioned grammars, devel-
opers are able to provide task spotter phrases and
synonyms reflecting how users might respond by
using the DUDE Development Environment. If
these are not already covered by the existing gram-
mar, DUDE automatically generates rules to cover
them.
The generated SRGS GRXML grammars are
used to populate the Voice XML pages and conse-
quently used by the Voice XML Platform Speech
recogniser. In this case, we deploy our system to
the Voxeo Platform (http://www.voxeo.com). As
well as the W3C standard SRGS GRXML, DUDE
is able to generate alternative grammar specifica-
tions such as SRGS ABNF (Augmented Backus-
Naur Form), JSGF ABNF (Java Speech Grammar
Format) and Nuance?s GSL (Grammar Specifica-
149
Figure 2: Example: using the DUDE Development Environment to define spotter phrases and other
information for the different BPM tasks
tion Language).
2.2 The Development Environment
As mentioned above, the DUDEDevelopment En-
vironment can be used to define system prompts
and add task spotter phrases and synonyms to the
grammars. Figure 2 shows the GUI with the BPM
on the left hand side and the properties pane for
the restaurants task on the right hand side. In this
pane the developer can define the system prompt,
the information to be presented to the user and the
spotter phrases. Here the developer is associating
the phrases ?restaurants, restaurant, somewhere to
eat....? with the restaurant task. This means that
if the user says ?I want somewhere to eat?, the
restaurant part of the BPM will be triggered. Note
that multi-word phrases may also be defined. The
defined spotters are automatically compiled into
the grammar for parsing and speech recognition.
By default all the lexical entries for answer-types
for the subtasks will already be present as spotter
phrases. DUDE checks for possible ambiguities,
for example if ?pizza? is a spotter for both cui-
sine type for a restaurant task and food type for a
shopping task, the system uses a clarification sub-
dialogue to resolve them at runtime.
Figure 3 shows the developer specifying the re-
quired linguistic information to automate the cui-
sine subtask of the restaurants task. Here the de-
veloper specifies the system prompt ?What type
of cuisine do you want?? and a phrase for im-
plicit confirmation of provided values, e.g. ?a [X]
restaurant?, where [X] is a variable that will be
replaced with the semantics of the speech recogni-
tion hypothesis for the user input. The developer
also specifies here the answer type that will resolve
the system prompt. There are predefined answer-
types extracted from the databases, and the devel-
oper can select and/or edit these, adding phrases
and synonyms. In addition, they have the ability
to define their own answer-types.
Figure 3: Example: using the DUDE Develop-
ment Environment to define prompts, answer sets,
and database mappings for the cuisine subtask
150
3 Deployment of the Generated Spoken
Dialogue System
The second part of the demonstration shows
a pre-built multimodal application running on
the iPhone (http://www.apple.com) and Google
Android phone (http://code.google.com//android).
This application allows the user to have a dialogue
about places of interest using The List website
(http://www.list.co.uk). Figure 4 shows screen-
shots of the iPhone, firstly with The List home-
page and then a page with content on Bar Roma,
an ?italian restaurant in Edinburgh? as requested
by the user through spoken dialogue.
Figure 4: DUDE-generated iPhone List Applica-
tion pushing relevant web content
Figure 5 shows the architecture of this system
whereby the DUDE server runs the spoken dia-
logue system (as outputted from the DUDEDevel-
opment Environment). This system dynamically
generates Voice XML pages whose dialogue move
and grammar is determined by the Information
State Update Dialogue Model. These Voice XML
pages are sent in real time to the Voice XML plat-
form (in our case Voxeo) which the user talks to by
placing a regular phone call. In addition, DUDE
communicates the relevant URL via a server con-
nection.
4 Summary
This paper describes a demonstration of the
DUDE Development Environment and its result-
ing spoken dialogue systems as deployed on a mo-
bile phone, specifically the iPhone and Google
Android. With the emergence of web-enabled
smart-phones, a new and innovative interactive
method is needed that combines web-surfing and
Figure 5: Architecture of deployed DUDE Appli-
cation on a mobile phone (e.g. the iPhone)
dialogue in order to get the user exactly the infor-
mation required in real time.
5 Acknowledgement
This project is funded by a Scottish Enterprise
Proof of Concept Grant (project number 8-ELM-
004). We gratefully acknowledge The List for giv-
ing us data for our prototype application.
References
Johan Bos, Ewan Klein, Oliver Lemon, and Tetsushi
Oka. 2003. DIPPER: Description and Formalisa-
tion of an Information-State Update Dialogue Sys-
tem Architecture. In 4th SIGdial Workshop on Dis-
course and Dialogue, pages 115?124, Sapporo.
Adam Cheyer and David Martin. 2001. The Open
Agent Architecture. Journal of Autonomous Agents
and Multi-Agent Systems, 4(1/2):143?148.
Oliver Lemon, Kallirroi Georgila, James Henderson,
and Matthew Stuttle. 2006. An ISU dialogue sys-
tem exhibiting reinforcement learning of dialogue
policies: generic slot-filling in the TALK in-car sys-
tem. In Proceedings of EACL, pages 119?122.
Stephanie Seneff. 2002. Response Planning and Gen-
eration in the Mercury Flight Reservation System.
Computer Speech and Language, 16.
S Williams. 1967. Business process modeling im-
proves administrative control. Automation, pages
44?50.
151
Multi-tasking and Collaborative Activities in Dialogue Systems
Oliver Lemon, Alexander Gruenstein, Alexis Battle, and Stanley Peters
Center for the Study of Language and Information
Stanford University, CA 94305
lemon,alexgru,ajbattle,peters@csli.stanford.edu
Abstract
We explain dialogue management tech-
niques for collaborative activities with hu-
mans, involving multiple concurrent tasks.
Conversational context for multiple con-
current activities is represented using a
?Dialogue Move Tree? and an ?Activity
Tree? which support multiple interleaved
threads of dialogue about different activi-
ties and their execution status. We also de-
scribe the incremental message selection,
aggregation, and generation method em-
ployed in the system.
1 Introduction
This paper describes implemented multi-modal dia-
logue systems1 which support collaboration with au-
tonomous devices in their execution of multiple con-
current tasks. We will focus on the particular mod-
elling and processing aspects which allow the sys-
tems to handle dialogues about multiple concurrent
tasks in a coherent and natural manner. Many con-
versations between humans have this property, and
dialogues between humans and semi-autonomous
devices will have this feature in as much as devices
are able to carry out activities concurrently. This
ability to easily interleave communication streams is
a very useful property of conversational interactions.
Humans are adept at carrying out conversations with
1This research was (partially) funded under the Wallenberg
laboratory for research on Information Technology and Au-
tonomous Systems (WITAS) Project, Linko?ping University, by
the Wallenberg Foundation, Sweden.
multiple threads, or topics, and this capability en-
ables fluid and efficient communication, and thus ef-
fective co-ordination of actions (see (Lemon et al,
2002) for a more extensive discussion). We will
show how to endow a dialogue system with some
of these capabilities.
The main issues which we address in this paper
are:
  Representation of dialogue context such that
collaborative activities and multi-tasking are
supported.
  Dialogue management methods such that free
and natural communication over several con-
versational topics is supported.
  Natural generation of messages in multi-
tasking collaborative dialogues.
In Section 2 we discuss the demands of multi-
tasking and collaboration with autonomous devices.
Section 3 covers the robot with which our current
dialogue system interacts, and the architecture of
the dialogue system. In Section 4 we introduce the
?joint activities? and Activity Models which repre-
sent collaborative tasks and handle multi-tasking in
an interface layer between the dialogue system and
autonomous devices. Section 5 presents the dia-
logue modelling and management techniques used
to handle multiple topics and collaborative activi-
ties. Section 6 surveys the message selection, ag-
gregation, and generation component of the system,
in the context of multi-tasking.
     Philadelphia, July 2002, pp. 113-124.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
2 Multi-tasking and Collaboration
A useful dialogue system for interaction with au-
tonomous devices will enable collaboration with hu-
mans in the planning and execution of tasks. Dia-
logue will be used to specify and clarify instructions
and goals for the device, to monitor its progress,
and also to jointly solve problems. Before we deal
with such issues in detail, we note that such devices
also have the following properties which are relevant
from the point of view of dialogue management:
  Devices exist within dynamic environments,
where new objects appear and are available for
discussion. Device sensors may give rise to
new information at any time, and this may need
to be communicated urgently.
  Devices may perform multiple concurrent ac-
tivities which may succeed, fail, become can-
celled, or be revised. These activities can be
topics of conversation.
(Allen et al, 2001) present a taxonomy of dia-
logue systems ranging from ?finite-state script? di-
alogues for simple tasks (such as making a long-
distance call) to the most complex ?agent-based
models? which cover dialogues where different pos-
sibilities, such as future plans, are discussed. Within
this taxonomy, a useful dialogue system for interac-
tion with autonomous devices must be located at or
near the ?agent-based? point since we wish to com-
municate with devices about their possible actions,
their plans, and the tasks they are currently attempt-
ing. For these reasons we built a dialogue manager
that represents (possibly collaborative) activities and
their execution status, and tracks multiple threads of
dialogue about concurrent and planned activities.
For these sorts of reasons it is clear that form-
filling or data-base query style dialogues (e.g. the
CSLU Toolkit, (McTear, 1998)) will not suffice here
(see (Elio and Haddadi, 1999; Allen et al, 2001) for
similar arguments).
3 The WITAS Dialogue System
In our current application, the autonomous system
is the WITAS2 UAV (?unmanned aerial vehicle?) ?
a small robotic helicopter with on-board planning
2See http://www.ida.liu.se/ext/witas
and deliberative systems, and vision capabilities (for
details see e.g. (Doherty et al, 2000)). This robot
helicopter will ultimately be controlled by the dia-
logue system developed at CSLI, though at the mo-
ment we interact with a simulated3 UAV. Mission
goals are provided by a human operator, and an on-
board planning system then responds. While the he-
licopter is airborne, an on-board active vision sys-
tem interprets the scene or focus below to interpret
ongoing events, which may be reported (via NL gen-
eration) to the operator (see Section 6). The robot
can carry out various ?activities? such as flying to a
location, or following a vehicle, or landing. These
activities are specified by the user during dialogue,
or can be initiated by the UAV?s on-board AI. In any
case, a major component of the dialogue, and a way
of maintaining its coherence, is tracking the state of
current or planned activities of the device.
A more interesting and problematic notion is that
of ?joint-activities? between an autonomous system
and a human operator. These are activities which
the autonomous system cannot complete alone, but
which require some human intervention. In our cur-
rent scenarios, the UAV?s vision system is not good
enough to determine whether a particular vehicle is
the one sought-after, and only the human operator
has the authority to determine this, so that human
and robot must collaborate in order to find and track
a vehicle. The dialogue in Figure 2 shows how a
typical interaction works4 (other capabilities, such
as clarification subdialogues, are covered in (Lemon
et al, 2001)). Note here that the user is able to make
explicit queries about the robot?s activities (both cur-
rent and future), that there are concurrent activi-
ties, and that conversational initiative centers around
the joint activities currently being specified and ex-
ecuted.
4 Activity Models
The idea of Activity Modelling in our system is
the vision that dialogue systems can, in generality,
be built for ?devices? which carry out certain well-
3Our UAV simulator uses KIF statements under JTP (the
Java Theorem Prover) to represent and non-montonically up-
date UAV state information.
4The system runs on a laptop computer under Windows
2000. Video footage of the system can be found at http:
//www-csli.stanford.edu/semlab/witas/
Figure 2: A demonstration of the WITAS dialogue system (November 2001)
Multi-modal Utterances Dialogue Move
Operator (O): Our job is to look for a red car Command (Joint Activity)
UAV (U): Ok. I am looking for one. Report (Confirm Activity)
O: Fly here please [+click on map] Command (Deictic)
U: Okay. I will fly to waypoint one Report (Confirm Activity)
U: Now taking off and flying there. Report (Current Activity)
O: Stop that. Go to the tower instead. Command, Revision
U: I have cancelled flying to waypoint one. I will fly to the
tower.
Report (Activity status)
O: What are you doing? Wh-question (Current Activity)
U: I am searching for a red car and flying to the tower Answer (Current Activity)
O: What will you do next? Wh-question (Planned Activity)
U: I have nothing planned. Answer(Planned Activity)
U: I see a red car on main street [display on map, show video
images], Is this the right car?
Report, Yn-question (Activity)
O: Yes, that?s the right car Yn-answer (Positive)
U: Okay. I am following it . Report (Current activity)
facilitator
OAA2
Synthesizer
Generator
Gemini
Parser and 
Recognizer
Speech
Festival
Display
Interactive Map
NL
SR
TTS
DM
GUI
Activities
Model
Interface
       
Dialogue Move Tree (DMT)
Activity Tree (AT)
System Agenda (SA)
Pending List (PL)
Modality Buffer (MB)
ROBOT
Salience List (SL)
Speech
Nuance
DIALOGUE MANAGER
Figure 1: The WITAS dialogue system architecture
defined activities (e.g. switch lights on, record on
channel   , send email  to  , search for vehicle  ),
and that an important part of the dialogue context to
be modelled in such a system is the device?s planned
activities, current activities, and their execution sta-
tus5. We choose to focus on building this class of
dialogue systems because we share with (Allen et
al., 2001), a version of the the Practical Dialogue
5Compare this with the motivation behind the ?Pragmatic
Adapter? idea of (LuperFoy et al, 1998).
Hypothesis:
?The conversational competence required
for practical dialogues, although still com-
plex, is significantly simpler to achieve
than general human conversational com-
petence.?
We also share with (Rich et al, 2001) the idea that
declarative descriptions of the goal decomposition
of activities (COLLAGEN?s ?recipes?, our ?Activ-
ity Models?) are a vital layer of representation, be-
tween a dialogue system and the device with which
it interacts.
In general we assume that a device is capable of
performing some ?atomic? activities or actions (pos-
sibly simultaneously), which are the lowest-level ac-
tions that it can perform. Some devices will only
know how to carry out sequences of atomic activ-
ities, in which case it is the dialogue system?s job
to decompose linguistically specified high-level ac-
tivities (e.g. ?record the film on channel 4 tonight?)
into a sequence of appropriate atomic actions for the
device. In this case the dialogue system is provided
with a declarative ?Activities Model? (see e.g. Fig-
ure 3) for the device which states how high-level
linguistically-specified activities can be decomposed
into sequences of atomic actions. This model con-
tains traditional planning constraints such as precon-
ditions and postconditions of actions. In this way, a
relatively ?stupid? device (i.e. with little or no plan-
ning capabilities) can be made into a more intelli-
gent device when it is dialogue-enabled.
At the other end of the spectrum, more intelli-
gent devices are able to plan their own sequences of
atomic actions, based on some higher level input. In
this case, it is the dialogue system?s role to translate
natural language into constraints (including tempo-
ral constraints) that the device?s planner recognizes.
The device itself then carries out planning, and in-
forms the dialogue manager of the sequence of ac-
tivities that it proposes. Dialogue can then be used
to re-specify constraints, revise activities, and mon-
itor the progress of tasks. We propose that the pro-
cess of decomposing a linguistically specified com-
mand (e.g. ?vacuum in the main bedroom and the
lounge, and before that, the hall?) into an appropri-
ate sequence of constraints for the device?s on-board
planner, is an aspect of ?conversational intelligence?
that can be added to devices by dialogue-enabling
them.
We are developing one representation and reason-
ing scheme to cover this spectrum of cases from de-
vices with no planning capabilities to some more
impressive on-board AI. Both dialogue manager
and robot/device have access to a single ?Activity
Tree? which is a shared representation of current
and planned activities and their execution status, in-
volving temporal and hierarchical ordering (in fact,
one can think of the Activity Tree as a Hierarchical
Task Network for the device). This tree is built top-
down by processing verbal input from the user, and
its nodes are then expanded by the device?s planner
(if it has one). In cases where no planner exists, the
dialogue manager itself expands the whole tree (via
the Activity Model for the device) until only leaves
with atomic actions are left for the device to execute
in sequence. The device reports completion of activ-
ities that it is performing and any errors that occur
for an activity.
Note that because the device and dialogue system
share the same representation of the device?s activ-
ities, they are always properly coordinated. They
also share responsibility for different aspects of con-
structing and managing the whole Activity Tree.
Note also that some activities can themselves be
speech acts, and that this allows us to build collabo-
rative dialogue into the system. For example, in Fig-
ure 3 the ASK-COMPLETE activity is a speech act,
generating a yes-no question to be answered by the
user.
4.1 An example Activity Model
An example LOCATE activity model for the UAV
is shown in Figure 3. It is used when constructing
parts of the activity tree involving commands such
as ?search for?, ?look for? and so on. For instance,
if the user says ?We?re looking for a truck?, that ut-
terance is parsed into a logical form involving the
structure (locate, np[det(a),truck]).
The dialogue manager then accesses the Activity
Model for LOCATE and adds a node to the Activ-
ity Tree describing it. The Activity Model speci-
fies what sub-activities should be invoked, and un-
der what conditions they should be invoked, what
the postconditions of the activity are. Activity Mod-
els are similar to the ?recipes? of (Rich et al, 2001).
For example, in Figure 3 the Activity Model for LO-
CATE states that,
  it uses the camera resource (so that any other
activity using the camera must be suspended,
or a dialogue about resource conflict must be
initiated),
  that the preconditions of the activity are that the
UAV must be airborne, with fuel and engine in-
dicators satisfactory,
  that the whole activity can be skipped if the
UAV is already ?locked-on? to the sought ob-
ject,
  that the postcondition of the activity is that the
UAV is ?locked-on? to the sought object,
  that the activity breaks into three sequen-
tial sub-activities: WATCH-FOR, FOLLOW-OBJ,
and ASK-COMPLETE.
Nodes on the Activity Tree can be either: ac-
tive, complete, failed, suspended, or canceled. Any
change in the state of a node (typically because of
a report from the robot) is placed onto the System
Agenda (see Section 5) for possible verbal report to
the user, via the message selection and generation
module (see Section 6).
Figure 3: A ?Locate? Activity Model for a UAV, exhibiting collaborative dialogue
Locate// locate is "find-by-type", collaborative activity.
// Breaks into subactivities: watch_for, follow, ask_complete.
{ResourcesUsed {camera;} // will be checked for conflicts.
PreConditions //check truth of KIF statements.
{(Status flight inair) (Status engine ok) (Status fuel ok);}
SkipConditions // skip this Activity if KIF condition true.
{(Status locked-on THIS.np);}
PostConditions// assert these KIF statements when completed.
{(Status locked-on THIS.np) ;}
Children SEQ //sequential sub-activities.
{TaskProperties
{command = "watch_for"; // basic robot action ---
np = THIS.np;} // set sensors to search.
TaskProperties
{command = "follow_obj"; //triggers complex activity --
np = THIS.np;} //following a candidate object.
TaskProperties //collaborative speech action:
{command = "ask_complete";//asks user whether this is
np = THIS.np; }}} //object we are looking for.
5 The Dialogue Context Model
Dialogue management falls into two parts ? dialogue
modelling (representation), and dialogue control (al-
gorithm). In this section we focus on the representa-
tional aspects, and section 5.2 surveys the main al-
gorithms. As a representation of conversational con-
text, the dialogue manager uses the following data
structures which make up the dialogue Information
State (IS);
  Dialogue Move Tree (DMT)
  Activity Tree (AT)
  System Agenda (SA)
  Pending List (PL)
  Salience List (SL)
  Modality Buffer (MB)
Figure 4 shows how the Dialogue Move Tree re-
lates to other parts of the dialogue manager as a
whole. The solid arrows represent possible update
functions, and the dashed arrows represent query
functions. For example, the Dialogue Move Tree
can update Salience List, System Agenda, Pend-
ing List, and Activity Tree, while the Activity Tree
can update only the System Agenda and send ex-
ecution requests to the robot, and it can query the
Activity Model (when adding nodes). Likewise, the
Message Generation component queries the System
Agenda and the Pending List, and updates the Dia-
logue Move Tree whenever a synthesized utterance
is produced.
Figure 5 shows an example Information State
logged by the system, displaying the interpretation
of the system?s utterance ?now taking off? as a re-
port about an ongoing ?go to the tower? activity (the
Pending List and System Agenda are empty, and
thus are not shown).
5.1 The Dialogue Move Tree
Dialogue management uses a set of abstract dia-
logue move classes which are domain independent
(e.g. command, activity-query, wh-question, revi-
sion,     ). Any ongoing dialogue constructs a par-
ticular Dialogue Move Tree (DMT) representing the
current state of the conversation, whose nodes are
DIALOGUE
ACTIVITY
MOVE
TREE
AGENDA
SYSTEM TREE
Activities)
(NPs,
(Selection and Aggregation)
SALIENCE
ACTIVITY
LAYER
speech
synthesis
INFORMATION
INDEXICAL
(Active Node List)
MESSAGE 
GENERATION
ACTIVITY
MODEL
DEVICE
LIST
PENDING
LIST
MODALITY
BUFFER
Map Display Inputs
(parsed human speech)
(mouse clicks)
Conversational Move Inputs
Figure 4: Dialogue Manager Architecture (solid arrows denote possible updates, dashed arrows represent
possible queries)
instances of the dialogue move classes, and which
are linked to nodes on the Activity Tree where ap-
propriate, via an activity tag (see below).
Incoming logical forms (LFs) from the pars-
ing process are always tagged with a dialogue
move (see e.g. (Ginzburg et al, 2001)), which pre-
cedes more detailed information about an utter-
ance. For instance the logical form: command([go],
[param-list ([pp-loc(to, arg([np(det([def],the),
[n(tower,sg)])]))])])
corresponds to the utterance ?go to the tower?,
which is flagged as a command.
A slightly more complex example is; re-
port(inform, agent([np([n(uav,sg)])]), compl-
activity([command([take-off])]))
which corresponds to ?I have taken off? ? a re-
port from the UAV about a completed ?taking-off?
activity.
The first problem in dialogue management is
to figure out how these incoming ?Conversational
Moves? relate to the current dialogue context. In
other words, what dialogue moves do they consti-
tute, and how do they relate to previous moves in
the conversation? In particular, given multi-tasking,
to which thread of the conversation does an incom-
ing utterance belong? We use the Dialogue Move
Tree to answer these questions:
1. A DMT is a history or ?message board? of
dialogue contributions, organized by ?thread?,
based on activities.
2. A DMT classifies which incoming utterances
can be interpreted in the current dialogue con-
text, and which cannot be. It thus delimits
a space of possible Information State update
functions.
3. A DMT has an Active Node List which con-
trols the order in which this function space is
searched 6.
4. A DMT classifies how incoming utterances are
to be interpreted in the current dialogue con-
text.
In general, then, we can think of the DMT as
representing a function space of dialogue Informa-
6It also defines an ordering on language models for speech
recognition.
tion State update functions. The details of any par-
ticular update function are determined by the node
type (e.g. command, question) and incoming dia-
logue move type and their contents, as well as the
values of Activity Tag and Agent.
Note that this notion of ?Dialogue Move Tree? is
quite different from previous work on dialogue trees,
in that the DMT does not represent a ?parse? of the
dialogue using a dialogue grammar (e.g. (Ahrenberg
et al, 1990)), but instead represents all the threads
in the dialogue, where a thread is the set of utter-
ances which serve a particular dialogue goal. In the
dialogue grammar approach, new dialogue moves
are attached to a node on the right frontier of the
tree, but in our approach, a new move can attach
to any thread, no matter where it appears in the
tree. This means that the system can flexibly in-
terpret user moves which are not directly related to
the current thread (e.g. a user can ignore a system
question, and give a new command, or ask their
own question). Finite-state representations of dia-
logue games have the restriction that the user is con-
strained by the dialogue state to follow a particular
dialogue path (e.g. state the destination, clarify, state
preferred time,     ). No such restriction exists with
DMTs, where dialogue participants can begin and
discontinue threads at any time.
We discuss this further below.
5.2 Interpretation and State Update
The central algorithm controlling dialogue manage-
ment has two main steps, Attachment, and Process
Node;
1. Attachment: Process incoming input conversa-
tional move   with respect to the current DMT
and Active Node List, and ?attach? a new node

interpreting   to the tree if possible.
2. Process Node: process the new node  , if it
exists, with respect to the current information
state. Perform an Information State update us-
ing the dialogue move type and content of  .
When an update function  exists, its effects de-
pend on the details of the incoming input   (in par-
ticular, to the dialogue move type and the contents
of the logical form) and the DMT node to which it
attaches. The possible attachments can be thought
of as adjacency pairs, and each dialogue move class
contains information about which node types it can
attach. For instance the command node type can at-
tach confirmation, yn-question, wh-question, and re-
port nodes.
Examples of different attachments available in our
current system can be seen in Figure 67. For exam-
ple, the first entry in the table states that a command
node, generated by the user, with activity tag  , is
able to attach any system confirmation move with
the same activity tag, any system yes-no question
with that tag, any system wh- question with that tag,
or any system report with that activity tag. Similarly,
the rows for wh-question nodes state that:
  a wh-question by the system with activity tag 
can attach a user?s wh-answer (if it is a possible
answer for that activity)
  a user?s wh-question can attach a system wh-
answer, and no particular activity need be spec-
ified.
These possible attachments delimit the ways in
which dialogue move trees can grow, and thus clas-
sify the dialogue structures which can be captured in
the current system. As new dialogue move types are
added to the system, this table is being extended to
cover other conversation types (e.g. tutoring (Clark
et al, 2001)).
It is worth noting that the node type created af-
ter attachment may not be the same as the dialogue
move type of the incoming conversational move   .
Depending on the particular node which attaches the
new input, and the move type of that input, the cre-
ated node may be of a different type. For exam-
ple, if a wh-question node attaches an input which is
simply a command, the wh-question node may inter-
pret the input as an answer, and attach a wh-answer.
These interpretation rules are local to the node to
which the input is attached. In this way, the DMT
interprets new input in context, and the pragmatics
of each new input is contextually determined, rather
than completely specified via parsing using conver-
sational move types. Note that Figure 6 does not
state what move type new input is attached as, when
it is attached.
7Where Activity Tags are not specified, attachment does not
depend on sharing of Activity Tags.
In the current system, if the user produces an ut-
terance which can attach to several nodes on the
DMT, only the ?most active? node (as defined by the
Active Node List) will attach the incoming move. It
would be interesting to explore such events as trig-
gers for clarification questions, in future work.
6 Message generation
Since the robot is potentially carrying out multiple
activities at once, a particular problem is how to de-
termine appropriate generation of utterances about
those activities, in a way which does not overload
the user with information, yet which establishes and
maintains appropriate context in a natural way.
Generation for dialogue systems in general is
problematic in that dialogue contributions arise in-
crementally, often in response to another partici-
pant?s utterances. For this reason, generation of
large pieces of text is not appropriate, especially
since the user is able to interrupt the system. Other
differences abound, for example that aggregation
rules must be sensitive to incremental aspects of
message generation.
As well as the general problems of message selec-
tion and aggregation in dialogue systems, this par-
ticular type of application domain presents specific
problems in comparison with, say, travel-planning
dialogue systems ? e.g. (Seneff et al, 1991). An au-
tonomous device will, in general, need to communi-
cate about,
  its perceptions of a changing environment,
  progress towards user-specified goals,
  execution status of activities or tasks,
  its own internal state changes,
  the progress of the dialogue itself.
For these reasons, the message selection and gen-
eration component of such a system needs to be
of wider coverage and more flexible than template-
based approaches, while remaining in real, or near-
real, time (Stent, 1999). As well as this, the system
must potentially be able to deal with a large band-
width stream of communications from the robot,
and so must be able to intelligently filter them for
?relevance? so that the user is not overloaded with
unimportant information, or repetitious utterances.
In general, the system should appear as ?natural? as
possible from the user?s point of view ? using the
same language as the user if possible (?echoing?),
using anaphoric referring expressions where possi-
ble, and aggregating utterances where appropriate.
A ?natural? system should also exhibit ?variability?
in that it can convey the same content in a variety
of ways. A further desirable feature is that the sys-
tem?s generated utterances should be in the cover-
age of the dialogue system?s speech recognizer, so
that system-generated utterances effectively prime
the user to speak in-grammar.
Consequently we attempted to implement the fol-
lowing features in message selection and generation:
relevance filtering; recency filtering; echoing; vari-
ability; aggregation; symmetry; real-time genera-
tion.
Our general method is to take as inputs to the pro-
cess various communicative goals of the system, ex-
pressed as logical forms, and use them to construct a
single new logical form to be input to Gemini?s Se-
mantic Head-Driven Generation algorithm (Shieber
et al, 1990), which produces strings for Festival
speech synthesis. We now describe how to use com-
plex dialogue context to produce natural generation
in multitasking contexts.
6.1 Message selection - filtering
Inputs to the selection and generation module are
?concept? logical forms (LFs) describing the com-
municative goals of the system. These are struc-
tures consisting of context tags (e.g. activity identi-
fier, dialogue move tree node, turn tag) and a con-
tent logical form consisting of a Dialogue Move
(e.g. report, wh-question), a priority tag (e.g. warn
or inform), and some additional content tags (e.g.
for objects referred to). An example input logical
form is, ?report(inform, agent(AgentID), cancel-
activity(ActivityID))?, which corresponds to the re-
port ?I have cancelled flying to the tower? when
AgentID refers to the robot and ActivityID refers to
a ?fly to the tower? task.
Items which the system will consider for genera-
tion are placed (either directly by the robot, or indi-
rectly by the Activity Tree) on the ?System Agenda?
(SA), which is the part of the dialogue Information
State which stores communicative goals of the sys-
tem. Communicative goals may also exist on the
?Pending List? (PL) which is the part of the infor-
mation state which stores questions that the system
has asked, but which the user has not answered, so
that they may be re-raised by the system. Only ques-
tions previously asked by the system can exist on the
Pending List.
Due to multi-tasking, at any time there is a num-
ber of ?Current Activities? which the user and sys-
tem are performing (e.g. fly to the tower, search for
a red car). These activities are topics of conversa-
tion (defining threads of the DMT) represented in
the dialogue information state, and the system?s re-
ports can be generated by them (in which case the
are tagged with that activity label) or can be rele-
vant to an activity in virtue of being about an object
which is in focus because it is involved in that activ-
ity.
Some system reports are more urgent that others
(e.g. ?I am running out of fuel?) and these carry the
label warning. Warnings are always relevant, no
matter what activities are current ? they always pass
the recency and relevance filters.
Echoing (for noun-phrases) is achieved by access-
ing the Salience List whenever generating referential
terms, and using whatever noun-phrase (if any) the
user has previously employed to refer to the object
in question. If the object is top of the salience list,
the generator will select an anaphoric expression.
The end result of our selection and aggregation
module (see section 6.2) is a fully specified logi-
cal form which is to be sent to the Semantic-Head-
Driven Generation component of Gemini (Shieber
et al, 1990). The bi-directionality of Gemini (i.e.
that we use the same grammar for both parsing and
generation) automatically confers a useful ?symme-
try? property on the system ? that it only utters sen-
tences which it can also understand. This means that
the user will not be misled by the system into em-
ploying out-of-vocabulary items, or out-of-grammar
constructions. Another side effect of this is that
the system utterances prime the user to make in-
grammar utterances, thus enhancing co-ordination
between user and system in the dialogues.
6.2 Incremental aggregation
Aggregation combines and compresses utterances to
make them more concise, avoid repetitious language
structure, and make the system?s speech more nat-
ural and understandable. In a dialogue system ag-
gregation should function incrementally because ut-
terances are generated on the fly. In dialogue sys-
tems, when constructing an utterance we often have
no information about the utterances that will follow
it, and thus the best we can do is to compress it
or ?retro-aggregate? it with utterances that preceded
it. Only occasionally does the System Agenda con-
tain enough unsaid utterances to perform reasonable
?pre-aggregation?.
Each dialogue move type (e.g. report, wh-
question) has its own aggregation rules, stored in
the class for that LF type. In each type, rules spec-
ify which other dialogue move types can aggregate
with it, and exactly how aggregation works. The
rules note identical portions of LFs and unify them,
and then combine the non-identical portions appro-
priately.
For example, the LF that represents the phrase ?I
will fly to the tower and I will land at the parking
lot?, will be converted to one representing ?I will fly
to the tower and land at the parking lot? according
to the compression rules. Similarly, ?I will fly to the
tower and fly to the hospital? gets converted to ?I
will fly to the tower and the hospital?.
The ?retro-aggregation? rules result in sequences
of system utterances such as, ?I have cancelled fly-
ing to the school. And the tower. And landing at the
base.?
7 Summary
We explained the dialogue modelling techniques
which we implemented in order to build a real-
time multi-modal conversational interface to an au-
tonomous device. The novel issues tackled by the
system and its dialogue model are that it is able to
manage conversations about multiple tasks and col-
laborative activities in a robust and natural way.
We argued that in the case of dialogues with
devices, a dialogue management mechanism has
to be particularly robust and flexible, especially
in comparison with finite-state or frame-based di-
alogue managers which have been developed for
information-seeking dialogues, such as travel plan-
ning, where topics of conversation are predeter-
mined. Another challenge was that conversations
may have multiple open topics at any one time, and
this complicates utterance interpretation and gener-
ation.
We discussed the dialogue context model and al-
gorithms used to produce a system with the follow-
ing features:
  supports multi-tasking, multiple topics, and
collaboration,
  support of commands, questions, revisions, and
reports, over a dynamic environment,
  multi-modal, mixed-initiative, open-ended dia-
logues,
  echoic and variable message generation, fil-
tered for relevance and recency
  asynchronous, real-time operation.
An video demonstration of the system is avail-
able at www-csli.stanford.edu/semlab/
witas/.
References
Lars Ahrenberg, Arne Jonsson, and Nils Dalhbeck. 1990.
Discourse representation and discourse management
for natural language interfaces. In In Proceedings of
the Second Nordic Conference on Text Comprehension
in Man and machine.
James Allen, Donna Byron, Myroslva Dzikovska, George
Ferguson, Lucian Galescu, and Amanda Stent. 2001.
Toward conversational human-computer interaction.
AI Magazine, 22(4):27?37.
Brady Clark, John Fry, Matt Ginzton, Stanley Pe-
ters, Heather Pon-Barry, and Zachary Thomsen-Gray.
2001. Automated tutoring dialogues for training in
shipboard damage control. In Proceedings of SIGdial
2001.
Patrick Doherty, Go?sta Granlund, Krzystof Kuchcinski,
Erik Sandewall, Klas Nordberg, Erik Skarman, and Jo-
han Wiklund. 2000. The WITAS unmanned aerial
vehicle project. In European Conference on Artificial
Intelligence (ECAI 2000).
Renee Elio and Afsaneh Haddadi. 1999. On abstract
task models and conversation policies. In Workshop
on Specifying and Implementing Conversation Poli-
cies, Autonomous Agents?99, Seattle.
Jonathan Ginzburg, Ivan A. Sag, and Matthew Purver.
2001. Integrating Conversational Move Types in
the Grammar of Conversation. In Bi-Dialog 2001?
Proceedings of the 5th Workshop on Formal Semantics
and Pragmatics of Dialogue, pages 45?56.
Beth-Ann Hockey, Gregory Aist, Jim Hieronymous,
Oliver Lemon, and John Dowding. 2002. Targeted
help: Embedded training and methods for evaluation.
In Proceedings of Intelligent Tutoring Systems (ITS).
(to appear).
Oliver Lemon, Anne Bracy, Alexander Gruenstein, and
Stanley Peters. 2001. Information states in a multi-
modal dialogue system for human-robot conversation.
In Peter Ku?hnlein, Hans Reiser, and Henk Zeevat, edi-
tors, 5th Workshop on Formal Semantics and Pragmat-
ics of Dialogue (Bi-Dialog 2001), pages 57 ? 67.
Oliver Lemon, Alexander Gruenstein, and Stanley Peters.
2002. Collaborative activities and multi-tasking in di-
alogue systems. Traitement Automatique des Langues
(TAL). Special Issue on Dialogue (to appear).
Susann LuperFoy, Dan Loehr, David Duff, Keith Miller,
Florence Reeder, and Lisa Harper. 1998. An architec-
ture for dialogue management, context tracking, and
pragmatic adaptation in spoken dialogue systems. In
COLING-ACL, pages 794 ? 801.
Micheal McTear. 1998. Modelling spoken dialogues
with state transition diagrams: Experiences with the
CSLU toolkit. In Proc 5th International Conference
on Spoken Language Processing.
Charles Rich, Candace Sidner, and Neal Lesh. 2001.
Collagen: applying collaborative discourse theory to
human-computer interaction. AI Magazine, 22(4):15?
25.
S. Seneff, L. Hirschman, and V. W. Zue. 1991. Interac-
tive problem solving and dialogue in the ATIS domain.
In Proceedings of the Fourth DARPA Speech and Nat-
ural Language Workshop. Morgan Kaufmann.
Stuart M. Shieber, Gertjan van Noord, Fernando C. N.
Pereira, and Robert C. Moore. 1990. Semantic-
head-driven generation. Computational Linguistics,
16(1):30?42.
Amanda Stent. 1999. Content planning and generation
in continuous-speech spoken dialog systems. In Pro-
ceedings of KI?99 workshop ?May I Speak Freely??.
Figure 5: A snapshot of an Information State (from the HTML system logs)
Utterance: ??now taking off?? (by System 11/7/01 4:50 PM)
Conversational Move:
report(inform,agent([np([n(uav,sg)])]),curr_activity([command([take_off])]))
Dialogue Move Tree (position on active node list in parens [0 = most active])
* Root (1)
Root
o Command (0)
command([go],[param_list([pp_loc(to,arg([np(det([def],the),[n(tower,
sg)])]))])]) [[dmtask0] current]
+ Report
report(inform,agent([np([n(uav,sg)])]),curr_activity([command
([take_off])]))[]
o Report
report(inform,agent([np([n(uav,sg)])]),confirm_activity([command([go],
[param_list([pp_loc(to,arg([np(det([def],the),[n(tower,sg)],
)]))])])])) [[dmtask0] current]
Activity Tree
* root
o [dmtask0] current
relation = SEQuential
command = go
pp = pp_loc(to,Args)
np = np(det([def],the),[n(tower,sg)])
+ [sim3] current
relation = none
command = take_off
pp = null, np = null
Salience List (least salient -- most salient)
* [np(det([def],the),[n(tower,sg)])] (speech)
* [np(det([def],the),[n(tower,sg)])] (speech)
Figure 6: Attachment in the Dialogue Move Classes
DMT Node Attaches
Node Type Activity
Tag
Speaker Node Type Activity
Tag
Speaker
command t user confirmation, t system
y-n question, t system
wh-question, t system
report t system
confirmation t system
report t system command t user
wh-question t system wh-answer t user
wh-question user wh-answer system
yn-question t system yn-answer t user
revision t user wh-question t system
yn-answer t user confirmation t system
wh-answer user confirmation system
wh-answer system confirmation user
root n/a n/a command, user
question, user
revision user
root n/a n/a report system
Figure 7: Part of the Graphical User Interface, showing a flight plan
Probabilistic Dialogue Modelling
Oliver Lemon
CSLI
Stanford University
lemon@csli.stanford.edu,
Prashant Parikh
IRCS
University of Pennsylvania
pjparikh@aol.com
Stanley Peters
CSLI
Stanford University
peters@csli.stanford.edu
Abstract
We show how Bayesian networks and re-
lated probabilistic methods provide an ef-
ficient way of capturing the complex bal-
ancing of different factors that determine
interpretation and generation in dialogue.
As a case study, we show how a prob-
abilistic approach can be used to model
anaphora resolution in dialogue1 .
1 Introduction
The use of probabilistic and decision-theoretic in-
ference in dialogue modelling and management has
been explored in preliminary fashion by (Pulman,
1996) and (Keizer, 2001). Probabilistic meth-
ods look promising when modelling systems where
there is uncertainty, and simple true/false judge-
ments obscure some of the subtleties of represen-
tation and processing that are required of an accu-
rate model. Dialogue systems are of this nature be-
cause uncertainty is present due to speech recogni-
tion noise, speech-act uncertainty, and so on. Epis-
temic uncertainty is rife in dialogue, and probability
distributions provide a natural model of the ambigu-
ities that thus arise. For these reasons it is natural to
explore probabilistic representations and algorithms
in dialogue management, rather than purely deter-
ministic models. We have experience building deter-
ministic dialogue managers (see e.g. (Lemon et al,
1This research was (partially) funded under the Wallenberg
laboratory for research on Information Technology and Au-
tonomous Systems (WITAS) Project, Linko?ping University, by
the Wallenberg Foundation, Sweden.
2001; Lemon et al, 2002)) which use deterministic
context update rules.
This paper briefly describes our construction of a
Bayes Net modelling dialogue context. We will con-
sider a series of examples of increasing complexity
involving anaphoric resolution in Section 3.1. We
will point out how they are to be resolved intuitively,
and then discuss how our Bayesian net fares. We
will see that many of the best insights of determin-
istic approaches (e.g. in the axiomatic BDI tradition
and in the planning literature) can be preserved, of-
ten in less brittle forms, in a probabilistic setting.
1.1 Probabilistic modelling ideas
Our approach to resolving anaphora (and dialogue
moves) was to generate a probability distribution of
the random variable of interest (e.g. salience of ref-
erent) and then choose the value of the variable cor-
responding to the highest probability as the interpre-
tation (e.g. the referent). This decision has a theo-
retical justification that can be found in a theorem in
(Parikh, 1990) in the context of his game-theoretic
model of language use. The theorem states that un-
der certain conditions (which hold in our context)
the correct interpretation of an utterance is the most
likely one.
2 Interpretation and Generation
The two major aspects of dialogue management are
the interpretation of incoming (user) utterances, and
the timely and appropriate generation of utterances
by the dialogue system. To cover these aspects we
have constructed a Bayes Net as shown in Figure 1.
     Philadelphia, July 2002, pp. 125-128.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
In the implementation of this network we used
CIspace?s Java-based Bayes Net toolkit.2
The conditional probability table for the nodes
representing the dialogue move at time   and
salience list at time   are obviously the core of
the network. These tables are too large to present
in this paper. We constructed them by hand, us-
ing heuristics gained from experience in program-
ming rule-based dialogue systems. In future, the ta-
bles could be learned from data, or could instantiate
continuous-valued functions of rule-based systems.
Salience
List
t?1
System
Utterance
t
User 
Dialogue
Move
t
User
Dialogue
Move
t?1
Dialogue
Move
System
t?1
List
Salience
t
t
Activity
Input 
User 
Logical 
Form t
Figure 1: A Prototype Bayes Net for dialogue man-
agement
3 Anaphora resolution
Several different factors enter into the resolution of
anaphora in a dialogue. How recently a potential
referent was referred to is one important factor, an-
other is the embedding activity within which the
anaphoric reference was made (e.g. the type of verb
phrase in which the referent appears), a third is the
intra-sentential location of the relevant noun phrases
in the preceding dialogue, a fourth is the relative
prominence of a potential referent in the dialogue
situation, and so on. The basic idea is that condi-
2www.cs.ubc.ca/labs/lci/CIspace/
tional probability distributions are generated dynam-
ically in the Bayesian network. When we look at a
distribution corresponding to a node we are inter-
ested in, then the most salient object in the context
will be the one whose value has the highest proba-
bility.
We point out that an obvious deterministic way
to rank different combinations of factors (in an
optimality-theoretic way, for example), and choose
the most salient object based on this ranking, does
not seem to work ? because any potential ranking
of principles (e.g. ?Recency overrides subject place-
ment?) would have to be context-dependent, and this
defeats the idea of a ranking. See the examples in
Figures 5 and 6.
3.1 Examples
Here we work with two basic activities of the
WITAS robot helicopter (see (Lemon et al, 2002))
which our dialogue system interfaces to ? moving
and searching. The helicopter can move to vari-
ous landmarks (e.g. the tower, the church) and way-
points, and can see various objects (e.g. landmarks,
vehicles). Our results use the network in Figure 1. In
reading the tables (the figures appear after the refer-
ences), use the following key:
U=user, S=system, r=red car, g=green car,
w=waypoint, s= search, m=move. All examples
start with an even distribution of 0.2 for all variables
(all objects are equally salient) at the start of each
dialogue.
3.1.1 Activity and recency
We will start with what may be the simplest type
of example of anaphoric reference, in Figure 2. Here
it is intuitively clear that ?it? is intended to pick out
the green car. The contribution of ?see it? is mod-
elled as an observed even distribution over all possi-
ble referents which can be seen (i.e.  and  each
get a 0.5 weight). The conditional probability table
for Salience List at time   is then used to compute the
new probability distribution over the object-activity
pairs ( 	
		
 ). Here we see that the
green car is the most salient after the user?s second
utterance (  ), and that this salience increases after
the utterance ?it?, because  was both the most re-
cent NP, and is also a possible object in the context
of the ?seeing? activity.
In the example in Figure 3, the anaphoric pronoun
?it? should pick out the red car and not the waypoint,
even though the waypoint was referred to more re-
cently. Intuitively, this is because the embedding ac-
tivity of looking for the red car is tied to the pro-
noun, and this fact overrides the most recent refer-
ent. Here, the waypoint is not a possible object in
the ?seeing? activity, whereas the red car has been
introduced as part of that activity. Thus the pronoun
?it? in the user?s final utterance has the effect of rais-
ing the probabilities of all the objects which can be
seen, and this in fact overrides the recency effect of
the utterance of ?waypoint?.
An extended example (not shown) shows how ac-
tivity information can outweigh recency in an inter-
leaved fashion and then that a newly introduced ref-
erent can become the most salient. Having consid-
ered the ways in which activity and recency interact
in determining salience for anaphoric resolution, we
then investigated adding another determining factor
in the model ? the syntactic placement of the refer-
ring expression.
3.1.2 Placement, activity, and recency
Figure 4 shows how subject placement influences
availability for anaphoric reference. Here, the sub-
ject (?red car?) of the user?s second utterance is in-
tuitively the one picked out by the later anaphoric
expression, and not the green car, even though ?the
green car? is the most recent NP. See Figure 4 for our
results, using an extension of the network in Figure
1, where the ?Activity   ? node was enriched to in-
clude syntactic information about the input ? specif-
ically, what referring expressions appear in subject
and object places. Note here that the red car be-
comes the most salient object after the user?s sec-
ond utterance. We model the referential import of
this sentence as an input triple ?     ?to the Activ-
ity   node ? denoting: red car (subject), no activity,
green car (object). The updated table for this node
ensures that objects in subject place are given more
weight than those in object place.
In Figure 5, the subject (?red car?) of the user?s
second utterance is intuitively the one picked out by
the later anaphoric expression, and not the green car,
even though ?the green car? is involved in the ?see-
ing? activity.
In Figure 6 the red car is most salient after the
second utterance, but the waypoint becomes more
salient, even though the red car was in subject po-
sition, because the waypoint is involved in the ac-
tivity of moving, as is the pronoun ?it?, and so is
a better candidate for anaphoric resolution. Com-
bined with Figure 5 this shows that no static ranking
of anaphoric binding principles will cover all situ-
ations, and that a probabilistic approach is useful ?
even as a theoretical model.
Obviously this model could be made more com-
plex with representations for direct and indirect ob-
jects, and so forth, but we leave this for future work.
4 Conclusion
We presented a Bayes Net which we have imple-
mented to deal with dialogue move interpretation
and reference resolution, and gave examples of its
use for weighing a variety of factors (recency, activ-
ity, placement) in anaphoric resolution in particular.
We saw that many of the insights of deterministic ap-
proaches (e.g. the WITAS Project, see (Lemon et al,
2002)) can be preserved, often in less brittle forms,
in a probabilistic setting. We also have unpublished
results for dialogue move classification.
References
Simon Keizer. 2001. A probabilistic approach to dia-
logue act clarification. In Proceedings of Bi-Dialog
2001.
Oliver Lemon, Anne Bracy, Alexander Gruenstein, and
Stanley Peters. 2001. Information states in a multi-
modal dialogue system for human-robot conversation.
In Peter Ku?hnlein, Hans Reiser, and Henk Zeevat, edi-
tors, 5th Workshop on Formal Semantics and Pragmat-
ics of Dialogue (Bi-Dialog 2001), pages 57 ? 67.
Oliver Lemon, Alexander Gruenstein, Alexis Battle, and
Stanley Peters. 2002. Multi-tasking and collabora-
tive activities in dialogue systems. In Proceedings
of 3rd SIGdial Workshop on Discourse and Dialogue,
Philadelphia. (to appear).
Prashant Parikh. 1990. Situations, games, and ambigu-
ity. In R. Cooper, K. Mukai, and J. Perry, editors, Situ-
ation Theory and its Applications I. CSLI Publications.
Prashant Parikh. 2001. The Use of Language. CSLI
Publications, Stanford, CA.
Stephen G. Pulman. 1996. Conversational games, belief
revision and bayesian networks. In 7th Computational
Linguistics in the Netherlands (CLIN) meeting.
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Search for the red car .22 .06 .06 .6 .06 red car
S: Okay
U: Go to the green car .066 .53 .018 .18 .206 green car
S: Okay
U: Can you see it? .0196 .7002 .0054 .054 .2206 green car
Figure 2: Probability distributions in example: Recency
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Do you see the red car? .18 .06 .06 .64 .06 red car
S: No
U: Go to the waypoint .054 .018 .718 .192 .008 waypoint
S: Okay
U: Do you see it? .1108 .1036 .2154 .313 .2572 red car
Figure 3: Probability distributions in example: Activity overrides recency
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Go to the green car .06 .524 .06 .06 .296 green car
S: Okay
U: The red car is beside the
green car
.333 .1899 .018 .333 .1261 red car
S: Okay
U: Can you see it? .1955 .1622 .0054 .3543 .2825 red car
Figure 4: Probability distributions in example: Subject placement overrides recency
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Search for the green car .06 .296 .06 .06 .524 green car
S: Okay
U: The red car is beside the
green car
.263 .2006 .018 .263 .2554 red car
S: Okay
U: Can you see it? .1796 .1622 .0054 .3282 .3206 red car
Figure 5: Probability distributions in example: Subject placement overrides activity
Utterance P(rm) P(gm) P(wm) P(rs) P(gs) Salient
U: Go to the waypoint .06 .06 .76 .06 .06 waypoint
R: The red car is at the way-
point
.333 .018 .298 .333 .018 red car
U: Did you get to it? .2888 .1582 .3288 .1444 .0913 waypoint
Figure 6: Probability distributions for example: Activity overrides subject placement
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 82?93, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Optimising Incremental Dialogue Decisions Using Information Density for
Interactive Systems
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
School of Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh, Scotland
n.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.uk
Abstract
Incremental processing allows system design-
ers to address several discourse phenomena
that have previously been somewhat neglected
in interactive systems, such as backchannels
or barge-ins, but that can enhance the re-
sponsiveness and naturalness of systems. Un-
fortunately, prior work has focused largely
on deterministic incremental decision mak-
ing, rendering system behaviour less flexible
and adaptive than is desirable. We present a
novel approach to incremental decision mak-
ing that is based on Hierarchical Reinforce-
ment Learning to achieve an interactive op-
timisation of Information Presentation (IP)
strategies, allowing the system to generate
and comprehend backchannels and barge-ins,
by employing the recent psycholinguistic hy-
pothesis of information density (ID) (Jaeger,
2010). Results in terms of average rewards
and a human rating study show that our learnt
strategy outperforms several baselines that are
not sensitive to ID by more than 23%.
1 Introduction
Recent work on incremental systems has shown
that adapting a system?s turn-taking behaviour to be
more human-like can improve the user?s experience
significantly, based on incremental models of auto-
matic speech recognition (ASR) (Baumann et al
2011), dialogue management (Buss et al2010), and
speech generation (Skantze and Hjalmarsson, 2010).
All of these approaches are based on the same gen-
eral abstract architecture of incremental processing
(Schlangen and Skantze, 2011). While this archi-
tecture offers inherently incremental mechanisms to
update and revise input hypotheses, it is affected
by a number of drawbacks, shared by determinis-
tic models of decision making in general: they rely
on hand-crafted rules which can be time-consuming
and expensive to produce, they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, they are unable to gener-
alise and adapt flexibly to unseen situations, and
they do not use automatic optimisation. Statisti-
cal approaches to incremental processing that ad-
dress some of these problems have been suggested
by Raux and Eskenazi (2009), who use a cost matrix
and decision theoretic principles to optimise turn-
taking in a dialogue system under the constraint that
users prefer no gaps and no overlap at turn bound-
aries. Also, DeVault et al2009) use maximum en-
tropy classification to support responsive overlap in
an incremental system by predicting the completions
of user utterances. Selfridge et al2011) use logis-
tic regression models to predict the stability and ac-
curacy of incremental speech recognition results to
enhance performance without causing delay. For re-
lated work on (deterministic) incremental language
generation, please see (Kilger and Finkler, 1995;
Purver and Otsuka, 2003).
Recent years have seen a number of data-driven
approaches to interactive systems that automatically
adapt their decisions to the dialogue context us-
ing Reinforcement Learning (Levin et al2000;
Walker, 2000; Young, 2000; Singh et al2002;
Pietquin and Dutoit, 2006; Henderson et al2008;
Cuaya?huitl et al2010; Thomson, 2009; Young et
al., 2010; Lemon, 2011; Janarthanam and Lemon,
2010; Rieser et al2010; Cuaya?huitl and Dethlefs,
82
2011; Dethlefs and Cuaya?huitl, 2011). While these
approaches have been shown to enhance the perfor-
mance and adaptivity of interactive systems, unfor-
tunately none of them has yet been combined with
incremental processing.
In this paper, we present a novel approach to in-
cremental decision making for output planning that
is based on Hierarchical Reinforcement Learning
(HRL). In particular, we address the problem of op-
timising IP strategies while allowing the system to
generate and comprehend backchannels and barge-
ins based on a partially data-driven reward func-
tion. Generating backchannels can be beneficial for
grounding in interaction. Similarly, barge-ins can
lead to more efficient interactions, e.g. when a sys-
tem can clarify a bad recognition result immediately
before acting based on a misrecognition.
A central concept to our approach is Information
Density (ID) (Jaeger, 2010), a psycholinguistic hy-
pothesis that human utterance production is sensitive
to a uniform distribution of information across the
utterance. This hypothesis has also been adopted for
low level output planning recently, see e.g. Rajku-
mar and White (2011). Our results in terms of av-
erage rewards and a human rating study show that a
learning agent that is sensitive to ID can learn when
it is most beneficial to generate feedback to a user,
and outperforms several other agents that are not
sensitive to ID.
2 Incremental Information Presentation
2.1 Information Presentation Strategies
Our example domain of application is the Infor-
mation Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al2010). This previous work
incrementally constructs IP strategies according to
the predicted user reaction, whereas our approach
focuses on whether and when to generate backchan-
nels and barge-ins and how to react to user barge-
ins in the context of dynamically changing input hy-
potheses. We therefore implement a simplified ver-
sion of Rieser et al model. Their system distin-
guished two steps: the selection of an IP strategy
and the selection of attributes to present to the user.
We assume here that the choice of attributes is deter-
mined by matching the types specified in the user in-
put, so that our system only needs to choose a strat-
egy for presenting its results. Attributes include cui-
sine, food quality, location, price range and service
quality of a restaurant. The system then performs a
database lookup and chooses among three main IP
strategies summary, comparison, recommendation
and several ordered combinations of these. Please
see Rieser et al2010) for details. Table 1 shows
examples of the main types of IP strategies that we
generate.
2.2 Backchannels and Barge-ins
An important advantage of incremental processing
can be the increased reactiveness of systems. In this
paper, we focus on the phenomena of backchannels
and barge-ins that can act as feedback in an interac-
tion for both user and system. Figure 1 shows some
examples. Backchannels can often be interpreted as
signals of grounding. Coming from the user, the sys-
tem may infer that the user is following the presenta-
tion of information or is confirming a piece of infor-
mation without trying to take the turn. Similarly, we
can allow a system to generate backchannels to the
user to confirm that it understands the user?s prefer-
ences, i.e. receives high confidence scores from the
ASR module. An important decision for a dialogue
system is then when to generate a backchannel?
Barge-ins typically occur in different situations.
The user may barge-in on the system to correct an
ASR error (such as ?Italian? instead of ?Indian? in
Figure 1) or the system may want to barge-in on the
user to confirm a low-confidence ASR hypothesis so
as to be able to start an immediate database look up
for results. In the former case, the user barging-in
on the system, we assume that the system has two
choices: yielding the turn to the user, or trying to
keep the turn. In the latter case, the system barging-
in on the user, the system would have to decide if and
when it would be beneficial to barge-in on a user ut-
terance. In the following sections, we will develop
a model of dialogue optimisation that can address
these question based on Hierarchical RL that opti-
mises system behaviour based on trade-offs defined
in terms of ID.
83
Type Example
Comparison The restaurant Roma is in the medium price range, but does not serve excellent food.
The restaurants Firenze and Verona both have great food but are more expensive. The
restaurant Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expen-
sive, but has great food and service.
Summary I found 24 Italian restaurants in the city centre that match your query. 11 of them are
in the medium price range, 5 are cheap and 8 are expensive.
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
Backchannel 1 (the system backchannels)
USR I want Italian food [500 ms] in the city centre. . .
SYS uh-huh
SYS OK. I found 24 Italian restaurants in the city centre. The
restaurant Roma is in the medium price range, but does not
have great food. The restaurants Firenze and Verona . . .
Backchannel 2 (the user backchannels)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 central Italian restaurants . . .
USR OK.
SYS The restaurant Verona has great food but is also a bit
expensive. The Roma is cheaper, but not as central as Verona . . .
Barge-ins 1 (the user barges-in on system)
USR I want Italian food in the centre of town . . .
SYS I found 35 Indian . . .
USR Not Indian, I want Italian.
SYS OK, Italian . . .
SYS I have 24 Italian restaurants . . .
Barge-ins 2 (the system barges-in on user)
USR I need an Italian restaurant that is located . . .
SYS I?m sorry. Did you say
Indian or Italian?
USR I said Italian. And in the centre of town please.
SYS OK, let me see. I have 24 Italian restaurants . . .
Figure 1: Example phenomena generated with the learnt
policy. The agent has learnt to produce backchannels
and barge-ins at the appropriate moment and alternative
strategies to deal with user barge-ins.
3 Information Theory
Information Theory as introduced by Shannon
(1948) is based on two main concepts: a communi-
cation channel through which information is trans-
ferred in bits and the information gain, i.e. the in-
formation load that each bit carries. For natural lan-
guage, the assumption is that people aim to com-
municate according to the channel?s capacity, which
corresponds to the hearer?s capacity in terms of cog-
nitive load. If they go beyond that, the cognitive load
of the listener gets too high. If they stay (far) below,
too little information is transferred per bit (i.e., the
utterance is inefficient or uninformative). The in-
formation gain of each word, which is indicative of
how close we are to the channel?s capacity, can be
computed using entropy measures.
3.1 Information Density
Psycholinguistic research has presented evidence for
users distributing information across utterances uni-
formly, so that each word is carrying roughly the
same amount of information. This has been ob-
served for phonetic phenomena based on words
(Bell et al2003) and syllables (Aylett and Turk,
2004), and for syntactic phenomena (Levy and
Jaeger, 2007; Jaeger, 2010). Relating ID to likeli-
hood, we can say that the less frequent a word is, the
more information it is likely to carry (Jaeger, 2010).
For example the word ?the? often has a high corpus
frequency but a low ID.
The ID is defined as the log-probability of an
event (i.e. a word) (Shannon, 1948; Levy and Jaeger,
2007), so that for an utterance u consisting of the
word sequence w1 . . . wi?1, we can compute the ID
at each point during the utterance as:
log 1P (u) =
n
?
i=1
log 1P (wi|w1 . . . wi?1)
(1)
While typically the context of a word is given by
all preceding words of the utterance, we follow Gen-
zel and Charniak (2002) in restricting our computa-
tion to tri-grams for computability reasons. Given a
84
language model of the domain, we can therefore op-
timise ID in system-generated discourse, where we
treat ID as ?an optimal solution to the problem of
rapid yet error-free communication in a noisy envi-
ronment? (Levy and Jaeger (2007), p.2). We will
now transfer the notion of ID to IP and investigate
the distribution of information over user restaurant
queries.
3.2 Information Density in User Utterances
We aim to use ID for incremental IP in two ways:
(1) to estimate the best moment for generating
backchannels or barge-ins to the user, and (2) to de-
cide whether to yield or keep the current system turn
in case of a user barge-in. While we do not have spe-
cific data on human barge-in behaviour, we know
from the work of (Jaeger, 2010), e.g., that ID influ-
ences human language production. We therefore hy-
pothesise a relationship between ID and incremen-
tal phenomena. A human-human data collection is
planned for the near future.
To compute the ID of user and system utterances
at each time step, we estimated an n?gram lan-
guage model (using Kneser-Ney smoothing) based
on a transcribed corpus of human subjects interact-
ing with a system for restaurant recommendations of
Rieser et al2011).1 The corpus contained user ut-
terances as exemplified in Figure 1 and allowed us to
compute the ID at any point during a user utterance.2
In this way, we can estimate points of low density
which may be eligible for a barge-in or a backchan-
nel. Figure 2 shows some example utterances drawn
from the corpus and their ID including the first sen-
tence from Figure 1. These examples were typical
for what could generally be observed from the cor-
pus. We see that while information is transmitted
with varying amounts of density, the main bits of in-
formation are transmitted at a scale between 2 and
7.
Due to a lack of human data for the system utter-
ances, we use the same corpus data to compute the
ID of system utterances.3 The learning agent can use
1Available at http://www.macs.hw.ac.uk/
ilabarchive/classicproject/data/login.php.
2Note that our model does not currently handle out-of-
domain words. In future work, we will learn when to seek clar-
ification.
3We plan a data collection of such utterances for the future,
1 2 3 4 5 6 7 8 9 10 11 12
0
2
4
6
8
10
12
14
16
18
20
In
fo
rm
at
io
n 
De
ns
ity
Time 
 
 
I want Italian food in the city centre.
Yes, I need a moderately priced restaurant in the New Chesterton area.
I need the address of a Thai restaurant.
Figure 2: Information Density for example utterances,
where peaks indicate places of high density.
this information to consider the trade-off of yielding
a current turn to the user or trying to keep it, e.g., in
case of a user barge-in given the ID of its own turn
and of the user?s incoming turn. Such decisions will
be made incrementally in our domain given dynam-
ically changing hypotheses of user input.
4 Incremental Utterance Optimisation
To optimise incremental decision making for an in-
teractive system given the optimisation measure of
ID, we formalise the dialogue module as a Hierar-
chical Reinforcement Learning agent and learn an
optimal action policy by mapping states to actions
and optimising a long-term reward signal. The di-
alogue states can be seen as representing the sys-
tem?s knowledge about the task, the user and the
environment. The dialogue actions correspond to
the system?s capabilities, such as present the re-
sults or barge-in on the user. They also handle in-
cremental updates in the system. In addition, we
need a transition function that specifies the way
that actions change the environment (as expressed
in the state representation) and a reward function
which specifies a numeric value for each action
taken. In this way, decision making can be seen
as a finite sequence of states, actions and rewards
{s0, a0, r1, s1, a1, ..., rt?1, st}, where the goal is to
induce an optimal strategy automatically using Rein-
forcement Learning (RL) (Sutton and Barto, 1998).
We used Hierarchical RL, rather than flat RL, be-
cause the latter is affected by the curse of dimen-
sionality, the fact that the state space grows expo-
nentially according to the state variables taken into
account. This affects the scalability of flat RL agents
but for now make the assumption that using the corpus data is
informative since they are from the same domain.
85
and limits their application to small-scale problems.
Since timing is crucial for incremental approaches,
where processing needs to be fast, we choose a hi-
erarchical setting for better scalability. We denote
the hierarchy of RL agents as M ij where the in-
dexes i and j only identify an agent in a unique
way, they do not specify the execution sequence of
subtasks, which is subject to optimisation. Each
agent of the hierarchy is defined as a Semi-Markov
Decision Process (SMDP) consisting of a 4-tuple
< Sij , Aij , T ij , Rij >. Here, Sij denotes the set of
states, Aij denotes the set of actions, and T ij is a
probabilistic state transition function that determines
the next state s? from the current state s and the per-
formed action a. Rij(s?, ? |s, a) is a reward function
that specifies the reward that an agent receives for
taking an action a in state s lasting ? time steps
(Dietterich, 1999). Since actions in SMDPs may
take a variable number of time steps to complete,
the variable ? represents this number of time steps.
The organisation of the learning process into dis-
crete time steps allows us to define incremental hy-
pothesis updates as state updates and transitions in
an SMDP. Whenever conditions in the learning en-
vironment change, such as the recogniser?s best hy-
pothesis of the user input, we represent them as tran-
sitions from one state to another. At each time step,
the agent checks for changes in its state represen-
tation and takes the currently best action according
to the new state. The best action in an incremental
framework can also include generating a backchan-
nel to the user to indicate the status of grounding
or barging-in to confirm an uncertain piece of infor-
mation. Once information has been presented to the
user, it is committed or realised. Realised informa-
tion is represented in the agent?s state, so that it can
monitor its own output.
Actions in a Hierarchical Reinforcement learner
can be either primitive or composite. The former
are single-step actions that yield single rewards, and
the latter are multi-step actions that correspond to
SMDPs and yield cumulative rewards. Decision
making occurs at any time step of an SMDP: after
each single-step action, we check for any updates
of the environment that require a system reaction or
change of strategy. If no system action is required
(e.g. because the user is speaking), the system can
decide to do nothing. The goal of each SMDP is to
find an optimal policy pi? that maximises the reward
for each visited state, according to
pi?ij(s) = argmaxa?A Q
?i
j(s, a), (2)
where Qij(s, a) specifies the expected cumulative re-
ward for executing action a in state s and then fol-
lowing pi?. We use HSMQ-Learning to induce dia-
logue policies, see (Cuaya?huitl, 2009), p. 92.
5 Experimental Setting
5.1 Hierarchy of Learning Agents
The HRL agent in Figure 3 shows how the tasks of
(1) dealing with incrementally changing input hy-
potheses, (2) choosing a suitable IP strategy and (3)
presenting information, are connected. Note that
we focus on a detailed description of models M10...3
here, which deal with barge-ins and backchannels
and are the core of this paper. Please see Dethlefs et
al. (2012) for details of an RL model that deals with
the remaining decisions.
Briefly, model M00 deals with dynamic input hy-
potheses. It chooses when to listen to an incoming
user utterance (M13 ) and when and how to present
information (M10...2) by calling and passing control
to a child subtask. The variable ?incrementalStatus?
characterises situations in which a particular (incre-
mental) action is triggered, such as a floor holder ?let
me see?, a correction or self-correction. The variable
?presStrategy? indicates whether a strategy for IP has
been chosen or not, and the variable ?userReaction?
shows the user?s reaction to an IP episode. The
?userSilence? variable indicates whether the user is
speaking or not. The detailed state and action space
of the agents is given in Figure 4. We distinguish ac-
tions for Information Presentation (IP), actions for
attribute presentation and ordering (Slot-ordering),
and incremental actions (Incremental).
Models M10...2 correspond to different ways of
presenting information to the user. They perform
attribute selection and ordering and then call the
child agents M20...4 for attribute realisation. When-
ever a user barges in over the system, these agents
will decide to either yield the turn to the user or to
try and keep the turn based on information density.
The variables representing the status of the cuisine,
86
Root
Summary Comparison Recommendation
Observe
 User
Present
Cuisine
Present
 Food
Present
Location
Present
  Price
Present
Service
Figure 3: Hierarchy of learning agent for incremental In-
formation Presentation and Slot Ordering.
food, location, price and service of restaurants indi-
cate whether the slot is of interest to the user (we as-
sume that 0 means that the user does not care about
this slot), and what input confidence score is cur-
rently associated with the value of the slot. For ex-
ample, if our current best hypothesis is that the user
is interested in Indian restaurants, the variable ?sta-
tusCuisine? will have a value between 1-3 indicating
the strength of this hypothesis. Once slots have been
presented to the user, they are realised and can only
be changed through a correction or self-correction.
Model M13 is called whenever the user is speak-
ing. The system?s main choice here is to remain
silent and listen to the user or barge-in to request
the desired cuisine, location, or price range of a
restaurant. This can be beneficial in certain situa-
tions, such as when the system is able to increase its
confidence for a slot from ?low? to ?high? through
barging-in with a direct clarification request, e.g.
?Did you say Indian?? (and thereby saving sev-
eral turns that may be based on a wrong hypoth-
esis). This can also be harmful in certain situa-
tions, though, assuming that users have a general
preference for not being barged-in on. The learning
agent will need to learn to distinguish these situa-
tions. This agent is also responsible for generating
backchannels and will over time learn the best mo-
ments to do this.
Models M20...4 choose surface forms for presenta-
tion to the user from hand-crafted templates. They
are not the focus of this paper, however, and there-
fore not presented in detail. The state-action space
size of this agent is roughly 1.5 million.4 The agent
4Note that a flat RL agent, in contrast, would need 8? 1025
million state-actions to represent this problem.
States M00
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
observeUser {0=unfilled,1=filled}
presStrategy {0=unfilled,1=filled}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions M00
IP: compare M11 , recommend M12 , summarise M10 , sum-
mariseCompare, summariseRecommend, summariseCompar-
eRecommend,
Incremental: correct, selfCorrect, holdFloor, observeUser
Goal State M00 0, 1, 1, 0, ?
States M10...2
IDSystem={0=low,1=medium, 2=high}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusQuality {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
turnType {0=holding, 1=resuming, 2=keeping, 3=yielding}
userBargeIn {0=false, 1=true}
Actions M10...2
Slot-ordering: presentCuisine M20 , presentQuality M21 ,
presentLocation M22 , presentPrice M23 , presentService M24 ,
Incremental: yieldTurn, keepTurn
Goal State M10...2 ?, ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, ?, ?
States M13
bargeInOnUser={0=undecided,1=yes, 2=no}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
Actions M13
Incremental: doNotBargeIn, bargeInCuisine, bargeInLocation,
bargeInPrice, backchannel
Goal State M13 >0, ?, 0 ? 4, 0 ? 4, 0 ? 4
States M20...4
IDSystem={0=low,1=medium, 2=high}
IDUser={0=low,1=medium, 2=high, 3=falling, 4=rising}
surfaceForm {0=unrealised,1=realised}
Actions M20...4
Surface Realisation: [alternative surface realisations]
e.g. ?$number$ restaurants serve $cuisine$ food?, ?$number$
places are located in $area$, etc.
Goal State M20...4 ?, ?, 1
Figure 4: The state and action space of the HRL agent.
The goal state is reached when all items (that the user
specified in the search query) have been presented. Ques-
tion marks mean that a variable does not affect the goal
state, which can be reached regardless of the variable?s
value.
reaches its goal state (defined w.r.t. the state vari-
87
ables in Fig. 4) when an IP strategy has been chosen
and all information has been presented.
5.2 The Simulated Environment
For a policy to converge, a learning agent typically
needs several thousand interactions in which it is ex-
posed to a multitude of different circumstances. For
our domain, we designed a simulated environment
with three main components addressing IP, incre-
mental input hypotheses and ID. Using this simula-
tion, we trained the agent for 10 thousand episodes,
where one episode corresponds to one recommenda-
tion dialogue.
5.2.1 Information Presentation
To learn a good IP strategy, we use a user simula-
tion5 by Rieser et al2010) which was estimated
from human data and uses bi-grams of the form
P (au,t|IPs,t), where au,t is the predicted user reac-
tion at time t to the system?s IP strategy IPs,t in state
s at time t. We distinguish the user reactions of se-
lect a restaurant, addMoreInfo to the current query to
constrain the search, and other. The last category is
usually considered an undesirable user reaction that
the system should learn to avoid. The simulation
uses linear smoothing to account for unseen situa-
tions. In this way, we can predict the most likely
user reaction to each system action. Even though
previous work has shown that n-gram-based simu-
lations can lead to dialogue inconsistencies, we as-
sume that for the present study this does not present
a problem, since we focus on generating single utter-
ances and on obtaining user judgements for single,
independent utterances.
5.2.2 Input Hypothesis Updates
While the IP strategies can be used for incremen-
tal and non-incremental dialogue, the second part of
the simulation deals explicitly with the dynamic en-
vironment updates that the system will need to be
sensitive to in an incremental setting. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores for each value are
5The simulation data are available from www.
classic-project.org.
shown in Table 2. A score of 0 means that the user
does not care about the attribute, 1 means that the
system?s confidence in the attribute?s value is low, 2
that the confidence is medium, and 3 means that the
confidence is high. A value of 4 means that the at-
tribute has already been realised, i.e. communicated
to the user. At the beginning of a learning episode,
we assign each attribute a possible value and con-
fidence score with equal probability. For food and
service quality, we assume that the user is never in-
terested in bad food or service. Subsequently, con-
fidence scores can change at each time step. In fu-
ture work these transition probabilities will be esti-
mated from a data collection, though the following
assumptions are realistic based on our experience.
We assume that a confidence score of 0 changes to
any other value with a likelihood of 0.05. A confi-
dence score of 1 changes with a probability of 0.3,
a confidence score of 2 with a probability of 0.1
and a confidence score of 3 with a probability of
0.03. Once slots have been realised, their value is
set to 4. They cannot be changed then without an ex-
plicit correction. We also assume that realised slots
change with a probability of 0.1. If they change,
we assume that half of the time, the user is the ori-
gin of the change (because they changed their mind)
and half of the time the system is the origin of the
change (because of an ASR or interpretation error).
Each time a confidence score is changed, it has a
probability of 0.5 for also changing its value. The
resulting input to the system are data structures of
the form present(cuisine=Indian), confidence=low.
The probability of observing this data structure in
our simulation is 0.1 (for Indian) ? 0.2 (for low
confidence) = 0.02. Its probability of changing
to present(cuisine=italian), confidence=high is 0.1
(for changing from low to medium) ? 0.05 (for
changing from Indian to Italian) = 0.005.
5.2.3 Information Density Updates
We simulate ID of user utterances based on proba-
bilistic context-free grammars (PCFG) that were au-
tomatically induced from the corpus data in Section
3.2 using the ABL algorithm (van Zaanen, 2000).
This algorithm takes a set of strings as input and
computes a context-free grammar as output by align-
ing strings based on Minimum Edit Distance. We
use the n?gram language models trained earlier to
88
Attribute Values Confidence
Cuisine Chinese, French, German, In-, 0, 1, 2, 3, 4
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Quality bad, adequate, good, very good 0, 1, 2, 3, 4
Location 7 distinct areas of the city 0, 1, 2, 3, 4
Price cheap, good-price-for-value,
expensive, very expensive 0, 1, 2, 3, 4
Service bad, adequate, good, very good 0, 1, 2, 3, 4
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
add probabilities to grammar rules. We use these
PCFGs to simulate user utterances to which the sys-
tem has to react. They can be meaningful utter-
ances such as ?Show me restaurants nearby? or less
meaningful fragments such as ?um let me see, do
you. . . hm?. The former type is more frequent in
the data, but both types can be simulated along with
their ID (clearly, the first type is more dense than the
second).
In addition to simulating user utterances, we
hand-crafted context-free grammars of system ut-
terances and augmented them with probabilities es-
timated using the same user corpus data as above
(where again, we make the assumption that this is
to some extent feasible given the shared domain).
We use the simulated system utterances to compute
varying degrees of ID for the system.
Both measures, the ID of user and system utter-
ances, can inform the system during learning to bal-
ance the trade-off between them for generating and
receiving backchannels and barge-ins.
5.3 A Reward Function for Incremental
Dialogue Based on Information Density
To train the HRL agent, we use a partially data-
driven reward function. For incremental IP, we use
rewards that are based on human intuition. The
agent receives
R =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+100 if the user selects an item,
0 if the user adds further con-
straints to the search,
-100 if the user does something else
or a self-correction,
-0.5 for the system holding a turn,
-1 otherwise.
The agent is encouraged to choose those sequences
of actions that lead to the user selecting a restaurant
as quickly as possible. If the agent is not sure what to
say (because planning has not finished), it can gen-
erate a floor holding marker, but should in any case
avoid a self-correction due to having started speak-
ing too early.
The remaining rewards are based on ID scores
computed incrementally during an interaction. The
agent receives the following rewards, where info-
Density(Usr) and infoDensity(Sys) refer to the ID of
the current user and system utterance, respectively,
as defined in Equation 1.
R =
?
?
?
?
?
?
?
-infoDensity(Usr) for keeping a turn,
barging-in or
a backchannel,
-infoDensity(Sys) for yielding a turn.
These two measures encourage the agent to consider
the trade-offs between its own ID and the one trans-
mitted by an incoming user utterance. Barging-in
on a user utterance at a low ID point then yields a
small negative reward, whereas barging-in on a user
utterance at a high ID point yields a high negative
reward. Both rewards are negative because barging-
in on the user always contains some risk. Similarly,
keeping a turn over a non-dense user utterance re-
ceives a smaller negative reward than keeping it over
a dense user utterance. A reward of ?2 is assigned
for barging-in over a user utterance fragment with a
falling ID to reflect results from a qualitative study
of our corpus data: humans tend to barge-in between
information peaks, so that a barge-in to clarify a low-
confidence slot appears immediately before the ID is
rising again for a new slot. The exact best moment
for barge-ins and backchannels to occur will be sub-
ject to optimisation.
89
6 Experimental Results
The agent learns to barge-in or generate backchan-
nels to users at points where the ID is low but rising.
In particular, the agent learns to barge-in right before
information density peaks in an incoming user utter-
ance to clarify or request slots that are still open from
the previous information density peak. If a user has
specified their desired cuisine type but the system
has received a low ASR confidence score for it, it
may barge-in to clarify the slot. This case was illus-
trated in the last example in Figure 1, where the sys-
tem clarified the previous (cuisine) slot (which is as-
sociated with a high ID) just before the user specifies
the location slot (which again would have a high ID).
The main benefit the system can gain through clar-
ification barge-ins is to avoid self-corrections when
having acted based on a low ASR confidence, lead-
ing to more efficient interactions.
The system learns to generate backchannels after
information peaks to confirm newly acquired slots
that have a high confidence. An example is shown
in the first dialogue fragment in Figure 1.
In addition, the system learns to yield its current
turn to a user that is barging-in if its own ID is low,
falling or rising, or if the ID of the incoming user
utterance is high. If the system?s own ID is high, but
the user?s is not, it will try to keep the turn.6 This is
exemplified in the third dialogue fragment in Figure
1.
We compare our learnt policy against two base-
lines. Baseline 1 was designed to always generate
barge-ins after an information peak in a user utter-
ance, i.e. when ID has just switched from high to
falling. We chose this baseline to confirm that users
indeed prefer barge-ins before information peaks
rather than at any point of low ID. Baseline 1 yields
a turn to a user barge-in if its own ID is low and tries
to keep it otherwise. Baseline 2 generates barge-ins
and backchannels randomly and at any point during
a user utterance. The decision of yielding or keeping
a turn in case of a user barge-in is also random. Both
baselines also use HRL to optimise their IP strategy.
We do not compare different IP strategies, which has
been done in detail by Rieser et al2010). All re-
6Incidentally, this also helps to prevent the system yielding
its turn to a user backchannel; cf. Example 2 in Fig. 1.
101 102 103 104
?120
?100
?80
?60
?40
?20
0
20
40
60
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
Learnt
Baseline1
Baseline2
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the HRL agent and its baselines.
sults are summarised in Table 3.
6.1 Average Rewards over Time
Figure 5 shows the performance of all systems in
terms of average rewards in simulation. The learnt
policy outperforms both baselines. While the learnt
policy and Baseline 1 appear to achieve similar per-
formance, an absolute comparison of the last 1000
episodes of each behaviour shows that the improve-
ment of the HRL agent over Baseline 1 corresponds
to 23.42%. The difference between the learnt policy
and its baselines is significant at p < 0.0001 accord-
ing to a paired t-test and has a high effect size of
r = 0.85.
The main reason for these different performances
is the moment each system will barge-in. Since
Baseline 1 barges-in on users after an information
peak, when ID may still be high, it continuously re-
ceives a negative reward reflecting the user prefer-
ence for late barge-ins. As a result of this contin-
uous negative reward, the agent will then learn to
avoid barge-ins altogether, which may in turn lead
to less efficient interactions because low confidence
ASR scores are clarified only late in the interaction.
The main problem of the random barge-ins of
Baseline 2 is that users may often have to restart
a turn because the system barged-in too early or
in the middle of an information peak. In addition,
Baseline 2 needs to occasionally self-correct its own
utterances because it started to present information
too early, when input hypotheses were not yet stable
enough to act upon them.
90
Policy Average Reward User Rating (%)
Learnt 55.54??,? 43%??
Baseline 1 45.0?? 26%
Baseline 2 1.47 31%
Table 3: Comparison of policies in terms of average re-
wards and user ratings. ? indicates a significant improve-
ment over Baseline 1 and ?? over Baseline 2.
6.2 Human Rating Study
To confirm our simulation-based results, we con-
ducted a user rating study on the CrowdFlower
crowd sourcing platform.7 Participants were
shown user utterances along with three options of
barging-in over them. For example: | I want
[OPTION 1] Italian food [OPTION 2] in the
city [OPTION 3] centre|, where OPTION 1 cor-
responds to the learnt policy, OPTION 2 to Baseline
2 and OPTION 3 to Baseline 1.
Users were asked to choose one option which they
considered the best moment for a barge-in. Partici-
pants in the study rated altogether 144 utterances.
They preferred the learnt system 63 times (43%),
Baseline 1 37 times (26%) and Baseline 2 44 times
(31%). This is statistically significant at p < 0.02
according to a Chi-Square test (?2 = 7.542, df =
2). In a separate test, directly comparing the learnt
policy and Baseline 1, learnt was chosen signifi-
cantly more often than Baseline 1; i.e. 79% of the
time (for 127 utterances, using a 1-tailed Sign test,
p < 0.0001). Finally, learnt was directly compared
to Baseline 2 and shown to be significantly more of-
ten chosen; i.e. 59% of the time (138 utterances, 1-
tailed Sign test, p < 0.025). These results provide
evidence that an optimisation of the timing of gener-
ating barge-ins and backchannels in incremental di-
alogue can be sensitive to fine-grained cues in evolv-
ing ID and therefore achieve a high level of adaptiv-
ity. Such sensitivity is difficult to hand-craft as can
be concluded w.r.t. the performance of Baseline 1,
which received similar rewards to learnt in simula-
tion, but is surprisingly beaten by the random Base-
line 2 here. This indicates a strong human dislike
for late barge-ins. The bad performance of Base-
line 2 in terms of average rewards was due to the
random barge-ins leading to less efficient dialogues.
7www.crowdflower.com
Regarding user ratings however, Baseline 2 was pre-
ferred over Baseline 1. This is most likely due to the
timing of barge-ins: since Baseline 2 has a chance
of barging-in at earlier occasions than Baseline 1,
it may have received better ratings. The evaluation
shows that humans care about timing of a barge-in
regarding the density of information that is currently
conveyed and dislike late barge-ins. ID is then useful
in determining when to barge-in. We can therefore
further conclude that ID can be a feasible optimisa-
tion criterion for incremental decision making.
7 Conclusion and Future Work
We have presented a novel approach to incremen-
tal dialogue decision making based on Hierarchical
RL combined with the notion of information den-
sity. We presented a learning agent in the domain of
IP for restaurant recommendations that was able to
generate backchannels and barge-ins for higher re-
sponsiveness in interaction. Results in terms of av-
erage rewards and a human rating study have shown
that a learning agent that is optimised based on a
partially data-driven reward function that addresses
information density can learn to decide when and if
it is beneficial to barge-in or backchannel on user
utterances and to deal with backchannels and barge-
ins from the user. Future work can take several di-
rections. Given that ID is a measure influencing
human language production, we could replace our
template-based surface realiser by an agent that op-
timises the information density of its output. Cur-
rently we learn the agent?s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situa-
tions could be achieved if the agent was able to learn
from ongoing interactions. Finally, we can confirm
the human results obtained from an overhearer-style
evaluation in a real interactive setting and explicitly
extend our language model to discourse phenomena
such as pauses or hesitations to take them into ac-
count in measuring ID.
Acknowledgements
The research leading to this work has received funding
from EC?s FP7 programmes: (FP7/2011-14) under grant
agreement no. 287615 (PARLANCE); (FP7/2007-13) un-
der grant agreement no. 216594 (CLASSiC); (FP7/2011-
14) under grant agreement no. 270019 (SPACEBOOK);
91
and (FP7/2011-16) under grant agreement no. 269427
(STAC). Many thanks to Michael White for discussion
of the original idea of using information density as an op-
timisation metric.
References
Matthew Aylett and Alice Turk. 2004. The smooth signal
redundancy hypothesis: A functional explanation for
the relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31?56.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Alan Bell, Dan Jurafsky, Eric Fossler-Lussier, Cynthia
Girand, Michelle Gregory, and Daniel Gildea. 2003.
Effects of disfluencies, predictability, and utterance
position on word form variation in english conver-
sation. Journal of the Acoustic Society of America,
113(2):1001?1024.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proceedings of 11th An-
nual SIGdial Meeting on Discourse and Dialogue.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011.
Spatially-aware Dialogue Control Using Hierarchi-
cal Reinforcement Learning. ACM Transactions on
Speech and Language Processing (Special Issue on
Machine Learning for Robust and Adaptive Spoken
Dialogue System), 7(3).
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2010. Evaluation of a hierar-
chical reinforcement learning spoken dialogue system.
Computer Speech and Language, 24(2):395?429.
Heriberto Cuaya?huitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. PhD Thesis,
University of Edinburgh, School of Informatics.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the International
Conference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Pro-
ceedings of the 10th Annual SigDial Meeting on Dis-
course and Dialogue, Queen Mary University, UK.
Thomas G. Dietterich. 1999. Hierarchical Reinforce-
ment Learning with the MAXQ Value Function De-
composition. Journal of Artificial Intelligence Re-
search, 13:227?303.
Dmitriy Genzel and Eugene Charniak. 2002. Entropy
Rate Constancy in Text. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 199?206.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2008. Hybrid Reinforcement/Supervised Learning of
Dialogue Policies from Fixed Data Sets. Computa-
tional Linguistics, 34(4):487?511.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage syntactic information density. Cog-
nitive Psychology, 61:23?62.
Srini Janarthanam and Oliver Lemon. 2010. Learning
to Adapt to Unknown Users: Referring Expression
Generation in Spoken Dialogue Systems. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 69?78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Oliver Lemon. 2011. Learning What to Say and How to
Say It: Joint Optimization of Spoken Dialogue Man-
agement and Natural Language Generation.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A Stochastic Model of Computer-Human Inter-
action for Learning Dialogue Strategies. IEEE Trans-
actions on Speech and Audio Processing, 8:11?23.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
Advances in Neural Information Processing Systems,
19.
Olivier Pietquin and Dutoit. 2006. A Probabilis-
tic Framework for Dialogue Simulation and Optimal
Strategy Learning. IEEE Transactions on Speech and
Audio Processing, 14(2):589?599.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Rajakrishnan Rajkumar and Michael White. 2011. Lin-
guistically Motivated Complementizer Choice in Sur-
face Realization. In Proceedings of the EMNLP-11
Workshop on Using Corpora in NLG, Edinburgh, Scot-
land.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proceedings of the 10th Conference of the
92
North American Chapter of the Association for Com-
putational Linguistics?Human Language Technolo-
gies (NAACL-HLT), Boulder, Colorado.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), Uppsala, Sweden.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
David Schlangen and Gabriel Skantze. 2011. A General,
Abstract Model of Incremental Dialogue Processing.
Dialogue and Discourse, 2(1).
Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-
son Williams. 2011. Stability and Accuracy in In-
cremental Speech Recognition. In Proceedings of the
12th Annual SigDial Meeting on Discourse and Dia-
logue, Portland, Oregon.
Claude Shannon. 1948. A Mathematical Theory of
Communications. Bell Systems Technical Journal,
27(4):623?656.
Satinder Singh, Diane Litman, Michael Kearns, and Mar-
ilyn Walker. 2002. Optimizing Dialogue Management
with Reinforcement Learning: Experiments with the
NJFun System. Journal of Artificial Intelligence Re-
search, 16:105?133.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proceedings of the 11th Annual SigDial Meeting on
Discourse and Dialogue, Tokyo, Japan.
Richard Sutton and Andrew Barto. 1998. Reinforcement
Learning: An Introduction. MIT Press, Cambridge,
MA.
Blaise Thomson. 2009. Statistical Methods for Spo-
ken Dialogue Management. Ph.D. thesis, University
of Cambridge.
Menno van Zaanen. 2000. Bootstrapping Syntax and
Recursion using Alignment-Based Learning. In Pro-
ceedings of the Seventeenth International Conference
on Machine Learning, ICML ?00, pages 1063?1070.
Marilyn Walker. 2000. An Application of Reinforcement
Learning to Dialogue Strategy Selection in a Spoken
Dialogue System for Email. Journal of Artificial In-
telligence Research (JAIR), 12:387?416.
Steve Young, Milica Gasic, Simon Keizer, Francois
Mairesse, Jost Schatzmann, Blaise Thomson, and Kai
Yu. 2010. The Hidden Information State Model: A
Practical Framework for POMDP-based Spoken Dia-
logue Management. Computer Speech and Language,
24(2):150?174.
Steve Young. 2000. Probabilistic Methods in Spoken
Dialogue Systems. Philosophical Transactions of the
Royal Society (Series A), 358(1769):1389?1402.
93
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 46?50,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
A Statistical Spoken Dialogue System using Complex User Goals and
Value Directed Compression
Paul A. Crook, Zhuoran Wang, Xingkun Liu and Oliver Lemon
Interaction Lab
School of Mathematical and Computer Sciences (MACS)
Heriot-Watt University, Edinburgh, UK
{p.a.crook, zhuoran.wang, x.liu, o.lemon}@hw.ac.uk
Abstract
This paper presents the first demonstration
of a statistical spoken dialogue system that
uses automatic belief compression to rea-
son over complex user goal sets. Reasoning
over the power set of possible user goals al-
lows complex sets of user goals to be rep-
resented, which leads to more natural dia-
logues. The use of the power set results in a
massive expansion in the number of belief
states maintained by the Partially Observ-
able Markov Decision Process (POMDP)
spoken dialogue manager. A modified form
of Value Directed Compression (VDC) is
applied to the POMDP belief states produc-
ing a near-lossless compression which re-
duces the number of bases required to rep-
resent the belief distribution.
1 Introduction
One of the main problems for a spoken dialogue
system (SDS) is to determine the user?s goal (e.g.
plan suitable meeting times or find a good Indian
restaurant nearby) under uncertainty, and thereby
to compute the optimal next system dialogue ac-
tion (e.g. offer a restaurant, ask for clarification).
Recent research in statistical SDSs has success-
fully addressed aspects of these problems through
the application of Partially Observable Markov
Decision Process (POMDP) approaches (Thom-
son and Young, 2010; Young et al 2010). How-
ever POMDP SDSs are currently limited by the
representation of user goals adopted to make sys-
tems computationally tractable.
Work in dialogue system evaluation, e.g.
Walker et al(2004) and Lemon et al(2006),
shows that real user goals are generally sets of
items, rather than a single item. People like to
explore possible trade offs between the attributes
of items.
Crook and Lemon (2010) identified this as a
central challenge for the field of spoken dialogue
systems, proposing the use of automatic compres-
sion techniques to allow for extended accurate
representations of user goals. This paper presents
a proof of concept of these ideas in the form of a
complete, working spoken dialogue system. The
POMDP dialogue manager (DM) of this demon-
stration system uses a compressed belief space
that was generated using a modified version of the
Value Directed Compression (VDC) algorithm
as originally proposed by Poupart (2005). This
demonstration system extends work presented by
Crook and Lemon (2011) in that it embeds the
compressed complex user goal belief space into a
working system and demonstrates planning (and
acting) in the compressed space.
2 Complex User Goals
The type of SDS task that we focus on is a limited-
domain query-dialogue, also known as a ?slot fill-
ing? task. The spoken dialogue system has knowl-
edge about some set of objects where these ob-
jects have attributes and these attributes can take
several values. An object can thus be described
by a conjunction of attribute-value pairs. A di-
alogue progresses with the system obtaining re-
quirements from the user which are specified in
terms of attribute values. The system should even-
tually present objects (search results) based upon
its understanding of the user?s requirement. The
dialogue ends when the user accepts one of the
domain objects.
Prior work on POMDP SDSs has assumed that
a user has a narrowly constrained goal (as speci-
46
User goal: cheap central Thai or expensive French
S: Hello, how can I help you?
U: I?m looking for a cheap central Thai restaurant.
S: You?re looking for budget, city centre, Thai
restaurants. ?
U: Or an expensive French restaurant anywhere.
S: You?re looking for non-budget French restaurants.
What ?
U: And cheap, city centre, Thai.
S: You?re looking for budget, city centre, Thai
restaurants. What about ?
U: (SIGH) And French.
S: You?re looking for budget, city centre, French
restaurants. What about ?
U: (HANG UP)
Table 1: Example dialogue using simple user goals.
S=System, U=User. ??? indicates the user barged in.
fied in terms of the domain objects) and thus the
role of the DM is one of reducing uncertainty un-
til its belief is strongly focused on a particular do-
main object. This has the unfortunate effect of
forcing users to select one domain object in order
to progress the dialogue, see Table 1. Note that the
example given is problematic not only because the
user wants two different food types but because
they have different requirements associated with
each, i.e. Thai restaurants should be in the centre
and cheap, while any French restaurants should be
expensive1 and can be located anywhere. To our
knowledge such a combination of goals with dif-
ferent attribute values cannot be straightforwardly
handled by comparable state-of-the-art statistical
SDSs which appear in the literature.
Crook and Lemon (2011) suggest that rather
than the DM assuming that the user has a single
narrowly constrained goal in mind, it should as-
sume that they want any possible sub-set of the
complete set of domain objects. Thus, instead
of maintaining the POMDP belief over individual
domain objects, it should be maintained over the
power set of domain objects. As an example see
Table 3 which is the power set for a domain with
three objects that can take two attributes u, v with
associated values u1, u2, u3 and v1 respectively.
The power set representation allows the
demonstration system to straightforwardly handle
previously problematic dialogues. See Table 2.
Of course this approach significantly expands the
1Interpreted as non-budget by the system since its
database only classifies restaurants as budget or non-budget.
User goal: cheap central Thai or expensive French
S: Hello, how can I help you?
U: I?m looking for a cheap central Thai restaurant.
S: You?re looking for budget, city centre, Thai
restaurants. ?
U: Or an expensive French restaurant anywhere.
S: You?re looking for budget, city centre, Thai
restaurants or non-budget French. What about
Chang Thai in Old Town?? Chang Thai?s set lunch
price is ? 7.95 ?
U: What alternatives do you have?
S: What about the Plumed Horse in Leith?? The
Plumed Horse is a Michelin starred French ...
Table 2: Example dialogue using complex user goals.
S=System, U=User. ?Old Town is part of the city cen-
tre. ?Leith is outside the centre.
state space of possible user goals, with the num-
ber of goal sets being equal to 2|domain objects| .
2.1 Automatic Compression
Even considering limited domains, POMDP state
spaces for SDSs grow very quickly. Thus the cur-
rent state-of-the-art in POMDP SDSs uses a vari-
ety of handcrafted compression techniques, such
as making several types of independence assump-
tion as discussed above.
Crook and Lemon (2010) propose replacing
handcrafted compressions with automatic com-
pression techniques. The idea is to use princi-
pled statistical methods for automatically reduc-
ing the dimensionality of belief spaces, but which
preserve useful distributions from the full space,
and thus can more accurately represent real user?s
goals.
2.2 VDC Algorithm
The VDC algorithm (Poupart, 2005) uses Krylov
iteration to compute a reduced state space. It finds
a set of linear basis vectors that can reproduce the
value2 of being in any of the original POMDP
states. Where, if a lossless VDC compression is
possible, the number of basis vectors is less than
the original number of POMDP states.
The intuition here is that if the value of taking
an action in a given state has been preserved then
planning is equally as reliable in the compressed
space as the in full space.
The VDC algorithm requires a fully specified
POMDP, i.e. ?S,A,O, T,?,R? where S is the set
2The sum of discounted future rewards obtained through
following some series of actions.
47
state goal set meaning: user?s goal is
s1 ? (empty set) none of the domain objects
s2 u=u1?v=v1 domain object 1
s3 u=u2?v=v1 domain object 2
s4 u=u3?v=v1 domain object 3
s5 (u=u1?v=v1) ? (u=u2?v=v1) domain objects 1 or 2
s6 (u=u1?v=v1) ? (u=u3?v=v1) domain objects 1 or 3
s7 (u=u2?v=v1) ? (u=u3?v=v1) domain objects 2 or 3
s8 (u=u1?v=v1) ? (u=u2?v=v1) ? (u=u3?v=v1) any of the domain objects
Table 3: Example of complex user goal sets.
of states, A is the set of actions, O is the set of ob-
servations, T conditional transition probabilities,
? conditional observation probabilities, and R is
the reward function. Since it iteratively projects
the rewards associated with each state and action
using the state transition and observation proba-
bilities, the compression found is dependent on
structures and regularities in the POMDP model.
The set of basis vectors found can be used to
project the POMDP reward, transition, and obser-
vation probabilities into the reduced state space
allowing the policy to be learnt and executed in
this state space.
Although the VDC algorithm (Poupart, 2005)
produces compressions that are lossless in terms
of the states? values, the set of basis vectors found
(when viewed as a transformation matrix) can be
ill-conditioned. This results in numerical instabil-
ity and errors in the belief estimation. The com-
pression used in this demonstration was produced
using a modified VDC algorithm that improves
the matrix condition by approximately selecting
the most independent basis vectors, thus improv-
ing numerical stability. It achieves near-lossless
state value compression while allowing belief es-
timation errors to be minimised and traded-off
against the amount of compression. Details of this
algorithm are to appear in a forthcoming publica-
tion.
3 System Description
3.1 Components
Input and output to the demonstration system is
using standard open source and commercial com-
ponents. FreeSWITCH (Minessale II, 2012) pro-
vides a platform for accepting incoming Voice
over IP calls, routing them (using the Media Re-
source Control Protocol (MRCP)) to a Nuance 9.0
Automatic Speech Recogniser (Nuance, 2012).
Output is similarly handled by FreeSWITCH
routing system responses via a CereProc Text-to-
Speech MRCP server (CereProc, 2012) in order
to respond to the user.
The heart of the demonstration system consists
of a State-Estimator server which estimates the
current dialogue state using the compressed state
space previously produced by VDC, a Policy-
Executor server that selects actions based on
the compressed estimated state, and a template
based Natural Language Generator server. These
servers, along with FreeSWITCH, use ZeroC?s
Internet Communications Engine (Ice) middle-
ware (ZeroC, 2012) as a common communica-
tions platform.
3.2 SDS Domain
The demonstration system provides a restaurant
finder system for the city of Edinburgh (Scot-
land, UK). It presents search results from a real
database of over 600 restaurants. The search
results are based on the attributes specified by
the user, currently; location, food type and
budget/non-budget.
3.3 Interface
The demonstration SDS is typically accessed over
the phone network. For debugging and demon-
stration purposes it is possible to visualise the
belief distribution maintained by the DM as dia-
logues progress. The compressed version of the
belief distribution is not a conventional proba-
bility distribution3 and its visualisation is unin-
formative. Instead we take advantage of the re-
versibility of the VDC compression and project
the distribution back onto the full state space. For
an example of the evolution of the belief distribu-
tion during a dialogue see Figure 1.
3The values associated with the basis vectors are not con-
fined to the range [0? 1].
48
#4096
10?7 10?6 10?5 0.0001 0.001
(a) Initial uniform distribution over the power set.
#2048
#2048
10?7 10?6 10?5 0.0001 0.001
(b) Distribution after user responds to greet.
#512
#3584
10?11 10?9 10?7 10?5 0.001
(c) Distribution after second user utterance.
Figure 1: Evolution of the belief distribution for the
example dialogue in Table 2. The horizontal length of
each bar corresponds to the probability of that com-
plex user goal state. Note that the x-axis uses a log-
arithmic scale to allow low probability values to be
seen. The y-axis is the set of complex user goals or-
dered by probability. Lighter shaded (green) bars indi-
cate complex user goal states corresponding to ?cheap,
central Thai? and ?cheap, central Thai or expensive
French anywhere? in figures (b) and (c) respectively.
The count ?#? indicates the number of states in those
groups.
4 Conclusions
We present a demonstration of a statistical SDS
that uses automatic belief compression to reason
over complex user goal sets. Using the power set
of domain objects as the states of the POMDP
DM allows complex sets of user goals to be rep-
resented, which leads to more natural dialogues.
To address the massive expansion in the number
of belief states, a modified form of VDC is used
to generate a compression. It is this compressed
space which is used by the DM for planning and
acting in response to user utterances. This is the
first demonstration of a statistical SDS that uses
automatic belief compression to reason over com-
plex user goal sets.
VDC and other automated compression tech-
niques reduce the human design load by automat-
ing part of the current POMDP SDS design pro-
cess. This reduces the knowledge required when
building such statistical systems and should make
them easier for industry to deploy.
Such compression approaches are not only ap-
plicable to SDSs but should be equally relevant
for multi-modal interaction systems where sev-
eral modalities are being combined in user-goal
or state estimation.
5 Future Work
The current demonstration system is a proof
of concept and is limited to a small number
of attributes and attribute-values. Part of our
ongoing work involves investigation of scaling.
For example, increasing the number of attribute-
values should produce more regularities across
the POMDP space. Does VDC successfully ex-
ploit these?
We are in the process of collecting corpora
for the Edinburgh restaurant domain mentioned
above with the aim that the POMDP observation
and transition statistics can be derived from data.
As part of this work we have launched a long
term, public facing outlet for testing and data col-
lection, see http:\\www.edinburghinfo.
co.uk. It is planned to make future versions of
the demonstration system discussed in this paper
available via this public outlet.
Finally we are investigating the applicability
of other automatic belief (and state) compression
techniques for SDSs, e.g. E-PCA (Roy and Gor-
don, 2002).
49
Acknowledgments
The research leading to these results was funded
by the Engineering and Physical Sciences Re-
search Council, UK (EPSRC) under project no.
EP/G069840/1 and was partially supported by the
EC FP7 projects Spacebook (ref. 270019) and
JAMES (ref. 270435).
References
CereProc. 2012. http://www.cereproc.com/.
Paul A. Crook and Oliver Lemon. 2010. Representing
uncertainty about complex user goals in statistical
dialogue systems. In proceedings of SIGdial.
Paul A. Crook and Oliver Lemon. 2011. Lossless
Value Directed Compression of Complex User Goal
States for Statistical Spoken Dialogue Systems. In
Proceedings of the Twelfth Annual Conference of
the International Speech Communication Associa-
tion (Interspeech).
Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating Effectiveness and Portabil-
ity of Reinforcement Learned Dialogue Strategies
with real users: the TALK TownInfo Evaluation. In
IEEE/ACL Spoken Language Technology.
Anthony Minessale II. 2012. FreeSWITCH. http:
//www.freeswitch.org/.
Nuance. 2012. Nuance Recognizer. http://www.
nuance.com.
P. Poupart. 2005. Exploiting Structure to Efficiently
Solve Large Scale Partially Observable Markov De-
cision Processes. Ph.D. thesis, Dept. Computer Sci-
ence, University of Toronto.
N. Roy and G. Gordon. 2002. Exponential Family
PCA for Belief Compression in POMDPs. In NIPS.
B. Thomson and S. Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
Marilyn Walker, S. Whittaker, A. Stent, P. Maloor,
J. Moore, M. Johnston, and G. Vasireddy. 2004.
User tailored generation in the match multimodal
dialogue system. Cognitive Science, 28:811?840.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2010. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language, 24(2):150?174.
ZeroC. 2012. The Internet Communications Engine
(Ice). http://www.zeroc.com/ice.html.
50
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 702?711,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Cluster-based Prediction of User Ratings for Stylistic Surface Realisation
Nina Dethlefs, Heriberto Cuaya?huitl, Helen Hastie, Verena Rieser and Oliver Lemon
Heriot-Watt University, Mathematical and Computer Sciences, Edinburgh
n.s.dethlefs@hw.ac.uk
Abstract
Surface realisations typically depend on
their target style and audience. A challenge
in estimating a stylistic realiser from data is
that humans vary significantly in their sub-
jective perceptions of linguistic forms and
styles, leading to almost no correlation be-
tween ratings of the same utterance. We ad-
dress this problem in two steps. First, we
estimate a mapping function between the
linguistic features of a corpus of utterances
and their human style ratings. Users are
partitioned into clusters based on the sim-
ilarity of their ratings, so that ratings for
new utterances can be estimated, even for
new, unknown users. In a second step, the
estimated model is used to re-rank the out-
puts of a number of surface realisers to pro-
duce stylistically adaptive output. Results
confirm that the generated styles are recog-
nisable to human judges and that predictive
models based on clusters of users lead to
better rating predictions than models based
on an average population of users.
1 Introduction
Stylistic surface realisation aims not only to find
the best realisation candidate for a semantic input
based on some underlying trained model, but also
aims to adapt its output to properties of the user,
such as their age, social group, or location, among
others. One of the first systems to address stylis-
tic variation in generation was Hovy (1988)?s
PAULINE, which generated texts that reflect dif-
ferent speaker attitudes towards events based on
multiple, adjustable features. Stylistic variation
in such contexts can often be modelled systemat-
ically as a multidimensional variation space with
several continuous dimensions, so that varying
stylistic scores indicate the strength of each di-
mension in a realisation candidate. Here, we fo-
cus on the dimensions of colloquialism, politeness
and naturalness. Assuming a target score on one
or more dimensions, candidate outputs of a data-
driven realiser can then be ranked according to
their predicted affinity with the target scores.
In this paper, we aim for an approach to stylis-
tic surface realisation which is on the one hand
based on natural human data so as to reflect stylis-
tic variation that is as natural as possible. On the
other hand, we aim to minimise the amount of
annotation and human engineering that informs
the design of the system. To this end, we esti-
mate a mapping function between automatically
identifiable shallow linguistic features character-
istic of an utterance and its human-assigned style
ratings. In addition, we aim to address the high
degree of variability that is often encountered in
subjective rating studies, such as assessments of
recommender systems (O?Mahony et al., 2006;
Amatriain et al., 2009), sentiment analysis (Pang
and Lee, 2005), or surface realisations, where user
ratings have been shown to differ significantly
(p<0.001) for the same utterance (Walker et al.,
2007). Such high variability can affect the per-
formance of systems which are trained from an
average population of user ratings. However, we
are not aware of any work that has addressed this
problem principally by estimating ratings for both
known users, for whom ratings exists, and un-
known users, for whom no prior ratings exist. To
achieve this, we propose to partition users into
clusters of individuals who assign similar ratings
to linguistically similar utterances, so that their
ratings can be estimated more accurately than
702
based on an average population of users. This is
similar to Janarthanam and Lemon (2014), who
show that clustering users and adapting to their
level of domain expertise can significantly im-
prove task success and user ratings. Our resulting
model is evaluated with realisers not originally
built to deal with stylistic variation, and produces
natural variation recognisable by humans.
2 Architecture and Domain
We aim to with generating restaurant recommen-
dations as part of an interactive system. To do
this, we assume that a generator input is provided
by a preceding module, e.g. the interaction man-
ager, and that the task of the surface realiser is
to find a suitable stylistically appropriate realisa-
tion. An example input is inform(food=Italian,
name=Roma), which could be expressed as The
restaurant Roma serves Italian food. A further
aspect is that users are initially unknown to the
system, but that it should adapt to them over time
by discovering their stylistic preferences. Fu-
ture work involves integrating the surface realiser
into the PARLANCE1 (Hastie et al., 2013) spo-
ken dialogue system with a method for triggering
the different styles. Here, we leave the question
of when different styles are appropriate as future
work and focus on being able to generate them.
The architecture of our model is shown in Fig-
ure 1. Training of the regression model from sty-
listically-rated human corpora is shown in the top-
left box (grey). Utterance ratings from human
judges are used to extract shallow linguistic fea-
tures as well as to estimate user clusters. Both
types of information inform the resulting stylis-
tic regression model. For surface realisation (top-
right box, blue), a semantic input from a preced-
ing model is given as input to a surface realiser.
Any realiser is suitable that returns a ranked list of
output candidates. The resulting list is re-ranked
according to stylistic scores estimated by the re-
gressor, so that the utterance which most closely
reflects the target score is ranked highest. The re-
ranking process is shown in the lower box (red).
3 Related Work
3.1 Stylistic Variation in Surface Realisation
Our approach is most closely related to work by
Paiva and Evans (2005) and Mairesse and Walker
1http://parlance-project.eu
User Clusters
Regressor Surface Realisation
Ranking + 
Evaluation
Figure 1: Architecture of stylistic realisation model.
Top left: user clusters are estimated from corpus ut-
terances described by linguistic features and ratings.
Top right: surface realisation ranks a list of output can-
didates based on a semantic input. These are ranked
stylistically given a trained regressor.
(2011), discussed in turn here. Paiva and Evans
(2005) present an approach that uses multivari-
ate linear regression to map individual linguistic
features to distinguishable styles of text. The ap-
proach works in three steps. First, a factor anal-
ysis is used to determine the relevant stylistic di-
mensions from a corpus of human text using shal-
low linguistic features. Second, a hand-crafted
generator is used to produce a large set of ut-
terances, keeping traces of each generator deci-
sion, and obtaining style scores for each output
based on the estimated factor model. The result
is a dataset of <generator decision, style score>
pairs which can be used in a correlation analy-
sis to identify the predictors of particular output
styles. During generation, the correlation equa-
tions inform the generator at each choice point so
as to best express the desired style. Unfortunately,
no human evaluation of the model is presented so
that it remains unclear to what extent the gener-
ated styles are perceivable by humans.
Closely related is work by Mairesse and Walker
(2011) who present the PERSONAGE system,
which aims to generate language reflecting par-
ticular personalities. Instead of choosing genera-
tor decisions by considering their predicted style
scores, however, Mairesse and Walker (2011) di-
rectly predict generator decisions based on tar-
get personality scores. To obtain the generator,
the authors first generate a corpus of utterances
which differ randomly in their linguistic choices.
All utterances are rated by humans indicating the
703
extent to which they reflect different personality
traits. The best predictive model is then chosen in
a comparison of several classifiers and regressors.
Mairesse and Walker (2011) are the first to evalu-
ate their generator with humans and show that the
generated personalities are indeed recognisable.
Approaches on replicating personalities in re-
alisations include Gill and Oberlander (2002) and
Isard et al. (2006). Porayska-Pomsta and Mellish
(2004) and Gupta et al. (2007) are approaches to
politeness in generation, based on the notion of
face and politeness theory, respectively.
3.2 User Preferences in Surface Realisation
Taking users? individual content preferences into
account for training generation systems can
positively affect their performance (Jordan and
Walker, 2005; Dale and Viethen, 2009). We are
interested in individual user perceptions concern-
ing the surface realisation of system output and
the way they relate to different stylistic dimen-
sions. Walker et al. (2007) were the first to show
that individual preferences exist for the perceived
quality of realisations and that these can be mod-
elled in trainable generation. They train two ver-
sions of a rank-and-boost generator, a first version
of which is trained on the average population of
user ratings, whereas a second one is trained on
the ratings of individual users. The authors show
statistically that ratings from different users are
drawn from different distributions (p<0.001) and
that significantly better performance is achieved
when training and testing on data of individual
users. In fact, training a model on one user?s rat-
ings and testing it on another?s performs as badly
as a random baseline. However, no previous work
has modelled the individual preferences of unseen
users?for whom no training data exists.
4 Estimation of Style Prediction Models
4.1 Corpora and Style Dimensions
Our domain of interest is the automatic generation
of restaurant recommendations that differ with re-
spect to their colloquialism and politeness and are
as natural as possible. All three stylistic dimen-
sion were identified from a qualitative analysis of
human domain data. To estimate the strength of
each of them in a single utterance, we collect user
ratings for three data sets that were collected un-
der different conditions and are freely available.
Corpus Colloquial Natural Polite
LIST 3.38 ? 1.5 4.06 ? 1.2 4.35 ? 0.8
MAI 3.95 ? 1.2 4.32 ? 1.0 4.27 ? 0.8
CLASSIC 4.29 ? 1.1 4.20 ? 1.2 3.64 ? 1.3
Table 1: Average ratings with standard deviations.
Ratings between datasets (except one) differ signifi-
cantly at p<0.01, using the Wilcoxon signed-rank test.
? LIST is a corpus of restaurant recommenda-
tions from the website The List.2 It consists
of professionally written reviews. An exam-
ple is ?Located in the heart of Barnwell, Bel-
uga is an excellent restaurant with a smart
menu of modern Italian cuisine.?
? MAI is a dataset collected by Mairesse et
al. (2010),3 using Amazon Mechanical Turk.
Turkers typed in recommendations for vari-
ous specified semantics; e.g. ?I recommend
the restaurant Beluga near the cathedral.?
? CLASSIC is a dataset of transcribed spoken
user utterances from the CLASSiC project.4
The utterances consist of user queries for
restaurants, such as ?I need an Italian
restaurant with a moderate price range.?
Our joint dataset consists of 1, 361 human ut-
terances, 450 from the LIST, 334 from MAI,
and 577 from CLASSIC. We asked users on the
CrowdFlower crowdsourcing platform5 to read
utterances and rate their colloquialism, politeness
and naturalness on a 1-5 scale (the higher the bet-
ter). The following questions were asked.
? Colloquialism: The utterance is colloquial,
i.e. could have been spoken.
? Politeness: The utterance is polite / friendly.
? Naturalness: The utterance is natural, i.e.
could have been produced by a human.
The question on naturalness can be seen as a gen-
eral quality check for our training set. We do
not aim to generate unnatural utterances. 167
users took part in our rating study leading to a
rated dataset of altogether 3, 849 utterances. All
users were from the USA. The average ratings per
dataset and stylistic dimension are summarised
in Table 1. From this, we can see that LIST ut-
terances were perceived as the least natural and
2http://www.list.co.uk/
3http://people.csail.mit.edu/francois/
research/bagel/
4http://www.classic-project.org/
5http://crowdflower.com/
704
colloquial, but as the most polite. CLASSIC ut-
terances were perceived as the most colloquial,
but the least polite, and MAI utterances were rated
as the most natural. Differences between ratings
for each dimension and dataset are significant at
p<0.01, using the Wilcoxon signed-rank test, ex-
cept the naturalness for MAI and CLASSIC.
Since we are mainly interested in the lexical
and syntactic features of utterances here, the fact
that CLASSIC utterances are spoken, whereas the
other two corpora are written, should not affect
the quality of the resulting model. Similarly, some
stylistic categories may seem closely related, such
as colloquialism and naturalness, or orthogonal
to each other, such as politeness and colloqui-
alism. However, while ratings for colloquialism
and naturalness are very close for the CLASSIC
dataset, they vary significantly for the two other
datasets (p<0.01). Also, the ratings for colloqui-
alim and politeness show a weak positive corre-
lation of 0.23, i.e. are not perceived as orthogo-
nal by users. These results suggest that all in all
our three stylistic categories are perceived as suf-
ficiently different from each other and suitable for
training to predict a spectrum of different styles.
Another interesting aspect is that individual
user ratings vary significantly, leading to a high
degree of variability for identical utterances. This
will be the focus of the following sections.
4.2 Feature Estimation
Table 2 shows the feature set we will use in our
regression experiments. We started from a larger
subset including 45 lexical and syntactic features
as well as unigrams and bigrams, all of which
could be identified from the corpus without man-
ual annotation. The only analysis tool we used
was the Stanford Parser,6 which identified certain
types of words (pronouns, wh-words) or the depth
of syntactic embedding. A step-wise regression
analysis was then carried out to identify those
features that contributed significantly (at p<0.01)
to the overall regression equation obtained per
stylistic dimension. Of all lexical features (uni-
grams and bigrams), the word with was the only
contributor. A related feature was the average tf-
idf score of the content words in an utterance.
6http://nlp.stanford.edu/software/
lex-parser.shtml
Feature Type
Length of utterance num
Presence of personal pronouns bool
Presence of WH words bool
with cue word bool
Presence of negation bool
Average length of content words num
Ave tf-idf score of content words num
Depth of syntactic embedding num
Table 2: Features used for regression, which were
identified as significant contributors (p<0.01) from a
larger feature set in a step-wise regression analysis.
4.3 Regression Experiments
Based on the features identified in Section 4.2, we
train a separate regressor for each stylistic dimen-
sion. The task of the regressor is to predict, based
on the extracted linguistic features of an utterance,
a score in the range of 1-5 for colloquialism, po-
liteness and naturalness. We compare: (1) a mul-
tivariate multiple regressor (MMR), (2) an M5P
decision tree regressor, (3) a support vector ma-
chine (SVM) with linear kernel, and (4) a ZeroR
classifier, which serves as a majority baseline. We
used the R statistics toolkit7 for the MMR and the
Weka toolkit8 for the remaining models.
Average User Ratings The regressors were first
trained to predict the average user ratings of an ut-
terance and evaluated in a 10-fold cross validation
experiment. Table 3 shows the results. Here, r
denotes the Pearson correlation coefficient, which
indicates the correlation between the predicted
and the actual user scores; R2 is the coefficient of
determination, which provides a measure of how
well the learnt model fits the data; and RMSE
refers to the Root Mean Squared Error, the error
between the predicted and actual user ratings.
We can observe that MMR achieves the best
performance for predicting colloquialism and nat-
uralness, whereas M5P best predicts politeness.
Unfortunately, all regressors achieve at best a
moderate correlation with human ratings. Based
on these results, we ran a correlation analysis for
all utterances for which more than 20 original
user ratings were available. The purpose was to
find out to what extent human raters agree with
each other. The results showed that user agree-
ment in fact ranges from a high positive corre-
7http://www.r-project.org/
8http://www.cs.waikato.ac.nz/ml/weka/
705
Model r R2 RMSE
Colloquial
MMR 0.50 0.25 0.85
SVM 0.47 0.22 0.86
M5P 0.48 0.23 0.85
ZeroR -0.08 0.006 0.97
Natural
MMR 0.30 0.09 0.78
SVM 0.24 0.06 0.81
M5P 0.27 0.07 0.78
ZeroR -0.09 0.008 0.81
Polite
MMR 0.33 0.11 0.71
SVM 0.31 0.09 0.73
M5P 0.42 0.18 0.69
ZeroR -0.09 0.008 0.76
Table 3: Comparison of regression models per dimen-
sion using average user ratings. The best model is
indicated in bold-face for the correlation coefficient.
Model r R2 RMSE
Colloquial
MMR 0.61 0.37 1.05
SVM 0.36 0.13 1.3
M5P 0.56 0.31 1.07
ZeroR -0.06 0.004 1.3
Natural
MMR 0.55 0.30 0.96
SVM 0.36 0.13 1.13
M5P 0.49 0.24 0.99
ZeroR -0.08 0.06 1.13
Polite
MMR 0.69 0.48 0.76
SVM 0.54 0.30 0.92
M5P 0.71 0.50 0.73
ZeroR -0.04 0.002 1.04
Table 4: Comparison of regression models per dimen-
sion using individual user ratings. The best model is
indicated in bold-face for the correlation coefficient.
lation of 0.79 to a moderate negative correlation
of ?0.55. The average is 0.04 (SD=0.95), i.e.
indicating no correlation between user ratings,
even for the same utterance. This observation is
partially in line with related work that has found
high diversity in subjective user ratings. Yeh and
Mellish (1997) report only 70% agreement of hu-
man judges on the best choice of referring ex-
pression. Amatriain et al. (2009) report incon-
sistencies in user ratings in recommender systems
with an RMSE range of 0.55 to 0.81 and argue
that this constitutes a lower bound for system per-
formance. This inconsistency is exacerbated by
raters recruited via crowdsourcing platforms as
in our study (Koller et al., 2010; Rieser et al.,
2011). However, while crowdsourced data have
been shown to contain substantially more noise
than data collected in a lab environment, they do
tend to reflect the general tendency of their more
controlled counterparts (Gosling et al., 2004).
Individual User Ratings Given that individual
preferences exist for surface realisation (Walker
et al., 2007), we included the user?s ID as a re-
gression feature and re-ran the experiments. The
hypothesis was that if users differ in their pref-
erences for realisation candidates, they may also
differ in terms of their perceptions of linguistic
styles. The results shown in Table 4 support this:
the obtained correlations are significantly higher
(p<0.001, using the Fisher r-to-z transformation)
than those without the user?s ID (though we are
still not able to model the full variation observed
in ratings). Importantly, this shows that user rat-
ings are intrinsically coherent (not random) and
that variation exists mainly for inter-user agree-
ment. This model performs satisfactorily for a
known population of users. However, it does not
allow the prediction of ratings of unknown users,
who we mostly encounter in generation.
5 Clustering User Rating Behaviour
5.1 Spectral Clustering
The goal of this section is to find a number of k
clusters which partition our data set of user rat-
ings in a way that users in one cluster rate ut-
terances with particular linguistic properties most
similarly to each other, while rating them most
dissimilarly to users in other clusters. We as-
sume a set of n data points x
1
. . . x
n
, which
in our case correspond to an individual user or
group of users, characterised in terms of word
bigrams, POS tag bigrams, and assigned rat-
ings of the utterance they rated. An example
is Beluga NNP serves VBZ Italian JJ food NN;
[col=5.0, nat=5.0, pol=4.0]. Features were cho-
sen as a subset of relevant features from the larger
set used for regression above.
Using spectral clustering (von Luxburg, 2007),
clusters can be identified from a set of eigenvec-
tors of an affinity matrix S derived from pair-wise
similarities between data points s
ij
= s(x
i
, x
j
)
using a symmetric and non-negative similarity
function. To do that, we use a cumulative simi-
larity based on the Kullback-Leibler divergence,
D(P,Q) =
?
i
p
i
log
2
(
p
i
q
i
) +
?
j
q
j
log
2
(
q
j
p
j
)
2
,
where P is a distribution of words, POS tags or
ratings in data point x
i
; and Q a similar distribu-
tion in data point x
j
. The lower the cumulative di-
706
0.
3
0.
4
0.
5
0.
6
Number of Clusters
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
1 3 5 7 9 20 40 60 80 100 167
Individual
Clusters
Average
Figure 2: Average correlation coefficient for different
numbers of clusters. For comparison, results from av-
erage and individual user ratings are also shown.
vergence between two data sets, the more similar
they are. To find clusters of similar users from the
affinity matrix S, we use the algorithm described
in Ng et al. (2001). It derives clusters by choosing
the k largest eigenvectors u
1
, u
2
, . . . , u
k
from the
Laplacian matrix L = D1/2?SD1/2 (where D is
a diagonal matrix), arranging them into columns
in a matrix U = [u
1
u
2
. . . u
k
] and then normalis-
ing them for length. The result is a new matrix T ,
obtained through t
ij
= u
ij
/(
?
k
u
2
ik
)
1/2
. The set
of clusters C
1
, . . . C
k
can then be obtained from T
using the K-means algorithm, where each row in
T serves as an individual data point. Finally, each
original data point x
i
(row i of T ) is assigned to a
cluster C
j
. In comparison to other clustering algo-
rithms, experiments by Ng et al. (2001) show that
spectral clustering is robust for convex and non-
convex data sets. The authors also demonstrate
why using K-means only is often not sufficient.
The main clusters obtained describe surface
realisation preferences by particular groups of
users. An example is the realisation of the loca-
tion of a restaurant as a prepositional phrase or as
a relative clause as in restaurant in the city centre
vs. restaurant located in the city centre; or the re-
alisation of the food type as an adjective, an Ital-
ian restaurant, vs. a clause, this restaurant serves
Italian food. Clusters can then be characterised as
different combinations of such preferences.
5.2 Results: Predicting Stylistic Ratings
Figure 2 shows the average correlation coefficient
r across dimensions in relation to the number
of clusters, in comparison to the results obtained
with average and individual user ratings. We can
see that the baseline without user information is
outperformed with as few as three clusters. From
30 clusters on, a medium correlation is obtained
until another performance jump occurs around 90
clusters. Evidently, the best performance would
be achieved by obtaining one cluster per user, i.e.
167 clusters, but nothing would be gained in this
way, and we can see that useful generalisations
can be made from much fewer clusters. Based on
the clusters found, we will now predict the ratings
of known and unknown users.
Known Users For known users, first of all, Fig-
ure 3 shows the correlations between the predicted
and actual ratings for colloquialism, politeness
and naturalness based on 90 user clusters. Cor-
relation coefficients were obtained using an MMR
regressor. We can see that a medium correlation is
achieved for naturalness and (nearly) strong cor-
relations are achieved for politeness and colloqui-
alism. This confirms that clustering users can help
to better predict their ratings than based on shal-
low linguistic features alone, but that more gener-
alisation is achieved than based on individual user
ratings that include the user?s ID as a regression
feature. The performance gain in comparison to
predicting average ratings is significant (p<0.01)
from as few as three clusters onwards.
Unknown Users We initially sort unknown
users into the majority cluster and then aim to
make more accurate cluster allocations as more
information becomes available. For example, af-
ter a user has assigned their first rating, we can
take it into account to re-estimate their cluster
more accurately. Clusters are re-estimated with
each new rating, based on our trained regression
model. While estimating a user cluster based on
linguistic features alone yields an average corre-
lation of 0.38, an estimation based on linguistic
features and a single rating alone already yields an
average correlation of 0.45. From around 30 rat-
ings, the average correlation coefficients achieved
are as good as for known users. More importantly,
though, estimations based on a single rating alone
significantly outperform ratings based on the av-
707
(a)
1 2 3 4 5
2
3
4
5
Correlation: Colloquialism
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
(b)
1 2 3 4 5
1
2
3
4
5
Correlation: Naturalness
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
(c)
1 2 3 4 5
1
2
3
4
5
Correlation: Politeness
Actual Ratings
Pr
ed
ict
ed
 R
at
in
gs
Figure 3: Correlations per dimension between actual and predicted user ratings based on 90 user clusters: (a)
Colloquialism (r = 0.57, p<0.001), (b) Naturalness (r = 0.49, p<0.001) and (c) Politeness (r = 0.59, p<0.001).
erage population of users (p<0.001). Fig. 4 shows
this process. It shows the correlation between pre-
dicted and actual user ratings for unknown users
over time. This is useful in interactive scenarios,
where system behaviour is refined as more infor-
mation becomes available (Cuaya?huitl and Deth-
lefs, 2011; Gas?ic? et al., 2011), or for incremental
systems (Skantze and Hjalmarsson, 2010; Deth-
lefs et al., 2012b; Dethlefs et al., 2012a).
0.
3
0.
4
0.
5
0.
6
Number of Ratings
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
1 2 3 4 5 6 7 8 9 10 15 20 30
90 Clusters
Ratings
Average
Figure 4: Average correlation coefficient for unknown
users with an increasing number of ratings. Results
from 90 clusters and average ratings are also shown.
6 Evaluation: Stylistically-Aware
Surface Realisation
To evaluate the applicability of our regression
model for stylistically-adaptive surface realisa-
tion, this section describes work that compares
four different surface realisers, which were not
originally developed to produce stylistic variation.
To do that, we first obtain the cluster for each in-
put sentence s: c? = argmin
c?C
?
x
D(P
x
s
|Q
x
c
),
where x refers to n-grams, POS tags or ratings
(see Section 5.1); P refers to a discrete probability
distribution of sentence s; and Q refers to a dis-
crete probability distribution of cluster c. The best
cluster is used to compute the style score of sen-
tence s using: score(s) =
?
n
i
?
i
f
i
(s), c
?
? F ,
where ?
i
are the weights estimated by the regres-
sor, and f
i
are the features of sentence s; see Table
2. The idea is that if well-phrased utterances can
be generated, whose stylistic variation is recog-
nisable to human judges, then our regressor can
be used in combination with any statistical sur-
face realiser. Note however that the stylistic vari-
ation observed depends on the stylistic spectrum
that each realiser covers. Here, our goal is mainly
to show that whatever stylistic variation exists in
a realiser can be recognised by our model.
6.1 Overview of Surface Realisers
In a human rating study, we compare four surface
realisers (ordered alphabetically), all of which
are able to return a ranked list of candidate re-
alisations for a semantic input. Please refer to
the references given for details of each system.
The BAGEL and SPaRKy realisers were compared
based on published ranked output lists.9
? BAGEL is a surface realiser based on dy-
namic Bayes Nets originally trained using
Active Learning by Mairesse et al. (2010).
It was shown to generate well-phrased utter-
ances from unseen semantic inputs.
? CRF (global) treats surface realisation as a
9Available from http://people.csail.mit.
edu/francois/research/bagel and http://
users.soe.ucsc.edu/
?
maw/downloads.html.
708
System Utterance
BAGEL Beluga is a moderately priced
restaurant in the city centre area.
Col = 4.0, Pol = 4.0, Nat = 4.0
CRF (global) Set in the city centre, Beluga is a
moderately priced location for the
celebration of the Italian spirit.
Col = 2.0, Pol = 5.0, Nat = 2.0
pCRU Beluga is located in the city centre
and serves cheap Italian food.
Col = 4.0, Pol = 3.0, Nat = 5.0
SPaRKy Beluga has the best overall quality
among the selected restaurants
since this Italian restaurant has
good decor, with good service.
Col = 3.0, Pol = 4.0, Nat = 5.0
Table 5: Example utterances for the BAGEL, CRF
(global), pCRU and SPaRKy realisers shown to users.
Sample ratings from individual users are also shown.
sequence labelling task: given a set of (ob-
served) linguistic features, it aims to find the
best (hidden) sequence of phrases realising a
semantic input (Dethlefs et al., 2013).
? pCRU is based on probabilistic context-
free grammars and generation is done using
Viterbi search, sampling (used here), or ran-
dom search. It is based on Belz (2008).
? SPaRKy is based on a rank-and-boost ap-
proach. It learns a mapping between the lin-
guistic features of a target utterance and its
predicted user ratings and ranks candidates
accordingly (Walker et al., 2007).
6.2 Results: Recognising Stylistic Variation
242 users from the USA took part in a rating study
on the CrowdFlower platform and rated altogether
1, 702 utterances, from among the highest-ranked
surface realisations above. For each utterance
they read, they rated the colloquialism, natura-
less and politeness based on the same questions
as in Section 4.1, used to obtain the training data.
Based on this, we compare the perceived strength
of each stylistic dimension in an utterance to the
one predicted by the regressor. Example utter-
ances and ratings are shown in Table 5. Results
are shown in Table 6 and confirm our observa-
tions: ratings for known users can be estimated
with a medium (or high) correlation based on
clusters of users who assign similar ratings to ut-
terances with similar linguistic features. We can
also see that such estimations do not depend on a
particular data set or realiser.
System Colloquial Polite Natural
BAGEL 0.78 0.66 0.69
CRF global 0.58 0.63 0.63
pCRU 0.67 0.42 0.77
SPaRKy 0.87 0.56 0.81
Table 6: Correlation coefficients between subjective
user ratings and ratings predicted by the regressor for
known users across data-driven surface realisers.
A novel aspect of our technique in compari-
son to previous work on stylistic realisation is
that it does not depend on the time- and resource-
intensive design of a hand-coded generator, as in
Paiva and Evans (2005) and Mairesse and Walker
(2011). Instead, it can be applied in conjunc-
tion with any system designer?s favourite realiser
and preserves the realiser?s original features by
re-ranking only its top n (e.g. 10) output candi-
dates. Our method is therefore able to strike a
balance between highly-ranked and well-phrased
utterances and stylistic adaptation. A current lim-
itation of our model is that some ratings can still
not be predicted with a high correlation with hu-
man judgements. However, even the medium cor-
relations achieved have been shown to be signif-
icantly better than estimations based on the aver-
age population of users (Section 5.2).
7 Conclusion and Future Work
We have presented a model of stylistic realisation
that is able to adapt its output along several stylis-
tic dimensions. Results show that the variation is
recognisable by humans and that user ratings can
be predicted for known as well as unknown users.
A model which clusters individual users based
on their ratings of linguistically similar utterances
achieves significantly higher performance than a
model trained on the average population of rat-
ings. These results may also play a role in other
domains in which users display variability in their
subjective ratings, e.g. recommender systems,
sentiment analysis, or emotion generation. Future
work may explore the use of additional cluster-
ing features as a more scalable alternative to re-
ranking. It also needs to determine how user feed-
back can be obtained during an interaction, where
asking users for ratings may be disruptive. Possi-
bilities include to infer user ratings from their next
dialogue move, or from multimodal information
such as hesitations or eye-tracking.
709
Acknowledgements This research was funded
by the EC FP7 programme FP7/2011-14 under
grant agreements no. 270019 (SPACEBOOK)
and no. 287615 (PARLANCE).
References
Xavier Amatriain, Josep M. Pujol, and Nuria Oliver.
2009. I like It... I Like It Not: Evaluating User Rat-
ings Noise in Recommender Systems. In In the 17th
International Conference on User Modelling, Adap-
tation, and Personalisation (UMAP), pages 247?
258, Trento, Italy. Springer-Verlag.
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language En-
gineering, 14(4):431?455.
Penelope Brown and Stephen Levinson. 1987. Some
Universals in Language Usage. Cambridge Univer-
sity Press, Cambridge, UK.
Heriberto Cuaya?huitl and Nina Dethlefs. 2011. Op-
timizing Situated Dialogue Management in Un-
known Environments. In INTERSPEECH, pages
1009?1012.
Robert Dale and Jette Viethen. 2009. Referring
Expression Generation Through Attribute-Based
Heuristics. In Proceedings of the 12th Euro-
pean Workshop on Natural Language Generation
(ENLG), Athens, Greece.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Genera-
tion for Spoken Dialogue Systems: Reducing the
Need for Fillers. In Proceedings of the Interna-
tional Conference on Natural Language Generation
(INLG), Chicago, Illinois, USA.
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl,
and Oliver Lemon. 2013. Conditional Random
Fields for Responsive Surface Realisation Using
Global Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Sofia, Bulgaria.
Michael Fleischman and Eduard Hovy. 2002. Emo-
tional Variation in Speech-Based Natural Language
Generation. In Proceedings of the 2nd International
Natural Language Generation Conference.
Milica Gas?ic?, Filip Jurc???c?ek, Blaise Thomson, Kai Yu,
and Steve Young. 2011. On-Line Policy Optimi-
sation of Spoken Dialogue Systems via Interaction
with Human Subjects. In Proceedings of the IEEE
Automatic Speech Recognition and Understanding
(ASRU) Workshop.
Alastair Gill and Jon Oberlander. 2002. Taking Care
of the Linguistic Features of Extraversion. In Pro-
ceedings of the 24th Annual Conference of the Cog-
nitive Science Society, pages 363?368, Fairfax, VA.
Samuel Gosling, Simine Vazire, Sanjay Srivastava,
and Oliver John. 2004. Should We Trust Web-
Based Studies? A Comparative Analysis of Six Pre-
conceptions About Internet Questionnaires. Ameri-
can Psychologist, 59(2):93?104.
Swati Gupta, Marilyn Walker, and Daniela Romano.
2007. How Rude Are You? Evaluating Politeness
and Affect in Interaction. In Proceedings of the
2nd International Conference on Affective Comput-
ing and Intelligent Interaction.
Helen Hastie, Marie-Aude Aufaure, Panos Alex-
opoulos, Heriberto Cuayhuitl, Nina Dethlefs,
James Henderson Milica Gasic, Oliver Lemon,
Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis,
Yves Vanrompay, Boris Villazon-Terrazas, and
Steve Young. 2013. Demonstration of the PAR-
LANCE System: A Data-Driven, Incremental, Spo-
ken Dialogue System for Interactive Search. In Pro-
ceedings of the 14th Annual Meeting of the Special
Interest Group on Discourse and Dialogue (SIG-
dial).
Eduard Hovy. 1988. Generating Natural Language
under Pragmatic Constraints. Lawrence Erlbaum
Associates, Hillsdale, NJ.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and Alignment in Generated
Dialogues. In Proceedings of the 4th International
Natural Language Generation Conference (INLG),
Sydney, Australia.
Srini Janarthanam and Oliver Lemon. 2014. Adaptive
generation in dialogue systems using dynamic user
modeling. Computational Linguistics. (in press).
Pamela Jordan and Marilyn Walker. 2005. Learning
Content Selection Rules for Generating Object De-
scriptions in Dialogue. Journal of Artificial Intelli-
gence Research, 24:157?194.
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, and Johanna Moore.
2010. The First Challenge on Generating Instruc-
tions in Virtual Environments. In M. Theune and
E. Krahmer, editors, Empirical Methods in Natu-
ral Language Generation, pages 337?361. Springer
Verlag, Berlin/Heidelberg.
Franc?ois Mairesse and Marilyn Walker. 2011. Con-
trolling User Perceptions of Linguistic Style: Train-
able Generation of Personality Traits. Computa-
tional Linguistics, 37(3):455?488, September.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Si-
mon Keizer, Blaise Thomson, Kai Yu, and Steve
Young. 2010. Phrase-based statistical language
generation using graphical models and active learn-
ing. In Proceedings of the Annual Meeting of the
710
Association for Computational Linguistics (ACL),
pages 1552?1561.
Andrew Ng, Michael Jordan, and Yair Weiss. 2001.
On Spectral Clustering: Analysis and an Algorithm.
In Advances in Neural Information Processing Sys-
tems, pages 849?856. MIT Press.
Michael O?Mahony, Neil Hurley, and Gue?nole? Sil-
vestre. 2006. Detecting Noise in Recommender
System Databases. In Proceedings of the Inter-
national Conference on Intelligent User Interfaces
(IUI)s. ACM Press.
Daniel Paiva and Roger Evans. 2005. Empirically-
Based Control of Natural Language Generation. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL),
Ann Arbor, Michigan, USA.
Bo Pang and Lillian Lee. 2005. Seeing Stars: Exploit-
ing Class Relationships for Sentiment Categoriza-
tion with Respect to Rating Scales. In Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL).
Kaska Porayska-Pomsta and Chris Mellish. 2004.
Modelling Politness in Natural Language Gener-
ation. In Proceedings of the 3rd International
Natural Language Generation Conference (INLG),
Brighton, UK.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual Sig-
Dial Meeting on Discourse and Dialogue, Tokyo,
Japan.
Ulrike von Luxburg. 2007. A Tutorial on Spectral
Clustering. Statistics and Computing, 17(4).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413?456.
Ching-long Yeh and Chris Mellish. 1997. An Empir-
ical Study on the Generation of Anaphora in Chi-
nese. Computational Linguistics, 23:169?190.
711
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 210?214,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Finding middle ground? Multi-objective Natural Language Generation
from time-series data
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
A Natural Language Generation (NLG)
system is able to generate text from non-
linguistic data, ideally personalising the
content to a user?s specific needs. In some
cases, however, there are multiple stake-
holders with their own individual goals,
needs and preferences. In this paper, we
explore the feasibility of combining the
preferences of two different user groups,
lecturers and students, when generating
summaries in the context of student feed-
back generation. The preferences of each
user group are modelled as a multivariate
optimisation function, therefore the task
of generation is seen as a multi-objective
(MO) optimisation task, where the two
functions are combined into one. This ini-
tial study shows that treating the prefer-
ences of each user group equally smooths
the weights of the MO function, in a way
that preferred content of the user groups is
not presented in the generated summary.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating summaries from
attributes whose values change over time. Content
selection is the task of choosing what to say, i.e.
what information to be included in a report (Re-
iter and Dale, 2000). Here, we consider the task
of automatically generating feedback summaries
for students describing their performance during
the lab of a computer science module over the
semester. This work is motivated by the fact that
different user groups have different preferences of
the content that should be conveyed in a summary,
as shown by Gkatzia et al. (2013).
Various factors can influence students? learning,
such as difficulty of the material (Person et al.,
1995), workload (Craig et al., 2004), attendance
in lectures (Ames, 1992) etc. These factors change
over time and can be interdependent. The different
stakeholders (i.e. lecturers and students) have dif-
ferent perceptions regarding what constitutes good
feedback. Therefore, when generating feedback,
we should take into account all preferences in or-
der to be able to produce feedback summaries that
are acceptable by both user groups.
Stakeholders often have conflicting goals, needs
and preferences, for example managers with em-
ployees or doctors with patients and relatives. In
our data, for instance, lecturers tend to comment
on the hours that a student studied, whereas the
students disprefer this content. Generating the
same summary for both groups allows for mean-
ingful further discussion with common ground.
Previous work on NLG systems that address
more than one user group use different versions of
a system for each different user group (Gatt et al.,
2009) or make use of User Models (Janarthanam
and Lemon, 2010; Thompson et al., 2004; Zuk-
erman and Litman, 2001). Here, we explore a
method that adapts to both expert preferences and
users simultaneously (i.e. lecturer and students
preferences), by applying Multi-Objective opti-
misation (MOO). MOO can be applied to situa-
tions where optimal decisions are sought in the
presence of trade-offs between conflicting objec-
tives (Chankong and Haimes, 1983). We explore
whether balancing the preferences of two user
groups can result in an adaptive system that is ac-
ceptable by all users. At the same time, the pro-
gramming effort is reduced as only one system
needs to be developed. Moreover, by pooling all
available data together, there is less need for an
extensive data collection.
In the next section, we present three systems:
one tuned for lecturers, one for students, and one
that attempts to find middle ground. In Section 3,
we describe an evaluation of these three systems
and in Section 4 we discuss the results. Finally, in
210
Section 5, directions for future work are discussed.
2 Methodology
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns
to take optimal sequences of actions so as to
maximize a cumulative reward (Sutton and Barto,
1998). Here we extend the framework proposed
by Gkatzia et al. (2013) whereby the content selec-
tion is seen as a Markov Decision problem and the
goal of the agent is to learn to take the sequence
of actions that leads to optimal content selection.
A Temporal Difference learning method (Sutton
and Barto, 1998) was used to train an agent for
content selection. Firstly, we will describe the
data in general. Secondly, we refer to the RL
system that adapts to lecturers? preferences as de-
scribed by Gkatzia et al. (2013). Thirdly, we will
describe how we collected data and developed a
methodology that adapts to students? preferences
and finally how we combined the knowledge of
both steps to develop an MO system. The three
systems (Lecturer-adapted, Student-adapted, MO)
share the same architecture but the difference lies
in the reward functions used for training.
2.1 The Data
For this study, the dataset described by Gkatzia
et al. (2013) was used. Table 1 shows an exam-
ple of this dataset that describes a student?s learn-
ing habits and a corresponding feedback summary
provided by a lecturer. The dataset is composed
of 37 similar instances. Each instance consists of
time-series information about the student?s learn-
ing routine and the selected templates that lectur-
ers used to provide feedback to this student. A
template is a quadruple consisting of an id, a fac-
tor (Table 1), a reference type (trend, weeks, aver-
age, other) and surface text. For instance, a tem-
plate can be (1, marks, trend, ?Your marks were
<trend>over the semester?). The lexical choice
for <trend>(i.e. increasing or decreasing) de-
pends on the values of time-series data. There
is a direct mapping between the values of factor
and reference type and the surface text. The time-
series attributes are listed in Table 1 (bottom left).
2.2 Time-series summarisation systems
Actions and states: The state consists of the time-
series data and the selected templates. In order to
explore the state space the agent selects a time-
series attribute (e.g. marks, deadlines etc.) and
then decides whether to talk about it or not. The
states and actions are similar for all systems.
Lecturer-adapted reward function
The reward function is derived from analysis with
linear regression of the provided dataset and is the
following cumulative multivariate function:
Reward
LECT
= a+
n
?
i=1
b
i
? x
i
+ c ? length
where X = {x
1
, x
2
, ..., x
n
} is the vector of
combinations of the data trends observed in the
time-series data and a particular reference type of
the factor. The value of x
i
is given by the function:
x
i
=
?
?
?
?
?
?
?
?
?
?
?
1, if the combination of a factor trend
and a particular reference type is
included in the feedback
0, if not.
The coefficients represent the preference level of
a factor to be selected and how to be conveyed
in the summary. Important factors are associated
with high positive coefficients and the unimpor-
tant ones with negative coefficients. In the train-
ing phase, the agent selects a factor and then de-
cides whether to talk about it or not. If it decides
to refer to a factor, the selection of the template is
performed deterministically, i.e. it selects the tem-
plate that results in higher reward. Length rep-
resents the number of factors selected for gener-
ation.
Student-adapted reward function
The Student-adapted system uses the same RL al-
gorithm as the Lecturer-adapted one. The differ-
ence lies in the reward function. The reward func-
tion used for training is of a similar style as the
Lecturer-adapted reward function. This function
was derived by manipulating the student ratings in
a previous experiment and estimating the weights
using linear regression in a similar way as Walker
et al. (1997) and Rieser et al. (2010).
Multi-objective function
The function used for the multi-objective method
is derived by weighting the sum of the individual
reward functions.
R
MO
= 0.5 ? R
LECT
+ 0.5 ? R
STUDENT
To reduce the confounding variables, we kept
the ordering of content in all systems the same.
3 Evaluation
The output of the above-mentioned three systems
were evaluated both in simulation and with real
211
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors factor trend
(1) marks trend other
(2) hours studied trend increasing
(3) understandability trend decreasing
(4) difficulty trend decreasing
(5) deadlines trend increasing
(6) health issues trend other
(7) personal issues trend decreasing
(8) lectures attended trend other
(9) revision trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think about whether
revising has improved your performance.
Table 1: Top left: example of the time-series raw data for feedback generation. Bottom left: example of
described trends. Right box: a target summary generated by an expert (bold signifies the chosen content).
users. Example summaries of all systems are pre-
sented in Table 2.
3.1 Evaluation in Simulation
26 summaries were produced by each system. The
output of each system was evaluated with the three
reward functions. Table 3 shows the results.
As expected, all systems score highly when
evaluated with the reward function for which
they were trained, with the second highest reward
scored from the MO function. Table 2 illustrates
this with the MO Policy clearly between the other
two policies. Moreover, the MO function reduces
the variability between summaries as is also re-
flected in the standard deviation given in Table 3.
We used BLEU (4-grams) (Papineni et al.,
2002) to measure the similarities between the
feedback summaries generated by the three sys-
tems. BLEU score is between 0-1 with values
closer to 1 indicating texts are more similar. Our
results demonstrate that the summaries generated
by the three systems are quite different (BLEU
score between 0.33 and 0.36). This shows that the
framework presented here is capable of producing
quite different summaries based on the various re-
ward functions.
3.2 Evaluation with real users
The goal of the evaluation is to determine whether
the end-user can pick up on the above-mentioned
differences in the feedback and rank them accord-
ing to their preferences. The output of the three
systems was ranked by 19 lecturers and 48 first-
year Computer Science students. Time-series data
of three students were presented on graphs to each
participant. They were also shown 3 feedback
summaries and they were asked to rank them in
terms of preference.
As we can see from Table 4, the two user groups
significantly preferred the output of the system
which was trained for their preferences (Mann-
Whitney U test, p < 0.05). Interestingly, lecturers
found both the outputs produced by the Lecturer-
adapted system and the Student-adapted system
significantly preferable (p < 0.05) to the output
produced by the MO system. In contrast, students
significantly preferred the output generated by the
Student-adapted system over the other two. Fi-
nally, both user groups rated the MO system 3rd,
but there is not a significant difference between
the student ratings for the MO system and the
Lecturer-adapted system.
4 Discussion
It is interesting to examine the weights derived
from the multiple-linear regression to determine
the preferences of the different user groups. For
instance, lecturers? most preferred content is
hours studied, therefore the reward function gives
high scores to summaries that mention the hours
212
Lecturer-adapted Student-adapted Multi-objective
Make sure you revise the learning
material and try to do the lab ex-
ercises again. You dedicated more
time studying the lecture material in
the beginning of the semester com-
pared to the end of the semester.
Have a think about what is prevent-
ing you from studying. Your under-
standing of the material could be
improved. Try going over the teach-
ing material again. You have had
other deadlines during weeks 5, 6,
8, 9 and 10. You may want to plan
your studying and work ahead. You
did not face any health problems
during the semester.
You found the lab exercises very
challenging. Make sure that you
have understood the taught material
and don?t hesitate to ask for clari-
fication. You dedicated more time
studying the lecture material in
the beginning of the semester com-
pared to the end of the semester.
Have a think about what is prevent-
ing you from studying. Your un-
derstanding of the material could
be improved. Try going over the
teaching material again. Revising
material during the semester will
improve your performance in the
lab.
Your attendance was varying over the
semester. Have a think about how to
use time in lectures to improve your un-
derstanding of the material. You found
the lab exercises very challenging. Make
sure that you have understood the taught
material and don?t hesitate to ask for
clarification. You dedicated more time
studying the lecture material in the be-
ginning of the semester compared to the
end of the semester. Have a think about
what is preventing you from studying.
You did not face any health problems
during the semester. You revised part
of the learning material. Have a think
whether revising has improved your per-
formance.
Table 2: Example outputs from the three different systems (bold signifies the chosen content).
Time-Series Summarisation Systems Lecturer Function Student Function MO Function
Lecturer-adapted system 243.82 (70.35) 51.99 (89.87) 114.12 (49.58)
Student-adapted system 72.54 (106.97) 213.75 (59.45) 127.76 (52.09)
MO system 123.67 (72.66) 153.79 (56.61) 164.84 (83.89)
Table 3: Average rewards (and standard deviation) assigned to summaries produced by the 3 systems.
Bold signifies higher reward.
Summarisation
Systems
Lecturer?s Rat-
ing
Student?s
Rating
Lecturer-adapted 1st (2.15)* 3rd (1.97)
Student-adapted 1st (2.01)* 1st* (2.22)
MO 2nd, 3rd (1.81) 3rd (1.79)
Table 4: Mode of the ratings for each user group
(*Mann-Whitney U test, p < 0.05, when compar-
ing each system to the MO system).
that a student studied in all cases (i.e. when the
hours studied increased, decreased, or remained
stable). This, however, does not factor heavily into
the student?s reward function.
Secondly, lecturers find it useful to give some
advice to students who faced personal issues dur-
ing the semester, such as advising them to talk to
their mentor. Students, on the other hand, like
reading about personal issues only when the num-
ber of issues they faced was increasing over the
semester, perhaps as this is the only trend that may
affect their performance. Students seem to mostly
prefer a feedback summary that mentions the un-
derstandability of the material when it increases
which is positive feedback. Finally, the only factor
that both groups agree on is that health issues is
negatively weighted and therefore not mentioned.
The MO reward function attempts to balance
the preferences of the two user groups. Therefore,
for this function, the coefficient for mentioning
health issues is also negative, however the other
coefficients are smoothed providing neither strong
negative or positive coefficients. This means that
there is less variability (see Table 3) but that per-
haps this function meets neither group?s criteria.
5 Conclusion and Future Work
In conclusion, we presented a framework for de-
veloping and evaluating various reward functions
for time-series summarisation of feedback. This
framework has been validated in that both simula-
tion and subjective studies show that each group
does indeed prefer feedback generated using a
highly tuned reward function, with lecturers being
slightly more open to variation. Further investiga-
tion is required as to whether it is indeed possible
to find middle ground between these two groups.
Choices for one group may be negatively rated
by the other and it might not be possible to find
middle ground but it is worth investigating further
other methods of reward function derivation using
stronger feature selection methods, such as Princi-
pal Component Analysis.
213
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):p261?71.
Chankong and Haimes. 1983. Multiobjective decision
making theory and methodology. In New York: El-
sevier Science Publishing.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning: an
exploratory look into the role of affect in learning
with autotutor. In Journal of Educational Media,
29:241-250.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
In Journal of AI Communications, 22:153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialogue systems: Evaluation with real users. In
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue.
K Papineni, S Roukos, T. Ward, and W. J Zhu. 2002.
BLEU: a method for automatic evaluation of ma-
chine translation. In 40th Annual meeting of the As-
sociation for Computational Linguistics.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. In Journal of Cogni-
tion and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. InCambridge Univer-
sity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. In MIT Press.
Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-
gley. 2004. A personalised system for conversa-
tional recommendations. In Journal of Artificial In-
telligence Research 21, 333-428.
Marilyn Walker, Diane Litman, Candace Kamm, and
Alicia Abella. 1997. PARADISE: A framework for
evaluating spoken dialogue agents. In 35th Annual
meeting of the Association for Computational Lin-
guistics.
Ingrid Zukerman and Diane Litman. 2001. Natu-
ral language processing and user modeling: Syner-
gies and limitations. In User Modeling and User-
Adapted Interaction, 11(1-2), 129-158.
214
Learning and Evaluation of Dialogue
Strategies for New Applications:
Empirical Methods for Optimization
from Small Data Sets
Verena Rieser?
School of GeoSciences/University of
Edinburgh
Oliver Lemon??
School of Mathematical and Computer
Sciences/Heriot-Watt University
We present a new data-driven methodology for simulation-based dialogue strategy learning,
which allows us to address several problems in the field of automatic optimization of dialogue
strategies: learning effective dialogue strategies when no initial data or system exists, and
determining a data-driven reward function. In addition, we evaluate the result with real users,
and explore how results transfer between simulated and real interactions. We use Reinforce-
ment Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated
environment which is ?bootstrapped? from small amounts of Wizard-of-Oz (WOZ) data. This
use of WOZ data allows data-driven development of optimal strategies for domains where no
working prototype is available. Using simulation-based RL allows us to find optimal policies
which are not (necessarily) present in the original data. Our results show that simulation-based
RL significantly outperforms the average (human wizard) strategy as learned from the data by
using Supervised Learning. The bootstrapped RL-based policy gains on average 50 times more
reward when tested in simulation, and almost 18 times more reward when interacting with real
users. Users also subjectively rate the RL-based policy on average 10% higher. We also show that
results from simulated interaction do transfer to interaction with real users, and we explicitly
evaluate the stability of the data-driven reward function.
1. Introduction
Statistical learning approaches, such as Reinforcement Learning (RL), for Spoken Dia-
logue Systems offer several potential advantages over the standard rule-based hand-
coding approach to dialogue systems development: a data-driven development cycle,
? Centre for Environmental Change and Sustainability, School of GeoSciences, Drummond Street,
Edinburgh EH89XP, UK. E-mail: verena.rieser@ed.ac.uk.
?? The Interaction Lab, School of Mathematical and Computer Sciences (MACS), Heriot-Watt University,
Edinburgh EH14 4AS, UK. E-mail: o.lemon@hw.ac.uk.
Submission received: 23 January 2009; revised submission received: 13 August 2010; accepted for publication:
13 September 2010.
? 2011 Association for Computational Linguistics
Computational Linguistics Volume 37, Number 1
provably optimal action policies, a precise mathematical model for action selection, pos-
sibilities for generalization to unseen states, and automatic optimization of competing
trade-offs in the objective function. See Young (2000), Lemon and Pietquin (2007), and
Frampton and Lemon (2009) for an introduction to dialogue strategy learning.
One of the major limitations of this approach is that it relies on a large quantity of
data being available. In cases when a fixed data set is used for learning (e.g., Walker
2000; Singh et al 2002; Henderson, Lemon, and Georgila 2008), the optimal policy can
only be discovered when it is present in the data set. (Note, by a policy being ?present
in a data set? we mean that the set of state-action mappings which define the policy
is contained in that data set. When a policy is not present in a data set, either some
states covered by the policy are not seen at all in that data, or the actions chosen by the
policy in some states are different to those seen in the data.) To overcome this problem,
simulated learning environments are being used to explore optimal policies which were
previously unseen in the data (e.g., Eckert, Levin, and Pieraccini 1997; Ai, Tetreault,
and Litman 2007; Young et al 2009). However, several aspects of the components of
this simulated environment are usually hand-crafted, and thus limit the scope of policy
learning. In particular, the optimization (or reward) function is often manually set (Paek
2006). In order to build simulation components from real data, annotated in-domain
dialogue corpora have to be available which explore a range of dialogue management
decisions. Collecting dialogue data without a working prototype is problematic, leaving
the developer with a classic ?chicken-or-egg? problem.
We therefore propose to learn dialogue strategies using simulation-based RL, where
the simulated environment is learned from small amounts of Wizard-of-Oz (WOZ) data.
In a WOZ experiment, a hidden human operator, the so-called ?wizard,? simulates
(partly or completely) the behavior of the application, while subjects are left in the belief
that they are interacting with a real system (Fraser and Gilbert 1991).
In contrast to preceding work, our approach enables strategy learning in domains
where no prior system is available. Optimized learned strategies are then available
from the first moment of on-line operation, and handcrafting of dialogue strategies
is avoided. This independence from large amounts of in-domain dialogue data allows
researchers to apply RL to new application areas beyond the scope of existing dialogue
systems. We call this method ?bootstrapping.?
In addition, our work is the first using a data-driven simulated environment. Previ-
ous approaches to simulation-based dialogue strategy learning usually handcraft some
of their components.
Of course, some human effort is needed in developing the WOZ environment and
annotating the collected data, although automatic dialogue annotation could be ap-
plied (Georgila et al 2009). The alternative?collecting data using hand-coded dialogue
strategies?would still require annotation of the user actions, and has the disadvantage
of constraining the system policies explored in the collected data. Therefore, WOZ data
allows exploration of a range of possible strategies, as intuitively generated by the
wizards, in contrast to using an initial system which can only explore a pre-defined
range of options.
However, WOZ experiments usually only produce a limited amount of data, and
the optimal policy is not likely to be present in the original small data set. Our method
shows how to use these data to build a simulated environment in which optimal
policies can be discovered. We show this advantage by comparing RL-based strategy
against a supervised strategy which captures average human wizard performance on
the dialogue task. This comparison allows us to measure relative improvement over the
training data.
154
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
The use of WOZ data has earlier been proposed in the context of RL. Williams and
Young (2004) use WOZ data to discover the state and action space for the design of
a Markov Decision Process (MDP). Prommer, Holzapfel, and Waibel (2006) use WOZ
data to build a simulated user and noise model for simulation-based RL. Although
both studies show promising first results, their simulated environments still contain
many hand-crafted aspects, which makes it hard to evaluate whether the success of the
learned strategy indeed originates from the WOZ data. Schatzmann et al (2007) propose
to ?bootstrap? with a simulated user which is entirely hand-crafted. In the following we
propose what is currently the most strongly data-driven approach to these problems. We
also show that the resulting policy performs well for real users. In particular we propose
a five-step procedure (see Figure 1):
1. We start by collecting data in a WOZ experiment, as described in Section 2.
2. From these data we train and test different components of our simulated
environment using Supervised Learning techniques (Section 3). In
Figure 1
Data-driven methodology for simulation-based dialogue strategy learning for new applications.
155
Computational Linguistics Volume 37, Number 1
particular, we extract a supervised policy, reflecting human (wizard)
performance on this task (see Section 3.3). We build a noise simulation
(Section 3.4), and two different user simulations (Section 3.5), as well as a
data-driven reward function (Section 3.6).
3. We then train and evaluate dialogue policies by interacting with the
simulated environment (Section 4).
4. Once the learned policies are ?good enough? in simulation, we test them
with real users (Section 5).
5. In addition, we introduce a final phase where we meta-evaluate the whole
framework (Section 6). This final step is necessary because WOZ
experiments only simulate human?computer interaction (HCI). We
therefore need to show that a strategy bootstrapped from WOZ data
indeed transfers to real HCI. We first show that the results between
simulated and real interaction are compatible (Section 6.1). We also
meta-evaluate the reward function, showing that it is a stable, accurate
estimate for real user?s preferences (Section 6.2).
Note that RL is fundamentally different to Supervised Learning (SL): RL is a statisti-
cal planning approach which allows us to find an optimal policy (sequences of actions)
with respect to an overall goal (Sutton and Barto 1998); SL, in contrast, is concerned with
deducing a function from training data for predicting/classifying events. This article
is not concerned with showing differences between SL and RL on a small amount of
data, but we use SL methods to capture the average human wizard strategy in the
original data, and show that simulation-based RL is able to find new policies that were
previously unseen.
We apply this framework to optimize multimodal information-seeking dialogue
strategies for an in-car digital music player. Dialogue Management and multimodal
output generation are two closely interrelated problems for information seeking dia-
logues: the decision of when to present information depends on how many pieces of
information to present and the available options for how to present them, and vice versa.
We therefore formulate the problem as a hierarchy of joint learning decisions which are
optimized together. We see this as a first step towards an integrated statistical model of
Dialogue Management and more advanced output planning/Natural Language Gener-
ation (Lemon 2008; Rieser and Lemon 2009b; Lemon 2011; Rieser, Lemon, and Liu 2010;
Janarthanam and Lemon 2010).
In the following, Section 2 describes the Wizard-of-Oz data collection (i.e., how
to collect appropriate data when no initial data or system exists), Section 3 explains
the construction of the simulated learning environment (including how to determine a
data-driven reward function), Section 4 presents training and evaluation of the learned
policies in simulation (i.e., how to learn effective dialogue strategies), Section 5 presents
the results of the tests with real users, and Section 6 presents a meta-evaluation of the
framework, including transfer results.
2. Wizard-of-Oz Data Collection
The corpus used for learning was collected in a multimodal study of German task-
oriented dialogues for an in-car music player application. The corpus was created
156
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
in the larger context of the TALK project1 and is also known as the SAMMIE corpus
(Kruijff-Korbayova? et al 2006). In contrast to conventional WOZ trials we were not only
interested in the users? behavior, but also in the behavior of our human wizards. This
study provides insights into natural strategies of information presentation as performed
by human wizards.
2.1 Experimental Setup
Six people played the role of an intelligent interface (the ?wizards?). The wizards
were able to speak freely and display search results on the screen by clicking on pre-
computed templates. Wizards? outputs were not restricted, in order to explore the
different ways they intuitively chose to present search results. Wizard?s utterances were
immediately transcribed and played back to the user with Text-To-Speech. Twenty-one
subjects (11 women, 10 men) were given a set of predefined tasks to perform, as well as
a primary driving task, using a driving simulator (Mattes 2003). The users were able to
speak, as well as make selections on the screen.
The experiment proceeded as follows. First the wizards were trained to use the
database interface and they were also given general instructions about how to interact
with the user. Training took 45 minutes, including five example tasks.
After the user arrived s/he was introduced to the driving simulator and had to
perform a short test drive. The users solved two sets of tasks with two tasks in each.
After each task the user filled out a task-specific questionnaire, in which they indicated
perceived task success and satisfaction on a five-point Likert scale. Finally, the user was
interviewed by the experiment leader following a questionnaire containing questions
similar to the PARADISE study (Walker, Kamm, and Litman 2000), including questions
on task ease, timing, multimodal and verbal presentation, as well as future use of such
systems. All subjects reported that they were convinced that they were interacting with
a real system.
To approximate speech recognition errors we used a tool that randomly deletes
parts of the transcribed utterances. Due to the fact that humans are able to make sense
of even heavily corrupted input, this method not only covers non-understandings, but
wizards also built up their own hypotheses about what the user really said, which can
lead to misunderstandings. The word deletion rate varied: 20% of the utterances were
weakly corrupted (= deletion rate of 20%), and 20% were strongly corrupted (= deletion
rate of 50%). In 60% of the cases the wizard saw the transcribed speech uncorrupted.
Example (1) illustrates the kind of corrupted utterances the wizard had to deal with.
(1) uncorrupted: ?Zu dieser Liste bitte Track ?Tonight? hinzufu?gen.?
[ ?Add track ?Tonight? to this list.?]
weakly corrupted: ?Zu dieser Liste bitte Track ?Tonight? . . . . ?
[?. . . track ?Tonight? to this list.?]
strongly corrupted: ?Zu . . . Track ?Tonight? . . . .
[?. . . track ?Tonight? to . . . ?]
There are some shortcomings of this technique, which we discuss in Rieser and
Lemon (2009a). However, the data are useful for our purposes because our main interest
1 TALK (Talk and Look: Tools for Ambient Linguistic Knowledge; www.talk-project.org) was funded by
the EU as project no. IST-507802 within the 6th Framework program.
157
Computational Linguistics Volume 37, Number 1
here is in multimodal presentation strategies (in the presence of some input noise).
Other studies have specifically targeted the Dialogue Management question of how to
handle ASR input noise (e.g., Stuttle, Williams, and Young 2004; Skantze 2005).
2.2 Data Collected
The corpus gathered with this set-up comprises 21 sessions and over 1,600 turns. Some
example dialogues can be found in Appendix B. Example (2) shows a typical multi-
modal presentation sub-dialogue from the corpus (translated from German). Note that
the wizard displays quite a long list of possible candidates on an (average sized) com-
puter screen, while the user is driving. This example illustrates that even for humans it
is difficult to find an ?optimal? solution to the problem we are trying to solve.
(2) User: ?Please search for music by Bjo?rk.?
Wizard: ?I found 43 items. The items are displayed on the screen.?
[displays list]
User: ?Please select Human Behaviour.?
Information was logged for each session, for example, the transcriptions of the
spoken utterances, the wizard?s database query and the number of results, and the
screen option chosen by the wizard. A rich set of contextual dialogue features was
also annotated, as listed in Section 3.1. Also see Rieser, Kruijff-Korbayova?, and Lemon
(2005).
Of the 793 wizard turns 22.3% were annotated as presentation strategies, re-
sulting in 177 instances for learning, where the six wizards contributed about equal
proportions.
A ?2 test on presentation strategy (comparing whether wizards chose to present
in multimodal or verbal modality) showed significant differences between wizards
(?2(1) = 34.21, p < .001). On the other hand, a Kruskal-Wallis test comparing user
preferences for the multimodal output showed no significant difference across wizards
(H(5)=10.94, p > .05).2 Mean performance ratings for the wizards? multimodal behavior
ranged from 1.67 to 3.5 on a five-point Likert scale. We also performed an analysis of
whether wizards improved their performance over time (learning effects). The results
show that the wizard?s average user satisfaction scores in general slightly decreased
with the number of sessions that they performed, however.
Observing significantly different strategies that are not significantly different in
terms of user satisfaction, we conjecture that the wizards converged on strategies which
were appropriate in certain contexts. To strengthen this hypothesis we split the data by
wizard and performed a Kruskal-Wallis test on multimodal behavior per session. Only
the two wizards with the lowest performance score showed no significant variation
across session, whereas the wizards with the highest scores showed the most varying
behavior. These results again indicate a context-dependent strategy.
In Section 3.1 we test this hypothesis (that good multimodal clarification strate-
gies are context-dependent) by using feature selection techniques in order to find the
2 The Kruskal-Wallis test is the non-parametric equivalent to a one-way ANOVA. Because the users
indicated their satisfaction on a five-point likert scale, an ANOVA which assumes normality would be
invalid.
158
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
features which are most predictive for the wizards? behavior. The dialogues show that
common ?mistakes? were that the wizards either displayed too much information on
the screen, see Example (1) in Appendix B, or the wizards fail to present results early
enough, see Example (2) in Appendix B. In general, users report that they get distracted
from driving if too much information is presented. On the other hand, users prefer
shorter dialogues (most of the user ratings are negatively correlated with dialogue
length).
These results indicate that we need to find a strategy given the competing trade-
offs between the number of results (large lists are difficult for users to process), the
length of the dialogue (long dialogues are tiring, but collecting more information can
result in more precise results), and the noise in the speech recognition environment
(in high noise conditions accurate information is difficult to obtain). In the following
we utilize the ratings from the user questionnaires to optimize a presentation strategy
using simulation-based RL.
3. Simulated Learning Environment
Simulation-based RL learns by interaction with a simulated environment (Sutton and
Barto 1998). We obtain the simulated components from the WOZ corpus using data-
driven methods. Although this requires quite a large effort, the exercise is important as
a case study for exploring the proposed methodology.
The employed database contains 438 items and is similar in retrieval ambiguity and
structure to the one used in the WOZ experiment. The dialogue system used for learning
implements a multimodal information presentation strategy which is untrained, but
comprises some obvious constraints reflecting the system logic (e.g., that only filled
slots can be confirmed), implemented as Information State Update (ISU) rules (see also
Heeman 2007; Henderson, Lemon, and Georgila 2008).
Other behavior which is hand-coded in the system is to greet the user in the
beginning of a dialogue and to provide help if the user requests help. The help function
provides the user with some examples of what to say next (see system prompt s6 in the
Example Dialogue in Table 1 in Appendix D). All other actions are left for optimization.
3.1 Feature Space
A state or context in our system is a dialogue ?information state? as defined in (Lemon
et al, 2005). We divide the types of information represented in the dialogue information
state into local features (constituting low-level and dialogue features), dialogue history
features, and user model features. We also defined features reflecting the application
environment (e.g., driving). The information state features are shown in Tables 1, 2,
and 3, and further described below. All features are automatically extracted from the
WOZ log-files (as described in Section 2.2), and are available at runtime in ISU-based
dialogue systems.
Local features. First, we extracted features present in the ?local? context of a wizard
action, as shown in Table 1, such as the number of matches returned from the database
query (DB), whether any words were deleted by the corruption algorithm (see Sec-
tion 2.1), and the previous user speech act (user-act) of the antecedent utterance. The
159
Computational Linguistics Volume 37, Number 1
Table 1
Contextual/information-state features: Local features.
Local features
DB: database matches (integer)
deletion: words deleted (yes/no)
user-act: add, repeat, y/n, change, others
user actions are annotated manually by two annotators (? = .632). Please see Table 1 in
Appendix A for detailed results on inter-annotator agreement.
Also, note that the deletion feature (and later delHist, and delUser) counts the
number of words deleted by the corruption tool (see Section 2.1) and serves as an
approximation to automatic speech recognition (ASR) confidence scores as observed by
the system. Equally, the human wizard will be able to infer when words in a sentence
were deleted and hence has a certain confidence that the input is complete.
Dialogue history features. The history features account for events in the whole dialogue
so far, that is, all information gathered before entering the presentation phase, as
shown in Table 2. We include features such as the number of questions that the wizard
asked so far (questHist), how often the screen output was already used (screenHist),
the average corruption rate so far (delHist), the dialogue length measured in turns
(dialogueLength), the dialogue duration in seconds (dialogueDuration), and whether
the user reacted to the screen output, either by verbally referencing (refHist), for
example, using expressions such as It?s item number 4, or by clicking (clickHist).
User model features. Under ?user model features? we consider features reflecting the wiz-
ards? responsiveness to the behavior and situation of the user. Each session comprises
four dialogues with one wizard. The user model features average the user?s behavior in
these dialogues so far, as shown in Table 3, such as how responsive the user is towards
the screen output, namely, how often this user clicks (clickUser) and how frequently
s/he used verbal references so far (refUser); how often the wizard had already shown a
screen output (screenUser) and how many questions were already asked (questUser);
how much the user?s speech was corrupted on average so far (delUser), that is, an
approximation of how well this user is recognized; and whether this user is currently
driving or not (driving). This information was available to the wizards.
Table 2
Contextual/information-state features: History features.
Dialogue History Features
questHist: number of questions (integer)
screenHist: number screen outputs (integer)
delHist: average corruption rate; no.wordsDeletedInDialogueSoFarno.utterancesInDialogueSoFar (real)
dialogueLength: length in turns (integer)
dialogueDuration: time in sec (real)
refHist: number of verbal user references to screen output (integer)
clickHist: number of click events (integer)
160
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 3
Contextual/information-state features: User model features.
User model features
clickUser: average number of clicks (real)
refUser: average number of verbal references (real)
delUser: average corruption rate for that user; no.wordsDeletedForUserSoFarno.utterancesForUserSoFar (real)
screenUser: average number of screens shown to that user (real)
questUser: average number of questions asked to user (real)
driving: user driving (yes/no)
Note that all these features are generic over information-seeking dialogues where
database results can be displayed on a screen; except for driving which only applies
to hands-and-eyes-busy situations. This potential feature space comprises 16 features,
many of them taking numerical attributes as values. Including them all in the state space
for learning would make the RL problem unnecessarily complex. In the next section we
describe automatic feature selection techniques, which help to reduce the feature space
to a subset which is most predictive of when and how to present search results.
3.1.1 Feature Selection. We use feature selection techniques to identify the context features
which are most predictable for the wizards choosing a specific action. We choose to
apply forward selection for all our experiments in order to not include redundant
features, given our large feature set. We use the following feature filtering methods:
correlation-based subset evaluation (CFS; Hall 2000) and a decision tree algorithm (rule-
based SL). We also apply a correlation-based ?2 ranking technique. Filtering techniques
account for inter-feature relations, selecting subsets of predictive features at the expense
of saying less about individual feature performance itself. Ranking techniques evaluate
each feature individually. For our experiments we use implementations of selection
techniques provided by the WEKA toolkit (Witten and Frank 2005).
First, we investigated the wizards? information acquisition strategy, namely, which
features are related to the wizards? decision when to present a list (presentInfo)?that
is, the task is to predict presentInfo vs. all other possible dialogue acts. None of the
feature selection techniques were able to identify any predictive feature for this task.
Next, we investigated the wizards? information presentation strategy, that is, which
features are related to the wizards? decision to present a list verbally (presentInfo-
verbal) or multi-modally (presentInfo-multimodal). All the feature selection tech-
niques consistently choose the feature DB (number of retrieved items from the database).
This result is maybe not very surprising, but it supports the claim that using feature
selection on WOZ data delivers valid results. Relevant features for other domains may
be less obvious. For example, Levin and Passonneau (2006) suggest the use of WOZ
data in order to discover the state space for error recovery strategies. For this task many
other contextual features may come into play, as shown by Gabsdil and Lemon (2004)
and Lemon and Konstas (2009) for automatic ASR re-ranking.
We use this information to construct the state space for RL, as described in the
following section, as well as using these feature selection methods to construct the
wizard strategy as described in Section 3.3.
161
Computational Linguistics Volume 37, Number 1
Figure 2
State-action space for hierarchical Reinforcement Learning.
3.2 MDP and Problem Representation
The structure of an information-seeking dialogue system consists of an information
acquisition phase, and an information presentation phase. For information acquisition
the task of the dialogue manager is to gather ?enough? search constraints from the user,
and then, ?at the right time,? to start the information presentation phase, where the pre-
sentation task is to present ?the right amount? of information in the right way?either
on the screen or listing the items verbally. What ?the right amount? actually means
depends on the application, the dialogue context, and the preferences of users. For
optimizing dialogue strategies information acquisition and presentation are two closely
interrelated problems and need to be optimized jointly: When to present information
depends on the available options for how to present them, and vice versa.
We therefore formulate the problem as an MDP, relating states to actions in a hierar-
chical manner (see Figure 2): Four actions are available for the information acquisition
phase; once the action presentInfo is chosen, the information presentation phase is
entered, where two different actions for output realization are available.
The state space is constructed semi-automatically. We manually enumerate the task-
related features needed to learn about the dialogue task. For example, we manually
specified the number of slots, and information about the ?grounded-ness? of the slots,
needed to learn confirmation strategies.3 We also added the features which were auto-
matically discovered by the feature selection techniques defined in Section 3.1.1.
The state-space comprises eight binary features representing the task for a four-
slot problem: filledSlot indicates whether a slot is filled, confirmedSlot indicates
whether a slot is confirmed. We also add the number of retrieved items (DB). We found
that human wizards especially pay attention to this feature, using the feature selection
techniques of Rieser and Lemon (2006b). The feature DB takes integer values between
1 and 438, resulting in 28 ? 438 = 112, 128 distinct dialogue states for the state space.
In total there are 4112,128 theoretically possible policies for information acquisition.4 For
3 Note that we simplified the notion of a slot being grounded as a binary feature, following Henderson,
Lemon, and Georgila (2008). More recent work uses more fine-grained notions of confidence in
user-provided information (e.g., Roque and Traum 2008), or the notion of ?belief states? in Partially
Observable Markov Decision Processes (e.g., Williams and Young 2007). This does lead to new policies in
information acquisition, but is not the focus of this article.
4 In practice, the policy space is smaller, as some combinations are not possible (e.g., a slot cannot be
confirmed before being filled). Furthermore, some incoherent action choices are excluded by the basic
system logic.
162
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
the presentation phase the DB feature is discretized, as we will further discuss in Section
3.7. For the information presentation phase there are 22
3
= 256 theoretically possible
policies.
3.3 Wizard Behavior
Our hypothesis is that simulation-based RL allows us to find optimal policies which are
superior to those present in the original data. Therefore we create a policy which mimics
the average wizard behavior, and this allows us to measure the relative improvements
over the training data (cf. Henderson, Lemon, and Georgila 2008). We create this base-
line by applying SL. For these experiments we use the WEKA toolkit (Witten and Frank
2005). We learn with the decision tree J4.8 classifier, WEKA?s implementation of the C4.5
system (Quinlan 1993), and rule induction JRIP, the WEKA implementation of RIPPER
(Cohen 1995). In particular, we learn models which predict the following wizard actions:
 Presentation timing: when the ?average? wizard starts the presentation
phase on a turn level (binary decision).
 Presentation modality: in which modality the list is presented (multimodal
vs. verbal).
We use annotated dialogue context features as input, as described in Section 3.1,
with feature selection techniques as described in Section 3.1.1. Both models are trained
using 10-fold cross validation, comparing the predicted labels against the true labels in
a hold-out test set. Table 4 presents the results for comparing the accuracy of the learned
classifiers against the majority baseline.
A data analysis shows that all of the wizards are more likely to show a graphic
on the screen when the number of database hits is ? 4. However, none of the wizards
strictly follows that strategy.
For presentation timing, none of the classifiers produces significantly improved
results. Hence, we conclude that there is no distinctive pattern observable by the SL
algorithms for when to present information. For strategy implementation we therefore
use a frequency-based approach following the distribution in the WOZ data: In 48% of
cases the baseline policy decides to present the retrieved items; for the rest of the time
the system follows a hand-coded strategy.
For learning presentation modality, both classifiers significantly outperform the ma-
jority baseline. The learned models both learn the same rule set, which can be rewritten
as in Listing 1. Note that this rather simple algorithm is meant to represent the average
strategy as learned by SL from the initial data (which then allows us to measure the
relative improvements of the RL-based strategy).
Table 4
Predicted accuracy for presentation timing and modality (with standard deviation ?).
majority baseline JRip J48
timing 52.0(? 2.2) 50.2(? 9.7) 53.5(? 11.7)
modality 51.0(? 7.0) 93.5(?11.5)* 94.6(? 10.0)*
* Statistically significant improvement at p < .05.
163
Computational Linguistics Volume 37, Number 1
Listing 1
Supervised Strategy
i f ( db<4){
return present InfoVerba l ; }
else {
return presentInfoMM ;}
3.4 Noise Simulation
One of the fundamental characteristics of HCI is an error-prone communication chan-
nel. Therefore, the simulation of channel noise is an important aspect of the learn-
ing environment. Previous work uses data-intensive simulations of ASR errors (e.g.,
Pietquin and Dutoit 2006; Schatzmann, Thomson, and Young 2007a). Because we only
have limited data available, we use a simple model simulating the effects of non- and
misunderstanding on the interaction, rather than the noise itself. This method is espe-
cially suited to learning from small data sets. From our data we estimate a 30% chance
of user utterances to be misunderstood, and 4% to be complete non-understandings,
which is a realistic estimation for deployed dialogue systems (cf. Litman and Pan 1999;
Carpenter et al 2001; Hirschberg, Litman, and Swerts 2001; Georgila, Henderson, and
Lemon 2005).
We simulate the effects that noise has on the user behavior, as well as for the task
accuracy.5 For the user side, the noise model defines the likelihood of the user accepting
or rejecting the system?s hypothesis (e.g., when the system utters a confirmation), that
is, in 30% of the cases the user rejects, in 70% the user agrees. These probabilities are
combined with the probabilities for user actions from the user simulation, as described
in the next section. For non-understandings we have the user simulation generating
Out-of-Vocabulary utterances with a chance of 4%. Furthermore, the noise model deter-
mines the likelihood of task accuracy as calculated in the reward function for learning.
A filled slot which is not confirmed by the user has a 30% chance of having been mis-
recognized, see Task Completion as defined in Section 3.6.
3.5 User Simulation
A user simulation is a predictive model of real user behavior used for automatic di-
alogue strategy development and testing. See Schatzmann et al (2006) for a compre-
hensive survey. Simulations on the intention/dialogue act level are most popular for
RL-based strategy learning, as they outperform the lower level approaches in terms of
robustness, portability, and scalability. For our domain, the user can either add new
information (add), repeat or paraphrase information which was already provided at an
earlier stage (repeat), give a simple yes/no answer (y/n), or change to a different topic
by providing a different slot value than the one asked for (change). Examples from the
corpus are given in Table 5 and in the dialogues listed in Appendix B. These actions are
annotated manually by two annotators (? = .632, see Appendix A).
5 Simulating the effects of noise, rather than the noise itself, is sufficient to learn presentation strategies in
the presence of noise (e.g., whether a slot has to be confirmed before a result can be presented). Note that
other work has focused on learning dialogue strategies under different noise conditions (e.g., Bohus et al
2006; Williams and Young 2007).
164
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 5
User action types and frequencies as annotated in the data.
# action type freq % example (original) translation
1 add 54 30.5 a?h, Ella Fitzgerald. er, Ella Fitzgerald.
3 repeat 57 32.2 ja, Smile ja. yes, Smile yes.
2 y/n 14 7.9 ja, in Ordnung. yes, that?s OK.
4 change 17 9.6 dann machen wir was anderes
und zwar ha?tte ich gern eine
Playlist mit drei Liedern.
Let?s try something else then. I
would like a playlist with three
songs.
others 35 19.8 ? no answer, comment, aside
In this work, we are challenged to learn user simulations from a small data set. We
first construct a simple bigram model in order to explore the quality of the data. Bigram
(or more general n-gram) models for user simulations were first introduced by Eckert,
Levin, and Pieraccini (1997, 1998). An n-gram?based user simulation predicts the user
action ?au,t at time t that is most probable given the dialogue history of system and user
actions, see Equation (1) where as,t denotes the system action at time t.
?au,t = argmaxau,tP(au,t|as,t, as,t?1, au,t?1, ..., au,t?n+1, as,t?n+1) (1)
The bigram model obtained from our WOZ data and the observed frequencies are
shown in Figure 3. When examining the distributions of user replies per system turn for
the bigram model, we can see that 25% of the state?action pairs have zero frequencies.
However, user simulations should allow the learner to also find strategies which are
not in the data. Especially when learning from small data sets, user simulations for
Figure 3
User action frequencies following a system act (bigram model): 25% zero frequencies for
state-action pairs due to data sparsity.
165
Computational Linguistics Volume 37, Number 1
automatic strategy training should cover the whole variety of possible user action for
each state in order to produce robust strategies. Ai, Tetreault, and Litman (2007), for
example, show that random models outperform more accurate ones if the latter fail to
provide enough coverage. On the other hand, user simulations used for testing should
be more accurate with respect to the data in order to test under realistic conditions (e.g.,
Mo?ller et al 2006).
We therefore apply two learning methods to deal with data sparsity (for n-gram
models): First, we develop a user simulation which is based on a new clustering tech-
nique; second, we apply smoothing (which is the standard technique applied to account
for zero frequencies in n-gram models).
3.5.1 Cluster-Based User Simulation. We introduce a cluster-based technique for building
user simulations from small amounts of data (see also Rieser and Lemon 2006a). A sim-
ilar approach has later been suggested by Schatzmann, Thomson, and Young (2007b),
called the ?summary-space mapping technique,? where similar states are summarized,
and a distribution of possible user behavior is assigned to a set of states, which we call
?clusters.? This method allows one to generate the full range of possible user behavior
in every state.
Cluster-based user simulations generate explorative user behavior which is similar
but not identical to user behavior observed in the original data. In contrast to the bigram
model, where the likelihood of the next user act is conditioned on the previous system
action, the likelihood for the cluster-based model is conditioned on a cluster of similar
system states (see Equation (2)).
?au,t ? argmaxau,tP(au,t|clusters,t?1) (2)
The underlying idea is that, with sparse training data, we want user simulations to be
?similar to real users in similar situation.? This user simulation should generate any
kind of observed user behavior in a context (as opposed to the zero frequencies for
sparse data), while still generating behavior which is pragmatically plausible in this sit-
uation. That is, we want our user simulation to generate behavior which is complete and
consistent with respect to the observed actions in the data. We also want our model to
generate actions which show some variability with respect to the observed behavior, that
is, a controlled degree of randomness. This variance will help us to explore situations
which are not observed in the data, which is especially valuable when building a model
from sparse training data (cf. Ai, Tetreault, and Litman 2007).
Clustering is applied in order to identify more general situations than the previ-
ously annotated system speech acts by grouping them according to their similarity. For
building such clusters we apply the Expectation-Maximization (EM) algorithm. The EM
algorithm is an incremental approach to clustering (Dempster, Laird, and Rubin 1977),
which fits parameters of Gaussian density distributions to the data. In order to define
similarity between system actions, we need to describe their (semantic) properties.
We therefore annotate the system acts using a fine-grained scheme by Rodriguez and
Schlangen (2004) and Rieser and Moore (2005), which allows classification of dialogue
acts in terms of different forms and functions.
We use a slightly modified version of the scheme, where we only use a subset of the
suggested annotation tags, while adding another level describing the output modality,
as summarized in Figure 4. In particular, the annotation scheme describes wizard
actions in terms of their communication level, which describes the linguistic target
after Clark (1996). We distinguish between utterances which aim to elicit acoustic
166
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Figure 4
Annotation scheme of discourse functions for wizard?s actions.
information (e.g., Sorry, can you please repeat? and utterances which aim to elicit fur-
ther information to uniquely identify the user?s reference (e.g., By which artist?). As
well as utterances trying to establish contact (e.g., Can you hear me?), and utterances
about the user?s intention (e.g., What do you want me to do?). The problem severity
describes which type of feedback the system requests from the user, namely, asking for
confirmation, for repetition, or for elaboration. The modality of the dialogue act
can either be verbal or multimodal.
Table 6 shows a mapping between system speech acts as described in Figure 2 and
the annotated discourse functions. We use these features for clustering the speech acts
according to their similarity in discourse function and form.
The EM algorithm generates three state clusters: The system acts askAQuestion and
implConfirm are summarized into cluster 1; explConf and reject are in cluster 2; and
presentListVerbal and presentListMM are in cluster 3. For every cluster we assign the
observed frequencies of user actions (i.e., all the user actions which occur with one of
the states belonging to that cluster), as shown in Figure 5.
3.5.2 Smoothed Bigram User Simulation. For our second user simulation model we apply
smoothing to a bigram model. We implement a simple smoothing technique called
?add-one smoothing? (Jurafsky and Martin 2000). This technique discounts some non-
zero counts in order to obtain probability mass that will be assigned to the zero counts.
We apply this technique to the original frequency-based bigram model. The resulting
model is shown in Figure 6.
In general, the smoothed model is closer to the original data than the cluster-based
one (thus being more realistic at the expense of allowing less exploratory behavior). In
the next section we introduce an evaluation metric which allows us to assess the level of
exploratory versus realistic user behavior as exhibited by the different user simulations.
Table 6
System speech acts and corresponding discourse functions.
speech act level severity modality
reject acoustic repetition verbal
explicitConfirm acoustic confirmation verbal
askAQuestion goal elaboration verbal
implicitConfirm goal confirmation+elaboration verbal
presentVerbal goal confirmation verbal
presentMM goal confirmation multimodal
167
Computational Linguistics Volume 37, Number 1
Figure 5
User action frequencies from the cluster-based user simulation.
Figure 6
User action frequencies from the smoothed bigram user simulation.
3.5.3 Evaluation of User Simulations. Several metrics have been proposed to evaluate
user simulations (e.g., Scheffler and Young 2001; Schatzmann, Georgila, and Young
2005; Ai and Litman 2006; Georgila, Henderson, and Lemon 2006; Williams 2007).
A good measure of dialogue similarity is based on the Kullback?Leibler (KL) diver-
168
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 7
Kullback?Leibler divergence scores for the different user simulations.
User simulations Baselines
smoothed cluster random majority
0.087 0.095 0.43 0.48
gence6 (as also used by Cuaya?huitl et al 2005; Jung et al 2009), which is defined as
follows:
DKL(P||Q) =
M
?
i=1
P(i) ? log P(i)
Q(i)
(3)
This metric measures the divergence between distributions P and Q in a context
with M responses. Ideally, the KL divergence between two similar distributions is close
to zero.
KL allows us to compare the raw probabilities as observed in the original data with
the probabilities generated by our user simulation models. We then compare the KL
results of the cluster-based and the smoothed user simulation against a random model
and a majority baseline (see Table 7). The random model is constructed by assigning
equal frequency to all four actions, whereas the majority baseline always predicts the
most frequent action in one context. The comparison against the random baseline tests
the hypothesis that our user simulations are more consistent with the observed data
than random behavior. The majority baseline represents the hypothesis that our user
simulation explores a significantly wider range of behavior than the most frequent user
action.
The user simulation models have a small divergence from the original data suggest-
ing that they are good simulations for training and testing policies. The smoothed and
the cluster-based model gain on average five times lower KL scores than the baselines.
We therefore conclude that both simulations show consistent (i.e., better than random)
as well as varying (i.e., better than the majority decision) behavior.
As mentioned previously, we want user simulations for policy training to allow
more exploration, whereas for testing we want user simulations which are more realis-
tic. We therefore choose to test with the smoothed model because its low KL score shows
that it is closest to the data, and we use the cluster-based simulation for training.
Note that the KL divergence only measures consistency with respect to specific
dialogue contexts. However, user simulations also need to be coherent with respect
to the dialogue history and the current user goal. We therefore model the user?s goal
(i.e., the song s/he is looking for) similar to ?agenda-based user models?(Schatzmann
et al 2007; Schatzmann, Thomson, and Young 2007b). The user goal corresponds to
a database entry, which is randomly chosen in the beginning of each dialogue. Every
time the user simulation generates a speech act, the corresponding value is chosen from
the goal record, dependent on the slot value the system was asking for.
6 Also known as information divergence, information gain, or relative entropy.
169
Computational Linguistics Volume 37, Number 1
3.6 Data-Driven Reward Modeling
The reward function defines the goal of the overall dialogue. For example, if it is most
important for the dialogue to be efficient, the function penalizes dialogue length, while
rewarding task success. In most previous work the reward function is manually set,
which makes it ?the most hand-crafted aspect? of RL (Paek 2006). For example, Williams
and Young (2007) use +10 points for task completion and ?1 point per turn, but there
is no empirical basis for this specific ratio. In contrast, we learn the reward model
from data, using a modified version of the PARADISE framework (Walker, Kamm, and
Litman 2000), following pioneering work by Walker, Fromer, and Narayanan (1998). In
PARADISE multiple linear regression is used to build a predictive model of subjective
user ratings (from questionnaires) from objective dialogue performance measures (such
as dialogue length). The subjective measure that we wish to optimize for our application
is Task Ease, a variable obtained by taking the average of two questions in the question-
naire.7 We use PARADISE to predict Task Ease from various input variables, via stepwise
regression. The chosen model comprises dialogue length in turns, task completion (as
manually annotated in the WOZ data), and the multimodal user score from the user
questionnaire, as shown in Equation (4) (R2 = .144,R2adjusted = .123).
TaskEase = ? 20.2 ? dialogueLength +
11.8 ? taskCompletion + 8.7 ? multimodalScore; (4)
This equation is used to calculate the overall reward for the information acquisition
phase. Task completion is calculated on-line during learning, penalizing all slots which
are filled but not confirmed. Slots that are filled but not confirmed have a 30% chance
of being incorrect according to the noise model (see Section 3.4). For the information
presentation phase, we compute a local reward. We relate the multimodal score (a
variable obtained by taking the average of four questions)8 to the number of items
presented (DB) for each modality, using curve fitting. In contrast to linear regression,
curve fitting does not assume a linear inductive bias, but it selects the most likely model
(given the data points) by function interpolation. The resulting models are shown in
Figure 7. The reward for multimodal presentation is a quadratic function that assigns a
maximal score to a strategy displaying 14.8 items (curve inflection point). The reward
for verbal presentation is a linear function assigning negative scores to all presented
items ? 4. The reward functions for information presentation intersect at no. items = 3.
A comprehensive evaluation of this reward function can be found in Section 6.2.
3.7 State Space Discretization
We use linear function approximation in order to learn with large state-action spaces.
Linear function approximation learns linear estimates for expected reward values of
actions in states represented as feature vectors. This is inconsistent with the idea of non-
linear reward functions (as introduced in the previous section). We therefore quantize
the state space for information presentation. We partition the database feature into three
7 ?The task was easy to solve?, ?I had no problems finding the information I wanted.?
8 ?I liked the combination of information being displayed on the screen and presented verbally?,
?Switching between modes did not distract me?, ?The displayed lists and tables contained on average the
right amount of information?, ?The information presented verbally was easy to remember.?
170
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Figure 7
Evaluation functions relating number of items presented in different modalities to multimodal
score.
bins, taking the first intersection point between verbal and multimodal reward and the
turning point of the multimodal function as discretization boundaries. Previous work
on learning with large databases commonly quantizes the database feature in order
to learn with large state spaces using manual heuristics (e.g., Levin, Pieraccini, and
Eckert 2000; Heeman 2007). Our quantization technique is more principled as it reflects
user preferences for multi-modal output. Furthermore, in previous work database items
were not only quantized in the state-space, but also in the reward function, resulting in a
direct mapping between quantized retrieved items and discrete reward values, whereas
our reward function still operates on the continuous values. In addition, the decision of
when to present a list (information acquisition phase) is still based on continuous DB
values. In future work we plan to engineer new state features in order to learn with
non-linear rewards while the state space is still continuous. A continuous representation
of the state space allows learning of more fine-grained local trade-offs between the
parameters, as demonstrated by Rieser and Lemon (2008a).
4. Training and Testing the Learned Policies in Simulation
We now train and test the multimodal presentation strategies by interacting with the
simulated learning environment. For the following RL experiments we used the REALL-
DUDE toolkit of Lemon et al (2006). The SHARSHA algorithm is employed for train-
ing, which adds hierarchical structure to the well known SARSA algorithm (Shapiro and
Langley 2002). The policy is trained with the cluster-based user simulation over 180k
system cycles, which results in about 20k simulated dialogues. In total, the learned strat-
egy has 371 distinct state-action pairs as presented in the look-up table in Appendix E.
We test the RL-based and the SL wizard baseline policy, as listed in Listing 1,
which allows us to measure relative improvement over the training data. We run 500
171
Computational Linguistics Volume 37, Number 1
test dialogues with the smoothed user simulation, as described in Section 3.5.2, so that
we are not training and testing on the same simulation. We then compare quantitative
dialogue measures by performing a paired t-test. In particular, we compare mean
values of the final rewards, number of filled and confirmed slots, dialogue length,
and items presented multimodally (MM items) and items presented verbally (verbal
items). RL performs significantly better (p < .001) than the baseline strategy. The only
non-significant difference is the number of items presented verbally, where both the RL
and the average wizard strategy settled on a threshold of fewer than four items. The
mean performance measures for simulation-based testing are shown in Table 8.
The major strength of the RL policy is that it learns to keep the dialogues reasonably
short (on average 5.9 system turns for RL versus 8.4 turns for SL wizard) by presenting
lists as soon as the number of retrieved items is within tolerance range for the respec-
tive modality (as reflected in the reward function). The SL strategy in contrast has
not learned the right timing nor an upper bound for displaying items on the screen
(note that the distribution for MM items is highly positively skewed with a maximum of
283 items being displayed). See example dialogues in Appendix C.
The results show that simulation-based RL with an environment bootstrapped
from WOZ data allows learning of robust strategies which significantly outperform the
strategies learned by SL from the original data set. This confirms our hypothesis that
simulation-based RL allows us to find optimal policies which are not easily discoverable
(by Supervised Learning) in the original data.
Furthermore, RL allows us to provide additional information about user prefer-
ences in the reward function, whereas SL simply mimics the data. In addition, RL
is based on delayed rewards, namely, the optimization of a final goal. For dialogue
systems we often have measures indicating how successful and/or satisfying the overall
performance of a strategy was, but it is hard to tell how exactly things should have been
done in a specific situation. This is what makes RL specifically attractive for dialogue
strategy learning. In the next section we test the learned strategy with real users.
5. Tests with Real Users
5.1 Experimental Design
For the user tests the RL policy is ported to a working ISU-based dialogue system via
table look-up (see table in Appendix E) , which indicates the action with the highest
expected reward for each state (cf. Singh et al 2002). The supervised average wizard
baseline is implemented using standard threshold-based update rules. The experimen-
Table 8
Comparison of results for SL wizard and RL-based strategies in simulation.
Measure SL wizard baseline RL Strategy
avg. turns 8.42(?3.04) 5.9(?2.4)***
avg. speech items 1.04(?.2) 1.1(?.3)
avg. MM items 61.37(?82.5) 11.2(?2.4)***
avg. reward ?1,741.3(?566.2) 44.06(?51.5)***
*** Significant difference between SL and RL at p < .001 (with standard deviation ?).
172
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 9
Comparison of mean user ratings for SL wizard baseline and RL policies (with standard
deviation ?).
Measure Wizard Strategy RL Strategy
Task Ease 4.78 (?1.84) 5.51 (?1.44)***
Timing 4.42 (?1.84) 5.36 (?1.46)***
MM Presentation 4.57 (?1.87) 5.32 (?1.62)***
Verbal Presentation 4.94 (?1.52) 5.55 (?1.38)***
Future Use 3.86 (?1.44) 4.68 (?1.39)***
*** Statistical significance at p < .001.
tal conditions are similar to the WOZ study, that is, we ask the users to solve similar
tasks, and use similar questionnaires.9 Furthermore, we decided to use typed user
input rather than ASR. The use of text input allows us to target the experiments to the
dialogue management decisions on presentation strategies, and prevents ASR quality
from interfering with the experimental results, especially because subjective user scores
are highly sensitive to ASR noise (Hajdinjak and Mihelic 2006). Both RL and SL wizard
policies are trained to handle noisy conditions, so that they usually confirm user input,
which makes dialogues longer but more reliable. The lack of noise in this experiment
means that confirmation happens more than is strictly required (although there are
still text input spelling mistakes), but the information presentation decisions are not
affected.
Seventeen subjects (8 women, 9 men) are given a set of 12 (6?2) predefined, ordered
tasks, which they solve by interaction with the RL-based and the SL-based average
wizard system in a cyclic order. As a secondary task users are asked to count certain
objects in a driving simulation. In total, 204 dialogues with 1,115 turns are gathered in
this set-up. See also Rieser (2008).
5.2 Results
In general, the users rate the RL-based policy significantly higher (p < .001) than the
SL-based average wizard policy. The results from a Wilcoxon Signed Ranks Test on
the user questionnaire data (see Table 9) show significantly improved Task Ease, better
presentation timing, more agreeable verbal and multimodal presentation, and that more
users would use the RL-based system in the future (Future Use). All the observed
differences have a medium effects size (r ? |.3|).
We also observe that female participants clearly favor the RL-based strategy,
whereas the ratings by male participants are more indifferent. Similar gender effects
are also reported by other studies on multimodal output presentation (e.g., Foster and
Oberlander 2006; Jokinen and Hurtig 2006).
Furthermore, we compare objective dialogue performance measures. The dialogues
of the RL strategy are significantly shorter (p < .005), while fewer items are dis-
played (p < .001), and the help function is used significantly less (p < .003). The mean
9 The WOZ study was performed in German, whereas the user tests are performed in English. Therefore, a
different database had to be used and task sets and user questionnaires had to be translated.
173
Computational Linguistics Volume 37, Number 1
performance measures for testing with real users are shown in Table 10. Also see
example dialogues in Appendix D. However, there is no significant difference for the
performance of the secondary driving task.
6. Meta Evaluation
We introduce a final phase where we meta-evaluate the whole framework. This final
step is necessary because WOZ experiments only simulate HCI. We therefore need to
show that a strategy bootstrapped from WOZ data indeed transfers to real HCI. We first
show that the results for simulated and real interaction are compatible (Section 6.1). We
also meta-evaluate the reward function, showing that it is a stable, accurate estimate for
real users? preferences (Section 6.2).
6.1 Transfer Between Simulated and Real Environments
We first test whether the results obtained in simulation transfer to tests with real users,
following Lemon, Georgila, and Henderson (2006). We evaluate the quality of the simu-
lated learning environment by directly comparing the dialogue performance measures
between simulated and real interaction. This comparison enables us to make claims
regarding whether a policy which is ?bootstrapped? from WOZ data is transferable to
real HCI. We first evaluate whether objective dialogue measures are transferable, using
a paired t-test, comparing overall mean performance.
For the RL policy there is no statistical difference in overall performance (reward),
dialogue length (turns), and the number of presented items (verbal and multimodal
items) between simulated and real interaction (see Figure 8). This fact (that the per-
formances are not different) indicates that the learned strategy transfers well to real
settings. For the SL wizard policy the dialogue length for real users is significantly
(t(101) = 5.5, p < .001, r = .48) shorter than in simulation. We conclude from an er-
ror analysis that this length difference is mainly due to the fact that real users tend
to provide the most ?informative? slot value (i.e., the most specific value from the
experimental task description) right at the beginning of the task (and therefore more
efficiently contribute to solve the task), whereas simulated users use a default ordering
of slot values and most of the time they provide the slot value that the system was asking
for (provide info). This difference becomes more prominent for the SL wizard policy
than for the RL-based policy, as the SL wizard policy in general asks more questions
before presenting the information. In future work the user simulation therefore should
learn optimal slot ordering.
Table 10
Comparison of results for SL average wizard and RL-based strategies with real users.
Measure Wizard Strategy RL Strategy
avg. turns 5.86(?3.2) 5.07(?2.9)***
avg. speech items 1.29(?.4) 1.2(?.4)
avg. MM items 52.2(?68.5) 8.73(?4.4)***
avg. reward ?628.2(?178.6) 37.62(?60.7)***
*** Significant difference between SL and RL at p < .001 (with standard deviation ?).
174
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Figure 8
Graph comparison of objective measures. SLs = SL policy in simulation; SLr = SL policy with
real users; RLs = RL policy in simulation; RLr = RL policy with real users.
6.2 Evaluation of the Learned Reward Function
We propose a new method for meta-evaluation of the reward (or ?objective?) function.
One major advantage of RL-based dialogue strategy development is that the dialogue
strategy can be automatically trained and evaluated using the same objective function
(Walker 2005). Despite its central aspect for RL, quality assurance for objective functions
has received little attention so far. In fact, as noted in Section 3.6, the reward function is
one of the most hand-coded aspects of RL (Paek 2006).
Here, we bring together two strands of research for evaluating the reward function:
One strand uses Reinforcement Learning to automatically optimize dialogue strategies
(e.g., Singh et al 2002; Henderson, Lemon, and Georgila 2008; Rieser and Lemon 2008a,
2008b); the other focuses on automatic evaluation of dialogue strategies (e.g., the PAR-
ADISE framework [Walker et al 1997]), and meta-evaluation of dialogue metrics (e.g.,
Engelbrecht and Mo?ler 2007; Paek 2007). Clearly, automatic optimization and evaluation
of dialogue policies, as well as quality control of the objective function, are closely inter-
related problems: How can we make sure that we optimize a system according to real
users? preferences?
In Section 3.6 we constructed a data-driven objective function using the PARADISE
framework, and used it for automatic dialogue strategy optimization, following work
by Walker, Former, and Narayanan (1998). However, it is not clear how reliable such
a predictive model is, that is, whether it indeed estimates real user preferences. The
175
Computational Linguistics Volume 37, Number 1
models obtained with PARADISE often fit the data poorly (Engelbrecht and Mo?ller 2007).
It is also not clear how general they are across different systems and user groups
(Walker, Kamm, and Litman 2000; Paek 2007). Furthermore, it is not clear how they per-
form when being used for automatic strategy optimization within the RL framework.
In the following we evaluate different aspects of the reward function. In Sec-
tion 6.2.1 we test the model stability in a test?retest comparison across different user
populations and data sets. In Section 6.2.2 we measure its prediction accuracy.
6.2.1 Reward Model Stability. We first test the reward model?s stability by re-constructing
it from the data gathered in the real user tests (see Section 5) and comparing it to the
original model constructed from the WOZ data. By replicating the regression model
on different data sets we test whether the automatic estimate of Task Ease general-
izes beyond the conditions and assumptions of a particular experimental design. The
resulting models are shown in Equations (5)?(7), where TaskEaseWOZ is the regression
model obtained from the WOZ data,10 TaskEaseSL is obtained from the user test data
running the supervised average wizard policy, and TaskEaseRL is obtained from the
user test data running the RL-based policy. They all reflect the same trends: Longer
dialogues (measured in turns) result in a lower Task Ease, whereas a good performance
in the multimodal information presentation phase (multimodal score) will positively
influence Task Ease. For the user tests almost all the tasks were completed; therefore
task completion was only chosen to be a predictive factor for the WOZ model.
TaskEaseWOZ = 1.58 + .12? taskCompl + .09 ? mmScore ? .20? dialogueLength (5)
TaskEaseSL = 3.50 + .54? mmScore ? .34 ? dialogueLength (6)
TaskEaseRL = 3.80 + .49 ? mmScore ? .36? dialogueLength (7)
To evaluate the obtained regression models we use two measures: how well they
fit the data and how close the functions are to each other (model replicability). Both
are measured using goodness-of-fit R2. For the WOZ model the data fit was rather
low (R2WOZ = .123),
11 whereas for the models obtained from the user tests the fit has
improved (R2RL = .48, and R
2
SL = .55).
Next, we compare how well the models from different data sets fit each other.
Although the models obtained from the user test data show almost perfect overlap
(R2 = .98), the (reduced) WOZ model differs (R2 = .22) in the sense that it assigns less
weight to dialogue length and the multimodal presentation score, and more weight
is assigned to task completion. Task completion did not play a role for the user tests,
as mentioned earlier. This shows that multimodal presentation and dialogue length
become even more important once the tasks are being completed. Overall, then, the
data-driven reward model is relatively stable across the different data sets (WOZ, real
users with the SL policy, and real users using the RL policy).
6.2.2 Reward Model Performance: Prediction Accuracy. We now investigate how well these
reward models generalize by testing their prediction accuracy. Previous research evalu-
10 In contrast to the model in Equation (4) we now include the constant in the regression.
11 For R2 we use the adjusted values.
176
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 11
Prediction accuracy for models within (1?3) and across (4?5) data sets.
ID train test RMSE % error
1 WOZ WOZ 0.82 16.42
2 SL SL 1.27 18.14
3 RL RL 1.06 15.14
4 RL SL 1.23 17.57
5 SL RL 1.03 14.71
ated two aspects: how well a given objective function/reward model is able to predict
unseen scores from the original system (Engelbrecht and Mo?ller 2007), and how well it
is able to predict unseen scores of a new/different system (Walker, Kamm, and Litman
2000). We evaluate these two aspects as well, the only difference is that we use the
Root Mean Standard Error (RMSE) instead of R2 for measuring the model?s prediction
accuracy. The RMSE is a frequently used measure of the differences between values
predicted by a model or an estimator and the values actually observed. It is defined over
[0,?], where 0 indicates perfect overlap. The maximum RMSE possible (= worst case)
in our set-up is 7 for SL/RL and 5 for WOZ. In order to present results from different
scales we also report the percentage of the RMSE of the maximum error (% error). RMSE
is (we argue) more robust for small data sets.12
R2 = 1?
?n
i=1(yi ? ?yi)2
?n
i=1(y ? y)
(8)
RMSE =
?
?
?
?
1
n
n
?
i=1
(yi ? ?yi)2 (9)
First, we measure the predictive power of our models within the same data set using
10-fold cross validation, and then across the different systems by testing models trained
on one system to predict perceived Task Ease scores for another system, following a
method introduced by Walker, Kamm, and Litman (2000).
The results for comparing the RMSE for training and testing within data sets (ID
1-3) and across data sets (ID 4?5) are shown in Table 11. RMSE measures the average of
the square of the ?error.? As such, lower RMSE values are better. The contrary is true
for R2, where ?1? indicates perfect overlap between two functions.
The results show that predictions according to PARADISE can lead to accurate test
results despite the low data fit. Whereas for the regression model obtained from the
WOZ data the fit was 10 times lower than for SL/RL, the prediction performance is
comparably good (see Table 11, ID 1?3). The models also generalize well across systems
(see Table 11, ID 4?5).
Table 12 visualizes the results (ID 1?3): Mean values for predicted and for true
ratings are plotted per turn (see Engelbrecht and Mo?ller 2007). The top two graphs in the
12 In particular, we argue that, by correcting for variance, R2 can lead to artificially good results when using
small tests sets (which typically vary more) and is sensitive to outliers (see Equation (8)). RMSE instead
measures the (root) mean difference between actual and predicted values (see Equation (9)).
177
Computational Linguistics Volume 37, Number 1
Table 12
Average Task Ease ratings for dialogues of different length (in turns); the solid lines are the true
ratings and the dashed line the predicted values; from top: RL, SL wizard, WOZ data.
178
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
table show that the predicted mean values are fairly accurate for the SL and RL objective
functions. The graph at the bottom indicates that the predictions are less accurate for the
WOZ data, especially for low numbers of turns. This seems to contradict the previous
results in Table 11, which show low error rates for the WOZ data. However, this is due
to the fact that most of the observations in the WOZ data set are in the region where the
predictions are accurate (i.e., most of the dialogues in the WOZ data are over 14 turns
long, where the curves converge).
We conclude that, according to our measures, an objective function obtained from
WOZ data is a valid first estimate of real users? preferences. Despite a low fit to the initial
data, the objective function obtained from WOZ data makes accurate and useful predic-
tions for automatic dialogue evaluation/reward. The models obtained from the tests
with a real system follow the same trends, but can be seen as more reliable estimates
of the objective function in this domain. In future work we will explore incrementally
training a system according to improved representations of real user preferences, for
example, gathered on-line from a deployed spoken dialogue system.
7. Conclusion
We have presented a new data-driven methodology for simulation-based dialogue
strategy learning. It allows us to address several problems in the field of automatic
optimization of dialogue strategies: learning effective dialogue strategies when no ini-
tial data or system exists, and determining a data-driven reward function. We learned
optimal strategies by interaction with a simulated environment which is bootstrapped
from a small amount of Wizard-of-Oz data, and we evaluated the result with real users.
The use of WOZ data allows us to develop optimal strategies for domains where no
working prototype is available. In contrast to previous work, the developed simulations
are largely data-driven and the reward function reflects real user preferences.
We compare the Reinforcement Learning?based strategy against a supervised strat-
egy which mimics the (human) wizards? policies from the original data. This compari-
son allows us to measure relative improvement over the training data. Our results show
that RL significantly outperforms the average wizard strategy in simulation as well as in
interactions with real users. The RL-based policy gains on average 50 times more reward
when tested in simulation, and almost 18 times more reward when interacting with
real users. The human users also subjectively rate the RL-based policy on average 10%
higher, and 49% higher for Task Ease. We also show that results obtained in simulation
are comparable to results for real users and we also evaluate the reward function. We
conclude that a strategy trained from WOZ data via our bootstrapping method works
well for real HCI.
Regarding scaling up such methods to larger databases, we would still quantize
the number of database hits into High, Medium, and Low (more hits would be added
to the ?High? bin). In fact, wizards seemed to treat everything over 60 database hits
equivalently (see Section 3.7). For learning, the state space grows linearly with the
number of possible database hits. Techniques such as linear function approximation are
useful for handling the resulting larger state spaces (Henderson, Lemon, and Georgila
2008).
In ongoing work we apply similar techniques to statistical planning for Natural
Language Generation in spoken dialogue (Lemon 2011, 2008; Janarthanam and Lemon
2008; Rieser and Lemon 2009b; Rieser, Lemon, and Liu 2010; Janarthanam and Lemon
2010), see the EC FP7 CLASSiC project: www.classic-project.org.
179
Computational Linguistics Volume 37, Number 1
Appendix A: Measuring Inter-Annotator Agreement on User Act Using the
? Statistics
Cohen?s Kappa (often simply called kappa, ?) is a measure of agreement between the
two individuals. We use this measure to assess inter-annotator agreement on user act,
following Carletta et al (1997). Detailed results are in the table.
Table 1
Inter-annotator agreement on user act with ? = .632.
annotator1 annotator2
user reply add change others repeat y/n total
add 58 5 2 1 0 66
change 4 11 0 0 0 15
others 9 5 21 1 0 36
repeat 9 0 7 9 0 25
y/n 2 0 2 1 30 35
total 82 21 32 12 30 177
180
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Appendix B: Wizard-of-Oz Example Dialogues
Table 1
Example dialogue from the SAMMIE WOZ corpus (mixed wizard strategy).
s1 User: Bitte suche das Lied ,Layla?.
Transl.: Search for the song ?Layla? please.
SA* user: add -> song-title(Layla)
Wizard: Wir haben das Lied gefunden. Es existiert 142 mal in verschiedenen
Ausfu?hrungen
Transl.: The song is retrieved. There are 142 different versions.
mmAction: [ Table with 142 entries is shown on the screen. ]
SA
wizard:
presentMM -> song-title (db:142)
s2 User: Bitte suche das Lied on dem Komponisten ,Eric Clapton?.
Transl.: Search for the song by the composer ?Eric Clapton? please.
mm
Action:
[ User does not react to what is displayed. ]
SA user: change -> artist(?Eric Clapton?)
Wizard: Es gibt das Lied von ,Eric Clapton? oder von ?Derek & the Dominos?.
Transl.: The song exists by ?Eric Clapton? or by ?Derek & the Dominos?.
SA
wizard:
presentVerbal -> song-title, artist (db:2)
s3 User: Bitte spiele das Lied von ,Eric Clapton Unplugged? ab.
Transl.: Play the song from ?Eric Clapton Unplugged? please.
SA user: repeat ->album(?Eric Clapton Unplugged?)
Wizard: Ich spiele das Lied jetzt ab.
Transl.: OK I am going to play the song now.
SA
wizard:
feedback/others
mmAction: [ music ]
user
ratings
taskEase: 2, timing:2, mmPresent.: 1 , verbalPresent: 3, futureUse:3
* SA = Speech Act
Comments:
In this example the wizard uses a mixed strategy presenting information multimodally in turn 1
and verbally in turn 2. The first time 142 items are presented multimodally by verbally reporting
the number of retrieved items while displaying them on the screen. The database used in the WOZ
experiments contained over a million songs and is searched with partial string match. Therefore
the retrieved items for ?Layla? can be as high as 142. The user does not directly react to what is
shown on the screen (either by referring to it or clicking), but continues providing information
until the wizard (verbally) presents two alternatives, where the user indirectly chooses one
by providing an album name which also disambiguates the artist. The user rankings from the
questionnaire are about average.
181
Computational Linguistics Volume 37, Number 1
Table 2
Example dialogue from the SAMMIE WOZ corpus (wizard not starting the presentation phase
early enough).
s1 User: Also, ich suche das Lied ,Smile?.
Transl.: Well, I am searching for the song ?Smile?.
SA user: add -> song-title(Smile)
Wizard: Welches Lied suchst du?
Transl.: What song are you searching for?
comment: [ Song name was deleted ]
SA
wizard:
reject(song-title)
s2 User: Smile.
Transl.: Smile.
SA user: repeat -> song-title(Smile)
Wizard: Du suchst das Lied ,Smile??
Transl.: Are you searching for the song ?Smile??
SA
wizard:
explConf(song-title)
s3 User: Ja, Smile ja.
Transl.: Yes, Smile yes.
SA user: y-answer -> song-title(Smile)
Wizard: Ich suche es... Ich habe vier Lieder mit dem einfachen Titel ,Smile?.
Transl.: I am searching... I found four songs with only ?Smile? as the title.
SA
wizard:
presentVerbal(song-title)
s4 User: Ja denn, das Lied bitte anzeigen.
Transl.: Very well then, why don?t you show it?
SA user: others
Wizard: OK.
Transl.: OK.
SA
wizard:
presentMM(song-title)
user
ratings
taskEase: 2, timing: 2, mmPresent.: 5, verbalPresent: 3, futureUse:3
Comments:
In this example the user is asking to see a short list on the screen (after resolving some under-
standing errors induced by the deletion tool), which is indicating that (a) the wizard did not enter
the presentation phase early enough, and (b) that for the user four items seems to be a reasonable
number to display. The user scores reflect this fact: The presentation mode got a maximal score
(which will influence learning towards showing short lists), whereas the task ease got a lower
score (task ease is used as an overall reward, i.e., this ?tells? the learner to enter the presentation
mode earlier).
182
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Appendix C: Example Dialogues from Simulated Interaction
Table 1
Example dialogue with the SL policy operating in the simulated environment.
s1 SL policy: greet (db:438)
sim.User: add->genre(Rock)
state: filled [slot1], confirmed [], db:142
s2 SL policy: implConfAskASlot -> genre, artist)
sim.User: repeat->genre(Rock)
state: filled [slot1], confirmed[slot1], db:142
s3 SL policy: present[mm] ->genre
sim.User: change->artist(Jimi Hendrix)
state: filled [slot1,slot2], confirmed [slot1], db:4
s4 SL policy: implConfAskASlot->artist, album
sim.User: add->album(Experience Hendrix: The Best of Jimi Hendrix)
state: filled [slot1,slot2,slot3], confirmed [slot1,slot2], db:4
s5 SL policy: implConfAskASlot->album, song-title
sim.User: add->song-title(Voodoo Child)
state: filled [slot1,slot2,slot3,slot4], confirmed
[slot1,slot2,slot3], db:1
s6 SL policy: explicitConfirm->song-title
sim.User: repeat->song-title(Voodoo Child)
state: filled [slot1,slot2,slot3,slot4], confirmed
[slot1,slot2,slot3,slot4], db:1
s7 SL policy: present[verbal] ->genre, artist, album, song-title
sim.User: yes-answer->yes(yes)
state filled [slot1,slot2,slot3,slot4], confirmed
[slot1,slot2,slot3,slot4], db:1
reward: lengthPenalty : ?141.4, complVal : 118, presentVal : 8.7? (4.33? 2151.23)
finalReward:-18701.43
Comments:
This example shows a dialogue where the SL policy is operating in the simulated environment.
Presentation timing and amount of presented information are not ideal (and therefore leading to
quite a long dialogue): In turn 2 the system presents a list of 238 items, whereas when only having
four alternatives (from turn 3 onwards) it fails to show a list.
183
Computational Linguistics Volume 37, Number 1
Table 2
Example dialogue with the RL policy operating in the simulated environment (mixed strategy).
s1 RL policy: greet (db: 438)
sim.User: add->artist(Nirvana)
state: filled [slot2], confirmed [], db:26
s2 RL policy: implConfAskASlot -> artist, album)
sim.User: add->album(MTV Unplugged)
state: filled [slot2,slot3], confirmed [slot2], db:14
s3 RL policy: present[mm] ->artist, album
sim.User: click->song-title(On a Plain),genre(Alternative)*
state: filled [slot1,slot2,slot3,slot4], confirmed
[slot2,slot3], db:1
s4 RL policy: present[verbal] ->artist, album, song-title
sim.User: yes-answer->yes(yes)
state: filled [slot1,slot2,slot3,slot4], confirmed
[slot1,slot2,slot3,slot4], db:1
reward: lengthPenalty : ?80.8, complVal : 118, presentVal : 8.7? (4.33 + 7.23)
finalReward:140.3
Comments:
This example shows the RL policy interacting in the simulated environment. The system follows
a mixed strategy: first presenting a list of 14 items on the screen and then presenting a unique
item verbally.
* the user acts add after a multimodal presentation are equivalent to clicking
Table 3
Example dialogue with the RL policy operating in the simulated environment: presenting
alternatives.
s1 RL policy: greet (db: 438)
sim.User: add->artist(Bob Dylan)
state: filled [slot2], confirmed [], db:56
s2 RL policy: implConf(artist)AskASlot(genre)
sim.User: change->song-title(Hurricane)
state: filled [slot2,slot4], confirmed [slot2], db:2
s3 RL policy: present[verbal] ->album1,album2
sim.User: add->album(Desire)
state: filled [slot2, slot3, slot4], confirmed [slot2,slot4], db:1
s4 RL policy: present[verbal] ->artist, album, song-title
sim.User: yes-answer->yes(yes)
state: filled [slot2, slot3, slot4], confirmed
[slot2, slot3, slot4], db:1
reward: lengthPenalty : ?80.8, complVal : 118, presentVal : 8.7? (4.327 + 3.395)
finalReward: 104.38
Comments:
This example shows the RL policy interacting in the simulated environment. The system presents
two alternatives and a single item using only verbal output.
184
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Appendix D: Example Dialogues from User Testing
Table 1
Example dialogue with the SL policy operating with real users.
s1 SL system: Hi, this is iTalk your talkative music player. How may I help you today?
SA system: greet (db: 438)
User: i would like to listen to a blues song
SA user: add->genre(Blues)
state: filled [slot1], confirmed [], db:175
s2 SL system: There are 175 items matching your query. A list of possible candidates are
shown on the screen.
SA system: presentMM-> genre (db:175.0)
User: ?
SA user: nothing (time out)
state: filled [slot1], confirmed [], db:175
s3 SL system: A Blues song. By which artist?
SA system: implConf(genre), askASlot(artist)
User: eric clapton
SA user: add->artist(?Eric Clapton) (db:25.0)
state: filled [slot1, slot2], confirmed [slot1], db:25
s4 SL system: Searching for music by Eric Clapton. Which album?
SA system: implConf(?Eric Clapton?), askASlot(album)
User: mtv unplugged
SA user: add->album(?MTV unplugged?)
state: filled [slot1, slot2, slot3], confirmed [slot1, slot2], db:13
s5 SL system: A song from the album MTV unplugged. What is the title of the song?
SA system: implConf(album), askASlot(song-title)
User: what do you have
SA user: request-help
state: filled [slot1, slot2, slot3], confirmed [slot1, slot2], db:13
s6 SL system: You have several options. For example: Layla , Nobody Knows You When
You Are Down and Out , Lonely Stranger , Old Love , and San Francisco Bay
Blues. Do you want to hear more?
SA system: give-help
User: no layla please
SA user: add->song-title(Layla)
state: filled [slot1, slot2, slot3, slot4], confirmed
[slot1, slot2, slot3], db:1
s7 SL system: You will now hear Layla by Eric Clapton from the album MTV unplugged.
Are you happy with this option?
SA system: presentVerbal->genre,artist,album,song-title
User: yes
SA user: yes-answer
state: filled [slot1, slot2, slot3, slot4], confirmed
[slot1, slot2, slot3, slot4], db:1
reward: lengthPenalty : ?141.4, complVal : 118, presentVal : 8.7? (4.33? 7.63)
finalReward: -6577.03
user ratings taskEase:2, timing:2, mmPresent:1, verbalPresent:2, futureUse:2
Comments:
This example shows the SL policy applied with real users. In turn 2 the user ignores the 175 items displayed,
then the system does not display the 13 retrieved items. The user asks for help.
185
Computational Linguistics Volume 37, Number 1
Table 2
Example dialogue with the RL policy operating with real users: multimodal and verbal
presentation.
s1 RL system: Hi. this is iTalk your talkative music player. How may I help you today?
SA system greet (db:438)
User: play me some blues
SA user: add->genre(Blues) (db:175)
state: filled [slot1], confirmed [], db:175
s2 RL system: Blues for type of music. By which artist?
SA system: implConf(genre),askASlot(artist)
User: from the album desire
SA user: change->album(Desire)(db:10)
state: filled [slot1, slot3], confirmed [slot1], db:10
s3 RL system: There are 10 items matching your query. A list of possible candidates are
shown on the screen.
SA system: presentMM->genre,album
User: [clicks]
SA user: click->genre,song-title
state: filled [slot1, slot2, slot3, slot4], confirmed [slot1, slot3], db:1
s4 RL system: You selected an item on the screen. Please wait a few moments. [. . . ] You will
now hear Mozambique by Bob Dylan from the album Desire. Are you happy
with this option
SA presentVerbal->genre,album,artist,song-title
User: yes
SA yes-answer
state: filled [slot1, slot2, slot3, slot4], confirmed
[slot1, slot2, slot3, slot4], db:1
reward: lengthPenalty : ?60.6, complVal : 118, presentVal : 8.7? (4.33 + 6.57)
finalReward: 152.16
user ratings taskEase:6, timing:7, mmPresent.:7 verbalPresent: 6 futureUse:6
Comments:
This example shows the RL policy interacting with real users. The system uses a mixed strategy, showing
multimodal output as soon as the number of retrieved items is below 15 (and therefore the reward will be
positive), and presenting one item verbally.
186
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Table 3
Example dialogue with the RL policy operating with real users: Verbal presentation of options.
s1 RL system: Hi. this is iTalk your talkative music player. How may I help you today?
SA system: greet (db:438)
User: i want to hear polly
SA user: add->song-title(Polly)
state: filled [slot4], confirmed [], db:2
s2 RL system: There are 2 results matching your query all by the artist Nirvana all from the
genre Alternative all having the song title Polly, but a different album. You
have the following options: Nevermind and MTV Unplugged. Which album
would you like?
SA system: presentVerbal->song-title,artist,genre
comment: systems fills unique slots.
User: mtv unplugged
SA user: add->album(?MTV unplugged?) (db:1)
state: filled [slot3, slot4], confirmed [song4], db:1
s3 RL system: There is only one result matching your query. Please wait a few moments.
[. . . ] You will now hear Polly by Nirvana from the album MTV unplugged.
Are you happy with this option ?
SA system: presentVerbal->song-title,artist,genre,album
User: yes
SA user: yes-answer
state: filled [slot1, slot2, slot3, slot4], confirmed
[slot1, slot2, slot3, slot4], db:1
reward: lengthPenalty : ?60.6, complVal : 118, presentVal : 8.7? (4.33 + 3.4)
finalReward: 124.6
user ratings taskEase:7, timing:7, mmPresent.:7 verbalPresent: 7 futureUse:6
Comments:
This example shows the RL policy interacting with real users. Two alternatives are presented verbally.
187
Computational Linguistics Volume 37, Number 1
Appendix E: Learned State Action Mappings
The table in this section presents the learned state action mappings, and is to be read
as follows. The first two columns constitute the state space. The first column shows the
slots that have been filled and/or confirmed. The slots are:
slot 1: genre
slot 2: artist
slot 3: album
slot 4: song title
The second column represents possible numbers of database hits. Note that the possible
number of items returned from the database is constrained by the structure of the task
(i.e., how combinations of different slots values constrain the search space).
The third column is the optimal action for that state. The ?x?s in the second column
denote the numbers of database hits that share the same optimal action (given the set
of filled and confirmed slots). Horizontal lines are drawn between sets of states with
different filled slots.
188
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
L
ea
rn
ed
S
ta
te
A
ct
io
n
M
ap
p
in
gs
st
at
e
sp
ac
e
sy
st
em
ac
ti
on
sl
ot
va
lu
es
n
o.
D
B
h
it
s
1
2
3
4
5
6
7
8
9
10
11
12
13
14
16
18
20
25
26
34
40
54
56
92
14
2
17
5
43
8
fi
ll
ed
[]
,c
on
fi
rm
ed
[]
x
as
kA
Sl
ot
fi
ll
ed
[s
lo
t1
],
co
n
fi
rm
ed
[]
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
],
co
n
fi
rm
ed
[s
lo
t1
]
x
x
x
as
kA
Sl
ot
fi
ll
ed
[s
lo
t2
],
co
n
fi
rm
ed
[]
x
x
x
x
x
x
im
p
lC
on
f
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t2
],
co
n
fi
rm
ed
[s
lo
t2
]
x
x
x
x
x
x
as
kA
Sl
ot
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t3
],
co
n
fi
rm
ed
[]
x
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t3
]
x
x
x
x
as
kA
Sl
ot
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t4
],
co
n
fi
rm
ed
[]
x
ex
p
lC
on
f
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
as
kA
Sl
ot
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
],
co
n
fi
rm
ed
[]
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
]
x
x
x
x
x
x
as
kA
Sl
ot
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
],
co
n
fi
rm
ed
[s
lo
t1
]
x
x
x
x
x
x
im
p
lC
on
f
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
],
co
n
fi
rm
ed
[s
lo
t2
]
x
x
x
x
x
x
im
p
lC
on
f
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t3
],
co
n
fi
rm
ed
[]
x
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t3
]
x
x
x
x
as
kA
Sl
ot
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
]
x
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t3
]
x
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
x
im
p
lC
on
f
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t4
]
x
as
kA
Sl
ot
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
]
x
im
p
lC
on
f
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
im
p
lC
on
f
x
x
x
p
re
se
n
tV
er
ba
l
189
Computational Linguistics Volume 37, Number 1
L
ea
rn
ed
S
ta
te
A
ct
io
n
M
ap
p
in
gs
st
at
e
sp
ac
e
sy
st
em
ac
ti
on
sl
ot
va
lu
es
n
o.
D
B
h
it
s
1
2
3
4
5
6
7
8
9
10
11
12
13
14
16
18
20
25
26
34
40
54
56
92
14
2
17
5
43
8
fi
ll
ed
[s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[]
x
x
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
]
x
x
as
kA
Sl
ot
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t2
]
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t3
]
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
x
x
ex
p
lC
on
f
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t4
]
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,
sl
ot
4]
,
co
n
fi
rm
ed
[s
lo
t2
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[]
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
]
x
x
x
as
kA
Sl
ot
x
x
x
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
]
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t3
]
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t1
]
x
x
x
x
x
x
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
]
x
x
x
im
p
lC
on
f
x
x
x
x
x
x
x
x
x
x
x
p
re
se
n
tM
M
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t2
]
x
x
x
x
x
x
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
],
co
n
fi
rm
ed
[s
lo
t3
]
x
x
x
x
x
x
x
x
x
x
x
x
x
x
im
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
x
im
p
lC
on
f
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t4
]
x
p
re
se
n
tM
M
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
p
re
se
n
tM
M
x
x
x
p
re
se
n
tV
er
ba
l
190
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
L
ea
rn
ed
S
ta
te
A
ct
io
n
M
ap
p
in
gs
st
at
e
sp
ac
e
sy
st
em
ac
ti
on
sl
ot
va
lu
es
n
o.
D
B
h
it
s
1
2
3
4
5
6
7
8
9
10
11
12
13
14
16
18
20
25
26
34
40
54
56
92
14
2
17
5
43
8
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t2
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t1
]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t1
,
sl
ot
2,
sl
ot
3,
sl
ot
4]
,
co
n
fi
rm
ed
[s
lo
t2
,
sl
ot
3,
sl
ot
4]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t3
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t2
]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
,s
lo
t4
]
x
p
re
se
n
tV
er
ba
l
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t3
]
x
ex
p
lC
on
f
fi
ll
ed
[s
lo
t1
,s
lo
t2
,s
lo
t3
,s
lo
t4
],
co
n
fi
rm
ed
[s
lo
t4
]
x
ex
p
lC
on
f
191
Computational Linguistics Volume 37, Number 1
Acknowledgments
We would like to thank Professor Manfred
Pinkal for discussing this work. We would
also like to thank the anonymous reviewers
for their useful comments. The research
leading to these results has received funding
from the European Community?s Seventh
Framework Programme (FP7, 2007?2013)
under grant agreement number 216594
(?Computational Learning in Adaptive
Systems for Spoken Conversation,? CLASSiC
project: www.classic-project.org), the EC
FP6 project ?TALK: Talk and Look, Tools for
Ambient Linguistic Knowledge? (IST 507802,
www.talk-project.org), from EPSRC grant
numbers EP/E019501/1 and EP/G069840/1,
and from the International Research Training
Group in Language Technology and
Cognitive Systems, Saarland University.
References
Ai, Hua and Diane Litman. 2006. Comparing
real-real, simulated-simulated, and
simulated-real spoken dialogue corpora.
In Proceedings of the AAAI Workshop on
Statistical and Empirical Approaches for
Spoken Dialogue Systems, Boston, MA.
Ai, Hua, Joel Tetreault, and Diane Litman.
2007. Comparing user simulation models
for dialog strategy learning. In Proceedings
of the North American Meeting of the
Association of Computational Linguistics
(NAACL), pages 1?4, Rochester, NY.
Bohus, Dan, Brian Langner, Antoine Raux,
Alan W. Black, Maxine Eskenazi, and Alex
Rudnicky. 2006. Online supervised
learning of non-understanding recovery
policies. In Proceedings of the IEEE/ACL
workshop on Spoken Language Technology
(SLT), Aruba, pages 170?173.
Carletta, Jean, Amy Isard, Stephen Isard,
Jacqueline C. Kowtko, Gwyneth
Doherty-Sneddon, and Anne H. Anderson.
1997. The reliability of a dialogue structure
coding scheme. Computational Linguistics,
23(1):13?31.
Carpenter, Paul, Chun Jin, Daniel Wilson,
Rong Zhang, Dan Bohus, and Alexander I.
Rudnicky. 2001. Is this conversation on
track? In Proceedings of the European
Conference on Speech Communication and
Technology (Eurospeech), page 2121,
Aalborg.
Clark, Herbert. 1996. Using Language.
Cambridge University Press.
Cohen, William W. 1995. Fast effective rule
induction. In Proceedings of the 12th
International Conference on Machine
Learning (ICML), pages 115?123, Tahoe
City, CA.
Cuaya?huitl, Heriberto, Steve Renals, Oliver
Lemon, and Hiroshi Shimodaira. 2005.
Human?Computer dialogue simulation
using Hidden Markov Models. In
Proceedings of the IEEE workshop on
Automatic Speech Recognition and
Understanding (ASRU), pages 290?295,
San Juan.
Dempster, A., N. Laird, and D. Rubin. 1977.
Maximum likelihood from incomplete
data via the EM algorithm. Journal of
Royal Statistical Society B, 39:1?38.
Eckert, W., E. Levin, and R. Pieraccini. 1997.
User modeling for spoken dialogue
system evaluation. In Proceedings of the
IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU),
pages 80?87, Santa Barbara, CA.
Eckert, W., E. Levin, and R. Pieraccini.
1998. Automatic evaluation of spoken
dialogue systems. In Proceedings of Formal
semantics and pragmatics of dialogue
(TWLT13), pages 99?110, Twente, NL.
Engelbrecht, Klaus-Peter and Sebastian
Mo?ller. 2007. Pragmatic usage of linear
regression models for the predictions of
user judgements. In Proceedings of the
8th SIGdial workshop on Discourse and
Dialogue, pages 291?294, Antwerp.
Foster, Mary Ellen and Jon Oberlander.
2006. Data-driven generation of
emphatic facial displays. In Proceedings
of the 11th Conference of the European
Chapter of the Association for
Computational Linguistics (EACL),
pages 353?360, Trento.
Frampton, Matthew and Oliver Lemon.
2009. Recent research advances in
Reinforcement Learning in Spoken
Dialogue Systems. Knowledge Engineering
Review, 24(4):375?408.
Fraser, Norman M. and G. Nigel Gilbert.
1991. Simulating speech systems. Computer
Speech and Language, 5:81?99.
Gabsdil, Malte and Oliver Lemon. 2004.
Combining acoustic and pragmatic
features to predict recognition
performance in spoken dialogue systems.
In Proceedings of the Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 343?350, Barcelona.
Georgila, K., J. Henderson, and O. Lemon.
2006. User simulation for spoken dialogue
systems: Learning and evaluation. In
Proceedings of the International Conference
of Spoken Language Processing (Interspeech/
ICSLP), Pittsburgh, PA, pages 1065?1068.
192
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
Georgila, Kallirroi, James Henderson, and
Oliver Lemon. 2005. Learning user
simulations for information state update
dialogue systems. In Proceedings of the
International Conference of Spoken Language
Processing (Interspeech/ICSLP),
pages 893?896, Lisbon, Portugal.
Georgila, Kallirroi, Oliver Lemon, James
Henderson, and Johanna Moore. 2009.
Automatic annotation of context and
speech acts for dialogue corpora.
Natural Language Engineering,
15(3):315?353.
Hajdinjak, Melita and France Mihelic. 2006.
The PARADISE evaluation framework:
Issues and findings. Computational
Linguistics, 32(2):263?272.
Hall, Mark. 2000. Correlation-based feature
selection for discrete and numeric class
machine learning. In Proceedings of the
17th International Conference on Machine
Learning (ICML), pages 359?366,
San Francisco, CA.
Heeman, Peter. 2007. Combining
Reinforcement Learning with
information-state update rules. In
Proceedings of the North American Chapter of
the Association for Computational Linguistics
Annual Meeting (NAACL), pages 268?275,
Rochester, NY.
Henderson, James, Oliver Lemon, and
Kallirroi Georgila. 2008. Hybrid
Reinforcement/Supervised Learning of
dialogue policies from fixed datasets.
Computational Linguistics, 34(4):487?513.
Hirschberg, Julia, Diane Litman, and Marc
Swerts. 2001. Identifying user corrections
automatically in spoken dialogue systems.
In Proceedings of the North American Meeting
of the Association of Computational
Linguistics (NAACL), pages 1?8,
Pittsburgh, PA.
Janarthanam, Srini and Oliver Lemon. 2008.
User simulations for online adaptation and
knowledge-alignment in troubleshooting
dialogue systems. In Proceedings of the 12th
SEMdial Workshop on the Semantics and
Pragmatics of Dialogues, pages 149?156,
London, UK.
Janarthanam, Srini and Oliver Lemon. 2010.
Learning to adapt to unknown users:
Referring expression generation in Spoken
Dialogue Systems. In Proceedings of the
Annual Meeting of the Association for
Computational Linguistics (ACL),
pages 69?78, Uppsala.
Jokinen, Kristiina and Topi Hurtig. 2006.
User expectations and real experience
on a multimodal interactive system.
In Proceedings of the International Conference
of Spoken Language Processing (Interspeech/
ICSLP), pages 1049?1055.
Jung, Sangkeun, Cheongjae Lee, Kyungduk
Kim, Minwoo Jeong, and Gary Geunbae
Lee. 2009. Data-driven user simulation for
automated evaluation of spoken dialog
systems. Computer Speech & Language,
23:479?509.
Jurafsky, Daniel and James H. Martin. 2000.
Speech and Language Processing. An
Introduction to Natural Language Processing,
Computational Linguistics, and Speech
Recognition. Prentice-Hall: New Jersey, US.
Kruijff-Korbayova?, Ivana, Tilmann Becker,
Nate Blaylock, Ciprian Gerstenberger,
Michael Kaisser, Peter Poller, Verena
Rieser, and Jan Schehl. 2006. The SAMMIE
corpus of multimodal dialogues with an
MP3 player. In Proceedings of the 5th
International Conference on Language
Resources and Evaluation (LREC),
pages 2018?2023, Genoa, Italy.
Lemon, Oliver. 2008. Adaptive natural
language generation in dialogue using
Reinforcement Learning. In Proceedings of
the 12th SEMdial Workshop on the Semantics
and Pragmatics of Dialogues, pages 149?156,
London, UK.
Lemon, Oliver. (2011). Learning what
to say and how to say it: joint optimization
of spoken dialogue management and
Natural Language Generation. Computer
Speech and Language, 2(2):210?221.
Lemon, Oliver, Kallirroi Georgila, and James
Henderson. 2006. Evaluating effectiveness
and portability of Reinforcement Learned
dialogue strategies with real users: The
TALK TownInfo evaluation. In Proceedings
of the IEEE/ACL Workshop on Spoken
Language Technology (SLT), Aruba,
pages 170?173.
Lemon, Oliver, Kallirroi Georgila, James
Henderson, Malte Gabsdil, Ivan
Meza-Ruiz, and Steve Young. 2005.
Deliverable D4.1: Integration of learning
and adaptivity with the ISU approach.
Technical report, TALK Project.
Lemon, Oliver and Ioannis Konstas. 2009.
User simulations for context-sensitive
speech recognition in spoken dialogue
systems. In European Conference of the
Association for Computational Linguistics,
pages 505?513, Athens.
Lemon, Oliver, Xingkun Liu, Daniel Shapiro,
and Carl Tollander. 2006. Hierarchical
Reinforcement Learning of dialogue
policies in a development environment for
dialogue systems: REALL-DUDE. In
193
Computational Linguistics Volume 37, Number 1
Proceedings of the 10th SEMdial Workshop on
the Semantics and Pragmatics of Dialogues,
pages 185?186, Potsdam, Germany.
Lemon, Oliver and Olivier Pietquin. 2007.
Machine Learning for spoken dialogue
systems. In Proceedings of the International
Conference of Spoken Language Processing
(Interspeech/ICSLP), pages 2685?2688,
Antwerp, Belgium.
Levin, E., R. Pieraccini, and W. Eckert. 2000.
A stochastic model of human?machine
interaction for learning dialog strategies.
IEEE Transactions on Speech and Audio
Processing, 8(1), pages 11?23.
Levin, Esther and Rebecca Passonneau. 2006.
A WOz variant with contrastive
conditions. In Proceedings Dialog-on-Dialog
Workshop, Interspeech, Pittsburgh, PA.
Litman, D. and S. Pan. 1999. Empirically
evaluating an adaptable spoken
dialogue system. In Proceedings of the
7th International Conference on User
Modeling, pages 55?64, Banff.
Mattes, Stefan. 2003. The Lane-Change-task
as a tool for driver distraction evaluation.
In H. Strasser, K. Kluth, H. Rausch, and
H. Bubb, editors, Quality of Work and
Products in Enterprises of the Future.
Ergonomica Verlag, Stuttgart, Germany,
pages 57?60.
Mo?ller, Sebastian, Roman Englert, Klaus
Engelbrecht, Verena Hafner, Anthony
Jameson, Antti Oulasvirta, Alexander
Raake, and Norbert Reithinger. 2006.
MeMo: Towards automatic usability
evaluation of spoken dialogue services
by user error simulations. In Proceedings of
the International Conference of Spoken
Language Processing (Interspeech/ICSLP),
Pittsburgh, PA, pages 1786?1789.
Paek, Tim. 2006. Reinforcement Learning for
spoken dialogue systems: Comparing
strengths and weaknesses for practical
deployment. In Proceedings Dialog-on-
Dialog Workshop, Interspeech, Pittsburgh, PA.
Paek, Tim. 2007. Toward evaluation that
leads to best practices: Reconciling
dialogue evaluation in research and
industry. In Proceedings of the NAACL-HLT
Workshop on Bridging the Gap: Academic and
Industrial Research in Dialog Technologies,
Rochester, NY, pages 40?47.
Pietquin, O. and T. Dutoit. 2006. A
probabilistic framework for dialog
simulation and optimal strategy learning.
IEEE Transactions on Audio, Speech and
Language Processing, 14(2):589?599.
Prommer, Thomas, Hartwig Holzapfel, and
Alex Waibel. 2006. Rapid simulation-
driven Reinforcement Learning of
multimodal dialog strategies in human-
robot interaction. In Proceedings of the
International Conference of Spoken Language
Processing (Interspeech/ICSLP), Pittsburgh,
PA, pages 1918?1924.
Quinlan, Ross. 1993. C4.5: Programs for
Machine Learning. Morgan Kaufmann,
San Francisco.
Rieser, Verena. 2008. Bootstrapping
Reinforcement Learning-based Dialogue
Strategies from Wizard-of-Oz data. Ph.D.
thesis, Saarbrueken Dissertations in
Computational Linguistics and Language
Technology, Vol 28.
Rieser, Verena, Ivana Kruijff-Korbayova?, and
Oliver Lemon. 2005. A corpus collection
and annotation framework for learning
multimodal clarification strategies. In
Proceedings of the 6th SIGdial Workshop on
Discourse and Dialogue, pages 97?106,
Lisbon.
Rieser, Verena and Oliver Lemon. 2006a.
Cluster-based user simulations for
learning dialogue strategies. In Proceedings
of the 9th International Conference of Spoken
Language Processing (Interspeech/ICSLP),
Pittsburgh, PA.
Rieser, Verena and Oliver Lemon. 2006b.
Using Machine Learning to explore human
multimodal clarification strategies. In
Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL),
pages 659?666, Sydney.
Rieser, Verena and Oliver Lemon. 2008a.
Does this list contain what you were
searching for? Learning adaptive dialogue
strategies for interactive question
answering. Natural Language Engineering,
15(1):55?72.
Rieser, Verena and Oliver Lemon. 2008b.
Learning effective multimodal dialogue
strategies from Wizard-of-Oz data:
Bootstrapping and evaluation. In
Proceedings of the 21st International
Conference on Computational Linguistics and
46th Annual Meeting of the Association for
Computational Linguistics (ACL/HLT),
pages 638?646, Columbus, OH.
Rieser, Verena and Oliver Lemon. 2009a.
Learning human multimodal dialogue
strategies. Natural Language Engineering,
16(1):3?23.
Rieser, Verena and Oliver Lemon. 2009b.
Natural Language Generation as planning
under uncertainty for Spoken Dialogue
Systems. In Proceedings of the Conference of
194
Rieser and Lemon Learning and Evaluation of Dialogue Strategies for New Applications
the European Chapter of the ACL (EACL),
pages 683?691, Athens.
Rieser, Verena, Oliver Lemon, and Xingkun
Liu. 2010. Optimising information
presentation for Spoken Dialogue
Systems. In Proceedings of the Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 1009?1018,
Uppsala.
Rieser, Verena and Johanna Moore. 2005.
Implications for generating clarification
requests in task-oriented dialogues. In
Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL), pages 239?246,
Ann Arbor, MI.
Rodriguez, Kepa J. and David Schlangen.
2004. Form, intonation and function
of clarification requests in German
task-orientated spoken dialogues. In
Proceedings of the 8th SEMdial Workshop on
the Semantics and Pragmatics of Dialogues,
pages 101?108, Barcelona.
Roque, Antonio and David Traum. 2008.
Degrees of grounding based on
evidence of understanding. In
Proceedings of SIGdial, pages 54?63,
Columbus, OH.
Schatzmann, J., K. Weilhammer, M. Stuttle,
and S. Young. 2006. A survey of statistical
user simulation techniques for
reinforcement-learning of dialogue
management strategies. Knowledge
Engineering Review, 21(2):97?126.
Schatzmann, Jost, Kallirroi Georgila, and
Steve Young. 2005. Quantitative evaluation
of user simulation techniques for Spoken
Dialogue Systems. In Proceedings of the 6th
SIGdial Workshop on Discourse and Dialogue,
pages 45?54, Lisbon.
Schatzmann, Jost, Blaise Thomson, Karl
Weilhammer, Hui Ye, and Steve Young.
2007. Agenda-based user simulation for
bootstrapping a POMDP dialogue system.
In Proceedings of the North American Meeting
of the Association of Computational
Linguistics (NAACL), pages 149?152,
Rochester, NY.
Schatzmann, Jost, Blaise Thomson, and Steve
Young. 2007a. Error simulation for training
statistical dialogue systems. In Proceedings
of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU),
pages 526?531, Kyoto.
Schatzmann, Jost, Blaise Thomson, and Steve
Young. 2007b. Statistical user simulation
with a hidden agenda. In Proceedings of the
8th SIGdial Workshop on Discourse and
Dialogue, pages 273?282, Antwerp.
Scheffler, Konrad and Steve Young. 2001.
Corpus-based dialogue simulation for
automatic strategy learning and
evaluation. In Proceedings NAACL
Workshop on Adaptation in Dialogue Systems,
Pittsburgh, PA, pages 64?70.
Shapiro, Dan and P. Langley. 2002.
Separating skills from preference: Using
learning to program by reward. In
Proceedings of the 19th International
Conference on Machine Learning (ICML),
pages 570?577, Sydney.
Singh, S., D. Litman, M. Kearns, and
M. Walker. 2002. Optimizing dialogue
management with Reinforcement
Learning: Experiments with the NJFun
system. JAIR, 16:105?133.
Skantze, Gabriel. 2005. Exploring human
error recovery strategies: Implications for
spoken dialogue systems. Speech
Communication, 43(3):325?341.
Stuttle, Matthew N., Jason D. Williams, and
Steve Young. 2004. A framework for
dialogue data collection with a simulated
ASR channel. In Proceedings of the
International Conference of Spoken Language
Processing (Interspeech/ICSLP), Jeju.
Sutton, R. and A. Barto. 1998. Reinforcement
Learning. MIT Press, Cambridge, MA.
Walker, M., C. Kamm, and D. Litman.
2000. Towards developing general
models of usability with PARADISE.
Natural Language Engineering, 6(3),
pages 363?377.
Walker, Marilyn. 2000. An application for
Reinforcement Learning to dialogue
strategy selection in a spoken dialogue
system for email. Artificial Intelligence
Research, 12:387?416.
Walker, Marilyn. 2005. Can we talk? Methods
for evaluation and training of spoken
dialogue system. Language Resources and
Evaluation, 39(1):65?75.
Walker, Marilyn, Jeanne Fromer, and
Shrikanth Narayanan. 1998. Learning
optimal dialogue strategies: A case study
of a spoken dialogue agent for email.
In Proceedings of the 36th Annual Meeting
of the Association for Computational
Linguistics and 17th International
Conference on Computational Linguistics
(ACL/COLING), pages 1345?1351,
Montreal.
Walker, Marylin, Diane Litman, Candace
Kamm, and Alicia Abella. 1997.
PARADISE: A general framework for
evaluating spoken dialogue agents. In
Proceedings of the 35th Annual General
Meeting of the Association for
195
Computational Linguistics Volume 37, Number 1
Computational Linguistics (ACL),
pages 271?280, Madrid.
Williams, J. and S. Young. 2007. Partially
observable Markov decision processes for
Spoken Dialog Systems. Computer Speech
and Language, 21(2):231?422.
Williams, Jason. 2007. A method for
evaluating and comparing user
simulations: The Cramer?von Mises
divergence. In Proceedings of the IEEE
Workshop on Automatic Speech Recognition
and Understanding (ASRU), pages 508?513,
Kyoto, Japan.
Williams, Jason and Steve Young. 2004. Using
Wizard-of-Oz simulations to bootstrap
Reinforcement-Learning-based dialog
management systems. In Proceedings of the
4th SIGDIAL Workshop on Discourse and
Dialogue, pages 135?139, Sapporo, Japan.
Witten, Ian H. and Eibe Frank. 2005. Data
Mining: Practical Machine Learning Tools and
Techniques (2nd Edition). Morgan Kaufmann,
San Francisco.
Young, Steve. 2000. Probabilistic methods in
spoken dialogue systems. Philosophical
Trans Royal Society (Series A),
358(1769):1389?1402.
Young, Steve, M. Gasic, S. Keizer, F. Mairesse,
J. Schatzmann, B. Thomson, and K. Yu.
2009. The Hidden Information State
Model: A practical framework for
POMDP-based spoken dialogue
management. Computer Speech and
Language, 24(2):150?174.
196
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 69?78,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning to Adapt to Unknown Users:
Referring Expression Generation in Spoken Dialogue Systems
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
s.janarthanam@ed.ac.uk
Oliver Lemon
Interaction Lab
Mathematics and Computer Science (MACS)
Heriot-Watt University
o.lemon@hw.ac.uk
Abstract
We present a data-driven approach to learn
user-adaptive referring expression gener-
ation (REG) policies for spoken dialogue
systems. Referring expressions can be dif-
ficult to understand in technical domains
where users may not know the techni-
cal ?jargon? names of the domain entities.
In such cases, dialogue systems must be
able to model the user?s (lexical) domain
knowledge and use appropriate referring
expressions. We present a reinforcement
learning (RL) framework in which the sys-
tem learns REG policies which can adapt
to unknown users online. Furthermore,
unlike supervised learning methods which
require a large corpus of expert adaptive
behaviour to train on, we show that effec-
tive adaptive policies can be learned from
a small dialogue corpus of non-adaptive
human-machine interaction, by using a RL
framework and a statistical user simula-
tion. We show that in comparison to
adaptive hand-coded baseline policies, the
learned policy performs significantly bet-
ter, with an 18.6% average increase in
adaptation accuracy. The best learned pol-
icy also takes less dialogue time (average
1.07 min less) than the best hand-coded
policy. This is because the learned poli-
cies can adapt online to changing evidence
about the user?s domain expertise.
1 Introduction
We present a reinforcement learning (Sutton and
Barto, 1998) framework to learn user-adaptive re-
ferring expression generation policies from data-
driven user simulations. A user-adaptive REG pol-
icy allows the system to choose appropriate ex-
pressions to refer to domain entities in a dialogue
Jargon: Please plug one end of the broadband
cable into the broadband filter.
Descriptive: Please plug one end of the thin
white cable with grey ends into the
small white box.
Table 1: Referring expression examples for 2 enti-
ties (from the corpus)
setting. For instance, in a technical support con-
versation, the system could choose to use more
technical terms with an expert user, or to use more
descriptive and general expressions with novice
users, and a mix of the two with intermediate users
of various sorts (see examples in Table 1).
In natural human-human conversations, dia-
logue partners learn about each other and adapt
their language to suit their domain expertise (Is-
sacs and Clark, 1987). This kind of adaptation
is called Alignment through Audience
Design (Clark and Murphy, 1982; Bell, 1984).
We assume that users are mostly unknown to
the system and therefore that a spoken dialogue
system (SDS) must be capable of observing the
user?s dialogue behaviour, modelling his/her do-
main knowledge, and adapting accordingly, just
like human interlocutors. Rule-based and super-
vised learning approaches to user adaptation in
SDS have been proposed earlier (Cawsey, 1993;
Akiba and Tanaka, 1994). However, such methods
require expensive resources such as domain ex-
perts to hand-code the rules, or a corpus of expert-
layperson interactions to train on. In contrast, we
present a corpus-driven framework using which
a user-adaptive REG policy can be learned using
RL from a small corpus of non-adaptive human-
machine interaction.
We show that these learned policies perform
better than simple hand-coded adaptive policies
in terms of accuracy of adaptation and dialogue
69
time. We also compared the performance of poli-
cies learned using a hand-coded rule-based simu-
lation and a data-driven statistical simulation and
show that data-driven simulations produce better
policies than rule-based ones.
In section 2, we present some of the related
work. Section 3 presents the dialogue data that
we used to train the user simulation. Section 4 and
section 5 describe the dialogue system framework
and the user simulation models. In section 6, we
present the training and in section 7, we present
the evaluation for different REG policies.
2 Related work
There are several ways in which natural language
generation (NLG) systems adapt to users. Some
of them adapt to a user?s goals, preferences, en-
vironment and so on. Our focus in this study
is restricted to the user?s lexical domain exper-
tise. Several NLG systems adapt to the user?s do-
main expertise at different levels of generation -
text planning (Paris, 1987), complexity of instruc-
tions (Dale, 1989), referring expressions (Reiter,
1991), and so on. Some dialogue systems, such
as COMET, have also incorporated NLG modules
that present appropriate levels of instruction to the
user (McKeown et al, 1993). However, in all the
above systems, the user?s knowledge is assumed to
be accurately represented in an initial user model
using which the system adapts its language. In
contrast to all these systems, our adaptive REG
policy knows nothing about the user when the con-
versation starts.
Rule-based and supervised learning approaches
have been proposed to learn and adapt during the
conversation dynamically. Such systems learned
from the user at the start and later adapted to the
domain knowledge of the users. However, they ei-
ther require expensive expert knowledge resources
to hand-code the inference rules (Cawsey, 1993) or
large corpus of expert-layperson interaction from
which adaptive strategies can be learned and mod-
elled, using methods such as Bayesian networks
(Akiba and Tanaka, 1994). In contrast, we present
an approach that learns in the absence of these ex-
pensive resources. It is also not clear how super-
vised and rule-based approaches choose between
when to seek more information and when to adapt.
In this study, we show that using reinforcement
learning this decision is learned automatically.
Reinforcement Learning (RL) has been suc-
cessfully used for learning dialogue management
policies since (Levin et al, 1997). The learned
policies allow the dialogue manager to optimally
choose appropriate dialogue acts such as instruc-
tions, confirmation requests, and so on, under
uncertain noise or other environment conditions.
There have been recent efforts to learn information
presentation and recommendation strategies using
reinforcement learning (Rieser and Lemon, 2009;
Hernandez et al, 2003; Rieser and Lemon, 2010),
and joint optimisation of Dialogue Management
and NLG using hierarchical RL has been pro-
posed by (Lemon, 2010). In contrast, we present a
framework to learn to choose appropriate referring
expressions based on a user?s domain knowledge.
Earlier, we reported a proof-of-concept work us-
ing a hand-coded rule-based user simulation (Ja-
narthanam and Lemon, 2009c).
3 The Wizard-of-Oz Corpus
We use a corpus of technical support dialogues
collected from real human users using a Wizard-
of-Oz method (Janarthanam and Lemon, 2009b).
The corpus consists of 17 dialogues from users
who were instructed to physically set up a home
broadband connection using objects like a wire-
less modem, cables, filters, etc. They listened to
the instructions from the system and carried them
out using the domain objects laid in front of them.
The human ?wizard? played the role of only an in-
terpreter who would understand what the user said
and annotate it as a dialogue act. The set-up ex-
amined the effect of using three types of referring
expressions (jargon, descriptive, and tutorial), on
the users.
Out of the 17 dialogues, 6 used a jargon strat-
egy, 6 used a descriptive strategy, and 5 used a
tutorial strategy1. The task had reference to 13
domain entities, mentioned repeatedly in the di-
alogue. In total, there are 203 jargon, 202 descrip-
tive and 167 tutorial referring expressions. Inter-
estingly, users who weren?t acquainted with the
domain objects requested clarification on some of
the referring expressions used. The dialogue ex-
changes between the user and system were logged
in the form of dialogue acts and the system?s
choices of referring expressions. Each user?s
knowledge of domain entities was recorded both
before and after the task and each user?s interac-
1The tutorial strategy uses both jargon and descriptive ex-
pressions together.
70
tions with the environment were recorded. We use
the dialogue data, pre-task knowledge tests, and
the environment interaction data to train a user
simulation model. Pre and post-task test scores
were used to model the learning behaviour of the
users during the task (see section 5).
The corpus also recorded the time taken to com-
plete each dialogue task. We used these data to
build a regression model to calculate total dialogue
time for dialogue simulations. The strategies were
never mixed (with some jargon, some descriptive
and some tutorial expressions) within a single con-
versation. Therefore, please note that the strate-
gies used for data collection were not adaptive and
the human ?wizard? has no role in choosing which
referring expression to present to the user. Due to
this fact, no user score regarding adaptation was
collected. We therefore measure adaptation objec-
tively as explained in section 6.1.
4 The Dialogue System
In this section, we describe the different modules
of the dialogue system. The interaction between
the different modules is shown in figure 1 (in
learning mode). The dialogue system presents the
user with instructions to setup a broadband con-
nection at home. In the Wizard of Oz setup, the
system and the user interact using speech. How-
ever, in our machine learning setup, they interact at
the abstract level of dialogue actions and referring
expressions. Our objective is to learn to choose
the appropriate referring expressions to refer to the
domain entities in the instructions.
Figure 1: System User Interaction (learning)
4.1 Dialogue Manager
The dialogue manager identifies the next instruc-
tion (dialogue act) to give to the user based on the
dialogue management policy pidm. Since, in this
study, we focus only on learning the REG policy,
the dialogue management is coded in the form of
a finite state machine. In this dialogue task, the
system provides two kinds of instructions - ob-
servation and manipulation. For observation in-
structions, users observe the environment and re-
port back to the system, and for the manipulation
instructions (such as plugging in a cable in to a
socket), they manipulate the domain entities in the
environment. When the user carries out an instruc-
tion, the system state is updated and the next in-
struction is given. Sometimes, users do not under-
stand the referring expressions used by the system
and then ask for clarification. In such cases, the
system provides clarification on the referring ex-
pression (provide clar), which is information to
enable the user to associate the expression with
the intended referent. The system action As,t (t
denoting turn, s denoting system) is therefore to
either give the user the next instruction or a clarifi-
cation. When the user responds in any other way,
the instruction is simply repeated. The dialogue
manager is also responsible for updating and man-
aging the system state Ss,t (see section 4.2). The
system interacts with the user by passing both the
system action As,t and the referring expressions
RECs,t (see section 4.3).
4.2 The dialogue state
The dialogue state Ss,t is a set of variables that
represent the current state of the conversation. In
our study, in addition to maintaining an overall di-
alogue state, the system maintains a user model
UMs,t which records the initial domain knowl-
edge of the user. It is a dynamic model that starts
with a state where the system does not have any
idea about the user. As the conversation pro-
gresses, the dialogue manager records the evi-
dence presented to it by the user in terms of his
dialogue behaviour, such as asking for clarifica-
tion and interpreting jargon. Since the model is
updated according to the user?s behaviour, it may
be inaccurate if the user?s behaviour is itself uncer-
tain. So, when the user?s behaviour changes (for
instance, from novice to expert), this is reflected
in the user model during the conversation. Hence,
unlike previous studies mentioned in section 2, the
user model used in this system is not always an ac-
curate model of the user?s knowledge and reflects
a level of uncertainty about the user.
71
Each jargon referring expression x is repre-
sented by a three valued variable in the dialogue
state: user knows x. The three values that each
variable takes are yes, no, not sure. The vari-
ables are updated using a simple user model up-
date algorithm. Initially each variable is set to
not sure. If the user responds to an instruction
containing the referring expression x with a clari-
fication request, then user knows x is set to no.
Similarly, if the user responds with appropriate in-
formation to the system?s instruction, the dialogue
manager sets user knows x is set to yes.
The dialogue manager updates the variables
concerning the referring expressions used in the
current system utterance appropriately after the
user?s response each turn. The user may have the
capacity to learn jargon. However, only the user?s
initial knowledge is recorded. This is based on the
assumption that an estimate of the user?s knowl-
edge helps to predict the user?s knowledge of the
rest of the referring expressions. Another issue
concerning the state space is its size. Since, there
are 13 entities and we only model the jargon ex-
pressions, the state space size is 313.
4.3 REG module
The REG module is a part of the NLG module
whose task is to identify the list of domain enti-
ties to be referred to and to choose the appropriate
referring expression for each of the domain enti-
ties for each given dialogue act. In this study, we
focus only on the production of appropriate refer-
ring expressions to refer to domain entities men-
tioned in the dialogue act. It chooses between the
two types of referring expressions - jargon and de-
scriptive. For example, the domain entity broad-
band filter can be referred to using the jargon ex-
pression ?broadband filter? or using the descrip-
tive expression ?small white box?2. We call this
the act of choosing the REG action. The tutorial
strategy was not investigated here since the corpus
analysis showed tutorial utterances to be very time
consuming. In addition, they do not contribute to
the adaptive behaviour of the system.
The REG module operates in two modes - learn-
ing and evaluation. In the learning mode, the REG
module is the learning agent. The REG mod-
ule learns to associate dialogue states with opti-
mal REG actions. This is represented by a REG
2We will use italicised forms to represent the domain enti-
ties (e.g. broadband filter) and double quotes to represent the
referring expressions (e.g. ?broadband filter?).
policy pireg : UMs,t ? RECs,t, which maps
the states of the dialogue (user model) to optimal
REG actions. The referring expression choices
RECs,t is a set of pairs identifying the refer-
ent R and the type of expression T used in the
current system utterance. For instance, the pair
(broadband filter, desc) represents the descriptive
expression ?small white box?.
RECs,t = {(R1, T1), ..., (Rn, Tn)}
In the evaluation mode, a trained REG policy in-
teracts with unknown users. It consults the learned
policy pireg to choose the referring expressions
based on the current user model.
5 User Simulations
In this section, we present user simulation models
that simulate the dialogue behaviour of a real hu-
man user. These external simulation models are
different from internal user models used by the
dialogue system. In particular, our model is the
first to be sensitive to a system?s choices of refer-
ring expressions. The simulation has a statistical
distribution of in-built knowledge profiles that de-
termines the dialogue behaviour of the user being
simulated. If the user does not know a referring
expression, then he is more likely to request clar-
ification. If the user is able to interpret the refer-
ring expressions and identify the references then
he is more likely to follow the system?s instruc-
tion. This behaviour is simulated by the action se-
lection models described below.
Several user simulation models have been pro-
posed for use in reinforcement learning of dia-
logue policies (Georgila et al, 2005; Schatzmann
et al, 2006; Schatzmann et al, 2007; Ai and Lit-
man, 2007). However, they are suited only for
learning dialogue management policies, and not
natural language generation policies. Earlier, we
presented a two-tier simulation trained on data
precisely for REG policy learning (Janarthanam
and Lemon, 2009a). However, it is not suited for
training on small corpus like the one we have at
our disposal. In contrast to the earlier model, we
now condition the clarification requests on the ref-
erent class rather than the referent itself to handle
data sparsity problem.
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a
user action Au,t (u denoting user). This can ei-
ther be a clarification request (cr) or an instruction
72
response (ir). We used two kinds of action selec-
tion models: corpus-driven statistical model and
hand-coded rule-based model.
5.1 Corpus-driven action selection model
In the corpus-driven model, the US produces a
clarification request cr based on the class of the
referent C(Ri), type of the referring expression
Ti, and the current domain knowledge of the user
for the referring expression DKu,t(Ri, Ti). Do-
main entities whose jargon expressions raised clar-
ification requests in the corpus were listed and
those that had more than the mean number of clar-
ification requests were classified as difficult
and others as easy entities (for example, ?power
adaptor? is easy - all users understood this
expression, ?broadband filter? is difficult).
Clarification requests are produced using the fol-
lowing model.
P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) ? RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment action EAu,t
is generated using the following model.
P (EAu,t|As,t) if Au,t! = cr(Ri, Ti)
Finally, the user action is an instruction re-
sponse which is determined by the system action
As,t. Instruction responses can be different in dif-
ferent conditions. For an observe and report in-
struction, the user issues a provide info action
and for a manipulation instruction, the user re-
sponds with an acknowledgement action and so
on.
P (Au,t = ir|EAu,t, As,t)
All the above models were trained on our cor-
pus data using maximum likelihood estimation and
smoothed using a variant of Witten-Bell discount-
ing. According to the data, clarification requests
are much more likely when jargon expressions
are used to refer to the referents that belong to
the difficult class and which the user doesn?t
livebox = 1 power adaptor = 1
wall phone socket = 1 broadband filter = 0
broadband cable = 0 ethernet cable = 1
lb power light = 1 lb power socket = 1
lb broadband light = 0 lb ethernet light = 0
lb adsl socket = 0 lb ethernet socket = 0
pc ethernet socket = 1
Table 2: Domain knowledge: an Intermediate
User
know about. When the system uses expressions
that the user knows, the user generally responds
to the instruction given by the system. These user
simulation models have been evaluated and found
to produce behaviour that is very similar to the
original corpus data, using the Kullback-Leibler
divergence metric (Cuayahuitl, 2009).
5.2 Rule-based action selection model
We also built a rule-based simulation using the
above models but where some of the parameters
were set manually instead of estimated from the
data. The purpose of this simulation is to in-
vestigate how learning with a data-driven statisti-
cal simulation compares to learning with a simple
hand-coded rule-based simulation. In this simula-
tion, the user always asks for a clarification when
he does not know a jargon expression (regardless
of the class of the referent) and never does this
when he knows it. This enforces a stricter, more
consistent behaviour for the different knowledge
patterns, which we hypothesise should be easier to
learn to adapt to, but may lead to less robust REG
policies.
5.3 User Domain knowledge
The user domain knowledge is initially set to one
of several models at the start of every conver-
sation. The models range from novices to ex-
perts which were identified from the corpus using
k-means clustering. The initial knowledge base
(DKu,initial) for an intermediate user is shown in
table 2. A novice user knows only ?power adap-
tor?, and an expert knows all the jargon expres-
sions. We assume that users can interpret the de-
scriptive expressions and resolve their references.
Therefore, they are not explicitly represented. We
only code the user?s knowledge of jargon expres-
sions. This is represented by a boolean variable
for each domain entity.
73
Corpus data shows that users can learn jargon
expressions during the conversation. The user?s
domain knowledge DKu is modelled to be dy-
namic and is updated during the conversation.
Based on our data, we found that when presented
with clarification on a jargon expression, users al-
ways learned the jargon.
if As,t = provide clar(Ri, Ti)
DKu,t+1(Ri, Ti) ? 1
Users also learn when jargon expressions are re-
peatedly presented to them. Learning by repetition
follows the pattern of a learning curve - the greater
the number of repetitions #(Ri, Ti), the higher the
likelihood of learning. This is modelled stochas-
tically based on repetition using the parameter
#(Ri, Ti) as follows (where (Ri, Ti) ? RECs,t) .
P (DKu,t+1(Ri, Ti) ? 1|#(Ri, Ti))
The final state of the user?s domain knowl-
edge (DKu,final) may therefore be different from
the initial state (DKu,initial) due to the learn-
ing effect produced by the system?s use of jar-
gon expressions. In most studies done previously,
the user?s domain knowledge is considered to be
static. However in real conversation, we found that
the users nearly always learned jargon expressions
from the system?s utterances and clarifications.
6 Training
The REG module was trained (operated in learn-
ing mode) using the above simulations to learn
REG policies that select referring expressions
based on the user expertise in the domain. As
shown in figure 1, the learning agent (REG mod-
ule) is given a reward at the end of every dialogue.
During the training session, the learning agent ex-
plores different ways to maximize the reward. In
this section, we discuss how to code the learning
agent?s goals as reward. We then discuss how the
reward function is used to train the learning agent.
6.1 Reward function
A reward function generates a numeric reward for
the learning agent?s actions. It gives high rewards
to the agent when the actions are favourable and
low rewards when they are not. In short, the re-
ward function is a representation of the goal of the
agent. It translates the agent?s actions into a scalar
value that can be maximized by choosing the right
action sequences.
We designed a reward function for the goal of
adapting to each user?s domain knowledge. We
present the Adaptation Accuracy score AA that
calculates how accurately the agent chose the ex-
pressions for each referent r, with respect to the
user?s knowledge. Appropriateness of an expres-
sion is based on the user?s knowledge of the ex-
pression. So, when the user knows the jargon ex-
pression for r, the appropriate expression to use is
jargon, and if s/he doesn?t know the jargon, an de-
scriptive expression is appropriate. Although the
user?s domain knowledge is dynamically chang-
ing due to learning, we base appropriateness on
the initial state, because our objective is to adapt to
the initial state of the user DKu,initial. However,
in reality, designers might want their system to ac-
count for user?s changing knowledge as well. We
calculate accuracy per referent RAr as the ratio
of number of appropriate expressions to the total
number of instances of the referent in the dialogue.
We then calculate the overall mean accuracy over
all referents as shown below.
RAr = #(appropriate expressions(r))#(instances(r))
AdaptationAccuracyAA = 1#(r)?rRAr
Note that this reward is computed at the end of
the dialogue (it is a ?final? reward), and is then
back-propagated along the action sequence that
led to that final state. Thus the reward can be com-
puted for each system REG action, without the
system having access to the user?s initial domain
knowledge while it is learning a policy.
Since the agent starts the conversation with
no knowledge about the user, it may try to use
more exploratory moves to learn about the user,
although they may be inappropriate. However,
by measuring accuracy to the initial user state,
the agent is encouraged to restrict its exploratory
moves and start predicting the user?s domain
knowledge as soon as possible. The system should
therefore ideally explore less and adapt more to
increase accuracy. The above reward function re-
turns 1 when the agent is completely accurate in
adapting to the user?s domain knowledge and it
returns 0 if the agent?s REC choices were com-
pletely inappropriate. Usually during learning, the
reward value lies between these two extremes and
the agent tries to maximize it to 1.
74
6.2 Learning
The REG module was trained in learning mode us-
ing the above reward function using the SHAR-
SHA reinforcement learning algorithm (with lin-
ear function approximation) (Shapiro and Langley,
2002). This is a hierarchical variant of SARSA,
which is an on-policy learning algorithm that up-
dates the current behaviour policy (see (Sutton
and Barto, 1998)). The training produced approx.
5000 dialogues. Two types of simulations were
used as described above: Data-driven and Hand-
coded. Both user simulations were calibrated to
produce three types of users: Novice, Int2 (in-
termediate) and Expert, randomly but with equal
probability. Novice users knew just one jargon
expression, Int2 knew seven, and Expert users
knew all thirteen jargon expressions. There was
an underlying pattern in these knowledge profiles.
For example, Intermediate users were those who
knew the commonplace domain entities but not
those specific to broadband connection. For in-
stance, they knew ?ethernet cable? and ?pc ether-
net socket? but not ?broadband filter? and ?broad-
band cable?.
Initially, the REG policy chooses randomly be-
tween the referring expression types for each do-
main entity in the system utterance, irrespective
of the user model state. Once the referring expres-
sions are chosen, the system presents the user sim-
ulation with both the dialogue act and referring ex-
pression choices. The choice of referring expres-
sion affects the user?s dialogue behaviour which in
turn makes the dialogue manager update the user
model. For instance, choosing a jargon expres-
sion could evoke a clarification request from the
user, which in turn prompts the dialogue manager
to update the user model with the new information
that the user is ignorant of the particular expres-
sion. It should be noted that using a jargon expres-
sion is an information seeking move which enables
the REG module to estimate the user?s knowledge
level. The same process is repeated for every dia-
logue instruction. At the end of the dialogue, the
system is rewarded based on its choices of refer-
ring expressions. If the system chooses jargon ex-
pressions for novice users or descriptive expres-
sions for expert users, penalties are incurred and if
the system chooses REs appropriately, the reward
is high. On the one hand, those actions that fetch
more reward are reinforced, and on the other hand,
the agent tries out new state-action combinations
to explore the possibility of greater rewards. Over
time, it stops exploring new state-action combina-
tions and exploits those actions that contribute to
higher reward. The REG module learns to choose
the appropriate referring expressions based on the
user model in order to maximize the overall adap-
tation accuracy.
Figure 2 shows how the agent learns using the
data-driven (Learned DS) and hand-coded simu-
lations (Learned HS) during training. It can be
seen in the figure 2 that towards the end the curve
plateaus signifying that learning has converged.
Figure 2: Learning curves - Training
7 Evaluation
In this section, we present the evaluation metrics
used, the baseline policies that were hand-coded
for comparison, and the results of evaluation.
7.1 Metrics
In addition to the adaptation accuracy mentioned
in section 6.1, we also measure other parame-
ters from the conversation in order to show how
learned adaptive policies compare with other poli-
cies on other dimensions. We calculate the time
taken (Time) for the user to complete the dialogue
task. This is calculated using a regression model
from the corpus based on number of words, turns,
and mean user response time. We also measure
the (normalised) learning gain (LG) produced by
using unknown jargon expressions. This is calcu-
lated using the pre and post scores from the user
domain knowledge (DKu) as follows.
Learning Gain LG = Post?Pre1?Pre
75
7.2 Baseline REG policies
In order to compare the performance of the learned
policy with hand-coded REG policies, three sim-
ple rule-based policies were built. These were
built in the absence of expert domain knowledge
and a expert-layperson corpus.
? Jargon: Uses jargon for all referents by de-
fault. Provides clarifications when requested.
? Descriptive: Uses descriptive expressions for
all referents by default.
? Switching: This policy starts with jargon
expressions and continues using them until
the user requests for clarification. It then
switches to descriptive expressions and con-
tinues to use them until the user complains.
In short, it switches between the two strate-
gies based on the user?s responses.
All the policies exploit the user model in sub-
sequent references after the user?s knowledge of
the expression has been set to either yes or no.
Therefore, although these policies are simple, they
do adapt to a certain extent, and are reasonable
baselines for comparison in the absence of expert
knowledge for building more sophisticated base-
lines.
7.3 Results
The policies were run under a testing condition
(where there is no policy learning or exploration)
using a data-driven simulation calibrated to simu-
late 5 different user types. In addition to the three
users - Novice, Expert and Int2, from the train-
ing simulations, two other intermediate users (Int1
and Int3) were added to examine how well each
policy handles unseen user types. The REG mod-
ule was operated in evaluation mode to produce
around 200 dialogues per policy distributed over
the 5 user groups.
Overall performance of the different policies in
terms of Adaptation Accuracy (AA), Time and
Learning Gain (LG) are given in Table 3. Fig-
ure 3 shows how each policy performs in terms of
accuracy on the 5 types of users.
We found that the Learned DS policy (i.e.
learned with the data-driven user simulation) is
the most accurate (Mean = 79.70, SD = 10.46)
in terms of adaptation to each user?s initial state
of domain knowledge. Also, it is the only pol-
icy that has more or less the same accuracy scores
Figure 3: Evaluation - Adaptation Accuracy
Policies AA Time T LG
Descriptive 46.15 7.44 0
Jargon 74.54 9.15* 0.97*
Switching 62.47 7.48 0.30
Learned HS 69.67 7.52 0.33
Learned DS 79.70* 8.08* 0.63*
* Significantly different from all oth-
ers (p < 0.05).
Table 3: Evaluation on 5 user types
over all different user types (see figure 3). It
should also be noted that the it generalised well
over user types (Int1 and Int3) which were un-
seen in training. Learned DS policy outperforms
all other policies: Learned HS (Mean = 69.67, SD
= 14.18), Switching (Mean = 62.47, SD = 14.18),
Jargon (Mean = 74.54, SD = 17.9) and Descrip-
tive (Mean = 46.15, SD = 33.29). The differences
between the accuracy (AA) of the Learned DS pol-
icy and all other policies were statistically signif-
icant with p < 0.05 (using a two-tailed paired t-
test). Although Learned HS policy is similar to
the Learned DS policy, as shown in the learning
curves in figure 2, it does not perform as well
when confronted with users types that it did not
encounter during training. The Switching policy,
on the other hand, quickly switches its strategy
(sometimes erroneously) based on the user?s clar-
ification requests but does not adapt appropriately
to evidence presented later during the conversa-
tion. Sometimes, this policy switches erroneously
because of the uncertain user behaviours. In con-
trast, learned policies continuously adapt to new
evidence. The Jargon policy performs better than
76
the Learned HS and Switching policies. This be-
cause the system can learn more about the user
by using more jargon expressions and then use
that knowledge for adaptation for known referents.
However, it is not possible for this policy to pre-
dict the user?s knowledge of unseen referents. The
Learned DS policy performs better than the Jargon
policy, because it is able to accurately predict the
user?s knowledge of referents unseen in the dia-
logue so far.
The learned policies are a little more time-
consuming than the Switching and Descriptive
policies but compared to the Jargon policy,
Learned DS takes 1.07 minutes less time. This is
because learned policies use a few jargon expres-
sions (giving rise to clarification requests) to learn
about the user. On the other hand, the Jargon pol-
icy produces more user learning gain because of
the use of more jargon expressions. Learned poli-
cies compensate on time and learning gain in order
to predict and adapt well to the users? knowledge
patterns. This is because the training was opti-
mized for accuracy of adaptation and not for learn-
ing gain or time taken. The results show that using
our RL framework, REG policies can be learned
using data-driven simulations, and that such a pol-
icy can predict and adapt to a user?s knowledge
pattern more accurately than policies trained us-
ing hand-coded rule-based simulations and hand-
coded baseline policies.
7.4 Discussion
The learned policies explore the user?s expertise
and predict their knowledge patterns, in order to
better choose expressions for referents unseen in
the dialogue so far. The system learns to iden-
tify the patterns of knowledge in the users with
a little exploration (information seeking moves).
So, when it is provided with a piece of evidence
(e.g. the user knows ?broadband filter?), it is able
to accurately estimate unknown facts (e.g. the user
might know ?broadband cable?). Sometimes, its
choices are wrong due to incorrect estimation of
the user?s expertise (due to stochastic behaviour
of the users). In such cases, the incorrect adapta-
tion move can be considered to be an information
seeking move. This helps further adaptation us-
ing the new evidence. By continuously using this
?seek-predict-adapt? approach, the system adapts
dynamically to different users. Therefore, with
a little information seeking and better prediction,
the learned policies are able to better adapt to users
with different domain expertise.
In addition to adaptation, learned policies learn
to identify when to seek information from the user
to populate the user model (which is initially set
to not sure). It should be noted that the sys-
tem cannot adapt unless it has some information
about the user and therefore needs to decisively
seek information by using jargon expressions. If
it seeks information all the time, it is not adapting
to the user. The learned policies therefore learn to
trade-off between information seeking moves and
adaptive moves in order to maximize the overall
adaptation accuracy score.
8 Conclusion
In this study, we have shown that user-adaptive
REG policies can be learned from a small cor-
pus of non-adaptive dialogues between a dialogue
system and users with different domain knowl-
edge levels. We have shown that such adaptive
REG policies learned using a RL framework adapt
to unknown users better than simple hand-coded
policies built without much input from domain ex-
perts or from a corpus of expert-layperson adap-
tive dialogues. The learned, adaptive REG poli-
cies learn to trade off between adaptive moves and
information seeking moves automatically to max-
imize the overall adaptation accuracy. Learned
policies start the conversation with information
seeking moves, learn a little about the user, and
start adapting dynamically as the conversation
progresses. We have also shown that a data-driven
statistical user simulation produces better policies
than a simple hand-coded rule-based simulation,
and that the learned policies generalise well to un-
seen users.
In future work, we will evaluate the learned
policies with real users to examine how well
they adapt, and examine how real users evalu-
ate them (subjectively) in comparison to baselines.
Whether the learned policies perform better or as
well as a hand-coded policy painstakingly crafted
by a domain expert (or learned using supervised
methods from an expert-layperson corpus) is an
interesting question that needs further exploration.
Also, it would also be interesting to make the
learned policy account for the user?s learning be-
haviour and adapt accordingly.
77
Acknowledgements
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
H. Ai and D. Litman. 2007. Knowledge consistent
user simulations for dialog systems. In Proceedings
of Interspeech 2007, Antwerp, Belgium.
T. Akiba and H. Tanaka. 1994. A Bayesian approach
for User Modelling in Dialogue Systems. In Pro-
ceedings of the 15th conference on Computational
Linguistics - Volume 2, Kyoto.
A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145?204.
A. Cawsey. 1993. User Modelling in Interactive Ex-
planations. User Modeling and User-Adapted Inter-
action, 3(3):221?247.
H. H. Clark and G. L. Murphy. 1982. Audience de-
sign in meaning and reference. In J. F. LeNy and
W. Kintsch, editors, Language and comprehension.
Amsterdam: North-Holland.
H. Cuayahuitl. 2009. Hierarchical Reinforcement
Learning for Spoken Dialogue Systems. Ph.D. the-
sis, University of Edinburgh, UK.
R. Dale. 1989. Cooking up referring expressions. In
Proc. ACL-1989.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc of Eu-
rospeech/Interspeech.
F. Hernandez, E. Gaudioso, and J. G. Boticario. 2003.
A Multiagent Approach to Obtain Open and Flexible
User Models in Adaptive Learning Communities. In
User Modeling 2003, volume 2702/2003 of LNCS.
Springer, Berlin / Heidelberg.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26?37.
S. Janarthanam and O. Lemon. 2009a. A Two-tier
User Simulation Model for Reinforcement Learning
of Adaptive Referring Expression Generation Poli-
cies. In Proc. SigDial?09.
S. Janarthanam and O. Lemon. 2009b. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG?09.
S. Janarthanam and O. Lemon. 2009c. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG?09.
O. Lemon. 2010. Learning what to say and how to say
it: joint optimization of spoken dialogue manage-
ment and Natural Language Generation. Computer
Speech and Language. (to appear).
E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proc. of ASRU97.
K. McKeown, J. Robin, and M. Tanenblatt. 1993. Tai-
loring Lexical Choice to the User?s Vocabulary in
Multimedia Explanation Generation. In Proc. ACL
1993.
C. L. Paris. 1987. The Use of Explicit User Models
in Text Generations: Tailoring to a User?s Level of
Expertise. Ph.D. thesis, Columbia University.
E. Reiter. 1991. Generating Descriptions that Exploit a
User?s Domain Knowledge. In R. Dale, C. Mellish,
and M. Zock, editors, Current Research in Natural
Language Generation, pages 257?285. Academic
Press.
V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL?09.
V. Rieser and O. Lemon. 2010. Optimising informa-
tion presentation for spoken dialogue systems. In
Proc. ACL. (to appear).
J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.
D. Shapiro and P. Langley. 2002. Separating skills
from preference: Using learning to program by re-
ward. In Proc. ICML-02.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
78
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1009?1018,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimising Information Presentation for Spoken Dialogue Systems
Verena Rieser
University of Edinburgh
Edinburgh, United Kingdom
verena.rieser@ed.ac.uk
Oliver Lemon
Heriot-Watt University
Edinburgh, United Kingdom
o.lemon@hw.ac.uk
Xingkun Liu
Heriot-Watt University
Edinburgh, United Kingdom
x.liu@hw.ac.uk
Abstract
We present a novel approach to Informa-
tion Presentation (IP) in Spoken Dialogue
Systems (SDS) using a data-driven statis-
tical optimisation framework for content
planning and attribute selection. First we
collect data in a Wizard-of-Oz (WoZ) ex-
periment and use it to build a supervised
model of human behaviour. This forms
a baseline for measuring the performance
of optimised policies, developed from this
data using Reinforcement Learning (RL)
methods. We show that the optimised poli-
cies significantly outperform the baselines
in a variety of generation scenarios: while
the supervised model is able to attain up to
87.6% of the possible reward on this task,
the RL policies are significantly better in 5
out of 6 scenarios, gaining up to 91.5% of
the total possible reward. The RL policies
perform especially well in more complex
scenarios. We are also the first to show
that adding predictive ?lower level? fea-
tures (e.g. from the NLG realiser) is im-
portant for optimising IP strategies accord-
ing to user preferences. This provides new
insights into the nature of the IP problem
for SDS.
1 Introduction
Work on evaluating SDS suggests that the Infor-
mation Presentation (IP) phase is the primary con-
tributor to dialogue duration (Walker et al, 2001),
and as such, is a central aspect of SDS design.
During this phase the system returns a set of items
(?hits?) from a database, which match the user?s
current search constraints. An inherent problem
in this task is the trade-off between presenting
?enough? information to the user (for example
helping them to feel confident that they have a
good overview of the search results) versus keep-
ing the utterances short and understandable.
In the following we show that IP for SDS can
be treated as a data-driven joint optimisation prob-
lem, and that this outperforms a supervised model
of human ?wizard? behaviour on a particular IP
task (presenting sets of search results to a user).
A similar approach has been applied to the
problem of Referring Expression Generation in di-
alogue (Janarthanam and Lemon, 2010).
1.1 Previous work on Information
Presentation in SDS
Broadly speaking, IP for SDS can be divided into
two main steps: 1) IP strategy selection and 2)
Content or Attribute Selection. Prior work has
presented a variety of IP strategies for structur-
ing information (see examples in Table 1). For ex-
ample, the SUMMARY strategy is used to guide the
user?s ?focus of attention?. It draws the user?s at-
tention to relevant attributes by grouping the cur-
rent results from the database into clusters, e.g.
(Polifroni and Walker, 2008; Demberg and Moore,
2006). Other studies investigate a COMPARE strat-
egy, e.g. (Walker et al, 2007; Nakatsu, 2008),
while most work in SDS uses a RECOMMEND strat-
egy, e.g. (Young et al, 2007). In a previous proof-
of-concept study (Rieser and Lemon, 2009) we
show that each of these strategies has its own
strengths and drawbacks, dependent on the partic-
ular context in which information needs to be pre-
sented to a user. Here, we will also explore pos-
sible combinations of the strategies, for example
SUMMARY followed by RECOMMEND, e.g. (Whittaker
et al, 2002), see Figure 1.
Prior work on Content or Attribute Selection
has used a ?Summarize and Refine? approach (Po-
lifroni and Walker, 2008; Polifroni and Walker,
2006; Chung, 2004). This method employs utility-
based attribute selection with respect to how each
attribute (e.g. price or food type in restaurant
1009
search) of a set of items helps to narrow down
the user?s goal to a single item. Related work ex-
plores a user modelling approach, where attributes
are ranked according to user preferences (Dem-
berg and Moore, 2006; Winterboer et al, 2007).
Our data collection (see Section 3) and training en-
vironment incorporate these approaches.
The work in this paper is the first to ap-
ply a data-driven method to this whole decision
space (i.e. combinations of Information Presenta-
tion strategies as well as attribute selection), and to
show the utility of both lower-level features (e.g.
from the NLG realiser) and higher-level features
(e.g. from Dialogue Management) for this prob-
lem. Previous work has only focused on individual
aspects of the problem (e.g. how many attributes
to generate, or when to use a SUMMARY), using a
pipeline model for SDS with DM features as input,
and where NLG has no knowledge of lower level
features (e.g. behaviour of the realiser). In Section
4.3 we show that lower level features significantly
influence users? ratings of IP strategies. In the fol-
lowing we use a Reinforcement Learning (RL) as a
statistical planning framework (Sutton and Barto,
1998) to explore the contextual features for mak-
ing these decisions, and propose a new joint opti-
misation method for IP strategies combining con-
tent structuring and attribute selection.
2 NLG as planning under uncertainty
We follow the overall framework of NLG as plan-
ning under uncertainty (Lemon, 2008; Rieser and
Lemon, 2009; Lemon, 2010), where each NLG ac-
tion is a sequential decision point, based on the
current dialogue context and the expected long-
term utility or ?reward? of the action. Other re-
cent approaches describe this task as planning, e.g.
(Koller and Petrick, 2008), or as contextual de-
cision making according to a cost function (van
Deemter, 2009), but not as a statistical planning
problem, where uncertainty in the stochastic envi-
ronment is explicitly modelled. Below, we apply
this framework to Information Presentation strate-
gies in SDS using Reinforcement Learning, where
the example task is to present a set of search results
(e.g. restaurants) to users. In particular, we con-
sider 7 possible policies for structuring the content
(see Figure 1): Recommending one single item,
comparing two items, summarising all of them,
or ordered combinations of those actions, e.g. first
summarise all the retrieved items and then recom-
mend one of them. The IP module has to decide
which action to take next, how many attributes to
mention, and when to stop generating.
Figure 1: Possible Information Presentation struc-
tures (X=stop generation)
3 Wizard-of-Oz data collection
In an initial Wizard-of-Oz (WoZ) study, we asked
humans (our ?wizards?) to produce good IP ac-
tions in different dialogue contexts, when interact-
ing in spoken dialogues with other humans (the
?users?), who believed that they were talking to an
automated SDS. The wizards were experienced re-
searchers in SDS and were familiar with the search
domain (restaurants in Edinburgh). They were in-
structed to select IP structures and attributes for
NLG so as to most efficiently allow users to find a
restaurant matching their search constraints. They
also received prior training on this task.
The task for the wizards was to decide which
IP structure to use next (see Section 3.2 for a
list of IP strategies to choose from), which at-
tributes to mention (e.g. cuisine, price range, lo-
cation, food quality, and/or service quality), and
whether to stop generating, given varying num-
bers of database matches, varying prompt reali-
sations, and varying user behaviour. Wizard ut-
terances were synthesised using a state-of-the-art
text-to-speech engine. The user speech input was
delivered to the wizard using Voice Over IP. Figure
2 shows the web-based interface for the wizard.
3.1 Experimental Setup and Data collection
We collected 213 dialogues with 18 subjects and 2
wizards (Liu et al, 2009). Each user performed a
total of 12 tasks, where no task set was seen twice
by any one wizard. The majority of users were
from a range of backgrounds in a higher educa-
tion institute, in the age range 20-30, native speak-
ers of English, and none had prior experience of
1010
Figure 2: Wizard interface. [A:] The wizard selects attribute values as specified by the user?s query. [B:] The retrieved
database items are presented in an ordered list. We use a User Modelling approach for ranking the restaurants, see e.g. (Polifroni
and Walker, 2008). [C:] The wizard then chooses which strategy and which attributes to generate next, by clicking radio buttons.
The attribute/s specified in the last user query are pre-selected by default. The strategies can only be combined in the orders as
specified in Figure 1. [D:] An utterance is automatically generated by the NLG realiser every time the wizard selects a strategy,
and is displayed in an intermediate text panel. [E:] The wizard can decide to add the generated utterance to the final output
panel or to start over again. The text in the final panel is sent to the user via TTS, once the wizard decides to stop generating.
Strategy Example utterance
SUMMARY no
UM
I found 26 restaurants, which have Indian cuisine. 11 of the restaurants are in the expensive price
range. Furthermore, 10 of the restaurants are in the cheap price range and 5 of the restaurants
are in the moderate price range.
SUMMARY UM 26 restaurants meet your query. There are 10 restaurants which serve Indian food and are in the
cheap price range. There are also 16 others which are more expensive.
COMPARE by
Item
The restaurant called Kebab Mahal is an Indian restaurant. It is in the cheap price range. And
the restaurant called Saffrani, which is also an Indian restaurant, is in the moderate price range.
COMPARE by
Attribute
The restaurant called Kebab Mahal and the restaurant called Saffrani are both Indian restaurants.
However, Kebab Mahal is in the cheap price range while Saffrani is moderately priced.
RECOMMEND The restaurant called Kebab Mahal has the best overall quality amongst the matching restau-
rants. It is an Indian restaurant, and it is in the cheap price range.
Table 1: Example realisations, generated when the user provided cuisine=Indian, and where the
wizard has also selected the additional attribute price for presentation to the user.
Spoken Dialogue Systems. After each task the
user answered a questionnaire on a 6 point Lik-
ert scale, regarding the perceived generation qual-
ity in that task. The wizards? IP strategies were
highly ranked by the users on average (4.7), and
users were able to select a restaurant in 98.6% of
the cases. No significant difference between the
wizards was observed.
The data contains 2236 utterances in total: 1465
wizard utterances and 771 user utterances. We au-
tomatically extracted 81 features (e.g #sentences,
#DBhits, #turns, #ellipsis)1 from the XML logfiles
after each dialogue. Please see (Rieser et al, 2009)
1The full corpus and list of features is available at
https://www.classic-project.org/corpora/
for more details.
3.2 NLG Realiser
In the Wizard-of-Oz environment we implemented
a NLG realiser for the chosen IP structures and
attribute choices, in order to realise the wizards?
choices in real time. This generator is based on
data from the stochastic sentence planner SPaRKy
(Stent et al, 2004). We replicated the variation ob-
served in SPaRKy by analysing high-ranking ex-
ample outputs (given the highest possible score
by the SPaRKy judges) and implemented the vari-
ance using dynamic sentence generation. The real-
isations vary in sentence aggregation, aggregation
operators (e.g. ?and?, period, or ellipsis), contrasts
1011
(e.g. ?however?, ?on the other hand?) and referring
expressions (e.g. ?it?, ?this restaurant?) used. The
length of an utterance also depends on the num-
ber of attributes chosen, i.e. the more attributes the
longer the utterance. All of these variations were
logged.
In particular, we realised the following IP strate-
gies (see examples in Table 1):
? SUMMARY of all matching restaurants with
or without a User Model (UM), following
(Polifroni and Walker, 2008). The approach
using a UM assumes that the user has cer-
tain preferences (e.g. cheap) and only tells
him about the relevant items, whereas the
approach with no UM lists all the matching
items.
? COMPARE the top 2 restaurants by Item (i.e.
listing all the attributes for the first item and
then for the other) or by Attribute (i.e. di-
rectly comparing the different attribute val-
ues).
? RECOMMEND the top-ranking restaurant (ac-
cording to UM).
Note that there was no discernible pattern in
the data about the wizards? decisions between
the UM/no UM and the byItem/byAttribute ver-
sions of the strategies. In this study we will
therefore concentrate on the higher level decisions
(SUMMARY vs. COMPARE vs. RECOMMEND) and model
these different realisations as noise in the realiser.
3.3 Supervised Baseline strategy
We analysed the WoZ data to explore the best-
rated strategies (the top scoring 50%, n = 205)
that were employed by humans for this task. Here
we used a variety of Supervised Learning meth-
ods to create a model of the highly rated wizard
behaviour. Please see (Rieser et al, 2009) for fur-
ther details. The best performing method was Rule
Induction (JRip). 2 The model achieved an accu-
racy of 43.19% which is significantly (p < .001)
better than the majority baseline of always choos-
ing SUMMARY (34.65%). 3 The resulting rule set is
shown in Figure 3.
2The WEKA implementation of (Cohen, 1995)?s RIPPER.
3Note that the low accuracy is due to data sparsity and
diverse behaviour of the wizards. However, in (Rieser et al,
2009) we show that this model is significantly different from
the policy learned using the worse scoring 50%.
IF (dbHits <= 9)& (prevNLG = summary):
THEN nlgStrategy=compare;
IF (dbHits = 1):
THEN nlgStrategy= Recommend;
IF(prevNLG=summaryRecommend)&(dbHits>=10):
THEN nlgStrategy= Recommend;
ELSE nlgStrategy=summary;
Figure 3: Rules learned by JRip for the wizard
model (?dbHits?= number of database matches,
?prevNLG?= previous NLG action)
The features selected by this model were only
?high-level? features, i.e. the input (previous ac-
tion, number of database hits) that an IP module
receives as input from a Dialogue Manager (DM).
We further analysed the importance of different
features using feature ranking and selection meth-
ods (Rieser et al, 2009), finding that the human
wizards in this specific setup did not pay signifi-
cant attention to any lower level features, e.g. from
surface realisation, although the generated output
was displayed to them (see Figure 2).
Nevertheless, note that the supervised model
achieves up to 87.6% of the possible reward on
this task, as we show in Section 5.2, and so can
be considered a serious baseline against which to
measure performance. Below, we will show that
Reinforcement Learning (RL) produces a signifi-
cant improvement over the strategies present in the
original data, especially in cases where RL has ac-
cess to ?lower level? features of the context.
4 The Simulation / Learning
Environment
Here we ?bootstrap? a simulated training environ-
ment from the WoZ data, following (Rieser and
Lemon, 2008).
4.1 User Simulations
User Simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al, 2007). A user simulation for
NLG is very similar, in that it is a predictive model
of the most likely next user act. 4 However, this
NLG predicted user act does not actually change
the overall dialogue state (e.g. by filling slots) but
it only changes the generator state. In other words,
4Similar to the internal user models applied in recent
work on POMDP (Partially Observable Markov Decision
Process) dialogue managers (Young et al, 2007; Henderson
and Lemon, 2008; Gasic et al, 2008) for estimation of user
act probabilities.
1012
the NLG user simulation tells us what the user is
most likely to do next, if we were to stop generat-
ing now.
We are most interested in the following user re-
actions:
1. select: the user chooses one of the pre-
sented items, e.g. ?Yes, I?ll take that one.?.
This reply type indicates that the Informa-
tion Presentation was sufficient for the user
to make a choice.
2. addInfo: The user provides more at-
tributes, e.g. ?I want something cheap.?. This
reply type indicates that the user has more
specific requests, which s/he wants to specify
after being presented with the current infor-
mation.
3. requestMoreInfo: The user asks for
more information, e.g. ?Can you recommend
me one??, ?What is the price range of the
last item??. This reply type indicates that the
system failed to present the information the
user was looking for.
4. askRepeat: The user asks the system to
repeat the same message again, e.g. ?Can you
repeat??. This reply type indicates that the
utterance was either too long or confusing for
the user to remember, or the TTS quality was
not good enough, or both.
5. silence: The user does not say anything.
In this case it is up to the system to take ini-
tiative.
6. hangup: The user closes the interaction.
We build user simulations using n-gram mod-
els of system (s) and user (u) acts, as first
introduced by (Eckert et al, 1997). In or-
der to account for data sparsity, we apply dif-
ferent discounting (?smoothing?) techniques in-
cluding back-off, using the CMU Statistical Lan-
guage Modelling toolkit (Clarkson and Rosen-
feld, 1997). We construct a bi-gram model5
for the users? reactions to the system?s IP struc-
ture decisions (P (au,t|IPs,t)), and a tri-gram
(i.e. IP structure + attribute choice) model for
predicting user reactions to the system?s com-
bined IP structure and attribute selection deci-
sions: P (au,t|IPs,t, attributess,t).
5Where au,t is the predicted next user action at time t,
IPs,t was the system?s Information Presentation action at t,
and attributess,t is the attributes selected by the system at t.
We evaluate the performance of these models
by measuring dialogue similarity to the original
data, based on the Kullback-Leibler (KL) diver-
gence, as also used by, e.g. (Cuaya?huitl et al,
2005; Jung et al, 2009; Janarthanam and Lemon,
2009). We compare the raw probabilities as ob-
served in the data with the probabilities generated
by our n-gram models using different discounting
techniques for each context, see table 2. All the
models have a small divergence from the origi-
nal data (especially the bi-gram model), suggest-
ing that they are reasonable simulations for train-
ing and testing NLG policies.
The absolute discounting method for the bi-
gram model is most dissimilar to the data, as is the
WittenBell method for the tri-gram model, i.e. the
models using these discounting methods have the
highest KL score. The best performing methods
(i.e. most similar to the original data), are linear
discounting for the bi-gram model and GoodTur-
ing for the tri-gram. We use the most similar user
models for system training, and the most dissimi-
lar user models for testing NLG policies, in order
to test whether the learned policies are robust and
adaptive to unseen dialogue contexts.
discounting method bi-gram US tri-gram US
WittenBell 0.086 0.512
GoodTuring 0.086 0.163
absolute 0.091 0.246
linear 0.011 0.276
Table 2: Kullback-Leibler divergence for the dif-
ferent User Simulations (US)
4.2 Database matches and ?Focus of
attention?
An important task of Information Presentation is
to support the user in choosing between all the
available items (and ultimately in selecting the
most suitable one) by structuring the current infor-
mation returned from the database, as explained in
Section 1.1. We therefore model the user?s ?fo-
cus of attention? as a feature in our learning ex-
periments. This feature reflects how the differ-
ent IP strategies structure information with dif-
ferent numbers of attributes. We implement this
shift of the user?s focus analogously to discover-
ing the user?s goal in Dialogue Management: ev-
ery time the predicted next user act is to add in-
1013
formation (addInfo), we infer that the user is
therefore only interested in a subset of the previ-
ously presented results and so the system will fo-
cus on this new subset of database items in the rest
of the generated utterance. For example, the user?s
focus after the SUMMARY (with UM) in Table 1 is
DBhits = 10, since the user is only interested in
cheap, Indian places.
4.3 Data-driven Reward function
The reward/evaluation function is constructed
from the WoZ data, using a stepwise linear regres-
sion, following the PARADISE framework (Walker
et al, 2000). This model selects the features
which significantly influenced the users? ratings
for the NLG strategy in the WoZ questionnaire.
We also assign a value to the user?s reactions
(valueUserReaction), similar to optimising task
success for DM (Young et al, 2007). This reflects
the fact that good IP strategies should help the
user to select an item (valueUserReaction =
+100) or provide more constraints addInfo
(valueUserReaction = ?0), but the user should
not do anything else (valueUserReaction =
?100). The regression in equation 1 (R2 =
.26) indicates that users? ratings are influenced by
higher level and lower level features: Users like to
be focused on a small set of database hits (where
#DBhits ranges over [1-100]), which will enable
them to choose an item (valueUserReaction),
while keeping the IP utterances short (where
#sentence is in the range [2-18]):
Reward = (?1.2)?#DBhits (1)
+(.121)? valueUserReaction
?(1.43)?#sentence
Note that the worst possible reward for an NLG
move is therefore (?1.20?100)? (.121?100)?
(18 ? 1.43) = ?157.84. This is achieved by pre-
senting 100 items to the user in 18 sentences6, in
such a way that the user ends the conversation un-
successfully. The top possible reward is achieved
in the rare cases where the system can immedi-
ately present 1 item to the user using just 2 sen-
tences, and the user then selects that item, i.e. Re-
ward =?(1.20?1)+(.121?100)?(2?1.43) =
8.06
6Note that the maximum possible number of sentences
generated by the realizer is 18 for the full IP sequence SUM-
MARY+COMPARE+RECOMMEND using all the attributes.
5 Reinforcement Learning experiments
We now formulate the problem as a Markov De-
cision Process (MDP), where states are NLG di-
alogue contexts and actions are NLG decisions.
Each state-action pair is associated with a transi-
tion probability, which is the probability of mov-
ing from state s at time t to state s? at time t+1 af-
ter having performed action a when in state s. This
transition probability is computed by the environ-
ment model (i.e. the user simulation and realiser),
and explicitly captures the uncertainty in the gen-
eration environment. This is a major difference
to other non-statistical planning approaches. Each
transition is also associated with a reinforcement
signal (or ?reward?) rt+1 describing how good the
result of action a was when performed in state s.
The aim of the MDP is to maximise long-term ex-
pected reward of its decisions, resulting in a policy
which maps each possible state to an appropriate
action in that state.
We treat IP as a hierarchical joint optimisation
problem, where first one of the IP structures (1-
3) is chosen and then the number of attributes is
decided, as shown in Figure 4. At each genera-
tion step, the MDP can choose 1-5 attributes (e.g.
cuisine, price range, location, food quality, and/or
service quality). Generation stops as soon as the
user is predicted to select an item, i.e. the IP task
is successful. (Note that the same constraint is op-
erational for the WoZ baseline.)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ACTION:
?
?IP:
?
?
?
SUMMARY
COMPARE
RECOMMEND
?
?
?
{
attr: 1-5
}
?
?
STATE:
?
?
?
?
?
?
?
?
?
?
?
attributes:
{
1-15
}
sentence:
{
2-18
}
dbHitsFocus:
{
1-100
}
userSelect:
{
0,1
}
userAddInfo:
{
0,1
}
userElse:
{
0,1
}
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 4: State-Action space for the RL-NLG
problem
States are represented as sets of NLG dia-
logue context features. The state space comprises
?lower-level? features about the realiser behaviour
(two discrete features representing the number of
attributes and sentences generated so far) and three
binary features representing the user?s predicted
next action, as well as ?high-level? features pro-
1014
vided by the DM (e.g. current database hits in the
user?s focus (dbHitsFocus)). We trained the
policy using the SHARSHA algorithm (Shapiro and
Langley, 2002) with linear function approximation
(Sutton and Barto, 1998), and the simulation envi-
ronment described in Section 4. The policy was
trained for 60,000 iterations.
5.1 Experimental Set-up
We compare the learned strategies against the WoZ
baseline as described in Section 3.3. For attribute
selection we choose a majority baseline (randomly
choosing between 3 or 4 attributes) since the at-
tribute selection models learned by Supervised
Learning on the WoZ data didn?t show significant
improvements.
For training, we used the user simulation model
most similar to the data, see Section 4.1. For
testing, we test with the different user simulation
model (the one which is most dissimilar to the
data).
We first investigate how well IP structure (with-
out attribute choice) can be learned in increas-
ingly complex generation scenarios. A genera-
tion scenario is a combination of a particular kind
of NLG realiser (template vs. stochastic) along
with different levels of variation introduced by cer-
tain features of the dialogue context. In general,
the stochastic realiser introduces more variation
in lower level features than the template-based re-
aliser. The Focus model introduces more varia-
tion with respect to #DBhits and #attributes as de-
scribed in Section 4.2. We therefore investigate
the following cases:
1.1. IP structure choice, Template realiser:
Predicted next user action varies according to
the bi-gram model (P (au,t|IPs,t)); Number
of sentences and attributes per IP strategy is
set by defaults, reflecting a template-based
realiser.
1.2. IP structure choice, Stochastic realiser:
IP structure where number of attributes per
NLG turn is given at the beginning of each
episode (e.g. set by the DM); Sentence gen-
eration according to the SPaRKy stochastic
realiser model as described in Section 3.2.
We then investigate different scenarios for
jointly optimising IP structure (IPS) and attribute
selection (Attr) decisions.
2.1. IPS+Attr choice, Template realiser:
Predicted next user action varies according
to tri-gram (P (au,t|IPs,t, attributess,t))
model; Number of sentences per IP structure
set to default.
2.2. IPS+Attr choice, Template realiser+Focus model:
Tri-gram user simulation with Template re-
aliser and Focus of attention model with
respect to #DBhits and #attributes as
described in section 4.2.
2.3. IPS+Attr choice, Stochastic realiser: Tri-
gram user simulation with sentence/attribute
relationship according to Stochastic realiser
as described in Section 3.2.
2.4. IPS+Attr choice, Stochastic realiser+Focus:
i.e. the full model = Predicted next user ac-
tion varies according to tri-gram model+
Focus of attention model + Sentence/attribute
relationship according to stochastic realiser.
5.2 Results
We compare the average final reward (see Equa-
tion 1) gained by the baseline against the trained
RL policies in the different scenarios for each
1000 test runs, using a paired samples t-test. The
results are shown in Table 3. In 5 out of 6 scenar-
ios the RL policy significantly (p < .001) outper-
forms the supervised baseline. We also report on
the percentage of the top possible reward gained
by the individual policies, and the raw percentage
improvement of the RL policy. Note that the best
possible (100%) reward can only be gained in rare
cases (see Section 4.3).
The learned RL policies show that lower level
features are important in gaining significant im-
provements over the baseline. The more complex
the scenario, the harder it is to gain higher rewards
for the policies in general (as more variation is in-
troduced), but the relative improvement in rewards
also increases with complexity: the baseline does
not adapt well to the variations in lower level fea-
tures whereas RL learns to adapt to the more chal-
lenging scenarios. 7
An overview of the range of different IP strate-
gies learned for each setup can be found in Table 4.
Note that these strategies are context-dependent:
the learner chooses how to proceed dependent on
7Note, that the baseline does reasonably well in scenarios
with variation introduced by only higher level features (e.g.
scenario 2.2).
1015
Scenario
Wizard Baseline
average Reward
RL average Reward
RL % - Baseline %
= % improvement
1.1 -15.82(?15.53) -9.90***(?15.38) 89.2% - 85.6%= 3.6%
1.2 -19.83(?17.59) -12.83***(?16.88) 87.4% - 83.2%= 4.2%
2.1 -12.53(?16.31) -6.03***(?11.89) 91.5% - 87.6%= 3.9%
2.2 -14.15(?16.60) -14.18(?18.04) 86.6% - 86.6%= 0.0%
2.3 -17.43(?15.87) -9.66***(?14.44) 89.3% - 84.6%= 4.7%
2.4 -19.59(?17.75) -12.78***(?15.83) 87.4% - 83.3%= 4.1%
Table 3: Test results for 1000 dialogues, where *** denotes that the RL policy is significantly (p < .001)
better than the Baseline policy.
the features in the state space at each generation
step.
Scenario strategies learned
1.1
RECOMMEND
COMPARE
COMPARE+RECOMMEND
SUMMARY
SUMMARY+COMPARE
SUMMARY+RECOMMEND
SUMMARY+COMPARE+RECOMMEND.
1.2
RECOMMEND
COMPARE
COMPARE+RECOMMEND
SUMMARY
SUMMARY+COMPARE
SUMMARY+RECOMMEND
SUMMARY+COMPARE+RECOMMEND.
2.1
RECOMMEND(5)
SUMMARY(2)
SUMMARY(2)+COMPARE(4)
SUMMARY(2)+COMPARE(1)
SUMMARY(2)+COMPARE(4)+RECOMMEND(5)
SUMMARY(2)+COMPARE(1)+RECOMMEND(5)
2.2
RECOMMEND(5)
SUMMARY(4)
SUMMARY(4)+RECOMMEND(5)
2.3
RECOMMEND(2)
SUMMARY(1)
SUMMARY(1)+COMPARE(4)
SUMMARY(1)+COMPARE(1)
SUMMARY(1)+COMPARE(4)+RECOMMEND(2)
2.4
RECOMMEND(2)
SUMMARY(2)
SUMMARY(2)+COMPARE(4)
SUMMARY(2)+RECOMMEND(2)
SUMMARY(2)+COMPARE(4)+RECOMMEND(2)
SUMMARY(2)+COMPARE(1)+RECOMMEND(2)
Table 4: RL strategies learned for the different sce-
narios, where (n) denotes the number of attributes
generated.
For example, the RL policy for scenario 1.1
learned to start with a SUMMARY if the initial num-
ber of items returned from the database is high
(>30). It will then stop generating if the user is
predicted to select an item. Otherwise, it contin-
ues with a RECOMMEND. If the number of database
items is low, it will start with a COMPARE and then
continue with a RECOMMEND, unless the user selects
an item. Also see Table 4. Note that the WoZ strat-
egy behaves as described in Figure 3.
In addition, the RL policy for scenario 1.2
learns to adapt to a more complex scenario:
the number of attributes requested by the DM
and produced by the stochastic sentence re-
aliser. It learns to generate the whole sequence
(SUMMARY+COMPARE+RECOMMEND) if #attributes is
low (<3), because the overall generated utterance
(final #sentences) is still relatively short. Other-
wise the policy is similar to the one for scenario
1.1.
The RL policies for jointly optimising IP strat-
egy and attribute selection learn to select the num-
ber of attributes according to the generation sce-
narios 2.1-2.4. For example, the RL policy learned
for scenario 2.1 generates a RECOMMEND with 5 at-
tributes if the database hits are low (<13). Oth-
erwise, it will start with a SUMMARY using 2 at-
tributes. If the user is predicted to narrow down
his focus after the SUMMARY, the policy continues
with a COMPARE using 1 attribute only, otherwise it
helps the user by presenting 4 attributes. It then
continues with RECOMMEND(5), and stops as soon
as the user is predicted to select one item.
The learned policy for scenario 2.1 generates
5.85 attributes per NLG turn on average (i.e. the
cumulative number of attributes generated in the
whole NLG sequence, where the same attribute
may be repeated within the sequence). This strat-
egy primarily adapts to the variations from the user
simulation (tri-gram model). For scenario 2.2 the
average number of attributes is higher (7.15) since
the number of attributes helps to narrow down the
user?s focus via the DBhits/attribute relationship
specified in section 4.2. For scenario 2.3 fewer
attributes are generated on average (3.14), since
here the number of attributes influences the sen-
tence realiser, i.e. fewer attributes results in fewer
sentences, but does not impact the user?s focus.
In scenario 2.4 all the conditions mentioned above
influence the learned policy. The average number
of attributes selected is still low (3.19).
In comparison, the average (cumulative) num-
1016
ber of attributes for the WoZ baseline is 7.10. The
WoZ baseline generates all the possible IP struc-
tures (with 3 or 4 attributes) but is restricted to use
only ?high-level? features (see Figure 3). By beat-
ing this baseline we show the importance of the
?lower-level? features. Nevertheless, this wizard
policy achieves up to 87.6% of the possible reward
on this task, and so can be considered a serious
baseline against which to measure performance.
The only case (scenario 2.2) where RL does not
improve significantly over the baseline is where
lower level features do not play an important role
for learning good strategies: scenario 2.2 is only
sensitive to higher level features (DBhits).
6 Conclusion
We have presented a new data-driven method for
Information Presentation (IP) in Spoken Dialogue
Systems using a statistical optimisation frame-
work for content structure planning and attribute
selection. This work is the first to apply a data-
driven optimisation method to the IP decision
space, and to show the utility of both lower-level
and higher-level features for this problem.
We collected data in a Wizard-of-Oz (WoZ)
experiment and showed that human ?wizards?
mostly pay attention to ?high-level? features from
Dialogue Management. The WoZ data was used
to build statistical models of user reactions to
IP strategies, and a data-driven reward function
for Reinforcement Learning (RL). We show that
lower level features significantly influence users?
ratings of IP strategies. We compared a model of
human behaviour (the ?human wizard baseline?)
against policies optimised using Reinforcement
Learning, in a variety of scenarios. Our optimised
policies significantly outperform the IP structuring
and attribute selection present in the WoZ data, es-
pecially when performing in complex generation
scenarios which require adaptation to, e.g. number
of database results, utterance length, etc. While
the human wizards were able to attain up to 87.6%
of the possible reward on this task, the RL poli-
cies are significantly better in 5 out of 6 scenarios,
gaining up to 91.5% of the total possible reward.
We have also shown that adding predictive
?lower level? features, e.g. from the NLG realiser
and a user reaction model, is important for learn-
ing optimal IP strategies according to user pref-
erences. Future work could include the predicted
TTS quality (Boidin et al, 2009) as a feature.
We are now working on testing the learned poli-
cies with real users, outside of laboratory condi-
tions, using a restaurant-guide SDS, deployed as a
VOIP service. Previous work in SDS has shown
that results for Dialogue Management obtained
with simulated users are able to transfer to eval-
uations with real users (Lemon et al, 2006).
This methodology provides new insights into
the nature of the IP problem, which has previously
been treated as a module following dialogue man-
agement with no access to lower-level context fea-
tures. The data-driven planning method applied
here promises a significant upgrade in the perfor-
mance of generation modules, and thereby of Spo-
ken Dialogue Systems in general.
Acknowledgments
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
Cedric Boidin, Verena Rieser, Lonneke van der Plas,
Oliver Lemon, and Jonathan Chevelu. 2009. Pre-
dicting how it sounds: Re-ranking alternative in-
puts to TTS using latent variables (forthcoming). In
Proc. of Interspeech/ICSLP, Special Session on Ma-
chine Learning for Adaptivity in Spoken Dialogue
Systems.
Grace Chung. 2004. Developing a flexible spoken dia-
log system using simulation. In Proc. of the Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
P.R. Clarkson and R. Rosenfeld. 1997. Statisti-
cal Language Modeling Using the CMU-Cambridge
Toolkit. In Proc. of ESCA Eurospeech.
William W. Cohen. 1995. Fast effective rule induction.
In Proceedings of the 12th International Conference
on Machine Learning (ICML).
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
Vera Demberg and Johanna D. Moore. 2006. Infor-
mation presentation in spoken dialogue systems. In
Proceedings of EACL.
1017
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and S. Young. 2008. Training and
Evaluation of the HIS POMDP Dialogue System in
Noise. In Proc. of SIGdial Workshop on Discourse
and Dialogue.
James Henderson and Oliver Lemon. 2008. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proc. of ACL.
Srinivasan Janarthanam and Oliver Lemon. 2009. A
Two-tier User Simulation Model for Reinforcement
Learning of Adaptive Referring Expression Genera-
tion Policies. In Proc. of SIGdial.
Srini Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: Referring expression
generation in spoken dialogue systems. In Proceed-
ings of ACL.
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Computer, Speech & Lan-
guage, 23:479?509.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Oliver Lemon, Kallirroi Georgila, and James Hender-
son. 2006. Evaluating Effectiveness and Portabil-
ity of Reinforcement Learned Dialogue Strategies
with real users: the TALK TownInfo Evaluation. In
IEEE/ACL Spoken Language Technology.
Oliver Lemon. 2008. Adaptive Natural Language
Generation in Dialogue using Reinforcement Learn-
ing. In Proceedings of SEMdial.
Oliver Lemon. 2010. Learning what to say and how to
say it: joint optimization of spoken dialogue man-
agement and Natural Language Generation. Com-
puter, Speech & Language, to appear.
Xingkun Liu, Verena Rieser, and Oliver Lemon. 2009.
A wizard-of-oz interface to study information pre-
sentation strategies for spoken dialogue systems. In
Proc. of the 1st International Workshop on Spoken
Dialogue Systems.
Crystal Nakatsu. 2008. Learning contrastive connec-
tives in sentence realization ranking. In Proc. of
SIGdial Workshop on Discourse and Dialogue.
Joseph Polifroni and Marilyn Walker. 2006. Learning
database content for spoken dialogue system design.
In Proc. of the IEEE/ACL workshop on Spoken Lan-
guage Technology (SLT).
Joseph Polifroni and Marilyn Walker. 2008. Inten-
sional Summaries as Cooperative Responses in Di-
alogue Automation and Evaluation. In Proceedings
of ACL.
Verena Rieser and Oliver Lemon. 2008. Learn-
ing Effective Multimodal Dialogue Strategies from
Wizard-of-Oz data: Bootstrapping and Evaluation.
In Proc. of ACL.
Verena Rieser and Oliver Lemon. 2009. Natural Lan-
guage Generation as Planning Under Uncertainty for
Spoken Dialogue Systems. In Proc. of EACL.
Verena Rieser, Xingkun Liu, and Oliver Lemon. 2009.
Optimal Wizard NLG Behaviours in Context. Tech-
nical report, Deliverable 4.2, CLASSiC Project.
Dan Shapiro and P. Langley. 2002. Separating skills
from preference: Using learning to program by re-
ward. In Proc. of the 19th International Conference
on Machine Learning (ICML).
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
Kees van Deemter. 2009. What game theory can do
for NLG: the case of vague language. In 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG).
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards developing general mod-
els of usability with PARADISE. Natural Language
Engineering, 6(3).
M. Walker, R. Passonneau, and J. Boland. 2001. Quan-
titative and qualitative evaluation of DARPA Com-
municator spoken dialogue systems. In Proc. of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413?456.
Steve Whittaker, Marilyn Walker, and Johanna Moore.
2002. Fish or Fowl: A Wizard of Oz evaluation
of dialogue strategies in the restaurant domain. In
Proc. of the International Conference on Language
Resources and Evaluation (LREC).
Andi Winterboer, Jiang Hu, Johanna D. Moore, and
Clifford Nass. 2007. The influence of user tailoring
and cognitive load on user performance in spoken
dialogue systems. In Proc. of the 10th International
Conference of Spoken Language Processing (Inter-
speech/ICSLP).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
1018
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 49?54,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Web-based Evaluation Framework for Spatial Instruction-Giving Systems
Srinivasan Janarthanam, Oliver Lemon, and Xingkun Liu
Interaction Lab
School of Mathematical and Computer Sciences
Heriot Watt University, Edinburgh
sc445,o.lemon,x.liu@hw.ac.uk
Abstract
We demonstrate a web-based environment for
development and testing of different pedes-
trian route instruction-giving systems. The
environment contains a City Model, a TTS
interface, a game-world, and a user GUI in-
cluding a simulated street-view. We describe
the environment and components, the metrics
that can be used for the evaluation of pedes-
trian route instruction-giving systems, and the
shared challenge which is being organised us-
ing this environment.
1 Introduction
Generating navigation instructions in the real world
for pedestrians is an interesting research problem
for researchers in both computational linguistics
and geo-informatics (Dale et al, 2003; Richter and
Duckham, 2008). These systems generate verbal
route directions for users to go from A to B, and
techniques range from giving ?a priori? route direc-
tions (i.e. all route information in a single turn) and
incremental ?in-situ? instructions, to full interactive
dialogue systems (see section 4). One of the major
problems in developing such systems is in evaluat-
ing them with real users in the real world. Such eval-
uations are expensive, time consuming and painstak-
ing to organise, and are carried out not just at the end
of the project but also during the development cycle.
Consequently, there is a need for a common platform
to effectively compare the performances of verbal
navigation systems developed by different teams us-
ing a variety of techniques (e.g. a priori vs. in-situ
or rule-based vs. machine learning).
This demonstration system brings together exist-
ing online data resources and software toolkits to
create a low-cost framework for evaluation of pedes-
trian route instruction systems. We have built a
web-based environment containing a simulated real
world in which users can simulate walking on the
streets of real cities whilst interacting with differ-
ent navigation systems. This evaluation framework
will be used in the near future to evaluate a series of
instruction-giving dialogue systems.
2 Related work
The GIVE challenge developed a 3D virtual in-
door environment for development and evaluation
of indoor pedestrian navigation instruction systems
(Koller et al, 2007; Byron et al, 2007). In this
framework, users can walk through a building with
rooms and corridors, similar to a first-person shooter
game. The user is instructed by a navigation sys-
tem that generates route instructions. The basic idea
was to have several such navigation systems hosted
on the GIVE server and evaluate them in the same
game worlds, with a number of users over the in-
ternet. Conceptually our work is very similar to the
GIVE framework, but its objective is to evaluate sys-
tems that instruct pedestrian users in the real world.
The GIVE framework has been successfully used for
comparative evaluation of several systems generat-
ing instructions in virtual indoor environments.
Another system, ?Virtual Navigator?, is a simu-
lated 3D environment that simulates the real world
for training blind and visually impaired people to
learn often-used routes and develop basic naviga-
tion skills (McGookin et al, 2010). The framework
49
uses haptic force-feedback and spatialised auditory
feedback to simulate the interaction between users
and the environment they are in. The users simulate
walking by using arrow keys on a keyboard and by
using a device that works as a 3D mouse to simulate
a virtual white cane. Auditory clues are provided
to the cane user to indicate for example the differ-
ence between rush hour and a quiet evening in the
environment. While this simulated environment fo-
cusses on the providing the right kind of tactile and
auditory feedback to its users, we focus on provid-
ing a simulated environment where people can look
at landmarks and navigate based on spatial and vi-
sual instructions provided to them.
User simulation modules are usually developed
to train and test reinforcement learning based in-
teractive spoken dialogue systems (Janarthanam and
Lemon, 2009; Georgila et al, 2006; Schatzmann et
al., 2006). These agents replace real users in interac-
tion with dialogue systems. However, these models
simulate the users? behaviours in addition to the en-
vironment in which they operate. Users? dialogue
and physical behaviour are dependent on a number
of factors such as a user?s preferences, goals, knowl-
edge of the environment, environmental constraints,
etc. Simulating a user?s behaviour realistically based
on many such features requires large amounts of
data. In contrast to this approach, we propose a sys-
tem where only the spatial and visual environment is
simulated.
See section 4 for a discussion of different pedes-
trian navigation systems.
3 Architecture
The evaluation framework architecture is shown in
figure 1. The server side consists of a broker module,
navigation system, gameworld server, TTS engine,
and a city model. On the user?s side is a web-based
client that consists of the simulated real-world and
the interaction panel.
3.1 Game-world module
Walking aimlessly in the simulated real world can be
a boring task. Therefore, instead of giving web users
navigation tasks from A to B, we embed navigation
tasks in a game-world overlaid on top of the simu-
lated real world. We developed a ?treasure hunting?
game which consists of users solving several pieces
of a puzzle to discover the location of the treasure
chest. In order to solve the puzzle, they interact with
game characters (e.g. a pirate) to obtain clues as to
where the next clue is. This sets the user a number of
navigation tasks to acquire the next clues until they
find the treasure. In order to keep the game interest-
ing, the user?s energy depletes as time goes on and
they therefore have limited time to find the treasure.
Finally, the user?s performance is scored to encour-
age users to return. The game characters and enti-
ties like keys, chests, etc. are laid out on real streets
making it easy to develop a game without develop-
ing a game-world. New game-worlds can be easily
scripted using Javascript, where the location (lati-
tude and longitude) and behaviour of the game char-
acters are defined. The game-world module serves
game-world specifications to the web-based client.
3.2 Broker
The broker module is a web server that connects the
web clients to their corresponding different naviga-
tion systems. This module ensures that the frame-
work works for multiple users. Navigation systems
are instantiated and assigned to new users when they
first connect to the broker. Subsequent messages
from the users will be routed to the assigned navi-
gation system. The broker communicates with the
navigation systems via a communication platform
thereby ensuring that different navigation systems
developed using different languages (such as C++,
Java, Python, etc) are supported.
3.3 Navigation system
The navigation system is the central component of
this architecture, which provides the user instruc-
tions to reach their destinations. Each navigation
system is run as a server remotely. When a user?s
client connects to the server, it instantiates a navi-
gation system object and assigns it to the user ex-
clusively. Every user is identified using a unique id
(UUID), which is used to map the user to his/her re-
spective navigation system. The navigation system
is introduced in the game scenario as a buddy sys-
tem that will help the user in his objective: find the
treasure. The web client sends the user?s location to
the system periodically (every few seconds).
50
Figure 1: Evaluation framework architecture
3.4 TTS engine
Alongside the navigation system we use the Cere-
proc text-to-speech engine that converts the utter-
ances of the system into speech. The URL of the
audio file is then sent to the client?s browser which
then uses the audio plugin to play the synthesized
speech to the user. The TTS engine need not be used
if the output modality of the system is just text.
3.5 City Model
The navigation system is supported by a database
called the City Model. The City Model is a GIS
database containing a variety of data required to sup-
port navigation tasks. It has been derived from an
open-source data source called OpenStreetMaps1. It
consists of the following:
? Street network data: the street network data
consists of nodes and ways representing junc-
tions and streets.
? Amenities: such as ATMs, public toilets, etc.
? Landmarks: other structures that can serve as
landmarks. E.g. churches, restaurants, etc.
The amenities and landmarks are represented as
nodes (with latitude and longitude information). The
City Model interface API consists of a number of
1www.openstreetmaps.org
subroutines to access the required information such
as the nearest amenity, distance or route from A to B,
etc. These subroutines provide the interface between
the navigation systems and the database.
3.6 Web-based client
The web-based client is a JavaScript/HTML pro-
gram running on the user?s web browser software
(e.g. Google Chrome). A snapshot of the webclient
is shown in figure 2. It has two parts: the streetview
panel and the interaction panel.
Streetview panel: the streetview panel presents a
simulated real world visually to the user. When
the page loads, a Google Streetview client (Google
Maps API) is created with an initial user coordinate.
Google Streetview is a web service that renders a
panoramic view of real streets in major cities around
the world. This client allows the web user to get a
panoramic view of the streets around the user?s vir-
tual location. A gameworld received from the server
is overlaid on the simulated real world. The user can
walk around and interact with game characters using
the arrow keys on his keyboard or the mouse. As the
user walks around, his location (stored in the form
of latitude and longitude coordinates) gets updated
locally. Streetview also returns the user?s point of
view (0-360 degrees), which is also stored locally.
Interaction panel: the web-client also includes an
51
interaction panel that lets the user interact with his
buddy navigation system. In addition to user lo-
cation information, users can also interact with the
navigation system using textual utterances or their
equivalents. We provide users with two types of in-
teraction panel: a GUI panel and a text panel. In the
GUI panel, there are GUI objects such as buttons,
drop-down lists, etc. which can be used to construct
requests and responses to the system. By clicking
the buttons, users can send abstract semantic repre-
sentations (dialogue actions) that are equivalent to
their textual utterances. For example, the user can
request a route to a destination by selecting the street
name from a drop down list and click on the Send
button. Similarly, users can click on ?Yes?, ?No?,
?OK?, etc. buttons to respond to the system?s ques-
tions and instructions. In the text panel, on the other
hand, users are free to type any request or response
they want. Of course, both types of inputs are parsed
by the navigation system. We also plan to add an ad-
ditional input channel that can stream user speech to
the navigation system in the future.
4 Candidate Navigation Systems
This framework can be used to evaluate a variety
of navigation systems. Route navigation has been
an interesting research topic for researchers in both
geoinformatics and computational linguistics alike.
Several navigation prototype systems have been de-
veloped over the years. Although there are several
systems that do not use language as a means of com-
munication for navigation tasks (instead using geo-
tagged photographs (Beeharee and Steed, 2006; Hi-
ley et al, 2008), haptics (Bosman et al, 2003), mu-
sic (Holland et al, 2002; Jones et al, 2008), etc), we
focus on systems that generate instructions in natu-
ral language. Therefore, our framework does not in-
clude systems that generate routes on 2D/3D maps
as navigation aids.
Systems that generate text/speech can be further
classified as follows:
? ?A priori? systems: these systems generate
route instructions prior to the users touring the
route. These systems describe the entire route
before the user starts navigating. Several web
services exist that generate such lists of step-
by-step instructions (e.g. Google/Bing direc-
tions).
? ?In-situ? or incremental route instruction sys-
tems: these systems generate route instructions
incrementally along the route. e.g. CORAL
(Dale et al, 2003). They keep track of the
user?s location and issue the next instruction
when the user reaches the next node on the
planned route. The next instruction tells the
user how to reach the new next node. Some
systems do not keep track of the user, but re-
quire the user to request the next instruction
when they reach the next node.
? Interactive navigation systems: these systems
are both incremental and interactive. e.g.
DeepMap (Malaka and Zipf, 2000). These
systems keep track of the user?s location and
proactively generate instructions based on user
proximity to the next node. In addition, they
can interact with users by asking them ques-
tions about entities in their viewshed. For ex-
ample ?Can you see a tower at about 100 feet
away??. Questions like these will let the system
assess the user?s location and thereby adapt its
instruction to the situated context.
5 Evaluation metrics
Navigation systems can be evaluated using two
kinds of metrics using this framework. Objective
metrics such as time taken by the user to finish
each navigation task and the game, distance trav-
elled, number of wrong turns, etc. can be directly
measured from the environment. Subjective met-
rics based on each user?s ratings of different features
of the system can be obtained through user satisfac-
tion questionnaires. In our framework, users are re-
quested to fill in a questionnaire at the end of the
game. The questionnaire consists of questions about
the game, the buddy, and the user himself, for exam-
ple:
? Was the game engaging?
? Would you play it again (i.e. another similar
gameworld)?
? Did your buddy help you enough?
52
Figure 2: Snapshot of the web client
? Were the buddy instructions easy to under-
stand?
? Were the buddy instructions ever wrong or mis-
placed?
? If you had the chance, will you choose the same
buddy in the next game?
? How well did you know the neighbourhood of
the gameworld before the game?
6 Evaluation scenarios
We aim to evaluate navigation systems under a vari-
ety of scenarios.
? Uncertain GPS: GPS positioning available in
smartphones is erroneous (Zandbergen and
Barbeau, 2011). Therefore, one scenario for
evaluation would be to test how robustly nav-
igation systems handle erroneous GPS signals
from the user?s end.
? Output modalities: the output of navigation
systems can be presented in two modalities:
text and speech. While speech may enable a
hands-free eyes-free navigation, text displayed
on navigation aids like smartphones may in-
crease cognitive load. We therefore believe it
will be interesting to evaluate the systems in
both conditions and compare the results.
? Noise in user speech: for systems that take
as input user speech, it is important to handle
noise in such a channel. Noise due to wind and
traffic is most common in pedestrian scenarios.
Scenarios with different levels of noise settings
can be evaluated.
? Adaptation to users: returning users may have
learned the layout of the game world. An inter-
esting scenario is to examine how navigation
systems adapt to user?s increasing spatial and
visual knowledge.
Errors in GPS positioning of the user and noise
in user speech can be simulated at the server end,
thereby creating a range of challenging scenarios to
evaluate the robustness of the systems.
7 The Shared Challenge
We plan to organise a shared challenge for outdoor
pedestrian route instruction generation, in which a
variety of systems can be evaluated. Participating
research teams will be able to use our interfaces
and modules to develop navigation systems. Each
team will be provided with a development toolkit
53
and documentation to setup the framework in their
local premises for development purposes. Devel-
oped systems will be hosted on our challenge server
and a web based evaluation will be organised in con-
sultation with the research community (Janarthanam
and Lemon, 2011).
8 Demonstration system
At the demonstration, we will present the evaluation
framework along with a demo navigation dialogue
system. The web-based client will run on a laptop
using a high-speed broadband connection. The nav-
igation system and other server modules will run on
a remote server.
Acknowledgments
The research has received funding from the
European Community?s Seventh Framework
Programme (FP7/2007-2013) under grant
agreement no. 216594 (SPACEBOOK project
www.spacebookproject.org).
References
Ashweeni K. Beeharee and Anthony Steed. 2006. A nat-
ural wayfinding exploiting photos in pedestrian navi-
gation systems. In Proceedings of the 8th conference
on Human-computer interaction with mobile devices
and services (2006).
S. Bosman, B. Groenendaal, J. W. Findlater, T. Visser,
M. de Graaf, and Panos Markopoulos. 2003. Gen-
tleGuide: An Exploration of Haptic Output for Indoors
Pedestrian Guidance. In Proceedings of 5th Interna-
tional Symposium, Mobile HCI 2003, Udine, Italy.
D. Byron, A. Koller, J. Oberlander, L. Stoia, and
K. Striegnitz. 2007. Generating Instructions in Vir-
tual Environments (GIVE): A challenge and evaluation
testbed for NLG. In Proceedings of the Workshop on
Shared Tasks and Comparative Evaluation in Natural
Language Generation.
Robert Dale, Sabine Geldof, and Jean-Philippe Prost.
2003. CORAL : Using Natural Language Generation
for Navigational Assistance. In Proceedings of the
Twenty-Sixth Australasian Computer Science Confer-
ence (ACSC2003), 4th7th February, Adelaide, South
Australia.
Kallirroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In Proceedings of Inter-
speech/ICSLP, pages 1065?1068.
Harlan Hiley, Ramakrishna Vedantham, Gregory Cuel-
lar, Alan Liuy, Natasha Gelfand, Radek Grzeszczuk,
and Gaetano Borriello. 2008. Landmark-based pedes-
trian navigation from collections of geotagged photos.
In Proceedings of the 7th International Conference on
Mobile and Ubiquitous Multimedia (MUM) 2008.
S. Holland, D. Morse, and H. Gedenryd. 2002. Audio-
gps: Spatial audio navigation with a minimal atten-
tion interface. Personal and Ubiquitous Computing,
6(4):253?259.
Srini Janarthanam and Oliver Lemon. 2009. A User Sim-
ulation Model for learning Lexical Alignment Policies
in Spoken Dialogue Systems. In European Workshop
on Natural Language Generation.
Srini Janarthanam and Oliver Lemon. 2011. The
GRUVE Challenge: Generating Routes under Uncer-
tainty in Virtual Environments. In Proceedings of
ENLG / Generation Challenges.
M. Jones, S. Jones, G. Bradley, N. Warren, D. Bainbridge,
and G. Holmes. 2008. Ontrack: Dynamically adapt-
ing music playback to support navigation. Personal
and Ubiquitous Computing, 12(7):513?525.
A. Koller, J. Moore, B. Eugenio, J. Lester, L. Stoia,
D. Byron, J. Oberlander, and K. Striegnitz. 2007.
Shared Task Proposal: Instruction Giving in Virtual
Worlds. In Workshop on Shared Tasks and Compar-
ative Evaluation in Natural Language Generation.
Rainer Malaka and Er Zipf. 2000. Deep Map - chal-
lenging IT research in the framework of a tourist in-
formation system. In Information and Communication
Technologies in Tourism 2000, pages 15?27. Springer.
D. McGookin, R. Cole, and S. Brewster. 2010. Vir-
tual navigator: Developing a simulator for independent
route learning. In Proceedings of Workshop on Haptic
Audio Interaction Design 2010, Denmark.
Kai-Florian Richter and Matt Duckham. 2008. Simplest
instructions: Finding easy-to-describe routes for navi-
gation. In Proceedings of the 5th international confer-
ence on Geographic Information Science.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. The Knowledge Engi-
neering Review, 21:97?126.
P. A. Zandbergen and S. J. Barbeau. 2011. Positional
accuracy of assisted gps data from high-sensitivity
gps-enabled mobile phones. Journal of Navigation,
64(3):381?399.
54
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1254?1263,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Conditional Random Fields for Responsive Surface Realisation using
Global Features
Nina Dethlefs, Helen Hastie, Heriberto Cuaya?huitl and Oliver Lemon
Mathematical and Computer Sciences
Heriot-Watt University, Edinburgh
n.s.dethlefs | h.hastie | h.cuayahuitl | o.lemon@hw.ac.uk
Abstract
Surface realisers in spoken dialogue sys-
tems need to be more responsive than con-
ventional surface realisers. They need to
be sensitive to the utterance context as well
as robust to partial or changing generator
inputs. We formulate surface realisation as
a sequence labelling task and combine the
use of conditional random fields (CRFs)
with semantic trees. Due to their extended
notion of context, CRFs are able to take
the global utterance context into account
and are less constrained by local features
than other realisers. This leads to more
natural and less repetitive surface realisa-
tion. It also allows generation from partial
and modified inputs and is therefore ap-
plicable to incremental surface realisation.
Results from a human rating study confirm
that users are sensitive to this extended no-
tion of context and assign ratings that are
significantly higher (up to 14%) than those
for taking only local context into account.
1 Introduction
Surface realisation typically aims to produce out-
put that is grammatically well-formed, natural and
cohesive. Cohesion can be characterised by lexical
or syntactic cues such as repetitions, substitutions,
ellipses, or connectives. In automatic language
generation, such properties can sometimes be dif-
ficult to model, because they require rich context-
awareness that keeps track of all (or much) of what
was generated before, i.e. a growing generation
history. In text generation, cohesion can span over
the entire text. In interactive settings such as gen-
eration within a spoken dialogue system (SDS), a
challenge is often to keep track of cohesion over
several utterances. In addition, since interactions
are dynamic, generator inputs from the dialogue
manager can sometimes be partial or subject to
subsequent modification. This has been addressed
by work on incremental processing (Schlangen
and Skantze, 2009). Since dialogue acts are passed
on to the generation module as soon as possible,
this can sometimes lead to incomplete generator
inputs (because the user is still speaking), or in-
puts that are subject to later modification (because
of an initial ASR mis-recognition).
In this paper, we propose to formulate surface
realisation as a sequence labelling task. We use
conditional random fields (Lafferty et al, 2001;
Sutton and McCallum, 2006), which are suitable
for modelling rich contexts, in combination with
semantic trees for rich linguistic information. This
combination is able to keep track of dependen-
cies between syntactic, semantic and lexical fea-
tures across multiple utterances. Our model can
be trained from minimally labelled data, which re-
duces development time and may (in the future)
facilitate an application to new domains.
The domain used in this paper is a pedestrian
walking around a city looking for information and
recommendations for local restaurants from an
SDS. We describe here the module for surface re-
alisation. Our main hypothesis is that the use of
global context in a CRF with semantic trees can
lead to surface realisations that are better phrased,
more natural and less repetitive than taking only
local features into account. Results from a human
rating study confirm this hypothesis. In addition,
we compare our system with alternative surface
realisation methods from the literature, namely, a
rank and boost approach and n-grams.
Finally, we argue that our approach lends itself
1254
to surface realisation within incremental systems,
because CRFs are able to model context across
full as well as partial generator inputs which may
undergo modifications during generation. As a
demonstration, we apply our model to incremen-
tal surface realisation in a proof-of-concept study.
2 Related Work
Our approach is most closely related to Lu et
al. (2009) who also use CRFs to find the best
surface realisation from a semantic tree. They
conclude from an automatic evaluation that using
CRF-based generation which takes long-range de-
pendencies into account outperforms several base-
lines. However, Lu et al?s generator does not take
context beyond the current utterance into account
and is thus restricted to local features. Further-
more, their model is not able to modify generation
results on the fly due to new or updated inputs.
In terms of surface realisation from graphical
models (and within the context of SDSs), our ap-
proach is also related to work by Georgila et al
(2002) and Dethlefs and Cuaya?huitl (2011b), who
use HMMs, Dethlefs and Cuaya?huitl (2011a) who
use Bayes Nets, and Mairesse et al (2010) who
use Dynamic Bayes Nets within an Active Learn-
ing framework. The last approach is also con-
cerned with generating restaurant recommenda-
tions within an SDS. Specifically, their system op-
timises its performance online, during the interac-
tion, by asking users to provide it with new textual
descriptions of concepts, for which it is unsure of
the best realisation. In contrast to these related
approaches, we use undirected graphical models
which are useful when the natural directionality
between the input variables is unknown.
In terms of surface realisation for SDSs, Oh and
Rudnicky (2000) present foundational work in us-
ing an n-gram-based system. They train a surface
realiser based on a domain-dependent language
model and use an overgeneration and ranking ap-
proach. Candidate utterances are ranked accord-
ing to a penalty function which penalises too long
or short utterances, repetitious utterances and ut-
terances which either contain more or less infor-
mation than required by the dialogue act. While
their approach is fast to execute, it has the dis-
advantage of not being able to model long-range
dependencies. They show that humans rank their
output equivalently to template-based generation.
Further, our approach is related to the SPaRKy
sentence generator (Walker et al, 2007). SPaRKy
was also developed for the domain of restaurant
recommendations and was shown to be equivalent
to or better than a carefully designed template-
based generator which had received high human
ratings in the past (Stent et al, 2002). It generates
sentences in two steps. First, it produces a ran-
domised set of alternative realisations, which are
then ranked according to a mapping from sentence
plans to predicted human ratings using a boosting
algorithm. As in our approach, SPaRKy distin-
guishes local and global features. Local features
take only information of the current tree node into
account, including its parents, siblings and chil-
dren, while global features take information of the
entire utterance into account. While SPaRKy is
shown to reach high output quality in compari-
son to a template-based baseline, the authors ac-
knowledge that generation with SPaRKy is rather
slow when applied in a real-time SDS. This could
present a problem in incremental settings, where
generation speed is of particular importance.
The SPaRKy system is also used by Rieser et
al. (2011), who focus on information presentation
strategies for restaurant recommendations, sum-
maries or comparisons within an SDS. Their sur-
face realiser is informed by the highest ranked
SPaRKy outputs for a particular information pre-
sentation strategy and will constitute one of our
baselines in the evaluation.
More work on trainable realisation for SDSs
generally includes Bulyko and Ostendorf (2002)
who use finite state transducers, Nakatsu and
White (2006) who use supervised learning, Varges
(2006) who uses chart generation, and Konstas
and Lapata (2012) who use weighted hypergraphs,
among others.
3 Cohesion across Utterances
3.1 Tree-based Semantic Representations
The restaurant recommendations we generate can
include any of the attributes shown in Table 1.
It is then the task of the surface realiser to find
the best realisation, including whether to present
them in one or several sentences. This often is
a sentence planning decision, but in our approach
it is handled using CRF-based surface realisation.
The semantic forms underlying surface realisation
can be produced in many ways. In our case, they
are produced by a reinforcement learning agent
which orders semantic attributes in the tree ac-
1255
Timing and Ordering
Surface Realisation
User
Interaction 
Micro-turn dialogue 
act, inform(food=Thai)
Semantic tree
String of words
intervening modules
speech
semantics of 
user utterance
(synthesised)
Manager
Figure 1: Architecture of our SDS with a focus on
the NLG components. While the user is speaking,
the dialogue manager sends dialogue acts to the
NLG module, which uses reinforcement learning
to order semantic attributes and produce a seman-
tic tree (see Dethlefs et al (2012b)). This paper fo-
cuses on surface realisation from these trees using
a CRF as shown in the surface realisation module.
Slot Example
ADDRESS The venue?s address is . . .
AREA It is located in . . .
FOOD The restaurant serves . . . cuisine.
NAME The restaurant?s name is . . .
PHONE The venue?s phone number is . . .
POSTCODE The postcode is . . .
QUALITY This is a . . . venue.
PRICE It is located in the . . . price range.
SIGNATURE The venue specialises in . . .
VENUE This venue is a . . .
Table 1: Semantic slots required for our domain
along with example realisations. Attributes can be
combined in all possible ways during generation.
cording to their confidence in the dialogue. This
is because SDSs can often have uncertainties with
regard to the user?s actual desired attribute values
due to speech recognition inaccuracies. We there-
fore model all semantic slots as probability distri-
butions, such as inform(food=Indian, 0.6) or in-
form(food=Italian, 0.4) and apply reinforcement
learning to finding the optimal sequence for pre-
sentation. Please see Dethlefs et al (2012b) for
details. Here, we simply assume that a semantic
form has been produced by a previous processing
module.
As shown in the architecture diagram in Fig-
ure 1, a CRF surface realiser takes a semantic
tree as input. We represent these as context-free
trees which can be defined formally as 4-tuples
Lexical
features
Syntactic
features
Semantic
features
The Beluga is a great Italian restaurant
y0 y1 y2
root
inform(
name=
Beluga)
The Beluga
root
inform(
venue=
Restaurant)
is a great Italian
inform(
type=
Italian)
root
restaurant
(a)
(b)
The 
Beluga
is a great
Italian
restaurant
other
phrases
(c)
Figure 2: (a) Graphical representation of a linear-
chain Conditional Random Field (CRF), where
empty nodes correspond to the labelled sequence,
shaded nodes to linguistic observations, and dark
squares to feature functions between states and ob-
servations; (b) Example semantic trees that are up-
dated at each time step in order to provide linguis-
tic features to the CRF (only one possible surface
realisation is shown and parse categories are omit-
ted for brevity); (c) Finite state machine of phrases
(labels) for this example.
{S, T,N,H}, where S is a start symbol, typically
the root node of the tree; T = {t0, t1, t2 . . . t|T |}
is a set of terminal symbols, corresponding to sin-
gle phrases; N = {n0, n1, n2 . . . n|N |} is a set of
non-terminal symbols corresponding to semantic
categories, and H = {h0, h1, h2 . . . h|H|} is a set
of production rules of the form n ? ?, where
n ? N , ? ? T ? N . The production rules rep-
resent alternatives at each branching node where
the CRF is consulted for the best available expan-
sion from the subset of possible ones. All nodes
in the tree are annotated with a semantic concept
(obtained from the semantic form) as well as their
parse category.
3.2 Conditional Random Fields for
Phrase-Based Surface Realisation
The main idea of our approach is to treat surface
realisation as a sequence labelling task in which a
sequence of semantic inputs needs to be labelled
with appropriate surface realisations. The task is
therefore to find a mapping between (observed)
1256
lexical, syntactic and semantic features and a (hid-
den) best surface realisation.
We use the linear-chain Conditional Random
Field (CRF) model for statistical phrase-based sur-
face realisation, see Figure 2 (a). This probabilis-
tic model defines the posterior probability of la-
bels (surface realisation phrases) y={y1, . . . , y|y|}
given features x={x1, . . . , x|x|} (informed by a se-
mantic tree, see Figure 2 (b)), as
P (y|x) = 1Z(x)
T?
t=1
exp
{ K?
k=1
?k?k(yt, yt?1, xt)
}
,
where Z(x) is a normalisation factor over all pos-
sible realisations (i.e. labellings) of x such that the
sum of all terms is one. The parameters ?k are
weights corresponding to feature functions ?k(.),
which are real values describing the label state y
at time t based on the previous label state yt?1 and
features xt. For example: from Figure 2 (c), ?k
might have the value ?k = 1.0 for the transition
from ?The Beluga? to ?is a great Italian?, and 0.0
elsewhere. The parameters ?k are set to maximise
the conditional likelihood of phrase sequences in
the training data set. They are estimated using the
gradient ascent algorithm.
After training, labels can be predicted for new
sequences of observations. The most likely phrase
sequence is expressed as
y ? = argmax
y
P (y|x),
which is computed using the Viterbi algorithm.
We use the Mallet package1 (McCallum, 2002) for
parameter learning and inference.
3.3 Feature Selection and Training
The following features define the generation con-
text used during training of the CRF. The genera-
tion context includes everything that has been gen-
erated for the current utterance so far. All features
can be obtained from a semantic input tree.
? Lexical items of parents and siblings,
? Semantic types in expansion,
? Semantic types of parents and siblings,
? Parse category of expansion,
? Parse categories of parents and siblings.
We use the StanfordParser2 (Marneffe et al, 2006)
to obtain the parse category for each tree node.
1http://mallet.cs.umass.edu/
2http://nlp.stanford.edu/software/
lex-parser.shtml
The semantics for each node are derived from the
input dialogue acts (these are listed in Table 1) and
are associated with nodes. The lexical items are
present in the generation context and are mapped
to semantic tree nodes.
As an example, for generating an utterance (la-
bel sequence) such as The Beluga is a great restau-
rant. It is located in the city centre., each gen-
eration step needs to take the features of the en-
tire generation history into account. This includes
all individual lexical items generated, the seman-
tic types used and the parse categories for each
tree node involved. For the first constituent, The
Beluga, this corresponds to the features {? BE-
GIN NAME} indicating the beginning of a sentence
(where empty features are omitted), the beginning
of a new generation context and the next semantic
slot required. For the second constituent, is a great
restaurant, the features are {THE BELUGA NAME
NP VENUE}, i.e. including the generation history
(with lexical items and parse category added for
the first constituent) and the semantics of the next
required slot, VENUE. In this way, a sequence of
surface form constituents is generated correspond-
ing to latent states in the CRF.
Since global utterance features capture the full
generation context (i.e. beyond the current ut-
terance), we are also able to model phenomena
such as co-references and pronouns. This is useful
for longer restaurant recommendations which may
span over more than one utterance. If the genera-
tion history already contains a semantic attribute,
e.g. the restaurant name, the CRF may afterwards
choose a pronoun, e.g. it, which has a higher like-
lihood than using the proper name again. Simi-
larly, the CRF may decide to realise a new attribute
as constituents of different order, such as a sen-
tence or PP, depending on the length, number and
parse categories of previously generated output. In
this way, our approach implicitly treats sentence
planning decisions such as the distribution of con-
tent over a set of messages in the same way as (or
as part of) surface realisation. A further capabil-
ity of our surface realiser is that it can generate
complete phrases from full as well as partial dia-
logue acts. This is useful in interactive contexts,
where we need as much robustness as possible. A
demonstration of this is given in Section 5 in an
application to incremental surface realisation.
To train the CRF, we used a data set of 552
restaurant recommendations from the website The
1257
List.3 The data contains recommendations such as
Located in the city centre, Beluga is a stylish yet
laid-back restaurant with a smart menu of modern
European cuisine.
3.4 Grammar Induction
The grammar g of surface realisation candidates
is obtained through an automatic grammar induc-
tion algorithm which can be run on unlabelled
data and requires only minimal human interven-
tion. This grammar defines the surface realisa-
tion space for the CRFs. We provide the human
corpus of restaurant recommendations from Sec-
tion 3.3 as input to grammar induction. The al-
gorithm is shown in Algorithm 1. It first identi-
fies all semantic attributes of interest in an utter-
ance, in our case those specified in Table 1, and re-
places them by a variable. These attributes include
food types, such as Mexican, Chinese, particular
parts of town, prices, etc. About 45% of them can
be identified based on heuristics. The remainder
needs to be hand-annotated at the moment, which
includes mainly attributes like restaurant names or
quality attributes, such as delicate, exquisite, etc.
Subsequently, all utterances are parsed using the
Stanford parser to obtain constituents and are inte-
grated into the grammar under construction. The
non-terminal symbols are named after the auto-
matically annotated semantic attributes contained
in their expansion, e.g. NAME QUALITY ? The
$name$ is of $quality$ quality. In this way, each
non-terminal symbol has a semantic representa-
tion and an associated parse category. In total, our
induced grammar contains more than 800 rules.
4 Evaluation
To evaluate our approach, we focus on a sub-
jective human rating study which aims to deter-
mine whether CRF-based surface realisation that
takes the full generation context into account,
called CRF (global), is perceived better by human
judges than one that uses a CRF but just takes local
context into account, called CRF (local). While
CRF (global) uses features from the entire genera-
tion history, CRF (local) uses only features from
the current tree branch. We assume that cohe-
sion can be identified by untrained judges as natu-
ral, well-phrased and non-repetitive surface forms.
To examine differences in methodology between
3http://www.list.co.uk
Algorithm 1 Grammar Induction.
1: function FINDGRAMMAR(utterances u, semantic at-
tributes a) return grammar
2: for each utterance u do
3: if u contains a semantic attribute from a, such as
venue, cuisine, etc. then
4: Find and replace the attribute by its semantic
variable, e.g. $venue$.
5: end if
6: Parse the sentence and induce a set of rules ??
?, where ? is a semantic variable and ? is its parse.
7: Traverse the parse tree in a top-down, depth-first
search and
8: if expansion ? exists then
9: continue
10: else if non-terminal ? exists then
11: add new expansion ? to ?.
12: else write new rule ?? ?.
13: end if
14: Write grammar.
15: end for
16: end function
CRFs and other state-of-the-art methods, we also
compare our system to two other baselines:
? CLASSiC corresponds to the system re-
ported in Rieser et al (2011),4 which gen-
erates restaurant recommendations based on
the SPaRKy system (Walker et al, 2007), and
has received high ratings in the past. SPaRKy
uses global utterance features.
? n-grams represents a simple 5-gram baseline
that is similar to Oh and Rudnicky (2000)?s
system. We will sample from the most likely
slot realisations that do not contain a repeti-
tion and include exactly the required slot val-
ues. Local context only is taken into account.
4.1 Human Rating Study
We carried out a user rating study on the Crowd-
Flower crowd sourcing platform.5 Each partici-
pant was shown part of a real human-system dia-
logue that emerged as part of the CLASSiC project
evaluation (Rieser et al, 2011). All dialogues
and data are freely available from http://www.
classic-project.org. Each dialogue contained
two variations for one of the utterances. These
variations were generated from two out of the four
systems described above. The order that these
were presented to the participant was counterbal-
anced. Table 2 gives an example of a dialogue seg-
ment presented to the participants.
4In Rieser et al (2011), this system is referred to as the
TIP system, which generates summaries, comparisons or rec-
ommendations for restaurants. For the present study, we com-
1258
SYS Thank you for calling the Cambridge Information
system. Your call will be recorded for research pur-
poses. You may ask for information about a place
to eat, such as a restaurant, a pub, or a cafe. How
may I help you?
USR I want to find an American restaurant which is in
the very expensive area.
SYS
A
The restaurant Gourmet Burger is an outstanding,
expensive restaurant located in the central area.
SYS
B
Gourmet Burger is a smart and welcoming restau-
rant. Gourmet Burger provides an expensive dining
experience with great food and friendly service. If
you?re looking for a central meal at an expensive
price.
USR What is the address and phone number?
SYS Gourmet Burger is on Regent Street and its phone
number is 01223 312598.
USR Thank you. Good bye.
Table 2: Example dialogue for participants to
compare alternative outputs in italics, USR=user,
SYS A=CRF (global), SYS B=CRF(local).
System Natural Phrasing Repetit.
CRF global 3.65 3.64 3.65
CRF local 3.10? 3.19? 3.13?
CLASSiC 3.53? 3.59 3.48?
n-grams 3.01? 3.09? 3.32?
Table 3: Subjective user ratings. Significance with
CRF (global) at p<0.05 is indicated as ?.
44 participants gave a total of 1,830 ratings of
utterances produced across the four systems. Flu-
ent speakers of English only were requested and
the participants were from the United States. They
were asked to rate each utterance on a 5 point Lik-
ert scale in response to the following questions
(where 5 corresponds to totally agree and 1 cor-
responds to totally disagree):
? The utterance was natural, i.e. it could have
been produced by a human. (Natural)
? The utterance was phrased well. (Phrasing)
? The utterance was repetitive. (Repetitive)
4.2 Results
We can see from Table 3 that across all the cate-
gories, the CRF (global) gets the highest overall
ratings. This difference is significant for all cat-
egories compared with CRF (local) and n-grams
(using a 1-sided Mann Whitney U-test, p < 0.001).
pare only with the subset of recommendations.
5http://www.crowdflower.com
Possibly this is because the local context taken
into account by both systems was not enough to
ensure cohesion across surface phrases. It is not
possible, e.g., to cover co-references within a lo-
cal context only or discourse markers that refer be-
yond the current utterance. This can lead to short
and repetitive phrases, such as Make your way to
Gourmet Burger. The food quality is outstanding.
The prices are expensive. generated by the n-gram
baseline.
The CLASSiC baseline, based on SPaRKy, was
the most competitive system in our comparison.
None-the-less CRF (global) is rated higher across
categories and significantly so for Natural (p <
0.05) and Repetitive (p < 0.005). For Phrasing,
there is a trend but not a significant difference (p
< 0.16). All comparisons are based on a 1-sided
Mann Whitney U-test. A qualitative comparison
between the CRF (global) and CLASSiC outputs
showed the following. CLASSiC utterances tend
to be longer and contain more sentences than CRF
(global) utterances. While CRF (global) often de-
cides to aggregate attributes into one sentence,
such as the Beluga is an outstanding restaurant
in the city centre, CLASSiC tends to rely more on
individual messages, such as The Beluga is an out-
standing restaurant. It is located in the city cen-
tre. A possible reason is that while CRF (global)
is able to take features beyond an utterance into
account, CLASSiC/SPaRKy is restricted to global
features of the current utterance.
We can further compare our results with Rieser
et al (2011) and Mairesse et al (2010) who also
generate restaurant recommendations and asked
similar questions to participants as we did. Rieser
et al (2011)?s system received an average rating
of 3.586 in terms of Phrasing which compares to
our 3.64. This difference is not significant, and
in line with the user ratings we observed for the
CLASSiC system above (3.59). Mairesse et al
(2010) achieved an average score of 4.05 in terms
of Natural in comparison to our 3.65. This differ-
ence is significant at p<0.05. Possibly their better
performance is due to the data set being more ?in
domain? than ours. They collected data from hu-
mans that was written specifically for the task that
the system was tested on. In contrast, our system
was trained on freely available data that was writ-
ten by professional restaurant reviewers. Unfortu-
nately, we cannot compare across other categories,
6This was rescaled from a 1-6 scale.
1259
USR1 I?m looking for a nice restaurant in the centre.
SYS1 inform(area=centre [0.2], food=Thai [0.3])
inform(name=Bangkok [0.3])
So you?re looking for a Thai . . .
USR2 [barges in] No, I?m looking for a restaurant
with good quality food.
SYS2 inform(quality=good [0.6], name=Beluga [0.6])
Oh sorry, so a nice restaurant located . . .
USR3 [barges in] . . . in the city centre.
SYS3 inform(area=centre [0.8])
Table 4: Example dialogue where the dialogue
manager needs to send incremental updates to the
NLG. Incremental surface realisation from seman-
tic trees for this dialogue is shown in Figure 3.
because the authors tested only for Phrasing and
Natural, respectively.
5 Incremental Surface Realisation
Recent years have seen increased interest in
incremental dialogue processing (Skantze and
Schlangen, 2009; Schlangen and Skantze, 2009).
The main characteristic of incremental architec-
tures is that instead of waiting for the end of a user
turn, they begin to process the input stream as soon
as possible, updating their processing hypotheses
as more information becomes available. From a
dialogue perspective, they can be said to work on
partial rather than full dialogue acts.
With respect to surface realisation, incremen-
tal NLG systems have predominantly relied on
pre-defined templates (Purver and Otsuka, 2003;
Skantze and Hjalmarsson, 2010; Dethlefs et al,
2012a), which limits the flexibility and quality of
output generation. Buschmeier et al (2012) have
presented a system which systematically takes
the user?s acoustic understanding problems into
account by pausing, repeating or re-phrasing if
necessary. Their approach is based on SPUD
(Stone et al, 2003), a constraint satisfaction-based
NLG architecture and marks important progress
towards more flexible incremental surface realisa-
tion. However, given the human labour involved in
constraint specification, cohesion is often limited
to a local context. Especially for long utterances
or such that are separated by user turns, this may
lead to surface form increments that are not well
connected and lack cohesion.
5.1 Application to Incremental SR
This section will discuss a proof-of-concept appli-
cation of our approach to incremental surface re-
alisation. Table 4 shows an example dialogue be-
tween a user and system that contains a number
of incremental phenomena that require hypothe-
sis updates, system corrections and user barge-
ins. Incremental surface realisation for this dia-
logue is shown in Figure 3, where processing steps
are indicated as bold-face numbers and are trig-
gered by partial dialogue acts that are sent from
the dialogue manager, such as inform(area=centre
[0.2]). The numbers in square brackets indicate
the system?s confidence in the attribute-value pair.
Once a dialogue act is observed by the NLG sys-
tem, a reinforcement learning agent determines the
order of attributes and produces a semantic tree, as
described in Section 3.1. Since the semantic forms
are constructed incrementally, new tree nodes can
be attached to and deleted from an existing tree,
depending on what kind of update is required.
In the dialogue in Table 4, the user first asks
for a nice restaurant in the centre. The dialogue
manager constructs a first attribute-value slot, in-
form(area=centre [0.2], . . . ), and passes it on to
NLG.7 In Figure 3, we can observe the corre-
sponding NLG action, a first tree is created with
just a root node and a node representing the area
slot (step 1). In a second step, the semantically
annotated node gets expanded into a surface form
that is chosen from a set of candidates (shown in
curly brackets). The CRF is responsible for this
last step. Since there is no preceding utterance, the
best surface form is chosen based on the semantics
alone. Active tree nodes, i.e. those currently under
generation, are indicated as asterisks in Figure 3.
Currently inactive nodes are shown as circles.
Step 3 then further expands the current tree
adding a node for the food type and the name of
a restaurant that the dialogue manager had passed.
We see here that attributes can either be primitive
or complex. Primitive attributes contain a single
semantic type, such as area, whereas complex at-
tributes contain multiple types, such as food, name
and need to be decomposed in a later processing
step (see steps 4 and 6). Step 5 again uses the CRF
7Note here that the information passed on to the NLG is
distinct from the dialogue manager?s own actions. In the ex-
ample, the NLG is asked to generate a recommendation, but
the dialogue manager actually decides to clarify the user?s
preferences due to low confidence. This scenario is an exam-
ple of generator inputs that may get revised afterwards.
1260
root
(1) inform
(area=centre)
(2) Right in the city centre,
{located in $area$, if 
you're looking to eat 
in $area$, in $area$, ...} 
inform(area=
centre)
(3) inform(food=Thai
        name=Bangkok)
Right in the city centre, 
root
(6) inform
(food=Thai)
(4) inform(name=
              Bangkok)
(5) Bangkok
{the $name$, 
it is called $name$,  ...}
root
inform(area=
centre)
Right in the city centre, 
inform(food=Thai, 
name=Bangkok)
root
inform(area=
centre)
Right in the city centre, 
(7) inform(quality=very
good, name=Beluga)
inform(name=
        Bangkok)
inform
(food=Thai)
Bangkok
root
inform(area=
centre)
inform(quality=nice, 
name=Beluga)
Right in the city centre, 
(8) inform(name=
Beluga)
(10) inform(quality=
very good)
(9) the Beluga
{$name$, the venue 
called $name$, ...}
(11) is of very good quality. 
{is a $quality$ venue, if you want $quality$ 
food, $quality$, a $quality$ place ...}
*
*
*
*
*
* *
**
*
**
*
Figure 3: Example of incremental surface realisation, where each generation step is indicated by a num-
ber. Active generation nodes are shown as asterisks and deletions are shown as crossed out. Lexical and
semantic features are associated with their respective nodes. Syntactic information in the form of parse
categories are also taken into account for surface realisation, but have been omitted in this figure.
to obtain the next surface realisation that connects
with the previous one (so that a sequence of real-
isation ?labels? appears: Right in the city centre
and Bangkok). It takes the full generation context
into account to ensure a globally optimal choice.
This is important, because the local context would
otherwise be restricted to a partial dialogue act,
which can be much smaller than a full dialogue
act and thus lead to short, repetitive sentences.
The dialogue continues as the system implicitly
confirms the user?s preferred restaurant (SYS1).
At this point, we encounter a user barge-in correct-
ing the desired choice. As a consequence, the dia-
logue manager needs to update its initial hypothe-
ses and communicate this to NLG. Here, the last
three tree nodes need to be deleted from the tree
because the information is no longer valid. This
update and the deletion is shown in step 7. After-
wards, the dialogue continues and NLG involves
mainly expanding the current tree into a full se-
quence of surface realisations for partial dialogue
acts which come together into a full utterance.
This example illustrates three incremental pro-
cessing steps: expansions, updates and deletions.
Expansions are the most frequent operation. They
add new partial dialogue acts to the semantic tree.
They also consult the CRF for the best surface
realisation. Since CRFs are not restricted by the
Markov condition, they are less constrained by lo-
cal context than other models and can take non-
local dependencies into account. For our applica-
tion, the maximal context is 9 semantic attributes
(for a surface form that uses all possible 10 at-
tributes). While their extended context aware-
ness can often make CRFs slow to train, they are
fast at execution and therefore very applicable to
the incremental scenario. For applications involv-
ing longer-spanning alternatives, such as texts or
paragraphs, the context of the CRF would likely
have to be constrained. Updates are triggered by
the hypothesis updates of the dialogue manager.
Whenever a new attribute comes in, it is checked
against the generator?s existing knowledge. If it
is inconsistent with previous knowledge, an up-
date is triggered and often followed by a deletion.
Whenever generated output needs to be modified,
old expansions and surface forms are deleted first,
before new ones can be expanded in their place.
5.2 Updates and Processing Speed Results
Since fast responses are crucial in incremental sys-
tems, we measured the average time our system
took for a surface realisation. The time is 100ms
on a MacBook Intel Core 2.6 Duo with 8GB in
1261
RAM. This is slightly better than other incremen-
tal systems (Skantze and Schlangen, 2009) and
much faster than state-of-the-art non-incremental
systems such as SPaRKy (Walker et al, 2007).
In addition, we measured the number of neces-
sary generation updates in comparison to a non-
incremental setting. Since updates take effect di-
rectly on partial dialogue acts, rather than the full
generated utterance, we require around 50% less
updates as if generating from scratch for every
changed input hypothesis. A qualitative analysis
of the generated outputs showed that the quality is
comparable to the non-incremental case.
6 Conclusion and Future Directions
We have presented a novel technique for surface
realisation that treats generation as a sequence la-
belling task by combining a CRF with tree-based
semantic representations. An essential property
of interactive surface realisers is to keep track of
the utterance context including dependencies be-
tween linguistic features to generate cohesive ut-
terances. We have argued that CRFs are well
suited for this task because they are not restricted
by independence assumptions. In a human rating
study, we confirmed that judges rated our output
as better phrased, more natural and less repetitive
than systems that just take local features into ac-
count. This also holds for a comparison with state-
of-the-art rank and boost or n-gram approaches.
Keeping track of the global context is also impor-
tant for incremental systems since generator inputs
can be incomplete or subject to modification. In a
proof-of-concept study, we have argued that our
approach is applicable to incremental surface real-
isation. This was supported by preliminary results
on the speed, number of updates and quality dur-
ing generation. As future work, we plan to test
our model in a task-based setting using an end-to-
end SDS in an incremental and non-incremental
setting. This study will contain additional evalu-
ation categories, such as the understandability or
informativeness of system utterances. In addition,
we may compare different sequence labelling al-
gorithms for surface realisation (Nguyen and Guo,
2007) or segmented CRFs (Sarawagi and Cohen,
2005) and apply our method to more complex sur-
face realisation domains such as text generation or
summarisation. Finally, we would like to explore
methods for unsupervised data labelling so as to
facilitate portability across domains further.
Acknowledgements
The research leading to this work was funded by
the EC FP7 programme FP7/2011-14 under grant
agreement no. 287615 (PARLANCE).
References
Ivan Bulyko and Mari Ostendorf. 2002. Efficient in-
tegrated response generation from multiple targets
using weighted finite state transducers. Computer
Speech and Language, 16:533?550.
Hendrik Buschmeier, Timo Baumann, Benjamin
Dosch, Stefan Kopp, and David Schlangen. 2012.
Incremental Language Generation and Incremental
Speech Synthesis. In Proceedings of the 13th An-
nual SigDial Meeting on Discourse and Dialogue
(SIGdial), Seoul, South Korea.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011a. Com-
bining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Genera-
tion in Situated Dialogue. In Proceedings of the 13th
European Workshop on Natural Language Genera-
tion (ENLG), Nancy, France.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011b.
Hierarchical Reinforcement Learning and Hidden
Markov Models for Task-Oriented Natural Lan-
guage Generation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), Portland, Oregon, USA.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012a. Optimising Incremental Dialogue
Decisions Using Information Density for Interac-
tive Systems. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-CoNLL), Jeju, South Korea.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012b. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of the International Con-
ference on Natural Language Generation (INLG),
Chicago, Illinois, USA.
Kallirroi Georgila, Nikos Fakotakis, and George
Kokkinakis. 2002. Stochastic Language Modelling
for Recognition and Generation in Dialogue Sys-
tems. TAL (Traitement automatique des langues)
Journal, 43(3):129?154.
Ioannis Konstas and Mirella Lapata. 2012. Concept-
to-text Generation via Discriminative Reranking. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 369?
378, Jeju Island, Korea.
John D. Lafferty, Andrew McCallum, and Fer-
nando C.N. Pereira. 2001. Conditional Random
1262
Fields: Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing (ICML), pages 282?289.
Wei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009.
Natural Language Generation with Tree Conditional
Random Fields. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), Singapore.
Franc?ois Mairesse, Filip Jurc???c?ek, Simon Keizer,
Blaise Thomson, Kai Yu, and Steve Young. 2010.
Phrase-Based Statistical Language Generation Us-
ing Graphical Models and Active Learning. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL), Uppsala,
Sweden.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC),
Genoa, Italy.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Crystal Nakatsu and Michael White. 2006. Learning
to Say It Well: Reranking Realizations by Predicted
Synthesis Quality. In In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics (COLING-ACL) 2006, pages 1113?1120,
Sydney, Australia.
Nam Nguyen and Yunsong Guo. 2007. Comparisons
of Sequence Labeling Algorithms and Extensions.
In Proceedings of the International Conference on
Machine Learning (ICML), Corvallis, OR, USA.
Alice Oh and Alexander Rudnicky. 2000. Stochas-
tic Language Generation for Spoken Dialogue Sys-
tems. In Proceedings of the ANLP/NAACL Work-
shop on Conversational Systems, pages 27?32, Seat-
tle, Washington, USA.
Matthew Purver and Masayuki Otsuka. 2003. In-
cremental Generation by Incremental Parsing. In
In Proceedings of the 6th UK Special-Interesting
Group for Computational Linguistics (CLUK) Col-
loquium.
Verena Rieser, Simon Keizer, Xingkun Liu, and Oliver
Lemon. 2011. Adaptive Information Presentation
for Spoken Dialogue Systems: Evaluation with Hu-
man Subjects. In Proceedings of the 13th Euro-
pean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Sunita Sarawagi and William Cohen. 2005. Semi-
Markov Conditional Random Fields for Information
Extraction. Advances in Neural Information Pro-
cessing.
David Schlangen and Gabriel Skantze. 2009. A Gen-
eral, Abstract Model of Incremental Dialogue Pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. To-
wards Incremental Speech Generation in Dialogue
Systems. In Proceedings of the 11th Annual SigDial
Meeting on Discourse and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Athens, Greece.
Amanda Stent, Marilyn Walker, Steve Whittaker, and
Preetam Maloor. 2002. User-tailored Generation
for Spoken Dialogue: An Experiment. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with Communicative Intentions: The SPUD
System. Computational Intelligence, 19:311?381.
Charles Sutton and Andrew McCallum. 2006. Intro-
duction to Conditional Random Fields for Relational
Learning. In Lise Getoor and Ben Taskar, editors,
Introduction to Statistical Relational Learning. MIT
Press.
Sebastian Varges. 2006. Overgeneration and Ranking
for Spoken Dialogue Systems. In Proceedings of the
Fourth International Natural Language Generation
Conference (INLG), Sydney, Australia.
Marilyn Walker, Amanda Stent, Franc?ois Mairesse,
and Rashmi Prasad. 2007. Individual and Do-
main Adaptation in Sentence Planning for Dia-
logue. Journal of Artificial Intelligence Research,
30(1):413?456.
1263
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1660?1668,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Evaluating a City Exploration Dialogue System Combining
Question-Answering and Pedestrian Navigation
Srinivasan Janarthanam1, Oliver Lemon1, Phil Bartie2, Tiphaine Dalmas2,
Anna Dickinson2, Xingkun Liu1, William Mackaness2, and Bonnie Webber2
1 The Interaction Lab, Heriot-Watt University
2 Edinburgh University
sc445@hw.ac.uk
Abstract
We present a city navigation and tourist
information mobile dialogue app with in-
tegrated question-answering (QA) and ge-
ographic information system (GIS) mod-
ules that helps pedestrian users to nav-
igate in and learn about urban environ-
ments. In contrast to existing mobile apps
which treat these problems independently,
our Android app addresses the prob-
lem of navigation and touristic question-
answering in an integrated fashion using
a shared dialogue context. We evaluated
our system in comparison with Samsung
S-Voice (which interfaces to Google nav-
igation and Google search) with 17 users
and found that users judged our system to
be significantly more interesting to inter-
act with and learn from. They also rated
our system above Google search (with the
Samsung S-Voice interface) for tourist in-
formation tasks.
1 Introduction
We present a mobile dialogue system (an Android
app) called Spacebook that addresses the problem
of pedestrian navigation and tourist information in
urban environments. There has been little prior
work that addresses these two problems - naviga-
tion and tourist information provision - in an inte-
grated way. By navigation, we refer to the prob-
lem of finding appropriate destinations to go to
and the task of wayfinding to reach them and by
tourist information provision we refer to the prob-
lem of meeting the informational needs of a user
about entities such as museums, statues and fa-
mous personalities. A dialogue system such as this
could serve as a personal tour guide to pedestrian
tourists as they walk around unknown cities. With
the proliferation of smartphones, there has been a
number of mobile apps developed to address these
problems. However these apps have the following
problems: first, they demand the user?s visual at-
tention because they predominantly present infor-
mation on a mobile screen. This can be dangerous
in urban environments, as well as being distract-
ing. Second, these apps address the problems of
navigation and tourist information independently
and therefore do not have a shared interaction con-
text. This means that users cannot switch between
information and navigation tasks in a natural and
fluid manner.
User1: Take me to the National Museum.
System2: The National Museum is about 300m away..
System3: At the KFC, turn left on to South Bridge
System4 : Near you is the statue of David Hume.
User2: Who is David Hume.
System5: David Hume was a Scottish philosopher....
User3: Tell me more about David Hume.
System6: He was one of the most important figures in..
System7: You should be able to see the museum ...
User4: Tell me more about the museum.
System8: The National Museum of Scotland is a....
Table 1: An example interaction with the evalu-
ated system
In contrast to many existing mobile apps,
Spacebook has a speech-only interface and ad-
dresses both problems in an integrated way. We
conjecture that with a speech-only interface, users
can immerse themselves in exploring the city,
and that because of the shared context they can
switch between navigation and tourist information
tasks more easily. Using the navigational context,
Spacebook pushes point-of-interest information
which can then initiate tourist information tasks
using the QA module. Table 1 presents an example
interaction with our system showing the integrated
use of navigation and question-answering capabil-
1660
ities. Utterances System4-8 show the system?s ca-
pability to push information about nearby points-
of-interest (PoI) during a navigation task and an-
swer followup questions using the QA system (in
utterances User2 and User3). The final 3 utter-
ances show a natural switch between navigation to
an entity and QA about that entity.
We investigate whether our system using a com-
bination of geographical information system (GIS)
and natural language processing (NLP) technolo-
gies would be a better companion to pedestrian
city explorers than the current state-of-the-art mo-
bile apps. We hypothesize that, (1) users will find
our speech-only interface to navigation efficient as
it allows them to navigate without having to re-
peatedly look at a map and (2), that users will
find a dialogue interface which integrates touris-
tic question-answering and navigation within a
shared context to be useful for finding information
about entities in the urban environment. We first
present some related work in section 2. We de-
scribe the architecture of the system in section 3.
We then present our experimental design, results
and analysis in sections 5, 6 and 7.
2 Related work
Mobile apps such as Siri, Google Maps Naviga-
tion, Sygic, etc. address the problem of naviga-
tion while apps like Triposo, Guidepal, Wikihood,
etc. address the problem of tourist information by
presenting the user with descriptive information
about various points of interest (PoI) in the city.
While some exploratory apps present snippets of
information about a precompiled list of PoIs, other
apps dynamically generate a list of PoIs arranged
based on their proximity to the users. Users can
also obtain specific information about PoIs using
Search apps. Also, since these navigation and ex-
ploratory/search apps do not address both prob-
lems in an integrated way, users need to switch
between them and therefore lose interaction con-
text.
While most apps address these two problems
independently, some like Google Now, Google
Field Trip, etc, mix navigation with exploration.
But such apps present information primarily vi-
sually on the screen for the user to read. Some
of these are available for download at the Google
Play Android app store1. Several dialogue and
natural language systems have addressed the issue
1https://play.google.com/store
of pedestrian navigation (Malaka and Zipf, 2000;
Raubal and Winter, 2002; Dale et al, 2003; Bar-
tie and Mackaness, 2006; Shroder et al, 2011;
Dethlefs and Cuaya?huitl, 2011). There has also
been recent interest in shared tasks for generat-
ing navigation instructions in indoor and urban en-
vironments (Byron et al, 2007; Janarthanam and
Lemon, 2011). Some dialogue systems deal with
presenting information concerning points of inter-
est (Ko et al, 2005; Kashioka et al, 2011) and in-
teractive question answering (Webb and Webber,
2009).
In contrast, Spacebook has the objective of
keeping the user?s cognitive load low and prevent-
ing users from being distracted (perhaps danger-
ously so) from walking in the city (Kray et al,
2003). Also, it allows users to interleave the two
sub-tasks seamlessly and can keep entities dis-
cussed in both tasks in shared context (as shown
in Table 1).
3 Architecture
The architecture of the Spacebook system is
shown in figure 1. Our architecture brings to-
gether Spoken Dialogue Systems (SDS), Geo-
graphic Information Systems (GIS) and Question-
Answering (QA) technologies (Janarthanam et al,
2012). Its essentially a spoken dialogue system
(SDS) consisting of an automatic speech recog-
niser (ASR), a semantic parser, an Interaction
Manager, an utterance generator and a text-to-
speech synthesizer (TTS). The GIS modules in
this architecture are the City Model, the Visibility
Engine, and the Pedestrian tracker. Users commu-
nicate with the system using a smartphone-based
client app (an Android app) that sends users? po-
sition, pace rate, and spoken utterances to the sys-
tem, and delivers synthesised system utterances to
the user.
Figure 1: System Architecture
1661
3.1 Dialogue interface
The dialogue interface consists of a speech recog-
nition module, an utterance parser, an interaction
manager, an utterance generator and a speech syn-
thesizer. The Nuance 9 speech recogniser with
a domain specific language model was used for
speech recognition. The recognised speech is cur-
rently parsed using a rule-based parser into dia-
logue acts and semantic content.
The Interaction Manager (IM) is the central
component of this architecture, which provides
the user with navigational instructions, pushes PoI
information and manages QA questions. It re-
ceives the user?s input in the form of a dialogue
act (DA), the user?s location (latitude and longi-
tude) and pace rate. Based on these inputs and the
dialogue context, it responds with system output
dialogue act, based on a dialogue policy. The IM
initiates the conversation with a calibration phase
where the user?s initial location and orientation are
obtained. The user can then initiate tasks that in-
terest him/her. These tasks include searching for
an entity (e.g. a museum or a restaurant), request-
ing navigation instructions to a destination, ask-
ing questions about the entities in the City Model,
and so on. When the user is mobile, the IM iden-
tifies points of interest2 on the route proximal to
the user. We call this ?PoI push?. The user is en-
couraged to ask for more information if he/she is
interested. The system also answers adhoc ques-
tions from the user (e.g. ?Who is David Hume??,
?What is the Old College??, etc) (see section 3.4).
Navigation instructions are given in-situ by ob-
serving user?s position continuously, in relation
to the next node (street junction) on the current
planned route, and they are given priority if in con-
flict with a PoI push at the same time. Navigation
instructions use landmarks near route nodes when-
ever possible (e.g. ?When you reach Clydesdale
Bank , keep walking forward?). The IM also in-
forms when users pass by recognisable landmarks,
just to reassure them that they are on track (e.g.
?You will pass by Tesco on the right?). In addition
to navigation instructions, the IM also answers
users? questions concerning the route, his/her lo-
cation, and location of and distance to the various
entities. Finally, the IM uses the city model?s Vis-
ibility Engine (VE) to determine whether the des-
tination is visible to the user (see section 3.3).
2Using high scoring ones when there are many, based on
tourist popularity ratings in the City Model.
The shared spatial and dialogue context em-
ploys a feature-based representation which is up-
dated every 1 second (for location), and after every
dialogue turn. Spatial context such as the user?s
coordinates, street names, PoIs and landmarks
proximal to the user, etc are used by PoI push-
ing and navigation. The dialogue context main-
tains the history of landmarks and PoIs pushed,
latest entities mentioned, etc to resolve anaphoric
references in navigation and QA requests, and to
deliver coherent dialogue responses. The IM re-
solves anaphoric references by keeping a record
of entities mentioned in the dialogue context. It
also engages in clarification sub-dialogues when
the speech recognition confidence scores are low.
The IM stores the name and type information for
each entity (such as landmark, building, etc) men-
tioned in navigation instructions and PoI pushes.
Subsequent references to these entities using ex-
pressions such as ?the museum?, ?the cafe? etc
are resolved by searching for the latest entity of
the given type. Pronouns are resolved to the last
mentioned entity.
The IM also switches between navigation, PoI
push, and QA tasks in an intelligent manner by
using the shared context to prioritise its utterances
from these different tasks. The utterance genera-
tor is a Natural Language Generation module that
translates the system DA into surface text which is
converted into speech using the Cereproc Text-to-
Speech Synthesizer using a Scottish female voice.
The only changes made were minor adjustments
to the pronunciation of certain place names.
3.2 Pedestrian tracker
Urban environments can be challenging with lim-
ited sky views, and hence limited line of sight
to satellites, in deep urban corridors. There is
therefore significant uncertainty about the user?s
true location reported by GNSS sensors on smart-
phones (Zandbergen and Barbeau, 2011). This
module improves on the reported user position
by combining smartphone sensor data (e.g. ac-
celerometer) with map matching techniques, to
determine the most likely location of the pedes-
trian (Bartie and Mackaness, 2012).
3.3 City Model
The City Model is a spatial database containing
information about thousands of entities in the city
of Edinburgh (Bartie and Mackaness, 2013). This
data has been collected from a variety of exist-
1662
ing resources such as Ordnance Survey, Open-
StreetMap, Google Places, and the Gazetteer for
Scotland. It includes the location, use class, name,
street address, and where relevant other properties
such as build date and tourist ratings. The model
also includes a pedestrian network (streets, pave-
ments, tracks, steps, open spaces) which is used
by an embedded route planner to calculate min-
imal cost routes, such as the shortest path. The
city model also consists of a Visibility Engine
that identifies the entities that are in the user?s
vista space (Montello, 1993). To do this it ac-
cesses a digital surface model, sourced from Li-
DAR, which is a 2.5D representation of the city
including buildings, vegetation, and land surface
elevation. The Visibility Engine uses this dataset
to offer a number of services, such as determining
the line of sight from the observer to nominated
points (e.g. which junctions are visible), and de-
termining which entities within the city model are
visible. Using these services, the IM determines if
the destination is visible or not.
3.4 Question-Answering server
The QA server currently answers a range of def-
inition and biographical questions such as, ?Tell
me more about the Scottish Parliament?, ?Who
was David Hume??, ?What is haggis??, and re-
quests to resume (eg. ?Tell me more?). QA
is also capable of recognizing out of scope re-
quests, that is, either navigation-related questions
that can be answered by computations from the
City Model and dealt with elsewhere in the sys-
tem (?How far away is the Scottish Parliament??,
?How do I get there??), or exploration queries
that cannot be handled yet (?When is the can-
non gun fired from the castle??). Question clas-
sification is entirely machine learning-based using
the SMO algorithm (Keerthi et al, 1999) trained
over 2013 annotated utterances. Once the question
has been typed, QA proceeds to focus detection
also using machine learning techniques (Mikhail-
sian et al, 2009). Detected foci include possi-
bly anaphoric expressions (?Who was he??, ?Tell
me more about the castle?). These expressions
are resolved against the dialogue history and ge-
ographical context. QA then proceeds to a tex-
tual search on texts from the Gazetteer of Scotland
(Gittings, 2012) and Wikipedia, and definitions
from WordNet glosses. The task is similar to TAC
KBP 2013 Entity Linking Track and named en-
tity disambiguation (Cucerzan, 2007). Candidate
answers are reranked using a trained confidence
score with the top candidate used as the final an-
swer. These are usually long, descriptive answers
and are provided as a flow of sentence chunks that
the user can interrupt (see table 2). The Interaction
Manager queries the QA model and pushes infor-
mation when a salient PoI is in the vicinity of the
user.
?Edinburgh?s most famous and historic thoroughfare,
which has formed the heart of the Old Town since
mediaeval times. The Royal Mile includes Castlehill,
the Lawnmarket, the Canongate and the Abbey Strand,
but, is officially known simply as the High Street.?
Table 2: QA output: query on ?Royal Mile?
3.5 Mobile client
The mobile client app, installed on an Android
smartphone (Samsung Galaxy S3), connects the
user to the dialogue system using a 3G data con-
nection. The client senses the user?s location us-
ing positioning technology using GNSS satellites
(GPS and GLONASS) which is sent to the dia-
logue system at the rate of one update every two
seconds. It also sends pace rate of the user from
the accelerometer sensor. In parallel, the client
also places a phone call using which the user com-
municates with the dialogue system.
4 Baseline system
The baseline system chosen for evaluation was
Samsung S-Voice, a state-of-the-art commercial
smartphone speech interface. S-Voice is a Sam-
sung Android mobile phone app that allows a user
to use the functionalities of device using a speech
interface. For example, the user can say ?Call
John? and it will dial John from the user?s con-
tacts. It launches the Google Navigation app when
users request directions and it activates Google
Search for open ended touristic information ques-
tions. The Navigation app is capable of providing
instructions in-situ using speech. We used the S-
Voice system for comparison because it provided
an integrated state-of-the-art interface to use both
a navigation app and also an information-seeking
app using the same speech interface. Users were
encouraged to use these apps using speech but
were allowed to use the GUI interface when us-
ing speech wasn?t working (e.g. misrecognition of
local names). Users obtained the same kind of in-
1663
formation (i.e. navigation directions, descriptions
about entities such as people, places, etc) from the
baseline system as they would from our system.
However, our system interacted with the user us-
ing the speech modality only.
5 Experimental design
Spacebook and the baseline were evaluated in the
summer of 2012. We evaluated both systems with
17 subjects in the streets of Edinburgh. There
were 11 young subjects (between 20 and 26 years,
mean=22 ? 2) and 6 older subjects (between 50
and 71 years, mean=61 ? 11). They were mostly
native English speakers (88%). 59% of the users
were regular smartphone users and their mean
overall time spent in the city was 76 months. The
test subjects had no previous experience with the
proposed system. They were recruited via email
adverts and mail shots. Subjects were given a task
sheet with 8 tasks in two legs (4 tasks per leg).
These tasks included both navigation and tourist
information tasks (see table 3). Subjects used our
system for one of the legs and the baseline system
for the other and the order was balanced. Each leg
took up to 30 mins to finish and the total duration
including questionnaires was about 1.5 hours. Fig-
ure 2 shows the route taken by the subjects. The
route is about 1.3 miles long. Subjects were fol-
lowed by the evaluator who made notes on their
behaviour (e.g. subject looks confused, subject
looks at or manipulates the phone, subject looks
around, etc).
Subjects filled in a demographic questionnaire
prior to the experiment. After each leg, they filled
in a system questionnaire (see appendix) rating
their experience. After the end of the experi-
ment, they filled out a comparative questionnaire
and were debriefed. They were optionally asked
to elaborate on their questionnaire ratings. Users
were paid ?20 after the experiment was over.
6 Results
Subjects were asked to identify tasks that they
thought were successfully completed. The per-
ceived task success rates of the two systems were
compared for each task using the Chi square test.
The results show that there is no statistically sig-
nificant difference between the two systems in
terms of perceived task success although the base-
line system had a better task completion rate in
tasks 1-3, 5 and 6. Our system performed better in
Figure 2: Task route
tourist information tasks (4, 7) (see table 4).
Task Our system Baseline p
T1 (N) 77.7 100 0.5058
T2 (TI) 88.8 100 0.9516
T3 (N) 100 100 NA
T4 (TI) 100 87.5 0.9516
T5 (N+TI) 62.5 100 0.1654
T6 (N+TI) 87.5 100 0.9516
T7 (TI) 100 55.5 0.2926
T8 (N) 75.0 88.8 0.9105
Table 4: % Perceived Task success - task wise
comparison (N - navigation task, TI - Tourist In-
formation task)
The system questionnaires that were filled out
by users after each leg were analysed. These
consisted of questions concerning each system to
be rated on a six point Likert scale (1-Strongly
Disagree, 2-Disagree, 3-Somewhat Disagree, 4-
Somewhat Agree, 5-Agree, 6-Strongly Agree).
The responses were paired and tested using a
Wilcoxon Sign Rank test. Median and Mode for
each system and significance in differences are
shown in table 5. Results show that although
our system is not performing significantly better
than the baseline system (SQ1-SQ10 except SQ7),
users seem to find it more understanding (SQ7)
and more interesting to interact with (SQ11) than
the baseline. We grouped the subjects by age
group and tested their responses. We found that
the young subjects (age group 20-26), also felt that
1664
Leg 1
(Task 1) Ask the system to guide you to the Red Fort restaurant.
(Task 2) You?ve heard that Mary Queen of Scots lived in Edinburgh. Find out about her.
(Task 3) Walk to the university gym.
(Task 4) Near the gym there is an ancient wall with a sign saying ?Flodden Wall?. Find out what that is.
Leg 2
(Task 5) Try to find John Knox House and learn about the man.
(Task 6) Ask the system to guide you to the Old College. What can you learn about this building?
(Task 7) Try to find out more about famous Edinburgh people and places, for example, David Hume,
John Napier, and Ian Rankin. Try to find information about people and places that you are personally
interested in or that are related to what you see around you.
(Task 8) Ask the system to guide you back to the Informatics Forum.
Table 3: Tasks for the user
they learned something new about the city using it
(SQ12) (p < 0.05) while the elderly (age group
50-71) didn?t. We also found statistically signifi-
cant differences in smartphone users rating for our
system on their learning compared to the baseline
(SQ12).
Subjects were also asked to choose between the
two systems given a number of requirements such
as ease of use, use for navigation, tourist infor-
mation, etc. There was an option to rank the sys-
tems equally (i.e. a tie). They were presented with
the same requirements as the system questionnaire
with one additional question - ?Overall which sys-
tem do you prefer?? (CQ0). Users? choice of sys-
tem based on a variety of requirements is shown
in table 6. Users? choice counts were tested us-
ing Chi-square test. Significant differences were
found in users? choice of system for navigation
and tourist information requirements. Users pre-
ferred the baseline system for navigation (CQ2)
and our system for touristic information (CQ3) on
the city. Although there was a clear choice of sys-
tems based on the two tasks, there was no signifi-
cant preference of one system over the other over-
all (CQ0). They chose our system as the most in-
teresting system to interact with (CQ11) and that
it was more informative than the baseline (CQ12).
Figure 3 shows the relative frequency between
user choices on comparative questions.
7 Analysis
Users found it somewhat difficult to navigate using
Spacebook (see comments in table 7). Although
the perceived task success shows that our system
was able to get the users to their destination and
there was no significant difference between the
two systems based on their questionnaire response
on navigation, they pointed out a number of issues
and suggested a number of modifications. Many
Figure 3: Responses to comparative questions
users noted that a visual map and the directional
arrow in the baseline system was helpful for nav-
igation. In addition, they noted that our system?s
navigation instructions were sometimes not satis-
factory. They observed that there weren?t enough
instructions coming from the system at street junc-
tions. They needed more confirmatory utterances
(that they are walking in the right direction) (5
users) and quicker recovery and notification when
walking the wrong way (5 users). They observed
that the use of street names was confusing some-
times. Some users also wanted a route summary
before the navigation instructions are given.
The problem with Spacebook?s navigation pol-
icy was that it did not, for example, direct the
user via easily visible landmarks (e.g. ?Head to-
wards the Castle?), and relies too much on street
names. Also, due to the latency in receiving GPS
information, the IM sometimes did not present in-
structions soon enough during evaluation. Some-
times it received erroneous GPS information and
therefore got the user?s orientation wrong. These
problems will be addressed in the future version.
Some users did find navigation instructions use-
ful because of the use of proximal landmarks such
1665
Question B Mode B Median S Mode S Median p
SQ1 - Ease of use 4 4 5 4 0.8207
SQ2 - Navigation 4 4 5 4 0.9039
SQ3 - Tourist Information 2 3 4 4 0.07323
SQ4 - Easy to understand 5 5 5 5 0.7201
SQ5 - Useful messages 5 4 5 4 1
SQ6 - Response time 5 5 2 2 0.2283
SQ7 - Understanding 3 3 5 4 0.02546
SQ8 - Repetitive 2 3 2 3 0.3205
SQ9 - Aware of user environment 5 5 4 4 0.9745
SQ10 - Cues for guidance 5 5 5 5 0.1371
SQ11 - Interesting to interact with 5 4 5 5 0.01799
SQ12 - Learned something new 5 4 5 5 0.08942
Table 5: System questionnaire responses (B=Baseline, S=our system)
Task Baseline Our system Tie p-
Preferred Preferred value
CQ0 23.52 35.29 41.17 0.66
CQ1 35.29 29.41 35.29 0.9429
CQ2 64.70 0 35.29 0.004
CQ3 17.64 64.70 17.64 0.0232
CQ4 35.29 29.41 23.52 0.8187
CQ5 23.52 52.94 23.52 0.2298
CQ6 23.52 29.41 35.29 0.8187
CQ7 17.64 47.05 35.29 0.327
CQ8 29.41 23.52 47.05 0.4655
CQ9 29.41 52.94 17.64 0.1926
CQ10 47.05 29.41 23.52 0.4655
CQ11 5.88 76.47 17.64 0.0006
CQ12 0 70.58 29.41 0.005
Table 6: User?s choice on comparative questions
(CQ are the same questions as SQ but requesting
a ranking of the 2 systems)
as KFC, Tesco, etc. (popular chain stores). Some
users also suggested that our system should have
a map and that routes taken should be plotted on
them for reference. Based on the ratings and ob-
servations made by the users, we conclude that our
first hypothesis that Spacebook would be more ef-
ficient for navigation than the baseline because of
its speech-only interface was inconclusive. We be-
lieve so because users? poor ratings for Spacebook
may be due to the current choice of dialogue pol-
icy for navigation. It may be possible to reassure
the user with a better dialogue policy with just the
speech interface. However, this needs further in-
vestigation.
Users found the information-search task inter-
esting and informative when they used Spacebook
(see sample user comments in table 8). They
also found push information on nearby PoIs un-
expected and interesting as they would not have
found them otherwise. Many users believed that
this could be an interesting feature that could help
tourists. They also found that asking questions and
finding answers was much easier with Spacebook
compared to the baseline system, where some-
times users needed to type search keywords in.
Another user observation was that they did not
have to stop to listen to information presented
by our system (as it was in speech) and could
carry on walking. However, with the baseline sys-
tem, they had to stop to read information off the
screen. Although users in general liked the QA
feature, many complained that Spacebook spoke
too quickly when it was presenting answers. Some
users felt that the system might lose context of the
navigation task if presented with a PoI question.
In contrast, some others noted Spacebook?s ability
to interleave the two tasks and found it to be an
advantage.
Users? enthusiasm for our system was observed
when (apart from the points of interest that were
in the experimental task list) they also asked spon-
taneous questions about James Watt, the Talbot
Rice gallery, the Scottish Parliament and Edin-
burgh Castle. Some of the PoIs that the system
pushed information about were the Royal College
of Surgeons, the Flodden Wall, the Museum of
Childhood, and the Scottish Storytelling Centre.
Our system answered a mean of 2.5 out of 6.55
questions asked by users in leg 1 and 4.88 out of
8.5 questions in leg 2. Please note that an utter-
ance is sent to QA if it is not parsed by the parser
and therefore some utterances may not be legit-
mate questions themselves. Users were pushed a
mean of 2.88 and 6.37 PoIs during legs 1 and 2.
There were a total of 17 ?tell me more? requests
requesting the system to present more information
(mean=1.35 ? 1.57).
Evaluators who followed the subjects noted that
the subjects felt difficulty using the baseline sys-
tem as they sometimes struggled to see the screen
1666
1. ?It?s useful when it says ?Keep walking? but it should say it more often.?
2. ?[Your system] not having a map, it was sometimes difficult to check how aware it was of my environment.?
3. ?[Google] seemed to be easier to follow as you have a map as well to help.?
4. ?It told me I had the bank and Kentucky Fried Chicken so I crossed the road because I knew it?d be somewhere over
beside them. I thought ?OK, great. I?m going the right way.? but then it didn?t say anything else. I like those kind of
directions because when it said to go down Nicolson Street I was looking around trying to find a street sign.?
5. ?The system keeps saying ?when we come to a junction, I will tell you where to go?, but I passed junctions and it
didn?t say anything. It should say ?when you need to change direction, I will tell you.??
6. ?I had to stop most of the times for the system to be aware of my position. If walking very slowly, its awareness of
both landmarks and streets is excellent.?
Table 7: Sample user comments on the navigation task
1. ?Google doesn?t *offer* any information. I would have to know what to ask for...?
2. ?Since many information is given without being asked for (by your system), one can discover new places and
landmarks even if he lives in the city. Great feature!!?
3. ?I didn?t feel confident to ask [your system] a question and still feel it would remember my directions?
4. ?Google could only do one thing at a time, you couldn?t find directions for a place whilst learning more.?
5. ?If she talked a little bit slower [I would use the system for touristic purposes]. She just throws masses of information
really, really quickly.?
Table 8: Sample user comments on the tourist information task
in bright sunlight. They sometimes had difficulty
identifying which way to go based on the route
plotted on the map. In comparison, subjects did
not have to look at the screen when they used
our system. Based on the ratings and observa-
tions made by the users about our system?s tourist
information features such as answering questions
and pushing PoI information, we have support for
our second hypothesis: that users find a dialogue
interface which integrates question-answering and
navigation within a shared context to be useful for
finding information about entities in the urban en-
vironment.
8 Future plans
We plan to extend Spacebook?s capabilities to ad-
dress other challenges in pedestrian navigation and
tourist information. Many studies have shown
that visible landmarks provide better cues for nav-
igation than street names (Ashweeni and Steed,
2006; Hiley et al, 2008). We will use visible
landmarks identified using the visibility engine to
make navigation instructions more effective, and
we plan to include entities in dialogue and visual
context as candidates for PoI push, and to imple-
ment an adaptive strategy that will estimate user
interests and push information that is of interest
to them. We are also taking advantage of user?s
local knowledge of the city to present navigation
instructions only for the part of the route that the
user does not have any knowledge of. These fea-
tures, we believe, will make users? experience of
the interface more pleasant, useful and informa-
tive.
9 Conclusion
We presented a mobile dialogue app called Space-
book to support pedestrian users in navigation
and tourist information gathering in urban envi-
ronments. The system is a speech-only interface
and addresses navigation and tourist information
in an integrated way, using a shared dialogue con-
text. For example, using the navigational context,
Spacebook can push point-of-interest information
which can then initiate touristic exploration tasks
using the QA module.
We evaluated the system against a state-of-the-
art baseline (Samsung S-Voice with Google Navi-
gation and Search) with a group of 17 users in the
streets of Edinburgh. We found that users found
Spacebook interesting to interact with, and that
it was their system of choice for touristic infor-
mation exploration tasks. These results were sta-
tistically significant. Based on observations and
user ratings, we conclude that our speech-only
system was less preferred for navigation and more
preferred for tourist information tasks due to fea-
tures such as PoI pushing and the integrated QA
module, when compared to the baseline system.
Younger users, who used Spacebook, even felt that
they learned new facts about the city.
Acknowledgments
The research leading to these results was funded by the Eu-
ropean Commission?s Framework 7 programme under grant
1667
agreement no. 270019 (SPACEBOOK project).
References
K. B. Ashweeni and A. Steed. 2006. A natural
wayfinding exploiting photos in pedestrian naviga-
tion systems. In Proceedings of the 8th conference
on Human-computer interaction with mobile devices
and services.
P. Bartie and W. Mackaness. 2006. Development
of a speech-based augmented reality system to sup-
port exploration of cityscape. Transactions in GIS,
10:63?86.
P. Bartie and W. Mackaness. 2012. D3.4 Pedestrian
Position Tracker. Technical report, The SPACE-
BOOK Project (FP7/2011-2014 grant agreement no.
270019).
P. Bartie and W. Mackaness. 2013. D3.1.2 The Space-
Book City Model. Technical report, The SPACE-
BOOK Project (FP7/2011-2014 grant agreement no.
270019).
D. Byron, A. Koller, J. Oberlander, L. Stoia, and
K. Striegnitz. 2007. Generating Instructions in Vir-
tual Environments (GIVE): A challenge and evalua-
tion testbed for NLG. In Proceedings of the Work-
shop on Shared Tasks and Comparative Evaluation
in Natural Language Generation.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings
of EMNLP-CoNLL.
R. Dale, S. Geldof, and J. Prost. 2003. CORAL : Using
Natural Language Generation for Navigational As-
sistance. In Proceedings of ACSC2003, Australia.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011. Hierar-
chical Reinforcement Learning and Hidden Markov
Models for Task-Oriented Natural Language Gener-
ation. In Proc. of ACL.
B. Gittings. 2012. The Gazetteer for Scotland -
http://www.scottish-places.info.
H. Hiley, R. Vedantham, G. Cuellar, A. Liuy,
N. Gelfand, R. Grzeszczuk, and G. Borriello. 2008.
Landmark-based pedestrian navigation from collec-
tions of geotagged photos. In Proceedings of the
7th Int. Conf. on Mobile and Ubiquitous Multimedia
(MUM).
S. Janarthanam and O. Lemon. 2011. The GRUVE
Challenge: Generating Routes under Uncertainty in
Virtual Environments. In Proceedings of ENLG.
S. Janarthanam, O. Lemon, X. Liu, P. Bartie, W. Mack-
aness, T. Dalmas, and J. Goetze. 2012. Integrat-
ing location, visibility, and Question-Answering in
a spoken dialogue system for Pedestrian City Explo-
ration. In Proc. of SIGDIAL 2012, S. Korea.
H. Kashioka, T. Misu, E. Mizukami, Y. Shiga,
K. Kayama, C. Hori, and H. Kawai. 2011. Multi-
modal Dialog System for Kyoto Sightseeing Guide.
In Asia-Pacific Signal and Information Processing
Association Annual Summit and Conference.
S.S. Keerthi, S. K. Shevade, C. Bhattacharyya, and
K. R. K. Murthy. 1999. Improvements to Platt?s
SMO Algorithm for SVM Classifier Design. Neural
Computation, 3:637?649.
J. Ko, F. Murase, T. Mitamura, E. Nyberg, M. Tateishi,
I. Akahori, and N. Hataoka. 2005. CAMMIA: A
Context-Aware Spoken Dialog System for Mobile
Environments. In IEEE ASRU Workshop.
C. Kray, K. Laakso, C. Elting, and V. Coors. 2003.
Presenting Route Instructions on Mobile Devices.
In Proceedings of IUI 03, Florida.
R. Malaka and A. Zipf. 2000. Deep Map - challenging
IT research in the framework of a tourist information
system. In Information and Communication Tech-
nologies in Tourism 2000, pages 15?27. Springer.
A. Mikhailsian, T. Dalmas, and R. Pinchuk. 2009.
Learning foci for question answering over topic
maps. In Proceedings of ACL 2009.
D. Montello. 1993. Scale and multiple psychologies
of space. In A. U. Frank and I. Campari, editors,
Spatial information theory: A theoretical basis for
GIS.
M. Raubal and S. Winter. 2002. Enriching wayfinding
instructions with local landmarks. In Second Inter-
national Conference GIScience. Springer, USA.
C.J. Shroder, W. Mackaness, and B. Gittings. 2011.
Giving the Right Route Directions: The Require-
ments for Pedestrian Navigation Systems. Transac-
tions in GIS, pages 419?438.
N. Webb and B. Webber. 2009. Special Issue on Inter-
active Question Answering: Introduction. Natural
Language Engineering, 15(1):1?8.
P. A. Zandbergen and S. J. Barbeau. 2011. Posi-
tional Accuracy of Assisted GPS Data from High-
Sensitivity GPS-enabled Mobile Phones. Journal of
Navigation, 64(3):381?399.
1668
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1231?1240,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Comparing Multi-label Classification with Reinforcement Learning for
Summarisation of Time-series Data
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
We present a novel approach for automatic
report generation from time-series data, in
the context of student feedback genera-
tion. Our proposed methodology treats
content selection as a multi-label (ML)
classification problem, which takes as in-
put time-series data and outputs a set of
templates, while capturing the dependen-
cies between selected templates. We show
that this method generates output closer to
the feedback that lecturers actually gener-
ated, achieving 3.5% higher accuracy and
15% higher F-score than multiple simple
classifiers that keep a history of selected
templates. Furthermore, we compare a
ML classifier with a Reinforcement Learn-
ing (RL) approach in simulation and using
ratings from real student users. We show
that the different methods have different
benefits, with ML being more accurate for
predicting what was seen in the training
data, whereas RL is more exploratory and
slightly preferred by the students.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating text from vari-
ables whose values change over time. We con-
sider the task of automatically generating feed-
back summaries for students describing their per-
formance during the lab of a Computer Science
module over the semester. Students? learning can
be influenced by many variables, such as difficulty
of the material (Person et al, 1995), other dead-
lines (Craig et al, 2004), attendance in lectures
(Ames, 1992), etc. These variables have two im-
portant qualities. Firstly, they change over time,
and secondly they can be dependent on or inde-
pendent of each other. Therefore, when generating
feedback, we need to take into account all vari-
ables simultaneously in order to capture potential
dependencies and provide more effective and use-
ful feedback that is relevant to the students.
In this work, we concentrate on content selec-
tion which is the task of choosing what to say,
i.e. what information is to be included in a report
(Reiter and Dale, 2000). Content selection deci-
sions based on trends in time-series data determine
the selection of the useful and important variables,
which we refer to here as factors, that should be
conveyed in a summary. The decisions of factor
selection can be influenced by other factors that
their values are correlated with; can be based on
the appearance or absence of other factors in the
summary; and can be based on the factors? be-
haviour over time. Moreover, some factors may
have to be discussed together in order to achieve
some communicative goal, for instance, a teacher
might want to refer to student?s marks as a moti-
vation for increasing the number of hours studied.
We frame content selection as a simple classifi-
cation task: given a set of time-series data, decide
for each template whether it should be included
in a summary or not. In this paper, with the term
?template? we refer to a quadruple consisting of an
id, a factor (bottom left of Table 1), a reference
type (trend, weeks, average, other) and surface
text. However, simple classification assumes that
the templates are independent of each other, thus
the decision for each template is taken in isolation
from the others, which is not appropriate for our
domain. In order to capture the dependencies in
the context, multiple simple classifiers can make
the decisions for each template iteratively. After
each iteration, the feature space grows by 1 fea-
ture, in order to include the history of the previous
template decisions. Here, we propose an alterna-
tive method that tackles the challenge of interde-
pendent data by using multi-label (ML) classifica-
tion, which is efficient in taking data dependencies
1231
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors trend
(1) marks (M) trend other
(2) hours studied (HS) trend increasing
(3) understandability (Und) trend decreasing
(4) difficulty (Diff) trend decreasing
(5) deadlines (DL) trend increasing
(6) health issues (HI) trend other
(7) personal issues (PI) trend decreasing
(8) lectures attended (LA) trend other
(9) revision (R) trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think whether revis-
ing has improved your performance.
Table 1: The table on the top left shows an example of the time-series raw data for feedback generation.
The table on the bottom left shows an example of described trends. The box on the right presents a target
summary (target summaries have been constructed by teaching staff).
into account and generating a set of labels (in our
case templates) simultaneously (Tsoumakas et al,
2010). ML classification requires no history, i.e.
does not keep track of previous decisions, and thus
has a smaller feature space.
Our contributions to the field are as follows: we
present a novel and efficient method for tackling
the challenge of content selection using a ML clas-
sification approach; we applied this method to the
domain of feedback summarisation; we present a
comparison with an optimisation technique (Rein-
forcement Learning), and we discuss the similari-
ties and differences between the two methods.
In the next section, we refer to the related work
on Natural Language Generation from time-series
data and on Content Selection. In Section 4.2, we
describe our approach and we carry out a compar-
ison with simple classification methods. In Sec-
tion 5, we present the evaluation setup and in Sec-
tion 6 we discuss the results, obtained in simula-
tion and with real students. Finally, in Section 8,
directions for future work are discussed.
2 Related Work
Natural Language Generation from time-series
data has been investigated for various tasks such
as weather forecast generation (Belz and Kow,
2010; Angeli et al, 2010; Sripada et al, 2004),
report generation from clinical data (Hunter et al,
2011; Gatt et al, 2009), narrative to assist children
with communication needs (Black et al, 2010) and
audiovisual debrief generation from sensor data
from Autonomous Underwater Vehicles missions
(Johnson and Lane, 2011).
The important tasks of time-series data sum-
marisation systems are content selection (what to
say), surface realisation (how to say it) and infor-
mation presentation (Document Planning, Order-
ing, etc.). In this work, we concentrate on content
selection. Previous methods for content selection
include Reinforcement Learning (Rieser et al,
2010); multi-objective optimisation (Gkatzia et
al., 2014); Gricean Maxims (Sripada et al, 2003);
Integer Linear Programming (Lampouras and An-
droutsopoulos, 2013); collective content selection
(Barzilay and Lapata, 2004); interest scores as-
signed to content (Androutsopoulos et al, 2013); a
combination of statistical and template-based ap-
proaches to NLG (Kondadadi et al, 2013); statis-
tical acquisition of rules (Duboue and McKeown,
2003) and the Hidden Markov model approach for
Content Selection and ordering (Barzilay and Lee,
2004).
Collective content selection (Barzilay and La-
pata, 2004) is similar to our proposed method in
that it is a classification task that predicts the tem-
plates from the same instance simultaneously. The
difference between the two methods lies in that the
1232
collective content selection requires the considera-
tion of an individual preference score (which is de-
fined as the preference of the entity to be selected
or omitted, and it is based on the values of entity
attributes and is computed using a boosting algo-
rithm) and the identification of links between the
entities with similar labels. In contrast, ML clas-
sification does not need the computation of links
between the data and the templates. ML classi-
fication can also apply to other problems whose
features are correlated, such as text classification
(Madjarov et al, 2012), when an aligned dataset is
provided.
ML classification algorithms have been divided
into three categories: algorithm adaptation meth-
ods, problem transformation and ensemble meth-
ods (Tsoumakas and Katakis, 2007; Madjarov
et al, 2012). Algorithm adaptation approaches
(Tsoumakas et al, 2010) extend simple classifi-
cation methods to handle ML data. For exam-
ple, the k-nearest neighbour algorithm is extended
to ML-kNN by Zhang and Zhou (2007). ML-
kNN identifies for each new instance its k nearest
neighbours in the training set and then it predicts
the label set by utilising the maximum a posteri-
ori principle according to statistical information
derived from the label sets of the k neighbours.
Problem transformation approaches (Tsoumakas
and Katakis, 2007) transform the ML classifica-
tion task into one or more simple classification
tasks. Ensemble methods (Tsoumakas et al, 2010)
are algorithms that use ensembles to perform ML
learning and they are based on problem transfor-
mation or algorithm adaptation methods. In this
paper, we applied RAkEL (Random k-labelsets)
(Tsoumakas et al, 2010): an ensemble problem
transformation method, which constructs an en-
semble of simple-label classifiers, where each one
deals with a random subset of the labels.
Finally, our domain for feedback generation is
motivated by previous studies (Law et al, 2005;
van den Meulen et al, 2010) who show that text
summaries are more effective in decision making
than graphs therefore it is advantageous to provide
a summary over showing users the raw data graph-
ically. In addition, feedback summarisation from
time-series data can be applied to the field of In-
telligent Tutoring Systems (Gross et al, 2012).
3 Data
The dataset consists of 37 instances referring to
the activities of 26 students. For a few students
there is more than 1 instance. An example of one
such instance is presented in Table 1. Each in-
stance includes time-series information about the
student?s learning habits and the selected tem-
plates that lecturers used to provide feedback to
this student. The time-series information includes
for each week of the semester: (1) the marks
achieved at the lab; (2) the hours that the stu-
dent spent studying; (3) the understandability of
the material; (4) the difficulty of the lab exercises
as assessed by the student; (5) the number of other
deadlines that the student had that week; (6) health
issues; (7) personal issues; (8) the number of lec-
tures attended; and (9) the amount of revision that
the student had performed. The templates describe
these factors in four different ways:
1. <trend>: referring to the trend of a fac-
tor over the semester (e.g. ?Your performance
was increasing...?),
2. <weeks>: explicitly describing the factor
value at specific weeks (e.g. ?In weeks 2, 3
and 9...?),
3. <average>: considering the average of a
factor value (e.g. ?You dedicated 1.5 hours
studying on average...?), and
4. <other>: mentioning other relevant infor-
mation (e.g. ?Revising material will improve
your performance?).
For the corpus creation, 11 lecturers selected the
content to be conveyed in a summary, given the
set of raw data (Gkatzia et al, 2013). As a result,
for the same student there are various summaries
provided by the different experts. This character-
istic of the dataset, that each instance is associated
with more than one solution, additionally moti-
vates the use of multi-label classification, which
is concerned with learning from examples, where
each example is associated with multiple labels.
Our analysis of the dataset showed that there
are significant correlations between the factors, for
example, the number of lectures attended (LA)
correlates with the student?s understanding of the
material (Und), see Table 2. As we will discuss
further in Section 5.1, content decisions are in-
fluenced by the previously generated content, for
example, if the lecturer has previously mentioned
health issues, mentioning hours studied has a high
probability of also being mentioned.
1233
Factor (1) M (2) HS (3) Und (4) Diff (5) DL (6) HI (7) PI (8) LA (9) R
(1) M 1* 0.52* 0.44* -0.53* -0.31 -0.30 -0.36* 0.44* 0.16
(2) HS 0.52* 1* 0.23 -0.09 -0.11 0.11 -0.29 0.32 0.47*
(3) Und 0.44* 0.23 1* -0.54* 0.03 -0.26 0.12 0.60* 0.32
(4) Diff -0.53* -0.09 -0.54* 1* 0.16 -0.06 0.03 -0.19 0.14
(5) DL -0.31 -0.11 0.03 0.16 1* 0.26 0.24 -0.44* 0.14
(6) HI -0.30 -0.11 -0.26 -0.06 0.26 1* 0.27 -0.50* 0.15
(7) PI -0.36* -0.29 0.12 0.03 0.24 0.27 1* -0.46* 0.34*
(8) LA 0.44* 0.32 0.60* -0.19 -0.44* -0.50* -0.46* 1* -0.12
(9) R 0.16 0.47* 0.03 0.14 0.14 0.15 0.34* -0.12 1*
Table 2: The table presents the Pearson?s correlation coefficients of the factors (* means p<0.05).
4 Methodology
In this section, the content selection task and the
suggested multi-label classification approach are
presented. The development and evaluation of the
time-series generation system follows the follow-
ing pipeline (Gkatzia et al, 2013):
1. Time-Series data collection from students
2. Template construction by Learning and
Teaching (L&T) expert
3. Feedback summaries constructed by lectur-
ers; random summaries rated by lecturers
4. Development of time-series generation sys-
tems (Section 4.2, Section 5.3): ML system,
RL system, Rule-based and Random system
5. Evaluation: (Section 5)
- Offline evaluation (Accuracy and Reward)
- Online evaluation (Subjective Ratings)
4.1 The Content Selection Task
Our learning task is formed as follows: given a
set of 9 time-series factors, select the content that
is most appropriate to be included in a summary.
Content is regarded as labels (each template rep-
resents a label) and thus the task can be thought of
as a classification problem. As mentioned, there
are 4 ways to refer to a factor: (1) describing the
trend, (2) describing what happened in every time
stamp, (3) mentioning the average and (4) making
another general statement. Overall, for all factors
there are 29 different templates
1
. An example of
the input data is shown in Table 1. There are two
decisions that need to be made: (1) whether to talk
about a factor and (2) in which way to refer to it.
Instead of dealing with this task in a hierarchical
way, where the algorithm will first learn whether
to talk about a factor and then to decide how to
1
There are fewer than 36 templates, because for some fac-
tors there are less than 4 possible ways of referring to them.
refer to it, we transformed the task in order to re-
duce the learning steps. Therefore, classification
can reduce the decision workload by deciding ei-
ther in which way to talk about it, or not to talk
about a factor at all.
4.2 The Multi-label Classification Approach
Traditional single-label classification is the task of
identifying which label one new observation is as-
sociated with, by choosing from a set of labels L
(Tsoumakas et al, 2010). Multi-label classifica-
tion is the task of associating an observation with
a set of labels Y ? L (Tsoumakas et al, 2010).
One set of factor values can result in various
sets of templates as interpreted by the different
experts. A ML classifier is able to make deci-
sions for all templates simultaneously and cap-
ture these differences. The RAndom k-labELsets
(RAkEL) (Tsoumakas et al, 2010) was applied
in order to perform ML classification. RAkEL is
based on Label Powerset (LP), a problem transfor-
mation method (Tsoumakas et al, 2010). LP ben-
efits from taking into consideration label correla-
tions, but does not perform well when trained with
few examples as in our case (Tsoumakas et al,
2010). RAkEL overcomes this limitation by con-
structing a set of LP classifiers, which are trained
with different random subsets of the set of labels
(Tsoumakas et al, 2010).
The LP method transforms the ML task, into
one single-label multi-class classification task,
where the possible set of predicted variables for
the transformed class is the powerset of labels
present in the original dataset. For instance, the set
of labels L = {temp
0
, temp
1
, ...temp
28
} could be
transformed to {temp
0,1,2
, temp
28,3,17,
...}. This
algorithm does not perform well when consider-
ing a large number of labels, due to the fact that
the label space grows exponentially (Tsoumakas
1234
Classifier Accuracy Precision Recall F score
(10-fold)
Decision Tree (no history) *75.95% 67.56 75.96 67.87
Decision Tree (with predicted history) **73.43% 65.49 72.05 70.95
Decision Tree (with real history) **78.09% 74.51 78.11 75.54
Majority-class (single label) **72.02% 61.73 77.37 68.21
RAkEL (multi-label) (no history) 76.95% 85.08 85.94 85.50
Table 3: Average, precision, recall and F-score of the different classification methods (T-test, * denotes
significance with p<0.05 and ** significance with p<0.01, when comparing each result to RAkEL).
et al, 2010). RAkEL tackles this problem by con-
structing an ensemble of LP classifiers and train-
ing each one on a different random subset of the
set of labels (Tsoumakas et al, 2010).
4.2.1 The Production Phase of RAkEL
The algorithm was implemented using the MU-
LAN Open Source Java library (Tsoumakas et
al., 2011), which is based on WEKA (Witten and
Frank, 2005). The algorithm works in two phases:
1. the production of an ensemble of LP algo-
rithms, and
2. the combination of the LP algorithms.
RAkEL takes as input the following parameters:
(1) the numbers of iterations m (which is devel-
oper specified and denotes the number of models
that the algorithm will produce), (2) the size of la-
belset k (which is also developer specified), (3) the
set of labels L, and (4) the training set D. During
the initial phase it outputs an ensemble of LP clas-
sifiers and the corresponding k-labelsets. A pseu-
docode for the production phase is shown below:
Algorithm 1 RAkEL production phase
1 : I n p u t : i t e r a t i o n s m, k l a b e l s e t s ,
l a b e l s L , t r a i n i n g d a t a D
2 : f o r i =0 t o m
3 : S e l e c t random k? l a b e l s e t from L
4 : T r a i n an LP on D
5 : Add LP t o ensemble
6 : end f o r
7 : Outpu t : t h e ensemble o f LPs
wi th c o r r e s p o n d i n g k? l a b e l s e t s
4.2.2 The Combination Phase
During the combination phase, the algorithm takes
as input the results of the production phase, i.e.
the ensemble of LPs with the corresponding k-
labelsets, the set of labels L, and the new instance
x and it outputs the result vector of predicted la-
bels for instance x. During run time, RAkEL es-
timates the average decision for each label in L
and if the average is greater than a threshold t (de-
termined by the developer) it includes the label in
the predicted labelset. We used the standard pa-
rameter values of t, k and m (t = 0.5, k = 3 and
m equals to 58 (2*29 templates)). In future, we
could perform parameter optimisation by using a
technique similar to (Gabsdil and Lemon, 2004).
5 Evaluation
Firstly, we performed a preliminary evaluation on
classification methods, comparing our proposed
ML classification with multiple iterated classifica-
tion approaches. The summaries generated by the
ML classification system are then compared with
the output of a RL system and two baseline sys-
tems in simulation and with real students.
5.1 Comparison with Simple Classification
We compared the RAkEL algorithm with single-
label (SL) classification. Different SL classifiers
were trained using WEKA: JRip, Decision Trees,
Naive Bayes, k-nearest neighbour, logistic regres-
sion, multi-layer perceptron and support vector
machines. It was found out that Decision Trees
achieved on average 3% higher accuracy. We,
therefore, went on to use Decision Trees that use
generation history in three ways.
Firstly, for Decision Tree (no history), 29
decision-tree classifiers were trained, one for each
template. The input of these classifiers were the
9 factors and each classifier was trained in order
to decide whether to include a specific template or
not. This method did not take into account other
selected templates ? it was only based on the time-
series data.
Secondly, for Decision Tree (with predicted
history), 29 classifiers were also trained, but this
time the input included the previous decisions
made by the previous classifiers (i.e. the history)
1235
as well as the set of time-series data in order to
emulate the dependencies in the dataset. For in-
stance, classifier n was trained using the data from
the 9 factors and the template decisions for tem-
plates 0 to n? 1.
Thirdly, for Decision Tree (with real his-
tory), the real, expert values were used rather
than the predicted ones in the history. The
above-mentioned classifiers are compared with,
the Majority-class (single label) baseline, which
labels each instance with the most frequent tem-
plate.
The accuracy, the weighted precision, the
weighted recall, and the weighted F-score of the
classifiers are shown in Table 3. It was found that
in 10-fold cross validation RAkEL performs sig-
nificantly better in all these automatic measures
(accuracy = 76.95%, F-score = 85.50%). Remark-
ably, ML achieves more than 10% higher F-score
than the other methods (Table 3). The average
accuracy of the single-label classifiers is 75.95%
(10-fold validation), compared to 73.43% of clas-
sification with history. The reduced accuracy of
the classification with predicted history is due to
the error in the predicted values. In this method,
at every step, the predicted outcome was used in-
cluding the incorrect decisions that the classifier
made. The upper-bound accuracy is 78.09% cal-
culated by using the expert previous decisions and
not the potentially erroneous predicted decisions.
This result is indicative of the significance of the
relations between the factors showing that the pre-
dicted decisions are dependent due to existing cor-
relations as discussed in Section 1, therefore the
system should not take these decisions indepen-
dently. ML classification performs better because
it does take into account these correlations and de-
pendencies in the data.
5.2 The Reinforcement Learning System
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns to
take optimal actions so as to maximise a cumu-
lative reward (Sutton and Barto, 1998). Content
selection is seen as a Markov Decision problem
and the goal of the agent is to learn to take the se-
quence of actions that leads to optimal content se-
lection. The Temporal Difference learning method
was used to train an agent for content selection.
Actions and States: The state consists of the
time-series data and the selected templates. In or-
der to explore the state space the agent selects a
factor (e.g. marks, deadlines etc.) and then decides
whether to talk about it or not.
Reward Function: The reward function reflects
the lecturers? preferences on summaries and is
derived through linear regression analysis of a
dataset containing lecturer constructed summaries
and ratings of randomly generated summaries.
Specifically, it is the following cumulative multi-
variate function:
Reward = a+
n
?
i=1
b
i
? x
i
+ c ? length
where X = {x
1
, x
2
, ..., x
n
} describes the com-
binations of the data trends observed in the time-
series data and a particular template. a, b and c are
the regression coefficients, and their values vary
from -99 to 221. The value of x
i
is given by the
function:
x
i
=
?
?
?
?
?
?
?
?
?
?
?
1, the combination of a factor trend
and a template type is included
in a summary
0, if not.
The RL system differs from the classification
system in the way it performs content selection.
In the training phase, the agent selects a factor and
then decides whether to talk about it or not. If the
agent decides to refer to a factor, the template is
selected in a deterministic way, i.e. from the avail-
able templates it selects the template that results in
higher expected cumulative future reward.
5.3 The Baseline Systems
We compared the ML system and the RL system
with two baselines described below by measuring
the accuracy of their outputs, the reward achieved
by the reward function used for the RL system,
and finally we also performed evaluation with stu-
dent users. In order to reduce the confounding
variables, we kept the ordering of content in all
systems the same, by adopting the ordering of the
rule-based system. The baselines are as follows:
1. Rule-based System: generates summaries
based on Content Selection rules derived by work-
ing with a L&T expert and a student (Gkatzia et
al., 2013).
2. Random System: initially, selects a factor
randomly and then selects a template randomly,
until it makes decisions for all factors.
1236
Time-Series Accuracy Reward Rating Mode (mean) Data Source
Summarisation Systems
Multi-label Classification 85% 65.4 7 (6.24) Lecturers? constructed summaries
Reinforcement Learning **66% 243.82 8 (6.54) Lecturers? ratings & summaries
Rule-based **65% 107.77 7, 8 (5.86) L&T expert
Random **45.2% 43.29 *2 (*4.37) Random
Table 4: Accuracy, average rewards (based on lecturers? preferences) and averages of the means of the
student ratings. Accuracy significance (Z-test) with RAkEL at p<0.05 is indicated as * and at p<0.01
as **. Student ratings significance (Mann Whitney U test) with RAkEL at p<0.05 is indicated as *.
6 Results
Each of the four systems described above gener-
ated 26 feedback summaries corresponding to the
26 student profiles. These summaries were evalu-
ated in simulation and with real student users.
6.1 Results in Simulation
Table 4 presents the accuracy, reward, and mode
of student rating of each algorithm when used to
generate the 26 summaries. Accuracy was esti-
mated as the proportion of the correctly classified
templates to the population of templates. In or-
der to have a more objective view on the results,
the score achieved by each algorithm using the
reward function was also calculated. ML clas-
sification achieved significantly higher accuracy,
which was expected as it is a supervised learning
method. The rule-based system and the RL sys-
tem have lower accuracy compared to the ML sys-
tem. There is evidently a mismatch between the
rules and the test-set; the content selection rules
are based on heuristics provided by a L&T Expert
rather than by the same pool of lecturers that cre-
ated the test-set. On the contrary, the RL is trained
to optimise the selected content and not to repli-
cate the existing lecturer summaries, hence there
is a difference in accuracy.
Accuracy measures how similar the generated
output is to the gold standard, whereas the reward
function calculates a score regarding how good
the output is, given an objective function. RL is
trained to optimise for this function, and therefore
it achieves higher reward, whereas ML is trained
to learn by examples, therefore it produces out-
put closer to the gold standard (lecturer?s produced
summaries). RL uses exploration and exploitation
to discover combinations of content that result in
higher reward. The reward represents predicted
ratings that lecturers would give to the summary.
The reward for the lecturers? produced summaries
is 124.62 and for the ML method is 107.77. The
ML classification system performed worse than
this gold standard in terms of reward, which is ex-
pected given the error in predictions (supervised
methods learn to reproduce the gold standard).
Moreover, each decision is rewarded with a dif-
ferent value as some combinations of factors and
templates have greater or negative regression coef-
ficients. For instance, the combination of the fac-
tors ?deadlines? and the template that corresponds
to <weeks> is rewarded with 57. On the other
hand, when mentioning the <average> difficulty
the summary is ?punished? with -81 (see descrip-
tion of the reward function in Section 5.2). Conse-
quently, a single poor decision in the ML classifi-
cation can result in much less reward.
6.2 Subjective Results with Students
37 first year computer science students partici-
pated in the study. Each participant was shown
a graphical representation of the time-series data
of one student and four different summaries gen-
erated by the four systems (see Figure 1). The or-
der of the presented summaries was randomised.
They were asked to rate each feedback summary
on a 10-point rating scale in response to the fol-
lowing statement: ?Imagine you are the following
student. How would you evaluate the following
feedback summaries from 1 to 10??, where 10 cor-
responds to the most preferred summary and 1 to
the least preferred.
The difference in ratings between the ML clas-
sification system, the RL system and the Rule-
based system is not significant (see Mode (mean)
in Table 4, p>0.05). However, there is a trend to-
wards the RL system. The classification method
reduces the generation steps, by making the de-
cision of the factor selection and the template se-
lection jointly. Moreover, the training time for the
classification method is faster (a couple of seconds
compared to over an hour). Finally, the student
1237
Figure 1: The Figure show the evaluation setup. Students were presenting with the data in a graphical
way and then they were asked to evaluate each summary in a 10-point Rating scale. Summaries displayed
from left to right: ML system, RL, rule-based and random.
significantly prefer all the systems over the ran-
dom.
7 Summary
We have shown that ML classification for sum-
marisation of our time-series data has an accuracy
of 76.95% and that this approach significantly out-
performs other classification methods as it is able
to capture dependencies in the data when mak-
ing content selection decisions. ML classifica-
tion was also directly compared to a RL method.
It was found that although ML classification is
almost 20% more accurate than RL, both meth-
ods perform comparably when rated by humans.
This may be due to the fact that the RL optimi-
sation method is able to provide more varied re-
sponses over time rather than just emulating the
training data as with standard supervised learn-
ing approaches. Foster (2008) found similar re-
sults when performing a study on generation of
emphatic facial displays. A previous study by
Belz and Reiter (2006) has demonstrated that au-
tomatic metrics can correlate highly with human
ratings if the training dataset is of high quality.
In our study, the human ratings correlate well to
the average scores achieved by the reward func-
tion. However, the human ratings do not correlate
well to the accuracy scores. It is interesting that
the two methods that score differently on various
automatic metrics, such as accuracy, reward, pre-
cision, recall and F-score, are evaluated similarly
by users.
The comparison shows that each method can
serve different goals. Multi-label classification
generates output closer to gold standard whereas
RL can optimise the output according to a reward
function. ML classification could be used when
the goal of the generation is to replicate phenom-
ena seen in the dataset, because it achieves high
accuracy, precision and recall. However, opti-
misation methods can be more flexible, provide
more varied output and can be trained for different
goals, e.g. for capturing preferences of different
users.
1238
8 Future Work
For this initial experiment, we evaluated with stu-
dents and not with lecturers, since the students are
the recipients of feedback. In future, we plan to
evaluate with students? own data under real cir-
cumstances as well as with ratings from lecturers.
Moreover, we plan to utilise the results from this
student evaluation in order to train an optimisation
algorithm to perform summarisation according to
students? preferences. In this case, optimisation
would be the preferred method as it would not be
appropriate to collect gold standard data from stu-
dents. In fact, it would be of interest to investi-
gate multi-objective optimisation techniques that
can balance the needs of the lecturers to convey
important content to the satisfaction of students.
9 Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):261?71.
Ion Androutsopoulos, Gerasimos Lampouras, and
Dimitrios Galanis. 2013. Generating natural lan-
guage descriptions from owl ontologies: the nat-
ural owl system. Atrificial Intelligence Research,
48:671?715.
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. In Conference on Empirical Methods
in Natural Language Processing (EMNLP).
Regina Barzilay and Mirella Lapata. 2004. Collec-
tive content selection for concept-to-text generation.
In Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT-EMNLP).
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics (HLT-NAACL).
Anja Belz and Eric Kow. 2010. Extracting parallel
fragments from comparable corpora for data-to-text
generation. In 6th International Natural Language
Generation Conference (INLG).
Anja Belz and Ehud Reiter. 2006. Comparing auto-
matic and human evaluation of nlg systems. In 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (ACL).
Rolf Black, Joe Reddington, Ehud Reiter, Nava
Tintarev, and Annalu Waller. 2010. Using NLG and
sensors to support personal narrative for children
with complex communication needs. In NAACL
HLT 2010 Workshop on Speech and Language Pro-
cessing for Assistive Technologies.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning:
an exploratory look into the role of affect in learn-
ing with autotutor. Journal of Educational Media,
29:241?250.
Pable Duboue and K.R. McKeown. 2003. Statistical
acquisition of content selection rules for natural lan-
guage generation. In Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing (EMNLP).
Mary Ellen Foster. 2008. Automated metrics that
agree with human judgements on generated output
for an embodied conversational agent. In 5th Inter-
national Natural Language Generation Conference
(INLG).
Malte Gabsdil and Oliver Lemon. 2004. Combining
acoustic and pragmatic features to predict recogni-
tion performance in spoken dialogue systems. In
42nd Annual Meeting of the Association for Com-
putational Linguistics (ACL).
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
AI Communications, 22: 153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation (ENLG).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014. Finding Middle Ground? Multi-objective
Natural Language Generation from time-series data.
In 14th Conference of the European Chapter of the
Association for Computational Linguistics (EACL)
(to appear).
Sebastian Gross, Bassam Mokbel, Barbara Hammer,
and Niels Pinkwart. 2012. Feedback provision
strategies in intelligent tutoring systems based on
clustered solution spaces. In J. Desel, J. M. Haake,
and C. Spannagel, editors, Tagungsband der 10. e-
Learning Fachtagung Informatik (DeLFI), number
P-207 in GI Lecture Notes in Informatics, pages 27?
38. GI.
1239
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. Bt-nurse:
Computer generation of natural language shift sum-
maries from complex heterogeneous medical data.
American Medical Informatics Association, 18:621-
624.
Nicholas Johnson and David Lane. 2011. Narrative
monologue as a first step towards advanced mis-
sion debrief for AUV operator situational aware-
ness. In 15th International Conference on Advanced
Robotics.
Ravi Kondadadi, Blake Howald, and Frank Schilder.
2013. A statistical nlg framework for aggregated
planning and realization. In 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL).
Gerasimos Lampouras and Ion Androutsopoulos.
2013. Using integer linear programming in concept-
to-text generation to produce more compact texts. In
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Anna S. Law, Yvonne Freer, Jim Hunter, Robert H.
Logie, Neil McIntosh, and John Quinn. 2005. A
comparison of graphical and textual presentations of
time series data to support medical decision making
in the neonatal intensive care unit. Journal of Clini-
cal Monitoring and Computing, pages 19: 183?194.
Gjorgji Madjarov, Dragi Kocev, Dejan Gjorgjevikj, and
Saso Dzeroski. 2012. An extensive experimen-
tal comparison of methods for multi-label learning.
Pattern Recognition, 45(9):3084?3104.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. Journal of Cognition
and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral language generation systems. Cambridge Uni-
versity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Generating english summaries of time se-
ries data using the gricean maxims. In 9th ACM in-
ternational conference on Knowledge discovery and
data mining (SIGKDD).
Somayajulu Sripada, Ehud Reiter, I Davy, and
K Nilssen. 2004. Lessons from deploying NLG
technology for marine weather forecast text gener-
ation. In PAIS session of ECAI-2004:760-764.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. MIT Press.
Grigorios Tsoumakas and Ioannis Katakis. 2007.
Multi-label classification: An overview. Inter-
national Journal Data Warehousing and Mining,
3(3):1?13.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis
Vlahavas. 2010. Random k-labelsets for multi-
label classification. IEEE Transactions on Knowl-
edge and Data Engineering, 99(1):1079?1089.
Grigorios Tsoumakas, Eleftherios Spyromitros-
Xioufis, Josef Vilcek, and Ioannis Vlahavas.
2011. Mulan: A java library for multi-label
learning. Journal of Machine Learning Research,
12(1):2411?2414.
Marian van den Meulen, Robert Logie, Yvonne Freer,
Cindy Sykes, Neil McIntosh, and Jim Hunter. 2010.
When a graph is poorer than 100 words: A com-
parison of computerised natural language genera-
tion, human generated descriptions and graphical
displays in neonatal intensive care. In Applied Cog-
nitive Psychology, 24: 77-89.
Ian Witten and Eibe Frank. 2005. Data mining: Practi-
cal machine learning tools and techniques. Morgan
Kaufmann Publishers.
Min-Ling Zhang and Zhi-Hua Zhou. 2007. Ml-knn: A
lazy learning approach to multi-label learning. Pat-
tern Recognition, 40(7):2038?2048.
1240
Generation under Uncertainty
Oliver Lemon
Heriot-Watt University
Edinburgh, United Kingdom
o.lemon@hw.ac.uk
Srini Janarthanam
Edinburgh University
Edinburgh, United Kingdom
s.janarthanam@ed.ac.uk
Verena Rieser
Edinburgh University
Edinburgh, United Kingdom
vrieser@inf.ed.ac.uk
Abstract
We invite the research community to con-
sider challenges for NLG which arise from
uncertainty. NLG systems should be able
to adapt to their audience and the genera-
tion environment in general, but often the
important features for adaptation are not
known precisely. We explore generation
challenges which could employ simulated
environments to study NLGwhich is adap-
tive under uncertainty, and suggest possi-
ble metrics for such tasks. It would be par-
ticularly interesting to explore how differ-
ent planning approaches to NLG perform
in challenges involving uncertainty in the
generation environment.
1 Introduction
We would like to highlight the design of NLG sys-
tems for environments where there may be incom-
plete or faulty information, where actions may not
always have the same results, and where there may
be tradeoffs between the different possible out-
comes of actions and plans.
There are various sources of uncertainty in sys-
tems which employ NLG techniques, for example:
? the current state of the user / audience (e.g.
their knowledge, preferred vocabulary, goals,
preferences....),
? the likely user reaction to the generated out-
put,
? the behaviour of related components (e.g. a
surface realiser, or TTS module),
? noise in the environment (for spoken output),
? ambiguity of the generated output.
The problem here is to generate output that
takes these types of uncertainty into account ap-
propriately. For example, you may need to choose
a referring expression for a user, even though you
are not sure whether they are an expert or novice in
the domain. In addition, the next time you speak
to that user, you need to adapt to new informa-
tion you have gained about them (Janarthanam and
Lemon, 2010). The issue of uncertainty for refer-
ring expression generation has been discussed be-
fore by (Reiter, 1991; Horacek, 2005).
Another example is in planning an Information
Presentation for a user, when you cannot know
with certainty how they will respond to it (Rieser
and Lemon, 2009; Rieser et al, 2010). In the worst
case, you may even be uncertain about the user?s
goals or information needs (as in ?POMDP? ap-
proaches to dialogue management (Young et al,
2009; Henderson and Lemon, 2008a)), but you
still need to generate output for them in an appro-
priate way.
In particular, in interactive applications of NLG:
? each NLG action changes the environment
state or context,
? the effect of each NLG action is uncertain.
Several recent approaches describe NLG tasks
as different kinds of planning, e.g. (Koller and Pet-
rick, 2008; Rieser et al, 2010; Janarthanam and
Lemon, 2010), or as contextual decision making
according to a cost function (van Deemter, 2009).
It would be very interesting to explore how differ-
ent approaches perform in NLG problems where
different types of uncertainty are present in the
generation environment.
In the following we discuss possible genera-
tion challenges arising from such considerations,
which we hope will lead to work on an agreed
shared challenge in this research community. In
section 2 we briefly review recent work showing
that simulated environments can be used to evalu-
ate generation under uncertainty, and in section 3
we discuss some possible metrics for such tasks.
Section 4 concludes by considering how a useful
generation challenge could be constructed using
similar methods.
2 Generation in Uncertain Simulated
Environments
Finding the best (or ?optimal?) way to generate
under uncertainty requires exploring the possible
outcomes of actions in stochastic environments.
Therefore, related research on Dialogue Strategy
learning has used data-driven simulated environ-
ments as a cheap and efficient way to explore un-
certainty (Lemon and Pietquin, 2007). However,
building good simulated environments is a chal-
lenge in its own right, as we illustrate in the fol-
lowing using the examples of Information Presen-
tation and Referring Expression Generation. We
also point out the additional challenges these sim-
ulations have to face when being used for NLG.
2.1 User Simulations for Information
Presentation
User Simulations can provide a model of proba-
ble, but uncertain, user reactions to NLG actions,
and we propose that they are a useful potential
direction for exploring and evaluate different ap-
proaches to handling uncertainty in generation.
User Simulations are commonly used to train
strategies for Dialogue Management, see for ex-
ample (Young et al, 2007). A user simulation for
Information Presentation is very similar, in that it
is a predictive model of the most likely next user
act. 1 However, this NLG predicted user act does
not actually change the overall dialogue state (e.g.
by filling slots) but it only changes the generator
state. In other words, this NLG user simulation
tells us what the user is most likely to do next, if
we were to stop generating now.
In addition to the challenges of building user
simulations for learning Dialogue policies, e.g.
modelling, evaluation, and available data sets
(Lemon and Pietquin, 2007), a crucial decision for
NLG is the level of detail needed to train sensible
1Similar to the internal user models applied in recent
work on POMDP (Partially Observable Markov Decision
Process) dialogue managers (Young et al, 2007; Henderson
and Lemon, 2008b; Gasic et al, 2008) for estimation of user
act probabilities.
policies. While high-level dialogue act descrip-
tions may be sufficient for dialogue policies, NLG
decisions may require a much finer level of detail.
The finer the required detail of user reactions, the
more data is needed to build data-driven simula-
tions.
For content selection in Information Presen-
tation tasks (choosing presentation strategy and
number of attributes), for example, the level of de-
scription can still be fairly abstract. We were most
interested in probability distributions over the fol-
lowing possible user reactions:
1. select: the user chooses one of the pre-
sented items, e.g. ?Yes, I?ll take that one.?.
This reply type indicates that the informa-
tion presentation was sufficient for the user
to make a choice.
2. addInfo: The user provides more at-
tributes, e.g. ?I want something cheap.?. This
reply type indicates that the user has more
specific requests, which s/he wants to specify
after being presented with the current infor-
mation.
3. requestMoreInfo: The user asks for
more information, e.g. ?Can you recommend
me one??, ?What is the price range of the
last item??. This reply type indicates that the
system failed to present the information the
user was looking for.
4. askRepeat: The user asks the system to
repeat the samemessage again, e.g. ?Can you
repeat??. This reply type indicates that the
utterance was either too long or confusing for
the user to remember, or the TTS quality was
not good enough, or both.
5. silence: The user does not say anything.
In this case it is up to the system to take ini-
tiative.
6. hangup: The user closes the interaction.
We have built user simulations using n-gram
models of system (s) and user (u) acts, as first
introduced by (Eckert et al, 1997). In order to
account for data sparsity, we apply different dis-
counting (?smoothing?) techniques including au-
tomatic back-off, using the CMU Statistical Lan-
guage Modelling toolkit (Clarkson and Rosenfeld,
1997). For example we have constructed a bi-
gram model2 for the users? reactions to the sys-
tem?s IP structure decisions (P (au,t|IPs,t)), and
a tri-gram (i.e. IP structure + attribute choice)
model for predicting user reactions to the system?s
combined IP structure and attribute selection deci-
sions: P (au,t|IPs,t, attributess,t).
We have evaluated the performance of these
models by measuring dialogue similarity to the
original data, based on the Kullback-Leibler (KL)
divergence, as also used by e.g. (Cuaya?huitl et al,
2005; Jung et al, 2009; Janarthanam and Lemon,
2009). We compared the raw probabilities as ob-
served in the data with the probabilities generated
by our n-gram models using different discounting
techniques for each context. All the models have a
small divergence from the original data (especially
the bi-gram model), suggesting that they are rea-
sonable simulations for training and testing NLG
policies (Rieser et al, 2010).
2.2 Other Simulated Components
In some systems, NLG decisions may also depend
on related components, such as the database, sub-
sequent generation steps, or the Text-to-Speech
module for spoken generation. Building simula-
tions for these components to capture their inher-
ent uncertainty, again, is an interesting challenge.
For example, one might want to adapt the gen-
erated output according to the predicted TTS qual-
ity. Therefore, one needs a model of the expected/
predicted TTS quality for a TTS engine (Boidin et
al., 2009).
Furthermore, NLG decisions might be inputs
to a stochastic sentence realiser, such as SPaRKy
(Stent et al, 2004). However, one might not have
a fully trained stochastic sentence realiser for this
domain (yet). In (Rieser et al, 2010) we therefore
modelled the variance as observed in the top rank-
ing SPaRKy examples.
2.3 Generating Referring Expressions under
uncertainty
In this section, we present an example user simu-
lation (US) model, that simulates the dialogue be-
haviour of users who react to referring expressions
depending on their domain knowledge. These ex-
ternal simulation models are different from inter-
nal user models used by dialogue systems. In
2Where au,t is the predicted next user action at time t,
IPs,t was the system?s Information Presentation action at t,
and attributess,t is the set of attributes selected by the sys-
tem at t.
particular, such models must be sensitive to a
system?s choices of referring expressions. The
simulation has a statistical distribution of in-built
knowledge profiles that determines the dialogue
behaviour of the user being simulated. Uncer-
tainty arises because if the user does not know a
referring expression, then he is more likely to re-
quest clarification. If the user is able to interpret
the referring expressions and identify the refer-
ences then he is more likely to follow the system?s
instruction. This behaviour is simulated by the ac-
tion selection models described below.
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). We used two kinds of action selection mod-
els: a corpus-driven statistical model and a hand-
coded rule-based model.
2.4 Corpus-driven action selection model
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). The US produces a clarification request cr
based on the class of the referent C(Ri), type of
the referring expression Ti, and the current domain
knowledge of the user for the referring expression
DKu,t(Ri, Ti). Domain entities whose jargon ex-
pressions raised clarification requests in the cor-
pus were listed and those that had more than the
mean number of clarification requests were clas-
sified as difficult and others as easy enti-
ties (for example, ?power adaptor? is easy - all
users understood this expression, ?broadband fil-
ter? is difficult). Clarification requests are
produced using the following model.
P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) ? RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment actionEAu,t
is generated using the following model.
P (EAu,t|As,t) if Au,t! = cr(Ri, Ti)
Finally, the user action is an instruction re-
sponse which is determined by the system ac-
tion As,t. Instruction responses can be ei-
ther provide info, acknowledgement or other
based on the system?s instruction.
P (Au,t = ir|EAu,t, As,t)
All the above models were trained on our cor-
pus data using maximum likelihood estimation
and smoothed using a variant of Witten-Bell dis-
counting. According to the data, clarification re-
quests are much more likely when jargon expres-
sions are used to refer to the referents that be-
long to the difficult class and which the user
doesn?t know about. When the system uses ex-
pressions that the user knows, the user gener-
ally responds to the instruction given by the sys-
tem. These user simulation models have been
evaluated and found to produce behaviour that is
very similar to the original corpus data, using the
Kullback-Leibler divergence metric (Janarthanam
and Lemon, 2010).
3 Metrics
Here we discuss some possible evaluation met-
rics that will allow different approaches to NLG
under uncertainty to be compared. We envisage
that other metrics should be explored, in particular
those measuring adaptivity of various types.
3.1 Adaptive Information Presentation
Given a suitable corpus, a data-driven evaluation
function can be constructed, using a stepwise lin-
ear regression, following the PARADISE frame-
work (Walker et al, 2000).
For example, in (Rieser et al, 2010) we
build a model which selects the features which
significantly influenced the users? ratings for
NLG strategies in a Wizard-of-Oz study. We
also assign a value to the user?s reactions
(valueUserReaction), similar to optimising task
success for DM (Young et al, 2007). This re-
flects the fact that good Information Presentation
strategies should help the user to select an item
(valueUserReaction = +100) or provide more
constraints addInfo (valueUserReaction =
?0), but the user should not do anything else
(valueUserReaction = ?100). The regression
in equation 1 (R2 = .26) indicates that users? rat-
ings are influenced by higher level and lower level
features: Users like to be focused on a small set
of database hits (where #DBhits ranges over [1-
100]), which will enable them to choose an item
(valueUserReaction), while keeping the IP ut-
terances short (where #sentence was in the range
[2-18]):
Reward = (?1.2)?#DBhits (1)
+(.121)? valueUserReaction
?(1.43)?#sentence
3.2 Measuring Adaptivity of Referring
Expressions
We have also designed a metric for the goal of
adapting referring expressions to each user?s do-
main knowledge. We present the Adaptation Ac-
curacy score AA that calculates how accurately
the agent chose the expressions for each referent
r, with respect to the user?s knowledge. Appro-
priateness of an expression is based on the user?s
knowledge of the expression. So, when the user
knows the jargon expression for r, the appropri-
ate expression to use is jargon, and if s/he doesn?t
know the jargon, an descriptive expression is ap-
propriate. Although the user?s domain knowledge
is dynamically changing due to learning, we base
appropriateness on the initial state, because our
objective is to adapt to the initial state of the user
DKu,initial. However, in reality, designers might
want their system to account for user?s changing
knowledge as well. We calculate accuracy per ref-
erent RAr as the ratio of number of appropriate
expressions to the total number of instances of the
referent in the dialogue. We then calculate the
overall mean accuracy over all referents as shown
below.
RAr =
#(appropriate expressions(r))
#(instances(r))
AdaptationAccuracyAA = 1#(r)?rRAr
4 Conclusion
We have invited the research community to con-
sider challenges for NLG which arise from uncer-
tainty. We argue that NLG systems, like dialogue
managers, should be able to adapt to their audi-
ence and the generation environment. However,
often the important features for adaptation are not
precisely known. We then summarised 2 potential
directions for such challenges ? example genera-
tion tasks which employ simulated uncertain en-
vironments to study adaptive NLG, and discussed
some possible metrics for such tasks. We hope
that this will lead to discussions on a shared chal-
lenge allowing comparison of different approaches
to NLG with respect to how well they handle un-
certainty.
Acknowledgments
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
Cedric Boidin, Verena Rieser, Lonneke van der Plas,
Oliver Lemon, and Jonathan Chevelu. 2009. Pre-
dicting how it sounds: Re-ranking alternative in-
puts to TTS using latent variables (forthcoming). In
Proc. of Interspeech/ICSLP, Special Session on Ma-
chine Learning for Adaptivity in Spoken Dialogue
Systems.
P.R. Clarkson and R. Rosenfeld. 1997. Statisti-
cal Language Modeling Using the CMU-Cambridge
Toolkit. In Proc. of ESCA Eurospeech.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and
Hiroshi Shimodaira. 2005. Human-computer dia-
logue simulation using hidden markov models. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
modeling for spoken dialogue system evaluation. In
Proc. of the IEEE workshop on Automatic Speech
Recognition and Understanding (ASRU).
M. Gasic, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and S. Young. 2008. Training and
Evaluation of the HIS POMDP Dialogue System in
Noise. In Proc. of SIGdial Workshop on Discourse
and Dialogue.
James Henderson and Oliver Lemon. 2008a. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proceedings of
ACL.
James Henderson and Oliver Lemon. 2008b. Mixture
Model POMDPs for Efficient Handling of Uncer-
tainty in Dialogue Management. In Proc. of ACL.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In ENLG.
Srinivasan Janarthanam and Oliver Lemon. 2009. A
Two-tier User Simulation Model for Reinforcement
Learning of Adaptive Referring Expression Genera-
tion Policies. In Proc. of SIGdial.
Srini Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: Referring expression
generation in spoken dialogue systems. In Proceed-
ings of ACL. (to appear).
Sangkeun Jung, Cheongjae Lee, Kyungduk Kim, Min-
woo Jeong, and Gary Geunbae Lee. 2009. Data-
driven user simulation for automated evaluation of
spoken dialog systems. Computer, Speech & Lan-
guage, 23:479?509.
Alexander Koller and Ronald Petrick. 2008. Experi-
ences with planning for natural language generation.
In ICAPS.
Oliver Lemon and Olivier Pietquin. 2007. Machine
learning for spoken dialogue systems. In Inter-
speech.
E. Reiter. 1991. Generating Descriptions that Exploit a
User?s Domain Knowledge. In R. Dale, C. Mellish,
and M. Zock, editors, Current Research in Natural
Language Generation, pages 257?285. Academic
Press.
Verena Rieser and Oliver Lemon. 2009. Natural lan-
guage generation as planning under uncertainty for
spoken dialogue systems. In EACL.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proceedings of ACL. (to appear).
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex in-
formation presentation in spoken dialog systems. In
Association for Computational Linguistics.
Kees van Deemter. 2009. What game theory can do
for NLG: the case of vague language. In 12th Eu-
ropean Workshop on Natural Language Generation
(ENLG).
Marilyn A. Walker, Candace A. Kamm, and Diane J.
Litman. 2000. Towards Developing General Mod-
els of Usability with PARADISE. Natural Lan-
guage Engineering, 6(3).
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to
Dialog Management. In ICASSP 2007.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2009. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language. To appear.
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 124?131,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Adaptive Referring Expression Generation in Spoken Dialogue Systems:
Evaluation with Real Users
Srinivasan Janarthanam
School of Informatics
University of Edinburgh
s.janarthanam@ed.ac.uk
Oliver Lemon
Interaction Lab
Mathematics and Computer Science
Heriot-Watt University
o.lemon@hw.ac.uk
Abstract
We present new results from a real-user
evaluation of a data-driven approach to
learning user-adaptive referring expres-
sion generation (REG) policies for spoken
dialogue systems. Referring expressions
can be difficult to understand in techni-
cal domains where users may not know
the technical ?jargon? names of the do-
main entities. In such cases, dialogue sys-
tems must be able to model the user?s (lex-
ical) domain knowledge and use appro-
priate referring expressions. We present
a reinforcement learning (RL) framework
in which the system learns REG policies
which can adapt to unknown users on-
line. For real users of such a system, we
show that in comparison to an adaptive
hand-coded baseline policy, the learned
policy performs significantly better, with
a 20.8% average increase in adaptation ac-
curacy, 12.6% decrease in time taken, and
a 15.1% increase in task completion rate.
The learned policy also has a significantly
better subjective rating from users. This is
because the learned policies adapt online
to changing evidence about the user?s do-
main expertise. We also discuss the issue
of evaluation in simulation versus evalua-
tion with real users.
1 Introduction
We present new results from an evaluation with
real users, for a reinforcement learning (Sutton
and Barto, 1998) framework to learn user-adaptive
referring expression generation policies from data-
driven user simulations. Such a policy allows the
system to choose appropriate expressions to re-
fer to domain entities in a dialogue setting. For
instance, in a technical support conversation, the
Jargon: Please plug one end of the broadband
cable into the broadband filter.
Descriptive: Please plug one end of the thin
white cable with grey ends into the
small white box.
Table 1: Referring expression examples for 2 enti-
ties (from the corpus)
system could choose to use more technical terms
with an expert user, or to use more descriptive and
general expressions with novice users, and a mix
of the two with intermediate users of various sorts
(see examples in Table 1).
In natural human-human conversations, dia-
logue partners learn about each other and adapt
their language to suit their domain expertise (Is-
sacs and Clark, 1987). This kind of adaptation
is called Alignment through Audience
Design (Clark and Murphy, 1982; Bell, 1984).
We assume that users are mostly unknown to
the system and therefore that a spoken dialogue
system (SDS) must be capable of observing the
user?s dialogue behaviour, modelling his/her do-
main knowledge, and adapting accordingly, just
like human interlocutors. Therefore unlike sys-
tems that use static user models, our system has to
dynamically model the user?s domain knowledge
in order to adapt during the conversation.
We present a corpus-driven framework for
learning a user-adaptive REG policy from a small
corpus of non-adaptive human-machine interac-
tion. We show that the learned policy performs
better than a simple hand-coded adaptive policy
in terms of accuracy of adaptation, dialogue time
and task completion rate when evaluated with real
users in a wizarded study.
In section 2, we present some of the related
work. Section 3 and section 4 describe the dia-
logue system framework and the user simulation
124
model. In section 5, we present the training and in
section 6, we present the evaluation for different
REG policies with real users.
2 Related work
Rule-based and supervised learning approaches
have been proposed to learn and adapt during
conversations dynamically. Such systems learn
from a user at the start and later adapt to the do-
main knowledge of the user. However, they either
require expensive expert knowledge resources to
hand-code the inference rules (Cawsey, 1993) or a
large corpus of expert-layperson interaction from
which adaptive strategies can be learned and mod-
elled, using methods such as Bayesian networks
(Akiba and Tanaka, 1994). In contrast, we present
an approach that learns in the absence of these
expensive resources. It is also not clear how su-
pervised approaches choose between when to seek
more information and when to adapt. In this study,
we show that using reinforcement learning this de-
cision is learned automatically.
Reinforcement Learning (RL) has been suc-
cessfully used for learning dialogue management
policies since (Levin et al, 1997). The learned
policies allow the dialogue manager to optimally
choose appropriate dialogue acts such as instruc-
tions, confirmation requests, and so on, under
uncertain noise or other environment conditions.
There have been recent efforts to learn infor-
mation presentation and recommendation strate-
gies using reinforcement learning (Hernandez et
al., 2003; Rieser and Lemon, 2009; Rieser and
Lemon, 2010), and joint optimisation of Dialogue
Management and NLG using hierarchical RL has
been proposed by (Lemon, 2010). In addition,
we present a framework to learn to choose appro-
priate referring expressions based on a user?s do-
main knowledge. Following a proof-of-concept
study using a hand-coded rule-based user simu-
lation (Janarthanam and Lemon, 2009c), we pre-
viously showed that adaptive REG policies can
be learned using an RL framework with data-
driven user simulations and that such policies per-
form better than simple hand-coded policies (Ja-
narthanam and Lemon, 2010).
3 The Dialogue System
In this section, we describe the different modules
of the dialogue system. The interaction between
the different modules is shown in figure 1 (in
learning mode). The dialogue system presents the
user with instructions to setup a broadband con-
nection at home. In the Wizard of Oz setup, the
system and the user interact using speech. How-
ever, in our machine learning setup, they interact at
the abstract level of dialogue actions and referring
expressions. Our objective is to learn to choose
the appropriate referring expressions to refer to the
domain entities in the instructions.
Figure 1: System User Interaction (learning)
3.1 Dialogue Manager
The dialogue manager identifies the next dialogue
act (As,t where t denotes turn, s denotes system)
to give to the user based on the dialogue man-
agement policy pidm. The dialogue management
is coded in the form of a finite state machine. In
this dialogue task, the system provides instructions
to either observe or manipulate the environment.
When users ask for clarifications on referring ex-
pressions, the system clarifies (provide clar) by
giving information to enable the user to associate
the expression with the intended referent. When
the user responds in any other way, the instruc-
tion is simply repeated. The dialogue manager
is also responsible for updating and managing the
system state Ss,t (see section 3.2). The system in-
teracts with the user by passing both the system
action As,t and the referring expressions RECs,t
(see section 3.3).
3.2 The dialogue state
The dialogue state Ss,t is a set of variables that
represent the current state of the conversation. In
our study, in addition to maintaining an overall di-
alogue state, the system maintains a user model
UMs,t which records the initial domain knowl-
edge of the user. It is a dynamic model that starts
125
with a state where the system does not have any
knowledge about the user. Since the model is up-
dated according to the user?s behaviour, it may be
inaccurate if the user?s behaviour is itself uncer-
tain. Hence, the user model used in this system is
not always an accurate model of the user?s knowl-
edge and reflects a level of uncertainty about the
user.
Each jargon referring expression x is repre-
sented by a three-valued variable in the dialogue
state: user knows x. The three values that each
variable takes are yes, no, not sure. The vari-
ables are updated using a simple user model up-
date algorithm after the user?s response each turn.
Initially each variable is set to not sure. If the
user responds to an instruction containing the re-
ferring expression x with a clarification request,
then user knows x is set to no. Similarly, if
the user responds with appropriate information to
the system?s instruction, the dialogue manager sets
user knows x is set to yes. Only the user?s ini-
tial knowledge is recorded. This is based on the
assumption that an estimate of the user?s initial
knowledge helps to predict the user?s knowledge
of the rest of the referring expressions.
3.3 REG module
The REG module is a part of the NLG module
whose task is to identify the list of domain enti-
ties to be referred to and to choose the appropriate
referring expression for each of the domain enti-
ties for each given dialogue act. In this study, we
focus only on the production of appropriate refer-
ring expressions to refer to domain entities men-
tioned in the dialogue act. It chooses between
the two types of referring expressions - jargon
and descriptive. For example, the domain entity
broadband filter can be referred to using the jar-
gon expression ?broadband filter? or using the de-
scriptive expression ?small white box?1. Although
adaptation is the primary goal, it should be noted
that in order to get an idea of the user the system
is dealing with, it needs to seek information using
jargon expressions.
The REG module operates in two modes - learn-
ing and evaluation. In the learning mode, the REG
module is the learning agent. The REG module
learns to associate dialogue states with optimal re-
ferring expressions. This is represented by a REG
1We will use italicised forms to represent the domain enti-
ties (e.g. broadband filter) and double quotes to represent the
referring expressions (e.g. ?broadband filter?).
policy pireg : UMs,t ? RECs,t, which maps
the states of the dialogue (user model) to opti-
mal referring expressions. The referring expres-
sion choices RECs,t is a set of pairs identifying
the referent R and the type of expression T used in
the current system utterance. For instance, the pair
(broadband filter, desc) represents the descriptive
expression ?small white box?.
RECs,t = {(R1, T1), ..., (Rn, Tn)}
In the evaluation mode, a trained REG policy in-
teracts with unknown users. It consults the learned
policy pireg to choose the referring expressions
based on the current user model.
4 User Simulations
In this section, we present user simulation mod-
els that simulate the dialogue behaviour of a real
human user. Several user simulation models have
been proposed for use in reinforcement learning
of dialogue policies (Georgila et al, 2005; Schatz-
mann et al, 2006; Schatzmann et al, 2007; Ai and
Litman, 2007). However, they are suited only for
learning dialogue management policies, and not
natural language generation policies. In particular,
our model is the first to be sensitive to a system?s
choices of referring expressions. Earlier, we pre-
sented a two-tier simulation trained on data pre-
cisely for REG policy learning (Janarthanam and
Lemon, 2009a). However, it is not suited for train-
ing on small corpus like the one we have at our
disposal. In contrast to the earlier model, we now
condition the clarification requests on the referent
class rather than the referent itself to handle the
data sparsity problem.
4.1 Corpus-driven action selection model
The user simulation (US) receives the system
action As,t and its referring expression choices
RECs,t at each turn. The US responds with a user
action Au,t (u denoting user). This can either be a
clarification request (cr) or an instruction response
(ir). The US produces a clarification request cr
based on the class of the referent C(Ri), type of
the referring expression Ti, and the current domain
knowledge of the user for the referring expression
DKu,t(Ri, Ti). Domain entities whose jargon ex-
pressions raised clarification requests in the cor-
pus were listed and those that had more than the
mean number of clarification requests were clas-
sified as difficult and others as easy enti-
ties (for example, power adaptor is easy - all
126
users understood this expression, broadband filter
is difficult). Clarification requests are pro-
duced using the following model.
P (Au,t = cr(Ri, Ti)|C(Ri), Ti, DKu,t(Ri, Ti))
where (Ri, Ti) ? RECs,t
One should note that the actual literal expres-
sion is not used in the transaction. Only the entity
that it is referring to (Ri) and its type (Ti) are used.
However, the above model simulates the process
of interpreting and resolving the expression and
identifying the domain entity of interest in the in-
struction. The user identification of the entity is
signified when there is no clarification request pro-
duced (i.e. Au,t = none). When no clarification
request is produced, the environment action EAu,t
is generated using the following model.
P (EAu,t|As,t) if Au,t! = cr(Ri, Ti)
Finally, the user action is an instruction re-
sponse which is determined by the system ac-
tion As,t. Instruction responses can be ei-
ther provide info, acknowledgement or other
based on the system?s instruction.
P (Au,t = ir|EAu,t, As,t)
All the above models were trained on our cor-
pus data using maximum likelihood estimation and
smoothed using a variant of Witten-Bell discount-
ing. The corpus contained dialogues between
a non-adaptive dialogue system and real users.
According to the data, clarification requests are
much more likely when jargon expressions are
used to refer to the referents that belong to the
difficult class and which the user doesn?t
know about. When the system uses expressions
that the user knows, the user generally responds to
the instruction given by the system.
4.2 User Domain knowledge
The user domain knowledge is initially set to one
of several models at the start of every conver-
sation. The models range from novices to ex-
perts which were identified from the corpus using
k-means clustering. A novice user knows only
?power adaptor?, an expert knows all the jargon
expressions and intermediate users know some.
We assume that users can interpret the descriptive
expressions and resolve their references. There-
fore, they are not explicitly represented. We only
code the user?s knowledge of jargon expressions
using boolean variables representing whether the
user knows the expression or not.
4.3 Corpus
We trained the action selection model on a small
corpus of 12 non-adaptive dialogues between real
users and a dialogue system. There were six
dialogues in which users interacted with a sys-
tem using just jargon expressions and six with a
system using descriptive expressions. For more
discussions on our user simulation models and
the corpus, please refer to (Janarthanam and
Lemon, 2009b; Janarthanam and Lemon, 2009a;
Janarthanam and Lemon, 2010).
5 Training
The REG module was trained (operated in learn-
ing mode) using the above simulations to learn
REG policies that select referring expressions
based on the user expertise in the domain. In
this section, we discuss how to code the learning
agent?s goals as reward. We then discuss how the
reward function is used to train the learning agent.
5.1 Reward function
We designed a reward function for the goal of
adapting to each user?s domain knowledge. We
present the Adaptation Accuracy score (AA) that
calculates how accurately the agent chose the ap-
propriate expressions for each referent r, with re-
spect to the user?s knowledge. So, when the user
knows the jargon expression for r, the appropri-
ate expression to use is jargon, and if s/he doesn?t
know the jargon, a descriptive expression is appro-
priate. Although the user?s domain knowledge is
dynamically changing due to learning, we base ap-
propriateness on the initial state, because our ob-
jective is to adapt to the initial state of the user
DKu,initial. However, in reality, designers might
want their system to account for user?s chang-
ing knowledge as well. We calculate accuracy
per referent RAr and then calculate the overall
mean adaptation accuracy (AA) over all referents
as shown below.
RAr = #(appropriate expressions(r))#(instances(r))
AdaptationAccuracyAA = 1#(r)?rRAr
5.2 Learning
The REG module was trained in learning mode us-
ing the above reward function using the SHAR-
SHA reinforcement learning algorithm (with lin-
ear function approximation) (Shapiro and Langley,
2002). This is a hierarchical variant of SARSA,
127
which is an on-policy learning algorithm that up-
dates the current behaviour policy (see (Sutton
and Barto, 1998)). The training produced approx.
5000 dialogues. The user simulation was cali-
brated to produce three types of users: Novice,
Intermediate and Expert, randomly but with equal
probability.
Initially, the REG policy chooses randomly be-
tween the referring expression types for each do-
main entity in the system utterance, irrespective
of the user model state. Once the referring ex-
pressions are chosen, the system presents the user
simulation with both the dialogue act and refer-
ring expression choices. The choice of referring
expression affects the user?s dialogue behaviour.
For instance, choosing a jargon expression could
evoke a clarification request from the user, based
on which, the dialogue manager updates the inter-
nal user model (UMs,t) with the new information
that the user is ignorant of the particular expres-
sion. It should be noted that using a jargon expres-
sion is an information seeking move which enables
the REG module to estimate the user?s knowledge
level. The same process is repeated for every dia-
logue instruction. At the end of the dialogue, the
system is rewarded based on its choices of refer-
ring expressions. If the system chooses jargon ex-
pressions for novice users or descriptive expres-
sions for expert users, penalties are incurred and if
the system chooses REs appropriately, the reward
is high. On the one hand, those actions that fetch
more reward are reinforced, and on the other hand,
the agent tries out new state-action combinations
to explore the possibility of greater rewards. Over
time, it stops exploring new state-action combina-
tions and exploits those actions that contribute to
higher reward. The REG module learns to choose
the appropriate referring expressions based on the
user model in order to maximize the overall adap-
tation accuracy. Figure 2 shows how the agent
learns using the data-driven (Learned DS) during
training. It can be seen in the figure 2 that towards
the end the curve plateaus, signifying that learning
has converged.
6 Evaluation
In this section, we present the details of the eval-
uation process, the baseline policy, the metrics
used, and the results. In a recent study, we eval-
uated the learned policy and several hand-coded
baselines with simulated users and found that
Figure 2: Learning curve - Training
the Learned-DS policy produced higher adapta-
tion accuracy than other policies (Janarthanam and
Lemon, 2010). An interesting issue for research
in this area is to what extent evaluation results ob-
tained in simulated environments transfer to eval-
uations with real users (Lemon et al, 2006).
6.1 Baseline system
In order to compare the performance of the learned
policy with a baseline, a simple rule-based policy
was built. This baseline was chosen because it per-
formed better in simulation, compared to a vari-
ety of other baselines (Janarthanam and Lemon,
2010). It uses jargon for all referents by default
and provides clarifications when requested. It ex-
ploits the user model in subsequent references af-
ter the user?s knowledge of the expression has
been set to either yes or no. Therefore, although
it is a simple policy, it adapts to a certain extent
(?locally?). We refer to this policy as the ?Jargon-
adapt? policy. It should be noted that this policy
was built in the absence of expert domain knowl-
edge and/or an expert-layperson corpus.
6.2 Process
We evaluated the two policies with real users.
36 university students from different backgrounds
(e.g. Arts, Humanities, Medicine and Engineer-
ing) participated in the evaluation. 17 users were
given a system with Jargon-adapt policy and 19
users interacted with a system with Learned-DS
policy. Each user was given a pre-task recognition
test to record his/her initial domain knowledge.
The experimenter read out a list of technical terms
and the user was asked to point out to the domain
entities laid out in front of them. They were then
128
given one of the two systems - learned or base-
line, to interact with. Following the system in-
structions, they then attempted to set up the broad-
band connection. When the dialogue had ended,
the user was given a post-task test where the recog-
nition test was repeated and their responses were
recorded. The user?s broadband connection setup
was manually examined for task completion (i.e.
the percentage of correct connections that they had
made in their final set-up). The user was given the
task completion results and was then given a user
satisfaction questionnaire to evaluate the features
of the system based on the conversation.
All users interacted with a wizarded system em-
ploying one of the two REG policies (see figure
3). The user?s responses were intercepted by a hu-
man interpreter (or ?wizard?) and were annotated
as dialogue acts, to which the automated dialogue
manager responded with a system dialogue action
(the dialogue policy was fixed). The wizards were
not aware of the policy used by the system. The
respective policies chose only the referring expres-
sions to generate the system utterance for the given
dialogue action. The system utterances were con-
verted to speech by a speech synthesizer (Cere-
proc) and were played to the user.
Figure 3: Wizarded Dialogue System
6.3 Metrics
In addition to the adaptation accuracy mentioned
in section 5.1, we also measure other parame-
ters from the conversation in order to show how
learned adaptive policies compare with other poli-
cies on other dimensions. We also measure the
learning effect on the users as (normalised) learn-
ing gain (LG) produced by using unknown jargon
expressions. This is calculated using the pre- and
post-test scores for the user domain knowledge
(DKu) as follows.
Metrics Jargon-adapt Learned-DS
AA 63.91 84.72 **
LG 0.59 0.61
DT 7.86 6.98 *
TC 84.7 99.8 **
* Statistical significance (p < 0.05).
** Statistical significance (p < 0.001).
Table 2: Evaluation with real users
Learning Gain LG = Post?Pre1?Pre
Dialogue time (DT) is the actual time taken for
the user to complete the task. We measured task
completion (TC) by examining the user?s broad-
band setup after the task was completed (i.e. the
percentage of correct connections that they had
made in their final set-up).
6.4 Results
We compare the performance of the two strategies
on real users using objective parameters and sub-
jective feedback scores. Tests for statistical sig-
nificance were done using Mann-Whitney test for
2 independent samples (due to non-parametric na-
ture of the data).
Table 2 presents the mean accuracy of adap-
tation (AA), learning gain (LG), dialogue time
(DT), and task completion (TC), produced by the
two strategies. The Learned-DS strategy pro-
duced more accurate adaptation than the Jargon-
adapt strategy (p<0.001, U=9.0, r=-0.81). Higher
accuracy of adaptation (AA) of the Learned-DS
strategy translates to less dialogue time (U=73.0,
p<0.05, r=-0.46) and higher task completion
(U=47.5, p<0.001, r=-0.72) than the Jargon-adapt
policy. However, there was no significant differ-
ence in learning gain (LG).
Table 3 presents how the users subjectively
scored on a agreement scale of 1 to 4 (with 1
meaning ?strongly disagree?), different features of
the system based on their conversations with the
two different strategies. Users? feedback on dif-
ferent features of the systems were not very differ-
ent from each other. However, users did feel that
it was easier to identify domain objects with the
Learned-DS strategy than the Jargon-adapt strat-
egy (U=104.0, p<0.05, r=-0.34). To our knowl-
edge, this is the first study to show a significant
improvement in real user ratings for a learned pol-
icy in spoken dialogue systems (normally, objec-
tive metrics show an improvement, but not subjec-
129
Feedback questions Jargon-adapt Learned-DS
Q1. Quality of voice 3.11 3.36
Q2. Had to ask too many questions 2.23 1.89
Q3. System adapted very well 3.41 3.58
Q4. Easy to identify objects 2.94 3.37 *
Q5. Right amount of dialogue time 3.23 3.26
Q6. Learned useful terms 2.94 3.05
Q7. Conversation was easy 3.17 3.42
Q8. Future use 3.22 3.47
* Statistical significance (p < 0.05).
Table 3: Real user feedback
tive scores (Lemon et al, 2006)).
6.5 Analysis
The results show that the Learned-DS strategy is
significantly better than the hand-coded Jargon-
Adapt policy in terms of adaptation accuracy, di-
alogue time, and task completion rate. The ini-
tial knowledge of the users (mean pre-task recog-
nition score) of the two groups were not signifi-
cantly different from each other (Jargon-adapt =
7.33, Learned-DS = 7.45). Hence there is no bias
on the user?s pre-task score towards any strategy.
While the Learned-DS system adapts well to its
users globally, the Jargon-adapt system adapted
only locally. This led to higher task completion
rate and lower dialogue time.
The Learned-DS strategy enabled the system to
adapt using the dependencies that it learned dur-
ing the training phase. For instance, when the user
asked for clarification on some referring expres-
sions (e.g. ?ethernet cable?), it used descriptive
expressions for domain objects like ethernet light
and ethernet socket. Such adaptation across ref-
erents enabled the Learned-DS strategy to score
better than the Jargon-adapt strategy. Since the
agent starts the conversation with no knowledge
about the user, it learned to use information seek-
ing moves (use jargon) at appropriate moments,
although they may be inappropriate. But since it
was trained to maximize the adaptation accuracy,
the agent also learned to restrict such moves and
start predicting the user?s domain knowledge as
soon as possible. By learning to trade-off between
information-seeking and adaptation, the Learned-
DS policy produced a higher adaptation with real
users with different domain knowledge levels.
The users however did not generally rate the
two policies differently. However, they did rate
it (significantly) easier to identify objects when
using the learned policy. For the other ratings,
users seemed to be not able to recognize the nu-
ances in the way the system adapted to them. They
could have been satisfied with the fact that the sys-
tem adapted better (Q3). This adaptation and the
fact that the system offered help when the users
were confused in interpreting the technical terms,
could have led the users to score the system well in
terms of future use (Q8), dialogue time (Q5), and
ease of conversation (Q7), but in common with ex-
periments in dialogue management (Lemon et al,
2006) it seems that users find it difficult to evaluate
these improvements subjectively. The users were
given only one of the two strategies and therefore
were not in a position to compare the two strate-
gies and judge which one is better. Results in table
3 lead us to conclude that perhaps users need to
compare two or more strategies in order to judge
the strategies better.
7 Conclusion
We presented new results from an evaluation with
real users. In this study, we have shown that user-
adaptive REG policies can be learned using an RL
framework and data-driven user simulations. It
learned to trade off between adaptive moves and
information seeking moves automatically to max-
imize the overall adaptation accuracy. The learned
policy started the conversation with information
seeking moves, learned a little about the user, and
started adapting dynamically as the conversation
progressed. We also showed that the learned pol-
icy performs better than a reasonable hand-coded
policy with real users in terms of accuracy of adap-
tation, dialogue time, task completion, and a sub-
jective evaluation. Finally, this paper provides
further evidence that evaluation results obtained
130
in simulated environments can transfer reliably to
evaluations with real users (Lemon et al, 2006).
Whether the learned policy would perform bet-
ter than a hand-coded policy which was painstak-
ingly crafted by a domain expert (or learned us-
ing supervised methods from an expert-layperson
corpus) is an interesting question that needs fur-
ther exploration. Also, it would also be interesting
to make the learned policy account for the user?s
learning behaviour and adapt accordingly. We also
believe that this framework can be extended to in-
clude other decisions in NLG besides REG (Deth-
lefs and Cuayahuitl, 2010).
Acknowledgements
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 216594 (CLASSiC project
www.classic-project.org) and from the
EPSRC, project no. EP/G069840/1.
References
H. Ai and D. Litman. 2007. Knowledge consistent
user simulations for dialog systems. In Proceedings
of Interspeech 2007, Antwerp, Belgium.
T. Akiba and H. Tanaka. 1994. A Bayesian approach
for User Modelling in Dialogue Systems. In Pro-
ceedings of the 15th conference on Computational
Linguistics - Volume 2, Kyoto.
A. Bell. 1984. Language style as audience design.
Language in Society, 13(2):145?204.
A. Cawsey. 1993. User Modelling in Interactive Ex-
planations. User Modeling and User-Adapted Inter-
action, 3(3):221?247.
H. H. Clark and G. L. Murphy. 1982. Audience de-
sign in meaning and reference. In J. F. LeNy and
W. Kintsch, editors, Language and comprehension.
Amsterdam: North-Holland.
N. Dethlefs and H. Cuayahuitl. 2010. Hierarchical Re-
inforcement Learning for Adaptive Text Generation.
In Proc. INLG 2010.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning User Simulations for Information State
Update Dialogue Systems. In Proc of Eu-
rospeech/Interspeech.
F. Hernandez, E. Gaudioso, and J. G. Boticario. 2003.
A Multiagent Approach to Obtain Open and Flexible
User Models in Adaptive Learning Communities. In
User Modeling 2003, volume 2702/2003 of LNCS.
Springer, Berlin / Heidelberg.
E. A. Issacs and H. H. Clark. 1987. References in
conversations between experts and novices. Journal
of Experimental Psychology: General, 116:26?37.
S. Janarthanam and O. Lemon. 2009a. A Two-tier
User Simulation Model for Reinforcement Learning
of Adaptive Referring Expression Generation Poli-
cies. In Proc. SigDial?09.
S. Janarthanam and O. Lemon. 2009b. A Wizard-of-
Oz environment to study Referring Expression Gen-
eration in a Situated Spoken Dialogue Task. In Proc.
ENLG?09.
S. Janarthanam and O. Lemon. 2009c. Learning Lexi-
cal Alignment Policies for Generating Referring Ex-
pressions for Spoken Dialogue Systems. In Proc.
ENLG?09.
S. Janarthanam and O. Lemon. 2010. Learning to
Adapt to Unknown Users: Referring Expression
Generation in Spoken Dialogue Systems. In Proc.
ACL?10.
O. Lemon, Georgila. K., and J. Henderson. 2006.
Evaluating Effectiveness and Portability of Rein-
forcement Learned Dialogue Strategies with real
users: the TALK TownInfo Evaluation. In
IEEE/ACL Spoken Language Technology.
O. Lemon. 2010. Learning what to say and how to say
it: joint optimization of spoken dialogue manage-
ment and Natural Language Generation. Computer
Speech and Language. (to appear).
E. Levin, R. Pieraccini, and W. Eckert. 1997. Learn-
ing Dialogue Strategies within the Markov Decision
Process Framework. In Proc. of ASRU97.
V. Rieser and O. Lemon. 2009. Natural Language
Generation as Planning Under Uncertainty for Spo-
ken Dialogue Systems. In Proc. EACL?09.
V. Rieser and O. Lemon. 2010. Optimising informa-
tion presentation for spoken dialogue systems. In
Proc. ACL. (to appear).
J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J.
Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement Learning of
Dialogue Management Strategies. Knowledge Engi-
neering Review, pages 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. J. Young. 2007. Agenda-based User Simula-
tion for Bootstrapping a POMDP Dialogue System.
In Proc of HLT/NAACL 2007.
D. Shapiro and P. Langley. 2002. Separating skills
from preference: Using learning to program by re-
ward. In Proc. ICML-02.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
131
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 209?212,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Representing Uncertainty about Complex User Goals in Statistical
Dialogue Systems
Paul A. Crook
Interaction Lab
Heriot-Watt University
Edinburgh, United Kingdom
p.a.crook@hw.ac.uk
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh, United Kingdom
o.lemon@hw.ac.uk
Abstract
We point out several problems in scaling-
up statistical approaches to spoken dia-
logue systems to enable them to deal with
complex but natural user goals, such as
disjunctive and negated goals and prefer-
ences. In particular, we explore restric-
tions imposed by current independence as-
sumptions in POMDP dialogue models.
This position paper proposes the use of
Automatic Belief Compression methods to
remedy these problems.
1 Introduction
One of the main problems for a spoken dia-
logue system is to determine the user?s goal (e.g.
plan suitable meeting times or find a good Indian
restaurant nearby) under uncertainty, and thereby
to compute the optimal next system dialogue ac-
tion (e.g. offer a restaurant, ask for clarification).
Recent research in statistical spoken dialogue sys-
tems (SSDS) has successfully addressed aspects of
these problems through the application of Partially
Observable Markov Decision Process (POMDP)
approaches (Thomson and Young, 2010; Young et
al., 2010). However POMDP SSDS are currently
limited by an impoverished representation of user
goals adopted to enable tractable learning.
Current POMDP SSDS state approximations
make it impossible to represent some plausible
user goals, e.g. someone who wants to know about
nearby cheap restaurants and high-quality ones
further away, or wants to schedule a meeting any-
time this week except monday afternoon (also see
Examples in Tables 1?3). This renders dialogue
management sub-optimal and makes it impossi-
ble to deal adequately with the following types of
user utterance: ?I?m looking for French or Ital-
ian food?, or ?Not Italian, unless it?s expensive?.
User utterances with negations and disjunctions of
various sorts are very natural, and exploit the full
power of natural language input. Moreover, work
in dialogue system evaluation, e.g. (Walker et al,
2004; Lemon et al, 2006), shows that real user
goals are generally sets of items with different fea-
tures, rather than a single item. People like to ex-
plore possible trade offs between features of items.
A central challenge for the field of spoken di-
alogue systems is therefore to: develop realistic
large-scale statistical approaches with an accurate,
extended representation of user goals.
In this paper we propose that the independence
assumptions that have guided POMDP SSDS de-
sign to date should be relaxed, user goal sets
should be introduced and that the subsequent ex-
plosion in the size of the state space should be
dealt with by employing Automatic Belief Com-
pression (ABC) techniques.
2 POMDP SSDS
Partially Observable Markov Decision Processes
(POMDPs) are Markov Decision Processes where
the system?s state is only partially observable, i.e.
there is uncertainty as to what the true state is.
The ability to account for uncertainty is crucial for
spoken dialogue systems because their knowledge
about the state is uncertain due to speech recogni-
tion errors and the fact that the user?s goals are not
directly observable. In POMDP models of spo-
ken dialogue (Williams and Young, 2005; Thom-
son and Young, 2010; Young et al, 2010) the dia-
logue policy (what the system should say next) is
based not on a single view of the current state of
the conversation, but on a probability distribution
over all possible states of the conversation. The
optimal POMDP SSDS dialogue act thus automat-
ically takes account of the uncertainty about the
user?s utterances and goals.
All work to date on POMDP SSDS has assumed
(i) that a user has a singular, fully constrained,
fixed goal and (ii) that there is one source of un-
209
User goal: good Italian but not Thai food User Goal: good Italian but not Thai food
POMDP SSDS (Example 1) ABC-SSDS (Example 2)
U1: What restaurants are nearby? U?1: What restaurants are nearby?
S1: There are 3 nearby. What type of food do you
want?
S?1: There are 3 nearby. What type of food do
you want?
U2: Not Italian, unless it?s really good U?2: Not Italian, unless it?s really good
S2: There are 2 Thai restaurants. S?2: Ok. Al Capone?s is highly rated. There are
?The compressed state (i.e. marginal likelihoods in Figure 1)
indicates that Italian food is dis-preferred.?
also 2 Thai restaurants nearby.
U3: No Thanks. Bye. U?3: Great. I?ll try Al Capone?s.
Table 1: State-of-the-art POMDP SSDS vs ABC-SSDS: Example dialogues 1 and 2. U=User, S=System
certainty; errors in speech recognition. The tradi-
tional role of the Dialogue Manager (DM) is there-
fore one of reducing uncertainty until its belief is
strongly focused on a single goal which strictly fits
the user?s stated constraints. Both the policies that
DMs currently follow and the design decisions im-
plemented to make SDSS tasks learnable use these
assumptions (Thomson and Young, 2010; Young
et al, 2010). The resulting systems force users
into selecting a single domain object as their goal
in order to progress the dialogue. The example
POMDP SSDS dialogues, Examples 1 and 3 (Ta-
bles 1 and 2), show the problems inherent in this
approach.
User goal: good Italian or good French food
S1: How can I help you?
U1: What restaurants are nearby?
S2: There are 6, what type of food do you
want?
U2: Err... Italian or French
S3: Do you want Italian or French food?
U3: Uh. either.
S4: Do you want Italian or French food?
U4: Ok , how about French then.
S5: There are 2 good French restaurants
nearby. What price range do you want?
U5: What about Italian?
S6: Did you say Italian?
U6: QUIT
Table 2: POMDP SSDS: Example 3
3 Independence Assumptions
Even considering limited domains, POMDP state
spaces grow very quickly. For example, consider
finding a user?s restaurant preference, which in-
volves getting 4 pieces of information, i.e. food-
type, city-area, price range, quality rating. Given 8
food types, 8 city areas, 3 price ranges and 3 qual-
ity ratings, coupled with 7 user actions and a 34 di-
alogue progress indicator1 then the dialogue state
space contains 8? 8? 3? 3? 7? 34 = 326, 592
states. A POMDP belief space is a probability
distribution over all these dialogue states, i.e. a
326, 592 dimensional real valued (R) space.
In order to render such large belief spaces
tractable, the current state of the art in POMDP
SSDS uses a variety of handcrafted compression
techniques, such as making several types of in-
dependence assumption. For example, by assum-
ing that users are only ever interested in one type
of food or one location, and that their interests
in food type, price range, quality, etc. are inde-
pendent, the 326, 592 real valued state space can
be reduced to a much smaller ?summary space?
(Williams and Young, 2005) consisting of, say,
4 ? R values2. See Figure 1 for a graphical de-
piction of such assumptions3.
As illustrated by Figure 1 the information lost
due to the independence assumptions mean that
these approaches are unable to support conversa-
tions such as that shown in Example 2 (Table 1).
4 Sets of User Goals
Getting rid of independence assumptions allows
the DM to reason and ask questions about the
user?s requirements in a more rational way. It can,
for example distinguish between the user want-
ing ?excellent Italian? or ?any Thai? versus only
?excellent? restaurants ? see Figure 1. However,
the resulting high dimensional real valued state
space can still only represent uncertainly over sin-
gular user goals (limited to single points in the
feature space, e.g. an excellent Italian restaurant).
1Whether each piece of information is obtained, con-
firmed or unknown.
2By considering only the maximum marginal likelihood
for each of the features.
3These apply after utterance U2/U?2 of Example 1.
210
ma
r
g
i
n
a
l
s
 
f
o
r
 
f
o
o
d
 
t
y
p
e
marginals for quality
 
 
 
 
 
 






 
 
 
 
 
 





italian
reasonable good excellent
Independence between
slots assumed
user?sgoal
thai
Figure 1: Assuming independence of features is
equivalent to marginalising across features. Here,
marginalisation incorrectly suppresses belief in
Italian. Thai retains a uniform belief (which ex-
ists across all restaurant types not yet mentioned).
To achieve a substantial gain in the flexibility of
SSDS we need to allow user?s goals that are sets
of points. Maintaining beliefs over ?sets of goals?
allows a POMDP DM to refine its belief in the
user?s requirements (managing speech recognition
errors) without forcing the user to specify a sin-
gular tightly constrained goal. The disadvantage
of this approach is a further expansion of the state
space.
5 Automatic Belief Compression
To allow for expansion of the state space, whilst
keeping its size tractable for policy learning, we
suggest replacing handcraft approaches with Au-
tomatic Belief Compression (ABC) techniques.
We propose to use proven, principled statisti-
cal learning methods for automatically reducing
the dimensionality of belief spaces, but which pre-
serve the useful distributions within the full space.
Two complementary methods that we are cur-
rently investigating are VDC (Poupart, 2005) and
E-PCA (Roy and Gordon, 2002; Roy et al, 2005).
These methods have been applied successfully in a
real-time daily living assistant with over 106 states
(St-Aubin et al, 2000; Hoey and Poupart, 2005;
Poupart et al, 2006) and to robotic navigation by
(Roy and Gordon, 2002; Roy et al, 2005). They:
? reduce the dimensionality of state spaces that
were previously intractable for POMDP solu-
tion methods, and
? automatically compress the representation of
belief space distributions to take advantage of
sparsity between likely distributions.
The tight coupling between some dia-
logue states and actions (e.g. a user?s goal
state travel-from-London and system act
confirm-from-London) has led some researchers
to conclude that compression techniques, such as
state aggregation, are not useful in the dialogue
domain (Williams and Young, 2007). However,
such tight coupling may not exist for all states,
indeed VDC has already been applied to a small
spoken dialogue system problem (Poupart, 2005)
where it was shown that compressions could be
found without losing any information4. Further,
for POMDP approaches the state is not the
dialogue state but the belief distribution over
dialogue states. Incompressibility at the dialogue
state level does not rule out compressibility of
belief distributions. Finally, our introduction
of sets for user goals should provide additional
possibilities for compression.
Our aim in applying ABC methods is to allow
POMDP SSDS to handle the much larger state
spaces that are required to achieve the expressive-
ness which we believe will be a real benefit to
users. We plan to do this for real world tasks, e.g. a
city search over 1000s of entities with an uncom-
pressed belief space of the order of 108 ? R.
6 Target Dialogues
In general, when a user starts a dialogue they
rarely have a singular goal in mind (Walker et al,
2004; Lemon et al, 2006). Their goal is not a fixed
point in the domain but instead can be thought
of as a (possibly disconnected) set of points, for
example either a nearby cheap restaurant or high-
quality one further away. The set represents trade
offs that the particular user is interested in. People
rarely communicate their goals in terms of such
distributions or trade offs, preferring to provide in-
formation in a piecemeal manner and thus incre-
mentally explore the domain.
In Examples 1?4 (Tables 1?3) we contrast the
operation of a current state-of-the-art POMDP
SSDS with our proposed ABC-SSDS system. The
user?s goal in Examples 3 and 4 (Tables 2 and 3)
is to explore what restaurants are nearby, with a
preference for French or Italian. Current POMDP
SSDS approaches assume that any spread of prob-
ability mass in the belief space represents uncer-
tainty which needs to be resolved. This gener-
ates problems for the POMDP SSDS in Example 3
since the user is forced into specifying one food
type at a time, resulting in an unwieldy confirma-
4Compressing a test problem of 433 states to 31 basis
functions, i.e. a summary space of 31 states.
211
User goal: good Italian or good French food
S?1: How can I help you?
U?1: What restaurants are nearby?
S?2: There are 6, what type of food do you
want?
U2?: Err... Italian or French
S?3: Ok, there are 2 good French restaurants
nearby, and one good Italian.
U?4: OK. Which is best quality?
S?3: Mamma Mia?s has the best rating.
U?5: Great. I?ll go there!
Table 3: Proposed ABC-SSDS: Example 4
tion step (S6 of Example 3) where the user is as-
sumed to have changed their mind. In contrast, the
proposed ABC-SSDS system can believe that the
user has requested information on the combined
set of French and Italian restaurants.
In Examples 1 and 2 (both shown in Table 1)
the user?s goal is to explore restaurants nearby, in-
cluding only well-rated Italians. Here the standard
POMDP SSDS is forced by its ?summary space?
(see marginals in Figure 1) to incorrectly represent
the user?s goal after U2 ?Not Italian, unless it?s
really good? by ruling out all Italian restaurants5.
The ABC-SSDS user is able to find the restaurant
of their choice, whereas the POMDP SSDS user?s
choice is artificially restricted, and they quit hav-
ing failed to find a suitable item.
The ABC-SSDS style of dialogue is clearly more
efficient than that of current POMDP SSDS. It
seems likely that users of such a system may also
find the style of the conversation more natural, and
may be more confident that their eventual choices
really meet their goals (Walker et al, 2004).
All of these hypotheses remain to be explored
in our future empirical work.
7 Conclusion
We present several problems for current POMDP
approaches to spoken dialogue systems, concern-
ing the representation of complex, but natural, user
goals. We propose the development of princi-
pled automatic methods for dimensionality reduc-
tion, in place of the ad-hoc assumptions and hand-
crafted compressions currently used.
In parallel we are also exploring: (i) what ap-
proaches are required for updating beliefs over
sets in real time ? in principle a method similar
5There are several ways to try to remedy this, but all have
problems.
to user goal state partitioning (Young et al, 2010)
would appear to be sufficient, (ii) what exploitable
bounds exist on the sets of goals that are commu-
nicable and (iii) to what extent the complexity of
user goal sets can be traded off against the overall
user experience.
Acknowledgments
Thanks to Dr. Jesse Hoey, the SIGdial reviewers
and the Engineering and Physical Sciences Re-
search Council (EPSRC) project EP/G069840/1.
References
J. Hoey and P. Poupart. 2005. Solving POMDPs with
Continuous or Large Discrete Observation Spaces.
In IJCAI.
O. Lemon, K. Georgila, and J. Henderson. 2006. Eval-
uating Effectiveness and Portability of Reinforce-
ment Learned Dialogue Strategies with real users:
the TALK TownInfo Evaluation. In IEEE/ACL Spo-
ken Language Technology.
P. Poupart, N. Vlassis, and J. Hoey. 2006. An An-
alytic Solution to Discrete Bayesian Reinforcement
Learning. In ICML.
P. Poupart. 2005. Exploiting Structure to Efficiently
Solve Large Scale Partially Observable Markov De-
cision Processes. Ph.D. thesis, Dept. Computer Sci-
ence, University of Toronto.
N. Roy and G. Gordon. 2002. Exponential Family
PCA for Belief Compression in POMDPs. In NIPS.
N. Roy, G. Gordon, and S. Thrun. 2005. Finding Ap-
proximate POMDP Solutions Through Belief Com-
pression. Artificial Intelligence Research, 22(1-40).
R. St-Aubin, J. Hoey, and C. Boutilier. 2000. Approx-
imate policy construction using decision diagrams.
In NIPS.
B. Thomson and S. Young. 2010. Bayesian update
of dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
M. Walker, S. Whittaker, A. Stent, P. Maloor, J. Moore,
M. Johnston, and G. Vasireddy. 2004. User tai-
lored generation in the match multimodal dialogue
system. Cognitive Science, 28:811?840.
J. Williams and S. Young. 2005. Scaling Up POMDPs
for Dialog Management: The ?Summary POMDP?
Method. In Proc. ASRU.
J. Williams and S. Young. 2007. Scaling POMDPs
for spoken dialog management. IEEE Transac-
tions on Audio, Speech, and Language Processing,
15(7):2116 ?2129, Sept.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2010. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language, 24(2):150?174.
212
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 2?7,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Spoken Dialog Challenge 2010: 
 Comparison of Live and Control Test Results 
Alan W Black1, Susanne Burger1, Alistair Conkie4, Helen Hastie2, Simon Keizer3,  Oliver 
Lemon2, Nicolas Merigaud2, Gabriel Parent1, Gabriel Schubiner1, Blaise Thomson3, Jason 
D. Williams4, Kai Yu3, Steve Young3 and Maxine Eskenazi1 
1Language Technologies Institute, Carnegie Mellon University, Pittsburgh, USA 
2Dept of Mathematical and Computer Science, Heriot-Watt University, Edinburgh, UK 
3Engineering Department, Cambridge University, Cambridge, UK 
4AT&T Labs ? Research, Florham Park, NJ, USA 
awb@cs.cmu.edu 
 
 
Abstract 
The Spoken Dialog Challenge 2010 was an 
exercise to investigate how different spo-
ken dialog systems perform on the same 
task.  The existing Let?s Go Pittsburgh Bus 
Information System was used as a task and 
four teams provided systems that were first 
tested in controlled conditions with speech 
researchers as users. The three most stable 
systems were then deployed to real callers.  
This paper presents the results of the live 
tests, and compares them with the control 
test results. Results show considerable 
variation both between systems and be-
tween the control and live tests.  Interest-
ingly, relatively high task completion for 
controlled tests did not always predict 
relatively high task completion for live 
tests.  Moreover, even though the systems 
were quite different in their designs, we 
saw very similar correlations between word 
error rate and task completion for all the 
systems.  The dialog data collected is 
available to the research community. 
1 Background 
The goal of the Spoken Dialog Challenge (SDC) is 
to investigate how different dialog systems per-
form on a similar task.  It is designed as a regularly 
recurring challenge. The first one took place in 
2010. SDC participants were to provide one or 
more of three things: a system; a simulated user, 
and/or an evaluation metric.   The task chosen for 
the first SDC was one that already had a large 
number of real callers. This had several advan-
tages. First, there was a system that had been used 
by many callers. Second, there was a substantial 
dataset that participants could use to train their sys-
tems.  Finally, there were real callers, rather than 
only lab testers.  Past work has found systems 
which appear to perform well in lab tests do not 
always perform well when deployed to real callers, 
in part because real callers behave differently than 
lab testers, and usage conditions can be considera-
bly different [Raux et al2005, Ai et al2008].  De-
ploying systems to real users is an important trait 
of the Spoken Dialog Challenge. 
The CMU Let?s Go Bus Information system 
[Raux et al2006] provides bus schedule informa-
tion for the general population of Pittsburgh.  It is 
directly connected to the local Port Authority, 
whose evening calls for bus information are redi-
rected to the automated system.  The system has 
been running since March 2005 and has served 
over 130K calls. 
The software and the previous years of dialog 
data were released to participants of the challenge 
to allow them to construct their own systems.  A 
number of sites started the challenge, and four sites 
successfully built systems, including the original 
CMU system. 
An important aspect of the challenge is that 
the quality of service to the end users (people in 
Pittsburgh) had to be maintained and thus an initial 
robustness and quality test was carried out on con-
tributed systems.  This control test provided sce-
narios over a web interface and required 
researchers from the participating sites to call each 
of the systems.  The results of this control test were 
published in [Black et al 2010] and by the individ-
ual participants [Williams et al 2010, Thomson et 
al. 2010, Hastie et al 2010] and they are repro-
2
duced below to give the reader a comparison with 
the later live tests. 
Important distinctions between the control 
test callers and the live test callers were that the 
control test callers were primarily spoken dialog 
researchers from around the world.  Although they 
were usually calling from more controlled acoustic 
conditions, most were not knowledgeable about 
Pittsburgh geography.     
As mentioned above, four systems took part 
in the SDC.  Following the practice of other chal-
lenges, we will not explicitly identify the sites 
where these systems were developed. We simply 
refer to them as SYS1-4 in the results.  We will, 
however, state that one of the systems is the system 
that has been running for this task for several 
years. The architectures of the systems cover a 
number of different techniques for building spoken 
dialog systems, including agenda based systems, 
VoiceXML and statistical techniques. 
2 Conditions of Control and Live tests 
For this task, the caller needs to provide the depar-
ture stop, the arrival stop and the time of departure 
or arrival in order for the system to be able to per-
form a lookup in the schedule database. The route 
number can also be provided and used in the 
lookup, but it is not necessary. The present live 
system covers the East End of Pittsburgh.  Al-
though the Port Authority message states that other 
areas are not covered, callers may still ask for 
routes that are not in the East End; in this case, the 
live system must say it doesn?t have information 
available.  Some events that affect the length of the 
dialog include whether the system uses implicit or 
explicit confirmation or some combination of both, 
whether the system has an open-ended first turn or 
a directed one, and whether it deals with requests 
for the previous and/or following bus (this latter 
should have been present in all of the systems). 
Just before the SDC started, the Port Author-
ity had removed some of its bus routes. The sys-
tems were required to be capable of informing the 
caller that the route had been canceled, and then 
giving them a suitable alternative. 
SDC systems answer live calls when the Port 
Authority call center is closed in the evening and 
early morning.  There are quite different types and 
volumes of calls over the different days of the 
week.  Weekend days typically have more calls, in 
part because the call center is open fewer hours on 
weekends.  Figure 1 shows a histogram of average 
calls per hour for the evening and the early morn-
ing of each day of the week. 
 
calls per weekday / ave per hour
0
1
2
3
4
5
6
7
8
9
10
Fr-
19-
0
Sa-
0-8
Sa-
16
-
0
Su-
0-8
Su-
16
-
0
Mo
-
0-7
Mo
-
19
-
0
Tu
-
0-7
Tu
-
19-
0
We
-
0-7
We
-
19
-
0
Th
-
0-7
Th
-
19-
0
Fr-
0-7
 
Figure 1: average number of calls per hour on weekends 
(dark bars) and weekdays. Listed are names of days and 
times before and after midnight when callers called the 
system. 
 
The control tests were set up through a simple 
web interface that presented 8 different scenarios 
to callers. Callers were given a phone number to 
call; each caller spoke to each of the 4 different 
systems twice.  A typical scenario was presented 
with few words, mainly relying on graphics in or-
der to avoid influencing the caller?s choice of vo-
cabulary.  An example is shown in Figure 2. 
 
 
 
Figure 2: Typical scenario for the control tests.  This 
example requests that the user find a bus from the cor-
ner of Forbes and Morewood (near CMU) to the airport, 
using bus route 28X, arriving by 10:45 AM. 
 
3
3 Control Test Results 
The logs from the four systems were labeled for 
task success by hand.  A call is successful if any of 
the following outputs are correctly issued: 
 
? Bus schedule for the requested departure and 
arrival stops for the stated bus number (if giv-
en). 
? A statement that there is no bus available for 
that route. 
? A statement that there is no scheduled bus at 
that time. 
 
We additionally allowed the following boundary 
cases: 
 
? A departure/arrival stop within 15 minutes 
walk. 
? Departure/arrival times within one hour of re-
quested time. 
? An alternate bus number that serves the re-
quested route. 
 
In the control tests, SYS2 had system connection 
issues that caused a number of calls to fail to con-
nect, as well as a poorer task completion.  It was 
not included in the live tests.  It should be pointed 
out that SYS2 was developed by a single graduate 
student as a class project while the other systems 
were developed by teams of researchers.  The re-
sults of the Control Tests are shown in Table 1 and 
are discussed further below. 
 
Table 1. Results of hand analysis of the four systems in 
the control test 
 The three major classes of system response 
are as follows.  no_info: this occurs when the sys-
tem gives neither a specific time nor a valid excuse 
(bus not covered, or none at that time).  no_info 
calls can be treated as errors (even though there 
maybe be valid reasons such as the caller hangs up 
because the bus they are waiting for arrives).  
donthave: identifies calls that state the requested 
bus is not covered by the system or that there is no 
bus at the requested time. pos_out: identifies calls 
where a specific time schedule is given.  Both 
donthave and pos_out calls may be correct or er-
roneous (e.g the given information is not for the 
requested bus,  the departure stop is wrong, etc). 
4 Live Tests Results 
In the live tests the actual Pittsburgh callers had 
access to three systems: SYS1, SYS3, and SYS4.  
Although engineering issues may not always be 
seen to be as relevant as scientific results, it is im-
portant to acknowledge several issues that had to 
be overcome in order to run the live tests. 
Since the Pittsburgh Bus Information System 
is a real system, it is regularly updated with new 
schedules from the Port Authority. This happens 
about every three months and sometimes includes 
changes in bus routes as well as times and stops. 
The SDC participants were given these updates 
and were allowed the time to make the changes to 
their systems. Making things more difficult is the 
fact that the Port Authority often only releases the 
schedules a few days ahead of the change. Another 
concern was that the live tests be run within one 
schedule period so that the change in schedule 
would not affect the results.   
The second engineering issue concerned 
telephony connectivity. There had to be a way to 
transfer calls from the Port Authority to the par-
ticipating systems (that were run at the participat-
ing sites, not at CMU) without slowing down or 
perturbing service to the callers.  This was 
achieved by an elaborate set of call-forwarding 
mechanisms that performed very reliably.  How-
ever, since one system was in Europe, connections 
to it were sometimes not as reliable as to the US-
based systems.  
 
 SYS1 SYS3 SYS4 
Total Calls 678 451 742 
Non-empty calls 633 430 670 
no_ info 18.5% 14.0% 11.0% 
donthave 26.4% 30.0% 17.6% 
donthave_corr 47.3% 40.3% 37.3% 
donthave_incorr 52.7% 59.7% 62.7% 
pos_out 55.1% 56.0% 71.3% 
pos_out_corr 86.8% 93.8% 91.6% 
pos_out_incorr 13.2% 6.2% 8.4% 
 
Table 2. Results of hand analysis of the three systems in 
the live tests.  Row labels are the same as in Table 1. 
 SYS1 SYS2 SYS3 SYS4 
Total Calls 91 61 75 83 
no_ info 3.3% 37.7% 1.3% 9.6% 
donthave 17.6% 24.6% 14.7% 9.6% 
donthave_corr 68.8% 33.3% 100.0% 100.0% 
donthave_incorr 31.3% 66.7% 0.0% 0.0% 
pos_out 79.1% 37.7% 84.0% 80.7% 
pos_out_corr 66.7% 78.3% 88.9% 80.6% 
pos_out_incorr 33.3% 21.7% 11.1% 19.4% 
4
We ran each of the three systems for multiple two 
day periods over July and August 2010.  This de-
sign gave each system an equal distribution of 
weekdays and weekends, and also ensured that 
repeat-callers within the same day experienced the 
same system. 
One of the participating systems (SYS4) 
could support simultaneous calls, but the other two 
could not and the caller would receive a busy sig-
nal if the system was already in use.  This, how-
ever, did not happen very often. 
Results of hand analysis of real calls are 
shown in Table 4 alongside the results for the Con-
trol Test for easy comparison.  In the live tests we 
had an additional category of call types ? empty 
calls (0-turn calls) ? which are calls where there 
are no user turns, for example because the caller 
hung up or was disconnected before saying any-
thing.  Each system had 14 days of calls and exter-
nal daily factors may change the number of calls. 
We do suspect that telephony issues may have pre-
vented some calls from getting through to SYS3 on 
some occasions.   
Table 3 provides call duration information for 
each of the systems in both the control and live 
tests. 
 
 
 Length (s) Turns/call Words/turn 
SYS1 control 155 18.29 2.87 (2.84) 
SYS1 live 111 16.24 2.15 (1.03) 
SYS2 control 147 17.57 1.63 (1.62) 
SYS3 control 96 10.28 2.73 (1.94) 
SYS3 live 80 9.56 2.22 (1.14) 
SYS4 control 154 14.70 2.25 (1.78) 
SYS4 live 126 11.00 1.63 (0.77) 
 
Table 3: For live tests, average length of each call, aver-
age number of turns per call, and average number of 
words per turn (numbers in brackets are standard devia-
tions). 
 
Each of the systems used a different speech 
recognizer.  In order to understand the impact of 
word error rate on the results, all the data were 
hand transcribed to provide orthographic transcrip-
tions of each user turn.   Summary word error sta-
tistics are shown in Table 4.   However, summary 
statistics do not show the correlation between word 
error rate and dialogue success.  To achieve this, 
following Thomson et al(2010), we computed a 
logistic regression of success against word error 
rate (WER) for each of the systems. Figure 3 
shows the regressions for the Control Tests and 
Figure 4 for the Live Tests.  
 
 SYS1 SYS3 SYS4 
Control 38.4 27.9 27.5 
Live 43.8 42.5 35.7 
 
Table 4: Average dialogue word error rate (WER). 
 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
es
s 
Ra
te
Sys4
Sys3
Sys1
 
Figure 3: Logistic regression of control test success vs 
WER for the three fully tested systems 
0 20 40 60 80 100
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
WER
Su
cc
e
ss
Sys1
Sys3
Sys4
 
Figure 4: Logistic regression of live success vs WER for 
the three fully tested systems 
 
5
In order to compare the control and live tests, 
we can calculate task completion as the percentage 
of calls that gave a correct result.  We include only 
non-empty calls (excluding 0-turn calls), and treat 
all no_info calls as being incorrect, even though 
some may be due to extraneous reasons such as the 
bus turning up (Table 5). 
 
 SYS1 SYS3 SYS4 
Control 64.9% (5.0%) 89.4% (3.6%) 74.6% (4.8%) 
Live 60.3% (1.9%) 64.6% (2.3%) 71.9% (1.7%) 
 
Table 5: Live and control test task completion (std. err).  
 
5 Discussion 
All systems had lower WER and higher task com-
pletion in the controlled test vs. the live test.  This 
agrees with past work [Raux et al2005, Ai et al
2008], and underscores the challenges of deploying 
real-world systems. 
For all systems, dialogs with controlled sub-
jects were longer than with live callers ? both in 
terms of length and number of turns.  In addition, 
for all systems, live callers used shorter utterances 
than controlled subjects.  Controlled subjects may 
be more patient than live callers, or perhaps live 
callers were more likely to abandon calls in the 
face of higher recognition error rates.   
Some interesting differences between the sys-
tems are evident in the live tests.  Looking at dia-
log durations, SYS3 used confirmations least often, 
and yielded the fastest dialogs (80s/call).  SYS1 
made extensive use of confirmations, yielding the 
most turns of any system and slightly longer dia-
logs (111s/call).  SYS4 was the most system-
directed, always collecting information one ele-
ment at a time.  As a result it was the slowest of the 
systems (126s/call), but because it often used im-
plicit confirmation instead of explicit confirmation, 
it had fewer turns/call than SYS1.   
For task completion, SYS3 performed best in 
the controlled trials, with SYS1 worst and SYS4 in 
between.  However in the live test, SYS4 per-
formed best, with SYS3 and SYS1 similar and 
worse.  It was surprising that task completion for 
SYS3 was the highest for the controlled tests yet 
among the lowest for the live tests.  Investigating 
this, we found that much of the variability in task 
completion for the live tests appears to be due to 
WER.  In the control tests SYS3 and SYS4 had 
similar error rates but the success rate of SYS3 was 
higher.  The regression in Figure 3 shows this 
clearly.   In the live tests SYS3 had a significantly 
higher word error rate and average success rate 
was much lower than in SYS4.   
It is interesting to speculate on why the rec-
ognition rates for SYS3 and SYS4 were different 
in the live tests, but were comparable in the control 
tests.  In a spoken dialogue system the architecture 
has a considerable impact on the measured word 
error rate.  Not only will the language model and 
use of dialogue context be different, but the dia-
logue design and form of system prompts will in-
fluence the form and content of user inputs.   Thus, 
word error rates do not just depend on the quality 
of the acoustic models ? they depend on the whole 
system design.  As noted above, SYS4 was more 
system-directed than SYS3 and this probably con-
tributed to the comparatively better ASR perform-
ance with live users.   In the control tests, the 
behavior of users (research lab workers) may have 
been less dependent on the manner in which users 
were prompted for information by the system.  
Overall, of course, it is user satisfaction and task 
success which matter. 
6 Corpus Availability and Evaluation 
The SDC2010 database of all logs from all systems 
including audio plus hand transcribed utterances, 
and hand defined success values is released 
through CMU?s Dialog Research Center 
(http://dialrc.org). 
One of the core goals of the Spoken Dialog 
Challenge is to not only create an opportunity for 
researchers to test their systems on a common plat-
form with real users, but also create common data 
sets for testing evaluation metrics.  Although some 
work has been done on this for the control test data 
(e.g. [Zhu et al2010]), we expect further evalua-
tion techniques will be applied to these data. 
One particular issue which arose during this 
evaluation concerned the difficulty of defining pre-
cisely what constitutes task success.  A precise de-
finition is important to developers, especially if 
reinforcement style learning is being used to opti-
mize the success.  In an information seeking task 
of the type described here, task success is straight-
forward when the user?s requirements can be satis-
fied but more difficult if some form of constraint 
relaxation is required.   For example, if the user 
6
asks if there is a bus from the current location to 
the airport ? the answer ?No.? may be strictly cor-
rect but not necessarily helpful.  Should this dia-
logue be scored as successful or not?  The answer 
?No, but there is a stop two blocks away where 
you can take the number 28X bus direct to the air-
port.? is clearly more useful to the user.  Should 
success therefore be a numeric measure rather than 
a binary decision?  And if a measure, how can it be 
precisely defined?  A second and related issue is 
the need for evaluation algorithms which deter-
mine task success automatically.   Without these, 
system optimization will remain an art rather than 
a science. 
7 Conclusions 
This paper has described the first attempt at an ex-
ercise to investigate how different spoken dialog 
systems perform on the same task.  The existing 
Let?s Go Pittsburgh Bus Information System was 
used as a task and four teams provided systems 
that were first tested in controlled conditions with 
speech researchers as users. The three most stable 
systems were then deployed ?live? with real call-
ers. Results show considerable variation both be-
tween systems and between the control and live 
tests.  Interestingly, relatively high task completion 
for controlled tests did not always predict rela-
tively high task completion for live tests.  This 
confirms the importance of testing on live callers, 
not just usability subjects. 
 The general organization and framework 
of the evaluation worked well.  The ability to route 
audio telephone calls to anywhere in the world us-
ing voice over IP protocols was critical to the suc-
cess of the challenge since it provides a way for 
individual research labs to test their in-house sys-
tems without the need to port them to a central co-
ordinating site. 
 Finally, the critical role of precise evalua-
tion metrics was noted and the need for automatic 
tools to compute them.  Developers need these at 
an early stage in the cycle to ensure that when sys-
tems are subsequently evaluated, the results and 
system behaviors can be properly compared.  
Acknowledgments 
Thanks to AT&T Research for providing telephony 
support for transporting telephone calls during the 
live tests.  This work was in part supported by the 
US National Science foundation under the project 
?Dialogue Research Center?.   
References  
Ai, H., Raux, A., Bohus, D., Eskenzai, M., and Litman, 
D.  (2008)  ?Comparing spoken dialog corpora col-
lected with recruited subjects versus real users?, Proc 
SIGDial, Columbus, Ohio, USA.  
Black, A., Burger, S., Langner, B., Parent, G., and Es-
kenazi, M. (2010) ?Spoken Dialog Challenge 2010?, 
SLT 2010, Berkeley, CA.  
Hastie, H., Merigaud, N., Liu, X and Oliver Lemon. 
(2010) ? ?Let?s Go Dude?, Using The Spoken Dia-
logue Challenge to Teach Spoken Dialogue Devel-
opment?, SLT 2010, Berkeley, CA. 
Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi, 
M.  (2005)  ?Let?s go public! Taking a spoken dialog 
system to the real world?, Interspeech 2005, Lisbon, 
Portugal. 
Raux, A., Bohus, D., Langner, B., Black, A., and Eske-
nazi, M. (2006) ?Doing Research on a Deployed 
Spoken Dialogue System: One Year of Let's Go! Ex-
perience?, Interspeech 2006 - ICSLP, Pittsburgh, PA.  
Thomson B., Yu, K. Keizer, S., Gasic, M., Jurcicek, F.,  
Mairesse, F. and Young, S. ?Bayesian Dialogue Sys-
tem for the Let?s Go Spoken Dialogue Challenge?, 
SLT 2010, Berkeley, CA. 
Williams, J., Arizmendi, I., and Conkie, A. ?Demonstra-
tion of AT&T ?Let?s Go?: A Production-Grade Statis-
tical Spoken Dialog System.? SLT 2010, Berkeley, 
CA. 
Zhu, Y., Yang, Z., Meng, H., Li, B., Levow, G., and 
King, I. (2010) ?Using Finite State Machines for 
Evaluating Spoken Dialog Systems?, SLT 2010, 
Berkeley, CA. 
7
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 142?151,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
?The day after the day after tomorrow?? A machine learning approach to
adaptive temporal expression generation:
training and evaluation with real users
Srinivasan Janarthanam, Helen Hastie, Oliver Lemon, Xingkun Liu
Interaction Lab
School of Mathematical and Computer Sciences (MACS)
Heriot-Watt University
{sc445, h.hastie, o.lemon, x.liu}@hw.ac.uk
Abstract
Generating Temporal Expressions (TE) that
are easy to understand, unambiguous, and rea-
sonably short is a challenge for humans and
Spoken Dialogue Systems. Rather than devel-
oping hand-written decision rules, we adopt a
data-driven approach by collecting user feed-
back on a variety of possible TEs in terms
of task success, ambiguity, and user prefer-
ence. The data collected in this work is freely
available to the research community. These
data were then used to train a simulated user
and a reinforcement learning policy that learns
an adaptive Temporal Expression generation
strategy for a variety of contexts. We evalu-
ate our learned policy both in simulation and
with real users and show that this data-driven
adaptive policy is a significant improvement
over a rule-based adaptive policy, leading to
a 24% increase in perceived task completion,
while showing a small increase in actual task
completion, and a 16% decrease in call dura-
tion. This means that dialogues are more ef-
ficient and that users are also more confident
about the appointment that they have agreed
with the system.
1 Introduction
Temporal Expressions are linguistic expressions that
are used to refer to a date and are often a source of
confusion in human-human, human-computer and
text interactions such as emails and instant messag-
ing. For example, ?Let?s meet next Sunday?? ?do
you mean Sunday this week or a week on Sunday??.
(Mccoy and Strube, 1999) state that changes in tem-
poral structure in text are often indicated by either
cue words and phrases (e.g. ?next Thursday?, ?this
week?, ?tomorrow?), a change in grammatical time
of the verb (e.g. present tense versus future tense),
or changes in aspect (e.g. atomic versus extended
events versus states as defined by (Moens and Steed-
man, 1988)). In this study, we will concentrate on
the first of these phenomena, generating TEs with
the optimal content and lexical choice.
Much work in the field of Natural Language Pro-
cessing concerns understanding and resolving these
temporal expressions in text (Gerber et al, 2002;
Pustejovsky et al, 2003; Ahn et al, 2007; Mazur
and Dale, 2007; Han et al, 2006), however, little
work has looked at how best to plan and realise tem-
poral expressions in order to minimize ambiguity
and confusion in a Spoken Dialogue System (SDS).
(Reiter et al, 2005) presented a data driven ap-
proach to generating TEs to refer to time in weather
forecast information where appropriate expressions
were identified using contextual features using su-
pervised learning. We adopt an adaptive, data-driven
reinforcement learning approach instead. Similar
data-driven approaches have been applied to infor-
mation presentation (Rieser et al, 2010; Walker et
al., 2007) where each Natural Language Generation
(NLG) action is a sequential decision point, based on
the current dialogue context and expected long-term
reward of that action. A data-driven approach has
also been applied to the problem of referring expres-
sion generation in dialogue for expert and novice-
users of a SDS (Janarthanam and Lemon, 2010).
However, to date, there has been no previous work
on adaptive data-driven approaches for temporal re-
ferring expression generation, where uncertainty in
142
the stochastic environment is explicitly modelled.
The data-driven approach to temporal expression
generation presented here is in the context of ap-
pointment scheduling dialogues. The fact that there
are multiple ways that a time slot can be referred to
leads to an interesting NLG problem of how best to
realise a TE for a particular individual in a particular
context for certain domains. For example, the fol-
lowing expressions all vary in terms of length, ambi-
guity, redundant information and users? preference:
?next Friday afternoon? or ?Friday next week at the
same time?, or ?in the afternoon, a week on Friday?.
Temporal Expressions contain two types of refer-
ences: absolute references such as ?Tuesday? and
?12th January?, and relative references such as ?to-
morrow? and ?this Tuesday?. Generating TEs there-
fore, involves both in selecting appropriate pieces of
information (date, day, time, month, and week) to
present and deciding how to present them (absolute
or relative reference).
Our objective here is to convey a target appoint-
ment slot to users using an expression that is optimal
in terms of the trade-off between understandability,
length and user preference.
2 Methodology
We address the issue of generating TEs by adopting
a data-driven approach that has four stages. Firstly,
we define Temporal Expression Units (TEU) as de-
scribed in Section 2.1. Secondly, we design and im-
plement a web-based data collection, gathering met-
rics on the TEUs in various contexts for a variety
of date types (Section 3). Thirdly, we train a user
simulation and use it to learn a policy using rein-
forcement learning techniques that generates the op-
timal combination of TEUs for each context (Sec-
tion 4). Finally, we deploy and evaluate this pol-
icy in a Spoken Dialogue System for appointment
scheduling and show that our learned policy per-
forms better than a hand-written, adaptive one (re-
sults presented in Section 5).
2.1 Temporal Expression Units
For this study, TEs are broken down into 5 cate-
gories or units (TEUs) presented in a fixed order:
DAY, DATE, MONTH, WEEK and TIME. Each of
these units can be expressed relative to the current
TEU Choices
DAY abs, rel, rc, nn
DATE abs, nn
MONTH abs, nn
WEEK abs, rel, nn
TIME abs, rc
Table 1: TEU choices where abs is absolute, rel is rela-
tive, rc is relative to context and nn is none
day and to the current context (i.e. previously men-
tioned dates). Specifically, there are 3 unit attributes:
absolute (e.g. DAY=abs ?Tuesday?); relative to cur-
rent day (e.g. DAY=rel ?tomorrow?); and relative to
context (e.g. DAY=rc ?the following day?).
Certain restrictions on possible TEU combina-
tions were imposed, for example, DATE=rc and
DAY=rel were combined to be just DAY=rel, and
some combinations were omitted on the basis that
it is highly unlikely that they would be uttered
in natural speech, for example WEEK=rel and
MONTH=abs would result in ?this week in Septem-
ber?. Finally, every TE has to contain a time (am or
pm for this application). The possible combinations
are summarised in Table 1.
3 Data Collection
The data collection experiment was in two parts
(Task 1 and Task 2) and was designed using the We-
bexp experimental software1. Webexp is a client-
server set up where a server application hosts the ex-
periment and stores the experimental files, logs and
results. The client side runs an applet on the user?s
web-browser.
In Task 1, participants listened to an audio file
containing a TE generated from absolute and rela-
tive TEUs (see Figure 1). No relative-context (rc)
TEUs were used in Task 1 since the dialogue ex-
cerpt presented was in isolation and therefore had
no context. Each participant was asked to listen to
10 different audio files in a sequence corresponding
to a variety of dates randomly chosen from 8 pos-
sible dates. The participant then had to identify the
correct appointment slot that the system is referring
to. There is scope for the participant to add multi-
ple answers in order to capture potential ambiguity
1http://www.webexp.info
143
Figure 1: Screen shot of Task 1 in the on-line data collection experiment
of a TE, and we report on this below. The 8 dates
that were used to generate the TEs fell into a two
week period in a single month which is in-line with
the evaluation set-up of the appointment scheduling
SDS discussed in Section 5.3.
For each date, the TE was randomly picked from a
set of 30 possible combinations of TEUs. Each TEU
was generated by a rule-based realiser and synthe-
sized using the Baratinoo synthesizer (France Tele-
com, 2011). This realiser generates text from a can-
didate list for each TEU based on the given date.
For example, if the slot currently being discussed
is Tuesday 7th, the realiser would generate ?tomor-
row? for DAY=rel; if the date in discussion was
Wednesday 8th then DAY=rel would be realised as
?the day after tomorrow?. There was potential for
overlap of stimuli, as any given TE for any given
date may be assessed by more than one participant.
Task 2 of the experiment was in two stages. In the
first stage (Task 2A), the participants are given to-
day?s date and the following dialogue excerpt; Op-
erator: ?We need to send out an engineer to your
home. The first available appointment is . . .? (see
Figure 2). They are then asked to listen to 5 audio
files of the system saying different TEs for the same
date and asked to rate preference on a scale of 1-6
(where 1 is bad and 6 is great.) For the second stage
(Task 2B), the dialogue is as follows; Operator: ?so
you can?t do Wednesday 8th September in the morn-
ing.? and then the participants are asked to listen
to 5 more audio files that are generated TEs includ-
ing relative context such as ?how about Thursday at
the same time??. This two-stage process is then re-
peated 4 times for each participant.
Table 2 summarizes the metrics collected in the
different parts of the experiment. The metric Dis-
tance is calculated in terms of the number of slots
from the current date to the target date (TD). In-
stances were grouped into four distance groups: G1:
TD is 1-2 slots away; G2: TD is 3-6 slots away; G3:
TD is 7-11 slots away and G4: TD more than 11
slots away. P replay is calcuated by the total num-
ber of replays divided by the total number of plays
for that temporal expression, i.e. the probability that
the temporal expression played is requested to be re-
played. P ambiguous is calculated by the number of
times a given temporal expression is given more than
1 interpretation divided by the total number of times
that the same given referring expression is answered.
In total there were 73 participants for Task 1 and
144
Figure 2: Screen shot of Task 2 in the on-line data collection experiment
730 TE samples collected. Although Task 2 directly
followed on from Task 1, there was a significant
drop out rate as only 48 participants completed the
second task resulting in 1,920 TE samples. Partici-
pants who completed both tasks were rewarded by a
chance to win an Amazon voucher.
3.1 Data Analysis
Figure 3 shows various metrics with respect to TE
absoluteness and relativeness is the number of ab-
solute and relative TEUs respectively. These two
graphs represent the state space that the genera-
tion policy described in Section 4 is exploring, trad-
ing off between various features such as Length,
taskSuccess and userPref.
As we can see, there is a tendency for average
taskSuccess to increase as absoluteness increases
whereas, for relativeness the distribution is more
even. The TE with the greatest taskSuccess has an
absoluteness of 4 and zero relativeness: DATE=abs,
MONTH=abs, WEEK=abs, TIME=abs (e.g. ?11th
September, the week starting the 10th, between 8am
and 10am?) and the TE with the least taskSuccess
has an absoluteness of only 2, again with no rela-
tiveness: DATE=abs, TIME=abs, (e.g. ?8th between
8am and 10am?).
Average userPref stays level and then decreases
if absoluteness is 5. We infer from this that al-
though long utterances that are completely explicit
are more clear in terms of taskSuccess, they are not
necessarily preferred by users. This is likely due
to TE length increasing. On average, the inclusion
of one relative expression is preferred over none at
all or two. The most preferred TE has an abso-
luteness of 3 with a relativeness of 2: DAY=rel,
DATE=abs, MONTH=abs, WEEK=rel, TIME=abs
(e.g. ?Tomorrow the 7th of September, this week,
between 8am and 10am?).
145
Figure 3: Graph showing the trade-offs between various metrics with respect to absoluteness and relativeness (number
of absolute/relative TEUs) in terms of probabilities or normalised values.
Metric Description Task
P ambiguous Probability that the expres-
sion is ambiguous to the
user
1
taskSuccess Correct slot identified 1
P replay Probability of replay (mea-
sure of understandability)
1 & 2
Length Expression length in terms
of number of TEUs that
are non null divided by the
total number of possible
TEUs (5)
1 & 2
wordLength Expression length in words
normalised over max num
of words (15)
1 & 2
userPref Preference rating of audio
from 1-6
2
Distance Distance from target date
(TD) to current date in
terms of number of slots
1 & 2
Table 2: Metrics collected in various parts of the experi-
ment
The probability of ambiguity and replay does not
seem to be affected by absoluteness. The most am-
biguous TE has an absoluteness of 3 and zero rela-
tiveness: DAY=abs MONTH=abs TIME=abs, (e.g.
?Tuesday September between 8am and 10am?) in-
dicating that a date is needed for precision. The
TEs that the participants were most likely to replay
tended to be short e.g. ?Tomorrow at the same time?.
This may be due to the clarity of the speech synthe-
siser.
4 Learning a TE generation policy
Reinforcement learning is a machine learning ap-
proach based on trial and error learning, in which
a learning agent learns to map sequences of ?opti-
mal? actions to environment or task states (Sutton
and Barto, 1998). In this framework the problem
of generating temporal expressions is presented as
a Markov Decision Process. The goal of the learn-
ing agent is to learn to choose those actions that ob-
tain maximum expected reward in the long run. In
this section, we present the reinforcement learning
setup for learning temporal expression generation
policies.
4.1 Actions and States
In this learning setup, we focus only on generating
the formal specification and treat the set of TEU
choices as the sequential actions of the learning
agent. Table 1 presents the choices that are available
for each TEU.
The actions are taken based on two factors: the
146
distance (in terms of time slots: morning or after-
noon appointments) between (1) the current date
and the target slot and (2) the current date and the
slot in context. Based on the distance, the target
slot was classified to belong to one of the four dis-
tance groups (G1-G4). The slot in context repre-
sents whether there was any other slot already men-
tioned in the conversation so far, so that the system
has an option to use ?relative context? expressions
to present day and time information. Information
concerning the target slot?s group and the slot in con-
text make up the state space of the Markov Decision
Process (MDP).
4.2 User Simulation
We built a user simulation to simulate the dialogue
behaviour of a user in appointment scheduling con-
versations based on the data from real users de-
scribed in Section 3. It responds to the TE used
by the system to refer to an appointment slot. It
responds by either accepting, rejecting, or clarify-
ing the offered slot based on the user?s own calen-
dar of available slots. For instance, the simulated
user rejects an offered slot if the user is not avail-
able at that time. If they accept or reject an offered
slot, the user is assumed to understand the TE unam-
biguously. However, if the user is unable to resolve
the appointment slot from the TE, it responds with a
clarification request. The simulation responded with
a dialogue action (Au,t) to TEs based on the sys-
tem?s dialogue act (As,t), system?s TE (TEs,t). The
following probabilistic model was used to generate
user dialogue actions:
P (Au,t|As,t, TEs,t, G,C,Cal)
In addition to TEs,t and As,t, other factors such as
distance between the target slot and the current slot
(G), the previous slot in context (C), and the user?s
calendar (Cal) were also taken into account. G is ei-
ther G1, G2, G3 or G4 as explained in Section 3. The
User?s dialogue action (Au,t) is one of the three: Ac-
cept slot, Reject slot or Request Clarification. The
probability of clarification request was calculated as
the average of the ambiguity and replay probabilities
seen in real user data.
4.3 Reward function
The learning agent was rewarded for each TE that it
generated. The reward given to the agent was based
on trade-offs between three variables: User prefer-
ence (UP), Length of the temporal expression (L),
and Clarification request probability (CR). UP for
each TE is obtained from Task 2 of the data collec-
tion. In the following reward function, UP is nor-
malised to be between 0 and 1. L is based on number
of TEUs used. The maximum number of TEUs that
can be used is 5 (i.e. DAY, DATE, WEEK, MONTH,
TIME). L is calculated as follows:
Length of TE (L) = No. of used TEUsMax. no. of TEUs
The clarification request (CR) is set to be 1 if the
user responds to the TE with a Request Clarification
and 0 otherwise. Reward is therefore calculated on
a turn-by-turn basis using the following formula:
Reward = UP ? 10.0 ? L ? 10.0 ? CR ? 10.0
In short, we chose a reward function that penalises
TEs that are long and ambiguous, and which rewards
TEs that users prefer. It also indirectly rewards task
success by penalising ambiguous TEs resulting in
clarification requests. This trade-off structure is evi-
dent from the data collection where TEs that are too
long are dispreferred by the users (see Figure 3). The
maximum possible reward is 6 (i.e. UP=1, CR=0,
L=2/5) and the minimum is -20 (i.e. UP=0, CR=1,
L=1). Note that other reward functions could be ex-
plored in future work, for example maximising only
for user preference or length.
4.4 Training
We trained a TE generation policy using the above
user simulation model for 10,000 runs using the
SARSA reinforcement learning algorithm (Sutton
and Barto, 1998). During the training phase, the
learning agent generated and presented TEs to the
user simulation. When a dialogue begins, there is no
appointment slot in context (i.e. C = 0). However,
if the user rejects the first slot, the dialogue system
sets C to 1 and presents the next slot. This is again
reset at the beginning of the next dialogue. The
agent was rewarded at the end of every turn based
on the user?s response, length of the TE, and user
preference scores as shown above. It gradually ex-
plored all possible combinations of TEUs and identi-
fied those TEUs in different contexts that maximize
147
Figure 4: Learning curve
the long-term reward. Figure 4 shows the learning
curve of the agent.
Table 3 presents the TE generation policy learned
by the agent. As one can observe, it used a mini-
mum number of TEUs to avoid length penalties in
the reward. In all cases, MONTH and WEEK in-
formation have not been presented at all. For target
slots that were closest (in group G1) and the farthest
(in group G4), it used relative forms of day (e.g. ?to-
morrow?, ?next Tuesday?, etc.). This is probably
because users dispreferred day information for in-
between slots (e.g. ?the day after the day after to-
morrow?). Also, MONTH information may have
been considered to be irrelevant due to the fact that
the two week window over which the data has been
collected do not span over two different months.
5 Evaluation
In this section, we present the baseline policies that
were evaluated along with the learned policy. We
then present the results of evaluation.
Slots Specification learned
1-2 DAY=rel;DATE=abs;MONTH=nn;
> 11 WEEK=nn;TIME=abs
3-11 DAY=nn;DATE=abs;MONTH=nn;
WEEK=nn;TIME=abs
Table 3: Learned policy
5.1 Baseline policies
The following are the baseline TEG policies:
1. Absolute policy: always use absolute for-
mats for all TEUs (i.e. DAY=abs; DATE=abs;
MONTH=abs; WEEK=abs; TIME=abs)
2. Minimal policy: always use a minimal format
with only date, month and time information in
their absolute forms (i.e. DAY=nn; DATE=abs;
MONTH=abs; WEEK=nn; TIME=abs)
3. Random policy: select possible formats ran-
domly for each TEU.
148
TEG Policy Average reward
Learned -0.071* (?3.75)
Absolute -4.084 (?4.36)
Minimal -1.340 (?4.2)
Random -8.21 (?7.72)
Table 4: Evaluation with simulated users (* p < 0.05,
two-tailed independent samples t-test)
5.2 Results
We evaluated the learned policy and the three other
hand-coded baseline TE generation policies with our
user simulation model. Each policy generated 1,000
TEs in different states. Table 4 present the results
of evaluation with simulated users. On average, the
learned policy scores higher than all the baseline
policies and the differences between the average re-
ward of the learned policy and the other baselines
are statistically significant. This shows that target
slots can be presented using different TEs depending
on how far they are from the current date and such
adaptation can produce less ambiguous, shorter and
user preferred expressions.
5.3 Evaluation with real users
The policy was also integrated into an NLG com-
ponent of a deployed Appointment Scheduling spo-
ken dialogue system. Please note that this is differ-
ent from the web environment in which the training
data was collected. Our data-driven policy was acti-
vated when the system informs the user of an avail-
able time slot. This system was compared to the
exact same system but with a rule-based adaptive
baseline system. In the rule-based policy MONTH,
DATE and TIME were always absolute, DAY was
relative if the target date was less than three days
away (i.e. ?today, tomorrow, day after tomorrow?),
and WEEK was always relative (i.e. ?this week, next
week?). All 5 information units were included in the
realisation (e.g. ?Thursday the 15th July in the after-
noon, next week?) although the order was slightly
different (DAY-DATE-MONTH-TIME-WEEK).
In this domain, the user tries to make an appoint-
ment for an engineer to visit their home. Each user
is given a set of 2-week calendars which shows their
availability and the goal is to arrange an appoint-
ment when both they and the engineer are available.
There were 12 possible scenarios that were evenly
rotated across participants and systems. Each sce-
nario is categorised in terms of scheduling difficulty
(Hard/Medium/Easy). Scheduling difficulty is cal-
culated for User Difficulty (UD) and System Diffi-
culty (SD) separately to assess the system?s mixed
initiative ability. Scheduling difficulty is calculated
as the ordinal of the first session that is free for both
the User and the System. Hard scenarios are with an
ordinal of 3 or 4; Medium with an ordinal of 2, and
Easy with an ordinal of 1. There are 4 scenarios in
each of these difficulty categories for both the user
and system. To give an example, in Scenario 10,
the user can schedule an appointment on Wednes-
day afternoon but he/she also has one free session
on the previous Tuesday afternoon when the engi-
neer is busy therefore UD = 2. For the system, in
this scenario, the first free session it has is on the
Wednesday afternoon therefore SD=1. In this case,
the scenario is easier for the system than the user be-
cause the system could just offer the first session that
it has free.
605 dialogues were collected and analysed. The
system was evaluated by employees at France Tele-
com and students of partner universities who have
never used the appointment scheduling system be-
fore. After each scenario, participants were then
asked to fill out a questionnaire on perceived task
success and 5 user satisfaction questions on a 6-
point Likert Scale (Walker et al, 2000). Results
from the real user study are summarised in Table 5.
The data-driven policy showed significant improve-
ment in Perceived Task Success (+23.7%) although
no significant difference was observed between the
two systems in terms of Actual Task Success (Chi-
square test, df=1). Perceived Task Success is users?
perception of whether they completed the task suc-
cessfully or not. Overall user satisfaction (the aver-
age score of all the questions) was also significantly
higher (+5%)2. Dialogues with the learned policy
were significantly shorter with lower Call Duration
in terms of time (-15.7%)2 and fewer average words
per system turn (-23.93%)2. Figure 5 shows the
length results in time for systems of varying UD and
SD. We can see that the data-driven adaptive policy
consistently results in a shorter dialogue across all
levels of difficulty. In summary, these results show
that using a policy trained on the data collected here
149
Parameters Learned Baseline
TEG TEG
Actual Task Success 80.05% 78.57%
Perceived Task Success 74.86%* 60.50%
User satisfaction 4.51* 4.30
No. system turns 22.8 23.2
Words per system turn 13.16* 17.3
Call duration 88.60 sec * 105.11 sec
Table 5: Results with real users (* statistically significant
difference at p<0.05)
results in shorter dialogues and greater confidence
in the user that they have had a successful dialogue.
Although the learned policy was trained to generate
optimal TEs within a two week window and there-
fore is not general policy for all TE generation prob-
lems, we believe that the data-driven approach that
we have followed can generalise to other TE gener-
ation tasks.
Figure 5: Graph comparing length of dialogues for user
(UD) and system difficulty (SD)
6 Conclusion
We have presented a principled statistical learning
method for generating Temporal Expressions (TEs)
that refer to appointment slots in natural language
utterances. We presented a method for gathering
data on TEs with an on-line experiment and showed
how we can use these data to generate TEs us-
ing a Markov Decision Process which can be opti-
mised using reinforcement learning techniques. We
showed that a TEG policy learned using our frame-
2independent two-tailed t-test p < 0.05
work performs signifcantly better than hand-coded
adaptive policies with real users as well as with sim-
ulated users.
The data collected in this work has been freely
released to the research community in 20113.
Acknowledgements
The research leading to these results has received
funding from the EC?s 7th Framework Programme
(FP7/2007-2013) under grant agreement no. 216594
(CLASSiC project www.classic-project.
org), (FP7/2011-2014) under grant agreement no.
248765 (Help4Mood project), (FP7/2011-2014) un-
der grant agreement no. 270435 (JAMES project),
(FP7/2011-2014) under grant agreement no. 270019
(SpaceBook project), and from the EPSRC, project
no. EP/G069840/1. We would also like to thank our
CLASSiC project colleagues at Cambridge Univer-
sity and France Telecom / Orange Labs.
References
D. Ahn, J. van Rantwijk, and M. de Rijke. 2007. A
Cascaded Machine Learning Approach to Interpret-
ing Temporal Expressions. In Proceedings of NAACL-
HLT 2007.
France Telecom. 2011. Baratinoo expressive speech syn-
thesiser. http://tts.elibel.tm.fr.
L. Gerber, L. Ferro, I. Mani, B. Sundheim, G. Wilson,
and R. Kozierok. 2002. Annotating Temporal Infor-
mation: From Theory to Practice. In Proceedings of
HLT.
B. Han, D. Gates, and L. Levin. 2006. Understanding
temporal expressions in emails. In HLT-NAACL 2006.
Srinivasan Janarthanam and Oliver Lemon. 2010. Learn-
ing to adapt to unknown users: referring expression
generation in spoken dialogue systems. In ACL ?10.
P. Mazur and R. Dale. 2007. The DANTE Temporal Ex-
pression Tagger. In Proceedings of the 3rd Language
and Technology Conference, Poznan, Poland.
Kathleen F. Mccoy and Michael Strube. 1999. Taking
time to structure discourse: Pronoun generation be-
yond accessibility. In Proc. of the 21th Annual Con-
ference of the Cognitive Science Society.
M. Moens and M. Steedman. 1988. Temporal ontology
and temporal reference. In Computational Linguistics,
volume 14(2), pages 15?28.
3Sec 2.6 at http://www.macs.hw.ac.uk/ilabarchive/classicproject/data/
150
J. Pustejovsky, J. Castano, R. Ingria, R. Sauri,
R. Gaizauskas, A. Setzer, G. Katz, and D. Radev.
2003. TimeML: Robust specification of event and
temporal expressions in text. In AAAI Spring Sympo-
sium on New Directions in Question-Answering, Stan-
ford, CA.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005. Choos-
ing words in computer-generated weather forecasts.
Artificial Intelligence, 167:137169.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In Proc. ACL 2010.
R. Sutton and A. Barto. 1998. Reinforcement Learning.
MIT Press.
Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-
man. 2000. Towards Developing General Models of
Usability with PARADISE. Natural Language Engi-
neering, 6(3).
Marilyn Walker, Amanda Stent, Franc?ois Mairesse, and
Rashmi Prasad. 2007. Individual and domain adap-
tation in sentence planning for dialogue. Journal of
Artificial Intelligence Research (JAIR), 30:413?456.
151
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 49?58,
Utica, May 2012. c?2012 Association for Computational Linguistics
Optimising Incremental Generation for Spoken Dialogue Systems:
Reducing the Need for Fillers
Nina Dethlefs, Helen Hastie, Verena Rieser and Oliver Lemon
Heriot Watt University
EH14 4AS, Edinburgh
n.s.dethlefs | h.hastie | v.t.rieser | o.lemon@hw.ac.uk
Abstract
Recent studies have shown that incremental
systems are perceived as more reactive, nat-
ural, and easier to use than non-incremental
systems. However, previous work on incre-
mental NLG has not employed recent ad-
vances in statistical optimisation using ma-
chine learning. This paper combines the two
approaches, showing how the update, revoke
and purge operations typically used in in-
cremental approaches can be implemented as
state transitions in a Markov Decision Process.
We design a model of incremental NLG that
generates output based on micro-turn inter-
pretations of the user?s utterances and is able
to optimise its decisions using statistical ma-
chine learning. We present a proof-of-concept
study in the domain of Information Presen-
tation (IP), where a learning agent faces the
trade-off of whether to present information as
soon as it is available (for high reactiveness)
or else to wait until input ASR hypotheses are
more reliable. Results show that the agent
learns to avoid long waiting times, fillers and
self-corrections, by re-ordering content based
on its confidence.
1 Introduction
Traditionally, the smallest unit of speech processing
for interactive systems has been a full utterance with
strict, rigid turn-taking. Components of these inter-
active systems, including NLG systems, have so far
treated the utterance as the smallest processing unit
that triggers a module into action. More recently,
work on incremental systems has shown that pro-
cessing smaller ?chunks? of user input can improve
the user experience (Skantze and Schlangen, 2009;
Buss et al, 2010; Skantze and Hjalmarsson, 2010;
Baumann et al, 2011). Incrementality in NLG sys-
tems enables the system designer to model several
dialogue phenomena that play a vital role in hu-
man discourse (Levelt, 1989) but have so far been
absent from NLG systems. These include more
natural turn-taking through rapid system responses,
grounding through the generation of backchannels
and feedback, and barge-ins (from both user and sys-
tem). In addition, corrections and self-corrections
through constant monitoring of user and system ut-
terances play an important role, enabling the system
to recover smoothly from a recognition error or a
change in the user?s preferences. Some examples of
the phenomena we are targeting are given in Fig. 1.
Skantze and Hjalmarsson (2010) present a model
of incremental speech generation in which input pro-
cessing and output planning are parallel processes
and the system can self-monitor its own genera-
tion process. In an evaluation with human users
they showed that their incremental system started
to speak significantly faster than a non-incremental
system (roughly 600 ms) and was perceived as sig-
nificantly more polite and efficient. Users also in-
dicated that they knew better when to start speaking
themselves. Alternative approaches to incremental
NLG include Kilger and Finkler (1995) who present
an early approach based on Tree-Adjoining Gram-
mar, and Purver and Otsuka (2003) who define an
incremental generator based on Dynamic Syntax.
Both of these generators can monitor their own out-
put and initiate corrections if necessary.
Over recent years, adaptive and data-driven ap-
49
Self-correction (the system made a mistake)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 Indian restaurants . . .
USR No, I want Italian.
SYS oh sorry . . .
SYS I have 24 Italian restaurants in the city centre . . .
Correction (the user changed their mind)
USR I want Indian food in the centre of town . . .
SYS There are 35 Indian . . .
USR Oh sorry, I meant Italian.
SYS OK, Italian . . .
SYS I have 24 Italian restaurants . . .
Re-ordering (from high to low confidence)
USR I want Italian food . . .
SYS [waits]
USR in the city centre.
SYS I have 120 places in the city centre . . .
USR I love Italian . . .
SYS 24 of them Italian . . .
Holding the floor
USR I want cheap Italian food . . .
SYS ok let me see
SYS I have 3 cheap Italian places . . .
Figure 1: Example phenomena generated with the trained
policy. The agent has learnt to produce backchannels
when the user pauses, monitor and (self-)correct its out-
put, and present information according to its confidence.
proaches to NLG have also been developed and
shown to outperform the previous (handcrafted,
rule-based) methods for specific problems (Rieser et
al., 2010; Janarthanam and Lemon, 2010; Dethlefs
and Cuaya?huitl, 2011). This work has established
that NLG can fruitfully be treated as a data-driven
statistical planning process, where the objective is
to maximise expected utility of the generated utter-
ances (van Deemter, 2009), by adapting them to the
context and user. Statistical approaches to sentence
planning and surface realisation have also been ex-
plored (Stent et al, 2004; Belz, 2008; Mairesse et
al., 2010; Angeli et al, 2010). The advantages of
data-driven methods are that NLG is more robust in
the face of noise, can adapt to various contexts and,
trained on real data, can produce more natural and
desirable variation in system utterances.
This paper describes an initial investigation into a
novel NLG architecture that combines incremental
processing with statistical optimisation. In order to
move away from conventional strict-turn taking, we
have to be able to model the complex interactions
observed in human-human conversation. Doing this
in a deterministic fashion through hand-written rules
would be time consuming and potentially inaccu-
rate, with no guarantee of optimality. In this paper,
we demonstrate that it is possible to learn incremen-
tal generation behaviour in a reward-driven fashion.
2 Previous Work: Incremental Processing
Architectures
The smallest unit of processing in incremental sys-
tems is called incremental unit (IU). Its instantia-
tion depends on the particular processing module. In
speech recognition, IUs can correspond to phoneme
sequences that are mapped onto words (Baumann
and Schlangen, 2011). In dialogue management, IUs
can correspond to dialogue acts (Buss et al, 2010).
In speech synthesis, IUs can correspond to speech
unit sequences which are mapped to segments and
speech plans (Skantze and Hjalmarsson, 2010). IUs
are typically linked to other IUs by two types of rela-
tions: same-level links connect IUs sequentially and
express relationships at the same level; grounded-in
links express hierarchical relations between IUs.
2.1 Buffer-Based Incremental Processing
A general abstract model of incremental process-
ing based on buffers and a processor was devel-
oped by Schlangen and Skantze (2009) and is illus-
trated in Figure 2. It assumes that the left buffer
of a module, such as the NLG module, receives
IUs from one or more other processing modules,
such as the dialogue manager. These input IUs are
then passed on to the processor, where they are
mapped to corresponding (higher-level) IUs. For
an NLG module, this could be a mapping from the
dialogue act present(cuisine=Indian) to the realisa-
tion ?they serve Indian food?. The resulting IUs are
passed on to the right buffer which co-incides with
the left buffer of another module (for example the
speech synthesis module in our example). Same-
level links are indicated as dashed arrows in Figure
2 and grounded-in links as stacked boxes of IUs.
The figure also shows that the mapping between
IUs can be a one-to-many mapping (IU1 and IU2
are mapped to IU3) or a one-to-one mapping (IU3 is
50
IU1 IU2
IU1 IU2
IU3
IU3 IU3
IU4
IU4
Left buffer Processor Right buffer
Left buffer Processor Right buffer
Figure 2: The buffer-based model showing two connected
modules (from Skantze and Hjalmarsson (2010).
IU1
IU2 IU3 IU4 IU5
IU6 IU7 IU8 IU9 . . .
Figure 3: The ISU-model for incremental processing
(adapted from Buss and Schlangen (2011)).
mapped to IU4). The model distinguishes four op-
erations that handle information processing: update,
revise, purge and commit. Whenever new IUs en-
ter the module?s left buffer, the module?s knowledge
base is updated to reflect the new information. Such
information typically corresponds to the current best
hypothesis of a preceding processing module. As
a property of incremental systems, however, such
hypotheses can be revised by the respective preced-
ing module and, as a result, the knowledge bases of
all subsequent modules need to be purged and up-
dated to the newest hypothesis. Once a hypothesis
is certain to not be revised anymore, it is commit-
ted. For concrete implementations of this model, see
Skantze and Schlangen (2009), Skantze and Hjal-
marsson (2010), Baumann and Schlangen (2011).
An implementation of an incremental dialogue
manager is based on the Information State Update
(ISU) model (Buss et al, 2010; Buss and Schlangen,
2011). The model is related in spirit to the buffer-
based architecture, but all of its input processing and
output planning is realised by ISU rules. This is true
for the incremental ?house-keeping? actions update,
revise, etc. and all types of dialogue acts. The in-
cremental ISU model is shown in Figure 3. Note
that this hierarchical architecture transfers well to
the ?classical? division of NLG levels into utterance
(IU1), content selection (IU2 - IU5) and surface re-
alisations (IU6 - IU9, etc.).
2.2 Beat-Driven Incremental Processing
In contrast to the buffer-based architectures, alterna-
tive incremental systems do not reuse previous par-
tial hypotheses of the user?s input (or the system?s
best output) but recompute them at each process-
ing step. We follow Baumann et al (2011) in call-
ing them ?beat-driven? systems. Raux and Eskenazi
(2009) use a cost matrix and decision theoretic prin-
ciples to optimise turn-taking in a dialogue system
under the constraint that users prefer no gaps and no
overlap at turn boundaries. DeVault et al (2009) use
maximum entropy classification to support respon-
sive overlap in an incremental system by predicting
the completions of user utterances.
2.3 Decision-making in Incremental Systems
Some of the main advantages of the buffer- and ISU-
based approaches include their inherently incremen-
tal mechanisms for updating and revising system hy-
potheses. They are able to process input of varying
size and type and, at the same time, produce arbi-
trarily complex output which is monitored and can
be modified at any time. On the other hand, current
models are based on deterministic decision making
and thus share some of the same drawbacks that non-
incremental systems have faced: (1) they rely on
hand-written rules which are time-consuming and
expensive to produce, (2) they do not provide a
mechanism to deal with uncertainty introduced by
varying user behaviour, and (3) they are unable to
generalise and adapt flexibly to unseen situations.
For NLG in particular, we have seen that incre-
mentality can enhance the responsiveness of sys-
tems and facilitate turn-taking. However, this ad-
vantage was mainly gained by the system produc-
ing semantically empty fillers such as um, let me
see, well, etc. (Skantze and Hjalmarsson, 2010). It
is an open research question whether such markers
of planning or turn-holding can help NLG systems,
but for now it seems that they could be reduced to
a minimum by optimising the timing and order of
Information Presentation. In the following, we de-
velop a model for incremental NLG that is based on
reinforcement learning (RL). It learns the best mo-
ment to present information to the user, when faced
with the options of presenting information as soon
as it becomes available or else waiting until the in-
51
Type Example
Comparison The restaurant Roma is in the medium price range, but does not have great food. The Firenze
and Verona both have great food but are more expensive. The Verona has good service, too.
Recommendation Restaurant Verona has the best overall match with your query. It is a bit more expensive,
but has great food and service.
Summary I have 43 Italian restaurants in the city centre that match your query. 10 of them are in the
medium price range, 5 are cheap and 8 are expensive.
Table 1: Examples of IP as a comparison, recommendation and summary for a user looking for Italian restaurants in
the city centre that have a good price for value.
put hypotheses of the system are more stable. This
also addresses the general trade-off that exists in in-
cremental systems between the processing speed of
a system and the output quality.
3 Information Presentation Strategies
Our domain of application will be the Informa-
tion Presentation phase in an interactive system
for restaurant recommendations, extending previous
work by Rieser et al (2010), (see also Walker et
al. (2004) for an alternative approach). Rieser et
al. incrementally construct IP strategies according
to the predicted user reaction, whereas our approach
focuses on timing and re-ordering of information
according to dynamically changing input hypothe-
ses. We therefore implement a simplified version
of Rieser et al?s model. Their system distinguished
two steps: the selection of an IP strategy and the
selection of attributes to present to the user. We as-
sume here that the choice of attributes is determined
by matching the types specified in the user input,
so that our system only needs to choose a strategy
for presenting its results (in the future, though, we
will include attribute selection into the decision pro-
cess). Attributes include cuisine, food quality, lo-
cation, price range and service quality of a restau-
rant. The system then performs a database lookup
and chooses among three main IP strategies sum-
mary, comparison, recommendation and several or-
dered combinations of these. Please see Rieser et al
(2010) for details. Table 1 shows examples of the
main types of presentation strategies we address.
4 Optimising Incremental NLG
To optimise the NLG process within an incremen-
tal model of dialogue processing, we define an RL
agent with incremental states and actions for the IP
task. An RL agent is formalised as a Markov De-
cision Process, or MDP, which is characterised as a
four-tuple < S,A, T,R >, where S is a set of states
representing the status of the NLG system and all in-
formation available to it, A is a set of NLG actions
that combine strategies for IP with handling incre-
mental updates in the system, T is a probabilistic
transition function that determines the next state s?
from the current state s and the action a according
to a conditional probability distribution P (s?|s, a),
and R is a reward function that specifies the reward
(a numeric value) that an agent receives for taking
action a in state s. Using such an MDP, the NLG
process can be seen as a finite sequence of states,
actions and rewards {s0, a0, r1, s1, a1, ..., rt?1, st},
where t is the time step. Note that a learning episode
falls naturally into a number of time steps at each of
which the agent observes the current state of the en-
vironment st, takes an action at and makes a tran-
sition to state st+1. This organisation into discrete
time steps, and the notion of a state space that is ac-
cessible to the learning agent at any time allows us to
implement the state update, revoke and purge opera-
tions typically assumed by incremental approaches
as state updates and transitions in an MDP. Any
change in the environment, such as a new best hy-
pothesis of the recogniser, can thus be represented
as a transition from one state to another. At each
time step, the agent then takes the currently best ac-
tion according to the new state. The best action in
an incremental framework can include correcting a
previous output, holding the floor as a marker of
planning, or to wait until presenting information.1
1We treat these actions as part of NLG content selection
here, but are aware that in alternative approaches, they could
52
States
incrementalStatus {0=none,1=holdFloor,2=correct,3=selfCorrect}
presStrategy {0=unfilled,1=filled}
statusCuisine {0=unfilled,1=low,2=medium,3=high,4=realised}
statusFood {0=unfilled,1=low,2=medium,3=high,4=realised}
statusLocation {0=unfilled,1=low,2=medium,3=high,4=realised}
statusPrice {0=unfilled,1=low,2=medium,3=high,4=realised}
statusService {0=unfilled,1=low,2=medium,3=high,4=realised}
userReaction {0=none,1=select,2=askMore,3=other}
userSilence={0=false,1=true}
Actions
IP: compare, recommend, summarise, summariseCompare,
summariseRecommend, summariseCompareRecommend,
Slot-ordering: presentCuisine, presentFood, presentLocation,
presentPrice, presentService,
Incremental: backchannel, correct, selfCorrect, holdFloor,
waitMore
Goal State 0, 1, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 0 ? 4, 1, 0 ? 1
Figure 4: The state and action space of the learning agent.
The goal state is reached when all items (that the user may
be interested in) have been presented.
Once information has been presented to the user,
it is committed or realised. We again represent re-
alised IUs in the agent?s state representation, so that
it can monitor its own output. The goal of an MDP
is to find an optimal policy pi? according to which
the agent receives the maximal possible reward for
each visited state. We use the Q-Learning algorithm
(Watkins, 1989) to learn an optimal policy according
to pi?(s) = argmaxa?A Q?(s, a), where Q? speci-
fies the expected reward for executing action a in
state s and then following policy pi?.
5 Experimental Setting
5.1 The State and Action Space
The agent?s state space needs to contain all infor-
mation relevant for choosing an optimal IP strat-
egy and an optimal sequence of incremental ac-
tions. Figure 4 shows the state and action space
of our learning agent. The states contain infor-
mation on the incremental and presentation sta-
tus of the system. The variable ?incrementalSta-
tus? characterises situations in which a particular
(incremental) action is triggered. For example, a
holdFloor is generated when the user has finished
speaking, but the system has not yet finished its
database lookup. A correction is needed when
also be the responsibility of a dialogue manager.
the system has to modify already presented infor-
mation (because the user changed their preferences)
and a selfCorrection is needed when previously
presented information is modified because the sys-
tem made a mistake (in recognition or interpreta-
tion). The variable ?presStrategy? indicates whether
a strategy for IP has been chosen. It is ?filled? when
this is the case, and ?unfilled? otherwise. The vari-
ables representing the status of the cuisine, food, lo-
cation, price and service indicate whether the slot
is of interest to the user (0 means that the user does
not care about it), and what input confidence score is
currently associated with its value. Once slots have
been presented, they are realised and can only be
changed through a correction or self-correction.
The variable ?userReaction? shows the user?s re-
action to an IP episode. The user can select a restau-
rant, provide more information to further constrain
the search or do something else. The ?userSilence?
variable indicates whether the user is speaking or
not. This can be relevant for holding the floor or
generating backchannels. The action set comprises
IP actions, actions which enable us to learn the or-
dering of slots, and actions which allow us to cap-
ture incremental phenomena. The complete state-
action space size of this agent is roughly 3.2 mil-
lion. The agent reaches its goal state (defined w.r.t.
the state variables in Figure 4) when an IP strategy
has been chosen and all relevant attributes have been
presented.
5.2 The Simulated Environment
Since a learning agent typically needs several thou-
sand interactions to learn a reasonable policy, we
train it in a simulated environment with two compo-
nents. The first one deals with different IP strategies
generally (not just for the incremental case), and the
second one focuses on incrementally updated user
input hypothesis during the interaction.
To learn a good IP strategy, we use a user simula-
tion by Rieser et al (2010),2 which was estimated
from human data and uses bi-grams of the form
P (au,t|IPs,t), where au,t is the predicted user re-
action at time t to the system?s IP strategy IPs,t in
state s at time t. We distinguish the user reactions of
2The simulation data are available from http://www.
classic-project.org/.
53
select a restaurant, addMoreInfo to the current query
to constrain the search, and other. The last category
is considered an undesired user reaction that the sys-
tem should learn to avoid. The simulation uses lin-
ear smoothing to account for unseen situations. In
this way, we can then predict the most likely user
reaction to each system action.
While the IP strategies can be used for incremen-
tal and non-incremental NLG, the second part of the
simulation deals explicitly with the dynamic envi-
ronment updates during an interaction. We assume
that for each restaurant recommendation, the user
has the option of filling any or all of the attributes
cuisine, food quality, location, price range and ser-
vice quality. The possible values of each attribute
and possible confidence scores are shown in Table 2
and denote the same as described in Section 5.1.
At the beginning of a learning episode, we as-
sign each attribute a possible value and confidence
score with equal probability. For food and service
quality, we assume that the user is never interested
in bad food or service. Subsequently, confidence
scores can change at each time step. (In future work
these transition probabilities will be estimated from
a data collection, though the following assumptions
are realistic, based on our experience.) We assume
that a confidence score of 0 changes to any other
value with a likelihood of 0.05. A confidence score
of 1 changes with a probability of 0.3, a confidence
score of 2 with a probability of 0.1 and a confidence
score of 3 with a probability of 0.03. Once slots
have been realised, their value is set to 4. They
cannot be changed then without an explicit correc-
tion. We also assume that realised slots change with
a probability of 0.1. If they change, we assume
that half of the time, the user is the origin of the
change (because they changed their mind) and half
of the time the system is the origin of the change
(because of an ASR or interpretation error). Each
time a confidence score is changed, it has a proba-
bility of 0.5 to also change its value. The resulting
input to the NLG system are data structures of the
form present(cuisine=Indian), confidence=low.
5.3 The Reward Function
The main trade-off to optimise for IP in an incre-
mental setting is the timing and order of presenta-
tion. The agent has to decide whether to present
Attribute Values Confidence
Cuisine Chinese, French, German, In-, 0, 1, 2, 3, 4
dian, Italian, Japanese, Mexi-
can, Scottish, Spanish, Thai
Food bad, adequate, good, very good 0, 1, 2, 3, 4
Location 7 distinct areas of the city 0, 1, 2, 3, 4
Price cheap, expensive, good-price-
for-value, very expensive 0, 1, 2, 3, 4
Service bad, adequate, good, very good 0, 1, 2, 3, 4
Table 2: User goal slots for restaurant queries with possi-
ble values and confidence scores.
information as soon as it becomes available or else
wait until confidence for input hypotheses is more
stable. Alternatively, it can reorder information to
account for different confidence scores. We assign
the following rewards3: +100 if the user selects
an item, 0 if the user adds more search constraints,
?100 if the user does something else or the sys-
tem needs to self-correct,?0.5 for holding the floor,
and ?1 otherwise. In addition, the agent receives
an increasing negative reward for the waiting time,
waiting time2 (to the power of two), in terms of the
number of time steps passed since the last item was
presented. This reward is theoretically ??. The
agent is thus penalised stronger the longer it delays
IP. The rewards for user reactions are assigned at the
end of each episode, all other rewards are assigned
after each time step. One episode stretches from the
moment that a user specifies their initial preferences
to the moment in which they choose a restaurant.
The agent was trained for 10 thousand episodes.
6 Experimental Results
After training, the RL agent has learnt the following
incremental IP strategy. It will present information
slots as soon as they become available if they have
a medium or high confidence score. The agent will
then order attributes so that those slots with the high-
est confidence scores are presented first and slots
with lower confidence are presented later (by which
time they may have achieved higher confidence). If
no information is known with medium or high con-
3Handcrafted rewards are sufficient for this proof-of-
concept study, and can be learned from data for future models
(Rieser and Lemon, 2011).
54
101 102 103 104
?100
?80
?60
?40
?20
0
20
40
60
80
100
Av
er
ag
e 
Re
wa
rd
Episodes
 
 
RL
Base1
Base2
Base3
Figure 5: Performance in terms of rewards (averaged over
10 runs) for the RL agent and its baselines.
fidence, the agent will hold the floor or wait. In this
way, it can prevent self-corrections and minimise
waiting time?both of which yield negative rewards.
It can thus start speaking very early (avoiding long
pauses or semantically empty utterances) and still
has a low likelihood of having to self-correct.
For a comparison of the learnt policy with possi-
ble hand-crafted policies (because current incremen-
tal NLG systems are rule-based), we designed three
baselines. Baseline 1 always presents information
as soon as it is available, i.e. never waits. Base-
line 2 always waits until all information is known
with high confidence (i.e. until all confidence scores
are 3). Baseline 3 was chosen to be more ambi-
tious. It always presents information as soon as
possible, using a decreasing order of confidence to
minimise self-corrections (i.e. very similar to the
learnt policy). It chooses randomly among slots with
equal confidence. All baseline policies have an op-
timised IP strategy (recommend/summary etc.) and
differ only in their incremental processing strategies.
Baseline 1 is most similar to the current approach
used in spoken dialogue systems, where the Dia-
logue Manager triggers an NLG component as soon
as a task-relevant user utterance is processed.
Here we do not compare the different strategies
for IP generally because this has been done by
Rieser et al (2010), even if not for incremental
NLG. Figure 5 shows the performance of all be-
haviours in terms of average rewards. Baseline 1
obtains on average roughly 100 reward points less
than the RL agent. This corresponds to the (neg-
ative) reward of one self-correction (?100). Since
information is always presented as soon as it is avail-
able, this baseline needs to produce on average one
Baseline 1 (present information as soon as
it becomes available)
USR Please show me Italian places
(cuisine = Indian;conf score = 0.2)
SYS I have found 35 Indian . . .
USR No, ITALIAN
(cuisine = Italian; conf score = 0.8)
SYS Oh, you meant Italian. I have 104 Italian places in Edin-
burgh. 24 in the city centre, 21 in the New Town, 14 in . . .
USR Show me some in the city centre
(location = centre; conf score = 0.6)
SYS OK. I found 24 Italian restaurants in the city centre . . .
Baseline 2 (always wait until confidence is high)
USR Do you have Italian restaurants in the centre of town?
cuisine = Italian; conf score = 0.4
location = centre; conf score = 0.2
SYS waits
USR Italian in the centre.
cuisine = Italian, conf score = 0.7
location = centre, conf score = 0.5
SYS I have 104 Italian restaurants.
USR waits
SYS waits
USR city centre please
location = centre, conf score = 0.7
SYS I have 24 Italian restaurants in the city centre . . .
Baseline 3 (present information in decreasing
order of confidence)
USR I want Italian food . . .
cuisine = Indian, conf score = 0.2
location = centre, conf score = 0.3
SYS hmm (holding turn) . . .
USR in the centre of town
location = centre, conf score = 0.9
SYS In the centre, let me see, Indian . . .
USR Italian, please.
cuisine = Italian, conf score = 0.7
SYS Oh I see. I have 24 Italian places in the centre . . .
Figure 6: Example dialogues generated with the baseline
policies for a user who wants Italian food in the city cen-
tre. Confidence scores for cuisine and location variables
for the restaurants are shown as updated.
self-correction per episode. Baseline 2 needs to wait
until all information is known with high confidence
and obtains on average 125 to 130 rewards less than
the RL agent. This corresponds to approximately
11 time steps of waiting (for input to reach higher
confidence) before presentation since 11 is (approxi-
mately) the square root of 130. Baseline 3 is roughly
a reward of ?10 worse than the RL agent?s be-
55
haviour, which is due to a combination of more self-
corrections, even if they just occur occasionally, and
a higher number of turn holding markers. The latter
is due to the baseline starting to present as soon as
possible, so that whenever all confidence scores are
too low to start presenting, a turn holding marker
is generated. The learning agent learns to outper-
form all baselines significantly, by presenting infor-
mation slots in decreasing order of confidence, com-
bined with waiting and holding the floor at appro-
priate moments. Anticipating the rewards for wait-
ing vs. holding the floor at particular moments is the
main reason that the learnt policy outperforms Base-
line 3. Subtle moments of timing as in this case are
difficult to hand-craft and more appropriately bal-
anced using optimisation. An absolute comparison
of the last 1000 episodes of each behaviour shows
that the improvement of the RL agent corresponds
to 126.8% over Baseline 1, to 137.7% over Baseline
2 and to 16.76% over Baseline 3. All differences are
significant at p < 0.001 according to a paired t-test
and have a high effect size r > 0.9. The high per-
centage improvement of the learnt policy over Base-
lines 1 and 2 is mainly due to the high numeric val-
ues chosen for the rewards as can be observed from
their qualitative behaviour. Thus, if the negative nu-
meric values of, e.g., a self-correction were reduced,
the percentage reward would reduce, but the pol-
icy would not change qualitatively. Figure 1 shows
some examples of the learnt policy including several
incremental phenomena. In contrast, Figure 6 shows
examples generated with the baselines.
7 Conclusion and Future Directions
We have presented a novel framework combining in-
cremental and statistical approaches to NLG for in-
teractive systems. In a proof-of-concept study in the
domain of Information Presentation, we optimised
the timing and order of IP. The learning agent op-
timises the trade-off of whether to present informa-
tion as soon as it becomes available (for high respon-
siveness) or else to wait until input hypotheses were
more stable (to avoid self-corrections). Results in a
simulated environment showed that the agent learns
to avoid self-corrections and long waiting times, of-
ten by presenting information in order of decreas-
ing confidence. It outperforms three hand-crafted
baselines due to its enhanced adaptivity. In pre-
vious work, incremental responsiveness has mainly
been implemented by producing semantically empty
fillers such as um, let me see, well, etc. (Skantze and
Hjalmarsson, 2010). Our work avoids the need for
these fillers by content reordering.
Since this paper has focused on a proof-of-
concept study, our goal has not been to demonstrate
the superiority of automatic optimisation over hand-
crafted behaviour. Previous studies have shown
the advantages of optimisation (Janarthanam and
Lemon, 2010; Rieser et al, 2010; Dethlefs et al,
2011). Rather, our main goal has been to demon-
strate that incremental NLG can be phrased as an op-
timisation problem and that reasonable action poli-
cies can be learnt so that an application within an
incremental framework is feasible. This observation
allows us to take incremental systems, which so far
have been restricted to deterministic decision mak-
ing, one step further in terms of their adaptability
and flexibility. To demonstrate the effectiveness of
a synergy between RL and incremental NLG on a
large scale, we would like to train a fully incremental
NLG system from human data using a data-driven
reward function. Further, an evaluation with human
users will be required to verify the advantages of dif-
ferent policies for Information Presentation.
Regarding the scalability of our optimisation
framework, RL systems are known to suffer from the
curse of dimensionality, the problem that their state
space grows exponentially according to the number
of variables taken into account. While the appli-
cation of flat RL is therefore limited to small-scale
problems, we can use RL with a divide-and-conquer
approach, hierarchical RL, which has been shown to
scale to large-scale NLG applications (Dethlefs and
Cuaya?huitl, 2011), to address complex NLG tasks.
Future work can take several directions. Cur-
rently, we learn the agent?s behaviour offline, be-
fore the interaction, and then execute it statistically.
More adaptivity towards individual users and situ-
ations could be achieved if the agent was able to
learn from ongoing interactions using online learn-
ing. In addition, current NLG systems tend to as-
sume that the user?s goals and situational circum-
stances are known with certainty. This is often an
unrealistic assumption that future work could ad-
dress using POMDPs (Williams and Young, 2007).
56
Acknowledgements
The research leading to this work has received fund-
ing from EC?s FP7 programmes: (FP7/2011-14)
under grant agreement no. 287615 (PARLANCE);
(FP7/2007-13) under grant agreement no. 216594
(CLASSiC); (FP7/2011-14) under grant agreement
no. 270019 (SPACEBOOK); (FP7/2011-16) under
grant agreement no. 269427 (STAC).
References
Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
simple domain-independent probabilistic approach to
generation. In Proc. of EMNLP, pages 502?512.
Timo Baumann and David Schlangen. 2011. Predict-
ing the Micro-Timing of User Input for an Incremental
Spoken Dialogue System that Completes a User?s On-
going Turn. In Proc. of 12th Annual SIGdial Meeting
on Discourse and Dialogue, Portland, OR.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Anja Belz. 2008. Automatic Generation of Weather
Forecast Texts Using Comprehensive Probabilistic
Generation-Space Models. Natural Language Engi-
neering, 14(4):431?455.
Okko Buss and David Schlangen. 2011. DIUM?An In-
cremental Dialogue Manager That Can Produce Self-
Corrections. In Proc. of the Workshop on the Seman-
tics and Pragmatics of Dialogue (SemDIAL / Los An-
gelogue), Los Angeles, CA.
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proc. of 11th Annual SIG-
dial Meeting on Discourse and Dialogue.
Nina Dethlefs and Heriberto Cuaya?huitl. 2011.
Combining Hierarchical Reinforcement Learning and
Bayesian Networks for Natural Language Generation
in Situated Dialogue. In Proc. of the 13th European
Workshop on Natural Language Generation (ENLG),
Nancy, France.
Nina Dethlefs, Heriberto Cuaya?huitl, and Jette Viethen.
2011. Optimising Natural Language Generation Deci-
sion Making for Situated Dialogue. In Proceedings of
the 12th Annual Meeting on Discourse and Dialogue
(SIGdial), Portland, Oregon, USA.
David DeVault, Kenji Sagae, and David Traum. 2009.
Can I finish? Learning when to respond to incremental
interpretation result in interactive dialogue. In Proc.
of the 10th Annual SigDial Meeting on Discourse and
Dialogue, Queen Mary University, UK.
Srini Janarthanam and Oliver Lemon. 2010. Learning to
Adapt to Unknown Users: Referring Expression Gen-
eration in Spoken Dialogue Systems. In Proc. of the
48th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 69?78, July.
Anne Kilger and Wolfgang Finkler. 1995. Incremen-
tal generation for real-time applications. Technical re-
port, DFKI Saarbruecken, Germany.
Willem Levelt. 1989. Speaking: From Intenion to Artic-
ulation. MIT Press.
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek, Simon
Keizer, Blaise Thomson, Kai Yu, and Steve Young.
2010. Phrase-based statistical language generation us-
ing graphical models and active learning. In Proc. of
the Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 1552?1561.
Matthew Purver and Masayuki Otsuka. 2003. Incremen-
tal Generation by Incremental Parsing. In Proceedings
of the 6th UK Special-Interesting Group for Computa-
tional Linguistics (CLUK) Colloquium.
Antoine Raux and Maxine Eskenazi. 2009. A Finite-
State Turn-Taking Model for Spoken Dialog Sys-
tems. In Proc. of the 10th Conference of the North
American Chapter of the Association for Compu-
tational Linguistics?Human Language Technologies
(NAACL-HLT), Boulder, Colorado.
Verena Rieser and Oliver Lemon. 2011. Reinforcement
Learning for Adaptive Dialogue Systems: A Data-
driven Methodology for Dialogue Management and
Natural Language Generation. Book Series: The-
ory and Applications of Natural Language Processing,
Springer, Berlin/Heidelberg.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Dia-
logue Systems. In Proc. of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL),
Uppsala, Sweden.
David Schlangen and Gabriel Skantze. 2009. A General,
Abstract Model of Incremental Dialogue Processing.
In Proc. of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Athens, Greece.
Gabriel Skantze and Anna Hjalmarsson. 2010. Towards
Incremental Speech Generation in Dialogue Systems.
In Proc. of the 11th Annual SigDial Meeting on Dis-
course and Dialogue, Tokyo, Japan.
Gabriel Skantze and David Schlangen. 2009. Incre-
mental Dialogue Processing in a Micro-Domain. In
Proc. of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics,
Athens, Greece.
Amanda Stent, Rashmi Prasad, and Marilyn Walker.
2004. Trainable sentence planning for complex infor-
mation presentation in spoken dialogue systems. In
57
Proc. of the Annual Meeting of the Association for
Computational Linguistics.
Kees van Deemter. 2009. What game theory can do for
NLG: the case of vague language. In 12th European
Workshop on Natural Language Generation (ENLG).
Marilyn Walker, Steve Whittaker, Amanda Stent, Pre-
taam Maloor, Johanna Moore, and G Vasireddy.
2004. Generation and Evaluation of User Tailored Re-
sponses in Multimodal Dialogue. Cognitive Science,
28(5):811?840.
Chris Watkins. 1989. Learning from Delayed Rewards.
PhD Thesis, King?s College, Cambridge, UK.
Jason Williams and Steve Young. 2007. Partially
Observable Markov Decision Processes for Spoken
Dialog Systems. Computer Speech and Language,
21(2):393?422.
58
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 134?136,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Integrating Location, Visibility, and Question-Answering in a Spoken
Dialogue System for Pedestrian City Exploration
Srinivasan Janarthanam1, Oliver Lemon1, Xingkun Liu1, Phil Bartie2,
William Mackaness2, Tiphaine Dalmas3 and Jana Goetze4
1Interaction Lab, Heriot-Watt University, Edinburgh
2 School of GeoSciences, University of Edinburgh
3School of Informatics, University of Edinburgh
4KTH Royal Institute of Technology, Stockholm, Sweden
sc445,o.lemon,x.liu@hw.ac.uk, philbartie@gmail.com,
william.mackaness@ed.ac.uk,
tiphaine.dalmas@aethys.com, jagoetze@kth.se
Abstract
We demonstrate a spoken dialogue-based in-
formation system for pedestrians. The system
is novel in combining geographic information
system (GIS) modules such as a visibility en-
gine with a question-answering (QA) system,
integrated within a dialogue system architec-
ture. Users of the demonstration system can
use a web-based version (simulating pedes-
trian movement using StreetView) to engage
in a variety of interleaved conversations such
as navigating from A to B, using the QA func-
tionality to learn more about points of interest
(PoI) nearby, and searching for amenities and
tourist attractions. This system explores a va-
riety of research questions involving the inte-
gration of multiple information sources within
conversational interaction.
1 Motivation
Although navigation and local information are avail-
able to users through smartphone apps, there are still
important problems such as how such information is
delivered safely and proactively, and without cogni-
tively overloading the user. (Kray et al, 2003) sug-
gested that cognitive load of information presented
in textual and speech-based interfaces is medium
and low respectively when compared to more com-
plicated visual interfaces. Our objective, therefore,
is to build a hands-free and eyes-free system that en-
gages the pedestrian user by presenting all informa-
tion and receiving user requests through speech only.
In addition, and in contrast to other mobile ap-
plications, this system is conversational ? meaning
that it accumulates information over time, and plans
its utterances to achieve long-term goals. It inte-
grates with a city model and a visibility engine (Bar-
tie and Mackaness, 2012) to identify points of inter-
ests and visibile landmarks for presentation, a pedes-
trian tracker to improve the GPS positioning of the
user and a question-answering (QA) system to en-
able users to explore information about the city more
freely than with a graphical interface.
Table 1 presents an example dialogue interaction
with the system showing the use of visibility infor-
mation and Question-Answering.
User: Take me to Princes Street.
System: Turn left on to South Bridge and
walk towards the tower in front of you.
...
System: Near you is the famous statue of David Hume.
User: Tell me more about David Hume.
System: David Hume is a Scottish philosopher....
Table 1: An example interaction with the system
2 Related work
There are several mobile apps such as Triposo, Trip-
wolf, and Guidepal that provide point of interest
information, and apps such as Google Navigation
that provide navigation instructions to users. How-
ever, they demand the user?s visual attention because
they predominantly present information on a mobile
screen. In contrast, ours is a speech only interface
in order to keep the user?s cognitive load low and
avoid users from being distracted (perhaps danger-
134
ously so) from their primary task.
Generating navigation instructions in the real
world for pedestrians is an interesting research
problem in both computational linguistics and geo-
informatics (Dale et al, 2003; Richter and Duck-
ham, 2008). CORAL is an NLG system that gener-
ates navigation instructions incrementally upon user
requests based on the user?s location (Dale et al,
2003). DeepMap is a system that interacts with
the user to improve positioning using GUI controls
(Malaka and Zipf, 2000). SmartKom is a dialogue
system that presents navigation information multi-
modally (Reithinger et al, 2003). There are also
several mobile apps developed to help low-vision
users with navigation instructions (see (Stent et al,
2010) for example). In contrast to these earlier sys-
tems we present navigational, point-of-interest and
amenity information in an integrated way with users
interacting eyes-free and hands-free through a head-
set connected to a smartphone.
3 Architecture
The architecture of the current system is shown in
figure 1. The server side consists of a dialogue in-
terface (parser, interaction manager, and generator),
a City Model, a Visibility Engine, a QA server and a
Pedestrian tracker. On the user?s side is a web-based
client that consists of the simulated real-world and
the interaction panel.
Figure 1: System Architecture
3.1 Dialogue interface
The dialogue interface consists of an utterance
parser, an interaction manager and an utterance gen-
erator. The interaction manager is the central com-
ponent of this architecture, which provides the user
navigational instructions and interesting PoI infor-
mation. It receives the user?s input in the form of a
dialogue act and the user?s location in the form of
latitude and longitude information. Based on these
inputs and the dialogue context, it responds with sys-
tem output dialogue act (DA), based on a dialogue
policy. The utterance generator is a natural language
generation module that translates the system DA into
surface text, using the Open CCG toolkit (White et
al., 2007).
3.2 Pedestrian tracker
Global Navigation Satellite Systems (GNSS) (e.g.
GPS, GLONASS) provide a useful positioning so-
lution with minimal user side setup costs, for loca-
tion aware applications. However urban environ-
ments can be challenging with limited sky views,
and hence limited line of sight to the satellites, in
deep urban corridors. There is therefore signifi-
cant uncertainty about the user?s true location re-
ported by GNSS sensors on smartphones (Zandber-
gen and Barbeau, 2011). This module improves on
the reported user position by combining smartphone
sensor data (e.g. accelerometer) with map matching
techniques, to determine the most likely location of
the pedestrian (Bartie and Mackaness, 2012).
3.3 City Model
The city model is a spatial database containing in-
formation about thousands of entities in the city of
Edinburgh. These data have been collected from a
variety of existing resources such as Ordnance Sur-
vey, OpenStreetMap and the Gazetteer for Scotland.
It includes the location, use class, name, street ad-
dress, and where relevant other properties such as
build date. The model also includes a pedestrian net-
work (streets, pavements, tracks, steps, open spaces)
which can be used to calculate minimal cost routes,
such as the shortest path.
3.4 Visibility Engine
This module identifies the entities that are in the
user?s vista space (Montello, 1993). To do this it
accesses a digital surface model, sourced from Li-
DAR, which is a 2.5D representation of the city in-
cluding buildings, vegetation, and land surface ele-
vation. The visibility engine uses this dataset to offer
a number of services, such as determining the line
135
of sight from the observer to nominated points (e.g.
which junctions are visible), and determining which
entities within the city model are visible. These met-
rics can be then used by the interaction manager
to generate effective navigation instructions. E.g.
?Walk towards the castle?, ?Can you see the tower
in front of you??, ?Turn left after the large building
on your left after the junction? and so on.
3.5 Question-Answering server
The QA server currently answers a range of defini-
tion questions. E.g., ?Tell me more about the Scot-
tish Parliament?, ?Who was David Hume??, etc. QA
identifies the entity focused on in the question us-
ing machine-learning techniques (Mikhailian et al,
2009), and then proceeds to a textual search on texts
from the Gazetteer of Scotland and Wikipedia, and
definitions from WordNet glosses. Candidates are
reranked using a trained confidence score with the
top candidate used as the final answer. This answer
is provided as a flow of sentence chunks that the user
can interrupt. This information can also be pushed
by the system when a salient entity appears in the
user?s viewshed.
4 Web-based User interface
For the purposes of this (necessarily non-mobile)
demonstration, we present a web-based interface
that simulates users walking in a 3D city environ-
ment. Users will be able to provide speech or text
input (if the demonstration environment is too noisy
for usable speech recognition as is often the case at
conference demonstration sessions).
The web-based client is a JavaScript/HTML pro-
gram running on the user?s web browser. For a
detailed description of this component, please re-
fer to (Janarthanam et al, 2012). It consists of two
parts: the Streetview panel and the Interaction panel.
The Streetview panel presents a simulated real world
visually to the user. A Google Streetview client
(Google Maps API) is created with an initial user
coordinate which then allows the web user to get
a panoramic view of the streets around the user?s
virtual location. The user can walk around using
the arrow keys on his keyboard or the mouse. The
system?s utterances are synthesized using Cereproc
text-to-speech engine and presented to the user.
Acknowledgments
The research has received funding from the Eu-
ropean Community?s 7th Framework Programme
(FP7/2007-2013) under grant agreement no. 270019
(SPACEBOOK project http://www.spacebook-
project.eu/).
References
P. Bartie and W. Mackaness. 2012. D3.4 Pedestrian Po-
sition Tracker. Technical report, The SPACEBOOK
Project (FP7/2011-2014 grant agreement no. 270019).
R. Dale, S. Geldof, and J. Prost. 2003. CORAL : Using
Natural Language Generation for Navigational Assis-
tance. In Proceedings of ACSC2003, South Australia.
S. Janarthanam, O. Lemon, and X. Liu. 2012. A web-
based evaluation framework for spatial instruction-
giving systems. In Proc. of ACL 2012, South Korea.
C. Kray, K. Laakso, C. Elting, and V. Coors. 2003. Pre-
senting route instructions on mobile devices. In Pro-
ceedings of IUI 03, Florida.
R. Malaka and A. Zipf. 2000. Deep Map - challenging IT
research in the framework of a tourist information sys-
tem. In Information and Communication Technologies
in Tourism 2000, pages 15?27. Springer.
A. Mikhailian, T. Dalmas, and R. Pinchuk. 2009. Learn-
ing foci for question answering over topic maps. In
Proceedings of ACL 2009.
D. Montello. 1993. Scale and multiple psychologies of
space. In A. U. Frank and I. Campari, editors, Spatial
information theory: A theoretical basis for GIS.
N. Reithinger, J. Alexandersson, T. Becker, A. Blocher,
R. Engel, M. Lckelt, J. Mller, N. Pfleger, P. Poller,
M. Streit, and V. Tschernomas. 2003. SmartKom -
Adaptive and Flexible Multimodal Access to Multiple
Applications. In Proceedings of ICMI 2003, Vancou-
ver, B.C.
K. Richter and M. Duckham. 2008. Simplest instruc-
tions: Finding easy-to-describe routes for navigation.
In Proceedings of the 5th Intl. Conference on Geo-
graphic Information Science.
A. J. Stent, S. Azenkot, and B. Stern. 2010. Iwalk: a
lightweight navigation system for low-vision users. In
Proc. of the ASSETS 2010.
M. White, R. Rajkumar, and S. Martin. 2007. Towards
Broad Coverage Surface Realization with CCG. In
Proc. of the UCNLG+MT workshop.
P. A. Zandbergen and S. J. Barbeau. 2011. Positional
Accuracy of Assisted GPS Data from High-Sensitivity
GPS-enabled Mobile Phones. Journal of Navigation,
64(3):381?399.
136
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 15?16,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Incremental Spoken Dialogue Systems: Tools and Data
Helen Hastie, Oliver Lemon, Nina Dethlefs
The Interaction Lab, School of Mathematics and Computer Science
Heriot-Watt University, Edinburgh, UK EH14 4AS
h.hastie, o.lemon, n.s.dethlefs@hw.ac.uk
Abstract
Strict-turn taking models of dialogue do not
accurately model human incremental process-
ing, where users can process partial input and
plan partial utterances in parallel. We discuss
the current state of the art in incremental sys-
tems and propose tools and data required for
further advances in the field of Incremental
Spoken Dialogue Systems.
1 Incremental Spoken Dialogue Systems
For Spoken Dialogue Systems (SDS) to be more fre-
quently adopted, advances in the state-of-the-art are
necessary to enable highly responsive and conversa-
tional systems. Traditionally, the unit of speech has
been a whole utterance with strict, rigid turn-taking
determined by a voice-activity detector. However,
a large body of psycholinguistic literature indicates
that human-human interaction is in fact incremen-
tal (Tanenhaus and Brown-Schmidt, 2008; Levelt,
1989). Using a whole utterance as the unit of choice
makes dialogues longer, unnatural and stilted and ul-
timately interferes with a user?s ability to focus on
their goal (Allen et al, 2001).
A new generation of Incremental SDS (ISDS) are
being developed that deal with ?micro-turns? (sub-
utterance processing units) resulting in dialogues
that are more fluid and responsive. Recent work
has shown that processing smaller ?chunks? of input
and output can improve the user experience (Aist et
al., 2007; Skantze and Schlangen, 2009; Buss et al,
2010; Baumann et al, 2011; Selfridge et al, 2011).
Incrementality enables the system designer to model
several dialogue phenomena that play a vital role
in human discourse (Levelt, 1989) but have so far
been absent from systems. These include more
natural turn-taking through rapid system responses,
grounding through the generation of backchannels
and feedback, and barge-ins (from both user and sys-
tem). In addition, corrections and self-corrections
through constant monitoring of user and system ut-
terances play an important role, enabling the system
to recover smoothly from a recognition error or a
change in user?s preferences. Some examples of the
phenomena we are targeting are given in Figure 1.
Parlance, a FP7 EC project1, is currently develop-
ing incremental systems for English and Mandarin.
The goal of Parlance is to develop mobile, interac-
tive, ?hyper-local? search through speech. Recent
trends in Information Retrieval are towards incre-
mental, interactive search. Spoken dialogue systems
can provide a truly natural medium for this type of
search, in particular for people on the move.
2 Tools and Data
The emphasis of the Parlance project is on data-
driven techniques for ISDS, thereby addressing the
problem of a lack of data for system develop-
ment. Although incremental dialogue phenomena
described in Figure 1 have been observed in human-
human dialogue, more task-based data is needed. It
is challenging to fabricate a situation where users
produce incremental discourse phenomena as in Fig-
ure 1 frequently and in a natural manner. Wizard-
1http://www.parlance-project.eu
15
Backchannels (when the user pauses)
USR I want Italian food [500 ms] in the centre of town . . .
SYS uh-huh
SYS OK. I found 24 Italian restaurants in the city centre. The
restaurant Roma is in the medium price range,. . .
Self-correction (the system made a mistake)
USR I want Italian food in the centre of town . . .
SYS OK. I found 35 Indian restaurants . . .
USR No, I want Italian.
SYS oh sorry . . .
SYS I have 24 Italian restaurants in the city centre . . .
Holding the floor
USR I want cheap Italian food . . .
SYS ok let me see
SYS I have 3 cheap Italian places . . .
Figure 1: Incremental phenomena observed in human-
human dialogue that systems should be able to model.
of-Oz experiments can be used to collect data from
the system side, but user-initiated phenomena, such
as the user changing his/her mind are more difficult
to instigate. Therefore, data collections of naturally
occurring incremental phenomena in human-human
settings will be essential for further development of
incremental systems. Such data can inform user sim-
ulations which provide means of training stochastic
SDS with less initial data and can compensate for
data sparsity. For example, in Dethlefs et al (2012)
the user simulation can change its mind and react to
different NLG strategies such as giving information
with partial input or waiting for complete input from
the user. Both the academic community and industry
would benefit from open access data, such as will be
collected in the Parlance project and made available
to the dialogue community2. There would also need
to be a clear path from academic research on ISDS
to industry standards such as VoiceXML to facilitate
adoption.
Various components and techniques of ISDS are
needed to handle ?micro-turns?. Challenges here
include recognizing and understanding partial user
input and back-channels; micro-turn dialogue man-
agement that can decide when to back-channel, self-
correct and hold-the-floor; incremental NLG that
can generate output while the user is still talking;
2As was done for CLASSiC project data at:
http://www.macs.hw.ac.uk/iLabArchive/CLASSiCProject/Data/login.php
and finally more flexible TTS that can handle barge-
in and understand when it has been interrupted.
In summary, in order to achieve highly natural,
responsive incremental systems, we propose using
data-driven techniques, for which the main issue is
lack of data. Carefully crafted task-based human-
human data collection and WoZ studies, user simu-
lations, shared data archives, and upgraded industry
standards are required for future work in this field.
Acknowledgments
The research leading to this work has received fund-
ing from the EC?s FP7 programme: (FP7/2011-14)
under grant agreement no. 287615 (PARLANCE).
References
Gregory Aist, James Allen, Ellen Campana, Lucian
Galescu, Carlos Gomez Gallo, Scott Stoness, Mary
Swift, and Michael Tanenhaus. 2007. Software ar-
chitectures for incremental understanding of human
speech. In Proceedings of SemDial / DECALOG.
James Allen, George Ferguson, and Amanda Stent. 2001.
An Architecture For More Realistic Conversational
Systems. In Proc. of Intelligent User Interfaces.
Timo Baumann, Okko Buss, and David Schlangen. 2011.
Evaluation and Optimisation of Incremental Proces-
sors. Dialogue and Discourse, 2(1).
Okko Buss, Timo Baumann, and David Schlangen. 2010.
Collaborating on Utterances with a Spoken Dialogue
Systen Using an ISU-based Approach to Incremental
Dialogue Management. In Proc. of SIGDIAL.
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need for
Fillers. In Proc of INLG, Chicago, Illinois, USA.
Willem Levelt. 1989. Speaking: From Intenion to Artic-
ulation. MIT Press.
Ethan Selfridge, Iker Arizmendi, Peter Heeman, and Ja-
son Williams. 2011. Stability and Accuracy in Incre-
mental Speech Recognition. In Proc. of SigDial.
Gabriel Skantze and David Schlangen. 2009. Incremen-
tal Dialogue Processing in a Micro-Domain. In Proc.
of EACL, Athens, Greece.
M.K. Tanenhaus and S. Brown-Schmidt. 2008. Lan-
guage processing in the natural world. In B.C.M
Moore, L.K. Tyler, and W.D. Marslen-Wilson, edi-
tors, The perception of speech: from sound to meaning,
pages 1105?1122.
16
Proceedings of the 14th European Workshop on Natural Language Generation, pages 115?124,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Generating student feedback from time-series data using Reinforcement
Learning
Dimitra Gkatzia, Helen Hastie, Srinivasan Janarthanam and Oliver Lemon
Department of Mathematical and Computer Sciences
Heriot-Watt University
Edinburgh, Scotland
{dg106, h.hastie, sc445, o.lemon} @hw.ac.uk
Abstract
We describe a statistical Natural Language
Generation (NLG) method for summarisa-
tion of time-series data in the context of
feedback generation for students. In this
paper, we initially present a method for
collecting time-series data from students
(e.g. marks, lectures attended) and use ex-
ample feedback from lecturers in a data-
driven approach to content selection. We
show a novel way of constructing a reward
function for our Reinforcement Learning
agent that is informed by the lecturers?
method of providing feedback. We eval-
uate our system with undergraduate stu-
dents by comparing it to three baseline
systems: a rule-based system, lecturer-
constructed summaries and a Brute Force
system. Our evaluation shows that the
feedback generated by our learning agent
is viewed by students to be as good as the
feedback from the lecturers. Our findings
suggest that the learning agent needs to
take into account both the student and lec-
turers? preferences.
1 Introduction
Data-to-text generation refers to the task of auto-
matically generating text from non-linguistic data
(Reiter and Dale, 2000). The goal of this work is
to develop a method for summarising time-series
data in order to provide continuous feedback to
students across the entire semester. As a case
study, we took a module in Artificial Intelligence
and asked students to fill out a very short diary-
type questionnaire on a weekly basis. Questions
included, for example, number of deadlines, num-
ber of classes attended, severity of personal issues.
These data were then combined with the marks
from the weekly lab reflecting the students? per-
formance. As data is gathered each week in the
lab, we now have a set of time-series data and our
goal is to automatically create feedback. The goal
is to present a holistic view through these diary en-
tries of how the student is doing and what factors
may be affecting performance.
Feedback is very important in the learning pro-
cess but very challenging for academic staff to
complete in a timely manner given the large num-
ber of students and the increasing pressures on
academics? time. This is where automatic feed-
back can play a part, providing a tool for teachers
that can give insight into factors that may not be
immediately obvious (Porayska-Pomsta and Mel-
lish, 2013). As reflected in NSS surveys1, stu-
dents are not completely satisfied with how feed-
back is currently delivered. The 2012 NSS survey,
for all disciplines reported an 83% satisfaction rate
with courses, with 70% satisfied with feedback.
This has improved from recent years (in 2006 this
was 60% for feedback) but shows that there is
still room for improvement in how teachers deliver
feedback and its content.
In the next section (Section 2) a discussion of
the related work is presented. In Section 3, a de-
scription of the methodology is given as well as
the process of the data collection from students,
the template construction and the data collection
with lecturers. In Section 4, the Reinforcement
Learning implementation is described. In Section
5, the evaluation results are presented, and finally,
in Sections 6 and 7, a conclusion and directions
for future work are discussed.
2 Related Work
Report generation from time-series data has been
researched widely and existing methods have been
used in several domains such as weather forecasts
(Belz and Kow, 2010; Angeli et al, 2010; Sripada
et al, 2004), clinical data summarisation (Hunter
1http://www.thestudentsurvey.com/
115
et al, 2011; Gatt et al, 2009), narrative to assist
children with communication needs (Black et al,
2010) and audiovisual debriefs from sensor data
from Autonomous Underwater Vehicles missions
(Johnson and Lane, 2011).
The two main challenges for time-series data
summarisation are what to say (Content Selec-
tion) and how to say it (Surface Realisation). In
this work we concentrate on the former. Previ-
ous methods for content selection include Gricean
Maxims (Sripada et al, 2003); collective con-
tent selection (Barzilay and Lapata, 2004); and
the Hidden Markov model approach for content
selection and ordering (Barzilay and Lee, 2004).
NLG systems tend to be very domain-specific
and data-driven systems that seek to simultane-
ously optimize both content selection and sur-
face realisation have the potential to be more
domain-independent, automatically optimized and
lend themselves to automatic generalization (An-
geli et al, 2010; Rieser et al, 2010; Dethlefs
and Cuayahuitl, 2011). Recent work on report
generation uses statistical techniques from Ma-
chine Translation (Belz and Kow, 2010), super-
vised learning (Angeli et al, 2010) and unsuper-
vised learning (Konstas and Lapata, 2012).
Here we apply Reinforcement Learning meth-
ods (see Section 4 for motivation) which have been
successfully applied to other NLG tasks, such as
Temporal Expressions Generation (Janarthanam
et al, 2011), Lexical Choice (Janarthanam and
Lemon, 2010), generation of adaptive restaurant
summaries in the context of a dialogue system
(Rieser et al, 2010) and generating instructions
(Dethlefs and Cuayahuitl, 2011).
3 Methodology
Figure 1: Methodology for data-driven feedback
report generation
Figure 1 shows graphically our approach to the de-
velopment of a generation system. Firstly, we col-
lected data from students including marks, demo-
graphic details and weekly study habits. Next, we
created templates for surface realisation with the
help of a Teaching and Learning expert. These
templates were used to generate summaries that
were rated by lecturers. We used these ratings to
train the learning agent. The output of the learning
agent (i.e. automatically generated feedback re-
ports) were finally evaluated by the students. Each
of these steps are discussed in turn.
3.1 Time-series Data Collection from
Students
The data were collected during the weekly lab ses-
sions of a Computer Science module which was
taught to third year Honours and MSc students
over the course of a 10 week semester. We re-
cruited 26 students who were asked to fill in a
web-based diary-like questionnaire. Initially, we
asked students to provide some demographic de-
tails (age, nationality, level of study). In addition,
students provided on a weekly basis, information
for nine factors that could influence their perfor-
mance. These nine factors were motivated from
the literature and are listed here in terms of effort
(Ames, 1992), frustration (Craig et al, 2004) , dif-
ficulty (Person et al, 1995; Fox, 1993) and per-
formance (Chi et al, 2001). Effort is measured
by three factors: (1) how many hours they studied;
(2) the level of revision they have done; (3) as well
as the number of lectures (of this module) they at-
tended. Frustration is measured by (4) the level
of understandability of the content; (5) whether
they have had other deadlines; and whether they
faced any (6) health and/or (7) personal issues and
at what severity. The difficulty of the lab exercises
is measured by (8) the students? perception of dif-
ficulty. Finally, (9) marks achieved by the students
in each weekly lab was used as a measure of their
performance.
3.2 Data Trends
Initially, the data were processed so as to iden-
tify the existing trend of each factor during the
semester, (e.g. number of lectures attending de-
creases). The tendencies of the data are estimated
using linear least-squares regression, with each
factor annotated as INCREASING, DECREAS-
ING or STABLE. In addition, for each student we
perform a comparison between the average of each
116
Type Description Examples
AVERAGE describes the factor data by either averaging the values given by
the student,
?You spent 2 hours studying the lecture material
on average?. (HOURS STUDIED)
or by comparing the student?s average with the class average
(e.g. if above the mean value for the class, we say that the ma-
terial is challenging).
?You found the lab exercises very challenging?.
(DIFFICULTY)
TREND discusses the trend of the data, e.g. increasing, decreasing or
stable.
?Your workload is increasing over the
semester?. (DEADLINES)
WEEKS talks about specific events that happened in one or more weeks. ?You have had other deadlines during weeks 5,
6 and 9?. (DEADLINES)
OTHER all other expressions that are not directly related to data. ?Revising material during the semester will im-
prove your performance?. (REVISION)
Table 1: The table explains the different template types.
factor and the class average of the same factor.
3.3 Template Generation
The wording and phrasing used in the templates to
describe the data were derived from working with
and following the advice of a Learning and Teach-
ing (L&T) expert. The expert provided consulta-
tion on how to summarise the data. We derived 4
different kinds of templates for each factor: AV-
ERAGE, TREND, WEEKS and OTHER based on
time-series data on plotted graphs. A description
of the template types is shown in Table 1.
In addition, the L&T expert consulted on how
to enhance the templates so that they are ap-
propriate for communicating feedback accord-
ing to the guidelines of the Higher Education
Academy (2009), for instance, by including moti-
vating phrases such as ?You may want to plan your
study and work ahead?.
3.4 Data Collection from Lecturers
The goal of the Reinforcement Learning agent is
to learn to generate feedback at least as well as
lecturers. In order to achieve this, a second data
collection was conducted with 12 lecturers partic-
ipating.
The data collection consisted of three stages
where lecturers were given plotted factor graphs
and were asked to:
1. write a free style text summary for 3 students
(Figure 2);
2. construct feedback summaries using the tem-
plates for 3 students (Figure 3);
3. rate random feedback summaries for 2 stu-
dents (Figure 4).
We developed the experiment using the Google
Web Toolkit for Web Applications, which facil-
itates the development of client-server applica-
tions. The server side hosts the designed tasks and
stores the results in a datastore. The client side is
responsible for displaying the tasks on the user?s
browser.
In Task 1, the lecturers were presented with the
factor graphs of a student (one graph per factor)
and were asked to provide a free-text feedback
summary for this student. The lecturers were en-
couraged to pick as many factors as they wanted
and to discuss the factors in any order they found
useful. Figure 2 shows an example free text sum-
mary for a high performing student where the lec-
turer decided to talk about lab marks and under-
standability. Each lecturer was asked to repeat this
task 3 times for 3 randomly picked students.
In Task 2, the lecturers were again asked to con-
struct a feedback summary but this time they were
given a range of sentences generated from the tem-
plates (as described in Section 2.3). They were
asked to use these to construct a feedback report.
The number of alternative utterances generated for
each factor varies depending on the factor and the
given data. In some cases, a factor can have 2 gen-
erated utterances and in other cases up to 5 (with
a mean of 3 for each factor) and they differenti-
ate in the style of trend description and wording.
Again the lecturer was free to choose which fac-
tors to talk about and in which order, as well as
to decide on the template style he/she prefers for
the realisation through the template options. Fig-
ure 3 shows an example of template selection for
the same student as in Figure 2.
In Task 3, the lecturers were presented with the
plotted factor graphs plus a corresponding feed-
back summary that was generated by randomly
choosing n factors and their templates, and were
asked to rate it in a scale between 0-100 (100 for
the best summary). Figure 4 shows an example of
117
Figure 2: The interface of the 1st task of the data collection: the lecturer consults the factor graphs and
provides feedback in a free text format.
Figure 3: The interface of the 2nd task of data collection: the lecturer consults the graphs and constructs
a feedback summary from the given templates (this graph refers to the same student as Figure 2).
a randomly generated summary for the same stu-
dent as in Figure 2.
4 Learning a Time-Series Generation
Policy
Reinforcement Learning (RL) is a machine learn-
ing technique that defines how an agent learns to
take optimal actions in a dynamic environment so
as to maximize a cumulative reward (Sutton and
Barto, 1998). In our framework, the task of con-
tent selection of time-series data is presented as a
Markov Decision problem. The goal of the agent
is to learn to choose a sequence of actions that
obtain the maximum expected reward in the long
run. In this section, we describe the Reinforce-
ment Learning setup for learning content selection
118
Figure 4: The interface of the 3rd task of data col-
lection: the lecturer consults the graphs and rates
the randomly generated feedback summary (this
graph refers to the same student as Figures 2 and
3).
from time-series data for feedback report gener-
ation. Summarisation from time-series data is an
open challenge and we aim to research other meth-
ods in the future, such as supervised learning, evo-
lutionary algorithms etc.
4.1 Actions and States
In this learning setup, we focused only on select-
ing the correct content, i.e. which factors to talk
about. The agent selects a factor and then decides
whether to talk about it or not. The state consists
of a description of the factor trends and the num-
ber of templates that have been selected so far. An
example of the initial state of a student can be:
<marks increased, lectures attended stable,
hours studied increased, understandability stable,
difficulty increased, health issues stable, per-
sonal issues stable, revision increased, 0>
The agent explores the state space by selecting a
factor and then by deciding whether to talk about
it or not. If the agent decides to talk about the
selected factor, it chooses the template in a greedy
way, i.e. it chooses for each factor the template
that results in a higher reward. After an action has
been selected, it is deleted from the action space.
4.1.1 Ordering
In order to find out in which order the lectur-
ers describe the factors, we transformed the feed-
back summaries into n-grams of factors. For in-
stance, a summary that talks about the student?s
performance, the number of lectures that he/she
attended, potential health problems and revision
done can be translated into the following ngram:
start, marks, lectures attended, health issues, re-
vision, end. We used the constructed n-grams to
compute the bigram frequency of the tokens in or-
der to identify which factor is most probable to be
referred to initially, which factors follow particu-
lar factors and which factor is usually talked about
in the end. It was found that the most frequent or-
dering is: start, marks, hours studied, understand-
ability, difficulty, deadlines, health issues, per-
sonal issues, lectures attended, revision, end.
4.2 Reward Function
The goal of the reward function is to optimise the
way lecturers generate and rate feedback. Given
the expert annotated summaries from Task 1, the
constructed summaries from Task 2 and the ratings
from Task 3, we derived the multivariate reward
function:
Reward = a +
n?
i=1
bi ? xi + c ? length
where X = {x1, x2, ..., xn} represents the
combinations between the data trends observed in
the time-series data and the corresponding lectur-
ers? feedback (i.e. whether they included a factor
to be realised or not and how). The value xi for
factor i is defined by the function:
xi =
?
?????
?????
1, the combination i of a factor trend
and a template type is included in
the feedback
0, if not.
For instance, the value of x1 is 1 if marks were
increased and this trend is realised in the feedback,
otherwise it is 0. In our domain n = 90 in order to
cover all the different combinations. The length
stands for the number of factors selected, a is the
intercept, bi and c are the coefficients for xi and
length respectively.
In order to model the reward function, we used
linear regression to compute the weights from the
data gathered from the lecturers. Therefore, the
reward function is fully informed by the data pro-
vided by the experts. Indeed, the intercept a, the
vector weights b and the weight c are learnt by
making use of the data collected by the lecturers
from the 3 tasks discussed in Section 3.4.
The reward function is maximized (Reward
= 861.85) for the scenario (i.e. each student?s
data), content selection and preferred template
style shown in Table 2 (please note that this sce-
nario was not observed in the data collection).
119
Factor Trend Template
difficulty stable NOT MENTIONED
hours studied stable TREND
understandability stable NOT MENTIONED
deadlines increase WEEKS
health issues stable WEEKS
personal issues stable WEEKS
lectures att. stable WEEKS
revision stable OTHER
marks increase TREND
Table 2: The table shows the scenario at which the
reward function is maximised.
The reward function is minimized (Reward =
-586.0359) for the scenario shown in Table 3
(please note that this scenario also was not ob-
served in the data collection).
Factor Trend Template
difficulty increase AVERAGE
hours studied stable NOT MENTIONED
understandability decrease AVERAGE
deadlines * *
health issues increase TREND
personal issues stable TREND
lectures att. stable NOT MENTIONED
revision stable AVERAGE
marks stable TREND
Table 3: The table shows the scenario at which the
reward function is minimised (* denotes multiple
options result in the same minimum reward).
4.3 Training
We trained a time-series generation policy
for 10,000 runs using the Tabular Temporal-
Difference Learning (Sutton and Barto, 1998).
During the training phase, the learning agent gen-
erated feedback summaries. When the construc-
tion of the summary begins, the length of the sum-
mary is 0. Each time that the agent adds a template
(by selecting a factor), the length is incremented,
thus changing the state. It repeats the process until
it decides for all factors whether to talk about them
or not. The agent is finally rewarded at the end of
the process using the Reward function described
in Section 3.2. Initially, the learning agent selects
factors randomly, but gradually learns to identify
factors that are highly rewarding for a given data
scenario. Figure 5 shows the learning curve of the
agent.
Figure 5: Learning curve for the learning agent.
The x-axis shows the number of summaries pro-
duced and y- axis the total reward received for
each summary.
5 Evaluation
We evaluated the system using the reward func-
tion and with students. In both these evaluations,
we compared feedback reports generated using
our Reinforcement Learning agent with four other
baseline systems. Here we present a brief descrip-
tion of the baseline systems.
Baseline 1: Rule-based system. This system
selects factors and templates for generation using a
set of rules. These hand-crafted rules were derived
from a combination of the L&T expert?s advice
and a student?s preferences and is therefore a chal-
lenging baseline and represents a middle ground
between the L&T expert?s advice and a student?s
preferences. An example rule is: if the mark aver-
age is less than 50% then refer to revision.
Baseline 2: Brute Force system. This system
performs a search of the state space, by exploring
randomly as many different feedback summaries
as possible. The Brute Force algorithm is shown
below:
Algorithm 1 Brute Force algorithm
I n p u t d a t a : D
f o r n = 0 . . . 1 0 , 0 0 0
c o n s t r u c t randomly f e e d b a c k [ n ]
a s s i g n getReward [ n ]
i f ge tReward [ n]>getReward [ n?1]
b e s t F e e d b a c k = f e e d b a c k [ n ]
e l s e
b e s t F e e d b a c k = f e e d b a c k [ n?1]
r e t u r n b e s t F e e d b a c k
In each run the algorithm constructs a feedback
summary, then it calculates its reward, using the
same reward function used for the Reinforcement
Learning approach, and if the reward of the new
feedback is better than the previous, it keeps the
120
new one as the best. It repeats this process for
10,000 times for each scenario. Finally, the algo-
rithm returns the summary that scored the highest
ranking.
Baseline 3: Lecturer-produced summaries.
These are the summaries produced by the lectur-
ers, as described in Section 2.4, for Task 2 using
template-generated utterances.
Baseline 4: Random system: The Random
system constructs feedback summaries by select-
ing factors and templates randomly as described in
Task 3 (in Section 3.4).
5.1 Evaluation with Reward Function
Table 4 presents the results of the evaluation per-
formed using the Reward Function, comparing
the learned policy with the four baseline systems.
Each system generated 26 feedback summaries.
On average the learned policy scores significantly
higher than any other baseline for the given sce-
narios (p <0.05 in a paired t-test).
Time-Series Summarisation Systems Reward
Learned 243.82
Baseline 1: Rule-based 107.77
Baseline 2: Brute Force 241.98
Baseline 3: Lecturers 124.62
Baseline 4: Random 43.29
Table 4: The table summarises the average re-
wards that are assigned to summaries produced
from the different systems.
5.2 Evaluation with Students
A subjective evaluation was conducted using 1st
year students of Computer Science as participants.
We recruited 17 students, who were all English na-
tive speakers. The participants were shown 4 feed-
back summaries in a random order, one generated
by the learned policy, one from the rule-based sys-
tem (Baseline 1), one from the Brute Force system
(Baseline 2) and one summary produced by a lec-
turer using the templates (Baseline 3). Given the
poor performance of the Random system in terms
of reward, Baseline 4 was omitted from this study.
Overall there were 26 different scenarios, as de-
scribed in Section 3.1. All summaries presented
to a participant were generated from the same sce-
nario. The participants then had to rank the sum-
maries in order of preference: 1 for the most pre-
ferred and 4 for the least preferred. Each partici-
pant repeated the process for 4.5 scenarios on aver-
age (the participant was allowed to opt out at any
stage). The mode values of the rankings of the
preferences of the students are shown in Table 5.
The web-based system used for the evaluation is
shown in Figure 6.
System Mode of Rankings
Learned 3rd
Baseline 3: Lecturers 3rd
Baseline 1: Rule-based 1st
Baseline 2: Brute Force 4th
Table 5: The table shows the mode value of the
rankings of the preference of the students.
We ran a Mann-Whitney?s U test to evaluate the
difference in the responses of our 4-point Likert
Scale question between the Learned system and
the other three baselines. It was found that, for
the given data, the preference of students for the
feedback generated by the Learned system is as
good as the feedback produced by the experts, i.e.
there is no significant difference between the mean
value of the rankings of the Learned system and
the lecturer-produced summaries (p = 0.8) (Base-
line 3).
The preference of the users for the Brute Force
system does not differ significantly from the sum-
maries generated by the Learned system (p =
0.1335). However, the computational cost of the
Brute Force is higher because each time that the
algorithm sees a new scenario it has to run ap-
proximately 3k times to reach a good summary (as
seen in Figure 7) and about 10k to reach an optimal
one, which corresponds to 46 seconds. This delay
would prohibit the use of such a system in time-
critical situations (such as defence) and in live sys-
tems such as tutoring systems. In addition, the
processing time would increase with more compli-
cated scenarios and if we want to take into account
the ordering of the content selection and/or if we
have more variables. In contrast, the RL method
needs only to be trained once.
Finally, the users significantly preferred the
summaries produced by the Rule-based system
(Baseline 1) to the summaries produced by the
Learned system. This is maybe because of the fact
that in the rule-based system some knowledge of
the end user?s preferences (i.e. students) was taken
into account in the rules which was not the case
in the other three systems. This fact suggests that
121
Figure 6: The interface for the evaluation: the students viewed the four feedback summaries and ranked
them in order of preference. From left to right, the summaries as generated by: an Expert (Baseline 3),
the Rule based system (Baseline 1), the Brute Force algorithm (Baseline 2), the Learned system.
Figure 7: The graphs shows the number of cycles
that the Brute Force algorithm needs to achieve
specific rewards.
students? preferences should be taken into account
as they are the receivers of the feedback. This can
also be generalised to other areas, where the ex-
perts and the end users are not the same group
of people. As the learned policy was not trained
to optimise for the evaluation criteria, in future,
we will explore reward functions that bear in mind
both the expert knowledge and the student?s pref-
erences.
6 Conclusion
We have presented a statistical learning approach
to summarisation from time-series data in the area
of feedback reports. In our reports, we took into
account the principles of good feedback provision
as instructed by the Higher Education Academy.
We also presented a method for data gathering
from students and lecturers and show how we can
use these data to generate feedback by presenting
the problem as a Markov Decision Process and
optimising it using Reinforcement Learning tech-
niques. We also showed a way of constructing a
data-driven reward function that can capture de-
pendencies between the time-series data and the
realisation phrases, in a similar way that the lec-
turers do when providing feedback. Finally, our
evaluation showed that the learned report genera-
tion policy generates reports as well as lecturers.
7 Future Work
We aim to conduct further qualitative research in
order to explore what factors and templates stu-
dents find useful to be included in the feedback
and inform our reward function with this informa-
tion as well as what we have observed in the lec-
turer data collection. This way, we hope, not only
to gain insights into what is important to students
and lecturers but also to develop a data-driven ap-
proach that, unlike the rule-based system, does not
require expensive and difficult-to-obtain expert in-
put from Learning and Teaching experts. In ad-
dition, we want to compare RL techniques with
supervised learning approaches and evolutionary
algorithms. Finally, we want to unify content se-
122
lection and surface realisation, therefore we will
extend the action space in order to include actions
for template selection.
8 Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
References
Carole Ames. 1992. Classrooms: Goals, Structures,
and Student Motivation. Journal of Educational Psy-
chology, 84(3):p261-71.
Gabor Angeli, Percy Liang and Dan Klein. 2010. A
simple domain-independent probabilistic approach
to generation. EMNLP ?10: Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing.
Regina Barzilay and Mirella Lapata. 2004. Collec-
tive content selection for concept-to-text generation.
HLT ?05: Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. HLT-NAACL
2004: Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computational Linguistics.
Anja Belz and Eric Kow. 2010. Extracting parallel
fragments from comparable corpora for data-to-text
generation. INLG ?10: Proceedings of the 6th Inter-
national Natural Language Generation Conference.
Rolf Black, Joe Reddington, Ehud Reiter, Nava
Tintarev, and Annalu Waller. 2010. Using NLG and
Sensors to Support Personal Narrative for Children
with Complex Communication Needs. SLPAT ?10:
Proceedings of the NAACL HLT 2010 Workshop on
Speech and Language Processing for Assistive Tech-
nologies.
Michelene T.H. Chi, Stephanie A. Siler, Heisawn
Jeong, Takashi Yamauchi, Robert G. Hausmann.
2001. Learning from human tutoring. Journal of
Cognitive Science, 25(4):471-533.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
Barry Gholson. 2004. Affect and learning: an ex-
ploratory look into the role of affect in learning with
AutoTutor. Journal of Educational Media, 29:241-
250.
Nina Dethlefs and Heriberto Cuayahuitl. 2011.
Combining hierarchical reinforcement learning and
bayesian networks for natural language generation
in situated dialogue. ENLG ?11: Proceedings of the
13th European Workshop on Natural Language Gen-
eration.
Barbara Fox. 1993. The Human Tutorial Dialogue
Project: Issues in the Design of Instructional Sys-
tems. Lawrence Erlbaum Associates, Hillsdale,
New Jersey.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood,Wendy Moncur, and So-
mayajulu Sripada. 2009. From Data to Text in the
Neonatal Intensive Care Unit: Using NLG Technol-
ogy for Decision Support and Information Manage-
ment. Journal of AI Communications, 22:153-186.
Higher Education Academy. 2009. Providing individ-
ual written feedback on formative and summative
assessments. http://www.heacademy.
ac.uk/assets/documents/resources/
database/id353_senlef_guide.pdf.
Last modified September 16.
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. BT-Nurse:
Computer Generation of Natural Language Shift
Summaries from Complex Heterogeneous Medical
Data. Journal of the American Medical Informatics
Association,18:621-624.
Srinivasan Janarthenam, Helen Hastie, Oliver Lemon,
Xingkun Liu. 2011. ?The day after the day after to-
morrow?? A machine learning approach to adaptive
temporal expression generation: training and evalu-
ation with real users. SIGDIAL ?11: Proceedings of
the SIGDIAL 2011 Conference.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive Referring Expression Generation in Spo-
ken Dialogue Systems: Evaluation with Real Users.
SIGDIAL ?10: Proceedings of the 11th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue.
Nicholas A. R. Johnson and David M. Lane. 2011.
Narrative Monologue as a First Step Towards Ad-
vanced Mission Debrief for AUV Operator Situa-
tional Awareness. In the 15th International Confer-
ence on Advanced Robotics.
Ioannis Konstas and Mirella Lapata. 2012. Unsuper-
vised concept-to-text generation with hypergraphs.
NAACL HLT ?12: Proceedings of the 2012 Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies.
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan and
Arthur C. Graesser. 1995. Pragmatics and Peda-
gogy: Conversational Rules and Politeness Strate-
gies May Inhibit Effective Tutoring. Journal of Cog-
nition and Instruction, 13(2):161-188.
Kaska Porayska-Pomsta and Chris Mellish. 2013.
Modelling human tutors? feedback to inform natural
language interfaces for learning. International Jour-
nal of Human-Computer Studies,71(6):703724.
123
Ehud Reiter and Robert Dale. 2000. Building Natural
Language Generation systems. Cambridge Univer-
sity Press.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising Information Presentation for Spoken Di-
alogue Systems. ACL ?10: Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
Somayajulu Sripada, Ehud Reiter, I Davy, and K
Nilssen. 2004. Lessons from Deploying NLG Tech-
nology for Marine Weather Forecast Text Gener-
ation. In Proceedings of PAIS session of ECAI-
2004:760-764.
Somayajulu Sripada, Ehud Reiter, Jim Hunter, and Jin
Yu. 2003. Generating English Summaries of Time
Series Data using the Gricean Maxims. KDD ?03:
Proceedings of the ninth ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment Learning. MIT Press.
124
Proceedings of the SIGDIAL 2013 Conference, pages 151?153,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A Multithreaded Conversational Interface for Pedestrian Navigation and
Question Answering
Srinivasan Janarthanam1, Oliver Lemon1, Xingkun Liu1, Phil Bartie2,
William Mackaness2, Tiphaine Dalmas3
1Interaction Lab, Heriot-Watt University, Edinburgh
2 School of GeoSciences, University of Edinburgh
3School of Informatics, University of Edinburgh
sc445,o.lemon,x.liu@hw.ac.uk, philbartie@gmail.com,
william.mackaness@ed.ac.uk, tiphaine.dalmas@aethys.com
Abstract
We demonstrate a conversational interface
that assists pedestrian users in navigat-
ing within urban environments and acquir-
ing tourist information by combining spo-
ken dialogue system, question-answering
(QA), and geographic information sys-
tem (GIS) technologies. In contrast to
existing mobile applications which treat
these problems independently, our An-
droid agent addresses the problem of navi-
gation and touristic question-answering in
an integrated fashion using a shared dia-
logue context with multiple interleaved di-
alogue threads. In this paper, we present
the architecture and features of our lat-
est system, extended from an earlier ver-
sion which was built and evaluated with
real users (Janarthanam et al, 2013). The
new features include navigation based on
visible landmarks, navigation adapted to
the user?s previous route knowledge, and
tourist information pushing based on vis-
ible and proximal points-of-interest. The
system also uses social media to infer
?popularity? of geographical entities.
1 Introduction
We demonstrate a conversational interface that ad-
dresses the problems of pedestrian navigation and
Question Answering (QA) in urban environments,
which is an extended version of the system eval-
uated in (Janarthanam et al, 2013). Our system
brings together Spoken Dialogue Systems (SDS)
and Question Answering (QA) with Geographic
Information Systems (GIS) for user positioning
and tracking, city modelling and urban visibility
modelling. There has been little prior work that
addresses these problems - navigation, visibility,
and tourist information provision - in an integrated
way. By navigation, we refer to the problem of
finding appropriate destinations to go to and the
task of wayfinding to reach them (Golledge, 1992;
Raubal and Winter, 2002) and by tourist informa-
tion provision we refer to the problem of meet-
ing the informational needs of a user about enti-
ties such as museums, statues, famous personal-
ities, etc. With the proliferation of smartphones,
there has been a number of mobile apps developed
to address these problems. However, these apps
have the following problems: first, they demand
the user?s visual attention because they predom-
inantly present information on a mobile screen.
This can be dangerous in urban environments, as
well as being distracting. Second, these apps ad-
dress the problems of navigation and tourist infor-
mation independently and do not employ a shared
interaction context. This means that users cannot
switch between tourist information and navigation
tasks in a natural and fluid manner.
User1: Take me to the National Museum.
System2: The National Museum is about 300 meters away ...
System3: At the KFC, turn left on to South Bridge ...
System4 : Near you is the statue of David Hume.
User2: Who is David Hume.
System5: David Hume was a Scottish philosopher....
User3: Tell me more about David Hume.
System6: He was one of the most important figures in.. ...
System7: You should be able to see the museum in front of you.
User4: Tell me more about the museum.
System8: The National Museum of Scotland is a....
Table 1: An example interaction with SpaceBook
In contrast to existing mobile apps, we present
a system that has a speech-only interface and ad-
dresses navigation and QA in an integrated way.
Table 1 presents an example interaction with
our system showing the integrated use of navi-
gation and question-answering capabilities. Ut-
151
terance System4-8 shows the system?s capability
to push information from the QA system about
nearby points-of-interest (PoI) during a navigation
task, which the user can then follow-up on (in ut-
terances User2 and User3). The final 3 utterances
show a natural switch between navigation to an en-
tity and QA about that entity.
2 Related work
Mobile applications such as Siri, Google Maps
Navigation, Sygic, etc. address the problem of
navigation while applications like Triposo, Guide-
pal, Wikihood, etc. address the problem of tourist
information by presenting the user with descrip-
tive information about various points of interest
(PoI) in the city. While some exploratory applica-
tions present snippets of information about a pre-
compiled list of PoI, others applications dynam-
ically generate a list of PoI arranged based on
their proximity to the users. Users can also ob-
tain specific information about PoI using Search
applications. Also, since these navigation and ex-
ploratory/search applications do not address both
problems in an integrated way, users need to
switch between them and therefore lose interac-
tion context.
While most applications address these two
problems independently, some like Google Now,
Google Field Trip, etc, mix navigation with ex-
ploration. However, such applications present in-
formation primarily visually on the screen for the
user to read. In contrast, our system has the objec-
tive of keeping the user?s cognitive load low and
preventing users from being distracted (perhaps
dangerously so) from walking in the city (Kray et
al., 2003). Also, our system allows users to inter-
leave the two sub-tasks seamlessly and can keep
entities discussed in both tasks in shared context
(as shown in Table 1).
Several systems have addressed the issue of
pedestrian navigation (Malaka and Zipf, 2000;
Dale et al, 2003; Heinroth and Buhler, 2008).
Some dialogue systems deal with presenting in-
formation concerning points of interest (Ko et al,
2005; Misu and Kawahara, 2007; Kashioka et al,
2011). In contrast to all these earlier work, we
demonstrate a system that deals with both naviga-
tion and tourist information issues in an integrated
fashion.
Figure 1: System Architecture
3 Multithreaded dialogue management
The architecture of the current system is shown
in figure 1. The Interaction Manager (IM) is
the central component of this architecture, which
provides the user with navigational instructions,
pushes PoI information and manages QA ques-
tions. It receives the user?s input in the form of
a dialogue act (DA) from the ASR module and
the user?s location (latitude and longitude), orien-
tation and speed from the Pedestrian Tracker mod-
ule. Based on these inputs and the dialogue con-
text, the IM responds with a system output dia-
logue act. The Interaction Manager manages the
conversation using five coversational threads: di-
alogue control, response, navigation, question an-
swering, and PoI pushing. These different threads
represent the state of different dimensions of the
user-system conversation that interleave with each
other. Each of these threads generates a dialogue
action based on a dialogue policy. A dialogue pol-
icy is a mapping between dialogue states and dia-
logue actions, which are semantic representations
of what the system wants to say next. Dialogue
actions from the five threads are stored in five sep-
arate queues.
The queues are assigned priorities that decide
the order in which items from the queues will
be popped. For instance, informing the user of
a PoI could be delayed if the user needs to be
given an instruction to turn at the junction he is
approaching. For this reason, priority is assigned
to dialogue threads as follows.
Priority 1. Dialogue control (calibration phase,
repeat request, clarifications etc)
Priority 2. Responding to user requests
Priority 3. System initiated navigation task actions
Priority 4. Responses to User initiated QA actions
Priority 5. PoI Push actions
152
Dialogue control The IM initiates the conversa-
tion with a calibration phase where the user?s ini-
tial location and orientation are obtained. In this
phase, the IM requests the user to walk a few yards
so that the pedestrian tracker can sense the user?s
location and orientation. During the course of the
coversation, the IM uses this thread to manage
repeat requests, issues with unparsed user utter-
ances, utterances that have low ASR confidence,
and so on. The dialogue control thread is used to
manage reference resolution in cases where refer-
ring expressions are underspecified.
Navigation The IM identifies the location of the
destination entity and queries the City Model for a
route plan. The plan provides information such as
numbers of exits at junctions, the exit number the
user should take, turn angle, popularity index of
the street, and the slope of the road. In an attempt
to adapt the route instructions to user route knowl-
edge, the IM first picks the most popular street in
the plan and asks the users if they can get to the
street on their own. Also, the IM queries the Visi-
bility Engine (VE) for highly salient visible land-
marks (computed using Flickr tags) that can used
to direct the user. Instructions based on visible
landmarks are given whenever possible.
Question Answering The system also answers
ad hoc questions from the user (e.g. ?Who is David
Hume??, ?What is the Old College??, etc). These
are sent to the QA server and answered based on
responses from the QA server. The dialogue pol-
icy here is to answer the user?s question with the
first snippet available and ask the user to request
for more if interested.
Pushing PoI Information When the user is mo-
bile, the IM identifies points of interest on the
route based on two factors: proximity and visibil-
ity. Proximity push is done by checking for PoIs
near the user using high-scoring ones when there
are many, based on tourist popularity ratings in the
City Model. Visibility push is done by querying
the VE for salient entities visible to the user that
may be worth pushing. The dialogue policy is to
introduce the PoI entity along with visual descrip-
tors if available. The IM queries the QA server for
snippets on entity and if available, pushes them the
first snippet to the user. The user is encouraged to
ask for more if interested.
4 Conclusion
We demonstrate a mobile conversational system
to support pedestrian users in navigation and
question-answering tasks in urban environments.
The system is a speech-only interface and inter-
leaves navigation and tourist information in an in-
tegrated way, using a shared dialogue context. For
example, using the navigational context, our sys-
tem can push point-of-interest information which
can then initiate touristic exploration tasks using
the QA module. An evaluation of an earlier ver-
sion was reported in (Janarthanam et al, 2013).
Acknowledgments
The research leading to these results was funded by the Eu-
ropean Commission?s Framework 7 programme under grant
agreement no. 270019 (SPACEBOOK project).
References
R. Dale, S. Geldof, and J. Prost. 2003. CORAL : Using Nat-
ural Language Generation for Navigational Assistance. In
Proceedings of ACSC2003, South Australia.
R. G. Golledge. 1992. Place recognition and wayfinding:
Making sense of space. Geoforum, 23.
T. Heinroth and D. Buhler. 2008. Arrigator: evaluation of
a speech-based pedestrian navigation system. In Proceed-
ings of 4th International Conference on Intelligent Envi-
ronments, 2008.
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dick-
inson, X. Liu, W. Mackaness, and B. Webber. 2013.
Evaluating a city exploration dialogue system combining
question-answering and pedestrian navigation. In Proc.
ACL 2013.
H. Kashioka, T. Misu, E. Mizukami, Y. Shiga, K. Kayama,
C. Hori, and H. Kawai. 2011. Multimodal Dialog System
for Kyoto Sightseeing Guide. In Asia-Pacific Signal and
Information Processing Association Conference.
J. Ko, F. Murase, T. Mitamura, E. Nyberg, M. Tateishi,
I. Akahori, and N. Hataoka. 2005. CAMMIA: A Context-
Aware Spoken Dialog System for Mobile Environments.
In IEEE Automatic Speech Recognition and Understand-
ing Workshop.
C. Kray, K. Laakso, C. Elting, and V. Coors. 2003. Present-
ing route instructions on mobile devices. In Proceedings
of IUI 03, Florida.
R. Malaka and A. Zipf. 2000. Deep Map - challenging IT
research in the framework of a tourist information sys-
tem. In Information and Communication Technologies in
Tourism 2000, pages 15?27. Springer.
T. Misu and T. Kawahara. 2007. An Interactive Framework
for Document Retrieval and Presentation with Question-
Answering Function in Restricted Domain. In Proc. of
the 26th IEA/AIE conference, pages 126?134.
M. Raubal and S. Winter. 2002. Enriching wayfinding in-
structions with local landmarks. In Second International
Conference GIScience. Springer, Boulder, USA.
153
Proceedings of the SIGDIAL 2013 Conference, pages 154?156,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Demonstration of the Parlance system: a data-driven,
incremental, spoken dialogue system for interactive search
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Gasic, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay, Boris Villazon-Terrazas, Steve Young
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
The Parlance system for interactive
search processes dialogue at a micro-
turn level, displaying dialogue phe-
nomena that play a vital role in hu-
man spoken conversation. These di-
alogue phenomena include more nat-
ural turn-taking through rapid sys-
tem responses, generation of backchan-
nels, and user barge-ins. The Par-
lance demonstration system differen-
tiates from other incremental systems
in that it is data-driven with an infras-
tructure that scales well.
1 Introduction
The Parlance system provides interactive
search through a Spoken Dialogue System
(SDS). This SDS aims to be incremental to al-
low for more natural spoken interaction. Tra-
ditionally, the smallest unit of speech process-
ing for interactive systems has been a full ut-
terance with strict, rigid turn-taking. The
Parlance architecture, however, is an incre-
mental framework that allows for processing
of smaller ?chunks? of user input, which en-
ables one to model dialogue phenomena such
as barge-ins and backchannels. This work is
carried out under the FP7 EC project Par-
lance 1, the goal of which is to develop inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
San Francisco. An example dialogue is given
in Table 1.
?Authors are in alphabetical order
1http://www.parlance-project.eu
SYS Thank you for calling the Parlance Restaurant
system. You may ask for information by cuisine
type, price range or area. How may I help you?
USR I want to find an Afghan restaurant.........which is
in the cheap price range.
SYS .......................................................[uhuhh]........
The Helmand Palace is a cheerful setting for au-
thentic Afghan cuisine.
USR What is the address and phone number?
SYS The address 2424 Van Ness Ave ....
Table 1: Example dialogue excerpt for restaurant in-
formation in San Francisco
2 Background
Previous work includes systems that can deal
with ?micro-turns? (i.e. sub-utterance process-
ing units), resulting in dialogues that are more
fluid and responsive. This has been backed up
by a large body of psycholinguistic literature
that indicates that human-human interaction
is in fact incremental (Levelt, 1989).
It has been shown that incremental dia-
logue behaviour can improve the user experi-
ence (Skantze and Schlangen, 2009; Baumann
et al, 2011; Selfridge et al, 2011) and en-
able the system designer to model several di-
alogue phenomena that play a vital role in
human discourse (Levelt, 1989) but have so
far been absent from systems. These dialogue
phenomena that will be demonstrated by the
Parlance system include more natural turn-
taking through rapid system responses, gener-
ation of backchannels and user barge-ins. The
system differentiates from other incremental
systems in that it is entirely data-driven with
an infrastructure that potentially scales well.
3 System Architecture
Figure 1 gives an overview of the Par-
lance system architecture, which maintains
154
LOCAL SEARCH ENGINE
AUTOMATIC SPEECH RECOGNITION
NLG
AUDIO I/O
TTS
BACKCHANNEL GENERATOR
IM
MIM
HUB
KNOWLEDGE BASE
WavePackets
1-Best Words
Segmentlabel
N-Best Phrase List
WavePackets
Micro-Turn Dialogue Act
System Dialogue Act
String Packets
StringPackets
VoIP Interface (PJSIP)
N-best Dialogue Act Units
 API call ( + metadata)
Search Response
Partial Dialogue Act (in case of interruption)
PartialString(in case of interruption)SPOKEN LANGUAGE UNDERSTANDING Decode from t0 to t1
Figure 1: Overview of the Parlance system
architecture
the modularity of a traditional SDS while at
the same time allowing for complex interaction
at the micro-turn level between components.
Each component described below makes use
of the PINC (Parlance INCremental) dialogue
act schema. In this scheme, a complete dia-
logue act is made up of a set of primitive di-
alogue acts which are defined as acttype-item
pairs. The PINC dialogue act scheme supports
incrementality by allowing SLU to incremen-
tally output primitive dialogue acts whenever
a complete acttype-item pair is recognised with
sufficient confidence. The complete dialogue
act is then the set of these primitive acts out-
put during the utterance.
3.1 Recognition and Understanding
The Automatic Speech Recogniser (ASR) and
Spoken Language Understanding (SLU) com-
ponents operate in two passes. The audio in-
put is segmented by a Voice Activity Detec-
tor and then coded into feature vectors. For
the first pass of the ASR2, a fast bigram de-
coder performs continuous traceback generat-
ing word by word output. During this pass,
while the user is speaking, an SLU module
called the ?segment decoder? is called incre-
2http://mi.eng.cam.ac.uk/research/dialogue/
ATK_Manual.pdf
mentally as words or phrases are recognised.
This module incrementally outputs the set of
primitive dialogue acts that can be detected
based on each utterance prefix. Here, the ASR
only provides the single best hypothesis, and
SLU only outputs a single set of primitive dia-
logue acts, without an associated probability.
On request from the Micro-turn Interaction
Manager (MIM), a second pass can be per-
formed to restore the current utterance using a
trigram language model, and return a full dis-
tribution over the complete phrase as a con-
fusion network. This is then passed to the
SLU module which outputs the set of alter-
native complete interpretations, each with its
associated probability, thus reflecting the un-
certainty in the ASR-SLU understanding pro-
cess.
3.2 Interaction Management
Figure 1 illustrates the role of the Micro-turn
Interaction Manager (MIM) component in the
overall Parlance architecture. In order to
allow for natural interaction, the MIM is re-
sponsible for taking actions such as listening to
the user, taking the floor, and generating back-
channels at the micro-turn level. Given various
features from different components, the MIM
selects a micro-turn action and sends it to the
IM and back-channel generator component to
generate a system response.
Micro-turn Interaction Manager A
baseline hand-crafted MIM was developed
using predefined rules. It receives turn-taking
information from the TTS, the audio-output
component, the ASR and a timer, and updates
turn-taking features. Based on the current
features and predefined rules, it generates
control signals and sends them to the TTS,
ASR, timer and HUB. In terms of micro-turn
taking, for example, if the user interrupts
the system utterance, the system will stop
speaking and listen to the user. The system
also outputs a short back-channel and stays in
user turn state if the user utterance provides
limited information.
Interaction Manager Once the MIM has
decided when the system should take the floor,
it is the task of the IM to decide what to say.
The IM is based on the partially observable
155
Markov decision process (POMDP) frame-
work, where the system?s decisions can be op-
timised via reinforcement learning. The model
adopted for Parlance is the Bayesian Update
of Dialogue State (BUDS) manager (Thom-
son and Young, 2010). This POMDP-based
IM factors the dialogue state into condition-
ally dependent elements. Dependencies be-
tween these elements can be derived directly
from the dialogue ontology. These elements
are arranged into a dynamic Bayesian network
which allows for their marginal probabilities
to be updated during the dialogue, compris-
ing the belief state. The belief state is then
mapped into a smaller-scale summary space
and the decisions are optimised using the nat-
ural actor critic algorithm.
HUB The HUB manages the high level flow
of information. It receives turn change infor-
mation from the MIM and sends commands
to the SLU/IM/NLG to ?take the floor? in the
conversation and generate a response.
3.3 Generation and TTS
We aim to automatically generate language,
trained from data, that is (1) grammatically
well formed, (2) natural, (3) cohesive and (4)
rapidly produced at runtime. Whilst the first
two requirements are important in any dia-
logue system, the latter two are key require-
ments for systems with incremental processing,
in order to be more responsive. This includes
generating back-channels, dynamic content re-
ordering (Dethlefs et al, 2012), and surface
generation that models coherent discourse phe-
nomena, such as pronominalisation and co-
reference (Dethlefs et al, 2013). Incremen-
tal surfacce generation requires rich context
awareness in order to keep track of all that has
been generated so far. We therefore treat sur-
face realisation as a sequence labelling task and
use Conditional Random Fields (CRFs), which
take semantically annotated phrase structure
trees as input, in order to represent long dis-
tance linguistic dependencies. This approach
has been compared with a number of compet-
itive state-of-the art surface realisers (Deth-
lefs et al, 2013), and can be trained from
minimally labelled data to reduce development
time and facilitate its application to new do-
mains.
The TTS component uses a trainable HMM-
based speech synthesizer. As it is a paramet-
ric model, HMM-TTS has more flexibility than
traditional unit-selection approaches and is es-
pecially useful for producing expressive speech.
3.4 Local Search and Knowledge Base
The domain ontology is populated by the local
search component and contains restaurants in
5 regional areas of San Francisco. Restaurant
search results are returned based on their lon-
gitude and latitude for 3 price ranges and 52
cuisine types.
4 Future Work
We intend to perform a task-based evaluation
using crowd-sourced users. Future versions
will use a dynamic Knowledge Base and User
Model for adapting to evolving domains and
personalised interaction respectively.
Acknowledgements
The research leading to this work was funded by the EC
FP7 programme FP7/2011-14 under grant agreement
no. 287615 (PARLANCE).
References
T. Baumann, O. Buss, and D. Schlangen. 2011. Eval-
uation and Optimisation of Incremental Processors.
Dialogue and Discourse, 2(1).
Nina Dethlefs, Helen Hastie, Verena Rieser, and Oliver
Lemon. 2012. Optimising Incremental Generation
for Spoken Dialogue Systems: Reducing the Need
for Fillers. In Proceedings of INLG, Chicago, USA.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and O. Lemon.
2013. Conditional Random Fields for Responsive
Surface Realisation Using Global Features. In Pro-
ceedings of ACL, Sofia, Bulgaria.
W. Levelt. 1989. Speaking: From Intenion to Articu-
lation. MIT Press.
E. Selfridge, I. Arizmendi, P. Heeman, and J. Williams.
2011. Stability and Accuracy in Incremental Speech
Recognition. In Proceedings of SIGDIAL, Portland,
Oregon.
G. Skantze and D. Schlangen. 2009. Incremental Dia-
logue Processing in a Micro-Domain. In Proceedings
of EACL, Athens, Greece.
B Thomson and S Young. 2010. Bayesian update of
dialogue state: A POMDP framework for spoken
dialogue systems. Computer Speech and Language,
24(4):562?588.
156
Proceedings of the SIGDIAL 2013 Conference, pages 223?232,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Training and evaluation of an MDP model for social multi-user
human-robot interaction
Simon Keizer, Mary Ellen Foster,
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh (UK)
{s.keizer,m.e.foster,o.lemon}@hw.ac.uk
Andre Gaschler, Manuel Giuliani
fortiss GmbH
Munich (Germany)
{gaschler,giuliani}@fortiss.org
Abstract
This paper describes a new approach to
automatic learning of strategies for social
multi-user human-robot interaction. Us-
ing the example of a robot bartender that
tracks multiple customers, takes their or-
ders, and serves drinks, we propose a
model consisting of a Social State Recog-
niser (SSR) which processes audio-visual
input and maintains a model of the social
state, together with a Social Skills Execu-
tor (SSE) which takes social state updates
from the SSR as input and generates robot
responses as output. The SSE is modelled
as two connected Markov Decision Pro-
cesses (MDPs) with action selection poli-
cies that are jointly optimised in interaction
with a Multi-User Simulation Environment
(MUSE). The SSR and SSE have been in-
tegrated in the robot bartender system and
evaluated with human users in hand-coded
and trained SSE policy variants. The re-
sults indicate that the trained policy out-
performed the hand-coded policy in terms
of both subjective (+18%) and objective
(+10.5%) task success.
1 Introduction
As the use of robot technology in the home as well
as in public spaces is increasingly gaining attention,
the need for effective and robust models for natural
and social human robot interaction becomes more
important. Whether it involves robot companions
(Vardoulakis et al, 2012), game-playing robots
(Klotz et al, 2011; Brooks et al, 2012; Cuaya?huitl
and Kruijff-Korbayova?, 2012), or robots that help
people with exercising (Fasola and Mataric, 2013),
human users should be able to interact with such
service robots in an effective and natural way, us-
ing speech as well as other modalities of commu-
nication. Furthermore, with the emergence of new
application domains there is a particular need for
methods that enable rapid development of mod-
els for such new domains. In this respect, data-
driven approaches are appealing for their capability
to automatically exploit empirical data to arrive at
realistic and effective models for interpreting user
behaviour, as well as to learn strategies for effective
system behaviour.
In spoken dialogue systems research, statisti-
cal methods for spoken language understanding,
dialogue management, and natural language gen-
eration have proven to be feasible for effective
and robust interactive systems (Rieser and Lemon,
2011; Lemon and Pietquin, 2012; Young et al,
2010; Young et al, 2013). Although such methods
have recently also been applied to (multi-modal)
human-robot interaction (Stiefelhagen et al, 2007;
Cuaya?huitl et al, 2012), work on multi-user human-
robot interaction has been limited to non-statistical,
hand-coded models (Klotz et al, 2011).
On the other hand, substantial work has been
done in the field of situated multi-party interaction
in general, including data-driven approaches. In
particular, Bohus & Horvitz (2009) have addressed
the task of recognising engagement intentions using
online learning in the setting of a screen-based em-
bodied virtual receptionist, and have also worked
on multi-party turn-taking in this context (Bohus
and Horvitz, 2011).
In this paper we describe a statistical approach
to automatic learning of strategies for selecting ef-
fective as well as socially appropriate robot actions
in a multi-user context. The approach has been de-
veloped using the example of a robot bartender (see
Figure 1) that tracks multiple customers, takes their
orders, and serves drinks. We propose a model con-
sisting of a Social State Recogniser (SSR) which
processes audio-visual input and maintains a model
of the social state, and a Social Skills Executor
(SSE) which takes social state updates from the
SSR as input and generates robot responses as out-
223
put. The SSE is modelled as a hierarchy of two con-
nected Markov Decision Processes (MDPs) with
action selection policies that are jointly optimised
in interaction with a Multi-User Simulation Envi-
ronment (MUSE).
Figure 1: The robot bartender with two customers
In the remainder of this paper we will describe
the robot system in more detail (Section 2), fol-
lowed by descriptions of the SSR (Section 3), the
SSE (Section 4), and MUSE (Section 5). In Sec-
tion 6 we then discuss in more detail the MDP
model for the SSE and the process of jointly opti-
mising the policies, and present evaluation results
on simulated data. Next, we present results of the
first evaluation of the integrated SSE-MDP compo-
nent with human users (Section 7). The paper is
concluded in Section 8.
2 Robot bartender system
The robot system we used for evaluating the models
is equipped with vision and speech input processing
modules, as well as modules controlling two robot
arms and a talking head. Based on observations
about the users in the scene and their behaviour, the
system must maintain a model of the social context,
and decide on effective and socially appropriate
responses in that context. Such a system must be
able to engage in, maintain, and close interactions
with users, take a user?s order by means of a spoken
conversation, and serve their drinks. The overall
aim is to generate interactive behaviour that is both
task- effective and socially appropriate: in addition
to efficiently taking orders and serving drinks, the
system should, e.g., deal with customers on a first-
come, first-served basis, and should manage the
customers? patience by asking them politely to wait
until the robot is done serving another customer.
As shown in Figure 1, the robot hardware con-
sists of a pair of manipulator arms with grippers,
mounted to resemble human arms, along with
an animatronic talking head capable of produc-
ing facial expressions, rigid head motion, and lip-
synchronised synthesised speech. The input sen-
sors include a vision system which tracks the loca-
tion, facial expressions, gaze behaviour, and body
language of all people in the scene in real time
(Pateraki et al, 2013), along with a linguistic pro-
cessing system (Petrick et al, 2012) combining a
speech recogniser with a natural-language parser
to create symbolic representations of the speech
produced by all users. More details of the architec-
ture and components are provided in (Foster et al,
2012). An alternative embodiment of the system is
also available on the NAO platform.
3 Social State Recogniser
The primary role of the Social State Recogniser
(SSR) is to turn the continuous stream of messages
produced by the low-level input and output com-
ponents of the system into a discrete representa-
tion of the world, the robot, and all entities in the
scene, integrating social, interaction-based, and
task-based properties. The state is modelled as a
set of relations such as facePos(A)=(x,y,z) or
closeToBar(A); see (Petrick and Foster, 2013)
for details on the representation used.
In addition to storing all of the low-level sensor
information, the SSR also infers additional rela-
tions that are not directly reported by the sensors.
For example, it fuses information from vision and
speech to determine which user should be assigned
to a recognised spoken contribution. It also pro-
vides a constant estimate of whether each customer
is currently seeking attention from the bartender
(seeksAttention(A)): the initial version of this
estimator used a hand-coded rule based on the ob-
servation of human behaviour in real bars (Huth
et al, 2012), while a later version (Foster, 2013)
makes use of a supervised learning classifier trained
on labelled recordings of humans interacting with
the first version of the robot bartender.
The SSR provides a query interface to allow
other system components access to the relations
stored in the state, and also publishes an updated
state to the SSE every time there is a change which
might require a system action in response (e.g.,
a customer appears, begins seeking attention, or
makes a drink order).
224
4 Social Skills Executor
The Social Skills Executor (SSE) controls the be-
haviour of the robot system, based on the social
state updates it receives from the SSR. The out-
put of the SSE consists of a combination of non-
communicative robot actions and/or communica-
tive actions with descriptions of their multi-modal
realisations. In the bartender domain, the non-
communicative actions typically involve serving
a specific drink to a specific user, whereas the com-
municative actions have the form of dialogue acts
(Bunt et al, 2010), directed at a specific user, e.g.
setQuestion(drink) (?What would you like to
drink??) or initialGreeting() (?Hello?).
In our design of the SSE, the decision making
process resulting in such outputs (including the ?no
action? output) consists of three stages: 1) social
multi-user coordination: managing the system?s
engagement with the users present in the scene (e.g.,
accept a user?s bid for attention, or proceed with an
engaged user), 2) single-user interaction: if pro-
ceeding with an engaged user, generating a high-
level response to that user, in the form of a com-
municative act or physical action (e.g., greeting the
user or serving him a drink), and 3) multi-modal
fission: selecting a combination of modalities for
realising a chosen response (e.g., a greeting can be
realised through speech and/or a nodding gesture).
One advantage of such a hierarchical design is that
strategies for the different stages can be developed
independently. Another is that it makes automatic
policy optimisation more scalable.
5 Multi-User Simulated Environment
In order to test and evaluate the SSE, as well as to
train SSE action selection policies, we developed
a Multi-User Simulated Environment (MUSE).
MUSE allows for rapidly exploring the large space
of possible states in which the SSE must select
actions. A reward function that incorporates in-
dividual rewards from all simulated users in the
environment is used to encode preferred system
behaviour in a principled way. A simulated user
assigns a reward if they are served the correct drink,
and gives penalties associated with their waiting
time and various other forms of undesired system
responses (see Section 6.1 for more details about
the reward function). All of this provides a practi-
cal platform for evaluating different strategies for
effective and socially appropriate behaviour. It also
paves the way for automatic optimisation of poli-
cies, for example by using reinforcement learning
techniques, as we will discuss in Section 6.1.
The simulated environment replaces the vision
and speech processing modules in the actual robot
bartender system, which means that it generates 1)
vision signals in every time-frame, and 2) speech
processing results, corresponding to sequences of
time-frames where a user spoke. The vision obser-
vations contain information about users that have
been detected, where they are in the scene, whether
they are speaking, and where their attention is di-
rected to. Speech processing results are represented
semantically, in the form of dialogue acts (e.g.,
inform(drink=coke), ?I would like a coke?). As
described in Section 3, the SSR fuses the vision and
speech input, for example to associate an incoming
dialogue act with a particular user.
The simulated signals are the result of combin-
ing the output from the simulated users in the en-
vironment. Each simulated user is initialised with
a random goal (in our domain a type of drink they
want to order), enters the scene at some point, and
starts bidding for attention at some point. Each
simulated user also maintains a state and gener-
ates responses given that state. These responses
include communicative actions directed at the bar-
tender, which are translated into a multi-channel
vision input stream processed by the SSR, and, in
case the user realises the action through speech,
a speech processing event after the user has fin-
ished speaking. Additionally, the simulated users
start with a given patience level, which is reduced
in every frame that the user is bidding for atten-
tion or being served by the system. If a user?s pa-
tience has reduced to zero, s/he gives up and leaves
the bar. However, it is increased by a given fixed
amount when the system politely asks the user to
wait, encoded as a pausing dialogue act. The be-
haviour of the simulated users is partly controlled
by a set of probability distributions that allow for
a certain degree of variation. These distributions
have been informed by statistics derived from a
corpus of human-human customer-bartender inter-
actions (Huth et al, 2012).
In addition to information about the simulated
users, MUSE also provides feedback about the
execution of robot actions to the SSR, in partic-
ular the start and end of all robot speech and non-
communicative robot actions. This type of informa-
tion simulates the feedback that is also provided in
the actual bartender system by the components that
directly control the robot head and arms. Figure 2
225
Figure 2: Social state recognition and social skills execution in a multi-user simulated environment.
shows the architecture of the system interacting
with the simulated environment.
6 MDP model for multi-user interaction
To enable automatic optimisation of strategies for
multi-user social interaction, the SSE model as de-
scribed in Section 4 was cast as a hierarchy of two
Markov Decision Processes (MDPs), correspond-
ing to the social multi-user coordination and single-
user interaction stages of decision making. Both
MDPs have their own state spaces S1 and S2, each
defined by a set of state features, extracted from
the estimated social state made available by the
SSR?see Tables 1 and 3. They also have their own
action setsA1 andA2, corresponding to the range
of decisions that can be made at the two stages (Ta-
bles 2 and 4), and two policies pi1 : S1 ? A1 and
pi2 : S2 ? A2, mapping states to actions.
6.1 Policy optimisation
Using the MDP model as described above, we
jointly optimise the two policies, based on the re-
wards received through the SSR from the simulated
environment MUSE. Since MUSE gives rewards
on a frame-by-frame basis, they are accumulated
in the social state until the SSR publishes a state
update. The SSE stores the accumulated reward
together with the last state encountered and action
taken in that state, after which that reward is reset
in the social state. After each session (involving
interactions with two users in our case), the set
of encountered state-action pairs and associated
rewards is used to update the policies.
The reward provided by MUSE in each frame
is the sum of rewards Ri given by each individual
simulated user i, and a number of general penalties
arising from the environment as a whole. User
rewards consist of a fixed reward in case their goal
is satisfied (i.e., when they have been served the
drink they wanted and ordered), a penalty in case
they are still waiting to be served, a penalty in case
they are engaged with the system but have not been
served their drink yet, and additional penalties, for
example when the system turns his attention to
another user when the user is still talking to it, or
when the system serves a drink before the user has
ordered, or when the system serves another drink
when the user already has been served their drink.
General penalties are given for example when the
system is talking while no users are present.
The policies are encoded as functions that assign
a value to each state-action pair; these so-called
Q-values are estimates of the long-term discounted
cumulative reward. Given the current state, the
policy selects the action with the highest Q-value:
pi(s) = arg max
a
Q(s, a) (1)
Using a Monte-Carlo Control algorithm (Sutton
and Barto, 1998), the policies are optimised by
running the SSR and SSE against MUSE and using
the received reward signal to update the Q-values
after each interaction sequence. During training,
the SSE uses an -greedy policy, i.e., it takes a
random exploration action with probability  = 0.2.
226
Index Feature Values
4 ? i Interaction status for user i + 1 nonEngaged/seeksAttention/engaged
4 ? i + 1 Location of user i + 1 notPresent/!closeToBar/closeToBar
4 ? i + 2 User i + 1 was served a drink no/yes
4 ? i + 3 User i + 1 asked to wait no/yes
Table 1: State features for the social multi-user coordination policy. For each user, 4 features are included
in the state space, resulting in 32 ? 22 = 36 states for interactions with up to 1 user, increasing to 1296
states for interactions with up to 2 users and 46, 656 states for up to 3 users.
Index Action
0 No action
3 ? i + 1 Ask user i + 1 to wait
3 ? i + 2 Accept bid for attention from user i + 1
3 ? i + 3 Proceed interaction with (engaged) user i + 1
Table 2: Actions for the social multi-user coordination policy.
In the policy update step, a discount factor ? = 0.95
is used, which controls the impact that rewards
received later in a session have on the value of state-
action pairs encountered earlier in that session.
Figure 3 shows the learning curve of a joint
policy optimisation, showing average rewards ob-
tained after running the SSE with trained policies
for 500 runs, at several stages of the optimisation
process (after every 2500 sessions/runs/iterations,
the trained policy was saved for evaluation). In this
particular setup, simulated users gave a reward of
550 upon goal completion but in the total score this
is reduced considerably due to waiting time (-2 per
frame), task completion time (-1 per frame) and
various other potential penalties. Also indicated
are the performance levels of two hand-coded SSE
policies, one of which uses a strategy of asking a
user to wait when already engaged with another
user (labelled HDC), and one in which that second
user is ignored until it is done with the engaged user
(labelled HDCnp). The settings for user patience
as discussed in Section 5 determine which of these
policies works best; ideally these settings should be
derived from data if available. Nevertheless, even
with the hand-coded patience settings, the learning
curve indicates that both policies are outperformed
in simulation after 10k iterations, suggesting that
the best strategy for managing user patience can be
found automatically.
7 Human user evaluation
The SSE described above has been integrated in
the full robot bartender system and evaluated for
the first time with human users. In the experiment,
both a hand-coded version and a trained version
of the SSE component were tested; see Table 6 in
Appendix A for the trajectory of state-action pairs
of an example session. The hand-coded version
uses the policy labelled HDC, not HDCnp (see
Section 6.1). In each of the sessions carried out, one
recruited subject and one confederate (one of the
experimenters) approached the bartender together
as clients and both tried to order a drink (coke or
lemonade). After each interaction, the subject filled
out the short questionnaire shown in Figure 4.
Q1: Did you successfully order a drink from the bartender?
[Y/N]
Please state your opinion on the following statements:
[ 1:strongly disagree; 2:disagree; 3:slightly disagree;
4:slightly agree; 5:agree; 6:strongly agree ]
Q2: It was easy to attract the bartender?s attention [1?6]
Q3: The bartender understood me well [1?6]
Q4: The interaction with the bartender felt natural [1?6]
Q5: Overall, I was happy about the interaction [1?6]
Figure 4: Questionnaire from the user study.
37 subjects took part in this study, resulting in a
total of 58 recorded drink-ordering interactions:
29 that used the hand-coded SSE for interaction
management, and 29 that used the trained SSE.
The results from the experiment are summarised
in Table 5. We analysed the results using a linear
mixed model, treating the SSE policy as a fixed fac-
tor and the subject ID as a random factor. Overall,
the pattern of the subjective scores suggests a slight
preference for the trained SSE version, although
227
Index Feature Values
0 Reactive pressure none/thanking/greeting/goodbye/apology
1 Status of user goal unknown/usrInf/sysExpConf/sysImpConf/
grounded/drinkServed/sysAsked
2 Own proc. state none/badASR
Table 3: State features for the single-user interaction policy. In this case, there are 5 ? 7 ? 2 = 70 states.
Index Action Example
0 No action
1 returnGreeting() ?Hello?
2 autoPositive() ?Okay?
3 acceptThanking() ?You?re welcome?
4 autoNegative() ?What did you say??
5 setQuestion(drink) ?What drink would you like??
6 acceptRequest(drink=x) + serveDrink(x) ?Here?s your coke?
Table 4: Actions for the single-user interaction policy, which correspond to possible dialogue acts, except
for ?no action? and serving a drink. The specific drink types required for two of the actions are extracted
from the fully specified user goal in the social state maintained by the SSR.
only the difference in perceived success was statis-
tically significant at the p < 0.05 level. The actual
success rate of the trained policy was also some-
what higher, although not significantly so. Also,
the interactions with the trained SSE took slightly
longer than the ones with the hand-coded SSE in
terms of the number of system turns (i.e., the num-
ber of times the SSE receives a state update and
selects a response action, excluding the times when
it selects a non-action); however, this did not have
any overall effect on the users? subjective ratings.
The higher success rate for the trained SSE could
be partly explained by the fact that fewer ASR prob-
lems were encountered when using this version;
however, since the SSE was not triggered when a
turn was discarded due to low-confidence ASR, this
would not have had an effect on the number of sys-
tem turns. There was another difference between
the hand-coded and trained policies that could have
affected both the success rate and the number of
system turns: for interactions in which a user has
not ordered yet, nor been asked for their order, the
hand-coded strategy randomly chooses between
asking the user for their order and doing nothing,
letting the user take the initiative to place the order,
whereas the trained policy always asks the user for
their order (this action has the highest Q-value, al-
though in fact the value for doing nothing in such
cases is also relatively high).
We also carried out a stepwise multiple linear
regression on the data from the user experiment
to determine which of the objective measures had
the largest effect, as suggested by the PARADISE
evaluation framework (Walker et al, 2000). The re-
sulting regression functions are shown in Figure 5.
In summary, all of the subjective responses were
significantly affected by the objective task success
(i.e., the number of drinks served); the number of
low-ASR turns also affected most of the responses,
while various measures of dialogue efficiency (such
as the system response time and the time taken to
serve drinks) also had a significant impact. In gen-
eral, these regression functions explain between
15?25% of the variance in the subjective measures.
As an initial analysis of the validity of the sim-
ulated environment, we compared the state distri-
bution of the simulated data accumulated during
policy optimisation with that of the human user
evaluation data. In terms of coverage, we found
that only 46% of all states encountered in the real
data were also encountered during training. How-
ever, many of these states do not occur very often
and many of them do not require any action by
the robot (a trained policy can easily be set to take
no-action for unseen states). If we only include
states that have been encountered at least 20 times,
the coverage increases to over 70%. For states en-
countered at least 58 times, the coverage is 100%,
though admittedly this covers only the 10 most
frequently encountered states. The similarity of
the two distributions can be quantified by comput-
ing the KL-divergence, but since such a number is
228
Figure 3: Learning curve for joint optimisation of SSE-MDP policies.
System NS PSucc* PAtt PUnd PNat POv NDSrvd NST NBAsr
SSE-TRA 29 97% 4.10 4.21 3.00 3.83 1.97 (98.5%) 7.38 3.14
SSE-HDC 29 79% 4.14 3.83 2.93 3.83 1.76 (88.0%) 6.86 3.82
TOTAL 58 88% 4.12 4.02 2.97 3.83 1.86 (93.0%) 7.12 3.48
Table 5: Overview of system performance results from the experiment. In the leftmost column SSE-TRA
and SSE-HDC refer to the trained and hand-coded SSE versions; the column NS indicates the number of
sessions; the columns PSucc (perceived success), PAtt (perceived attention recognition), PUnd (perceived
understanding), PNat (perceived naturalness), and POv (perceived overall performance) give average
scores resulting from the 5 respective questionnaire questions; NDSrvd indicates the average number of
drinks served per session (out of 2 maximum ? the percentage is given in brackets); NST indicates the
average number of system turns per session; while NBAsr indicates the average number of cases where
the user speech was ignored because the ASR confidence was below a predefined threshold. The marked
column indicates that the difference between the two SSE versions was significant at the p < 0.05 level.
hard to interpret in itself, this will only be useful
if there were a state distribution from an alterna-
tive simulator or an improved version of MUSE for
comparison.
8 Conclusion
In this paper we presented a new approach to au-
tomatic learning of strategies for social multi-user
human-robot interaction, demonstrated using the
example of a robot bartender that tracks multiple
customers, takes their orders, and serves drinks.
We presented a model consisting of a Social State
Recogniser (SSR) which processes audio-visual in-
put and maintains a model of the social state, and
a Social Skills Executor (SSE) which takes social
state updates from the SSR as input and generates
robot responses as output. The main contribution
of this work has been a new MDP-based model
for the SSE, incorporating two connected MDPs
with action selection policies that are jointly op-
timised in interaction with a Multi-User Simula-
tion Environment (MUSE). In addition to showing
promising evaluation results with simulated data,
we also presented results from a first evaluation of
the SSE component with human users. The experi-
ments showed that the integrated SSE component
worked quite well, and that the trained SSE-MDP
achieved higher subjective and objective success
rates (+18% and +10.5% respectively).
Our model currently only utilises two policies,
but in more complex scenarios the task could be
further modularised and extended by introducing
more MDPs, for example for multimodal fission
and natural language generation. The approach of
using a hierarchy of MDPs has some similarity with
the Hierarchical Reinforcement Learning (HRL)
approach which uses a hierarchy of Semi-Markov
Decision Processes (SMDPs). In (Cuaya?huitl et al,
229
PSucc = 0.88 + 0.14 ? N(NDSrvd) ? 0.07 ? N(NBAsr) (r2 = 0.21)
PAtt = 4.12 + 0.76 ? N(NDSrvd) ? 0.46 ? N(RTm) ? 0.38 ? N(FDTm) (r2 = 0.22)
PUnd = 4.02 + 0.41 ? N(NDSrvd) ? 0.36 ? N(NBAsr) ? 0.40 ? N(NST) ? 0.41 ? N(RTm) ? 0.39 ? N(STm) (r2 = 0.24)
PNat = 2.97 + 0.36 ? N(NDSrvd) ? 0.29 ? N(NBAsr) ? 0.31 ? N(NST) ? 0.44 ? N(RTm) (r2 = 0.16)
POv = 3.83 + 0.65 ? N(NDSrvd) ? 0.38 ? N(NBAsr) ? 0.52 ? N(RTm) (r2 = 0.24)
Figure 5: PARADISE regression functions from the user study. The labels are the same as those in Table 5,
with the following additions: RTm is the mean system response time per user, STm is the mean serving
time per user, and FDTm is the mean time to serve the first drink; all times are measured in milliseconds.
N represents a Z score normalisation function (Cohen, 1995).
2012) for example, this hierarchy is motivated by
the identification of multiple tasks that the robot
can carry out and for which multiple SMDP agents
are defined. In every step of the interaction, control
lies with a single SMDP agent somewhere in the
hierarchy; once it arrives at its final state it returns
control to its parent SMDP. An additional transi-
tion model is introduced to permit switching from
an incomplete SMDP to another SMDP at the same
level, making interactions more flexible. In our ap-
proach, control always starts at the top level MDP
and lower level MDPs are triggered depending on
the action taken by their parent MDP. For social
interaction with multiple users, flexible switching
between interactions with different users is impor-
tant, so an arguably more sophisticated HRL ap-
proach to multi-user interaction will rely heavily
on the transition model. Another approach to mod-
ularising the task domain through multiple policies
is described in (Lison, 2011), where ?meta-control?
of the policies relies on an activation vector. As in
the HRL SMDP approach, this approach has not
been applied in the context of multi-user interaction.
In any case, a more thorough and possibly experi-
mental analysis comparing our approach with these
other approaches would be worth investigating.
In the future, we plan to extend our MDP model
to a POMDP (Partially Observable MDP) model,
taking uncertainty about both speech and visual
input into account in the optimisation of SSE poli-
cies by incorporating alternative hypotheses and
confidence scores provided by the input modules
into the social state. Since hand-coding strategies
becomes more challenging in the face of increased
uncertainty due to noisy input, the appeal of auto-
matic strategy learning in a POMDP framework
becomes even stronger. In a previous offline ver-
sion of our combined SSR and SSE, we have shown
in preliminary simulation experiments that even in
an MDP setting, an automatically trained SSE pol-
icy outperforms a hand-coded policy when noise is
added to the speech channel (Keizer et al, 2013).
Another direction of research is to annotate the
data collected in the described experiment for fur-
ther analysis and use it to improve the features of
the simulated environment. The improved models
should lead to trained policies that perform better
when evaluated again with human users. We will
also make use of the findings of the PARADISE
regression to fine-tune the reward function used
for policy optimisation: note that two of the main
features indicated by the PARADISE procedure?
task success and dialogue efficiency?are already
those included in the current reward function, and
we will add a feature to account for the effects of
ASR performance. We are also considering using
collected data for direct supervised or off-policy
reinforcement learning of SSE strategies.
Finally, we aim to extend our domain both in
terms of interactive capabilities (e.g., handling com-
munication problems, social obligations manage-
ment, turn-taking) and task domain (e.g., handling
more than the current maximum of 2 users, group
orders, orders with multiple items). In order to
make the (PO)MDP model more scalable and thus
keeping the learning algorithms tractable, we also
aim to incorporate techniques such as value func-
tion approximation into our model.
Acknowledgments
The research leading to these results has received
funding from the European Union?s Seventh Frame-
work Programme (FP7/2007?2013) under grant
agreement no. 270435, JAMES: Joint Action for
Multimodal Embodied Social Systems, http://
james-project.eu/. Thanks to Ingmar Kessler
for help in running the user experiment.
230
References
Dan Bohus and Eric Horvitz. 2009. Learning to pre-
dict engagement with a spoken dialog system in
open-world settings. In Proceedings SIGdial, Lon-
don, UK.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn
taking in situated dialog: Study, lessons, and direc-
tions. In Proceedings SIGdial, Portland, OR.
A. Brooks, J. Gray, G. Hoffman, A. Lockerd, H. Lee,
and C. Breazeal. 2012. Robot?s play: Interactive
games with sociable machines. Computers in Enter-
tainment, 2(3).
H. Bunt, J. Alexandersson, J. Carletta, J.-W. Choe, A.C.
Fang, K. Hasida, K. Lee, V. Petukhova, A. Popescu-
Belis, L. Romary, C. Soria, and D. Traum. 2010.
Towards an ISO standard for dialogue act annotation.
In Proceedings LREC, Valletta, Malta.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Boston.
Heriberto Cuaya?huitl and Ivana Kruijff-Korbayova?.
2012. An interactive humanoid robot exhibiting flex-
ible sub-dialogues. In Proceedings NAACL HLT,
Montreal, Canada.
H. Cuaya?huitl, I. Kruijff-Korbayova?, and N. Dethlefs.
2012. Hierarchical dialogue policy learning using
flexible state transitions and linear function approxi-
mation. In Proceedings COLING, Mumbai, India.
Juan Fasola and Maja J. Mataric. 2013. A socially as-
sistive robot exercise coach for the elderly. Journal
of Human Robot Interaction, 2(3). To appear.
Mary Ellen Foster, Andre Gaschler, Manuel Giuliani,
Amy Isard, Maria Pateraki, and Ronald P. A. Pet-
rick. 2012. Two people walk into a bar: Dynamic
multi-party social interaction with a robot agent. In
Proceedings ICMI, Santa Monica, CA.
Mary Ellen Foster. 2013. How can I help you?
Comparing engagement classification strategies for
a robot bartender. Submitted.
K. Huth, S. Loth, and J.P. De Ruiter. 2012. Insights
from the bar: A model of interaction. In Proceedings
of Formal and Computational Approaches to Multi-
modal Communication.
Simon Keizer, Mary Ellen Foster, Zhuoran Wang, and
Oliver Lemon. 2013. Machine learning of social
states and skills for multi-party human-robot inter-
action. Submitted.
David Klotz, Johannes Wienke, Julia Peltason, Britta
Wrede, Sebastian Wrede, Vasil Khalidov, and Jean-
Marc Odobez. 2011. Engagement-based multi-
party dialog with a humanoid robot. In Proceedings
SIGdial, Portland, OR.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-driven Methods for Adaptive Spoken Dialogue
Systems: Computational Learning for Conversa-
tional Interfaces. Springer.
Pierre Lison. 2011. Multi-policy dialogue manage-
ment. In Proceedings SIGdial, Portland, OR.
Maria Pateraki, Markos Sigalas, Georgios Chliveros,
and Panos Trahanias. 2013. Visual human-robot
communication in social settings. In the Work-
shop on Semantics, Identification and Control of
Robot-Human-Environment Interaction, held within
the IEEE International Conference on Robotics and
Automation (ICRA).
Ronald P. A. Petrick and Mary Ellen Foster. 2013.
Planning for social interaction in a robot bartender
domain. In Proceedings ICAPS, Rome, Italy.
Ronald P. A. Petrick, Mary Ellen Foster, and Amy Isard.
2012. Social state recognition and knowledge-level
planning for human-robot interaction in a bartender
domain. In AAAI 2012 Workshop on Grounding Lan-
guage for Physical Systems, Toronto, ON, Canada,
July.
Verena Rieser and Oliver Lemon. 2011. Rein-
forcement Learning for Adaptive Dialogue Systems.
Springer.
R. Stiefelhagen, H. Ekenel, C. Fu?gen, P. Gieselmann,
H. Holzapfel, F. Kraft, K. Nickel, M. Voit, and
A. Waibel. 2007. Enabling multimodal human-
robot interaction for the Karlsruhe humanoid robot.
IEEE Transactions on Robotics, 23(5):840?851.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
L. Pfeifer Vardoulakis, L. Ring, B. Barry, C. Sidner,
and T. Bickmore. 2012. Designing relational agents
as long term social companions for older adults. In
Proceedings IVA, Santa Cruz, CA.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usabil-
ity with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Blaise Thomson, and Kai Yu. 2010. The
Hidden Information State model: a practical frame-
work for POMDP based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
Steve Young, M. Gas?ic?, B. Thomson, and J. Williams.
2013. POMDP-based statistical spoken dialogue
systems: a review. Proceedings of the IEEE. To
appear.
231
Appendix A: Example session with two guests ordering a drink
Timestamp Level 1 MDP Level 2 MDP DescriptionState features Action State features Action
13:28:45:966 0 1 0 0 0 0 0 0 0 - - A1 visible, but not close to bar; no response
generated yet.
13:28:48:029 1 2 0 0 0 0 0 0 2 - - A1 not close to bar and seeking attention: BT
acknowledges this and engages with A1.
13:28:53:680 3 2 0 0 1 2 0 0 4 - - A2 visible, close to the bar, and seeking atten-
tion; BT is already engaged with A1 and there-
fore asks A2 to wait.
13:28:55:715 3 2 0 0 1 2 0 1 3 0 0 0 1 BT continues his interaction with A1 and asks
for their order.
13:28:56:928 3 2 0 0 1 2 0 1 3 0 6 0 0 BT continues with A1 and waits for them to
order.
13:28:56:928 3 2 0 0 1 2 0 1 3 0 6 0 0 Same as above: BT still waiting for A1?s order.
. . . Due to repeated ASR failures, this state action
pair is encountered several times.
13:29:52:066 3 2 0 0 1 2 0 1 3 0 1 0 2 A1?s has now been successfully recognised; BT
serves the ordered drink to A1.
13:30:12:013 3 2 1 0 1 2 0 1 5 - - A2 still seeking attention; BT can now acknowl-
edge this and engage with A1.
13:30:13:307 1 2 1 0 3 2 0 1 6 0 0 0 1 BT continues with A2 and asks for their order.
13:30:14:475 1 2 1 0 3 2 0 0 6 0 6 0 0 BT continues with A2 and waits for them to
order
13:30:17:737 1 2 1 0 3 2 0 0 6 0 1 0 2 A2?s recognised; BT serves ordered drink to A2.
13:30:37:623 1 2 1 0 3 2 1 0 0 - - Both A1 and A2 have been served; BT does
nothing
13:30:41:440 1 2 1 0 3 2 1 0 0 - - Same as above.
. . .
Table 6: SSE-MDP trajectory for one session from the evaluation data, showing the states and response
actions taken for both MDPs. The states are represented via their value indices, corresponding to Tables 1
and 3; the action indices similarly correspond to the actions in Tables 2 and 4. In the descriptions, A1 and
A2 refer to the first and second user detected; BT refers to the bartender.
232
Proceedings of the SIGDIAL 2013 Conference, pages 314?318,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Impact of ASR N-Best Information on Bayesian Dialogue Act Recognition
Heriberto Cuaya?huitl, Nina Dethlefs, Helen Hastie, Oliver Lemon
School of Mathematical and Computer Sciences,
Heriot-Watt University, Edinburgh, UK
{h.cuayahuitl,n.s.dethlefs,h.hastie,o.lemon}@hw.ac.uk
Abstract
A challenge in dialogue act recognition
is the mapping from noisy user inputs to
dialogue acts. In this paper we describe
an approach for re-ranking dialogue act
hypotheses based on Bayesian classifiers
that incorporate dialogue history and Au-
tomatic Speech Recognition (ASR) N-best
information. We report results based on
the Let?s Go dialogue corpora that show
(1) that including ASR N-best information
results in improved dialogue act recogni-
tion performance (+7% accuracy), and (2)
that competitive results can be obtained
from as early as the first system dialogue
act, reducing the need to wait for subse-
quent system dialogue acts.
1 Introduction
The primary challenge of a Dialogue Act Recog-
niser (DAR) is to find the correct mapping be-
tween a noisy user input and its true dialogue
act. In standard ?slot-filling? dialogue sys-
tems a dialogue act is generally represented as
DialogueActType(attribute-value pairs), see Sec-
tion 3. While a substantial body of research has
investigated different types of models and meth-
ods for dialogue act recognition in spoken dia-
logue systems (see Section 2), here we focus on
re-ranking the outputs of an existing DAR for eval-
uation purposes. In practice the re-ranker should
be part of the DAR itself. We propose to use mul-
tiple Bayesian classifiers to re-rank an initial set
of dialogue act hypotheses based on information
from the dialogue history as well as ASR N-best
lists. In particular the latter type of information
helps us to learn mappings between dialogue acts
and common mis-recognitions. We present exper-
imental results based on the Let?s Go dialogue cor-
pora which indicate that re-ranking hypotheses us-
ing ASR N-best information can lead to improved
recognition. In addition, we compare the recogni-
tion accuracy over time and find that high accuracy
can be obtained with as little context as one system
dialogue act, so that there is often no need to take
a larger context into account.
2 Related Work
Approaches to dialogue act recognition from spo-
ken input have explored a wide range of meth-
ods. (Stolcke et al, 2000) use HMMs for dialogue
modelling, where sequences of observations cor-
respond to sequences of dialogue act types. They
also explore the performance with decision trees
and neural networks and report their highest ac-
curacy at 65% on the Switchboard corpus. (Zim-
mermann et al, 2005) also use HMMs in a joint
segmentation and classification model. (Grau et
al., 2004) use a combination of Naive Bayes and
n-grams with different smoothing methods. Their
best models achieve an accuracy of 66% on En-
glish Switchboard data and 89% on a Spanish cor-
pus. (Sridhar et al, 2009; Wright et al, 1999)
both use a maximum entropy classifier with n-
grams to classify dialogue acts using prosodic fea-
tures. (Sridhar et al, 2009) report an accuracy of
up to 74% on Switchboard data and (Wright et al,
1999) report an accuracy of 69% on the DCIEM
Maptask Corpus. (Bohus and Rudnicky, 2006)
maintain an N-best list of slot values using logis-
tic regression. (Surendran and Levow, 2006) use
a combination of linear support vector machines
(SVMs) and HMMs. They report an accuracy of
65.5% on the HCRC MapTask corpus and con-
clude that SVMs are well suited for sparse text and
dense acoustic features. (Gamba?ck et al, 2011)
use SVMs within an active learning framework.
They show that while passive learning achieves an
accuracy of 77.8% on Switchboard data, the ac-
tive learner achieves up to 80.7%. (Henderson et
al., 2012) use SVMs for dialogue act recognition
from ASR word confusion networks.
314
Speech 
Recognizer
Dialogue Act 
Recognizer Dialogue Act Re-Ranker
n-best 
list
n-best 
list
n-best 
list
(e.g. Let's Go parser)
speech
scored re-scored
  Following 
Components
Figure 1: Pipeline architecture for dialogue act recognition and re-ranking component. Here, the input
is a list of dialogue acts with confidence scores, and the output is the same list of dialogue acts but with
recomputed confidence scores. A dialogue act is represented as DialogueActType(attribute-value pairs).
Several authors have presented evidence in
favour of Bayesian methods. (Keizer and op den
Akker, 2007) have shown that Bayesian DARs
can outperform baseline classifiers such as deci-
sion trees. More generally, (Ng and Jordan, 2001)
show that generative classifiers (e.g. Naive Bayes)
reach their asymptotic error faster than discrimina-
tive ones. As a consequence, generative classifiers
are less data intensive than discriminative ones.
In addition, several authors have investigated
dialogue belief tracking. While our approach
is related to belief tracking, we focus here on
spoken language understanding under uncertainty
rather than estimating user goals. (Williams, 2007;
Thomson et al, 2008) use approximate inference
to improve the scalability of Bayes nets for be-
lief tracking and (Lison, 2012) presents work on
improving their scalability through abstraction.
(Mehta et al, 2010) model user intentions through
the use of probabilistic ontology trees.
Bayes nets have also been applied to other
dialogue-related tasks, such as surface realisa-
tion within dialogue (Dethlefs and Cuaya?huitl,
2011) or multi-modal dialogue act recognition
(Cuaya?huitl and Kruijff-Korbayova?, 2011). In the
following, we will explore a dialogue act recogni-
tion technique based on multiple Bayesian classi-
fiers and show that re-ranking with ASR N-best in-
formation can improve recognition performance.
3 Re-Ranking Dialogue Acts Using
Multiple Bayesian Networks
Figure 1 shows an illustration of our dialogue act
re-ranker within a pipeline architecture. Here, pro-
cessing begins with the user?s speech being inter-
preted by a speech recogniser, which produces a
first N-best list of hypotheses. These hypotheses
are subsequently passed on and interpreted by a
dialogue act recogniser, which in our case is rep-
resented by the Let?s Go parser. The parser pro-
duces a first set of dialogue act hypotheses, based
on which our re-ranker becomes active. A full
dialogue act in our scenario consists of three el-
ements: dialogue act types, attributes (or slots),
and slot values. An example dialogue act is in-
form(from=Pittsburgh Downtown). The dialogue
act re-ranker thus receives a list of hypotheses
in the specified form (triples) from its preceding
module (a DAR or in our case the Let?s Go parser)
and its task is to generate confidence scores that
approximate true label (i.e. the dialogue act really
spoken by a user) as closely as possible.
We address this task by using multiple Bayesian
classifiers: one for classifying a dialogue act type,
one for classifying a set of slots, and the rest for
classifying slot values. The use of multiple classi-
fiers is beneficial for scalability purposes; for ex-
ample, assuming 10 dialogue act types, 10 slots,
10 values per slot, and no other dialogue con-
text results in a joint distribution of 1011 parame-
ters. Since a typical dialogue system is required to
model even larger joint distributions, our adopted
approach is to factorize them into multiple inde-
pendent Bayesian networks (with combined out-
puts). A multiple classifier system is a power-
ful solution to complex classification problems in-
volving a large set of inputs and outputs. This
approach not only decreases training time but has
also been shown to increase the performance of
classification (Tax et al, 2000).
A Bayesian Network (BN) models a joint prob-
ability distribution over a set of random variables
and their dependencies, see (Bishop, 2006) for
an introduction to BNs. Our motivation for us-
ing multiple BNs is to incorporate a fairly rich di-
alogue context in terms of what the system and
user said at lexical and semantic levels. In con-
trast, using a single BN for all slots with rich di-
alogue context faces scalability issues, especially
for slots with large numbers of domain values,
and is therefore not an attractive option. We
denote our set of Bayesian classifiers as ? =
{?dat, ?att, ..., ?val(i)}, where BN ?dat is used to
rank dialogue act types, BN ?att is used to rank
attributes, and the other BNs (?val(i)) are used to
315
rank values for each slot i. The score of a user
dialogue act (< d, a, v >) is computed as:
P (d, a, v) = 1Z
?
P (d|pad)P (a|paa)P (v|pav),
where d is a dialogue act type, a is an attribute
(or slot), v is a slot value, pax is a parent random
variable, andZ is a normalising constant. This im-
plies that the score of a dialogue act is the product
of probabilities of dialogue act type and slot-value
pairs. For dialogue acts including multiple slot-
value pairs, the product above can be extended ac-
cordingly. The best and highest ranked hypothesis
(from space H) can be obtained according to:
< d, a, v >?= arg max
<d,a,v>?H
P (d, a, v).
In the following, we describe our experimental
setting. Here, the structure and parameters of our
classifiers will be estimated from a corpus of spo-
ken dialogues, and we will use the equations above
for re-ranking user dialogue acts. Finally, we re-
port results comparing Bayesian classifiers that
make use of ASR N-best information and dialogue
context against Bayesian classifiers that make pre-
dictions based on the dialogue context alone.
4 Experiments and Results
4.1 Data
Our experiments are based on the Let?s Go corpus
(Raux et al, 2005). Let?s Go contains recorded in-
teractions between a spoken dialogue system and
human users who make enquiries about the bus
schedule in Pittsburgh. Dialogues are driven by
system-initiative and query the user sequentially
for five slots: an optional bus route, a departure
place, a destination, a desired travel date, and a
desired travel time. Each slot needs to be explic-
itly (or implicity) confirmed by the user. Our anal-
yses are based on a subset of this data set contain-
ing 779 dialogues with 7275 turns, collected in the
Summer of 2010. From these dialogues, we used
70% for training our classifiers and the rest for
testing (with 100 random splits). Briefly, this data
set contains 12 system dialogue act types1, 11 user
dialogue act types2, and 5 main slots with varia-
tions3. The number of slot values ranges between
1ack, cant help, example, expl conf, go back, hello,
impl conf, more buses, request, restart, schedule, sorry.
2affirm, bye, go back, inform, negate, next bus, prevbus,
repeat, restart, silence, tellchoices.
3date.absday, date.abmonth, date.day, date.relweek, from,
route, time.ampm, time.arriveleave, time.hour, time.minute,
time.rel, to.
*
Figure 2: Bayesian network for probabilistic rea-
soning of locations (variable ?from desc?), which
incorporates ASR N-best information in the vari-
able?from desc nbest? and dialogue history in-
formation in the remaining random variables.
102 and 103 so that the combination of all possi-
ble dialogue act types, attributes and values leads
to large amounts of triplets. While the majority
of user inputs contain one user dialogue act, the
average number of system dialogue acts per turn
is 4.2. Note that for the user dialogue act types,
we also model silence explicitly. This is often not
considered in dialogue act recognisers: since the
ASR will always try to recognise something out
of any input (even background noise), typical dia-
logue act recognisers will then try to map the ASR
output onto a semantic interpretation.
4.2 Bayesian Networks
We trained our Bayesian networks in a supervised
learning manner and used 43 discrete features (or
random variables) plus a class label (also discrete).
The feature set is described by three main subsets:
25 system-utterance-level binary features4 derived
from the system dialogue act(s) in the last turn; 17
user-utterance-level binary features5 derived from
(a) what the user heard prior to the current turn,
or (b) what keywords the system recognised in its
4System utterance features: heardAck, heardCantHelp,
heardExample, heardExplConf, heardGoBackDAT, heard-
Hello, heardImplConf, heardMoreBuses, heardRequest,
heardRestartDAT, heardSchedule, heardSorry, heardDate,
heardFrom, heardRoute, heardTime, heardTo, heardNext,
heardPrevious, heardGoBack, heardChoices, heardRestart,
heardRepeat, heardDontKnow, lastSystemDialActType.
5User utterance features: hasRoute, hasFrom, hasTo, has-
Date, hasTime, hasYes, hasNo, hasNext, hasPrevious, has-
GoBack, hasChoices, hasRestart, hasRepeat, hasDontKnow,
hasBye, hasNothing, duration in secs. (values=0,1,2,3,4,>5).
316
list of speech recognition hypotheses; and 1 word-
level non-binary feature (* nbest) corresponding
to the slot values in the ASR N-best lists.
Figure 2 shows the Bayes net corresponding to
the classifier used to rank location names. The
random variable from desc is the class label, the
random variable from desc nbest (marked with an
asterisk) incorporates slot values from the ASR
N-best lists, and the remaining variables model
dialogue history context. The structure of our
Bayesian classifiers were derived from the K2 al-
gorithm6, and their parameters were derived from
maximum likelihood estimation. In addition, we
performed probabilistic inference using the Junc-
tion tree algorithm7. Based on these data and
tools, we trained 14 Bayesian classifiers: one for
scoring dialogue act types, one for scoring at-
tributes (slots), and the rest for scoring slot values.
4.3 Experimental Results
We compared 7 different dialogue act recognisers
in terms of classification accuracy. The compar-
ison was made against gold standard data from
a human-labelled corpus. (Semi-Random) is a
recogniser choosing a random dialogue act from
the Let?s Go N-best parsing hypotheses. (Inci) is
our proposed approach considering a context of i
system dialogue acts, and (Ceiling) is a recogniser
choosing the correct dialogue act from the Let?s
Go N-best parsing hypotheses. The latter was used
as a gold standard from manual annotations, which
reflects the proportion of correct labels in the N-
best parsing hypotheses.
We also assessed the impact of ASR N-best in-
formation on probabilistic inference. To this end,
we compared Bayes nets with a focus on the ran-
dom variable ?* nbest?, which in one case con-
tains induced distributions from data and in the
other case contains an equal distribution of slot
values. Our hypothesis is that the former setting
will lead to better performance.
Figure 3 shows the classification accuracy of
our dialogue act recognisers. The first point to no-
tice is that the incorporation of ASR N-best infor-
mation makes an important difference. The per-
formance of recogniser IncK (K being the num-
ber of system dialogue acts) is 66.9% without
ASR N-best information and 73.9% with ASR N-
best information (the difference is significant8 at
6www.cs.waikato.ac.nz/ml/weka/
7www.cs.cmu.edu/?javabayes/Home/8Based on a two-sided Wilcoxon Signed-Rank test.
Semi?Random Inc0 Inc1 Inc2 Inc3 IncK Ceiling40
45
50
55
60
65
70
75
80
85
90
Dialogue Act Recogniser
Clas
sifica
tion 
Accu
racy
 (%)
 
 
Without ASR N?Best InformationWith ASR N?Best Information
Figure 3: Bayesian dialogue act recognisers show-
ing the impact of ASR N-best information.
p < 0.05). The latter represents a substantial im-
provement over the semi-random baseline (62.9%)
and Lets Go dialogue act recognizer (69%), both
significant at p < 0.05. A second point to notice is
that the differences between Inci (? i>0) recognis-
ers were not significant. We can say that the use of
one system dialogue act as context is as competi-
tive as using a larger set of system dialogue acts.
This suggests that dialogue act recognition carried
out at early stages (e.g. after the first dialogue act)
in an utterance does not degrade recognition per-
formance. The effect is possibly domain-specific
and generalisations remain to be investigated.
Generally, we were able to observe that more
than half of the errors made by the Bayesian clas-
sifiers were due to noise in the environment and
caused by the users themselves, which interfered
with ASR results. Detecting when users do not
convey dialogue acts to the system is therefore still
a standing challenge for dialogue act recognition.
5 Conclusion and Future Work
We have described a re-ranking approach for user
dialogue act recognition. Multiple Bayesian clas-
sifiers are used to rank dialogue acts from a set of
dialogue history features and ASR N-best infor-
mation. Applying our approach to the Let?s Go
data we found the following: (1) that including
ASR N-best information results in improved di-
alogue act recognition performance; and (2) that
competitive results can be obtained from as early
as the first system dialogue act, reducing the need
to include subsequent ones.
Future work includes: (a) a comparison of our
317
Bayesian classifiers with other probabilistic mod-
els and forms of training (for example by us-
ing semi-supervised learning), (b) training dia-
logue act recognisers in different (multi-modal and
multi-task) domains, and (c) dealing with random
variables that contain very large domain values.
6 Acknowledgements
This research was funded by the EC FP7 pro-
gramme under grant agreement no. 287615 (PAR-
LANCE) and no. 270019 (SPACEBOOK).
Sample Re-Ranked User Inputs
User input: ?forty six d?
N-Best List of Dialogue Acts Let?s Go Score Bayesian Score
inform(route=46a) 3.33E-4 1.9236763E-6
inform(route=46b) 1.0E-6 1.5243509E-16
inform(route=46d) 0.096107 7.030841E-4
inform(route=46k) 0.843685 4.9941495E-10
silence() NA 0
User input: ?um jefferson hills to mckeesport?
N-Best List of Dialogue Acts Let?s Go Score Bayesian Score
inform(from=mill street) 7.8E-4 3.5998527E-16
inform(from=mission street) 0.015577 3.5998527E-16
inform(from=osceola street) 0.0037 3.5998527E-16
inform(from=robinson township) 0.007292 3.5998527E-16
inform(from=sheraden station) 0.001815 3.1346254E-8
inform(from=brushton) 2.45E-4 3.5998527E-16
inform(from=jefferson) 0.128727 0.0054255757
inform(from=mckeesport) 0.31030 2.6209198E-4
silence() NA 0
References
[Bishop2006] Christopher M. Bishop. 2006. Pattern Recog-
nition and Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secaucus, NJ,
USA.
[Bohus and Rudnicky2006] D. Bohus and A. Rudnicky.
2006. A k hypotheses + other? belief updating model. In
AAAI Workshop on Statistical and Empirical Approaches
to Spoken Dialogue Systems.
[Cuaya?huitl and Kruijff-Korbayova?2011] H. Cuaya?huitl and
I. Kruijff-Korbayova?. 2011. Learning human-robot di-
alogue policies combining speech and visual beliefs. In
IWSDS, pages 133?140.
[Dethlefs and Cuaya?huitl2011] Nina Dethlefs and Heriberto
Cuaya?huitl. 2011. Combining Hierarchical Reinforce-
ment Learning and Bayesian Networks for Natural Lan-
guage Generation in Situated Dialogue. In ENLG, Nancy,
France.
[Gamba?ck et al2011] Bjo?rn Gamba?ck, Fredrik Olsson, and
Oscar Ta?ckstro?m. 2011. Active Learning for Dialogue
Act Classification. In INTERSPEECH, pages 1329?1332.
[Grau et al2004] Sergio Grau, Emilio Sanchis, Maria Jose
Castro, and David Vilar. 2004. Dialogue Act Classifi-
cation Using a Bayesian Approach. In SPECOM.
[Henderson et al2012] Matthew Henderson, Milica Gasic,
Blaise Thomson, Pirros Tsiakoulis, Kai Yu, and Steve
Young. 2012. Discriminative spoken language under-
standing using word confusion networks. In SLT, pages
176?181.
[Keizer and op den Akker2007] Simon Keizer and Rieks
op den Akker. 2007. Dialogue Act Recognition Under
Uncertainty Using Bayesian Networks. Natural Language
Engineering, 13(4):287?316.
[Lison2012] Pierre Lison. 2012. Probabilistic dialogue mod-
els with prior domain knowledge. In SIGDIAL Confer-
ence, pages 179?188.
[Mehta et al2010] Neville Mehta, Rakesh Gupta, Antoine
Raux, Deepak Ramachandran, and Stefan Krawczyk.
2010. Probabilistic ontology trees for belief tracking in
dialog systems. In SIGDIAL Conference, pages 37?46.
[Ng and Jordan2001] Andrew Y. Ng and Michael I. Jordan.
2001. On discriminative vs. generative classifiers: A com-
parison of logistic regression and naive bayes. In NIPS,
pages 841?848.
[Raux et al2005] Antoine Raux, Brian Langner, Dan Bohus,
Alan W. Black, and Maxine Eskenazi. 2005. Let?s
go public! Taking a Spoken Dialog System to the Real
World. In INTERSPEECH, pages 885?888.
[Sridhar et al2009] Vivek Kumar Rangarajan Sridhar, Srini-
vas Bangalore, and Shrikanth Narayanan. 2009. Com-
bining Lexical, Syntactic and Prosodic Cues for Improved
Online Dialog Act Tagging. Computer Speech & Lan-
guage, 23(4):407?422.
[Stolcke et al2000] Andreas Stolcke, Klaus Ries, Noah Coc-
caro, Elizabeth Shriberg, Rebecca A. Bates, Daniel Juraf-
sky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema,
and Marie Meteer. 2000. Dialog Act Modeling for Auto-
matic Tagging and Recognition of Conversational Speech.
Computational Linguistics, 26(3):339?373.
[Surendran and Levow2006] Dinoj Surendran and Gina-Anne
Levow. 2006. Dialog Act Tagging with Support Vec-
tor Machines and Hidden Markov Models. In INTER-
SPEECH.
[Tax et al2000] David M. Tax, Martijn van Breukelen,
Robert P. Duin, and Josef Kittler. 2000. Combining mul-
tiple classifiers by averaging or by multiplying? Pattern
Recognition, 33(9):1475?1485, September.
[Thomson et al2008] Blaise Thomson, Jost Schatzmann, and
Steve Young. 2008. Bayesian update of dialogue state for
robust dialogue systems. In ICASSP, pages 4937?4940.
[Williams2007] Jason D. Williams. 2007. Using particle fil-
ters to track dialogue state. In ASRU, pages 502?507.
[Wright et al1999] H. Wright, Massimo Poesio, and Stephen
Isard. 1999. Using high level dialogue information for
dialogue act recognition using prosodic features. In Pro-
ceedings of an ESCA Tutorial and Research Workshop on
Dialogue and Prosody, pages 139?143, Eindhoven, The
Netherlands.
[Zimmermann et al2005] Matthias Zimmermann, Yang Liu,
Elizabeth Shriberg, and Andreas Stolcke. 2005. Toward
Joint Segmentation and Classification of Dialog Acts in
Multiparty Meetings. In MLMI, pages 187?193.
318
Proceedings of the SIGDIAL 2013 Conference, pages 423?432,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A Simple and Generic Belief Tracking Mechanism for the Dialog State
Tracking Challenge: On the believability of observed information
Zhuoran Wang and Oliver Lemon
Interaction Lab, MACS, Heriot-Watt University
Edinburgh, EH14 4AS, United Kingdom
{zhuoran.wang; o.lemon}@hw.ac.uk
Abstract
This paper presents a generic dialogue
state tracker that maintains beliefs over
user goals based on a few simple domain-
independent rules, using basic probability
operations. The rules apply to observed
system actions and partially observable
user acts, without using any knowledge
obtained from external resources (i.e.
without requiring training data). The core
insight is to maximise the amount of in-
formation directly gainable from an error-
prone dialogue itself, so as to better lower-
bound one?s expectations on the perfor-
mance of more advanced statistical tech-
niques for the task. The proposed method
is evaluated in the Dialog State Track-
ing Challenge, where it achieves compara-
ble performance in hypothesis accuracy to
machine learning based systems. Conse-
quently, with respect to different scenarios
for the belief tracking problem, the poten-
tial superiority and weakness of machine
learning approaches in general are investi-
gated.
1 Introduction
Spoken dialogue system (SDS) can be modelled
as a decision process, in which one of the main
problems researchers try to overcome is the un-
certainty in tracking dialogue states due to error-
prone outputs from automatic speech recognition
(ASR) and spoken language understanding (SLU)
components (Williams, 2012). Recent advances
in SDS have demonstrated that maintaining a dis-
tribution over a set of possible (hidden) dialogue
states and optimising dialogue policies with re-
spect to long term expected rewards can signifi-
cantly improve the interaction performance (Roy
et al, 2000; Williams and Young, 2007a). Such
methods are usually developed under a partially
observable Markov decision process (POMDP)
framework (Young et al, 2010; Thomson and
Young, 2010; Williams, 2010), where the distribu-
tion over dialogue states is called a ?belief? and is
modelled as a posterior updated every turn given
an observation. Furthermore, instead of simply
taking the most probable (or highest confidence
score) hypothesis of the user act as in ?traditional?
handcrafted systems, the observation here may
consist of an n-best list of the SLU hypotheses (di-
alogue acts) with (normalised) confidence scores.
See (Henderson and Lemon, 2008; Williams and
Young, 2007b; Thomson et al, 2010; Young et al,
2013) for more details of POMDP-based SDS.
It is understandable that beliefs more accurately
estimating the true dialogue states will ease the
tuning of dialogue policies, and hence can result
in better overall system performance. The accu-
racy of belief tracking has been studied in depth
by Williams (2012) based on two SDS in public
use. Here the effects of several mechanisms are
analysed, which can alter the ?most-believed? dia-
logue state hypothesis (computed using a genera-
tive POMDP model) from the one derived directly
from an observed top SLU hypothesis. Williams?s
work comprehensively explores how and why a
machine learning approach (more specifically the
generative model proposed in (Williams, 2010))
functions in comparison with a naive baseline.
However, we target a missing intermediate anal-
ysis in this work: how much information one
can gain purely from the SLU n-best lists (and
the corresponding confidence scores), without any
prior knowledge either being externally learned
(using data-driven methods) or designed (based on
domain-specific strategies), but beyond only con-
sidering the top SLU hypotheses. We explain this
idea in greater detail as follows.
Firstly, we can view the belief update procedure
in previous models as re-constructing the hidden
423
dialogue states (or user goals) based on the previ-
ous belief, a current observation (normally an SLU
n-best list), and some prior knowledge. The prior
knowledge can be observation probabilities given
a hidden state, the previous system action and/or
dialogue histories (Young et al, 2010; Thom-
son and Young, 2010; Williams, 2010), or prob-
abilistic domain-specific ontologies (Mehta et al,
2010), where the probabilities can be either trained
on a collection of dialogue examples or manually
assigned by human experts. In such models, a
common strategy is to use the confidence scores in
the observed n-best list as immediate information
substituted into the model for belief computation,
which implies that the performance of such belief
tracking methods to a large extent depends on the
reliability of the confidence scores. On the other
hand, since the confidence scores may reflect the
probabilities of the occurrences of corresponding
user acts (SLU hypotheses), a belief can also be
maintained based on basic probability operations
on those events (as introduced in this paper). Such
a belief will advance the estimation obtained from
top SLU hypotheses only, and can serve as a base-
line to justify how much further improvement is
actually contributed by the use of prior knowledge.
Note that the fundamental method in this paper re-
lies on the assumption that confidence scores carry
some useful information, and their informative-
ness will affect the performance of the proposed
method as will be seen in our experiments (Sec-
tion 5).
Therefore, this paper presents a generic belief
tracker that maintains beliefs over user goals only
using information directly observable from the di-
alogue itself, including SLU n-best list confidence
scores and user and system behaviours, such as
a user not disconfirming an implicit confirma-
tion of the system, or the system explicitly re-
jecting a query (since no matching item exists),
etc. The belief update is based on simple proba-
bility operations and a few very general domain-
independent rules. The proposed method was
evaluated in the Dialog State Tracking Challenge
(DSTC) (Williams et al, 2013). A systematic
analysis is then conducted to investigate the ex-
tent to which machine learning can advance this
naive strategy. Moreover, the results show the per-
formance of the proposed method to be compara-
ble to other machine learning based approaches,
which, in consideration of the simplicity of its im-
plementation, suggests that another practical use
of the proposed method could be as a module
in an initial system installation to collect training
data for machine learning techniques, in addition
to functioning as a baseline for further analysing
them.
The remainder of this paper is organised as fol-
lows. Section 2 reviews some basic mathematical
background, based on which Section 3 introduces
the proposed belief tracker. Section 4 briefly de-
scribes the DSTC task. The evaluation results and
detailed analysis are illustrated in Section 5. Fi-
nally, we further discuss in Section 6 and conclude
in Section 7.
2 Basic Mathematics
We first review some basic mathematics, which
provide the fundamental principles for our be-
lief tracker. Let P (X) denote the probability of
the occurrence of an event X , then the proba-
bility of X not occurring is simply P (?X) =
1 ? P (X). Accordingly, if X occurs at a time
with probability P1(X), and at a second time, it
occurs with probability P2(X) independently of
the first time, then the overall probability of its
occurrence is P (X) = 1 ? P1(?X)P2(?X) =
1 ? (1 ? P1(X))(1 ? P2(X)). To generalise,
we can say that in a sequence of k independent
events, if the probability of X occurring at the ith
time is Pi(X), the overall probability of X hav-
ing occurred at least once among the k chances
is P (X) = 1 ??ki=1 Pi(?X) = 1 ?
?k
i=1(1 ?
Pi(X)). This quantity can also be computed re-
cursively as:
P t(X) = 1? (1? P t?1(X))(1? Pt(X)) (1)
where P t(X) denotes the value of P (X) after t
event occurring chances, and we let P 0(X) = 0.
Now we consider another situation. Let A be
a binary random variable. Suppose that we know
the prior probability of A being true is Pr(A). If
there is a chance where with probability P (B) we
will observe an event B independent of A, and we
assume that if B happens, we must set A to false,
then after this, the probability of A still being true
will become P (A = true) = Pr(A) ? P (?B) =
Pr(A)(1? P (B)).
3 A Generic Belief Tracker
In this section, we will take the semantics defined
in the bus information systems of DSTC as
424
examples to explain our belief tracker. Without
losing generality, the principle applies to other
domains and/or semantic representations. The
SDS we are interested in here is a turn-based
slot-filling task. In each turn, the system executes
an action and receives an observation. The
observation is an SLU n-best list, in which each
element could be either a dialogue act without
taking any slot-value arguments (e.g. affirm()
or negate()) or an act presenting one or more
slot-value pairs (e.g. deny(route=64a) or
inform(date.day=today, time.ampm=
am)), and normalised confidence scores are
assigned to those dialogue act hypotheses. In
addition, we follow a commonly used assumption
that the user?s goal does not change during a
dialogue unless an explicit restart action is
performed.
3.1 Tracking Marginal Beliefs
Since a confidence score reflects the probability
of the corresponding dialogue act occurring in the
current turn, we can apply the probability opera-
tions described in Section 2 plus some ?common
sense? rules to track the marginal probability of a
certain goal being stated by the user during a di-
alogue trajectory, which is then used to construct
our beliefs over user goals. Concretely, we start
from an initial belief b0 with zero probabilities for
all the slot-value hypotheses and track the beliefs
over individual slot-value pairs as follows.
3.1.1 Splitting-Merging Hypotheses
Firstly, in each turn, we split those dialogue acts
with more than one slot-value pairs into single
slot-value statements and merge those identical
statements among the n-best list by summing over
their confidence scores, to yield marginal confi-
dence scores for individual slot-value representa-
tions. For example, an n-best list observation:
inform(date.day=today, time.ampm=am) 0.7
inform(date.day=today) 0.3
after the splitting-merging procedure will become:
inform(date.day=today) 1
inform(time.ampm=am) 0.7
3.1.2 Applying Rules
Let Pt(u, s, v) denote the marginal confidence
score for a user dialogue act u(s = v) at turn
t. Then the belief bt(s, v) for the slot-value pair
(s, v) is updated as:
? Rule 1: If u = inform, then bt(s, v) =
1? (1? bt?1(s, v))(1? Pt(u, s, v)).
? Rule 2: If u = deny, then bt(s, v) =
bt?1(s, v)(1? Pt(u, s, v)).
In addition, motivated by some strategies com-
monly used in rule-based systems (Bohus and
Rudnicky, 2005), we consider the effects of cer-
tain system actions on the beliefs as well. Let a(h)
be one of the system actions performed in turn t,
where h stands for a set of n slot-value arguments
taken by a, i.e. h = {(s1, v1), . . . , (sn, vn)}. We
check:
? Rule 3: If a is an implicit or explicit confir-
mation action (denoted by impl-conf and
expl-conf, respectively) and an affirm
or negate user act u is observed with con-
fidence score Pt(u):
? Rule 3.1: If u = affirm, then
bt(si, vi) = 1 ? (1 ? bt?1(si, vi))(1 ?
Pt(u)), ?(si, vi) ? h.
? Rule 3.2: If u = negate, then
bt(si, vi) = bt?1(si, vi)(1 ? Pt(u)),
?(si, vi) ? h.
? Rule 4: Otherwise, if a is an impl-conf
action, and there are no affirm/negate
user acts observed, and no information pre-
sented in a is re-informed or denied in the
current turn, then we take all (si, vi) ? h as
being affirmed by the user with probability 1.
However, note that, the marginal probabilities
b(s, v) computed using the above rules do not nec-
essarily yield valid beliefs, because sometimes we
may have?v b(s, v) > 1 for a given slot s. When
this occurs, a reasonable solution is to seek a
multinomial vector b?(s, ?) that minimises the sym-
metrised Kullback-Leibler (KL) divergence be-
tween b(s, ?) and itself. It can be checked that
solving such an optimisation problem is actually
equivalent to simply normalising b(s, ?), for which
the proof is omitted here but can be found in Ap-
pendix B.
Finally, we consider an extra fact that normally
a user will not insist on a goal if he/she has been
notified by the system that it is impossible to sat-
isfy. (In the DSTC case, such notifications cor-
respond to those canthelp.* system actions.)
Therefore, we have:
425
? Rule 5: If the system has explicitly disabled
a hypothesis h, we will block the generation
of any hypotheses containing h in the be-
lief tracking procedure, until the dialogue fin-
ishes.
Note here, if h is a marginal hypothesis, elimi-
nating it from our marginal belief will result in
joint hypotheses (see Section 3.2) containing h
also being blocked, but if h is a joint representa-
tion, we will only block the generation of those
joint hypothesis containing h, without affecting
any marginal belief.
3.2 Constructing Joint Representations
Beliefs over joint hypotheses can then be con-
structed by probabilistic disjunctions of those
marginal representations. For example, given two
marginal hypotheses (s1, v1) and (s2, v2) (s1 6=
s2) with beliefs b(s1, v1) and b(s2, v2) respec-
tively, one can compute the beliefs of their joint
representations as:
bjoint(s1 = v1, s2 = v2) = b(s1, v1)b(s2, v2)
bjoint(s1 = v1, s2 = null) = b(s1, v1)b(s2,null)
bjoint(s1 = null, s2 = v2) = b(s1,null)b(s2, v2)
where null represents that none of the current
hypotheses for the corresponding slot is correct,
i.e. b(s,null) stands for the belief that the in-
formation for slot s has never been presented by
the user, and can be computed as b(s,null) =
1??v b(s, v).
3.3 Limitations
The insight of the proposed approach is to explore
the upper limit of the observability one can ex-
pect from an error-prone dialogue itself. Never-
theless, this method has two obvious deficiencies.
Firstly, the dialogue acts in an SLU n-best list
are assumed to be independent events, hence er-
ror correlations cannot be handled in this method
(which is also a common drawback of most ex-
isting models as discussed by Williams (2012)).
Modelling error correlations requires statistics on
a certain amount of data, which implies a poten-
tial space of improvement left for machine learn-
ing techniques. Secondly, the model is designed
to be biased on the accuracy of marginal be-
liefs rather than that of joint beliefs. The be-
liefs for joint hypotheses in this method can only
lower-bound the true probability, as the observ-
able dependencies among some slot-value pairs
are eliminated by the splitting-merging and re-
joining procedures described above. For exam-
ple, in the worst case, a multi-slot SLU hypoth-
esis inform(s1 = v1, s2 = v2) with a confi-
dence score p < 1 may yield two marginal be-
liefs b(s1, v1) = p and b(s2, v2) = p,1 then the
re-constructed joint hypothesis will have its belief
bjoint(s1 = v1, s2 = v2) = p2, which is exponen-
tially reduced compared to the originally observed
confidence score. However, the priority between
the marginal hypotheses and the joint representa-
tions to a greater extent depends on the action se-
lection strategy employed by the system.
4 Description of DSTC
DSTC (Williams et al, 2013) is a public eval-
uation of belief tracking (a.k.a. dialogue state
tracking) models based on the data collected
from different dialogue systems that provide bus
timetables for Pittsburgh, Pennsylvania, USA.
The dialogue systems here were fielded by three
anonymised groups (denoted as Group A, B, and
C).
There are 4 training sets (train1a,
train1b, train2 and train3) and 4
test sets (test1. . .4) provided, where all the
data logs are transcribed and labelled, except
train1b which is transcribed but not labelled
(and contains a much larger number of dialogues
than others). It is known in advance to partici-
pants that test1 was collected using the same
dialogue system from Group A as train1* and
train2, test2 was collected using a different
version of Group A?s dialogue manager but is
to a certain extent similar to the previous ones,
train3 and test3 were collected using the
same dialogue system from Group B (but the
training set for this scenario is relatively smaller
than that for test1), and test4 was collected
using Group C?s system totally different from any
of the training sets.
The evaluation is based on several different met-
rics2, but considering the nature of our system, we
will mainly focus on the hypothesis accuracy, i.e.
1The worst case happens when (s1, v1) and (s2, v2) are
stated for the first time in the dialogue and cannot merge with
any other marginal hypotheses in the current turn, as their
marginal beliefs will remain p without being either propa-
gated by the belief update rules, or increased by the merging
procedure.
2Detailed descriptions of these metrics can be found in the
DSTC handbook at http://research.microsoft.
com/en-us/events/dstc/
426
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 1
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 2
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 3
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 4
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 21
2
3
4
5
6
7
8
9
10
 
 
Baseline Our system Team 1 Team 3 Team 4 Team 5 Team 6 Team 7 Team 8 Team 9
Figure 1: Hypothesis accuracy on the four test sets: the columns in each schedule, from left to right,
stand for the ensemble, mixed-domain, in-domain and out-of-domain system groups, except for test4
where the last three groups are merged into the right-hand side column.
percentage of turns in which the tracker?s 1-best
hypothesis is correct, but with the receiver operat-
ing characteristic (ROC) performance briefly dis-
cussed as well. In addition, there are 3 ?sched-
ules? for determining which turns to include when
measuring a metric: schedule 1 ? including
all turns, schedule 2 ? including a turn for a
given concept only if that concept either appears
on the SLU n-best list in that turn, or if the sys-
tem action references that concept in that turn, and
schedule 3 ? including only the turn before the
restart system action (if there is one), and the
last turn of the dialogue.
5 Evaluation and Analysis
The method proposed in this paper corresponds to
Team 2, Entry 1 in the DSTC submissions. In
the following analysis, we will compare it with
the 26 machine learning models submitted by the
other 8 anonymised participant teams plus a base-
line system (Team 0, Entry 1) that only con-
siders the top SLU result.
Each team can submit up to 5 systems, whilst
the systems from a same team may differ from
each other in either the statistical model or the
training data selection (or both of them). There is
a brief description of each system available after
the challenge. For the convenience of analysis and
illustration, on each test set we categorise these
systems into the following groups: in-domain ?
systems trained only using the data sets which
are similar (including the ?to-some-extent-similar?
ones) to the particular test set, out-of-domain ?
systems trained on the data sets which are to-
tally different from the particular test set, mixed-
domain ? systems trained on a mixture of the in-
domain and out-of-domain data, and ensemble ?
systems combining multiple models to generate
their final output. (The ensemble systems here are
all trained on the mixed-domain data.) Note that,
427
0 0.2 0.4 0.6 0.8 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7 TEST 1
 
 CorrectIncorrect
0 0.2 0.4 0.6 0.8 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7 TEST 2
 
 CorrectIncorrect
0 0.2 0.4 0.6 0.8 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9 TEST 3
 
 CorrectIncorrect
0 0.2 0.4 0.6 0.8 1
0.1
0.2
0.3
0.4
0.5
0.6
0.7 TEST 4
 
 CorrectIncorrect
Figure 2: Distributions of SLU confidence scores on the four test sets: The x-axis stands for the confi-
dence score interval, and the y-axis stands for the occurrence rate.
for test4 there are no in-domain data available,
so all those non-ensemble systems are merged into
one group. Detailed system categorisation on each
test set can be found in Appendix A.
5.1 Hypothesis Accuracy
We plot the hypothesis accuracy of our method
(red dashed line) on the 4 test sets in compari-
son with the baseline system (blue dotted line) and
other systems in Figure 1, where different mark-
ers are used to identify the systems from different
teams. Here we use the overall accuracy of the
marginal hypotheses (all) and the accuracy of
the joint hypotheses (joint) to sketch the gen-
eral performance of the systems, without looking
into the result for each individual slot.
It can be seen that the proposed method pro-
duces more accurate marginal and joint hypothe-
ses than the baseline on all the test sets and in
all the schedules. Moreover, generally speak-
ing, further improvement can be achieved by prop-
erly designed machine learning techniques. For
example, some systems from Team 6, especially
their in-domain and ensemble ones, almost consis-
tently outperform our approach (as well as most of
the models from the other teams) in all the above
tasks. In addition, the following detailed trends
can be found.
Firstly, and surprisingly, our method tends
to be more competitive when measured using
schedule 1 and schedule 3 than using
schedule 2. As schedule 2 is supposed to
measure system performance on the concepts that
are in focus, and to prevent a belief tracker receiv-
ing credit for new guesses about those concepts
not in focus, the results disagree with our origi-
nal expectation of the proposed method. A possi-
ble explanation here is that some machine learning
models tend to give a better belief estimation when
a concept is in focus, however their correct top hy-
potheses might more easily be replaced by other
incorrect ones when the focus on the concepts in
those correct hypotheses are lost (possibly due to
improperly assigned correlations among the con-
cepts). In this sense, our method is more robust,
as the beliefs will not change if their correspond-
ing concepts are not in focus.
428
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 1
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 2
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 3
all joint all joint all joint0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
SCHEDULE 1 SCHEDULE 2 SCHEDULE 3
TEST 4
0 0.2 0.4 0.6 0.8 1 1.2 .4 1.6 1.8 21
2
3
4
5
6
7
8
9
10
 
 
Baseline Our system Team 1 Team 3 Team 4 Team 5 Team 6 Team 7 Team 8 Team 9
Figure 3: ROC equal error rate on the four test sets: The columns in each schedule, from left to right,
stand for the ensemble, mixed-domain, in-domain and out-of-domain system groups, except for test4
where the last three groups are merged into the right-hand side column.
Secondly, the proposed method had been sup-
posed to be more preferable when there are no (or
not sufficient amount of) in-domain training data
available for those statistical methods. Initial evi-
dence to support this point of view can be observed
from the results on test1, test2 and test3.
More concretely, when the test data distribution
becomes less identical to the training data distri-
bution on test2, out system outperforms most
of the other systems except those from Team 6
(and a few others in the schedule 2/all task
only), compared to its middle-level performance
on test1. Similarly, on test3 when the amount
of available in-domain training data is small, our
approach gives more accurate beliefs than most of
the others with only a few exceptions in each sce-
nario, even if extra out-of-domain data are used to
enlarge the training set for many systems. How-
ever, the results on test4 entirely contradicts the
previous trend, where a significant number of ma-
chine learning techniques perform better than our
domain-independent rules without using any in-
domain training data at all. We analyse such re-
sults in detail as follows.
To explain the unexpected outcome on test4,
our first concern is the influence of Rule 4, which
is relatively ?stronger? and more artificial than
the other rules. Hence, for the four test sets,
we compute the percentage of dialogues where a
impl-conf system action occurs. The statistics
show that the occurrence rates of the implicit con-
firmation system actions in test1. . .4 are 0.01,
0, 0.94 and 0.67, respectively. This means that
the two very extreme cases happen in test3 and
test2 (the situation in test1 is very similar to
test2), and the result for test4 is roughly right
in the middle of them, which suggests that Rule
4 will not be the main factor to affect our per-
formance on test4. Therefore, we further look
into the distributions of the SLU confidence scores
across these different test sets. A normalised his-
togram of the confidence scores for correct and
incorrect SLU hypotheses observed in each test
set is plotted in Figure 2. Here we only consider
429
the SLU hypotheses that will actually contribute
during our belief tracking processes, i.e. only the
inform, deny, affirm and negate user dia-
logue acts. It can be found that the dialogue sys-
tem used to collect the data in test4 tends to
produce significantly more ?very confident? SLU
hypotheses (those with confidence scores greater
than 0.8) than the dialogue systems used for col-
lecting the other test sets, where, however, a con-
siderable proportion of its highly confident hy-
potheses are incorrect. In such a case, our system
would be less capable in revising those incorrect
hypotheses with high confidence scores than many
machine learning techniques, since it to a greater
extent relies on the confidence scores to update the
beliefs. This finding indicates that statistical ap-
proaches will be helpful when observed informa-
tion is less reliable.
5.2 Discussions on the ROC Performance
Besides the hypothesis accuracy, another impor-
tant issue will be the ability of the beliefs to dis-
criminate between correct and incorrect hypothe-
ses. Williams (2012) suggests that a metric to
measure such performance of a system is the ROC
curve. Note that, in the DSTC task, most of the
systems from the other teams are based on dis-
criminative models (except two systems, a simple
generative model from Team 3 and a deep neural
network method from Team 1), which are opti-
mised specifically for discrimination. Unsurpris-
ingly, our approach becomes much less competi-
tive when evaluated based on the ROC curve met-
rics, as illustrated in Figure 3 using the ROC equal
error rate (EER) for the all and joint scenar-
ios. (ERR stands for the intersection of the ROC
curve with the diagonal, i.e. where the false ac-
cept rate equals the false reject rate. The smaller
the ERR value, the better a system?s performance
is.) However, our argument on this point is that
since an optimised POMDP policy is not a linear
classifier but has a manifold decision surface (Cas-
sandra, 1998), the ROC curves may not be able to
accurately reflect the influence of beliefs on a sys-
tem?s decision quality, for which further investiga-
tions will be needed in our future work.
6 Further Discussions
In this paper, we made the rules for our belief
tracker as generic as possible, in order to ensure
the generality of the proposed mechanism. How-
ever, in practice, it is extendable by using more
detailed rules to address additional phenomena if
those phenomena are deterministically identifiable
in a particular system. For example, when the sys-
tem confirms a joint hypothesis (s1 = v1, s2 =
v2) and the user negates it and only re-informs one
of the two slot-values (e.g. inform(s1 = v?1)),
one may consider that it is more reasonable to only
degrade the belief on s1 = v1 instead of reducing
the beliefs on both s1 = v1 and s2 = v2 syn-
chronously as we currently do in Rule 3.2. How-
ever, the applicability of this strategy will depend
on whether it is possible to effectively determine
such a compact user intention from an observed
SLU n-best list without ambiguities.
7 Conclusions
This paper introduces a simple rule-based belief
tracker for dialogue systems, which can maintain
beliefs over both marginal and joint representa-
tions of user goals using only the information ob-
served within the dialogue itself (i.e. without need-
ing training data). Based on its performance in
the DSTC task, potential advantages and disad-
vantages of machine learning techniques are anal-
ysed. The analysis here is more focused on general
performance of those statistical approaches, where
our concerns include the similarity of distributions
between the training and test data, the adequacy of
available training corpus, as well as the SLU confi-
dence score distributions. Model-specific features
for different machine learning systems are not ad-
dressed at this stage. Considering its competitive-
ness and simplicity of implementation, we suggest
that the proposed method can serve either as a rea-
sonable baseline for future research on dialogue
state tracking problems, or a module in an ini-
tial system installation to collect training data for
those machine learning techniques.
Acknowledgments
The research leading to these results was sup-
ported by the EC FP7 projects JAMES (ref.
270435) and Spacebook (ref. 270019). We thank
Jason D. Williams for fruitful comments on an ear-
lier version of this paper. We also acknowledge
helpful discussions with Simon Keizer and Herib-
erto Cuaya?huitl.
430
References
Dan Bohus and Alexander I. Rudnicky. 2005. Con-
structing accurate beliefs in spoken dialog systems.
In Proceedings of the IEEE Workshop on Automatic
Speech Recognition and Understanding, pages 272?
277.
Anthony R. Cassandra. 1998. Exact and Approxi-
mate Algorithms for Partially Observable Markov
Decision Processes. Ph.D. thesis, Brown University,
Providence, RI, USA.
James Henderson and Oliver Lemon. 2008. Mixture
model POMDPs for efficient handling of uncertainty
in dialogue management. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 73?76.
Neville Mehta, Rakesh Gupta, Antoine Raux, Deepak
Ramachandran, and Stefan Krawczyk. 2010. Prob-
abilistic ontology trees for belief tracking in dialog
systems. In Proceedings of the 11th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 37?46.
Nicholas Roy, Joelle Pineau, and Sebastian Thrun.
2000. Spoken dialogue management using proba-
bilistic reasoning. In Proceedings of the 38th An-
nual Meeting on Association for Computational Lin-
guistics, pages 93?100.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A POMDP framework for
spoken dialogue systems. Computer Speech and
Language, 24(4):562?588.
Blaise Thomson, Filip Jurc?ic?ek, Milica Gas?ic?, Simon
Keizer, Francois Mairesse, Kai Yu, and Steve Young.
2010. Parameter learning for POMDP spoken dia-
logue models. In Proceedings of IEEE Workshop on
Spoken Language Technology.
Jason D. Williams and Steve Young. 2007a. Partially
observable Markov decision processes for spoken
dialog systems. Computer Speech and Language,
21(2):393?422.
Jason D. Williams and Steve Young. 2007b. Scal-
ing POMDPs for spoken dialog management. IEEE
Transactions on Audio, Speech, and Language Pro-
cessing, 15(7):2116?2129.
Jason D. Williams, Antoine Raux, Deepak Ramachan-
dran, and Alan W. Black. 2013. The Dialog State
Tracking Challenge. In Proceedings of the 14th An-
nual Meeting of the Special Interest Group on Dis-
course and Dialogue.
Jason D. Williams. 2010. Incremental partition re-
combination for efficient tracking of multiple dia-
log states. In Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing, pages 5382?5385.
Jason D. Williams. 2012. Challenges and oppor-
tunities for state tracking in statistical spoken dia-
log systems: Results from two public deployments.
IEEE Journal of Selected Topics in Signal Process-
ing, 6(8):959?970.
Steve Young, Milica Gas?ic?, Simon Keizer, Francois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The Hidden Information State model:
a practical framework for POMDP-based spoken di-
alogue management. Computer Speech and Lan-
guage, 24(2):150?174.
Steve Young, Milica Gas?ic?, Blaise Thomson, and Ja-
son D. Williams. 2013. POMDP-based statistical
spoken dialog systems: A review. Proceedings of
the IEEE, 101(5):1160?1179.
A System Categorisation
Table 1 shows detailed categorisation of the sys-
tems submitted to DSTC, where TiEj stands for
Team i, Entry j.
ensemble
T6E3, T6E4, T9E1, T9E2, T9E3
T9E4, T9E5
mixed-domain non-ensemblefortest4
T1E1, T3E1, T3E2, T3E3, T4E1
T5E2, T5E4, T5E5, T8E4, T8E5
in-domain
test1 T5E1
test2 T6E1, T8E1, T8E2 T5E3
test3 T6E2, T6E5, T8E3 T7E1
out-of-domain
test1
test2 T6E2, T6E5, T8E3
test3 T6E1, T8E1, T8E2
Table 1: Categorisation of the systems submitted
to DSTC.
B Symmetrised KL-divergence
Minimisation
We prove the following proposition to support our
discussions in the end of Section 3.1.
Proposition 1 Let p ? RN be an arbitrary N -
dimensional non-negative vector (i.e. p ? 0).
Let p? = p?p?1 , where ? ? ?1 stands for the `1-norm of a vector. Then p? is the solution of the
optimisation problem. minq?0,?q?1=1 DSKL(p?q),
where DSKL(p?q) denotes the symmetrised KL-
divergence between p and q, defined as:
DSKL(p?q) = DKL(p?q) + DKL(q?p) (2)
=
?
i
pi log
pi
qi
+
?
i
qi log
qi
pi
431
and pi and qi denote the ith element in p and q
respectively.
Proof Let q? = arg minq?0,?q?1=1 DSKL(p?q).
Firstly, using the facts that limx?0 x log xy ? 0
and limx?0 y log yx ? +?, ?y > 0, one can eas-ily prove that if pi = 0 then q?i = 0, and pi 6= 0
then q?i 6= 0, because otherwise the objective value
of Eq. (2) will become unbounded.
Therefore, we only consider the case p > 0 and
q > 0. By substituting pi = p?i?p?1 into Eq. (2),
we obtain:
DSKL(p?q) = ?p?1
?
i
p?i log
?p?1p?i
qi
+
?
i
qi log
qi
?p?1p?i
= ?p?1
(?
i
p?i log
p?i
qi
+
?
i
p?i log ?p?1
)
+
?
i
qi log
qi
p?i
?
?
i
qi log ?p?1
= ?p?1
?
i
p?i log
p?i
qi
+
?
i
qi log
qi
p?i
+(?p?1 ? 1) log ?p?1
= ?p?1DKL(p??q) + DKL(q?p?)
+(?p?1 ? 1) log ?p?1
? (?p?1 ? 1) log ?p?1
where we use the facts that?i p?i = 1,
?
i qi = 1,
DKL(p??q) ? 0 and DKL(q?p?) ? 0, since p? and
q are valid distributions. It can be found that the
minimum (?p?1 ? 1) log ?p?1 is only achievable
when DKL(p??q) = 0 and DKL(q?p?) = 0, i.e. q =
p?, which proves Proposition 1.
432
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 48?52,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Multi-threaded Interaction Management for
Dynamic Spatial Applications
Srinivasan Janarthanam
Interaction Lab
Heriot-Watt University
Edinburgh
sc445@hw.ac.uk
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh
o.lemon@hw.ac.uk
Abstract
We present a multi-threaded Interaction
Manager (IM) that is used to track differ-
ent dimensions of user-system conversa-
tions that are required to interleave with
each other in a coherent and timely man-
ner. This is explained in the context of
a spoken dialogue system for pedestrian
navigation and city question-answering,
with information push about nearby or vis-
ible points-of-interest (PoI).
1 Introduction
We present a multi-threaded Interaction Manager
(IM) that is used to track different dimensions of
user-system conversations and interleave the dif-
ferent converational threads coherently. The IM
that we present interacts with the user in a spa-
tial domain and interleaves navigation informa-
tion along with historical and cultural information
about the entities that users can see around them.
In addition, it aims to answer questions that users
might have about those entities. This presents
a complex conversational situation where several
conversational threads have to be interleaved in
such a way that the system utterances are pre-
sented to the user at the right time but in a pri-
oritised order, and with bridging utterances when
threads are interrupted and resumed. For instance,
a navigation instruction may be important (since
the user is walking up to a junction at which they
need to turn) and therefore it needs to be spoken
before continuing information presentation about
an entity or answering other ongoing questions.
2 Related work
Previously, multi-threaded interaction was used
to handle multiple simultaneous tasks in human-
robot interaction (HRI) scenarios (Lemon and
Gruenstein, 2004). This idea also turns out to be
important for cases where humans are interacting
with a variety of different web-services in paral-
lel. Human multitasking in dialogue is discussed
in (Yang et al., 2008).
(Lemon and Gruenstein, 2004) presented a
multi-threaded dialogue management approach
for managing several concurrent tasks in an HRI
scenario. The robot could, for example be flying
to a location while simultaneously searching for
a vehicle, and utterances about both tasks could
be interleaved. Here, conversational threads were
managed using a representation called the ?Dia-
logue Move Tree?, which represented conversa-
tional threads as branches of the tree, linked to an
?Activity Tree? which represented the states of on-
going robot tasks (deliver medical supplies, fly to a
waypoint, search for a truck), which could be ac-
tive simultaneously. The situation for our pedes-
trian navigation and information system is simi-
lar - concurrent tasks need to be managed coher-
ently via conversation. The approach adopted in
this paper is similar to (Lemon and Gruenstein,
2004). However, in this work we separate out
a domain-general thread called ?dialogue control?
which handles generic issues like clarification of
reference across all tasks. This increasing modu-
larisation of the dialogue threads makes it possible
to learn individual dialogue policies for each one,
in future work.
(Nakano et al., 2008) presented an approach
where one of the several expert modules handling
different tasks is activated based on the user input,
but only one verbal expert is active at any one time.
In contrast to this, we present an approach where
several thread managers each handling a different
task can be activated in parallel and their outputs
stored and retrieved based on priority.
3 Multi-threaded IM
The Interaction Manager (IM) is the central com-
ponent of any spoken dialogue system architec-
48
Figure 1: Interaction Manager Architecture
ture. Generally, it takes as input the user?s utter-
ances in the form of dialogue acts from the parser
and identifies the next dialogue action to present to
the user. Dialogue about a domain task is managed
using a dialogue strategy or policy (e.g. (Young,
2000; Lemon and Pietquin, 2007)). A dialogue
policy is a mapping between dialogue states and
dialogue actions, which are semantic representa-
tions of what the system should say next.
In order to handle multiple tasks simul-
taneously, we present an architecture for a
multi-threaded interaction manager that treats
conversation about each domain task as a thread.
These conversational threads are interleaved and
managed using techniques such as multi-queuing,
priority based pushing, and queue revision. We
describe these techniques below. The architecture
of the Interaction Manager is shown in figure 1.
Multi-threading and queuing
In order to manage complex interactions involving
several conversational tasks/topics, we propose
that the each task be handled by a thread manager
within the interaction management framework.
Each such manager will handle a conversational
thread using a dialogue policy. Each thread
manager will be fed with the input from the user
and the dialogue actions generated will be stored
in separate queues. This approach allows the
interaction manager to produce several dialogue
actions at the same time although for different
conversational tasks.
Prioritised Queue Management
Dialogue actions from the several threads are
stored in separate queues. The queues can be
assigned priorities that decide the order in which
items from the queues will be popped. The
dialogue actions in the queues are pushed to the
user based on an order of priority (see below).
This priority can either be fixed or dynamic based
on context. The system and user engagement
should also be checked so that system utterances
are pushed only when the system and user are not
speaking already.
Queue Revision: resuming and bridging
The dialogue actions are generated and stored in
queues. Therefore, there is a difference between
the time they are generated and time that they are
pushed. Therefore dialogue actions in the queues
are revised periodically to reflect changes in con-
text. Obsolete dialogue actions will have to re-
moved for two reasons. Firstly, pushing them to
the user may make the conversation incoherent be-
cause the system may be speaking about an entity
that is no longer relevant and secondly, these obso-
lete dialogue actions may delay other other impor-
tant dialogue actions from being pushed on time.
In addition, it may also be useful to edit the dia-
logue actions to include discourse markers to sig-
nify topic change (Yang et al., 2008) and bridge
49
phrases to reintroduce a previous topic. We dis-
cuss some examples later in section 4.3.
4 SPACEBOOK Interaction Manager
As a part of the SpaceBook EU FP7 project,
we implemented the above design for a multi-
threaded interaction manager that presents the user
with navigational instructions, pushes PoI infor-
mation, and manages QA questions (Janarthanam
et al., 2013). It receives the user?s input in the
form of a dialogue act (DA) from the ASR mod-
ule and the user?s location (latitude and longitude),
orientation, and speed from the Pedestrian Tracker
module. Based on these inputs and the dialogue
context, the IM responds with a system output di-
alogue act. It should be noted that the location
coordinates of the user are sent to the IM every 2
seconds. This allows the IM to generate location
aware information at a high frequency. In addition,
the IM has to deal with incoming requests and re-
sponses from the user?s spoken inputs. With the
possibility of system utterances being generated
at a frequency of one every two seconds, there is
a need for an efficient mechanism to manage the
conversation and reduce the risk of overloading
the user with information. These tasks are treated
as separate conversational threads.
4.1 Conversational Threads
The SpaceBook IM manages the conversation
using five conversational threads using dedicated
task managers. Three threads: ?navigation?,
?question answering? and ?PoI pushing?, represent
the core tasks of our system. In addition, for
handling the issues in dialogue management,
we introduce two threads: ?dialogue control?
and ?request response?. These different threads
represent the state of different dimensions of the
user-system conversation that need to interleave
with each other coherently. Each of the threads
is managed by a thread manager using a dialogue
policy. Each thread can generate a dialogue ac-
tion depending on the context, as described below:
Dialogue Control
During the course of the conversation, the IM uses
this thread to manage user requests for repetition,
issues with unparsed (i.e. not understood) user
utterances, utterances that have low ASR confi-
dence, and so on. The dialogue control thread is
also used to manage reference resolution in cases
where referring expressions are underspecified.
The IM resolves anaphoric references by keeping
a record of entities mentioned in the dialogue
context. It stores the name and type information
for each entity (such as landmark, building, etc)
mentioned in previous utterances by either user
or system. Subsequent user references to these
entities using expressions such as ?the museum?,
?the cafe?, and so on, are resolved by searching
for the latest entity of the given type. In cases
where the IM cannot resolve the referent, it asks
the user to clarify.
Request Response
The user can also initiate tasks that interest
him/her at anytime during the conversation.
These tasks include searching for an entity (e.g.
a museum or a restaurant), requesting navigation
instructions to a destination, and asking questions
about the entities in the city database such as their
location (?Where is X??, ?How far is X??). Dur-
ing navigation, users might want to ask questions
about the destination, ask for next instructions,
etc. All these user requests are handled using
the request response thread. For instance, when
the user asks for directions, the IM resolves the
destination entity (perhaps using clarification)
in the city model and acknowledges the user
request. The task is then further handled using the
Navigation thread.
Navigation
The IM identifies the location of the destination
entity and queries a city database (Bartie and
Mackaness, 2013) for a route plan. Using the route
plan, the navigation thread presents step-by-step
instructions to the user based on the current loca-
tion and orientation of the user. The IM contin-
uously monitors users to determine if at any time
they are deviating from the planned route and pro-
vides corrective instructions. As users get near
to the next node on the route plan, the next in-
struction is given. The IM uses highly salient vis-
ible landmarks (Bartie et al., 2013) and popular
landmarks near the nodes to instruct the user (e.g.
?When you reach Clydesdale Bank, turn left on
to Nicolson Square?). The IM also informs users
when they pass by recognisable landmarks, just to
reassure them that they are on the right track (e.g.
?You will pass by Tesco on the right?). When the
user is close to his/her destination, the IM deter-
mines whether the destination is visible to the user,
informs the user, and closes the task.
50
Usr: I want to go to the National Gallery.
Sys: Ok. I am now looking for directions to
the National Gallery.
Sys: The National Gallery is on Queens Street
at about 300 meters away.
....
Sys: Can you see the Bank of Scotland about
300 meters away on your left? It has a big
green dome.
Usr: Yes. I can see it.
Sys: Walk towards it.
Sys: Can you see the National Gallery in front
of you at about 100 meters? It is a building
with Roman architecture.
Usr: Yes.
Sys: You have now reached your destination.
Question Answering
The system also answers ad hoc questions from
the user (e.g. ?Who is David Hume??, ?What is
the Old College??, ?Who was William Wallace?,
etc). These are sent to the QA server and answered
based on responses from the Question-Answering
(QA) server (Janarthanam et al., 2013). The
dialogue policy here is to answer the user?s
question with the first snippet available and ask
the user to request for more if more snippets are
available and he or she is interested.
Pushing PoI Information
When the user is mobile, the IM identifies pop-
ular points of interest (PoI) on the route based on
two factors: proximity and visibility. The dialogue
policy is to introduce the PoI, query the QA server
for snippets and push the first snippet to the user.
The user is encouraged to ask for more informa-
tion if he/she is interested.
Sys: In front of you, about 200 meters away is
Old College. It has a grey dome on top.
Sys: Situated on South Bridge, Old College is
. . .
Sys: Ask for more information if interested.
4.2 Priority assignment in SpaceBook
Priority is assigned to the above dialogue threads
as follows:
Priority 1. Dialogue control (repeat request, clari-
fications etc)
Priority 2. Responding to user requests
Priority 3. System initiated navigation task actions
Priority 4. Responses to User-initiated QA actions
Priority 5. PoI Push actions
For instance, informing the user of a PoI could be
delayed if the user needs to be given an instruction
to turn at the junction he is approaching.
4.3 Queue revision and bridging utterances
The queues need to be revised at regular intervals
in order to keep the information in them relevant
to context. For instance, the dialogue action of in-
forming the user of his/her location is deleted after
5 seconds, as this tends to become obsolete. Sim-
ilarly, dialogue actions corresponding to informa-
tion segments in PoI and QA queues are edited to
inform the utterance generator of other interven-
ing dialogue actions so that it can use appropri-
ate bridge phrases to reintroduce the focus of the
conversational thread. For instance, as shown in
the example below, the utterance generator inserts
a bridge phrase (i.e. ?More on Old College?) to
reintroduce the focus of the PoI push task because
of the intervening user request and the subsequent
system response.
Sys: In front of you, about 200 meters away is
the Old College. It has a grey dome on top.
User: Where am I?
Sys: You are on Chambers street.
Sys: More onOld College. Situated on South
Bridge, the Old College is......
5 Conclusion
We presented an architecture for a multi-threaded
Interaction Manager that can handle multiple con-
versational tasks. We also described an implemen-
tation of the architecture in a dynamic spatial en-
vironment. The SpaceBook IM is a multi-tasking
IM that aims to interleave navigation information
along with historical information about the enti-
ties users can see around them. In addition, it aims
to answer questions users might have about those
entities.
Acknowledgements
The research leading to these results has received
funding from the European Community?s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 270019 (SPACEBOOK project
www.spacebook-project.org).
51
References
P. Bartie and W. Mackaness. 2013. D3.1.2: The SpaceBook
City Model. Technical report, The SPACEBOOK Project
(FP7/2011-2014 grant agreement no. 270019).
P. Bartie, W. Mackaness, M. Fredriksson, and J. Konigsmann.
2013. D2.1.2 Final Viewshed Component. Technical
report, The SPACEBOOK Project (FP7/2011-2014 grant
agreement no. 270019).
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dick-
inson, X. Liu, W. Mackaness, and B. Webber. 2013.
Evaluating a city exploration dialogue system combining
question-answering and pedestrian navigation. In Proc.
ACL 2013.
Oliver Lemon and Alexander Gruenstein. 2004. Mul-
tithreaded context for robust conversational interfaces:
context-sensitive speech recognition and interpretation of
corrective fragments. ACM Transactions on Computer-
Human Interaction (ACM TOCHI), 11(3):241? 267.
Oliver Lemon and Olivier Pietquin. 2007. Machine learning
for spoken dialogue systems. In Interspeech.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and Hi-
roshi Tsujino. 2008. A framework for building conversa-
tional agents based on a multi-expert model. In Proceed-
ings of the 9th SIGdial Workshop on Discourse and Dia-
logue, SIGdial ?08, pages 88?91, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fan Yang, Peter A. Heeman, and Andrew Kun. 2008.
Switching to real-time tasks in multi-tasking dialogue.
In Proceedings of the 22Nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 1025?1032, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Steve Young. 2000. Probabilistic methods in spoken dia-
logue systems. Philosophical Transactions of the Royal
Society (Series A), 358(1769):1389?1402.
52
Proceedings of the SIGDIAL 2014 Conference, pages 60?68,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Learning non-cooperative dialogue behaviours
Ioannis Efstathiou
Interaction Lab
Heriot-Watt University
ie24@hw.ac.uk
Oliver Lemon
Interaction Lab
Heriot-Watt University
o.lemon@hw.ac.uk
Abstract
Non-cooperative dialogue behaviour has
been identified as important in a vari-
ety of application areas, including educa-
tion, military operations, video games and
healthcare. However, it has not been ad-
dressed using statistical approaches to di-
alogue management, which have always
been trained for co-operative dialogue.
We develop and evaluate a statistical dia-
logue agent which learns to perform non-
cooperative dialogue moves in order to
complete its own objectives in a stochas-
tic trading game. We show that, when
given the ability to perform both coopera-
tive and non-cooperative dialogue moves,
such an agent can learn to bluff and to lie
so as to win games more often ? against
a variety of adversaries, and under var-
ious conditions such as risking penalties
for being caught in deception. For exam-
ple, we show that a non-cooperative dia-
logue agent can learn to win an additional
15.47% of games against a strong rule-
based adversary, when compared to an op-
timised agent which cannot perform non-
cooperative moves. This work is the first
to show how an agent can learn to use non-
cooperative dialogue to effectively meet
its own goals.
1 Introduction
Research in automated conversational systems has
almost exclusively focused on the case of coopera-
tive dialogue, where a dialogue system?s core goal
is to assist humans in particular tasks, such as buy-
ing airline tickets (Walker et al., 2001) or finding
a place to eat (Young et al., 2010). Gricean co-
operative principles have been shown to emerge
from multi-agent decision theory, in a language
task modelled using Decentralised Partially Ob-
servable Markov Decision Processes (Vogel et al.,
2013a), and in related work conversational impli-
cature was argued to be a by-product of agents
who maximise joint utility (Vogel et al., 2013b).
However, non-cooperative dialogues, where an
agent may act to satisfy its own goals rather than
those of other participants, are also of practi-
cal and theoretical interest (Georgila and Traum,
2011), and the game-theoretic underpinnings of
non-Gricean behaviour are actively being investi-
gated (Asher and Lascarides, 2008). For example,
it may be advantageous for an automated agent not
to be fully cooperative when trying to gather in-
formation from a human, and when trying to per-
suade, argue, or debate, when trying to sell them
something, when trying to detect illegal activity
(for example on internet chat sites), or in the area
of believable characters in video games and edu-
cational simulations (Georgila and Traum, 2011;
Shim and Arkin, 2013). Another arena in which
non-cooperative dialogue behaviour is desirable is
in negotiation (Traum, 2008), where hiding infor-
mation (and even outright lying) can be advanta-
geous. Furthermore, deception is considered to be
an essential part of successful military operations.
According to Sun Tzu ?All warfare is based on de-
ception? and Machiavelli clearly states in The Dis-
courses that ?Although deceit is detestable in all
other things, yet in the conduct of war it is laud-
able and honorable?(Arkin, 2010). Indeed, Den-
nett argues that deception capability is required for
higher-order intentionality in AI (Dennett, 1997).
A complementary research direction in recent
years has been the use of machine learning meth-
ods to automatically optimise cooperative dia-
logue management - i.e. the decision of what di-
alogue move to make next in a conversation, in
order to maximise an agent?s overall long-term ex-
pected utility, which is usually defined in terms of
meeting a user?s goals (Young et al., 2010; Rieser
60
and Lemon, 2011). This research has shown how
robust and efficient dialogue management strate-
gies can be learned from data, but has only ad-
dressed the case of cooperative dialogue. These
approaches use Reinforcement Learning with a re-
ward function that gives positive feedback to the
agent only when it meets the user?s goals.
An example of the type of non-cooperative dia-
logue behaviour which we are generating in this
work is given by agent B in the following dia-
logue:
A: ?I will give you a sheep if you give me a wheat?
B: ?No?
B: ?I really need rock? [B actually needs wheat]
A: ?OK... I?ll give you a wheat if you give me
rock?
B: ?OK?
Here, A is deceived into providing the wheat
that B actually needs, because A believes that B
needs rock rather than wheat. Similar behaviour
can be observed in trading games such as Settlers
of Catan (Afantenos et al., 2012).
1.1 Non-cooperative dialogue and
implicature
Our trading dialogues are linguistically coop-
erative (according to the Cooperative Principle
(Grice, 1975)) since their linguistic meaning is
clear from both sides and successful information
exchange occurs. Non-linguistically though they
are non-cooperative, since they they aim for per-
sonal goals. Hence they violate Attardo?s Per-
locutionary Cooperative Principle (PCP) (Attardo,
1997). In our non-cooperative environment, the
manipulative utterances such as ?I really need
sheep? can imply that ?I don?t really need any of
the other two resources?, as both of the players are
fully aware that three different resources exist in
total and more than one is needed to win the game,
so therefore they serve as scalar implicatures (Vo-
gel et al., 2013b). Hence we will show that the
LA learns how to include scalar implicatures in
its dialogue to successfully deceive its adversary
by being cooperative on the locutionary level and
non-cooperative on the perlocutionary level.
1.2 Structure of the paper
In this paper we investigate whether a learn-
ing agent endowed with non-cooperative dialogue
moves and a ?personal? reward function can learn
how to perform non-cooperative dialogue. Note
that the reward will not be given for perform-
ing non-cooperative moves themselves, but only
for winning trading games. We therefore explore
whether the agent can learn the advantages of be-
ing non-cooperative in dialogue, in a variety of
settings. This is similar to (Vogel et al., 2013a)
who show how cooperativity emerges from multi-
agent decision making, though in our case we
show the emergence of non-cooperative dialogue
behaviours.
We begin with the case of a simple but challeng-
ing 2-player trading game, which is stochastic and
involves hidden information.
In section 2 we describe and motivate the trad-
ing game used in this work, and in section 3 we
describe the Learning Agent. In section 4 we ex-
plain the different adversaries for experimentation,
in section 5 we provide results, and in section 6 we
conclude and discuss areas for future work.
2 The Trading Game
To investigate non-cooperative dialogues in a con-
trolled setting we created a 2-player, sequential,
non-zero-sum game with imperfect information
called ?Taikun?. Motivated by the principle of
Occam?s razor we shaped this game as simply as
possible, while including the key features of a re-
source trading game. The precise goal was also to
implement mechanics that are not restrictive for
the future of this research and therefore can be
flexibly extended to capture different aspects of
trading and negotiation. We call the 2 players the
?adversary? and the ?learning agent? (LA).
The two players can trade three kinds of re-
sources with each other sequentially, in a 1-for-1
manner, in order to reach a specific number of re-
sources that is their individual goal. The player
who first attains their goal resources wins. Both
players start the game with one resource of each
type (wheat, rock, and sheep). At the beginning
of each round the game updates the number of re-
sources of both players by either removing one of
them or adding two of them, thereby making the
opponent?s state (i.e. the cards that they hold) un-
observable. In the long run, someone will eventu-
ally win even if no player ever trades. However,
effective trading can provide a faster victory.
2.1 ?Taikun? game characteristics
Taikun is a sequential, non-cooperative, non-zero-
sum game, with imperfect information, where:
61
? The goal is to reach either 4 or 5 of two
specific resources (4 wheat and 5 rocks
for the learning agent and 4 wheat and
5 sheep for the adversary). The players
share a goal resource (wheat).
? Each round consists of an update of re-
sources turn, the learning agent?s trad-
ing proposal turn (and adversary?s ac-
ceptance or rejection), and finally the
adversary?s trading proposal turn (and
LA?s acceptance or rejection).
? The update turn, which is a hidden ac-
tion, changes one of the resources of
each player at random by +2 or -1.
? When a resource is ?capped?, that is if
its number is 5 or more, then no update
rule can be applied to it. Trading can
still change its quantity though.
2.2 Actions (Trading Proposals)
Trade occurs through trading proposals that may
lead to acceptance from the other player. In an
agent?s turn only one ?1-for-1? trading proposal
may occur, or nothing (7 actions in total):
1. I will do nothing
2. I will give you a wheat if you give me a rock
3. I will give you a wheat if you give me a sheep
4. I will give you a rock if you give me a wheat
5. I will give you a rock if you give me a sheep
6. I will give you a sheep if you give me a wheat
7. I will give you a sheep if you give me a rock
Agents respond by either saying ?No? or ?OK?
in order to reject or accept the other agent?s pro-
posal.
2.3 Non-cooperative dialogue moves
In our second experiment three manipulative ac-
tions are added to the learning agent?s set of ac-
tions:
1. ?I really need wheat?
2. ?I really need rock?
3. ?I really need sheep?
The adversary believes these statements, result-
ing in modifying their probabilities of making cer-
tain trades.
Note that in the current model we assume that
only these 3 manipulative actions potentially have
an effect on the adversary?s reasoning about the
game. An alternative would be to allow all
the trading utterances to have some manipulative
power. For example the LA?s uttering ?I will give
you a wheat if you give me a rock? could lead the
adversary to believe that the LA currently needs
rock. For the present work, we prefer to sepa-
rate out the manipulative actions explicitly, so as
to first study their effects in the presence of non-
manipulative dialogue actions. In future work, we
will consider the case where all trading proposals
can cause adversaries to change their game strat-
egy.
3 The Learning Agent (LA)
The game state can be represented by the learning
agent?s set of resources, its adversary?s set of re-
sources, and a trading proposal (if any) currently
under consideration. We track up to 19 of each
type of resource, and have a binary variable repre-
senting whose turn it is. Therefore there are 20 x
20 x 20 x 2 = 16,000 states.
The learning agent (LA) plays the game and
learns while perceiving only its own set of re-
sources. This initial state space can later be ex-
tended with elements of history (previous dialogue
moves) and estimates of the other agent?s state
(e.g. beliefs about what the adversary needs).
The LA is aware of its winning condition (to ob-
tain 4 wheat and 5 rocks) in as much as it experi-
ences a large final reward when reaching this state.
It learns how to achieve the goal state through
trial-and-error exploration while playing repeated
games.
The LA is modelled as a Markov Decision Pro-
cess (Sutton and Barto, 1998): it observes states,
selects actions according to a policy, transitions to
a new state (due to the adversary?s move and/or a
update of resources), and receives rewards at the
end of each game. This reward is then used to up-
date the policy followed by the agent.
The rewards that were used in these experi-
ments were 1,000 for the winning case, 500 for
a draw and -100 when losing a game. The win-
ning and draw cases have the same goal states and
that would initially suggest the same reward but
they can be achieved through different strategies.
Experiments that we have conducted using either
the above rewards or the same rewards for win and
62
draw have verified this. The learning agent?s per-
formance is slightly better when the reward for a
win is 1000 and 500 for a draw.
The LA was trained using a custom SARSA(?)
learning method (Sutton and Barto, 1998) with an
initial exploration rate of 0.2 that gradually decays
to 0 at the end of the training games. After exper-
imenting with the learning parameters we found
that with ? equal to 0.4 and ? equal to 0.9 we ob-
tain the best results for our problem and therefore
these values have been used in all of the experi-
ments that follow.
4 Adversaries
We investigated performance with several differ-
ent adversaries. As a baseline, we first need
to know how well a LA which does not have
non-cooperative moves at its disposal can per-
form against a rational rule-based adversary. Our
hypothesis is then that a LA with additional
non-cooperative moves can outperform this base-
line case when the adversary becomes somewhat
gullible.
A ?gullible? adversary is one who believes state-
ments such as ?I really need rock? and then acts so
as to restrict the relevant resource(s) from the LA.
Our experiments (see experiments 3.1-3.3) show
that this gullible behaviour may originate from
sound reasoning. The adversary confronts in this
case a very important dilemma. It suddenly does
not know if it should stay with its goal-oriented
strategy (baseline) or instead it should boycott the
LA?s stated needed resources. A priori, both of
these strategies might be equally successful, and
we will show that their performances are indeed
very close to each other.
4.1 Rule-based adversary: experiment 1
This strategy was designed to form a challenging
rational adversary for measuring baseline perfor-
mance. It cannot be manipulated at all, and non-
cooperative dialogue moves will have no effect on
it ? it simply ignores statements like ?I really need
wheat?.
The strict rule-based strategy of the adversary
will never ask for a resource that it does not need
(in this case rocks). Furthermore, if it has an avail-
able non-goal resource to give then it will offer it.
It only asks for resources that it needs (goal re-
sources: wheat and sheep). In the case where it
does not have a non-goal resource (rocks) to offer
then it offers a goal resource only if its quantity
is more than it needs, and it asks for another goal
resource if it is needed.
Following the same reasoning, when replying
to the LA?s trading proposals, the adversary will
never agree to receive a non-goal resource (rock).
It only gives a non-goal resource (rock) for another
one that it needs (wheat or sheep). It also agrees to
make a trade in the special case where it will give
a goal resource of which it has more than it needs
for another one that it still needs. This is a strong
strategy that wins a significant number of games.
In fact, it takes about 100,000 training games be-
fore the LA is able to start winning more games
than this adversary, and a random LA policy loses
66% of games against this adversary (See Table 1,
LA policy ?Random?).
4.2 Gullible adversary: experiment 2
The adversary in this case retains the above strict
base-line policy but it is also susceptible to the
non-cooperative moves of the LA, as explained
above. For example, if the LA utters ?I really need
rock?, weights of actions which transfer rock from
the adversary will decrease, and the adversary will
then be less likely to give rock to the LA. Con-
versely, the adversary is then more likely to give
the other two resources to the LA. In this way the
LA has the potential to mislead the adversary into
trading resources that it really needs.
4.3 The restrictive adversaries: experiments
3.1, 3.2, 3.3
Here we investigate performance against adver-
saries who cannot be manipulated, but their strat-
egy is to always restrict the LA from gaining a spe-
cific type of resource. We need to explore how
well a manipulated adversary (for example one
who will no longer give rocks that only its op-
ponent needs) performs. This will show us the
potential advantage to be gained by manipulation
and most important, it will generalise our prob-
lem by showing that the restriction (boycott) of a
resource that only the opponent needs, or of a re-
source that both of the players need, are actually
reasonably good strategies compared to the base-
line case (Experiment 1). Hence, the manipulated
adversary has indeed a reason for choosing to re-
strict resources (Experiment 2) rather than staying
with its rule-based strategy. In other words it has
a rational reason to become gullible and fall in the
learning agent?s trap.
63
4.4 Gullible-based adversary with risk of
exposure: experiments 4.1, 4.2
Here we extend the problem to include possi-
ble negative consequences of manipulative LA ac-
tions. The adversary begins each game with a
probability of detecting manipulation, that expo-
nentially increases after every one of the LA?s ma-
nipulative actions. In more detail, every time the
LA performs a manipulation, there is an additional
chance that the adversary notices this (starts at 1-
in-10 and increases after every manipulative move,
up to 100% in the case of the 10th manipulative
attempt). The consequence of being detected is
that the adversary will refuse to trade with the LA
any further in that game (experiment 4.1), or that
the adversary automatically wins the game (exper-
iment 4.2). In these two cases there is always a
risk associated with attempting to manipulate, and
the LA has to learn how to balance the potential
rewards with this risk.
5 Results
The LA was trained over 1.5 million games against
each adversary for the cases of the rule-based (ex-
periment 1), gullible (experiment 2) and restrictive
adversaries (experiments 3.1, 3.2, 3.3). The result-
ing policies were tested in 20 thousand games.
For reasons of time, the LA was trained for only
35 thousand games for the case of the gullible
adversary who stops trading when the LA be-
comes exposed (experiment 4.1), and 350 thou-
sand games for the gullible adversary who wins
the game when the LA becomes exposed (experi-
ment 4.2). In the former case we used 2 thousand
testing games and in the latter 20 thousand.
5.1 Baseline performance: Experiment 1
The LA scored a winning performance of 49.5%
against 45.555% for the adversary, with 4.945%
draws (Table 1), in the 20 thousand test games,
see Figure 1. This represents the baseline perfor-
mance that the LA is able to achieve against an
adversary who cannot be manipulated at all. This
shows that the game is ?solvable? as an MDP prob-
lem, and that a reinforcement learning agent can
outperform a strict hand-coded adversary.
Here, the learning agent?s strategy mainly fo-
cuses on offering the sheep resource that it does
not need for the rocks that does need (for example
action7 > action2 > action6 > action3 Ta-
ble 2). It is also interesting to notice that the LA
learnt not to use action 3 at all (gives 1 wheat that
they both need for 1 sheep that only the adversary
needs). Hence its frequency is 0. The actions 4 and
5 are never accepted by the adversary so their role
in both of the experiments is similar to that of the
action 1 (do nothing). The rejections of the adver-
sary?s trades dominate the acceptances with a ratio
of 94 to 1 as our learning agent learns to become
negative towards the adversarial trading proposals
and therefore to prohibit its strategy.
Figure 1: Learning Agent?s reward-victory graph
over 1.5 million training games of Experiment 1.
5.2 Non-cooperative actions: Experiment 2
In Experiment 2 the learning agent scored a
winning performance of 59.17% against only
39.755% of its adversary, having 1.075% draws
(Table 1), in the 20 thousand test games, see Fig-
ure 2.
Similarly to the previous experiment, the LA?s
strategy focuses again mainly on action 7, by of-
fering the sheep resource that it does not need for
rocks that it needs (Table 2). However in this case
we also notice that the LA has learnt to use ac-
tion 2 very often, exploiting cases where it will
win by giving the wheat resource that they both
need for a rock that only it needs. This is a result
of its current manipulation capabilities. The high
frequency manipulative actions 8 (?I really need
wheat?) and 9 (?I really need rock?) assist in de-
ceiving its adversary by hiding information, there-
fore significantly reinforcing its strategy as they
both indirectly result in gaining sheep that only the
adversary needs (experiment 3.2).
Rejections to adversarial trading offers over the
64
acceptances were again the majority in this exper-
iment. However in this case they are significantly
fewer than before, with a ratio of only 2.5 to 1,
as our learning agent is now more eager to accept
some trades because it has triggered them itself by
appropriately manipulating its adversary.
Figure 2: Learning Agent?s reward-victory graph
in 1.5 million training games of Experiment 2.
In Experiment 1 the LA?s dominating strategy
(mainly based on requiring the rocks resource
from its adversary) provides it with a difference
in winning performance of +3.945%. In Experi-
ment 2 the adversary, further being deceived by the
learning agent?s hiding information actions, loses
19.415% more often than the learning agent.
Action Exp. 1 Exp. 2
number frequency frequency
1. Do nothing 81969 144727
2. Give wheat for rock 8077 46028
3. Give wheat for sheep 0 10358
4. Give rock for wheat 80578 62874
5. Give rock for sheep 78542 55627
6. Give sheep for wheat 6429 24687
7. Give sheep for rock 23888 31132
8. I really need wheat - 68974
9. I really need rock - 87123
10. I really need sheep - 18
Table 2: Frequencies of LA actions.
Table 2 shows that the LA?s strategy in Experi-
ment 1 mainly focuses on requiring rocks from the
adversary by offering sheep (for example action 7
> action 2 or 6). In Experiment 2 the agent?s strat-
egy is similar. However, it is now enhanced by the
frequent use of the manipulative actions 8 and 9
(both hide information). The LA gathers mainly
sheep (8 and 9) through its manipulation and then
wheat (9 > 8) that the adversary needs to win. It
also offers them ?selectively? back (2 and 7) for
rock that only it needs in order to win.
5.3 Restrictive adversaries: Experiment 3
In experiment 3 the LA uses no manipulative ac-
tions. It is the same LA as that of Experiment 1. It
is trained and then tested against 3 different types
of restrictive adversaries. The first one (Experi-
ment 3.1) never gives wheat, the second one (Ex-
periment 3.2) never gives rocks, and the third one
never gives sheep (Experiment 3.3). They all act
randomly regarding the other 2 resources which
are not restricted. In the first case (adversary re-
stricts wheat that they both need), the LA scored
a winning performance of 50.015% against 47.9%
of its adversary, having 2.085% draws in the 20
thousand test games. In the second case (adver-
sary restricts rocks that the LA only needs), the LA
scored a winning performance of 53.375% against
44.525% of its adversary, having 2.1% draws in
the 20 thousand test games. In the third case
(adversary restricts sheep that only itself needs),
the LA scored a winning performance of 62.21%
against 35.13% of its adversary, having 2.66%
draws in the 20 thousand test games. These re-
sults show that restricting the resource that only
the opponent needs (i.e. LA only needs rocks) and
especially the resource that they both need (i.e.
wheat) can be as effective as the strategy followed
by the rule-based adversary (see Table 1). The dif-
ference in the performances for the former case
(rock) is +8.85% and for the latter (wheat) only
+2.115%. That means the adversary has indeed
a reason to believe that boycotting its opponent?s
resources could be a winning opposing strategy,
motivating its gullibility in experiment 2 (section
5.2).
1
5.4 Non-cooperative actions and risk of
exposure: Experiment 4.1 (adversary
stops trading)
In this case when the LA is exposed by the adver-
sary then the latter does not trade for the rest of the
1
Further experiments showed that having the same num-
ber of goal resources (i.e. both need 4 of their own goal re-
sources, rather than 5) still produces similar results.
65
Exp. Learning Agent policy Adversary policy LA wins Adversary wins Draws
Random Baseline 32% 66% 2%
1 SARSA Baseline 49.5% 45.555% 4.945%
2 SARSA + Manipulation Baseline + Gullible 59.17%* 39.755% 1.075%
3.1 SARSA Restrict wheat 50.015%* 47.9% 2.085%
3.2 SARSA Restrict rock 53.375%* 44.525% 2.1%
3.3 SARSA Restrict sheep 62.21%* 35.13% 2.66%
4.1 SARSA + Manipulation Basel. + Gull. + Expos.(no trade) 53.2%* 45.15% 1.65%
4.2 SARSA + Manipulation Basel. + Gull. + Expos.(win game) 36.125% 61.15% 2.725%
Table 1: Performance (% wins) in testing games, after training. (*= significant improvement over base-
line, p < 0.05)
game. The LA scored a winning performance of
53.2% against 45.15% for this adversary, having
1.65% draws in the 2 thousand test games, see Fig-
ure 3. This shows that the LA managed to locate a
successful strategy that balances the use of the ma-
nipulative actions and the normal trading actions
with the risk of exposure (Table 3). In more de-
tail, the strategy that the LA uses here makes fre-
quent use of the manipulative actions 8 (?I really
need wheat?) and 9 (?I really need rock?) again
which mainly result in the collection of sheep that
only its adversary needs to win. Restriction of a
resource that the opponent only needs is a good
strategy (as our experiment 3.2 suggests) and the
LA managed to locate that and exploit it. The next
highest frequency action (excluding actions 4 and
5 that mostly lead to rejection from the adversary
as it also follows its rule-based strategy) is 7 (?I
will give you a sheep if you give me a rock?) that is
exclusively based on the LA?s goal and along with
6 they ?selectively? give back the sheep for goal re-
sources. Rejections to adversary?s proposals over
the acceptances were in a ratio of approximately
4 to 1. The LA is quite eager (in contrast to the
baseline case of experiment 1) to accept the adver-
sary?s proposals as it has already triggered them
by itself through deception.
5.5 Non-cooperative actions and risk of
exposure: Experiment 4.2 (adversary
wins the game)
In this case if the LA becomes exposed by the ad-
versary then the latter wins the game. The LA
scored a winning performance of 36.125% against
61.15% of its adversary, having 2.725% draws in
20 thousand test games, see Figure 4. It is the
only case where the LA so far has not yet found
a strategy that wins more often than its adversary,
and therefore in future work a larger set of training
games will be used. Note that this was only trained
for 350 thousand games ? we expect better perfor-
mance with more training. In fact, here we would
expect a good policy to perform at least as well as
experiment 1, which would be the case of learning
never to use manipulative actions, since they are
so dangerous. Indeed, a good policy could be to
lie (action 10) only once, at the start of a dialogue,
and then to follow the policy of experiment 2. This
would lead to a winning percentage of about 49%
(the 59% of experiment 2 minus a 10% loss for the
chance of being detected after 1 manipulation).
The LA has so far managed to locate a strat-
egy that again balances the use of the manipula-
tive actions and that of the normal ones with the
risk of losing the game as a result of exposure
(Table 3). According to Figure 4 we notice that
the LA gradually learns how to do that. How-
ever its performance is not yet desirable, as it is
still only slightly better than that of the Random
case against the Baseline (Table 1). It is interest-
ing though to see that the strategy that the LA uses
here makes frequent use of the action 10 (?I really
need sheep?) that lies. On the other hand, the ac-
tions 8 and 9 are almost non-existent. That results
in accepting wheat that they both need and rocks
that it only needs, showing that the main focus of
the manipulation is on the personal goal. The LA
has learned so far in this case that by lying it can
get closer to its personal goal. Rejections to adver-
sary?s proposals over the acceptances resulted in a
ratio of approximately 1.7 to 1, meaning that the
LA is again quite eager to accept the adversarial
trading proposals that it has triggered already by
itself through lying.
We report further results on this scenario in
an updated version of this paper (Efstathiou and
66
Lemon, 2014).
Action Exp. 4.1 Exp. 4.2
number frequency frequency
1 Do nothing 8254 74145
2 Give wheat for rock 2314 3537
3 Give wheat for sheep 1915 4633
4 Give rock for wheat 5564 46120
5 Give rock for sheep 4603 57031
6 Give sheep for wheat 2639 2737
7 Give sheep for rock 3132 3105
8 I really need wheat 7200 4
9 I really need rock 7577 7
10 I really need sheep 548 19435
Table 3: Frequencies of LA actions.
Figure 3: Learning Agent?s reward-victory graph
in 35 thousand training games of Experiment 4.1.
6 Conclusion & Future Work
We showed that a statistical dialogue agent can
learn to perform non-cooperative dialogue moves
in order to enhance its performance in trad-
ing negotiations. This demonstrates that non-
cooperative dialogue strategies can emerge from
statistical approaches to dialogue management,
similarly to the emergence of cooperative be-
haviour from multi-agent decision theory (Vogel
et al., 2013a).
In future work we will investigate more com-
plex non-cooperative situations. For example a
real dialogue example of this kind is taken from
Figure 4: Learning Agent?s reward-victory graph
in 350 thousand training games of Experiment 4.2.
the ?Settlers of Catan? game corpus (Afantenos et
al., 2012):
? A: Do you have rock?
? B: I?ve got lots of wheat [in fact, B has
a rock]
? A: I?ll give you 2 clay for a rock
? B: How about 2 clay for a wheat?
? A: I?ll give 1 clay for 3 wheat
? B: Ok, it?s a deal.
In future more adversarial strategies will also be
applied, and the learning problem will be made
more complex (e.g. studying ?when? and ?how
often? an agent should try to manipulate its ad-
versary). Alternative methods will also be con-
sidered such as adversarial belief modelling with
the application of interactive POMDPs (Partially
Observable Markov Decision Processes) (Gmy-
trasiewicz and Doshi, 2005). The long-term goal
of this work is to develop intelligent agents that
will be able to assist (or even replace) users in in-
teraction with other human or artificial agents in
various non-cooperative settings (Shim and Arkin,
2013), such as education, military operations, vir-
tual worlds and healthcare.
References
Stergos Afantenos, Nicholas Asher, Farah Benamara,
Anais Cadilhac, Cedric Degremont, Pascal De-
nis, Markus Guhe, Simon Keizer, Alex Lascarides,
Oliver Lemon, Philippe Muller, Soumya Paul, Ver-
ena Rieser, and Laure Vieu. 2012. Developing a
67
corpus of strategic conversation in The Settlers of
Catan. In Proceedings of SemDial 2012.
R. Arkin. 2010. The ethics of robotics deception. In
1st International Conference of International Asso-
ciation for Computing and Philosophy, pages 1?3.
N. Asher and A. Lascarides. 2008. Commitments, be-
liefs and intentions in dialogue. In Proc. of SemDial,
pages 35?42.
S. Attardo. 1997. Locutionary and perlocutionary co-
operation: The perlocutionary cooperative principle.
Journal of Pragmatics, 27(6):753?779.
Daniel Dennett. 1997. When Hal Kills, Who?s to
Blame? Computer Ethics. In Hal?s Legacy:2001?s
Computer as Dream and Reality.
Ioannis Efstathiou and Oliver Lemon. 2014. Learn-
ing to manage risk in non-cooperative dialogues. In
under review.
Kallirroi Georgila and David Traum. 2011. Reinforce-
ment learning of argumentation dialogue policies in
negotiation. In Proc. INTERSPEECH.
Piotr J. Gmytrasiewicz and Prashant Doshi. 2005. A
framework for sequential planning in multi-agent
settings. Journal of Artificial Intelligence Research,
24:49?79.
Paul Grice. 1975. Logic and conversation. Syntax and
Semantics, 3.
Verena Rieser and Oliver Lemon. 2011. Reinforce-
ment Learning for Adaptive Dialogue Systems: A
Data-driven Methodology for Dialogue Manage-
ment and Natural Language Generation. Theory
and Applications of Natural Language Processing.
Springer.
J. Shim and R.C. Arkin. 2013. A Taxonomy of Robot
Deception and its Benefits in HRI. In Proc. IEEE
Systems, Man, and Cybernetics Conference.
R. Sutton and A. Barto. 1998. Reinforcement Learn-
ing. MIT Press.
David Traum. 2008. Exrended abstract: Computa-
tional models of non-cooperative dialogue. In Proc.
of SIGdial Workshop on Discourse and Dialogue.
Adam Vogel, Max Bodoia, Christopher Potts, and Dan
Jurafsky. 2013a. Emergence of Gricean Maxims
from Multi-Agent Decision Theory. In Proceedings
of NAACL 2013.
Adam Vogel, Christopher Potts, and Dan Jurafsky.
2013b. Implicatures and Nested Beliefs in Approx-
imate Decentralized-POMDPs. In Proceedings of
ACL 2013.
M. Walker, R. Passonneau, and J. Boland. 2001. Quan-
titative and qualitative evaluation of DARPA Com-
municator spoken dialogue systems. In Proc. of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
Steve Young, M. Gasic, S. Keizer, F. Mairesse,
J. Schatzmann, B. Thomson, and K. Yu. 2010. The
Hidden Information State Model: a practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
68
Proceedings of the SIGDIAL 2014 Conference, pages 260?262,
Philadelphia, U.S.A., 18-20 June 2014. c?2014 Association for Computational Linguistics
The Parlance Mobile Application for Interactive Search in
English and Mandarin
Helen Hastie, Marie-Aude Aufaure?, Panos Alexopoulos,
Hugues Bouchard, Catherine Breslin, Heriberto Cuay?huitl, Nina Dethlefs,
Milica Ga?i?, James Henderson, Oliver Lemon, Xingkun Liu, Peter Mika, Nesrine Ben Mustapha,
Tim Potter, Verena Rieser, Blaise Thomson, Pirros Tsiakoulis, Yves Vanrompay,
Boris Villazon-Terrazas, Majid Yazdani, Steve Young and Yanchao Yu
email: h.hastie@hw.ac.uk. See http://parlance-project.eu for full list of affiliations
Abstract
We demonstrate a mobile application in
English and Mandarin to test and eval-
uate components of the Parlance di-
alogue system for interactive search un-
der real-world conditions.
1 Introduction
With the advent of evaluations ?in the wild?,
emphasis is being put on converting re-
search prototypes into mobile applications that
can be used for evaluation and data col-
lection by real users downloading the ap-
plication from the market place. This is
the motivation behind the work demonstrated
here where we present a modular framework
whereby research components from the Par-
lance project (Hastie et al., 2013) can be
plugged in, tested and evaluated in a mobile
environment.
The goal of Parlance is to perform inter-
active search through speech in multiple lan-
guages. The domain for the demonstration
system is interactive search for restaurants in
Cambridge, UK for Mandarin and San Fran-
cisco, USA for English. The scenario is that
Mandarin speaking tourists would be able to
download the application and use it to learn
about restaurants in English speaking towns
and cities.
2 System Architecture
Here, we adopt a client-server approach as il-
lustrated in Figure 1 for Mandarin and Figure
2 for English. The front end of the demon-
stration system is an Android application that
calls the Google Automatic Speech Recogni-
tion (ASR) API and sends the recognized user
utterance to a server running the Interaction
?Authors are in alphabetical order
Manager (IM), Spoken Language Understand-
ing (SLU) and Natural Language Generation
(NLG) components.
Figure 1: Overview of the Parlance Man-
darin mobile application system architecture
Figure 2: Overview of the Parlance En-
glish mobile application system architecture
extended to use the Yahoo API to populate
the application with additional restaurant in-
formation
When the user clicks the Start button, a di-
alogue session starts. The phone application
first connects to the Parlance server (via
the Java Socket Server) to get the initial sys-
tem greeting which it speaks via the Google
260
Text-To-Speech (TTS) API. After the system
utterance finishes the recognizer starts to lis-
ten for user input to send to the SLU compo-
nent. The SLU converts text into a semantic
interpretation consisting of a set of triples of
communicative function, attribute, and (op-
tionally) value1. Probabilities can be associ-
ated with candidate interpretations to reflect
uncertainty in either the ASR or SLU. The
SLU then passes the semantic interpretation
to the IM within the same server.
Chinese sentences are composed of strings of
characters without any space to mark words as
other languages do, for example:
In order to correctly parse and understand
Chinese sentences, Chinese word segmenta-
tions must be performed. To do this segmen-
tation, we use the Stanford Chinese word seg-
mentor2, which relies on a linear-chain condi-
tional random field (CRF) model and treats
word segmentation as a binary decision task.
The Java Socket Server then sends the seg-
mented Chinese sentence to the SLU on the
server.
The IM then selects a dialogue act, accesses
the database and in the case of English passes
back the list of restaurant identification num-
bers (ids) associated with the relevant restau-
rants. For the English demonstration system,
these restaurants are displayed on the smart
phone as seen in Figures 4 and 5. Finally,
the NLG component decides how best to re-
alise the restaurant descriptions and sends the
string back to the phone application for the
TTS to realise. The example output is illus-
trated in Figure 3 for Mandarin and Figure 4
for English.
As discussed above, the Parlance mobile
application can be used as a test-bed for com-
paring alternative techniques for various com-
ponents. Here we discuss two such compo-
nents: IM and NLG.
1This has been implemented for English; Mandarin
uses the rule-based Phoenix parser.
2http://nlp.stanford.edu/projects/chinese-
nlp.shtml
Figure 3: Screenshot and translation of the
Mandarin system
Figure 4: Screenshot of dialogue and the list
of recommended restaurants shown on a map
and in a list for English
2.1 Interaction Management
The Parlance Interaction Manager is based
on the partially observable Markov decision
process (POMDP) framework, where the sys-
tem?s decisions can be optimised via reinforce-
ment learning. The model adopted for Par-
lance is the Bayesian Update of Dialogue
State (BUDS) manager (Thomson and Young,
2010). This POMDP-based IM factors the di-
alogue state into conditionally dependent ele-
ments. Dependencies between these elements
can be derived directly from the dialogue on-
tology. These elements are arranged into a dy-
namic Bayesian network which allows for their
marginal probabilities to be updated during
the dialogue, comprising the belief state. The
belief state is then mapped into a smaller-scale
summary space and the decisions are optimised
using the natural actor critic algorithm. In the
Parlance application, hand-crafted policies
261
Figure 5: Screenshot of the recommended
restaurant for the English application
can be compared to learned ones.
2.2 Natural Language Generation
As mentioned above, the server returns the
string to be synthesised by the Google TTS
API. This mobile framework allows for testing
of alternative approaches to NLG. In particu-
lar, we are interested in comparing a surface re-
aliser that uses CRFs against a template-based
baseline. The CRFs take semantically anno-
tated phrase structure trees as input, which it
uses to keep track of rich linguistic contexts.
Our approach has been compared with a num-
ber of competitive state-of-the art surface real-
izers (Dethlefs et al., 2013), and can be trained
from example sentences with annotations of se-
mantic slots.
2.3 Local Search and Knowledge Base
For the English system, the domain database is
populated by the search Yahoo API (Bouchard
and Mika, 2013) with restaurants in San Fran-
sisco. These restaurant search results are
returned based on their longitude and lati-
tude within San Francisco for 5 main areas, 3
price categories and 52 cuisine types contain-
ing around 1,600 individual restaurants.
The Chinese database has been partially
translated from an English database for restau-
rants in Cambridge, UK and search is based
on 3 price categories, 5 areas and 35 cuisine
types having a total of 157 restaurants. Due
to the language-agnostic nature of the Par-
lance system, only the name and address
fields needed to be translated.
3 Future Work
Investigating application side audio compres-
sion and audio streaming over a mobile in-
ternet connection would enable further assess-
ment of the ASR and TTS components used
in the original Parlance system (Hastie et
al., 2013). This would allow for entire research
systems to be plugged directly into the mobile
interface without the use of third party ASR
and TTS.
Future work also involves developing a feed-
back mechanism for evaluation purposes that
does not put undue effort on the user and put
them off using the application. In addition,
this framework can be extended to leverage
hyperlocal and social information of the user
when displaying items of interest.
Acknowledgements
The research leading to this work was funded
by the EC FP7 programme FP7/2011-14
under grant agreement no. 287615 (PAR-
LANCE).
References
H. Bouchard and P. Mika. 2013. Interactive hy-
perlocal search API. Technical report, Yahoo
Iberia, August.
N. Dethlefs, H. Hastie, H. Cuay?huitl, and
O. Lemon. 2013. Conditional Random Fields
for Responsive Surface Realisation Using Global
Features. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (ACL), Sofia, Bulgaria.
H. Hastie, M.A. Aufaure, P. Alexopoulos,
H. Cuay?huitl, N. Dethlefs, M. Gasic,
J. Henderson, O. Lemon, X. Liu, P. Mika,
N. Ben Mustapha, V. Rieser, B. Thomson,
P. Tsiakoulis, Y. Vanrompay, B. Villazon-
Terrazas, and S. Young. 2013. Demonstration
of the PARLANCE system: a data-driven
incremental, spoken dialogue system for in-
teractive search. In Proceedings of the 14th
Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL), Metz,
France, August.
B. Thomson and S. Young. 2010. Bayesian up-
date of dialogue state: A POMDP framework
for spoken dialogue systems. Computer Speech
and Language, 24(4):562?588.
262
Proceedings of the 8th International Natural Language Generation Conference, pages 138?142,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Multi-adaptive Natural Language Generation using Principal Component
Regression
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon
School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
{dg106, h.hastie, o.lemon}@hw.ac.uk
Abstract
We present FeedbackGen, a system that
uses a multi-adaptive approach to Natu-
ral Language Generation. With the term
?multi-adaptive?, we refer to a system
that is able to adapt its content to dif-
ferent user groups simultaneously, in our
case adapting to both lecturers and stu-
dents. We present a novel approach to
student feedback generation, which simul-
taneously takes into account the prefer-
ences of lecturers and students when de-
termining the content to be conveyed in
a feedback summary. In this framework,
we utilise knowledge derived from rat-
ings on feedback summaries by extract-
ing the most relevant features using Prin-
cipal Component Regression (PCR) anal-
ysis. We then model a reward function
that is used for training a Reinforcement
Learning agent. Our results with stu-
dents suggest that, from the students? per-
spective, such an approach can generate
more preferable summaries than a purely
lecturer-adapted approach.
1 Introduction
Summarisation of time-series data refers to the
task of automatically generating reports from at-
tributes whose values change over time. Content
selection is the task of choosing what to say, i.e.
what information is to be included in a report (Re-
iter and Dale, 2000). We consider the task of auto-
matically generating feedback summaries for stu-
dents describing their performance during the lab
of a computer science module over the semester.
Various factors can influence students? learn-
ing such as difficulty of the material (Person et
al., 1995), workload (Craig et al., 2004), atten-
dance in lectures (Ames, 1992), etc. These fac-
tors change over time and can be interdependent.
In addition, different stakeholders often have con-
flicting goals, needs and preferences, for example
managers with employees, or doctors with patients
and relatives, or novice and expert users. In our
data, for instance, lecturers tend to comment on
the hours that the student studied, whereas the stu-
dents disprefer this content. In our previous work,
we showed that lecturers and students have dif-
ferent perceptions regarding what constitutes good
feedback (Gkatzia et al., 2013). Here, we present a
novel approach to generation by adapting its con-
tent to two user groups simultaneously. Producing
the same summary for two groups is important as
it allows for shared context and meaningful further
discussion and reduces development time.
2 Related Work
Previous work on NLG systems that address more
than one user group employs different versions of
a system for each different user group (Gatt et al.,
2009; Hunter et al., 2011; Mahamood and Reiter,
2011), makes use of User Models (Janarthanam
and Lemon, 2010; Thompson et al., 2004; Zuker-
man and Litman, 2001) or personalises the output
to individual users using rules (Reiter et al., 1999).
Our proposed system adapts the output to the pref-
erences of more than one user type1, lecturers and
students, but instead of developing many different
systems or using User Models that describe differ-
ent users, it attempts to model the middle ground
between the preferences.
In order to identify the users? preferences, we
apply Principal Components Regression (PCR
(Jolliffe, 1982)) analysis to two datasets that con-
tain lecturers? and students? ratings and identify
the most important variables from the principal
components, which are then included in a reward
function. This hand-crafted reward function is
used for training an RL agent for summarisation
1Our approach is different to multi-objective optimisa-
tion.
138
Raw Data
factors week 2 week 3 ... week 10
marks 5 4 ... 5
hours studied 1 2 ... 3
... ... ... ... ...
Trends from Data
factors trend
(1) marks trend other
(2) hours studied trend increasing
(3) understandability trend decreasing
(4) difficulty trend decreasing
(5) deadlines trend increasing
(6) health issues trend other
(7) personal issues trend decreasing
(8) lectures attended trend other
(9) revision trend decreasing
Summary
Your overall performance was excellent
during the semester. Keep up the good
work and maybe try some more challeng-
ing exercises. Your attendance was vary-
ing over the semester. Have a think about
how to use time in lectures to improve your
understanding of the material. You spent 2
hours studying the lecture material on
average. You should dedicate more time
to study. You seem to find the material
easier to understand compared to the
beginning of the semester. Keep up the
good work! You revised part of the learn-
ing material. Have a think whether revis-
ing has improved your performance.
Table 1: The table on the top left shows an example of the time-series data. The table on the bottom left
shows an example of described trends. The box on the right presents a target summary.
of time-series data. Our previous work showed
that when comparing RL and supervised learning
in the context of student feedback generation, stu-
dents preferred the output generated by the RL
system (Gkatzia et al., 2014a). Therefore, here, we
used RL rather than a supervised learning method.
The work described here builds on work reported
in (Gkatzia et al., 2014b), which uses as a reward
function the average of the Lecturer-adapted and
Student-adapted reward functions. However, that
method seems to cancel out the preferences of the
two groups whereas PCR is able to identify rele-
vant content for both groups.
In the next section, we describe the data used,
and the methodology for the multi-adaptive NLG,
as well as two alternative systems. In Section 4,
we describe the comparison of these three systems
in a subjective evaluation and present the results in
Section 5. A discussion follows in Section 6 and
finally, future work is discussed in Section 7.
3 Methodology
Reinforcement Learning is a machine learning
technique that defines how an agent learns to take
optimal sequences of actions so as to maximize a
cumulative reward (Sutton and Barto, 1998). In
our framework, the task of summarisation of time-
series data is modelled as a Markov Decision Pro-
cess, where the decisions on content selection cor-
respond to a sequence of actions (see Section 3.2).
Temporal Difference (TD) learning (Sutton and
Barto, 1990) is used for training three agents in
a simulated environment to learn to make optimal
content selection decisions:
1. by adapting to both groups simultaneously,
2. by adapting to lecturers,
3. by adapting to students.
3.1 The Data
For this study, the dataset described in (Gkatzia et
al., 2013) was used. Table 1 presents an exam-
ple of this dataset that describes a student?s learn-
ing factors and an aligned feedback summary pro-
vided by a lecturer. The dataset is composed of
37 similar instances. Each instance consists of
time-series information about the student?s learn-
ing routine and the selected templates that lec-
turers used to provide feedback to this particu-
lar student. A template is a quadruple consist-
ing of an id, a factor (bottom left of Ta-
ble 1), a reference type (trend, week, aver-
age, other) and surface text. For instance,
a template can be (1, marks, trend, ?Your marks
were <trend>over the semester?). The lexical
choice for <trend>(i.e. increasing or decreasing)
depends on the values of time-series data. There
is a direct mapping between the values of factor
139
and reference type and the surface text. The time-
series factors are listed in Table 1.
3.2 Actions and states
The state consists of the time-series data and the
number of factors which have so far been selected
to be talked about (the change of the value of this
variable consequently introduces a state change).
In order to explore the state space the agent se-
lects a time-series factor (e.g. marks, deadlines
etc.) and then decides whether to talk about it or
not, until all factors have been considered.
3.3 Reward function
The reward function is the following cumulative
multivariate function:
Reward = a+
n?
i=1
bi ? xi + c ? length
where X = {x1, x2, ..., xn} describes the cho-
sen combinations of the factor trends observed in
the time-series data and a particular template (i.e.
the way of mentioning a factor). a, b and c are the
correlation coefficients and length describes the
number of factors selected to be conveyed in the
feedback summary. The value of xi is given by
the function:
xi =
?
?
?
1, the combination of a factor trend
and a template type is included
0, if not.
The coefficients represent the level of preference
for a factor to be selected and the way it is con-
veyed in the summary. In the training phase, the
agent selects a factor and then decides whether to
talk about it or not. If the agent decides to refer
to a factor, the selection of the template is then
performed in a deterministic way, i.e. it selects the
template that results in higher reward.
Each rated summary is transformed into a vec-
tor of 91 binary features. Each feature describes
both (1) the trend of a factor (e.g. marks increas-
ing, see also Table 1) and (2) the way that this
factor could be conveyed in the summary (e.g.
one possible way is referring to average, another
possible way is referring to increasing/decreasing
trend). If both conditions are met, the value of
the feature is 1, otherwise 0. The 91 binary fea-
tures describe all the different possible combina-
tions. For both the Lecturer-adapted and Student-
adapted systems, the reward function is derived
from a linear regression analysis of the provided
dataset, similarly to Walker et al. (1997) and
Rieser et al. (2010).
3.3.1 Multi-adaptive Reward Function
In order to derive a reward function that finds a
balance between the two above mentioned sys-
tems, we use PCR to reduce the dimensionality
of the data and thus reduce the introduced noise.
Through PCR we are able to reduce the number
of features and identify components of factors that
are deemed important to both parties to be used in
the reward function.
PCR is a method that combines Principal Com-
ponent Analysis (PCA) (Jolliffe, 1986) with lin-
ear regression. PCA is a technique for reducing
the dataset dimensionality while keeping as much
of the variance as possible. In PCR, PCA is ini-
tially performed to identify the principal compo-
nents, in our case, the factors that contribute the
most to the variance. Then, regression is applied
to these principal components to obtain a vector
of estimated coefficients. Finally, this vector is
transformed back into the general linear regres-
sion equation. After performing this analysis on
both datasets (students and lecturers), we choose
the most important (i.e. the ones that contribute
the most to the variance) commoncomponents or
features resulting in 18 features which were used
in the reward function. We then design a hand-
crafted reward function taking into account this
PCR analysis. The five most important features
are shown in Table 2.
factor trend way it is mentioned
(1) marks stable average
(2) hours studied decreasing trend
(3) health issues decreasing weeks
(4) lectures attended stable average
(5) personal issues increasing trend
Table 2: The top 5 features out of the 18 selected
through PCR analysis.
4 Evaluation
FeedbackGen is evaluated with real users against
two alternative systems: one that adapts to lectur-
ers? preferences and one that adapts to students?
preferences. The output of the three systems is
ranked by 30 computer science students from a va-
riety of years of study. Time-series data of three
students are presented on graphs to each partici-
pant, along with three feedback summaries (each
one generated by a different system), in random
order, and they are asked to rank them in terms of
preference.
140
Student-adapted {Ranking: 1st*} FeedbackGen {Ranking: 2nd*} Lecturer-adapted {Ranking: 3rd*}
You did well at weeks 2, 3, 6, 8, 9 and 10,
but not at weeks 4, 5 and 7. Have a think
about how you were working well and
try to apply it to the other labs. Your at-
tendance was varying over the semester.
Have a think about how to use time in lec-
tures to improve your understanding of
the material. You found the lab exercises
not very challenging. You could try out
some more advanced material and exer-
cises. You dedicated more time study-
ing the lecture material in the beginning
of the semester compared to the end of
the semester. Have a think about what
is preventing you from studying. Revis-
ingmaterial during the semester will im-
prove your performance in the lab.
Your overall performance was
very good during the semester.
Keep up the good work and maybe
try some more challenging exer-
cises. You found the lab exer-
cises not very challenging. You
could try out some more advanced
material and exercises. You dedi-
cated more time studying the lec-
ture material in the beginning of
the semester compared to the end
of the semester. Have a think about
what is preventing you from study-
ing. You have had other dead-
lines during weeks 6 and 8. You
may want to plan your studying and
work ahead.
Your overall performance was very
good during the semester. Keep up the
good work and maybe try some more
challenging exercises. You found the
lab exercises not very challenging. You
could try out some more advanced mate-
rial and exercises. You dedicated more
time studying the lecture material in the
beginning of the semester compared to
the end of the semester. Have a think
about what is preventing you from study-
ing. You have had other deadlines during
weeks 6 and 8. You may want to plan
your studying and work ahead. You did
not face any health problems during the
semester. You did not face any personal
issues during the semester.
Table 3: The table presents example outputs from the three different systems in order of highest ranked
(bold signifies the chosen template content, * denotes significance with p <0.05 after comparing each
system with each other using Mann Whitney U test).
5 Results
Table 3 shows three summaries that have been
generated by the different systems. As we can see
from Table 3, students significantly prefer the out-
put of the system that is trained for their prefer-
ences. In contrast, students significantly dispre-
fer the system that is trained for lecturers? pref-
erences. Finally, they rank as second the system
that captures the preferences of both lecturers and
students, which shows that it might be feasible to
find middle ground between the preferences of two
user groups. Significance testing is done using
a Mann Whitney U test (p <0.05), performing a
pair-wise comparison.
6 Discussion
The weights derived from the linear regression
analysis vary from the Lecturer-adapted func-
tion to the Student-adapted function. For in-
stance, the lecturers? most preferred content is
hours studied. This, however, does not factor
heavily into the student?s reward function, apart
from the case where hours studied are de-
creasing or remain stable (see also Table 2).
Students like reading about
personal issues when the number of issues
they faced was increasing over the semester. On
the other hand, lecturers find it useful to give
advice to all students who faced personal issues
during the semester, hencepersonal issues
are included in the top 18 features (Table 2).
Moreover, students seem to mostly prefer a feed-
back summary that mentions the understandability
of the material when it increases, which is positive
feedback.
As reflected in Table 2, the analysis of PCR
showed that both groups found it useful to refer
to the average of marks when they remain stable.
In addition, both groups found understandability
when it increases useful, for a variety of reasons,
for example lecturers might find it useful to en-
courage students whereas students might prefer to
receive positive feedback. Both groups also agree
on hours studied as described earlier. On the
other hand, both groups find mentioning the stu-
dents? difficulty when it decreases as positive.
7 Future Work
In the future, we plan to evaluate our methodol-
ogy with lecturers and a larger sample of students
across different disciplines. Moreover, we aim to
port our methodology to a different domain, and
try to find the middle ground between the pref-
erences of novices and expert users when sum-
marising medical data while providing first aid.
Finally, we want to compare the methodology pre-
sented here to a multi-objective optimisation ap-
proach (Fonseca and Flemming, 1993), where the
preferences of each user group will be modelled as
two different optimisation functions.
Acknowledgements
The research leading to this work has re-
ceived funding from the EC?s FP7 programme:
(FP7/2011-14) under grant agreement no. 248765
(Help4Mood).
141
References
Carole Ames. 1992. Classrooms: Goals, structures,
and student motivation. Journal of Educational Psy-
chology, 84(3):p261?71.
Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,
and Barry Gholson. 2004. Affect and learning: an
exploratory look into the role of affect in learning
with autotutor.
Carlos Fonseca and Peter Flemming. 1993. Genetic
algorithms for multiobjective optimization: Formu-
lation, discussion and generalization. In 5th Inter-
national Conference on Genetic Algorithms.
Albert Gatt, Francois Portet, Ehud Reiter, James
Hunter, Saad Mahamood, Wendy Moncur, and So-
mayajulu Sripada. 2009. From data to text in the
neonatal intensive care unit: Using NLG technology
for decision support and information management.
AI Communications, 22: 153-186.
Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-
narthanam, and Oliver Lemon. 2013. Generating
student feedback from time-series data using Rein-
forcement Learning. In 14th European Workshop in
Natural Language Generation (ENLG).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014a. Comparing multi-label classification with
reinforcement learning for summarisation of time-
series data. In 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.
2014b. Finding Middle Ground? Multi-objective
Natural Language Generation from Time-series
data. In 14th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL).
Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,
Cindy Sykes, and D Westwater. 2011. Bt-nurse:
Computer generation of natural language shift sum-
maries from complex heterogeneous medical data.
American Medical Informatics Association, 18:621-
624.
Srinivasan Janarthanam and Oliver Lemon. 2010.
Adaptive referring expression generation in spoken
dialogue systems: Evaluation with real users. In
11th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL).
Ian T. Jolliffe. 1982. A note on the use of principal
components in regression. Journal of the Royal Sta-
tistical Society, Series C: 31(3):300?303.
Ian Jolliffe. 1986. Principal Component Analysis.
Springer-Verlag.
Saad Mahamood and Ehud Reiter. 2011. Generating
Affective Natural Language for Parents of Neona-
tal Infants. In 13th European Workshop in Natural
Language Generation (ENLG).
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and
Arthur C. Graesser. 1995. Pragmatics and peda-
gogy: Conversational rules and politeness strategies
may inhibit effective tutoring. Journal of Cognition
and Instruction, 13(2):161-188.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral language generation systems. Cambridge Uni-
versity Press.
Ehud Reiter, Roma Robertson, and Liesl Osman. 1999.
Types of knowledge required to personalise smoking
cessation letters. Artificial Intelligence in Medicine:
Proceedings of the Joint European Conference on
Artificial Intelligence in Medicine and Medical De-
cision Making.
Verena Rieser, Oliver Lemon, and Xingkun Liu. 2010.
Optimising information presentation for spoken dia-
logue systems. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Richard Sutton and Andrew Barto. 1990. Time deriva-
tive models of pavlovian reinforcement. Learning
and Computational Neuroscience: Foundations of
Adaptive Networks, pages 497?537.
Richart Sutton and Andrew Barto. 1998. Reinforce-
ment learning. MIT Press.
Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-
gley. 2004. A personalised system for conversa-
tional recommendations. Journal of Artificial Intel-
ligence Research, 21(1).
Marilyn Walker, Diane J Litman, Candace Kamm, and
Alicia Abella. 1997. PARADISE: A framework
for evaluating spoken dialogue agents. In 8th con-
ference on European chapter of the Association for
Computational Linguistics (EACL).
Ingrid Zukerman and Diane Litman. 2001. Natu-
ral language processing and user modeling: Syner-
gies and limitations. In User Modeling and User-
Adapted Interaction, 11(1-2).
142
