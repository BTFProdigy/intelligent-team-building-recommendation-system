Determining the Specificity of Terms using Compositional and Con-
textual Information 
 
Pum-Mo Ryu 
Department of Electronic Engineering and Computer Science 
KAIST 
Pum-Mo.Ryu@kaist.ac.kr 
 
 
Abstract 
This paper introduces new specificity de-
termining methods for terms using com-
positional and contextual information. 
Specificity of terms is the quantity of 
domain specific information that is con-
tained in the terms. The methods are 
modeled as information theory like meas-
ures. As the methods don?t use domain 
specific information, they can be applied 
to other domains without extra processes. 
Experiments showed very promising re-
sult with the precision of 82.0% when the 
methods were applied to the terms in 
MeSH thesaurus. 
1. Introduction 
Terminology management concerns primarily 
with terms, i.e., the words that are assigned to 
concepts used in domain-related texts. A term is 
a meaningful unit that represents a specific con-
cept within a domain (Wright, 1997). 
Specificity of a term represents the quantity of 
domain specific information contained in the 
term. If a term has large quantity of domain spe-
cific information, specificity value of the term is 
large; otherwise specificity value of the term is 
small. Specificity of term X is quantified to posi-
tive real number as equation (1). 
( )Spec X R+?                      (1) 
Specificity of terms is an important necessary 
condition in term hierarchy, i.e., if X1 is one of 
ancestors of X2, then Spec(X1) is less than 
Spec(X2). Specificity can be applied in automatic 
construction and evaluation of term hierarchy.  
When domain specific concepts are repre-
sented as terms, the terms are classified into two 
categories based on composition of unit words. In 
the first category, new terms are created by add-
ing modifiers to existing terms. For example ?in-
sulin-dependent diabetes mellitus? was created 
by adding modifier ?insulin-dependent? to its 
hypernym ?diabetes mellitus? as in Table 1. In 
English, the specific level terms are very com-
monly compounds of the generic level term and 
some modifier (Croft, 2004). In this case, compo-
sitional information is important to get their 
meaning. In the second category, new terms are 
created independently to existing terms. For ex-
ample, ?wolfram syndrome? is semantically re-
lated to its ancestor terms as in Table 1. But it 
shares no common words with its ancestor terms. 
In this case, contextual information is used to 
discriminate the features of the terms.  
 
Node Number Terms 
C18.452.297 diabetes mellitus 
C18.452.297.267 insulin-dependent diabetes mellitus 
C18.452.297.267.960 wolfram syndrome 
Table 1.   Subtree of MeSH1 tree. Node numbers 
represent hierarchical structure of terms 
 
Contextual information has been mainly used 
to represent the characteristics of terms. (Cara-
ballo, 1999A) (Grefenstette, 1994) (Hearst, 1992) 
(Pereira, 1993) and (Sanderson, 1999) used con-
textual information to find hyponymy relation 
between terms. (Caraballo, 1999B) also used 
contextual information to determine the specific-
ity of nouns. Contrary, compositional informa-
tion of terms has not been commonly discussed. 
                                                          
1 MeSH is available at  http://www.nlm.nih.gov/mesh. MeSH 2003 was used 
in this research. 
We propose new specificity measuring meth-
ods based on both compositional and contextual 
information. The methods are formulated as in-
formation theory like measures. Because the 
methods don't use domain specific information, 
they are easily adapted to terms of other domains. 
This paper consists as follow: compositional 
and contextual information is discussed in section 
2, information theory like measures are described 
in section 3, experiment and evaluation is dis-
cussed in section 4, finally conclusions are drawn 
in section 5. 
2. Information for Term Specificity 
In this section, we describe compositional infor-
mation and contextual information. 
2.1. Compositional Information 
By compositionality, the meaning of whole term 
can be strictly predicted from the meaning of the 
individual words (Manning, 1999). Many terms 
are created by appending modifiers to existing 
terms. In this mechanism, features of modifiers 
are added to features of existing terms to make 
new concepts. Word frequency and tf.idf value 
are used to quantify features of unit words. Inter-
nal modifier-head structure of terms is used to 
measure specificity incrementally. 
We assume that terms composed of low fre-
quency words have large quantity of domain in-
formation. Because low frequency words appear 
only in limited number of terms, the words can 
clearly discriminate the terms to other terms. 
tf.idf, multiplied value of term frequency (tf) 
and inverse document frequency (idf), is widely 
used term weighting scheme in information re-
trieval (Manning, 1999). Words with high term 
frequency and low document frequency get large 
tf.idf value. Because a document usually dis-
cusses one topic, and words of large tf.idf values 
are good index terms for the document, the words 
are considered to have topic specific information. 
Therefore, if a term includes words of large tf.idf 
value, the term is assumed to have topic or do-
main specific information. 
If the modifier-head structure of a term is 
known, the specificity of the term is calculated 
incrementally starting from head noun. In this 
manner, specificity value of a term is always lar-
ger than that of the base (head) term. This result 
answers to the assumption that more specific 
term has larger specificity value. However, it is 
very difficult to analyze modifier-head structure 
of compound noun. We use simple nesting rela-
tions between terms to analyze structure of terms. 
A term X is nested to term Y, when X is substring 
of Y (Frantzi, 2000) as follows: 
 
Definition 1 If two terms X and Y are terms in 
same category and X is nested in Y as W1XW2, 
then X is base term, and W1 and W2 are modifiers 
of X. 
 
For example two terms, ?diabetes mellitus? 
and ?insulin dependent diabetes mellitus?, are all 
disease names, and the former is nested in the 
latter. In this case, ?diabetes mellitus? is base 
term and ?insulin dependent? is modifier of ?in-
sulin dependent diabetes mellitus? by definition 1. 
If multiple terms are nested in a term, the longest 
term is selected as head term. Specificity of Y is 
measured as equation (2). 
1 2( ) ( ) ( ) ( )Spec Y Spec X Spec W Spec W? ?= + ? + ? (2) 
where Spec(X), Spec(W1), and Spec(W2) are 
specificity values of X, W1, W2 respectively. ?  
and ? , real numbers between 0 and 1, are 
weighting schemes for specificity of modifiers. 
They are obtained experimentally. 
2.2. Contextual Information 
There are some problems that are hard to address 
using compositional information alone. Firstly, 
although features of ?wolfram syndrome? share 
many common features with features of ?insulin-
dependent diabetes mellitus? in semantic level, 
they don?t share any common words in lexical 
level. In this case, it is unreasonable to compare 
two specificity values measured based on compo-
sitional information alone. Secondly, when sev-
eral words are combined to a term, there are 
additional semantic components that are not pre-
dicted by unit words. For example, ?wolfram 
syndrome? is a kind of ?diabetes mellitus?. We 
can not predict ?diabetes mellitus? from two 
separate words ?wolfram? and ?syndrome?. Fi-
nally, modifier-head structure of some terms is 
ambiguous. For instance, ?vampire slayer? might 
be a slayer who is vampire or a slayer of vam-
pires. Therefore contextual is used to comple-
ment these problems. 
Contextual information is distribution of sur-
rounding words of target terms. For example, the 
distribution of co-occurrence words of the terms, 
the distribution of predicates which have the 
terms as arguments, and the distribution of modi-
fiers of the terms are contextual information. 
General terms usually tend to be modified by 
other words. Contrary, domain specific terms 
don?t tend to be modified by other words, be-
cause they have sufficient information in them-
selves (Caraballo, 1999B). Under this assumption, 
we use probabilistic distribution of modifiers as 
contextual information. Because domain specific 
terms, unlike general words, are rarely modified 
in corpus, it is important to collect statistically 
sufficient modifiers from given corpus. Therefore 
accurate text processing, such as syntactic pars-
ing, is needed to extract modifiers. As Cara-
ballo?s work was for general words, they 
extracted only rightmost prenominals as context 
information. We use Conexor functional depend-
ency parser (Conexor, 2004) to analyze the struc-
ture of sentences. Among many dependency 
functions defined in Conexor parser, ?attr? and 
?mod? functions are used to extract modifiers 
from analyzed structures. If a term or modifiers 
of the term do not occur in corpus, specificity of 
the term can not be measured using contextual 
information 
3. Specificity Measuring Methods 
In this section, we describe information theory 
like methods using compositional and contextual 
information. Here, we call information theory 
like methods, because some probability values 
used in these methods are not real probability, 
rather they are relative weight of terms or words. 
Because information theory is well known for-
malism describing information, we adopt the 
mechanism to measure information quantity of 
terms. 
In information theory, when a message with 
low probability occurs on channel output, the 
amount of surprise is large, and the length of bits 
to represent this message becomes long. There-
fore the large quantity of information is gained 
by this message (Haykin, 1994). If we consider 
the terms in a corpus as messages of a channel 
output, the information quantity of the terms can 
be measured using various statistics acquired 
from the corpus. A set of terms is defined as 
equation (3) for further explanation. 
{ |1 }kT t k n= ? ?                   (3) 
where tk is a term and n  is total number of terms. 
In next step, a discrete random variable X is de-
fined as equation (4). 
{ |1 }
( ) Prob( )
k
k k
X x k n
p x X x
= ? ?
= =                 (4) 
where xk is an event of a term tk occurs in corpus, 
p(xk) is the probability of event xk. The informa-
tion quantity, I(xk), gained after observing the 
event xk, is defined by the logarithmic function. 
Finally I(xk) is used as specificity value of tk as 
equation (5). 
( ) ( ) log ( )k k kSpec t I x p x? = ?        (5) 
In equation (5), we can measure specificity of 
tk, by estimating p(xk). We describe three estimat-
ing methods of p(xk) in following sections. 
3.1. Compositional Information based 
Method (Method 1) 
In this section, we describe a method using com-
positional information introduced in section 2.1. 
This method is divided into two steps: In the first 
step, specificity values of all words are measured 
independently. In the second step, the specificity 
values of words are summed up. For detail de-
scription, we assume that a term tk consists of one 
or more words as equation (6). 
1 2...k mt w w w=                       (6) 
where wi is i-th word in tk. In next step, a discrete 
random variable Y is defined as equation (7). 
{ |1 }
( ) Prob( )
i
i i
Y y i m
p y Y y
= ? ?
= =                (7) 
where yi is an event of a word wi occurs in term tk, 
p(yi) is the probability of event yi. Information 
quantity, I(xk), in equation (5) is redefined as 
equation (8) based on previous assumption. 
1
( ) ( ) log ( )
m
k i i
i
I x p y p y
=
= ??          (8) 
where I(xk) is average information quantity of all 
words in tk. Two information sources, word fre-
quency, tf.idf are used to estimate p(yi). In this 
mechanism, p(yi) for informative words should 
be smaller than that of non informative words. 
When word frequency is used to quantify fea-
tures of words, p(yi) in equation (8) is estimated 
as equation (9). 
( )( ) ( )
( )
i
i MLE i
j
j
freq wp y p w
freq w
? = ?         (9) 
where freq(w) is frequency of word w in corpus, 
PMLE(wi) is maximum likelihood estimation of 
P(wi), and j is index of all words in corpus. In 
this equation, as low frequency words are infor-
mative, P(yi) for the words becomes small. 
When tf.idf is used to quantify features of 
words, p(yi) in equation (8) is estimated as equa-
tion (10). 
( )( ) ( ) 1
( )
i
i MLE i
j
j
tf idf wp y p w
tf idf w
?? = ? ??    (10) 
where tf?idf(w) is tf.idf value of word w. In this 
equation, as words of large tf.idf values are in-
formative, p(yi) of the words becomes small. 
3.2. Contextual Information based Method 
(Method 2)  
In this section, we describe a method using con-
textual information introduced in section 2.2. 
Entropy of probabilistic distribution of modifiers 
for a term is defined as equation (11). 
( ) ( , ) log ( , )mod k i k i k
i
H t p mod t p mod t= ??  (11) 
where p(modi,tk) is the probability of modi modi-
fies tk and is estimated as equation (12). 
( , )( , )
( , )
i k
MLE i k
j k
j
freq mod tp mod t
freq mod t
= ?      (12) 
where freq(modi,tk) is number of frequencies that 
modi modifies tk in corpus, j is index of all modi-
fiers of tk in corpus. The entropy calculated by 
equation (11) is the average information quantity 
of all (modi,tk) pairs. Specific terms have low en-
tropy, because their modifier distributions are 
simple. Therefore inversed entropy is assigned to 
I(xk) in equation (5) to make specific terms get 
large quantity of information as equation (13). 
1
( ) max( ( )) ( )k mod i mod ki nI x H t H t? ?? ?       (13) 
where the first term of approximation is the 
maximum value among modifier entropies of all 
terms. 
3.3. Hybrid Method (Method 3) 
In this section, we describe a hybrid method to 
overcome shortcomings of previous two methods. 
This method measures term specificity as equa-
tion (14). 
1( ) 1 1( ) (1 )( )
( ) ( )
k
Cmp k Ctx k
I x
I x I x
? ?
?
+ ?
  (14) 
where ICmp(xk) and ICtx(xk) are normalized I(xk) 
values between 0 and 1, which are measured by 
compositional and contextual information based 
methods respectively. (0 1)? ?? ?  is weight of two 
values. If 0.5? = , the equation is harmonic mean 
of two values. Therefore I(xk) becomes large 
when two values are equally large. 
4. Experiment and Evaluation 
In this section, we describe the experiments and 
evaluate proposed methods. For convenience, we 
simply call compositional information based 
method, contextual information based method, 
hybrid method as method 1, method 2, method 3 
respectively.  
4.1. Evaluation 
A sub-tree of MeSH thesaurus is selected for ex-
periment. ?metabolic diseases(C18.452)? node is 
root of the subtree, and the subtree consists of 
436 disease names which are target terms of 
specificity measuring. A set of journal abstracts 
was extracted from MEDLINE2 database using 
the disease names as quires. Therefore, all the 
abstracts are related to some of the disease names. 
The set consists of about 170,000 abstracts 
(20,000,000 words). The abstracts are analyzed 
using Conexor parser, and various statistics are 
extracted: 1) frequency, tf.idf of the disease 
names, 2) distribution of modifiers of the disease 
names, 3) frequency, tf.idf of unit words of the 
disease names. 
The system was evaluated by two criteria, 
coverage and precision. Coverage is the fraction 
                                                          
2 MEDLINE is a database of biomedical articles serviced by National Library 
of Medicine, USA. (http://www.nlm.nih.gov) 
of the terms which have specificity values by 
given measuring method as equation (15). 
#     
#    
of terms with specificityc
of all terms
=        (15) 
Method 2 gets relatively lower coverage than 
method 1, because method 2 can measure speci-
ficity when both the terms and their modifiers 
appear in corpus. Contrary, method 1 can meas-
ure specificity of the terms, when parts of unit 
words appear in corpus. Precision is the fraction 
of relations with correct specificity values as 
equation (16). 
#   ( , )   
#    ( , )
of R p c with correct specificityp
of all R p c
=  (16) 
where R(p,c) is a parent-child relation in MeSH 
thesaurus, and this relation is valid only when 
specificity of two terms are measured by given 
method. If child term c has larger specificity 
value than that of parent term p, then the relation 
is said to have correct specificity values. We di-
vided parent-child relations into two types. Rela-
tions where parent term is nested in child term 
are categorized as type I. Other relations are 
categorized as type II. There are 43 relations in 
type I and 393 relations in type II. The relations 
in type I always have correct specificity values 
provided structural information method described 
section 2.1 is applied. 
We tested prior experiment for 10 human sub-
jects to find out the upper bound of precision. 
The subjects are all medical doctors of internal 
medicine, which is closely related division to 
?metabolic diseases?. They were asked to iden-
tify parent-child relation of given two terms. The 
average precisions of type I and type II were 
96.6% and 86.4% respectively. We set these val-
ues as upper bound of precision for suggested 
methods.  
Specificity values of terms were measured 
with method 1, method 2, and method 3 as Table 
2. In method 1, word frequency based method, 
word tf.idf based method, and structure informa-
tion added methods were separately experi-
mented. Two additional methods, based on term 
frequency and term tf.idf, were experimented to 
compare compositionality based method and 
whole term based method. Two methods which 
showed the best performance in method 1 and 
method 2 were combined into method 3. 
Word frequency and tf.idf based method 
showed better performance than term based 
methods. This result indicates that the informa-
tion of terms is divided into unit words rather 
than into whole terms. This result also illustrate 
basic assumption of this paper that specific con-
cepts are created by adding information to exist-
ing concepts, and new concepts are expressed as 
new terms by adding modifiers to existing terms. 
Word tf.idf based method showed better preci-
sion than word frequency based method. This 
result illustrate that tf.idf of words is more infor-
mative than frequency of words. 
Method 2 showed the best performance, preci-
sion 70.0% and coverage 70.2%, when we 
counted modifiers which modify the target terms 
two or more times. However, method 2 showed 
worse performance than word tf.idf and structure 
based method. It is assumed that sufficient con-
textual information for terms was not collected 
from corpus, because domain specific terms are 
rarely modified by other words. 
Method 3, hybrid method of method 1 (tf.idf 
of words, structure information) and method 2, 
showed the best precision of 82.0% of all, be-
cause the two methods interacted complementary. 
Precision 
Methods 
Type I Type II Total 
Coverage
Human subjects(Average) 96.6 86.4 87.4  
Term frequency 100.0 53.5 60.6 89.5 
Term tf?idf 52.6 59.2 58.2 89.5 
Word Freq. 0.37 72.5 69.0 100.0 
Word Freq.+Structure (?=?=0.2) 100.0 72.8 75.5 100.0 
Word tf?idf 44.2 75.3 72.2 100.0 
Compositional 
Information 
Method 
(Method 1) Word tf?idf +Structure (?=?=0.2) 100.0 76.6 78.9 100.0 
Contextual Information Method (Method 2) (mod cnt>1) 90.0 66.4 70.0 70.2 
Hybrid Method (Method 3)  (tf?idf + Struct, ?=0.8) 95.0 79.6 82.0 70.2 
Table 2. Experimental results (%) 
The coverage of this method was 70.2% which 
equals to the coverage of method 2, because the 
specificity value is measured only when the 
specificity of method 2 is valid. In hybrid method, 
the weight value 0.8? =  indicates that composi-
tional information is more informatives than con-
textual information when measuring the 
specificity of domain-specific terms. The preci-
sion of 82.0% is good performance compared to 
upper bound of 87.4%.  
4.2. Error Analysis 
One reason of the errors is that the names of 
some internal nodes in MeSH thesaurus are cate-
gory names rather disease names. For example, 
as ?acid-base imbalance (C18.452.076)? is name 
of disease category, it doesn't occur as frequently 
as other real disease names. 
Other predictable reason is that we didn?t con-
sider various surface forms of same term. For 
example, although ?NIDDM? is acronym of ?non 
insulin dependent diabetes mellitus?, the system 
counted two terms independently. Therefore the 
extracted statistics can?t properly reflect semantic 
level information. 
If we analyze morphological structure of terms, 
some errors can be reduced by internal structure 
method described in section 2.1. For example, 
?nephrocalcinosis? have modifier-head structure 
in morpheme level; ?nephro? is modifier and 
?calcinosis? is head. Because word formation 
rules are heavily dependent on the domain spe-
cific morphemes, additional information is 
needed to apply this approach to other domains. 
5. Conclusions 
This paper proposed specificity measuring meth-
ods for terms based on information theory like 
measures using compositional and contextual 
information of terms. The methods are experi-
mented on the terms in MeSH thesaurus. Hybrid 
method showed the best precision of 82.0%, be-
cause two methods complemented each other. As 
the proposed methods don't use domain depend-
ent information, the methods easily can be 
adapted to other domains. 
In the future, the system will be modified to 
handle various term formations such as abbrevi-
ated form. Morphological structure analysis of 
words is also needed to use the morpheme level 
information. Finally we will apply the proposed 
methods to terms of other domains and terms in 
general domains such as WordNet. 
Acknowledgements 
This work was supported in part by Ministry of 
Science & Technology of Korean government 
and Korea Science & Engineering Foundation. 
References  
Caraballo, S. A. 1999A. Automatic construction of a 
hypernym-labeled noun hierarchy from text Cor-
pora. In the proceedings of ACL 
Caraballo, S. A.  and Charniak, E. 1999B. Determin-
ing the Specificity of Nouns from Text. In the pro-
ceedings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language Processing 
and Very Large Corpora 
Conexor. 2004. Conexor Functional Dependency 
Grammar Parser. http://www.conexor.com 
Frantzi, K., Anahiadou, S. and Mima, H. 2000. Auto-
matic recognition of multi-word terms: the C-
value/NC-value method. Journal of Digital Librar-
ies, vol. 3, num. 2 
Grefenstette, G. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers 
Haykin, S. 1994. Neural Network. IEEE Press, pp. 444 
Hearst, M. A. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. In proceedings of 
ACL 
Manning, C. D. and Schutze, H. 1999. Foundations of 
Statistical Natural Language Processing. The MIT 
Presss 
Pereira, F., Tishby, N., and Lee, L. 1993. Distributa-
tional clustering of English words. In the proceed-
ings of ACL 
Sanderson, M. 1999. Deriving concept hierarchies 
from text. In the Proceedings of the 22th Annual 
ACM S1GIR Conference on Research and Devel-
opment in Information Retrieval 
Wright, S. E., Budin, G.. 1997. Handbook of Term 
Management: vol. 1. John Benjamins publishing 
company 
William Croft. 2004. Typology and Universals. 2nd ed. 
Cambridge Textbooks in Linguistics, Cambridge 
Univ. Press 
Determining the Specificity of Terms based on Information Theoretic 
Measures 
Pum-Mo Ryu and Key-Sun Choi 
Dept. EECS/KORTERM KAIST 
373-1 Guseong-dong Yuseong-gu 
305-701 Daejeon 
Korea 
pmryu@world.kaist.ac.kr, kschoi@world.kaist.ac.kr 
 
 
Abstract 
This paper introduces new specificity 
determining methods for terms based on 
information theoretic measures. The 
specificity of terms represents the quantity of 
domain specific information that is contained 
in the terms. Compositional and contextual   
information of terms are used in proposed 
methods. As the methods don?t rely on domain 
dependent information, they can be applied to 
other domains without extra processes.  
Experiments showed very promising results 
with the precision 82.0% when applied to the 
terms in MeSH thesaurus. 
1 Introduction 
The specificity of terms represents the quantity of 
domain specific information contained in the terms. 
If a term has large quantity of domain specific 
information, the specificity of the term is high. The 
specificity of a term X is quantified to positive real 
number as equation (1). 
( )Spec X R+?                           (1) 
The specificity is a kind of necessary condition 
for term hierarchy, i.e., if X1 is one of ancestors of 
X2, then Spec(X1) is less than Spec(X2). Thus this 
condition can be applied to automatic construction 
or evaluation of term hierarchy. The specificity 
also can be applied to automatic term recognition. 
Many domain specific terms are multiword 
terms. When domain specific concepts are 
represented as multiword terms, the terms are 
classified into two categories based on composition 
of unit words. In the first category, new terms are 
created by adding modifiers to existing terms. For 
example ?insulin-dependent diabetes mellitus? was 
created by adding modifier ?insulin-dependent? to 
its hypernym ?diabetes mellitus? as in Table 1. In 
English, the specific level terms are very 
commonly compounds of the generic level term 
and some modifier (Croft, 2004). In this case, 
compositional information is important to get 
meaning of the terms. In the second category, new 
terms are independent of existing terms. For 
example, ?wolfram syndrome? is semantically 
related to its ancestor terms as in Table 1. But it 
shares no common words with its ancestor terms. 
In this case, contextual information is important to 
get meaning of the terms. 
 
Node Number Terms 
C18.452.297 diabetes mellitus 
C18.452.297.267 insulin-dependent diabetes mellitus 
C18.452.297.267.960 wolfram syndrome 
Table 1 Subtree of MeSH1 thesaurus. Node 
numbers represent hierarchical structure of terms 
Contextual information has been mainly used to 
represent the meaning of terms in previous works. 
(Grefenstette, 1994) (Pereira, 1993) and 
(Sanderson, 1999) used contextual information to 
find hyponymy relation between terms. (Caraballo, 
1999) also used contextual information to 
determine the specificity of nouns. Contrary, 
compositional information of terms has not been 
commonly discussed. We propose new specificity 
measuring methods based on both compositional 
and contextual information. The methods are 
formulated as information theory like measures. 
This paper consists as follow; new specificity 
measuring methods are introduced in section 2, and 
the experiments and evaluation on the methods are 
discussed in section 3, finally conclusions are 
drawn in section 4. 
2 Specificity Measuring Methods 
In this section, we describe information theory like 
methods to measure the specificity of terms. Here, 
we call information theory like methods, because 
some probability values used in these methods are 
                                                     
1 MeSH is available at http://www.nlm.nih.gov/mesh. 
MeSH 2003 was used in this research. 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 87
not real probability, rather they are relative weight 
of terms or words.  
In information theory, when a low probability 
message occurs on channel output, the quantity of 
surprise is large, and the length of bits to represent 
the message becomes long. Thus the large quantity 
of information is gained by the message (Haykin, 
1994). If we regard the terms in corpus as the 
messages of channel output, the information 
quantity of the terms can be measured using 
information theory. A set of target terms is defined 
as equation (2) for further explanation. 
{ |1 }kT t k n= ? ?                         (2) 
where tk is a term. In next step, a discrete random 
variable X is defined as equation (3). 
{ |1 }   ( ) Prob( )k k kX x k n p x X x= ? ? = =    (3) 
where xk is an event of tk is observed in corpus, 
p(xk) is the probability of xk. The information 
quantity, I(xk), gained after observing xk, is used as 
the specificity of tk as equation (4). 
( ) ( ) log ( )k k kSpec t I x p x? = ?              (4) 
By equation (4), we can measure the specificity 
of tk, by estimating p(xk). We describe three 
estimating methods for p(xk) in following sections. 
2.1 Compositional Information based Method 
(Method 1) 
By compositionality, the meaning of a term can be 
strictly predicted from the meaning of the 
individual words (Manning, 1999). This method is 
divided into two steps: In the first step, the 
specificity of each word is measured independently. 
In the second step, the specificity of composite 
words is summed up. For detail description, we 
assume that tk consists of one or more words as 
equation (5). 
1 2...k mt w w w=                           (5) 
where wi is i-th word in tk. In next step, a discrete 
random variable Y is defined as equation (6). 
{ |1 }   ( ) Prob( )i i iY y i m p y Y y= ? ? = =       (6) 
where yi is an event of wi occurs in term tk, p(yi) is 
the probability of yi. Information quantity, I(xk), in 
equation (4) is redefined as equation (7) based on 
previous assumption. 
1
( ) ( ) log ( )
m
k i i
i
I x p y p y
=
= ??                 (7) 
where I(xk) is average information quantity of all 
words in tk. In this mechanism, p(yi) of informative 
words should be smaller than that of non 
informative words. Two information sources, word 
frequency, tf.idf are used to estimate p(yi)  
independently. 
We assume that if a term is composed of low 
frequency words, the term have large quantity of 
domain information. Because low frequency words 
appear in limited number of terms, they have high 
discriminating ability. On this assumption, p(yi) in 
equation (7) is estimated as relative frequency of wi 
in corpus. In this estimation, P(yi) for low 
frequency words becomes small. 
tf.idf is widely used term weighting scheme in 
information retrieval (Manning, 1999). We assume 
that if a term is composed of high tf.idf words, the 
term have domain specific information. On this 
assumption, p(yi) in equation (7) is estimated as 
equation (8). 
( )( ) ( ) 1
( )
i
i MLE i
j
j
tf idf wp y p w
tf idf w
?? = ? ??        (8) 
where tf?idf(w) is tf.idf value of w. In this equation, 
p(yi) of high tf.idf words becomes small. 
If the modifier-head structure is known, the 
specificity of the term is calculated incrementally 
starting from head noun. In this manner, the 
specificity of the term is always larger than that of 
the head term. This result answers to the 
assumption that more specific term has higher 
specificity. We use simple nesting relations 
between terms to analyze modifier-head structure 
as follows (Frantzi, 2000): 
 
Definition 1 If two terms X and Y are terms in 
same semantic category and X is nested in Y as 
W1XW2, then X is head term, and W1 and W2 are 
modifiers of X. 
 
For example, because ?diabetes mellitus? is 
nested in ?insulin dependent diabetes mellitus? and 
two terms are all disease names, ?diabetes 
mellitus? is head term and ?insulin dependent? is 
modifier. The specificity of Y is measured as 
equation (9). 
1 2( ) ( ) ( ) ( )Spec Y Spec X Spec W Spec W? ?= + ? + ?    (9) 
where Spec(X), Spec(W1), and Spec(W2) are the 
specificity of X, W1, W2 respectively. ?  and ?  are 
weighting schemes for the specificity of modifiers. 
They are found by experimentally. 
2.2 Contextual Information based Method 
(Method 2)  
There are some problems that are hard to address 
using compositional information alone. Firstly, 
although two disease names, ?wolfram syndrome? 
and ?insulin-dependent diabetes mellitus?, share 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology88
many common features in semantic level, they 
don?t share any common words in lexical level. In 
this case, it is unreasonable to compare two 
specificity values based on compositional 
information. Secondly, when several words are 
combined into one term, there are additional 
semantic components that are not predicted by unit 
words. For example, ?wolfram syndrome? is a kind 
of ?diabetes mellitus?. We can not predict the 
meaning of ?diabetes mellitus? from two separate 
words ?wolfram? and ?syndrome?. Thus we use 
contextual information to address these problems. 
General terms are frequently modified by other 
words in corpus. Because domain specific terms 
have sufficient information in themselves, they are 
rarely modified by other words, (Caraballo, 1999). 
Under this assumption, we use probability 
distribution of modifiers as contextual information. 
Collecting sufficient modifiers from given corpus 
is very important in this method. To this end, we 
use Conexor functional dependency parser 
(Conexor, 2004) to analyze the structure of 
sentences. Among many dependency functions 
defined in the parser, ?attr? and ?mod? functions 
are used to extract modifiers from analyzed 
structures. This method can be applied the terms 
that are modified by other words in corpus. 
Entropy of modifiers for a term is defined as 
equation (10). 
( ) ( , ) log ( , )mod k i k i k
i
H t p mod t p mod t= ??    (10) 
where p(modi,tk) is the probability of modi modifies 
tk and it is estimated as relative frequency of modi 
in all modifiers of tk. The entropy calculated by 
equation (10) is the average information quantity 
of all (modi,tk) pairs. Because domain specific 
terms have simple modifier distributions, the 
entropy of the terms is low. Therefore inversed 
entropy is assigned to I(xk) in equation (4) to make 
specific terms get large quantity of information. 
1
( ) max( ( )) ( )k mod i mod ki nI x H t H t? ?? ?           (11) 
where the first term of approximation is the 
maximum modifier entropy of all terms. 
2.3 Hybrid Method (Method 3) 
In this section, we describe hybrid method to 
overcome shortcomings of previous two methods. 
In this method the specificity is measured as 
equation (12). 
1( ) 1 1( ) (1 )( )
( ) ( )
k
Cmp k Ctx k
I x
I x I x
? ?
?
+ ?
      (12) 
where ICmp(xk) and ICtx(xk) are information quantity 
measured by method1 and method 2 respectively. 
They are normalized value between 0 and 1. 
(0 1)? ?? ?  is weight of two values. If 0.5? = , the 
equation is harmonic mean of two values. 
Therefore I(xk) becomes large when two values are 
equally large. 
3 Experiments and Evaluation 
In this section, we describe our experiments and 
evaluate proposed methods.  
We select a subtree of MeSH thesaurus for the 
experiment. ?metabolic diseases(C18.452)? node is 
root of the subtree, and the subtree consists of 436 
disease names which are target terms for 
specificity measuring. We used MEDLINE 2 
database corpus (170,000 abstracts, 20,000,000 
words) to extract statistical information. 
Each method was evaluated by two criteria, 
coverage and precision. Coverage is the fraction of 
the terms which have the specificity by given 
method. Method 2 gets relatively lower coverage 
than method 1, because method 2 can measure the 
specificity only when both the terms and their 
modifiers occur in corpus. Method 1 can measure 
the specificity whenever parts of composite words 
appear in corpus. Precision is the fraction of 
correct specificity relations values as equation (13). 
#   ( , )   
#    ( , )
of R p c with correct specificityp
of all R p c
=   (13) 
where R(p,c) is a parent-child relation in MeSH 
thesaurus. If child term c has larger specificity than 
that of parent term p, then the relation is said to 
have correct specificity. We divided parent-child 
relations into two types. Relations where parent 
term is nested in child term are categorized as type 
I. Other relations are categorized as type II. There 
are 43 relations in type I and 393 relations in type 
II. The relations in type I always have correct 
specificity provided modifier-head information 
method described in section 2.1 is applied. 
We tested prior experiment for 10 human 
subjects to find out the upper bound of precision. 
The subjects are all medical doctors of internal 
medicine, which is closely related division to 
?metabolic diseases?. They were asked to identify 
parent-child relationship for given term pairs. The 
average precisions of type I and type II were 
96.6% and 86.4% respectively. We set these values 
as upper bound of precision for suggested methods. 
                                                     
2  MEDLINE is a database of biomedical articles 
serviced by National Library of Medicine, USA. 
(http://www.nlm.nih.gov) 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology 89
The specificity of terms was measured with 
method 1, method 2, and method 3 as Table 2. 
Two additional methods, based on term frequency 
and term tf.idf, were experimented to compare 
compositionality based methods and term based 
methods. 
Method 1 showed better performance than term 
based methods. This result illustrate basic 
assumption of this paper that specific concepts are 
created by adding information to existing concepts, 
and new concepts are expressed as new terms by 
adding modifiers to existing terms. Word tf.idf 
based method showed better precision than word 
frequency based method. This result illustrate that 
tf.idf of words is more informative than frequency 
of words. 
Method 3 showed the best precision, 82.0%, 
because the two methods interacted 
complementary. In hybrid method, the weight 
value 0.8? =  indicates that compositional 
information is more informative than contextual 
information for the specificity of domain specific 
terms.  
One reason of the errors is that the names of 
some internal nodes in MeSH thesaurus are 
category names rather disease names. For example, 
as ?acid-base imbalance (C18.452.076)? is name 
of disease category, it doesn't occur as frequently 
as other real disease names. Other predictable 
reason is that we didn?t consider various surface 
forms of same term. For example, although 
?NIDDM? is acronym of ?non insulin dependent 
diabetes mellitus?, the system counted two terms 
separately. Therefore the extracted statistics can?t 
properly reflect semantic level information. 
4 Conclusion 
This paper proposed specificity measuring 
methods for terms based on information theory like 
measures using compositional and contextual 
information of terms. The methods are 
experimented on the terms in MeSH thesaurus. 
Hybrid method showed the best precision of 82.0%, 
because two methods complemented each other. 
As the proposed methods don't use domain 
dependent information, they can be adapted to 
other domains without extra processes. 
In the future, we will modify the system to 
handle various term formations such as abbreviated 
form. Finally we will apply the proposed methods 
to the terms of other specific domains. 
5 Acknowledgements 
This work was supported in part by Ministry of 
Science & Technology, Ministry of Culture & 
Tourism of Korean government, and Korea 
Science & Engineering Foundation. 
References  
Caraballo, S. A., Charniak, E. 1999. Determining 
the Specificity of Nouns from Text. Proceedings 
of the Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and 
Very Large Corpora 
Conexor. 2004. Conexor Functional Dependency 
Grammar Parser. http://www.conexor.com 
Croft, W. 2004. Typology and Universals. 2nd ed. 
Cambridge Textbooks in Linguistics, Cambridge 
Univ. Press 
Frantzi, K., et. al. 2000. Automatic recognition of 
multi-word terms: the C-value/NC-value method. 
Journal of Digital Libraries, vol. 3, num. 2 
Grefenstette, G. 1994. Explorations in Automatic 
Thesaurus Discovery. Kluwer Academic 
Publishers 
Haykin, S. 1994. Neural Network. IEEE Press 
Manning, C. D. and Schutze, H. 1999. 
Foundations of Statistical Natural Language 
Processing. The MIT Presss 
Pereira, F., Tishby, N., and Lee, L. 1993. 
Distributational clustering of English words. 
Proceedings of ACL 
Sanderson, M. 1999. Deriving concept hierarchies 
from text. Proceedings of ACM S1GIR 
Precision 
Methods 
Type I Type II Total 
Coverage
Human subjects(Average) 96.6 86.4 87.4  
Term frequency 100.0 53.5 60.6 89.5 
Term tf?idf 52.6 59.2 58.2 89.5 
Word Freq. 37.2 72.5 69.0 100.0 
Word Freq.+Structure (?=?=0.2) 100.0 72.8 75.5 100.0 
Word tf?idf 44.2 75.3 72.2 100.0 
Compositional 
Information 
Method 
(Method 1) Word tf?idf +Structure (?=?=0.2) 100.0 76.6 78.9 100.0 
Contextual Information Method (Method 2) (mod cnt>1) 90.0 66.4 70.0 70.2 
Hybrid Method (Method 3)  (tf?idf + Struct, ?=0.8) 95.0 79.6 82.0 70.2 
Table 2. Experimental results (%) 
CompuTerm 2004 Poster Session  -  3rd International Workshop on Computational Terminology90
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 41?48,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Taxonomy Learning using Term Specificity and Similarity 
 
Pum-Mo Ryu 
 
Computer Science Division, KAIST 
KORTERM/BOLA 
Korea 
pmryu@world.kaist.ac.kr 
Key-Sun Choi 
 
Computer Science Division, KAIST 
KORTERM/BOLA 
Korea 
kschoi@cs.kaist.ac.kr 
 
 
 
Abstract 
Learning taxonomy for technical terms is 
difficult and tedious task, especially 
when new terms should be included. The 
goal of this paper is to assign taxonomic 
relations among technical terms. We pro-
pose new approach to the problem that 
relies on term specificity and similarity 
measures. Term specificity and similarity 
are necessary conditions for taxonomy 
learning, because highly specific terms 
tend to locate in deep levels and semanti-
cally similar terms are close to each other 
in taxonomy. We analyzed various fea-
tures used in previous researches in view 
of term specificity and similarity, and ap-
plied optimal features for term specificity 
and similarity to our method.  
1 Introduction 
Taxonomy is a collection of controlled vocabu-
lary terms organized into a hierarchical structure. 
Each term in a taxonomy is one or more parent-
child relationships to other terms in the taxon-
omy. Taxonomies are useful artifacts for orga-
nizing many aspects of knowledge. As compo-
nents of ontologies, taxonomies can provide an 
organizational model for a domain (domain on-
tology), or a model suitable for specific tasks 
(task ontologies) (Burgun & Bodenreider, 2001). 
However their wide usage is still hindered by 
time-consuming, cost-ineffective building proc-
esses. 
The main paradigms of taxonomy learning are 
on the one hand pattern based approaches and on 
the other hand distributional hypothesis based 
approaches. The former is approaches based on 
matching lexico-syntactic patterns which convey 
taxonomic relations in a corpus (Hearst, 1992; 
Iwanska et al, 2000), and the latter is statistical 
approaches based on the distribution of context 
in corpus (Cimiano et al, 2005; Yamamoto et al, 
2005; Sanderson & Croft, 1999). The former fea-
tures a high precision and low recall compared to 
the latter. The quality of learned relations is 
higher than those of statistical approaches, while 
the patterns are rarely applied in real corpus. It is 
also difficult to improve performance of pattern 
based approaches because they are simple and 
clear. So, many researches have been focused on 
raising precision of statistical approaches. 
We introduce new distributional hypothesis 
based taxonomy learning method using term 
specificity and term similarity. Term specificity 
is a measure of information quantity of terms in 
given domain. When a term has much domain 
information, the term is highly specific to the 
domain, and vice versa (Ryu & Choi, 2005). Be-
cause highly specific terms tend to locate in low 
level in domain taxonomy, term specificity can 
be used as a necessary condition for taxonomy 
learning. Term similarity is degree of semantic 
overlap among terms. When two terms share 
many common characteristics, they are semanti-
cally similar to each other. Term similarity can 
be another necessary condition for taxonomy 
learning, because semantically similar terms lo-
cate near by in given domain taxonomy. The two 
conditions are generally valid for terms in a taxo-
nomic relation, while terms satisfying the condi-
tions do not always have taxonomic relation. So 
they are necessary conditions for taxonomy 
learning. 
Based on these conditions, it is highly prob-
able that term t1 is an ancestor of term t2 in do-
main taxonomy TD, when t1 and t2 are semanti-
cally similar enough and the specificity of t1 is 
lower than that of t2 in D as in Figure 1. However, 
t1 is not an ancestor of t3 even though the speci-
41
ficity of t1 is lower than that of t3 because t1 is not 
similar to t3 on the semantic level. 
 
t1
t2 t3
Similarity
Specificity 
high
low
Depth
high
low
 
Figure 1. Term specificity and term similarity in 
a domain taxonomy TD 
 
The strength of this method lies in its ability to 
adopt different optimal features for term specific-
ity and term similarity. Most of current re-
searches relied on single feature such as adjec-
tives of terms, verb-argument relation, or co-
occurrence ratio in documents according to their 
methods. Firstly, we analyze characteristics of 
features for taxonomy learning in view of term 
specificity and term similarity to show that the 
features embed characteristics of specificity and 
similarity, and finally apply optimal features to 
our method.  
Additionally we tested inside information of 
terms to measure term specificity and similarity. 
As multiword terms cover the larger part of tech-
nical terms, lexical components are featuring 
information representing semantics of terms 
(Cerbah, 2000). 
The remainder of this paper is organized fol-
lows. Characteristics of term specificity are de-
scribed in Section 2, while term similarity and its 
features are addressed in Section 3. Our taxon-
omy learning method is discussed in Section 4. 
Experiment and evaluation are discussed in Sec-
tion 5, and finally, conclusions are drawn in Sec-
tion 6. 
2 Term Specificity 
Specificity is degree of detailed information of 
an object about given target object. For example, 
if an encyclopedia contains detailed information 
about ?IT domain?, then the encyclopedia is ?IT 
specific encyclopedia?. In this context, specificity 
is a function of objects and target object to real 
number. Traditionally term specificity is widely 
used in information retrieval systems to weight 
index terms in documents (S. Jones, 1972; Ai-
zawa, 2003; Wong & Yao, 1992). In information 
retrieval context, term specificity is function of 
index terms and documents. On the other hand, 
term specificity is the function of terms and tar-
get domains in taxonomy learning context (Ryu 
& Choi 2005). Term specificity to a domain is 
quantified to a positive real number as shown in 
Eq. (1). 
 
( | )Spec t D R+?                                              (1) 
 
where t is a term, and Spec(t|D) is the specificity 
of t in a given domain D. We simply use Spec(t) 
instead of Spec(t|D) assuming a particular do-
main D in this paper.  
Understanding the relation between domain 
concepts and their lexicalization methods is 
needed, before we describe term specificity 
measuring methods. Domain specific concepts 
can be distinguished by a set of what we call 
?characteristics?. More specific concepts are cre-
ated by adding characteristics to the set of char-
acteristics of existing concepts. Let us consider 
two concepts: C1 and C2. C1 is an existing con-
cept and C2 is a newly created concept by com-
bining new characteristics to the characteristic 
set of C1. In this case, C1 is an ancestor of C2 
(ISO, 2000). When domain specific concepts are 
lexicalized as terms, the terms' word-formation is 
classified into two categories based on the com-
position of component words. In the first cate-
gory, new terms are created by adding modifiers 
to existing terms. Figure 2 shows a subtree of 
financial ontology. For example ?current asset? 
was created by adding the modifier ?current? to 
its hypernym ?asset?. In this case, inside informa-
tion is a good evidence to represent the charac-
teristics. In the second category, new terms are 
created independently of existing terms. For ex-
ample, ?cache?, ?inventory?, and ?receivable? 
share no common words with their hypernyms 
?current asset? and ?asset?. In this case, outside 
information is used to differentiate the character-
istics of the terms. 
 
asset
current asset fixed asset
cache inventory receivable intangibleasset  
Figure 2. Subtree of financial ontology 
 
There are many kinds of inside and outside in-
formation to be used in measuring term specific-
ity. Distribution of adjective-term relation and 
verb-argument dependency relation are colloca-
tion based statistics. Distribution of adjective-
term relation refers to the idea that specific nouns 
are rarely modified, while general nouns are fre-
42
quently modified in text. This feature has been 
discussed to measure specificity of nouns in 
(Caraballo, 1999; Ryu & Choi, 2005) and to 
build taxonomy of Japanese nouns (Yamamoto et 
al., 2005). Inversed specificity of a term can be 
measured by entropy of adjectives as shown Eq. 
(2). 
 
1( ) ( | ) log ( | )adj
adj
Spec t P adj t P adj t? = ??              (2) 
 
where P(adj|t), the probability that adj modifies t, 
is estimated as freq(adj,t)/freq(t). The entropy is 
the average information quantity of all (adj,t) 
pairs for term t. Specific terms have low entropy, 
because their adjective distributions are simple. 
For verb-argument distribution, we assume 
that domain specific terms co-occur with selected 
verbs which represent special characteristics of 
terms while general terms are associated with 
multiple verbs. Under this assumption, we make 
use of syntactic dependencies between verbs ap-
pearing in the corpus and their arguments such as 
subjects and objects. For example, ?inventory?1, 
in Figure 2, shows a tendency to be objects of 
specific verbs like ?increase? and ?reduce?. This 
feature was used in (Cimiano et al, 2005) to 
learn concept hierarchy. Inversed specificity of a 
term can be measured by entropy of verb-
argument relations as Eq. (3). 
 
1( ) ( | ) log ( | )
arg
arg
v arg arg
v
Spec t P t v P t v? = ??             (3) 
 
where P(t|varg), the probability that t is argument 
of varg, is estimated as freq(t,varg)/freq(varg). The 
entropy is the average information quantity of all 
(t,varg) pairs for term t. 
Conditional probability of term co-occurrence 
in documents was used in (Sanderson & Croft, 
1999) to build term taxonomy. This statistics is 
based on the assumption that, for two terms, ti 
and tj, ti is said to subsume tj if the following two 
conditions hold, 
 
P(ti|tj) = 1 and P(tj|ti)<1                                     (4) 
 
In other words, ti subsumes tj if the documents 
which tj occurs in are a subset of the documents 
which ti occurs in, therefore ti can be parent of tj 
in taxonomy. Although a good number of term 
pairs are found that adhere to the two subsump-
                                                 
1 ?Inventory? consists of a list of goods and materials held 
available in stock (http://en.wikipedia.org/wiki/Inventory). 
tion conditions, it is noticed that many are just 
failing to be included because a few occurrences 
of the subsumed term, tj, does not co-occur with 
ti. Subsequently, the conditions are relaxed and 
subsume function is defined as Eq. (5). In case of 
P(ti|tj)>P(tj|ti), subsume(ti,tj) returns 1, otherwise 
returns 0. 
 
1  if ( | ) ( | )
( , )
0  otherwise                  
i j j i
i j
P t t P t t
subsume t t
>?= ??
           (5) 
 
We apply this function to calculate term speci-
ficity as shown Eq. (6) where a term is specific 
when it is subsumed by most of other terms. 
Specificity of t is determined by the ratio of 
terms that subsume t over all co-occurring terms. 
 
1
( , )
( )
jj n
coldoc
subsume t t
Spec t
n
? ?= ?                      (6) 
 
where n is number of terms co-occurring terms 
with t. 
Finally, inside-word information is important 
to compute specificity for multiword terms. Con-
sider a term t that consists of two words like t = 
w1w2. Two words, w1 and w2, have their unique 
characteristics and the characteristics are 
summed up to the characteristic of t. Mutual in-
formation is used to estimate the association be-
tween a term and its component words. Let 
T={t1,?,tN} be a set of terms found in a corpus, 
and W={w1,?,wM} be a set of component words 
composing the terms in T. Assume a joint prob-
ability distribution P(ti,wj), probability of wj is a 
component of ti, is given for ti and wj. Mutual 
information between ti and wj compares the prob-
ability of observing ti and wj together and the 
probability of observing ti and wj independently. 
The mutual information represents the reduction 
of uncertainty about ti when wj is observed. The 
summed mutual information between ti and W, as 
in Eq. (7), is total reduction of uncertainty about 
ti when all component words are observed. 
 
( , )
( ) log
( ) ( )
j
i j
in i
w W i j
P t w
Spec t
P t P w?
= ?                            (7) 
 
This equation indicates that wj which is highly 
associated to ti contributes specificity of ti. For 
example, ?debenture bond? is more specific con-
cept than ?financial product?. Intuitively, ?deben-
ture? is highly associated to ?debenture bond? 
43
compared with ?bond? to ?debenture bond? or 
?financial?, ?product? to ?financial product?. 
3 Term Similarity 
We evaluate four statistical and lexical features, 
related to taxonomy learning, in view of term 
similarity. Three statistical features have been 
used in existing taxonomy learning researches. 
(Sanderson & Croft, 1999) used conditional 
probability of co-occurring terms in same docu-
ment in taxonomy learning process as shown in 
Eq. (4). This feature can be used to measure 
similarity of terms. If two terms co-occur in 
common documents, they are semantically simi-
lar to each other. Based on this assumption, we 
can calculate term similarity by comparing the 
frequency of co-occurring ti and tj together and 
the frequency of occurring ti and tj independently, 
as Eq. (8). 
 
2* ( , )
( , )
( ) ( )
i j
coldoc i j
i j
df t t
Sim t t
df t df t
= +
                           (8) 
 
where df(ti,tj) is number of documents in which 
both ti and tj co-occur, df(ti) is number of docu-
ments in which ti occurs.  
(Yamamoto et al, 2005) used adjective pat-
terns to make characteristics vectors for terms in 
Complementary Similarity Measure (CSM). Al-
though CSM was initially designed to extract 
superordinate-subordinate relations, it is a simi-
larity measure by itself. They proposed two CSM 
measures; one is for binary images in which val-
ues in feature vectors are 0 or 1, and the other is 
for gray-scale images in which values in feature 
vectors are 0 through 1. We adapt gray-scale 
measure in similarity calculation, because it 
showed better performance in their research. 
(Cimiano et al, 2005) applied Formal Concept 
Analysis (FCA) to extract taxonomies from a 
text corpus. They modeled the context of a term 
as a vector representing syntactic dependencies. 
Similarity based on verb-argument dependencies 
is calculated using cosine measure as Eq. (9). 
 
2 2
( | ) ( | )
( , )
( | ) ( | )
arg
arg
arg arg
i arg j argv V
v i j
i arg j arg
v V v V
P t v P t v
Sim t t
P t v P t v
?
? ?
= ?? ?
 (9) 
 
where P(t|varg), the probability that t is argument 
of varg, is estimated as freq(t,varg)/freq(varg). 
Above three similarity measures are valid when 
terms, ti and tj, appear in corpus one or more 
times. 
The last similarity measure is based on inside 
information of terms. Because many domain 
terms are multiword terms, component words are 
clues for term similarity. If two terms share 
many common words, they share common char-
acteristics in given domain. For example, four 
words ?asset?, ?current asset?, ?fixed asset? and 
?intangible asset? share characteristics related to 
?asset? as in Figure 2. This similarity measure is 
shown in Eq. (10). 
 
2* ( , )
( , )
| | | |
i j
in i j
i j
cwc t t
Sim t t
t t
= +
                                (10) 
 
where |t| is word count of t, and cwc(ti,tj) is 
common word count in ti and tj. Simin(ti,tj) is 
valid when cwc(ti,tj)>0. Because cwc(ti,tj)=0 for 
most of term pairs, it is difficult to catch reliable 
results for all possible term pairs. 
4 Taxonomy Learning Process 
We model taxonomy learning process as a se-
quential insertion of new terms to current taxon-
omy. New taxonomy starts with empty state, and 
changes to rich taxonomic structure with the re-
peated insertion of terms as depicted in Figure 3. 
Terms to be inserted are sorted by term specific-
ity values. Term insertion based on the increas-
ing order of term specificity is natural, because 
the taxonomy grows from top to down with term 
insertion process in increasing specificity se-
quence. 
 
?
SpecificityHigh Low
Specificity
High
Low
Term sequence
Taxonomy
tnew
tnew
 
Figure 3. Terms are inserted to taxonomy in the 
sequence of specificity 
 
According to above assumption, our system 
selects possible hypernyms of a new term, tnew in 
current taxonomy as following steps: 
 
? Step 1: Select n-most similar terms to tnew 
from current taxonomy 
? Step 2: Select candidate hypernyms of tnew 
from n-most similar terms. Specificity of 
candidate hypernyms is less than that of tnew. 
44
? Step 3: Insert tnew as hyponyms of candidate 
hypernyms 
For example, suppose t2, t4, t5 and t6, are four 
most similar terms to tnew in Figure 4. Two terms 
t2 and t4 are selected as candidate hypernyms of 
tnew, because specificity of the terms is less than 
specificity of tnew. 
 
t1
t2 t3
t4 t5 t6
t7 t8 t9
tnew
t10
Spec(t1) = 1.0
Spec(t3) = 1.5Spec(t2) = 1.5
Spec(t4) = 2.0 Spec(t5) = 3.0
Spec(t7) = 4.0 Spec(t8) = 3.5
Spec(t6) = 2.4
Spec(t9) = 2.5
Spec(tnew) = 2.3
Spec(t10) = 3.0
S
pecificity
High
Low
 
Figure 4. Selection of candidate hypernyms of 
tnew from taxonomy using term specificity and 
similarity 
5 Experiment and Evaluation 
We applied our taxonomy learning method to set 
of terms in existing taxonomy. We removed all 
relations from the taxonomy, and made new 
taxonomic relations among the terms. The 
learned taxonomy was then compared to original 
taxonomy. Our experiment is composed of four 
steps. Firstly, we calculated term specificity us-
ing specificity measures discussed in chapter 2, 
secondly, we calculated term similarity using 
similarity measures described in chapter 3, 
thirdly, we applied the best specificity and simi-
larity features to our taxonomy building process, 
and finally, we evaluated our method and com-
pared with other taxonomy learning methods. 
Finance ontology 2  which was developed 
within the GETESS project (Staab et al, 1999) 
was used in our experiment. We slightly modi-
fied original ontology. We unified different ex-
pressions of same concept to identical expression. 
For example, 'cd-rom drive' and 'cdrom drive' are 
unified as 'cd-rom drive' because the former is 
more usual expression than the latter. We also 
removed terms that are not descendents of 'root' 
node to make the taxonomy have single root 
node. The taxonomy consists of total 1,819 
nodes and 1,130 distinct nodes. Maximum and 
average depths are 15 and 5.5 respectively, and 
                                                 
2 The ontology can be downloaded at http://www.aifb.uni-
karlsruhe.de/WBS/pci/FinanceGoldStandard.isa. P. Cimiano 
and his colleagues added English labels for the originally 
German labeled nodes (Cimiano et al, 2005) 
maximum and average children nodes are 32 and 
3.5 respectively. 
We considered Reuters215783 corpus, over 3.1 
million words in title and body fields. We parsed 
the corpus using Connexor functional depend-
ency parser4 and extracted various statistics: term 
frequency, distribution of adjectives, distribution 
of co-occurring frequency in documents, and 
verb-argument distribution. 
5.1 Term Specificity 
Term specificity was evaluated based on three 
criteria: recall, precision and F-measure. Recall 
is the fraction of the terms that have specificity 
values by the given measuring method. Precision 
is the fraction of relations with correct specificity 
values. F-measure is a harmonic mean of preci-
sion and recall into a single measure of overall 
performance. Precision (Pspec), recall (Rspec), F-
measure (Fspec) is defined as follows: 
 
#     
#    
#   ( , )   
#   ( , )
spec
valid
spec
valid
of terms with specificity
R
of all terms
of R p c with correct specificity
P
of R p c
=
=
  (11) 
 
where Rvalid(p,c) is a valid parent-child relation in 
original taxonomy, and a relation is valid when 
the specificity of two terms are measured by the 
given method. If the specificity of child term, c, 
is larger than that of parent term, p, then the rela-
tion is correct. 
We tested four specificity measuring methods 
discussed in section 2 and the result is shown in 
Table 1. Specadj showed the highest precision as 
we anticipated. Because domain specific terms 
have sufficient information in themselves; they 
are rarely modified by other words in real text. 
However, Specadj showed the lowest recall for 
data sparseness problem. As mentioned above, it 
is hard to collect sufficient adjectives for domain 
specific terms from text. Specvarg showed the 
lowest precision. This result indicates that distri-
bution of verb-argument relation is less corre-
lated to term specificity. Specin showed the high-
est recall because it measures term specificity 
using component words contrary to other meth-
ods. Speccoldoc showed comparable precision and 
recall. 
                                                 
3 
http://www.daviddlewis.com/resources/testcollections/reute
rs21578/ 
4 http://www.connexor.com/ 
45
We harmonized Specin and Specadj to Specin/adj 
as described in (Ryu & Choi, 2005) to take ad-
vantages of both inside and outside information. 
Harmonic mean of two specificity values was 
used in Specin/adj method. Specin/adj showed the 
highest F-measure because precision was higher 
than that of Specin and recall was equal to that of 
Specin. 
 
Table 1. Precision, recall and F-measure for term 
specificity 
Method Precision Recall F-measure
Specadj 0.795 0.609  0.689 
Specvarg 0.663 0.702  0.682 
Speccoldoc 0.717 0.702  0.709 
Specin 0.728 0.907  0.808 
Specin/adj 0.731 0.907  0.810 
5.2 Term Similarity 
We evaluated similarity measures by comparing 
with taxonomy based similarity measure. (Bu-
danitsky & Hirst, 2006) calculated correlation 
coefficients (CC) between human similarity rat-
ings and the five WordNet based similarity 
measures. Among the five computational meas-
ures, (Leacock & Chodorow, 1998)?s method 
showed the highest correlation coefficients, even 
though all of the measures showed similar rang-
ing from 0.74 to 0.85. This result means that tax-
onomy based similarity is highly correlated to 
human similarity ratings. We can indirectly 
evaluate our similarity measures by comparing to 
taxonomy based similarity measure, instead of 
direct comparison to human rating. If applied 
similarity measure is qualified, the calculated 
similarity will be highly correlated to taxonomy 
based similarity. Leacock and Chodorow pro-
posed following formula for computing the 
scaled semantic similarity between terms t1 and t2 
in taxonomy. 
 
1 2
1 2
( , )
( , ) log
2 max ( )LC
t Taxonomy
len t t
Sim t t
depth t
?
= ? ?
             (12) 
 
where the denominator includes the maximum 
depth of given taxonomy, and len(t1, t2) is num-
ber of edges in the shortest path between word t1 
and t2 in the taxonomy.  
Besides CC with ontology based similarity 
measures, recall of a similarity measures is also 
important evaluation factor. We defined recall of 
similarity measure, RSim, as the fraction of the 
term pairs that have similarity values by the 
given measuring method as Eq. (13). 
 
#     
#     Sim
similarity measured term pairs
R
all possible term pairs
=           (13) 
 
We also defined F-measure for a similarity 
measure, Fsim, as harmonic means of CC and Rsim. 
Because CC is a kind of precision, Fsim is overall 
measure of precision and recall. 
We calculated term similarity between all pos-
sible term pairs in finance ontology using the 
measures described in section 3. Additionally we 
introduced new similarity measure Simin/varg 
which is combined similarity of Simvarg and Simin. 
Simvarg and Simin between two terms are harmo-
nized to Simin/varg. We also calculated SimLC 
based on finance ontology, and calculated CC 
between SimLC and results of other measures. 
Figure 5 shows variation of CC and recall as 
threshold of similarity changes from 0.0 to 1.0 
for five similarity measures. Threshold is directly 
proportional to CC and inversely proportional to 
recall in ideal case. We normalized all similarity 
values to [0.0, 1.0] in each measure. CC grows as 
threshold increases in Simcoldoc and Simvarg as we 
expected. CC of CSM measure, Simcsm, increased 
as threshold increased and decreased when 
threshold is over 0.6. For example two terms ?as-
set? and ?current asset? are very similar to each 
other based on SimLC measure, because edge 
count between two terms is one in finance ontol-
ogy. The former can be modified many adjec-
tives such as ?intangible?, ?tangible?, ?new? and 
?estimated?, while the latter is rarely modified by 
other adjectives in corpus because it was already 
extended from ?asset? by adding adjective ?cur-
rent?. Therefore, semantically similar terms do 
not always have similar adjective distributions. 
CC between Simin and SimLC showed high curve 
in low threshold, but downed as threshold in-
creased. Similarity value above 0.6 is insignifi-
cant, because it is hard to be over 0.6 using Eq. 
(10). For example, similarity between ?executive 
board meeting? and ?board meeting? is 0.8, the 
maximum similarity in our test set. The average 
of inside-word similarity is 0.41. 
Simvarg showed higher recall than other meas-
ures. This means that verb-argument relation is 
more abundant than other features in corpus. 
SimIn showed the lowest recall because we could 
get valid similarity using Eq. (10). Simvarg 
showed higher F-measure when threshold is over 
0.2. This result illustrate that verb-argument rela-
tion is adequate feature to similarity calculation. 
46
The combined similarity measure, Simin/varg, 
complement shortcomings of SimIn and Simvarg. 
SimIn showed high CC but low recall. Contrarily 
Simvarg showed low CC but high recall. Simin/varg 
showed the highest F-measure. 
5.3 Taxonomy learning 
In order to evaluate our approach we need to as-
sess how good the automatically learned tax-
onomies reflect a given domain. The goodness is 
evaluated by the similarity of automatically 
learned taxonomy to reference taxonomy. We 
used (Cimiano et al, 2005)?s ontology evaluation 
method in which lexical recall (LRTax), precision 
(PTax) and F-measure (FTax) of learned taxonomy 
are defined based on the notion of taxonomy 
overlap. LRTax is defined as the ratio of number 
of common terms in learned taxonomy and refer-
ence taxonomy over number of terms in refer-
ence taxonomy. PTax is defined as ratio of taxon-
omy overlap of learned taxonomy to reference 
taxonomy. FTax is harmonic mean of LRTax and 
PTax. 
 
0.0
0.2
0.4
0.6
0.8
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
C
C
0.00
0.04
0.08
0.12
0.16
0.20
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
F
 m
ea
su
re
Sim(coldoc) Sim(CSM) Sim(varg)
Sim(In) Sim(In/Varg)
0.00
0.04
0.08
0.12
0.16
0.20
0.24
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Threshold
Re
ca
ll
 
Figure 5 Correlation coefficient between SimLC 
and other similarity measures. Recall and F-
measure of similarity measures 
We generated four taxonomies, Tcoldoc, Tcsm, 
Tfca, Tspec/sim, using four taxonomy learning meth-
ods: term co-occurring method, CSM method, 
FCA method and our method. We applied Spe-
cin/adj in specificity measuring and Simin/varg in 
similarity calculation because they showed the 
highest F-measure. In our method, the most 
probable one term was selected as hypernym of 
newly inserted term in each learning step.  
Figure 6 shows variations of lexical recall, 
precision and F-measure of four methods as 
threshold changes. Threshold in each method 
represent different information to each other. 
Threshold in Tcsm is variation of CSM values. 
Threshold in Tcoldoc is variation of probability of 
two terms co-occur in a document. Threshold in 
Tfca is normalized frequency of contexts. Thresh-
old in Tspec/sim, is variation of similarity. 
Tspec/sim showed the highest lexical recall. 
Lexical recall is tightly related to recall in simi-
larity measures. Simin/varg showed the highest re-
call in similarity measures. Tfca and Tcsm showed 
higher precision than other taxonomies. It is as-
sumed that  precision  of  taxonomy  depends  on 
 
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold
Le
xi
ca
l R
ec
al
l
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold
F-
M
ea
su
re
CSM COLDOC SPEC/SIM FCA
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Threshold
P
re
ci
si
on
 
Figure 6. Lexical recall, precision and F-measure 
of taxonomy learning methods 
 
47
the precision of specificity measures and the CC 
of similarity measures. In actual case, Simvarg 
showed the most plausible curve in CC and Spe-
cadj showed the highest precision in specificity. 
Verb-argument relation and adjective-term rela-
tion are used in FCA and CSM methods respec-
tively. Tspec/sim and Tcoldoc showed higher F-
measure curve than other two taxonomies due to 
high lexical recall. Although our method showed 
plausible F-measure, it showed the lowest preci-
sion. So other combination of similarity and 
specificity measures are needed to improve pre-
cision of learned taxonomy. 
6 Conclusion 
We have presented new taxonomy learning 
method with term similarity and specificity taken 
from domain-specific corpus. It can be applied to 
different domains as it is; and, if we have a syn-
tactic parser available, to different languages. We 
analyzed the features used in previous researches 
in view of term specificity and similarity. In this 
analysis, we found that the features embed the 
characteristics of both conditions. 
Compared to previous approaches, our method 
has advantages in that we can use different fea-
tures for term specificity and similarity. It makes 
easy to analyze errors in taxonomy learning step, 
whether the wrong relations are caused by speci-
ficity errors or by similarity errors. The main 
drawback of our method, as it is now, is that the 
effect of wrong located terms in upper level 
propagates to lower levels.  
Until now, researches on automatic ontology 
learning especially taxonomic relation showed 
very low precision. Human experts? intervention 
is inevitable in automatic learning process to 
make applicable taxonomy. Future work is to 
make new model where human experts and sys-
tem work interactively in ontology learning 
process in order to balance cost and precision. 
Reference 
S. Caraballo, E. Charniak. 1999. Determining the 
Specificity of Nouns from Text. Proceedings of the 
1999 Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora, pp. 63-70 
P. Cimiano, A. Hotho, S.Staab. 2005. Learning Con-
cept Hierarchies from Text Corpora using Formal 
Concept Analysis. Journal of AI Research, Vol. 24, 
pp. 305-339 
M. Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. Proceedings of the 
14th International Conference on Computational 
Linguistics 
L. Iwanska, N. Mata and K. Kruger. 2000. Fully 
automatic acquisition of taxonomic knowledge 
from large corpora of texts. In Iwanska, L. & 
Shapiro, S. (Eds.), Natural Language Processing 
and Knowledge Processing, pp. 335-345, 
MIT/AAAI Press. 
E. Yamamoto, K. Kanzaki and H. Isahara. 2005. Ex-
traction of Hierarchies Based on Inclusion of Co-
occurring Words with Frequency Information. 
Proceedings of 9th International Joint Conference 
on Artificial Intelligence, pp. 1160-1167 
A. Burgun, O. Bodenreider. 2001. Aspects of the 
Taxonomic Relation in the Biomedical Domain, 
Proceedings of International Conference on For-
mal Ontology in Information Systems, pp. 222-233 
Mark Sanderson and Bruce Croft. 1999. Deriving 
concept hierarchies from text. Proceedings of the 
22th Annual ACM S1GIR Conference on Research 
and Development in Information Retrieval, pp. 
206-213, 1999 
Karen Sparck Jones. 1972. Exhausitivity and Speci-
ficity Journal of Documentation Vol. 28, Num. 1, 
pp. 11-21 
S.K.M. Wong, Y.Y. Yao. 1992. An Information-
Theoretic Measure of Term Specificity, Journal of 
the American Society for Information Science, Vol. 
43, Num. 1. pp.54-61 
ISO 704. 2000. Terminology work-Principle and 
methods. ISO 704 Second Edition 
A. Aizawa. 2003. An information-theoretic perspec-
tive of tf-idf measures. Journal of Information 
Processing and Management, vol. 39 
Alexander Budanitsky, Graeme Hirst. 2006 Evaluat-
ing WordNet-based Measures of Lexical Semantic 
Relatedness. Computational Linguistics. Vol. 32 
NO. 1, pp. 13-47(35) 
Claudia Leacock, Martin Chodorow. 1998. Combin-
ing local context and WordNet similarity for word 
sense identification. In Christian Fellbaum, editor, 
WordNet: An Electronic Lexical Database. The 
MIT Press, pp. 265-283 
Pum-Mo Ryu, Key-Sun Choi. 2005. An Information-
Theoretic Approach to Taxonomy Extraction for 
Ontology Learning, In P. Buitelaar et al (eds.), On-
tology Learning from Text: Methods, Evaluation 
and Applications, Vol. 123, Frontiers in Artificial 
Intelligence and Applications, IOS Press 
Farid Cerbah. 2000. Exogeneous and Endogeneous 
Approaches to Semantic Categorization of Un-
known Technical Terms. Proceedings of the 18th 
International Conference on Computational Lin-
guistics, vol. 1, pp. 145-151 
48
