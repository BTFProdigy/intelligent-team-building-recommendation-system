Domain Portability in Speech-to-Speech Translation
Alon Lavie, Lori Levin, Tanja Schultz, Chad Langley, Benjamin Han
Alicia Tribble, Donna Gates, Dorcas Wallace and Kay Peterson
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
alavie@cs.cmu.edu
1. INTRODUCTION
Speech-to-speech translation has made significant advances over
the past decade, with several high-visibility projects (C-STAR, Verb-
mobil, the Spoken Language Translator, and others) significantly
advancing the state-of-the-art. While speech recognition can cur-
rently effectively deal with very large vocabularies and is fairly
speaker independent, speech translation is currently still effective
only in limited, albeit large, domains. The issue of domain porta-
bility is thus of significant importance, with several current research
efforts designed to develop speech-translation systems that can be
ported to new domains with significantly less time and effort than
is currently possible.
This paper reports on three experiments on portability of a speech-
to-speech translation system between semantic domains.1 The ex-
periments were conducted with the JANUS system [5, 8, 12], ini-
tially developed for a narrow travel planning domain, and ported
to the doctor-patient domain and an extended tourism domain. The
experiments cover both rule-based and statistical methods, and hand-
written as well as automatically learned rules. For rule-based sys-
tems, we have investigated the re-usability of rules and other knowl-
edge sources from other domains. For statistical methods, we have
investigated how much additional training data is needed for each
new domain. We are also experimenting with combinations of
hand-written and automatically learned components. For speech
recognition, we have conducted studies of what parameters change
when a recognizer is ported from one domain to another, and how
these changes affect recognition performance.
2. DESCRIPTION OF THE INTERLINGUA
The first two experiments concern the analysis component of our
interlingua-based MT system. The analysis component takes a sen-
tence as input and produces an interlingua representation as output.
We use a task-oriented interlingua [4, 3] based on domain actions.
Examples of domain actions are giving information about the on-
set of a symptom (e.g., I have a headache) or asking a patient
1We have also worked on the issue of portability across languages
via our interlingua approach to translation [3] and on portability of
speech recognition across languages [10].
.
to perform some action (e.g., wiggle your fingers). The interlin-
gua, shown in the example below, has five main components: (1) a
speaker tag such as a: for doctor (agent) and c: for a patient (cus-
tomer), (2) a speech act, in this case, give-information (3)
some concepts (+body-state and+existence), and (4) some
arguments (body-state-spec= andbody-location=), and
(5) some sub-arguments (identifiability=no and
inside=head).
I have a pain in my head.
c:give-information+existence+body-state
(body-state-spec=(pain,identifiability=no),
body-location=(inside=head))
3. EXPERIMENT 1:
EXTENSION OF SEMANTIC GRAMMAR
RULES BY HAND AND BY AUTOMATIC
LEARNING
Experiment 1 concerns extension of the coverage of semantic
grammars in the medical domain. Semantic grammars are based
on semantic constituents such as request information phrases (e.g.,
I was wondering : : : ) and location phrases (e.g., in my right arm)
rather than syntactic constituents such as noun phrases and verb
phrases. In other papers [12, 5], we have described how our mod-
ular grammar design enhances portability across domains. The
portable grammar modules are the cross-domain module, contain-
ing rules for things like greetings, and the shared module, contain-
ing rules for things like times, dates, and locations. Figure 1 shows
a parse tree for the sentence How long have you had this pain? XDM
indicates nodes that were produced by cross-domain rules. MED in-
dicates nodes that were produced by rules from the new medical
domain grammar.
The preliminary doctor-patient grammar focuses on three med-
ical situations: give-information+existence ? giving
information about the existence of a symptom (I have been get-
ting headaches); give-information+onset ? giving infor-
mation about the onset of a symptom (The headaches started three
months ago); and give-information+occurrence ? giv-
ing information about the onset of an instance of the symptoms
(The headaches start behind my ears). Symptoms are expressed
as body-state (e.g., pain), body-object (e.g., rash), and
body-event (e.g., bleeding).
Our experiment on extendibility was based on a hand written
seed grammar that was extended by hand and by automatic learn-
ing. The seed grammar covered the domain actions mentioned
above, but did not cover very many ways to phrase each domain
action. For example, it might have covered The headaches started
[request-information+existence+body-state]::MED
( WH-PHRASES::XDM
( [q:duration=]::XDM ( [dur:question]::XDM ( how long ) ) )
HAVE-GET-FEEL::MED ( GET ( have ) ) you
HAVE-GET-FEEL::MED ( HAS ( had ) )
[super_body-state-spec=]::MED
( [body-state-spec=]::MED
( ID-WHOSE::MED
( [identifiability=]
( [id:non-distant] ( this ) ) )
BODY-STATE::MED ( [pain]::MED ( pain ) ) ) ) )
Figure 1: Parser output with nodes produced by medical and cross-domain grammars.
Seed Extended Learned
IF 37.2 37.2 31.3
Domain Action 37.2 37.2 31.3
Speech Act
Recall 43.3 48.2 49.3
Precision 71.0 75.0 45.8
Concept List
Recall 2.2 10.1 32.5
Precision 12.5 42.2 25.1
Top-Level Arguments
Recall 0.0 7.2 29.6
Precision 0.0 42.2 34.4
Top-Level Values
Recall 0.0 8.3 29.8
Precision 0.0 50.0 39.2
Sub-Level Arguments
Recall 0.0 28.3 14.1
Precision 0.0 48.2 12.6
Sub-level Values
Recall 1.2 28.3 14.1
Precision 6.2 48.2 12.9
Table 1: Comparison of seed grammar, human-extended grammar, and machine-learned grammar on unseen data
three months ago but not I started getting the headaches three months
ago. The seed grammar was extended by hand and by automatic
learning to cover a development set of 133 utterances. The re-
sult was two new grammars, a human-extended grammar and a
machine-learned grammar, referred to as the extended and learned
grammars in Table 1. The two new grammars were then tested on
132 unseen sentences in order to compare generality of the rules.
Results are reported only for 83 of the 132 sentences which were
covered by the current interlingua design. The remaining 49 sen-
tences were not covered by the current interlingua design and were
not scored. Results are shown in Table 1.
The parsed test sentences were scored in comparison to a hand-
coded correct interlingua representation. Table 1 separates results
for six components of the interlingua: speech act, concepts, top-
level arguments, top-level values, sub-level arguments, and sub-
level values, in addition to the total interlingua, and the domain
action (speech act and concepts combined). The components of the
interlingua were described in Section 2.
The scores for the total interlingua and domain action are re-
ported as percent correct. The scores for the six components of the
interlingua are reported as average percent precision and recall. For
example, if the correct interlingua for a sentence has two concepts,
and the parser produces three, two of which are correct and one of
which is incorrect, the precision is 66% and the recall is 100%.
Several trends are reflected in the results. Both the human-ex-
tended grammar and the machine-learned grammar show improved
performance over the seed grammar. However, the human extended
grammar tended to outperform the automatically learned grammar
in precision, whereas the automatically learned grammar tended to
outperform the human extended grammar in recall. This result is to
be expected: humans are capable of formulating correct rules, but
may not have time to analyze the amount of data that a machine can
analyze. (The time spent on the human extended grammar after the
seed grammar was complete was only five days.)
Grammar Induction: Our work on automatic grammar induc-
tion for Experiment 1 is still in preliminary stages. At this point,
we have experimented with completely automatic induction (no in-
teraction with a user)2 of new grammar rules starting from a core
grammar and using a development set of sentences that are not
parsable according to the core grammar. The development sen-
tences are tagged with the correct interlingua, and they do not stray
from the concepts covered by the core grammar ? they only cor-
respond to alternative (previously unseen) ways of expressing the
same set of covered concepts. The automatic induction is based
on performing tree matching between a skeletal tree representation
obtained from the interlingua, and a collection of parse fragments
2Previous work on our project [2] investigated learning of grammar
rules with user interaction.
[give-information+onset+symptom]
[manner=]
[sudden]
suddenly
[symptom-location=]
DETP
DET
POSS
my
BODYLOCATION
BODYFLUID
[urine]
urine
became [adj:symptom-name=]
ADJ-SYMPTOM
FUNCTION-ADJ-VALS [attribute=]
[color_attribute]
colored
[abnormal]
dis
Parse chunk #1 Parse chunk #2 Parse chunk #3
Original interlingua:
give-information+onset+symptom
(symptom-name=(abnormal,attribute=color_attribute),symptom-location=urine,
manner=sudden)
Learned Grammar Rule:
s[give-information+onset+symptom]
( [manner=] [symptom-location=] *+became [adj:symptom-name=] )
Figure 2: A reconstructed parse tree from the Interlingua
that is derived from parsing the new sentence with the core gram-
mar. Extensions to the existing rules are hypothesized in a way that
would produce the correct interlingua representation for the input
utterance.
Figure 2 shows a tree corresponding to an automatically learned
rule. The input to the learning algorithm is the interlingua (shown
in bold boxes in the figure) and three parse chunks (circled in the
figure). The dashed edges are augmented by the learning algorithm.
4. EXPERIMENT 2:
PORTING TO A NEW DOMAIN
USING A HYBRID RULE-BASED AND
STATISTICAL ANALYSIS APPROACH
We are in the process of developing a new alternative analysis
approach for our interlingua-based speech-translation systems that
combines rule-based and statistical methods and we believe inher-
ently supports faster porting into new domains. The main aspects
of the approach are the following. Rather than developing com-
plete semantic grammars for analyzing utterances into our interlin-
gua (either completely manually, or using grammar induction tech-
niques), we separate the task into two main levels. We continue to
develop and maintain rule-based grammars for phrases that corre-
spond to argument-level concepts of our interlingua representation
(e.g., time expressions, locations, symptom-names, etc.). However,
instead of developing grammar rules for assembling the argument-
level phrases into appropriate domain actions, we apply machine
learning and classification techniques [1] to learn these mappings
from a corpus of interlingua tagged utterances. (Earlier work on
this task is reported in [6].)
We believe this approach should prove to be more suitable for
fast porting into new domains for the following reasons. Many of
the required argument-level phrase grammars for a new domain are
likely to be covered by already existing grammar modules, as can
be seen by examining the XDM (cross-domain) nodes in Figure 1.
The remaining new phrase grammars are fairly fast and straightfor-
ward to develop. The central questions, however, are whether the
statistical methods used for classifying strings of arguments into
domain actions are accurate enough, and what amounts of tagged
data are required to obtain reasonable levels of performance. To
assess this last question, we tested the performance of the current
speech-act and concept classifiers for the expanded travel-domain
when trained with increasing amounts of training data. The results
of these experiments are shown in Figure 3. We also report the
performance of the domain-action classification derived from the
combined speech-act and concepts. As can be seen, performance
reaches a relative plateau at around 4000-5000 utterances. We see
these results as indicative that this approach should indeed prove to
be significantly easier to port to new domains. Creating a tagged
database of this order of magnitude can be done in a few weeks,
rather than the months required for complete manual grammar de-
velopment time.
5. EXPERIMENT 3:
PORTING THE SPEECH RECOGNIZER
TO NEW DOMAINS
When the speech recognition components (acoustic models, pro-
nunciation dictionary, vocabulary, and language model) are ported
across domains and languages mainly three types of mismatches
Speech Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Concept Sequence Classification Accuracy for 16-
fold Cross-Validation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Dialog Act Classification Accuracy for 16-fold 
Cross-Validation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
500 1000 2000 3000 4000 5000 6009
Training Set Size
M
ea
n 
A
cc
ur
ac
y
Figure 3: Performance of Speech-Act, Concept, and Domain-Action Classifiers Using Increasing Amounts of Training Data
Baseline Systems WER on Different Tasks [%]
BN (Broadcast News) h4e98 1, all F-conditions 18.5
ESST (scheduling and travel planning domain) 24.3
BN+ESST 18.4
C-STAR (travel planning domain) 20.2
Adaptation!Meeting Recognition
ESST on meeting data 54.1
BN on meeting data 44.2
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (16 meetings) 38.7
BN+ESST on meeting data 42.2
+ language model interpolation (16 meetings) 39.0
Adaptation! Doctor-Patient Domain
C-STAR on doctor-patient data 34.1
+ language model interpolation ( 34 dialogs) 25.1
Table 2: Recognition Results
occur: (1) mismatches in recording condition; (2) speaking style
mismatches; as well as (3) vocabulary and language model mis-
matches. In the past these problems have mostly been solved by
collecting large amounts of acoustic data for training the acoustic
models and development of the pronunciation dictionary, as well
as large text data for vocabulary coverage and language model cal-
culation. However, especially for highly specialized domains and
conversational speaking styles, large databases cannot always be
provided. Therefore, our research has focused on the problem of
how to build LVCSR systems for new tasks and languages [7, 9]
using only a limited amount of data. In this third experiment we
investigate the results of porting the speech recognition component
of our MT system to different new domains. The experiments and
improvements were conducted with the Janus Speech Recognition
Toolkit JRTk [13].
Table 2 shows the results of porting four baseline speech recog-
nition systems to the doctor-patient domain, and to the meeting do-
main. The four baseline systems are trained on Broadcast News
(BN), English SpontaneousScheduling Task (ESST), combined BN
and ESST, and the travel planning domain of the C-STAR consor-
tium (http://www.c-star.org). The given tasks illustrate
a variety of domain size, speaking styles and recording conditions
ranging from clean spontaneous speech in a very limited domain
(ESST, C-STAR) to highly conversational multi-party speech in an
extremely broad domain (Meeting). As a consequence the error
rates on the meeting data are quite high but using MAP (Maximum
A Posteriori) acoustic model adaptation and language model adap-
tation the error rate can be reduced by about 10.2% relative over the
BN baseline system. With the doctor-patient data the drop in error
rate was less severe which can be explained by the similar speaking
style and recording conditions for C-STAR and doctor-patient data.
Details about the applied recognition engine can be found in [10]
for ESST and [11] for the BN system.
6. ACKNOWLEDGMENTS
The research work reported here was funded in part by the DARPA
TIDES Program and supported in part by the National Science
Foundation under Grant number 9982227. Any opinions, findings
and conclusions or recomendations expressed in this material are
those of the author(s) and do not necessarily reflect the views of the
National Science Foundation (NSF) or DARPA.
7. REFERENCES
[1] W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. TiMBL: Tilburg Memory Based Learner, version 3.0
Reference Guide. Technical Report Technical Report 00-01,
ILK, 2000. Avaliable at http://ilk.kub.nl/ ilk/papers/ilk0001.ps.gz.
[2] M. Gavalda`. Epiphenomenal Grammar Acquisition with
GSG. In Proceedings of the Workshop on Conversational
Systems of the 6th Conference on Applied Natural Language
Processing and the 1st Conference of the North American
Chapter of the Association for Computational Linguistics
(ANLP/NAACL-2000), Seattle, U.S.A, May 2000.
[3] L. Levin, D. Gates, A. Lavie, F. Pianesi, D. Wallace,
T. Watanabe, and M. Woszczyna. Evaluation of a Practical
Interlingua for Task-Oriented Dialogue. In Workshop on
Applied Interlinguas: Practical Applications of Interlingual
Approaches to NLP, Seattle, 2000.
[4] L. Levin, D. Gates, A. Lavie, and A. Waibel. An Interlingua
Based on Domain Actions for Machine Translation of
Task-Oriented Dialogues. In Proceedings of the International
Conference on Spoken Language Processing (ICSLP?98),
pages Vol. 4, 1155?1158, Sydney, Australia, 1998.
[5] L. Levin, A. Lavie, M. Woszczyna, D. Gates, M. Gavalda`,
D. Koll, and A. Waibel. The Janus-III Translation System.
Machine Translation. To appear.
[6] M. Munk. Shallow statistical parsing for machine translation.
Master?s thesis, University of Karlsruhe, Karlsruhe,
Germany, 1999. http://www.is.cs.cmu.edu/papers/
speech/masters-thesis/MS99.munk.ps.gz.
[7] T. Schultz and A. Waibel. Polyphone Decision Tree
Specialization for Language Adaptation. In Proceedings of
the ICASSP, Istanbul, Turkey, 2000.
[8] A. Waibel. Interactive Translation of Conversational Speech.
Computer, 19(7):41?48, 1996.
[9] A. Waibel, P. Geutner, L. Mayfield-Tomokiyo, T. Schultz,
and M. Woszczyna. Multilinguality in Speech and Spoken
Language Systems. Proceedings of the IEEE, Special Issue
on Spoken Language Processing, 88(8):1297?1313, 2000.
[10] A. Waibel, H. Soltau, T. Schultz, T. Schaaf, and F. Metze.
Multilingual Speech Recognition, chapter From Speech Input
to Augmented Word Lattices, pages 33?45. Springer Verlag,
Berlin, Heidelberg, New York, artificial Intelligence edition,
2000.
[11] A. Waibel, H. Yu, H. Soltau, T. Schultz, T. Schaaf, Y. Pan,
F. Metze, and M. Bett. Advances in Meeting Recognition.
Submitted to HLT 2001, January 2001.
[12] M. Woszczyna, M. Broadhead, D. Gates, M. Gavalda`,
A. Lavie, L. Levin, and A. Waibel. A Modular Approach to
Spoken Language Translation for Large Domains. In
Proceedings of Conference of the Association for Machine
Translation in the Americas (AMTA?98), Langhorn, PA,
October 1998.
[13] T. Zeppenfeld, M. Finke, K. Ries, and A. Waibel.
Recognition of Conversational Telephone Speech using the
Janus Speech Engine. In Proceedings of the ICASSP?97,
Mu?nchen, Germany, 1997.
 
	 1  	Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
 
		SPEECHALATOR: TWO-WAY SPEECH-TO-SPEECH TRANSLATION IN YOUR HAND
Alex Waibel
 
, Ahmed Badran
 
, Alan W Black
 
, Robert Frederking
 
, Donna Gates
 
Alon Lavie
 
, Lori Levin
 
, Kevin Lenzo

, Laura Mayfield Tomokiyo
Juergen Reichert

, Tanja Schultz   , Dorcas Wallace   , Monika Woszczyna , Jing Zhang
 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA

Cepstral, LLC,

Multimodal Technologies Inc,

Mobile Technologies Inc.
speechalator@speechinfo.org
ABSTRACT
This demonstration involves two-way automatic speech-
to-speech translation on a consumer off-the-shelf PDA. This
work was done as part of the DARPA-funded Babylon project,
investigating better speech-to-speech translation systems for
communication in the field. The development of the Speecha-
lator software-based translation system required addressing
a number of hard issues, including a new language for the
team (Egyptian Arabic), close integration on a small device,
computational efficiency on a limited platform, and scalable
coverage for the domain.
1. BACKGROUND
The Speechalator was developed in part as the next genera-
tion of automatic voice translation systems. The Phrasalator
is a one-way device that can recognize a set of pre-defined
phrases and play a recorded translation, [1]. This device
can be ported easily to new languages, requiring only a
hand translation of the phrases and a set of recorded sen-
tences. However, such a system severely limits communica-
tion as the translation is one way, thus reducing one party?s
responses to simple pointing and perhaps yes and no.
The Babylon project addresses the issues of two-way
communication where either party can use the device for
conversation. A number of different groups throughout the
US were asked to address specific aspects of the task, such
as different languages, translation techniques and platform
specifications. The Pittsburgh group was presented with
three challenges. First, we were to work with Arabic, a lan-
guage with which the group had little experience, to test our
capabilities in moving to new languages quickly. Second,
we were instructed to use an interlingua approach to trans-
lation, where the source language is translated into an in-
termediate form that is shared between all languages. This
step streamlines expansion to new languages, and CMU has
a long history in working with interlingua based translation
systems. Third, we were constrained to one portable PDA-
class device to host the entire two-way system: two recog-
nizers, two translation engines, and two synthesizers.
2. RECOGNITION
We used an HMM-based recognizer, developed by Multi-
modal Technologies Inc, which has been specifically tuned
for PDAs. The recognizer allows a grammar to be tightly
coupled with the recognizer, which offers important effi-
ciencies considering the limited computational power of the
device. With only minor modification we were able to gen-
erate our interlingua interchange format (IF) representation
directly as output from the recognizer, removing one mod-
ule from the process.
MTI?s recognizer requires under 1M of memory with
acoustic models of around 3M per language. Special op-
timizations deal with the slow processor and ensure low
use of memory during decoding. The Arabic models were
bootstrapped from the GlobalPhone [2] Arabic collections
as well as data collected as part of this project.
3. TRANSLATION
As part of this work we investigated two different tech-
niques for translation, both interlingua based. The first was
purely knowledge-based, following our previous work [3].
The engine developed for this was too large to run on the
device, although we were able to run the generation part off-
line seamlessly connected by a wireless link from the hand-
held device. The second technique we investigated used
a statistical training method to build a model to translate
structured interlingua IF to text in the target language. Be-
cause this approach was developed with the handheld in
mind, it is efficient enough to run directly on the device,
and is used in this demo.
4. SYNTHESIS
The synthesis engine is Cepstral?s Theta system. As the
Speechalator runs on very small hardware devices (at least
small compared to standard desktops), it was important that
the synthesis footprint remained as small as possible.
The speechalator is to be used for people with little ex-
posure to synthetic speech, and the output quality must be
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 29-30
                                                         Proceedings of HLT-NAACL 2003
very high. Cepstral?s unit selection voices, tailored to the
domain, meet the requirements for both quality and size.
Normal unit selection voices may take hundreds of megabytes,
but the 11KHz voices developed by Cepstral were around 9
megabytes each.
5. ARABIC
The Arabic language poses a number of challenges for any
speech translation system. The first problem is the wide
range of dialects of the language. Just as Jamaican and
Glaswegian speakers may find it difficult to understand each
other?s dialect of English, Arabic speakers of different di-
alects may find it impossible to communicate.
Modern Standard Arabic (MSA) is well-defined and widely
understood by educated speakers across the Arab world.
MSA is principally a written language and not a spoken lan-
guage, however. Our interest was in dealing with a normal
spoken dialect, and we chose Egyptian Arabic; speakers of
that dialect were readily accessible to us, and media influ-
ences have made it perhaps the most broadly understood of
the regional dialects.
Another feature of Arabic is that the written form, ex-
cept in specific rare cases, does not include vowels. For
speech recognition and synthesis, this makes pronunciations
hard. Solutions have been tested for recognition where the
vowels are not explicitly modeled, but implicitly modeled
by context. This would not work well for synthesis; we have
defined an internal romanization, based on the CallHome
[4] romanization, from which full phonetic forms can easily
be derived. This romanization is suitable for both recog-
nizer and synthesis systems, and can easily be transformed
into the Arabic script for display.
6. SYSTEM
The end-to-end system runs on a standard Pocket PC de-
vice. We have tested it on a number of different machines,
including various HP (Compaq) iPaq machines (38xx 39xx)
and Dell Axims. It can run on 32M machines, but runs best
on a 64M machine with about 40M made available for pro-
gram space. Time from the end of spoken input to start of
translated speech is around 2-4 seconds depending on the
length of the sentence and the actual processor. We have
found StrongARM 206MHz processors, found on the older
Pocket PCs, slightly faster than XScale 400MHz, though no
optimization for the newer processors has been attempted.
Upon startup, the user is presented with the screen as
shown in Figure 1. A push-to-talk button is used and the
speaker speaks in his language. The recognized utterance
is first displayed, with the translation following, and the ut-
terance is then spoken in the target language. Buttons are
provided for replaying the output and for switching the in-
put to the other language.
7. DISCUSSION
The current demonstration is designed for the medical inter-
view domain, with the doctor speaking English and the pa-
tient speaking Arabic. At this point in the project no formal
evaluation has taken place. However, informally, in office-
like acoustic environments, accuracy within domain is well
over 80%.
Arabic input Screen
Speechalator snapshot
8. REFERENCES
[1] Sarich, A., ?Phraselator, one-way speech translation
system,? http://www.sarich.com/translator/, 2001.
[2] T. Schultz and A. Waibel, ?The globalphone project:
Multilingual lvcsr with janus-3,? in Multilingual Infor-
mation Retrieval Dialogs: 2nd SQEL Workshop, Plzen,
Czech Republic, 1997, pp. 20?27.
[3] A. Lavie, et al ?A multi-perspective evaluation of
the NESPOLE! speech-to-speech translation system,?
in Proceedings of ACL 2002 workshop on Speech-to-
speech Translation: Algorithms and Systems, Philadel-
phia, PA., 2002.
[4] Linguistic Data Consortium, ?Callhome egyptian ara-
bic speech,? 1997.
A THAI SPEECH TRANSLATION SYSTEM  FOR MEDICAL DIALOGS
Tanja Schultz, Dorcas Alexander, Alan W Black, Kay Peterson, Sinaporn Suebvisai, Alex Waibel
Language Technologies Institute, Carnegie Mellon University
E-mail: tanja@cs.cmu.edu
1. Introduction
In this paper we present our activities towards a Thai
Speech-to-Speech translation system. We investigated in
the design and implementation of a prototype system. For
this purpose we carried out research on bootstrapping a
Thai speech recognition system, developing a translation
component, and building an initial Thai synthesis system
using our existing tools.
2. Speech Recognition
The language adaptation techniques developed in our lab
[5] enables us to rapidly bootstrap a speech recognition
system in a new target language given very limited amount
of training data. The Thailand?s National Electronics and
Technology Center gave us the permission to use their
Thai speech data collected in the hotel reservation domain.
They provided us with a 6 hours text and speech database
recorded from native Thai speakers. We divided the data
into three speaker disjoint sets, 34 speakers were used for
training, 4 speakers for development, and another 4
speakers for evaluation. The provided transcriptions were
manually pre-segmented and given in Thai script. We
transformed the Thai script into a Roman script
representation by concatenating the phoneme
representation of the Thai word given in the pronunciation
dictionary. The motivation for this romanization step was
threefold: (1) it makes it easier for non-Thai researchers to
work with the Roman representation like in the grammar
development, (2) the romanized output basically provides
the pronunciation which makes things easier for the speech
synthesis component, and (3) our speech engine currently
does not handle Thai characters.
In our first Thai speech engine we decided to disregard the
tone information. Since tone is a distinctive feature in the
Thai language, disregarding the tone increases the number
of homographs. In order to limit this number, we
distinguished those word candidates by adding a tag that
represents the tone. The resulting dictionary consists of
734 words which cover the given 6-hours database.
Building on our earlier studies which showed that
multilingual seed models outperform monolingual ones
[5], we applied phonemes taken from seven languages,
namely Chinese, Croatian, French, German, Japanese,
Spanish, and Turkish as seed models for the Thai phone
set. Table 1 describes the performance of the Thai speech
recognition component for different acoustic model sizes
(context-independent vs. 500 and 1000 tri-phone models).
The results indicate that a Thai speech recognition engine
can be built by using the bootstrapping approach with a
reasonable amount of speech data. Even the very initial
system bootstrapped from multilingual seed models gives
a performance above 80% word accuracy. The good
performance might be an artifact from the very limited
domain with a compact and closed vocabulary.
System Dev Test Eval Test
Context-Independent 85.62% 83.63%
Context-Dependent (500) 86.99% 84.44%
Context-Dependent (1000) 84.63% 82.71%
Table1: Word accuracy [%] in Thai language
3. Machine Translation
The Machine Translation (MT) component of our current
Thai system is based on an interlingua called the
Interchange Format (IF). The IF developed by CMU has
been expanded and now encompasses concepts in both the
travel and medical domains, as well as many general-use
or cross-domain concepts in many different languages [4].
Interlingua-based MT has several advantages, namely: (1)
it abstracts away from variations in syntax across
languages, providing potentially deep analysis of meaning
without relying on information pertinent only to one
particular language pair, (2) modules for analysis and
generation can be developed monolingually, with
additional reference only to the second "language" of the
interlingua, (3) the speaker can be given a paraphrase in
his or her own language, which can help verify the
accuracy of the analysis and be used to alert the listener to
inaccurate translations, and (4) translation systems can be
extended to new languages simply by hooking up new
monolingual modules for analysis and/or generation,
eliminating the need to develop a completely new system
for each new language pair.
Thai has some particular characteristics which we
addressed in IF and appear in the grammars as follows:
1) The use of a term to indicate the gender of the person:
Thai: zookhee kha1
Eng: okay (ending)
s[acknowledge] (zookhee *[speaker=])
2) An affirmation that means more than simply "yes."
Thai: saap khrap
Eng: know (ending)
s[affirm+knowledge](saap *[speaker=])
3) The separation from the main verb of terms for
feasibility and other modalities.
Thai: rvv khun ca paj dooj thxksii
kyydaaj
Eng: or you will go by taxi [can too]
s[give-information+feasibility+trip]
(*DISC-RHET [who=] ca paj
[locomotion=] [feasibility=])
4. Language Generation
For natural language generation from interlingua for Thai
and English, we are currently investigating two options: a
knowledge-based generation with the pseudo-unification
based GenKit generator developed at CMU, which
employs manually written semantic/syntactic grammars
and lexicons, and a statistical generation operating on a
training corpus of aligned interlingua and natural language
correspondences. Performance tests as well as the amount
and quality of training data will decide which approach
will be pursued in the future.
5. Speech Synthesis
First, we built a limited domain Thai voice in the Festival
Speech Synthesis System [1]. Limited Domain voices can
achieve very high quality voice output [2], and can be easy
to construct if the domain is constrained. Our initial voice
targeted the Hotel Reservation domain and we constructed
235 sentence that covered the aspects of our immediate
interest. Using the tools provided in FestVox [1], we
recorded, auto-labeled, and built a synthetic voice.
In supporting any new language in synthesis, a number of
language specific issues first had to be addressed. As with
our other speech-to-speech translation projects we share
the phoneme set between the recognizer and the
synthesizer. The second important component is the
lexicon. The pronunciation of Thai words from Thai script
is not straightforward, but there is a stronger relationship
between the orthography and pronunciation than in
English. For this small set of initial words we constructed
an explicit lexicon by hand with the output vocabulary of
522 words. The complete Thai limited domain voice uses
unit selection concatenative synthesis. Unlike our other
limited domain synthesizers, where they have a limited
vocabulary, we tag each phone with syllable and tone
information in selection making the result more fluent, and
a little more general.
Building on our previous Thai work in pronunciation of
Thai words [3], we have used the lexicon and statistically
trained letter to sound rules to bootstrap the required word
coverage. With a pronunciation model we can select
suitable phonetically balanced text (both general and in-
domain) from which we are able to record and build a
more general voice.
6. Demonstration Prototype System
Our current version is a two-way speech-to-speech
translation system between Thai and English for dialogs in
the medical domain where the English speaker is a doctor
and the Thai speaker is a patient. The translated speech
input will be spoken using the built voice. At the moment,
the coverage is very limited due to the simplicity of the
used grammars. The figure shows the interface of our
prototype system.
Acknowledgements
This work was partly funded by LASER-ACTD. The
authors thank Thailand?s National Electronics and
Computer Technology Center for giving the permission to
use their database and dictionary for this task.
References
[1] Black, A. and Lenzo, K. (2000) "Building Voices in the
Festival Speech Synthesis System", http://festvox.org
[2] Black, A. and Lenzo, K. (2000) "Limited Domain Synthesis",
ICSLP2000, Beijing, China.
[3] Chotmongkol, A. and Black, A. (2000) "Statistically trained
orthographic to sound models for Thai", ICSLP2000,
Beijing, China.
[4] Lavie A. and Levin L. and Schultz T. and Langley C. and
Han B., Tribble, A., Gates D., Wallace D. and Peterson K.
(2001) ?Domain Portability in Speech-to-speech
Translation?,  HLT, San Diego, March 2001.
[5] Schultz, T. and Waibel, A. (2001) ?Language Independent
and Language Adaptive Acoustic Modeling for Speech
Recognition?, Speech Communication, Volume 35, Issue 1-
2, pp. 31-51, August 2001.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 17?20,
New York, June 2006. c?2006 Association for Computational Linguistics
Thai Grapheme-Based Speech Recognition 
 
 
Paisarn Charoenpornsawat, Sanjika Hewavitharana, Tanja Schultz 
Interactive Systems Laboratories, School of Computer Science 
Carnegie Mellon University 
Pittsburgh, PA 15213 
{paisarn, sanjika, tanja}@cs.cmu.edu 
 
 
 
 
Abstract 
In this paper we present the results for 
building a grapheme-based speech recogni-
tion system for Thai. We experiment with 
different settings for the initial context in-
dependent system, different number of 
acoustic models and different contexts for 
the speech unit. In addition, we investigate 
the potential of an enhanced tree clustering 
method as a way of sharing parameters 
across models. We compare our system 
with two phoneme-based systems; one that 
uses a hand-crafted dictionary and another 
that uses an automatically generated dic-
tionary. Experiment results show that the 
grapheme-based system with enhanced tree 
clustering outperforms the phoneme-based 
system using an automatically generated 
dictionary, and has comparable results to 
the phoneme-based system with the hand-
crafted dictionary. 
1 Introduction 
Large vocabulary speech recognition systems tra-
ditionally use phonemes as sub-word units. This 
requires a pronunciation dictionary, which maps 
the orthographic representation of words into a 
sequence of phonemes. The generation of such a 
dictionary is both time consuming and expensive 
since it often requires linguistic knowledge of the 
target language. Several approaches to automatic 
dictionary generation have been introduced in the 
past with varying degrees of success (Besling, 
1994; Black et al, 1998).  Nevertheless, these 
methods still require post editing by a human ex-
pert or using another manually generated pronun-
ciation dictionary. 
As a solution to this problem, grapheme-based 
speech recognition (GBSR) has been proposed re-
cently (Kanthak and Ney, 2002). Here, instead of 
phonemes, graphemes ? orthographic representa-
tion of a word ? are used as the sub word units.  
This makes the generation of the pronunciation 
dictionary a trivial task. GBSR systems have been 
successfully applied to several European languages 
(Killer et al, 2003). However, because of the gen-
erally looser relation of graphemes to pronuncia-
tion than phonemes, the use of context dependent 
modeling techniques and the sharing of parameters 
across different models are of central importance.  
The variations in the pronunciation of phonemes 
in different contexts are usually handled by cluster-
ing the similar contexts together. In the traditional 
approach, decision trees are used to cluster poly-
phones ? a phoneme in a specific context ? to-
gether. Due to computational and memory 
constraints, individual trees are grown for each 
sub-state of each phoneme. This does not allow the 
sharing of parameters across polyphones with dif-
ferent center phonemes. Enhanced tree clustering 
(Yu and Schultz, 2003) lifts this constraint by 
growing trees which cover multiple phonemes. 
In this paper we present our experiments on ap-
plying grapheme-based speech recognition for 
Thai language. We compare the performance of the 
grapheme-based system with two phoneme-based 
systems, one using a hand-crafter dictionary, and 
the other using an automatically generated diction-
17
ary. In addition, we observe the effect of the en-
hanced tree clustering on the grapheme-based rec-
ognition system. 
2 Grapheme-to-Phoneme Relation in Thai 
In the grapheme-based approach, the pronunciation 
dictionary is constructed by splitting a word into its 
constituent letters. Previous experiments have 
shown that the quality of the grapheme-based rec-
ognizer is highly dependent on the nature of the 
grapheme-to-phoneme relation of a specific lan-
guage (Killer, 2003). In this section we have a 
closer look at the grapheme-to-phoneme relation in 
Thai. 
Thai, an alphabetical language, has 44 letters for 
21 consonant sounds, 19 letters for 24 vowel 
sounds (9 short vowels, 9 long vowels and 6 diph-
thongs), 4 letters for tone markers (5 tones), few 
special letters, and numerals. There are some char-
acteristics of Thai writing that can cause problems 
for GBSR: 
 Some vowel letters can appear before, after, 
above or below a consonant letter. e.g. In the 
word ?   ? (/mae:w/), the vowel ?   ? (/ae:/) 
appears before the consonant ? ? (/m/). 
 Some vowel and consonant letters can be com-
bined together to make a new vowel. e.g.  In 
the word ? ? /mua/, the vowel ?ua? is com-
posed of a vowel letter ?   ? and a consonant 
letter ? ?. 
 Some vowels are represented by more than one 
vowel letter For example, the vowel /ae/ re-
quires two vowel letters: ?   ? and ? ?. To make 
a syllable, a consonant is inserted in between 
the two vowel letters. e.g. ?  
	  ? (/lae/). The 
consonant ? 	 ? (/l/) is in the middle. 
 In some syllables, vowels letters are not ex-
plicitly written. e.g. The word ?  ? (/yok/) 
consists of two consonant letter, ?  ? (/y/) and 
?  ? (/k/). There is no letter to represent the 
vowel /o/. 
 The special letter ?  

 ?, called Karan, is a dele-
tion marker. If it appears above a consonant, 
that consonant will be ignored. Sometimes, it 
can also delete the immediately preceding con-
sonant or the whole syllable.  
To make the relationship between graphemes and 
phonemes in Thai as close as possible we apply 
two preprocess steps:  
 Reordering of graphemes when a vowel comes 
before a consonant. 
 Merging multiple letters representing a single 
phoneme into one symbol.  
We use simple heuristic rules for this purpose; 10 
rules for reordering and 15 for merging. In our ini-
tial experiments, reordering alone gave better re-
sults than reordering plus merging. Hence, we only 
used reordering rules for the rest of the experi-
ments.  
3 Thai Grapheme-Based Speech Recognition  
In this section, we explain the details of our Thai 
GBSR system. We used the Thai GlobalPhone 
corpus (Suebvisai et.al., 2005) as our data set, 
which consists of read-speech in the news domain. 
The corpus contains 20 hours of recorded speech 
from 90 native Thai speakers consisting of 14k 
utterances. There are approximately 260k words 
covering a vocabulary of about 7,400 words. For 
testing we used 1,181 utterances from 8 different 
speakers. The rest was used for training. The lan-
guage model was built on news articles and gave a 
trigram perplexity of 140 and an OOV-rate of 
1.4% on the test set. 
To start building the acoustic models for Thai, 
we first used a distribution that equally divided the 
number of frames among the graphemes. This was 
then trained for six iterations followed by writing 
the new labels. We repeated these steps six times. 
As can be seen in Table 1, the resulting system 
(Flat-Start) had poor performance. Hence we de-
cided to bootstrap from a context independent 
acoustic model of an existing phoneme-based 
speech recognition (PBSR) systems. 
3.1 Bootstrapping  
We trained two grapheme-based systems by boot-
strapping from the acoustic models of two different 
PBSR systems. The first system (Thai) was boot-
strapped from a Thai PBSR system (Suebvisai et 
al., 2005) trained on the same corpus. The second 
system (Multilingual) was bootstrapped from the 
acoustic models trained on the multilingual 
GlobalPhone corpus (Schultz and Waibel, 1998) 
which shares acoustic models of similar sounds 
across multiple languages. In mapping phones to 
graphemes, when a grapheme can be mapped to 
18
several different phones we selected the one which 
occurs more frequently.  
Both systems were based on trigraphemes (+/- 
1) with 500 acoustic models. Training was identi-
cal to the Flat-Start system. Table 1 compares the 
word error rates (WER) of the three systems on the 
test set.  
Flat-Start Multilingual Thai 
37.2% 27.0 % 26.4 % 
Table 1: Word error rates in % of GBSR systems 
with different bootstrapping techniques 
Results show that the two bootstrapped systems 
have comparable results, while Thai system gives 
the lowest WER. For the rest of the experiments 
we used the system bootstrapped from the multi-
lingual acoustic models. 
3.2 Building Context Dependent Systems 
For the context dependent systems, we trained two 
systems each with different polygrapheme units; 
one with trigrapheme (+/- 1), and another with 
quintgrapheme (+/-2). 
The question set used in building the context 
dependent system was manually constructed by 
using the question set from the Thai PBSR system. 
Then we replaced every phoneme in the question 
set by the appropriate grapheme(s).  In addition, 
we compared two different acoustic model sizes; 
500 and 2000 acoustic models. 
Table 2 shows the recognition results for the re-
sulting GBSR systems.  
Speech Unit 500 models 2000 models 
Trigrapheme 26.0 % 26.0 % 
Quintgrapheme 27.0 % 30.3 % 
Table 2: Word error rates in % of GBSR systems using 
different speech units and the # of models. 
The system with 500 acoustic models based on 
trigraphemes produced the best results. The higher 
WER for the quintgrapheme system might be due 
to the data sparseness. 
3.3 Enhanced Tree Clustering (ETC) 
Yu and Schultz (2003) introduced a tree clustering 
approach that allows the sharing of parameters 
across phonemes. In this enhanced tree clustering, 
a single decision tree is constructed for all sub-
states of all phonemes. The clustering procedure 
starts with all the polyphones at the root of the tree. 
The decision tree can ask questions regarding the 
identity of the center phoneme and its neighboring 
phonemes, plus the sub-state identity (be-
gin/middle/end). At each node, the question that 
yields the highest information gain is chosen and 
the tree is split. This process is repeated until the 
tree reaches a certain size. Enhanced tree clustering 
is well suited to implicitly capture the pronuncia-
tion variations in speech by allowing certain poly-
phones that are pronounced similarly to share the 
same set of parameters. Mimer et al (2004) shows 
that this approach can successfully be applied to 
grapheme based speech recognition by building 
separate trees for each sub-state for consonants and 
vowels.  
For the experiments on enhanced tree clustering, 
we used the same setting as the grapheme-based 
system. Instead of growing a single tree, we built 
six separate trees ? one each for begin, middle and 
end sub-states of vowels and consonants. Apart 
from the question set used in the grapheme-based 
system, we added singleton questions, which ask 
about the identity of different graphemes in a cer-
tain context. To apply the decision tree algorithm, 
a semi-continuous recognition system was trained. 
Since the number of models that share the same 
codebook drastically increases, we increased the 
number of Gaussians per codebook. Two different 
values were tested; 500 (ETC-500) and 1500 
(ETC-1500) Gaussians. Table 4 shows the recogni-
tion results on the test set, after applying enhanced 
tree clustering to the system based on trigraphemes 
(MUL-TRI).  
 500 models 2000 models 
MUL-TRI 26.0 % 26.0 % 
ETC-500 16.9 % 18.0 % 
ETC-1500 18.1 % 19.0 % 
Table 3: Word error rate in % for the enhance tree  
clustering method 
As can be seen from Table 3, the enhanced tree 
clustering has significant improvement over the 
best grapheme-based system. ETC-500 with rela-
tively lesser number of parameters has outper-
formed ETC-1500 system. Performance decreases 
when we increase the number of leaf nodes in the 
tree, from 500 to 2000. A closer look at the cluster 
trees that used the enhanced clustering reveals that 
19
50~100 models share parameters across different 
center graphemes. 
4 Grapheme vs. Phoneme based SR 
To evaluate our grapheme-based approach with the 
traditional phoneme-based approach, we compared 
the best GBSR system with two phoneme-based 
systems.  
The first system (PB-Man) uses a manually cre-
ated dictionary and is identical to (Suebvisai et al, 
2005) except that we used triphones as the speech 
unit. The second system (PB-LTS) uses an auto-
matically generated dictionary using letter-to-
sound rules. To generate the dictionary in PB-LTS, 
we used the letter-to-sound rules in Festival (Black 
1998) speech synthesis system trained with 20k 
words. We also applied the same reordering rules 
used in the GBSR system as described in section 2. 
Both the systems have 500 acoustic models based 
on triphones.  
Table 4 gives the WER for the two systems, on 
the test set. Best results from GBSR systems are 
also reproduced here for the comparison. 
 
Phoneme-based 
Using manual dictionary (PB-Man) 16.0 % 
Using automatic dictionary (PB-LTS) 24.5% 
Grapheme-based 
MUL-TRI 26.0 % 
MUL-TRI with ETC (ETC-500) 16.9 % 
Table 4: Word error rates in % of GBSR and 
PBSR systems 
As expected, the manually generated dictionary 
gives the best performance. The performance be-
tween PB-LTS and grapheme based system are 
comparable. ETC-500 system has a significantly 
better performance than the automatically gener-
ated dictionary, and almost the same results as the 
phoneme-based baseline. This shows that graph-
eme-based speech recognition coupled with the 
enhanced tree clustering can be successfully ap-
plied to Thai speech recognition without the need 
for a manually generated dictionary. 
5 Conclusions 
In this paper we presented the results for applying 
grapheme-based speech recognition to Thai lan-
guage. We experimented with different settings for 
the initial context independent system, different 
number of acoustic models and different contexts 
for the polygraphemes. We also tried the enhanced 
tree clustering method as a means of sharing pa-
rameters across models. The results show that the 
system with 500 acoustic models based on tri-
graphemes produce the best results. Additionally, 
the enhanced tree clustering significantly improves 
the recognition accuracy of the grapheme-based 
system. Our system outperformed a phoneme-
based system that uses an automatically generated 
dictionary. These results are very promising since 
they show that the grapheme-based approach can 
be successfully used to generate speech recognition 
systems for new languages using little linguistic 
knowledge.  
References  
Stefan Besling. 1994. ?Heuristical and Statistical Meth-
ods for Grapheme-to-Phoneme Conversion. In Pro-
ceedings of Konvens. Vienna, Austria. 
Alan W. Black, Kevin Lenzo and Vincent Pagel. 1998. 
Issues in Building General Letter to Sound Rules. In 
Proceedings of the ESCA Workshop on Speech Syn-
thesis, Australia. 
Sebastian Kanthak and Hermann Ney. 2002. Context-
dependent Acoustic Modeling using Graphemes for 
Large Vocabulary Speech Recognition. In Proceed-
ings of the ICASSP. Orlando, Florida. 
Mirjam Killer, Sebastian St?ker, and Tanja Schultz. 
2003. Grapheme Based Speech Recognition. In Pro-
ceeding of the Eurospeech. Geneva, Switzerland. 
Borislava Mimer, Sebastian St?ker, and Tanja Schultz. 
2004. Flexible Decision Trees for Grapheme Based 
Speech Recognition. In Proceedings of the 15th Con-
ference Elektronische Sprachsignalverarbeitung 
(ESSV), Cotbus, Germany, September. 
Tanja Schultz and Alex Waibel. 1998. Development of 
Multi-lingual Acoustic Models in the GlobalPhone 
Project. In Proceedings of the 1st  Workshop on Text, 
Speech, and Dialogue (TSD), Brno, Czech Republic. 
Sinaporn Suebvisai, Paisarn Charoenpornsawat, Alan 
Black and et.al. 2005 Thai Automatic Speech Recog-
nition.  Proceedings of ICASSP, Philadelphia, Penn-
sylvania. 
Hua Yu and Tanja Schultz. 2003. Enhanced Tree Clus-
tering with Single Pronunciation dictionary for Con-
versational Speech Recognition. In Proceedings of 
the 8th Eurospeech, Geneva, Switzerland.  
20
Proceedings of NAACL HLT 2007, Companion Volume, pages 89?92,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Geometric Interpretation of
Non-Target-Normalized Maximum Cross-channel Correlation
for Vocal Activity Detection in Meetings
Kornel Laskowski
interACT, Universita?t Karlsruhe
Karlsruhe, Germany
kornel@ira.uka.de
Tanja Schultz
interACT, Carnegie Mellon University
Pittsburgh PA, USA
tanja@cs.cmu.edu
Abstract
Vocal activity detection is an impor-
tant technology for both automatic speech
recognition and automatic speech under-
standing. In meetings, standard vocal
activity detection algorithms have been
shown to be ineffective, because partici-
pants typically vocalize for only a frac-
tion of the recorded time and because,
while they are not vocalizing, their channels
are frequently dominated by crosstalk from
other participants. In the present work,
we review a particular type of normaliza-
tion of maximum cross-channel correlation,
a feature recently introduced to address the
crosstalk problem. We derive a plausible
geometric interpretation and show how the
frame size affects performance.
1 Introduction
Vocal activity detection (VAD) is an important tech-
nology for any application with an automatic speech
recognition (ASR) front end. In meetings, partic-
ipants typically vocalize for only a fraction of the
recorded time. Their temporally contiguous contri-
butions should be identified prior to ASR in order to
leverage speaker adaptation schemes and language
model constraints, and to associate recognized out-
put with specific speakers (who said what). Segmen-
tation into such contributions is informed primarily
by VAD on a frame-by-frame basis.
Individual head-mounted microphone (IHM)
recordings of meetings present a particular challenge
for VAD, due to crosstalk from other participants.
Most state-of-the-art VAD systems for meetings rely
on decoding in a binary speech/non-speech space,
assuming independence among participants, but are
increasingly relying on features specifically designed
to address the crosstalk issue (Wrigley et al, 2005).
A feature which has attracted attention since its
use in VAD post-processing in (Pfau et al, 2001)
is the maximum cross-channel correlation (XC),
max? ?jk (?), between channels j and k, where ? is
the lag. When designing features descriptive of the
kth channel, XC is frequently normalized by the en-
ergy in the target1 channel k (Wrigley et al, 2003).
Alternately, XC can be normalized by the energy in
the non-target channel j (Laskowski et al, 2004),
a normalization which we refer to here as NT-Norm,
extending the Norm and S-Norm naming conventions
in (Wrigley et al, 2005). Table 1 shows several types
of normalizations which have been explored.
Normalization of XC Mean Min Max
(none) maxj 6=k ?jk(?) [2][4] [2][4] [2][4]
Norm maxj 6=k ?jk(?)?kk(0) [2][4] [2][4] [2][4]
S-Norm maxj 6=k ?jk(?)?
?jj(0)?kk(0)
[2][4][5] [2][4] [1][2][4]
NT-Norm maxj 6=k ?jk(?)?jj(0) [3] [6] [6]
Table 1: Normalizations and statistics of cross-
channel correlation features to describe channel k.
In [1], a median-smoothed version was used in post-
processing. In [3], the sum (JMXC) was used in-
stead of the mean. In [5], cross-correlation was com-
puted over samples and features. In [6], the mini-
mum and the maximum were jointly referred to as
NMXC. References in bold depict features selected
by an automatic feature selection algorithm in [2] and
[4]. (1:(Pfau et al, 2001), 2:(Wrigley et al, 2003),
3:(Laskowski et al, 2004), 4:(Wrigley et al, 2005),
5:(Huang, 2005), 6:(Boakye and Stolcke, 2006))
1The target/non-target terms are due to (Boakye and
Stolcke, 2006).
89
The present work revisits NT-Norm normalization,
which has been successfully used in a threshold de-
tector (Laskowski et al, 2004), in automatic initial
label assignment (Laskowski and Schultz, 2006), and
as part of a two-state decoder feature vector (Boakye
and Stolcke, 2006). Our main contribution is a geo-
metric interpretation of NT-Norm XC, in Section 2.
We also describe, in Section 3, several contrastive
experiments, and discuss the results in Section 4.
2 Geometric Interpretation
We propose an interpretable geometric approxima-
tion to NT-Norm XC for channel k,
?k,j =
max? ?jk (?)
?jj
, ?j 6=k (1)
We assume the simplified response in the kth IHM
microphone at a distance dk from a single point
source s (t) to be
mk (t) .= Ak
( 1
dk
s
(
t? dkc
)
+ ?k (t)
)
, (2)
where c, Ak and ?k (t) are the speed of sound, the
gain of microphone k, and source-uncorrelated noise
at microphone k, respectively. Cross-channel corre-
lation is approximated over a frame of size ? by
?jk (?) =
?
?
AjAk
djdk
s (t) s (t? ?) dt , (3)
where ? ? (dj ? dk) /c. Letting Ps ?
?
? s2 (t) dt and
P?k ?
?
? ?2k (t) dt,
?jj (0) = A2j
(
1
d2j
Ps + P?j
)
, (4)
max
?
?jk (?) =
AjAk
djdk
Ps , (5)
respectively, as the maximum of ?jk (?) occurs at
?? = (dk ? dj) /c. In consequence,
max? ?jk (?)
?jj (0)
? djdk
, (6)
provided that
Ak
Aj
?
?1? P?j1
d2j
Ps + P?j
?
? ? 1 , (7)
i.e., under assumptions of similar microphone gains,
a non-negligible farfield signal-to-noise ratio at each
microphone, and the simplifications embodied in
Equation 2, NT-Norm XC approximates the relative
distances of 2 microphones to the single point source
s (t). We stress that this approximation requires no
side knowledge about the true positions of the par-
ticipants or of their microphones.
Importantly, this interpretation is valid only if ??
lies within the integration window ? in Equation 3.
In (Boakye and Stolcke, 2006), the authors showed
that when the analysis window is 25 ms, the NMXC
feature is not as robust as frame-level energy flooring
followed by cross-channel normalization (NLED).
3 Experimental Setup
3.1 VAD and ASR Systems
Our multispeaker VAD system, shown in Figure 1,
was introduced in (Laskowski and Schultz, 2006).
Rather than detecting the 2-state speech (V) vs.
non-speech (N ) activity of each partipant indepen-
dently, the system implements a Viterbi search for
the best path through a 2K-state vocal interac-
tion space, where K is the number of participants.
Segmentation consists of three passes: initial la-
bel assignment (ILA), described in the next subsec-
tion, for acoustic model training; simultaneous multi-
participant Viterbi decoding; and smoothing to pro-
duce segments for ASR. In the current work, during
decoding, we limit the maximum number of simulta-
neously vocalizing participants to 3.
This system is an improved version of that fielded
in the NIST Rich Transcription 2006 Meeting Recog-
nition evaluation (RT06s)2, to produce automatic
segmentation in the IHM condition on conference
meetings. The ASR system which we use in this
paper is as described in (Fu?gen et al, 2007).
3.2 Unsupervised ILA
For unsupervised labeling of the test audio, prior to
acoustic model training, we employ the criterion
q? [k] =
?
?
?
V if
?
j 6=k
log
(
max? ?jk(?)
?jj(0)
)
> 0
N otherwise .
(8)
Assuming equality in Equation 6, this corresponds
to declaring a participant as vocalizing when the dis-
tance between the location of the dominant sound
source and that participant?s microphone is smaller
than the geometric mean of the distances from the
source to the remaining microphones, ie. when
K?1
?
?
j 6=k
dj > dk (9)
2http://www.nist.gov/speech/tests/rt/
90
AM
TRAINING
VITERBI
DECODING
AMILA
multichannel audio
REFRAMING
SMOOTHING ASR
Pass 1
WER
?
?
`
q?
?
q?q?F
Figure 1: VAD system architecture, with 4 error measurement points. Symbols as in the text.
We refer to this algorithm as ILAave. For contrast we
also consider ILAmin, with the sum in Equation 8 re-
placed by the minimum over j 6=k. This corresponds
to declaring a participant as vocalizing when the dis-
tance between the location of the dominant sound
source and that participant?s microphone is smaller
than the distance from the source to any other mi-
crophone. We do not consider ILAmax, whose inter-
pretation in light of Equation 6 is not useful.
3.3 Data
The data used in the described experiments con-
sist of two datasets from the NIST RT-05s and
RT-06s evaluations. The data which had been
used for VAD system improvement, rt05s eval*,
is the complete rt05s eval set less one meeting,
NIST 20050412-1303. This meeting was excluded
as it contains a participant without a microphone, a
condition known a priori to be absent in rt06s eval;
we use the latter in its entirety.
3.4 Description of Experiments
The experiments we present aim to compare ILAave
and ILAmin, and to show how the size of the inte-
gration window, ?, affects system performance. As
our VAD decoder operates at a frame size of 100ms,
we introduce a reframing step between the ILA com-
ponent and both AM training and decoding; see Fig-
ure 1. V is assigned to each 100ms frame if 50% or
more of the frame duration is assigned V by ILA;
otherwise, the 100ms frame is assigned an N label.
We measure performance in four locations within
the combined VAD+ASR system architecture, also
shown in Figure 1. We compute a VAD frame er-
ror just after reframing (q?F ), just after decoding
(q?), and just after smoothing (? (q?)). This er-
ror is the sum of the miss rate (MS), and the false
alarm rate excluding intervals of all-participant si-
lence (FAX), computed against unsmoothed word-
level forced alignment references. We use this met-
ric for comparative purposes only, across the vari-
ous measurement points. We also use first-pass ASR
word error rates (WERs), after lattice rescoring, as
a final measure of performance impact.
We evaluate, over a range of ILA frame sizes, the
performance of ILAave(3), with a maximum number
of simultaneously vocalizing participants of 3, and
for the contrastive ILAmin. We note that ILAmin
is capable of declaring at most one microphone at a
time as being worn by a current speaker. As a re-
sult, construction of acoustic models for overlapped
vocal activity states, described in (Laskowski and
Schultz, 2006), results in states of at most 2 simul-
taneously vocalizing participants. We therefore refer
to ILAmin as ILAmin(2), and additionally consider
ILAave(2), in which states with 3 simultaneously vo-
calizing participants are removed.
4 Results and Discussion
We show the results of our experiments in Ta-
ble 2. First-pass WERs, using reference segmenta-
tion (.stm), vary by 1.3% absolute (abs) between
rt05s eval and rt06s eval. We also note that re-
moving the one meeting with a participant without
a microphone reduces the rt05s eval manual seg-
mentation WER by 1.7% abs. WERs obtained with
automatic segmentation should be compared to the
manual segmentation WERs for each set.
As the q?F columns shows, ILAmin(2) entails sig-
nificantly more VAD errors than ILAave. Notably,
although we do not show the breakdown, ILAmin(2)
is characterized by fewer false alarms, but misses
much more speech than ILAave(2). This is due in
part to its inability to identify simultaneous talk-
ers. However, following acoustic model training and
use (q?), the VAD error rates between the two algo-
rithms are approximately equal.
In studying the WERs for each ILA algorithm in-
dependently, the variation across ILA frame sizes in
the range 25?100 ms can be significant: for example,
it is 1.2% abs for ILAmin(2) on rt06s eval, com-
pared to the difference with manual segmentation of
3.1% abs. Error curves, as a function of ILA frame
size, are predominantly shallow parabolas, except at
75 ms (notably for ILAmin(2) at q?F ); we believe that
91
VAD, rt05s WER, 1st passILA ? q?F q? ? (q?) 05 05* 06
a 100 31.3 16.7 16.0 39.0 34.1 39.6
v 75 33.6 16.6 15.9 38.9 34.1 39.9
e 50 35.2 16.7 16.0 38.8 34.0 39.33 25 36.8 17.3 16.3 39.6 34.2 39.7
a 100 31.3 15.8 15.2 37.8 34.4 39.7
v 75 33.6 15.6 15.0 37.9 34.4 39.6
e 50 35.2 15.8 15.2 37.6 34.3 39.32 25 36.8 16.4 15.6 38.1 34.3 39.5
m 100 43.4 15.8 14.7 38.2 35.2 39.3
i 75 51.9 15.6 14.6 38.1 35.2 39.3
n 50 47.1 15.7 14.6 37.9 35.1 40.12 25 47.7 16.2 14.9 38.1 35.4 40.5
refs 9.5 9.5 9.5 36.1 34.4 37.4
Table 2: VAD errors, measured at three points in our
system, and first-pass WERs for rt05s eval (05),
as well as first-pass WERs for rt05s eval* (05*)
and rt06s eval (06). Results are shown for 3 con-
trastive VAD systems (ILAave(3), ILAave(2) and
ILAmin(2)), and 4 ILA frame sizes (100ms, 75ms,
50ms, and 25ms).
this is because 75 ms does not divide evenly into the
decoder frame size of 100 ms, causing more deletions
across the reframing step than for other ILA frame
sizes. Error minima appear for an ILA frame size
somewhere between 50 ms and 75 ms, for both ASR
and post-decoding VAD errors.
Although (Pfau et al, 2001) considered a maxi-
mum lag of 250 samples (15.6ms, or 5m at the speed
of sound), their computation of S-Norm XC used
a rectangular window. Here, as in (Laskowski and
Schultz, 2006) and (Boakye and Stolcke, 2006), we
use a Hamming window. Our results suggest that a
large, broadly tapered window is important for Equa-
tion 6 to hold.
The table also shows that for datasets with-
out uninstrumented participants, rt05s eval*
and rt06s eval, ILAmin(2) is outperformed by
ILAave(2) by as much as 1.1% abs in WER, espe-
cially at small frame sizes. The difference for the full
rt05s eval dataset is smaller. The results also sug-
gest that reducing the maximum degree of simulta-
neous vocalization from 3 to 2 during decoding is an
effective means of reducing errors (ASR insertions,
not shown) for uninstrumented participants.
5 Conclusions
We have derived a geometric approximation for a
particular type of normalization of maximum cross-
channel correlation, NT-Norm XC, recently intro-
duced for multispeaker vocal activity detection. Our
derivation suggests that it is effectively comparing
the distance between each speaker?s mouth and each
microphone. This is novel, as geometry is most often
inferred using the lag of the crosscorrelation maxi-
mum, rather than its amplitude.
Our experiments suggest that frame sizes of 50?75
ms lead to WERs which are lower than those for ei-
ther 100 ms or 25 ms by as much as 1.2% abs; that
ILAave outperforms ILAmin as an initial label as-
signment criterion; and that reducing the degree of
simultaneous vocalization during decoding may ad-
dress problems due to uninstrumented participants.
6 Acknowledgments
This work was partly supported by the European
Union under the integrated project CHIL (IST-
506909), Computers in the Human Interaction Loop.
References
K. Boakye and A. Stolcke. 2006. Improved Speech
Activity Detection Using Cross-Channel Features for
Recognition of Multiparty Meetings. Proc. of INTER-
SPEECH, Pittsburgh PA, USA, pp1962?1965.
C. Fu?gen, S. Ikbal, F. Kraft, K. Kumatani, K. Laskowski,
J. McDonough, M. Ostendorf, S. Stu?ker, and M.
Wo?lfel. 2007. The ISL RT-06S Speech-to-Text Evalu-
ation System. Proc. of MLMI, Springer Lecture Notes
in Computer Science 4299, pp407?418.
Z. Huang and M. Harper. 2005. Speech Activity Detec-
tion on Multichannels of Meeting Recordings. Proc. of
MLMI, Springer Lecture Notes in Computer Science
3869, pp415?427.
K. Laskowski, Q. Jin, and T. Schultz. 2004.
Crosscorrelation-based Multispeaker Speech Activity
Detection. Proc. of INTERSPEECH, Jeju Island,
South Korea, pp973?976.
K. Laskowski and T. Schultz. 2006. Unsupervised Learn-
ing of Overlapped Speech Model Parameters for Multi-
channel Speech Activity Detection in Meetings. Proc.
of ICASSP, Toulouse, France, I:993?996.
T. Pfau and D. Ellis and A. Stolcke. 2001. Multi-
speaker Speech Activity Detection for the ICSI Meet-
ing Recorder. Proc. of ASRU, Madonna di Campiglio,
Italy, pp107?110.
S. Wrigley, G. Brown, V. Wan, and S. Renals. 2003.
Feature Selection for the Classification of Crosstalk
in Multi-Channel Audio. Proc. of EUROSPEECH,
Geneva, Switzerland, pp469?472.
S. Wrigley, G. Brown, V. Wan, and S. Renals. 2005.
Speech and Crosstalk Detection in Multichannel Au-
dio. IEEE Trans. on Speech and Audio Processing,
13:1, pp84?91.
92
Proceedings of NAACL HLT 2007, Companion Volume, pages 129?132,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ADVANCES IN THE CMU/INTERACT ARABIC GALE TRANSCRIPTION SYSTEM   
 
Mohamed Noamany, Thomas Schaaf*, Tanja Schultz 
 
InterACT, Language Technologies Institute, Carnegie Mellon University  
Pittsburgh, PA 15213 
{mfn,tschaaf,tanja@cs.cmu.edu} 
 
                                                 
* Now with Toshiba Research Europe Ltd, Cambridge, United Kingdom 
ABSTRACT 
This paper describes the CMU/InterACT effort in 
developing an Arabic Automatic Speech Recognition 
(ASR) system for broadcast news and conversations 
within the GALE 2006 evaluation. Through the span of 
9 month in preparation for this evaluation we improved 
our system by 40% relative compared to our legacy 
system. These improvements have been achieved by 
various steps, such as developing a vowelized system, 
combining this system with a non-vowelized one, 
harvesting transcripts of TV shows from the web for 
slightly supervised training of acoustic models, as well 
as language model adaptation, and finally fine-tuning 
the overall ASR system. 
Index Terms? Speech recognition, Vowelization, 
GALE, Arabic, Slightly supervised training, web data. 
 
1. INTRODUCTION 
The goal of the GALE (Global Autonomous Language 
Exploitation) program is to develop and apply computer 
software technologies to absorb, analyze and interpret 
huge volumes of speech and text in multiple languages 
and make them available in English. In a long run this 
requires to combine techniques from text 
summarization, information retrieval, machine 
translation, and automatic speech recognition.  NIST 
will perform regular evaluations and the first evaluation 
took place recently. This paper describes improvements 
in the CMU Modern Standard Arabic (MSA) system 
through the span of 9 months in preparation for this 
evaluation.  
One of the language characteristics and challenges of 
Arabic is that some vowels are omitted in the written 
form. These vowels carry grammatical case information 
and may change the meaning of a word. Modeling the 
vowels in the pronunciation dictionary was found to 
give improvements over un-vowelized pronunciations 
[4].  In this paper we achieved another significant 
improvement by combining a vowelized with a non-
vowelized system. Furthermore, we got gains by 
collecting and utilizing web transcripts from TV show, 
which include broadcast conversations.  
 
2. SYSTEM DESCRIPTION 
Our MSA speech recognition system is based on the 
Janus Recognition Toolkit JRTk [9] and the IBIS 
decoder [10].  
Before decoding the audio, an automatic segmentation 
step and a speaker clustering step is performed. The 
segmentation step aims at excluding those segments that 
contain no speech, such as music or background noise.  
The remaining segments are clustered into speaker 
clusters such that all adaptation and normalization steps 
can be processed on clusters as batches. 
 
From the incoming 16 kHz audio signal we extract for 
each segment power spectral features using a FFT with 
a 10ms frame-shift and a 16ms Hamming window. From 
these we compute 13 Mel-Frequency Cepstral 
Coefficients (MFCC) per frame and perform a cepstral 
mean as well as variance normalization on a cluster 
basis. To incorporate dynamic features we concatenate 
15 adjacent MFCC frames (?7) and project these 195 
dimensional features into a 42 dimensional space using 
a transform found by linear discriminate analysis 
(LDA). We use the context-dependent codebooks as 
classes for finding the LDA transform [2]. On top of the 
LDA we apply a single maximum likelihood trained 
Semi-Tied-Covariance (STC) matrix. 
 
The general decoding setup employs a first pass in 
which a speaker independent acoustic model without 
vocal tract length normalization (VTLN) and no 
adaptation is used. The hypotheses of a cluster from the 
first pass are then used to estimate the VTLN warping 
factors to warp the power spectrum using the maximum 
likelihood approach described in [8]. After the VTLN 
factors are found, the same hypotheses are considered to 
estimate a feature space adaptation (FSA) using a 
constrained MLLR (CMLLR) transform. Then a model 
space adaptation is performed using maximum 
likelihood linear regression with multiple regression 
classes. The regression classes are found through 
clustering of the Gaussians in the acoustic model. The 
second pass decoding uses a speaker adaptive trained 
129
acoustic model, in which the adaptation was performed 
using a single CMLLR transform per speaker. 
For the non-vowelized system, we applied a grapheme-
to-phoneme approach to automatically generate the 
pronunciation dictionary. For the vowelized system we 
used the same phoneme set as in the non-vowelized 
system but extended it with the 3 short vowels, which 
do not appear in the writing system. Both systems are 2-
pass system as described above and employ Cepstral 
Mean Normalization (CMN), MLLR, Semi-tied 
covariance (STC), and Feature space adaptation (FSA).  
 
For the development of context dependent acoustic 
models we applied an entropy-based polyphone 
decision tree clustering process using context questions 
of maximum width ?2, resulting in shared quin-phones. 
In addition we included word-boundary tags into the 
pronunciation dictionary, which can be asked for in the 
decision tree can ask for word-boundary tags. The non-
vowelized system uses 4000 phonetically-tied quin-
phones with a total of 305,000 Gaussians. The non-
vowelized system has 5000 codebooks with a total of 
308,000 Gaussians.  
In total we used 190 hours for acoustic training. 
These consist of 40 hours Broadcast news (BN) from 
manually transcribed FBIS data, 50 hours BN LDC-
TDT4 selected from 85 hours using a slightly 
supervised approach as described in [3], and 30 hours 
Broadcast conversation (BC) recorded from Al-jazeera 
TV, and 70 hours (40hrs BN, 30hrs BC) from LDC-
GALE data. For quality reasons we removed some of 
the most recent GALE data from acoustic model 
training.  
4. LANGUAGE MODELING 
The Arabic Giga word corpus distributed by LDC is 
currently the major Arabic text resource for language 
modeling. Since this corpus only covers broadcast news, 
we spidered the web to cover broadcast conversational 
data. We found transcripts for Arabic talk shows on the 
Al-jazeera web site www.al-jazeera.net and collected all 
data available from 1998 to 2005. We excluded all 
material from 2006 to comply the evaluation rules 
which prohibit the use of any data starting February 
2006. In addition to the mentioned data we collected 
BN data from the following source: Al-Akhbar 
(Egyptian daily newspaper 08/2000 to 12/2005) and 
Akhbar Elyom (Egyptian weekly newspaper 08/2000 to 
12/2005). Furthermore, we used unsupervised training 
transcripts from 750 hours BN created and shared by 
IBM. 
 
For language modeling building we used the SRILM 
tool kit from SRI [5]. Since we have 2 kinds of data, 
Broadcast News and Conversation, we built various 
individual 4-grams language models. 11 models were 
then interpolated to create one language model. The 
interpolation weights were selected based on a held out 
data set from BN and BC sources. We found that the 
data from Al-jazeera (both BN & BC) has the highest 
weight comparing to other sources. The resulting final 
language model uses a total number of n-grams is 126M 
and a vocabulary of 219k words. The perplexity of the 
language model is 212 on a test set containing BC and 
BN data. 
5. TV WEB TRANSCRIPTS 
Most of our acoustic and language model training data 
comes from broadcast news. However, since GALE 
targets broadcast news as well as conversations we 
looked for an effective method to increase the training 
data for Arabic BC. We made use of the fact that some 
Arabic TV stations place transcripts for their program 
on the web. These transcripts lack time stamp but 
include acceptable quality of the transcription. 
However, one challenge is that the transcriptions are not 
complete in that they do not include transcripts of 
commercials or any news break that may interrupt the 
show.  In total we recorded 50 hours of Broadcast 
conversation shows from Al-jazeera and used them in 
our acoustic model and language model training by 
performing the following procedures:  
? We manually selected shows from Al-jazeera TV 
? We used a scheduler to automatically start the 
recording of the selected shows.  
? We spidered the web to collect corresponding show 
transcripts from their web site www.aljazeera.net.  
? We automatically processed the transcripts to 
convert the html files to text, convert numbers to 
words and remove any non-Arabic words in the 
shows.  
? We added these shows to our LM data with high 
weight, built a biased LM, and used this LM to 
decode the recorded shows.  
? We aligned the reference (transcripts without time 
stamps) with the decoder output that may contain 
speech recognition errors.  
? We selected only the portions that are correct; we 
did not select any portion with number of words 
less than 3 correct consecutive words.  
? Based on the above criteria we finally selected 30 
hours out of the total 40 hours recordings. 
? We clustered utterances based on BIC criteria 
approach described in [7]. 
  
As a result, we managed to project the time stamp in the 
original transcript such that it can be used for training. 
Using these 30 hours of data resulted in a 7% relative 
improvement on RT04. Since RT04 is broadcast news, 
we expect even higher gains on broadcast 
conversational data. It is worth mentioning that we 
130
applied the same slightly supervised approach to the 
TDT4 data which is a low quality quick transcription. 
We selected 50 out of 80 hours and achieved an 
improvement of 12% relative. The gain was higher 
since at the time of these experiments we had only 40 
hours of training from FBIS data, therefore more than 
doubled the amount of training data by adding TDT4. 
 
6. NON-VOWELIZED SYSTEM 
Arabic spelling is mostly phonemic; there is a close 
letter-to-sound correspondence. We used a grapheme-
to-phoneme approach similar to [1]. Our phoneme set 
contains 37 phonemes plus three special phonemes for 
silence, non-speech events, and non-verbal effects, such 
as hesitation. 
 We preprocessed the text by mapping the 3 shapes of 
the grapheme for glottal stops to one shape at the 
beginning of the word since these are frequently miss-
transcribed. This preprocessing step leads to 20% 
reduction in perplexity of our language model and 0.9% 
improvements in the final WER performance on RT04. 
Preprocessing of this kind appears to be appropriate 
since the target of the project is not transcription but 
speech translation and the translation community 
applies the same pre-processing. We used a vocabulary 
of 220K words selected by including all words 
appearing in the acoustic transcripts and the most 
frequent words occurring in the LM. The OOV rate is 
1.7% on RT04. Table 1 shows the performance of our 
Speaker-Independent (SI) and Speaker-Adaptive (SA) 
non-vowelized system on the RT04 set. 
 
   Table 1: Non-vowelized System Results 
            System     WER on RT04 (%) 
Non-Vowelized         (SI)           25.3 
Non-Vowelized         (SA)           20.8 
 
7. VOWELIZED SYSTEM 
Written MSA lacks vowels, thus native speakers add 
them during reading. Vowels are written only in 
children books or traditional religious books. To restore 
vowels for a 129K vocabulary [4], we performed the 
following steps:   
? Buckwalter morphological analyzer (BMA) (found 
106K out of 129K entries). 
? If a word is not vowelized by the analyzer, we 
check for its vowelization in the LDC Arabic Tree-
Bank (additional 5k entries found). 
? If the word did not appear in any of those, we used 
the written non-vowelized word form.  
In total 11k entries could not be resolved by either the 
BMA or the Treebank. 
This vowelization step resulted in 559,035 
pronunciations for the 129k words in our vocabulary, 
i.e. we have on average 5 pronunciations per word. To 
reduce the number of pronunciation variants we 
performed a forced alignment and excluded 
pronunciations which did not occur in the training 
corpus. This results in 407,754 pronunciations, which is 
a relative reduction of about 27%. For system training 
we used the same vocabulary and applied the same 
training procedure as in the non-vowelized system for 
acoustic model training.  
As Table 2 shows, we achieved a very good gain of 
1.3% absolute on the SI pass and 1.5% on the SA pass, 
both benchmarked on RT04 (compare Table 1). We 
envision to seeing even higher improvements after 
estimating and applying probability priors to multiple 
pronunciation and after vowelizing the remainder 11k 
words that had not been covered by BMA or the Tree-
Bank.  
 
   Table2: Vowelized System Results 
           System  WER on RT04 (%) 
Vowelized         (SI)        24.0 
Vowelized         (SA)       19.3 
 
8. COMBINING VOWELIZED & NON-
VOWELIZED SYSTEM 
After seeing significant improvements by vowelization, 
we investigated the performance gain through cross-
adapting the vowelized system with the non-vowelized 
system. The vowelized system cross adapted with the 
SA non-vowelized gave us 1.3 over the vowelized 
system  adapted on the SI vowelized system. We used a 
3-pass decoding strategy, in which the first pass uses the 
speaker independent (SI) vowelized system, the second 
pass uses the speaker adaptive (SA) non-vowelized 
system, and the third, final pass, uses the speaker 
adaptive vowelized system.  Some challenges for the 
cross-adaptations had to be overcomed, for instance to 
cross adapt the non-vowelized system on the vowelized 
system, we had to remove the vowels to have a non-
vowelized transcript. Since the phoneme set of the non-
vowelized system is a subset of the phoneme set of the 
vowelized system, we could simply exclude the vowel 
phonemes from the vowelized system.  Furthermore, the 
search vocabulary is the same and so is the language 
model.  
The main changes are the pronunciation dictionary and 
the decision tree. We tried different combination 
schemes, e.g. by starting with the non-vowelized 
system, then the vowelized, and then the non-vowelized 
but found that none outperforms the combination 
reported here in terms of WER. In addition starting with 
the non-vowelized SI pass is much faster than the 
vowelized SI system (4.5RT compared to 9RT). 
131
 Table 3: Non-vowelized & vowelized System 
Combination 
            System    WER on RT04 (%) 
Vowelized         (SI)           24.0 
Non-Vowelized (SA)           19.9 
Vowelized         (SA)           18.3 
 
9.  ACOUSTIC MODEL PARAMETER TUNING 
We started our legacy system with 40 hours and until it 
reached 90 hours we were using the same number of 
codebooks (3000) and same number of Gaussians (64) 
per codebook. With the increase of training data from 
90 hours to 190 hours we investigated the effect of 
increasing the number of codebooks and Gaussians. 
Also, we were using merge and split training (MAS) 
and STC only for the adapted pass; we furthermore 
investigated the effect of using it for the SI pass. We 
found that using MAS & STC on the SI pass gave us a 
gain of 5% relative on the SI pass. In addition we found 
that the ideal number of codebooks is 5000 for the non-
vowelized system resulting in a gain of 5.3% relative on 
the SI pass.  We expect to see further gains on the SA 
pass. Table 4 summarizes the system performance using 
different parameter sizes and training schemes.  
 
Table 4: System Performance vs.Model Size  
#codebooks MAS #Gausians Voc System WER(%) 
3K - 64K 129 Non-
vow(NV) 
29.6 
3K Mas 64K 129      NV 28.3 
5K Mas 64K 129      NV 27.9 
5K Mas 100K 129      NV 27.6 
5K Mas 100K 200 nv+tv 
TRANS 
26.3 
3K Mas 100K 200 vow+ 
tvTRANS 
24.0 
 
10. SYSTEM EVOLUTION 
Table 5 shows the gains we achieved at major milestone 
stages while building the system. The key improvements 
are due to adding data collected from the web, 
Vowelization, and combining the vowelized and non-
vowelized systems. Tuning the acoustic models 
parameters gave us a good gain and finally the 
interpolation of different language model for different 
sources gave additional improvements. The real-time 
behavior of the system improved from 20RT to 10 RT 
while loosing only 0.2% which is in acceptable trade-
off. Recently, we gained 3.5% relative applying 
discriminative training (MMIE).  
 
11. CONCLUSION 
We presented the CMU 2006 GALE ASR Arabic 
system. It can be seen that we achieved 40% 
improvements over our legacy system. 
Table 5: System Progress WER (%) 
LEGACY SYSTEM  32.7 
STC+VTLN 30.1 
SPEED FROM 20RT TO 10RT 30.3 
FROM 3 TO 4GM+BETTER 
SEGMENTATION 
28.4 
TDT4 TRANSCIPTS SELECTION 
REFINEMENT 
26.3 
CLUSTERING REFINEMENT & 
RETRAINING 
25.5 
MORE LM DATA +INTERPOLATING 11 LMS 24.2 
ADDITION Q3 OF LDC DATA 23.6 
ACOUSTIC MODEL PARAMETER TUNING 20.7 
MMIE 20.0 
COMBINED SYSTEMS (VOW+NON-VOW) 18.3 
 
We combined a vowelized and a non-vowelized system 
and achieved 4.0% relative over the vowelized system. 
Also, we managed to use TV web transcript as a method 
to cover the shortage of training data specially the 
broadcast conversation. Currently, we are exploring 
more on the vowelized system by adding weights to 
different multiple pronunciations and adding 
vowelization to words not covered by the morphological 
analyzer or the tree-bank. 
12. ACKNOWLEGMENTS 
We also would like to thank Qin Jin for applying her 
automatic clustering techniques to the web data.  
 
13. REFERENCES 
[1] J. Billa, M. Noamany, A. Srivastava,  D. Liu,  R. Stone, J. 
Xu,  J. Makhoul, and F. Kubala, ?Audio Indexing of 
Arabic Broadcast News?,   International Conference on 
Acoustics, Speech, and Signal Processing  (ICASSP), 
May 2002. 
[2] H. Yu, Y-C. Tam, T. Schaaf, S. St?ker, Q. Jin, M. 
Noamany, and T. Schultz ?The ISL RT04 Mandarin 
Broadcast News Evaluation System?, EARS Rich 
Transcription workshop, Palisades, NY, 2004. 
[3] L. Nguyen et al, ?Light supervision in acoustic model 
training,? ICASSP , Montreal, QC, Canada, May 2004. 
 [4] M. Afify et al,"Recent progress in Arabic broadcast news 
transcription at BBN", In INTERSPEECH-2005. 
[5] A. Stolcke SRILM- An Extensible Language Modeling 
ToolKit  ICSLP. 2002, Denver, Colorado. 
 [6] T.Buckwalter, ?Issues in Arabic Orthography and 
morphology Analysis?, COLING 2004, Geneva, 2004. 
[7] Q. Jin, T. Schultz, ?Speaker segmentation and clustering 
in meetings?, ICSLP, 2004. 
[8] P. Zhan, M. Westphal, "Speaker Normalization Based On 
Frequency Warping", ICASSP 1997, Munich, Germany. 
[9] M. Finke, et al, "The Karlsruhe Verbmobil Speech 
Recognition Engine," International Conference on 
Acoustics, Speech, and Signal Processing, ICASSP,1997. 
[10] H. Soltau, et al, "A One Pass-Decoder Based On 
Polymorphic Linguistic Context",  ASRU 2001, Trento, 
Italy, 2001 
132
Proceedings of NAACL HLT 2009: Short Papers, pages 149?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incremental Adaptation of Speech-to-Speech Translation
Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,
Tanja Schultz, Ian Lane, Alex Waibel and Alan W. Black
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.edu
Abstract
In building practical two-way speech-to-speech
translation systems the end user will always wish
to use the system in an environment different from
the original training data. As with all speech sys-
tems, it is important to allow the system to adapt
to the actual usage situations. This paper investi-
gates how a speech-to-speech translation system can
adapt day-to-day from collected data on day one to
improve performance on day two. The platform is
the CMU Iraqi-English portable two-way speech-
to-speech system as developed under the DARPA
TransTac program. We show how machine transla-
tion, speech recognition and overall system perfor-
mance can be improved on day 2 after adapting from
day 1 in both a supervised and unsupervised way.
1 Introduction
As speech-to-speech translation systems move from the
laboratory into field deployment, we quickly see that mis-
match in training data with field use can degrade the per-
formance of the system. Retraining based on field us-
age is a common technique used in all speech systems
to improve performance. In the case of speech-to-speech
translation we would particularly like to be able to adapt
the system based on its usage automatically without hav-
ing to ship data back to the laboratory for retraining. This
paper investigates the scenario of a two-day event. We
wish to improve the system for the second day based on
the data collected on the first day.
Our system is designed for eyes-free use and hence
provides no graphical user interface. This allows the user
to concentrate on his surrounding environment during an
operation. The system only provides audio control and
feedback. Additionally the system operates on a push-to-
talk method. Previously the system (Hsiao et al, 2006;
Bach et al, 2007) needed 2 buttons to operate, one for the
English speaker and the other one for the Iraqi speaker.
W i i c o n t r o l l e r
M i c & L i g h t
L o u d  s p e a k e r
Figure 1: The users interact with the system
To make the system easier and faster to use, we propose
to use a single button which can be controlled by the En-
glish speaker. We mounted a microphone and a Wii re-
mote controller together as shown in 1.
Since the Wii controller has an accelerometer which
can be used to detect the orientation of the controller, this
feature can be applied to identify who is speaking. When
the English speaker points towards himself, the system
will switch to English-Iraqi translation. However, when
the Wii is pointed towards somebody else, the system will
switch to Iraqi-English translation. In addition, we attach
a light on the Wii controller providing visual feedback.
This can inform an Iraqi speaker when to start speaking.
The overall system is composed of five major compo-
nents: two automatic speech recognition (ASR) systems,
a bidirectional statistical machine translation (SMT) sys-
tem and two text-to-speech (TTS) systems.
2 Data Scenario
The standard data that is available for the TransTac
project was collected by recording human interpreter
mediated dialogs between war fighters and Iraqi native
speakers in various scenarios. The dialog partners were
aware that the data was being collected for training ma-
chine based translation devices, but would often talk di-
rectly to the human interpreter rather than pretending it
was an automatic device. This means that the dialog
149
partners soon ignored the recording equipment and used
a mostly natural language, using informal pronunciation
and longer sentences with more disfluencies than we find
in machine mediated translation dialogs.
Most users mismatch their language when they com-
municate using an automatic speech-to-speech transla-
tion system. They often switch to a clearer pronuncia-
tion and use shorter and simpler sentences with less dis-
fluency. This change could have a significant impact on
speech recognition and machine translation performance
if a system was originally trained on data from the inter-
preter mediated dialogs.
For this reason, additional data was collected during
the TransTac meeting in June of 2008. This data was
collected with dialog partners using the speech-to-speech
translation systems from 4 developer participants in the
TransTac program. The dialog partners were given a de-
scription of the specific scenario in form of a rough script
and had to speak their sentences into the translation sys-
tems. The dialog partners were not asked to actually react
to the potentially incorrect translations but just followed
the script, ignoring the output of the translation system.
This has the effect that the dialog partners are no longer
talking to a human interpreter, but to a machine, press-
ing push-to-talk buttons etc. and will change their speech
patterns accordingly.
The data was collected over two days, with around 2
hours of actual speech per day. This data was transcribed
and translated, resulting in 864 and 824 utterance pairs
on day 1 and 2, respectively.
3 ASR LM Adaptation
This section describes the Iraqi ASR system and how we
perform LM adaptation on the day 1 data to improve ASR
performance on day 2. The CMU Iraqi ASR system is
trained with around 350 hours of audio data collected un-
der the TransTac program. The acoustic model is speaker
independent but incremental unsupervised MLLR adap-
tation is performed to improve recognition. The acous-
tic model has 6000 codebooks and each codebook has
at most 64 Gaussian mixtures determined by merge-and-
split training. Semi-tied covariance and boosted MMI
discriminative training is performed to improve the model
(Povey et al, 2009). The features for the acoustic model
is the standard 39-dimension MFCC and we concatenate
adjacent 15 frames and perform LDA to reduce the di-
mension to 42 for the final feature vectors. The language
model of the ASR system is a trigram LM trained on the
audio transcripts with around three million words with
Kneser-Ney smoothing (Stolcke, 2002).
To perform LM adaptation for the ASR system, we use
the ASR hypotheses from day 1 to build a LM. This LM
is then interpolated with the original trigram LM to pro-
duce an adapted LM for day 2. We also evaluate the effect
of having transcribers provide accurate transcription ref-
erences for day 1 data, and see how it may improve the
performance on day 2. We compare unigram, bigram and
trigram LMs for adaptation. Since the amount of day 1
data is much smaller than the whole training set and we
do not assume transcription of day 1 is always available,
the interpolation weight is chosen of be 0.9 for the orig-
inal trigram LM and 0.1 for the new LM built from the
day 1 data. The WER of baseline ASR system on day 1
is 32.0%.
Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref
31.3 30.9 31.2 31.1 30.6 30.5 30.4
Table 1: Iraqi ASR?s WER on day 2 using different adaptation
schemes for day 1 data
The results in Table 1 show that the ASR benefits from
LM adaptation. Adapting day 1 data can slightly improve
the performance of day 2. The improvement is larger
when day 1 transcript is available which is expected. The
result also shows that the unigram LM is the most robust
model for adaptation as it works reasonably well when
transcripts are not available, whereas bigram and trigram
LM are more sensitive to the ASR errors made on day 1.
Day 1 Day 2
No ASR adaptation 29.39 27.41
Unsupervised ASR adaptation 31.55 27.66
Supervised ASR adaptation 32.19 27.65
Table 2: Impact of ASR adaptation to SMT
Table 2 shows the impact of ASR adaptation on the
performance of the translation system in BLEU (Papineni
et al, 2002). In these experiments we only performed
adaptation on ASR and still using the baseline SMT com-
ponent. There is no obvious difference between unsuper-
vised and supervised ASR adaptation on performance of
SMT on day 2. However, we can see that the difference
in WER on day 2 of unsupervised and supervised ASR
adaptation is relatively small.
4 SMT Adaptation
The Iraqi-English SMT system is trained with around
650K sentence pairs collected under the TransTac pro-
gram. We used PESA phrase extraction (Vogel, 2005)
and a suffix array language model (Zhang and Vogel,
2005). To adapt SMT components one approach is to op-
timize LM interpolation weights by minimizing perplex-
ity of the 1-best translation output (Bulyko et al, 2007).
Related work including (Eck et al, 2004) attempts to use
information retrieval to select training sentences similar
to those in the test set. To adapt the SMT components
we use a domain-specific LM on top of the background
150
language models. This approach is similar to the work
in (Chen et al, 2008). sThe adaptation framework is 1)
create a domain-specific LM via an n-best list of day 1
machine translation hypothesis, or day 1 translation ref-
erences; 2) re-tune the translation system on day 1 via
minimum error rate training (MERT) (Venugopal and Vo-
gel, 2005).
Use Day 1 Day 2
Baseline 29.39 27.41
500 Best 1gramLM 29.18 27.23
MT Hypos 2gramLM 29.53 27.50
3gramLM 29.36 27.23
Table 3: Performance in BLEU of unsupervised adaptation.
The first question we would like to address is whether
our adaptation obtains improvements via an unsupervised
manner. We take day 1 baseline ASR hypothesis and use
the baseline SMT to get the MT hypothesis and a 500-
best list. We train a domain LM using the 500-best list
and use the MT hypotheses as the reference in MERT. We
treat day 1 as a development set and day 2 as an unseen
test set. In Table 3 we compare the performance of four
systems: the baseline which does not have any adaptation
steps; and 3 adapted systems using unigram, bigram and
trigram LMs build from 500-best MT hypotheses.
Use Day 1 Day 2
Baseline (no tune) 29.39 27.41
Baseline (tune) 29.49 27.30
500 Best 1gramLM 30.27 28.29
MT Hypos 2gramLM 30.39 28.30
3gramLM 28.36 24.64
MT Ref 1gramLM MT Ref 30.53 28.35
Table 4: Performance in BLEU of supervised adaptation.
Experimental results from unsupervised adaptation did
not show consistent improvements but suggest we may
obtain gains via supervised adaptation. In supervised
adaptation, we assume we have day 1 translation refer-
ences. The references are used in MERT. In Table 4 we
show performances of two additional systems which are
the baseline system without adaptation but tuned toward
day 1, and the adapted system which used day 1 trans-
lation references to train a unigram LM (1gramLM MT
Ref). The unigram and bigram LMs from 500-best and
unigram LM from MT day 1 references perform rela-
tively similar on day 2. Using a trigram 500-best LM
returned a large degradation and this LM is sensitive to
the translation errors on day1
5 Joint Adaptation
In Sections 3 and 4 we saw that individual adaptation
helps ASR to reduce WER and SMT to increase BLEU
ASR SMT Day 1 Day 2
No adaptation No adaptation 29.39 27.41
Unsupervised ASR 1gramLM 500-Best 32.07 28.65
adaptation with MT Hypo
1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83
Supervised ASR 1gramLM 500-Best 32.48 28.59
adaptation with MT Hypo
1gramLM transcription 1gramLM MT Ref 32.68 28.60
Table 5: Performance in BLEU of joint adaptation.
score. The next step in validating the adaptation frame-
work was to check if the joint adaptation of ASR and
SMT on day 1 data will lead to improvements on day
2. Table 5 shows the combination of ASR and SMT
adaptation methods. Improvements are obtained by us-
ing both ASR and SMT adaptation. Joint adaptation con-
sistently gained more than one BLEU point improvement
on day 2. Our best system is unsupervised ASR adapta-
tion via 1gramLM of ASR day 1 transcription coupled
with supervised SMT adaptation via 1gramLM of day
1 translation references. An interesting result is that to
have a better result on day 2 our approach only requires
translation references on day 1. We selected 1gramLM
of 500-best MT hypotheses to conduct the experiments
since there is no significant difference between 1gramLM
and 2gramLM on day 2 as showed in Table 3.
6 Selective Adaptation
The previous results indicate that we require human
translation references on day 1 data to get improved per-
formance on day 2. However, our goal is to make a better
system on day 2 but try to minimize human efforts on day
1. Therefore, we raise two questions: 1) Can we still ob-
tain improvements by not using all of day 1 data? and 2)
Can we obtain more improvements?
To answer these questions we performed oracle exper-
iments when we take the translation hypotheses on day
1 of the baseline SMT and compare them with transla-
tion references, then select sentences which have BLEU
scores higher than a threshold. The subset of day 1 sen-
tences is used to perform supervised adaptation in a sim-
ilar way showed in section 5. These experiments also
simulate the situation when we have a perfect confidence
score for machine translation hypothesis selection. Table
6 shows results when we use various portions of day 1 to
perform adaptation. By using day 1 sentences which have
smoothed sentence BLEU scores higher than 10 or 20 we
have very close performance with adaptation by using all
day 1 data. The results also show that by using 416 sen-
tences which have sentence BLEU score higher than 40
on day 1, our adapted translation components outperform
the baseline. Performance starts degrading after 50. Ex-
perimental results lead to the answer for question 1) that
151
by using less day 1 data our adapted translation compo-
nents still obtain improvements compare with the base-
line, and 2) we did not see that using less data will lead
us to a better performance compare with using all day 1
data.
No. sents Day 1 Day 2
Baseline 29.39 27.41
? 0 864 30.27 28.29
? 10 797 31.15 28.27
? 20 747 30.81 28.24
? 30 585 30.04 27.71
? 40 416 29.72 27.65
? 50 296 30.06 27.04
Correct 98 29.18 27.19
Table 6: Performance in BLEU of selective adaptation
W i c o n t r o l e M & L g
h r n u
d
r c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  
h r n u c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 520?527,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Bilingual-LSA Based LM Adaptation for Spoken Language Translation
Yik-Cheung Tam and Ian Lane and Tanja Schultz
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213
{yct,ian.lane,tanja}@cs.cmu.edu
Abstract
We propose a novel approach to crosslingual
language model (LM) adaptation based on
bilingual Latent Semantic Analysis (bLSA).
A bLSA model is introduced which enables
latent topic distributions to be efficiently
transferred across languages by enforcing
a one-to-one topic correspondence during
training. Using the proposed bLSA frame-
work crosslingual LM adaptation can be per-
formed by, first, inferring the topic poste-
rior distribution of the source text and then
applying the inferred distribution to the tar-
get language N-gram LM via marginal adap-
tation. The proposed framework also en-
ables rapid bootstrapping of LSA models
for new languages based on a source LSA
model from another language. On Chinese
to English speech and text translation the
proposed bLSA framework successfully re-
duced word perplexity of the English LM by
over 27% for a unigram LM and up to 13.6%
for a 4-gram LM. Furthermore, the pro-
posed approach consistently improved ma-
chine translation quality on both speech and
text based adaptation.
1 Introduction
Language model adaptation is crucial to numerous
speech and translation tasks as it enables higher-
level contextual information to be effectively incor-
porated into a background LM improving recogni-
tion or translation performance. One approach is
to employ Latent Semantic Analysis (LSA) to cap-
ture in-domain word unigram distributions which
are then integrated into the background N-gram
LM. This approach has been successfully applied
in automatic speech recognition (ASR) (Tam and
Schultz, 2006) using the Latent Dirichlet Alloca-
tion (LDA) (Blei et al, 2003). The LDA model can
be viewed as a Bayesian topic mixture model with
the topic mixture weights drawn from a Dirichlet
distribution. For LM adaptation, the topic mixture
weights are estimated based on in-domain adapta-
tion text (e.g. ASR hypotheses). The adapted mix-
ture weights are then used to interpolate a topic-
dependent unigram LM, which is finally integrated
into the background N-gram LM using marginal
adaptation (Kneser et al, 1997)
In this paper, we propose a framework to per-
form LM adaptation across languages, enabling the
adaptation of a LM from one language based on the
adaptation text of another language. In statistical
machine translation (SMT), one approach is to ap-
ply LM adaptation on the target language based on
an initial translation of input references (Kim and
Khudanpur, 2003; Paulik et al, 2005). This scheme
is limited by the coverage of the translation model,
and overall by the quality of translation. Since this
approach only allows to apply LM adaptation af-
ter translation, available knowledge cannot be ap-
plied to extend the coverage. We propose a bilingual
LSA model (bLSA) for crosslingual LM adaptation
that can be applied before translation. The bLSA
model consists of two LSA models: one for each
side of the language trained on parallel document
corpora. The key property of the bLSA model is that
520
the latent topic of the source and target LSA mod-
els can be assumed to be a one-to-one correspon-
dence and thus share a common latent topic space
since the training corpora consist of bilingual paral-
lel data. For instance, say topic 10 of the Chinese
LSA model is about politics. Then topic 10 of the
English LSA model is set to also correspond to pol-
itics and so forth. During LM adaptation, we first
infer the topic mixture weights from the source text
using the source LSA model. Then we transfer the
inferred mixture weights to the target LSA model
and thus obtain the target LSA marginals. The chal-
lenge is to enforce the one-to-one topic correspon-
dence. Our proposal is to share common variational
Dirichlet posteriors over the topic mixture weights
of a document pair in the LDA-style model. The
beauty of the bLSA framework is that the model
searches for a common latent topic space in an un-
supervised fashion, rather than to require manual in-
teraction. Since the topic space is language indepen-
dent, our approach supports topic transfer in multi-
ple language pairs in O(N) where N is the number of
languages.
Related work includes the Bilingual Topic Ad-
mixture Model (BiTAM) for word alignment pro-
posed by (Zhao and Xing, 2006). Basically, the
BiTAM model consists of topic-dependent transla-
tion lexicons modeling Pr(c|e, k) where c, e and
k denotes the source Chinese word, target English
word and the topic index respectively. On the
other hand, the bLSA framework models Pr(c|k)
and Pr(e|k) which is different from the BiTAM
model. By their different modeling nature, the bLSA
model usually supports more topics than the BiTAM
model. Another work by (Kim and Khudanpur,
2004) employed crosslingual LSA using singular
value decomposition which concatenates bilingual
documents into a single input supervector before
projection.
We organize the paper as follows: In Section 2,
we introduce the bLSA framework including La-
tent Dirichlet-Tree Allocation (LDTA) (Tam and
Schultz, 2007) as a correlated LSA model, bLSA
training and crosslingual LM adaptation. In Sec-
tion 3, we present the effect of LM adaptation on
word perplexity, followed by SMT experiments re-
ported in BLEU on both speech and text input in
Section 3.3. Section 4 describes conclusions and fu-
ASR hypo
Chinese LSA English LSA
Chinese N?gram LM English N?gram LM
Chinese ASR Chinese?>English SMT
Chinese?English
Adapt Adapt
MT hypoTopic distribution
Parallel document corpus
Chinese text English text
Figure 1: Topic transfer in bilingual LSA model.
ture works.
2 Bilingual Latent Semantic Analysis
The goal of a bLSA model is to enforce a one-
to-one topic correspondence between monolingual
LSA models, each of which can be modeled using
an LDA-style model. The role of the bLSA model
is to transfer the inferred latent topic distribution
from the source language to the target language as-
suming that the topic distributions on both sides are
identical. The assumption is reasonable for parallel
document pairs which are faithful translations. Fig-
ure 1 illustrates the idea of topic transfer between
monolingual LSA models followed by LM adapta-
tion. One observation is that the topic transfer can be
bi-directional meaning that the ?flow? of topic can
be from ASR to SMT or vice versa. In this paper,
we only focus on ASR-to-SMT direction. Our tar-
get is to minimize the word perplexity on the target
language through LM adaptation. Before we intro-
duce the heuristic of enforcing a one-to-one topic
correspondence, we describe the Latent Dirichlet-
Tree Allocation (LDTA) for LSA.
2.1 Latent Dirichlet-Tree Allocation
The LDTA model extends the LDA model in which
correlation among latent topics are captured using a
Dirichlet-Tree prior. Figure 2 illustrates a depth-two
Dirichlet-Tree. A tree of depth one simply falls back
to the LDA model. The LDTA model is a generative
model with the following generative process:
1. Sample a vector of branch probabilities bj ?
521
Dir(.)Dir(.) Dir(.)
Dir(.)
topic 1 topic 4Latent topics topic K
j=1
j=2 j=3
Figure 2: Dirichlet-Tree prior of depth two.
Dir(?j) for each node j = 1...J where ?j de-
notes the parameter (aka the pseudo-counts of
its outgoing branches) of the Dirichlet distribu-
tion at node j.
2. Compute the topic proportions as:
?k =
?
jc
b?jc(k)jc (1)
where ?jc(k) is an indicator function which sets
to unity when the c-th branch of the j-th node
leads to the leaf node of topic k and zero other-
wise. The k-th topic proportion ?k is computed
as the product of branch probabilities from the
root node to the leaf node of topic k.
3. Generate a document using the topic multino-
mial for each word wi:
zi ? Mult(?)
wi ? Mult(?.zi)
where ?.zi denotes the topic-dependent uni-
gram LM indexed by zi.
The joint distribution of the latent variables (topic
sequence zn1 and the Dirichlet nodes over child
branches bj) and an observed document wn1 can be
written as follows:
p(wn1 , zn1 , bJ1 ) = p(bJ1 |{?j})
n
?
i
?wizi ? ?zi
where p(bJ1 |{?j}) =
J
?
j
Dir(bj;?j)
?
?
jc
b?jc?1jc
Similar to LDA training, we apply the variational
Bayes approach by optimizing the lower bound of
the marginalized document likelihood:
L(wn1 ; ?,?)=Eq[log
p(wn1 , zn1 , bJ1 ; ?)
q(zn1 , bJ1 ; ?)
]
=Eq[log p(wn1 |zn1 )] + Eq[log
p(zn1 |bJ1 )
q(zn1 )
]
+Eq[log
p(bJ1 ; {?j})
q(bJ1 ; {?j})
]
where q(zn1 , bJ1 ; ?) =
?n
i q(zi) ?
?J
j q(bj) is a fac-
torizable variational posterior distribution over the
latent variables parameterized by ? which are deter-
mined in the E-step. ? is the model parameters for
a Dirichlet-Tree {?j} and the topic-dependent uni-
gram LM {?wk}. The LDTA model has an E-step
similar to the LDA model:
E-Step:
?jc = ?jc +
n
?
i
K
?
k
qik ? ?jc(k) (2)
qik ? ?wik ? eEq[log ?k] (3)
where
Eq[log ?k] =
?
jc
?jc(k)Eq[log bjc]
=
?
jc
?jc(k)
(
?(?jc)??(
?
c
?jc)
)
where qik denotes q(zi = k) meaning the variational
topic posterior of word wi. Eqn 2 and Eqn 3 are
executed iteratively until convergence is reached.
M-Step:
?wk ?
n
?
i
qik ? ?(wi, w) (4)
where ?(wi, w) is a Kronecker Delta function. The
alpha parameters can be estimated with iterative
methods such as Newton-Raphson or simple gradi-
ent ascent procedure.
2.2 Bilingual LSA training
For the following explanations, we assume that our
source and target languages are Chinese and En-
glish respectively. The bLSA model training is a
522
two-stage procedure. At the first stage, we train
a Chinese LSA model using the Chinese docu-
ments in parallel corpora. We applied the varia-
tional EM algorithm (Eqn 2?4) to train a Chinese
LSA model. Then we used the model to compute
the term eEq[log ?k] needed in Eqn 3 for each Chinese
document in parallel corpora. At the second stage,
we apply the same eEq [log ?k] to bootstrap an English
LSA model, which is the key to enforce a one-to-one
topic correspondence. Now the hyper-parameters of
the variational Dirichlet posteriors of each node in
the Dirichlet-Tree are shared among the Chinese and
English model. Precisely, we apply only Eqn 3 with
fixed eEq [log ?k] in the E-step and Eqn 4 in the M-step
on {?wk} to bootstrap an English LSA model. No-
tice that the E-step is non-iterative resulting in rapid
LSA training. In short, given a monolingual LSA
model, we can rapidly bootstrap LSA models of new
languages using parallel document corpora. Notice
that the English and Chinese vocabulary sizes do not
need to be similar. In our setup, the Chinese vo-
cabulary comes from the ASR system while the En-
glish vocabulary comes from the English part of the
parallel corpora. Since the topic transfer can be bi-
directional, we can perform the bLSA training in a
reverse manner, i.e. training an English LSA model
followed by bootstrapping a Chinese LSA model.
2.3 Crosslingual LM adaptation
Given a source text, we apply the E-step to estimate
variational Dirichlet posterior of each node in the
Dirichlet-Tree. We estimate the topic weights on the
source language using the following equation:
??(CH)k ?
?
jc
(
?jc
?
c? ?jc?
)?jc(k)
(5)
Then we apply the topic weights into the target LSA
model to obtain an in-domain LSA marginals:
PrEN (w) =
K
?
k=1
?(EN)wk ? ??
(CH)
k (6)
We integrate the LSA marginal into the target back-
ground LM using marginal adaptation (Kneser et al,
1997) which minimizes the Kullback-Leibler diver-
gence between the adapted LM and the background
LM:
Pra(w|h) ?
(
Prldta(w)
Prbg(w)
)?
? Prbg(w|h) (7)
Likewise, LM adaptation can take place on the
source language as well due to the bi-directional na-
ture of the bLSA framework when target-side adap-
tation text is available. In this paper, we focus on
LM adaptation on the target language for SMT.
3 Experimental Setup
We evaluated our bLSA model using the Chinese?
English parallel document corpora consisting of the
Xinhua news, Hong Kong news and Sina news. The
combined corpora contains 67k parallel documents
with 35M Chinese (CH) words and 43M English
(EN) words. Our spoken language translation sys-
tem translates from Chinese to English. The Chinese
vocabulary comes from the ASR decoder while the
English vocabulary is derived from the English por-
tion of the parallel training corpora. The vocabulary
sizes for Chinese and English are 108k and 69k re-
spectively. Our background English LM is a 4-gram
LM trained with the modified Kneser-Ney smooth-
ing scheme using the SRILM toolkit on the same
training text. We explore the bLSA training in both
directions: EN?CH and CH?EN meaning that an
English LSA model is trained first and a Chinese
LSA model is bootstrapped or vice versa. Exper-
iments explore which bootstrapping direction yield
best results measured in terms of English word per-
plexity. The number of latent topics is set to 200 and
a balanced binary Dirichlet-Tree prior is used.
With an increasing interest in the ASR-SMT cou-
pling for spoken language translation, we also eval-
uated our approach with Chinese ASR hypotheses
and compared with Chinese manual transcriptions.
We are interested to see the impact due to recog-
nition errors on the ASR hypotheses compared to
the manual transcriptions. We employed the CMU-
InterACT ASR system developed for the GALE
2006 evaluation. We trained acoustic models with
over 500 hours of quickly transcribed speech data re-
leased by the GALE program and the LM with over
800M-word Chinese corpora. The character error
rates on the CCTV, RFA and NTDTV shows in the
RT04 test set are 7.4%, 25.5% and 13.1% respec-
tively.
523
Topic index Top words
?CH-40? flying, submarine, aircraft, air, pilot, land, mission, brand-new
?EN-40? air, sea, submarine, aircraft, flight, flying, ship, test
?CH-41? satellite, han-tian, launch, space, china, technology, astronomy
?EN-41? space, satellite, china, technology, satellites, science
?CH-42? fire, airport, services, marine, accident, air
?EN-42? fire, airport, services, department, marine, air, service
Table 1: Parallel topics extracted by the bLSA
model. Top words on the Chinese side are translated
into English for illustration purpose.
-3.05e+08
-3e+08
-2.95e+08
-2.9e+08
-2.85e+08
-2.8e+08
-2.75e+08
-2.7e+08
 2  4  6  8  10  12  14  16  18  20
Tr
ain
ing
 lo
g 
lik
eli
ho
od
# of training iterations
bootstrapped EN LSA
monolingual EN LSA
Figure 3: Comparison of training log likelihood of
English LSA models bootstrapped from a Chinese
LSA and from a flat monolingual English LSA.
3.1 Analysis of the bLSA model
By examining the top-words of the extracted paral-
lel topics, we verify the validity of the heuristic de-
scribed in Section 2.2 which enforces a one-to-one
topic correspondence in the bLSA model. Table 1
shows the latent topics extracted by the CH?EN
bLSA model. We can see that the Chinese-English
topic words have strong correlations. Many of them
are actually translation pairs with similar word rank-
ings. From this viewpoint, we can interpret bLSA as
a crosslingual word trigger model. The result indi-
cates that our heuristic is effective to extract parallel
latent topics. As a sanity check, we also examine the
likelihood of the training data when an English LSA
model is bootstrapped. We can see from Figure 3
that the likelihood increases monotonically with the
number of training iterations. The figure also shows
that by sharing the variational Dirichlet posteriors
from the Chinese LSA model, we can bootstrap an
English LSA model rapidly compared to monolin-
gual English LSA training with both training proce-
dures started from the same flat model.
LM (43M) CCTV RFA NTDTV
BG EN unigram 1065 1220 1549
+CH?EN (CH ref) 755 880 1113
+EN?CH (CH ref) 762 896 1111
+CH?EN (CH hypo) 757 885 1126
+EN?CH (CH hypo) 766 896 1129
+CH?EN (EN ref) 731 838 1075
+EN?CH (EN ref) 747 848 1087
Table 2: English word perplexity (PPL) on the RT04
test set using a unigram LM.
3.2 LM adaptation results
We trained the bLSA models on both CH?EN and
EN?CH directions and compared their LM adapta-
tion performance using the Chinese ASR hypothe-
ses (hypo) and the manual transcriptions (ref) as in-
put. We adapted the English background LM using
the LSA marginals described in Section 2.3 for each
show on the test set.
We first evaluated the English word perplexity us-
ing the EN unigram LM generated by the bLSA
model. Table 2 shows that the bLSA-based LM
adaptation reduces the word perplexity by over 27%
relative compared to an unadapted EN unigram LM.
The results indicate that the bLSA model success-
fully leverages the text from the source language and
improves the word perplexity on the target language.
We observe that there is almost no performance dif-
ference when either the ASR hypotheses or the man-
ual transcriptions are used for adaptation. The result
is encouraging since the bLSA model may be in-
sensitive to moderate recognition errors through the
projection of the input adaptation text into the latent
topic space. We also apply an English translation
reference for adaptation to show an oracle perfor-
mance. The results using the Chinese hypotheses are
not too far off from the oracle performance. Another
observation is that the CH?EN bLSA model seems
to give better performance than the EN?CH bLSA
model. However, their differences are not signifi-
cant. The result may imply that the direction of the
bLSA training is not important since the latent topic
space captured by either language is similar when
parallel training corpora are used. Table 3 shows the
word perplexity when the background 4-gram En-
glish LM is adapted with the tuning parameter ? set
524
LM (43M, ? = 0.7) CCTV RFA NTDTV
BG EN 4-gram 118 212 203
+CH?EN (CH ref) 102 191 179
+EN?CH (CH ref) 102 198 179
+CH?EN (CH hypo) 102 193 180
+EN?CH (CH hypo) 103 198 180
+CH?EN (EN ref) 100 186 176
+EN?CH (EN ref) 101 190 176
Table 3: English word perplexity (PPL) on the RT04
test set using a 4-gram LM.
 100
 105
 110
 115
 120
 125
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
En
gli
sh
 W
or
d 
Pe
rp
lex
ity
Beta
CCTV (CER=7.4%)
BG 4-gram
+bLSA (CH reference)
+bLSA (CH ASR hypo)
+bLSA (EN reference)
Figure 4: Word perplexity with different ? using
manual reference or ASR hypotheses on CCTV.
to 0.7. Figure 4 shows the change of perplexity with
different ?. We see that the adaptation performance
using the ASR hypotheses or the manual transcrip-
tions are almost identical on different ? with an op-
timal value at around 0.7. The results show that the
proposed approach successfully reduces the perplex-
ity in the range of 9?13.6% relative compared to an
unadapted baseline on different shows when ASR
hypotheses are used. Moreover, we observe simi-
lar performance using ASR hypotheses or manual
Chinese transcriptions which is consistent with the
results on Table 2. On the other hand, it is interest-
ing to see that the performance gap from the oracle
adaptation is somewhat related to the degree of mis-
match between the test show and the training condi-
tion. The gap looks wider on the RFA and NTDTV
shows compared to the CCTV show.
3.3 Incorporating bLSA into Spoken Language
Translation
To investigate the effectiveness of bLSA LM adap-
tation for spoken language translation, we incorpo-
rated the proposed approach into our state-of-the-art
phrase-based SMT system. Translation performance
was evaluated on the RT04 broadcast news evalua-
tion set when applied to both the manual transcrip-
tions and 1-best ASR hypotheses. During evalua-
tion two performance metrics, BLEU (Papineni et
al., 2002) and NIST, were computed. In both cases, a
single English reference was used during scoring. In
the transcription case the original English references
were used. For the ASR case, as utterance segmen-
tation was performed automatically, the number of
sentences generated by ASR and SMT differed from
the number of English references. In this case, Lev-
enshtein alignment was used to align the translation
output to the English references before scoring.
3.4 Baseline SMT Setup
The baseline SMT system consisted of a non adap-
tive system trained using the same Chinese-English
parallel document corpora used in the previous ex-
periments (Sections 3.1 and 3.2). For phrase extrac-
tion a cleaned subset of these corpora, consisting of
1M Chinese-English sentence pairs, was used. SMT
decoding parameters were optimized using man-
ual transcriptions and translations of 272 utterances
from the RT04 development set (LDC2006E10).
SMT translation was performed in two stages us-
ing an approach similar to that in (Vogel, 2003).
First, a translation lattice was constructed by match-
ing all possible bilingual phrase-pairs, extracted
from the training corpora, to the input sentence.
Phrase extraction was performed using the ?PESA?
(Phrase Pair Extraction as Sentence Splitting) ap-
proach described in (Vogel, 2005). Next, a search
was performed to find the best path through the lat-
tice, i.e. that with maximum translation-score. Dur-
ing search reordering was allowed on the target lan-
guage side. The final translation result was that
hypothesis with maximum translation-score, which
is a log-linear combination of 10 scores consist-
ing of Target LM probability, Distortion Penalty,
Word-Count Penalty, Phrase-Count and six Phrase-
Alignment scores. Weights for each component
score were optimized to maximize BLEU-score on
the development set using MER optimization as de-
scribed in (Venugopal et al, 2005).
525
Translation Quality - BLEU (NIST)
SMT Target LM CCTV RFA NTDTV ALL
Manual Transcription
Baseline LM: 0.162 (5.212) 0.087 (3.854) 0.140 (4.859) 0.132 (5.146)
bLSA (bLSA-Adapted LM): 0.164 (5.212) 0.087 (3.897) 0.143 (4.864) 0.134 (5.162)
1-best ASR Output
CER (%) 7.4 25.5 13.1 14.9
Baseline LM: 0.129 (4.15) 0.051 (2.77) 0.086 (3.50) 0.095 (3.90)
bLSA (bLSA-Adapted LM): 0.132 (4.16) 0.050 (2.79) 0.089 (3.53) 0.096 (3.91)
Table 4: Translation performance of baseline and bLSA-Adapted Chinese-English SMT systems on manual
transcriptions and 1-best ASR hypotheses
3.5 Performance of Baseline SMT System
First, the baseline system performance was evalu-
ated by applying the system described above to the
reference transcriptions and 1-best ASR hypotheses
generated by our Mandarin speech recognition sys-
tem. The translation accuracy in terms of BLEU and
NIST for each individual show (?CCTV?, ?RFA?,
and ?NTDTV?), and for the complete test-set, are
shown in Table 4 (Baseline LM). When applied to
the reference transcriptions an overall BLEU score
of 0.132 was obtained. BLEU-scores ranged be-
tween 0.087 and 0.162 for the ?RFA?, ?NTDTV? and
?CCTV? shows, respectively. As the ?RFA? show
contained a large segment of conversational speech,
translation quality was considerably lower for this
show due to genre mismatch with the training cor-
pora of newspaper text.
For the 1-best ASR hypotheses, an overall BLEU
score of 0.095 was achieved. For the ASR case,
the relative reduction in BLEU scores for the RFA
and NTDTV shows is large, due to the significantly
lower recognition accuracies for these shows. BLEU
score is also degraded due to poor alignment of ref-
erences during scoring.
3.6 Incorporation of bLSA Adaptation
Next, the effectiveness of bLSA based LM adapta-
tion was evaluated. For each show the target En-
glish LM was adapted using bLSA-adaptation, as
described in Section 2.3. SMT was then applied us-
ing an identical setup to that used in the baseline ex-
periments.
The translation accuracy when bLSA adaptation
was incorporated is shown in Table 4. When ap-
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
CCTV RFA NTDTV All shows
BLE
U
Baseline-LM bLSA Adapted LM
Figure 5: BLEU score for those 25% utterances
which resulted in different translations after bLSA
adaptation (manual transcriptions)
plied to the manual transcriptions, bLSA adaptation
improved the overall BLEU-score by 1.7% relative
(from 0.132 to 0.134). For all three shows bLSA
adaptation gained higher BLEU and NIST metrics.
A similar trend was also observed when the pro-
posed approach was applied to the 1-best ASR out-
put. On the evaluation set a relative improvement in
BLEU score of 1.0% was gained.
The semantic interpretation of the majority of ut-
terances in broadcast news are not affected by topic
context. In the experimental evaluation it was ob-
served that only 25% of utterances produced differ-
ent translation output when bLSA adaptation was
performed compared to the topic-independent base-
line. Although the improvement in translation qual-
ity (BLEU) was small when evaluated over the en-
tire test set, the improvement in BLEU score for
526
these 25% utterances was significant. The trans-
lation quality for the baseline and bLSA-adaptive
system when evaluated only on these utterances is
shown in Figure 5 for the manual transcription case.
On this subset of utterances an overall improvement
in BLEU of 0.007 (5.7% relative) was gained, with
a gain of 0.012 (10.6% relative) points for the ?NT-
DTV? show. A similar trend was observed when ap-
plied to the 1-best ASR output. In this case a rel-
ative improvement in BLEU of 12.6% was gained
for ?NTDTV?, and for ?All shows? 0.007 (3.7%)
was gained. Current evaluation metrics for trans-
lation, such as ?BLEU?, do not consider the rela-
tive importance of specific words or phrases during
translation and thus are unable to highlight the true
effectiveness of the proposed approach. In future
work, we intend to investigate other evaluation met-
rics which consider the relative informational con-
tent of words.
4 Conclusions
We proposed a bilingual latent semantic model
for crosslingual LM adaptation in spoken language
translation. The bLSA model consists of a set of
monolingual LSA models in which a one-to-one
topic correspondence is enforced between the LSA
models through the sharing of variational Dirich-
let posteriors. Bootstrapping a LSA model for a
new language can be performed rapidly with topic
transfer from a well-trained LSA model of another
language. We transfer the inferred topic distribu-
tion from the input source text to the target lan-
guage effectively to obtain an in-domain target LSA
marginals for LM adaptation. Results showed that
our approach significantly reduces the word per-
plexity on the target language in both cases using
ASR hypotheses and manual transcripts. Interest-
ingly, the adaptation performance is not much af-
fected when ASR hypotheses were used. We eval-
uated the adapted LM on SMT and found that the
evaluation metrics are crucial to reflect the actual
improvement in performance. Future directions in-
clude the exploration of story-dependent LM adap-
tation with automatic story segmentation instead of
show-dependent adaptation due to the possibility of
multiple stories within a show. We will investigate
the incorporation of monolingual documents for po-
tentially better bilingual LSA modeling.
Acknowledgment
This work is partly supported by the Defense Ad-
vanced Research Projects Agency (DARPA) under
Contract No. HR0011-06-2-0001. Any opinions,
findings and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
References
D. Blei, A. Ng, and M. Jordan. 2003. Latent Dirichlet
Allocation. In Journal of Machine Learning Research,
pages 1107?1135.
W. Kim and S. Khudanpur. 2003. LM adaptation using
cross-lingual information. In Proc. of Eurospeech.
W. Kim and S. Khudanpur. 2004. Cross-lingual latent
semantic analysis for LM. In Proc. of ICASSP.
R. Kneser, J. Peters, and D. Klakow. 1997. Language
model adaptation using dynamic marginals. In Proc.
of Eurospeech, pages 1971?1974.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proc. of ACL.
M. Paulik, C. Fu?gen, T. Schaaf, T. Schultz, S. Stu?ker, and
A. Waibel. 2005. Document driven machine transla-
tion enhanced automatic speech recognition. In Proc.
of Interspeech.
Y. C. Tam and T. Schultz. 2006. Unsupervised language
model adaptation using latent semantic marginals. In
Proc. of Interspeech.
Y. C. Tam and T. Schultz. 2007. Correlated latent seman-
tic model for unsupervised language model adaptation.
In Proc. of ICASSP.
A. Venugopal, A. Zollmann, and A. Waibel. 2005. Train-
ing and evaluation error minimization rules for statis-
tical machine translation. In Proc. of ACL.
S. Vogel. 2003. SMT decoder dissected: Word reorder-
ing. In Proc. of ICNLPKE.
S. Vogel. 2005. PESA: Phrase pair extraction as sentence
splitting. In Proc. of the Machine Translation Summit.
B. Zhao and E. P. Xing. 2006. BiTAM: Bilingual topic
admixture models for word alignment. In Proc. of
ACL.
527
IMPROVEMENTS IN NON-VERBAL CUE IDENTIFICATION USING MULTILINGUAL
PHONE STRINGS
Tanja Schultz, Qin Jin, Kornel Laskowski, Alicia Tribble, Alex Waibel
Interactive Systems Laboratories
Carnegie Mellon University
E-mail:
 
tanja,qjin,kornel,atribble,ahw  @cs.cmu.edu
1. INTRODUCTION
Today?s state-of-the-art front-ends for multilingual speech-
to-speech translation systems apply monolingual speech
recognizers trained for a single language and/or accent.
The monolingual speech engine is usually adaptable to an
unknown speaker over time using unsupervised training
methods; however, if the speaker was seen during training,
their specialized acoustic model will be applied, since it
achieves better performance. In order to make full use of
specialized acoustic models in this proposed scenario, it is
necessary to automatically identify the speaker with high
accuracy. Furthermore, monolingual speech recognizers
currently rely on the fact that language and/or accent will
be selected beforehand by the user. This requires the user?s
cooperation and an interface which easily allows for such
selection. Both requirements are awkward and error-prone,
especially when translation services are provided for many
languages using small devices like PDAs or telephones. For
these scenarios, front-ends are desired which automatically
identify the spoken language or accent. We believe that
the automatic identification of an utterance?s non-verbal
cues, such as language, accent and speaker, are necessary to
the successful deployment of speech-to-speech translation
systems.
Currently, approaches based on Gaussian Mixture Models
(GMMs) [1] are the most widely and successfully used
methods for speaker identification. Although GMMs have
been applied successfully to close-speaking microphone
scenarios under matched training and testing conditions,
their performance degrades dramatically under mismatched
conditions. For language and accent identification, phone
recognition together with phone N-gram modeling has been
the most successful approach in the past [2]. More recently,
Kohler introduced an approach for speaker recognition
where a phonotactic N-gram model is used [3].
In [4], we extended Kohler?s approach to accent and lan-
guage identification as well as to speaker identification un-
der mismatched conditions. The term ?mismatched condi-
tion? describes a situation in which the testing conditions,
e.g. microphone distance, are quite different from what had
been seen during training. In that work, we explored a com-
mon framework for the identification of language, accent
and speaker using multilingual phone strings produced by
phone recognizers trained on data from different languages.
In this paper, we propose and evaluate some improvements,
comparing classification accuracy as well as realtime per-
formance in our framework. Furthermore, we investigate
the benefits that are to be drawn from additional phone rec-
ognizers.
2. THE MULTILINGUAL PHONE STRING
APPROACH
The basic idea of the multilingual phone string approach
is to use phone strings produced by different context-
independent phone recognizers instead of traditional
short-term acoustic vectors [6]. For the classification of an
audio segment into one of  classes of a specific non-verbal
cue,  such phone recognizers together with 
phonotactic N-gram models produce an  matrix of
features. A best class estimate is made based solely on this
feature matrix. The process relies on the availability of
 phone recognizers, and the training of  N-gram
models on their output.
By using information derived from phonotactics rather than
directly from acoustics, we expect to cover speaker idiosyn-
crasy and accent-specific pronunciations. Since this infor-
mation is provided from complementary phone recognizers,
we anticipate greater robustness under mismatched condi-
tions. Furthermore, the approach is somewhat language in-
dependent since the recognizers are trained on data from
different languages.
2.1. Phone Recognition
The experiments presented here were conducted using
two versions of phone recognizers borrowed without
modification from the GlobalPhone project [5]. All were
                                            Association for Computational Linguistics.
                         Algorithms and Systems, Philadelphia, July 2002, pp. 101-108.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
trained using our Janus Recognition Toolkit (JRTk).
25
30
35
40
45
50
25 30 35 40 45
Ph
on
em
e 
Er
ro
r R
at
e 
[%
]
Number of Phonemes
CHDE
FR
JA
KR
PO
SPTU
50
Fig. 1. Error rate vs number of phones for the baseline
GlobalPhone phone recognizer set
The first set of phone recognizers, which we refer to as
our baseline, includes recognizers for: Mandarin Chinese
(CH), German (DE), French (FR), Japanese (JA), Croatian
(KR), Portuguese (PO), Spanish (SP) and Turkish (TU).
For each language, the acoustic model consists of a context-
independent 3-state HMM system with 128 Gaussians per
state. The Gaussians are on 13 Mel-scale cepstral coeffi-
cients with first and second order derivatives and power.
Following cepstral mean subtraction, linear discriminant
analysis reduces the input vector to 32 dimensions.
The second set consists of extended phone recognizers,
available in 12 languages. Arabic (AR), Korean (KO),
Russian (RU) and Swedish (SW) are available in this set
in addition to the languages named above for the baseline
set. The 12 new phone recognizers were derived from
an improved generation of context dependent LVCSR
systems which also include vocal tract normalization
(VTLN) for speaker normalization. For decoding, we
used an unsupervised scheme to find the best warp fac-
tor for a test speaker and calculate a viterbi alignment
based on that speaker?s best warp factor. To improve
system speed, we reduced the number of Gaussians per
state from 128 to 16; in addition, the feature dimension
was halved from 32 to 16 using linear discriminant analysis.
Figure 1 shows the phone error rates in relation to the num-
ber of modeled phones for eight languages. The error rate
correlates with the number of phones used to model this lan-
guage. Turkish seems to be an exception to this finding. The
error analysis showed that this is due to a very high substi-
audio
phone string
phone string
	

	

 
	 
  
  
  

Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 450?458,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Joint Learning of Preposition Senses and
Semantic Roles of Prepositional Phrases
Daniel Dahlmeier
1
, Hwee Tou Ng
1,2
, Tanja Schultz
3
1
NUS Graduate School for Integrative Sciences and Engineering
2
Department of Computer Science, National University of Singapore
3
Cognitive Systems Lab, University of Karlsruhe
{danielhe,nght}@comp.nus.edu.sg
tanja@ira.uka.de
Abstract
The sense of a preposition is related to the
semantics of its dominating prepositional
phrase. Knowing the sense of a prepo-
sition could help to correctly classify the
semantic role of the dominating preposi-
tional phrase and vice versa. In this pa-
per, we propose a joint probabilistic model
for word sense disambiguation of preposi-
tions and semantic role labeling of prepo-
sitional phrases. Our experiments on the
PropBank corpus show that jointly learn-
ing the word sense and the semantic role
leads to an improvement over state-of-the-
art individual classifier models on the two
tasks.
1 Introduction
Word sense disambiguation (WSD) and seman-
tic role labeling (SRL) are two key components
in natural language processing to find a semantic
representation for a sentence. Semantic role la-
beling is the task of determining the constituents
of a sentence that represent semantic arguments
with respect to a predicate and labeling each with
a semantic role. Word sense disambiguation tries
to determine the correct meaning of a word in a
given context. Ambiguous words occur frequently
in normal English text.
One word class which is both frequent and
highly ambiguous is preposition. The different
senses of a preposition express different relations
between the preposition complement and the rest
of the sentence. Semantic roles and word senses
offer two different inventories of ?meaning? for
prepositional phrases (PP): semantic roles distin-
guish between different verb complements while
word senses intend to fully capture the preposition
semantics at a more fine-grained level. In this pa-
per, we use the semantic roles from the PropBank
corpus and the preposition senses from the Prepo-
sition Project (TPP). Both corpora are explained
in more detail in the following section. The re-
lationship between the two inventories (PropBank
semantic roles and TPP preposition senses) is not
a simple one-to-one mapping, as we can see from
the following examples:
? She now lives with relatives [in
sense1
Alabama.]
ARGM-LOC
? The envelope arrives [in
sense1
the mail.]
ARG4
? [In
sense5
separate statements]
ARGM-LOC
the two
sides said they want to have ?further discus-
sions.?
In the first two examples, the sense of the preposi-
tion in is annotated as sense 1 (?surrounded by or
enclosed in?), following the definitions of the TPP,
but the semantic roles are different. In the first
example the semantic role is a locative adjunctive
argument (ARGM-LOC), while in the second ex-
ample it is ARG4 which denotes the ?end point or
destination? of the arriving action
1
. In the first and
third example, the semantic roles are the same, but
the preposition senses are different, i.e., sense 1
and sense 5 (?inclusion or involvement?).
Preposition senses and semantic roles provide
two different views on the semantics of PPs.
Knowing the semantic role of the PP could be
helpful to successfully disambiguate the sense of
the preposition. Likewise, the preposition sense
could provide valuable information to classify the
semantic role of the PP. This is especially so for
the semantic roles ARGM-LOC and ARGM-TMP,
where we expect a strong correlation with spatial
and temporal preposition senses respectively.
In this paper, we propose a probabilistic model
for joint inference on preposition senses and se-
mantic roles. For each prepositional phrase that
1
http://verbs.colorado.edu/framesets/arrive-v.html
450
has been identified as an argument of the pred-
icate, we jointly infer its semantic role and the
sense of the preposition that is the lexical head of
the prepositional phrase. That is, our model maxi-
mizes the joint probability of the semantic role and
the preposition sense.
Previous research has shown the benefit of
jointly learning semantic roles of multiple con-
stituents (Toutanova et al, 2008; Koomen et al,
2005). In contrast, our joint model makes pre-
dictions for a single constituent, but multiple tasks
(WSD and SRL) .
Our experiments show that adding the SRL
information leads to statistically significant im-
provements over an independent, state-of-the-art
WSD classifier. For the SRL task, we show statis-
tically significant improvements of our joint model
over an independent, state-of-the-art SRL clas-
sifier for locative and temporal adjunctive argu-
ments, even though the overall improvement over
all semantic roles is small. To the best of our
knowledge, no previous research has attempted to
perform preposition WSD and SRL of preposi-
tional phrases in a joint learning approach.
The remainder of this paper is structured as fol-
lows: First, we give an introduction to the WSD
and SRL task. Then, in Section 3, we describe the
individual and joint classifier models. The details
of the data set used in our experiments are given
in Section 4. In Section 5, we present experiments
and results. Section 6 summarizes related work,
before we conclude in the final section.
2 Task Description
This section gives an introduction to preposition
sense disambiguation and semantic role labeling
of prepositional phrases.
2.1 Preposition Sense Disambiguation
The task of word sense disambiguation is to find
the correct meaning of a word, given its context.
Most prior research on word sense disambigua-
tion has focused on disambiguating the senses of
nouns, verbs, and adjectives, but not on preposi-
tions. Word sense disambiguation can be framed
as a classification task. For each preposition, a
classifier is trained on a corpus of training exam-
ples annotated with preposition senses, and tested
on a set of unseen test examples.
To perform WSD for prepositions, it is neces-
sary to first find a set of suitable sense classes.
We adopt the sense inventory from the Preposition
Project (TPP) (Litkowski and Hargraves, 2005)
that was also used in the SemEval 2007 preposi-
tion WSD task (Litkowski and Hargraves, 2007).
TPP is an attempt to create a comprehensive lex-
ical database of English prepositions that is suit-
able for use in computational linguistics research.
For each of the over 300 prepositions and phrasal
prepositions, the database contains a set of sense
definitions, which are based on the Oxford Dic-
tionary of English. Every preposition has a set
of fine-grained senses, which are grouped together
into a smaller number of coarse-grained senses. In
our experiments, we only focus on coarse-grained
senses since better inter-annotator agreement can
be achieved on coarse-grained senses, which also
results in higher accuracy of the trainedWSD clas-
sifier.
2.2 Semantic Role Labeling
The task of semantic role labeling in the context
of PropBank (Palmer et al, 2005) is to label tree
nodes with semantic roles in a syntactic parse tree.
The PropBank corpus adds a semantic layer to
parse trees from the Wall Street Journal section of
the Penn Treebank II corpus (Marcus et al, 1993).
There are two classes of semantic roles: core argu-
ments and adjunctive arguments. Core arguments
are verb sense specific, i.e., their meaning is de-
fined relative to a specific verb sense. They are
labeled with consecutive numbers ARG0, ARG1,
etc. ARG0 usually denotes the AGENT and ARG1
the THEME of the event. Besides the core ar-
guments, a verb can have a number of adjunc-
tive arguments that express more general proper-
ties like time, location, or manner. They are la-
beled as ARGM plus a functional tag, e.g., LOC for
locative or TMP for temporal modifiers. Preposi-
tional phrases can appear as adjunctive arguments
or core arguments.
The standard approach to semantic role labeling
is to divide the task into two sequential sub-tasks:
identification and classification. During the identi-
fication phase, the system separates the nodes that
fill some semantic roles from the rest. During the
classification phase, the system assigns the exact
semantic roles for all nodes that are identified as
arguments. In this paper, we focus on the classi-
fication phase. That is, we assume that preposi-
tional phrases that are semantic arguments have
been identified correctly and concentrate on the
451
task of determining the semantic role of preposi-
tional phrases. The reason is that argument identi-
fication mostly relies on syntactic features, like the
path from the constituent to the predicate (Pradhan
et al, 2005). Consider, for example, the phrase in
the dark in the sentence: ?We are in the dark?, he
said. The phrase is clearly not an argument to the
verb say. But if we alter the syntactic structure
of the sentence appropriately (while the sense of
the preposition in remains unchanged), the same
phrase suddenly becomes an adjunctive argument:
In the dark, he said ?We are?. On the other hand,
we can easily find examples, where in has a differ-
ent sense, but the phrase always fills some seman-
tic role:
? In a separate manner, he said . . .
? In 1998, he said . . .
? In Washington, he said . . .
This illustrates that the preposition sense is inde-
pendent of whether the PP is an argument or not.
Thus, a joint learning model for argument identifi-
cation and preposition sense is unlikely to perform
better than the independent models.
3 Models
This section describes the models for preposition
sense disambiguation and semantic role labeling.
We compare three different models for each
task: First, we implement an independent model
that only uses task specific features from the liter-
ature. This serves as the baseline model. Second,
we extend the baseline model by adding the most
likely prediction of the other task as an additional
feature. This is equivalent to a pipeline model of
classifiers that feeds the prediction of one classifi-
cation step into the next stage. Finally, we present
a joint model to determine the preposition sense
and semantic role that maximize the joint proba-
bility.
3.1 WSD model
Our approach to building a preposition WSD clas-
sifier follows that of Lee and Ng (2002), who eval-
uated a set of different knowledge sources and
learning algorithms for WSD. However, in this pa-
per we use maximum entropy models
2
(instead of
support vector machines (SVM) reported in (Lee
2
Zhang Le?s Maximum Entropy Modeling Toolkit,
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
and Ng, 2002)), because maximum entropy mod-
els output probability distributions, unlike SVM.
This property is useful in the joint model, as we
will see later. Maxent models have been success-
fully applied to various NLP tasks and achieve
state-of-the-art performance. There are two train-
ing parameters that have to be adjusted for maxent
models: the number of training iterations and the
Gaussian smoothing parameter. We find optimal
values for both parameters through 10-fold cross-
validation on the training set.
For every preposition, a baseline maxent model
is trained using a set of features reported in
the state-of-the-art WSD system of Lee and
Ng (2002). These features encode three knowl-
edge sources:
? Part-of-speech (POS) of surrounding words
? Single words in the surrounding context
? Local collocations
For part-of-speech features, we include the POS
tags of surrounding tokens from the same sentence
within a window of seven tokens around the target
prepositions. All tokens (i.e., all words and punc-
tuation symbols) are considered. We use the Penn
Treebank II POS tag set.
For the knowledge source single words in the
surrounding context, we consider all words from
the same sentence. The input sentence is tokenized
and all tokens that do not contain at least one al-
phabetical character (such as punctuation symbols
and numbers) and all words that appear on a stop-
word list are removed. The remaining words are
converted to lower case and replaced by their mor-
phological root form. Every unique morphologi-
cal root word contributes one binary feature, in-
dicating whether or not the word is present in the
context. The position of a word in the sentence is
ignored in this knowledge source.
The third knowledge source, local collocations,
encodes position-specific information of words
within a small window around the target prepo-
sition. For this knowledge source, we consider
unigrams, bigrams, and trigrams from a window
of seven tokens. The position of the target prepo-
sition inside the n-gram is marked with a special
character ? ?. Words are converted to lower case,
but no stemming or removal of stopwords is per-
formed. If a token falls outside the sentence, it is
replaced by the empty token symbol nil.
During testing, the maxent model computes the
452
conditional probability of the sense, given the fea-
ture representation of the surrounding context c.
The classifier outputs the sense that receives the
highest probability:
s? = argmax
s
P (s|?(c)) (1)
where ?(?) is a feature map from the surrounding
context to the feature representation.
To ensure that our model is competitive, we
tested our system on the data set from the SemEval
2007 preposition WSD task (Litkowski and Har-
graves, 2007). Our baseline classifier achieved a
coarse-grained accuracy of 70.7% (micro-average)
on the official test set. This would have made our
system the second best system in the competition,
behind the MELB-YB system (Ye and Baldwin,
2007).
We also investigate the effect of the semantic
role label by adding it as a feature to the base-
line model. This pipeline model is inspired by the
work of Dang and Palmer (2005) who investigated
the role of SRL features in verb WSD. We add
the semantic role of the prepositional phrase dom-
inating the preposition as a feature to the WSD
model. During training, the PropBank gold SRL
label is used. During testing, we rely on the base-
line SRL model (to be introduced in the next sub-
section) to predict the semantic role of the prepo-
sitional phrase. This is equivalent to first per-
forming semantic role labeling and adding the out-
put as a feature to the WSD classifier. In ear-
lier experiments, we found that training on gold
SRL labels gave better results than training on
automatically predicted SRL labels (using cross-
validation). Note that our approach uses automati-
cally assigned SRL labels during testing, while the
system of Dang and Palmer (2005) only uses gold
SRL labels.
3.2 SRL model
Our semantic role labeling classifier is also based
on maxent models. It has been shown that max-
imum entropy models achieve state-of-the-art re-
sults on SRL (Xue and Palmer, 2004; Toutanova
et al, 2008). Again, we find optimal values
for the training parameters through 10-fold cross-
validation on the training set.
By treating SRL as a classification problem, the
choice of appropriate features becomes a key is-
sue. Features are encoded as binary-valued func-
tions. During testing, the maxent model computes
Baseline Features (Gildea and Jurafsky, 2002)
pred predicate lemma
path path from constituent to predicate
ptype syntactic category (NP, PP, etc.)
pos relative position to the predicate
voice active or passive voice
hw syntactic head word of the phrase
sub-cat rule expanding the predicate?s parent
Advanced Features (Pradhan et al, 2005)
hw POS POS of the syntactic head word
PP hw/POS head word and POS of the rightmost
NP child if the phrase is a PP
first/last word first/last word and POS in the con-
stituent
parent ptype syntactic category of the parent node
parent hw/POS head word and POS of the parent
sister ptype phrase type of left and right sister
sister hw/POS head word and POS of left and right
sister
temporal temporal key words present
partPath partial path predicate
proPath projected path without directions
Feature Combinations (Xue and Palmer, 2004)
pred & ptype predicate and phrase type
pred & hw predicate and head word
pred & path predicate and path
pred & pos predicate and relative position
Table 1: SRL features for the baseline model
the conditional probability P (a|t, p, v) of the ar-
gument label a, given the parse tree t, predicate p,
and constituent node v. The classifier outputs the
semantic role with the highest probability:
a? = argmax
a
P (a|t, p, v) (2)
= argmax
a
P (a|?(t, p, v)) (3)
where ?(?, ?, ?) is a feature map to an appropriate
feature representation.
For our baseline SRL model, we adopt the fea-
tures used in other state-of-the-art SRL systems,
which include the seven baseline features from the
original work of Gildea and Jurafsky (2002), addi-
tional features taken from Pradhan et al (2005),
and feature combinations which are inspired by
the system in Xue and Palmer (2004). Table 1 lists
the features we use for easy reference.
In the pipeline model, we investigate the use-
fulness of the preposition sense as a feature for
SRL by adding the preposition lemma concate-
nated with the sense number (e.g., on 1) as a fea-
ture. During training, the gold annotated prepo-
sition sense is used. During testing, the sense is
automatically tagged by the baseline WSD model.
This is equivalent to first running the WSD clas-
sifier for all prepositions, and adding the output
preposition sense as a feature to our baseline SRL
453
system.
3.3 Joint Inference Model
The two previous models seek to maximize the
probability of the semantic role and the preposi-
tion sense individually, thus ignoring possible de-
pendencies between the two. Instead of maximiz-
ing the individual probabilities, we would like to
maximize the joint probability of the semantic role
and the preposition sense, given the parse tree,
predicate, constituent node, and surrounding con-
text.
?
(a, s) = argmax
(a,s)
P (a, s|t, p, v, c) (4)
We assume that the probability of the semantic
role is already determined by the syntactic parse
tree t, the predicate p, and the constituent node v,
and is conditionally independent of the remaining
surrounding context c given t, p, and v. Likewise,
we assume that the probability of the preposition
sense is conditionally independent of the parse tree
t, predicate p, and constituent v, given the sur-
rounding context c and the semantic role a. This
assumption allows us to factor the joint probability
into an SRL and a WSD component:
?
(a, s) = argmax
(a,s)
P (a|t, p, v)?P (s|c, a) (5)
= argmax
(a,s)
P (a|?(t, p, v))?P (s|?(c, a))(6)
We observe that the first component in our joint
model corresponds to the baseline SRL model
and the second component corresponds to the
WSD pipeline model. Because our maxent mod-
els output a complete probability distribution, we
can combine both components by multiplying the
probabilities. Theoretically, the joint probability
could be factored in the other way, by first com-
puting the probability of the preposition sense and
then conditioning the SRL model on the predicted
preposition sense. However, in our early exper-
iments, we found that this approach gave lower
classification accuracy.
During testing, the classifier seeks to find the
tuple of semantic role and preposition sense that
maximizes the joint probability. For every se-
mantic role, the classifier computes its probability
given the SRL features, and multiplies it by the
probability of the most likely preposition sense,
given the context and the semantic role. The tu-
ple that receives the highest joint probability is the
final output of the joint classifier.
Semantic Role Total Training Test
ARG0 28 15 13
ARG1 374 208 166
ARG2 649 352 297
ARG3 111 67 44
ARG4 177 91 86
ARGM-ADV 141 101 40
ARGM-CAU 31 23 8
ARGM-DIR 28 19 9
ARGM-DIS 29 9 20
ARGM-EXT 61 42 19
ARGM-LOC 954 668 286
ARGM-MNR 316 225 91
ARGM-PNC 115 78 37
ARGM-PRD 1 1 0
ARGM-REC 1 0 1
ARGM-TMP 838 563 275
Total 3854 2462 1392
Table 2: Number of annotated prepositional
phrases for each semantic role
4 Data Set
The joint model uses the probability of a prepo-
sition sense, given the semantic role of the dom-
inating prepositional phrase. To estimate this
probability, we need a corpus which is annotated
with both preposition senses and semantic roles.
Unfortunately, PropBank is not annotated with
preposition senses. Instead, we manually anno-
tated the seven most frequent prepositions in four
sections of the PropBank corpus with their senses
from the TPP dictionary. According to Juraf-
sky and Martin (2008), the most frequent English
prepositions are: of, in, for, to, with, on and at (in
order of frequency). Our counts on Sections 2 to
21 of PropBank revealed that these top 7 prepo-
sitions account for about 65% of all prepositional
phrases that are labeled with semantic roles.
The annotation proceeds in the following way.
First, we automatically extract all sentences which
have one of the prepositions as the lexical head of
a prepositional phrase. The position of the prepo-
sition is marked in the sentence. By only consid-
ering prepositional phrases, we automatically ex-
clude occurrences of the word to before infinitives
and instances of particle usage of prepositions,
such as phrasal verbs. The extracted prepositions
are manually tagged with their senses from the
TPP dictionary. Idiomatic usage of prepositions
like for example or in fact, and complex preposi-
tion constructions that involve more than one word
(e.g., because of, instead of, etc.) are excluded by
the annotators and compiled into a stoplist.
We annotated 3854 instances of the top 7 prepo-
454
Preposition Total Training Test
at 404 260 144
for 478 307 171
in 1590 1083 507
of 97 51 46
on 408 246 162
to 532 304 228
with 345 211 134
Total 3854 2462 1392
Table 3: Number of annotated prepositional
phrases for each preposition
sitions in Sections 2 to 4 and 23 of the PropBank
corpus. The data shows a strong correlation be-
tween semantic roles and preposition senses that
express a spatial or temporal meaning. For the
preposition in, 90.8% of the instances that ap-
pear inside an ARGM-LOC are tagged with sense 1
(?surrounded by or enclosed in?) or sense 5 (?in-
clusion or involvement?). 94.6% of the instances
that appear inside an ARGM-TMP role are tagged
with sense 2 (?period of time?). Our counts fur-
thermore show that about one third of the anno-
tated prepositional phrases fill core roles and that
ARGM-LOC and ARGM-TMP are the most fre-
quent roles. The detailed breakdown of semantic
roles is shown in Table 2.
To see how consistent humans can perform the
annotation task, we computed the inter-annotator
agreement between two annotators on Section 4 of
the PropBank corpus. We found that the two anno-
tators assigned the same sense in 86% of the cases.
Although not directly comparable, it is interesting
to note that this figure is similar to inter-annotator
agreement for open-class words reported in previ-
ous work (Palmer et al, 2000). In our final data
set, all labels were tagged by the same annotator,
which we believe makes our annotation reason-
ably consistent across different instances. Because
we annotate running text, not all prepositions have
the same number of annotated instances. The
numbers for all seven prepositions are shown in
Table 3. In our experiments, we use Sections 2 to 4
to train the models, and Section 23 is kept for test-
ing. Although our experiments are limited to three
sections of training data, it still allows us to train
competitive SRL models. Pradhan et al (2005)
have shown that the benefit of using more training
data diminishes after a few thousand training in-
stances. We found that the accuracy of our SRL
baseline model, which is trained on the 5275 sen-
tences of these three sections, is only an absolute
Baseline
Pipeline
Joint
  30%
  40%
  50%
  60%
  70%
  80%
  90%
at for in of on to with total
Ac
cur
acy
Figure 1: Classification accuracy of the WSD
models for the seven most frequent prepositions
in test section 23
3.89% lower than the accuracy of the same model
when it is trained on twenty sections (71.71% ac-
curacy compared to 75.60% accuracy).
5 Experiments and Results
We evaluate the performance of the joint model on
the annotated prepositional phrases in test section
23 and compare the results with the performance
of the baseline models and the pipeline models.
Figure 1 shows the classification accuracy of the
WSD models for each of the seven prepositions in
the test section. The results show that the pipeline
model and the joint model perform almost equally,
with the joint model performing marginally better
in the overall score. The detailed scores are given
in Table 4. Both models outperform the baseline
classifier for three of the seven prepositions: at,
for, and to. For the prepositions in, of, and on, the
SRL feature did not affect the WSD classification
accuracy significantly. For the preposition with,
the classification accuracy even dropped by about
6%.
Performing the student?s t-test, we found that
the improvement for the prepositions at, for, and
to is statistical significant (p < 0.05), as is the
overall improvement. This confirms our hypoth-
esis that the semantic role of the prepositional
phrase is a strong hint for the preposition sense.
However, our results also show that it is the
SRL feature that brings the improvement, not the
joint model, because the pipeline and joint model
achieve about the same performance.
For the SRL task, we report the classification
accuracy over all annotated prepositional phrases
in the test section and the F
1
measure for the se-
mantic roles ARGM-LOC and ARGM-TMP. Fig-
455
Preposition Baseline Pipeline Joint
at 70.83 78.47
?
78.47
?
for 41.52 49.12
?
49.12
?
in 62.33 61.74 61.93
of 43.48 43.48 43.48
on 51.85 51.85 52.47
to 58.77 67.11
?
66.67
?
with 44.78 38.06 38.06
Total 56.54 58.76
?
58.84
?
Table 4: Classification accuracy of the baseline,
pipeline, and joint model on the WSD task in test
section 23, statistically significant improvements
over the baseline are marked with an (*)
Baseline
Pipeline
Joint
  65%
  70%
  75%
  80%
  85%
  90%
Argm?LOC Argm?TMP Overall
 
f1?
me
asu
re
Figure 2: F
1
measure of the SRL models for
ARGM-LOC and ARGM-TMP, and overall accu-
racy on prepositional phrases in test section 23
ure 2 shows the results. The joint model shows
a small performance increase of 0.43% over the
baseline in the overall accuracy. Adding the
preposition sense as a feature, on the other hand,
significantly lowers the accuracy by over 2%. For
ARGM-LOC and ARGM-TMP, the joint model im-
proves the F
1
measure by about 1.3% each. The
improvement of the joint model for these roles
is statistically significant (p ? 0.05, student?s t-
test). Simply adding the preposition sense in the
pipeline model again lowers the F
1
measure. The
detailed results are listed in Table 5.
Semantic Role Baseline Pipeline Joint
ARGM-LOC(F
1
) 72.88 71.54 74.27*
ARGM-TMP(F
1
) 81.87 79.43 83.24*
Overall(A) 71.71 69.47 72.14
Table 5: F
1
measure and accuracy of the baseline,
pipeline, and joint model on the SRL task in test
section 23, statistically significant improvements
over the baseline are marked with an (*)
Our SRL experiments show that a pipeline
model degrades the performance. The reason is
the relatively high degree of noise in the WSD
classification and that the pipeline model does not
discriminate whether the previous classifier pre-
dicts the extra feature with high or low confi-
dence. Instead, the model only passes on the 1-
best WSD prediction, which can cause the next
classifier to make a wrong classification based on
the erroneous prediction of the previous step. In
principle, this problem can be mitigated by train-
ing the pipeline model on automatically predicted
labels using cross-validation, but in our case we
found that automatically predicted WSD labels
decreased the performance of the pipeline model
even more. In contrast, the joint model computes
the full probability distribution over the semantic
roles and preposition senses. If the noise level in
the first classification step is low, the joint model
and the pipeline model perform almost identically,
as we have seen in the previousWSD experiments.
But if the noise level is high, the joint model can
still improve while the pipeline model drops in
performance. Our experiments show that the joint
model is more robust in the presence of noisy fea-
tures than the pipeline model.
6 Related Work
There is relatively less prior research on preposi-
tions and prepositional phrases in the NLP com-
munity. O?Hara and Wiebe (2003) proposed a
WSD system to disambiguate function tags of
prepositional phrases. An extended version of
their work was recently presented in (O?Hara and
Wiebe, 2009). Ye and Baldwin (2006) extended
their work to a semantic role tagger specifically
for prepositional phrases. Their system first classi-
fies the semantic roles of all prepositional phrases
and later merges the output with a general SRL
system. Ye and Baldwin (2007) used semantic
role tags from surrounding tokens as part of the
MELB-YB preposition WSD system. They found
that the SRL features did not significantly help
their classifier, which is different from our find-
ings. Dang and Palmer (2005) showed that se-
mantic role features are helpful to disambiguate
verb senses. Their approach is similar to our
pipeline WSD model, but they do not present re-
sults with automatically predicted semantic roles.
Toutanova et al (2008) presented a re-ranking
model to jointly learn the semantic roles of mul-
tiple constituents in the SRL task. Their work
dealt with joint learning in SRL, but it is not di-
rectly comparable to ours. The difference is that
456
Toutanova et al attempt to jointly learn semantic
role assignment of different constituents for one
task (SRL), while we attempt to jointly learn two
tasks (WSD and SRL) for one constituent. Be-
cause we only look at one constituent at a time,
we do not have to restrict ourselves to a re-ranking
approach like Toutanova et al, but can calculate
the full joint probability distribution of both tasks.
Andrew et al (2004) propose a method to learn a
joint generative inference model from partially la-
beled data and apply their method to the problems
of word sense disambiguation for verbs and deter-
mination of verb subcategorization frames. Their
motivation is similar to ours, but they focus on
learning from partially labeled data and they in-
vestigate different tasks.
None of these systems attempted to jointly learn
the semantics of the prepositional phrase and the
preposition in a single model, which is the main
contribution of our work reported in this paper.
7 Conclusion
We propose a probabilistic model to jointly clas-
sify the semantic role of a prepositional phrase
and the sense of the associated preposition. We
show that learning both tasks together leads to an
improvement over competitive, individual models
for both subtasks. For the WSD task, we show
that the SRL information improves the classifi-
cation accuracy, although joint learning does not
significantly outperform a simpler pipeline model
here. For the SRL task, we show that the joint
model improves over both the baseline model and
the pipeline model, especially for temporal and lo-
cation arguments. As we only disambiguate the
seven most frequent prepositions, potentially more
improvement could be gained by including more
prepositions into our data set.
Acknowledgements
This research was supported by a research grant
R-252-000-225-112 from National University of
Singapore Academic Research Fund.
References
Galen Andrew, Trond Grenager, and Christopher D.
Manning. 2004. Verb Sense and Subcategorization:
Using Joint Inference to Improve Performance on
Complementary Tasks. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 150?157.
Hoa Trang Dang and Martha Palmer. 2005. The
Role of Semantic Roles in Disambiguating Verb
Senses. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), pages 42?49.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing. Prentice-Hall, Inc. Up-
per Saddle River, NJ, USA.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of the 9th Conference on Computational
Natural Language Learning (CoNLL 2005), pages
181?184.
Yoong Keok Lee and Hwee Tou Ng. 2002. An Empir-
ical Evaluation of Knowledge Sources and Learn-
ing Algorithms for Word Sense Disambiguation. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2002), pages 41?48.
Kenneth C. Litkowski and Orin Hargraves. 2005. The
Preposition Project. In Proceedings of the 2nd ACL-
SIGSEM Workshop on The Linguistic Dimensions of
Prepositions and Their Use in Computational Lin-
guistic Formalisms and Applications, pages 171?
179.
Kenneth C. Litkowski and Orin Hargraves. 2007.
SemEval-2007 Task 06: Word-Sense Disambigua-
tion of Prepositions. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations (Se-
mEval 2007), pages 24?29.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Tom O?Hara and Janyce Wiebe. 2003. Preposi-
tion Semantic Classification via Penn Treebank and
FrameNet. In Proceedings of the 7th Conference on
Computational Natural Language Learning (CoNLL
2003), pages 79?86.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting Se-
mantic Role Resources for Preposition Disambigua-
tion. Computational Linguistics, 35(2):151?184.
Martha Palmer, Hoa Trang Dang, and Joseph Rosen-
zweig. 2000. Sense Tagging the Penn Treebank. In
Proceedings of the 2nd International Conference on
Language Resources and Evaluation (LREC 2000).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?105.
457
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005. Support Vector Learning for Semantic
Argument Classification. Machine Learning, 60(1?
3):11?39.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Se-
mantic Role Labeling. Computational Linguistics,
34(2):161?191.
Nianwen Xue and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of the 2004 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2004),
pages 88?94.
Patrick Ye and Timothy Baldwin. 2006. Seman-
tic Role Labeling of Prepositional Phrases. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 5(3):228?244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Se-
mantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval
2007), pages 241?244.
458
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 148?155,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Modeling Vocal Interaction for Text-Independent
Participant Characterization in Multi-Party Conversation
Kornel Laskowski
Cognitive Systems Labs
Universita?t Karlsruhe
Karlsruhe, Germany
kornel@ira.uka.de
Mari Ostendorf
Dept. of Electrical Engineering
University of Washington
Seattle WA, USA
mo@ee.washington.edu
Tanja Schultz
Cognitive Systems Labs
Universita?t Karlsruhe
Karlsruhe, Germany
tanja@ira.uka.de
Abstract
An important task in automatic conversation
understanding is the inference of social struc-
ture governing participant behavior. We ex-
plore the dependence between several social
dimensions, including assigned role, gender,
and seniority, and a set of low-level features
descriptive of talkspurt deployment in a mul-
tiparticipant context. Experiments conducted
on two large, publicly available meeting cor-
pora suggest that our features are quite useful
in predicting these dimensions, excepting gen-
der. The classification experiments we present
exhibit a relative error rate reduction of 37% to
67% compared to choosing the majority class.
1 Introduction
An important task in automatic conversation under-
standing is the inference of social structure govern-
ing participant behavior; in many conversations, the
maintenance or expression of that structure is an
implicit goal, and may be more important than the
propositional content of what is said.
There are many social dimensions along which
participants may differ (Berger, Rosenholtz and
Zelditch, 1980). Research in social psychology has
shown that such differences among participants en-
tail systematic differences in observed turn-taking
and floor-control patterns (e.g. (Bales, 1950), (Tan-
nen, 1996), (Carletta, Garrod and Fraser-Krauss,
1998)), and that participant types are not indepen-
dent of the types and sizes of conversations in which
they appear. In the present work, we consider the
dimensions of assigned role, gender, and senior-
ity level. We explore the predictability of these
dimensions from a set of low-level speech activ-
ity features, namely the probabilities of initiating
and continuing talkspurts in specific multipartici-
pant contexts, estimated from entire conversations.
For our purposes, talkspurts (Norwine and Murphy,
1938) are contiguous intervals of speech, with in-
ternal pauses no longer than 0.3 seconds. Features
derived from talkspurts are not only easier to com-
pute than higher-level lexical, prosodic, or dialogue
act features, they are also applicable to scenarios in
which only privacy-sensitive data (Wyatt et al 2007)
is available. At the current time, relatively little is
known about the predictive power of talkspurt tim-
ing in the context of large multi-party corpora.
As stated, our primary goal is to quantify the de-
pendence between specific types of speech activity
features and specific social dimensions; however,
doing so offers several additional benefits. Most
importantly, the existence of significant dependence
would suggest that multiparticipant speech activity
detectors (Laskowski, Fu?gen and Schultz, 2007) re-
lying on models conditioned on such attributes may
outperform those relying on general models. Fur-
thermore, conversational dialogue systems deployed
in multi-party scenarios may be perceived as more
human-like, by humans, if their talkspurt deploy-
ment strategies are tailored to the personalities they
are designed to embody.
Computational work which is most similar to that
presented here includes the inference of static dom-
inance (Rienks and Heylen, 2005) and influence
(Rienks et al, 2006) rankings. In that work, the au-
thors employed several speech activity features dif-
fering from ours in temporal scale and normaliza-
148
tion. Notably, their features are not probabilities
which are directly employable in a speech activity
detection system. In addition, several higher-level
features were included, such as topic changes, par-
ticipant roles, and rates of phenomena such as turns
and interruptions, and these were shown to yield the
most robust performance. Our aim is also similar
to that in (Vinciarelli, 2007) on radio shows, where
the proposed approach relies on the relatively fixed
temporal structure of production broadcasts, a prop-
erty which is absent in spontaneous conversation.
Although (Vinciarelli, 2007) also performs single-
channel speaker diarization, he does not explore be-
havior during vocalization overlap.
Aside from the above, the focus of the major-
ity of existing research characterizing participants
is the detection of dynamic rather than static roles
(i.e. (Banerjee and Rudnicky, 2004), (Zancanaro et
al, 2006), (Rienks et al, 2006)). From a mathe-
matical perspective, the research presented here is
a continuation of our earlier work on meeting types
(Laskowski, Ostendorf and Schultz, 2007), and we
rely on much of that material in the presentation
which follows.
2 Characterizing Participants
Importantly, we characterize participants in entire
groups, rather than characterizing each participant
independently. Doing so allows us to apply con-
straints on the group as a whole, eliminating the
need for hypothesis recombination (in the event that
more than one participant is assigned a role which
was meant to be unique). Additionally, treating
groups holistically allows for modeling the interac-
tions between specific pairs of participant types.
For each conversation or meeting1 of K partici-
pants, we compute a feature vector F, in which all
one-participant and two-participant speech activity
features are found in a particular order, typically im-
posed by microphone channel or seating assignment
(the specific features are described in Section 4).
The goal is to find the most likely group assignment
of participant labels that account for the observed
F. In (Laskowski, Ostendorf and Schultz, 2007), it
was shown that meeting types in a large meeting cor-
1
?Conversation? and ?meeting? will be used interchange-
ably in the current work.
pus can be successfully inferred from F using this
approach; here, we employ the same framework to
classify participant types in the K-length vector g,
for the group as a whole:
g? = arg max
g?G
P (g |F )
= arg max
g?G
P (g )
? ?? ?
MM
P (F |g )
? ?? ?
BM
, (1)
where MM and BM are the membership and behav-
ior models, respectively, and G is the set of all pos-
sible assignments of g.
In the remainder of this section, we define the
participant characteristics we explore, which include
assigned role, gender, and seniority. We treat these
as separate tasks, applying the same classification
framework. We also show how our definitions pro-
vide search space constraints on Equation 1.
2.1 Conversations with Unique Roles
Given a meeting of K participants, we consider a set
of roles R = {R1, R2, ? ? ? , RK} and assign to each
participant k, 1?k?K, exactly one role in R. An
example group assignment is the vector r1 of length
K, where r1 [k] = Rk. The set R of group assign-
ment alternatives r ? R is given by permutations
? : R 7? R, where ? ? SK , the symmetric group on
K symbols2. The number of elements in R is iden-
tically the number of unique permutations in SK , a
quantity known as its order |SK | = K!.
To identify the most likely group assignment r? =
?? (r1) given the set F of observables, we iterate
over the K! elements of SK using
?? = arg max
??SK
P (F |? (r1) ) , (2)
where we have elided the prior P (? ) assuming that
it is uniform. Following the application of Equa-
tion 2, the most likely role of participant k is given
by ?? (r1) [k].
Alternately, we may be interested in identifying
only a subset of the roles in R, namely a leader, or
a manager. In this case, participant roles are drawn
from L = {L,?L}, under the constraint that exactly
one participant is assigned the role L. The set L of
2For an overview of group theoretic notions and notation,
we refer the reader to (Rotman, 1995).
149
alternative group assignments has K indicator vec-
tor members lj , 1?j?K, where lj [k] is L for k = j
and ?L otherwise.3 We iterate over the indicator
vectors to obtain
j? = arg max
j?{1,???,K}
P (F | lj ) , (3)
assuming uniform priors P ( lj ). Following the ap-
plication of Equation 3, j? is the index of the most
likely L participant.
We note that this framework for unique role clas-
sification is applicable to classifying unique ranks,
without first having to collapse them into non-
unique rank classes as was necessary in (Rienks et
al., 2006).
2.2 Conversations with Non-Unique Roles
The second type of inference we consider is for di-
mensions in which roles are not unique, i.e. where
participants are in principle drawn independently
from a set of alternatives. This naturally includes
dimensions such as gender, seniority, age, etc.
As an example, we treat the case of gender. Par-
ticipant genders are drawn independently from H =
{~,|}. The set of group assignment alternatives h
is given by the Cartesian product HK , of 2K unique
elements. We search for the most likely group as-
signment h?, given the observables F, by iterating
over these elements using
h? = arg max
h?HK
P (h ) P (F |h ) . (4)
Once h? is found, the gender of each participant k is
available in h? [k].
A similar scenario is found for seniority, when
it is not uniquely ranked. We assume a set of
NS mutually exclusive seniority levels Si ? S =
{S1, S2, ? ? ? , SNS}, 1?i?NS . During search, each
participant?s seniority level is drawn independently
from S , leading to group assignments s ? SK , of
which there are NKS . As for gender, we iterate over
these to find
s? = arg max
s?SK
P ( s ) P (F | s ) . (5)
The seniority of participant k, following the applica-
tion of Equation 5, is s? [k].
3For completeness, we note that each lj corresponds to a
permutation ? : L 7? L of l1, and that ? ? ?? ?, the cyclic sub-
group generated by ? , where ? is the permutation (1, 2, ? ? ? ,K).
3 Data
In the current work, we use two different corpora of
multi-party meetings. The first, the scenario subset
of the AMI Meeting Corpus (Carletta, 2007), con-
sists of meetings involving K = 4 participants who
play different specialist roles in a product design
team. We have observed the recommended division
of this data into: AMITRAINSET of 98 meetings;
AMIDEVSET of 20 meetings; and AMIEVALSET,
also of 20 meetings. Although each participant takes
part in approximately 4 meetings, the 3 sets are dis-
joint in participants. We use only the provided word
alignments of these meetings. The corpus is accom-
panied by metadata which specifies the gender and
assigned role of each participant.
The second corpus consists of the Bed, Bmr,
and Bro meeting types in the ICSI Meeting Cor-
pus (Janin et al, 2003). Each meeting is identified
by one of {Bed,Bmr,Bro}, as well as a numerical
identifier d. We have divided these meetings into:
ICSITRAINSET, consisting of the 33 meetings for
which d mod 4 ? {1, 2}; ICSIDEVSET, consist-
ing of the 18 meetings for which d mod 4 ? 3;
and ICSIEVALSET, consisting of the 16 meetings for
which d mod 4 ? 0. These three sets are not dis-
joint in participants, and the number of instrumented
participants K varies from meeting to meeting, be-
tween 3 and 9. The corpus is accompanied by meta-
data specifying the gender, age, and education level
of each participant. We use only the forced align-
ments of these meetings, available in the accompa-
nying MRDA Corpus (Shriberg et al 2004).
4 Features
Our observation space is the complete K-participant
vocal interaction on-off pattern description for a
meeting C, a discretized version of which we denote
as qt ? {0, 1}K for 1?t?T , where T is the dura-
tion of C in terms of the number of 100 ms frames.
Details regarding the discretization (and subsequent
feature computation) can be found in (Laskowski,
Ostendorf and Schultz, 2007).
We compute from qt the following features4
which are the elements of F: fV Ik , the probabil-
4Feature type superscripts indicate talkspurt initiation (I) or
continuation (C), for either single-participant vocalization (V )
or vocalization overlap (O).
150
ity that participant k initiates vocalization at time t
when no-one else was speaking at t ? 1; fV Ck , the
probability that participant k continues vocalization
at time t when no-one else was speaking at t ? 1;
fOIk,j , the probability that participant k initiates vo-
calization at time t when participant j was speaking
at t ? 1; and fOCk,j the probability that participant k
continues vocalization at time t when participant j
was speaking at t? 1. Values of the features, which
are time-independent probabilities, are estimated us-
ing a variant of the Ising model (cf. (Laskowski, Os-
tendorf and Schultz, 2007)). Additionally, we com-
pute a feature fVk , the probability that participant
k vocalizes at time t, and single-participant aver-
ages of the two-participant features: ?fOIk,j ?j , ?fOIj,k ?j ,
?fOCk,j ?j , and ?fOCj,k ?j . The complete feature vector
for a conversation of K participants then consists of
7K one-participant features, and 2(K2 ? K) two-
participant features.
We note that multiple phenomena contribute to
the overlap features. The features fOIk,j are based
on counts from interruptions, backchannels, and pre-
cise floor handoffs. The features fOCk,j are based on
counts from interruptions, attempts to hold the floor,
and backchannels. Both feature types also contain
counts incurred during schism, when the conversa-
tion splits into two sub-conversations.
5 Models
Since K may change from meeting to meeting, the
size of the feature vector F must be considered vari-
able. We therefore factor the behavior model, as-
suming that all features are mutually independent
and that each is described by its own univariate
Gaussian model N
(
?, ?2
)
. These parameters are
maximum likelihood estimates from the fk and fk,j
values in a training set of conversations. In most of
these experiments, where the number of classes is
small, no parameter smoothing is needed.
For the cases where the group prior is not uniform
and participant types are not unique, the member-
ship model assumes independent participant types
and has the general form
P (g ) =
K?
k=1
P (g [k] ) , (6)
where P (g [k] ) is the probability that the k-th par-
ticipant is type g [k]. This model is used for gen-
der (P (h)) and seniority (P (s)). The probabilities
of specific types are maximum likelihood estimates
from the training data.
6 Assigned Role Classification
6.1 Classifying Unique Roles
For unique role classification, we use the AMI Meet-
ing Corpus. All meetings consist of K = 4 par-
ticipants, and each participant is assigned one of
four roles: project manager (PM), marketing expert
(ME), user interface designer (UI), or industrial de-
signer (ID).
As mentioned in Section 2.1, classifying the
unique role of all participants, jointly, involves
enumerating over the possible permutations of
{PM, ME, UI, ID}. We use AMITRAINSET to train
the behavior model, and then classify AMIDEVSET
using Equation 2, one feature type at a time, to iden-
tify the best 3 feature types for this task; develop-
ment experiments suggest that classification rates
level off after a small handful of the best perform-
ing feature types is included. Those feature types
were found to be fV Ik , ?fOIk,j ?j , and fOIk,j , capturing
the probability of initiating a talkspurt in silence, of
initiating a talkspurt when someone else is speak-
ing, and of initiating a talkspurt when a participant
in a specific other role is speaking, respectively. On
AMIEVALSET, these feature types lead to single-
feature-type 4-way classification rates of 41%, 29%,
and 53%, respectively. When all three types are used
together (3K+K2 features in total), the rate is 53%.
Accuracy when all feature types are used is 46%, in-
dicating that some feature types are detrimental to
this task.
The confusion matrix for classification using the
three best feature types is shown in Table 1. The
matrix shows that association between the reference
assignment of PM, as well as of UI, and the hypoth-
esized assignment based on the three feature types
mentioned is statistically significant. On the other
hand, assignment of ID and ME does not deviate
significantly from chance.
6.2 Finding the Manager
Using the same data as above, we explore the sim-
plified task of finding a specific participant type. We
151
HypRef
ID ME PM UI
ID 8 6 4 2
ME 5 8 4 3
PM 3 4 ++12 ? 1
UI 4 2 ?? 0 ++14
Table 1: Confusion matrix for role classification on
AMIEVALSET; reference assignment is found in the rows,
hypothesized assignment in columns. Correctly classified
roles, along the diagonal, are highlighted in bold. Statis-
tical significance of association at the p < 0.005 level
per class, using a 2?2 ?2-test, is shown using ?++? and
????, for above chance and below chance values, re-
spectively; the same is true of ?+? and ???, for signifi-
cance at the 0.005 ? p < 0.05 level.
equate the project manager role with L, and the re-
maining roles with ?L. This is justified by the AMI
meeting scenario, in which participant groups take a
product design from start to prototype, and in which
the project manager is expected to make the group
run smoothly.
The behavior model, trained on AMITRAINSET,
is applied using Equation 3 to determine the most
likely index j? of the leader L, given the observed
F, from among the K = 4 alternatives. To select
the best 3 feature types, we once again use AMIDE-
VSET; these turn out to be the same as those for role
classification, namely fV Ik , ?fOIk,j ?j , and fOIk,j . Using
these three feature types individually, we are able
to identify the leader PM in 12 of the 20 meetings
in AMIEVALSET. When all three are used together,
the identification rate is 60%. However, when all
feature types are used, the identification rate climbs
to 75%. Since all participants are equally likely to
be the leader, the baseline for comparison is random
guessing (25% accuracy).
Figure 1 shows the distribution of two of the se-
lected features, fV Ik and fOIk,j , for the data in AMI-
TRAINSET; we also show the first standard de-
viation of the single-Gaussian diagonal-covariance
models induced. We first note that fV Ik and fOIk,j
are correlated, i.e. that the probability of beginning
a talkspurt in silence is correlated with the proba-
bility of beginning a talkspurt when someone else
is speaking. L consistently begins more talkspurts,
both in silence and during other people?s speech. It
0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04
0
0.005
0.01
0.015
0.02
0.025
(?L,?L)
(?L,L)
(L,?L)
feature fVI
fe
at
ur
e 
fO
I
(?L,?L)
(?L,L)
(L,?L)
Figure 1: Distribution of (fV Ik , fOIk,j ) pairs for each of
(?L,?L), (?L,L), and (L,?L). Ellipses are centered
on AMITRAINSET means and encompass one standard
deviation.
is also interesting that ?L is slightly less likely to
initiate a talkspurt when L is already speaking than
when another ?L is. This suggests that ?L partic-
ipants consistently observe the L-status of the al-
ready speaking party when contemplating talkspurt
production. Finally, we note that neither the proba-
bility of continuing a talkspurt fV Ck (related to talk-
spurt duration) nor fVk (related to overall amount of
talk) are by themselves good L/?L discriminators.
7 Gender Classification
Gender classification is an example of a task with a
Cartesian search space. For these experiments, we
use the AMI Meeting Corpus and the ICSI Meet-
ing Corpus. In both corpora, gender is encoded in
the first letter of each participant?s unique identifier.
The ratio of male to female occurrences is 2 : 1
in AMITRAINSET, and 4 : 1 in ICSITRAINSET.
Choosing the majority class leads to gender classi-
fication rates of 65% and 81% on AMIEVALSET and
ICSIEVALSET, respectively.
We enumerate alternative group assignments us-
ing Equation 4. Somewhat surprisingly, no single
feature type leads to AMIEVALSET or ICSIEVALSET
classification rates higher than those obtained by hy-
pothesizing all participants to be male. On AMIDE-
VSET, one feature type (fOIk,j ) yields negligibly bet-
ter accuracy, but does not generalize to the corre-
152
sponding evaluation data. Furthermore, the associ-
ation between reference gender labels and hypothe-
sized gender labels, on both evaluation sets, does not
appear to be statistically significant at the p < 0.05
level. This finding that males and females do not
differ significantly in their deployment of talkspurts
is likely a consequence of the social structure of the
particular groups studied. The fact that AMI roles
are acted may also have an effect.
8 Seniority Classification
As a second example of non-unique roles, we at-
tempt to classify participant seniority. For these
experiments, we use the ICSI Meeting corpus, in
which each participant?s education level appears as
an optional, self-reported attribute. We have man-
ually clustered these attributes into NS = 3 mu-
tually exclusive seniority categories.5 Each partic-
ipant?s seniority is drawn independently from S =
{GRAD, PHD, PROF}; a breakdown for ICSITRAIN-
SET is shown in Table 2. Choosing the majority
class (P (PHD) = 0.444 on ICSITRAINSET) yields
a classification accuracy of 45% on ICSIEVALSET.
We note that in this data, education level is closely
correlated with age group.
Number ofSeniority
spkrs occur meets
GRAD 15 81 33
PHD 13 87 29
PROF 3 28 28
all 31 196 33
Table 2: Breakdown by seniority S in ICSITRAINSET by
the number of unique participants (spkrs), the number of
occurrences (occur), and the number of meetings (meets)
in which each seniority occurs.
8.1 Classifying Participant Types
Independently of Conversation Types
We first treat the problem of classifying participant
seniority levels independently of the type of conver-
sation being studied. We identify the most likely se-
5GRAD includes ?Grad?, as well as ?Undergrad?,
?B.A.?, and ?Finished BA in 2001?, due to their small
number of exemplars; PHD includes ?PhD? and ?Postdoc?;
and PROF includes ?Professor? only.
niority assignment for all participants using Equa-
tion 5. The best three feature types, determined
using ICSIDEVSET, are fVk , fOIk,j , and fOCk,j (repre-
senting the probability of speaking, of beginning a
talkspurt when a specific seniority participant is al-
ready speaking, and of continuing a talkspurt when
a specific seniority participant is speaking), yield-
ing single-feature-type classification rates of 52%,
59%, and 59%, respectively. When used together,
these three feature types produce the confusion ma-
trix shown in Table 3 and a rate of 61%, better than
when all feature types are used (58%). This rep-
resents a 28% relative error reduction over chance.
As can be seen in the table, association between the
reference and hypothesized seniority assignments is
statistically significant on unseen data. It is also
evident that confusion between GRAD and PROF is
lower than between more proximate seniority levels.
HypRef
GRAD PHD PROF
GRAD ++11 26 3
PHD ? 2 ++41 ? 3
PROF 0 ?? 6 ++10
Table 3: Confusion matrix for seniority classification on
ICSIEVALSET; reference assignment is found in the rows,
hypothesized assignment in columns. Highlighting and
use of ?++?, ?+?, ???, and ???? as in Table 1.
Figure 2 shows the distribution of (fVk , fOCk,j )
pairs in ICSITRAINSET, together with the first stan-
dard deviation, for each combination of the al-
ready speaking seniority participant and the senior-
ity participant initiating a new talkspurt (except for
(PROF, PROF), since there is at most one PROF in
each ICSITRAINSET meeting).
As is clear from the figure, PROF participants in
this data talk more than either of the two other se-
niority types. The figure also demonstrates a differ-
ence of behavior during speech overlap. The four
ellipses describing GRAD behavior when overlap-
ping with any of the other three classes, as well as
PHD behavior when overlapping with GRAD partic-
ipants, are relatively broad and indicate the absence
of strong tendency or preference. However, PHD
participants are more likely to continue vocalizing in
overlap with other PHD participants, and even more
likely to continue through overlap with PROF partic-
153
0 0.2 0.4
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(GRAD,*)
(PHD,GRAD)
(PHD,PHD)
(PHD,PROF)
(PROF,GRAD)
(PROF,PHD)
feature fV
fe
at
ur
e 
fO
C
(GRAD,GRAD)
(GRAD,PHD)
(GRAD,PROF)
(PHD,GRAD)
(PHD,PHD)
(PHD,PROF)
(PROF,GRAD)
(PROF,PHD)
Figure 2: Distribution of (fVk , fOCk,j ) feature value pairs
for each of the (k, j) participant pairs (GRAD, GRAD),
(GRAD, PHD), (GRAD, PROF), (PHD, GRAD),
(PHD, PHD), (PHD, PROF), (PROF, GRAD), and
(PROF, PHD). Ellipses are centered on ICSITRAIN-
SET means and encompass one standard deviation.
ipants. A similar trend is apparent for PROF partici-
pants: the mean likelihood that they continue vocal-
izing in overlap with GRAD participants lies below
??? (bottom 17%) of their model with PHD partic-
ipants. We believe that the senior researchers in this
data are consciously minimizing their overlap with
students, who talk less, to make it easier for the lat-
ter to speak up.
8.2 Conditioning on Conversation Type
We now repeat the experiments in the previous sec-
tion, but condition the behavior and membership
models on meeting type t:
s? = arg max
s?SK
?
t?T
P ( t ) P ( s | t )
P (F | s , t ) , (7)
where t ? T = {Bed,Bmr,Bro}.
Performance using maximum likelihood esti-
mates for the behavior model P (F | s , t ) results
in a seniority classification rate on ICSIEVALSET of
61%, i.e. no improvement over conversation-type-
independent classification. We suspect this is due
to the smaller amounts of training material. To ver-
ify this assumption, we smooth the maximum like-
lihood estimates, ?Si,t, ?2Si,t, towards the maximum
likelihood conversation-type-independent estimates,
?Si , ?Si , using
??Si,t = ??Si,t + (1 ? ?)?Si , (8)
??2Si,t = ??Si,t + (1 ? ?) ?
2
Si , (9)
where the value of ? = 0.7 was selected using
ICSIDEVSET. This leads to a rate of 63% on IC-
SIEVALSET. Furthermore, if instead of estimating
the prior on conversation type P (t) from the train-
ing data, we use our meeting type estimates from
(Laskowski, Ostendorf and Schultz, 2007), the clas-
sification rate increases to 67%. A control experi-
ment in which the true type ttest of each test meeting
is known, i.e. P (t) = 1 if ttest = t and 0 otherwise,
shows that the maximum accuracy achievable under
optimal P (t) estimation is 73%.
9 Conclusions
We have explored several socially meaningful parti-
tions of participant populations in two large multi-
party meeting corpora. These include assigned role,
leadership (embodied by a manager position), gen-
der, and seniority. Our proposed classifier, which
can represent participants in groups rather than in-
dependently, is able to leverage the observed differ-
ences between specific pairs of participant classes.
Using only low-level features capturing when partic-
ipants choose to vocalize relative to one another, it
attains relative error rate reductions on unseen data
of 37%, 67%, and 40% over chance on classifying
role, leadership, and seniority, respectively. We have
also shown that the same classifier, using the same
features, cannot discriminate between genders in ei-
ther corpus.
A comparison of the proposed feature types and
their performance on the tasks we have explored is
shown in Table 4. Consistently, the most useful fea-
ture types appear to be the probability of initiating
a talkspurt in silence, and the probability of initiat-
ing a talkspurt when a participant of a specific type
is already speaking. Additionally, on the ICSI Meet-
ing Corpus, the probability of speaking appears to be
dependent on seniority, and the probability of con-
tinuing to vocalize in overlap with another partici-
pant appears to depend on the seniority of the lat-
ter. Finally, we note that, for seniority classification
on the unseen ICSIEVALSET, the top 3 feature types
outperform the best single feature type, indicating a
154
degree of feature type complementarity; this is also
true for L-detection on AMIEVALSET when all fea-
ture types, as opposed to the single best feature type,
are used.
Feature AMI ICSI
Type R L H H S S|t?
fVk 44 ? ? ? *52 *57
fV Ik *41 *60 ? ? 52 56
fV Ck 34 ? ? ? ? 62
?fOIj,k ?j 44 ? ? ? 47 56
?fOIk,j ?j *29 *60 ? ? 49 59
fOIk,j *53 *60 64 ? *59 *59
?fOCj,k ?j 24 ? ? ? ? 57
?fOCk,j ?j ? ? ? ? 54 59
fOCk,j ? ? ? ? *59 *63
top 3* 53 60 ? ? 61 67
all 46 75 43 47 58 57
priors 25 25 65 81 45 45
Table 4: Comparative classification performance for 3
experiments on AMIEVALSET and 3 experiments on IC-
SIEVALSET, per feature type; R, L, H, and S as defined
in Section 2. Also shown is performance on the best three
feature types (selected using development data) and all
feature types, as well as that when choosing the major-
ity class (?prior?), informed by training data priors; for
R and L classification, ?prior? performance is equal to
random guessing. ??? indicates that a feature type, by
itself, did not perform above the corresponding ?prior?
rate; top-3 feature type selection indicated by ?*?.
Our results not only suggest new, easy-to-
compute, low-level features for the automatic clas-
sification of participants into socially meaningful
types, but also offer scope for informing turn-taking
or talkspurt-deployment policies in conversational
agents deployed in multi-party settings. Addition-
ally, they suggest that implicit models of certain
equivalence classes may lead to improved perfor-
mance on other tasks, such as multi-participant vo-
cal activity detection.
Acknowledgments
We would like to thank Jean Carletta for help-
ful comments during the final preparation of this
manuscript, and Liz Shriberg for access to the ICSI
MRDA Corpus.
References
R. Bales. 1950. Interaction Process Analysis. Addison-
Wesley Press, Inc.
S. Banerjee and A. Rudnicky. 2004. Using simple speech
based features to detect the state of a meeting and
the roles of the meeting participants. Proc. INTER-
SPEECH, pp.2189-2192.
J. Berger, S. Rosenholtz, M. Zelditch Jr. 1980. Status
Organizing Processes. Annual Review of Sociology,
6:479-508.
J. Carletta, S. Garrod, and H. Fraser-Krauss. 1998. Com-
munication and placement of authority in workplace
groups ? The consequences for innovation. Small
Group Research, 29(5):531-559.
J. Carletta. 2007. Unleashing the killer corpus: Expe-
riences in creating the multi-everything AMI Meeting
Corpus. Language Resources and Evaluation Journal,
41(2):181?190.
A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke,
and C. Wooters. 2003. The ICSI Meeting Corpus.
Proc. ICASSP, pp.364?367.
K. Laskowski, M. Ostendorf, and T. Schultz. 2007. Mod-
eling vocal interaction for text-independent classifica-
tion of conversation type. Proc. SIGdial, pp.194-201.
K. Laskowski, C. Fu?gen, and T. Schultz. 2007. Simulta-
neous multispeaker segmentation for automatic meet-
ing recognition. Proc. EUSIPCO, pp.1294-1298.
A. Norwine and O. Murphy. 1938. Characteristic time
intervals in telephonic conversation. Bell System Tech-
nical Journal, 17:281-291.
R. Rienks and D. Heylen. 2005. Dominance detection
in meetings using easily obtainable features. Proc.
MLMI.
R. Rienks, D. Zhang, D. Gatica-Perez, and W. Post.
2006. Detection and application of influence rankings
in small-group meetings. Proc. ICMI.
J. Rotman. 1995. An Introduction to the Theory of
Groups. Springer-Verlag New York, Inc.
E. Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Car-
vey. 2004. The ICSI Meeting Recorder Dialog Act
(MRDA) Corpus. Proc. SIGdial, pp.97?100.
D. Tannen. 1996. Gender & Discourse. Oxford Univer-
sity Press, USA.
A. Vinciarelli. 2007. Speakers role recognition in mul-
tiparty audio recordings using social network analysis
and duration distribution modeling. IEEE Trans. Mul-
timedia, 9(6):1215-1226.
D. Wyatt, J. Bilmes, T. Choudhury, and H. Kautz.
2007. A privacy-sensitive approach to modeling
multi-person conversations. Proc. IJCAI, pp.1769?
1775.
M. Zancanaro, B. Lepri, and F. Pianesi. 2006. Automatic
detection of group functional roles in face to face in-
teractions. Proc. ICMI.
155
Advances in Meeting Recognition
Alex Waibel    , Hua Yu   , Martin Westphal  , Hagen Soltau  ,
Tanja Schultz    , Thomas Schaaf  , Yue Pan   , Florian Metze  , Michael Bett  
Interactive Systems Laboratories
  Carnegie Mellon University, Pittsburgh, PA, USA
 Universita?t Karlsruhe, Fakulta?t fu?r Informatik, Karlsruhe, Germany
http://www.is.cs.cmu.edu/
tanja@cs.cmu.edu
1. INTRODUCTION
Speech recognition has advanced considerably, but has been lim-
ited almost entirely either to situations in which close speaking mi-
crophones are natural and acceptable (telephone, dictation, com-
mand&control, etc.) or in which high-quality recordings are en-
sured. Furthermore, most recognition applications involve con-
trolled recording environments, in which the user turns the recog-
nition event on and off and speaks cooperatively for the purpose of
being recognized.
Unfortunately, the majority of situations in which humans speak
with each other fall outside of these limitations. When we meet
with others, we speak without turning on or off equipment, or we
don?t require precise positioning vis a vis the listener. Recogni-
tion of speech during human encounters, or ?meeting recognition?,
therefore represents the ultimate frontier for speech recognition, as
it forces robustness, knowledge of context, and integration in an
environment and/or human experience.
2. CHALLENGES
Over the last three years we have explored meeting recogni-
tion at the Interactive Systems Laboratories [5, 6, 7]. Meeting
recognition is performed as one of the components of a ?meeting
browser?; a search retrieval and summarization tool that provides
information access to unrestricted human interactions and encoun-
ters. The system is capable of automatically constructing a search-
able and browsable audiovisual database of meetings. The meet-
ings can be described and indexed in somewhat unorthodox ways,
including by what has been said (speech), but also by who said
it (speaker&face ID), where (face, pose, gaze, and sound source
tracking), how (emotion tracking), and why, and other meta-level
descriptions such as the purpose and style of the interaction, the fo-
cus of attention, the relationships between the participants, to name
a few (see [1, 2, 3, 4]).
The problem of speech recognition in unrestricted human meet-
ings is formidable. Error rates for standard recognizers are 5-10
times higher than for dictation tasks. Our explorations based on
LVCSR systems trained on BN, reveal that several types of mis-
.
matches are to blame [6]:
 Mismatched and/or degraded recording conditions (remote,
different microphone types),
 Mismatched dictionaries and language models (typically ideo-
synchratic discussions highly specialized on a topic of inter-
est for a small group and therefore very different from other
existing tasks),
 Mismatched speaking-style (informal, sloppy, multiple speak-
ers talking in a conversational style instead of single speakers
reading prepared text).
In the following sections, we describe experiments and improve-
ments based on our Janus Speech Recognition Toolkit JRTk [8]
applied to transcribing meeting speech robustly.
3. EXPERIMENTAL SETUP
As a first step towards unrestricted human meetings each speaker
is equipped with a clip-on lapel microphone for recording. By this
choice interferences can be reduced but are not ruled out com-
pletely. Compared to a close-talking headset, there is significant
channel cross-talk. Quite often one can hear multiple speakers on
a single channel. Since meetings consist of highly specialized top-
ics, we face the problem of a lack of training data. Large databases
are hard to collect and can not be provided on demand. As a conse-
quence we have focused on building LVCSR systems that are robust
against mismatched conditions as described above. For the purpose
of building a speech recognition engine on the meeting task, we
combined a limited set of meeting data with English speech and text
data from various sources, namely Wall Street Journal (WSJ), En-
glish Spontaneous Scheduling Task (ESST), Broadcast News (BN),
Crossfire and Newshour TV news shows. The meeting data con-
sists of a number of internal group meeting recordings (about one
hour long each), of which fourteen are used for experiments in this
paper. A subset of three meetings were chosen as the test set.
4. SPEECH RECOGNITION ENGINE
To achieve robust performance over a range of different tasks, we
trained our baseline system on Broadcast News (BN). The system
deploys a quinphone model with 6000 distributions sharing 2000
codebooks. There are about 105K Gaussians in the system. Vocal
Tract Length Normalization and cluster-based Cepstral Mean Nor-
malization are used to compensate for speaker and channel varia-
tions. Linear Discriminant Analysis is applied to reduce feature di-
mensionality to 42, followed by a diagonalization transform (Maxi-
mum Likelihood Linear Transform). A 40k vocabulary and trigram
System WER on Different Tasks [%]
BN (h4e98 1) F0-condition 9.6
BN (h4e98 1) all F-conditions 18.5
BN+ESST (h4e98 1) all F-conditions 18.4
Newshour 20.8
Crossfire 25.6
Improvements on Meeting Recognition
Baseline ESST system 54.1
Baseline BN system 44.2
+ acoustic training BN+ESST 42.2
+ language model interpolation (14 meetings) 39.0
Baseline BN system
+ acoustic MAP Adaptation (10h meeting data) 40.4
+ language model interpolation (14 meetings) 38.7
Table 1: Recognition Results on BN and Meeting Task
language model are used. The baseline language model is trained
on the BN corpus.
Our baseline system has been evaluated across the above men-
tioned tasks resulting in the word error rates shown in Table 1.
While we achieve a first pass WER of 18.5% on all F-conditions
and 9.6% on the F0-conditions in the Broadcast News task, the
word error rate of 44.2% on meeting data is quite high, reflecting
the challenges of this task. Results on the ESST system [9] are even
worse with a WER of 54.1% which results from the fact that ESST
is a highly specialized system trained on noise-free but spontaneous
speech in the travel domain.
4.1 Acoustic and Language Model Adaptation
The BN acoustic models have been adapted to the meeting data
thru Viterbi training, MLLR (Maximum Likelihood Linear Regres-
sion), and MAP (Maximum A Posteriori) adaptation. To improve
the robustness towards the unseen channel conditions, speaking
mode and training/test mismatch, we trained a system ?BN+ESST?
using a mixed training corpus. The comparison of the results in-
dicate that the mixed system is more robust (44.2%  42.2%),
without loosing the good performance on the original BN test set
(18.5% vs. 18.4%).
To tackle the lack of training corpus, we investigated linear inter-
polation of the BN and the meeting (MT) language model. Based
on a cross-validation test we calculated the optimal interpolation
weight and achieved a perplexity reduction of 21.5% relative com-
pared to the MT-LM and more than 50% relative compared to the
BN-LM. The new language model gave a significant improvement
decreasing the word error rate to 38.7%. Overall the error rate was
reduced by 	
  relative (44.2%  38.7%) compared to the BN
baseline system.
4.2 Model Combination based Acoustic Map-
ping (MAM)
For the experiments on meeting data reported above we have
used comparable recording conditions as each speaker in the meet-
ing has been wearing his or her own lapel microphone. Frequently
however this assumption does not apply. We have also carried out
experiments aimed at producing robust recognition when micro-
phones are positioned at varying distances from the speaker. In this
case data, specific for the microphone distance and SNR found in
the test condition is unavailable. We therefore apply a new method,
Model Combination based Acoustic Mapping (MAM) to the recog-
nition of speech at different distances. MAM was originally pro-
posed for recognition in different car noise environments, please
refer to [10, 11] for details.
MAM estimates an acoustic mapping on the log-spectral domain
in order to compensate for noise condition mismatches between
training and test. During training, the generic acoustic models  




Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 206?211,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Combination of Recurrent Neural Networks and Factored Language
Models for Code-Switching Language Modeling
Heike Adel
heike.adel@student.kit.edu
Ngoc Thang Vu
Institute for Anthropomatics, Karlsruhe Institute of Technology (KIT)
thang.vu@kit.edu
Tanja Schultz
tanja.schultz@kit.edu
Abstract
In this paper, we investigate the appli-
cation of recurrent neural network lan-
guage models (RNNLM) and factored
language models (FLM) to the task of
language modeling for Code-Switching
speech. We present a way to integrate part-
of-speech tags (POS) and language in-
formation (LID) into these models which
leads to significant improvements in terms
of perplexity. Furthermore, a comparison
between RNNLMs and FLMs and a de-
tailed analysis of perplexities on the dif-
ferent backoff levels are performed. Fi-
nally, we show that recurrent neural net-
works and factored language models can
be combined using linear interpolation to
achieve the best performance. The final
combined language model provides 37.8%
relative improvement in terms of perplex-
ity on the SEAME development set and
a relative improvement of 32.7% on the
evaluation set compared to the traditional
n-gram language model.
Index Terms: multilingual speech processing,
code switching, language modeling, recurrent
neural networks, factored language models
1 Introduction
Code-Switching (CS) speech is defined as speech
that contains more than one language (?code?). It
is a common phenomenon in multilingual com-
munities (Auer, 1999a). For the automated pro-
cessing of spoken communication in these sce-
narios, a speech recognition system must be able
to handle code switches. However, the compo-
nents of speech recognition systems are usually
trained on monolingual data. Furthermore, there
is a lack of bilingual training data. While there
have been promising research results in the area
of acoustic modeling, only few approaches so far
address Code-Switching in the language model.
Recently, it has been shown that recurrent neu-
ral network language models (RNNLMs) can im-
prove perplexity and error rates in speech recogni-
tion systems in comparison to traditional n-gram
approaches (Mikolov et al, 2010; Mikolov et al,
2011). One reason for that is their ability to han-
dle longer contexts. Furthermore, the integration
of additional features as input is rather straight-
forward due to their structure. On the other hand,
factored language models (FLMs) have been used
successfully for languages with rich morphology
due to their ability to process syntactical features,
such as word stems or part-of-speech tags (Bilmes
and Kirchhoff, 2003; El-Desoky et al, 2010).
The main contribution of this paper is the appli-
cation of RNNLMs and FLMs to the challenging
task of Code-Switching. Furthermore, the two dif-
ferent models are combined using linear interpo-
lation. In addition, a comparison between them is
provided including a detailed analysis to explain
their results.
2 Related Work
For this work, three different topics are investi-
gated and combined: linguistic investigation of
Code-Switching, recurrent neural network lan-
guage modeling and factored language models.
In (Muysken, 2000; Poplack, 1978; Bokamba,
1989), it is observed that code switches occur at
positions in an utterance where they do not violate
the syntactical rules of the involved languages. On
the one hand, Code-Switching can be regarded as
a speaker dependent phenomenon (Auer, 1999b;
Vu, Adel et al, 2013). On the other hand, par-
ticular Code-Switching patterns are shared across
speakers (Poplack, 1980). It can be observed that
part-of-speech tags may predict Code-Switching
points more reliable than words themselves. The
206
authors of (Solorio et al, 2008a) predict Code-
Switching points using several linguistic features,
such as word form, language ID, part-of-speech
tags or the position of the word relative to the
phrase (BIO). The best result is obtained by com-
bining those features. In (Chan et.al., 2006), four
different kinds of n-gram language models are
compared to predict Code-Switching. It is dis-
covered that clustering all foreign words into their
part-of-speech classes leads to the best perfor-
mance.
In the last years, neural networks have been used
for a variety of tasks, including language model-
ing (Mikolov et al, 2010). Recurrent neural net-
works are able to handle long-term contexts since
the input vector does not only contain the cur-
rent word but also the previous hidden layer. It
is shown that these networks outperform tradi-
tional language models, such as n-grams which
only contain very limited histories. In (Mikolov
et al, 2011), the network is extended by factoriz-
ing the output layer into classes to accelerate the
training and testing processes. The input layer
can be augmented to model features, such as part-
of-speech tags (Shi et al, 2011; Adel, Vu et al,
2013). In (Adel, Vu et al, 2013), recurrent neural
networks are applied to Code-Switching speech. It
is shown that the integration of POS tags into the
neural network, which predicts the next language
as well as the next word, leads to significant per-
plexity reductions.
A factored language model refers to a word as a
vector of features, such as the word itself, morpho-
logical classes, POS tags or word stems. Hence, it
provides another possibility to integrate syntacti-
cal features into the language modeling process.
In (Bilmes and Kirchhoff, 2003), it is shown that
factored language models are able to outperform
standard n-gram techniques in terms of perplexity.
In the same paper, generalized parallel backoff is
introduced. This technique can be used to general-
ize traditional backoff methods and to improve the
performance of factored language models. Due to
the integration of various features, it is possible to
handle rich morphology in languages like Arabic
or Turkish (Duh and Kirchhoff, 2004; El-Desoky
et al, 2010).
3 Code-Switching Language Modeling
3.1 Motivation
Since there is a lack of Code-Switching data, lan-
guage modeling is a challenging task. Traditional
n-gram approaches may not provide reliable esti-
mates. Hence, more general features than words
should be integrated into the language models.
Therefore, we apply recurrent neural networks and
factored language models. As features, we use
part-of-speech tags and language identifiers.
3.2 Using Recurrent Neural Networks As
Language Model
This section describes the structure of the recur-
rent neural network (RNNLM) that we use as
Code-Switching language model. It has been pro-
posed in (Adel, Vu et al, 2013) and is illustrated
in figure 1.
w(t)
 f(t)
s(t)
 y(t)
 c(t)
U1  V
WU2
Figure 1: RNNLM for Code-Switching
(based upon a figure in (Mikolov et al, 2011))
Vectorw(t), which represents the current word us-
ing 1-of-N coding, forms the input of the recur-
rent neural network. Thus, its dimension equals
the size of the vocabulary. Vector s(t) con-
tains the state of the network and is called ?hid-
den layer?. The network is trained using back-
propagation through time (BPTT), an extension of
the back-propagation algorithm for recurrent neu-
ral networks. With BPTT, the error is propagated
through recurrent connections back in time for a
specific number of time steps t. Hence, the net-
work is able to remember information for several
time steps. The matrices U1, U2, V , and W con-
tain the weights for the connections between the
layers. These weights are learned during the train-
ing phase. Moreover, the output layer is factorized
207
into classes which provide language information.
In this work, four classes are used: English, Man-
darin, other languages and particles. Vector c(t)
contains the probabilities for each class and vector
y(t) provides the probabilities for each word given
its class. Hence, the probability P (wi|history) is
computed as shown in equation 1.
P (wi|history) = P (ci|s(t))P (wi|ci, s(t)) (1)
It is intended to not only predict the next word but
also the next language. Hence according to equa-
tion 1, the probability of the next language is com-
puted first and then the probability of each word
given the language. Furthermore, a vector f(t)
is added to the input layer. It provides features
(in this work part-of-speech tags) corresponding
to the current word. Thus, not only the current
word is activated but also its features. Since the
POS tags are integrated into the input layer, they
are also propagated into the hidden layer and back-
propagated into its history s(t). Hence, not only
the previous feature is stored in the history but also
features from several time steps in the past.
3.3 Using Factored Language Models
Factored language models (FLM) are another ap-
proach to integrate syntactical features, such as
part-of-speech tags or language identifiers into the
language modeling process. Each word is re-
garded as a sequence of features which are used
for the computation of the n-gram probabilities.
If a particular sequence of features has not been
detected in the training data, backoff techniques
will be applied. For our task of Code-Switching,
we develop two different models: One model with
only part-of-speech tags as features and one model
including also language information tags. Un-
fortunately, the number of possible parameters is
rather high: Different feature combinations from
different time steps can be used to predict the
next word (conditioning factors), different back-
off paths and different smoothing methods may
be applied. To detect useful parameters, the ge-
netic algorithm described in (Duh and Kirchhoff,
2004) is used. It is an evolution-inspired technique
that encodes the parameters of an FLM as binary
strings (genes). First, an initializing set of genes is
generated. Then, a loop follows that evaluates the
fitness of the genes and mutates them until their
average fitness is not improved any more. As fit-
ness value, the inverse perplexity of the FLM cor-
responding to the gene on the development set is
Wt-1        Pt-1     Pt-2
Wt-1        Pt-2 Wt-1        Pt-1
Pt-2 Wt-1 Pt-1
unigram
Figure 2: Backoff graph of the FLM
used. Hence, parameter solutions with lower per-
plexities are preferred in the selection of the genes
for the following iteration. In (Duh and Kirch-
hoff, 2004), it is shown that this genetic method
outperforms both knowledge-based and random-
ized choices. For the case of part-of-speech tags
as features, the method results in three condition-
ing factors: the previous word Wt?1 and the two
previous POS tags Pt?1 and Pt?2. The backoff
graph obtained by the algorithm is illustrated in
figure 2. According to the result of the genetic al-
gorithm, different smoothing methods are used at
different backoff levels: For the backoff from three
factors to two factors, Kneser-Ney discounting is
applied. If the probabilities for the factor combi-
nation Wt?1Pt?2 could not be estimated reliably,
absolute discounting is used. In all other cases,
Witten-Bell discounting is applied. An overview
of the different smoothing methods can be found
in (Rosenfeld, 2000).
4 Experiments and Results
4.1 Data Corpus
SEAME (South East Asia Mandarin-English) is a
conversational Mandarin-English Code-Switching
speech corpus recorded from Singaporean and
Malaysian speakers (D.C. Lyu et al, 2011). It
was used for the research project ?Code-Switch?
jointly performed by Nanyang Technological Uni-
versity (NTU) and Karlsruhe Institute of Technol-
ogy (KIT). The recordings consist of spontanously
spoken interviews and conversations of about 63
hours of audio data. For this task, we deleted all
hesitations and divided the transcribed words into
four categories: English words, Mandarin words,
particles (Singaporean and Malaysian discourse
particles) and others (other languages). These cat-
egories are used as language information in the
language models. The average number of Code-
Switching points between Mandarin and English
208
is 2.6 per utterance and the duration of monolin-
gual segments is quite short: The average dura-
tion of English and Mandarin segments is only
0.67 seconds and 0.81 seconds respectively. In to-
tal, the corpus contains 9,210 unique English and
7,471 unique Mandarin vocabulary words. We di-
vided the corpus into three disjoint sets (training,
development and test set) and assigned the data
based on several criteria (gender, speaking style,
ratio of Singaporean and Malaysian speakers, ra-
tio of the four categories, and the duration in each
set). Table 1 lists the statistics of the corpus in
these sets.
Train set Dev set Eval set
# Speakers 139 8 8
Duration(hrs) 59.2 2.1 1.5
# Utterances 48,040 1,943 1,018
# Token 525,168 23,776 11,294
Table 1: Statistics of the SEAME corpus
4.2 POS Tagger for Code-Switching Speech
To be able to assign part-of-speech tags to our
bilingual text corpus, we apply the POS tagger
described in (Schultz et al, 2010) and (Adel, Vu
et al, 2013). It consists of two different mono-
lingual (Stanford log-linear) taggers (Toutanova
et al, 2003; Toutanova et al, 2000) and a com-
bination of their results. While (Solorio et al,
2008b) passes the whole Code-Switching text to
both monolingual taggers and combines their re-
sults using different heuristics, in this work, the
text is splitted into different languages first. The
tagging process is illustrated in figure 3.
Mandarin is determined as matrix language (the
main language of an utterance) and English as em-
bedded language. If three or more words of the
embedded language are detected, they are passed
to the English tagger. The rest of the text is passed
to the Mandarin tagger, even if it contains foreign
words. The idea is to provide the tagger as much
context as possible. Since most English words in
the Mandarin segments are falsely tagged as nouns
by the Mandarin tagger, a postprocessing step is
applied. It passes all foreign words of the Man-
darin segments to the English tagger in order to
replace the wrong tags with the correct ones.
Wt-1 P2un-igr-gamu?ut-i? PiW???a?n-igr-gamu?u?gnP?
??1a21
?igr-gauP?-i???u?a?a?a????? ??PiPig1a21
??1-gga u? ut-i?- Pi??1-gga u? u?gnP?
?r1?1 ?1?1?gnP??g?i1? Pi a?PiPig1a21
????a?Pig?
Figure 3: Tagging of Code-Switching speech
4.3 Evaluation
For evaluation, we compute the perplexity of each
language model on the SEAME development and
evaluation set und perform an analysis of the dif-
ferent back-off levels to understand in detail the
behavior of each language model. A traditional 3-
gram LM trained with the SEAME transcriptions
serves as baseline.
4.3.1 LM Performance
The language models are evaluated in terms of per-
plexity. Table 2 presents the results on the devel-
opment and test set.
Model dev set test set
Baseline 3-gram 285.87 285.25
FLM (pos) 263.57 271.57
FLM (pos + lid) 263.84 276.99
RNNLM (pos) 233.50 268.05
RNNLM (pos + lid) 219.85 239.21
Table 2: Perplexity results
It can be noticed that both the RNNLM and the
FLM model outperform the traditional 3-gram
model. Hence, adding syntactical features im-
proves the word prediction. For the FLM, it leads
to no improvement to add the language identifier
as feature. In contrast, clustering the words into
their languages on the output layer of the RNNLM
leads to lower perplexities.
209
4.3.2 Backoff Level Analysis
To understand the different results of the RNNLM
and the FLM, an analysis similar to the one de-
scribed in (Oparin et al, 2012) is performed. For
each word, the backoff-level of the n-gram model
is observed. Then, a level-dependent perplexity is
computed for each model as shown in equation 2.
PPLk = 10
? 1Nk
?
wk
log10P (wk|hk) (2)
In the equation, k denotes the backoff-level, Nk
the number of words on this level, wk the current
word and hk its history. Table 3 shows how often
each backoff-level is used and presents the level-
dependent perplexities of each model on the de-
velopment set.
1-gram 2-gram 3-gram
# occurences 6894 11628 6226
Baseline 3-gram 5,786.24 165.82 28.28
FLM (pos) 4,950.31 147.70 30.99
RNNLM 3,231.02 151.67 21.24
Table 3: Backoff-level-dependent PPLs
In case of backoff to the 2-gram, the FLM pro-
vides the best perplexity, while for the 3-gram and
backoff to the 1-gram, the RNNLM performs best.
This may be correlated with the better over-all per-
plexity of the RNNLM in comparison to the FLM.
Nevertheless, the backoff to the 2-gram is used
about twice as often as the backoff to the 1-gram
or the 3-gram.
4.4 LM Interpolation
The different results of RNNLM and FLM show
that they provide different estimates of the next
word. Thus, a combination of them may reduce
the perplexities of table 2. Hence, we apply lin-
ear interpolation to the probabilities of each two
models as shown in equation 3.
P (w|h) = ??PM1(w|h)+(1??)?PM2(w|h) (3)
The equation shows the computation of the pob-
ability for word w given its history h. PM1 de-
notes the probability provided by the first model
and PM2 the probability from the second model.
Table 4 shows the results of this experiment. The
weights are optimized on the development set.
The interpolation of RNNLM and FLM leads to
the best results. This may be caused by the supe-
rior backoff-level-dependent PPLs in comparison
PPL PPL
Model weight on dev on eval
FLM + 3-gram 0.7, 0.3 211.13 227.57
RNNLM + 3-gram 0.8, 0.2 206.49 227.08
RNNLM + FLM 0.6, 0.4 177.79 192.08
Table 4: Perplexities after interpolation
to the 3-gram model. While the RNNLM performs
better for the 3-gram and for the backoff to the 1-
gram, the FLM performs the best in case of back-
off to the 2-gram which is used more often than
the other levels (table 3).
5 Conclusions
In this paper, we presented two different methods
for language modeling of Code-Switching speech:
Recurrent neural networks and factored language
models. We integrated part-of-speech tags and
language information to improve the performance
of the language models. In addition, we ana-
lyzed their behavior on the different backoff lev-
els. While the FLM performed better in case of
backoff to the 2-gram, the RNNLM led to a bet-
ter over-all performance. Finally, the models were
combined using linear interpolation. The com-
bined language model provided 37.8% relative im-
provement in terms of perplexity on the SEAME
development set and a relative improvement of
32.7% on the evaluation set compared to the tra-
ditional n-gram LM.
References
H. Adel, N.T. Vu, F. Kraus, T. Schlippe, and T. Schultz.
2013 Recurrent Neural Network Language Model-
ing for Code Switching Conversational Speech In:
Proceedings of ICASSP 2013.
P. Auer 1999 Code-Switching in Conversation , Rout-
ledge.
P. Auer 1999 From codeswitching via language mixing
to fused lects toward a dynamic typology of bilin-
gual speech In: International Journal of Bilingual-
ism, vol. 3, no. 4, pp. 309-332.
J.A. Bilmes and K. Kirchhoff. 2003 Factored Lan-
guage Models and Generalized Parallel Backoff In:
Proceedings of NAACL, 2003.
E.G. Bokamba 1989 Are there syntactic constraints on
code-mixing? In: World Englishes, vol. 8, no. 3, pp.
277-292.
J.Y.C. Chan, PC Ching, T. Lee, and H. Cao 2006
Automatic speech recognition of Cantonese-English
210
code-mixing utterances In: Proceeding of Inter-
speech 2006.
K. Duh and K. Kirchhoff. 2004. Automatic Learning
of Language Model Structure, pg 148. In: Proceed-
ings of the 20th international conference on Compu-
tational Linguistics.
A. El-Desoky, R. Schlu?ter, H.Ney 2010 AHybrid Mor-
phologically Decomposed Factored Language Mod-
els for Arabic LVCSR In: NAACL 2010.
D.C. Lyu, T.P. Tan, E.S. Cheng, H. Li 2011 An Anal-
ysis of Mandarin-English Code-Switching Speech
Corpus: SEAME In: Proceedings of Interspeech
2011.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993 Building a large annotated corpus of english:
The penn treebank In: Computational Linguistics,
vol. 19, no. 2, pp. 313330.
T. Mikolov, M. Karafiat, L. Burget, J. Jernocky and S.
Khudanpur. 2010 Recurrent Neural Network based
Language Model In: Proceedings of Interspeech
2010.
T. Mikolov, S. Kombrink, L. Burget, J. Jernocky and
S. Khudanpur. 2011 Extensions of Recurrent Neu-
ral Network Language Model In: Proceedings of
ICASSP 2011.
P. Muysken 2000 Bilingual speech: A typology of
code-mixing In: Cambridge University Press, vol.
11.
I. Oparin, M. Sundermeyer, H. Ney, J.-L. Gauvain
2012 Performance analysis of Neural Networks
in combination with n-gram language models In:
ICASSP, 2012.
S. Poplack 1978 Syntactic structure and social func-
tion of code-switching , Centro de Estudios Puertor-
riquenos, City University of New York.
S. Poplack 1980 Sometimes ill start a sentence in
spanish y termino en espanol: toward a typology of
code-switching In: Linguistics, vol. 18, no. 7-8, pp.
581-618.
R. Rosenfeld 2000 Two decades of statistical language
modeling: Where do we go from here? In: Proceed-
ings of the IEEE 88.8 (2000): 1270-1278.
T. Schultz, P. Fung, and C. Burgmer, 2010 Detecting
code-switch events based on textual features.
Y. Shi, P. Wiggers, M. Jonker 2011 Towards Recurrent
Neural Network Language Model with Linguistics
and Contextual Features In: Proceedings of Inter-
speech 2011.
T. Solorio, Y. Liu 2008 Part-of-speech tagging for
English-Spanish code-switched text In: Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing. Association for Computa-
tional Linguistics, 2008.
T. Solorio, Y. Liu 2008 Learning to predict code-
switching points In: Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing. Association for Computational Linguis-
tics, 2008.
K. Toutanova, C.D. Manning 2000 Enriching the
knowledge sources used in a maximum entropy part-
of-speech tagger In: Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natu-
ral language processing and very large corpora: held
in conjunction with the 38th Annual Meeting of the
Association for Computational Linguistics, vol. 13.
K. Toutanova, D. Klein, C.D. Manning, and Y. Singer
2003 Feature-rich part-of-speech tagging with a
cyclic dependency network In: Proceedings of
NAACL 2003.
N.T. Vu, D.C. Lyu, J. Weiner, D. Telaar, T. Schlippe, F.
Blaicher, E.S. Chng, T. Schultz, H. Li 2012 A First
Speech Recognition System For Mandarin-English
Code-Switch Conversational Speech In: Proceed-
ings of Interspeech 2012.
N.T. Vu, H. Adel, T. Schultz 2013 An Investigation of
Code-Switching Attitude Dependent Language Mod-
eling In: In Statistical Language and Speech Pro-
cessing, First International Conference, 2013.
N. Xue, F. Xia, F.D. Chiou, and M. Palmer 2005 The
penn chinese treebank: Phrase structure annotation
of a large corpusk In: Natural Language Engineer-
ing, vol. 11, no. 2, pp. 207.
211
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 48?53
Manchester, August 2008
Speech Translation for Triage of Emergency Phonecalls in Minority 
Languages 
Udhyakumar Nallasamy, Alan W Black, Tanja 
Schultz, Robert Frederking 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA 15213  USA 
udhay@cmu.edu, 
{awb,ref,tanja}@cs.cmu.edu 
Jerry Weltman 
Louisiana State University 
Baton Rouge,  
Louisiana 70802  USA 
jweltm2@lsu.edu 
 
Abstract 
We describe Ayudame, a system de-
signed to recognize and translate Spanish 
emergency calls for better dispatching. 
We analyze the research challenges in 
adapting speech translation technology to 
9-1-1 domain. We report our initial re-
search in 9-1-1 translation system design, 
ASR experiments, and utterance classifi-
cation for translation.  
1 Introduction 
In the development of real-world-applicable lan-
guage technologies, it is good to find an applica-
tion with a significant need, and with a complex-
ity that appears to be within the capabilities of 
current existing technology.  Based on our ex-
perience in building speech-to-speech translation, 
we believe that some important potential uses of 
the technology do not require a full, complete 
speech-to-speech translation system; something 
much more lightweight can be sufficient to aid 
the end users (Gao et al 2006). 
A particular task of this kind is dealing with 
emergency call dispatch for police, ambulance, 
fire and other emergency services (in the US the 
emergency number is 9-1-1).  A dispatcher must 
answer a large variety of calls and, due to the 
multilingual nature of American society, they 
may receive non-English calls and be unable to 
service them due to lack of knowledge of the 
caller language. 
                                               
 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
    Figure 1. Ayudame system architecture 
 
As a part of a pilot study into the feasibility of 
dealing with non-English calls by a mono-lingual 
English-speaking dispatcher, we have designed a 
translation system that will aid the dispatcher in 
communicating without understanding the 
caller?s language. 
48
The fundamental idea is to use utterance clas-
sification of the non-English input.  The non-
English is first recognized by a speech recogni-
tion system; then the output is classified into a 
small number of domain-specific classes called 
Domain Acts (DAs) that can indicate directly to 
the dispatcher the general intended meaning of 
the spoken phrase.  Each DA may have a few 
important parameters to be translated, such as 
street addresses (Levin et al 2003; Langley 
2003). The dispatcher can then select from a lim-
ited number of canned responses to this through 
a simple menu system.  We believe the reduction 
in complexity of such a system compared to a 
full speech-to-speech translation will be advanta-
geous because it should be much cheaper to con-
struct, easier to port to new languages, and, im-
portantly, sufficient to do the job of processing 
emergency calls.  
In the ?NineOneOne? project, we have de-
signed an initial prototype system, which we call 
?Ayudame? (Spanish word for ?Help me?).  Fig-
ure 1 gives an overview of the system architec-
ture. 
2 The NineOneOne Domain 
Our initial interest in this domain was due to con-
tact from the Cape Coral Police Department 
(CCPD) in Florida.  They were interested in in-
vestigating how speech-to-speech translations 
could be used in emergency 9-1-1 dispatch sys-
tems.  Most current emergency dispatching cen-
ters use some proprietary human translation ser-
vice, such as Language Line (Language Line 
Services).  Although this service provides human 
translation services for some 180 languages, it is 
far from ideal.  Once the dispatcher notes that the 
caller cannot speak/understand English, they 
must initiate the call to Language Line, including 
identifying themselves to the Language Line op-
erator, before the call can actually continue.  This 
delay can be up to a minute, which is not ideal in 
an emergency situation.    
After consulting with CCPD, and collecting a 
number of example calls, it was clear that full 
speech-to-speech translation was not necessary 
and that a limited form of translation through 
utterance classification (Lavie et al 2001) might 
be sufficient to provide a rapid response to non-
English calls.  The language for our study is 
Spanish.  Cape Coral is on the Gulf Coast of  
Florida and has fewer Spanish speakers than e.g. 
the Miami area, but still sufficient that a number 
of calls are made to their emergency service in 
Spanish, yet many of their operators are not 
sufficiently fluent in Spanish to deal with the 
calls. 
There are a number of key pieces of 
information that a dispatcher tries to collect 
before passing on the information to the 
appropriate emergency service.  This includes 
things like location, type of emergency, urgency, 
if anyone is hurt, if the situation is dangerous, 
etc.  In fact many dispaching organizations have 
existing, well-defined  policies on what 
information they should collect for different 
types of emergencies.  
3 Initial system design 
Based on the domain's characteristics, in addition 
to avoiding full-blown translation, we are follow-
ing a highly asymmetrical design for the system 
(Frederking et al 2000).  The dispatcher is al-
ready seated at a workstation, and we intend to 
keep them ?in the loop?, for both technical and 
social reasons.  So in the dispatcher-to-caller di-
rection, we can work with text and menus, sim-
plifying the technology and avoiding some cog-
nitive complexity for the operator.  So in the dis-
patcher-to-caller direction we require  
 no English ASR, 
 no true English-to-Spanish MT, and 
 simple, domain-limited, Spanish speech 
synthesis. 
The caller-to-dispatcher direction is much more 
interesting. In this direction we require 
 Spanish ASR that can handle emotional 
spontaneous telephone speech in mixed 
dialects, 
 Spanish-to-English MT, but 
 no English Speech Synthesis. 
We have begun to consider the user interfaces 
for Ayudame as well.  For ease of integration 
with pre-existing dispatcher workstations, we 
have chosen to use a web-based graphical inter-
face.  For initial testing of the prototype, we plan 
to run in ?shadow? mode, in parallel with live 
dispatching using the traditional approach.  Thus 
Ayudame will have a listen-only connection to 
the telephone line, and will run a web server to 
interact with the dispatcher.  Figure 2 shows an 
initial design of the web-based interface.  There 
are sections for a transcript, the current caller 
utterance, the current dispatcher response 
choices, and a button to transfer the interaction to 
a human translator as a fall-back option.  For 
each utterance, the DA classification is displayed 
in addition to the actual utterance (in case the 
dispatcher knows some Spanish). 
49
 Figure 2. Example of initial GUI design 
 
4 Automatic Speech Recognition 
An important requirement for such a system is 
the ability to be able to recognize the incoming 
non-English speech with a word error rate suffi-
ciently low for utterance classification and pa-
rameter translation to be possible.  The issues in 
speech recognition for this particular domain in-
clude: telephone speech (which is through a lim-
ited bandwidth channel); background noise (the 
calls are often from outside or in noisy places); 
various dialects of Spanish, and potential stressed 
speech.  Although initially we expected a sub-
stantial issue with recognizing stressed speakers, 
as one might expect in emergency situations, in 
the calls we have collected so far, although it is 
not a negligible issue, it is far less important that 
we first expected. 
The Spanish ASR system is built using the 
Janus Recognition Toolkit (JRTk) (Finke et al 
1997) featuring the HMM-based IBIS decoder 
(Soltau et al 2001). Our speech corpus consists 
of 75 transcribed 9-1-1 calls, with average call 
duration of 6.73 minutes (min: 2.31 minutes, 
max: 13.47 minutes). The average duration of 
Spanish speech (between interpreter and caller) 
amounts to 4.8 minutes per call. Each call has 
anywhere from 46 to 182 speaker turns with an 
average of 113 speaker turns per call. The turns 
that have significant overlap between speakers 
are omitted from the training and test set. The 
acoustic models are trained on 50 Spanish 9-1-1 
calls, which amount to 4 hours of speech data.  
 
 
The system uses three-state, left-to-right, sub-
phonetically tied acoustic models with 400 con-
text-dependent distributions with the same num-
ber of codebooks. Each codebook has 32 gaus-
sians per state. The front-end feature extraction 
uses standard 39 dimensional Mel-scale cepstral 
coefficients and applies Linear Discriminant 
Analysis (LDA) calculated from the training 
data. The acoustic models are seeded with initial 
alignments from GlobalPhone Spanish acoustic 
models trained on 20 hours of speech recorded 
from native Spanish speakers (Schultz et al 
1997). The vocabulary size is 65K words. The 
language model consists of a trigram model 
trained on the manual transcriptions of 40 calls 
and interpolated with a background model 
trained on GlobalPhone Spanish text data con-
sisting of 1.5 million words (Schultz et al 1997). 
The interpolation weights are determined using 
the transcriptions of 10 calls (development set). 
The test data consists of 15 telephone calls from 
different speakers, which amounts to a total of 1 
hour. Both development and test set calls con-
sisted of manually segmented and transcribed 
speaker turns that do not have a significant over-
lap with other speakers. The perplexity of the test 
set according to the language model is 96.7. 
The accuracy of the Spanish ASR on the test 
set is 76.5%.  This is a good result for spontane-
ous telephone-quality speech by multiple un-
known speakers, and compares favourably to the 
ASR accuracy of other spoken dialog systems.  
We had initially planned to investigate novel 
ASR techniques designed for stressed speech and 
multiple dialects, but to our surprise these do not 
50
seem to be required for this application.  Note 
that critical information such as addresses will be 
synthesized back to the caller for confirmation in 
the full system. So, for the time-being we will 
concentrate on the accuracy of the DA classifica-
tion until we can show that improving ASR accu-
racy would significantly help. 
5 Utterance Classification 
As mentioned above, the translation approach we 
are using is based on utterance classification. The 
Spanish to English translation in the Ayudame 
system is a two-step process. The ASR 
hypothesis is first classified into domain-specific 
Domain Acts (DA). Each DA has a 
predetermined set of parameters. These 
parameters are identified and translated using a 
rule-based framework.  For this approach to be 
accomplished with reasonable effort levels, the 
total number of types of parameters and their 
complexity must be fairly limited in the domain, 
such as addresses and injury types. This section 
explains our DA tagset and classification 
experiments. 
5.1 Initial classification and results 
The initial evaluation (Nallasamy et al 2008) 
included a total of 845 manually labeled turns in 
our 9-1-1 corpus. We used a set of 10 tags to an-
notate the dialog turns. The distribution of the 
tags are listed below 
 
Tag (Representation) Frequency 
Giving Name 80 
Giving Address 118 
Giving Phone number 29 
Requesting Ambulance 8 
Requesting Fire Service 11 
Requesting Police 24 
Reporting Injury/Urgency 61 
Yes 119 
No 24 
Others 371 
Table 1. Distribution of first-pass tags in the 
corpus. 
 
We extracted bag-of-word features and trained a 
Support Vector Machine (SVM) classifier (Bur-
ges, 1998) using the above dataset. A 10-fold 
stratified cross-validation has produced an aver-
age accuracy of 60.12%. The accuracies of indi-
vidual tags are listed below. 
 
Tag Accuracy  
  (%) 
Giving Name 57.50 
Giving Address 38.98 
Giving Phone number 48.28 
Req. Ambulance 62.50 
Req. Fire Service 54.55 
Req. Police 41.67 
Reporting Injury/Urgency 39.34 
Yes 52.94 
No 54.17 
Others 75.74 
Table 2. Classification accuracies of first-pass 
tags. 
5.2 Tag-set improvements 
We improved both the DA tagset and the 
classification framework in our second-pass 
classification, compared to our initial  
experiment. We had identified several issues in 
our first-pass classification:  
 We had forced each dialog turn to have a 
single tag. However, the tags and the 
dialog turns don?t conform to this 
assumption. For example, the dialog 
?Yes, my husband has breathing prob-
lem. We are at two sixty-one Oak 
Street?1 should get 3 tags: ?Yes?, ?Giv-
ing-Address?, ?Requesting-Ambulance?.  
 Our analysis of the dataset alo showed 
that the initial set of tags are not 
exhaustive enough to cover the whole 
range of dialogs required to be translated 
and conveyed to the dispatcher.  
We made several iterations over the tagset to 
ensure that it is both compact and achieves  
requisite coverage. The final tag set consists of 
67 entries. We manually annotated 59 calls with 
our new tagset using a web interface. The 
distribution of the top 20 tags is listed below. 
The whole list of tags can be found in the 
NineOneOne project webpage: 
http://www.cs.cmu.edu/~911/ 
 
                                               
1
 The dialog is English Translation of  ?s?, mi esposo le falta 
el aire. es ac? en el dos sesenta y uno Oak Street?. It is 
extracted from the transcription of a CCPD 9-1-1 
emergency call, with address modified to protect privacy 
51
Tag (Representation) Frequency 
Yes 227 
Giving-Address 133 
Giving-Location 113 
Giving-Name 107 
No 106 
Other 94 
OK 81 
Thank-You 51 
Reporting-Conflict 43 
Describing-Vehicle 42 
Giving-Telephone-Number 40 
Hello 36 
Reporting-Urgency-Or-Injury 34 
Describing-Residence 28 
Dont-Know 19 
Dont-Understand 16 
Giving-Age 15 
Goodbye 15 
Giving-Medical-Symptoms 14 
Requesting-Police 12 
Table 3. Distribution of top 20 second-pass 
tags 
 
The new tagset is hierarchical, which allows 
us to evaluate the classifier at different levels of 
the hierarchy, and eventually select the best 
trade-off between the number of tags and 
classification accuracy. For example, the first 
level of tags for reporting incidents includes the 
five most common incidents, viz, Reporting-
Conflict, Reporting-Robbery, Reporting-Traffic-
accident, Reporting-Urgency-or-Injury and 
Reporting-Fire. The second level of tags are used 
to convey more detailed information about the 
above incidents (eg. Reporting-Weapons in the 
case of conflict) or rare incidents (eg. Reporting-
Animal-Problem). 
5.3 Second-pass classification and Results 
We also improved our classification 
framework to allow multiple tags for a single 
turn and to easily accomodate any new tags in 
the future. Our earlier DA classification used a 
multi-class classifier, as each turn was restricted 
to have a single tag. To accomodate multiple tags 
for a single turn, we trained binary classifiers for 
each tag. All the utterances of the corresponding 
tag are marked positive examples and the rest are 
marked as negative examples. Our new data set 
has 1140 dialog turns and 1331 annotations. Note 
that the number of annotations is more than the 
number of labelled turns as each turn may have 
multiple tags. We report classification accuracies 
in the following table for each tag based on 10-
fold cross-validation: 
 
Tag (Representation) Accuracy   
   (%) 
Yes 87.32 
Giving-Address 42.71 
Giving-Location 87.32 
Giving-Name 42.71 
No 37.63 
Other 54.98 
OK 72.5 
Thank-You 41.14 
Reporting-Conflict 79.33 
Describing-Vehicle 96.82 
Giving-Telephone-Number 39.37 
Hello 38.79 
Reporting-Urgency-Or-Injury 49.8 
Describing-Residence 92.75 
Dont-Know 41.67 
Dont-Understand 36.03 
Giving-Age 64.95 
Goodbye 87.27 
Giving-Medical-Symptoms 47.44 
Requesting-Police 79.94 
Table 4. Classification accuracies of 
individual second-pass tags 
 
The average accuracy of the 20 tags is 
58.42%. Although multiple classifiers increase 
the computational complexity during run-time, 
they are independent of each other, so we can run 
them in parallel. To ensure the consistency and 
clarity of the new tag set, we had a second 
annotator label 39 calls. The inter-coder 
agreement (Kappa coefficient) between the two 
annotators is 0.67. This is considered substantial 
agreement between the annotators, and confirms 
the consistency of the tag set. 
6 Conclusion 
The work reported here demonstrates that we can 
produce Spanish ASR for Spanish emergency 
calls with reasonable accuracy (76.5%), and clas-
sify manual transcriptions of these calls with rea-
sonable accuracy (60.12% on the original tagset, 
52
58.42% on the new, improved tagset).  We be-
lieve these results are good enough to justify the 
next phase of research, in which we will develop, 
user-test, and evaluate a full pilot system. We are 
also investigating a number of additional tech-
niques to improve the DA classification accura-
cies.  Further we believe that we can design the 
overall dialog system to ameliorate the inevitable 
remaining misclassifications, based in part on the 
confusion matrix of actual errors (Nallasamy et 
al, 2008).  But only actual user tests of a pilot 
system will allow us to know whether an even-
tual deployable system is really feasible. 
Acknowledgements 
This project is funded by NSF Grant No: IIS-
0627957 ?NineOneOne: Exploratory Research 
on Recognizing Non-English Speech for Emer-
gency Triage in Disaster Response?. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the 
authors and do not necessarily reflect the views 
of sponsors. 
References 
Burges C J C, A tutorial on support vector machines 
for pattern recognition, In Proc. Data Mining and 
Knowledge Discovery, pp 2(2):955-974, USA, 
1998 
Finke M, Geutner P, Hild H, Kemp T, Ries K and 
Westphal M, The Karlsruhe-Verbmobil Speech 
Recognition Engine, In Proc. IEEE International 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP), pp. 83-86, Germany, 1997 
Frederking R, Rudnicky A, Hogan C and Lenzo K, 
Interactive Speech Translation in the Diplomat 
Project, Machine Translation Journal 15(1-2), 
Special issue on Spoken Language Translation, pp. 
61-66, USA, 2000 
Gao Y, Zhou B, Sarikaya R, Afify M, Kuo H, Zhu W, 
Deng Y, Prosser C, Zhang W and Besacier L, IBM 
MASTOR SYSTEM: Multilingual Automatic 
Speech-to-Speech Translator, In Proc. First Inter-
national Workshop on Medical Speech Translation, 
pp. 53-56, USA, 2006 
Langley C, Domain Action Classification and Argu-
ment Parsing for Interlingua-based Spoken Lan-
guage Translation. PhD thesis, Carnegie Mellon 
University, Pittsburgh, PA, 2003 
Language Line Services http://www.languageline.com 
Lavie A, Balducci F, Coletti P, Langley C, Lazzari G, 
Pianesi F, Taddei L and Waibel A, Architecture 
and Design Considerations in NESPOLE!: a 
Speech Translation System for E-Commerce Ap-
plications,. In Proc. Human Language Technolo-
gies (HLT), pp 31-34, USA, 2001 
Levin L, Langley C, Lavie A, Gates D, Wallace D and 
Peterson K, Domain Specific Speech Acts for Spo-
ken Language Translation, In Proc. 4th SIGdial 
Workshop on Discourse and Dialogue, pp. 208-
217, Japan, 2003 
Nallasamy U, Black A, Schultz T and Frederking R, 
NineOneOne: Recognizing and Classifying Speech 
for Handling Minority Language Emergency Calls, 
In Proc. 6th International conference on Language 
Resources and Evaluation (LREC), Morocco, 2008 
NineOneOne project webpage 
[www.cs.cmu.edu/~911] 
Schultz T, Westphal M and Waibel A, The 
GlobalPhone Project: Multilingual LVCSR with 
JANUS-3, In Proc. Multilingual Information Re-
trieval Dialogs: 2nd SQEL Workshop, pp. 20-27, 
Czech Republic, 1997 
Soltau H, Metze F, F?ugen C and Waibel A, A One 
Pass-Decoder Based on Polymorphic Linguistic 
Context Assignment, In Proc. IEEE workshop on 
Automatic Speech Recognition and Understanding 
(ASRU), Italy, 2001 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
53
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 34?41,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Exploration of the Impact of Maximum Entropy in Recurrent Neural
Network Language Models for Code-Switching Speech
Ngoc Thang Vu
1,2
and Tanja Schultz
1
1
Karlsruhe Institute of Technology (KIT),
2
University of Munich (LMU), Germany
thangvu@cis.lmu.de, tanja.schultz@kit.edu
Abstract
This paper presents our latest investiga-
tions of the jointly trained maximum en-
tropy and recurrent neural network lan-
guage models for Code-Switching speech.
First, we explore extensively the integra-
tion of part-of-speech tags and language
identifier information in recurrent neu-
ral network language models for Code-
Switching. Second, the importance of
the maximum entropy model is demon-
strated along with a various of experi-
mental results. Finally, we propose to
adapt the recurrent neural network lan-
guage model to different Code-Switching
behaviors and use them to generate artifi-
cial Code-Switching text data.
1 Introduction
The term Code-Switching (CS) denotes speech
which contains more than one language. Speakers
switch their language while they are talking. This
phenomenon appears very often in multilingual
communities, such as in India, Hong Kong or Sin-
gapore. Furthermore, it increasingly occurs in for-
mer monolingual cultures due to the strong growth
of globalization. In many contexts and domains,
speakers switch more often between their native
language and English within their utterances than
in the past. This is a challenge for speech recog-
nition systems which are typically monolingual.
While there have been promising approaches to
handle Code-Switching in the field of acoustic
modeling, language modeling is still a great chal-
lenge. The main reason is a shortage of training
data. Whereas about 50h of training data might
be sufficient for the estimation of acoustic mod-
els, the transcriptions of these data are not enough
to build reliable language models. In this paper,
we focus on exploring and improving the language
model for Code-switching speech and as a result
improve the automatic speech recognition (ASR)
system on Code-Switching speech.
The main contribution of the paper is the exten-
sive investigation of jointly trained maximum en-
tropy (ME) and recurrent neural language models
(RNN LMs) for Code-Switching speech. We re-
visit the integration of part-of-speech (POS) tags
and language identifier (LID) information in recur-
rent neural network language models and the im-
pact of maximum entropy on the language model
performance. As follow-up to our previous work
in (Adel, Vu et al., 2013), here we investigate
whether a recurrent neural network alone without
using ME is a suitable model for Code-Switching
speech. Afterwards, to directly use the RNN LM
in the decoding process of an ASR system, we
convert the RNN LM into the n-gram language
model using the text generation approach (Deoras
et al., 2011; Adel et al., 2014); Furthermore moti-
vated by the fact that Code-Switching is speaker
dependent (Auer, 1999b; Vu et al., 2013), we
first adapt the recurrent neural network language
model to different Code-Switching behaviors and
then generate artificial Code-Switching text data.
This allows us to train an accurate n-gram model
which can be used directly during decoding to im-
prove ASR performance.
The paper is organized as follows: Section 2
gives a short overview of related works. In Sec-
tion 3, we describe the jointly trained maximum
entropy and recurrent neural network language
models and their extension for Code-Switching
speech. Section 4 gives a short description of the
SEAME corpus. In Section 5, we summarize the
most important experiments and results. The study
is concluded in Section 6 with a summary.
2 Related Work
This section gives a brief introduction about the
related research regarding Code-Switching and re-
34
current language models.
In (Muysken, 2000; Poplack, 1978; Bokamba,
1989), the authors observed that code switches
occur at positions in an utterance following syn-
tactical rules of the involved languages. Code-
Switching can be regarded as a speaker depen-
dent phenomenon (Auer, 1999b; Vu et al., 2013).
However, several particular Code-Switching pat-
terns are shared across speakers (Poplack, 1980).
Furthermore, part-of-speech tags might be useful
features to predict Code-Switching points. The
authors of (Solorio et al., 2008b; Solorio et al.,
2008a) investigate several linguistic features, such
as word form, LID, POS tags or the position of
the word relative to the phrase for Code-Switching
prediction. Their best result is obtained by com-
bining all those features. (Chan et al., 2006)
compare four different kinds of n-gram langua-
ge models to predict Code-Switching. They dis-
cover that clustering all foreign words into their
POS classes leads to the best performance. In (Li
et al., 2012; Li et al., 2013), the authors propose
to integrate the equivalence constraint into lan-
guage modeling for Mandarin and English Code-
Switching speech recorded in Hong Kong.
In the last years, neural networks have been
used for a variety of tasks, including language
modeling (Mikolov et al., 2010). Recurrent neu-
ral networks are able to handle long-term contexts
since the input vector does not only contain the
current word but also the previous hidden layer.
It is shown that these networks outperform tradi-
tional language models, such as n-grams which
only contain very limited histories. In (Mikolov
et al., 2011a), the network is extended by factoriz-
ing the output layer into classes to accelerate the
training and testing processes. The input layer
can be augmented to model features, such as POS
tags (Shi et al., 2011; Adel, Vu et al., 2013). Fur-
thermore, artificial text can be automatically gen-
erated using recurrent neural networks to enlarge
the amount of training data (Deoras et al., 2011;
Adel et al., 2014).
3 Joint maximum entropy and recurrent
neural networks language models for
Code-Switching
3.1 Recurrent neural network language
models
The idea of RNN LMs is illustrated in Figure 1.
Vector w(t) forms the input of the recurrent neu-
Figure 1: RNN language model
ral network. It represents the current word using
1-of-N coding. Thus, its dimension equals the
size of the vocabulary. Vector s(t) contains the
state of the network - ?hidden layer?. The network
is trained using back-propagation through time
(BPTT), an extension of the back-propagation
algorithm for recurrent neural networks. With
BPTT, the error is propagated through recurrent
connections back in time for a specific number of
time steps t. Hence, the network is able to capture
a longer history than a traditional n-gram LM. The
matrices U , V and W contain the weights for the
connections between the layers. These weights are
learned during the training phase.
To accelerate the training process, (Mikolov et
al., 2011a) factorized the output layer into classes
based on simple frequency binning. Every word
belongs to exactly one class. Vector c(t) contains
the probabilities for each class and vector w(t)
provides the probabilities for each word given its
class. Hence, the probability P (w
i
|history) is
computed as shown in equation 1.
P (w
i
|history) = P (c
i
|s(t))P (w
i
|c
i
, s(t)) (1)
Furthermore in (Mikolov et al., 2011b), the au-
thors proposed to jointly train the RNN with ME
- RMM-ME - to improve the language model and
also ASR performance. The ME can be seen as
a weight matrix which directly connects the in-
put with the output layer as well as the input with
the class layer. This weight matrix can be trained
jointly with the recurrent neural network. ?Direct-
order? and ?direct connection? are the two impor-
tant parameters which define the length of history
and the number of the trained connections.
35
3.2 Code-Switching language models
To adapt RNN LMs to the Code-Switching task,
(Adel, Vu et al., 2013) analyzed the SEAME cor-
pus and observed that there are words and POS
tags which might have a high potential to predict
Code-Switching points. Therefore, it has been
proposed to integrate the POS and LID informa-
tion into the RNN LM. The idea is to factorize
the output layer into classes which provide lan-
guage information. By doing that, it is intended
to not only predict the next word but also the
next language. Hence according to equation 1, the
probability of the next language is computed first
and then the probability of each word given the
language. In that work, four classes were used:
English, Mandarin, other languages and particles.
Moreover, a vector f(t) which contains the POS
information is added to the input layer. This vec-
tor provides the corresponding POS of the current
word. Thus, not only the current word is activated
but also its features. Since the POS tags are in-
tegrated into the input layer, they are also propa-
gated into the hidden layer and back-propagated
into its history s(t). Hence, not only the previous
features are stored in the history but also features
from several time steps in the past.
In addition to that previous work, the experi-
ments in this paper aim to explore the source of
the improvements observed in (Adel, Vu et al.,
2013). We now clearly distinguish between the
impacts due to the long but unordered history of
the RNN and the effects of the maximum entropy
model which also captures information about the
most recent word and POS tag in the history.
4 SEAME corpus
To conduct research on Code-Switching speech
we use the SEAME corpus (South East Asia
Mandarin-English). It is a conversational
Mandarin-English Code-Switching speech corpus
recorded by (D.C. Lyu et al., 2011). Originally, it
was used for the research project ?Code-Switch?
which was jointly performed by Nanyang Tech-
nological University (NTU) and Karlsruhe Insti-
tute of Technology (KIT) from 2009 until 2012.
The corpus contains 63 hours of audio data which
has been recorded and manually transcribed in
Singapore and Malaysia. The recordings consist
of spontaneously spoken interviews and conver-
sations. The words can be divided into four lan-
guage categories: English words (34.3% of all to-
kens), Mandarin words (58.6%), particles (Singa-
porean and Malayan discourse particles, 6.8% of
all tokens) and others (other languages, 0.4% of
all tokens). In total, the corpus contains 9,210
unique English and 7,471 unique Mandarin words.
The Mandarin character sequences have been seg-
mented into words manually. The language dis-
tribution shows that the corpus does not contain a
clearly predominant language. Furthermore, the
number of Code-Switching points is quite high:
On average, there are 2.6 switches between Man-
darin and English per utterance. Additionally, the
duration of the monolingual segments is rather
short: More than 82% of the English segments and
73% of the Mandarin segments last less than one
second. The average duration of English and Man-
darin segments is only 0.67 seconds and 0.81 sec-
onds, respectively. This corresponds to an average
length of monolingual segments of 1.8 words in
English and 3.6 words in Mandarin.
For the task of language modeling and speech
recognition, the corpus has been divided into three
disjoint sets: training, development and evaluation
set. The data is assigned to the three different sets
based on the following criteria: a balanced distri-
bution of gender, speaking style, ratio of Singa-
porean and Malaysian speakers, ratio of the four
language categories, and the duration in each set.
Table 1 lists the statistics of the SEAME corpus.
Training Dev Eval
# Speakers 139 8 8
Duration(hours) 59.2 2.1 1.5
# Utterances 48,040 1,943 1,029
# Words 575,641 23,293 11,541
Table 1: Statistics of the SEAME corpus
5 Experiments and Results
This section presents all the experiments and re-
sults regarding language models and ASR on the
development and the evaluation set of the SEAME
corpus. However, the parameters were tuned only
on the development set.
5.1 LM experiments
5.1.1 Baseline n-gram
The n-gram language model served as the baseline
in this work. We used the SRI language model
toolkit (Stolcke, 2002) to build the CS 3-gram
baseline from the SEAME training transcriptions
36
containing all words of the transcriptions. Modi-
fied Kneser-Ney smoothing (Rosenfeld, 2000) was
applied. In total, the vocabulary size is around
16k words. The perplexities (PPLs) are 268.4 and
282.9 on the development and evaluation set re-
spectively.
5.1.2 Exploration of ME and of the
integration of POS and LID in RNN
To investigate the effect of POS and LID integra-
tion into the RNN LM and the importance of the
ME, different RNN LMs were trained.
The first experiment aims at investigating the
importance of using LID information for output
layer factorization. All the results are summarized
in table 2. The first RNNLM was trained with a
hidden layer of 50 nodes and without using output
factorization and ME. The PPLs were 250.8 and
301.1 on the development and evaluation set, re-
spectively. We observed some gains in terms of
PPL on the development set but not on the eval-
uation set compared to the n-gram LM. Even us-
ing ME and factorizing the output layer into four
classes based on frequency binning (fb), the same
trend could be noticed - only the PPL on the devel-
opment set was improved. Four classes were used
to have a fair comparison with the output factor-
ization with LID. However after including the LID
information into the output layer, the PPLs were
improved on both data sets. On top of that, using
ME provides some additional gains. The results
indicate that LID is a useful information source
for the Code-Switching task. Furthermore, the im-
provements are independent of the application of
ME.
Model Dev Eval
CS 3-gram 268.4 282.9
RNN LM 250.8 301.1
RNN-ME LM 246.6 287.9
RNN LM with fb 246.0 287.3
RNN-ME LM with fb 256.0 294.0
RNN LM with LID 241.5 274.4
RNN-ME LM with LID 237.9 269.3
Table 2: Effect of output layer factorization
In the second experiment we investigated the
use of POS information and the effect of the ME.
The results in Table 3 show that an integration of
POS without ME did not give any further improve-
ment compared to RNN LM. The reason could lie
in the fact that a RNN can capture a long history
but not the information of the word order. Note
that in the syntactic context, the word order is one
of the most important information. However us-
ing ME allows using the POS of the previous time
step to predict the next language and also the next
word, the PPL was improved significantly on de-
velopment and evaluation set. These results reveal
that POS is a reasonable trigger event which can
be used to support Code-Switching prediction.
Model Dev Eval
CS 3-gram 268.4 282.9
RNN LM 250.8 301.1
RNN-ME LM 246.6 287.9
RNN LM with POS 250.6 298.3
RNN-ME LM with POS 233.5 268.0
Table 3: Effect of ME on the POS integration into
the input layer
Finally, we trained an LM by integrating the
POS tags and factorizing the output layer with LID
information. Again without applying ME, we ob-
served that POS information is not helpful to im-
prove the RNN LM. Using the ME provides a big
gain in terms of PPL on both data sets. We ob-
tained a PPL of 219.8 and 239.2 on the develop-
ment and evaluation set respectively.
Model Dev Eval
CS 3-gram 268.4 282.9
RNN LM 250.8 301.1
RNN-ME LM 246.6 287.9
RNN LM with POS + LID 243.9 277.1
RNN-ME LM with POS+ LID 219.8 239.2
Table 4: Effect of ME on the integration of POS
and the output layer factorization using LID
5.1.3 Training parameters
Moreover, we investigated the effect of different
parameters, such as the backpropagation through
time (BPTT) step, the direct connection order and
the amount of direct connections on the perfor-
mance of the RNN-ME LMs. Therefore, different
LMs were trained with varying values for these
parameters. For each parameter change, the re-
maining parameters were fixed to the most suitable
value which has been found so far.
First, we varied the BPTT step from 1 to 5. The
BPTT step defines the length of the history which
is incorporated to update the weight matrix of the
37
RNN. The larger the BPTT step is, the longer is the
history which is used for learning. Table 5 shows
the perplexities on the SEAME development and
evaluation sets with different BPTT steps. The
results indicate that increasing BPTT might im-
prove the PPL. The best PPL can be obtained with
a BPTT step of 4. The big loss in terms of PPL
by using a BPTT step of 5 indicates that too long
histories might hurt the language model perfor-
mance. Another reason might be the limitation of
the training data.
BPTT 1 2 3 4 5
Dev 244.7 224.6 222.8 219.8 266.8
Eval 281.1 241.4 242.8 239.2 284.5
Table 5: Effect of the BPTT step
It has been shown in the previous section, that
ME is very important to improve the PPL espe-
cially for the Code-Switching task, we also trained
several RNN-ME LMs with various values for ?di-
rect order? and ?direct connection?. Table 6 and
7 summarize the PPL on the SEAME develop-
ment and evaluation set. The results reveal that
the larger the direct order is, the lower is the PPL.
We observed consistent PPL improvement by in-
creasing the direct order. However, the gain seems
to be saturated after a direct order of 3 or 4. In this
paper, we choose to use a direct order of 4 to train
the final model.
Direct order 1 2 3 4
Dev 238.6 231.7 220.5 219.8
Eval 271.8 261.4 240.7 239.2
Table 6: Effect of the direct order
Since the ?direct order? is related to the length
of the context, the size of the ?direct connection? is
a trade off between the size of the language model
and also the amount of the training data. Higher
?direct connection? leads to a larger model and
might improve the PPL if the amount of training
data is enough to train all the direct connection
weights. The results with four different data points
(50M, 100M, 150M and 200M) show that the best
model can be obtained on SEAME data set by us-
ing 100M of direct connection.
5.1.4 Artificial Code-Switching text
generation using RNN
The RNN LM demonstrates a great improvement
over the traditional n-gram language model. How-
#Connection 50M 100M 150M 200M
Dev 226.2 219.8 224.7 224.6
Eval 244.7 239.2 243.7 242.0
Table 7: Effect of the number of direct connections
ever, it is inefficient to use the RNN LM directly
in the decoding process of an ASR system. In or-
der to convert the RNN into a n-gram language
model, a text generation method which was pro-
posed in (Deoras et al., 2011) can be applied.
Moreover, it allows to generate more training data
which might be useful to improve the data sparsity
of the language modeling task for Code-Switching
speech. In (Deoras et al., 2011), the authors ap-
plied the Gibb sampling method to generate artifi-
cial text based on the probability distribution pro-
vided by the RNNs. We applied that technique
in (Adel et al., 2014) to generate Code-Switching
data and were able to improve the PPL and ASR
performance on CS speech. In addition to that pre-
vious work, we now propose to use several Code-
Switching attitude dependent language models in-
stead of the final best RNN LM.
Code-Switching attitude dependent language
modeling Since POS tags might have a potential
to predict Code-Switch points, (Vu et al., 2013)
performed an analysis of these trigger POS tags
on a speaker level. The CS rate for each tag was
computed for each speaker. Afterwards, we calcu-
lated the minimum, maximum and mean values as
well as standard deviations. We observed that the
spread between minimum and maximum values is
quite high for most of the tags. It indicates that al-
though POS information may trigger a CS event,
it is rather speaker dependent.
Motivated by this observation, we performed k-
mean clustering of the training text into three dif-
ferent portions of text data which describe differ-
ent Code-Switching behaviors (Vu et al., 2013).
Afterwards, the LM was adapted with each text
portion to obtain Code-Switching attitude depen-
dent language models. By using these models, we
could improve both PPL and ASR performance for
each speaker.
Artificial text generation To generate artificial
text, we first adapted the best RNN-ME LM de-
scribed in the previous section to three different
Code-Switching attitudes. Afterwards, we gen-
erated three different text corpora based on these
specific Code-Switching attitudes. Each corpus
38
contains 100M tokens. We applied the SRILM
toolkit (Stolcke, 2002) to train n-gram language
model and interpolated them linearly with the
weight =
1
3
. Table 8 shows the perplexity of the
resulting n-gram models on the SEAME develop-
ment and evaluation set. To make a comparison,
we also used the unadapted best RNN-ME LM to
generate two different texts, one with 300M to-
kens and another one with 235M tokens (Adel et
al., 2014). The results show that the n-gram LMs
trained with only the artificial text data can not
outperform the baseline CS 3-gram. However they
provide some complementary information to the
baseline CS 3-gram LM. Therefore, when we in-
terpolated them with the baseline CS 3-gram, the
PPL was improved all the cases. Furthermore by
using the Code-Switching attitude dependent lan-
guage models to generate artificial CS text data,
the PPL was slightly improved compared to using
the unadapted one. The final 3-gram model (Final
3-gram) was built by interpolating all the Code-
Switching attitude dependent 3-gram and the base-
line CS 3-gram. It has a PPL of 249.3 and 266.9
on the development set and evaluation set.
Models Dev Eval
CS 3-gram 268.4 282.9
300M words text 391.3 459.5
+ CS 3-gram 250.0 270.9
235M words text 385.1 454.6
+ CS 3-gram 249.5 270.5
100M words text I 425.4 514.4
+ CS 3-gram 251.4 274.5
100M words text II 391.8 421.6
+ CS 3-gram 251.6 266.4
100M words text III 390.3 428.1
+ CS 3-gram 250.6 266.9
Interpolation of I, II and III 377.5 416.1
+ CS 3-gram (Final n-gram) 249.3 266.9
RNN-ME LM + POS + LID 219.8 239.2
Table 8: PPL of the N-gram models trained with
artificial text data
5.2 ASR experiments
For the ASR experiments, we applied BioKIT, a
dynamic one-pass decoder (Telaar et al., 2014).
The acoustic model is speaker independent and
has been trained with all the training data. To ex-
tract the features, we first trained a multilayer per-
ceptron (MLP) with a small hidden layer with 40
nodes. The output of this hidden layer is called
bottle neck features and is used to train the acous-
tic model. The MLP has been initialized with a
multilingual multilayer perceptron as described in
(Vu et al., 2012). The phone set contains English
and Mandarin phones, filler models for continu-
ous speech (+noise+, +breath+, +laugh+) and an
additional phone +particle+ for Singaporean and
Malayan particles. The acoustic model applied
a fully-continuous 3-state left-to-right HMM. The
emission probabilities were modeled with Gaus-
sian mixture models. We used a context dependent
acoustic model with 3,500 quintphones. Merge-
and-split training was applied followed by six it-
erations of Viterbi training. To obtain a dictio-
nary, the CMU English (CMU Dictionary, 2014)
and Mandarin (Hsiao et al., 2008) pronunciation
dictionaries were merged into one bilingual pro-
nunciation dictionary. Additionally, several rules
from (Chen et al., 2010) were applied which gen-
erate pronunciation variants for Singaporean En-
glish.
As a performance measure for decoding Code-
Switching speech, we used the mixed error rate
(MER) which applies word error rates to En-
glish and character error rates to Mandarin seg-
ments (Vu et al., 2012). With character error
rates for Mandarin, the performance can be com-
pared across different word segmentations. Ta-
ble 9 shows the results of the baseline CS 3-gram
LM, the 3-gram LM trained with 235M artificial
words interpolated with CS 3-gram LM and the fi-
nal 3-gram LM described in the previous section.
Compared to the baseline system, we are able to
improve the MER by up to 3% relative. Further-
more, a very small gain can be observed by using
the Code-Switching attitude dependent language
model compared to the unadapted best RNN-ME
LM.
Model Dev Eval
CS 3-gram 40.0% 34.3%
235M words text + CS-3gram 39.4% 33.4%
Final 3-gram 39.2% 33.3%
Table 9: ASR results on SEAME data
6 Conclusion
This paper presents an extensive investigation of
the impact of maximum entropy in recurrent neu-
ral network language models for Code-Switching
39
speech. The experimental results reveal that fac-
torization of the output layer of the RNN us-
ing LID always improved the PPL independent
whether the ME is used. However, the integra-
tion of the POS tags into the input layer only im-
proved the PPL in combination with ME. The best
LM can be obtained by jointly training the ME
and the RNN LM with POS integration and fac-
torization using LID. Moreover, using the RNN-
ME LM allows generating artificial CS text data
and therefore training an n-gram LM which car-
ries the information of the RNN-ME LM. This can
be directly used during decoding to improve ASR
performance on Code-Switching speech. On the
SEAME development and evaluation set, we ob-
tained an improvement of up to 18% relative in
terms of PPL and 3% relative in terms of MER.
7 Acknowledgment
This follow-up work on exploring the impact of
maximum entropy in recurrent neural network lan-
guage models for Code-Switching speech was mo-
tivated by the very useful comments and sugges-
tions of the SLSP reviewers, for which we are very
grateful.
References
H. Adel, N.T. Vu, F. Kraus, T. Schlippe, and T. Schultz.
Recurrent Neural Network Language Modeling for
Code Switching Conversational Speech In: Pro-
ceedings of ICASSP 2013.
H. Adel, K. Kirchhoff, N.T. Vu, D.Telaar, T. Schultz
Comparing Approaches to Convert Recurrent Neu-
ral Networks into Backoff Language Models For Ef-
ficient Decoding In: Proceedings of Interspeech
2014.
P. Auer Code-Switching in Conversation Routledge
1999.
P. Auer From codeswitching via language mixing to
fused lects toward a dynamic typology of bilingual
speech In: International Journal of Bilingualism,
vol. 3, no. 4, pp. 309-332, 1999.
E.G. Bokamba Are there syntactic constraints on code-
mixing? In: World Englishes, vol. 8, no. 3, pp. 277-
292, 1989.
J.Y.C. Chan, PC Ching, T. Lee, and H. Cao Au-
tomatic speech recognition of Cantonese-English
code-mixing utterances In: Proceeding of Inter-
speech 2006.
W. Chen, Y. Tan, E. Chng, H. Li The development of a
Singapore English call resource In: Proceedings of
Oriental COCOSDA, 2010.
Carnegie Mellon University CMU pronoun-
cation dictionary for English Online:
http://www.speech.cs.cmu.edu/cgi-bin/cmudict,
retrieved in July 2014
D.C. Lyu, T.P. Tan, E.S. Cheng, H. Li An Analysis of
Mandarin-English Code-Switching Speech Corpus:
SEAME In: Proceedings of Interspeech 2011.
A. Deoras, T. Mikolov, S. Kombrink, M. Karafiat, S.
Khudanpur Variational approximation of long-span
language models for LVCSR In: Proceedings of
ICASSP 2011.
R. Hsiao, M. Fuhs, Y. Tam, Q. Jin, T. Schultz The
CMU-InterACT 2008 Mandarin transcription sys-
tem In: Procceedings of ICASSP 2008.
Y. Li, P. Fung Code-Switch Language Model with
Inversion Constraints for Mixed Language Speech
Recognition In: Proceedings of COLING 2012.
Y. Li, P. Fung Improved mixed language speech recog-
nition using asymmetric acoustic model and lan-
guage model with Code-Switch inversion constraints
In: Proceedings of ICASSP 2013.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
Building a large annotated corpus of english: The
penn treebank In: Computational Linguistics, vol.
19, no. 2, pp. 313-330, 1993.
T. Mikolov, M. Karafiat, L. Burget, J. Jernocky and S.
Khudanpur. Recurrent Neural Network based Lan-
guage Model In: Proceedings of Interspeech 2010.
T. Mikolov, S. Kombrink, L. Burget, J. Jernocky and
S. Khudanpur. Extensions of Recurrent Neural Net-
work Language Model In: Proceedings of ICASSP
2011.
T. Mikolov, A. Deoras, D. Povey, L. Burget, J.H. Cer-
nocky Strategies for Training Large Scale Neu-
ral Network Language Models In: Proceedings of
ASRU 2011.
P. Muysken Bilingual speech: A typology of code-
mixing In: Cambridge University Press, vol. 11.
S. Poplack Syntactic structure and social function
of code-switching , Centro de Estudios Puertor-
riquenos, City University of New York.
S. Poplack Sometimes i?ll start a sentence in spanish
y termino en espanol: toward a typology of code-
switching In: Linguistics, vol. 18, no. 7-8, pp. 581-
618.
D. Povey, A. Ghoshal, et al. The Kaldi speech recogni-
tion toolkit In: Proceedings of ASRU 2011.
R. Rosenfeld Two decades of statistical language mod-
eling: Where do we go from here? In: Proceedings
of the IEEE 88.8 (2000): 1270-1278.
T. Schultz, P. Fung, and C. Burgmer, Detecting code-
switch events based on textual features.
40
Y. Shi, P. Wiggers, M. Jonker Towards Recurrent Neu-
ral Network Language Model with Linguistics and
Contextual Features In: Proceedings of Interspeech
2011.
T. Solorio, Y. Liu Part-of-speech tagging for English-
Spanish code-switched text In: Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics, 2008.
T. Solorio, Y. Liu Learning to predict code-switching
points In: Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing. As-
sociation for Computational Linguistics, 2008.
A. Stolcke SRILM-an extensible language modeling
toolkit. In: Proceedings of Interspeech 2012.
D. Telaar, et al. BioKIT - Real-time Decoder For
Biosignal Processing In: Proceedings of Inter-
speech 2014.
N.T. Vu, D.C. Lyu, J. Weiner, D. Telaar, T. Schlippe,
F. Blaicher, E.S. Chng, T. Schultz, H. Li A First
Speech Recognition System For Mandarin-English
Code-Switch Conversational Speech In: Proceed-
ings of Interspeech 2012.
N.T. Vu, H. Adel, T. Schultz An Investigation of Code-
Switching Attitude Dependent Language Modeling
In: In Statistical Language and Speech Processing,
First International Conference, 2013.
N.T. Vu, F. Metze, T. Schultz Multilingual bottleneck
features and its application for under-resourced lan-
guages In: Proceedings of SLTU, 2012.
41
