Proceedings of NAACL HLT 2009: Short Papers, pages 245?248,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Quadratic Features and Deep Architectures for Chunking
Joseph Turian and James Bergstra and Yoshua Bengio
Dept. IRO, Universite? de Montre?al
Abstract
We experiment with several chunking models.
Deeper architectures achieve better gener-
alization. Quadratic filters, a simplification
of a theoretical model of V1 complex cells,
reliably increase accuracy. In fact, logistic
regression with quadratic filters outperforms
a standard single hidden layer neural network.
Adding quadratic filters to logistic regression
is almost as effective as feature engineering.
Despite predicting each output label indepen-
dently, our model is competitive with ones
that use previous decisions.
1 Introduction
There are three general approaches to improving
chunking performance: engineer better features,
improve inference, and improve the model.
Manual feature engineering is a common direc-
tion. One technique is to take primitive features
and manually compound them. This technique is
common, and most NLP systems use n-gram based
features (Carreras and Ma`rquez, 2003; Ando and
Zhang, 2005, for example). Another approach is
linguistically motivated feature engineering, e.g.
Charniak and Johnson (2005).
Other works have looked in the direction of
improving inference. Rather than predicting each
decision independently, previous decisions can be
included in the inference process. In this work,
we use the simplest approach of modeling each
decision independently.
The third direction is by using a better model. If
modeling capacity can be added without introducing
too many extra degrees of freedom, generalization
could be improved. One approach for compactly
increasing capacity is to automatically induce
intermediate features through the composition of
non-linearities, for example SVMs with a non-linear
kernel (Kudo and Matsumoto, 2001), inducing
compound features in a CRF (McCallum, 2003),
neural networks (Henderson, 2004; Bengio and Le-
Cun, 2007), and boosting decision trees (Turian and
Melamed, 2006). Recently, Bergstra et al (2009)
showed that capacity can be increased by adding
quadratic filters, leading to improved generalization
on vision tasks. This work examines how well
quadratic filters work for an NLP task. Compared to
manual feature engineering, improved models are
appealing because they are less task-specific.
We experiment on the task of chunking (Sang and
Buchholz, 2000), a syntactic sequence labeling task.
2 Sequence labeling
Besides Collobert and Weston (2008), previous
work on sequence labeling usually use previous
decisions in predicting output labels. Here we do
not take advantage of the dependency between suc-
cessive output labels. Our approach predicts each
output label independently of the others. This allows
us to ignore inference during training: The model
maximizes the conditional likelihood of each output
label independent of the output label of other tokens.
We use a sliding window approach. The output
label for a particular focus token xi is predicted
based upon k? tokens before and after xi. The entire
window is of size k = 2 ? k? + 1. Nearly all work on
sequence labeling uses a sliding window approach
(Kudo and Matsumoto, 2001; Zhang et al, 2002;
245
204 204 204 204 204 204 204
150 150 150 150 150 150 150tok
win
out
in
400
23
?h
?q
?q
Figure 1: Illustration of our baseline I-T-W-O model (see
Secs. 4 and 5.1). The input layer comprises seven tokens
with 204 dimensions each. Each token is passed through
a shared 150-dimensional token feature extractor. These
7 ? 150 features are concatenated and 400 features are
extracted from them in the window layer. These 400 fea-
tures are the input to the final 23-class output prediction.
Feature extractors ?q and ?h are described in Section 3.
Carreras and Ma`rquez, 2003; Ando and Zhang,
2005, for example). We assume that each token x
can be transformed into a real-valued feature vector
?(x) with l entries. The feature function will be
described in Section 4.
A standard approach is as follows: We first
concatenate the features of k tokens into one vector
[?(xi?k?), . . . , ?(xi+k?)] of length k ? l entries. We can
then pass [?(xi?k?), . . . , ?(xi+k?)] to a feature extractor
over the entire window followed by an output
log-linear layer.
Convolutional architectures can help when there
is a position-invariant aspect to the input. In machine
vision, parameters related to one part of the image
are sometimes restricted to be equal to parameters
related to another part (LeCun et al, 1998). A
convolutional approach to sequence labeling is as
follows: At the lowest layer we extract features from
individual tokens using a shared feature extractor.
These higher-level individual token features are then
concatenated, and are passed to a feature extractor
over the entire window.
In our baseline approach, we apply one convolu-
tional layer of feature extraction to each token (one
token layer), followed by a concatenation, followed
by one layer of feature extraction over the entire
window (one window layer), followed by a 23-D
output prediction using multiclass logistic regres-
sion. We abbreviate this architecture as I-T-W-O
(inputtokenwindowoutput). See Figure 1 for
an illustration of this architecture.
3 Quadratic feature extractors
The most common feature extractor in the literature
is a linear filter h followed by a non-linear squashing
(activation) function ?:
f (x) = ?(h(x)), h(x) = b +Wx. (1)
In our experiments, we use the softsign squash-
ing function ?(z) = z/(1 + |z|). n-class lo-
gistic regression predicts ?(h(x)), where softmax
?i(z) = exp(zi)/?k exp(zk). Rust et al (2005) argues
that complex cells in the V1 area of visual cortex
are not well explained by Eq. 1, but are instead
better explained by a model that includes quadratic
interactions between regions of the receptive field.
Bergstra et al (2009) approximate the model of
Rust et al (2005) with a simpler model of the
form given in Eq. 2.? In this model, the pre-squash
transformation q includes J quadratic filters:
f (x) = ?(q(x)), q(x) =
?
?????????
b +Wx +
?
? J?
j=1
(V jx)2
?
?????????
(2)
where b,W, and V1 . . .VJ are tunable parameters.
In the vision experiments of Bergstra et al
(2009), using quadratic filters improved the gen-
eralization of the trained architecture. We were
interested to see if the increased capacity would
also be beneficial in language tasks. For our logistic
regression (I-O) experiments, the architecture is
specifically I??qO, i.e. output O is the softmax
? applied to the quadratic transform q of the input
I. Like Bergstra et al (2009), in architectures with
hidden layers, we apply the quadratic transform q
in all layers except the final layer, which uses linear
transform h. For example, I-T-W-O is specifically
I??qT??qW??hO, as shown in Figure 1. Future
work will explore if generalization is improved by
using q in the final layer.
4 Features
Here is a detailed description of the types of features
we use, with number of dimensions:
? embeddings. We map each word to a real-valued
50-dimensional embedding. These embeddings
were obtained by Collobert and Weston (2008), and
? Bergstra et al (2009) do not use a sqrt in Eq. 2. We found that
sqrt improves optimization and gives better generalization.
246
were induced based upon a purely unsupervised
training strategy over the 631 million words in the
English Wikipedia.
? POS-tag. Part-of-speech tags were assigned auto-
matically, and are part of the CoNLL data. 45 dim.
? label frequencies. Frequency of each label
assigned to this word in the training and validation
data. From Ando and Zhang (2005). 23 dim.
? type(first character). The type of the first charac-
ter of the word. type(x) = ?A? if x is a capital letter,
?a? if x is a lowercase letter, ?n? if x is a digit, and x
otherwise. From Collins (2002). 20 dim.
? word length. The length of the word. 20 dim.
? compressed word type. We convert each char-
acter of the word into its type. We then remove any
repeated consecutive type. For example, ?Label-
making? ? ?Aa-a?. From Collins (2002). 46 dim.
The last three feature types are based upon ortho-
graphic information. There is a combined total of
204 features per token.
5 Experiments
We follow the conditions in the CoNLL-2000
shared task (Sang and Buchholz, 2000). Of the 8936
training sentences, we used 1000 randomly sampled
sentences (23615 words) for validation.
5.1 Training details
The optimization criterion used during training is
the maximization of the sum (over word positions)
of the per-token log-likelihood of the correct deci-
sion. Stochastic gradient descent is performed using
a fixed learning rate ? and early stopping. Gradients
are estimated using a minibatch of 8 examples. We
found that a learning rate of 0.01, 0.0032, or 0.001
was most effective.
In all our experiments we use a window size
of 7 tokens. In preliminary experiments, smaller
windows yielded poorer results, and larger ones
were no better. Layer sizes of extracted features
were chosen to optimize validation F1.
5.2 Results
We report chunk F-measure (F1). In some tables
we also report Acc, the per-token label accuracy,
post-Viterbi decoding.
Figure 2 shows that using quadratic filters reliably
improves generalization on all architectures. For
the I-T-W-O architecture, quadratic filters increase
91.5%
92%
92.5%
93%
93.5%
94%
94.5%
95%
0  1  2  4  8  1691.5%
92%
92.5%
93%
93.5%
94%
94.5%
95%
# of quadratic filters
I-T-W-O (baseline)I-W-O (1 hidden layer NN)I-O (LogReg)
Figure 2: Validation F1 (y-axis) as we vary the number
of quadratic filters (x-axis), over different model archi-
tectures. Both architecture depth and quadratic filters
improve validation F1.
Architecture #qf Acc F1
I-O 16 96.45 93.94
I-W(400)-O 4 96.66 94.39
I-T(150)-W(566)-O 2 96.85 94.77
I-T(150)-W(310)-W(310)-O 4 96.87 94.82
Table 1: Architecture experiments on validation data.
The first column describes the layers in the architecture.
(The architecture in Figure 1 is I-T(150)-W(400)-O.)
The second column gives the number of quadratic filters.
For each architecture, the layer sizes and number of
quadratic filters are chosen to maximize validation F1.
Deeper architectures achieve higher F1 scores.
validation F1 by an absolute 0.31. Most surpris-
ingly, logistic regression with 16 filters achieves
F1=93.94, which outperforms the 93.83 of a stan-
dard (0 filter) single hidden layer neural network.
With embeddings as the only features, logreg
with 0 filters achieves F1=85.36. By adding all
features, we can raise the F1 to 91.96. Alternately,
by adding 16 filters, we can raise the F1 to 91.60. In
other words, adding filters is nearly as effective as
our manual feature engineering.
Table 1 shows the result of varying the overall
architecture. Deeper architectures achieve higher
F1 scores. Table 2 compares the model as we lesion
off different features. POS tags and the embeddings
were the most important features.
We applied our best model overall (I-T-W-W-O
in Table 1) to the test data. Results are shown in
247
Feature set Acc F1
default 96.81 94.69
no orthographic features 96.84 94.62
no label frequencies 96.77 94.58
no POS tags 96.60 94.22
no embeddings 96.40 93.97
only embeddings 96.18 93.53
Table 2: Results on validation of varying the feature set,
for the architecture in Figure 1 with 4 quadratic filters.
NP F1 Prc Rcl F1
AZ05 94.70 94.57 94.20 94.39
KM01 94.39 93.89 93.92 93.91
I-T-W-W-O 94.44 93.72 93.91 93.81
CM03 94.41 94.19 93.29 93.74
SP03 94.38 - - -
Mc03 93.96 - - -
AZ05- - 93.83 93.37 93.60
ZDJ02 93.89 93.54 93.60 93.57
Table 3: Test set results for Ando and Zhang (2005), Kudo
and Matsumoto (2001), our I-T-W-W-O model, Carreras
and Ma`rquez (2003), Sha and Pereira (2003), McCallum
(2003), Zhang et al (2002), and our best I-O model.
AZ05- is Ando and Zhang (2005) using purely supervised
training, not semi-supervised training. Scores are noun
phrase F1, and overall chunk precision, recall, and F1.
Table 3. We are unable to compare to Collobert and
Weston (2008) because they use a different training
and test set. Our model predicts all labels in the
sequence independently. All other works in Table 3
use previous decisions when making the current
label decision. Our approach is nonetheless compet-
itive with approaches that use this extra information.
6 Conclusions
Many NLP approaches underfit important linguistic
phenomena. We experimented with new techniques
for increasing chunker model capacity: adding
depth (automatically inducing intermediate features
through the composition of non-linearities), and
including quadratic filters. Higher accuracy was
achieved by deeper architectures, i.e. ones with
more intermediate layers of automatically tuned fea-
ture extractors. Although they are a simplification of
a theoretical model of V1 complex cells, quadratic
filters reliably improved generalization in all archi-
tectures. Most surprisingly, logistic regression with
quadratic filters outperformed a single hidden layer
neural network without. Also, with logistic regres-
sion, adding quadratic filters was almost as effective
as manual feature engineering. Despite predicting
each output label independently, our model is
competitive with ones that use previous decisions.
Acknowledgments
Thank you to Ronan Collobert, Le?on Bottou, and
NEC Labs for access to their word embeddings, and
to NSERC and MITACS for financial support.
References
R. Ando and T. Zhang. A high-performance semi-
supervised learning method for text chunking. In ACL,
2005.
Y. Bengio and Y. LeCun. Scaling learning algorithms
towards AI. In Large Scale Kernel Machines. 2007.
J. Bergstra, G. Desjardins, P. Lamblin, and Y. Bengio.
Quadratic polynomials learn better image features. TR
1337, DIRO, Universite? de Montre?al, 2009.
X. Carreras and L. Ma`rquez. Phrase recognition by
filtering and ranking with perceptrons. In RANLP, 2003.
E. Charniak and M. Johnson. Coarse-to-fine n-best pars-
ing and MaxEnt discriminative reranking. In ACL, 2005.
M. Collins. Ranking algorithms for named entity extrac-
tion: Boosting and the voted perceptron. In ACL, 2002.
R. Collobert and J. Weston. A unified architecture for
natural language processing: Deep neural networks with
multitask learning. In ICML, 2008.
J. Henderson. Discriminative training of a neural
network statistical parser. In ACL, 2004.
T. Kudo and Y. Matsumoto. Chunking with support
vector machines. In NAACL, 2001.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient
based learning applied to document recognition. IEEE,
86(11):2278?2324, November 1998.
A. McCallum. Efficiently inducing features of condi-
tional random fields. In UAI, 2003.
N. Rust, O. Schwartz, J. A. Movshon, and E. Simoncelli.
Spatiotemporal elements of macaque V1 receptive fields.
Neuron, 46(6):945?956, 2005.
E. T. Sang and S. Buchholz. Introduction to the
CoNLL-2000 shared task: Chunking. In CoNLL, 2000.
F. Sha and F. C. N. Pereira. Shallow parsing with
conditional random fields. In HLT-NAACL, 2003.
J. Turian and I. D. Melamed. Advances in discriminative
parsing. In ACL, 2006.
T. Zhang, F. Damerau, and D. Johnson. Text chunking
based on a generalization of Winnow. JMLR, 2, 2002.
248
Unsupervised Sense Disambiguation Using Bilingual Probabilistic Models
Indrajit Bhattacharya
Dept. of Computer Science
University of Maryland
College Park, MD,
USA
indrajit@cs.umd.edu
Lise Getoor
Dept. of Computer Science
University of Maryland
College Park, MD,
USA
getoor@cs.umd.edu
Yoshua Bengio
Dept. IRO
Universit?e de Montr?eal
Montr?eal, Qu?ebec,
Canada
bengioy@IRO.UMontreal.CA
Abstract
We describe two probabilistic models for unsuper-
vised word-sense disambiguation using parallel cor-
pora. The first model, which we call the Sense
model, builds on the work of Diab and Resnik
(2002) that uses both parallel text and a sense in-
ventory for the target language, and recasts their ap-
proach in a probabilistic framework. The second
model, which we call the Concept model, is a hier-
archical model that uses a concept latent variable to
relate different language specific sense labels. We
show that both models improve performance on the
word sense disambiguation task over previous unsu-
pervised approaches, with the Concept model show-
ing the largest improvement. Furthermore, in learn-
ing the Concept model, as a by-product, we learn a
sense inventory for the parallel language.
1 Introduction
Word sense disambiguation (WSD) has been a cen-
tral question in the computational linguistics com-
munity since its inception. WSD is fundamental to
natural language understanding and is a useful in-
termediate step for many other language process-
ing tasks (Ide and Veronis, 1998). Many recent
approaches make use of ideas from statistical ma-
chine learning; the availability of shared sense defi-
nitions (e.g. WordNet (Fellbaum, 1998)) and recent
international competitions (Kilgarrif and Rosen-
zweig, 2000) have enabled researchers to compare
their results. Supervised approaches which make
use of a small hand-labeled training set (Bruce
and Wiebe, 1994; Yarowsky, 1993) typically out-
perform unsupervised approaches (Agirre et al,
2000; Litkowski, 2000; Lin, 2000; Resnik, 1997;
Yarowsky, 1992; Yarowsky, 1995), but tend to be
tuned to a specific corpus and are constrained by
scarcity of labeled data.
In an effort to overcome the difficulty of find-
ing sense-labeled training data, researchers have be-
gun investigating unsupervised approaches to word-
sense disambiguation. For example, the use of par-
allel corpora for sense tagging can help with word
sense disambiguation (Brown et al, 1991; Dagan,
1991; Dagan and Itai, 1994; Ide, 2000; Resnik and
Yarowsky, 1999). As an illustration of sense disam-
biguation from translation data, when the English
word bank is translated to Spanish as orilla, it is
clear that we are referring to the shore sense of bank,
rather than the nancial institution sense.
The main inspiration for our work is Diab and
Resnik (2002), who use translations and linguistic
knowledge for disambiguation and automatic sense
tagging. Bengio and Kermorvant (2003) present
a graphical model that is an attempt to formalize
probabilistically the main ideas in Diab and Resnik
(2002). They assume the same semantic hierarchy
(in particular, WordNet) for both the languages and
assign English words as well as their translations
to WordNet synsets. Here we present two variants
of the graphical model in Bengio and Kermorvant
(2003), along with a method to discover a cluster
structure for the Spanish senses. We also present
empirical word sense disambiguation results which
demonstrate the gain brought by this probabilistic
approach, even while only using the translated word
to provide disambiguation information.
Our first generative model, the Sense Model,
groups semantically related words from the two
languages into senses, and translations are gener-
ated by probabilistically choosing a sense and then
words from the sense. We show that this improves
on the results of Diab and Resnik (2002).
Our next model, which we call the Concept
Model, aims to improve on the above sense struc-
ture by modeling the senses of the two languages
separately and relating senses from both languages
through a higher-level, semantically less precise
concept. The intuition here is that not all of the
senses that are possible for a word will be relevant
for a concept. In other words, the distribution over
the senses of a word given a concept can be expected
to have a lower entropy than the distribution over
the senses of the word in the language as a whole.
In this paper, we look at translation data as a re-
source for identification of semantic concepts. Note
that actual translated word pairs are not always good
matches semantically, because the translation pro-
cess is not on a word by word basis. This intro-
duces a kind of noise in the translation, and an addi-
tional hidden variable to represent the shared mean-
ing helps to take it into account. Improved perfor-
mance over the Sense Model validates the use of
concepts in modeling translations.
An interesting by-product of the Concept Model
is a semantic structure for the secondary language.
This is automatically constructed using background
knowledge of the structure for the primary language
and the observed translation pairs. In the model,
words sharing the same sense are synonyms while
senses under the same concept are semantically re-
lated in the corpus. An investigation of the model
trained over real data reveals that it can indeed
group related words together.
It may be noted that predicting senses from trans-
lations need not necessarily be an end result in it-
self. As we have already mentioned, lack of labeled
data is a severe hindrance for supervised approaches
to word sense disambiguation. At the same time,
there is an abundance of bilingual documents and
many more can potentially be mined from the web.
It should be possible using our approach to (noisily)
assign sense tags to words in such documents, thus
providing huge resources of labeled data for super-
vised approaches to make use of.
For the rest of this paper, for simplicity we will
refer to the primary language of the parallel docu-
ment as English and to the secondary as Spanish.
The paper is organized as follows. We begin by for-
mally describing the models in Section 2. We de-
scribe our approach for constructing the senses and
concepts in Section 3. Our algorithm for learning
the model parameters is described in Section 4. We
present experimental results in Section 5 and our
analysis in Section 6. We conclude in Section 7.
2 Probabilistic Models for Parallel
Corpora
We motivate the use of a probabilistic model by il-
lustrating that disambiguation using translations is
possible even when a word has a unique transla-
tion. For example, according to WordNet, the word
prevention has two senses in English, which may
be abbreviated as hindrance (the act of hindering
or obstruction) and control (by prevention, e.g. the
control of a disease). It has a single translation in
our corpus, that being prevenci ?on. The first En-
glish sense, hindrance, also has other words like
bar that occur in the corpus and all of these other
words are observed to be translated in Spanish as
the word obstrucci?on. In addition, none of these
other words translate to prevenci ?on. So it is not
unreasonable to suppose that the intended sense for
prevention when translated as prevenci ?on is differ-
ent from that of bar. Therefore, the intended sense
is most likely to be control. At the very heart of
the reasoning is probabilistic analysis and indepen-
dence assumptions. We are assuming that senses
and words have certain occurrence probabilities and
that the choice of the word can be made indepen-
dently once the sense has been decided. This is the
flavor that we look to add to modeling parallel doc-
uments for sense disambiguation. We formally de-
scribe the two generative models that use these ideas
in Subsections 2.2 and 2.3.
T
We Ws
Te Ts
C
WsWeword
concept
sense
b) Concept Modela) Sense Model
Figure 1: Graphical Representations of the a) Sense
Model and the b) Concept Model
2.1 Notation
Throughout, we use uppercase letters to denote ran-
dom variables and lowercase letters to denote spe-
cific instances of the random variables. A transla-
tion pair is (   ,   ) where the subscript  and 
indicate the primary language (English) and the sec-
ondary language (Spanish).   	


and   
  
 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724?1734,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning Phrase Representations using RNN Encoder?Decoder
for Statistical Machine Translation
Kyunghyun Cho
Bart van Merri
?
enboer Caglar Gulcehre
Universit?e de Montr?eal
firstname.lastname@umontreal.ca
Dzmitry Bahdanau
Jacobs University, Germany
d.bahdanau@jacobs-university.de
Fethi Bougares Holger Schwenk
Universit?e du Maine, France
firstname.lastname@lium.univ-lemans.fr
Yoshua Bengio
Universit?e de Montr?eal, CIFAR Senior Fellow
find.me@on.the.web
Abstract
In this paper, we propose a novel neu-
ral network model called RNN Encoder?
Decoder that consists of two recurrent
neural networks (RNN). One RNN en-
codes a sequence of symbols into a fixed-
length vector representation, and the other
decodes the representation into another se-
quence of symbols. The encoder and de-
coder of the proposed model are jointly
trained to maximize the conditional prob-
ability of a target sequence given a source
sequence. The performance of a statisti-
cal machine translation system is empiri-
cally found to improve by using the con-
ditional probabilities of phrase pairs com-
puted by the RNN Encoder?Decoder as an
additional feature in the existing log-linear
model. Qualitatively, we show that the
proposed model learns a semantically and
syntactically meaningful representation of
linguistic phrases.
1 Introduction
Deep neural networks have shown great success in
various applications such as objection recognition
(see, e.g., (Krizhevsky et al., 2012)) and speech
recognition (see, e.g., (Dahl et al., 2012)). Fur-
thermore, many recent works showed that neu-
ral networks can be successfully used in a num-
ber of tasks in natural language processing (NLP).
These include, but are not limited to, language
modeling (Bengio et al., 2003), paraphrase detec-
tion (Socher et al., 2011) and word embedding ex-
traction (Mikolov et al., 2013). In the field of sta-
tistical machine translation (SMT), deep neural
networks have begun to show promising results.
(Schwenk, 2012) summarizes a successful usage
of feedforward neural networks in the framework
of phrase-based SMT system.
Along this line of research on using neural net-
works for SMT, this paper focuses on a novel neu-
ral network architecture that can be used as a part
of the conventional phrase-based SMT system.
The proposed neural network architecture, which
we will refer to as an RNN Encoder?Decoder, con-
sists of two recurrent neural networks (RNN) that
act as an encoder and a decoder pair. The en-
coder maps a variable-length source sequence to a
fixed-length vector, and the decoder maps the vec-
tor representation back to a variable-length target
sequence. The two networks are trained jointly to
maximize the conditional probability of the target
sequence given a source sequence. Additionally,
we propose to use a rather sophisticated hidden
unit in order to improve both the memory capacity
and the ease of training.
The proposed RNN Encoder?Decoder with a
novel hidden unit is empirically evaluated on the
task of translating from English to French. We
train the model to learn the translation probabil-
ity of an English phrase to a corresponding French
phrase. The model is then used as a part of a stan-
dard phrase-based SMT system by scoring each
phrase pair in the phrase table. The empirical eval-
uation reveals that this approach of scoring phrase
pairs with an RNN Encoder?Decoder improves
the translation performance.
We qualitatively analyze the trained RNN
Encoder?Decoder by comparing its phrase scores
with those given by the existing translation model.
The qualitative analysis shows that the RNN
Encoder?Decoder is better at capturing the lin-
guistic regularities in the phrase table, indirectly
explaining the quantitative improvements in the
overall translation performance. The further anal-
ysis of the model reveals that the RNN Encoder?
Decoder learns a continuous space representation
of a phrase that preserves both the semantic and
syntactic structure of the phrase.
1724
2 RNN Encoder?Decoder
2.1 Preliminary: Recurrent Neural Networks
A recurrent neural network (RNN) is a neural net-
work that consists of a hidden state h and an
optional output y which operates on a variable-
length sequence x = (x
1
, . . . , x
T
). At each time
step t, the hidden state h
?t?
of the RNN is updated
by
h
?t?
= f
(
h
?t?1?
, x
t
)
, (1)
where f is a non-linear activation func-
tion. f may be as simple as an element-
wise logistic sigmoid function and as com-
plex as a long short-term memory (LSTM)
unit (Hochreiter and Schmidhuber, 1997).
An RNN can learn a probability distribution
over a sequence by being trained to predict the
next symbol in a sequence. In that case, the output
at each timestep t is the conditional distribution
p(x
t
| x
t?1
, . . . , x
1
). For example, a multinomial
distribution (1-of-K coding) can be output using a
softmax activation function
p(x
t,j
= 1 | x
t?1
, . . . , x
1
) =
exp
(
w
j
h
?t?
)
?
K
j
?
=1
exp
(
w
j
?
h
?t?
)
,
(2)
for all possible symbols j = 1, . . . ,K, where w
j
are the rows of a weight matrix W. By combining
these probabilities, we can compute the probabil-
ity of the sequence x using
p(x) =
T
?
t=1
p(x
t
| x
t?1
, . . . , x
1
). (3)
From this learned distribution, it is straightfor-
ward to sample a new sequence by iteratively sam-
pling a symbol at each time step.
2.2 RNN Encoder?Decoder
In this paper, we propose a novel neural network
architecture that learns to encode a variable-length
sequence into a fixed-length vector representation
and to decode a given fixed-length vector rep-
resentation back into a variable-length sequence.
From a probabilistic perspective, this new model
is a general method to learn the conditional dis-
tribution over a variable-length sequence condi-
tioned on yet another variable-length sequence,
e.g. p(y
1
, . . . , y
T
?
| x
1
, . . . , x
T
), where one
x1 x2 xT
yT' y2 y1
c
Decoder
Encoder
Figure 1: An illustration of the proposed RNN
Encoder?Decoder.
should note that the input and output sequence
lengths T and T
?
may differ.
The encoder is an RNN that reads each symbol
of an input sequence x sequentially. As it reads
each symbol, the hidden state of the RNN changes
according to Eq. (1). After reading the end of
the sequence (marked by an end-of-sequence sym-
bol), the hidden state of the RNN is a summary c
of the whole input sequence.
The decoder of the proposed model is another
RNN which is trained to generate the output se-
quence by predicting the next symbol y
t
given the
hidden state h
?t?
. However, unlike the RNN de-
scribed in Sec. 2.1, both y
t
and h
?t?
are also con-
ditioned on y
t?1
and on the summary c of the input
sequence. Hence, the hidden state of the decoder
at time t is computed by,
h
?t?
= f
(
h
?t?1?
, y
t?1
, c
)
,
and similarly, the conditional distribution of the
next symbol is
P (y
t
|y
t?1
, y
t?2
, . . . , y
1
, c) = g
(
h
?t?
, y
t?1
, c
)
.
for given activation functions f and g (the latter
must produce valid probabilities, e.g. with a soft-
max).
See Fig. 1 for a graphical depiction of the pro-
posed model architecture.
The two components of the proposed RNN
Encoder?Decoder are jointly trained to maximize
the conditional log-likelihood
max
?
1
N
N
?
n=1
log p
?
(y
n
| x
n
), (4)
1725
where ? is the set of the model parameters and
each (x
n
,y
n
) is an (input sequence, output se-
quence) pair from the training set. In our case,
as the output of the decoder, starting from the in-
put, is differentiable, we can use a gradient-based
algorithm to estimate the model parameters.
Once the RNN Encoder?Decoder is trained, the
model can be used in two ways. One way is to use
the model to generate a target sequence given an
input sequence. On the other hand, the model can
be used to score a given pair of input and output
sequences, where the score is simply a probability
p
?
(y | x) from Eqs. (3) and (4).
2.3 Hidden Unit that Adaptively Remembers
and Forgets
In addition to a novel model architecture, we also
propose a new type of hidden unit (f in Eq. (1))
that has been motivated by the LSTM unit but is
much simpler to compute and implement.
1
Fig. 2
shows the graphical depiction of the proposed hid-
den unit.
Let us describe how the activation of the j-th
hidden unit is computed. First, the reset gate r
j
is
computed by
r
j
= ?
(
[W
r
x]
j
+
[
U
r
h
?t?1?
]
j
)
, (5)
where ? is the logistic sigmoid function, and [.]
j
denotes the j-th element of a vector. x and h
t?1
are the input and the previous hidden state, respec-
tively. W
r
and U
r
are weight matrices which are
learned.
Similarly, the update gate z
j
is computed by
z
j
= ?
(
[W
z
x]
j
+
[
U
z
h
?t?1?
]
j
)
. (6)
The actual activation of the proposed unit h
j
is
then computed by
h
?t?
j
= z
j
h
?t?1?
j
+ (1? z
j
)
?
h
?t?
j
, (7)
where
?
h
?t?
j
= ?
(
[Wx]
j
+
[
U
(
r h
?t?1?
)]
j
)
. (8)
In this formulation, when the reset gate is close
to 0, the hidden state is forced to ignore the pre-
vious hidden state and reset with the current input
1
The LSTM unit, which has shown impressive results in
several applications such as speech recognition, has a mem-
ory cell and four gating units that adaptively control the in-
formation flow inside the unit, compared to only two gating
units in the proposed hidden unit. For details on LSTM net-
works, see, e.g., (Graves, 2012).
z
r
h h
~ x
Figure 2: An illustration of the proposed hidden
activation function. The update gate z selects
whether the hidden state is to be updated with
a new hidden state
?
h. The reset gate r decides
whether the previous hidden state is ignored. See
Eqs. (5)?(8) for the detailed equations of r, z, h
and
?
h.
only. This effectively allows the hidden state to
drop any information that is found to be irrelevant
later in the future, thus, allowing a more compact
representation.
On the other hand, the update gate controls how
much information from the previous hidden state
will carry over to the current hidden state. This
acts similarly to the memory cell in the LSTM
network and helps the RNN to remember long-
term information. Furthermore, this may be con-
sidered an adaptive variant of a leaky-integration
unit (Bengio et al., 2013).
As each hidden unit has separate reset and up-
date gates, each hidden unit will learn to capture
dependencies over different time scales. Those
units that learn to capture short-term dependencies
will tend to have reset gates that are frequently ac-
tive, but those that capture longer-term dependen-
cies will have update gates that are mostly active.
In our preliminary experiments, we found that
it is crucial to use this new unit with gating units.
We were not able to get meaningful result with an
oft-used tanh unit without any gating.
3 Statistical Machine Translation
In a commonly used statistical machine translation
system (SMT), the goal of the system (decoder,
specifically) is to find a translation f given a source
sentence e, which maximizes
p(f | e) ? p(e | f)p(f),
where the first term at the right hand side is called
translation model and the latter language model
(see, e.g., (Koehn, 2005)). In practice, however,
most SMT systems model log p(f | e) as a log-
linear model with additional features and corre-
1726
sponding weights:
log p(f | e) =
N
?
n=1
w
n
f
n
(f , e) + logZ(e), (9)
where f
n
and w
n
are the n-th feature and weight,
respectively. Z(e) is a normalization constant that
does not depend on the weights. The weights are
often optimized to maximize the BLEU score on a
development set.
In the phrase-based SMT framework
introduced in (Koehn et al., 2003) and
(Marcu and Wong, 2002), the translation model
log p(e | f) is factorized into the translation
probabilities of matching phrases in the source
and target sentences.
2
These probabilities are
once again considered additional features in the
log-linear model (see Eq. (9)) and are weighted
accordingly to maximize the BLEU score.
Since the neural net language model was pro-
posed in (Bengio et al., 2003), neural networks
have been used widely in SMT systems. In
many cases, neural networks have been used to
rescore translation hypotheses (n-best lists) (see,
e.g., (Schwenk et al., 2006)). Recently, however,
there has been interest in training neural networks
to score the translated sentence (or phrase pairs)
using a representation of the source sentence as
an additional input. See, e.g., (Schwenk, 2012),
(Son et al., 2012) and (Zou et al., 2013).
3.1 Scoring Phrase Pairs with RNN
Encoder?Decoder
Here we propose to train the RNN Encoder?
Decoder (see Sec. 2.2) on a table of phrase pairs
and use its scores as additional features in the log-
linear model in Eq. (9) when tuning the SMT de-
coder.
When we train the RNN Encoder?Decoder, we
ignore the (normalized) frequencies of each phrase
pair in the original corpora. This measure was
taken in order (1) to reduce the computational ex-
pense of randomly selecting phrase pairs from a
large phrase table according to the normalized fre-
quencies and (2) to ensure that the RNN Encoder?
Decoder does not simply learn to rank the phrase
pairs according to their numbers of occurrences.
One underlying reason for this choice was that the
existing translation probability in the phrase ta-
ble already reflects the frequencies of the phrase
2
Without loss of generality, from here on, we refer to
p(e | f) for each phrase pair as a translation model as well
pairs in the original corpus. With a fixed capacity
of the RNN Encoder?Decoder, we try to ensure
that most of the capacity of the model is focused
toward learning linguistic regularities, i.e., distin-
guishing between plausible and implausible trans-
lations, or learning the ?manifold? (region of prob-
ability concentration) of plausible translations.
Once the RNN Encoder?Decoder is trained, we
add a new score for each phrase pair to the exist-
ing phrase table. This allows the new scores to en-
ter into the existing tuning algorithm with minimal
additional overhead in computation.
As Schwenk pointed out in (Schwenk, 2012),
it is possible to completely replace the existing
phrase table with the proposed RNN Encoder?
Decoder. In that case, for a given source phrase,
the RNN Encoder?Decoder will need to generate
a list of (good) target phrases. This requires, how-
ever, an expensive sampling procedure to be per-
formed repeatedly. In this paper, thus, we only
consider rescoring the phrase pairs in the phrase
table.
3.2 Related Approaches: Neural Networks in
Machine Translation
Before presenting the empirical results, we discuss
a number of recent works that have proposed to
use neural networks in the context of SMT.
Schwenk in (Schwenk, 2012) proposed a simi-
lar approach of scoring phrase pairs. Instead of the
RNN-based neural network, he used a feedforward
neural network that has fixed-size inputs (7 words
in his case, with zero-padding for shorter phrases)
and fixed-size outputs (7 words in the target lan-
guage). When it is used specifically for scoring
phrases for the SMT system, the maximum phrase
length is often chosen to be small. However, as the
length of phrases increases or as we apply neural
networks to other variable-length sequence data,
it is important that the neural network can han-
dle variable-length input and output. The pro-
posed RNN Encoder?Decoder is well-suited for
these applications.
Similar to (Schwenk, 2012), Devlin et al.
(Devlin et al., 2014) proposed to use a feedfor-
ward neural network to model a translation model,
however, by predicting one word in a target phrase
at a time. They reported an impressive improve-
ment, but their approach still requires the maxi-
mum length of the input phrase (or context words)
to be fixed a priori.
1727
Although it is not exactly a neural network they
train, the authors of (Zou et al., 2013) proposed
to learn a bilingual embedding of words/phrases.
They use the learned embedding to compute the
distance between a pair of phrases which is used
as an additional score of the phrase pair in an SMT
system.
In (Chandar et al., 2014), a feedforward neural
network was trained to learn a mapping from a
bag-of-words representation of an input phrase to
an output phrase. This is closely related to both the
proposed RNN Encoder?Decoder and the model
proposed in (Schwenk, 2012), except that their in-
put representation of a phrase is a bag-of-words.
A similar approach of using bag-of-words repre-
sentations was proposed in (Gao et al., 2013) as
well. Earlier, a similar encoder?decoder model us-
ing two recursive neural networks was proposed
in (Socher et al., 2011), but their model was re-
stricted to a monolingual setting, i.e. the model
reconstructs an input sentence. More recently, an-
other encoder?decoder model using an RNN was
proposed in (Auli et al., 2013), where the de-
coder is conditioned on a representation of either
a source sentence or a source context.
One important difference between the pro-
posed RNN Encoder?Decoder and the approaches
in (Zou et al., 2013) and (Chandar et al., 2014) is
that the order of the words in source and tar-
get phrases is taken into account. The RNN
Encoder?Decoder naturally distinguishes between
sequences that have the same words but in a differ-
ent order, whereas the aforementioned approaches
effectively ignore order information.
The closest approach related to the proposed
RNN Encoder?Decoder is the Recurrent Contin-
uous Translation Model (Model 2) proposed in
(Kalchbrenner and Blunsom, 2013). In their pa-
per, they proposed a similar model that consists
of an encoder and decoder. The difference with
our model is that they used a convolutional n-gram
model (CGM) for the encoder and the hybrid of
an inverse CGM and a recurrent neural network
for the decoder. They, however, evaluated their
model on rescoring the n-best list proposed by the
conventional SMT system and computing the per-
plexity of the gold standard translations.
4 Experiments
We evaluate our approach on the English/French
translation task of the WMT?14 workshop.
4.1 Data and Baseline System
Large amounts of resources are available to build
an English/French SMT system in the framework
of the WMT?14 translation task. The bilingual
corpora include Europarl (61M words), news com-
mentary (5.5M), UN (421M), and two crawled
corpora of 90M and 780M words respectively.
The last two corpora are quite noisy. To train
the French language model, about 712M words of
crawled newspaper material is available in addi-
tion to the target side of the bitexts. All the word
counts refer to French words after tokenization.
It is commonly acknowledged that training sta-
tistical models on the concatenation of all this
data does not necessarily lead to optimal per-
formance, and results in extremely large mod-
els which are difficult to handle. Instead, one
should focus on the most relevant subset of the
data for a given task. We have done so by
applying the data selection method proposed in
(Moore and Lewis, 2010), and its extension to bi-
texts (Axelrod et al., 2011). By these means we
selected a subset of 418M words out of more
than 2G words for language modeling and a
subset of 348M out of 850M words for train-
ing the RNN Encoder?Decoder. We used the
test set newstest2012 and 2013 for data
selection and weight tuning with MERT, and
newstest2014 as our test set. Each set has
more than 70 thousand words and a single refer-
ence translation.
For training the neural networks, including the
proposed RNN Encoder?Decoder, we limited the
source and target vocabulary to the most frequent
15,000 words for both English and French. This
covers approximately 93% of the dataset. All the
out-of-vocabulary words were mapped to a special
token ([UNK]).
The baseline phrase-based SMT system was
built using Moses with default settings. This sys-
tem achieves a BLEU score of 30.64 and 33.3 on
the development and test sets, respectively (see Ta-
ble 1).
4.1.1 RNN Encoder?Decoder
The RNN Encoder?Decoder used in the experi-
ment had 1000 hidden units with the proposed
gates at the encoder and at the decoder. The in-
put matrix between each input symbol x
?t?
and the
hidden unit is approximated with two lower-rank
matrices, and the output matrix is approximated
1728
Models
BLEU
dev test
Baseline 30.64 33.30
RNN 31.20 33.87
CSLM + RNN 31.48 34.64
CSLM + RNN + WP 31.50 34.54
Table 1: BLEU scores computed on the develop-
ment and test sets using different combinations of
approaches. WP denotes a word penalty, where
we penalizes the number of unknown words to
neural networks.
similarly. We used rank-100 matrices, equivalent
to learning an embedding of dimension 100 for
each word. The activation function used for
?
h in
Eq. (8) is a hyperbolic tangent function. The com-
putation from the hidden state in the decoder to
the output is implemented as a deep neural net-
work (Pascanu et al., 2014) with a single interme-
diate layer having 500 maxout units each pooling
2 inputs (Goodfellow et al., 2013).
All the weight parameters in the RNN Encoder?
Decoder were initialized by sampling from an
isotropic zero-mean (white) Gaussian distribution
with its standard deviation fixed to 0.01, except
for the recurrent weight parameters. For the re-
current weight matrices, we first sampled from a
white Gaussian distribution and used its left singu-
lar vectors matrix, following (Saxe et al., 2014).
We used Adadelta and stochastic gradient
descent to train the RNN Encoder?Decoder
with hyperparameters  = 10
?6
and ? =
0.95 (Zeiler, 2012). At each update, we used 64
randomly selected phrase pairs from a phrase ta-
ble (which was created from 348M words). The
model was trained for approximately three days.
Details of the architecture used in the experi-
ments are explained in more depth in the supple-
mentary material.
4.1.2 Neural Language Model
In order to assess the effectiveness of scoring
phrase pairs with the proposed RNN Encoder?
Decoder, we also tried a more traditional approach
of using a neural network for learning a target
language model (CSLM) (Schwenk, 2007). Espe-
cially, the comparison between the SMT system
using CSLM and that using the proposed approach
of phrase scoring by RNN Encoder?Decoder will
clarify whether the contributions from multiple
neural networks in different parts of the SMT sys-
tem add up or are redundant.
We trained the CSLM model on 7-grams
from the target corpus. Each input word
was projected into the embedding space R
512
,
and they were concatenated to form a 3072-
dimensional vector. The concatenated vector was
fed through two rectified layers (of size 1536 and
1024) (Glorot et al., 2011). The output layer was
a simple softmax layer (see Eq. (2)). All the
weight parameters were initialized uniformly be-
tween ?0.01 and 0.01, and the model was trained
until the validation perplexity did not improve for
10 epochs. After training, the language model
achieved a perplexity of 45.80. The validation set
was a random selection of 0.1% of the corpus. The
model was used to score partial translations dur-
ing the decoding process, which generally leads to
higher gains in BLEU score than n-best list rescor-
ing (Vaswani et al., 2013).
To address the computational complexity of
using a CSLM in the decoder a buffer was
used to aggregate n-grams during the stack-
search performed by the decoder. Only when
the buffer is full, or a stack is about to
be pruned, the n-grams are scored by the
CSLM. This allows us to perform fast matrix-
matrix multiplication on GPU using Theano
(Bergstra et al., 2010; Bastien et al., 2012).
?60 ?50 ?40 ?30 ?20 ?10 0?14
?12
?10
?8
?6
?4
?2
0
RNN Scores (log)
TM 
Scor
es (lo
g)
Figure 3: The visualization of phrase pairs ac-
cording to their scores (log-probabilities) by the
RNN Encoder?Decoder and the translation model.
4.2 Quantitative Analysis
We tried the following combinations:
1. Baseline configuration
2. Baseline + RNN
3. Baseline + CSLM + RNN
4. Baseline + CSLM + RNN + Word penalty
1729
Source Translation Model RNN Encoder?Decoder
at the end of the [a la fin de la] [?r la fin des ann?ees] [?etre sup-
prim?es `a la fin de la]
[`a la fin du] [`a la fin des] [`a la fin de la]
for the first time [r
c
? pour la premir?ere fois] [?et?e donn?es pour
la premi`ere fois] [?et?e comm?emor?ee pour la
premi`ere fois]
[pour la premi`ere fois] [pour la premi`ere fois ,]
[pour la premi`ere fois que]
in the United States
and
[? aux ?tats-Unis et] [?et?e ouvertes aux
?
Etats-
Unis et] [?et?e constat?ees aux
?
Etats-Unis et]
[aux Etats-Unis et] [des Etats-Unis et] [des
?
Etats-Unis et]
, as well as [?s , qu?] [?s , ainsi que] [?re aussi bien que] [, ainsi qu?] [, ainsi que] [, ainsi que les]
one of the most [?t ?l? un des plus] [?l? un des plus] [?etre retenue
comme un de ses plus]
[l? un des] [le] [un des]
(a) Long, frequent source phrases
Source Translation Model RNN Encoder?Decoder
, Minister of Commu-
nications and Trans-
port
[Secr?etaire aux communications et aux trans-
ports :] [Secr?etaire aux communications et aux
transports]
[Secr?etaire aux communications et aux trans-
ports] [Secr?etaire aux communications et aux
transports :]
did not comply with
the
[vestimentaire , ne correspondaient pas `a des]
[susmentionn?ee n? ?etait pas conforme aux]
[pr?esent?ees n? ?etaient pas conformes `a la]
[n? ont pas respect?e les] [n? ?etait pas conforme
aux] [n? ont pas respect?e la]
parts of the world . [
c
? gions du monde .] [r?egions du monde con-
sid?er?ees .] [r?egion du monde consid?er?ee .]
[parties du monde .] [les parties du monde .]
[des parties du monde .]
the past few days . [le petit texte .] [cours des tout derniers jours .]
[les tout derniers jours .]
[ces derniers jours .] [les derniers jours .] [cours
des derniers jours .]
on Friday and Satur-
day
[vendredi et samedi `a la] [vendredi et samedi `a]
[se d?eroulera vendredi et samedi ,]
[le vendredi et le samedi] [le vendredi et samedi]
[vendredi et samedi]
(b) Long, rare source phrases
Table 2: The top scoring target phrases for a small set of source phrases according to the translation
model (direct translation probability) and by the RNN Encoder?Decoder. Source phrases were randomly
selected from phrases with 4 or more words. ? denotes an incomplete (partial) character. r is a Cyrillic
letter ghe.
The results are presented in Table 1. As ex-
pected, adding features computed by neural net-
works consistently improves the performance over
the baseline performance.
The best performance was achieved when we
used both CSLM and the phrase scores from the
RNN Encoder?Decoder. This suggests that the
contributions of the CSLM and the RNN Encoder?
Decoder are not too correlated and that one can
expect better results by improving each method in-
dependently. Furthermore, we tried penalizing the
number of words that are unknown to the neural
networks (i.e. words which are not in the short-
list). We do so by simply adding the number of
unknown words as an additional feature the log-
linear model in Eq. (9).
3
However, in this case we
3
To understand the effect of the penalty, consider the set
of all words in the 15,000 large shortlist, SL. All words x
i
/?
SL are replaced by a special token [UNK] before being scored
by the neural networks. Hence, the conditional probability of
any x
i
t
/? SL is actually given by the model as
p (x
t
= [UNK] | x
<t
) = p (x
t
/? SL | x
<t
)
=
X
x
j
t
/?SL
p
?
x
j
t
| x
<t
?
? p
?
x
i
t
| x
<t
?
,
where x
<t
is a shorthand notation for x
t?1
, . . . , x
1
.
were not able to achieve better performance on the
test set, but only on the development set.
4.3 Qualitative Analysis
In order to understand where the performance im-
provement comes from, we analyze the phrase pair
scores computed by the RNN Encoder?Decoder
against the corresponding p(f | e) from the trans-
lation model. Since the existing translation model
relies solely on the statistics of the phrase pairs in
the corpus, we expect its scores to be better esti-
mated for the frequent phrases but badly estimated
for rare phrases. Also, as we mentioned earlier
in Sec. 3.1, we further expect the RNN Encoder?
Decoder which was trained without any frequency
information to score the phrase pairs based rather
on the linguistic regularities than on the statistics
of their occurrences in the corpus.
We focus on those pairs whose source phrase is
long (more than 3 words per source phrase) and
As a result, the probability of words not in the shortlist is
always overestimated. It is possible to address this issue by
backing off to an existing model that contain non-shortlisted
words (see (Schwenk, 2007)) In this paper, however, we opt
for introducing a word penalty instead, which counteracts the
word probability overestimation.
1730
Source Samples from RNN Encoder?Decoder
at the end of the [`a la fin de la] (?11)
for the first time [pour la premi`ere fois] (?24) [pour la premi`ere fois que] (?2)
in the United States and [aux
?
Etats-Unis et] (?6) [dans les
?
Etats-Unis et] (?4)
, as well as [, ainsi que] [,] [ainsi que] [, ainsi qu?] [et UNK]
one of the most [l? un des plus] (?9) [l? un des] (?5) [l? une des plus] (?2)
(a) Long, frequent source phrases
Source Samples from RNN Encoder?Decoder
, Minister of Communica-
tions and Transport
[ , ministre des communications et le transport] (?13)
did not comply with the [n? tait pas conforme aux] [n? a pas respect l?] (?2) [n? a pas respect la] (?3)
parts of the world . [arts du monde .] (?11) [des arts du monde .] (?7)
the past few days . [quelques jours .] (?5) [les derniers jours .] (?5) [ces derniers jours .] (?2)
on Friday and Saturday [vendredi et samedi] (?5) [le vendredi et samedi] (?7) [le vendredi et le samedi] (?4)
(b) Long, rare source phrases
Table 3: Samples generated from the RNN Encoder?Decoder for each source phrase used in Table 2. We
show the top-5 target phrases out of 50 samples. They are sorted by the RNN Encoder?Decoder scores.
Figure 4: 2?D embedding of the learned word representation. The left one shows the full embedding
space, while the right one shows a zoomed-in view of one region (color?coded). For more plots, see the
supplementary material.
frequent. For each such source phrase, we look
at the target phrases that have been scored high
either by the translation probability p(f | e) or
by the RNN Encoder?Decoder. Similarly, we per-
form the same procedure with those pairs whose
source phrase is long but rare in the corpus.
Table 2 lists the top-3 target phrases per source
phrase favored either by the translation model
or by the RNN Encoder?Decoder. The source
phrases were randomly chosen among long ones
having more than 4 or 5 words.
In most cases, the choices of the target phrases
by the RNN Encoder?Decoder are closer to ac-
tual or literal translations. We can observe that the
RNN Encoder?Decoder prefers shorter phrases in
general.
Interestingly, many phrase pairs were scored
similarly by both the translation model and the
RNN Encoder?Decoder, but there were as many
other phrase pairs that were scored radically dif-
ferent (see Fig. 3). This could arise from the
proposed approach of training the RNN Encoder?
Decoder on a set of unique phrase pairs, discour-
aging the RNN Encoder?Decoder from learning
simply the frequencies of the phrase pairs from the
corpus, as explained earlier.
Furthermore, in Table 3, we show for each of
the source phrases in Table 2, the generated sam-
ples from the RNN Encoder?Decoder. For each
source phrase, we generated 50 samples and show
the top-five phrases accordingly to their scores.
We can see that the RNN Encoder?Decoder is
able to propose well-formed target phrases with-
out looking at the actual phrase table. Importantly,
the generated phrases do not overlap completely
with the target phrases from the phrase table. This
encourages us to further investigate the possibility
of replacing the whole or a part of the phrase table
1731
Figure 5: 2?D embedding of the learned phrase representation. The top left one shows the full represen-
tation space (5000 randomly selected points), while the other three figures show the zoomed-in view of
specific regions (color?coded).
with the proposed RNN Encoder?Decoder in the
future.
4.4 Word and Phrase Representations
Since the proposed RNN Encoder?Decoder is not
specifically designed only for the task of machine
translation, here we briefly look at the properties
of the trained model.
It has been known for some time that
continuous space language models using
neural networks are able to learn seman-
tically meaningful embeddings (See, e.g.,
(Bengio et al., 2003; Mikolov et al., 2013)). Since
the proposed RNN Encoder?Decoder also projects
to and maps back from a sequence of words into
a continuous space vector, we expect to see a
similar property with the proposed model as well.
The left plot in Fig. 4 shows the 2?D embedding
of the words using the word embedding matrix
learned by the RNN Encoder?Decoder. The pro-
jection was done by the recently proposed Barnes-
Hut-SNE (van der Maaten, 2013). We can clearly
see that semantically similar words are clustered
with each other (see the zoomed-in plots in Fig. 4).
The proposed RNN Encoder?Decoder naturally
generates a continuous-space representation of a
phrase. The representation (c in Fig. 1) in this
case is a 1000-dimensional vector. Similarly to the
word representations, we visualize the representa-
tions of the phrases that consists of four or more
words using the Barnes-Hut-SNE in Fig. 5.
From the visualization, it is clear that the RNN
Encoder?Decoder captures both semantic and syn-
tactic structures of the phrases. For instance, in
the bottom-left plot, most of the phrases are about
the duration of time, while those phrases that are
syntactically similar are clustered together. The
bottom-right plot shows the cluster of phrases that
are semantically similar (countries or regions). On
the other hand, the top-right plot shows the phrases
that are syntactically similar.
5 Conclusion
In this paper, we proposed a new neural network
architecture, called an RNN Encoder?Decoder
that is able to learn the mapping from a sequence
1732
of an arbitrary length to another sequence, possi-
bly from a different set, of an arbitrary length. The
proposed RNN Encoder?Decoder is able to either
score a pair of sequences (in terms of a conditional
probability) or generate a target sequence given a
source sequence. Along with the new architecture,
we proposed a novel hidden unit that includes a re-
set gate and an update gate that adaptively control
how much each hidden unit remembers or forgets
while reading/generating a sequence.
We evaluated the proposed model with the task
of statistical machine translation, where we used
the RNN Encoder?Decoder to score each phrase
pair in the phrase table. Qualitatively, we were
able to show that the new model is able to cap-
ture linguistic regularities in the phrase pairs well
and also that the RNN Encoder?Decoder is able to
propose well-formed target phrases.
The scores by the RNN Encoder?Decoder were
found to improve the overall translation perfor-
mance in terms of BLEU scores. Also, we
found that the contribution by the RNN Encoder?
Decoder is rather orthogonal to the existing ap-
proach of using neural networks in the SMT sys-
tem, so that we can improve further the perfor-
mance by using, for instance, the RNN Encoder?
Decoder and the neural net language model to-
gether.
Our qualitative analysis of the trained model
shows that it indeed captures the linguistic regu-
larities in multiple levels i.e. at the word level as
well as phrase level. This suggests that there may
be more natural language related applications that
may benefit from the proposed RNN Encoder?
Decoder.
The proposed architecture has large potential
for further improvement and analysis. One ap-
proach that was not investigated here is to re-
place the whole, or a part of the phrase table by
letting the RNN Encoder?Decoder propose target
phrases. Also, noting that the proposed model is
not limited to being used with written language,
it will be an important future research to apply the
proposed architecture to other applications such as
speech transcription.
Acknowledgments
KC, BM, CG, DB and YB would like to thank
NSERC, Calcul Qu?ebec, Compute Canada, the
Canada Research Chairs and CIFAR. FB and HS
were partially funded by the European Commis-
sion under the project MateCat, and by DARPA
under the BOLT project.
References
[Auli et al.2013] Michael Auli, Michel Galley, Chris
Quirk, and Geoffrey Zweig. 2013. Joint language
and translation modeling with recurrent neural net-
works. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1044?1054.
[Axelrod et al.2011] Amittai Axelrod, Xiaodong He,
and Jianfeng Gao. 2011. Domain adaptation via
pseudo in-domain data selection. In Proceedings of
the ACL Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 355?362.
[Bastien et al.2012] Fr?ed?eric Bastien, Pascal Lamblin,
Razvan Pascanu, James Bergstra, Ian J. Goodfellow,
Arnaud Bergeron, Nicolas Bouchard, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. Deep Learning and Unsupervised Fea-
ture Learning NIPS 2012 Workshop.
[Bengio et al.2003] Yoshua Bengio, R?ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neu-
ral probabilistic language model. J. Mach. Learn.
Res., 3:1137?1155, March.
[Bengio et al.2013] Y. Bengio, N. Boulanger-
Lewandowski, and R. Pascanu. 2013. Advances
in optimizing recurrent networks. In Proceedings
of the 38th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP 2013),
May.
[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: a CPU
and GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
[Chandar et al.2014] Sarath Chandar, Stanislas Lauly,
Hugo Larochelle, Mitesh Khapra, Balaraman Ravin-
dran, Vikas Raykar, and Amrita Saha. 2014. An au-
toencoder approach to learning bilingual word repre-
sentations. arXiv:1402.1454 [cs.CL], Febru-
ary.
[Dahl et al.2012] George E. Dahl, Dong Yu, Li Deng,
and Alex Acero. 2012. Context-dependent pre-
trained deep neural networks for large vocabulary
speech recognition. IEEE Transactions on Audio,
Speech, and Language Processing, 20(1):33?42.
[Devlin et al.2014] Jacob Devlin, Rabih Zbib,
Zhongqiang Huang, Thomas Lamar, Richard
Schwartz, , and John Makhoul. 2014. Fast and
robust neural network joint models for statistical
machine translation. In Proceedings of the ACL
2014 Conference, ACL ?14, pages 1370?1380.
1733
[Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau
Yih, and Li Deng. 2013. Learning semantic repre-
sentations for the phrase translation model. Techni-
cal report, Microsoft Research.
[Glorot et al.2011] X. Glorot, A. Bordes, and Y. Ben-
gio. 2011. Deep sparse rectifier neural networks. In
AISTATS?2011.
[Goodfellow et al.2013] Ian J. Goodfellow, David
Warde-Farley, Mehdi Mirza, Aaron Courville, and
Yoshua Bengio. 2013. Maxout networks. In
ICML?2013.
[Graves2012] Alex Graves. 2012. Supervised Se-
quence Labelling with Recurrent Neural Networks.
Studies in Computational Intelligence. Springer.
[Hochreiter and Schmidhuber1997] S. Hochreiter and
J. Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735?1780.
[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Two recurrent continuous
translation models. In Proceedings of the ACL Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1700?1709.
[Koehn et al.2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54.
[Koehn2005] P. Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Machine
Translation Summit X, pages 79?86, Phuket, Thai-
land.
[Krizhevsky et al.2012] Alex Krizhevsky, Ilya
Sutskever, and Geoffrey Hinton. 2012. Ima-
geNet classification with deep convolutional neural
networks. In Advances in Neural Information
Processing Systems 25 (NIPS?2012).
[Marcu and Wong2002] Daniel Marcu and William
Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Pro-
ceedings of the ACL-02 Conference on Empirical
Methods in Natural Language Processing - Volume
10, EMNLP ?02, pages 133?139.
[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and
their compositionality. In Advances in Neural Infor-
mation Processing Systems 26, pages 3111?3119.
[Moore and Lewis2010] Robert C. Moore and William
Lewis. 2010. Intelligent selection of language
model training data. In Proceedings of the ACL
2010 Conference Short Papers, ACLShort ?10,
pages 220?224, Stroudsburg, PA, USA.
[Pascanu et al.2014] R. Pascanu, C. Gulcehre, K. Cho,
and Y. Bengio. 2014. How to construct deep recur-
rent neural networks. In Proceedings of the Second
International Conference on Learning Representa-
tions (ICLR 2014), April.
[Saxe et al.2014] Andrew M. Saxe, James L. McClel-
land, and Surya Ganguli. 2014. Exact solutions
to the nonlinear dynamics of learning in deep lin-
ear neural networks. In Proceedings of the Second
International Conference on Learning Representa-
tions (ICLR 2014), April.
[Schwenk et al.2006] Holger Schwenk, Marta R. Costa-
Juss`a, and Jos?e A. R. Fonollosa. 2006. Continuous
space language models for the iwslt 2006 task. In
IWSLT, pages 166?173.
[Schwenk2007] Holger Schwenk. 2007. Continuous
space language models. Comput. Speech Lang.,
21(3):492?518, July.
[Schwenk2012] Holger Schwenk. 2012. Continuous
space translation models for phrase-based statisti-
cal machine translation. In Martin Kay and Chris-
tian Boitet, editors, Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLIN), pages 1071?1080.
[Socher et al.2011] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Andrew Y. Ng, and Christopher D.
Manning. 2011. Dynamic pooling and unfolding
recursive autoencoders for paraphrase detection. In
Advances in Neural Information Processing Systems
24.
[Son et al.2012] Le Hai Son, Alexandre Allauzen, and
Franc?ois Yvon. 2012. Continuous space transla-
tion models with neural networks. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ?12,
pages 39?48, Stroudsburg, PA, USA.
[van der Maaten2013] Laurens van der Maaten. 2013.
Barnes-hut-sne. In Proceedings of the First Inter-
national Conference on Learning Representations
(ICLR 2013), May.
[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. De-
coding with large-scale neural language models im-
proves translation. Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1387?1392.
[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
an adaptive learning rate method. Technical report,
arXiv 1212.5701.
[Zou et al.2013] Will Y. Zou, Richard Socher,
Daniel M. Cer, and Christopher D. Manning.
2013. Bilingual word embeddings for phrase-based
machine translation. In Proceedings of the ACL
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1393?1398.
1734
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384?394,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Word representations:
A simple and general method for semi-supervised learning
Joseph Turian
De?partement d?Informatique et
Recherche Ope?rationnelle (DIRO)
Universite? de Montre?al
Montre?al, Que?bec, Canada, H3T 1J4
lastname@iro.umontreal.ca
Lev Ratinov
Department of
Computer Science
University of Illinois at
Urbana-Champaign
Urbana, IL 61801
ratinov2@uiuc.edu
Yoshua Bengio
De?partement d?Informatique et
Recherche Ope?rationnelle (DIRO)
Universite? de Montre?al
Montre?al, Que?bec, Canada, H3T 1J4
bengioy@iro.umontreal.ca
Abstract
If we take an existing supervised NLP sys-
tem, a simple and general way to improve
accuracy is to use unsupervised word
representations as extra word features. We
evaluate Brown clusters, Collobert and
Weston (2008) embeddings, and HLBL
(Mnih & Hinton, 2009) embeddings
of words on both NER and chunking.
We use near state-of-the-art supervised
baselines, and find that each of the three
word representations improves the accu-
racy of these baselines. We find further
improvements by combining different
word representations. You can download
our word features, for off-the-shelf use
in existing NLP systems, as well as our
code, here: http://metaoptimize.
com/projects/wordreprs/
1 Introduction
By using unlabelled data to reduce data sparsity
in the labeled training data, semi-supervised
approaches improve generalization accuracy.
Semi-supervised models such as Ando and Zhang
(2005), Suzuki and Isozaki (2008), and Suzuki
et al (2009) achieve state-of-the-art accuracy.
However, these approaches dictate a particular
choice of model and training regime. It can be
tricky and time-consuming to adapt an existing su-
pervised NLP system to use these semi-supervised
techniques. It is preferable to use a simple and
general method to adapt existing supervised NLP
systems to be semi-supervised.
One approach that is becoming popular is
to use unsupervised methods to induce word
features?or to download word features that have
already been induced?plug these word features
into an existing system, and observe a significant
increase in accuracy. But which word features are
good for what tasks? Should we prefer certain
word features? Can we combine them?
A word representation is a mathematical object
associated with each word, often a vector. Each
dimension?s value corresponds to a feature and
might even have a semantic or grammatical
interpretation, so we call it a word feature.
Conventionally, supervised lexicalized NLP ap-
proaches take a word and convert it to a symbolic
ID, which is then transformed into a feature vector
using a one-hot representation: The feature vector
has the same length as the size of the vocabulary,
and only one dimension is on. However, the
one-hot representation of a word suffers from data
sparsity: Namely, for words that are rare in the
labeled training data, their corresponding model
parameters will be poorly estimated. Moreover,
at test time, the model cannot handle words that
do not appear in the labeled training data. These
limitations of one-hot word representations have
prompted researchers to investigate unsupervised
methods for inducing word representations over
large unlabeled corpora. Word features can be
hand-designed, but our goal is to learn them.
One common approach to inducing unsuper-
vised word representation is to use clustering,
perhaps hierarchical. This technique was used by
a variety of researchers (Miller et al, 2004; Liang,
2005; Koo et al, 2008; Ratinov & Roth, 2009;
Huang & Yates, 2009). This leads to a one-hot
representation over a smaller vocabulary size.
Neural language models (Bengio et al, 2001;
Schwenk & Gauvain, 2002; Mnih & Hinton,
2007; Collobert & Weston, 2008), on the other
hand, induce dense real-valued low-dimensional
384
word embeddings using unsupervised approaches.
(See Bengio (2008) for a more complete list of
references on neural language models.)
Unsupervised word representations have
been used in previous NLP work, and have
demonstrated improvements in generalization
accuracy on a variety of tasks. But different word
representations have never been systematically
compared in a controlled way. In this work, we
compare different techniques for inducing word
representations, evaluating them on the tasks of
named entity recognition (NER) and chunking.
We retract former negative results published in
Turian et al (2009) about Collobert and Weston
(2008) embeddings, given training improvements
that we describe in Section 7.1.
2 Distributional representations
Distributional word representations are based
upon a cooccurrence matrix F of size W?C, where
W is the vocabulary size, each row Fw is the ini-
tial representation of word w, and each column Fc
is some context. Sahlgren (2006) and Turney and
Pantel (2010) describe a handful of possible de-
sign decisions in contructing F, including choice
of context types (left window? right window? size
of window?) and type of frequency count (raw?
binary? tf-idf?). Fw has dimensionality W, which
can be too large to use Fw as features for word w in
a supervised model. One can map F to matrix f of
size W ? d, where d  C, using some function g,
where f = g(F). fw represents word w as a vector
with d dimensions. The choice of g is another de-
sign decision, although perhaps not as important
as the statistics used to initially construct F.
The self-organizing semantic map (Ritter &
Kohonen, 1989) is a distributional technique
that maps words to two dimensions, such that
syntactically and semantically related words are
nearby (Honkela et al, 1995; Honkela, 1997).
LSA (Dumais et al, 1988; Landauer et al,
1998), LSI, and LDA (Blei et al, 2003) induce
distributional representations over F in which
each column is a document context. In most of the
other approaches discussed, the columns represent
word contexts. In LSA, g computes the SVD of F.
Hyperspace Analogue to Language (HAL) is
another early distributional approach (Lund et al,
1995; Lund & Burgess, 1996) to inducing word
representations. They compute F over a corpus of
160 million word tokens with a vocabulary size W
of 70K word types. There are 2?W types of context
(columns): The first or second W are counted if the
word c occurs within a window of 10 to the left or
right of the word w, respectively. f is chosen by
taking the 200 columns (out of 140K in F) with
the highest variances. ICA is another technique to
transform F into f . (Va?yrynen & Honkela, 2004;
Va?yrynen & Honkela, 2005; Va?yrynen et al,
2007). ICA is expensive, and the largest vocab-
ulary size used in these works was only 10K. As
far as we know, ICA methods have not been used
when the size of the vocab W is 100K or more.
Explicitly storing cooccurrence matrix F can be
memory-intensive, and transforming F to f can
be time-consuming. It is preferable that F never
be computed explicitly, and that f be constructed
incrementally. R?ehu?r?ek and Sojka (2010) describe
an incremental approach to inducing LSA and
LDA topic models over 270 millions word tokens
with a vocabulary of 315K word types. This is
similar in magnitude to our experiments.
Another incremental approach to constructing f
is using a random projection: Linear mapping g is
multiplying F by a random matrix chosen a pri-
ori. This random indexing method is motivated
by the Johnson-Lindenstrauss lemma, which states
that for certain choices of random matrix, if d is
sufficiently large, then the original distances be-
tween words in F will be preserved in f (Sahlgren,
2005). Kaski (1998) uses this technique to pro-
duce 100-dimensional representations of docu-
ments. Sahlgren (2001) was the first author to use
random indexing using narrow context. Sahlgren
(2006) does a battery of experiments exploring
different design decisions involved in construct-
ing F, prior to using random indexing. However,
like all the works cited above, Sahlgren (2006)
only uses distributional representation to improve
existing systems for one-shot classification tasks,
such as IR, WSD, semantic knowledge tests, and
text categorization. It is not well-understood
what settings are appropriate to induce distribu-
tional word representations for structured predic-
tion tasks (like parsing and MT) and sequence la-
beling tasks (like chunking and NER). Previous
research has achieved repeated successes on these
tasks using clustering representations (Section 3)
and distributed representations (Section 4), so we
focus on these representations in our work.
3 Clustering-based word representations
Another type of word representation is to induce
a clustering over words. Clustering methods and
385
distributional methods can overlap. For example,
Pereira et al (1993) begin with a cooccurrence
matrix and transform this matrix into a clustering.
3.1 Brown clustering
The Brown algorithm is a hierarchical clustering
algorithm which clusters words to maximize the
mutual information of bigrams (Brown et al,
1992). So it is a class-based bigram language
model. It runs in time O(V ?K2), where V is the size
of the vocabulary and K is the number of clusters.
The hierarchical nature of the clustering means
that we can choose the word class at several
levels in the hierarchy, which can compensate for
poor clusters of a small number of words. One
downside of Brown clustering is that it is based
solely on bigram statistics, and does not consider
word usage in a wider context.
Brown clusters have been used successfully in
a variety of NLP applications: NER (Miller et al,
2004; Liang, 2005; Ratinov & Roth, 2009), PCFG
parsing (Candito & Crabbe?, 2009), dependency
parsing (Koo et al, 2008; Suzuki et al, 2009), and
semantic dependency parsing (Zhao et al, 2009).
Martin et al (1998) presents algorithms for
inducing hierarchical clusterings based upon word
bigram and trigram statistics. Ushioda (1996)
presents an extension to the Brown clustering
algorithm, and learn hierarchical clusterings of
words as well as phrases, which they apply to
POS tagging.
3.2 Other work on cluster-based word
representations
Lin and Wu (2009) present a K-means-like
non-hierarchical clustering algorithm for phrases,
which uses MapReduce.
HMMs can be used to induce a soft clustering,
specifically a multinomial distribution over pos-
sible clusters (hidden states). Li and McCallum
(2005) use an HMM-LDA model to improve
POS tagging and Chinese Word Segmentation.
Huang and Yates (2009) induce a fully-connected
HMM, which emits a multinomial distribution
over possible vocabulary words. They perform
hard clustering using the Viterbi algorithm.
(Alternately, they could keep the soft clustering,
with the representation for a particular word token
being the posterior probability distribution over
the states.) However, the CRF chunker in Huang
and Yates (2009), which uses their HMM word
clusters as extra features, achieves F1 lower than
a baseline CRF chunker (Sha & Pereira, 2003).
Goldberg et al (2009) use an HMM to assign
POS tags to words, which in turns improves
the accuracy of the PCFG-based Hebrew parser.
Deschacht and Moens (2009) use a latent-variable
language model to improve semantic role labeling.
4 Distributed representations
Another approach to word representation is to
learn a distributed representation. (Not to be
confused with distributional representations.)
A distributed representation is dense, low-
dimensional, and real-valued. Distributed word
representations are called word embeddings. Each
dimension of the embedding represents a latent
feature of the word, hopefully capturing useful
syntactic and semantic properties. A distributed
representation is compact, in the sense that it can
represent an exponential number of clusters in the
number of dimensions.
Word embeddings are typically induced us-
ing neural language models, which use neural
networks as the underlying predictive model
(Bengio, 2008). Historically, training and testing
of neural language models has been slow, scaling
as the size of the vocabulary for each model com-
putation (Bengio et al, 2001; Bengio et al, 2003).
However, many approaches have been proposed
in recent years to eliminate that linear dependency
on vocabulary size (Morin & Bengio, 2005;
Collobert & Weston, 2008; Mnih & Hinton, 2009)
and allow scaling to very large training corpora.
4.1 Collobert and Weston (2008) embeddings
Collobert and Weston (2008) presented a neural
language model that could be trained over billions
of words, because the gradient of the loss was
computed stochastically over a small sample of
possible outputs, in a spirit similar to Bengio and
Se?ne?cal (2003). This neural model of Collobert
and Weston (2008) was refined and presented in
greater depth in Bengio et al (2009).
The model is discriminative and non-
probabilistic. For each training update, we
read an n-gram x = (w1, . . . ,wn) from the corpus.
The model concatenates the learned embeddings
of the n words, giving e(w1) ? . . . ? e(wn), where
e is the lookup table and ? is concatenation.
We also create a corrupted or noise n-gram
x? = (w1, . . . ,wn?q, w?n), where w?n , wn is chosen
uniformly from the vocabulary.1 For convenience,
1In Collobert and Weston (2008), the middle word in the
386
we write e(x) to mean e(w1) ? . . . ? e(wn). We
predict a score s(x) for x by passing e(x) through
a single hidden layer neural network. The training
criterion is that n-grams that are present in the
training corpus like x must have a score at least
some margin higher than corrupted n-grams like
x?. Specifically: L(x) = max(0, 1? s(x) + s(x?)). We
minimize this loss stochastically over the n-grams
in the corpus, doing gradient descent simultane-
ously over the neural network parameters and the
embedding lookup table.
We implemented the approach of Collobert and
Weston (2008), with the following differences:
? We did not achieve as low log-ranks on the
English Wikipedia as the authors reported in
Bengio et al (2009), despite initially attempting
to have identical experimental conditions.
?We corrupt the last word of each n-gram.
? We had a separate learning rate for the em-
beddings and for the neural network weights.
We found that the embeddings should have a
learning rate generally 1000?32000 times higher
than the neural network weights. Otherwise, the
unsupervised training criterion drops slowly.
? Although their sampling technique makes train-
ing fast, testing is still expensive when the size of
the vocabulary is large. Instead of cross-validating
using the log-rank over the validation data as
they do, we instead used the moving average of
the training loss on training examples before the
weight update.
4.2 HLBL embeddings
The log-bilinear model (Mnih & Hinton, 2007) is
a probabilistic and linear neural model. Given an
n-gram, the model concatenates the embeddings
of the n ? 1 first words, and learns a linear model
to predict the embedding of the last word. The
similarity between the predicted embedding and
the current actual embedding is transformed
into a probability by exponentiating and then
normalizing. Mnih and Hinton (2009) speed up
model evaluation during training and testing by
using a hierarchy to exponentially filter down
the number of computations that are performed.
This hierarchical evaluation technique was first
proposed by Morin and Bengio (2005). The
model, combined with this optimization, is called
the hierarchical log-bilinear (HLBL) model.
n-gram is corrupted. In Bengio et al (2009), the last word in
the n-gram is corrupted.
5 Supervised evaluation tasks
We evaluate the hypothesis that one can take an
existing, near state-of-the-art, supervised NLP
system, and improve its accuracy by including
word representations as word features. This
technique for turning a supervised approach into a
semi-supervised one is general and task-agnostic.
However, we wish to find out if certain word
representations are preferable for certain tasks.
Lin and Wu (2009) finds that the representations
that are good for NER are poor for search query
classification, and vice-versa. We apply clus-
tering and distributed representations to NER
and chunking, which allows us to compare our
semi-supervised models to those of Ando and
Zhang (2005) and Suzuki and Isozaki (2008).
5.1 Chunking
Chunking is a syntactic sequence labeling task.
We follow the conditions in the CoNLL-2000
shared task (Sang & Buchholz, 2000).
The linear CRF chunker of Sha and Pereira
(2003) is a standard near-state-of-the-art baseline
chunker. In fact, many off-the-shelf CRF imple-
mentations now replicate Sha and Pereira (2003),
including their choice of feature set:
? CRF++ by Taku Kudo (http://crfpp.
sourceforge.net/)
? crfsgd by Le?on Bottou (http://leon.
bottou.org/projects/sgd)
? CRFsuite by by Naoaki Okazaki (http://
www.chokkan.org/software/crfsuite/)
We use CRFsuite because it makes it sim-
ple to modify the feature generation code,
so one can easily add new features. We
use SGD optimization, and enable negative
state features and negative transition fea-
tures. (?feature.possible transitions=1,
feature.possible states=1?)
Table 1 shows the features in the baseline chun-
ker. As you can see, the Brown and embedding
features are unigram features, and do not partici-
pate in conjunctions like the word features and tag
features do. Koo et al (2008) sees further accu-
racy improvements on dependency parsing when
using word representations in compound features.
The data comes from the Penn Treebank, and
is newswire from the Wall Street Journal in 1989.
Of the 8936 training sentences, we used 1000
randomly sampled sentences (23615 words) for
development. We trained models on the 7936
387
? Word features: wi for i in {?2,?1, 0,+1,+2},
wi ? wi+1 for i in {?1, 0}.
? Tag features: wi for i in {?2,?1, 0,+1,+2},
ti ? ti+1 for i in {?2,?1, 0,+1}. ti ? ti+1 ? ti+2
for i in {?2,?1, 0}.
? Embedding features [if applicable]: ei[d] for i
in {?2,?1, 0,+1,+2}, where d ranges over the
dimensions of the embedding ei.
? Brown features [if applicable]: substr(bi, 0, p)
for i in {?2,?1, 0,+1,+2}, where substr takes
the p-length prefix of the Brown cluster bi.
Table 1: Features templates used in the CRF chunker.
training partition sentences, and evaluated their
F1 on the development set. After choosing hy-
perparameters to maximize the dev F1, we would
retrain the model using these hyperparameters on
the full 8936 sentence training set, and evaluate
on test. One hyperparameter was l2-regularization
sigma, which for most models was optimal at 2 or
3.2. The word embeddings also required a scaling
hyperparameter, as described in Section 7.2.
5.2 Named entity recognition
NER is typically treated as a sequence prediction
problem. Following Ratinov and Roth (2009), we
use the regularized averaged perceptron model.
Ratinov and Roth (2009) describe different
sequence encoding like BILOU and BIO, and
show that the BILOU encoding outperforms BIO,
and the greedy inference performs competitively
to Viterbi while being significantly faster. Ac-
cordingly, we use greedy inference and BILOU
text chunk representation. We use the publicly
available implementation from Ratinov and Roth
(2009) (see the end of this paper for the URL). In
our baseline experiments, we remove gazetteers
and non-local features (Krishnan & Manning,
2006). However, we also run experiments that
include these features, to understand if the infor-
mation they provide mostly overlaps with that of
the word representations.
After each epoch over the training set, we
measured the accuracy of the model on the
development set. Training was stopped after the
accuracy on the development set did not improve
for 10 epochs, generally about 50?80 epochs
total. The epoch that performed best on the
development set was chosen as the final model.
We use the following baseline set of features
from Zhang and Johnson (2003):
? Previous two predictions yi?1 and yi?2
? Current word xi
? xi word type information: all-capitalized,
is-capitalized, all-digits, alphanumeric, etc.
? Prefixes and suffixes of xi, if the word contains
hyphens, then the tokens between the hyphens
? Tokens in the window c =
(xi?2, xi?1, xi, xi+1, xi+2)
? Capitalization pattern in the window c
? Conjunction of c and yi?1.
Word representation features, if present, are used
the same way as in Table 1.
When using the lexical features, we normalize
dates and numbers. For example, 1980 becomes
*DDDD* and 212-325-4751 becomes *DDD*-
*DDD*-*DDDD*. This allows a degree of abstrac-
tion to years, phone numbers, etc. This delexi-
calization is performed separately from using the
word representation. That is, if we have induced
an embedding for 12/3/2008 , we will use the em-
bedding of 12/3/2008 , and *DD*/*D*/*DDDD*
in the baseline features listed above.
Unlike in our chunking experiments, after we
chose the best model on the development set, we
used that model on the test set too. (In chunking,
after finding the best hyperparameters on the
development set, we would combine the dev
and training set and training a model over this
combined set, and then evaluate on test.)
The standard evaluation benchmark for NER
is the CoNLL03 shared task dataset drawn from
the Reuters newswire. The training set contains
204K words (14K sentences, 946 documents), the
test set contains 46K words (3.5K sentences, 231
documents), and the development set contains
51K words (3.3K sentences, 216 documents).
We also evaluated on an out-of-domain (OOD)
dataset, the MUC7 formal run (59K words).
MUC7 has a different annotation standard than
the CoNLL03 data. It has several NE types that
don?t appear in CoNLL03: money, dates, and
numeric quantities. CoNLL03 has MISC, which
is not present in MUC7. To evaluate on MUC7,
we perform the following postprocessing steps
prior to evaluation:
1. In the gold-standard MUC7 data, discard
(label as ?O?) all NEs with type NUM-
BER/MONEY/DATE.
2. In the predicted model output on MUC7 data,
discard (label as ?O?) all NEs with type MISC.
388
These postprocessing steps will adversely affect
all NER models across-the-board, nonetheless
allowing us to compare different models in a
controlled manner.
6 Unlabled Data
Unlabeled data is used for inducing the word
representations. We used the RCV1 corpus, which
contains one year of Reuters English newswire,
from August 1996 to August 1997, about 63
millions words in 3.3 million sentences. We
left case intact in the corpus. By comparison,
Collobert and Weston (2008) downcases words
and delexicalizes numbers.
We use a preprocessing technique proposed
by Liang, (2005, p. 51), which was later used
by Koo et al (2008): Remove all sentences that
are less than 90% lowercase a?z. We assume
that whitespace is not counted, although this
is not specified in Liang?s thesis. We call this
preprocessing step cleaning.
In Turian et al (2009), we found that all
word representations performed better on the
supervised task when they were induced on the
clean unlabeled data, both embeddings and Brown
clusters. This is the case even though the cleaning
process was very aggressive, and discarded more
than half of the sentences. According to the
evidence and arguments presented in Bengio et al
(2009), the non-convex optimization process for
Collobert and Weston (2008) embeddings might
be adversely affected by noise and the statistical
sparsity issues regarding rare words, especially
at the beginning of training. For this reason, we
hypothesize that learning representations over the
most frequent words first and gradually increasing
the vocabulary?a curriculum training strategy
(Elman, 1993; Bengio et al, 2009; Spitkovsky
et al, 2010)?would provide better results than
cleaning.
After cleaning, there are 37 million words (58%
of the original) in 1.3 million sentences (41% of
the original). The cleaned RCV1 corpus has 269K
word types. This is the vocabulary size, i.e. how
many word representations were induced. Note
that cleaning is applied only to the unlabeled data,
not to the labeled data used in the supervised tasks.
RCV1 is a superset of the CoNLL03 corpus.
For this reason, NER results that use RCV1
word representations are a form of transductive
learning.
7 Experiments and Results
7.1 Details of inducing word representations
The Brown clusters took roughly 3 days to induce,
when we induced 1000 clusters, the baseline in
prior work (Koo et al, 2008; Ratinov & Roth,
2009). We also induced 100, 320, and 3200
Brown clusters, for comparison. (Because Brown
clustering scales quadratically in the number of
clusters, inducing 10000 clusters would have
been prohibitive.) Because Brown clusters are
hierarchical, we can use cluster supersets as
features. We used clusters at path depth 4, 6, 10,
and 20 (Ratinov & Roth, 2009). These are the
prefixes used in Table 1.
The Collobert and Weston (2008) (C&W)
embeddings were induced over the course of a
few weeks, and trained for about 50 epochs. One
of the difficulties in inducing these embeddings is
that there is no stopping criterion defined, and that
the quality of the embeddings can keep improving
as training continues. Collobert (p.c.) simply
leaves one computer training his embeddings
indefinitely. We induced embeddings with 25, 50,
100, or 200 dimensions over 5-gram windows.
In comparison to Turian et al (2009), we use
improved C&W embeddings in this work:
? They were trained for 50 epochs, not just 20
epochs.
? We initialized all embedding dimensions uni-
formly in the range [-0.01, +0.01], not [-1,+1].
For rare words, which are typically updated only
143 times per epoch2, and given that our embed-
ding learning rate was typically 1e-6 or 1e-7, this
means that rare word embeddings will be concen-
trated around zero, instead of spread out randomly.
The HLBL embeddings were trained for 100
epochs (7 days).3 Unlike our Collobert and We-
ston (2008) embeddings, we did not extensively
tune the learning rates for HLBL. We used a learn-
ing rate of 1e-3 for both model parameters and
embedding parameters. We induced embeddings
with 100 dimensions over 5-gram windows, and
embeddings with 50 dimensions over 5-gram win-
dows. Embeddings were induced over one pass
2A rare word will appear 5 (window size) times per
epoch as a positive example, and 37M (training examples per
epoch) / 269K (vocabulary size) = 138 times per epoch as a
corruption example.
3The HLBL model updates require fewer matrix mul-
tiplies than Collobert and Weston (2008) model updates.
Additionally, HLBL models were trained on a GPGPU,
which is faster than conventional CPU arithmetic.
389
approach using a random tree, not two passes with
an updated tree and embeddings re-estimation.
7.2 Scaling of Word Embeddings
Like many NLP systems, the baseline system con-
tains only binary features. The word embeddings,
however, are real numbers that are not necessarily
in a bounded range. If the range of the word
embeddings is too large, they will exert more
influence than the binary features.
We generally found that embeddings had zero
mean. We can scale the embeddings by a hy-
perparameter, to control their standard deviation.
Assume that the embeddings are represented by a
matrix E:
E ? ? ? E/stddev(E) (1)
? is a scaling constant that sets the new standard
deviation after scaling the embeddings.
(a)
 93.6
 93.8
 94
 94.2
 94.4
 94.6
 94.8
 0.001  0.01  0.1  1
Va
lid
ati
on
 F1
Scaling factor ?
C&W, 50-dim
HLBL, 50-dimC&W, 200-dimC&W, 100-dim
HLBL, 100-dimC&W, 25-dim
baseline
(b)
 89
 89.5
 90
 90.5
 91
 91.5
 92
 92.5
 0.001  0.01  0.1  1
Va
lid
ati
on
 F1
Scaling factor ?
C&W, 200-dimC&W, 100-dimC&W, 25-dimC&W, 50-dim
HLBL, 100-dim
HLBL, 50-dim
baseline
Figure 1: Effect as we vary the scaling factor ? (Equa-
tion 1) on the validation set F1. We experiment with
Collobert and Weston (2008) and HLBL embeddings of var-
ious dimensionality. (a) Chunking results. (b) NER results.
Figure 1 shows the effect of scaling factor ?
on both supervised tasks. We were surprised
to find that on both tasks, across Collobert and
Weston (2008) and HLBL embeddings of various
dimensionality, that all curves had similar shapes
and optima. This is one contributions of our
work. In Turian et al (2009), we were not
able to prescribe a default value for scaling the
embeddings. However, these curves demonstrate
that a reasonable choice of scale factor is such that
the embeddings have a standard deviation of 0.1.
7.3 Capacity of Word Representations
(a)
 94.1
 94.2
 94.3
 94.4
 94.5
 94.6
 94.7
 100  320  1000  3200
 25  50  100  200
Va
lid
ati
on
 F1
# of Brown clusters
# of embedding dimensions
C&W
HLBL
Brown
baseline
(b)
 90
 90.5
 91
 91.5
 92
 92.5
 100  320  1000  3200
 25  50  100  200
Va
lid
ati
on
 F1
# of Brown clusters
# of embedding dimensions
C&W
Brown
HLBL
baseline
Figure 2: Effect as we vary the capacity of the word
representations on the validation set F1. (a) Chunking
results. (b) NER results.
There are capacity controls for the word
representations: number of Brown clusters, and
number of dimensions of the word embeddings.
Figure 2 shows the effect on the validation F1 as
we vary the capacity of the word representations.
In general, it appears that more Brown clusters
are better. We would like to induce 10000 Brown
clusters, however this would take several months.
In Turian et al (2009), we hypothesized on
the basis of solely the HLBL NER curve that
higher-dimensional word embeddings would give
higher accuracy. Figure 2 shows that this hy-
pothesis is not true. For NER, the C&W curve is
almost flat, and we were suprised to find the even
25-dimensional C&W word embeddings work so
well. For chunking, 50-dimensional embeddings
had the highest validation F1 for both C&W and
HLBL. These curves indicates that the optimal
capacity of the word embeddings is task-specific.
390
System Dev Test
Baseline 94.16 93.79
HLBL, 50-dim 94.63 94.00
C&W, 50-dim 94.66 94.10
Brown, 3200 clusters 94.67 94.11
Brown+HLBL, 37M 94.62 94.13
C&W+HLBL, 37M 94.68 94.25
Brown+C&W+HLBL, 37M 94.72 94.15
Brown+C&W, 37M 94.76 94.35
Ando and Zhang (2005), 15M - 94.39
Suzuki and Isozaki (2008), 15M - 94.67
Suzuki and Isozaki (2008), 1B - 95.15
Table 2: Final chunking F1 results. In the last section, we
show how many unlabeled words were used.
System Dev Test MUC7
Baseline 90.03 84.39 67.48
Baseline+Nonlocal 91.91 86.52 71.80
HLBL 100-dim 92.00 88.13 75.25
Gazetteers 92.09 87.36 77.76
C&W 50-dim 92.27 87.93 75.74
Brown, 1000 clusters 92.32 88.52 78.84
C&W 200-dim 92.46 87.96 75.51
C&W+HLBL 92.52 88.56 78.64
Brown+HLBL 92.56 88.93 77.85
Brown+C&W 92.79 89.31 80.13
HLBL+Gaz 92.91 89.35 79.29
C&W+Gaz 92.98 88.88 81.44
Brown+Gaz 93.25 89.41 82.71
Lin and Wu (2009), 3.4B - 88.44 -
Ando and Zhang (2005), 27M 93.15 89.31 -
Suzuki and Isozaki (2008), 37M 93.66 89.36 -
Suzuki and Isozaki (2008), 1B 94.48 89.92 -
All (Brown+C&W+HLBL+Gaz), 37M 93.17 90.04 82.50
All+Nonlocal, 37M 93.95 90.36 84.15
Lin and Wu (2009), 700B - 90.90 -
Table 3: Final NER F1 results, showing the cumulative
effect of adding word representations, non-local features, and
gazetteers to the baseline. To speed up training, in combined
experiments (C&W plus another word representation),
we used the 50-dimensional C&W embeddings, not the
200-dimensional ones. In the last section, we show how
many unlabeled words were used.
7.4 Final results
Table 2 shows the final chunking results and Ta-
ble 3 shows the final NER F1 results. We compare
to the state-of-the-art methods of Ando and Zhang
(2005), Suzuki and Isozaki (2008), and?for
NER?Lin and Wu (2009). Tables 2 and 3 show
that accuracy can be increased further by combin-
ing the features from different types of word rep-
resentations. But, if only one word representation
is to be used, Brown clusters have the highest ac-
curacy. Given the improvements to the C&W em-
beddings since Turian et al (2009), C&W em-
beddings outperform the HLBL embeddings. On
chunking, there is only a minute difference be-
tween Brown clusters and the embeddings. Com-
(a)
 0
 50
 100
 150
 200
 250
0 1 10 100 1K 10K 100K 1M
# o
f p
er-
tok
en
 er
ror
s (t
est
 se
t)
Frequency of word in unlabeled data
C&W, 50-dim
Brown, 3200 clusters
(b)
 0
 50
 100
 150
 200
 250
0 1 10 100 1K 10K 100K 1M
# o
f p
er-
tok
en
 er
ror
s (t
est
 se
t)
Frequency of word in unlabeled data
C&W, 50-dim
Brown, 1000 clusters
Figure 3: For word tokens that have different frequency
in the unlabeled data, what is the total number of per-token
errors incurred on the test set? (a) Chunking results. (b) NER
results.
bining representations leads to small increases in
the test F1. In comparison to chunking, combin-
ing different word representations on NER seems
gives larger improvements on the test F1.
On NER, Brown clusters are superior to the
word embeddings. Since much of the NER F1
is derived from decisions made over rare words,
we suspected that Brown clustering has a superior
representation for rare words. Brown makes
a single hard clustering decision, whereas the
embedding for a rare word is close to its initial
value since it hasn?t received many training
updates (see Footnote 2). Figure 3 shows the total
number of per-token errors incurred on the test
set, depending upon the frequency of the word
token in the unlabeled data. For NER, Figure 3 (b)
shows that most errors occur on rare words, and
that Brown clusters do indeed incur fewer errors
for rare words. This supports our hypothesis
that, for rare words, Brown clustering produces
better representations than word embeddings that
haven?t received sufficient training updates. For
chunking, Brown clusters and C&W embeddings
incur almost identical numbers of errors, and
errors are concentrated around the more common
391
words. We hypothesize that non-rare words have
good representations, regardless of the choice
of word representation technique. For tasks like
chunking in which a syntactic decision relies upon
looking at several token simultaneously, com-
pound features that use the word representations
might increase accuracy more (Koo et al, 2008).
Using word representations in NER brought
larger gains on the out-of-domain data than on the
in-domain data. We were surprised by this result,
because the OOD data was not even used during
the unsupervised word representation induction,
as was the in-domain data. We are curious to
investigate this phenomenon further.
Ando and Zhang (2005) present a semi-
supervised learning algorithm called alternating
structure optimization (ASO). They find a low-
dimensional projection of the input features that
gives good linear classifiers over auxiliary tasks.
These auxiliary tasks are sometimes specific
to the supervised task, and sometimes general
language modeling tasks like ?predict the missing
word?. Suzuki and Isozaki (2008) present a semi-
supervised extension of CRFs. (In Suzuki et al
(2009), they extend their semi-supervised ap-
proach to more general conditional models.) One
of the advantages of the semi-supervised learning
approach that we use is that it is simpler and more
general than that of Ando and Zhang (2005) and
Suzuki and Isozaki (2008). Their methods dictate
a particular choice of model and training regime
and could not, for instance, be used with an NLP
system based upon an SVM classifier.
Lin and Wu (2009) present a K-means-like
non-hierarchical clustering algorithm for phrases,
which uses MapReduce. Since they can scale
to millions of phrases, and they train over 800B
unlabeled words, they achieve state-of-the-art
accuracy on NER using their phrase clusters.
This suggests that extending word representa-
tions to phrase representations is worth further
investigation.
8 Conclusions
Word features can be learned in advance in an
unsupervised, task-inspecific, and model-agnostic
manner. These word features, once learned, are
easily disseminated with other researchers, and
easily integrated into existing supervised NLP
systems. The disadvantage, however, is that ac-
curacy might not be as high as a semi-supervised
method that includes task-specific information
and that jointly learns the supervised and unsu-
pervised tasks (Ando & Zhang, 2005; Suzuki &
Isozaki, 2008; Suzuki et al, 2009).
Unsupervised word representations have been
used in previous NLP work, and have demon-
strated improvements in generalization accuracy
on a variety of tasks. Ours is the first work to
systematically compare different word repre-
sentations in a controlled way. We found that
Brown clusters and word embeddings both can
improve the accuracy of a near-state-of-the-art
supervised NLP system. We also found that com-
bining different word representations can improve
accuracy further. Error analysis indicates that
Brown clustering induces better representations
for rare words than C&W embeddings that have
not received many training updates.
Another contribution of our work is a default
method for setting the scaling parameter for
word embeddings. With this contribution, word
embeddings can now be used off-the-shelf as
word features, with no tuning.
Future work should explore methods for
inducing phrase representations, as well as tech-
niques for increasing in accuracy by using word
representations in compound features.
Replicating our experiments
You can visit http://metaoptimize.com/
projects/wordreprs/ to find: The word
representations we induced, which you can
download and use in your experiments; The code
for inducing the word representations, which you
can use to induce word representations on your
own data; The NER and chunking system, with
code for replicating our experiments.
Acknowledgments
Thank you to Magnus Sahlgren, Bob Carpenter,
Percy Liang, Alexander Yates, and the anonymous
reviewers for useful discussion. Thank you to
Andriy Mnih for inducing his embeddings on
RCV1 for us. Joseph Turian and Yoshua Bengio
acknowledge the following agencies for re-
search funding and computing support: NSERC,
RQCHP, CIFAR. Lev Ratinov was supported by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any
opinions, findings, and conclusion or recommen-
dations expressed in this material are those of the
author and do not necessarily reflect the view of
the Air Force Research Laboratory (AFRL).
392
References
Ando, R., & Zhang, T. (2005). A high-
performance semi-supervised learning method
for text chunking. ACL.
Bengio, Y. (2008). Neural net language models.
Scholarpedia, 3, 3881.
Bengio, Y., Ducharme, R., & Vincent, P. (2001).
A neural probabilistic language model. NIPS.
Bengio, Y., Ducharme, R., Vincent, P., & Jauvin,
C. (2003). A neural probabilistic language
model. Journal of Machine Learning Research,
3, 1137?1155.
Bengio, Y., Louradour, J., Collobert, R., &
Weston, J. (2009). Curriculum learning. ICML.
Bengio, Y., & Se?ne?cal, J.-S. (2003). Quick train-
ing of probabilistic neural nets by importance
sampling. AISTATS.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003).
Latent dirichlet alocation. Journal of Machine
Learning Research, 3, 993?1022.
Brown, P. F., deSouza, P. V., Mercer, R. L., Pietra,
V. J. D., & Lai, J. C. (1992). Class-based n-gram
models of natural language. Computational
Linguistics, 18, 467?479.
Candito, M., & Crabbe?, B. (2009). Improving gen-
erative statistical parsing with semi-supervised
word clustering. IWPT (pp. 138?141).
Collobert, R., & Weston, J. (2008). A unified
architecture for natural language processing:
Deep neural networks with multitask learning.
ICML.
Deschacht, K., & Moens, M.-F. (2009). Semi-
supervised semantic role labeling using the
Latent Words Language Model. EMNLP (pp.
21?29).
Dumais, S. T., Furnas, G. W., Landauer, T. K.,
Deerwester, S., & Harshman, R. (1988). Using
latent semantic analysis to improve access to
textual information. SIGCHI Conference on
Human Factors in Computing Systems (pp.
281?285). ACM.
Elman, J. L. (1993). Learning and development
in neural networks: The importance of starting
small. Cognition, 48, 781?799.
Goldberg, Y., Tsarfaty, R., Adler, M., & Elhadad,
M. (2009). Enhancing unlexicalized parsing
performance using a wide coverage lexicon,
fuzzy tag-set mapping, and EM-HMM-based
lexical probabilities. EACL.
Honkela, T. (1997). Self-organizing maps of
words for natural language processing applica-
tions. Proceedings of the International ICSC
Symposium on Soft Computing.
Honkela, T., Pulkki, V., & Kohonen, T. (1995).
Contextual relations of words in grimm tales,
analyzed by self-organizing map. ICANN.
Huang, F., & Yates, A. (2009). Distributional rep-
resentations for handling sparsity in supervised
sequence labeling. ACL.
Kaski, S. (1998). Dimensionality reduction by
random mapping: Fast similarity computation
for clustering. IJCNN (pp. 413?418).
Koo, T., Carreras, X., & Collins, M. (2008).
Simple semi-supervised dependency parsing.
ACL (pp. 595?603).
Krishnan, V., & Manning, C. D. (2006). An
effective two-stage model for exploiting non-
local dependencies in named entity recognition.
COLING-ACL.
Landauer, T. K., Foltz, P. W., & Laham, D. (1998).
An introduction to latent semantic analysis.
Discourse Processes, 259?284.
Li, W., & McCallum, A. (2005). Semi-supervised
sequence modeling with syntactic topic models.
AAAI.
Liang, P. (2005). Semi-supervised learning
for natural language. Master?s thesis, Mas-
sachusetts Institute of Technology.
Lin, D., & Wu, X. (2009). Phrase clustering
for discriminative learning. ACL-IJCNLP (pp.
1030?1038).
Lund, K., & Burgess, C. (1996). Producing
highdimensional semantic spaces from lexical
co-occurrence. Behavior Research Methods,
Instrumentation, and Computers, 28, 203?208.
Lund, K., Burgess, C., & Atchley, R. A. (1995).
Semantic and associative priming in high-
dimensional semantic space. Cognitive Science
Proceedings, LEA (pp. 660?665).
Martin, S., Liermann, J., & Ney, H. (1998). Algo-
rithms for bigram and trigram word clustering.
Speech Communication, 24, 19?37.
Miller, S., Guinness, J., & Zamanian, A. (2004).
Name tagging with word clusters and discrim-
inative training. HLT-NAACL (pp. 337?342).
393
Mnih, A., & Hinton, G. E. (2007). Three
new graphical models for statistical language
modelling. ICML.
Mnih, A., & Hinton, G. E. (2009). A scalable
hierarchical distributed language model. NIPS
(pp. 1081?1088).
Morin, F., & Bengio, Y. (2005). Hierarchical
probabilistic neural network language model.
AISTATS.
Pereira, F., Tishby, N., & Lee, L. (1993). Distri-
butional clustering of english words. ACL (pp.
183?190).
Ratinov, L., & Roth, D. (2009). Design chal-
lenges and misconceptions in named entity
recognition. CoNLL.
Ritter, H., & Kohonen, T. (1989). Self-organizing
semantic maps. Biological Cybernetics,
241?254.
Sahlgren, M. (2001). Vector-based semantic
analysis: Representing word meanings based
on random labels. Proceedings of the Semantic
Knowledge Acquisition and Categorisation
Workshop, ESSLLI.
Sahlgren, M. (2005). An introduction to random
indexing. Methods and Applications of Seman-
tic Indexing Workshop at the 7th International
Conference on Terminology and Knowledge
Engineering (TKE).
Sahlgren, M. (2006). The word-space model:
Using distributional analysis to represent syn-
tagmatic and paradigmatic relations between
words in high-dimensional vector spaces.
Doctoral dissertation, Stockholm University.
Sang, E. T., & Buchholz, S. (2000). Introduction
to the CoNLL-2000 shared task: Chunking.
CoNLL.
Schwenk, H., & Gauvain, J.-L. (2002). Connec-
tionist language modeling for large vocabulary
continuous speech recognition. International
Conference on Acoustics, Speech and Signal
Processing (ICASSP) (pp. 765?768). Orlando,
Florida.
Sha, F., & Pereira, F. C. N. (2003). Shal-
low parsing with conditional random fields.
HLT-NAACL.
Spitkovsky, V., Alshawi, H., & Jurafsky, D.
(2010). From baby steps to leapfrog: How ?less
is more? in unsupervised dependency parsing.
NAACL-HLT.
Suzuki, J., & Isozaki, H. (2008). Semi-supervised
sequential labeling and segmentation using
giga-word scale unlabeled data. ACL-08: HLT
(pp. 665?673).
Suzuki, J., Isozaki, H., Carreras, X., & Collins, M.
(2009). An empirical study of semi-supervised
structured conditional models for dependency
parsing. EMNLP.
Turian, J., Ratinov, L., Bengio, Y., & Roth, D.
(2009). A preliminary evaluation of word
representations for named-entity recognition.
NIPS Workshop on Grammar Induction, Repre-
sentation of Language and Language Learning.
Turney, P. D., & Pantel, P. (2010). From frequency
to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research.
Ushioda, A. (1996). Hierarchical clustering of
words. COLING (pp. 1159?1162).
Va?yrynen, J., & Honkela, T. (2005). Compar-
ison of independent component analysis and
singular value decomposition in word context
analysis. AKRR?05, International and Interdis-
ciplinary Conference on Adaptive Knowledge
Representation and Reasoning.
Va?yrynen, J. J., & Honkela, T. (2004). Word cat-
egory maps based on emergent features created
by ICA. Proceedings of the STeP?2004 Cogni-
tion + Cybernetics Symposium (pp. 173?185).
Finnish Artificial Intelligence Society.
Va?yrynen, J. J., Honkela, T., & Lindqvist, L.
(2007). Towards explicit semantic features
using independent component analysis. Pro-
ceedings of the Workshop Semantic Content
Acquisition and Representation (SCAR). Stock-
holm, Sweden: Swedish Institute of Computer
Science.
R?ehu?r?ek, R., & Sojka, P. (2010). Software frame-
work for topic modelling with large corpora.
LREC.
Zhang, T., & Johnson, D. (2003). A robust risk
minimization based named entity recognition
system. CoNLL.
Zhao, H., Chen, W., Kit, C., & Zhou, G.
(2009). Multilingual dependency learning: a
huge feature engineering method to semantic
dependency parsing. CoNLL (pp. 55?60).
394
Tutorial Abstracts of ACL 2012, page 5,
Jeju, Republic of Korea, 8 July 2012. c?2012 Association for Computational Linguistics
Deep Learning for NLP (without Magic)
Richard Socher Yoshua Bengio? Christopher D. Manning
richard@socher.org bengioy@iro.umontreal.ca, manning@stanford.edu
Computer Science Department, Stanford University
? DIRO, Universite? de Montre?al, Montre?al, QC, Canada
1 Abtract
Machine learning is everywhere in today?s NLP, but
by and large machine learning amounts to numerical
optimization of weights for human designed repre-
sentations and features. The goal of deep learning
is to explore how computers can take advantage of
data to develop features and representations appro-
priate for complex interpretation tasks. This tuto-
rial aims to cover the basic motivation, ideas, mod-
els and learning algorithms in deep learning for nat-
ural language processing. Recently, these methods
have been shown to perform very well on various
NLP tasks such as language modeling, POS tag-
ging, named entity recognition, sentiment analysis
and paraphrase detection, among others. The most
attractive quality of these techniques is that they can
perform well without any external hand-designed re-
sources or time-intensive feature engineering. De-
spite these advantages, many researchers in NLP are
not familiar with these methods. Our focus is on
insight and understanding, using graphical illustra-
tions and simple, intuitive derivations. The goal of
the tutorial is to make the inner workings of these
techniques transparent, intuitive and their results in-
terpretable, rather than black boxes labeled ?magic
here?.
The first part of the tutorial presents the basics of
neural networks, neural word vectors, several simple
models based on local windows and the math and
algorithms of training via backpropagation. In this
section applications include language modeling and
POS tagging.
In the second section we present recursive neural
networks which can learn structured tree outputs as
well as vector representations for phrases and sen-
tences. We cover both equations as well as applica-
tions. We show how training can be achieved by a
modified version of the backpropagation algorithm
introduced before. These modifications allow the al-
gorithm to work on tree structures. Applications in-
clude sentiment analysis and paraphrase detection.
We also draw connections to recent work in seman-
tic compositionality in vector spaces. The princi-
ple goal, again, is to make these methods appear in-
tuitive and interpretable rather than mathematically
confusing. By this point in the tutorial, the audience
members should have a clear understanding of how
to build a deep learning system for word-, sentence-
and document-level tasks.
The last part of the tutorial gives a general
overview of the different applications of deep learn-
ing in NLP, including bag of words models. We will
provide a discussion of NLP-oriented issues in mod-
eling, interpretation, representational power, and op-
timization.
5
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 78?85,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Overcoming the Curse of Sentence Length for Neural Machine
Translation using Automatic Segmentation
Jean Pouget-Abadie
?
Ecole Polytechnique, France
Dzmitry Bahdanau
?
Jacobs University Bremen, Germany
Bart van Merri
?
enboer Kyunghyun Cho
Universit?e de Montr?eal, Canada
Yoshua Bengio
Universit?e de Montr?eal, Canada
CIFAR Senior Fellow
Abstract
The authors of (Cho et al., 2014a) have
shown that the recently introduced neural
network translation systems suffer from
a significant drop in translation quality
when translating long sentences, unlike
existing phrase-based translation systems.
In this paper, we propose a way to ad-
dress this issue by automatically segment-
ing an input sentence into phrases that can
be easily translated by the neural network
translation model. Once each segment has
been independently translated by the neu-
ral machine translation model, the trans-
lated clauses are concatenated to form a
final translation. Empirical results show
a significant improvement in translation
quality for long sentences.
1 Introduction
Up to now, most research efforts in statistical ma-
chine translation (SMT) research have relied on
the use of a phrase-based system as suggested
in (Koehn et al., 2003). Recently, however, an
entirely new, neural network based approach has
been proposed by several research groups (Kalch-
brenner and Blunsom, 2013; Sutskever et al.,
2014; Cho et al., 2014b), showing promising re-
sults, both as a standalone system or as an addi-
tional component in the existing phrase-based sys-
tem. In this neural network based approach, an en-
coder ?encodes? a variable-length input sentence
into a fixed-length vector and a decoder ?decodes?
a variable-length target sentence from the fixed-
length encoded vector.
It has been observed in (Sutskever et al., 2014),
(Kalchbrenner and Blunsom, 2013) and (Cho et
al., 2014a) that this neural network approach
?
Research done while these authors were visiting Uni-
versit?e de Montr?eal
works well with short sentences (e.g., / 20
words), but has difficulty with long sentences (e.g.,
' 20 words), and particularly with sentences that
are longer than those used for training. Training
on long sentences is difficult because few available
training corpora include sufficiently many long
sentences, and because the computational over-
head of each update iteration in training is linearly
correlated with the length of training sentences.
Additionally, by the nature of encoding a variable-
length sentence into a fixed-size vector representa-
tion, the neural network may fail to encode all the
important details.
In this paper, hence, we propose to translate sen-
tences piece-wise. We segment an input sentence
into a number of short clauses that can be confi-
dently translated by the model. We show empiri-
cally that this approach improves translation qual-
ity of long sentences, compared to using a neural
network to translate a whole sentence without seg-
mentation.
2 Background: RNN Encoder?Decoder
for Translation
The RNN Encoder?Decoder (RNNenc) model is
a recent implementation of the encoder?decoder
approach, proposed independently in (Cho et al.,
2014b) and in (Sutskever et al., 2014). It consists
of two RNNs, acting respectively as encoder and
decoder.
The encoder of the RNNenc reads each word in
a source sentence one by one while maintaining a
hidden state. The hidden state computed at the end
of the source sentence then summarizes the whole
input sentence. Formally, given an input sentence
x = (x
1
, ? ? ? , x
T
x
), the encoder computes
h
t
= f (x
t
, h
t?1
) ,
where f is a nonlinear function computing the next
hidden state given the previous one and the current
input word.
78
x1 x2 xT
yT' y2 y1
c
Decoder
Encoder
Figure 1: An illustration of the RNN Encoder?
Decoder. Reprinted from (Cho et al., 2014b).
From the last hidden state of the encoder, we
compute a context vector c on which the decoder
will be conditioned:
c = g(h
T
x
),
where g may simply be a linear affine transforma-
tion of h
T
x
.
The decoder, on the other hand, generates each
target word at a time, until the end-of-sentence
symbol is generated. It generates a word at a time
given the context vector (from the encoder), a pre-
vious hidden state (of the decoder) and the word
generated at the last step. More formally, the de-
coder computes at each time its hidden state by
s
t
= f (y
t?1
, s
t?1
, c) .
With the newly computed hidden state, the de-
coder outputs the probability distribution over all
possible target words by:
p(f
t,j
= 1 | f
t?1
, . . . , f
1
, c) =
exp
(
w
j
h
?t?
)
?
K
j
?
=1
exp
(
w
j
?
h
?t?
)
, (1)
where f
t,j
is the indicator variable for the j-th
word in the target vocabulary at time t and only
a single indicator variable is on (= 1) each time.
See Fig. 1 for the graphical illustration of the
RNNenc.
The RNNenc in (Cho et al., 2014b) uses a spe-
cial hidden unit that adaptively forgets or remem-
bers the previous hidden state such that the acti-
vation of a hidden unit h
?t?
j
at time t is computed
by
h
?t?
j
= z
j
h
?t?1?
j
+ (1? z
j
)
?
h
?t?
j
,
where
?
h
?t?
j
=f
(
[Wx]
j
+
[
U
(
r h
?t?1?
)]
)
,
z
j
=?
(
[W
z
x]
j
+
[
U
z
h
?t?1?
]
j
)
,
r
j
=?
(
[W
r
x]
j
+
[
U
r
h
?t?1?
]
j
)
.
z
j
and r
j
are respectively the update and reset
gates.  is an element-wise multiplication. In
the remaining of this paper, we always assume that
this hidden unit is used in the RNNenc.
Although the model in (Cho et al., 2014b) was
originally trained on phrase pairs, it is straight-
forward to train the same model with a bilin-
gual, parallel corpus consisting of sentence pairs
as has been done in (Sutskever et al., 2014). In
the remainder of this paper, we use the RNNenc
trained on English?French sentence pairs (Cho et
al., 2014a).
3 Automatic Segmentation and
Translation
One hypothesis explaining the difficulty encoun-
tered by the RNNenc model when translating long
sentences is that a plain, fixed-length vector lacks
the capacity to encode a long sentence. When en-
coding a long input sentence, the encoder may lose
track of all the subtleties in the sentence. Con-
sequently, the decoder has difficulties recovering
the correct translation from the encoded represen-
tation. One solution would be to build a larger
model with a larger representation vector to in-
crease the capacity of the model at the price of
higher computational cost.
In this section, however, we propose to segment
an input sentence such that each segmented clause
can be easily translated by the RNN Encoder?
Decoder. In other words, we wish to find a
segmentation that maximizes the total confidence
score which is a sum of the confidence scores of
the phrases in the segmentation. Once the confi-
dence score is defined, the problem of finding the
best segmentation can be formulated as an integer
programming problem.
Let e = (e
1
, ? ? ? , e
n
) be a source sentence com-
posed of words e
k
. We denote a phrase, which is a
subsequence of e, with e
ij
= (e
i
, ? ? ? , e
j
).
79
We use the RNN Encoder?Decoder to measure
how confidently we can translate a subsequence
e
ij
by considering the log-probability log p(f
k
|
e
ij
) of a candidate translation f
k
generated by the
model. In addition to the log-probability, we also
use the log-probability log p(e
ij
| f
k
) from a re-
verse RNN Encoder?Decoder (translating from a
target language to source language). With these
two probabilities, we define the confidence score
of a phrase pair (e
ij
, f
k
) as:
c(e
ij
, f
k
) =
log p(f
k
| e
ij
) + log q(e
ij
| f
k
)
2 |log(j ? i+ 1)|
,
(2)
where the denominator penalizes a short segment
whose probability is known to be overestimated by
an RNN (Graves, 2013).
The confidence score of a source phrase only is
then defined as
c
ij
= max
k
c(e
ij
, f
k
). (3)
We use an approximate beam search to search for
the candidate translations f
k
of e
ij
, that maximize
log-likelihood log p(f
k
|e
ij
) (Graves et al., 2013;
Boulanger-Lewandowski et al., 2013).
Let x
ij
be an indicator variable equal to 1 if we
include a phrase e
ij
in the segmentation, and oth-
erwise, 0. We can rewrite the segmentation prob-
lem as the optimization of the following objective
function:
max
x
?
i?j
c
ij
x
ij
= x ? c (4)
subject to ?k, n
k
= 1
n
k
=
?
i,j
x
ij
1
i?k?j
is the number of source
phrases chosen in the segmentation containing
word e
k
.
The constraint in Eq. (4) states that for each
word e
k
in the sentence one and only one of the
source phrases contains this word, (e
ij
)
i?k?j
, is
included in the segmentation. The constraint ma-
trix is totally unimodular making this integer pro-
gramming problem solvable in polynomial time.
Let S
k
j
be the first index of the k-th segment
counting from the last phrase of the optimal seg-
mentation of subsequence e
1j
(S
j
:= S
1
j
), and s
j
be the corresponding score of this segmentation
(s
0
:= 0). Then, the following relations hold:
s
j
= max
1?i?j
(c
ij
+ s
i?1
), ?j ? 1 (5)
S
j
=argmax
1?i?j
(c
ij
+ s
i?1
), ?j ? 1 (6)
With Eq. (5) we can evaluate s
j
incrementally.
With the evaluated s
j
?s, we can compute S
j
as
well (Eq. (6)). By the definition of S
k
j
we find the
optimal segmentation by decomposing e
1n
into
e
S
k
n
,S
k?1
n
?1
, ? ? ? , e
S
2
n
,S
1
n
?1
, e
S
1
n
,n
, where k is the
index of the first one in the sequence S
k
n
. This
approach described above requires quadratic time
with respect to sentence length.
3.1 Issues and Discussion
The proposed segmentation approach does not
avoid the problem of reordering clauses. Unless
the source and target languages follow roughly the
same order, such as in English to French transla-
tions, a simple concatenation of translated clauses
will not necessarily be grammatically correct.
Despite the lack of long-distance reordering
1
in
the current approach, we find nonetheless signifi-
cant gains in the translation performance of neural
machine translation. A mechanism to reorder the
obtained clause translations is, however, an impor-
tant future research question.
Another issue at the heart of any purely neu-
ral machine translation is the limited model vo-
cabulary size for both source and target languages.
As shown in (Cho et al., 2014a), translation qual-
ity drops considerably with just a few unknown
words present in the input sentence. Interestingly
enough, the proposed segmentation approach ap-
pears to be more robust to the presence of un-
known words (see Sec. 5). One intuition is that the
segmentation leads to multiple short clauses with
less unknown words, which leads to more stable
translation of each clause by the neural translation
model.
Finally, the proposed approach is computation-
ally expensive as it requires scoring all the sub-
phrases of an input sentence. However, the scoring
process can be easily sped up by scoring phrases
in parallel, since each phrase can be scored inde-
pendently. Another way to speed up the segmen-
tation, other than parallelization, would be to use
1
Note that, inside each clause, the words are reordered
automatically when the clause is translated by the RNN
Encoder?Decoder.
80
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
25
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
25
30
35
40
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
(a) RNNenc without
segmentation
(b) RNNenc with segmentation
(c) Moses
Figure 2: The BLEU scores achieved by (a) the RNNenc without segmentation, (b) the RNNenc
with the penalized reverse confidence score, and (c) the phrase-based translation system Moses on a
newstest12-14.
an existing parser to segment a sentence into a set
of clauses.
4 Experiment Settings
4.1 Dataset
We evaluate the proposed approach on the task
of English-to-French translation. We use a bilin-
gual, parallel corpus of 348M words selected
by the method of (Axelrod et al., 2011) from
a combination of Europarl (61M), news com-
mentary (5.5M), UN (421M) and two crawled
corpora of 90M and 780M words respectively.
2
The performance of our models was tested
on news-test2012, news-test2013, and
news-test2014. When comparing with the
phrase-based SMT system Moses (Koehn et al.,
2007), the first two were used as a development set
for tuning Moses while news-test2014 was
used as our test set.
To train the neural network models, we use only
the sentence pairs in the parallel corpus, where
both English and French sentences are at most 30
words long. Furthermore, we limit our vocabu-
lary size to the 30,000 most frequent words for
both English and French. All other words are con-
sidered unknown and mapped to a special token
([UNK]).
In both neural network training and automatic
segmentation, we do not incorporate any domain-
specific knowledge, except when tokenizing the
original text data.
2
The datasets and trained Moses models can be down-
loaded from http://www-lium.univ-lemans.fr/
?
schwenk/cslm_joint_paper/ and the website of
ACL 2014 Ninth Workshop on Statistical Machine Transla-
tion (WMT 14).
4.2 Models and Approaches
We compare the proposed segmentation-based
translation scheme against the same neural net-
work model translations without segmentation.
The neural machine translation is done by an RNN
Encoder?Decoder (RNNenc) (Cho et al., 2014b)
trained to maximize the conditional probability
of a French translation given an English sen-
tence. Once the RNNenc is trained, an approxi-
mate beam-search is used to find possible transla-
tions with high likelihood.
3
This RNNenc is used for the proposed
segmentation-based approach together with an-
other RNNenc trained to translate from French to
English. The two RNNenc?s are used in the pro-
posed segmentation algorithm to compute the con-
fidence score of each phrase (See Eqs. (2)?(3)).
We also compare with the translations of a con-
ventional phrase-based machine translation sys-
tem, which we expect to be more robust when
translating long sentences.
5 Results and Analysis
5.1 Validity of the Automatic Segmentation
We validate the proposed segmentation algorithm
described in Sec. 3 by comparing against two
baseline segmentation approaches. The first one
randomly segments an input sentence such that the
distribution of the lengths of random segments has
its mean and variance identical to those of the seg-
ments produced by our algorithm. The second ap-
proach follows the proposed algorithm, however,
using a uniform random confidence score.
From Table 1 we can clearly see that the pro-
3
In all experiments, the beam width is 10.
81
Model Test set
No segmentation 13.15
Random segmentation 16.60
Random confidence score 16.76
Proposed segmentation 20.86
Table 1: BLEU score computed on
news-test2014 for two control experi-
ments. Random segmentation refers to randomly
segmenting a sentence so that the mean and
variance of the segment lengths corresponded to
the ones our best segmentation method. Random
confidence score refers to segmenting a sentence
with randomly generated confidence score for
each segment.
posed segmentation algorithm results in signifi-
cantly better performance. One interesting phe-
nomenon is that any random segmentation was
better than the direct translation without any seg-
mentation. This indirectly agrees well with the
previous finding in (Cho et al., 2014a) that the
neural machine translation suffers from long sen-
tences.
5.2 Importance of Using an Inverse Model
0 2 4 6 8 10
Max. number of unknown words
?9
?8
?7
?6
?5
?4
?3
?2
?1
0
B
L
E
U
s
c
o
r
e
d
e
c
r
e
a
s
e
With segm.
Without segm.
Figure 3: BLEU score loss vs. maximum number
of unknown words in source and target sentence
when translating with the RNNenc model with and
without segmentation.
The proposed confidence score averages the
scores of a translation model p(f | e) and an in-
verse translation model p(e | f) and penalizes for
short phrases. However, it is possible to use alter-
nate definitions of confidence score. For instance,
one may use only the ?direct? translation model or
varying penalties for phrase lengths.
In this section, we test three different confidence
score:
p(f | e) Using a single translation model
p(f | e) + p(e | f) Using both direct and reverse
translation models without the short phrase
penalty
p(f | e) + p(e | f) (p) Using both direct and re-
verse translation models together with the
short phrase penalty
The results in Table 2 clearly show the impor-
tance of using both translation and inverse trans-
lation models. Furthermore, we were able to get
the best performance by incorporating the short
phrase penalty (the denominator in Eq. (2)). From
here on, thus, we only use the original formula-
tion of the confidence score which uses the both
models and the penalty.
5.3 Quantitative and Qualitative Analysis
Model Dev Test
A
l
l
RNNenc 13.15 13.92
p(f | e) 12.49 13.57
p(f | e) + p(e | f) 18.82 20.10
p(f | e) + p(e | f) (p) 19.39 20.86
Moses 30.64 33.30
N
o
U
N
K
RNNenc 21.01 23.45
p(f | e) 20.94 22.62
p(f | e) + p(e | f) 23.05 24.63
p(f | e) + p(e | f) (p) 23.93 26.46
Moses 32.77 35.63
Table 2: BLEU scores computed on the develop-
ment and test sets. See the text for the description
of each approach. Moses refers to the scores by
the conventional phrase-based translation system.
The top five rows consider all sentences of each
data set, whilst the bottom five rows includes only
sentences with no unknown words
As expected, translation with the proposed ap-
proach helps significantly with translating long
sentences (see Fig. 2). We observe that trans-
lation performance does not drop for sentences
of lengths greater than those used to train the
RNNenc (? 30 words).
Similarly, in Fig. 3 we observe that translation
quality of the proposed approach is more robust
82
Source Between the early 1970s , when the Boeing 747 jumbo defined modern long-haul travel , and
the turn of the century , the weight of the average American 40- to 49-year-old male increased
by 10 per cent , according to U.S. Health Department Data .
Segmentation [[ Between the early 1970s , when the Boeing 747 jumbo defined modern long-haul travel ,]
[ and the turn of the century , the weight of the average American 40- to 49-year-old male] [
increased by 10 per cent , according to U.S. Health Department Data .]]
Reference Entre le d?ebut des ann?ees 1970 , lorsque le jumbo 747 de Boeing a d?efini le voyage long-courrier
moderne , et le tournant du si`ecle , le poids de l? Am?ericain moyen de 40 `a 49 ans a augment?e
de 10 % , selon les donn?ees du d?epartement am?ericain de la Sant?e .
With
segmentation
Entre les ann?ees 70 , lorsque le Boeing Boeing a d?efini le transport de voyageurs modernes ; et
la fin du si`ecle , le poids de la moyenne am?ericaine moyenne `a l? ?egard des hommes a augment?e
de 10 % , conform?ement aux donn?ees fournies par le U.S. Department of Health Affairs .
Without
segmentation
Entre les ann?ees 1970 , lorsque les avions de service Boeing ont d?epass?e le prix du travail , le
taux moyen ?etait de 40 % .
Source During his arrest Ditta picked up his wallet and tried to remove several credit cards but they
were all seized and a hair sample was taken fom him.
Segmentation [[During his arrest Ditta] [picked up his wallet and tried to remove several credit cards but they
were all seized and] [a hair sample was taken from him.]]
Reference Au cours de son arrestation , Ditta a ramass?e son portefeuille et a tent?e de retirer plusieurs cartes
de cr?edit , mais elles ont toutes ?et?e saisies et on lui a pr?elev?e un ?echantillon de cheveux .
With
segmentation
Pendant son arrestation J? ai utilis?e son portefeuille et a essay?e de retirer plusieurs cartes de
cr?edit mais toutes les pi`eces ont ?et?e saisies et un ?echantillon de cheveux a ?et?e enlev?e.
Without
segmentation
Lors de son arrestation il a tent?e de r?ecup?erer plusieurs cartes de cr?edit mais il a ?et?e saisi de tous
les coups et des blessures.
Source ?We can now move forwards and focus on the future and on the 90 % of assets that make up a
really good bank, and on building a great bank for our clients and the United Kingdom,? new
director general, Ross McEwan, said to the press .
Segmentation [[?We can now move forwards and focus on the future] [and] [on the 90 % of assets that make
up a really good bank, and on building] [a great bank for our clients and the United Kingdom,?]
[new director general, Ross McEwan, said to the press.]]
Reference ?Nous pouvons maintenant aller de l?avant , nous pr?eoccuper de l?avenir et des 90 % des actifs
qui constituent une banque vraiment bonne et construire une grande banque pour la client`ele et
pour le Royaume Uni?, a dit le nouveau directeur g?en?eral Ross McEwan `a la presse .
With
segmentation
?Nous pouvons maintenant passer `a l?avenir et se concentrer sur l avenir ou sur les 90 % d actifs
qui constituent une bonne banque et sur la construction une grande banque de nos clients et du
Royaume-Uni? Le nouveau directeur g?en?eral Ross Ross a dit que la presse.
Without
segmentation
?Nous pouvons maintenant passer et ?etudier les 90 % et mettre en place une banque importante
pour la nouvelle banque et le directeur g?en?eral? a soulign?e le journaliste .
Source There are several beautiful flashes - the creation of images has always been one of Chouinard?s
strong points - like the hair that is ruffled or the black fabric that extends the lines.
Segmentation [[There are several beautiful flashes - the creation of images has always been one of Chouinard?s
strong points -] [like the hair that is ruffled or the black fabric that extends the lines.]]
Reference Il y a quelques beaux flashs - la cr?eation d?images a toujours ?et?e une force chez Chouinard -
comme ces ch eveux qui s??ebouriffent ou ces tissus noirs qui allongent les lignes .
With
segmentation
Il existe plusieurs belles images - la cr?eation d images a toujours ?et?e l un de ses points forts .
comme les cheveux comme le vernis ou le tissu noir qui ?etend les lignes.
Without
segmentation
Il existe plusieurs points forts : la cr?eation d images est toujours l un des points forts .
Source Without specifying the illness she was suffering from, the star performer of ?Respect? confirmed
to the media on 16 October that the side effects of a treatment she was receiving were ?difficult?
to deal with.
Segmentation [[Without specifying the illness she was suffering from, the star performer of ?Respect?] [con-
firmed to the media on 16 October that the side effects of a treatment she was receiving were]
[?difficult? to deal with.]]
Reference Sans pr?eciser la maladie dont elle souffrait , la c?el`ebre interpr`ete de Respect avait affirm?e aux
m?edias le 16 octobre que les effets secondaires d?un traitement qu?elle recevait ?etaient ?diffi-
ciles?.
With
segmentation
Sans pr?eciser la maladie qu?elle souffrait la star de l? ??uvre? de ?respect?. Il a ?et?e confirm?e
aux m?edias le 16 octobre que les effets secondaires d?un traitement ont ?et?e rec?us. ?difficile? de
traiter .
Without
segmentation
Sans la pr?ecision de la maladie elle a eu l?impression de ?marquer le 16 avril? les effets d?un tel
?traitement?.
Table 3: Sample translations with the RNNenc model taken from the test set along with the source
sentences and the reference translations.
83
Source He nevertheless praised the Government for responding to his request for urgent assis-
tance which he first raised with the Prime Minister at the beginning of May .
Segmentation [He nevertheless praised the Government for responding to his request for urgent assis-
tance which he first raised ] [with the Prime Minister at the beginning of May . ]
Reference Il a n?eanmoins f?elicit?e le gouvernement pour avoir r?epondu `a la demande d? aide urgente
qu?il a pr?esent?ee au Premier ministre d?ebut mai .
With
segmentation
Il a n?eanmoins f?elicit?e le Gouvernement de r?epondre `a sa demande d? aide urgente qu?il
a soulev
?
ee . avec le Premier ministre d?ebut mai .
Without
segmentation
Il a n?eanmoins f?elicit?e le gouvernement de r?epondre `a sa demande d? aide urgente qu?il
a adress
?
ee au Premier Ministre d?ebut mai .
Table 4: An example where an incorrect segmentation negatively impacts fluency and punctuation.
to the presence of unknown words. We suspect
that the existence of many unknown words make
it harder for the RNNenc to extract the meaning of
the sentence clearly, while this is avoided with the
proposed segmentation approach as it effectively
allows the RNNenc to deal with a less number of
unknown words.
In Table 3, we show the translations of ran-
domly selected long sentences (40 or more words).
Segmentation improves overall translation quality,
agreeing well with our quantitative result. How-
ever, we can also observe a decrease in transla-
tion quality when an input sentence is not seg-
mented into well-formed sentential clauses. Addi-
tionally, the concatenation of independently trans-
lated segments sometimes negatively impacts flu-
ency, punctuation, and capitalization by the RN-
Nenc model. Table 4 shows one such example.
6 Discussion and Conclusion
In this paper we propose an automatic segmen-
tation solution to the ?curse of sentence length?
in neural machine translation. By choosing an
appropriate confidence score based on bidirec-
tional translation models, we observed significant
improvement in translation quality for long sen-
tences.
Our investigation shows that the proposed
segmentation-based translation is more robust to
the presence of unknown words. However, since
each segment is translated in isolation, a segmen-
tation of an input sentence may negatively impact
translation quality, especially the fluency of the
translated sentence, the placement of punctuation
marks and the capitalization of words.
An important research direction in the future is
to investigate how to improve the quality of the
translation obtained by concatenating translated
segments.
Acknowledgments
The authors would like to acknowledge the sup-
port of the following agencies for research funding
and computing support: NSERC, Calcul Qu?ebec,
Compute Canada, the Canada Research Chairs
and CIFAR.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 355?362. Association for Compu-
tational Linguistics.
Nicolas Boulanger-Lewandowski, Yoshua Bengio, and
Pascal Vincent. 2013. Audio chord recognition with
recurrent neural networks. In ISMIR.
Kyunghyun Cho, Bart van Merri?enboer, Dzmitry Bah-
danau, and Yoshua Bengio. 2014a. On the
properties of neural machine translation: Encoder?
Decoder approaches. In Eighth Workshop on Syn-
tax, Semantics and Structure in Statistical Transla-
tion, October.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014b. Learning phrase representa-
tions using rnn encoder-decoder for statistical ma-
chine translation. In Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), October. to appear.
A. Graves, A. Mohamed, and G. Hinton. 2013. Speech
recognition with deep recurrent neural networks.
ICASSP.
A. Graves. 2013. Generating sequences with recurrent
neural networks. arXiv:1308.0850 [cs.NE],
August.
Nal Kalchbrenner and Phil Blunsom. 2013. Two re-
current continuous translation models. In Proceed-
ings of the ACL Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1700?1709. Association for Computational Linguis-
tics.
84
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Annual meet-
ing of the association for computational linguistics
(acl). Prague, Czech Republic. demonstration ses-
sion.
Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014.
Anonymized. In Anonymized. under review.
85
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103?111,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
On the Properties of Neural Machine Translation: Encoder?Decoder
Approaches
Kyunghyun Cho Bart van Merri
?
enboer
Universit?e de Montr?eal
Dzmitry Bahdanau
?
Jacobs University Bremen, Germany
Yoshua Bengio
Universit?e de Montr?eal, CIFAR Senior Fellow
Abstract
Neural machine translation is a relatively
new approach to statistical machine trans-
lation based purely on neural networks.
The neural machine translation models of-
ten consist of an encoder and a decoder.
The encoder extracts a fixed-length repre-
sentation from a variable-length input sen-
tence, and the decoder generates a correct
translation from this representation. In this
paper, we focus on analyzing the proper-
ties of the neural machine translation us-
ing two models; RNN Encoder?Decoder
and a newly proposed gated recursive con-
volutional neural network. We show that
the neural machine translation performs
relatively well on short sentences without
unknown words, but its performance de-
grades rapidly as the length of the sentence
and the number of unknown words in-
crease. Furthermore, we find that the pro-
posed gated recursive convolutional net-
work learns a grammatical structure of a
sentence automatically.
1 Introduction
A new approach for statistical machine transla-
tion based purely on neural networks has recently
been proposed (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014). This new approach, which
we refer to as neural machine translation, is in-
spired by the recent trend of deep representational
learning. All the neural network models used in
(Sutskever et al., 2014; Cho et al., 2014) consist of
an encoder and a decoder. The encoder extracts a
fixed-length vector representation from a variable-
length input sentence, and from this representation
the decoder generates a correct, variable-length
target translation.
?
Research done while visiting Universit?e de Montr?eal
The emergence of the neural machine transla-
tion is highly significant, both practically and the-
oretically. Neural machine translation models re-
quire only a fraction of the memory needed by
traditional statistical machine translation (SMT)
models. The models we trained for this paper
require only 500MB of memory in total. This
stands in stark contrast with existing SMT sys-
tems, which often require tens of gigabytes of
memory. This makes the neural machine trans-
lation appealing in practice. Furthermore, un-
like conventional translation systems, each and ev-
ery component of the neural translation model is
trained jointly to maximize the translation perfor-
mance.
As this approach is relatively new, there has not
been much work on analyzing the properties and
behavior of these models. For instance: What
are the properties of sentences on which this ap-
proach performs better? How does the choice of
source/target vocabulary affect the performance?
In which cases does the neural machine translation
fail?
It is crucial to understand the properties and be-
havior of this new neural machine translation ap-
proach in order to determine future research di-
rections. Also, understanding the weaknesses and
strengths of neural machine translation might lead
to better ways of integrating SMT and neural ma-
chine translation systems.
In this paper, we analyze two neural machine
translation models. One of them is the RNN
Encoder?Decoder that was proposed recently in
(Cho et al., 2014). The other model replaces the
encoder in the RNN Encoder?Decoder model with
a novel neural network, which we call a gated
recursive convolutional neural network (grConv).
We evaluate these two models on the task of trans-
lation from French to English.
Our analysis shows that the performance of
the neural machine translation model degrades
103
quickly as the length of a source sentence in-
creases. Furthermore, we find that the vocabulary
size has a high impact on the translation perfor-
mance. Nonetheless, qualitatively we find that the
both models are able to generate correct transla-
tions most of the time. Furthermore, the newly
proposed grConv model is able to learn, without
supervision, a kind of syntactic structure over the
source language.
2 Neural Networks for Variable-Length
Sequences
In this section, we describe two types of neural
networks that are able to process variable-length
sequences. These are the recurrent neural net-
work and the proposed gated recursive convolu-
tional neural network.
2.1 Recurrent Neural Network with Gated
Hidden Neurons
z
rh h~ x
(a) (b)
Figure 1: The graphical illustration of (a) the re-
current neural network and (b) the hidden unit that
adaptively forgets and remembers.
A recurrent neural network (RNN, Fig. 1 (a))
works on a variable-length sequence x =
(x
1
,x
2
, ? ? ? ,x
T
) by maintaining a hidden state h
over time. At each timestep t, the hidden state h
(t)
is updated by
h
(t)
= f
(
h
(t?1)
,x
t
)
,
where f is an activation function. Often f is as
simple as performing a linear transformation on
the input vectors, summing them, and applying an
element-wise logistic sigmoid function.
An RNN can be used effectively to learn a dis-
tribution over a variable-length sequence by learn-
ing the distribution over the next input p(x
t+1
|
x
t
, ? ? ? ,x
1
). For instance, in the case of a se-
quence of 1-of-K vectors, the distribution can be
learned by an RNN which has as an output
p(x
t,j
= 1 | x
t?1
, . . . ,x
1
) =
exp
(
w
j
h
?t?
)
?
K
j
?
=1
exp
(
w
j
?
h
?t?
)
,
for all possible symbols j = 1, . . . ,K, where w
j
are the rows of a weight matrix W. This results in
the joint distribution
p(x) =
T
?
t=1
p(x
t
| x
t?1
, . . . , x
1
).
Recently, in (Cho et al., 2014) a new activation
function for RNNs was proposed. The new activa-
tion function augments the usual logistic sigmoid
activation function with two gating units called re-
set, r, and update, z, gates. Each gate depends on
the previous hidden state h
(t?1)
, and the current
input x
t
controls the flow of information. This is
reminiscent of long short-term memory (LSTM)
units (Hochreiter and Schmidhuber, 1997). For
details about this unit, we refer the reader to (Cho
et al., 2014) and Fig. 1 (b). For the remainder of
this paper, we always use this new activation func-
tion.
2.2 Gated Recursive Convolutional Neural
Network
Besides RNNs, another natural approach to deal-
ing with variable-length sequences is to use a re-
cursive convolutional neural network where the
parameters at each level are shared through the
whole network (see Fig. 2 (a)). In this section, we
introduce a binary convolutional neural network
whose weights are recursively applied to the input
sequence until it outputs a single fixed-length vec-
tor. In addition to a usual convolutional architec-
ture, we propose to use the previously mentioned
gating mechanism, which allows the recursive net-
work to learn the structure of the source sentences
on the fly.
Let x = (x
1
,x
2
, ? ? ? ,x
T
) be an input sequence,
where x
t
? R
d
. The proposed gated recursive
convolutional neural network (grConv) consists of
four weight matrices W
l
, W
r
, G
l
and G
r
. At
each recursion level t ? [1, T ? 1], the activation
of the j-th hidden unit h
(t)
j
is computed by
h
(t)
j
= ?
c
?
h
(t)
j
+ ?
l
h
(t?1)
j?1
+ ?
r
h
(t?1)
j
, (1)
where ?
c
, ?
l
and ?
r
are the values of a gater that
sum to 1. The hidden unit is initialized as
h
(0)
j
= Ux
j
,
where U projects the input into a hidden space.
104
?h
~
(a) (b) (c) (d)
Figure 2: The graphical illustration of (a) the recursive convolutional neural network and (b) the proposed
gated unit for the recursive convolutional neural network. (c?d) The example structures that may be
learned with the proposed gated unit.
The new activation
?
h
(t)
j
is computed as usual:
?
h
(t)
j
= ?
(
W
l
h
(t)
j?1
+W
r
h
(t)
j
)
,
where ? is an element-wise nonlinearity.
The gating coefficients ??s are computed by
?
?
?
c
?
l
?
r
?
?
=
1
Z
exp
(
G
l
h
(t)
j?1
+G
r
h
(t)
j
)
,
where G
l
,G
r
? R
3?d
and
Z =
3
?
k=1
[
exp
(
G
l
h
(t)
j?1
+G
r
h
(t)
j
)]
k
.
According to this activation, one can think of
the activation of a single node at recursion level t
as a choice between either a new activation com-
puted from both left and right children, the acti-
vation from the left child, or the activation from
the right child. This choice allows the overall
structure of the recursive convolution to change
adaptively with respect to an input sample. See
Fig. 2 (b) for an illustration.
In this respect, we may even consider the pro-
posed grConv as doing a kind of unsupervised
parsing. If we consider the case where the gat-
ing unit makes a hard decision, i.e., ? follows an
1-of-K coding, it is easy to see that the network
adapts to the input and forms a tree-like structure
(See Fig. 2 (c?d)). However, we leave the further
investigation of the structure learned by this model
for future research.
3 Purely Neural Machine Translation
3.1 Encoder?Decoder Approach
The task of translation can be understood from the
perspective of machine learning as learning the
Economic growth has slowed down in recent years .
La croissance ?conomique a ralenti ces derni?res ann?es .
[z  ,z  , ... ,z  ]1 2 d
Encode
Decode
Figure 3: The encoder?decoder architecture
conditional distribution p(f | e) of a target sen-
tence (translation) f given a source sentence e.
Once the conditional distribution is learned by a
model, one can use the model to directly sample
a target sentence given a source sentence, either
by actual sampling or by using a (approximate)
search algorithm to find the maximum of the dis-
tribution.
A number of recent papers have proposed to
use neural networks to directly learn the condi-
tional distribution from a bilingual, parallel cor-
pus (Kalchbrenner and Blunsom, 2013; Cho et al.,
2014; Sutskever et al., 2014). For instance, the au-
thors of (Kalchbrenner and Blunsom, 2013) pro-
posed an approach involving a convolutional n-
gram model to extract a vector of a source sen-
tence which is decoded with an inverse convolu-
tional n-gram model augmented with an RNN. In
(Sutskever et al., 2014), an RNN with LSTM units
was used to encode a source sentence and starting
from the last hidden state, to decode a target sen-
tence. Similarly, the authors of (Cho et al., 2014)
proposed to use an RNN to encode and decode a
pair of source and target phrases.
At the core of all these recent works lies an
encoder?decoder architecture (see Fig. 3). The
encoder processes a variable-length input (source
sentence) and builds a fixed-length vector repre-
sentation (denoted as z in Fig. 3). Conditioned on
the encoded representation, the decoder generates
105
a variable-length sequence (target sentence).
Before (Sutskever et al., 2014) this encoder?
decoder approach was used mainly as a part of the
existing statistical machine translation (SMT) sys-
tem. This approach was used to re-rank the n-best
list generated by the SMT system in (Kalchbren-
ner and Blunsom, 2013), and the authors of (Cho
et al., 2014) used this approach to provide an ad-
ditional score for the existing phrase table.
In this paper, we concentrate on analyzing the
direct translation performance, as in (Sutskever et
al., 2014), with two model configurations. In both
models, we use an RNN with the gated hidden
unit (Cho et al., 2014), as this is one of the only
options that does not require a non-trivial way to
determine the target length. The first model will
use the same RNN with the gated hidden unit as
an encoder, as in (Cho et al., 2014), and the second
one will use the proposed gated recursive convo-
lutional neural network (grConv). We aim to un-
derstand the inductive bias of the encoder?decoder
approach on the translation performance measured
by BLEU.
4 Experiment Settings
4.1 Dataset
We evaluate the encoder?decoder models on the
task of English-to-French translation. We use the
bilingual, parallel corpus which is a set of 348M
selected by the method in (Axelrod et al., 2011)
from a combination of Europarl (61M words),
news commentary (5.5M), UN (421M) and two
crawled corpora of 90M and 780M words respec-
tively.
1
We did not use separate monolingual data.
The performance of the neural machien transla-
tion models was measured on the news-test2012,
news-test2013 and news-test2014 sets ( 3000 lines
each). When comparing to the SMT system, we
use news-test2012 and news-test2013 as our de-
velopment set for tuning the SMT system, and
news-test2014 as our test set.
Among all the sentence pairs in the prepared
parallel corpus, for reasons of computational ef-
ficiency we only use the pairs where both English
and French sentences are at most 30 words long to
train neural networks. Furthermore, we use only
the 30,000 most frequent words for both English
and French. All the other rare words are consid-
1
All the data can be downloaded from http:
//www-lium.univ-lemans.fr/
?
schwenk/cslm_
joint_paper/.
ered unknown and are mapped to a special token
([UNK]).
4.2 Models
We train two models: The RNN Encoder?
Decoder (RNNenc)(Cho et al., 2014) and the
newly proposed gated recursive convolutional
neural network (grConv). Note that both models
use an RNN with gated hidden units as a decoder
(see Sec. 2.1).
We use minibatch stochastic gradient descent
with AdaDelta (Zeiler, 2012) to train our two mod-
els. We initialize the square weight matrix (transi-
tion matrix) as an orthogonal matrix with its spec-
tral radius set to 1 in the case of the RNNenc and
0.4 in the case of the grConv. tanh and a rectifier
(max(0, x)) are used as the element-wise nonlin-
ear functions for the RNNenc and grConv respec-
tively.
The grConv has 2000 hidden neurons, whereas
the RNNenc has 1000 hidden neurons. The word
embeddings are 620-dimensional in both cases.
2
Both models were trained for approximately 110
hours, which is equivalent to 296,144 updates and
846,322 updates for the grConv and RNNenc, re-
spectively.
4.2.1 Translation using Beam-Search
We use a basic form of beam-search to find a trans-
lation that maximizes the conditional probability
given by a specific model (in this case, either the
RNNenc or the grConv). At each time step of
the decoder, we keep the s translation candidates
with the highest log-probability, where s = 10
is the beam-width. During the beam-search, we
exclude any hypothesis that includes an unknown
word. For each end-of-sequence symbol that is se-
lected among the highest scoring candidates the
beam-width is reduced by one, until the beam-
width reaches zero.
The beam-search to (approximately) find a se-
quence of maximum log-probability under RNN
was proposed and used successfully in (Graves,
2012) and (Boulanger-Lewandowski et al., 2013).
Recently, the authors of (Sutskever et al., 2014)
found this approach to be effective in purely neu-
ral machine translation based on LSTM units.
2
In all cases, we train the whole network including the
word embedding matrix. The embedding dimensionality was
chosen to be quite large, as the preliminary experiments
with 155-dimensional embeddings showed rather poor per-
formance.
106
Model Development Test
A
l
l
RNNenc 13.15 13.92
grConv 9.97 9.97
Moses 30.64 33.30
Moses+RNNenc
?
31.48 34.64
Moses+LSTM
?
32 35.65
N
o
U
N
K
RNNenc 21.01 23.45
grConv 17.19 18.22
Moses 32.77 35.63
Model Development Test
A
l
l
RNNenc 19.12 20.99
grConv 16.60 17.50
Moses 28.92 32.00
N
o
U
N
K
RNNenc 24.73 27.03
grConv 21.74 22.94
Moses 32.20 35.40
(a) All Lengths
(b) 10?20 Words
Table 1: BLEU scores computed on the development and test sets. The top three rows show the scores on
all the sentences, and the bottom three rows on the sentences having no unknown words. (?) The result
reported in (Cho et al., 2014) where the RNNenc was used to score phrase pairs in the phrase table. (?)
The result reported in (Sutskever et al., 2014) where an encoder?decoder with LSTM units was used to
re-rank the n-best list generated by Moses.
When we use the beam-search to find the k best
translations, we do not use a usual log-probability
but one normalized with respect to the length of
the translation. This prevents the RNN decoder
from favoring shorter translations, behavior which
was observed earlier in, e.g., (Graves, 2013).
5 Results and Analysis
5.1 Quantitative Analysis
In this paper, we are interested in the properties
of the neural machine translation models. Specif-
ically, the translation quality with respect to the
length of source and/or target sentences and with
respect to the number of words unknown to the
model in each source/target sentence.
First, we look at how the BLEU score, reflect-
ing the translation performance, changes with re-
spect to the length of the sentences (see Fig. 4 (a)?
(b)). Clearly, both models perform relatively well
on short sentences, but suffer significantly as the
length of the sentences increases.
We observe a similar trend with the number of
unknown words, in Fig. 4 (c). As expected, the
performance degrades rapidly as the number of
unknown words increases. This suggests that it
will be an important challenge to increase the size
of vocabularies used by the neural machine trans-
lation system in the future. Although we only
present the result with the RNNenc, we observed
similar behavior for the grConv as well.
In Table 1 (a), we present the translation perfor-
mances obtained using the two models along with
the baseline phrase-based SMT system.
3
Clearly
the phrase-based SMT system still shows the su-
perior performance over the proposed purely neu-
ral machine translation system, but we can see that
under certain conditions (no unknown words in
both source and reference sentences), the differ-
ence diminishes quite significantly. Furthermore,
if we consider only short sentences (10?20 words
per sentence), the difference further decreases (see
Table 1 (b).
Furthermore, it is possible to use the neural ma-
chine translation models together with the existing
phrase-based system, which was found recently in
(Cho et al., 2014; Sutskever et al., 2014) to im-
prove the overall translation performance (see Ta-
ble 1 (a)).
This analysis suggests that that the current neu-
ral translation approach has its weakness in han-
dling long sentences. The most obvious explana-
tory hypothesis is that the fixed-length vector rep-
resentation does not have enough capacity to en-
code a long sentence with complicated structure
and meaning. In order to encode a variable-length
sequence, a neural network may ?sacrifice? some
of the important topics in the input sentence in or-
der to remember others.
This is in stark contrast to the conventional
phrase-based machine translation system (Koehn
et al., 2003). As we can see from Fig. 5, the
conventional system trained on the same dataset
(with additional monolingual data for the language
model) tends to get a higher BLEU score on longer
3
We used Moses as a baseline, trained with additional
monolingual data for a 4-gram language model.
107
Source She explained her new position of foreign affairs and security policy representative as a reply to a
question: ?Who is the European Union? Which phone number should I call??; i.e. as an important step
to unification and better clarity of Union?s policy towards countries such as China or India.
Reference Elle a expliqu?e le nouveau poste de la Haute repr?esentante pour les affaires ?etrang`eres et la politique de
d?efense dans le cadre d?une r?eponse `a la question: ?Qui est qui `a l?Union europ?eenne?? ?A quel num?ero
de t?el?ephone dois-je appeler??, donc comme un pas important vers l?unicit?e et une plus grande lisibilit?e
de la politique de l?Union face aux ?etats, comme est la Chine ou bien l?Inde.
RNNEnc Elle a d?ecrit sa position en mati`ere de politique ?etrang`ere et de s?ecurit?e ainsi que la politique de l?Union
europ?eenne en mati`ere de gouvernance et de d?emocratie .
grConv Elle a expliqu?e sa nouvelle politique ?etrang`ere et de s?ecurit?e en r?eponse `a un certain nombre de questions
: ?Qu?est-ce que l?Union europ?eenne ? ? .
Moses Elle a expliqu?e son nouveau poste des affaires ?etrang`eres et la politique de s?ecurit?e repr?esentant en
r?eponse `a une question: ?Qui est l?Union europ?eenne? Quel num?ero de t?el?ephone dois-je appeler??;
c?est comme une ?etape importante de l?unification et une meilleure lisibilit?e de la politique de l?Union
`a des pays comme la Chine ou l?Inde .
Source The investigation should be complete by the end of the year when the findings will be presented to
Deutsche Bank?s board of managing directors - with recommendations for action.
Reference L?examen doit ?etre termin?e d?ici la fin de l?ann?ee, ensuite les r?esultats du conseil d?administration de la
Deutsche Bank doivent ?etre pr?esent?es - avec recommandation, d? habitude.
RNNEnc L??etude devrait ?etre termin?ee `a la fin de l? ann?ee, lorsque les conclusions seront pr?esent?ees au conseil
d?administration de la Deutsche Bank, conseil d?association avec des mesures.
grConv L?enqu?ete devrait ?etre termin?ee `a la fin de l?ann?ee o`u les conclusions seront pr?esent?ees par le conseil
d?administration de la BCE `a la direction des recommandations.
Moses L?enqu?ete devrait ?etre termin?e d?ici la fin de l?ann?ee lorsque les r?esultats seront pr?esent?es `a la Deutsche
Bank conseil des directeurs g?en?eraux - avec des recommandations .
Source And there are thorny mechanical questions that must be resolved during that time, like how to bal-
ance the state?s mandate of ?adequate access? to licensed marijuana with its prohibitions on cannabis
businesses within 1,000 feet of a school, park, playground or child care center.
Reference Pendant ce temps, des questions pratiques restent en suspens: comment ?equilibrer le mandat de l??etat
qui garantit un acc`es appropri?e `a la marijuana agr?e?ee et interdit l?installation de commerces de vente de
cannabis dans un rayon de 30 km autour d?une ?ecole, d?un parc, d?un terrain de jeu ou d?une cr`eche.
RNNEnc Il y a des questions pr?eventives qui se posent quant `a l??equilibre des droits de l?enfant dans les limites
d?une entreprise de collecte de sang.
grConv De fac?on g?en?erale, il y a des raisons de s?ecurit?e pour que les entreprises aient acc`es `a des milliers de
centres de p?eche, d?eau ou de recherche.
Moses Et il y a des probl`emes m?ecaniques complexes qui doivent ?etre r?esolues au cours de cette p?eriode,
comme la mani`ere d??equilibrer le mandat de ?l?acc`es ad?equat? permis de marijuana avec l?interdiction
du cannabis aux entreprises de 1000 pieds d?une ?ecole de jeu ou de parc, le service de garde.
(a) Long Sentences
Source There is still no agreement as to which election rules to follow.
Reference Toutefois il n?existe toujours pas d?accord selon quel r`eglement de vote il faut proc?eder.
RNNEnc Il n?y a pas encore d?accord sur les r`egles ?electorales.
grConv Il n?y a pas encore d?accord sur la question des ?elections `a suivre.
Moses Il y a toujours pas d?accord sur l??election des r`egles `a suivre.
Source Many of these ideas may have been creative, but they didn?t necessarily work.
Reference Beaucoup de ces id?ees ?etaient cr?eatives mais elles n?ont pas forc?ement fonctionn?e.
RNNEnc Bon nombre de ces id?ees ont peut-?etre ?et?e cr?eatrices, mais elles ne s?appliquaient pas n?ecessairement.
grConv Beaucoup de ces id?ees peuvent ?etre cr?eatives, mais elles n?ont pas fonctionn?e.
Moses Beaucoup de ces id?ees ont pu ?etre cr?eatif, mais ils n?ont pas n?ecessairement.
Source There is a lot of consensus between the Left and the Right on this subject.
Reference C?est qu?il y a sur ce sujet un assez large consensus entre gauche et droite.
RNNEnc Il existe beaucoup de consensus entre la gauche et le droit `a la question.
grConv Il y a un consensus entre la gauche et le droit sur cette question.
Moses Il y a beaucoup de consensus entre la gauche et la droite sur ce sujet.
Source According to them, one can find any weapon at a low price right now.
Reference Selon eux, on peut trouver aujourd?hui `a Moscou n?importe quelle arme pour un prix raisonnable.
RNNEnc Selon eux, on peut se trouver de l?arme `a un prix trop bas.
grConv En tout cas, ils peuvent trouver une arme `a un prix tr`es bas `a la fois.
Moses Selon eux, on trouve une arme `a bas prix pour l?instant.
(b) Short Sentences
Table 2: The sample translations along with the source sentences and the reference translations.
108
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
(a) RNNenc
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
(b) grConv
0 2 4 6 8 10
Max. number of unknown words
10
12
14
16
18
20
22
24
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
(c) RNNenc
Figure 4: The BLEU scores achieved by (a) the RNNenc and (b) the grConv for sentences of a given
length. The plot is smoothed by taking a window of size 10. (c) The BLEU scores achieved by the RNN
model for sentences with less than a given number of unknown words.
sentences.
In fact, if we limit the lengths of both the source
sentence and the reference translation to be be-
tween 10 and 20 words and use only the sentences
with no unknown words, the BLEU scores on the
test set are 27.81 and 33.08 for the RNNenc and
Moses, respectively.
Note that we observed a similar trend even
when we used sentences of up to 50 words to train
these models.
5.2 Qualitative Analysis
Although BLEU score is used as a de-facto stan-
dard metric for evaluating the performance of a
machine translation system, it is not the perfect
metric (see, e.g., (Song et al., 2013; Liu et al.,
2011)). Hence, here we present some of the ac-
tual translations generated from the two models,
RNNenc and grConv.
In Table. 2 (a)?(b), we show the translations of
some randomly selected sentences from the de-
velopment and test sets. We chose the ones that
have no unknown words. (a) lists long sentences
(longer than 30 words), and (b) short sentences
(shorter than 10 words). We can see that, despite
the difference in the BLEU scores, all three mod-
els (RNNenc, grConv and Moses) do a decent job
at translating, especially, short sentences. When
the source sentences are long, however, we no-
tice the performance degradation of the neural ma-
chine translation models.
Additionally, we present here what type of
structure the proposed gated recursive convolu-
tional network learns to represent. With a sample
sentence ?Obama is the President of the United
States?, we present the parsing structure learned
by the grConv encoder and the generated transla-
tions, in Fig. 6. The figure suggests that the gr-
0 10 20 30 40 50 60 70 80
Sentence length
0
5
10
15
20
25
30
35
40
B
L
E
U
s
c
o
r
e
Source text
Reference text
Both
Figure 5: The BLEU scores achieved by an SMT
system for sentences of a given length. The plot
is smoothed by taking a window of size 10. We
use the solid, dotted and dashed lines to show the
effect of different lengths of source, reference or
both of them, respectively.
Conv extracts the vector representation of the sen-
tence by first merging ?of the United States? to-
gether with ?is the President of? and finally com-
bining this with ?Obama is? and ?.?, which is
well correlated with our intuition. Note, however,
that the structure learned by the grConv is differ-
ent from existing parsing approaches in the sense
that it returns soft parsing.
Despite the lower performance the grConv
showed compared to the RNN Encoder?Decoder,
4
we find this property of the grConv learning a
grammar structure automatically interesting and
believe further investigation is needed.
4
However, it should be noted that the number of gradient
updates used to train the grConv was a third of that used to
train the RNNenc. Longer training may change the result,
but for a fair comparison we chose to compare models which
were trained for an equal amount of time. Neither model was
trained to convergence.
109
Obama is the President of the United States .
++++++++
+++++++
++++++
+++++
++++
+++
++
+ Translations
Obama est le Pr?esident des
?
Etats-Unis . (2.06)
Obama est le pr?esident des
?
Etats-Unis . (2.09)
Obama est le pr?esident des Etats-Unis . (2.61)
Obama est le Pr?esident des Etats-Unis . (3.33)
Barack Obama est le pr?esident des
?
Etats-Unis . (4.41)
Barack Obama est le Pr?esident des
?
Etats-Unis . (4.48)
Barack Obama est le pr?esident des Etats-Unis . (4.54)
L?Obama est le Pr?esident des
?
Etats-Unis . (4.59)
L?Obama est le pr?esident des
?
Etats-Unis . (4.67)
Obama est pr?esident du Congr`es des
?
Etats-Unis .(5.09)
(a) (b)
Figure 6: (a) The visualization of the grConv structure when the input is ?Obama is the President of
the United States.?. Only edges with gating coefficient ? higher than 0.1 are shown. (b) The top-10
translations generated by the grConv. The numbers in parentheses are the negative log-probability.
6 Conclusion and Discussion
In this paper, we have investigated the property
of a recently introduced family of machine trans-
lation system based purely on neural networks.
We focused on evaluating an encoder?decoder ap-
proach, proposed recently in (Kalchbrenner and
Blunsom, 2013; Cho et al., 2014; Sutskever et al.,
2014), on the task of sentence-to-sentence trans-
lation. Among many possible encoder?decoder
models we specifically chose two models that dif-
fer in the choice of the encoder; (1) RNN with
gated hidden units and (2) the newly proposed
gated recursive convolutional neural network.
After training those two models on pairs of
English and French sentences, we analyzed their
performance using BLEU scores with respect to
the lengths of sentences and the existence of un-
known/rare words in sentences. Our analysis re-
vealed that the performance of the neural machine
translation suffers significantly from the length of
sentences. However, qualitatively, we found that
the both models are able to generate correct trans-
lations very well.
These analyses suggest a number of future re-
search directions in machine translation purely
based on neural networks.
Firstly, it is important to find a way to scale up
training a neural network both in terms of com-
putation and memory so that much larger vocabu-
laries for both source and target languages can be
used. Especially, when it comes to languages with
rich morphology, we may be required to come up
with a radically different approach in dealing with
words.
Secondly, more research is needed to prevent
the neural machine translation system from under-
performing with long sentences. Lastly, we need
to explore different neural architectures, especially
for the decoder. Despite the radical difference in
the architecture between RNN and grConv which
were used as an encoder, both models suffer from
the curse of sentence length. This suggests that it
may be due to the lack of representational power
in the decoder. Further investigation and research
are required.
In addition to the property of a general neural
machine translation system, we observed one in-
teresting property of the proposed gated recursive
convolutional neural network (grConv). The gr-
Conv was found to mimic the grammatical struc-
ture of an input sentence without any supervision
on syntactic structure of language. We believe this
property makes it appropriate for natural language
processing applications other than machine trans-
lation.
Acknowledgments
The authors would like to acknowledge the sup-
port of the following agencies for research funding
and computing support: NSERC, Calcul Qu?ebec,
Compute Canada, the Canada Research Chairs
and CIFAR.
110
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 355?362. Association for Compu-
tational Linguistics.
Nicolas Boulanger-Lewandowski, Yoshua Bengio, and
Pascal Vincent. 2013. Audio chord recognition with
recurrent neural networks. In ISMIR.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), October. to appear.
Alex Graves. 2012. Sequence transduction with re-
current neural networks. In Proceedings of the
29th International Conference on Machine Learning
(ICML 2012).
A. Graves. 2013. Generating sequences with recurrent
neural networks. arXiv:1308.0850 [cs.NE],
August.
S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. Neural Computation, 9(8):1735?
1780.
Nal Kalchbrenner and Phil Blunsom. 2013. Two re-
current continuous translation models. In Proceed-
ings of the ACL Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1700?1709. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2011. Better evaluation metrics lead to better ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 375?384. Association for Computa-
tional Linguistics.
Xingyi Song, Trevor Cohn, and Lucia Specia. 2013.
BLEU deconstructed: Designing a better MT eval-
uation metric. In Proceedings of the 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLING), March.
Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014.
Anonymized. In Anonymized.
Matthew D. Zeiler. 2012. ADADELTA: an adap-
tive learning rate method. Technical report, arXiv
1212.5701.
111
