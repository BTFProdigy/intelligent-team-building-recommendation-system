Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 183?190,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards A Modular Data Model For Multi-Layer Annotated Corpora
Richard Eckart
Department of English Linguistics
Darmstadt University of Technology
64289 Darmstadt, Germany
eckart@linglit.tu-darmstadt.de
Abstract
In this paper we discuss the current meth-
ods in the representation of corpora anno-
tated at multiple levels of linguistic organi-
zation (so-called multi-level or multi-layer
corpora). Taking five approaches which
are representative of the current practice
in this area, we discuss the commonalities
and differences between them focusing on
the underlying data models. The goal of
the paper is to identify the common con-
cerns in multi-layer corpus representation
and processing so as to lay a foundation
for a unifying, modular data model.
1 Introduction
Five approaches to representing multi-layer anno-
tated corpora are reviewed in this paper. These re-
flect the current practice in the field and show the
requirements typically posed on multi-layer cor-
pus applications. Multi-layer annotated corpora
keep annotations at different levels of linguistic
organization separate from each other. Figure 1
illustrates two annotation layers on a transcrip-
tion of an audio/video signal. One layer contains
a functional annotation of a sentence in the tran-
scription. The other contains a phrase structure
annotation and Part-of-Speech tags for each word.
Layers and signals are coordinated by a common
timeline.
The motivation for this research is rooted
in finding a proper data model for PACE-Ling
(Sec. 2.2). The ultimate goal of our research is to
create a modular extensible data model for multi-
layer annotated corpora. To achieve this, we aim
to create a data model based on the current state-
of-the-art that covers all current requirements and
Figure 1: Multi-layer annotation on multi-modal
base data
then decompose it into exchangeable components.
We identify and discuss objects contained in four
tiers commonly playing an important role in multi-
layer corpus scenarios (see Fig. 2): medial, loca-
tional, structural and featural tiers. These are gen-
eralized categories that are in principle present in
any multi-layer context, but come in different in-
carnations. Since query language and data model
are closely related, common query requirements
are also surveyed and examined for modular de-
composition. While parts of the suggested data
model and query operators are implemented by the
projects discussed here, so far no comprehensive
implementation exists.
2 Data models
There are three purposes data models can serve.
The first purpose is context suitability. A data
model used for this purpose must reflect as well
as possible the data the user wants to query. The
second purpose is storage. The data model used
in the database backend can be very different from
183
the one exposed to the user, e.g. hierarchical struc-
tures may be stored in tables, indices might be
kept to speed up queries, etc. The third purpose
is exchange and archival. Here the data model, or
rather the serialization of the data model, has to be
easily parsable and follow a widely used standard.
Our review focuses on the suitability of data
models for the first purpose. As extensions of
the XML data model are used in most of the ap-
proaches reviewed here, a short introduction to
this data model will be given first.
Figure 2: Tiers and objects
2.1 XML
Today XML has become the de-facto standard
representation format for annotated text corpora.
While the XML standard specifies a data model
and serialization format for XML, a semantics
is largely left to be defined for a particular ap-
plication. Many data models can be mapped to
the XML data model and serialized to XML (cf.
Sec. 2.5).
The XML data model describes an ordered tree
and defines several types of nodes. We examine
a simplification of this data model here, limited
to elements, attributes and text nodes. An ele-
ment (parent) can contain children: elements and
text nodes. Elements are named and can carry at-
tributes, which are identified by a name and bear a
value.
This data model is immediately suitable for sim-
ple text annotations. For example in a positional
annotation, name-value pairs (features) can be as-
signed to tokens, which are obtained via tokeniza-
tion of a text. These features and tokens can
be represented by attributes and text nodes. The
XML data model requires that both share a parent
element which binds them together. Because the
XML data model defines a tree, an additional root
element is required to govern all positional anno-
tation elements.
If the tree is constructed in such a way that
one particular traversal strategy yields all tokens
in their original order, then the data model is ca-
pable of covering all tiers: medial tier (textual
base data), locational tier (sequential token order),
structural tier (tokens) and featural tier (linguis-
tic feature annotations). The structural tier can be
expanded by adding additional elements en-route
from the root element to the text nodes (leaves).
In this way hierarchical structures can be modeled,
for instance constituency structures. However, the
XML data model covers these tiers only in a lim-
ited way. For example, tokens can not overlap
each other without destroying the linear token or-
der and thus sacrificing the temporal tier, a prob-
lem commonly known as overlapping hierarchies.
2.2 PACE-Ling
PACE-Ling (Bartsch et al, 05) aims at develop-
ing register profiles of texts from mechanical engi-
neering (domain: data processing in construction)
based on the multi-dimensional model of Systemic
Functional Linguistics (SFL) (Halliday, 04).
The XML data model is a good foundation for
this project as only written texts are analyzed, but
SFL annotation requires multiple annotation lay-
ers with overlapping hierarchies. To solve this
problem, the project applies a strategy known as
stand-off annotation, first discussed in the context
of SFL in (Teich et al, 05) and based on previous
work by (Teich et al, 01). This strategy separates
the annotation data from the base data and intro-
duces references from the annotations to the base
data, thus allowing to keep multiple layers of an-
notations on the same base data separate.
The tools developed in the project treat anno-
tation data in XML from any source as separate
annotation layers, provided the text nodes in each
layer contain the same base data. The base data is
extracted and kept in a text file and the annotation
layers each in an XML file. The PACE-Ling data
model substitutes text nodes from the XML data
model by segments. Segments carry start and end
attributes which specify the location of the text in
the text file.
An important aspect of the PACE-Ling ap-
proach is minimal invasiveness. The minimally
invasive change of only substituting text nodes by
segments and leaving the rest of the original an-
notation file as it is, makes conversion between
the original format and the format needed by the
PACE-Ling tools very easy.
184
2.3 NITE XML Toolkit
The NITE XML toolkit (NXT) (Carletta et al, 04)
was created with the intention to provide a frame-
work for building applications working with anno-
tated multi-modal data. NXT is based on the NITE
Object Model (NOM) which is an extension of the
XML data model. NOM features a similar separa-
tion of tiers as the PACE-Ling data model, but is
more general.
NOM uses a continuous timeline to coordinate
annotations. Instead of having dedicated segment
elements, any annotation element can have special
start and end attributes that anchor it to the time-
line. This makes the data model less modular, be-
cause support for handling other locational strate-
gies than a timeline can not be added by changing
the semantics of segments (cf. Sec. 3.2).
NXT can deal with audio, video and textual
base data, but due to being limited to the concept
of a single common timeline, it is not possible to
annotate a specific region in one video frame.
NOM introduces a new structural relation be-
tween annotation elements. Arbitrary links can be
created by adding a pointer to an annotation ele-
ment bearing a reference to another annotation ele-
ment which designates the first annotation element
to be a parent of the latter. Each pointer carries a
role attribute describing its use.
Using pointers, arbitrary directed graphs can be
overlaid on annotation layers and annotation el-
ements can have multiple parents, one from the
layer structure and any number of parents indi-
cated by pointer references. This facilitates the
reuse of annotations, e.g. when a number of an-
notations are kept that apply to words, the bound-
aries of words can be defined in one annotation
layer and the other annotations can refer to that
via pointers instead of defining the word bound-
aries explicitly in each layer. Using these pointers
in queries is cumbersome, because they have to be
processed one at a time (Evert et al, 03).
2.4 Deutsch Diachron Digital
The goal of Deutsch Diachron Digital (DDD)
(Faulstich et al, 05) is the creation of a diachronic
corpus, ranging from the earliest Old High Ger-
man or Old Saxon texts from the 9th century up to
Modern German at the end of the 19th century.
DDD requires each text to be available in sev-
eral versions, ranging from the original facsimile
over several transcription versions to translations
into a modern language stage. This calls for a
high degree of alignment between those versions
as well as the annotations on those texts. Due to
the vast amount of data involved in the project, the
data model is not mapped to XML files, but to a
SQL database for a better query performance.
The DDD data model can be seen as an exten-
sion of NOM. Because the corpus contains mul-
tiple versions of documents, coordination of an-
notations and base data along a single timeline is
not sufficient. Therefore DDD segments refer to a
specific version of a document.
DDD defines how alignments are modeled, thus
elevating them from the level of structural anno-
tation to an independent object in the structural
tier: an alignment as a set of elements or segments,
each of which is associated with a role.
Treating alignments as an independent object is
reasonable because they are conceptually different
from pointers and it facilitates providing an effi-
cient storage for alignments.
2.5 ATLAS
The ATLAS project (Laprun et al, 02) imple-
ments a three tier data model model, resembling
the separation of medial, locational and annota-
tion tiers. This approach features two character-
istic traits setting it apart from the others. First
the data model is not inspired by XML, but by
Annotation Graphs (AGs) (Bird & Liberman, 01).
Second, it does not put any restriction on the kind
of base data by leaving the semantics of segments
and anchors undefined.
The ATLAS data model defines signals, ele-
ments, attributes, pointers, segments and anchors.
Signals are base data objects (text, audio, etc.). El-
ements are related to each other only using point-
ers. While elements and pointers can be used to
form trees, the ATLAS data model does not en-
force this. As a result, the problem of overlapping
hierarchies does not apply to the model. Elements
are not contained within layers, instead they carry
a type. However all elements of the same type can
be interpreted as belonging to one layer. Segments
do not carry start and end attributes, they carry a
number of anchors. How exactly anchors are real-
ized depends on the signals and is not specified in
the data model.
The serialization format of ATLAS (AIF) is an
XML dialect, but does not use the provisions for
modeling trees present in the XML data model to
185
represent structural annotations as e.g. NXT does.
The annotation data is stored as a flat set of ele-
ments, pointers, etc., which precludes the efficient
use of existing tools like XPath to do structural
queries. This is especially inconvenient as the AT-
LAS project does not provide a query language
and query engine yet.
2.6 ISO 24610-1 - Feature Structures
The philosophy behind (ISO-24610-1, 06) is dif-
ferent from that of the four previous approaches.
Here the base data is an XML document con-
forming to the TEI standard (Sperberg-McQueen
& Burnard, 02). XML elements in the TEI base
data can reference feature stuctures. A feature
structure is a single-rooted graph, not necessarily
a tree. The inner nodes of the graph are typed ele-
ments, the leaves are values, which can be shared
amongst elements using pointers or can be ob-
tained functionally from other values.
While in the four previously discussed ap-
proaches the annotations contain references to the
base data in the leaves of the annotation structure,
here the base data contains references to the root
of the annotation structures. This is a powerful
approach to identifying features of base data seg-
ments, but it is not very well suited for represent-
ing constituent hierarchies.
Feature structures put a layer of abstraction on
top of the facilities provided by XML. XML val-
idation schemes are used only to check the well-
formedness of the serialization but not to validate
the features structures. For this purpose feature
structure declarations (FSD) have been defined.
3 A comprehensive data model
This section suggests a data model covering the
objects that have been discussed in the context of
the approaches presented in Sections 2.1-2.6. See
Figure 3 for an overview.
3.1 Objects of the medial tier
We use the term base data for any data we want
to annotate. A single instance of base data is
called signal. Signals can be of many different
kinds such as images (e.g. scans of facsimiles) or
streams of text, audio or video data.
Figure 3: Comprehensive data model
3.2 Objects of the locational tier
Signals live in a virtual multi-dimensional signal
space1. Each point of a signal is mapped to a
unique point in signal space and vice versa. A
segment identifies an area of signal space using a
number of anchors, which uniquely identify points
in signal space.
Depending on the kind of signal the dimen-
sions of signal space have to be interpreted dif-
ferently. For instance streams have a single di-
mension: time. At each point along the time axis,
we may find a character or sound sample. Other
kinds of signals can however have more dimen-
sions: height, width, depth, etc. which can be con-
tinuous or discrete, bounded or open. For instance,
a sheet of paper has two bounded and continuous
dimensions: height and width. Thus a segment to
capture a paragraph may have to describe a poly-
gon. A single sheet of paper does not have a time
dimension, however when multiple sheets are ob-
served, these can be interpreted as a third dimen-
sion of discrete time.
3.3 Objects of the annotational tiers
An annotation element has a name and can have
features, pointers and segments. A pointer is a
typed directed reference to one or more elements.
Elements relate to each other in different ways: di-
rectly by structural relations of the layer, pointers
and alignments and indirectly by locational and
medial relations (cf. Fig. 4).
An annotation layer contains elements and de-
fines structural relations between them, e.g. domi-
nance or neighborhood relations.
1(Laprun et al, 02) calls this feature space. This label is
not used here to avoid suggesting a connection to the featural
tier.
186
An alignment defines an equivalence class of el-
ements, to each of which a role can be assigned.
Pointers can be used for structural relations that
cross-cut the structural model of a layer or to
create a relation across layer boundaries. Each
pointer carries a role that specifies the kind of re-
lation it models. Pointers allow an element to have
multiple parents and to refer to other elements
across annotation layers.
Features have a name and a value. They are al-
ways bound to an annotation element and cannot
exist on their own. For the time being we use this
simple definition of a feature, as it mirrors the con-
cept of XML attributes. However, future work has
to analyze if the ISO 24610 feature structures can
and should be modelled as a part of the structural
tier or if the featural tier should be extended.
4 Query
To make use of annotated corpora, query methods
need to be defined. Depending on the data storage
model that is used, different query languages are
possible, e.g. XQuery for XML or SQL for rela-
tional databases. But these complicate query for-
mulating because they are tailored to query a low
level data storage model rather than a high level
annotation data model.
A high level query language is necessary to get a
good user acceptance and to achieve independence
from lower level data models used to represent an-
notation data in an efficient way. NXT comes with
NQL (Evert et al, 03), a sophisticated declarative
high level query language. NQL is implemented
in a completely new query engine instead of us-
ing XPath, XQuery or SQL. LPath, another recent
development (Bird et al, 06), is a path-like query
language. It is a linguistically motivated extension
of XPath with additional axes and operators that
allow additional queries and simplify others.
In some cases XML or SQL databases are sim-
ply not suited for a specific query. While we might
be able to do regular expression matches on textual
base data in a SQL or XML environment, doing
a similar operation on video base data is beyond
their scope.
The NXT project plans a translation of NQL to
XQuery in order to use existing XQuery engines.
LPath and DDD map high level query languages
to SQL. (Grust et al, 04) are working on translat-
ing XQuery to SQL. The possibility of translating
high level query languages into lower level query
languages seems a good point for modularization.
4.1 Structural queries
Structural query operators are strongly tied to the
structure of annotation layers, because they reflect
the structural relations inside a layer. However, we
also define structural relations such as alignments
and pointers that exist independently of layers (cf.
Sec. 3.3). The separation between pointers, align-
ments and different kinds of layers offers potential
for modularization
Layers allowing only for positional annotations
know only one structural relation: the neigh-
borhood relation between two adjacent positions.
Layers following the XML data model know
parent-child relations and neighborhood relations.
Layers with different internal structures may offer
other relations. A number of possible relations is
shown in Figure 4.
Figure 4: Structural relations and crossing to other
tiers
While the implementation of query operators
depends on the internal layer structure, the syn-
tax does not necessarily have to be different. For
instance a following(a) operator of a positional
layer will yield all elements following element
a. A hierarchical layer can have two kinds of
following operators, one that only yields siblings
following a and one yielding all elements follow-
ing a. Here a choice has to be made if one of these
operators is similar enough to the following(a)
to share that name without confusing the user.
Operators to follow pointers or alignments can
be implemented independently of the layer struc-
ture.
XPath or LPath (Bird et al, 06) are path-like
query languages specifically suited to access hier-
archically structured data, but neither directly sup-
ports alignments, pointers or the locational tier.
In the context of XQuery, XPath can be extended
with user-defined functions that could be used to
provide this access, but using such functions in
path statements can become awkward. It may be a
better idea to extend the path language instead.
187
Structural queries could look like this:
? Which noun phrases are inside verb phrases?
//VP//NP
Result: a set of annotation elements.
? Anaphora are annotated using a pointer with
the role ?anaphor?. What do determiners in
the corpus refer to?
//DET/=>anaphor
Result: a set of annotation elements.
? Translated elements are aligned in an align-
ment called ?translation?. What are the trans-
lations of the current element?
self/#translation
Result: a set of annotation elements.
4.2 Featural queries
If we use the simple definition of features from
Section 3.3, there is only one operator native to
the featural tier that can be used to access the an-
notation element associated with a feature. If we
use the complex definition from ISO 24610, the
operators of the featural tier are largely the same
as in hierarchically structured annotation layers.
Operators to test the value of a feature can not
strictly be assigned to the featural tier. Using the
simple definition, the value of a feature is some
typed atomic value. The query language has to
provide generic operators to compare atomic val-
ues like strings or numbers with each other. E.g.
XPath provides a weakly typed system that pro-
vides such operators.
Queries involving features could look like this:
? What is the value of the ?PoS? feature of the
current annotation element?
self/@PoS
Result: a string value.
? What elements have a feature called ?PoS?
with the value ?N??
//*[@PoS=?N?]
Result: a set of annotation elements.
4.3 Locational queries
Locational queries operate on segment data. The
inner structure of segments reflects the structure
of signal space and different kinds of signals re-
quire different operators. Most of the time opera-
tors working on single continuous dimensions, e.g.
a timeline, will be used. An operator working on
higher dimensions could be an intersection opera-
tor of two dimensional signal space areas (scan of
a newspaper page, video frames, etc.).
Queries involving locations could look like this:
? What parts of segments a and b overlap?
overlap($a,$b)
Result: the empty set or a segment defining
the overlapping part.
? Merge segments a and b.
merge($a, $b)
Result: if a and b overlap, the result is a new
segment that covers both, otherwise the re-
sults is a set consisting of a and b.
? Is segment a following segment b?
is-following($a, $b)
Result: true or false.
Locational operators are probably best bundled
into modules by the kind of locational structure
they support: a module for sequential data such as
text or audio, one for two-dimensional data such
as pictures, and so on.
4.4 Medial queries
Medial query operators access base data, but often
they take locational arguments or return locational
information. When a medial operator is used to
access textual base data, the result is a string. As
with feature values, such a string could be evalu-
ated by a query language that supports some prim-
itive data types.
Assume there is a textual signal named ?plain-
text?. Queries on base data could look like this:
? Where does the string ?rapid? occur?
signal(?plaintext?)/?rapid?
Result: a set of segments.
? Where does the string ?prototyping? occur to
the right of the location of ?rapid??
signal(?plaintext?)/
?rapid?>>?prototyping?
Result: a set of segments.
? What is the base data between offset 5 and 9
of the signal ?plaintext??
signal(?plaintext?)/<{5,9}>
Result: a portion of base data (e.g. a string).
If the base data is an audio or video stream, the
type system of most query languages is likely to
188
be insufficient. In such a case a module provid-
ing support for audio or video storage should also
provide necessary query operators and data type
extensions to the query engine.
4.5 Projection between annotational and
medial tiers
So far we have considered crossing the borders be-
tween the structural and featural tiers and between
the locational and medial tiers. Now we examine
the border between the locational and structural
tier. An operator can be used to collect all loca-
tional data associated with an annotation element
and its children:
seg(//S/VP/)
The result would be a set of potentially overlap-
ping segments. Depending on the query, it will
be necessary to merge overlapping segments to get
a list of non-overlappping segments. Assume we
have a recorded interview annotated for speakers
and at some point speaker A and B speak at the
same time. We want to listen to all parts of the
interview in which speakers A or B speak. If we
query without merging overlapping segments, we
will hear the part in which both speak at the same
time twice.
Similar decisions have to be made when pro-
jecting up from a segment into the structural layer.
Figure 5 shows a hierarchical annotation struc-
ture. Only the elements W1, W2 and W3 bear
segments that anchor them to the base data at the
points A-D.
Figure 5: Example structure
When projecting up from the segment {B,D}
there are a number of potentially desirable results.
Some are given here:
1. no result: because there is no annotation ele-
ment that is anchored to {B,D}.
2. W2 and W3: because both are anchored to
an area inside {B,D}.
3. Phrase 2, W2 and W3: because applying the
seg operator to either element yields seg-
ments inside {B,D}.
4. Phrase 2 only: because applying the seg op-
erator to this element yields an area that cov-
ers exactly {B,D}.
5. Phrase 1, Phrase 2: because applying the
seg operator to either element yields seg-
ments containing {B,D}.
The query language has to provide operators
that enable the user to choose the desired result.
Queries that yield the desired results could look
like in Figure 6. Here the same-extent operator
takes two sets of segments and returns those seg-
ments that are present in both lists and have the
same start and end positions. The anchored oper-
ator takes an annotation element and returns true
if the element is anchored. The contains operator
takes two sets of segments a and b and returns all
segments from set b that are contained in an area
covered by any segment in set a. The grow opera-
tor takes a set of segments and returns a segment,
which starts at the smallest offset and ends at the
largest offset present in any segment of the input
list. In the tests an empty set is interpreted as false
and a non-empty set as true.
1. //*[same-extent(seg(.),
<{B,D}>)]
2. //*[anchored(.) and
contains(<{B,D}>, seg(.))]
3. //*[contains(<{B,D}>, seg(.))]
4. //*[same-extent(grow(seg(.)),
<{B,D}>)]
5. //*[contains(seg(.)), <{B,D}>]
Figure 6: Projection examples
5 Conclusion
Corpus-based research projects often choose to
implement custom tools and encoding formats.
Small projects do not want to lose valuable time
learning complex frameworks and adapting them
to their needs. They often employ a custom XML
format to be able to use existing XML processing
tools like XQuery or XSLT processors.
189
ATLAS or NXT are very powerful, yet they
suffer from lack of accessibility to programmers
who have to adapt them to project-specific needs.
Most specialized annotation editors do not build
upon these frameworks and neither offer conver-
sion tools between their data formats.
Projects such as DDD do not make use of the
frameworks, because they are not easily extensi-
ble, e.g. with a SQL backend instead of an XML
storage. Instead, again a high level query language
is developed and a completely new framework is
created which works with a SQL backend.
In the previous sections, objects from selected
approaches with different foci in their work with
annotated corpora have been collected and forged
into a comprehensive data model. The potential
for modularization of corpus annotation frame-
works has been shown with respect to data models
and query languages. As a next step, an existing
framework should be taken and refactored into an
extensible modular architecture. From a practical
point of view reusing existing technology as much
as possible is a desirable goal. This means reusing
existing facilities provided for XML data, such as
XPath, XQuery and XSchema and where neces-
sary trying to extend them, instead of creating a
new data model from scratch. For the annotational
tiers, as LPath has shown, a good starting point to
do so is to extend existing languages like XPath.
Locational and medial operators seem to be best
implemented as XQuery functions. The possibil-
ity to map between SQL and XML provides ac-
cess to additional efficient resources for storing
and querying annotation data. Support for various
kinds of base data or locational information can be
encapsulated in modules. Which modules exactly
should be created and what they should cover in
detail has to be further examined.
Acknowledgements
Many thanks go to Elke Teich and Peter
Fankhauser for their support. Part of this research
was financially supported by Hessischer Innova-
tionsfonds and PACE (Partners for the Advance-
ment of Collaborative Engineering Education).
References
S. Bartsch, R. Eckart, M. Holtz & E. Teich 2005.
Corpus-based register profiling of texts form me-
chanical engineering In Proceedings of Corpus Lin-
guistics, Birmingham, UK, July 2005.
S. Bird & M. Liberman 2001. A Formal Framework
for Linguistic Annotation In Speech Communica-
tion 33(1,2), pp 23-60
S. Bird, Y. Chen, S. B. Davidson, H. Lee and Y.
Zheng. 2006. Designing and Evaluating an XPath
Dialect for Linguistic Queries. In Proceedings of the
22nd International Conference on Data Engineer-
ing, ICDE 2006, 3-8 April 2006, Atlanta, GA, USA
J. Carletta, D. McKelvie, A. Isard, A. Mengel, M. Klein
& M.B. M?ller 2004 A generic approach to soft-
ware support for linguistic annotation using XML
In G. Sampson and D. McCarthy (eds.), Corpus Lin-
guistics: Readings in a Widening Discipline. Lon-
don and NY: Continuum International.
S. Evert, J. Carletta, T. J. O?Donnell, J. Kilgour, A.
Vo?gele & H. Voormann 2003. The NITE Object
Model v2.1 http://www.ltg.ed.ac.uk/NITE/
documents/NiteObjectModel.v2.1.pdf
L. C. Faulstich, U. Leser & A. Lu?deling 2005. Storing
and querying historical texts in a relational database
In Informatik-Bericht 176, Institut fu?r Informatik,
Humboldt-Universita?t zu Berlin, 2005.
T. Grust and S. Sakr and J. Teubner 2002. XQuery
on SQL Hosts In Proceedings of the 30th Int?l Con-
ference on Very Large Data Bases (VLDB) Toronto,
Canada, Aug. 2004.
M.A.K. Halliday. 2004. Introduction to Functional
Grammar. Arnold, London. Revised by CMIM
Matthiessen
C. Laprun, J.G. Fiscus, J. Garofolo, S. Pa-
jot 2002. A practical introduction to AT-
LAS In Proceedings LREC 2002 Las Palmas
http://www.nist.gov/speech/atlas/download/lrec2002-
atlas.pdf
M. Laurent Romary (chair) and TC 37/SC 4/WG 2
2006. Language resource management - Feature
structures - Part 1: Feature structure representation.
In ISO 24610-1.
C. M. Sperberg-McQueen & L. Burnard, (eds.) 2002.
TEI P4: Guidelines for Electronic Text Encoding
and Interchange. Text Encoding Initiative Con-
sortium. XML Version: Oxford, Providence, Char-
lottesville, Bergen
E. Teich, P. Fankhauser, R. Eckart, S. Bartsch, M.
Holtz. 2005. Representing SFL-annotated corpora.
In Proceedings of the First Computational Systemic
Functional Grammar Workshop (CSFG), Sydney,
Australia.
E. Teich, S. Hansen, and P. Fankhauser. 2001. Rep-
resenting and querying multi-layer corpora. In
Proceedings of the IRCS Workshop on Linguistic
Databases, pages 228-237, University of Pennsyl-
vania, Philadelphia, 11-13 December.
190
Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006, pages 86?93,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Corpus annotation by generation
Elke Teich
TU Darmstadt
Darmstadt, Germany
teich@linglit.tu-darmstadt.de
John A. Bateman
Universita?t Bremen
Bremen, Germany
bateman@uni-bremen.de
Richard Eckart
TU Darmstadt
Darmstadt, Germany
eckart@linglit.tu-darmstadt.de
Abstract
As the interest in annotated corpora is
spreading, there is increasing concern with
using existing language technology for
corpus processing. In this paper we ex-
plore the idea of using natural language
generation systems for corpus annotation.
Resources for generation systems often fo-
cus on areas of linguistic variability that
are under-represented in analysis-directed
approaches. Therefore, making use of
generation resources promises some sig-
nificant extensions in the kinds of anno-
tation information that can be captured.
We focus here on exploring the use of
the KPML (Komet-Penman MultiLingual)
generation system for corpus annotation.
We describe the kinds of linguistic infor-
mation covered in KPML and show the
steps involved in creating a standard XML
corpus representation from KPML?s gener-
ation output.
1 Introduction
Many high-quality, theory-rich language process-
ing systems can potentially be applied to corpus
processing. However, the application of exist-
ing language technology, such as lexical and/or
grammatical resources as well as parsers, turns out
not to be as straightforward as one might think
it should be. Using existing computational lexi-
cons or thesauri, for instance, can be of limited
value because they do not contain the domain-
specific vocabulary that is needed for a partic-
ular corpus. Similarly, most existing grammat-
ical resources for parsing have restricted cover-
age in precisely those areas of variation that are
now most in need of corpus-supported investiga-
tion (e.g., predicate-argument structure, informa-
tion structure, rhetorical structure). Apart from
limited coverage, further issues that may impede
the ready application of parsers in corpus process-
ing include:
? Annotation relevance. Specialized, theory-
specific parsers (also called ?deep parsers?;
e.g., LFG or HPSG parsers) have been built
with theoretical concerns in mind rather than
appliability to unrestricted text. They may
thus produce information that is not annota-
tionally relevant (e.g., many logically equiv-
alent readings of a single clause).
? Usability. Deep parsers are highly complex
tools that require expert knowledge. The ef-
fort in acquiring this expert knowledge may
be too high relative to the corpus processing
task.
? Completeness. Simple parsers (commonly
called ?shallow parsers?), on the other
hand, produce only one type of anno-
tationally relevant information (e.g., PoS,
phrase/dependency structure). Other desir-
able kinds of information are thus lack-
ing (e.g., syntactic functions, semantic roles,
theme-rheme).
? Output representation. Typically, a parsing
output is represented in a theory-specific way
(e.g., in the case of LFG or HPSG parsers,
a feature structure). Such output does not
conform to the common practices in corpus
representation.1 Thus, it has to be mapped
onto one of the standardly used data mod-
els for corpora (e.g., annotation graphs (Bird
and Liberman, 2001) or multi-layer hier-
archies (Sperberg-McQueen and Huitfeldt,
2001; Teich et al, 2001)) and transformed
to a commonly employed format, typically
XML.
1This is in contrast to the output representation of shal-
low parsers which have often been developed with the goal
of corpus processing.
86
In spite of these difficulties, there is a general
consensus that the reward for exploring deep pro-
cessing techniques to build up small to medium-
scale corpus resources lies in going beyond the
kinds of linguistic information typically covered
by treebanks (cf. (Baldwin et al, 2004; Cahill et
al., 2002; Frank et al, 2003)).
In this paper, we would like to contribute to this
enterprise by adding a novel, yet complementary
perspective on theory-rich, high-quality corpus an-
notation. In a reappraisal of the potential contribu-
tion of natural language generation technology for
providing richly annotated corpora, we explore the
idea of annotation by generation. Although this
may at first glance seem counter-intuitive, in fact a
generator, similar to a parser, creates rather com-
plex linguistic descriptions (which are ultimately
realized as strings). In our current investigations,
we are exploring the use of these complex linguis-
tic descriptions for creating annotations. We be-
lieve that this may offer a worthwhile alternative
or extension of corpus annotation methods which
may alleviate some of the problems encountered
in parsing-based approaches.
The generation system we are using is the KPML
(Komet-Penman MultiLingual; (Bateman, 1997))
system. One potential advantage of KPML over
other generation systems and over many parsing
systems is its multi-stratal design. The kinds
of linguistic information included in KPML range
from formal-syntactic (PoS, phrase structure) to
functional-syntactic (syntactic functions), seman-
tic (semantic roles/frames) and discoursal (e.g.,
theme-rheme, given-new). Also, since KPML has
been applied to generate texts from a broad spec-
trum of domains, its lexicogrammatical resources
cover a wide variety of registers?another poten-
tial advantage in the analysis of unrestricted text.
As well as our general concern with investigat-
ing the possible benefits of applying generation
resources to the corpus annotation task, we are
also more specifically concerned with a series of
experiments involving the KPML system as such.
Here, for example, we are working towards the
construction of ?treebanks? based on the theory of
Systemic-Functional Linguistics (SFL; (Halliday,
2004)), so as to be able to empirically test some of
SFL?s hypotheses concerning patterns of instantia-
tion of the linguistic system in authentic texts. An-
notating the variety of linguistic categories given
in SFL manually is very labor-intensive and an au-
tomated approach is clearly called for. We are also
working towards a more detailed comparison of
the coverage of the lexicogrammatical resources
of KPML with those of parsing systems that are
similarly theoretically-dedicated (e.g., the HPSG-
based English Resource Grammar (ERG) (Copes-
take and Flickinger, 2002) contained in LinGO
(Oepen et al, 2002)). Thus, the idea presented
here is also motivated by the need to provide a ba-
sis for comparing grammar coverage across pars-
ing and generation systems more generally.
The remainder of the paper is organized as fol-
lows. First, we present the main features of the
KPML system (Section 2). Second, we describe the
steps involved in annotation by generation, from
the generation output (KPML internal generation
record) to an XML representation and its refine-
ment to an XML multi-layer representation (Sec-
tion 3). Section 4 concludes the paper with a criti-
cal assessment of the proposed approach and a dis-
cussion of the prospects for application in the con-
struction of corpora comparable in size and qual-
ity to existing treebanks (such as, for example, the
Penn Treebank for English (Marcus et al, 1993)
or the TIGER Treebank for German (Brants et al,
2002)). Since our description here has the status
of a progress report of work still in its beginning
stages, we cannot yet provide the results of de-
tailed evaluation. In the final section, therefore, we
emphasize the concrete steps that we are currently
taking in order to be able carry out the detailed
evaluations necessary.
2 Natural language generation with
KPML
The KPML system is a mature grammar devel-
opment environment for supporting large-scale
grammar engineering work for natural language
generation using multilingual systemic-functional
grammars (Bateman et al, 2005). Grammars
within this framework consist of large lattices of
grammatical features, each of which brings con-
straints on syntactic structure. The features are
also linked back to semantic configurations so that
they can be selected appropriately when given a
semantic specification as input. The result of gen-
erating with a systemic-functional grammar with
KPML is then a rich feature-based representation
distributed across a relatively simple structural
backbone. Each node of the syntactic represen-
tation corresponds to an element of structure and
87
typically receives on the order of 50-100 linguistic
features, called the feature selection. Since within
systemic-functional grammars, it is the features
of the feature selection that carry most of the de-
scriptive load, we can see each feature selection as
an exhaustive description of its associated syntac-
tic constituent. Generation within KPML normally
proceeds on the basis of a semantic input specifi-
cation which triggers particular feature selections
from the grammar via a mediating linguistic ontol-
ogy.
The features captured in a systemic-functional
generation resource are drawn from the four com-
ponents of functional meaning postulated within
systemic-functional grammar: the ideational, ex-
pressing content-related decisions, the logical, ex-
pressing logical dependencies, the interpersonal,
expressing interactional, evaluative and speech act
information, and the textual, expressing how each
element contributes to an unfolding text. It is in
this extremely rich combination of features that
we see significant value in exploring the re-use of
such grammars for annotation purposes and cor-
pus enrichment.
For annotation purposes, we employ some
of the alternative modes of generation that
are provided by the full grammar development
environment?it is precisely these that allow for
ready incorporation and application within the cor-
pus annotation task. One of the simplest ways in
which generation can be achieved during grammar
development, for example, is by directly select-
ing linguistic features from the grammar. This can
therefore mimic directly the task of annotation: if
we consider a target sentence (or other linguistic
unit) to be annotated, then selecting the necessary
features to generate that unit is equivalent to anno-
tating that unit in a corpus with respect to a very
extensive set of corpus annotation features.
Several additional benefits immediately acrue
from the use of a generator for this task. First,
the generator actually constructs the sentence (or
other unit) as determined by the feature selection.
This means that it is possible to obtain immedi-
ate feedback concerning the correctness and com-
pleteness of the annotation choices with respect to
the target. A non-matching structure can be gener-
ated if: (a) an inappropriate linguistic feature has
been selected, (b) the linguistic resources do not
cover the target to be annotated, or (c) a combina-
tion of these. In order to minimise the influence
of (b), we only work with large-scale grammatical
resources whose coverage is potentially sufficient
to cover most of the target corpus. Further cor-
pus instances that lie beyond the capabilities of the
generation grammar used are an obvious source of
requirements for extensions to that grammar.
Second, the architecture of the KPML system
also allows for other kinds of annotation support.
During grammar development it is often required
that guidance is given directly to the semantics-
grammar linking mappings: this is achieved by
providing particular ?answers? to pre-defined ?in-
quiries?. This allows for a significantly more
abstract and ?intention?-near interaction with the
grammatical resource that can be more readily
comprehensible to a user than the details of the
grammatical features. This option is therefore also
available for annotation.
Moreover, the semantic specifications used rely
on a specified linguistic ontology that defines par-
ticular semantic types. These types can also be
used directly in order to constrain whole collec-
tions of grammatical features. Providing this kind
of guidance during annotation can also, on the one
hand, simplify the process of annotation while, on
the other, produce a semantic level of annotation
for the corpus.
In the following sections, we see a selection of
these layers of information working in annotation
in more detail, showing that the kinds of informa-
tion produced during generation corresponds ex-
tremely closely to the kinds of rich annotations
currently being targetted for sophisticated corpus
presentation.
3 Creating corpus annotations from
KPML output
3.1 KPML output
The output produced by KPML when being used
for generation is a recursive structure with the cho-
sen lexical items at the leaves. Figure 1 shows the
output tree for the sample sentence ?However they
will step up their presence in the next year?.
The nodes of this structure may be freely an-
notated by the user or application system to con-
tain further information: e.g., for passing through
hyperlinks and URLs directly with the semantics
when generating hypertext. Most users simply see
the result of flattening this structure into a string:
the generated sentence or utterance.
This result retains only a fraction of the in-
88
Figure 1: Tree generated by KPML
formation that is employed by the generator dur-
ing generation. Therefore, since we are using
the grammar development environment rather than
simply the generator component, we also have the
possibility of working directly with the internal
structures that KPML employs for display and de-
bugging of resources during development. These
internal structures contain a complete record of
the information provided to the generation pro-
cess and the generator decisions (including which
grammatical features have been selected) that have
been made during the construction of each unit.
This internal record structure is again a recursive
structure corresponding directly to the syntactic
structure of the generated result and with each
node having the information slots:
constituent:
{identifier, \\ unique id for the unit
concept, \\ link to the semantic concept expressed
spelling, \\ the substring for this portion of structure
gloss, \\ a label for use in inter-lineal glosses
features, \\ the set of grammatical features for this unit
lexeme, \\ the lexeme chosen to cover this unit (if any)
annotation, \\ user-specified information
functions \\ the grammatical functions the unit expresses
}
An extract from such an internal record structure
encoded in XML is given in the Appendix (5.1).
To support annotation, we make use of the XML-
export capabilities of KPML (cf. (Bateman and
Hartley, 2000)) in order to provide these com-
pleted structures in a form suitable for passing on
to the next stage of corpus annotation within an
XML-based multi-layer framework.
3.2 XML multi-layer representation
Systemic-functional analysis is inherently multi-
dimensional in that SFL adopts more than one view
on a linguistic unit. Here, we focus on three anno-
tationally relevant dimensions: axis (features and
functions), unit (clause, group/phrase, word, mor-
pheme) and metafunction (ideational, logical, in-
terpersonal and textual). Each metafunction may
chunk up a given string (e.g., a clause unit) in
Figure 2: Generation output viewed as multi-layer
annotation
<sfglayer metafunction="IDEATIONAL">
However,
<segment functions="AGENT">they</segment>
will step up
<segment functions="DIRECTCOMPLEMENT GOAL MEDIUM">
their presence
</segment>
<segment functions="TIMELOCATIVE">
in the next year
</segment>
.
</sfglayer>
Figure 3: Metafunction+Function layers
different ways, thus potentially creating overlap-
ping hierarchies. This is depicted schematically
for the running example in Figure 2. For instance,
in this example, according to the textual meta-
function, ?however they? constitutes a segment
(Theme) and according to the interpersonal meta-
function, ?they will? constitutes another segment
(Mood).
In order to be able to use the KPML output for
annotation purposes, we adopt a multi-layer model
that allows the representation of these different de-
scriptional dimensions as separate layers superim-
posed on a given string (cf. (Teich et al, 2005)).
The transformation from the KPML output to the
concrete multi-layer model adopted is defined in
XSLT.
From the KPML internal record structure we
use the information slots of identifier, spelling,
features, and functions. Each entry in the func-
tion slot is associated with one metafunctional as-
pect. For each metafunctional aspect, an annota-
tion layer is created for each constituent unit (e.g.,
a clause) holding all associated functions together
with the substrings they describe (see Figure 3 for
the ideational functions contained in the clause in
the running example).
An additional layer holds the complete con-
stituent structure of the clause (cf. Figure 4 for the
corresponding extract from the running example),
89
<constituent unit="-TOP-"
selexp="LEXICAL-VERB-TERM-RESOLUTION...">
<token features="HOWEVER">However,</token>
<constituent unit="TOPICAL"
selexp="THEY-PRONOUN...">
<token features="THEY PLURAL-FORM">they</token>
</constituent>
<token features="OUTCLASSIFY-REDUCED...">will</token>
<token features="DO-VERB...">step up</token>
<constituent unit="DIRECTCOMPLEMENT"
selexp="NOMINAL-TERM-RESOLUTION OBLIQUE...">
<constituent unit="DEICTIC"
selexp="THEIR GENITIVE NONSUPERLATIVE...">
<token features="THEIR PLURAL-FORM">their</token>
</constituent>
<token features="...COMMON-NOUN...">presence</token>
</constituent>
<constituent unit="TIMELOCATIVE"
selexp="IN STRONG-INCLUSIVE UNORDERED...">
<token features="IN">in</token>
<constituent unit="MINIRANGE"
selexp="NOMINAL-TERM-RESOLUTION...">
<token features="THE">the</token>
<constituent unit="STATUS"
selexp="QUALITY-TERM-RESOLUTION...">
<token features="...ADJECTIVE">next</token>
</constituent>
<token features="...COMMON-NOUN...">year .</token>
</constituent>
</constituent>
</constituent>
Figure 4: Constituent+Feature layer
i.e., the phrasal constituents and their features:
<constituent unit="..." selexp="...">
</constituent>
and the tokens and their (lexical) features:
<token features="..."> ... </token>
Thus, the KPML generation output, which di-
rectly reflects the trace of the generation process,
is reorganized into a meaningful corpus represen-
tation. Information not relevant to annotation can
be ignored without loss of information concerning
the linguistic description. The resulting represen-
tation for the running example is shown in the Ap-
pendix (5.2).2
4 Discussion
Although it is clear that the kind of informational
structures produced during generation with more
developed KPML grammars align quite closely
with that targetted by sophisticated corpus anno-
tation, there are several issues that need to be ad-
dressed in order to turn this process into a prac-
tical annotation alternative. Those which we are
currently investigating centre around usability and
coverage.
2To improve readability, we provide the integrated repre-
sentation rather than the stand-off representation which aligns
the different layers by using character offsets.
Usability/effort. Users need to be trained in pro-
viding information to guide the generation pro-
cess. This guidance is either in the form of di-
rect selections of grammatical features, in which
case the user needs to know when the features ap-
ply, or in the form of semantic specifications, in
which case the user needs information concerning
the appropriate semantic classification according
to the constructs of the linguistic ontology. One of
the methods by which the problem of knowing the
import of grammatical features may be alleviated
is to link each feature with sets of already anno-
tated/generated corpus examples. Thus, if a user
is unsure concerning a feature, she can call for
examples to be displayed in which the particular
linguistic unit carrying the feature is highlighted.
Even more useful is a further option which shows
not only examples containing the feature, but con-
trasting examples showing where the feature has
applied and where it has not. This provides users
with online training during the use of the system
for annotation. The mechanisms for showing ex-
amples and contrasting sets of generated sentences
for each feature were originally provided as part
of a teaching aid built on top of KPML: this allows
students to explore a grammar by means of the ef-
fects that each set of contrasting features brings
for generated structures. For complex grammars
this appears to offer a viable alternative to precise
documentation?especially for less skilled users.
Coverage. When features have been selected, it
may still be the case that the correct target string
has not been generated due to limited coverage
of grammar and/or semantics. This is indicative
of the need to extend the grammatical resources
further. A further alternative that we are explor-
ing is to allow users to specify the correspondence
between the units generated and the actual target
string more flexibly. This is covered by two cases:
(i) that additional material is in the target string
that was not generated, and (ii) that the surface
order of constituents is not exactly that produced
by the generator. In both cases we can refine the
stand-off annotation so that the structural result
of generation can be linked to the actual string.
Thus manual correction consists of minor align-
ment statements between generated structure and
string.
Certain other information that may not be avail-
able to the generator, such as lexical entries, can be
constructed semi-automatically on-the-fly, again
90
using the information produced in the generation
process (i.e., by collecting the lexical classifica-
tion features and adding lexemes containing those
features). This method can be applied for all open
word classes.
Next steps. In our future work, we will be car-
rying out an extensive annotation experiment with
the prediction that annotation time is not higher
than for interactive annotation from a parsing per-
spective. TIGER, for example, reports 10 min-
utes per sentence as an average annotation time.
We expect an experienced KPML user to be sig-
nificantly faster because the process of generation
or feature selection explicitly leads the annotator
through precisely those features that are relevant
and possible given the connectivity of the feature
lattice defined by the grammar. Annotation then
proceeds first by selecting the features that apply
and then by aligning the generated structure with
the corpus instance: both potentially rather rapid
stages. Also, we would expect to achieve similar
coverage as reported by (Baldwin et al, 2004) for
ERG when applied to a random 20,000 string sam-
ple of the BNC due to the coverage of the existing
grammars.
The results of such investigations will be SFL-
treebanks, analogous to such treebanks produced
using dependency approaches, LFG, HPSG, etc.
These treebanks will then support the subsequent
learning of annotations for automatic processing.
Acknowledgment. This work was partially supported
by Hessischer Innovationsfond of TU Darmstadt and PACE
(Partners for the Advancement of Collaborative Engineering
Education: www.pacepartners.org).
References
T. Baldwin, E. M. Bender, D. Flickinger, A. Kim, and
S. Oepen. 2004. Road-testing the EnglishResource
Grammar over the British National Corpus. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC) 2004,
Lisbon, Portugal.
J. A. Bateman and A. F. Hartley. 2000. Target
suites for evaluating the coverage of text generators.
In Proceedings of the 3rd International Conference
on Language Resources and Evaluation (LREC),
Athens, Greece.
J. A. Bateman, I. Kruijff-Korbayova?, and G.-J. Krui-
jff. 2005. Multilingual resource sharing across
both related and unrelated languages: An imple-
mented, open-source framework for practical natu-
ral language generation. Research on Language and
Computation, 3(2):191?219.
J. A. Bateman. 1997. Enabling technology for multi-
lingual natural language generation: the KPML de-
velopment environment. Journal of Natural Lan-
guage Engineering, 3(1):15?55.
S. Bird and M. Liberman. 2001. A formal framework
for linguistic annotation. Speech Communication,
33(1-2):23?60.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, Sozopol.
A. Cahill, M. McCarthy, J. van Genabith, and A. Way.
2002. Automatic annotation of the Penn-Treebank
with LFG f-structure information. In Proceedings of
the 3rd International Conference on Language Re-
sources and Evaluation (LREC) 2002, Las Palmas,
Spain.
A. Copestake and D. Flickinger. 2002. An open-source
grammar development environment and broad cov-
erage English grammar using HPSG. In Proceed-
ings of the 2nd International Conference on Lan-
guage Resources and Evaluation (LREC), Athens,
Greece.
A. Frank, L. Sadler, J. van Genabith, and A. Way.
2003. From treebank resources to LFG f-structures.
Automatic f-structure annotation of treebank trees
and CFGs extracted from treebanks. In A. Abeille,
editor, Treebanks. Building and using syntactically
annotated corpora, pages 367?389. Kluwer Aca-
demic Publishers, Dordrecht, Boston, London.
MAK Halliday. 2004. Introduction to Functional
Grammar. Arnold, London.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
S. Oepen, E. Callahan, D. Flickinger, C. D. Manning,
and K. Toutanova. 2002. LinGO Redwoods. A rich
and dynamic treebank for HPSG. In Workshop on
Parser Evaluation, 3rd International Conference on
Language Resources and Evaluation (LREC), Las
Palmas, Spain.
C.M. Sperberg-McQueen and C. Huitfeldt. 2001.
GODDAG: A Data Structure for Overlapping Hi-
erarchies. In Proceedings of PODDP?00 and
DDEP?00, New York.
E. Teich, S. Hansen, and P. Fankhauser. 2001. Rep-
resenting and querying multi-layer corpora. In
Proceedings of the IRCS Workshop on Linguistic
Databases, University of Pennsylvania, Philadel-
phia.
E. Teich, P. Fankhauser, R. Eckart, S. Bartsch, and
M. Holtz. 2005. Representing SFL-annotated cor-
pus resources. In Proceedings of the 1st Computa-
tional Systemic Functional Workshop, Sydney, Aus-
tralia.
91
5 Appendix
5.1 Extract from generation record (clause level)
<example>
<name>REUTERS29</name>
<generatedForm>However, they will step up their presence in the next year.</generatedForm>
<targetForm>But they will step up their presence in the next year.</targetForm>
<structures><constituent id="G3324" semantics="STEP-3278">
<functions>
<function metafunction="UNKNOWN">SENTENCE</function></functions>
<features/>
<subconstituents><constituent id="G3308" semantics="RR62-3289">
<functions>
<function metafunction="TEXTUAL">TEXTUAL</function>
<function metafunction="TEXTUAL">CONJUNCTIVE</function></functions>
<features>
<f>HOWEVER</f></features>
<subconstituents><string>However,</string></subconstituents>
</constituent><constituent id="G3310" semantics="PERSON-3291">
<functions>
<function metafunction="TEXTUAL">TOPICAL</function>
<function metafunction="INTERPERSONAL">SUBJECT</function>
<function metafunction="UNIFYING">ACTOR</function>
<function metafunction="IDEATIONAL">AGENT</function></functions>
<features/>
<subconstituents><constituent id="G3309" semantics="PERSON-3291">
<functions>
<function metafunction="LOGICAL">THING</function></functions>
<features>
<f>THEY</f>
<f>PLURAL-FORM</f></features>
<subconstituents><string>they </string></subconstituents>
</constituent>
</subconstituents>
</constituent><constituent id="G3311" semantics="ST59-3280-3297-3302">
<functions>
<function metafunction="LOGICAL">TEMPO0</function>
<function metafunction="INTERPERSONAL">FINITE</function></functions>
<features>
<f>OUTCLASSIFY-REDUCED</f>
<f>OUTCLASSIFY-NEGATIVE-AUX</f>
<f>FUTURE-AUX</f>
<f>PLURAL-FORM</f>
<f>THIRDPERSON-FORM</f></features>
<subconstituents><string>will </string></subconstituents>
</constituent><constituent id="G3312" semantics="STEP-3278">
<functions>
<function metafunction="UNIFYING">AUXSTEM</function>
<function metafunction="LOGICAL">VOICE</function>
<function metafunction="LOGICAL">LEXVERB</function>
<function metafunction="LOGICAL">PROCESS</function></functions>
<features>
<f>DO-VERB</f>
<f>EFFECTIVE-VERB</f>
<f>DISPOSAL-VERB</f>
<f>STEM</f></features>
<subconstituents><string>step up </string></subconstituents>
</constituent><constituent id="G3316" semantics="PRESENCE-3292-3306">
<functions>
<function metafunction="IDEATIONAL">DIRECTCOMPLEMENT</function>
<function metafunction="IDEATIONAL">GOAL</function>
<function metafunction="IDEATIONAL">MEDIUM</function></functions>
</constituent></subconstituents></constituent></structures>
<selectionexpressions>
<selexp sem="STEP-3278"><unit>-TOP-</unit><f>LEXICAL-VERB-TERM-RESOLUTION</f>
<f>DO-NEEDING-VERBS</f><f>AUXSTEM-VOICE</f><f>REAL</f><f>NON-MOTION-CLAUSE</f>
<f>PLURAL-FINITE</f><f>PLURAL-SUBJECT</f><f>TOPICAL-INSERT</f> ...
</selexp>
<selexp>...</selexp>
...
</selectionexpressions>
</example>
5.2 Multi-layer representation of generation record
Metafunction+Function layers
<sfglayer metafunction="UNKNOWN">
<segment functions="SENTENCE">
However, they will step up their presence in the next year .
</segment>
</sfglayer>
<sfglayer metafunction="UNIFYING">
However,
<segment functions="ACTOR">they</segment>
will
<segment functions="AUXSTEM">step up</segment>
their presence in the next year .
</sfglayer>
<sfglayer metafunction="TEXTUAL">
<segment functions="TEXTUAL CONJUNCTIVE">However,</segment>
<segment functions="TOPICAL">they</segment>
will step up their presence in the next year .
</sfglayer>
92
<sfglayer metafunction="LOGICAL">
However,
<segment functions="THING">they</segment>
<segment functions="TEMPO0">will</segment>
<segment functions="VOICE LEXVERB PROCESS">step up</segment>
<segment functions="THING">their</segment>
<segment functions="THING">presence</segment>
in the
<segment functions="QUALITY">next</segment>
<segment functions="THING">year .</segment>
</sfglayer>
<sfglayer metafunction="INTERPERSONAL">
However,
<segment functions="SUBJECT">they</segment>
<segment functions="FINITE">will</segment>
step up
<segment functions="DEICTIC">their</segment>
presence in
<segment functions="DEICTIC">the</segment>
next year .
</sfglayer>
<sfglayer metafunction="IDEATIONAL">
However,
<segment functions="AGENT">they</segment>
will step up
<segment functions="DIRECTCOMPLEMENT GOAL MEDIUM">
their presence
</segment>
<segment functions="TIMELOCATIVE">
<segment functions="MINORPROCESS">in</segment>
<segment functions="MINIRANGE">
the
<segment functions="STATUS">next</segment>
year .
</segment>
</segment>
</sfglayer>
Constituent+Feature layer
<constituent id="G3324" unit="-TOP-"
selexp="LEXICAL-VERB-TERM-RESOLUTION DO-NEEDING-VERBS AUXSTEM-VOICE REAL NON-MOTION-CLAUSE TOPICAL-INSERT ...">
<token features="HOWEVER">However,</token>
<constituent id="G3310" unit="TOPICAL"
selexp="THEY-PRONOUN NONDEMONSTRATIVE-SPECIFIC-PRONOUN NOMINATIVE NONSUPERLATIVE NONREPRESENTATION NONPARTITIVE ...">
<constituent id="G3309" unit="TOPICAL">
<token features="THEY PLURAL-FORM">they</token>
</constituent>
</constituent>
<token
features="OUTCLASSIFY-REDUCED OUTCLASSIFY-NEGATIVE-AUX FUTURE-AUX PLURAL-FORM THIRDPERSON-FORM">
will
</token>
<constituent id="G3312" unit="-TOP-">
<token features="DO-VERB EFFECTIVE-VERB DISPOSAL-VERB STEM">
step up
</token>
</constituent>
<constituent id="G3316" unit="DIRECTCOMPLEMENT"
selexp="NOMINAL-TERM-RESOLUTION OBLIQUE NONSUPERLATIVE NONREPRESENTATION NONPARTITIVE NONQUANTIFIED NOMINAL-GROUP ...">
<constituent id="G3314" unit="DEICTIC"
selexp="THEIR GENITIVE NONSUPERLATIVE NONREPRESENTATION NONPARTITIVE NONQUANTIFIED NOMINAL-GROUP ...">
<constituent id="G3313" unit="DEICTIC">
<token features="THEIR PLURAL-FORM">their</token>
</constituent>
</constituent>
<constituent id="G3315" unit="DIRECTCOMPLEMENT">
<token
features="OUTCLASSIFY-PROPERNOUN NOUN COMMON-NOUN COUNTABLE SINGULAR-FORM NOUN">
presence
</token>
</constituent>
</constituent>
<constituent id="G3323" unit="TIMELOCATIVE"
selexp="IN STRONG-INCLUSIVE UNORDERED TEMPORAL-PROCESS LOCATION-PROCESS SPATIO-TEMPORAL-PROCESS PREPOSITIONAL-PHRASE ...">
<token features="IN">in</token>
<constituent id="G3322" unit="MINIRANGE"
selexp="NOMINAL-TERM-RESOLUTION OBLIQUE NONSUPERLATIVE NONREPRESENTATION NONPARTITIVE NONQUANTIFIED NOMINAL-GROUP ...">
<token features="THE">the</token>
<constituent id="G3320" unit="STATUS"
selexp="QUALITY-TERM-RESOLUTION SIMPLEX-QUALITY NOTINTENSIFIED NONSCALABLE CONGRUENT-ADJECTIVAL-GROUP ...">
<constituent id="G3319" unit="STATUS">
<token features="OUTCLASSIFY-DEGREE-ADJ ADJ-NEUTRAL-FORM ADJECTIVE">
next
</token>
</constituent>
</constituent>
<constituent id="G3321" unit="MINIRANGE">
<token features="OUTCLASSIFY-PROPERNOUN NOUN COMMON-NOUN COUNTABLE SINGULAR-FORM NOUN">
year .
</token>
</constituent>
</constituent>
</constituent>
</constituent>
93
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 85?90,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
CSNIPER
Annotation-by-query for non-canonical constructions in large corpora
Richard Eckart de Castilho, Iryna Gurevych
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science
Technische Universita?t Darmstadt
http://www.ukp.tu-darmstadt.de
Sabine Bartsch
English linguistics
Department of Linguistics and Literary Studies
Technische Universita?t Darmstadt
http://www.linglit.tu-darmstadt.de
Abstract
We present CSNIPER (Corpus Sniper), a
tool that implements (i) a web-based multi-
user scenario for identifying and annotating
non-canonical grammatical constructions in
large corpora based on linguistic queries and
(ii) evaluation of annotation quality by mea-
suring inter-rater agreement. This annotation-
by-query approach efficiently harnesses expert
knowledge to identify instances of linguistic
phenomena that are hard to identify by means
of existing automatic annotation tools.
1 Introduction
Linguistic annotation by means of automatic pro-
cedures, such as part-of-speech (POS) tagging, is
a backbone of modern corpus linguistics; POS
tagged corpora enhance the possibilities of corpus
query. However, many linguistic phenomena are
not amenable to automatic annotation and are not
readily identifiable on the basis of surface features.
Non-canonical constructions (NCCs), which are the
use-case of the tool presented in this paper, are a
case in point. NCCs, of which cleft-sentences are
a well-known example, raise a number of issues that
prevent their reliable automatic identification in cor-
pora. Yet, they warrant corpus study due to the rel-
atively low frequency of individual instances, their
deviation from canonical construction patterns and
frequent ambiguity. This makes them hard to distin-
guish from other, seemingly similar constructions.
Expert knowledge is thus required to reliably iden-
tify and annotate such phenomena in sufficiently
large corpora like the 100 mil. word British National
Corpus (BNC Consortium, 2007). This necessitates
manual annotation which is time-consuming and
error-prone when carried out by individual linguists.
To overcome these issues, CSNIPER implements
a web-based multi-user annotation scenario in which
linguists formulate and refine queries that identify
a given linguistic construction in a corpus and as-
sess the query results to distinguish instances of the
phenomenon under study (true positives) from such
examples that are wrongly identified by the query
(false positives). Each expert linguist thus acts as a
rater rather than an annotator. The tool records as-
sessments made by each rater. A subsequent evalua-
tion step measures the inter-rater agreement. The ac-
tual annotation step is deferred until after this evalu-
ation in order to achieve high annotation confidence.
Query
Assess
Evaluate
Annotate
review
assessments
refine
query
Figure 1: Annotation-by-query workflow
CSNIPER implements an annotation-by-query ap-
proach which entails the following interlinking func-
tionalities (see fig. 1):
Query development: Corpus queries can be de-
veloped and refined within the tool. Based on query
results which are assessed and labeled by the user,
queries can be systematically evaluated and refined
for precision. This transfers some of the ideas of
85
relevance feedback, which is a common method of
improving search results in information retrieval, to
a linguistic corpus query system.
Assessment: Query results are presented to the
user as a list of sentences with optional additional
context; the user assesses and labels each sentence
as representing or not representing an instance of the
linguistic phenomenon under study. The tool imple-
ments a function that allows the user to comment
on decisions and to temporarily mark sentences with
uncertain assessments for later review.
Evaluation: Evaluation is a central functional-
ity of CSNIPER serving three purposes. 1) It in-
tegrates with the query development by providing
feedback to refine queries and improve query pre-
cision. 2) It provides information on sentences not
labeled consistently by all users, which can be used
to review the assessments. 3) It calculates the inter-
rater agreement which is used in the corpus annota-
tion step to ensure high annotation confidence.
Corpus annotation: By assessing and labeling
query results as correct or wrong, raters provide the
tool with their annotation decisions. CSNIPER anno-
tates the corpus with those annotation decisions that
exceed a certain inter-rater agreement threshold.
This annotation-by-query approach of querying,
assessing, evaluating and annotating allows multiple
distributed raters to incrementally improve query re-
sults and achieve high quality annotations. In this
paper, we show how such an approach is well-suited
for annotation tasks that require manual analysis
over large corpora. The approach is generalizable
to any kind of linguistic phenomena that can be lo-
cated in corpora on the basis of queries and require
manual assessment by multiple expert raters.
In the next two sections, we are providing a more
detailed description of the use-case driving the de-
velopment of CSNIPER (sect. 2) and discuss why ex-
isting tools do not provide viable solutions (sect. 3).
Sect. 4 discusses CSNIPER and sect. 5 draws some
conclusions and offers an outlook on the next steps.
2 Non-canonical grammatical
constructions
The initial purpose of CSNIPER is the corpus-based
study of so-called non-canonical grammatical con-
structions (NCC) (examples (2) - (5) below):
1. The media was now calling Reagan the front-
runner. (canonical)
2. It was Reagan whom the media was now calling
the frontrunner. (it-cleft)
3. It was the media who was now calling Reagan
the frontrunner. (it-cleft)
4. It was now that the media were calling Reagan
the frontrunner. (it-cleft)
5. Reagan the media was not calling the front-
runner. (inversion)
NCCs are linguistic constructions that deviate
in characteristic ways from the unmarked lexico-
grammatical patterning and informational ordering
in the sentence. This is exemplified by the con-
structions of sentences (2) - (5) above. While ex-
pressing the same propositional content, the order
of information units available through the permissi-
ble grammatical constructions offers interesting in-
sights into the constructional inventory of a lan-
guage. It also opens up the possibility of comparing
seemingly closely related languages in terms of the
sets of available related constructions as well as the
relations between instances of canonical and non-
canonical constructions.
In linguistics, a cleft sentence is defined as a com-
plex sentence that expresses a single proposition
where the clefted element is co-referential with the
following clause. E.g., it-clefts are comprised of the
following constituents:
dummy
subject it
main verb
to be
clefted
element
clause
The NCCs under study pose interesting chal-
lenges both from a linguistic and a natural language
processing perspective. Due to their deviation from
the canonical constructions, they come in a vari-
ety of potential construction patterns as exemplified
above. Non-canonical constructions can be expected
to be individually rarer in any given corpus than their
canonical counterparts. Their patterns of usage and
their discourse functions have not yet been described
exhaustively, especially not in representative corpus
studies because they are notoriously hard to identify
without suitable software. Their empirical distribu-
tion in corpora is thus largely unknown.
A major task in recognizing NCCs is distin-
guishing them from structurally similar construc-
86
tions with default logical and propositional content.
An example of a particular difficulty from the do-
main of it-clefts are anaphoric uses of it as in (6) be-
low that do not refer forward to the following clause,
but are the antecedents of entities previously intro-
duced in the context of preceding sentences. Other
issues arise in cases of true relative clauses as exem-
plified in (7) below:
6. London will be the only capital city in Eu-
rope where rail services are expected to make
a profit,? he added. It is a policy that could lead
to economic and environmental chaos. [BNC:
A9N-s400]
7. It is a legal manoeuvre that declined in cur-
rency in the ?80s. [BNC: B1L-s576]
Further examples of NCCs apart from the it-clefts
addressed in this paper are wh-clefts and their sub-
types, all-clefts, there-clefts, if-because-clefts and
demonstrative clefts as well as inversions. All of
these are as hard to identify in a corpus as it-clefts.
The linguistic aim of our research is a comparison
of non-canonical constructions in English and Ger-
man. Research on these requires very large corpora
due to the relatively low frequency of the individ-
ual instances. Due to the ambiguous nature of many
NCC candidates, automatically finding them in cor-
pora is difficult. Therefore, multiple experts have to
manually assess candidates in corpora.
Our approach does not aim at the exhaustive an-
notation of all NCCs. The major goal is to improve
the understanding of the linguistic properties and us-
age of NCCs. Furthermore, we define a gold stan-
dard to evaluate algorithms for automatic NCC iden-
tification. In our task, the total number of NCCs in
any given corpus is unknown. Thus, while we can
measure the precision of queries, we cannot mea-
sure their recall. To address this, we exhaustively
annotate a small part of the corpus and extrapolate
the estimated number of total NCC candidates.
In summary, the requirements for a tool to support
multi-user annotation of NCCs are as follows:
1. querying large linguistically pre-processed
corpora and query refinement
2. assessment of sentences that are true instances
of NCCs in a multi-user setting
3. evaluation of inter-rater agreement and query
precision
In the following section, we review previous work
to support linguistic annotation tasks.
3 Related work
We differentiate three categories of linguistic tools
which all partially fulfill our requirements: querying
tools, annotation tools, and transformation tools.
Linguistic query tools: Such tools allow to query
a corpus using linguistic features, e.g. part-of-
speech tags. Examples are ANNIS2 (Zeldes et al,
2009) and the IMS Open Corpus Workbench (CWB)
(Christ, 1994). Both tools provide powerful query
engines designed for large linguistically annotated
corpora. Both are server-based tools that can be used
concurrently by multiple users. However, they do
not allow to assess the query results.
Linguistic annotation tools: Such tools allow
the user to add linguistic annotations to a corpus.
Examples are MMAX2 (Mu?ller and Strube, 2006)
and the UIMA CAS Editor1. These tools typically
display a full document for the user to annotate. As
NCCs appear only occasionally in a text, such tools
cannot be effectively applied to our task, as they of-
fer no linguistic query capabilities to quickly locate
potential NCCs in a large corpus.
Linguistic transformation tools: Such tools al-
low the creation of annotations using transforma-
tion rules. Examples are TextMarker (Kluegl et al,
2009) and the UAM CorpusTool (O?Donnell, 2008).
A rule has the form category := pattern and creates
new annotation of the type category on any part of
a text matching pattern. A rule for the annotation
of passive clauses in the UAM CorpusTool could be
passive-clause := clause + containing be% partici-
ple. These tools do not support the assessment of
the results, though. In contrast to the querying tools,
transformation tools are not specifically designed to
operate efficiently on large corpora. Thus, they are
hardly productive for our task, which requires the
analysis of large corpora.
4 CSNIPER
We present CSNIPER, an annotation tool for non-
canonical constructions. Its main features are:
1http://uima.apache.org/
87
Figure 2: Search form
Annotation-by-query ? Sentences potentially
containing a particular type of NCC are retrieved us-
ing a query. If the sentence contains the NCC of
interest, the user manually labels it as correct and
otherwise wrong. Annotations are generated based
on the users? assessments.
Distributed multi-user setting ? Our web-based
tool supports multiple users concurrently assessing
query results. Each user can only see and edit their
own assessments and has a personal query history.
Evaluation ? The evaluation module provides in-
formation on assessments, number of annotated in-
stances, query precision and inter-rater agreement.
4.1 Implementation and data
CSNIPER is implemented in Java and uses the CWB
as its linguistic search engine (cf. sect. 3). Assess-
ments are stored in a MySQL database. Currently,
the British National Corpus (BNC) is used in our
study. Apache UIMA and DKPro Core2 are used
for linguistic pre-processing, format conversion, and
to drive the indexing of the corpora. In particular,
DKPro Core includes a reader for the BNC and a
writer for the CWB. As the BNC does not carry
lemma annotations, we add them using the DKPro
TreeTagger (Schmid, 1994) module.
4.2 Query (Figure 2)
The user begins by selecting a 1? corpus and a
2? construction type (e.g. It-Cleft). A query can be
chosen from a 3? list of examples, from the 4? per-
sonal query history, or a new 5? query can be en-
tered. The query is applied to find instances of that
construction (e.g. ?It? /VCC[] /PP[] /RC[]). Af-
ter pressing the 6? Submit query button, the tool
presents the user with a KWIC view of the query
results (fig. 3). At this point, the user may choose to
2http://www.ukp.tu-darmstadt.de/
research/current-projects/dkpro/
refine and re-run the query.
As each user may use different queries, they will
typically assess different sets of query results. This
can yield a set of sentences labeled by a single user
only. Therefore, the tool can display those sentences
for assessment that other users have assessed, but the
current user has not. This allows getting labels from
all users for every NCC candidate.
4.3 Assessment (Figure 3)
If the query results match the expectation, the user
can switch to the assessment mode by clicking the
7? Begin assessment button. At this point, an An-
notationCandidate record is created in the database
for each sentence unless a record is already present.
These records contain the offsets of the sentence in
the original text, the sentence text and the construc-
tion type. In addition, an AnnotationCandidateLabel
record is created for each sentence to hold the as-
sessment to be provided by the user.
In the assessment mode, an additional 8? Label
column appears in the KWIC view. Clicking in this
column cycles through the labels correct, wrong,
check and nothing. When the user is uncertain, the
label check can be used to mark candidates for later
review. The view can be 9? filtered for those sen-
tences that need to be assessed, those that have been
assessed, or those that have been labeled with check.
A 10? comment can be left to further describe difficult
cases or to justify decisions. All changes are imme-
diately saved to the database, so the user can stop
assessing at any time and resume the process later.
The proper assessment of a sentence as an in-
stance of a particular construction type sometimes
depends on the context found in the preceding and
following sentences. For this purpose, clicking on
the 11? book icon in the KWIC view displays the
sentence in its larger context (fig. 4). POS tags are
shown in the sentence to facilitate query refinement.
4.4 Evaluation (Figure 5)
The evaluation function provides an overview of the
current assessment state (fig. 5). We support two
evaluation views: by construction type and by query.
By construction type: In this view, one or more
12? corpora, 13? types, and 14? users can be selected
for evaluation. For these, all annotation candidates
and the respective statistics are displayed. It is pos-
88
Figure 3: KWIC view of query results and assessments
sible to 15? filter for correct, wrong, disputed, incom-
pletely assessed, and unassessed candidates. A can-
didate is disputed if it is not labeled consistently by
all selected users. A candidate is incompletely as-
sessed if at least one of the selected users labeled
it and at least one other did not. Investigating dis-
puted cases and 16? inter-rater agreement per type
using Fleiss? Kappa (Fleiss, 1971) are the main uses
of this view. The inter-rater agreement is calculated
using only candidates labeled by all selected users.
By query: In this view, query precision and as-
sessment completeness are calculated for a set of
17? queries and 18? users. The query precision is cal-
culated from the labeled candidates as:
precision =
|TP |
|TP |+ |FP |
We treat a candidate as a true positive (TP) if:
1) the number of correct labels is larger than the
number of wrong labels; 2) the ratio of correct labels
compared to the number of raters exceeds a given
19? threshold. Candidates are conversely treated as
false positives (FPs) if the number of wrong labels
is larger and the threshold is exceeded. The thresh-
old controls the confidence of the TP and, thus, of
the annotations generated from them (cf. sect. 4.5).
Figure 4: Sentence context view with POS tags
If a candidate is neither TP nor FP, it is unknown
(UNK). When calculating precision, UNK candi-
dates are counted as FP. The estimated precision is
the precision to be expected if TP and FP are equally
distributed over the set of candidates. It takes into
account only the currently known TP and FP and ig-
nores the UNK candidates. Both values are the same
once all candidates have been labeled by all users.
4.5 Annotation
When the assessment process is complete, corpus
annotations can be generated from the assessed can-
didates. Here, we employ the thresholded major-
ity vote approach that we also use to determine the
TP/FP in sect. 4.4. Annotations for the respective
NCC type are added directly to the corpus. The aug-
mented corpus can be used in further exploratory
work. Alternatively, a file with all assessed candi-
dates can be generated to serve as training data for
identification methods based on machine learning.
5 Conclusions
We have presented CSNIPER, a tool for the an-
notation of linguistic phenomena whose investiga-
tion requires the analysis of large corpora due to
a relatively low frequency of instances and whose
identification requires expert knowledge to distin-
guish them from other similar constructions. Our
tool integrates the complete functionality needed for
the annotation-by-query workflow. It provides dis-
tributed multi-user annotation and evaluation. The
feedback provided by the integrated evaluation mod-
ule can be used to systematically refine queries and
improve assessments. Finally, high-confidence an-
notations can be generated from the assessments.
89
Figure 5: Evaluation by query and by NCC type
The annotation-by-query approach can be gener-
alized beyond non-canonical constructions to other
linguistic phenomena with similar properties. An
example could be metaphors, which typically also
appear with comparatively low frequency and re-
quire expert knowledge to be annotated. We plan
to integrate further automatic annotations and query
possibilities to support such further use-cases.
Acknowledgments
We would like to thank Erik-La?n Do Dinh, who assisted
in implementing CSNIPER as well as Gert Webelhuth and
Janina Rado for testing and providing valuable feedback.
This work has been supported by the Hessian research
excellence program ?Landes-Offensive zur Entwicklung
Wissenschaftlich-o?konomischer Exzellenz? (LOEWE) as
part of the research center ?Digital Humanities? and by
the Volkswagen Foundation as part of the Lichtenberg-
Professorship Program under grant No. I/82806.
Data cited herein have been extracted from the British
National Corpus, distributed by Oxford University Com-
puting Services on behalf of the BNC Consortium. All
rights in the texts cited are reserved.
References
BNC Consortium. 2007. The British National Corpus,
version 3 (BNC XML Edition). Distributed by Oxford
University Computing Services p.p. the BNC Consor-
tium, http://www.natcorp.ox.ac.uk/.
Oliver Christ. 1994. A modular and flexible architec-
ture for an integrated corpus query system. In Proc.
of the 3rd Conference on Computational Lexicography
and Text Research (COMPLEX?94), pages 23?32, Bu-
dapest, Hungary, Jul.
Joseph L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. In Psychological Bulletin,
volume 76 (5), pages 378?381. American Psychologi-
cal Association, Washington, DC.
Peter Kluegl, Martin Atzmueller, and Frank Puppe.
2009. TextMarker: A tool for rule-based informa-
tion extraction. In Christian Chiarcos, Richard Eckart
de Castilho, and Manfred Stede, editors, Proc. of the
Biennial GSCL Conference 2009, 2nd UIMA@GSCL
Workshop, pages 233?240. Gunter Narr Verlag, Sep.
Christoph Mu?ller and Michael Strube. 2006. Multi-level
annotation of linguistic data with MMAX2. In Sabine
Braun, Kurt Kohn, and Joybrato Mukherjee, editors,
Corpus Technology and Language Pedagogy: New Re-
sources, New Tools, New Methods, pages 197?214. Pe-
ter Lang, Frankfurt am Main, Germany, Aug.
Mick O?Donnell. 2008. The UAM CorpusTool: Soft-
ware for corpus annotation and exploration. In Car-
men M. et al Bretones Callejas, editor, Applied Lin-
guistics Now: Understanding Language and Mind
/ La Lingu???stica Aplicada Hoy: Comprendiendo el
Lenguaje y la Mente, pages 1433?1447. Almer??a: Uni-
versidad de Almer??a.
Helmut Schmid. 1994. Improvements in part-of-speech
tagging with an application to German. In Proc. of Int.
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK, Sep.
Amir Zeldes, Julia Ritz, Anke Lu?deling, and Christian
Chiarcos. 2009. ANNIS: A search tool for multi-
layer annotated corpora. In Proc. of Corpus Linguis-
tics 2009, Liverpool, UK, Jul.
90
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1?6,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
WebAnno: A Flexible, Web-based and Visually Supported
System for Distributed Annotations
Seid Muhie Yimam1,3 Iryna Gurevych2,3 Richard Eckart de Castilho2 Chris Biemann1
(1) FG Language Technology, Dept. of Computer Science, Technische Universita?t Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universita?t Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
We present WebAnno, a general pur-
pose web-based annotation tool for a wide
range of linguistic annotations. Web-
Anno offers annotation project manage-
ment, freely configurable tagsets and the
management of users in different roles.
WebAnno uses modern web technology
for visualizing and editing annotations in
a web browser. It supports arbitrarily
large documents, pluggable import/export
filters, the curation of annotations across
various users, and an interface to farming
out annotations to a crowdsourcing plat-
form. Currently WebAnno allows part-of-
speech, named entity, dependency parsing
and co-reference chain annotations. The
architecture design allows adding addi-
tional modes of visualization and editing,
when new kinds of annotations are to be
supported.
1 Introduction
The creation of training data precedes any sta-
tistical approach to natural language processing
(NLP). Linguistic annotation is a process whereby
linguistic information is added to a document,
such as part-of-speech, lemmata, named entities,
or dependency relations. In the past, platforms
for linguistic annotations were mostly developed
ad-hoc for the given annotation task at hand, used
proprietary formats for data exchange, or required
local installation effort. We present WebAnno, a
browser-based tool that is immediately usable by
any annotator with internet access. It supports an-
notation on a variety of linguistic levels (called an-
notation layers in the remainder), is interoperable
with a variety of data formats, supports annotation
project management such as user management, of-
fers an adjudication interface, and provides qual-
ity management using inter-annotator agreement.
Furthermore, an interface to crowdsourcing plat-
forms enables scaling out simple annotation tasks
to a large numbers of micro-workers. The added
value of WebAnno, as compared to previous an-
notation tools, is on the one hand its web-based
interface targeted at skilled as well as unskilled
annotators, which unlocks a potentially very large
workforce. On the other hand, it is the support for
quality control, annotator management, and adju-
dication/curation, which lowers the entrance bar-
rier for new annotation projects. We created Web-
Anno to fulfill the following requirements:
? Web-based: Distributed work, no installation
effort, increased availability.
? Interface to crowdsourcing: unlocking a very
large distributed workforce.
? Quality and user management: Integrated
different user roles support (administra-
tor, annotator, and curator), inter-annotator
agreement measurement, data curation, and
progress monitoring.
? Flexibility: Support of multiple annotation
layers, pluggable import and export formats,
and extensibility to other front ends.
? Pre-annotated and un-annotated documents:
supporting new annotations, as well as man-
ual corrections of existing, possibly auto-
matic annotations.
? Permissive open source: Usability of our tool
in future projects without restrictions, under
the Apache 2.0 license.
In the following section, we revisit related work
on annotation tools, which only partially fulfill the
aforementioned requirements. In Section 3, the ar-
chitecture as well as usage aspects of our tool are
lined out. The scope and functionality summary
1
of WebAnno is presented in Section 4. Section 5
elaborates on several use cases of WebAnno, and
Section 6 concludes and gives an outlook to fur-
ther directions.
2 Related Work
GATE Teamware (Bontcheva et al, 2010) is prob-
ably the tool that closely matches our requirements
regarding quality management, annotator manage-
ment, and support of a large set of annotation lay-
ers and formats. It is mostly web-based, but the
annotation is carried out with locally downloaded
software. An interface to crowdsourcing platforms
is missing. The GATE Teamware system is heav-
ily targeted towards template-based information
extraction. It sets a focus on the integration of au-
tomatic annotation components rather than on the
interface for manual annotation. Besides, the over-
all application is rather complex for average users,
requires considerable training and does not offer
an alternative simplified interface as it would be
required for crowdsourcing.
General-purpose annotation tools like MMAX2
(Mu?ller and Strube, 2006) or WordFreak (Morton
and LaCivita, 2003) are not web-based and do not
provide annotation project management. They are
also not sufficiently flexible regarding different an-
notation layers. The same holds for specialized
tools for single annotation layers, which we can-
not list here for the sake of brevity.
With the brat rapid annotation tool (Stenetorp
et al, 2012), for the first time a web-based open-
source annotation tool was introduced, which sup-
ports collaborative annotation for multiple anno-
tation layers simultaneously on a single copy of
the document, and is based on a client-server ar-
chitecture. However, the current version of brat
has limitations such as: (i) slowness for docu-
ments of more than 100 sentences, (ii) limits re-
garding file formats, (iii) web-based configuration
of tagsets/tags is not possible and (iv) configuring
the display of multiple layers is not yet supported.
While we use brat?s excellent visualization front
end in WebAnno, we decided to replace the server
layer to support the user and quality management,
and monitoring tools as well as to add the interface
to crowdsourcing.
3 System Architecture of WebAnno
The overall architecture of WebAnno is depicted
in Figure 1. The modularity of the architecture,
Figure 1: System architecture, organized in user,
front end, back end and persistent data storage.
which is mirrored in its open-source implementa-
tion1, makes it possible to easily extend the tool or
add alternative user interfaces for annotation lay-
ers that brat is less suited for, e.g. for constituent
structure. In Section 3.1, we illustrate how differ-
ent user roles are provided with different graphical
user interfaces, and show the expressiveness of the
annotation model. Section 3.2 elaborates on the
functionality of the back end, and describes how
data is imported and exported, as well as our im-
plementation of the persistent data storage.
3.1 Front End
All functionality of WebAnno is accessible via
a web browser. For annotation and visualiza-
tion of annotated documents, we adapted the brat
rapid annotation tool. Changes had to be made to
make brat interoperate with the Apache Wicket,
on which WebAnno is built, and to better integrate
into the WebAnno experience.
3.1.1 Project Definition
The definition and the monitoring of an annota-
tion project is conducted by a project manager (cf.
Figure 1) in a project definition form. It supports
creating a project, loading un-annotated or pre-
annotated documents in different formats2, adding
annotator and curator users, defining tagsets, and
configuring the annotation layers. Only a project
manager can administer a project. Figure 2 illus-
trates the project definition page with the tagset
editor highlighted.
1Available for download at (this paper is based on v0.3.0):
webanno.googlecode.com/
2Formats: plain text, CoNLL (Nivre et al, 2007), TCF
(Heid et al, 2010), UIMA XMI (Ferrucci and Lally, 2004)
2
Figure 2: The tagset editor on the project definition page
3.1.2 Annotation
Annotation is carried out with an adapted ver-
sion of the brat editor, which communicates with
the server via Ajax (Wang et al, 2008) using the
JSON (Lin et al, 2012) format. Annotators only
see projects they are assigned to. The annotation
page presents the annotator different options to set
up the annotation environment, for customization:
? Paging: For heavily annotated documents or
very large documents, the original brat vi-
sualization is very slow, both for displaying
and annotating the document. We use a pag-
ing mechanism that limits the number of sen-
tences displayed at a time to make the perfor-
mance independent of the document size.
? Annotation layers: Annotators usually work
on one or two annotations layers, such as
part-of-speech and dependency or named en-
tity annotation. Overloading the annota-
tion page by displaying all annotation layers
makes the annotation and visualization pro-
cess slower. WebAnno provides an option to
configure visible/editable annotation layers.
? Immediate persistence: Every annotation is
sent to the back end immediately and per-
sisted there. An explicit interaction by the
user to save changes is not required.
3.1.3 Workflow
WebAnno implements a simple workflow to track
the state of a project. Every annotator works on a
separate version of the document, which is set to
the state in progress the first time a document is
opened by the annotator. The annotator can then
mark it as complete at the end of annotation at
which point it is locked for further annotation and
can be used for curation. Such a document cannot
be changed anymore by an annotator, but can be
used by a curator. A curator can mark a document
as adjudicated.
3.1.4 Curation
The curation interface allows the curator to open a
document and compare annotations made by the
annotators that already marked the document as
complete. The curator reconciles the annotation
with disagreements. The curator can either decide
on one of the presented alternatives, or freely re-
annotate. Figure 3 illustrates how the curation in-
terface detects sentences with annotation disagree-
ment (left side of Figure 3) which can be used to
navigate to the sentences for curation.
3.1.5 Monitoring
WebAnno provides a monitoring component, to
track the progress of a project. The project man-
ager can check the progress and compute agree-
ment with Kappa and Tau (Carletta, 1996) mea-
sures. The progress is visualized using a matrix of
annotators and documents displaying which docu-
ments the annotators have marked as complete and
which documents the curator adjudicated. Fig-
ure 4 shows the project progress, progress of in-
dividual annotator and completion statistics.
3
Figure 3: Curation user interface (left: sentences
with disagreement; right: merging editor)
3.1.6 Crowdsourcing
Crowdsourcing is a way to quickly scale annota-
tion projects. Distributing a task that otherwise
will be performed by a controlled user group has
become much easier. Hence, if quality can be en-
sured, it is an alternative to high quality annotation
using a large number of arbitrary redundant anno-
tations (Wang et al, 2013). For WebAnno, we
have designed an approach where a source doc-
ument is split into small parts that get presented
to micro-workers in the CrowdFlower platform3.
The crowdsourcing component is a separate mod-
ule that handles the communication via Crowd-
Flower?s API, the definition of test items and job
parameters, and the aggregation of results. The
crowdsourced annotation appears as a virtual an-
notator in the tool.
Since it is not trivial to express complex anno-
tation tasks in comparably simple templates suit-
able for crowdsourcing (Biemann, 2013), we pro-
ceed by working out crowdsourcing templates and
strategies per annotation layer. We currently only
support named entity annotation with predefined
templates. However, the open and modular archi-
tecture allows to add more crowdsourced annota-
tion layers.
3.2 Back End
WebAnno is a Java-based web application that
may run on any modern servlet container. In mem-
ory and on the file system, annotations are stored
3www.crowdflower.com
Figure 4: Project monitoring
as UIMA CAS objects (Ferrucci and Lally, 2004).
All other data is persisted in an SQL database.
3.2.1 Data Conversion
WebAnno supports different data models that re-
flect the different communication of data between
the front end, back end, and the persistent data
storage. The brat data model serves exchanging
data between the front end and the back end.
The documents are stored in their original for-
mats. For annotations, we use the type system
from the DKPro Core collection of UIMA compo-
nents (Eckart de Castilho and Gurevych, 2009)4.
This is converted to the brat model for visualiza-
tion. Importing documents and exporting anno-
tations is implemented using UIMA reader and
writer components from DKPro Core as plug-ins.
Thus, support for new formats can easily be added.
To provide quick reaction times in the user inter-
face, WebAnno internally stores annotations in a
binary format, using the SerializedCasReader and
SerializedCasWriter components.
3.2.2 Persistent Data Storage
Project definitions including project name and de-
scriptions, tagsets and tags, and user details are
kept in a database, whereas the documents and an-
notations are stored in the file system. WebAnno
supports limited versioning of annotations, to pro-
tect against the unforeseen loss of data. Figure 5
shows the database entity relation diagram.
4code.google.com/p/dkpro-core-asl/
4
Figure 5: WebAnno database scheme
4 Scope and Functionality Summary
WebAnno supports the production of linguistically
annotated corpora for different natural language
processing applications. WebAnno implements
ease of usage and simplicity for untrained users,
and provides:
? Annotation via a fast, and easy-to-use web-
based user interface.
? Project and user management.
? Progress and quality monitoring.
? Interactive curation by adjudicating disagree-
ing annotations from multiple users.
? Crowdsourcing of annotation tasks.
? Configurable annotation types and tag sets.
5 Use Cases
WebAnno currently allows to configure different
span and arc annotations. It comes pre-configured
with the following annotation layers from the
DKPro Core type system:
Span annotations
? Part-of-Speech (POS) tags: an annotation
task on tokens. Currently, POS can be added
to a token, if not already present, and can be
modified. POS annotation is a prerequisite of
dependency annotation (Figure 6).
Figure 6: Parts-of-speech & dependency relations
Figure 7: Co-reference & named entites
? Named entities: a multiple span annotation
task. Spans can cover multiple adjacent to-
kens, nest and overlap (Figure 7), but cannot
cross sentence boundaries.
Arc Annotations
? Dependency relations: This is an arc annota-
tion which connects two POS tag annotations
with a directed relation (Figure 6).
? Co-reference chains: The co-reference chain
is realized as a set of typed mention spans
linked by typed co-reference relation arcs.
The co-reference relation annotation can
cross multiple sentences and is represented in
co-reference chains (Figure 7).
The brat front end supports tokens and sub-
tokens as a span annotation. However, tokens are
currently the minimal annotation units in Web-
Anno, due to a requirement of supporting the TCF
file format (Heid et al, 2010). Part-of-speech an-
notation is limited to singles token, while named
entity and co-reference chain annotations may
span multiple tokens. Dependency relations are
implemented in such a way that the arc is drawn
from the governor to the dependent (or the other
way around, configurable), while co-reference
chains are unidirectional and a chain is formed by
referents that are transitively connected by arcs.
Based on common practice in manual annota-
tion, every user works on their own copy of the
same document so that no concurrent editing oc-
curs. We also found that displaying all annotation
layers at the same time is inconvenient for anno-
tators. This is why WebAnno supports showing
5
and hiding of individual annotation layers. The
WebAnno curation component displays all anno-
tation documents from all users for a given source
document, enabling the curator to visualize all of
the annotations with differences at a time. Unlike
most of the annotation tools which rely on config-
uration files, WebAnno enables to freely configure
all parameters directly in the browser.
6 Conclusion and Outlook
WebAnno is a new web-based linguistic annota-
tion tool. The brat annotation and GUI front end
have been enhanced to support rapidly process-
ing large annotation documents, configuring the
annotation tag and tagsets in the browser, speci-
fying visible annotation layers, separating anno-
tation documents per user, just to name the most
important distinctions. Besides, WebAnno sup-
ports project definition, import/export of tag and
tagsets. Flexible support for importing and ex-
porting different data formats is handled through
UIMA components from the DKPro Core project.
The monitoring component of WebAnno helps the
administrator to control the progress of annotators.
The crowdsourcing component of WebAnno pro-
vides a unique functionality to distribute the an-
notation to a large workforce and automatically
integrate the results back into the tool via the
crowdsourcing server. The WebAnno annotation
tool supports curation of different annotation doc-
uments, displaying annotation documents created
by users in a given project with annotation dis-
agreements. In future work, WebAnno will be en-
hanced to support several other front ends to han-
dle even more annotation layers, and to provide
more crowdsourcing templates. Another planned
extension is a more seamless integration of lan-
guage processing tools for pre-annotation.
Acknowledgments
We would like to thank Benjamin Milde and Andreas
Straninger, who assisted in implementing WebAnno, as well
as Marc Reznicek, Nils Reiter and the whole CLARIN-D F-
AG 7 for testing and providing valuable feedback. The work
presented in this paper was funded by a German BMBF grant
to the CLARIN-D project, the Hessian LOEWE research ex-
cellence program as part of the research center ?Digital Hu-
manities? and by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant No. I/82806.
References
Chris Biemann. 2013. Creating a system for lexical substi-
tutions from scratch using crowdsourcing. Lang. Resour.
Eval., 47(1):97?122, March.
Kalina Bontcheva, Hamish Cunningham, Ian Roberts, and
Valentin Tablan. 2010. Web-based collaborative corpus
annotation: Requirements and a framework implementa-
tion. In New Challenges for NLP Frameworks workshop
at LREC-2010, Malta.
Jean Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. In Computational Linguistics,
Volume 22 Issue 2, pages 249?254.
Richard Eckart de Castilho and Iryna Gurevych. 2009.
DKPro-UGD: A Flexible Data-Cleansing Approach to
Processing User-Generated Discourse. In Online-
proceedings of the First French-speaking meeting around
the framework Apache UIMA, LINA CNRS UMR 6241 -
University of Nantes, France.
David Ferrucci and Adam Lally. 2004. UIMA: An Architec-
tural Approach to Unstructured Information Processing in
the Corporate Research Environment. In Journal of Natu-
ral Language Engineering 2004, pages 327?348.
Ulrich Heid, Helmut Schmid, Kerstin Eckart, and Erhard
Hinrichs. 2010. A Corpus Representation Format for
Linguistic Web Services: the D-SPIN Text Corpus Format
and its Relationship with ISO Standards. In Proceedings
of LREC 2010, Malta.
Boci Lin, Yan Chen, Xu Chen, and Yingying Yu. 2012.
Comparison between JSON and XML in Applications
Based on AJAX. In Computer Science & Service System
(CSSS), 2012, Nanjing, China.
Thomas Morton and Jeremy LaCivita. 2003. WordFreak:
an open tool for linguistic annotation. In Proceedings of
NAACL-2003, NAACL-Demonstrations ?03, pages 17?18,
Edmonton, Canada.
Christoph Mu?ller and Michael Strube. 2006. Multi-level an-
notation of linguistic data with MMAX2. In S. Braun,
K. Kohn, and J. Mukherjee, editors, Corpus Technology
and Language Pedagogy: New Resources, New Tools,
NewMethods, pages 197?214. Peter Lang, Frankfurt a.M.,
Germany.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 Shared Task on Dependency Parsing.
In Proceedings of the CoNLL Shared Task Session of
EMNLP-CoNLL 2007, pages 915?932, Prague, Czech Re-
public.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. brat:
a Web-based Tool for NLP-Assisted Text Annotation. In
Proceedings of the Demonstrations at EACL-2012, Avi-
gnon, France.
Qingling Wang, Qin Liu, Na Li, and Yan Liu. 2008. An
Automatic Approach to Reengineering Common Website
with AJAX. In 4th International Conference on Next Gen-
eration Web Services Practices, pages 185?190, Seoul,
South Korea.
Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan. 2013.
Perspectives on Crowdsourcing Annotations for Natural
Language Processing. In Language Resources And Eval-
uation, pages 9?31. Springer Netherlands.
6
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 91?96,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
Automatic Annotation Suggestions and
Custom Annotation Layers in WebAnno
Seid Muhie Yimam
1
Richard Eckart de Castilho
2
Iryna Gurevych
2,3
Chris Biemann
1
(1) FG Language Technology, Dept. of Computer Science, Technische Universit?at Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universit?at Darmstadt
(3) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.{lt,ukp}.tu-darmstadt.de
Abstract
In this paper, we present a flexible ap-
proach to the efficient and exhaustive man-
ual annotation of text documents. For this
purpose, we extend WebAnno (Yimam et
al., 2013) an open-source web-based an-
notation tool.
1
While it was previously
limited to specific annotation layers, our
extension allows adding and configuring
an arbitrary number of layers through a
web-based UI. These layers can be an-
notated separately or simultaneously, and
support most types of linguistic annota-
tions such as spans, semantic classes, de-
pendency relations, lexical chains, and
morphology. Further, we tightly inte-
grate a generic machine learning compo-
nent for automatic annotation suggestions
of span annotations. In two case studies,
we show that automatic annotation sug-
gestions, combined with our split-pane UI
concept, significantly reduces annotation
time.
1 Introduction
The annotation of full text documents is a costly
and time-consuming task. Thus, it is important to
design annotation tools in such a way that the an-
notation process can happen as swiftly as possible.
To this end, we extend WebAnno with the capabil-
ity of suggesting annotations to the annotator.
A general-purpose web-based annotation tool
can greatly lower the entrance barrier for linguistic
annotation projects, as tool development costs and
preparatory work are greatly reduced. WebAnno
1.0 only partially fulfilled desires regarding gen-
erality: Although it covered already more kinds
1
WebAnno is open-source software under the terms of the
Apache Software License 2.0. This paper describes v1.2:
http://webanno.googlecode.com
of annotations than most other tools, it supported
only a fixed set of customizable annotation lay-
ers (named entities, part-of-speech, lemmata, co-
reference, dependencies). Thus, we also remove a
limitation of the tool, which was previously bound
to specific, hardcoded annotation layers.
We have generalized the architecture to support
three configurable generic structures: spans, rela-
tions, and chains. These support all of the original
layers and allow the user to define arbitrary custom
annotation layers based on either of these struc-
tures. Additionally, our approach allows maintain-
ing multiple properties on annotations, e.g. to sup-
port morphological annotations, while previously
only one property per annotation was supported.
Automatic suggestion of annotations is based
on machine learning, which is common practice
in annotation tools. However, most of existing
web-based annotation tools, such as GATE (Cun-
ningham et al., 2011) or brat (Stenetorp et al.,
2012), depend on external preprocessing and post-
processing plugins or on web services. These tools
have limitations regarding adaptability (difficulty
to adapt to other annotation tasks), reconfigurabil-
ity (generating a classifier when new features and
training documents are available is complicated),
and reusability (requires manual intervention to
add newly annotated documents into the iteration).
For our approach, we assume that an annota-
tor actually does manually verify all annotations
to produce a completely labeled dataset. This task
can be sped up by automatically suggesting anno-
tations that the annotator may then either accept
or correct. Note that this setup and its goal differs
from an active learning scenario, where a system
actively determines the most informative yet unan-
notated example to be labeled, in order to quickly
arrive at a high-quality classifier that is then to be
applied to large amounts of unseen data.
Our contribution is the integration of machine
learning into the tool to support exhaustive an-
91
notation of documents providing a shorter loop
than comparable tools (Cunningham et al., 2011;
Stenetorp et al., 2012), because new documents
are added to the training set as soon as they are
completed by the annotators. The machine learn-
ing support currently applies to sequence classifi-
cation tasks only. It is complemented by our ex-
tension allowing to define custom annotation lay-
ers, making it applicable to a wide range of anno-
tation tasks with only little configuration effort.
Section 2 reviews related work about the uti-
lization of automatic supports and customiza-
tion of annotation schemes in existing annotation
tools. The integration of automatic suggestions
into WebAnno, the design principles followed, and
two case studies are explained in Section 3. Sec-
tion 4 presents the implementation of customiz-
able annotation layers into the tool. Finally, Sec-
tion 5 summarizes the main contributions and fu-
ture directions of our work.
2 Related Work
Automatic annotation support The impact of
using lexical and statistical resources to produce
pre-annotation automatically to increase the anno-
tation speed has been studied widely for various
annotation tasks. For the task of medical named
entity labeling, Lingren et al. (2013) investigate
the impact of automatic suggestions on annotation
speed and potential biases using dictionary-based
annotations. This technique results in 13.83% to
21.5% time saving and in an inter-annotator agree-
ment (IAA) increase by several percentage points.
WordFreak (Morton and LaCivita, 2003) in-
cludes an automation component, where instances
with a low machine learning confidence are pre-
sented for annotation in an active learning setup.
Beck et al. (2013) demonstrate that the use of ac-
tive learning for machine translation reduces the
annotation effort and show a reduced annotation
load on three out of four datasets.
The GoldenGATE editor (Sautter et al., 2007)
integrates NLP tools and assistance features for
manual XML editing. The tool is used in correct-
ing/editing an automatically annotated document
with an editor where both text and XML markups
are modified. GoldenGATE is merely used to fa-
cilitate the correction of an annotation while pre-
annotation is conducted outside of the tool.
Automatic annotation support in brat (Stenetorp
et al., 2012) was carried out for a semantic class
disambiguation task to investigate how such au-
tomation facilitates the annotators? progress. They
report a 15.4% reduction in total annotation time.
However, the automation process in brat 1) de-
pends on bulk annotation imports and web service
configurations, which is labor intensive, 2) is task
specific so that it requires a lot of effort to adapt it
to different annotation tasks, 3) there is no way of
using the corrected result for the next iteration of
training the automatic tool.
The GATE Teamware (Bontcheva et al., 2013)
automation component is most similar to our
work. It is based either on plugins and externally
trained classification models, or uses web services.
Thus, it is highly task specific and requires exten-
sive configuration. The automatic annotation sug-
gestion component in our tool, in contrast, is easily
configurable and adaptable to different annotation
tasks and allows the use of annotations from the
current annotation project.
Custom annotation layers Generic annotation
data models are typically directed graph models
(e.g. GATE, UIMA CAS (G?otz and Suhre, 2004),
GrAF (Ide and Suderman, 2007)). In addition, an
annotation schema defines possible kinds of anno-
tations, their properties and relations. While these
models offer great expressiveness and flexibility, it
is difficult to adequately transfer their power into
a convenient annotation editor. For example, one
schema may prescribe that the part-of-speech tag
is a property on a Token annotation, another one
may prescribe that the tag is a separate annotation,
which is linked to the token. An annotator should
not be exposed to these details in the UI and should
be able to just edit a part-of-speech tag, ignorant of
the internal representation.
This problem is typically addressed in two
ways. Either, the full complexity of the annota-
tion model is exposed to the annotator, or the an-
notation editor uses a simplified model. The first
approach can easily lead to an unintuitive UI and
make the annotation an inconvenient task. The
second approach (e.g. as advocated by brat) re-
quires the implementation of specific import and
export filters to transform between the editor data
model and the generic annotation data models.
We propose a third approach integrating a con-
figurable mapping between a generic annotation
model (UIMA CAS) and a simplified editing
model (brat) directly into the annotation tool.
Thus, we avoid exposing the full complexity of
92
the generic model to the user and also avoid the
necessity for implementing import/export filters.
Similar approaches have already been used to map
annotation models to visualization modules (cf.
(Zeldes et al., 2009)), but have, to our knowledge,
not been used in an annotation editor. Our ap-
proach is different from schema-based annotation
editors (e.g. GATE), which employ a schema as
a template of properties and controlled vocabular-
ies that can be used to annotate documents, but
which do not allow to map structures inherent in
annotations, like relations or chains, to respective
concepts in the UI.
3 Automatic Annotation Suggestions
It is the purpose of the automatic annotation sug-
gestion component to increase the annotation ef-
ficiency, while maintaining the quality of annota-
tions. The key design principle of our approach is
a split-pane (Figure 1) that displays automatic an-
notation suggestions in the suggestion pane (lower
part) and only verified or manual ones in the anno-
tation pane (upper part). In this way, we force the
annotators to review each automatic suggestion as
to avoid overlooking wrong suggestions.
Figure 1: Split-pane UI. Upper: the annotation
pane, which should be completed by the annotator.
Lower: the suggestion pane, displaying predic-
tions or automatic suggestions, and coding their
status in color. This examples shows automatic
suggestions for parts-of-speech. Unattended anno-
tations are rendered in blue, accepted annotations
in grey and rejected annotations in red. Here, the
last five POS annotations have been attended, four
have been accepted by clicking on the suggestion,
and one was rejected by annotating it in the anno-
tation pane.
3.1 Suggestion modes
We distinguish three modes of automatic annota-
tion suggestion:
Correction mode In this mode, we import doc-
uments annotated by arbitrary external tools and
present them to the user in the suggestion pane
of the annotation page. This mode is specifi-
cally appropriate for annotation tasks where a pre-
annotated document contains several possibilities
for annotations in parallel, and the user?s task is
to select the correct annotation. This allows to
leverage specialized external automatic annotation
components, thus the tool is not limited to the in-
tegrated automation mechanism.
Repetition mode In this mode, further occur-
rences of a word annotated by the user are high-
lighted in the suggestion pane. To accept sugges-
tions, the user can simply click on them in the sug-
gestion pane. This basic ? yet effective ? sugges-
tion is realized using simple string matching.
Learning mode For this mode, we have inte-
grated MIRA (Crammer and Singer, 2003), an ex-
tension of the perceptron algorithm for online ma-
chine learning which allows for the automatic sug-
gestions of span annotations. MIRA was selected
because of its relatively lenient licensing, its good
performance even on small amounts of data, and
its capability of allowing incremental classifier up-
dates. Results of automatic tagging are displayed
in the suggestion pane. Our architecture is flexible
to integrate further machine learning tools.
3.2 Suggestion Process
The workflow to set up an automatically supported
annotation project consists of the following steps.
Importing annotation documents We can im-
port documents with existing annotations (manual
or automatic). The annotation pane of the automa-
tion page allows users to annotate documents and
the suggestion pane is used for the automatic sug-
gestion as shown in Figure 1. The suggestion pane
facilitates accepting correct pre-annotations with
minimal effort.
Configuring features For the machine learning
tool, it is required to define classification features
to train a classifier. We have designed a UI where
a range of standard classification features for se-
quence tagging can be configured. The features
include morphological features (prefixes, suffixes,
and capitalization), n-grams, and other layers as a
feature (for example POS annotation as a feature
93
Figure 2: Configuring an annotation suggestion: 1) layers for automation, 2) different features, 3) training
documents, 4) start training classifier.
for named entity recognition). While these stan-
dard features do not lead to state-of-the-art per-
formance on arbitrary tasks, we have found them
to perform very well for POS tagging, named en-
tity recognition, and chunking. Figure 2 shows the
feature configuration in the project settings.
Importing training documents We offer two
ways of providing training documents: importing
an annotated document in one of the supported file
formats, such as CoNLL, TCF, or UIMA XMI; or
using existing annotation documents in the same
project that already have been annotated.
Starting the annotation suggestion Once fea-
tures for a training layer are configured and train-
ing documents are available, automatic annotation
is possible. The process can be started manually
by the administrator from the automation settings
page, and it will be automatically re-initiated when
additional documents for training become avail-
able in the project. While the automatic annotation
is running in the background, users still can work
on the annotation front end without being affected.
Training and creating a classifier will be repeated
only when the feature configuration is changed or
when a new training document is available.
Display results on the monitoring page Af-
ter the training and automatic annotation are com-
pleted, detailed information about the training data
such as the number of documents (sentence, to-
kens), features used for each layer, F-score on
held-out data, and classification errors are dis-
played on the monitoring page, allowing an esti-
mation whether the automatic suggestion is use-
ful. The UI also shows the status of the training
process (not started, running, or finished).
3.3 Case Studies
We describe two case studies that demonstrate lan-
guage independence and flexibility with respect to
sequence label types of our automatic annotation
suggestions. In the first case study, we address the
task of POS tagging for Amharic as an example of
an under-resourced language. Second, we explore
German named entity recognition.
3.3.1 Amharic POS tagging
Amharic is an under-resourced language in the
Semitic family, mainly spoken in Ethiopia. POS
tagging research for Amharic is mostly conducted
as an academic exercise. The latest result re-
ported by Gebre (2009) was about 90% accuracy
using the Walta Information Center (WIC) corpus
of about 210,000 tokens (1065 news documents).
We intentionally do not use the corpus as training
data because of the reported inconsistencies in the
tagging process (Gebre, 2009). Instead, we man-
ually annotate Amharic documents for POS tag-
ging both to test the performance of the automa-
tion module and to produce POS-tagged corpora
for Amharic. Based upon the work by Petrov et al.
(2012) and Ethiopian Languages Research Cen-
ter (ELRC) tagset, we have designed 11 POS tags
equivalent to the Universal POS tags. The tag DET
is not included as Amharic denotes definiteness as
noun suffixes.
We collected some Amharic documents from an
online news portal.
2
Preprocessing of Amharic
documents includes the normalization of charac-
ters and tokenization (sentence and word bound-
2
http://www.ethiopianreporter.com/
94
Figure 3: Example Amharic document. The red
tags in the suggestion pane have not been con-
firmed by the annotator.
ary detection). Initially, we manually annotated 21
sentences. Using these, an iterative automatic an-
notation suggestion process was started until 300
sentences were fully annotated. We obtained an
F-score of 0.89 with the final model. Hence the
automatic annotation suggestion helps in decreas-
ing the total annotation time, since the user has
to manually annotate only one out of ten words,
while being able to accept most automatic sugges-
tions. Figure 3 shows such an Amharic document
in WebAnno.
3.3.2 German Named Entity Recognition
A pilot Named Entity Recognition (NER) project
for German was conducted by Benikova et al.
(2014). We have used the dataset ? about 31,000
sentences, over 41,000 NE annotations ? for train-
ing NER. Using this dataset, an F-score of about
0.8 by means of automatic suggestions was ob-
tained, which leads to an increase in annotation
speed of about 21% with automatic suggestion.
4 Customs Annotation Layers
The tasks in which an annotation editor can be em-
ployed depends on the expressiveness of the un-
derlying annotation model. However, fully expos-
ing the expressive power in the UI can make the
editor inconvenient to use.
We propose an approach that allows the user
to configure a mapping of an annotation model to
concepts well-supported in a web-based UI. In this
way, we can avoid to expose all details of the an-
notation model in the UI, and remove the need to
implement custom import/export filters.
WebAnno 1.0 employs a variant of the annota-
tion UI provided by brat, which offers the concepts
of spans and arcs. Based on these, WebAnno 1.2
implements five annotation layers: named entity,
part-of-speech, lemmata, co-reference, and depen-
dencies. In the new WebAnno version, we gener-
alized the support for these five layers into three
Figure 4: UI for custom annotation layers.
structural categories: span, relation (arc), and
chain. Each of these categories is handled by a
generic adapter which can be configured to sim-
ulate any of the original five layers. Based on
this generalization, the user can now define cus-
tom layers (Figure 4).
Additionally, we introduced a new concept of
constraints. For example, NER spans should not
cross sentence boundaries and attach to whole to-
kens (not substrings of tokens). Such constraints
not only help preventing the user from making in-
valid annotations, but can also offer extra conve-
nience. We currently support four hard-coded con-
straints:
Lock to token offsets Defines if annotation
boundaries must coincide with token boundaries,
e.g. named entities, lemmata, part-of-speech, etc.
For the user?s convenience, the annotation is auto-
matically expanded to include the full token, even
if only a part of a token is selected during annota-
tion (span/chain layers only).
Allow multiple tokens Some kinds of annota-
tions may only cover a single token, e.g. part-of-
speech, while others may cover multiple tokens,
e.g. named entities (span/chain layers only).
Allow stacking Controls if multiple annotations
of the same kind can be at the same location, e.g.
if multiple lemma annotations are allowed per to-
ken. For the user?s convenience, an existing an-
notation is replaced if a new annotation is created
when stacking is not allowed.
Allow crossing sentence boundaries Certain
annotations, e.g. named entities or dependency de-
lations, may not cross sentence boundaries, while
others need to, e.g. coreference chains.
Finally, we added the ability to define multiple
properties for annotations to WebAnno. For exam-
ple, this can be use to define a custom span-based
morphology layer with multiple annotation prop-
erties such as gender, number, case, etc.
95
5 Conclusion and Outlook
We discussed two extensions of WebAnno: the
tight and generic integration of automatic annota-
tion suggestions for reducing the annotation time,
and the web-based addition and configuration of
custom annotation layers.
While we also support the common practice
of using of external tools to automatically pre-
annotate documents, we go one step further by
tightly integrating a generic sequence classifier
into the tool that can make use of completed an-
notation documents from the same project. In two
case studies, we have shown quick convergence
for Amharic POS tagging and a substantial reduc-
tion in annotation time for German NER. The key
concept here is the split-pane UI that allows to dis-
play automatic suggestions, while forcing the an-
notator to review all of them.
Allowing the definition of custom annotation
layers in a web-based UI is greatly increasing
the number of annotation projects that potentially
could use our tool. While it is mainly an engineer-
ing challenge to allow this amount of flexibility
and to hide its complexity from the user, it is a ma-
jor contribution in the transition from specialized
tools towards general-purpose tools.
The combination of both ? custom layers and
automatic suggestions ? gives rise to the rapid
setup of efficient annotation projects. Adding to
existing capabilities in WebAnno, such as cura-
tion, agreement computation, monitoring and fine-
grained annotation project definition, our contri-
butions significantly extend the scope of annota-
tion tasks in which the tool can be employed.
In future work, we plan to support annota-
tion suggestions for non-span structures (arcs and
chains), and to include further machine learning
algorithms.
Acknowledgments
The work presented in this paper was funded by a German
BMBF grant to the CLARIN-D project, the Hessian LOEWE
research excellence program as part of the research center
?Digital Humanities? and by the Volkswagen Foundation as
part of the Lichtenberg-Professorship Program under grant
No. I/82806.
References
Daniel Beck, Lucia Specia, and Trevor Cohn. 2013. Re-
ducing annotation effort for quality estimation via active
learning. In Proc. ACL 2013 System Demonstrations,
Sofia, Bulgaria.
Darina Benikova, Chris Biemann, and Marc Reznicek. 2014.
NoSta-D Named Entity Annotation for German: Guide-
lines and Dataset. In Proc. LREC 2014, Reykjavik, Ice-
land.
Kalina Bontcheva, H. Cunningham, I. Roberts, A. Roberts,
V. Tablan, N. Aswani, and G. Gorrell. 2013. GATE
Teamware: a web-based, collaborative text annota-
tion framework. Language Resources and Evaluation,
47(4):1007?1029.
Koby Crammer and Yoram Singer. 2003. Ultraconservative
online algorithms for multiclass problems. In Journal of
Machine Learning Research 3, pages 951 ? 991.
Hamish Cunningham, D. Maynard, K. Bontcheva, V. Tablan,
N. Aswani, I. Roberts, G. Gorrell, A. Funk, A. Roberts,
D. Damljanovic, T. Heitz, M. A. Greenwood, H. Saggion,
J. Petrak, Y. Li, and W. Peters. 2011. Text Processing with
GATE (Version 6). University of Sheffield Department of
Computer Science, ISBN 978-0956599315.
Binyam Gebrekidan Gebre. 2009. Part-of-speech tagging for
Amharic. In ISMTCL Proceedings, International Review
Bulag, PUFC.
T. G?otz and O. Suhre. 2004. Design and implementation
of the UIMA Common Analysis System. IBM Systems
Journal, 43(3):476 ?489.
Nancy Ide and Keith Suderman. 2007. GrAF: A graph-based
format for linguistic annotations. In Proc. Linguistic An-
notation Workshop, pages 1?8, Prague, Czech Republic.
Todd Lingren, L. Deleger, K. Molnar, H. Zhai, J. Meinzen-
Derr, M. Kaiser, L. Stoutenborough, Q. Li, and I. Solti.
2013. Evaluating the impact of pre-annotation on anno-
tation speed and potential bias: natural language process-
ing gold standard development for clinical named entity
recognition in clinical trial announcements. In Journal of
the American Medical Informatics Association, pages 951
? 991.
Thomas Morton and Jeremy LaCivita. 2003. WordFreak: an
open tool for linguistic annotation. In Proc. NAACL 2003,
demonstrations, pages 17?18, Edmonton, Canada.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2012. A
universal part-of-speech tagset. In Proc LREC 2012, Is-
tanbul, Turkey.
Guido Sautter, Klemens B?ohm, Frank Padberg, and Walter
Tichy. 2007. Empirical Evaluation of Semi-automated
XML Annotation of Text Documents with the GoldenGATE
Editor. Budapest, Hungary.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c, Tomoko
Ohta, Sophia Ananiadou, and Jun?ichi Tsujii. 2012. brat:
a Web-based Tool for NLP-Assisted Text Annotation. In
Proc. EACL 2012 Demo Session, Avignon, France.
Seid Muhie Yimam, Iryna Gurevych, Richard Eckart
de Castilho, and Chris Biemann. 2013. WebAnno: A
flexible,web-based and visually supported system for dis-
tributed annotations. In Proc. ACL 2013 System Demon-
strations, pages 1?6, Sofia, Bulgaria.
Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian Chiar-
cos. 2009. ANNIS: A search tool for multi-layer anno-
tated corpora. In Proc. Corpus Linguistics 2009, Liver-
pool, UK.
96
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 1?11,
Dublin, Ireland, August 23rd 2014.
A broad-coverage collection of portable NLP components
for building shareable analysis pipelines
Richard Eckart de Castilho
1
Iryna Gurevych
1,2
(1) Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Dept. of Computer Science, Technische Universit?at Darmstadt
(2) Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
http://www.ukp.tu-darmstadt.de
Abstract
Due to the diversity of natural language processing (NLP) tools and resources, combining them
into processing pipelines is an important issue, and sharing these pipelines with others remains
a problem. We present DKPro Core, a broad-coverage component collection integrating a wide
range of third-party NLP tools and making them interoperable. Contrary to other recent endeav-
ors that rely heavily on web services, our collection consists only of portable components dis-
tributed via a repository, making it particularly interesting with respect to sharing pipelines with
other researchers, embedding NLP pipelines in applications, and the use on high-performance
computing clusters. Our collection is augmented by a novel concept for automatically selecting
and acquiring resources required by the components at runtime from a repository. Based on these
contributions, we demonstrate a way to describe a pipeline such that all required software and
resources can be automatically obtained, making it easy to share it with others, e.g. in order to
reproduce results or as examples in teaching, documentation, or publications.
1 Introduction
Sharing is a central concept to scientific work and to software development. In science, information about
experimental setups and results is shared with fellow researchers, not only to disseminate new insights,
but also to allow others to validate results or to improve on them. In software development, libraries
and component-based architectures are a central mechanism to promote the reuse of software. Portable
software must operate in the same way across system platforms. In the context of scientific research, this
is an important factor related to the reproducibility of results created from software-based experiments.
The NLP software landscape provides a wealth of reusable software in the form of NLP tools ad-
dressing language analysis at different levels from tokenization to sentiment analysis. These tools are
combined into NLP pipelines that form essential parts of experiments in natural language research and
beyond, e.g. in the emerging digital humanities. Therefore, it is essential that such pipelines can easily
be shared between researchers, to reproduce results, to evolve experiments, and to allow for a better
understanding of the exact details of an experiment (cf. Fokkens et al. (2013)).
Analyzing the current state of the art, we find that despite considerable effort that has been going into
processing frameworks enabling interoperability, workbenches to build and run pipelines, and all kinds
of online services, it is still not possible to create a readily shareable description of an NLP pipeline. A
pipeline description is basically a configuration file referencing the components and resources used by
the pipeline. Currently, these references are ambiguous, e.g. because they do not incorporate version
information. This causes a reproducibility problem, e.g. when a pipeline is part of an experiment,
because the use of a different version can easily lead to different results. A sharable description must be
self-contained in the sense that it uniquely identifies all involved components and resources, permitting
the execution environment for the pipeline to be set up reproducibly, in the best case automatically.
Currently, the task of setting up the environment is largely left to the user and requires time and diligence.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
In this paper, we present a novel concept for self-contained NLP pipeline descriptions supported by
a broad-coverage collection of interoperable NLP components. Our approach is enabled by the com-
bination of distributing portable NLP components and resources through a repository and by an auto-
configuration mechanism allowing components to select suitable resources at runtime and to obtain them
automatically from the repository. Our contributions facilitate the sharing of pipelines, e.g. as part of
publications or examples in documentation, and allow users to maintain control by providing the ability
to create backups of components and resources for a later reproduction of results.
Section 2 reflects on the state of the art and identifies the need for a broad-coverage component col-
lection of portable components, as well as the need for self-contained pipeline descriptions. Section 3.1
describes a novel concept for the automatic selection and acquisition of resources. Section 3.2 presents
a broad-coverage collection of portable components integrating this concept. Section 3.3 demonstrates a
shareable workflow based on these contributions. Section 4 gives further examples of how our contribu-
tions could be applied. Finally, Section 5 summarizes the paper and suggests future research directions.
2 State of the art
In this section, we examine the current state of the art related to describing NLP pipelines, kinds of
component collections, publishing of components and resources, and the selection of resources to use
with a component. We start by defining the terminology used throughout the rest of this paper.
Definition of terminology We make a distinction between a tool and a component. Most NLP tools are
standalone tools addressing one particular task, e.g. dependency parsing, relying on separate tokenizers,
part-of-speech (POS) taggers, etc. These tools cannot be easily combined into pipelines, because their
input/output formats are often not compatible and because they lack a uniform programming interface.
We speak of a component when a tool has been integrated into a processing framework, usually by im-
plementing an adapter between the tool and the framework. The framework defines a uniform program-
ming interface, data model, and processing model enabling interoperability between the components.
Through integration with the framework, components become easily composeable into pipelines. A
pipeline consists of components processing input documents one after the other and passing the output
on to the next component. Each component adds annotations to the document, e.g. sentence and token
boundaries, POS tags, syntactic constituents, or dependency relations, etc. These steps build upon each
other, e.g. a component for dependency parsing requires at least sentences, tokens, and POS tags.
Many components are generic engines that require some resource (e.g. a probabilistic model or knowl-
edge base) that configures them for a specific language, tagset, domain, etc. We use the term resource
selection for the task of choosing a resource and configuring a component to use it. The task of obtaining
the resource we call resource acquisition.
As Thompson et al. (2011) point out, achieving a consensus on the exact representation of different
linguistic theories as annotations and thereby attaining full conceptual interoperability between com-
ponents from different vendors is currently not considered feasible. Thus, frameworks leave the type
system (kinds of annotations) unspecified. Therefore, the technical integration with a framework alone
does not make the tools interoperable on the conceptual level (cf. Chiarcos et al. (2008)). As a conse-
quence, multiple component collections exist, each providing interoperable components centered around
a particular combination of processing framework and type system (e.g. Buyko and Hahn (2008), Kano
et al. (2011), Wu et al. (2013)). Each of these defines its own concepts of tokens, sentence, syntactic
structures, discourse structures, etc. Yet, even these type systems leave certain aspects underspecified,
e.g. the various tagsets used to categorize parts-of-speech, syntactic constituents, etc.
2.1 Sharing pipelines
Processing frameworks offer a way to construct pipeline descriptions that instruct the framework to
configure and execute a pipeline of NLP components.
GATE (Cunningham et al., 2002) and Apache UIMA (Ferrucci and Lally, 2004) are currently the most
prominent processing frameworks. Both describe pipelines by means of XML documents. These refer to
individual components by name and expect that the user has taken precautions that these components are
2
accessible by the framework. Neither framework includes provisions to automatically obtain the compo-
nents or resources they require, e.g. from a repository (Section 2.2). In fact, the pipeline descriptions do
not contain sufficient information to uniquely identify components. Components are referred to only by
name, but not by version. The same is true for resources which are often referred to only by filename.
Thus, both of the major processing frameworks to not offer self-contained descriptions for pipelines.
When such pipeline descriptions are shared with another person, the recipient requires additional infor-
mation about which exact versions of tools and resources are needed to run the pipeline.
2.2 Publishing components and resources
In this section, we examine different approaches to publishing NLP components and resources so that
they can be more easily found, accessed, or obtained in order to execute a particular pipeline.
Directories META-SHARE (Thompson et al., 2011) and the CLARIN Virtual Language Observatory
(VLO) (Uytvanck et al., 2010) are two directories of language resources, including NLP tools and re-
sources. These directories currently target primarily human users and offer rich metadata and a user
interface to browse it or to find specific kinds of entries. However, these directories to not contain suffi-
cient information to programmatically download the software or resources, or to access them as services.
Repositories Repositories are online services from which components and resources can be obtained.
The Central Repository (2014) is a repository within the Java-ecosystem used to distribute Java li-
braries and resources they require, so-called artifacts. It relies on concepts that have evolved around
the Maven project (Sonatype Company, 2008). Meanwhile, these are supported by many build tools,
development environments, and even by some programming languages (cf. Section 3.3). Several NLP
tools (e.g. ClearNLP (2014), Stanford CoreNLP (Manning et al., 2014), MaltParser (Nivre et al., 2007),
ClearTK (Ogren et al., 2009)) are already distributed via this medium, some including their resources.
There are many Maven repositories on the internet. They are organized as a loosely federated network.
The Central Repository merely serves as the default point of contact built into clients. Repositories have
the ability to access each other and to cache those artifacts required by their immediate users. This
provides resilience against network failures or remote data loss. Artifacts can be addressed across the
federation by a set of coordinates (groupId, artifactId, and version).
Another kind of repositories are plug-in repositories, such as those used by GATE (Cunningham et al.,
2002). From these, the user can conveniently download and install components within the GATE work-
bench. These plug-in repositories are specific to GATE, whereas the Maven repositories are a generic
infrastructure widely used by the Java community and that is supported by many tools and applications.
Online Workbenches While many NLP tools are offered as portable software for offline use, we
observe a trend in recent years towards offering NLP tools as web-services for online use, sometimes as
the only way to access them. Hinrichs et al. (2010) cite incompatibilities between the software and the
user?s machine and insufficiently powerful workstations as reasons for this approach. Another reason
may be the ability to set up a walled garden in which the service provider is able to control the use of
services, e.g. to academic researchers or to paying commercial customers.
Argo (Rak et al., 2013) is a web-based workbench. It offers access to a collection of UIMA-based
NLP-services that can be executed in different environments. Rak et al. mention in particular a cluster
environment but also plan support for a number of cloud platforms. For this reason, we assume that most
of the components are integrated into Argo as portable software that can be deployed to these platforms
on-demand. Yet, it appears that the components are only accessible through Argo and that they are not
distributed separately for use in other UIMA-based environments.
U-Compare (Kano et al., 2011) is a Java application for building and running UIMA-based pipelines
and comparing their results. While some components accessible through the workbench run locally,
many components are only stubs calling out to web-services running at different remote locations.
WebLicht (Hinrichs et al., 2010) is a distributed infrastructure of NLP services hosted at different lo-
cations. They exchange data in an XML format called TCF (Text Corpus Format). Pipelines can be built
3
and run using the web-based WebLicht workbench. Within this walled garden platform, authenticated
academic users have access to resources that are free for academic research, but not otherwise.
Online Marketplaces AnnoMarket (Tablan et al., 2013) is another distributed infrastructure of NLP
services based on GATE. It does not seem to offer a workbench to compose custom pipelines. Instead,
it offers a set of pre-configured components and exposes them as web-services to be programmatically
accessed. It is the only commercial offering in this overview that the user has to pay for.
Note on service-based approaches Service-based approaches have also been taken in other scientific
domains to facilitate the creation of shareable and repeatable experiments, e.g. on platforms such as
myExperiment (Goble et al., 2010). However, G?omez-P?erez et al. (2013) found service-based workflows
to be subject to decay as services are updated and change their input/output formats, their results, or
as they become temporarily unavailable due to network problems. We also expect they can become
permanently unavailable, e.g. due to a lack of funding unless supported by a sound business model.
Furthermore, to our knowledge none of the offerings above allow the user to export their pipelines in-
cluding all necessary software and resources, e.g. to make a backup or to deploy it on a private computing
infrastructure, e.g. a private cloud or cluster system.
2.3 Component collections
We define a component collection as a set of interoperable components. The interoperability between the
components is enabled by conventions that are typically rendered as a common annotation type system,
a common API, or both.
Standalone NLP tools Most NLP tools are not comprehensive suites that cover all tasks from tok-
enization to e.g. coreference resolution, but are rather standalone tools addressing only a particular task,
e.g. dependency parsing, relying on separate tokenizers, POS-taggers, etc. Examples of such standalone
tools are MaltParser (Nivre et al., 2007) and HunPos (Hal?acsy et al., 2007). The major part of the analysis
logic is implemented within the tool, such that they tend not to rely significantly on third-party libraries.
However, many third-party resources can be found on the internet for popular standalone tools.
NLP tool suites Some vendors offer tool suites that cover multiple analysis tasks, e.g. ClearNLP,
CoreNLP, and OpenNLP. They consist of a set of interoperable tools. Some even go so far as to in-
clude a proprietary processing framework and pipeline mechanism. For example, CoreNLP allows the
user to implement custom analysis components and to register them with their framework. OpenNLP,
on the other hand, provides UIMA-wrappers for their tools. These wrappers are configurable for dif-
ferent UIMA type systems, but unfortunately the configuration mechanism is not powerful enough to
accommodate for the design of various major type systems.
We also refer to such tool suites as single vendor collections. As for standalone tools, the major part
of the analysis logic is a part of the suite and tends not to rely significantly on third-party libraries. Also,
again many third-parties offer resources for popular tool suites.
Special purpose collections Special purpose collections combine NLP tools into a comprehensive
modular pipeline for a specific purpose.
The Apache cTAKES project (Savova et al., 2010) offers a UIMA-based pipeline for the analysis of
medical records which includes components from ClearNLP, OpenNLP, and more for the basic language
analysis. These third-party components are used in conjunction with resources created specifically for
the domain of medical records. Higher-level tasks use original components from the project, e.g. to
identify drugs or relations specific to the medical domain.
Broad-coverage collections Broad-coverage collections cover multiple analysis tasks, but they do not
focus on a specific purpose. Instead, they provide the user with a choice for each analysis task by
integrating tools from different vendors capable of doing the same task. Because the languages supported
by each tool differ, this allows the collection to cover more languages than individual tools or even tool
suites alone. Additionally, broad-coverage collections allow comparing different tools against each other.
4
The U-Compare workbench (Kano et al., 2011) focusses specifically on the ability to compare tools
against each other. It offers a GUI for building analysis pipelines and comparing their results. U-Compare
also offers a collection of UIMA-based components centered around the U-Compare type system. It
started integrating analysis tools primarily from the biomedical domain, but many more tools were inte-
grated as part of the META-NET project (Thompson et al., 2011). This makes the collection accessible
through U-Compare one of the largest collections of interoperable NLP components available.
ClearTK (Ogren et al., 2009) is actually a machine-learning framework based on Apache UIMA.
However, it also integrates various NLP tools from different vendors and for this reason we list it under
the broad-coverage collections. The tools are integrated to provide features for the machine-learning
algorithms. The main reason for ClearTK not to use components from other existing UIMA component
collections may have been the lack of a comprehensive UIMA component collection for NLP at the time
ClearTK was in its early stages.
Note on cross-collection interoperability An alternative to broad-coverage collections that integrate
many tools and make them interoperable would be to achieve cross-collection interoperability. That
means, many vendors would provide small collections or even individual components and the end users
would combine them into pipelines as desired. However, even within a framework like UIMA or GATE,
a) some conventions, like a common type system, would need to be respected, b) extensive mapping
between the individual components would be required, or c) the components would need to be adaptable
to arbitrary type systems through configuration. Until at least one of these points has been resolved in
a user-friendly way, we consider broad-coverage collections to be the most convenient solution for the
user. The insights gained in building the broad-coverage collections may eventually contribute to finding
solutions for these problems.
2.4 Resource selection and acquisition
Many NLP tools are generic, language-independent engines that are parametrized for a particular lan-
guage with a resource, e.g. a probabilistic model, a set of rules, or another knowledge base. We call this
resource selection. The selection can happen manually or automatically.
Manual selection is required, for example, in ClearTK or GATE. Components that require a resource
offer a parameter pointing to the location from where this resource can be loaded, typically a location on
the local file system. This entails that the resource is either bundled with the component or that the user
must find and download the resource to the local machine. We call this step resource acquisition.
U-Compare (Kano et al., 2011), on the other hand, offers some components preconfigured with re-
sources for certain languages. In particular, components that call out to remote web-services tend to
support multiple languages. Based on the language they are invoked for, the service employs a particular
resource. However, in this case the users cannot invoke the service with a custom resource from their
local machine. Portable components that are bundled with U-Compare also allow for custom resources.
2.5 Need for shareable pipelines based on portable components
Current workflow descriptions are inconvenient to share with others because they are not self-contained.
They do not uniquely identify components and resources. The responsibility to obtain, and install com-
ponents and resources is largely left to the user. Web-based workbenches and marketplaces provide some
remedy in this aspect as they remove the need for any local installation by the user. However, such online
service-based approaches have been found to be a cause of workflow decay (G?omez-P?erez et al., 2013).
In consequence, we find that a shareable pipeline should rely on portable software and resources that
can be automatically obtained from a repository. Once obtained, these remain within the control of the
user, e.g. to create backups, or to run them on alternative environments, such as a private compute cluster.
In the latter case, the use of remote services would likely cause a performance bottleneck. To make such
an approach to shareable pipelines attractive, it must be supported by a broad-coverage collection from
which pipelines can be assembled for various tasks.
5
3 Contributions
3.1 Automatic selection and acquisition of resources
We present a novel approach to the configuration of components with resources based on the data being
processed. Resources are stored in a repository from where a component can obtain them on demand.
The approach is based on a set of coordinates to address resources: tool, language, variant, and version.
In many cases, this removes the need for the user to explicitly configure the resource to be used.
By overriding specific coordinates (mainly variant), the user can choose between different resources.
Additionally, the user can disable resource-resolution via coordinates and instruct the component to use
a model at a specific location, e.g. to use custom model from the local file system.
As an example, consider a part-of-speech-tagger component being used to process English text:
? tool ? this coordinate is uniquely identified by the component being used, e.g. opennlp-tagger.
? language ? this coordinate is obtained from the data being processed by the tagger, e.g. en or de.
? variant ? as there can be multiple applicable resources per language, this coordinate is used to
choose one of the resources. A default variant is provided by the component, possibly a different
variant depending on the language, e.g. fast or accurate.
? version ? resources are versioned, just as components are. New versions of a resource are created
to fix bad data, to extend the data on which the resource is based, or to make it compatible with a
new version of a tool. We note that generally, the versioning of tools and resources is independent
of each other: a resource may be compatible with multiple versions of a tool and multiple versions
of a resource may be compatible with one specific version of a tool. Furthermore, some vendors
do not version resources properly or at all. For example, by comparing hash values, we observed
that from version to version only some of the models packaged with CoreNLP change, while others
remain identical. We also found the models (and even binaries) of TreeTagger (Schmid, 1994) to
change from time to time without any apparent change in version. As a consequence, we decided
to consequently use a time-based versioning scheme for resources. The independence between tool
and resource versions also has another effect: users find it hard to manually select a resource version
compatible with the tool version they use. Thus, we maintain a list of resources and default versions
with each component and use it to fill in the version coordinate.
To operationalize this concept, we translate these coordinates into Maven coordinates and use
these to resolve the resource against the Maven repository infrastructure. For example the coordi-
nates [tool: opennlp-tagger, language: en, variant: maxent, version: 20120616.1] would be trans-
lated into [groupId: de.tudarmstadt.ukp.dkpro.core, artifactId: de.tudarmstadt.ukp.dkpro.core.opennlp-
model-tagger-en-maxent, version: 20120616.1].
Mind that some vendors already distribute resources for their tools via Maven repositories (cf. Sec-
tion 2.2), but they do so at their own coordinates, e.g. at [groupId: com.clearnlp, artifactId: clearnlp-
general-en-dep, version: 1.2] and these resources can become of a significant size.
1
To avoid repub-
lishing resources at coordinates matching our naming scheme, the artifact at the translated coordinates
serves only as a proxy that does not contain the resource itself. Instead, it contains a redirection to the
artifact containing the actual resource. This allows us to maintain a common coordinate scheme for all
resources while being able to incorporate existing third-party resources. It also allows us to maintain
additional metadata, e.g. about tagsets. When vendors do not distribute their resources via Maven, we
package them and distribute them via our own public repository ? if their license does not prohibit this.
3.2 The DKPro Core broad-coverage component collection or portable components
We presentDKPro Core, a broad-coverage collection of NLP components based on the UIMA processing
framework. Our collection relies only on portable software and resources and it is distributed via the
Maven repository infrastructure. It also served as a use-case and test-bed for the development of our
resource selection mechanism (Section 3.1). DKPro Core is provided as open-source software.
2
1
For example, the ClearNLP dependency parser model for general English (version 1.2) has about 721 MB.
2
https://code.google.com/p/dkpro-core-asl/
6
Task Components Languages
Language identification 2 de, en, es, fr, +65
Tokenization and sentence boundary detection 5 de, en, es, fr, +25
Lemmatization 7 de, en
Stemming 1 de, en, es, fr, +11
Part-of-speech tagging 9 de, en, es, fr, +14
Morphological analysis 2 de, en, fr, it, +1
Named entity recognition 2 de, en, es, nl
Chunking 1 en
Constituency parsing 3 de, en, fr, zh, +1
Dependency parsing 5 de, en, es, fr, +7
Coreference analysis 1 en
Semantic role labelling 1 en
Spell checking and grammar checking 3 de, en, es, fr, +25
Figure 1: Analysis tasks covered by the DKPro Core component collection
The collection targets users with a strong interest in the ability to programmatically assemble pipelines,
e.g. as part of dynamic scientific experiments or within NLP-enabled applications. For this reason,
our collection employs the Apache uimaFIT library (Apache UIMA Community, 2013) to allow the
implementation of pipelines with only a few lines of code (cf. Section 3.3).
Table 1 provides an overview over the analysis tasks currently covered by the collection.
3
Addi-
tionally, our collection provides diverse input/output modules that support different file formats ranging
from simple text, over various corpus formats (CoNLL, TIGER-XML, BNC-XML, TCF, etc.), to tool-
specific formats (IMSOpen CorpusWorkbench (Evert and Hardie, 2011), TGrep2 (Rohde, 2005), several
UIMA-specific formats, etc.). These enable the processing of corpora from many sources and the further
processing of results with specialized tools.
We primarily integrate third-party tools with the UIMA framework and include only few original com-
ponents, mainly for reading and writing the different supported data formats. Our work focusses on the
concerns related to interoperability and usability, such as the resource selection mechanism (Section 3.1).
It is our policy to integrate only third-party tools that are properly versioned and that are distributed via
the Central Repository, generally including their full source code.
4
As a considerable portion of the tools
we integrate do not initially meet this requirement, we regularly reach out to the respective communities
and either help them publishing their tools the Central Repository or offer to do so on their behalf. The
DKPro Core components themselves are also distributed via the Central Repository.
3.3 Self-contained executable pipeline example
We define a self-contained pipeline description as uniquely identifying all required components and
resources. Assuming that the results of the pipeline are fully defined by these and by the input data,
such a self-contained pipeline should allow for reproducible results. In particular, the results must not
influenced by the platform the pipeline is run on.
We take a step further making self-contained pipelines also convenient for the users by removing
the need to manually obtain and install the required components and resources. To do so, we rely on
a generic bootstrapping mechanism which is capable of extracting the information about the required
artifacts from the pipeline description and of obtaining them automatically from a repository.
We achieve this goal most illustratively through a combination of these ingredients: our auto-
configuration mechanism (Section 3.1) which removes the need for explicit configuration and which
identifies and fetches the required resources from the repository at runtime; our component collection
(Section 3.2) that is published through a Maven repository; Groovy (2014) and its Grape
5
subsystem
serving as a bootstrapping mechanism to fetch the components from the repository; uimaFIT providing
a concise way of assembling a pipeline of UIMA components in Groovy and making it executable.
Listing 1 demonstrates such a self-contained and executable pipeline. The example consists of three
3
Unfortunately, we cannot give a full account of the actually integrated third-party tools here, due to the lack of space.
4
An exception to this rule are tools that need to be integrated as binaries because they are not implemented in Java.
5
Groovy Adaptable Packaging Engine: http://groovy.codehaus.org/Grape
7
Listing 1: Executable pipeline implemented in Groovy
1 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,
2 module=?de.tudarmstadt.ukp.dkpro.core.textcat-asl?, version=?1.6.1?)
3 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,
4 module=?de.tudarmstadt.ukp.dkpro.core.stanfordnlp-gpl?, version=?1.6.1?)
5 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,
6 module=?de.tudarmstadt.ukp.dkpro.core.maltparser-asl?, version=?1.6.1?)
7 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,
8 module=?de.tudarmstadt.ukp.dkpro.core.io.text-asl?, version=?1.6.1?)
9 @Grab(group=?de.tudarmstadt.ukp.dkpro.core?,
10 module=?de.tudarmstadt.ukp.dkpro.core.io.conll-asl?, version=?1.6.1?)
11
12 import de.tudarmstadt.ukp.dkpro.core.textcat.
*
;
13 import de.tudarmstadt.ukp.dkpro.core.stanfordnlp.
*
;
14 import de.tudarmstadt.ukp.dkpro.core.maltparser.
*
;
15 import de.tudarmstadt.ukp.dkpro.core.io.text.
*
;
16 import de.tudarmstadt.ukp.dkpro.core.io.conll.
*
;
17 import static org.apache.uima.fit.factory.AnalysisEngineFactory.
*
;
18 import static org.apache.uima.fit.factory.CollectionReaderFactory.
*
;
19 import static org.apache.uima.fit.pipeline.SimplePipeline.
*
;
20
21 runPipeline(
22 createReaderDescription(TextReader,
23 TextReader.PARAM_SOURCE_LOCATION, args[0]),
24 createEngineDescription(LanguageIdentifier),
25 createEngineDescription(StanfordSegmenter),
26 createEngineDescription(StanfordPosTagger),
27 createEngineDescription(MaltParser),
28 createEngineDescription(Conll2006Writer,
29 Conll2006Writer.PARAM_TARGET_LOCATION, args[1]));
sections. Lines 1-10 identify the components used in the pipeline by name and version. Lines 12-19
are necessary boilerplate code making the components accessible within the Groovy script. Lines 21-29
employ uimaFIT to assemble and run a pipeline consisting of components from our collection.
When the Groovy script representing the pipeline is executed, it downloads all required artifacts.
Afterwards, these artifacts remain on the user?s system and they can be used again for a subsequent
execution of the script. The user may also create backups of these artifacts or transfer them to a different
system. Thus, in contrast to pipelines that rely on online services, our approach allows the user to
maintain control over the involved software and resources.
The example pipeline given in Listing 1 can indeed be run on any computer that has Groovy installed.
It is a life example of a self-contained NLP pipeline shared as part of a scientific publication. By means
of this example, we demonstrate that we have reached our goal of providing a concept for shareable
pipelines based on portable components and resources.
Due to its conciseness, we consider the Groovy script to provide the most illustrative example of the
benefits provided by our contributions. However, there are alternative ways to operationalize our con-
cepts. Alternatively we could use a Jython (2014) script and jip
6
to resolve the Maven dependencies, Java
and Maven, or a variety of other JVM-based languages and build tools supporting Maven repositories.
4 Applications
The DKPro Core collection has already been successfully used for linguistic pre-processing in various
tasks, including, but not limited to, temporal tagging (Str?otgen and Gertz, 2010), text segmentation based
on topic models (Riedl and Biemann, 2012), and textual entailment (Noh and Pad?o, 2013).
The portable components and resources from our collection can be integrated into online workbenches
and can be run on cloud platforms by users that find this convenient. Combined with our concept for
executable pipelines, users can be enabled to export self-contained pipelines from such workbenches and
to archive them for later reproduction. Additionally, users can und run the pipelines on private hardware,
possibly on sensitive data which users do not feel comfortable submitting to the cloud. We believe that
service-based offerings should be based as much as possible on portable software, and we focussed in
this paper on improving the availability and convenience of using such portable NLP software. Thus, we
consider our approach not to be competing with service-based approaches but rather as complementing
them.
6
https://pypi.python.org/pypi/jip
8
Our concept of automatically selecting and acquiring resources can be immediately transferred to
other component collections. Although our component collection is based on UIMA, this aspect has
been implemented independent of the processing framework. Having experienced the convenience of-
fered by this concept, we believe that integrating a pluggable resource resolving mechanism directly into
processing frameworks such as GATE or UIMA would be beneficial. A pluggable mechanism would be
important because we expect that the underlying repository infrastructures and coordinate systems are
likely to evolve over time. For example, we could envisage an integrated resolving mechanism that al-
lows combining the rich metadata offered by directories such as META-SHARE or the Virtual Language
Observatory with the ability to automatically acquire software and resources offered by Maven or with
the ability of invoking NLP tools as services such as via AnnoMarket.
Our concept of rendering self-contained pipelines as executable scripts facilitates the sharing of
pipelines. This can be either only the script which then downloads its dependencies upon execution,
or the dependencies can be resolved beforehand and included with the script. The concise pipeline de-
scription is also useful for examples in teaching, in documentation, or on community platforms like Stack
Overflow.
7
We offer Groovy- and Jython-based quick-start examples for the DKPro Core collection to
new users.
5 Summary and future work
In this paper, we have presented a novel concept for implementing shareable NLP pipelines supported
by a broad-coverage collection of interoperable NLP components. Our approach is enabled by the com-
bination of distributing portable NLP components and resources through a repository infrastructure and
by an auto-configuration mechanism allowing components to select suitable resources at runtime and to
obtain them automatically from the repository.
We have demonstrated that our contributions enable a concise and self-contained pipeline description,
which can easily be shared, e.g. as examples in teaching, documentation, or publications. The reliance
on portable artifacts allow the user to maintain control, e.g. by creating backups of the involved artifacts
to reproduce results at a later time, even if the original repository may no longer be available.
In the future, we plan to investigate a mechanism to automatically detect misalignments between
resources and components within a pipeline to provide the user with an indication when suboptimal
results may occur and what may cause them. This is necessary because components in the collection
are interoperable at the level of annotation types, whereas tagsets and tokenization are simply passed
through. While this is a common approach, it leads to the situation that the results may be negatively
affected due to diverging tokenizations used while generate the resources for the components. Also, the
automatic resource selection mechanism may currently choose resources with incompatible tagsets, e.g.
a POS-tagger model producing tagset X while a subsequent dependency parser would require tagset Y .
We also plan to extend the resource selection process to support additional metadata. Eventually, the
variant coordinate should be replaced by a more fine-grained mechanism to select resources based e.g.
on the domain, tagset, or other characteristics.
Acknowledgements
The project was partially funded by means of the German Federal Ministry of Education and Research
(BMBF) under the promotional reference 01UG1110D, and partially by the Volkswagen Foundation as
part of the Lichtenberg-Professorship Program under grant No. I/82806. The authors take the responsi-
bility for the contents.
References
Apache UIMA Community. 2013. Apache uimaFIT guide and reference, version 2.0.0. Technical report, Apache
UIMA.
7
http://stackoverflow.com (Last accesses: 2014-02-14)
9
Ekaterina Buyko and Udo Hahn. 2008. Fully embedded type systems for the semantic annotation layer. In ICGL
2008 - Proceedings of First International Conference on Global Interoperability for Language Resources, pages
26?33, Hong Kong.
Central Repository. 2014. The Central Repository. URL http://search.maven.org (Last accessed:
2014-03-19), March. Sonatype Inc. (http://www.sonatype.org/central).
Christian Chiarcos, Stefanie Dipper, Michael G?otze, Ulf Leser, Anke L?udeling, Julia Ritz, and Manfred Stede.
2008. A flexible framework for integrating annotations from different tools and tagsets. Traitement Automatique
des Langues, 49(2):271?293.
ClearNLP. 2014. Version 2.0.2 - fast and robust NLP components implemented in Java. URL http:
//opennlp.apache.org (Last accessed: 2014-03-19), January.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002. GATE: an architecture for
development of robust HLT applications. In Proceedings of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 168?175, Philadelphia, Pennsylvania, USA, July. Association for Computational
Linguistics.
Stefan Evert and Andrew Hardie. 2011. Twenty-first century corpus workbench: Updating a query architecture
for the new millennium. In Proceedings of the Corpus Linguistics 2011 conference, Birmingham, UK, July.
University of Birmingham.
David Ferrucci and Adam Lally. 2004. UIMA: an architectural approach to unstructured information processing
in the corporate research environment. Natural Language Engineering, 10(3-4):327?348.
Antske Fokkens, Marieke van Erp, Marten Postma, Ted Pedersen, Piek Vossen, and Nuno Freire. 2013. Offspring
from reproduction problems: What replication failure teaches us. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1691?1701, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Carole A Goble, Jiten Bhagat, Sergejs Aleksejevs, Don Cruickshank, Danius Michaelides, David Newman, Mark
Borkum, Sean Bechhofer, Marco Roos, Peter Li, et al. 2010. myExperiment: a repository and social network
for the sharing of bioinformatics workflows. Nucleic acids research, 38(suppl 2):W677?W682.
Jos?e Manuel G?omez-P?erez, Esteban Garc?a-Cuesta, Jun Zhao, Aleix Garrido, Jos?e Enrique Ruiz, and Graham
Klyne. 2013. How reliable is your workflow: Monitoring decay in scholarly publications. In Proceedings
of the 3rd Workshop on Semantic Publishing (SePublica 2013) at 10th Extended Semantic Web Conference,
page 75, Montpellier, France, May.
Groovy. 2014. Version 2.2.2 - A dynamic language for the Java platform. URL http://groovy.codehaus.
org (Last accessed: 2014-03-19, February.
P?eter Hal?acsy, Andr?as Kornai, and Csaba Oravecz. 2007. Hunpos ? an open source trigram tagger. In Proceedings
of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings
of the Demo and Poster Sessions, pages 209?212, Prague, Czech Republic, June. Association for Computational
Linguistics.
Marie Hinrichs, Thomas Zastrow, and Erhard Hinrichs. 2010. WebLicht: Web-based LRT Services in a Distributed
eScience Infrastructure. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10), pages 489?493, Valletta, Malta, May. European Language
Resources Association (ELRA).
Jython. 2014. Jython: Python for the Java Platform. URL http://www.jython.org (Last accessed:
2014-03-19.
Yoshinobu Kano, Makoto Miwa, Kevin Bretonnel Cohen, Lawrence E. Hunter, Sophia Ananiadou, and Jun?ichi
Tsujii. 2011. U-Compare: A modular NLP workflow construction and evaluation system. IBM Journal of
Research and Development, 55(3):11:1?11:10, May.
Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014.
The stanford corenlp natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the As-
sociation for Computational Linguistics: System Demonstrations, pages 55?60, Baltimore, Maryland, June.
Association for Computational Linguistics.
10
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using UIMA to structure an open platform for textual entailment. In
Peter Kl?ugl, Richard Eckart de Castilho, and Katrin Tomanek, editors, Proceedings of the 3rd Workshop on
Unstructured Information Management Architecture (UIMA@GSCL 2013), pages 26?33, Darmstadt, Germany,
Sep. CEUR-WS.org.
Philip V. Ogren, Philipp G. Wetzler, and Steven J. Bethard. 2009. ClearTK: a framework for statistical natural
language processing. In Christian Chiarcos, Richard Eckart de Castilho, and Manfred Stede, editors, Proceed-
ings of the Biennial GSCL Conference 2009, 2nd UIMA@GSCL Workshop, pages 241?248, Potsdam, Germany,
September. Gunter Narr Verlag.
Rafal Rak, Andrew Rowley, Jacob Carter, and Sophia Ananiadou. 2013. Development and analysis of nlp
pipelines in argo. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics:
System Demonstrations, pages 115?120, Sofia, Bulgaria, August. Association for Computational Linguistics.
Martin Riedl and Chris Biemann. 2012. Text segmentation with topic models. JLCL, 27(1):47?69.
Douglas LT Rohde. 2005. Tgrep2 user manual version 1.15. Massachusetts Institute of Technology. http://ted-
lab.mit.edu/dr/Tgrep2.
Guergana K. Savova, James J. Masanz, Philip V. Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C. Kipper-Schuler,
and Christopher G. Chute. 2010. Mayo clinical text analysis and knowledge extraction system (cTAKES):
architecture, component evaluation and applications. Journal of the American Medical Informatics Association,
17(5):507?513.
Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International
Conference on New Methods in Language Processing, pages 44?49, Manchester, UK.
Sonatype Company. 2008. Maven: The Definitive Guide. O?Reilly Media, September. ISBN: 9780596517335.
Jannik Str?otgen and Michael Gertz. 2010. HeidelTime: High Quality Rule-Based Extraction and Normalization
of Temporal Expressions. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages
321?324, Uppsala, Sweden, July. Association for Computational Linguistics.
Valentin Tablan, Kalina Bontcheva, Ian Roberts, Hamish Cunningham, and Marin Dimitrov. 2013. Annomarket:
An open cloud platform for nlp. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 19?24, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Paul Thompson, Yoshinobu Kano, John McNaught, Steve Pettifer, Teresa Attwood, John Keane, and Sophia Ana-
niadou. 2011. Promoting interoperability of resources in meta-share. In Proceedings of the Workshop on
Language Resources, Technology and Services in the Sharing Paradigm, pages 50?58, Chiang Mai, Thailand,
November. Asian Federation of Natural Language Processing.
Dieter Van Uytvanck, Claus Zinn, Daan Broeder, Peter Wittenburg, and Mariano Gardellini. 2010. Virtual Lan-
guage Observatory: The portal to the language resources and technology universe. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, edi-
tors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC?10),
pages 900?903, Valletta, Malta, may. European Language Resources Association (ELRA).
Stephen Wu, Vinod Kaggal, Dmitriy Dligach, James Masanz, Pei Chen, Lee Becker, Wendy Chapman, Guergana
Savova, Hongfang Liu, and Christopher Chute. 2013. A common type system for clinical natural language
processing. Journal of Biomedical Semantics, 4(1):1.
11
